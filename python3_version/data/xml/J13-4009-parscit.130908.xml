<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998995333333333">
Generation of Compound Words in
Statistical Machine Translation into
Compounding Languages
</title>
<author confidence="0.995068">
Sara Stymne
</author>
<affiliation confidence="0.984386">
Uppsala University*
</affiliation>
<author confidence="0.957648">
Nicola Cancedda
</author>
<affiliation confidence="0.880392">
Xerox Research Centre Europe**
</affiliation>
<author confidence="0.776634">
Lars Ahrenberg
</author>
<affiliation confidence="0.611706">
Link¨oping University†
</affiliation>
<bodyText confidence="0.992550466666667">
In this article we investigate statistical machine translation (SMT) into Germanic languages,
with a focus on compound processing. Our main goal is to enable the generation of novel
compounds that have not been seen in the training data. We adopt a split-merge strategy, where
compounds are split before training the SMT system, and merged after the translation step.
This approach reduces sparsity in the training data, but runs the risk of placing translations of
compound parts in non-consecutive positions. It also requires a postprocessing step of compound
merging, where compounds are reconstructed in the translation output. We present a method for
increasing the chances that components that should be merged are translated into contiguous
positions and in the right order and show that it can lead to improvements both by direct
inspection and in terms of standard translation evaluation metrics. We also propose several new
methods for compound merging, based on heuristics and machine learning, which outperform
previously suggested algorithms. These methods can produce novel compounds and a translation
with at least the same overall quality as the baseline. For all subtasks we show that it is
useful to include part-of-speech based information in the translation process, in order to handle
compounds.
</bodyText>
<sectionHeader confidence="0.996872" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.987652">
In many languages including most of the Germanic (German, Swedish, etc.) and Uralic
(Finnish, Hungarian, etc.) language families, so-called closed compounds are used
</bodyText>
<note confidence="0.738749">
* Department of Linguistics and Philology, Uppsala University, Box 635, 751 26 Uppsala, Sweden.
</note>
<email confidence="0.945181">
E-mail: sara.stymne@lingfil.uu.se.
</email>
<affiliation confidence="0.721073">
** Xerox Research Centre Europe, 6 chemin de Maupertuis, 38240 Meylan, France.
</affiliation>
<email confidence="0.917797">
E-mail: nicola.cancedda@xrce.xerox.com.
</email>
<affiliation confidence="0.899745">
† Department of Computer and Information Science, Link¨oping University, 58183 Link¨oping, Sweden.
</affiliation>
<email confidence="0.989232">
E-mail: lars.ahrenberg@liu.se.
</email>
<note confidence="0.9061802">
Submission received: 25 April 2012; revised submission received: 30 November 2012; accepted for
publication: 8 January 2013.
doi:10.1162/COLI a 00162
© 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.999385125">
productively. Closed compounds are written as single words without spaces or other
word boundaries, as in German Goldring. We will refer to these languages as compound-
ing languages. In English, on the other hand, compounds are generally open, that is,
written as two words as in gold ring.
This difference in compound orthography leads to problems for statistical ma-
chine translation (SMT). For translation into a compounding language, often fewer
compounds than in normal texts are produced. This can be due to the fact that the
desired compounds are missing in the training data or that they have not been aligned
correctly. When a compound is the idiomatic word choice in the translation, systems
often produce separate words, genitive or other alternative constructions, or translate
only one part of the compound. For an SMT system to cope with the productivity of the
phenomenon, any effective strategy should be able to correctly process compounds that
have never been seen in the training data as such, although possibly their components
have, either in isolation or within a different compound.
Previous work (e.g., Koehn and Knight 2003) has shown that compound splitting
improves translation from compounding languages into English. In this article we ex-
plore several aspects of the less-researched area of compound treatment for translation
into such languages, using three Germanic languages (German, Swedish, and Danish)
as examples.1 The assumption is that splitting compounds will also improve translation
for this translation direction and lead to more natural translations. The strategy we
adopt is to split compounds in the training data, and to merge them in the translation
output. Our overall goal is to improve translation quality by productively generating
compounds in SMT systems.
The main contributions of the article are as follows:
</bodyText>
<listItem confidence="0.997371">
• Demonstrating improved coalescence (adjacency and order) of compound
parts in translation through the use of sequence models based on
customized part-of-speech sets and count features
• Designing and evaluating several heuristic methods for compound
merging that outperforms previous heuristic merging methods
• Designing and evaluating a novel method for compound merging based
on sequence labeling
• Demonstrating the ability of these merging methods to generate novel
unseen compounds
</listItem>
<bodyText confidence="0.999819444444444">
In addition, we report effects on translation performance from a number of variations
in the methods for compound splitting and merging.
The rest of the article is structured as follows. Section 2 gives an overview of
compound formation in the three target languages used in this work. Section 3 reports
related work on compound processing for machine translation. Section 4 describes the
compound processing strategy we use and Section 5 describes compound splitting.
Section 6 addresses compound coalescence, followed by compound merging in Sec-
tion 7. In Section 8 we present experimental results and in Section 9 we state our
conclusions.
</bodyText>
<note confidence="0.907577">
1 This work is a synthesis and extension of Stymne (2008); Stymne and Holmqvist (2008); Stymne,
Holmqvist, and Ahrenberg (2008); Stymne (2009); and Stymne and Cancedda (2011).
</note>
<page confidence="0.980155">
1068
</page>
<note confidence="0.930495">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<sectionHeader confidence="0.838777" genericHeader="categories and subject descriptors">
2. Closed Compounds in German, Swedish, and Danish
</sectionHeader>
<bodyText confidence="0.994079846153846">
Compounds in German, Swedish, and Danish are generally closed, written without
word boundaries, as exemplified for German in Example (1). Compounds can be made
up of two (1a) or more (1b) parts where parts may also be coordinated (1c). In a few
cases compounds are written with a hyphen (1d), often when one of the parts is a
proper name or an abbreviation. Most compounds are nouns (1a–1e), but they can
also be adjectives (1f), verbs (1g), and adverbs (1h). English translations of compounds
can be written as open compounds with separate words (1a) or with hyphens (1f), as
other constructions, possibly with inserted function words (1g) and reordering (1b),
or as single words (1e). Generally the last part of the compound is the compound
head, that is, it conveys the main meaning of the compound, and determines its part
of speech. The other parts, compound modifiers, modify the meaning of the com-
pound head in some way and need not have the same part of speech as the full
compound.
</bodyText>
<listItem confidence="0.7140364375">
(1) a. Regierungskonferenz intergovernmental conference
Regierung+Konferenz government conference
b. Friedensnobelpreistr¨ager Nobel Peace Prize laureate
Frieden+Nobel+Preis+Tr¨ager peace Nobel prize bearer
c. See- und Binnenh¨afen sea and inland ports
See- und Binnen+H¨afen sea and interior ports
d. EU-Mitgliedstaaten EU member states
EU-Mitglied+Staaten EU member states
e. Jahrtausend millennium
Jahr+tausend year thousand
f. dunkelblau dark-blue
dunkel+blau dark blue
g. kennenlernen get to know
kennen+lernen know learn
h. gr¨osstenteils in most instances
gr¨ossten+teils largest partly
</listItem>
<bodyText confidence="0.9993674">
Compound modifiers often have a special form, such as the addition of an “s”
to the base form of Regierung in Example (1a). We will refer to these form variants
as compounding forms. Table 1 exemplifies the type of operations used in Swedish,
German, and Danish to form compounding forms. There are many more alternative
compounding forms in Swedish and German than in Danish. For an overview of
the possible compounding forms in German see Langer (1998) or K¨urschner (2003),
in Danish see K¨urschner (2003), and in Swedish see Thorell (1981) or Stymne and
Holmqvist (2008). Some compounding forms coincide with paradigmatic forms, such
as German Jahres that can also be genitive, and Stadien that can also be plural, from
Table 1. There are different views on whether to treat these forms as paradigmatic forms
or as compounding forms. We follow Langer (1998) in viewing them as compounding
forms, because they often do not correspond to plural or possessive semantics. Many
individual compound modifiers have more than one possible compounding form. In
Example (2) we provide examples of several possible forms of the modifier Kind (child)
in German compounds.
</bodyText>
<page confidence="0.989181">
1069
</page>
<note confidence="0.504511">
Computational Linguistics Volume 39, Number 4
</note>
<listItem confidence="0.6877226">
(2) 0 Kind+phase (child-caring period)
+s Kinds+lage (fetal position)
+es Kindes+unterhalt (child support)
+er Kinder+film (children’s film)
+- Ein-Kind-Politik (one-child policy)
</listItem>
<bodyText confidence="0.998586714285714">
In some cases concatenating two words would lead to three identical consecutive
consonants. In the Scandinavian languages, there is a spelling rule that does not allow
this, and three identical consonants are reduced to two, as in Example (3SV). This spelling
rule was also used for some German compounds before 1996, when it was changed by
a spelling reform, so that nowadays three identical consecutive consonants are never
reduced to two at compound boundaries in German (Institut f¨ur Deutsche Sprache
1998), as shown in Example (3DE).
</bodyText>
<tableCaption confidence="0.978306">
Table 1
</tableCaption>
<figure confidence="0.972541">
Operations used for forming compounding forms with examples.
Type Examples
Null operation
Addition
Deletion
umweltfreundlich (environmentally-friendly)
DE 0
Umwelt+freundlich (environment friendly)
naturkatastrof (natural disaster)
SV 0
natur+katastrof (nature disaster)
h˚andbagage (hand luggage)
DA 0
h˚and+bagage (hand luggage)
Jahreswechsel (turn of the year)
DE +es
Jahr+Wechsel (year change)
kvalitetstecken (quality mark)
SV +s
kvalitet+tecken(quality sign)
spillekonsol (game console)
DA +e
spill+konsol(game console)
Lymphreaktion (lymphatic response)
DE -e Lymphe+Reaktion (lymph response)
flickskola (girls’ school)
SV -a
flicka+skola (girl school)
Stadienexperte(stadium expert)
DE -on/+en
Stadion+Experte (stadium expert)
arbetsolycka (industrial accident)
Combination SV -e/+s
arbete+olycka (work accident)
DA -e/+s embedsmand (civil servant)
embede+mand(job man)
</figure>
<note confidence="0.570895">
Umlaut V¨olkerrecht(international law)
DE ”+er
Volk+Recht(people right)
br¨odrak¨arlek (brotherly love)
SV ”-er/+ra
broder+k¨arlek (brother love)
børnesko (children’s shoe)
DA ”+e
barn+sko (child shoe)
</note>
<page confidence="0.962292">
1070
</page>
<note confidence="0.905247">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<figure confidence="0.45778075">
(3) SV tullagstiftning customs legislation
tull+lagstiftning custom legislation
DE Zelllinie cell line
Zell+Linie cell line
</figure>
<bodyText confidence="0.998912692307692">
Compounding is common and productive; new compounds can be readily formed
and understood. This is confirmed in a number of corpus studies. In German, com-
pounds have been shown to make up 5–7% of tokens and 43–47% of types in news
text (Baroni, Matiasek, and Trost 2002; Schiller 2005). If function words are removed,
an even higher number of the tokens are compounds; in both Swedish and German
10% of the content words in a news text have been found to be compounds (Hedlund
2002). That compounding is productive means that it is likely that a high number of
compounds have a very low frequency in texts. Baroni, Matiasek, and Trost (2002) found
that 83% of the compounds in a large German news corpus occur less than five times.
In Swedish, compounds are the most common type of hapax words, that is, words that
occur only once in a text (Carlberger et al. 2005). The most common type of compound
is the noun+noun compound, which makes up 62% of the compounds in the German
news corpus of Baroni, Matiasek, and Trost.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
3. Related Work
</sectionHeader>
<bodyText confidence="0.99997275862069">
The problems arising from differences in compounding strategies in translation from
German into English have been addressed by several authors. The most common archi-
tecture for translation from German is to split compounds in a preprocessing step prior
to training and translation using some automatic method, which has been suggested
both for SMT (Nießen and Ney 2000; Koehn and Knight 2003; Popovi´c, Stein, and Ney
2006; Holmqvist, Stymne, and Ahrenberg 2007) and example-based MT (Brown 2002).
German compounds are split into their component parts in a preprocessing step and the
translation model is then trained between modified German and English. At translation
time, the German source text is also run through a compound splitter. In the studies
cited here, only one splitting option is given as input to the decoder, which can be
problematic in case the splitting is wrong, or if any of the parts are unknown. In Dyer
(2009) several splitting options were given to the decoder in the form of a lattice. It
is, however, not straightforward to use lattices during training, and in order to solve
this, the training corpus was doubled, one part being without splits and the other part
having the best splitting option for each word.
For translation into German, Popovi´c, Stein, and Ney (2006) investigated three
different strategies for compound processing. The first was to split compounds dur-
ing training and after translation merge compound parts back into full compounds,
the second merged English compounds prior to training instead of splitting German
compounds, and the third used compound splitting only to improve word alignment.
The split–merge strategy gave the best results, but using splitting only for alignment
gave similar results. The merging of English compounds led to an improvement over a
baseline without compound processing, but was not as good as the other two strategies.
Popovi´c, Stein, and Ney also presented one of few previous suggestions for compound
merging. Each word in the translation output was looked up in a list of compound parts,
and merged with the next word if it resulted in a known compound. This method led
to improved overall translation results from English to German. The drawback of this
method is that novel compounds cannot be merged. It might also merge words that
should not be merged, but that happen to coincide with known compounds.
</bodyText>
<page confidence="0.99325">
1071
</page>
<note confidence="0.794334">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.99957368">
Fraser (2009) merged split German compounds after translation from English, by
applying a second phrase-based SMT (PBSMT) system trained on German with split
compounds and normal German. No separate results were presented for this extension
alone, but in combination with other morphological processing the strategy led to worse
results than the baseline. This method also suffers from the same drawbacks as that of
Popovi´c, Stein, and Ney (2006), that novel compounds cannot be merged and words
that should not be merged can still be.
Koehn, Arun, and Hoang (2008) discussed the treatment of hyphenated compounds
for translation into German. They used a separate mark-up token for hyphened com-
pounds, where the hyphen was split into a separate token, and marked by a symbol.
The impact on the translation result was small.
Botha, Dyer, and Blunsom (2012) discussed the approach of using customized
language models to target German compounds. They presented hierarchical Pitman-
Yor language models, where the compound head is conditioned on the words pre-
ceding the full compound, and the compound modifiers are modeled by a reverse
compound language model. They used this model as a replacement of a standard
language model for SMT and found that although the perplexity of the language model
was reduced, there were only minor improvements on Bleu for the SMT task. In this
case the SMT pipeline was left unchanged, meaning that novel compounds were not
considered.
Compound merging has also been performed for speech recognition. An example
of this is Berton, Fetter, and Regel-Brietzmann (1996), who extended the word graphs
output by a German speech recognizer with possible compounds by combining edges
of words during a lexical search. The final hypotheses were then identified from the
graph using dynamic programming techniques. Compound merging for speech recog-
nition is a somewhat different problem than for machine translation, however, because
coalescence is not an issue, as compared with SMT, where there is no guarantee that the
order of the parts in the translation output is correct.
Another somewhat related problem to compound merging is that of detection of er-
roneously split compounds in human text, which is faced by grammar checkers. Writing
compounds as separate words, with spaces between parts, is a common writing error
in compounding languages. Carlberger et al. (2005) described a system for Swedish that
used handwritten rules to identify, among other errors, erroneously split compounds.
The rules used parts of speech and morphological features. On a classified gold standard
of writing errors they had a recall of 46% and a precision of 39% for identifying split
compounds, indicating that it is a difficult problem to find split compounds in free,
unmarked text.
There is also work on morphological merging, which is needed when the target
words have been split into morphs in the training corpus. Virpioja et al. (2007) marked
morphs with a symbol and merged all marked words with the next word for translation
between Finnish, Swedish, and Danish, without showing any improvements over an
unmarked baseline. This strategy does have the advantage of being able to merge novel
word forms, but has a drawback in that it can merge parts into non-words if the parts
are misplaced in the translation output.
El-Kahlout and Oflazer (2006) used a similar symbol-based merging strategy for
translation from English into Turkish, but with the addition of morphographemic rules.
They had positive results when performing limited splitting and grouping the split
morphs, but not when splitting all morphs. They reported that there were problems
with the order of morphs in the output. Badr, Zbib, and Glass (2008) reported results for
translation from English to Arabic, where they used a combination merging method
</bodyText>
<page confidence="0.981501">
1072
</page>
<note confidence="0.522097">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<bodyText confidence="0.999961317073171">
where forms were picked from the corpus for known combinations of morphs and
words, and generated based on handwritten recombination rules otherwise, which
led to improvements over the baseline. They also used the morphs+POS as factors
in a factored translation model (Koehn and Hoang 2007) where surface forms were
generated from this information; this gave a small improvement on a large corpus, but
at the cost of high runtime. El Kholy and Habash (2010) extended the merging scheme of
Badr, Zbib, and Glass (2008) by using the conditional probability and a language model
score to pick the best known merging option. They showed a small effect of this on an
MT task, even though this strategy was the best option in an intrinsic evaluation based
on human reference translations.
Compound splitting has been addressed in many articles, as a separate task (Schiller
2005) or targeted for applications such as information retrieval (Holz and Biemann
2008), speech recognition (Larson et al. 2000), grammar checking (Sj¨obergh and Kann
2004), lexicon acquisition (Kokkinakis 2001), word prediction (Baroni, Matiasek, and
Trost 2002), and machine translation (Koehn and Knight 2003; Dyer 2009; Fritzinger
and Fraser 2010; Macherey et al. 2011).
The most successful strategies that address compound processing for MT apply
compound splitting as a preprocessing step before training the translation models.
Koehn and Knight (2003) presented an empirical splitting algorithm targeted at SMT
from German to English. They split words in all possible places, and considered a
splitting option valid if all its parts had been seen as words in a monolingual corpus.
They allowed the addition of -s or -es at all splitting points. If there were several valid
splitting options they chose one based on the number of splits, the geometric mean of
part frequencies, or based on alignment data. They evaluated the splitting algorithms
intrinsically on a gold standard of manually split noun phrases and on machine trans-
lation of noun phrases. The best results for PBSMT were achieved by using either the
geometric mean, or the highest number of splits. There were no correlations between
translation results and the intrinsic evaluation.
Several other researchers have also explored compound splitting for translation
from German to English. Nießen and Ney (2000) used a morpho-syntactic analyzer
for splitting German compounds prior to translation. Popovi´c, Stein, and Ney (2006)
used the geometric mean version from Koehn and Knight (2003) as well as the mor-
phosyntactic algorithm from Nießen and Ney (2000) for splitting, with similar posi-
tive results for both options. Fritzinger and Fraser (2010) combined linguistic analysis
with corpus-driven scoring and showed an improvement compared to using only a
corpus-driven approach. Macherey et al. (2011) described a corpus-driven method that
learns compounding form transformations of a language in addition to just compound
splitting, and showed an improvement for translation from several languages into
English compared to a baseline without compound treatment. Dyer (2010) suggested
a compound splitting method based on sequence labeling, which gave good results for
lattice-based translation from German.
</bodyText>
<sectionHeader confidence="0.955865" genericHeader="method">
4. Compound Translation
</sectionHeader>
<bodyText confidence="0.999333">
For translation into a compounding language, we adopt the compound processing
strategy suggested by Popovi´c, Stein, and Ney (2006). The process is:
</bodyText>
<listItem confidence="0.7852265">
1. Split compounds on the target (compounding language) side of the
training corpus.
</listItem>
<page confidence="0.968006">
1073
</page>
<bodyText confidence="0.124175">
Computational Linguistics Volume 39, Number 4
</bodyText>
<listItem confidence="0.9990058">
2. Learn a translation model from source (e.g., English) into
decomposed-target (e.g., decomposed-German).
3. At translation time, translate using the learned model from source into
decomposed-target.
4. Apply a postprocessing merge step to reconstruct compounds.
</listItem>
<bodyText confidence="0.999781">
The merging step must solve two problems: Identify which words should be merged
into compounds, and choose the correct compounding form for the compound mod-
ifiers. The first problem can become hopelessly difficult if the translation did not put
components nicely side by side and in the correct order. Preliminary to merging, then,
the problem of coalescence needs to be addressed, that is, translations where compound
elements are correctly positioned should be promoted.
Figure 1 gives an overview of the translation process with compound processing.
Compounds are split before training the translation system, and merged after transla-
tion. We use factored decoding (Koehn and Hoang 2007), where features other than just
surface words can be used by the system. In our case we tag the data with parts of
speech and use a factored model with POS-tags on the target side, which allows us to
have a POS-sequence model, beside the standard language model. This configuration
has a very small overhead compared with decoding without factors. As we show, using
part-of-speech tags on the target side helps to improve the coalescence of compound
parts, and can be used to guide the merging process.
For the tuning step there are two options: either we can split the development set
and tune with a translation with split compounds compared with a reference with split
</bodyText>
<figureCaption confidence="0.790158">
Figure 1
</figureCaption>
<bodyText confidence="0.850648">
The SMT system architecture.
</bodyText>
<page confidence="0.984181">
1074
</page>
<note confidence="0.523548">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<bodyText confidence="0.999913666666667">
compounds, or we can merge compounds in the translation output, before performing
the optimization. We found empirically that we got the best results using the second
approach, of performing compound merging during the tuning process. When we
tuned on split texts, the results were more unstable, and especially the number of words
in the translation output varied substantially. We thus use merging also during the
tuning process in all experiments, as shown in Figure 1.
</bodyText>
<sectionHeader confidence="0.925658" genericHeader="method">
5. Compound Splitting
</sectionHeader>
<bodyText confidence="0.994274222222222">
Our method for compound splitting is based on Koehn and Knight (2003) and Stymne
(2008). For each word all possible segmentations are explored, with the restrictions that
all parts must have at least three characters, and the last part, the compound head, must
have the same part-of-speech tag as the word itself. Hyphens are treated as additions
to compound modifiers, just as +s or +e. Segmentations are scored with the arithmetic
mean of frequencies for each part in the training corpus and the segmentation with the
highest score is chosen.
We have investigated several variants of the basic method and their effects on
compound translation. The variants investigated are:
</bodyText>
<listItem confidence="0.952788090909091">
• Using geometric or arithmetic mean for choosing the highest frequency
candidate split
• Restricting the length of each split part, either to three or four characters
• Restricting the highest number of parts per compound to two, or allowing
any number of compound parts
• Allowing all known compounding forms (c.f. Table 1), or restricting them
to the most common ones
• Restricting the last part of the compound to be known from the corpus
with the same part-of-speech tag as the full compound, or allowing all
possible parts-of-speech tags
6. Promoting Coalescence of Compounds
</listItem>
<bodyText confidence="0.9994356">
In this section we describe our approach to improve the coalescence of compounds,
which is based on POS-sequence models. We first present the representation schemes
we use for compounds, which form the basis of the sequence model approach, and then
describe the sequence modeling approach in more detail. Finally, we discuss how POS-
tags can be used for count features.
</bodyText>
<subsectionHeader confidence="0.999718">
6.1 Representation of Compound Parts
</subsectionHeader>
<bodyText confidence="0.99887">
As a result of compound splitting the segmentation of words is changed, and we know
from Table 1 that compounding forms often do not coincide with any forms that can be
used as standalone words. This raises several design decisions:
</bodyText>
<listItem confidence="0.744944">
1. Should alternative forms of the same compound modifier be normalized
to a canonical form?
</listItem>
<page confidence="0.954269">
1075
</page>
<figure confidence="0.301215">
Computational Linguistics Volume 39, Number 4
</figure>
<listItem confidence="0.973962">
2. Should compound modifiers be marked with a special symbol? Or should
the separation between compound parts be marked?
3. How should compound parts be tagged in a factored system?
</listItem>
<bodyText confidence="0.994985625">
Although apparently innocuous, these decisions do have some influence on the whole
process. In this work we have used three combinations of marking and normalization,
and three different tagsets.
In Example (4a), called the unmarked scheme, compound modifiers are normalized
(kamps-&gt;kamp) and the words carry no special marking. In Example (4b), called the
marked scheme, compound modifiers are not normalized, but they are marked with
the symbol “#”. For these two marking schemes, an extended tagset, EPOS, is used. It
contains tags for compound modifiers that also indicate the part of speech of the head
word, such as “N-modif” if the head is a noun (N) or “ADJ-modif” if the head is an
adjective (ADJ).
In Example (4c), called the sepmarked scheme, the split itself is marked, by using
the special token “@#@” to mark split points, and compound modifiers are normalized.
In this case we use a standard POS-tagset with the addition of a COMP-tag for the
inserted split-token, and use the POS-tags that were found for the compound modifiers
when they were looked up in the monolingual corpus during splitting. We call this
tagset the SPOS-tagset.
</bodyText>
<listItem confidence="0.993306230769231">
(4) a. fem+kamps+seger:N (pentathlon (five battle) victory)
fem:N-modif kamp:N-modif seger:N
b. fem+kamps+seger:N (pentathlon (five battle) victory)
fem#:N-modif kamps#:N-modif seger:N
c. fem+kamps+seger:N (pentathlon (five battle) victory)
fem:NUM @#@:COMP kamp:N @#@:COMP seger:N
As a third alternative tagset we use a modified variant of the EPOS-tagset, where
distinctions among parts of speech that are not relevant to the formation of compounds
are blurred. This reduces the tagset to only a few tags, as in the case where only nouns
are split:
• N-modif – all parts of a split compound except the last
• N – the last part of the compound (its head) and all other nouns
• X – all other tokens
</listItem>
<bodyText confidence="0.994854444444444">
We call this tagset the reduced POS-tagset (RPOS). The RPOS-tagset could easily be
extended to other types of compounds—for example, by extending it to five tags by
also including ADJ and ADJ-modif if we want to split adjectives as well. Using the
RPOS-tagset, the POS-based sequence model will only be useful for controlling the
order and form of compound parts. With the EPOS-tagset it also aids in controlling
the order of other words. All POS-based sequence models are trained on POS-tagged
data, where the tags have been modified after applying a compound splitting algorithm.
When referring to either of the three tagsets EPOS, RPOS, or SPOS, we will use the
designation *POS.
</bodyText>
<page confidence="0.985312">
1076
</page>
<note confidence="0.667199">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<subsectionHeader confidence="0.999367">
6.2 Part-of-Speech–Based Sequence Models
</subsectionHeader>
<bodyText confidence="0.999982464285714">
If compounds are split in the training data, then there is no guarantee that translations
of components will end up in contiguous positions and in the correct order. This
is primarily a language model problem, and we will model it as such by applying
sequence models on the customized part-of-speech sets. These sequence models can
be either standard language models trained on texts where split compound modifiers
are marked with symbols, or they can be POS-sequence models trained on the specially
designed tagsets, such as EPOS. Both these types of models can encourage compound
parts to occur in the correct order; the former, however, is a more lightweight approach,
because it relies on surface words.
A language model solution to compound coalescence could be viewed as a soft
constraint in the decoder, which encourages good sequences of compound parts over
bad sequences. An alternative to this would have been a hard constraint in the decoder
that prohibits compound parts to be placed in an incorrect order. We opted for a
soft constraint approach because we found that it gave sufficiently good results, and
because previous work on soft versus hard constraints for other areas has shown that
soft constraints give more stable and generally better results, for instance, for phrase
cohesion (Cherry 2008).
As described in the previous section, we can add special POS tags and symbols to
identify compound modifiers. Table 2 shows examples of the different representation
schemes. We can then train a *POS n-gram sequence model using any of the *POS-
tagsets, which naturally steers the decoder towards translations with good relative
placement of these components.
A lightweight model that gives some additional information of the order of com-
pound parts compared to a language model on unmarked data is to train language mod-
els on texts where compounds are represented either using the marked or sepmarked
schemes but without parts-of-speech tags. These models are, however, not as strong as
the *POS-based models, since they cannot generalize from surface words, and mainly
can aid in keeping known compounds together.
</bodyText>
<subsectionHeader confidence="0.999014">
6.3 Sequence Models as Count Features
</subsectionHeader>
<bodyText confidence="0.998306666666667">
We expect a *POS-based n-gram sequence model to learn to discourage sequences
unseen in the training data, such as the sequence of compound parts not followed by
a suitable head. Such a generative LM, however, might also have a tendency to bias
</bodyText>
<tableCaption confidence="0.934477">
Table 2
</tableCaption>
<footnote confidence="0.778793">
Examples of representation schemes for the German phrase die Fremdsprachenkenntnisse
[the knowledge offoreign languages / foreign language knowledge], originally tagged as
DET N(oun).
Original die Fremdsprachenkenntnisse
Unmarked die fremd sprache kenntnisse
Marked die fremd# sprachen# kenntnisse
Sepmarked die fremd @#@ sprache @#@ kenntnisse
EPOS DET N-Modif N-Modif N
RPOS X N-Modif N-Modif N
SPOS DET ADJ COMP N COMP N
</footnote>
<page confidence="0.981571">
1077
</page>
<table confidence="0.459343">
Computational Linguistics Volume 39, Number 4
</table>
<tableCaption confidence="0.99494">
Table 3
</tableCaption>
<table confidence="0.930982">
Tag combinations in the translation output.
Combination Judgment Boost Punish
N-Modif N Good 1 0
N-Modif N-Modif Good 1 0
</table>
<equation confidence="0.722609666666667">
N-Modif &lt;/s&gt; Bad 0 1
N-Modif X Bad 0 1
all other combinations Neutral 0 0
</equation>
<bodyText confidence="0.999852458333333">
lexical selection towards translations with fewer compounds, since the corresponding
tag sequences might be more common in text. To compensate for this bias, we experi-
ment with injecting a little dose of a priori knowledge, and add a count feature, which
explicitly counts the number of occurrences of RPOS-sequences that we deem good
and bad in the translation output. Table 3 gives an overview of the possible bigram
combinations, using the three-symbol tagset, plus sentence beginning and end markers,
and their judgment as good, bad, or neutral.
We define two new feature functions: the boost model counting the number of
occurrences of Good sequences, and the punish model, counting the occurrences of
Bad sequences, as indicated in Table 3. In the punish model we want to punish the two
bad combinations, where compound modifiers are placed in isolation without a suitable
head. In the boost model, we want to support the formation of compounds by rewarding
the two good combinations. We definitely want to boost the combination of a compound
part with its head. In addition, we can boost the formation of long compounds by
boosting the combination of two compound modifiers as well. The boost and punish
models can be used either in isolation or combined, with or without a further *POS
n-gram sequence model.
To exemplify the two models, consider the translation hypothesis given in Exam-
ple (5). The word pairs skogs bruks and bruks plan, marked in bold in the example,
constitute good sequences of a modifier followed by another modifier or a head, and
would give the count 2 with a boost model. The word sequence in italics, skogs av,
constitutes a bad sequence, since the preposition av (of) cannot be a compound head,
and would give the count 1 to a punish model. The other word sequences do not
influence either of these models because they do not involve any compound modifiers.
</bodyText>
<listItem confidence="0.711523">
(5) En|X skogs|N-Modif bruks|N-Modif plan|N ger|X en|X ¨oversikt|N skogs|N-Modif av|X
A forest cultivation plan gives an overview forest of
7. Compound Merging
</listItem>
<bodyText confidence="0.99576625">
Once a translation is generated using a system trained on split compounds, a post-
processing step is required to merge components back into compounds. All methods
we are aware of only consider consecutive tokens for merging: We stick to this assump-
tion, having delegated to the methods described earlier the task to promote a good
relative positioning of component translations. For all pairs of consecutive tokens we
have to decide whether to combine them or not. Depending on the language and on
preprocessing choices, we might also have to decide whether to apply any boundary
transformation such as, for example, inserting -s between components.
</bodyText>
<page confidence="0.99717">
1078
</page>
<note confidence="0.857324">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<figureCaption confidence="0.96465">
Figure 2
</figureCaption>
<bodyText confidence="0.9985325">
Overview of the reverse normalization algorithm, exemplified using the compound
Rechtsstaatsprinzip [rule-of-law principle / right-state principle]. The chosen option for each
strategy is circled.
In this section we first describe reverse normalization, then we describe our new
heuristic merging methods and modifications to existing heuristics. Finally we describe
a novel sequence-labeling formulation for compound merging.
</bodyText>
<subsectionHeader confidence="0.989577">
7.1 Reverse Normalization
</subsectionHeader>
<bodyText confidence="0.9645398125">
For compound modifiers that were normalized in the training data the reverse process
(reverse normalization) is needed at merging time to recreate the correct form for the
specific compound. We designed a method for reverse normalization that is based on
corpus frequencies of compound modifiers and compounds that can be collected during
compound splitting.
The strategy is illustrated and exemplified in Figure 2.2 First we look up all known
compounding forms, and their frequencies for all the compound modifiers. In Figure 2,
for instance, we found three options for the word recht [right], with rechts being the most
common option. Next, we try all combinations of forms to form a full compound, and if
any matches are found we pick the most frequent match. If this fails we have two back-
up strategies. First, we try to find known compounding forms for pairs of parts, starting
from left to right, looking up frequencies of the resulting forms from adding two parts.
If that fails we use our second back-up strategy, which is to use the most common form
of each modifier, and concatenate those. For binary compounds, the second strategy is
2 Nouns in German are capitalized. This is normally dealt with as further recasing postprocessing, and is
an orthogonal problem from the one we deal with here.
</bodyText>
<page confidence="0.973736">
1079
</page>
<note confidence="0.278918">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.995602">
superfluous, because there is only one pair of compound parts, and the second strategy
is always used.
</bodyText>
<subsectionHeader confidence="0.993696">
7.2 Heuristic Approaches to Compound Merging
</subsectionHeader>
<bodyText confidence="0.995590391304348">
We have investigated three major approaches to heuristic compound merging. Our
major contribution is a novel merging method based on part-of-speech matching. We
contrast this method with adaptations of previous merging suggestions based on sym-
bols and word lists. We also suggest improvements to these methods, and combination
methods that combine the strengths of word lists and parts of speech.
7.2.1 POS-Based Merging. The POS-match algorithm uses the fact that it is possible to
have several output factors besides surface form in a factored translation system. It
merges words that are marked as compound modifiers in either the EPOS or RPOS
tagsets if the next POS-tag matches. As described in Section 6.1, the part of speech of a
compound modifier is based on the part of speech of its head word, so a word is consid-
ered matching if the next word is a compound modifier of the same type, or a head with
a matching part of speech. In addition, if the next word does not match, the modifier
could be part of a coordinated compound, which is checked by seeing if the next word is
a conjunction, in which case a hyphen is added to the modifier. We investigated versions
of the algorithm both with and without treatment of coordinated compounds.
If a compound modifier is followed by anything other than a matching part of
speech or a conjunction it has most likely been misplaced in the translation process.
These items are left as they are in the translation output, which is often fine, because
only compound parts that occur as separate words in a corpus are split. When two
matching compound modifiers are merged, the process is iterated to see if the next word
is a matching compound modifier, head, or conjunction. This allows compounds with
an arbitrary number of parts to be merged.
In summary, the POS-based merging algorithm has the following steps:
</bodyText>
<listItem confidence="0.98750625">
• Step through each word+POS pair from left to right3
• If a compound-POS, X-MODIF, is found:
– Remove mark-up of the part if present
– Store the compound part
</listItem>
<bodyText confidence="0.91880475">
– When the next POS is a matching part, X-MODIF:
Remove mark-up of the part if present
Store the compound part
– If the next POS is a matching head, X:
Store the compound head
– If at least two parts have been found (either several modifiers
or a head):
Perform reverse normalization on the stored parts if parts are
normalized
Merge all parts
For Swedish and Danish: remove a consonant if any of the
merges resulted in three identical consecutive consonants
</bodyText>
<page confidence="0.758297">
3 The words that are processed in the inner if-clause are skipped in the outer loop.
1080
</page>
<bodyText confidence="0.973392297297297">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
– If the next POS is a conjunction and no head was found:
Add a hyphen at the end of the compound part
This heuristic has the advantage over previous merging algorithms that it can form
novel compounds while reducing the risk of erroneous merging through the matching
constraint. It is also the only merging method we are aware of that addresses coordi-
nated compounds. However, it requires a factored decoder that can carry part-of-speech
tags through the translation process. It also requires tagsets where compound modifiers
are marked based on the compound head, such as EPOS or RPOS. In the current form
the POS-match strategy cannot be used for the SPOS-tagset that does not use head-
based tags for compound modifiers, because we cannot enforce the matching constraint
in this way. It would be possible to design a POS-match strategy based on standard tags,
which for instance allowed merging of noun+noun, but not of noun+preposition. Such
a strategy requires linguistic knowledge and customization for each language, however.
7.2.2 Other Merging Heuristics. The symbol-based method is inspired by work on mor-
phology merging (El-Kahlout and Oflazer 2006; Virpioja et al. 2007). It merges words
that are marked with a symbol with the next word in the marked scheme. In the
sepmarked scheme, when a standard symbol is found, the words on both sides of it
are merged. These algorithms have the disadvantage, compared with the POS-match
algorithm, that it is more likely that words are merged into non-compounds, since no
matching check is carried out.
We have also investigated methods based on word lists, proposed by Popovi´c, Stein,
and Ney (2006). These methods use frequency word lists compiled at split time. Three
types of lists were used: lists of compound modifiers, of compounds, and of words. If a
compound modifier is encountered, it is checked whether merging it with the next word
results in either another compound modifier, or a compound or word. Unlike Popovi´c,
Stein, and Ney (2006), we perform this process recursively, to allow compounds with
several parts. Again, reverse normalization is performed when needed. Still, no novel
compounds can be formed, and coordinated compounds are not handled. The method
does not merge words into non-words, but there is another risk, that of merging
words that should be separate in a specific context, but that happen to form a valid
compound or other word when combined, such as the examples in Example (6). It has
the advantage over the other proposed methods that it can be used on output from
any MT decoder, without the use of customized POS-tags or symbols in the output.
We investigate the usage of two types of frequency lists for this strategy, either a list
of compounds, which can be created as a byproduct of the splitting algorithm, like
Popovi´c, Stein, and Ney (2006), or by using a list of all words in a corpus.
</bodyText>
<listItem confidence="0.71876975">
(6) DE bei der (at the)
beider (both)
SV f¨or sm˚a (too small)
f¨orsm˚a (spurn)
</listItem>
<bodyText confidence="0.999935166666667">
We empirically verified that the list-based heuristics tend to misfire quite often,
leading to too many compounds, such as merging the words in Example (6). We thus
modified them in two ways: (1) by additionally requiring the head word to be a content
word and (2) by requiring the generated compound to be more frequent in a corpus than
the corresponding bigram of isolated words. The head word restriction can block some
erroneous merges such as those in Example (6DE), whereas the frequency restriction can
</bodyText>
<page confidence="0.937918">
1081
</page>
<note confidence="0.266832">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.9945715">
potentially block both examples in Example (6). Compound and bigram frequencies can
be computed on any available monolingual corpus in the domain of interest. We also
investigated combinations of the heuristics by using either the union or intersection of
merges from two different strategies.
</bodyText>
<subsectionHeader confidence="0.984236">
7.3 Compound Merging as Sequence Labeling
</subsectionHeader>
<bodyText confidence="0.97182255">
Besides extending and combining existing heuristics, we propose a novel formulation of
compound merging as a sequence labeling problem. The opposite problem, compound
splitting, has successfully been cast as a sequence labeling problem before (Dyer 2010),
but here we apply this formulation in the opposite direction.
Depending on choices made at compound splitting time, this task can be either a
binary or multi-class classification task. If compound parts were kept as-is, the merging
task is a simple concatenation of two words, and each separation point must receive a
binary label encoding whether the two tokens should be merged. If compounds were
normalized at splitting time, the compound form has to be restored before concatenat-
ing the parts. This can be modeled either as a multi-class classifier that has the possible
boundary transformations as its classes or in a two-step process where the form of
words are modeled in a separate step after the binary merging decision. In the latter
case it is possible to use the reverse normalization process described in Section 7.1. In
this work we limited our attention to binary classification.
Consider for instance translating into German the English in Example (7).
(7) Europe should promote the knowledge of foreign languages
Assuming that the training corpus did not contain occurrences of the pair (knowledge
of foreign languages,‘fremdsprachenkenntnisse’) but contained occurrences of (knowl-
edge,‘kenntnisse’), (foreign,‘fremd’), and (languages,‘sprachen’), then the translation
model from English into decomposed German could be able to produce Example (8).
</bodyText>
<listItem confidence="0.518089">
(8) Europa sollte fremd sprachen kenntnisse f¨ordern
</listItem>
<bodyText confidence="0.9992004">
We cast the problem of merging compounds as one of making a series of correlated
binary decisions, one for each pair of consecutive words, each deciding whether the
whitespace between the two words should be suppressed (label 1) or not (label 0). In the
example case, the correct labeling for the sentence would be {0,0,1,1,0}, reconstructing
the correct German as shown in Example (9).
</bodyText>
<listItem confidence="0.720804">
(9) Europa sollte fremdsprachenkenntnisse f¨ordern
</listItem>
<bodyText confidence="0.9998878">
Although in principle one could address each atomic merging decision indepen-
dently, it seems intuitive that a decision taken at one point should influence merging
decisions in neighboring separation points, especially because compounds can be made
up of more than two parts. For this reason, instead of a simple (binary or n-ary)
classification problem, we prefer a sequence labeling formulation.
Depending on the choice of the features, this approach has the potential to be truly
productive, that is, to form new compounds in an unrestricted way. As discussed,
in fact, the list-based heuristics can only form compounds that were observed in the
training data or in some suitable monolingual corpus, and are thus not productive.
The POS-match heuristic is more flexible, but is still limited in that it can only form
</bodyText>
<page confidence="0.931407">
1082
</page>
<bodyText confidence="0.96661374074074">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
a compound if a modifying element (non-head) has been observed and tagged as such
in the training data.
The array of sequence labeling algorithms potentially suitable to our problem is
fairly broad, including hidden Markov models (Rabiner 1989), conditional random
fields (CRFs) (Lafferty, McCallum, and Pereira 2001), Semi-CRFs (Sarawagi and
Cohen 2004), structured perceptrons (Collins 2002), structured support vector machines
(Tsochantaridis et al. 2005), Max-Margin Markov networks (Taskar, Guestrin, and Koller
2003), and more. Because the focus of this work is on the application rather than on a
comparison among alternative structured learning approaches, we limited ourselves to
a single implementation. Considering its good scaling capabilities, capability to handle
strongly redundant and overlapping features, and widespread recognition in the NLP
community, we chose to use CRFs.
7.3.1 Features. Each sequence item (i.e., each separation point between words) is repre-
sented by means of a vector of features. Our aim was to include features representing
the knowledge available to the heuristics, such as part-of-speech tags, frequencies for
compounds and bigrams, as well as comparisons between them. Features were also
inspired by previous work on compound splitting, with the intuition that features that
are useful for splitting compounds could also be useful for merging. Character n-grams
have successfully been used for splitting Swedish compounds, as the only knowledge
source by Brodda (1979), and as one of several knowledge sources by Sj¨obergh and
Kann (2004). Friberg (2007) tried to normalize letters, besides using the original letters.
Although she was not successful, we still believe in the potential of this feature. Larson
et al. (2000) used frequencies of prefixes and suffixes from a corpus as a basis of their
method for splitting German compounds. We used the following features where -N
refers to the nth position before the merge point, and +N to the nth position after the
merge point:
</bodyText>
<listItem confidence="0.998114818181818">
• Previous tag
• Surface words: word-2, word-1, word+1, bigram word-1–word+1
• Parts of speech: POS-2, POS-1, POS+1, bigram POS-1–POS+1
• Character n-grams around the merge point
– three-character suffix of word-1
– three-character prefix of word+1
– Combinations crossing the merge points: 1+3, 3+1, 3+3 characters
• Normalized character n-grams around the merge point, where characters
are replaced by phonetic approximations and grouped according to
phonetic distribution, see Figure 3 (only for Swedish)
• Frequencies from the training corpus, binned by the following method:
</listItem>
<equation confidence="0.833474333333333">
�
10�log10(f )� if f &gt; 1
f¯ _
</equation>
<bodyText confidence="0.956296">
f otherwise
for the following items:
</bodyText>
<listItem confidence="0.9452785">
– Bigram: word-1,word+1
– Compound resulting from merging word-1,word+1
</listItem>
<page confidence="0.93732">
1083
</page>
<figure confidence="0.986024047619048">
Computational Linguistics Volume 39, Number 4
# vowels (soft versus hard)
s/[aou˚a]/a/g;
s/[eiy¨a¨o´e]/e/g;
# consonant combinations and
# spelling alternations
s/ng/N/g;
s/gn/G/g;
s/ck/K/g;
s/-[lhgd]j/J/g;
s/-ge/Je/g;
s/-ske/Se/g;
s/-s[kt]?j/S/g;
s/-s?ch/S/g;
s/-tj/T/g;
s/-ke/Te/g;
#consonants grouping
s/[ptk]/p/g;
s/[bdg]/b/g;
s/[lvw]/l/g;
s/[cqxz]/q/g;
</figure>
<figureCaption confidence="0.840319">
Figure 3
</figureCaption>
<bodyText confidence="0.768059">
Transformations performed for normalizing Swedish characters (Perl notation).
</bodyText>
<listItem confidence="0.971038333333333">
– Word-1 as a true prefix of words in the corpus
– Word+1 as a true suffix of words in the corpus
• Frequency comparisons of two different frequencies in the training corpus,
</listItem>
<bodyText confidence="0.9808085">
classified into four categories: freq1 = freq2 = 0, freq1 &lt; freq2,
freq1 = freq2, freq1 &gt; freq2
– word-1,word+1 as bigram vs. compound
– word-1 as true prefix vs. single word
– word+1 as true suffix vs. single word
7.3.2 Training Data for the Sequence Labeler. Because features are strongly lexicalized, a
suitably large training data set is required to prevent overfitting, ruling out the possibil-
ity of manual labeling.
We created our training data automatically, using a subset of the compound merg-
ing heuristics described in Section 7.2.2, plus an additional heuristic enabled by the
availability, when estimating parameters for the CRF, of a reference translation: Merge
if two tokens are observed combined in the reference translation (possibly as a sub-
sequence of a longer word). We compared multiple alternative combinations of heuris-
tics on a validation data set. The validation and test data were created by applying all
heuristics, and then manually having the positive instances checked.
A first possibility to automatically generate a training data set consists in applying
the compound splitting preprocessing of choice to the target side of the parallel training
corpus for the SMT system: Separation points where merges should occur are thus
trivially identified. In practice, however, merging decisions will need to be taken on
the noisy output of the SMT system, and not on the clean training data. To acquire
training data that is similar to the test data, we could have held out from SMT training
a large fraction of the training data, used the trained SMT to translate the source side
</bodyText>
<page confidence="0.929159">
1084
</page>
<bodyText confidence="0.938882375">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
of it, and then label decision points according to the heuristics. This would, however,
imply making a large fraction of the data unavailable to the training of the SMT system.4
We thus settled for a compromise: We trained the SMT system on the whole training
data, translated the whole source side, and then labeled decision points according to
the heuristics. The translations we obtain are thus biased, of higher quality than those
we should expect to obtain on unseen data. Nevertheless they are substantially more
similar to real SMT output than the reference translations with automatic splits.
</bodyText>
<sectionHeader confidence="0.894256" genericHeader="evaluation">
8. Experiments
</sectionHeader>
<bodyText confidence="0.999907666666667">
In this section we report experimental results for the strategies proposed in this article.
We first describe, in Section 8.1, the overall experimental set-up. We then report on the
experiments, as follows:
</bodyText>
<listItem confidence="0.999663181818182">
• In Section 8.2 we report on general effects of compound processing.
• Section 8.3 reports effects on coalescence of different representation
schemes. Here we do not vary merging or splitting.
• Section 8.4 reports on compound splitting and the way different parameter
settings affect translation quality. Here we do not vary representation
scheme or merging.
• Section 8.5 reports results from varying the merging process. In particular
we compare the novel sequence labeling method with the heuristic
methods. Here we do not vary representation scheme or splitting.
• In Section 8.6 we apply the overall best strategies to corpora of different
sizes and to out-of-domain data.
</listItem>
<subsectionHeader confidence="0.962377">
8.1 Experimental Set-up
</subsectionHeader>
<bodyText confidence="0.999968583333333">
We performed experiments on translation from English into German, Swedish, and
Danish, all of which have closed compounds. We tested all experimental conditions on
Europarl (Koehn 2005) for translation from English to German. We also give contrasting
results on English–Swedish Europarl and for an automotive corpus for translation from
English to Swedish and Danish. The automotive corpus was gathered from translation
memory data. The two corpora are quite different. The automotive corpus is from a
limited domain, and of a homogeneous nature, whereas Europarl is more diverse, and
tends to have a more complex language than the automotive corpus. Table 4 summa-
rizes the sizes for the corpora that we used in Sections 8.2–8.5. The German test set is
the test2007 set from the WMT 2008 workshop.5 We have chosen to use a smaller part
of Europarl, to reduce runtimes and allow more experiments. In Section 8.6 we report
results from scaling experiments and on out-of-domain data.
</bodyText>
<footnote confidence="0.999166666666667">
4 This option can also be extended so that the training data is split into chunks where each chunk is
translated by a system that is trained on the remaining chunks, a strategy that has been successfully used
for parse reranking (Collins and Koo 2005). Because we found that using fresh data did not give any
significant improvements on the merging task except with little training data (see Section 8.5.2, Table 22),
we choose not to use this approach since it would be very time consuming and thus impractical.
5 http://www.statmt.org/wmt08.
</footnote>
<page confidence="0.906728">
1085
</page>
<table confidence="0.611128">
Computational Linguistics Volume 39, Number 4
</table>
<tableCaption confidence="0.997406">
Table 4
</tableCaption>
<table confidence="0.952139375">
Overview of the experimental settings for the experiments in Sections 8.2–8.5.
Name Europarl German Europarl Swedish Auto Swedish Auto Danish
Corpus Europarl Europarl Automotive Automotive
Languages En→De En→Sv En→Sv En→Da
Training sentences 701,157 701,157 329,090 168,047
Avg. target sentence length 20.5 19.4 9.3 9.2
Dev sentences 500 500 2,000 1,000
Test sentences 2,000 2,000 1,000 1,000
</table>
<bodyText confidence="0.999061857142857">
We used factored translation (Koehn and Hoang 2007) in our experiments, with
both surface words and part-of-speech tags on the target side, with a sequence model
on parts-of-speech. For part-of-speech tagging we used TreeTagger (Schmid 1994) for
German, an in-house hidden Markov model tagger based on Cutting et al. (1992) for
Danish and Swedish, and for Swedish also the Granska tagger (Carlberger and Kann
1999). For the majority of experiments we used the Moses decoder (Koehn et al. 2007),
which is a standard phrase-based statistical decoder, which allows factored decoding.
For word alignment, Giza++ (Och and Ney 2003) was used and for language modeling
we used SRILM (Stolcke 2002). For parameter optimization we used minimum error
rate training (Och 2003). For each experiment we ran minimum error rate training three
times in order to reduce the effect of optimizer instability, and report the average result
and standard deviation. In the merging experiments based on sequence labeling we
used the CRF++ toolkit.6 For the merging experiment with sequence labeling, we used
the Matrax decoder (Simard et al. 2005) on the automotive corpus. Matrax is a phrase-
based decoder that allows discontiguous phrases, and parameter optimization based
on gradient descent for smoothed NIST. We extended the original Matrax decoder with
factored decoding on the target side.
Compounds were split before training using the corpus-based method described in
Section 5. Except for the experiments comparing different compound merging methods,
we used the POS-match merging algorithm developed by us.
We report results on three metrics: Bleu (Papineni et al. 2002), NIST (Doddington
2002), and Meteor. For Meteor we use the version tuned on adequacy and fluency
(Lavie and Agarwal 2007) for German, and the original version with default weights
(Banerjee and Lavie 2005) for Swedish and Danish, since there is no tuned version for
those languages. For Bleu and Meteor we use the %-notation. Significance testing was
performed using approximate randomization (Riezler and Maxwell 2005), with 10,000
iterations, on output based on three optimizer runs, as recommended by Clark et al.
(2011).
</bodyText>
<subsectionHeader confidence="0.975034">
8.2 General Effects of the Compound Processing Strategy
</subsectionHeader>
<bodyText confidence="0.99997375">
One effect of compound splitting is that the number of word types is reduced. Tables 5
and 6 show the number of types (number of unique words) and tokens (total number of
words) and the type/token ratio for Europarl in the different representation schemes.
The type count and the type/token ratio in the baseline system are much higher in
</bodyText>
<footnote confidence="0.80494">
6 http://crfpp.sourceforge.net/.
</footnote>
<page confidence="0.986797">
1086
</page>
<note confidence="0.934174">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<tableCaption confidence="0.795643">
Table 5
Type and token counts and ratio for the German Europarl corpus.
</tableCaption>
<table confidence="0.9962605">
System Tokens Types Ratio
German baseline 14,356,051 184,215 1.28%
English marked 15,674,728 93,746 0.60%
unmarked 15,674,728 81,806 0.52%
sepmarked 17,007,929 81,808 0.48%
15,158,429 63,692 0.42%
</table>
<tableCaption confidence="0.8615885">
Table 6
Type and token counts and ratio for the Swedish Europarl corpus.
</tableCaption>
<table confidence="0.9994218">
System Tokens Types Ratio
baseline 13,603,062 182,000 1.34%
Swedish marked 14,401,784 107,047 0.74%
unmarked 14,401,784 100,492 0.70%
English 15,043,321 67,044 0.45%
</table>
<bodyText confidence="0.999575423076923">
German and Swedish than in English, and is drastically reduced after compound
splitting in all markup schemes, even though it is still higher than in English. One
reason for the type count still being higher in German and Swedish than in English is
morphological complexity, which is low in English and higher in German and Swedish.
The type count is higher in the marked representation scheme than in the other schemes
due to the fact that compound modifiers are marked, and do not coincide with other
words as they do in the other schemes.
We believe that the fact that compound splitting leads to both type and token
counts that are more similar to English is a positive thing, because it likely contributes
to making the two languages structurally more similar, which makes the SMT pro-
cess easier. This is supported by Birch, Osborne, and Koehn (2008), who showed that
morphological complexity of a target language correlates negatively with Bleu scores.
A lower type/token ratio translates into fewer out of vocabulary (OOV) words, and
can give more accurate probability estimates for the SMT translation and language
models.
These changes in the number of types and tokens could potentially influence the
translation process. In Tables 7 and 8 we show the effect of splitting on average phrase
length and average phrase length ratio in the phrase table and during translation in the
baseline and in the unmarked system. For the unmarked system we also compensated
for the split compounds by counting phrase length as if compounds were merged using
the POS-match heuristic. For both languages the lengths and ratios are similar between
the baseline and the unmarked system when we compensate for split compounds. This
indicates that the type of phrases used are not much affected by the higher number of
tokens in the split system. We can also see that the phrases used for translations are on
average longer on the source side and shorter on the target side, with a very different
ratio than the phrases in the phrase table.
</bodyText>
<page confidence="0.961971">
1087
</page>
<table confidence="0.594639">
Computational Linguistics Volume 39, Number 4
</table>
<tableCaption confidence="0.987089">
Table 7
</tableCaption>
<table confidence="0.988106666666667">
Average phrase lengths and English/German length ratios in the phrase table and during
translation for German Europarl. German-m is German with merged compounds, where we
calculated the phrase length as if compounds were merged using the POS-match method.
Baseline Unmarked
English German English German German-m
Phrase table Phrase length 1.95 2.76 1.91 2.88 2.68
Phrase ratio 0.86 0.81 0.86
Translation Phrase length 2.46 2.29 2.48 2.51 2.33
Phrase ratio 1.19 1.10 1.17
</table>
<tableCaption confidence="0.992633">
Table 8
</tableCaption>
<table confidence="0.740552375">
Average phrase lengths and English/Swedish length ratios in the phrase table and during
translation for Swedish Europarl. Swedish-m is Swedish with merged compounds, where we
calculated the phrase length as if compounds were merged using the POS-match method.
Baseline Unmarked
English Swedish English Swedish Swedish-m
Phrase table Phrase length
Phrase ratio
Translation Phrase length
</table>
<figure confidence="0.901895444444444">
Phrase ratio
1.96 2.81 2.70
0.85 0.88
2.61 2.38 2.29
1.75 1.22
1.95 2.76
0.86
2.51 2.22
1.21
</figure>
<subsectionHeader confidence="0.988008">
8.3 Promoting Compound Coalescence
</subsectionHeader>
<bodyText confidence="0.999776681818182">
In this section we describe three sets of experiments that investigate the use of the *POS-
tagsets in sequence models and as count features. In the first experiment we explore the
effect of different representation schemes for translation into German, and in the second
experiment we do the same, on a smaller scale, for Swedish. In the third experiment
we investigate the effect of the RPOS-tagset and the boost and punish models. In all
experiments merging was performed using the POS-match heuristic (see Section 7.2.1),
except for the sepmarked scheme that used the SPOS-tagset that does not allow this
type of POS-match, and for which the symbol merging algorithm (see Section 7.2.2)
was used.
8.3.1 Experiment 1. In the first experiment we investigated different compound rep-
resentation schemes for German Europarl. Table 9 shows the results using different
representation schemes, comparing them with two baselines, with and without a POS-
sequence model. There is generally a small significant improvement when a POS-based
sequence model is used compared with the same model without a POS-based sequence
model. Especially for the systems with compound splitting, it was clearly worse not to
use *POS-models, and all such systems perform significantly worse than both baselines
on most metrics. The representation scheme with sepmarked words and SPOS-tags did
not perform well, which can both be due to less power of the mark-up system, and
to the fact that it cannot use the POS-match merging heuristic. The two systems with
EPOS-models do perform well though, and are on par with the factored baseline, and
mostly better than the unfactored baseline. The marked and unmarked systems with
EPOS perform similarly, with no significant differences between them. It is thus hard to
</bodyText>
<page confidence="0.993387">
1088
</page>
<note confidence="0.934617">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<tableCaption confidence="0.99817">
Table 9
</tableCaption>
<table confidence="0.96905675">
Translation results for different representation schemes on German Europarl. Significant
differences (positive or negative) from the baseline are marked *(5% level) and **(1% level), and
differences from baseline+POS are marked similarly with #.
Bleu NIST Meteor
Baseline 20.0 (0.3) 5.94 (0.04) 27.5 (0.2)
Baseline+POS 20.2 (0.1)** 5.93 (0.02) 27.6 (0.2)
Unmarked 19.7 (0.2)**## 5.90 (0.04)**# 27.4 (0.2)##
Marked 19.7 (0.2)**## 5.91 (0.02)** 27.5 (0.1)
Sepmarked 19.0 (0.1)**## 5.81 (0.02)**## 27.0 (0.0)**##
EPOS-unmarked 20.1 (0.0)** 5.97 (0.03)* 27.8 (0.1)**
EPOS-marked 19.9 (0.1) 5.96 (0.02)* 27.7 (0.1)*
SPOS-sepmarked 19.4 (0.3)**## 5.83 (0.01)**## 27.3 (0.2)*##
</table>
<bodyText confidence="0.999581684210526">
say which of these classification systems are preferable, but at least it is clear that the
use of EPOS-tags are useful.
A more detailed analysis was performed of the compound parts in the output. The
outcomes of the merging process were classified into four groups: known compounds;
novel compounds; parts of coordinated compounds; and unmerged, single parts. They
were further classified into good or bad outcomes. Compounds were judged as bad
if they formed non-words or had the wrong form, and compound parts were judged
as bad if they should have been merged with the next word, or did not work as a
standalone word.
Table 10 shows the results of this analysis. The majority of the merged compounds
are known from the training corpus for all representation schemes. There is, again,
a marked difference between the systems that use POS-match merging and the sep-
marked systems that do not have that information. The sepmarked system found the
highest number of novel compounds, but also had the highest error rate, which shows
that it is useful to match POS-tags. The EPOS systems have a higher number of novel
compounds than the marked and unmarked options without an EPOS model. All these
systems had a low error rate of the novel compounds, however. Very few errors were
due to reverse normalization; In the EPOS-unmarked system there were only three such
errors.
</bodyText>
<tableCaption confidence="0.995872">
Table 10
</tableCaption>
<table confidence="0.935355">
Analysis of merged compounds from different representation schemes. The sepmarked systems
do not do any matching, and can thus not leave any single parts.
EPOS- EPOS- SPOS- unmarked marked sepmarked
unmarked marked sepmarked
Known 3,339 3,375 3,594 3,747 3,587 3,762
Good 168 105 176 104 93 245
Novel
Bad 20 8 97 10 7 64
Coordinated Good 43 42 43 42 44 37
Bad 9 3 9 22 5 7
Single part Good 6 5 – 136 33 –
Bad 11 16 – 52 46 –
Total 3,596 3,554 3,919 4,113 3,815 4,115
1089
Computational Linguistics Volume 39, Number 4
</table>
<bodyText confidence="0.999559363636364">
Generally, the percentage of bad parts or compounds is lower for the systems with a
*POS-sequence model, which shows that the sequence model is useful for the ordering
of compound parts. The number of single compound parts is also much higher for the
systems without a POS sequence model.
The number of compounds found by the splitting algorithm in the German ref-
erence text was 4,472. All systems produce fewer compounds than this number. The
numbers in Table 10 cannot be directly compared to the baseline system, however,
because we do not know which words in its output are compounds.
An indication of how many compounds there are in a text is the number of long
words. In the reference text there are 231 word types with at least 20 characters and
1,178 word types with at least 15 characters, which we used as the limits for long words.
Other length limits give similar results. Table 11 gives data on absolute numbers of
these long word types and recall, precision, and F-score compared with long words
found in the reference for the different systems. The distribution of long words in the
systems with compound processing generally follows the distribution of compounds
from Table 10, which indicates that this is a good indicator of the number of compounds.
Both baseline systems have fewer long words than all compound processing systems,
indicating that the split-merge strategy does indeed help in increasing the number of
compounds in the translation output. The SPOS systems had the highest number of long
words; for up to 15 characters there are even more long words than in the reference.
The EPOS systems have the same number for up to 15 characters, but a bit lower for
up to 20 characters. The EPOS systems also have the highest recall of all systems for
both character lengths. The F-scores are similar for the baseline systems and the EPOS
systems, with the baseline higher on precision, and the EPOS systems higher on recall.
Although the baseline has a higher precision, the absolute number of overlapping words
is still higher in many of the systems with compound processing—for instance, for
words of up to 20 characters there are 431 in EPOS-unmarked compared with 388 in
baseline+POS. It is important to notice, however, that the reference is not a trustworthy
gold standard, because there are many possible alternative good translations, and the
real quality of long words are likely underestimated. Thus, as can be seen in Table 10, the
clear majority of produced compounds in the EPOS systems are judged acceptable, even
though the precision of overlap for long words with the reference is at most 47% for the
EPOS models.
</bodyText>
<tableCaption confidence="0.982945">
Table 11
</tableCaption>
<table confidence="0.942031533333333">
Number of long word types in the translations, and R(ecall), P(recision), and F(-score) compared
with long word types in the reference.
≥20 chars F ≥15 chars F
Number R P Number R P
Reference 231 – – – 1,178 – – –
Baseline 157 .27 .49 .35 743 .33 .53 .41
Baseline+POS 151 .26 .49 .34 733 .33 .53 .41
Unmarked 192 .27 .40 .32 826 .34 .49 .40
Marked 217 .29 .38 .33 862 .35 .48 .40
Sepmarked 279 .28 .28 .28 1,031 .33 .38 .35
EPOS-unmarked 231 .30 .37 .33 936 .36 .46 .40
EPOS-marked 233 .30 .37 .34 895 .36 .47 .41
SPOS-sepmarked 290 .30 .30 .30 1,043 .34 .39 .36
1090
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</table>
<tableCaption confidence="0.989123">
Table 12
</tableCaption>
<table confidence="0.974577166666667">
Results for Swedish Europarl. Significant improvements over baseline+POS are marked
**(1% level).
System Bleu NIST Meteor
Baseline+POS 21.6 (0.0) 6.11 (0.00) 57.8 (0.1)
EPOS-unmarked 22.0 (0.1) ** 6.14 (0.01) 58.3 (0.1)**
EPOS-marked 21.9 (0.0) 6.21 (0.00) ** 58.3 (0.0)**
</table>
<bodyText confidence="0.987565923076923">
8.3.2 Experiment 2. To further test whether a compound processing strategy using a
customized tagset is useful, we performed experiments on an additional language pair,
English–Swedish. In this case we always used the successful EPOS-tagset, in combina-
tion with either the marked or unmarked representation of compounds.
Table 12 shows the results for translation into Swedish. Both systems with com-
pound processing have higher scores on all metrics. The difference is significant on
Meteor, and on either Bleu or NIST for the two systems with compound processing.
The only significant differences between using marked and unmarked representations
is that the marked system is better on NIST.
To investigate compound translation specifically, we manually classified the system
translations corresponding to the first 100 compounds in the reference text that had
a clear counterpart in the English source text. As good translations we considered
identical translations to the reference, and alternative translations with the same mean-
ing, which we also distinguished between other compounds, single words, or other
constructions. We also had a category for word groups that were translated as separate
words, but should have been compounded, split compounds.
The result of this evaluation can be seen in Table 13. There are more translations
that are identical to the reference in the two systems with splitting, but the total number
of identical and alternative translations is approximately the same in the three systems.
The number of split compounds is higher in the baseline system. The unmarked system
produces more split compounds and partial translations than the marked system. This
can be seen as an indication of marking having an effect, which, however, is not clear in
the automatic evaluation.
We also investigated the quality of the merged compounds, especially with regard
to the reverse normalization that is needed in the unmarked systems, where compound-
ing forms were normalized. There were no merging errors in the marked system. In the
</bodyText>
<tableCaption confidence="0.966588">
Table 13
</tableCaption>
<table confidence="0.935881916666667">
Analysis of the translations of 100 source items yielding compounds in a Swedish reference text.
Baseline+POS EPOS-Unmarked EPOS-Marked
Identical comp. 48 53 57
Alt. compound 14 9 10
Alt. word 16 16 12
Alt. other 9 8 9
Split compound 7 5 3
Partly transl. 4 7 4
No equivalent 0 0 2
OOV 2 2 3
1091
Computational Linguistics Volume 39, Number 4
</table>
<tableCaption confidence="0.7886785">
Table 14
Results of coalescence experiment on the automotive corpus. Scores that are significantly
different from the baseline are marked *(5% level), and differences from Baseline+POS are
marked with #.
</tableCaption>
<table confidence="0.9992269">
Bleu Auto Danish Meteor Bleu Auto Swedish Meteor
NIST NIST
Baseline 81.3 (0.0) 9.74 (0.01) 89.5 (0.1) 67.7 (0.3) 9.96 (0.02) 85.4 (0.1)
Baseline+POS 81.4 (0.1) 9.73 (0.02) 89.5 (0.1) 67.8 (0.1) 9.94 (0.03) 85.1 (0.1)
EPOS 80.6 (0.2)* 9.67 (0.02) 89.1 (0.2) 68.5 (0.2) 10.06 (0.01)* 85.5 (0.2)
RPOS 80.9 (0.2) 9.69 (0.03) 89.4 (0.2) 68.5 (0.2)* 10.07 (0.02)*# 85.8 (0.1)#
boost 81.0 (0.3) 9.72 (0.03) 89.7 (0.2) 68.3 (0.1) 10.05 (0.01)* 85.7 (0.0)#
punish 80.7 (0.1) 9.70 (0.01) 89.4 (0.1) 68.3 (0.2) 10.04 (0.01)* 85.7 (0.1)#
RPOS+boost 81.0 (0.1) 9.73 (0.01) 89.7 (0.2) 68.2 (0.2) 10.04 (0.02) 85.7 (0.2)
RPOS+punish 81.0 (0.1) 9.71 (0.03) 89.6 (0.2) 68.2 (0.2) 10.04 (0.03)* 85.6 (0.2)#
</table>
<bodyText confidence="0.999422181818182">
unmarked system there were only two minor errors due to failed reverse normalization.
The first error is a missing insertion of an +s for medlem/+s/l¨ander (member countries) and
in the second case, *samh¨all/-e+s/politiska (socio-political), a combination change -e/+s is
wrong.
8.3.3 Experiment 3. In the third set of experiments with factored translation models we
investigated the RPOS tagsets and the use of count features for the Danish and Swedish
automotive corpus and German Europarl. Compound parts were merged using the
POS-match heuristic.
Results on the two automotive corpora are shown in Table 14. The scores are very
high, which is due to the fact that it is an easy domain with many repetitive sentence
types. In this case there are no significant differences between the two baselines when
using a POS-model. For Swedish, the results are overall higher for the systems with
compound splitting, a difference that is significant for some systems and metrics. Over-
all, the RPOS system performs best for Swedish, with results significantly better than at
least one baseline on all metrics. For Danish, on the other hand, the only significant
difference between any baseline and system with splitting is for the EPOS system,
which is slightly worse than the baseline with POS. For the other systems there are
no significant differences to the baseline.
Table 15 shows results using RPOS for German Europarl. For this corpus neither the
RPOS model nor the boost and punish models works well. They all give significantly
worse results than the baseline. As we have seen before, however, the EPOS model gives
competitive results to the baseline for this corpus.
</bodyText>
<tableCaption confidence="0.801987">
Table 15
Results of coalescence experiment on German Europarl. Scores that are significantly different
from the Baseline+POS are marked with **(1% level).
</tableCaption>
<table confidence="0.9997275">
System Bleu NIST Meteor
Baseline+POS 20.2 (0.1) 5.93 (0.02) 27.6 (0.2)
EPOS 20.1 (0.1) 5.97 (0.03) 27.7 (0.1)
RPOS 16.5 (0.1)** 5.32 (0.03)** 25.0 (0.2)**
boost 16.3 (0.2)** 5.29 (0.03)** 24.8 (0.3)**
punish 16.6 (0.1)** 5.33 (0.03)** 25.1 (0.2)**
</table>
<page confidence="0.978406">
1092
</page>
<note confidence="0.683021">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<bodyText confidence="0.999871333333333">
Overall, we found that it was successful to use *POS-sequence models in a factored
translation model in order to handle compound coalescence. Our best models with
compound splitting perform at least on par with the baseline, and sometimes better. We
also showed that there are other advantages to using compound processing, especially
that the number of long words are similar to the baseline, which is not the case for
our baseline systems, which have too few long words. We also showed that *POS-
sequence models are essential for the compound processing approach to be successful.
On Europarl the EPOS-tagset was the most successful, whereas the RPOS-tagset and
count features could help for the automotive domain.
</bodyText>
<subsectionHeader confidence="0.987826">
8.4 Influence of Splitting Strategies
</subsectionHeader>
<bodyText confidence="0.947307228571428">
In the following experiments we investigated how compound splitting strategies of
different quality influenced the suggested compound processing method. In order to do
this we performed an intrinsic evaluation of several splitting methods, and compared
it to translation results using the same splitting strategies. In this experiment we used
German Europarl.
We used a modified version of the splitting strategy of Koehn and Knight (2003) as
a basis for the comparison, and applied it to all nouns, adjectives, adverbs, and verbs of
minimum length six characters into parts of minimum three characters, by allowing all
splits where the parts were found in a corpus, and were tagged as content words. Parts
were allowed to be modified by all compound suffixes from Langer (1998). The best
splitting option, which can be no split, was chosen based on the arithmetic mean of the
corpus frequencies of the parts. We also imposed the restriction on the compound head
that its part-of-speech tag needs to be the same as for the full word. We call this system
arith. In other variants of splitting strategies, one feature at the time was changed, based
on the arith system:
geom – using the geometric mean instead of the arithmetic mean
eager – choosing the split with the highest number of parts, instead of using the
arithmetic mean
part4 – limiting the length of compound parts to four characters
max2 – limiting the maximum number of parts per compound to two
common – only allowing the common compound suffixes -s, -es, -n, -nen
anypos – not using part-of-speech tags
The corpus frequencies were gathered from the target side of the training corpus. We
compared the systems with compound splitting with a factored baseline system without
any compound processing.
To measure the success of the different compound splitting algorithms we per-
formed an intrinsic evaluation. We used the gold standard from Stymne (2008), created
by manually annotating the first 5,000 words of the test text for one-to-one correspon-
dence with the English reference text, similar to Koehn and Knight (2003). A one-to-
one correspondence occurs when the words in a German compound are translated
as separate words in English. In addition there can be inserted function words. As
an example, Medienfreiheit is in one-to-one correspondence with freedom of the media,
since the two German parts Medien and Freiheit corresponds to two separate words,
media and freedom. The two function words of, the are ignored. Out of the 5,000 words
174 were compounds in one-to-one correspondence with English. The result of the
</bodyText>
<page confidence="0.958548">
1093
</page>
<note confidence="0.46777">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.981809088235295">
one-to-one evaluation is shown in Table 16. The same metrics and categories as in Koehn
and Knight (2003) were used:
correct split: words that were correctly split
correct not: words that should not be split and were not
wrong not: words that should be split but were not
wrong faulty: words that were split but in an incorrect way
wrong split: words that should not be split but were
precision: (correct split) / (correct split + wrong faulty + wrong split)
recall: (correct split) / (correct split + wrong faulty + wrong not)
accuracy: (correct) / (correct + wrong)
The splitting options have their strengths on different metrics, with three different
methods having the best results for the three metrics used. Compared to the arith
method it can be seen that both imposing length restrictions and using the geometric
mean increases the results on all three metrics. Limiting the number of parts gives
the highest recall. Using only common compound suffixes gives higher precision, and
not using part-of-speech gives lower precision. The baseline system without splitting
actually has the highest accuracy. To a large extent this is due to the fact that the test set
is taken from running text, with a high number of non-compounds.
Overall, the results for all splitting variations are quite low. However, most of the
strategies tend to split too much rather than too little, which is preferable, because the
phrase-based translation system has the possibility of recovering from over-splitting
by handling compounds that are split into too many parts in consistent phrase pairs.
Evaluation towards a one-to-one gold standard is rather harsh because the splitting
algorithm has no knowledge of the corresponding English structure. If we instead use a
gold standard of linguistically motivated compounds, which are many more (545 for the
5,000 word set), precision is much higher for all systems, with mostly a lower recall. For
the arith system, for instance, the precision is more than doubled at .597 with a slightly
lower recall of .522.
The results for the translation task from English into German are shown in
Table 17. In this experiment we used a different selection of 701K training sentences
from Europarl than in Section 8.3, which resulted in overall lower scores both for the
baseline and for the systems with compound processing. Overall, there are very small
differences between both the baseline system and the systems with different types of
splitting, and most of the small differences between the systems are not significant. The
</bodyText>
<tableCaption confidence="0.825003333333333">
Table 16
One-to-one correspondence of split compounds compared with a manually annotated gold
standard for the different splitting methods.
</tableCaption>
<table confidence="0.9797211">
Correct not Wrong split Precision Metrics Accuracy
split not faulty Recall
baseline 0 4,826 174 0 0 – 0 .966
arith 99 4,504 22 52 323 .209 .572 .921
geom 109 4,614 33 31 213 .309 .630 .945
eager 43 4,243 18 112 584 .058 .249 .857
part4 120 4,692 36 17 135 .441 .693 .962
max2 133 4,546 29 11 281 .313 .769 .936
common 99 4,714 58 16 113 .434 .572 .963
anypos 100 4,216 8 65 611 .128 .578 .863
</table>
<page confidence="0.852438">
1094
</page>
<note confidence="0.799968">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<tableCaption confidence="0.805307">
Table 17
Translation results using different splitting methods for German Europarl. Significant changes
from the baseline are marked *(5% level) and **(1% level).
</tableCaption>
<table confidence="0.997891111111111">
Bleu NIST Meteor
baseline+POS 19.1 (0.1) 5.79 (0.02) 26.8 (0.0)
arith 19.0 (0.0) 5.79 (0.01) 26.6 (0.1)
geom 18.9 (0.1)* 5.78 (0.02) 26.5 (0.2)**
eager 18.8 (0.1)** 5.80 (0.01) 26.6 (0.1)
part4 18.8 (0.1)** 5.79 (0.01) 26.5 (0.1)*
max2 18.8 (0.1) 5.80 (0.01) 26.6 (0.1)
common 18.9 (0.1) 5.78 (0.01) 26.6 (0.1)
anypos 18.9 (0.1) 5.75 (0.01)** 26.6 (0.1)
</table>
<bodyText confidence="0.999587923076923">
arith, max2, and common systems are on par with the baseline on all metrics, with no
significant differences, whereas the other systems are worse than the baseline on at least
one metric.
On the intrinsic evaluation there were quite large differences between the systems,
which are not found on the translation task. This is similar to previous work by Koehn
and Knight (2003) for translation in the other direction, where systems similar to the ea-
ger and geom system performed similarly on a translation task, while the eager system
was much worse on their intrinsic evaluation. In our evaluation only the eager system
is significantly worse than any other system on Bleu, where it is worse than the baseline
and the arith system. The arith system overall is competitive with both the baseline and
the other split systems, despite the relatively low results on the intrinsic evaluation.
Overall, it seems that the translation task is not very sensitive to the quality of the
splitting strategy. As in previous research (Koehn and Knight 2003; Stymne 2008) there
are no clear relations between the intrinsic evaluation and the MT evaluation. Both
systems with low intrinsic accuracy (such as anypos) and with high accuracy (such as
geom) tend to be worse than other systems on at least some MT metrics, even though the
differences are small. We thus think that for the task of translation into compounding
languages, the MT performance cannot be predicted based on intrinsic evaluations.
In a previous similar study using a smaller corpus (Stymne 2008), we found
somewhat bigger differences between the systems, and in that study most of the
systems with splitting were better than the baseline. In that study, too, the arith system
was among the best performing on the MT task, even though many of the differences
to the other systems with splitting were not significant. Because the arith strategy
consistently has given good results for the MT task, we chose to use it in our other
experiment. There would, however, have been other reasonable choices of splitting,
such as using the max2 system.
</bodyText>
<subsectionHeader confidence="0.978924">
8.5 Compound Merging
</subsectionHeader>
<bodyText confidence="0.999977714285714">
In the following studies we investigated the impact of the compound merging strategy
used. We compared previously suggested strategies to the new strategies we have
developed. We first present results for heuristic merging strategies, and then go on to
compare the best heuristic methods to a sequence labeling merging strategy. For the
heuristic merging we used German Europarl, and for the comparison with sequence
labeling we used German Europarl and the automotive corpus for Swedish and Danish.
In these experiments we used the arith compound splitting method from Section 8.4.
</bodyText>
<page confidence="0.973724">
1095
</page>
<note confidence="0.456651">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.994272730769231">
8.5.1 Heuristic Merging. Three main types of merging algorithms were investigated in
this study. The first group, inspired by Popovi´c, Stein, and Ney (2006), is based on
frequency lists of words or compounds, and of parts, compiled at split-time. The sec-
ond group uses symbols to guide merging, inspired by work on morphology merging
(Virpioja et al. 2007). The third group takes *POS-tags for compounds into account, so
that merging takes place if and only if the part-of-speech tags match. In addition, we
examined combined merging methods, using either the union or intersection of merges
proposed by different methods. We also extended the list- and symbol-based methods
by a restriction that the head of the compound should have a compounding part of
speech, that is, a noun, adjective, or verb. By using these additions and also combina-
tions of the main algorithms, a total of eleven algorithms were explored, as summarized
in Table 18. For all algorithms, compounds can have an arbitrary number of parts.
In these experiments we used the unmarked representation scheme, which means
that compound modifiers were normalized, but not marked with any symbol, and we
used the EPOS-tagset. To handle normalization, we used the reverse normalization pro-
cess in combination with all merging strategies. If there were any compound modifiers
that could not be combined with the next word, in any of the algorithms, that part was
left as a single word. For frequency calculations for the list-based strategies we used the
German part of the SMT training corpus.
Table 19 shows the translation results using the different merging algorithms. The
merging methods based on lists all give significantly worse results than the baseline
on all metrics, except when combined with the symbol method. The compound-list+
method is the best method of the systems that only uses list information; it is signif-
icantly better than word-list and compound-list on all metrics, but still significantly
worse than the baseline, symbol, and POS-match systems. When using the head-pos
restriction, the results get even better, but here POS-tag information is used in addition
</bodyText>
<tableCaption confidence="0.938465">
Table 18
</tableCaption>
<bodyText confidence="0.425412">
Merging algorithms.
</bodyText>
<subsectionHeader confidence="0.97092">
Name Description
</subsectionHeader>
<bodyText confidence="0.984494933333333">
word-list Merges each token that has been seen as a compound part
with the next part if it results in a known word
word-list + head-pos As word-list, but only words where the last part is a noun,
adjective, or verb are merged
compound-list As word-list, but for known compounds from split-time,
not for all known words
compound-list+ As compound-list, but also includes a check that the
frequency of the resulting compound is higher than the
frequency of the unmerged bigram
symbol Merges all tokens that are marked as compound modifiers
with the next token
symbol + head-pos As symbol, but only merges words where the head is a noun,
adjective, or verb
symbol ∧ word-list Merges marked modifier parts if the result is a known word
POS-match Merges tokens with a compound part-of-speech tag with the
</bodyText>
<footnote confidence="0.6903392">
next token, if their tags match
POS-match + coord As POS-match, but also adds a hyphen if a compound
modifier is followed by the conjunction und [and]
POS-match ∧ compound-list+ Merges tokens if they are selected by both of these methods
POS-match ∨ compound-list+ Merges tokens if they are selected by either of these methods
</footnote>
<page confidence="0.993954">
1096
</page>
<note confidence="0.932631">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<tableCaption confidence="0.99759">
Table 19
</tableCaption>
<table confidence="0.9448162">
Translation results for German Europarl using an EPOS-model and the unmarked representation
scheme. Significant differences (positive or negative) from the baseline are marked **(1% level).
Bleu NIST Meteor
20.2 (0.1) 5.93 (0.02) 27.6 (0.2)
17.9 (0.0) ** 5.70 (0.02) ** 25.8 (0.1)**
19.3 (0.0) ** 5.83 (0.03) ** 27.1 (0.1)**
18.1 (0.0) ** 5.61 (0.02) ** 26.0 (0.1)**
18.7 (0.0) ** 5.66 (0.03) ** 26.6 (0.1)**
20.0 (0.0) 5.96 (0.03) 27.7 (0.1)
20.0 (0.0) 5.95 (0.03) 27.8 (0.1)
20.0 (0.0) ** 5.95 (0.03) 27.8 (0.1)
20.1 (0.1) 5.97 (0.03) 27.7 (0.1)
20.1 (0.0) 5.97 (0.03) 27.8 (0.1)
19.0 (0.1) ** 5.70 (0.03) ** 26.7 (0.1)**
18.8 (0.0) ** 5.74 (0.03) ** 26.3 (0.1)**
</table>
<figure confidence="0.882851333333333">
baseline+POS
word-list
word-list + head-pos
compound-list
compound-list+
symbol
symbol + head-pos
symbol n word-list
POS-match
POS-match + coord
POS-match n compound-list+
POS-match V compound-list+
</figure>
<bodyText confidence="0.979574066666666">
to only frequency list information. There are no significant differences between the
baseline and the systems with symbol or POS-match merging, but the trend on most
metrics is that the POS-match systems have a slightly higher score than the other
methods. However, as shown in Section 8.3 (Table 10), POS-match does block some
erroneous merging. Especially with EPOS the order is so good that the matching con-
straint is rarely needed. Adding treatment of coordinated compounds to the POS-match
algorithm changes scores marginally, but again, as shown in Table 10, it does improve
merging for coordinated compounds. The combination of POS-match and compound-
list+ was not successful, neither as a union nor as an intersection of the strategies.
Table 19 only shows results with an EPOS-model. The result pattern for systems without
an EPOS-model are similar to using an EPOS-model, but lower.
Table 20 shows the number of merges performed by applying the different algo-
rithms. The word-list–based method produces the highest number of merges, including
many cases that should not be merged. The number of merges is greatly reduced by the
head-pos restriction, or by combining the word-list method with some other merging
</bodyText>
<tableCaption confidence="0.935276">
Table 20
</tableCaption>
<table confidence="0.932941894736842">
Number of merges for the different merging algorithms. The numbers can be compared with the
number of compounds found by the splitting algorithm in the reference text, which is 4,472.
with EPOS without EPOS
5,275 5,897
4,161 4,752
4,460 5,116
3,843 4,427
4,431 5,144
4,323 4,832
4,178 4,753
4,363 4,867
4,361 4,865
3,502 4,018
4,706 5,281
word-list
word-list + head-pos
compound-list
compound-list+
symbol
</table>
<footnote confidence="0.651218666666667">
symbol + head-pos
symbol n word-list
POS-match
POS-match + coord
POS-match n compound-list+
POS-match V compound-list+
</footnote>
<page confidence="0.794436">
1097
</page>
<note confidence="0.48534">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.999416621621622">
strategy. An investigation of the output of the word-list–based method shows that it
often merges common words that incidentally form a new word, such as bei [at] and der
[the] to beider [both]. Another type of error is due to errors in the corpus, such as merging
umwelt [environment] and und [and], which exists as a mistyped word in the corpus, but is
not a correct German word. These two error types are often prohibited by the head-pos
restrictions and by the symbol and POS-match algorithms. The extensions of the simple
word-list method avoid these types of errors, resulting in a lower number of merges.
The compound-list+ method has the overall lowest number of splits, but it still is the
best of the methods based only on frequency lists, without the use of POS or symbols.
With the EPOS-model there are very small differences in number of merges between
the methods based on symbol and POS-match, although this difference is much larger
without the EPOS-model. For instance, the difference is 68 merges between symbol and
POS-match with the EPOS-model, whereas without the EPOS-model the difference is
277. This difference shows that the EPOS-model clearly helps in improving coalescence.
The choice of merging method thus has a large impact on the final translation re-
sults. For merging to be successful we found that some knowledge source that is passed
through the translation process, such as EPOS-tags, is needed. The pure word-list–based
method performed the worst of all systems in most cases. On the automatic metrics, the
POS-match and symbol methods gave about the same results as the baseline; we believe
there are other strengths to this method as compared to the baseline system, however,
such as the production of a higher number of compounds. A further advantage of the
POS-match strategy is that it can form novel compounds, which is not the case for either
the baseline or the word-list–based methods.
8.5.2 Merging as Sequence Labeling. Finally, we performed experiments where we com-
pared our suggestion of a sequence labeling merging method to heuristic merging.
We chose two heuristic strategies, the POS-match strategy, which gave the overall best
results, and compound-list+, which gave the best results of the word-list based merging
strategies without any POS or symbol information. We also found that the intersection
of these two strategies worked well on some of our corpora, and decided to include that
as well. In addition, we used a reference-based heuristic for creating the CRF training
data, which identified merges resulting in compounds that are present in the reference
data. Such a heuristic cannot be used at translation-time, however, because there is
no reference data then. In this section we will refer to these heuristics as list for the
compound-list+ heuristic, POS for the POS-match strategy, and ref for the reference-
based heuristic.
For these experiments we used three data sets, as summarized in Table 21. For
the training data for the sequence labeler we translated all of the training data, except
</bodyText>
<tableCaption confidence="0.8349305">
Table 21
Overview of the experimental settings for comparing heuristic and sequence labeling merging.
Training sentences are taken from translated SMT training data, except extra data, which was
not used for SMT training.
</tableCaption>
<table confidence="0.9575616">
Corpus Europarl German Auto Swedish Auto Danish
Compounds split N, V, Adj N, V, Adj N
POS tagsets POS POS RPOS
Training sentences CRF 249,388 317,398 164,702
Extra training sentences CRF – – 163,201
</table>
<page confidence="0.966007">
1098
</page>
<note confidence="0.767166">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<bodyText confidence="0.99995294">
one-word sentences, which were filtered out, for the smaller automotive corpora. For
the Europarl experiment, we limited the training data to the 250,000 first sentences of
the training data, and again filtered out one-word sentences. For the Danish corpora we
also had access to additional data, which we used to control for the effect of reusing
SMT training data. For the machine learning we wanted to use separate development
and test sets. We chose to use the test sets from Table 4 for development testing, where
we used the first 1,000 sentences for German Europarl, and picked new unseen 1,000
sentence test sets for the final evaluation. For frequency calculations of compounds and
compound parts that were needed for compound splitting and some of the compound
merging strategies, we used the target side of the SMT training data.
In these experiments we used the unmarked representation scheme. For German
Europarl we used the unmarked scheme as it is with a binary sequence labeler. Reverse
normalization can be performed afterwards as for the other strategies. For the automo-
tive corpus we used the unmarked scheme without normalization.
We compared alternative combinations of the heuristics on our three validation
data sets (see Figure 4). In order to estimate the amount of false negatives for all three
heuristics, we inspected the first 100 sentences of each validation set, looking for words
that should be merged, but were not marked by any of the heuristics. In no case could
we find any such words, so we thus assumed that between them, the heuristics can
find the overwhelming majority of all compounds to be merged. The differences across
domains, Europarl and automotive, is quite striking, with a much higher number of
compounds found by using the reference-based heuristic for the automotive corpus.
We conducted a round of preliminary experiments to identify the best combination
of the heuristics available at training time (compound-list+, POS-match, and reference-
based) to use to create automatically the training data for the CRF. The best results on
the validation data are obtained by different combinations of heuristics for the three
data sets, as could be expected by the different distribution of errors in Figure 4. In the
following experiments we trained the CRF using for each data set the combination of
heuristics corresponding to leaving out the gray portions of the Venn diagrams. This
sort of preliminary optimization requires hand-labeling a certain amount of data. Based
on our experiments, skipping this optimization and just using ref∨POS (the optimal
configuration for the German–English Europarl corpus) seems to be a reasonable alter-
native.
The validation data was also used to set a frequency cut-off for feature occurrences
(set at 3 in the following experiments) and to tune the regularization parameter in
the CRF objective function. Results are largely insensitive to variations in these hyper-
parameters, especially to the CRF regularization parameter.
For the Danish automotive corpus we had access to training data that had not been
used to train the SMT system, so we could test the performance of the CRF when trained
on that data as compared with training on the possibly biased data that was used to
train the SMT system. Table 22 shows the results with the two types of training data.
For the smallest training size, 20,000 sentences, the unseen training data is significantly
better on recall and F-score. For the other sizes, there are no significant differences when
the same amount of training data is used. When we added the unseen data to the SMT
data, we only saw small non-significant improvements. This indicates that it is feasible
to use translated SMT training data for the sequence labeler. We still decided to use all
available training data for our subsequent experiments on Danish Europarl.
The overall merging results of the heuristics and the best sequence labeler are
shown in Table 23. Notice how the list and POS heuristics have complementary sets
of false negatives: when merging on the union of the two heuristics, the number of false
</bodyText>
<page confidence="0.995496">
1099
</page>
<figure confidence="0.96752825">
Computational Linguistics Volume 39, Number 4
Europarl, German
Automotive, Swedish
Automotive, Danish
</figure>
<figureCaption confidence="0.986494">
Figure 4
</figureCaption>
<bodyText confidence="0.9976146">
Evaluation of the different heuristics on validation files from the three corpora. The number
in each region of the Venn diagrams indicates the number of times a certain combination of
heuristics fired (i.e., the number of positives for that combination). The two smaller numbers
below indicate the number of true and false positive, respectively. Venn diagram regions
corresponding to unreliable combinations of heuristics have corresponding figures on a
gray background. OK means that a large fraction of the Venn cell was inspected, and no false
positive was found.
negatives decreases drastically, in general compensating for the inevitable increase in
false positives.
Among the heuristics, the combination of the improved list heuristic and the POS-
based heuristic has a significantly higher recall and F-score than either heuristic alone
in all cases on test data, and in several cases on validation data. The list heuristic alone
performs reasonably well on the Swedish data set, but has a very low recall on the
German and Danish data sets. In all four cases the SMT training data has been used for
the list used by the heuristic, so this is unexpected, especially considering the fact that
</bodyText>
<page confidence="0.994622">
1100
</page>
<note confidence="0.95294">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<tableCaption confidence="0.985425">
Table 22
</tableCaption>
<table confidence="0.9251305">
Experimental results for CRF on Danish Automotive, with different training data and sizes.
SMT is the same data that was used to train the SMT system, and new is additional data.
Data
SMT
SMT
SMT
SMT
new
new
new
new
all SMT + new
all SMT + new
all SMT + new
all SMT + new
Size Precision Recall F-score
20K .977 .970 .974
50K .979 .977 .978
100K .977 .980 .978
165K .977 .986 .982
20K .977 .984 .981
50K .975 .986 .981
100K .978 .991 .984
163K .978 .991 .984
165K + 20K .978 .991 .984
165K + 50K .982 .993 .988
165K + 100K .980 .991 .985
165K + 163K .978 .993 .985
</table>
<tableCaption confidence="0.993615">
Table 23
</tableCaption>
<bodyText confidence="0.49988725">
Precision, Recall, and F-score for compound merging methods based on heuristics or sequence
labeling on validation data and on held-out test data. The superscripts mark the systems that are
significantly worse than the system in question (l-list, p-POS, lp-list∨POS, c-best CRF
configuration).
</bodyText>
<table confidence="0.999786">
 |Validation data Precision Test data F-score
Precision Recall F-score Recall
German Europarl
list .967 .724 .828 .952 .717 .818
POS .991l,lp .978l .984l .997l,lp .980l .989l
list∨POS .967 .999l,p,c .983l .963l 1l,p,c .981l,p
CRF (ref∨POS) .982l,lp .990l,p .986l,p .983l,lp .996l,p .989l,lp
Swedish auto
list .989p,lp .994p .991p .990 .977 .984
POS .976 .963 .969 .992lp .974 .983
list∨POS .972 1p .986p .982 .998l,p,c .990l,p
CRF (ref∨list) .987p,lp .998p .993p,lp .987 .987 .987
Danish auto
list .925 .760 .835 .991lp .764 .863
POS .981l,lp .964l .972l,lp .978 .929l .954l
list∨POS .925 .986l,p .955l .976 .988l,p,c .982l,p,c
CRF (ref∨list∨POS) .978l,lp .993l,p .985l,p,lp .978 .966l,p .972l,p
</table>
<bodyText confidence="0.975041571428571">
the Danish data set is in the same domain as one of the Swedish data sets. The Danish
training data is smaller than the Swedish data though, which might be an influencing
factor. It is possible that this heuristic could perform better on the other data sets given
more data for frequency calculations.
The sequence labeler is competitive with the heuristics; on F-score it is only signif-
icantly worse than any of the heuristics once, for Danish auto test data, and in several
cases it has a significantly higher F-score than some of the heuristics. The sequence
</bodyText>
<page confidence="0.979549">
1101
</page>
<note confidence="0.583627">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.999957518518519">
labeler has a higher precision, significantly so in four cases, than the best heuristic, the
combination heuristic, which is positive (because erroneously merged compounds are
usually more disturbing for a reader or post-editor than non-merged compounds).
The differences between the systems on MT metrics are generally small and mostly
not significant, as could be expected, since the differences were small on the intrinsic
evaluation. For German Europarl only 10–739 out of 29,786 words are actually different
between any pair of systems. The only noticeable differences between the systems are
that the list system is worse than the other systems for Danish and German. The list
strategy is competitive for Swedish automotive, which is much bigger than Danish, and
in one of our previous experiments, with a large Swedish Europarl corpus (Stymne and
Cancedda 2011), confirming our intuition that the list strategy tends to work better with
large corpora. Even though we do not see much effect on MT metrics, we still think
the small differences on the intrinsic evaluation are important. Compound merging
is a postprocessing process, which operates directly on the actual translation results.
This is quite different from compound splitting, as described in Section 8.4, which is
performed as part of the preprocessing and where we cannot be sure on the impact of
the preprocessing on the translation process.
The sequence labeler has the advantage over the heuristics that it is able to merge
completely novel compounds, whereas the list strategy can only merge compounds
that it has seen, and the POS-based strategy can create novel compounds, but only
with known modifiers. An inspection of the test data showed that there were some
novel compounds merged by the sequence labeler that were not identified with either
of the heuristics. In the test data we found knap+start (button start) and vand+nedsxnkning
(water submersion) for Danish, and kvarts sekel (quarter century) and bostad(s)+ers¨attning
(housing grant) for Swedish. This confirms that the sequence labeler, trained from auto-
matically labeled data based on heuristics, can learn to merge new compounds that the
heuristics themselves cannot find.
</bodyText>
<subsectionHeader confidence="0.995375">
8.6 Effects of Corpus Size and Domain
</subsectionHeader>
<bodyText confidence="0.995341666666667">
Most of the previous experiments were performed on relatively small data sets, and it
has been shown that effects of preprocessing strategies sometimes are larger on small
corpora (e.g., NieBen and Ney 2004; Popovi´c and Ney 2006). To investigate this issue,
we first present a scaling experiment for small data sets from German Europarl, and
then report results on two large data sets for translation from English to German on
Europarl and news data.
Table 24 shows the result for baseline and EPOS systems trained on 100K and 440K
sentences. In the smaller data condition the results are similar for the baseline and the
EPOS system. With 440K data the EPOS system is significantly better than the baseline
</bodyText>
<tableCaption confidence="0.891571333333333">
Table 24
Results for translation on German Europarl with small training data sets. Significant
improvements over the respective baseline+POS are marked **(1% level).
</tableCaption>
<table confidence="0.9986852">
System Bleu NIST Meteor
100K baseline+POS 16.8 (0.0) 5.37 (0.02) 24.6 (0.1)
100K EPOS 16.9 (0.1) 5.39 (0.02) 24.8 (0.1)
440K baseline+POS 19.3 (0.1) 5.76 (0.05) 26.7 (0.2)
440K EPOS 19.7 (0.2) ** 5.83 (0.03) ** 27.0 (0.2)**
</table>
<page confidence="0.915354">
1102
</page>
<note confidence="0.875471">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<tableCaption confidence="0.59942125">
Table 25
Overview of the corpus for large evaluation sets, and the corpus sizes in sentences. The names of
the corpora refer to those used for the WMT evaluations. For the tuning data only a subset of the
full data was used.
</tableCaption>
<table confidence="0.99960275">
WMT08 WMT10 Size
Name/type Size Name/type
Translation models Europarl 1,054,688 Europarl+NewsComm 1,462,401
Language model 1 Europarl 1,412,546 Europarl+NewsComm 1,885,872
Language model 2 – News 17,449,589
Tuning data dev2006 600 news-test2008 1,025
Test Set Europarl test2007 2,000 –
Test Set News nc-test2007 2,007 news-test2009 2,525
</table>
<tableCaption confidence="0.985834">
Table 26
</tableCaption>
<table confidence="0.9954335">
Results on WMT08 data on Europarl and News test sets. Significantly different results from the
baseline are marked with *(5% level) and **(1% level), significantly different results from
baseline+POS are marked similarly with #.
System Bleu Europarl Meteor Bleu News Meteor
NIST NIST
Baseline 19.9 (0.1) 5.89 (0.01) 27.6 (0.0) 14.3 (0.3) 5.61 (0.09) 24.0 (0.5)
Baseline+POS 20.1 (0.1)* 5.90 (0.01) 27.8 (0.2) 14.4 (0.3)* 5.60 (0.09) 23.9 (0.4)
EPOS-unmarked 20.1 (0.0) 5.95 (0.01)*# 27.6 (0.2) 14.7 (0.4)**# 5.60 (0.07) 24.1 (0.6)*
</table>
<bodyText confidence="0.999901809523809">
on all three metrics. We thus see that our compound processing method works well
on medium-sized data sets as well, and is competitive to the baseline on very small
data sets.
In our final experiments we used larger data sets from the workshop of statistical
machine transition, year 2008 and 2010, which we will refer to as WMT08 and WMT10.7
The corpora used are presented in Table 25. In these experiments we also investigated
the effects of in- and out-of-domain data. For WMT08 we trained and tuned on only
Europarl data, and tested both in-domain, on Europarl and out-of-domain, on News.
For WMT10 we trained on a mix of data, and tested on News. For these experiments
we used morphologically enriched POS-tagsets. For WMT08 we used a commercial
dependency parser for the morphology (Tapanainen and J¨arvinen 1997), and for
WMT10 we used RFTagger (Schmid and Laws 2008). For the systems with split com-
pounds we used the EPOS-tagset based on morphological tags. We used the POS-match
merging algorithm.
Table 26 shows the results on the WMT08 evaluation. On Europarl the system with
compound processing and an EPOS-model is significantly better than both baselines on
NIST, and on the News domain it is significantly better than both baselines on Bleu. On
the other metrics the EPOS system is on par with the baseline for both domains. The
differences between the two baselines are mostly small. Overall, the scores are much
lower on the out-of-domain data. Table 27 shows the results for the WMT10 evaluation,
where more data was used to train the systems. The EPOS system is significantly better
</bodyText>
<footnote confidence="0.664847">
7 http://www.statmt.org/wmt08/, http://www.statmt.org/wmt10/.
</footnote>
<page confidence="0.901975">
1103
</page>
<note confidence="0.405378">
Computational Linguistics Volume 39, Number 4
</note>
<tableCaption confidence="0.98564">
Table 27
</tableCaption>
<table confidence="0.9881818">
Results on WMT10 data on News test set. Significance shown as in Table 26.
System Bleu NIST Meteor
Baseline 13.5 (0.1) 5.29 (0.04) 21.0 (0.2)
Baseline+POS 13.8 (0.1)* 5.35 (0.01) 21.3 (0.2)
EPOS-unmarked 14.1 (0.3)**## 5.35 (0.07)** 21.4 (0.1)*
</table>
<bodyText confidence="0.861340666666667">
than the baseline without POS on all metrics, and also significantly better than the
baseline with POS on Bleu. This shows that the suggested compound processing is
effective also on larger data sets and out-of-domain data.
</bodyText>
<sectionHeader confidence="0.994981" genericHeader="conclusions">
9. Conclusion
</sectionHeader>
<bodyText confidence="0.999985171428571">
In this article, we have investigated several methods for splitting and merging com-
pounds when translating into languages with closed compounds, with the goal of
generating novel compounds. We have described methods for promoting coalescence
and for deciding if and how to merge words that are either competitive with, or superior
to, any previously published method. A common trait of all these methods is that they
promote coalescence and allow formation of novel compounds by using customized
*POS-tagsets. This shows that the integration of light-weight linguistic information
through POS-tags is useful for compound processing into compounding languages.
We believe that some of the techniques, such as customized POS-sequence models,
could be used to target morphological phenomena other than compounding as well. We
also believe that the suggested methods are likely to be useful for other compounding
languages such as Finnish or Japanese.
For promoting compound coalescence we introduced additional LMs based on
customized POS-tagsets, and with dedicated SMT model features counting the number
of sequences known a priori to be desirable and undesirable. Experiments showed
consistently good results when using the extended tagset EPOS in sequence models.
The other tagsets were less successful, but on an automotive corpora a restricted tagset,
RPOS worked well both in sequence models and using count features.
For compound splitting, previously proposed strategies for the opposite translation
direction could be modified to improve performance when translating into German. We
also confirmed previous results that showed no correlations between intrinsic evalua-
tions of compound splitting and translation results in the other translation direction.
For merging we designed a heuristic algorithm based on part-of-speech match-
ing, which gave consistently good results. We also improved existing heuristics based
on frequency lists, which worked well on some data sets. We furthermore cast the
compound merging problem as a sequence labeling problem, opening it to solutions
based on a broad array of models and algorithms. We experimented with one model,
conditional random fields, designed a set of easily computed features reaching beyond
the information accessed by the heuristics, and showed that it gives very competitive
results, even trained on automatically labeled data.
Our goal was to identify methods that can generate novel compounds. Depending
on the choice of the features, the sequence labeling approach has the potential to be
truly productive, that is, to form new compounds in an unrestricted way. The list-based
heuristic is not productive: It can only form a compound if this was already observed
as such. The POS-match heuristic yields some productivity. Because it uses special
</bodyText>
<page confidence="0.982477">
1104
</page>
<note confidence="0.74649">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<bodyText confidence="0.99990168">
POS-tags for compound modifiers, it can form a compound provided its head has been
seen either alone or as a head, and its modifier(s) have been seen elsewhere, possibly
separately, as modifier(s) of compounds. The sequence labeling approach can decide to
merge two consecutive words even if neither was ever seen before in a compound.
In the merging step we only considered the order of words as they appear in
the 1-best output. Other options would include also attempting reordering of words
in the output in addition to merging, or to use lattice or n-best output and perform
compound merging and reranking in combination. However, we believe that the use
of sequence models for coalescence gives sufficiently good results and minimizes the
potential impact of more costly options.
In this work we mainly viewed the translation system as a black box. Our mod-
ifications to the PBSMT pipeline were based on pre- and postprocessing and on the
use of *POS-sequence models. It is possible that further improvements can be had by
integrating compound processing tighter into the decoder.
The rate of novel compounds is relatively low in the data sets used in our experi-
ments, mainly due to the fact that most experiments are carried out with in-domain test
data. However, the best methods tend to vary between the domain-restricted automo-
tive corpora and Europarl. The importance of effective compound splitting and merging
can be expected to grow in domain adaptation settings, outside the scope of this work.
Overall we have shown that systems with compound processing give consistently
good results when they use POS-based language models and use either the POS-match
heuristic or sequence labeling for compound merging. The difference between a base-
line using a POS-model but no compound splitting and the split-merge EPOS-models
is small in all experiments and does not seem to increase with the size of the training
corpus. However, only the latter can produce novel compounds.
</bodyText>
<sectionHeader confidence="0.997297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994652111111111">
We would like to thank Maria Holmqvist
and Tam´as Ga´al for valuable discussions
about different aspects of the work in this
article. We also thank the anonymous
reviewers for their valuable comments on an
earlier draft. The work presented in the
article was mostly conducted while S.
Stymne was at Link¨oping University and
Xerox Research Centre Europe.
</bodyText>
<sectionHeader confidence="0.998806" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987221571428571">
Badr, Ibrahim, Rabih Zbib, and James Glass.
2008. Segmentation for English-to-Arabic
statistical machine translation. In
Proceedings of the 46th Annual Meeting of the
ACL: Human Language Technologies, Short
papers, pages 153–156, Columbus, OH.
Banerjee, Satanjeev and Alon Lavie. 2005.
METEOR: An automatic metric for MT
evaluation with improved correlation
with human judgments. In Proceedings
of the Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or
Summarization at ACL’05, pages 65–72,
Ann Arbor, MI.
Baroni, Marco, Johannes Matiasek, and
Harald Trost. 2002. Predicting the
components of German nominal
compounds. In Proceedings of the 15th
European Conference on Artificial Intelligence
(ECAI), pages 470–474, Amsterdam.
Berton, Andr´e, Pablo Fetter, and Peter
Regel-Brietzmann. 1996. Compound
words in large-vocabulary German
speech recognition systems. In Proceedings
of the Fourth International Conference
on Spoken Language Processing,
pages 1,165–1,168, Philadelphia, PA.
Birch, Alexandra, Miles Osborne, and
Philipp Koehn. 2008. Predicting success
in machine translation. In Proceedings of
the 2008 Conference on Empirical Methods
in Natural Language Processing,
pages 745–754, Honolulu, HI.
Botha, Jan A., Chris Dyer, and Phil Blunsom.
2012. Bayesian language modelling
of German compounds. In Proceedings
of the 24th International Conference on
Computational Linguistics, pages 341–356,
Mumbai.
Brodda, Benny. 1979. N˚agot om de svenska
ordens fonotax och morfotax: Iakttagelse
med utg˚angspunkt fr˚an experiment
</reference>
<page confidence="0.966428">
1105
</page>
<note confidence="0.519541">
Computational Linguistics Volume 39, Number 4
</note>
<reference confidence="0.995535618644068">
med automatisk morfologisk analys. In PILUS
nr 38. Inst. f¨or lingvistik, Stockholms
universitet, Sweden.
Brown, Ralf D. 2002. Corpus-driven splitting
of compound words. In Proceedings of the
9th International Conference of Theoretical and
Methodological Issues in Machine Translation,
pages 12–21, Keihanna.
Carlberger, Johan, Rickard Domeij, Viggo
Kann, and Ola Knutsson. 2005. The
development and performance of a
grammar checker for Swedish: A language
engineering perspective. In Ola Knutsson,
editor, Developing and Evaluating Language
Tools for Writers and Learners of Swedish.
Ph.D. thesis, Royal Institute of Technology
(KTH), Stockholm, Sweden.
Carlberger, Johan and Viggo Kann. 1999.
Implementing an efficient part-of-speech
tagger. Software Practice and Experience,
29:815–832.
Cherry, Colin. 2008. Cohesive phrase-based
decoding for statistical machine
translation. In Proceedings of the
46th Annual Meeting of the ACL: Human
Language Technologies, pages 72–80,
Columbus, OH.
Clark, Jonathan H., Chris Dyer, Alon Lavie,
and Noah A. Smith. 2011. Better
hypothesis testing for statistical machine
translation: Controlling for optimizer
instability. In Proceedings of the 49th Annual
Meeting of the ACL: Human Language
Technologies, pages 176–181, Portland, OR.
Collins, Michael. 2002. Discriminative
training methods for hidden Markov
models: Theory and experiments with
perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in
Natural Language Processing, pages 1–8,
Philadelphia, PA.
Collins, Michael and Terry Koo. 2005.
Discriminative reranking for natural
language parsing. Computational
Linguistics, 31(1):25–69.
Cutting, Doug, Julian Kupiec, Jan Pedersen,
and Penelope Sibun. 1992. A practical
part-of-speech tagger. In Proceedings
of the Third Conference on Applied Natural
Language Processing, pages 133–140, Trento.
Doddington, George. 2002. Automatic
evaluation of machine translation quality
using n-gram co-occurence statistics.
In Proceedings of the Second International
Conference on Human Language Technology,
pages 228–231, San Diego, CA.
Dyer, Chris. 2009. Using a maximum entropy
model to build segmentation lattices for
MT. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of
the NAACL, pages 406–414, Boulder, CO.
Dyer, Chris. 2010. A Formal Model of
Ambiguity and its Applications in Machine
Translation. Ph.D. thesis, University of
Maryland, USA.
El-Kahlout, ˙Ilknur Durgar and Kemal
Oflazer. 2006. Initial explorations in
English to Turkish statistical machine
translation. In Proceedings of the Workshop
on Statistical Machine Translation,
pages 7–14, New York, NY.
El Kholy, Ahmed and Nizar Habash. 2010.
Techniques for Arabic morphological
detokenization and orthographic
denormalization. In LREC 2010 Workshop
on Language Resources and Human Language
Technology for Semitic Languages,
pages 45–51, Valletta.
Fraser, Alexander. 2009. Experiments in
morphosyntactic processing for translating
to and from German. In Proceedings of the
Fourth Workshop on Statistical Machine
Translation, pages 115–119, Athens.
Friberg, Karin. 2007. Decomposing Swedish
compounds using memory-based learning.
In Proceedings of the 16th Nordic Conference
on Computational Linguistics
(NODALIDA’07), pages 224–230, Tartu.
Fritzinger, Fabienne and Alexander Fraser.
2010. How to avoid burning ducks:
Combining linguistic analysis and
corpus statistics for German compound
processing. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation
and MetricsMATR, pages 224–234, Uppsala.
Hedlund, Turid. 2002. Compounds in
dictionary-based cross-language
information retrieval. Information Research,
7(2). Available at http://InformationR.
net/ir/7-2/paper128.html. Accessed
January 29, 2013.
Holmqvist, Maria, Sara Stymne, and Lars
Ahrenberg. 2007. Getting to know Moses:
Initial experiments on German–English
factored translation. In Proceedings of the
Second Workshop on Statistical Machine
Translation, pages 181–184, Prague.
Holz, Florian and Chris Biemann. 2008.
Unsupervised and knowledge-free
learning of compound splits and
periphrases. In Proceedings of the
9th International Conference on Intelligent
Text Processing and Computational
Linguistics (CICLING), pages 117–127,
Haifa.
Institut f¨ur Deutsche Sprache. 1998.
Rechtschreibreform (Aktualisierte
Ausgabe). IDS Sprachreport,
</reference>
<page confidence="0.848443">
1106
</page>
<note confidence="0.573418">
Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT
</note>
<reference confidence="0.999794567796611">
Extra-Ausgabe Dezember 1998.
Mannheim, Germany.
Koehn, Philipp. 2005. Europarl: A parallel
corpus for statistical machine translation.
In Proceedings of MT Summit X,
pages 79–86, Phuket.
Koehn, Philipp, Abhishek Arun, and Hieu
Hoang. 2008. Towards better machine
translation quality for the German–English
language pairs. In Proceedings of the Third
Workshop on Statistical Machine Translation,
pages 139–142, Columbus, OH.
Koehn, Philipp and Hieu Hoang. 2007.
Factored translation models. In Proceedings
of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning,
pages 868–876, Prague.
Koehn, Philipp, Hieu Hoang, Alexandra
Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical
machine translation. In Proceedings of the
45th Annual Meeting of the ACL, Demo and
Poster Sessions, pages 177–180, Prague.
Koehn, Philipp and Kevin Knight. 2003.
Empirical methods for compound
splitting. In Proceedings of the 10th
Conference of the EACL, pages 187–193,
Budapest.
Kokkinakis, Dimitros. 2001. A Framework
for the Acquisition of Lexical Knowledge:
Description and Applications. Ph.D. thesis,
G¨oteborg University, Sweden.
K¨urschner, Sebastian. 2003. Von
Volk-s-musik und Sport-ø-geist im
Lemming-ø-land – Af folk-e-musik og
sport-s-˚and i lemming-e-landet:
Fugenelemente im Deutschen und
D¨anischen – eine kontrastive
Studie zu einem Grenzfall der
Morphologie. Master’s thesis,
Albert-Ludwigs-Universit¨at,
Freiburg, Germany.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data.
In Proceedings of the 18th International
Conference on Machine Learning,
pages 282–289, Williamstown, MA.
Langer, Stefan. 1998. Zur Morphologie
und Semantik von Nominalkomposita.
In Tagungsband der 4. Konferenz zur
Verarbeitung nat¨urlicher Sprache
(KONVENS), pages 83–97, Bonn.
Larson, Martha, Daniel Willett, Joachim
K¨ohler, and Gerhard Rigoll. 2000.
Compound splitting and lexical unit
recombination for improved performance
of a speech recognition system for German
parliamentary speeches. In Proceedings
of the Sixth International Conference on
Spoken Language Processing, volume 3,
pages 945–948, Beijing.
Lavie, Alon and Abhaya Agarwal. 2007.
METEOR: An automatic metric for MT
evaluation with high levels of correlation
with human judgments. In Proceedings of
the Second Workshop on Statistical Machine
Translation, pages 228–231, Prague.
Macherey, Klaus, Andrew Dai, David Talbot,
Ashok Popat, and Franz Och. 2011.
Language-independent compound
splitting with morphological operations.
In Proceedings of the 49th Annual Meeting of
the ACL: Human Language Technologies,
pages 1,395–1,404, Portland, OR.
Nießen, Sonja and Hermann Ney. 2000.
Improving SMT quality with
morpho-syntactic analysis. In Proceedings
of the 18th International Conference on
Computational Linguistics, pages
1,081–1,085, Saarbr¨ucken.
Nießen, Sonja and Hermann Ney. 2004.
Statistical machine translation with scarce
resources using morpho-syntactic
information. Computational Linguistics,
30(2):181–204.
Och, Franz Josef. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of the 42nd Annual
Meeting of the ACL, pages 160–167,
Sapporo.
Och, Franz Josef and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19–51.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Meeting of the ACL,
pages 311–318, Philadelphia, PA.
Popovi´c, Maja and Hermann Ney. 2006.
Statistical machine translation with a
small amount of bilingual training data.
In 5th LREC SALTMIL Workshop on
Minority Languages, pages 25–29, Genoa.
Popovi´c, Maja, Daniel Stein, and Hermann
Ney. 2006. Statistical machine translation
of German compound words. In
Proceedings of FinTAL – 5th International
Conference on Natural Language Processing,
pages 616–624, Turku.
</reference>
<page confidence="0.786165">
1107
</page>
<reference confidence="0.995326770642201">
Computational Linguistics Volume 39, Number 4
Rabiner, Lawrence R. 1989. A tutorial on
hidden Markov models and selected
applications in speech recognition.
Proceedings of IEEE, 77(2):257–286.
Riezler, Stefan and John T. Maxwell.
2005. On some pitfalls in automatic
evaluation and significance testing for
MT. In Proceedings of the Workshop on
Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization at ACL’05,
pages 57–64, Ann Arbor, MI.
Sarawagi, Sunita and William W. Cohen.
2004. Semi-Markov conditional random
fields for information extraction.
In Advances in Neural Information
Processing Systems 17 (NIPS),
pages 1,185–1,192, Cambridge, MA.
Schiller, Anne. 2005. German compound
analysis with WFSC. In Proceedings of the
Finite State Methods and Natural Language
Processing, pages 239–246, Helsinki.
Schmid, Helmut. 1994. Probabilistic
part-of-speech tagging using decision
trees. In Proceedings of the International
Conference on New Methods in Language
Processing, pages 44–49, Manchester.
Schmid, Helmut and Florian Laws. 2008.
Estimation of conditional probabilities
with decision trees and an application to
fine-grained POS tagging. In Proceedings
of the 22nd International Conference on
Computational Linguistics, pages 777–784,
Manchester.
Simard, Michel, Nicola Cancedda, Bruno
Cavestro, Marc Dymetman, Eric Gaussier,
Cyril Goutte, Kenji Yamada, Philippe
Langlais, and Arne Mauser. 2005.
Translating with non-contiguous phrases.
In Proceedings of the Human Language
Technology Conference and the Conference on
Empirical Methods in Natural Language
Processing, pages 755–762, Vancouver.
Sj¨obergh, Jonas and Viggo Kann. 2004.
Finding the correct interpretation of
Swedish compounds, a statistical
approach. In Proceedings of the 4th
International Conference on Language
Resources and Evaluation (LREC’04),
pages 899–902, Lisbon.
Stolcke, Andreas. 2002. SRILM—an
extensible language modeling toolkit.
In Proceedings of the Seventh International
Conference on Spoken Language Processing,
pages 901–904, Denver, CO.
Stymne, Sara. 2008. German compounds in
factored statistical machine translation.
In Proceedings of GoTAL – 6th International
Conference on Natural Language Processing,
pages 464–475, Gothenburg.
Stymne, Sara. 2009. A comparison of
merging strategies for translation of
German compounds. In Proceedings of the
EACL 2009 Student Research Workshop,
pages 61–69, Athens.
Stymne, Sara and Nicola Cancedda. 2011.
Productive generation of compound
words in statistical machine translation.
In Proceedings of the Sixth Workshop
on Statistical Machine Translation,
pages 250–260, Edinburgh.
Stymne, Sara and Maria Holmqvist.
2008. Processing of Swedish compounds
for phrase-based statistical machine
translation. In Proceedings of the
12th Annual Conference of the European
Association for Machine Translation,
pages 180–189, Hamburg.
Stymne, Sara, Maria Holmqvist, and Lars
Ahrenberg. 2008. Effects of morphological
analysis in translation between German
and English. In Proceedings of the Third
Workshop on Statistical Machine Translation,
pages 135–138, Columbus, OH.
Tapanainen, Paso and Timo J¨arvinen.
1997. A nonprojective dependency parser.
In Proceedings of the 5th Conference on
Applied Natural Language Processing,
pages 64–71, Washington, DC.
Taskar, Ben, Carlos Guestrin, and Daphne
Koller. 2003. Max-margin Markov
networks. In Advances in Neural
Information Processing Systems 16 (NIPS),
pages 25–32, Vancouver.
Thorell, Olof.1981. Svensk ordbildningsl¨ara.
Esselte Studium, Stockholm, Sweden.
Tsochantaridis, Ioannis, Thorsten Joachims,
Thomas Hofmann, and Altun Yasemin.
2005. Large margin methods for structured
and interdependent output variables.
Journal of Machine Learning Research,
6:1,453–1,484.
Virpioja, Sami, Jaako J. V¨ayrynen, Mathias
Creutz, and Markus Sadeniemi. 2007.
Morphology-aware statistical machine
translation based on morphs induced
in an unsupervised manner.
In Proceedings of MT Summit XI,
pages 491–498, Copenhagen.
</reference>
<page confidence="0.996247">
1108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.703692">
<title confidence="0.997636666666667">Generation of Compound Words in Statistical Machine Translation into Compounding Languages</title>
<author confidence="0.9703035">Sara Stymne Nicola Cancedda</author>
<affiliation confidence="0.952166">Research Centre</affiliation>
<author confidence="0.881156">Lars Ahrenberg</author>
<abstract confidence="0.992935933333333">In this article we investigate statistical machine translation (SMT) into Germanic languages, with a focus on compound processing. Our main goal is to enable the generation of novel compounds that have not been seen in the training data. We adopt a split-merge strategy, where compounds are split before training the SMT system, and merged after the translation step. This approach reduces sparsity in the training data, but runs the risk of placing translations of compound parts in non-consecutive positions. It also requires a postprocessing step of compound merging, where compounds are reconstructed in the translation output. We present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order and show that it can lead to improvements both by direct inspection and in terms of standard translation evaluation metrics. We also propose several new methods for compound merging, based on heuristics and machine learning, which outperform previously suggested algorithms. These methods can produce novel compounds and a translation with at least the same overall quality as the baseline. For all subtasks we show that it is useful to include part-of-speech based information in the translation process, in order to handle compounds.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ibrahim Badr</author>
<author>Rabih Zbib</author>
<author>James Glass</author>
</authors>
<title>Segmentation for English-to-Arabic statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the ACL: Human Language Technologies, Short papers,</booktitle>
<pages>153--156</pages>
<location>Columbus, OH.</location>
<marker>Badr, Zbib, Glass, 2008</marker>
<rawString>Badr, Ibrahim, Rabih Zbib, and James Glass. 2008. Segmentation for English-to-Arabic statistical machine translation. In Proceedings of the 46th Annual Meeting of the ACL: Human Language Technologies, Short papers, pages 153–156, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at ACL’05,</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="56259" citStr="Banerjee and Lavie 2005" startWordPosition="8834" endWordPosition="8837">ation based on gradient descent for smoothed NIST. We extended the original Matrax decoder with factored decoding on the target side. Compounds were split before training using the corpus-based method described in Section 5. Except for the experiments comparing different compound merging methods, we used the POS-match merging algorithm developed by us. We report results on three metrics: Bleu (Papineni et al. 2002), NIST (Doddington 2002), and Meteor. For Meteor we use the version tuned on adequacy and fluency (Lavie and Agarwal 2007) for German, and the original version with default weights (Banerjee and Lavie 2005) for Swedish and Danish, since there is no tuned version for those languages. For Bleu and Meteor we use the %-notation. Significance testing was performed using approximate randomization (Riezler and Maxwell 2005), with 10,000 iterations, on output based on three optimizer runs, as recommended by Clark et al. (2011). 8.2 General Effects of the Compound Processing Strategy One effect of compound splitting is that the number of word types is reduced. Tables 5 and 6 show the number of types (number of unique words) and tokens (total number of words) and the type/token ratio for Europarl in the d</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Banerjee, Satanjeev and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at ACL’05, pages 65–72, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Johannes Matiasek</author>
<author>Harald Trost</author>
</authors>
<title>Predicting the components of German nominal compounds.</title>
<date>2002</date>
<booktitle>In Proceedings of the 15th European Conference on Artificial Intelligence (ECAI),</booktitle>
<pages>470--474</pages>
<location>Amsterdam.</location>
<marker>Baroni, Matiasek, Trost, 2002</marker>
<rawString>Baroni, Marco, Johannes Matiasek, and Harald Trost. 2002. Predicting the components of German nominal compounds. In Proceedings of the 15th European Conference on Artificial Intelligence (ECAI), pages 470–474, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Berton</author>
<author>Pablo Fetter</author>
<author>Peter Regel-Brietzmann</author>
</authors>
<title>Compound words in large-vocabulary German speech recognition systems.</title>
<date>1996</date>
<booktitle>In Proceedings of the Fourth International Conference on Spoken Language Processing,</booktitle>
<pages>1--165</pages>
<location>Philadelphia, PA.</location>
<marker>Berton, Fetter, Regel-Brietzmann, 1996</marker>
<rawString>Berton, Andr´e, Pablo Fetter, and Peter Regel-Brietzmann. 1996. Compound words in large-vocabulary German speech recognition systems. In Proceedings of the Fourth International Conference on Spoken Language Processing, pages 1,165–1,168, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Predicting success in machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>745--754</pages>
<location>Honolulu, HI.</location>
<marker>Birch, Osborne, Koehn, 2008</marker>
<rawString>Birch, Alexandra, Miles Osborne, and Philipp Koehn. 2008. Predicting success in machine translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 745–754, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan A Botha</author>
<author>Chris Dyer</author>
<author>Phil Blunsom</author>
</authors>
<title>Bayesian language modelling of German compounds.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics,</booktitle>
<pages>341--356</pages>
<location>Mumbai.</location>
<marker>Botha, Dyer, Blunsom, 2012</marker>
<rawString>Botha, Jan A., Chris Dyer, and Phil Blunsom. 2012. Bayesian language modelling of German compounds. In Proceedings of the 24th International Conference on Computational Linguistics, pages 341–356, Mumbai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benny Brodda</author>
</authors>
<title>N˚agot om de svenska ordens fonotax och morfotax: Iakttagelse med utg˚angspunkt fr˚an experiment med automatisk morfologisk analys.</title>
<date>1979</date>
<booktitle>In PILUS nr 38. Inst. f¨or lingvistik, Stockholms universitet,</booktitle>
<contexts>
<context position="47357" citStr="Brodda (1979)" startWordPosition="7445" endWordPosition="7446">eatures. Each sequence item (i.e., each separation point between words) is represented by means of a vector of features. Our aim was to include features representing the knowledge available to the heuristics, such as part-of-speech tags, frequencies for compounds and bigrams, as well as comparisons between them. Features were also inspired by previous work on compound splitting, with the intuition that features that are useful for splitting compounds could also be useful for merging. Character n-grams have successfully been used for splitting Swedish compounds, as the only knowledge source by Brodda (1979), and as one of several knowledge sources by Sj¨obergh and Kann (2004). Friberg (2007) tried to normalize letters, besides using the original letters. Although she was not successful, we still believe in the potential of this feature. Larson et al. (2000) used frequencies of prefixes and suffixes from a corpus as a basis of their method for splitting German compounds. We used the following features where -N refers to the nth position before the merge point, and +N to the nth position after the merge point: • Previous tag • Surface words: word-2, word-1, word+1, bigram word-1–word+1 • Parts of </context>
</contexts>
<marker>Brodda, 1979</marker>
<rawString>Brodda, Benny. 1979. N˚agot om de svenska ordens fonotax och morfotax: Iakttagelse med utg˚angspunkt fr˚an experiment med automatisk morfologisk analys. In PILUS nr 38. Inst. f¨or lingvistik, Stockholms universitet, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf D Brown</author>
</authors>
<title>Corpus-driven splitting of compound words.</title>
<date>2002</date>
<booktitle>In Proceedings of the 9th International Conference of Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>12--21</pages>
<contexts>
<context position="11962" citStr="Brown 2002" startWordPosition="1788" endWordPosition="1789">und, which makes up 62% of the compounds in the German news corpus of Baroni, Matiasek, and Trost. 3. Related Work The problems arising from differences in compounding strategies in translation from German into English have been addressed by several authors. The most common architecture for translation from German is to split compounds in a preprocessing step prior to training and translation using some automatic method, which has been suggested both for SMT (Nießen and Ney 2000; Koehn and Knight 2003; Popovi´c, Stein, and Ney 2006; Holmqvist, Stymne, and Ahrenberg 2007) and example-based MT (Brown 2002). German compounds are split into their component parts in a preprocessing step and the translation model is then trained between modified German and English. At translation time, the German source text is also run through a compound splitter. In the studies cited here, only one splitting option is given as input to the decoder, which can be problematic in case the splitting is wrong, or if any of the parts are unknown. In Dyer (2009) several splitting options were given to the decoder in the form of a lattice. It is, however, not straightforward to use lattices during training, and in order t</context>
</contexts>
<marker>Brown, 2002</marker>
<rawString>Brown, Ralf D. 2002. Corpus-driven splitting of compound words. In Proceedings of the 9th International Conference of Theoretical and Methodological Issues in Machine Translation, pages 12–21, Keihanna.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Carlberger</author>
<author>Rickard Domeij</author>
<author>Viggo Kann</author>
<author>Ola Knutsson</author>
</authors>
<title>The development and performance of a grammar checker for Swedish: A language engineering perspective.</title>
<date>2005</date>
<booktitle>In Ola Knutsson, editor, Developing and Evaluating Language Tools for Writers and Learners of Swedish. Ph.D. thesis, Royal Institute of Technology (KTH),</booktitle>
<location>Stockholm, Sweden.</location>
<contexts>
<context position="11294" citStr="Carlberger et al. 2005" startWordPosition="1680" endWordPosition="1683">i, Matiasek, and Trost 2002; Schiller 2005). If function words are removed, an even higher number of the tokens are compounds; in both Swedish and German 10% of the content words in a news text have been found to be compounds (Hedlund 2002). That compounding is productive means that it is likely that a high number of compounds have a very low frequency in texts. Baroni, Matiasek, and Trost (2002) found that 83% of the compounds in a large German news corpus occur less than five times. In Swedish, compounds are the most common type of hapax words, that is, words that occur only once in a text (Carlberger et al. 2005). The most common type of compound is the noun+noun compound, which makes up 62% of the compounds in the German news corpus of Baroni, Matiasek, and Trost. 3. Related Work The problems arising from differences in compounding strategies in translation from German into English have been addressed by several authors. The most common architecture for translation from German is to split compounds in a preprocessing step prior to training and translation using some automatic method, which has been suggested both for SMT (Nießen and Ney 2000; Koehn and Knight 2003; Popovi´c, Stein, and Ney 2006; Holm</context>
<context position="16311" citStr="Carlberger et al. (2005)" startWordPosition="2482" endWordPosition="2485">identified from the graph using dynamic programming techniques. Compound merging for speech recognition is a somewhat different problem than for machine translation, however, because coalescence is not an issue, as compared with SMT, where there is no guarantee that the order of the parts in the translation output is correct. Another somewhat related problem to compound merging is that of detection of erroneously split compounds in human text, which is faced by grammar checkers. Writing compounds as separate words, with spaces between parts, is a common writing error in compounding languages. Carlberger et al. (2005) described a system for Swedish that used handwritten rules to identify, among other errors, erroneously split compounds. The rules used parts of speech and morphological features. On a classified gold standard of writing errors they had a recall of 46% and a precision of 39% for identifying split compounds, indicating that it is a difficult problem to find split compounds in free, unmarked text. There is also work on morphological merging, which is needed when the target words have been split into morphs in the training corpus. Virpioja et al. (2007) marked morphs with a symbol and merged all</context>
</contexts>
<marker>Carlberger, Domeij, Kann, Knutsson, 2005</marker>
<rawString>Carlberger, Johan, Rickard Domeij, Viggo Kann, and Ola Knutsson. 2005. The development and performance of a grammar checker for Swedish: A language engineering perspective. In Ola Knutsson, editor, Developing and Evaluating Language Tools for Writers and Learners of Swedish. Ph.D. thesis, Royal Institute of Technology (KTH), Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Carlberger</author>
<author>Viggo Kann</author>
</authors>
<title>Implementing an efficient part-of-speech tagger. Software Practice and Experience,</title>
<date>1999</date>
<pages>29--815</pages>
<contexts>
<context position="54810" citStr="Carlberger and Kann 1999" startWordPosition="8610" endWordPosition="8613">motive Languages En→De En→Sv En→Sv En→Da Training sentences 701,157 701,157 329,090 168,047 Avg. target sentence length 20.5 19.4 9.3 9.2 Dev sentences 500 500 2,000 1,000 Test sentences 2,000 2,000 1,000 1,000 We used factored translation (Koehn and Hoang 2007) in our experiments, with both surface words and part-of-speech tags on the target side, with a sequence model on parts-of-speech. For part-of-speech tagging we used TreeTagger (Schmid 1994) for German, an in-house hidden Markov model tagger based on Cutting et al. (1992) for Danish and Swedish, and for Swedish also the Granska tagger (Carlberger and Kann 1999). For the majority of experiments we used the Moses decoder (Koehn et al. 2007), which is a standard phrase-based statistical decoder, which allows factored decoding. For word alignment, Giza++ (Och and Ney 2003) was used and for language modeling we used SRILM (Stolcke 2002). For parameter optimization we used minimum error rate training (Och 2003). For each experiment we ran minimum error rate training three times in order to reduce the effect of optimizer instability, and report the average result and standard deviation. In the merging experiments based on sequence labeling we used the CRF+</context>
</contexts>
<marker>Carlberger, Kann, 1999</marker>
<rawString>Carlberger, Johan and Viggo Kann. 1999. Implementing an efficient part-of-speech tagger. Software Practice and Experience, 29:815–832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
</authors>
<title>Cohesive phrase-based decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the ACL: Human Language Technologies,</booktitle>
<pages>72--80</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="29804" citStr="Cherry 2008" startWordPosition="4622" endWordPosition="4623">ds. A language model solution to compound coalescence could be viewed as a soft constraint in the decoder, which encourages good sequences of compound parts over bad sequences. An alternative to this would have been a hard constraint in the decoder that prohibits compound parts to be placed in an incorrect order. We opted for a soft constraint approach because we found that it gave sufficiently good results, and because previous work on soft versus hard constraints for other areas has shown that soft constraints give more stable and generally better results, for instance, for phrase cohesion (Cherry 2008). As described in the previous section, we can add special POS tags and symbols to identify compound modifiers. Table 2 shows examples of the different representation schemes. We can then train a *POS n-gram sequence model using any of the *POStagsets, which naturally steers the decoder towards translations with good relative placement of these components. A lightweight model that gives some additional information of the order of compound parts compared to a language model on unmarked data is to train language models on texts where compounds are represented either using the marked or sepmarked</context>
</contexts>
<marker>Cherry, 2008</marker>
<rawString>Cherry, Colin. 2008. Cohesive phrase-based decoding for statistical machine translation. In Proceedings of the 46th Annual Meeting of the ACL: Human Language Technologies, pages 72–80, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the ACL: Human Language Technologies,</booktitle>
<pages>176--181</pages>
<location>Portland, OR.</location>
<contexts>
<context position="56577" citStr="Clark et al. (2011)" startWordPosition="8883" endWordPosition="8886">ging algorithm developed by us. We report results on three metrics: Bleu (Papineni et al. 2002), NIST (Doddington 2002), and Meteor. For Meteor we use the version tuned on adequacy and fluency (Lavie and Agarwal 2007) for German, and the original version with default weights (Banerjee and Lavie 2005) for Swedish and Danish, since there is no tuned version for those languages. For Bleu and Meteor we use the %-notation. Significance testing was performed using approximate randomization (Riezler and Maxwell 2005), with 10,000 iterations, on output based on three optimizer runs, as recommended by Clark et al. (2011). 8.2 General Effects of the Compound Processing Strategy One effect of compound splitting is that the number of word types is reduced. Tables 5 and 6 show the number of types (number of unique words) and tokens (total number of words) and the type/token ratio for Europarl in the different representation schemes. The type count and the type/token ratio in the baseline system are much higher in 6 http://crfpp.sourceforge.net/. 1086 Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT Table 5 Type and token counts and ratio for the German Europarl corpus. System Tokens Types Ratio</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Clark, Jonathan H., Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the ACL: Human Language Technologies, pages 176–181, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="46236" citStr="Collins 2002" startWordPosition="7281" endWordPosition="7282"> suitable monolingual corpus, and are thus not productive. The POS-match heuristic is more flexible, but is still limited in that it can only form 1082 Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT a compound if a modifying element (non-head) has been observed and tagged as such in the training data. The array of sequence labeling algorithms potentially suitable to our problem is fairly broad, including hidden Markov models (Rabiner 1989), conditional random fields (CRFs) (Lafferty, McCallum, and Pereira 2001), Semi-CRFs (Sarawagi and Cohen 2004), structured perceptrons (Collins 2002), structured support vector machines (Tsochantaridis et al. 2005), Max-Margin Markov networks (Taskar, Guestrin, and Koller 2003), and more. Because the focus of this work is on the application rather than on a comparison among alternative structured learning approaches, we limited ourselves to a single implementation. Considering its good scaling capabilities, capability to handle strongly redundant and overlapping features, and widespread recognition in the NLP community, we chose to use CRFs. 7.3.1 Features. Each sequence item (i.e., each separation point between words) is represented by me</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, Michael. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 1–8, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="53651" citStr="Collins and Koo 2005" startWordPosition="8434" endWordPosition="8437">ex language than the automotive corpus. Table 4 summarizes the sizes for the corpora that we used in Sections 8.2–8.5. The German test set is the test2007 set from the WMT 2008 workshop.5 We have chosen to use a smaller part of Europarl, to reduce runtimes and allow more experiments. In Section 8.6 we report results from scaling experiments and on out-of-domain data. 4 This option can also be extended so that the training data is split into chunks where each chunk is translated by a system that is trained on the remaining chunks, a strategy that has been successfully used for parse reranking (Collins and Koo 2005). Because we found that using fresh data did not give any significant improvements on the merging task except with little training data (see Section 8.5.2, Table 22), we choose not to use this approach since it would be very time consuming and thus impractical. 5 http://www.statmt.org/wmt08. 1085 Computational Linguistics Volume 39, Number 4 Table 4 Overview of the experimental settings for the experiments in Sections 8.2–8.5. Name Europarl German Europarl Swedish Auto Swedish Auto Danish Corpus Europarl Europarl Automotive Automotive Languages En→De En→Sv En→Sv En→Da Training sentences 701,15</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Collins, Michael and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Cutting</author>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Penelope Sibun</author>
</authors>
<title>A practical part-of-speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<pages>133--140</pages>
<location>Trento.</location>
<contexts>
<context position="54719" citStr="Cutting et al. (1992)" startWordPosition="8595" endWordPosition="8598">rman Europarl Swedish Auto Swedish Auto Danish Corpus Europarl Europarl Automotive Automotive Languages En→De En→Sv En→Sv En→Da Training sentences 701,157 701,157 329,090 168,047 Avg. target sentence length 20.5 19.4 9.3 9.2 Dev sentences 500 500 2,000 1,000 Test sentences 2,000 2,000 1,000 1,000 We used factored translation (Koehn and Hoang 2007) in our experiments, with both surface words and part-of-speech tags on the target side, with a sequence model on parts-of-speech. For part-of-speech tagging we used TreeTagger (Schmid 1994) for German, an in-house hidden Markov model tagger based on Cutting et al. (1992) for Danish and Swedish, and for Swedish also the Granska tagger (Carlberger and Kann 1999). For the majority of experiments we used the Moses decoder (Koehn et al. 2007), which is a standard phrase-based statistical decoder, which allows factored decoding. For word alignment, Giza++ (Och and Ney 2003) was used and for language modeling we used SRILM (Stolcke 2002). For parameter optimization we used minimum error rate training (Och 2003). For each experiment we ran minimum error rate training three times in order to reduce the effect of optimizer instability, and report the average result and</context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>Cutting, Doug, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, pages 133–140, Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Human Language Technology,</booktitle>
<pages>228--231</pages>
<location>San Diego, CA.</location>
<contexts>
<context position="56077" citStr="Doddington 2002" startWordPosition="8806" endWordPosition="8807">e labeling, we used the Matrax decoder (Simard et al. 2005) on the automotive corpus. Matrax is a phrasebased decoder that allows discontiguous phrases, and parameter optimization based on gradient descent for smoothed NIST. We extended the original Matrax decoder with factored decoding on the target side. Compounds were split before training using the corpus-based method described in Section 5. Except for the experiments comparing different compound merging methods, we used the POS-match merging algorithm developed by us. We report results on three metrics: Bleu (Papineni et al. 2002), NIST (Doddington 2002), and Meteor. For Meteor we use the version tuned on adequacy and fluency (Lavie and Agarwal 2007) for German, and the original version with default weights (Banerjee and Lavie 2005) for Swedish and Danish, since there is no tuned version for those languages. For Bleu and Meteor we use the %-notation. Significance testing was performed using approximate randomization (Riezler and Maxwell 2005), with 10,000 iterations, on output based on three optimizer runs, as recommended by Clark et al. (2011). 8.2 General Effects of the Compound Processing Strategy One effect of compound splitting is that t</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>Doddington, George. 2002. Automatic evaluation of machine translation quality using n-gram co-occurence statistics. In Proceedings of the Second International Conference on Human Language Technology, pages 228–231, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
</authors>
<title>Using a maximum entropy model to build segmentation lattices for MT.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the NAACL,</booktitle>
<pages>406--414</pages>
<location>Boulder, CO.</location>
<contexts>
<context position="12400" citStr="Dyer (2009)" startWordPosition="1863" endWordPosition="1864">n suggested both for SMT (Nießen and Ney 2000; Koehn and Knight 2003; Popovi´c, Stein, and Ney 2006; Holmqvist, Stymne, and Ahrenberg 2007) and example-based MT (Brown 2002). German compounds are split into their component parts in a preprocessing step and the translation model is then trained between modified German and English. At translation time, the German source text is also run through a compound splitter. In the studies cited here, only one splitting option is given as input to the decoder, which can be problematic in case the splitting is wrong, or if any of the parts are unknown. In Dyer (2009) several splitting options were given to the decoder in the form of a lattice. It is, however, not straightforward to use lattices during training, and in order to solve this, the training corpus was doubled, one part being without splits and the other part having the best splitting option for each word. For translation into German, Popovi´c, Stein, and Ney (2006) investigated three different strategies for compound processing. The first was to split compounds during training and after translation merge compound parts back into full compounds, the second merged English compounds prior to train</context>
<context position="19018" citStr="Dyer 2009" startWordPosition="2913" endWordPosition="2914">to pick the best known merging option. They showed a small effect of this on an MT task, even though this strategy was the best option in an intrinsic evaluation based on human reference translations. Compound splitting has been addressed in many articles, as a separate task (Schiller 2005) or targeted for applications such as information retrieval (Holz and Biemann 2008), speech recognition (Larson et al. 2000), grammar checking (Sj¨obergh and Kann 2004), lexicon acquisition (Kokkinakis 2001), word prediction (Baroni, Matiasek, and Trost 2002), and machine translation (Koehn and Knight 2003; Dyer 2009; Fritzinger and Fraser 2010; Macherey et al. 2011). The most successful strategies that address compound processing for MT apply compound splitting as a preprocessing step before training the translation models. Koehn and Knight (2003) presented an empirical splitting algorithm targeted at SMT from German to English. They split words in all possible places, and considered a splitting option valid if all its parts had been seen as words in a monolingual corpus. They allowed the addition of -s or -es at all splitting points. If there were several valid splitting options they chose one based on </context>
</contexts>
<marker>Dyer, 2009</marker>
<rawString>Dyer, Chris. 2009. Using a maximum entropy model to build segmentation lattices for MT. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the NAACL, pages 406–414, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
</authors>
<title>A Formal Model of Ambiguity and its Applications in Machine Translation.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland, USA.</institution>
<contexts>
<context position="20942" citStr="Dyer (2010)" startWordPosition="3206" endWordPosition="3207"> Koehn and Knight (2003) as well as the morphosyntactic algorithm from Nießen and Ney (2000) for splitting, with similar positive results for both options. Fritzinger and Fraser (2010) combined linguistic analysis with corpus-driven scoring and showed an improvement compared to using only a corpus-driven approach. Macherey et al. (2011) described a corpus-driven method that learns compounding form transformations of a language in addition to just compound splitting, and showed an improvement for translation from several languages into English compared to a baseline without compound treatment. Dyer (2010) suggested a compound splitting method based on sequence labeling, which gave good results for lattice-based translation from German. 4. Compound Translation For translation into a compounding language, we adopt the compound processing strategy suggested by Popovi´c, Stein, and Ney (2006). The process is: 1. Split compounds on the target (compounding language) side of the training corpus. 1073 Computational Linguistics Volume 39, Number 4 2. Learn a translation model from source (e.g., English) into decomposed-target (e.g., decomposed-German). 3. At translation time, translate using the learne</context>
<context position="43092" citStr="Dyer 2010" startWordPosition="6799" endWordPosition="6800">Volume 39, Number 4 potentially block both examples in Example (6). Compound and bigram frequencies can be computed on any available monolingual corpus in the domain of interest. We also investigated combinations of the heuristics by using either the union or intersection of merges from two different strategies. 7.3 Compound Merging as Sequence Labeling Besides extending and combining existing heuristics, we propose a novel formulation of compound merging as a sequence labeling problem. The opposite problem, compound splitting, has successfully been cast as a sequence labeling problem before (Dyer 2010), but here we apply this formulation in the opposite direction. Depending on choices made at compound splitting time, this task can be either a binary or multi-class classification task. If compound parts were kept as-is, the merging task is a simple concatenation of two words, and each separation point must receive a binary label encoding whether the two tokens should be merged. If compounds were normalized at splitting time, the compound form has to be restored before concatenating the parts. This can be modeled either as a multi-class classifier that has the possible boundary transformation</context>
</contexts>
<marker>Dyer, 2010</marker>
<rawString>Dyer, Chris. 2010. A Formal Model of Ambiguity and its Applications in Machine Translation. Ph.D. thesis, University of Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>˙Ilknur Durgar El-Kahlout</author>
<author>Kemal Oflazer</author>
</authors>
<title>Initial explorations in English to Turkish statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>7--14</pages>
<location>New York, NY.</location>
<contexts>
<context position="17281" citStr="El-Kahlout and Oflazer (2006)" startWordPosition="2641" endWordPosition="2644">lt problem to find split compounds in free, unmarked text. There is also work on morphological merging, which is needed when the target words have been split into morphs in the training corpus. Virpioja et al. (2007) marked morphs with a symbol and merged all marked words with the next word for translation between Finnish, Swedish, and Danish, without showing any improvements over an unmarked baseline. This strategy does have the advantage of being able to merge novel word forms, but has a drawback in that it can merge parts into non-words if the parts are misplaced in the translation output. El-Kahlout and Oflazer (2006) used a similar symbol-based merging strategy for translation from English into Turkish, but with the addition of morphographemic rules. They had positive results when performing limited splitting and grouping the split morphs, but not when splitting all morphs. They reported that there were problems with the order of morphs in the output. Badr, Zbib, and Glass (2008) reported results for translation from English to Arabic, where they used a combination merging method 1072 Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT where forms were picked from the corpus for known comb</context>
<context position="40128" citStr="El-Kahlout and Oflazer 2006" startWordPosition="6309" endWordPosition="6312">marked based on the compound head, such as EPOS or RPOS. In the current form the POS-match strategy cannot be used for the SPOS-tagset that does not use headbased tags for compound modifiers, because we cannot enforce the matching constraint in this way. It would be possible to design a POS-match strategy based on standard tags, which for instance allowed merging of noun+noun, but not of noun+preposition. Such a strategy requires linguistic knowledge and customization for each language, however. 7.2.2 Other Merging Heuristics. The symbol-based method is inspired by work on morphology merging (El-Kahlout and Oflazer 2006; Virpioja et al. 2007). It merges words that are marked with a symbol with the next word in the marked scheme. In the sepmarked scheme, when a standard symbol is found, the words on both sides of it are merged. These algorithms have the disadvantage, compared with the POS-match algorithm, that it is more likely that words are merged into non-compounds, since no matching check is carried out. We have also investigated methods based on word lists, proposed by Popovi´c, Stein, and Ney (2006). These methods use frequency word lists compiled at split time. Three types of lists were used: lists of </context>
</contexts>
<marker>El-Kahlout, Oflazer, 2006</marker>
<rawString>El-Kahlout, ˙Ilknur Durgar and Kemal Oflazer. 2006. Initial explorations in English to Turkish statistical machine translation. In Proceedings of the Workshop on Statistical Machine Translation, pages 7–14, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed El Kholy</author>
<author>Nizar Habash</author>
</authors>
<title>Techniques for Arabic morphological detokenization and orthographic denormalization.</title>
<date>2010</date>
<booktitle>In LREC 2010 Workshop on Language Resources and Human Language Technology for Semitic Languages,</booktitle>
<pages>45--51</pages>
<location>Valletta.</location>
<marker>El Kholy, Habash, 2010</marker>
<rawString>El Kholy, Ahmed and Nizar Habash. 2010. Techniques for Arabic morphological detokenization and orthographic denormalization. In LREC 2010 Workshop on Language Resources and Human Language Technology for Semitic Languages, pages 45–51, Valletta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
</authors>
<title>Experiments in morphosyntactic processing for translating to and from German.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>115--119</pages>
<location>Athens.</location>
<contexts>
<context position="13927" citStr="Fraser (2009)" startWordPosition="2108" endWordPosition="2109">essing, but was not as good as the other two strategies. Popovi´c, Stein, and Ney also presented one of few previous suggestions for compound merging. Each word in the translation output was looked up in a list of compound parts, and merged with the next word if it resulted in a known compound. This method led to improved overall translation results from English to German. The drawback of this method is that novel compounds cannot be merged. It might also merge words that should not be merged, but that happen to coincide with known compounds. 1071 Computational Linguistics Volume 39, Number 4 Fraser (2009) merged split German compounds after translation from English, by applying a second phrase-based SMT (PBSMT) system trained on German with split compounds and normal German. No separate results were presented for this extension alone, but in combination with other morphological processing the strategy led to worse results than the baseline. This method also suffers from the same drawbacks as that of Popovi´c, Stein, and Ney (2006), that novel compounds cannot be merged and words that should not be merged can still be. Koehn, Arun, and Hoang (2008) discussed the treatment of hyphenated compound</context>
</contexts>
<marker>Fraser, 2009</marker>
<rawString>Fraser, Alexander. 2009. Experiments in morphosyntactic processing for translating to and from German. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 115–119, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Friberg</author>
</authors>
<title>Decomposing Swedish compounds using memory-based learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th Nordic Conference on Computational Linguistics (NODALIDA’07),</booktitle>
<pages>224--230</pages>
<location>Tartu.</location>
<contexts>
<context position="47443" citStr="Friberg (2007)" startWordPosition="7459" endWordPosition="7460">d by means of a vector of features. Our aim was to include features representing the knowledge available to the heuristics, such as part-of-speech tags, frequencies for compounds and bigrams, as well as comparisons between them. Features were also inspired by previous work on compound splitting, with the intuition that features that are useful for splitting compounds could also be useful for merging. Character n-grams have successfully been used for splitting Swedish compounds, as the only knowledge source by Brodda (1979), and as one of several knowledge sources by Sj¨obergh and Kann (2004). Friberg (2007) tried to normalize letters, besides using the original letters. Although she was not successful, we still believe in the potential of this feature. Larson et al. (2000) used frequencies of prefixes and suffixes from a corpus as a basis of their method for splitting German compounds. We used the following features where -N refers to the nth position before the merge point, and +N to the nth position after the merge point: • Previous tag • Surface words: word-2, word-1, word+1, bigram word-1–word+1 • Parts of speech: POS-2, POS-1, POS+1, bigram POS-1–POS+1 • Character n-grams around the merge p</context>
</contexts>
<marker>Friberg, 2007</marker>
<rawString>Friberg, Karin. 2007. Decomposing Swedish compounds using memory-based learning. In Proceedings of the 16th Nordic Conference on Computational Linguistics (NODALIDA’07), pages 224–230, Tartu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Fritzinger</author>
<author>Alexander Fraser</author>
</authors>
<title>How to avoid burning ducks: Combining linguistic analysis and corpus statistics for German compound processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>224--234</pages>
<location>Uppsala.</location>
<contexts>
<context position="19046" citStr="Fritzinger and Fraser 2010" startWordPosition="2915" endWordPosition="2918"> best known merging option. They showed a small effect of this on an MT task, even though this strategy was the best option in an intrinsic evaluation based on human reference translations. Compound splitting has been addressed in many articles, as a separate task (Schiller 2005) or targeted for applications such as information retrieval (Holz and Biemann 2008), speech recognition (Larson et al. 2000), grammar checking (Sj¨obergh and Kann 2004), lexicon acquisition (Kokkinakis 2001), word prediction (Baroni, Matiasek, and Trost 2002), and machine translation (Koehn and Knight 2003; Dyer 2009; Fritzinger and Fraser 2010; Macherey et al. 2011). The most successful strategies that address compound processing for MT apply compound splitting as a preprocessing step before training the translation models. Koehn and Knight (2003) presented an empirical splitting algorithm targeted at SMT from German to English. They split words in all possible places, and considered a splitting option valid if all its parts had been seen as words in a monolingual corpus. They allowed the addition of -s or -es at all splitting points. If there were several valid splitting options they chose one based on the number of splits, the ge</context>
<context position="20515" citStr="Fritzinger and Fraser (2010)" startWordPosition="3145" endWordPosition="3148"> achieved by using either the geometric mean, or the highest number of splits. There were no correlations between translation results and the intrinsic evaluation. Several other researchers have also explored compound splitting for translation from German to English. Nießen and Ney (2000) used a morpho-syntactic analyzer for splitting German compounds prior to translation. Popovi´c, Stein, and Ney (2006) used the geometric mean version from Koehn and Knight (2003) as well as the morphosyntactic algorithm from Nießen and Ney (2000) for splitting, with similar positive results for both options. Fritzinger and Fraser (2010) combined linguistic analysis with corpus-driven scoring and showed an improvement compared to using only a corpus-driven approach. Macherey et al. (2011) described a corpus-driven method that learns compounding form transformations of a language in addition to just compound splitting, and showed an improvement for translation from several languages into English compared to a baseline without compound treatment. Dyer (2010) suggested a compound splitting method based on sequence labeling, which gave good results for lattice-based translation from German. 4. Compound Translation For translation</context>
</contexts>
<marker>Fritzinger, Fraser, 2010</marker>
<rawString>Fritzinger, Fabienne and Alexander Fraser. 2010. How to avoid burning ducks: Combining linguistic analysis and corpus statistics for German compound processing. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 224–234, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Turid Hedlund</author>
</authors>
<title>Compounds in dictionary-based cross-language information retrieval.</title>
<date>2002</date>
<journal>Information Research,</journal>
<volume>7</volume>
<issue>2</issue>
<note>Available at http://InformationR. net/ir/7-2/paper128.html. Accessed</note>
<contexts>
<context position="10911" citStr="Hedlund 2002" startWordPosition="1613" endWordPosition="1614"> in SMT (3) SV tullagstiftning customs legislation tull+lagstiftning custom legislation DE Zelllinie cell line Zell+Linie cell line Compounding is common and productive; new compounds can be readily formed and understood. This is confirmed in a number of corpus studies. In German, compounds have been shown to make up 5–7% of tokens and 43–47% of types in news text (Baroni, Matiasek, and Trost 2002; Schiller 2005). If function words are removed, an even higher number of the tokens are compounds; in both Swedish and German 10% of the content words in a news text have been found to be compounds (Hedlund 2002). That compounding is productive means that it is likely that a high number of compounds have a very low frequency in texts. Baroni, Matiasek, and Trost (2002) found that 83% of the compounds in a large German news corpus occur less than five times. In Swedish, compounds are the most common type of hapax words, that is, words that occur only once in a text (Carlberger et al. 2005). The most common type of compound is the noun+noun compound, which makes up 62% of the compounds in the German news corpus of Baroni, Matiasek, and Trost. 3. Related Work The problems arising from differences in comp</context>
</contexts>
<marker>Hedlund, 2002</marker>
<rawString>Hedlund, Turid. 2002. Compounds in dictionary-based cross-language information retrieval. Information Research, 7(2). Available at http://InformationR. net/ir/7-2/paper128.html. Accessed January 29, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Holmqvist</author>
<author>Sara Stymne</author>
<author>Lars Ahrenberg</author>
</authors>
<title>Getting to know Moses: Initial experiments on German–English factored translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>181--184</pages>
<location>Prague.</location>
<marker>Holmqvist, Stymne, Ahrenberg, 2007</marker>
<rawString>Holmqvist, Maria, Sara Stymne, and Lars Ahrenberg. 2007. Getting to know Moses: Initial experiments on German–English factored translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 181–184, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Holz</author>
<author>Chris Biemann</author>
</authors>
<title>Unsupervised and knowledge-free learning of compound splits and periphrases.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th International Conference on Intelligent Text Processing and Computational Linguistics (CICLING),</booktitle>
<pages>117--127</pages>
<location>Haifa.</location>
<contexts>
<context position="18783" citStr="Holz and Biemann 2008" startWordPosition="2879" endWordPosition="2882">s information; this gave a small improvement on a large corpus, but at the cost of high runtime. El Kholy and Habash (2010) extended the merging scheme of Badr, Zbib, and Glass (2008) by using the conditional probability and a language model score to pick the best known merging option. They showed a small effect of this on an MT task, even though this strategy was the best option in an intrinsic evaluation based on human reference translations. Compound splitting has been addressed in many articles, as a separate task (Schiller 2005) or targeted for applications such as information retrieval (Holz and Biemann 2008), speech recognition (Larson et al. 2000), grammar checking (Sj¨obergh and Kann 2004), lexicon acquisition (Kokkinakis 2001), word prediction (Baroni, Matiasek, and Trost 2002), and machine translation (Koehn and Knight 2003; Dyer 2009; Fritzinger and Fraser 2010; Macherey et al. 2011). The most successful strategies that address compound processing for MT apply compound splitting as a preprocessing step before training the translation models. Koehn and Knight (2003) presented an empirical splitting algorithm targeted at SMT from German to English. They split words in all possible places, and </context>
</contexts>
<marker>Holz, Biemann, 2008</marker>
<rawString>Holz, Florian and Chris Biemann. 2008. Unsupervised and knowledge-free learning of compound splits and periphrases. In Proceedings of the 9th International Conference on Intelligent Text Processing and Computational Linguistics (CICLING), pages 117–127, Haifa.</rawString>
</citation>
<citation valid="true">
<title>Institut f¨ur Deutsche Sprache.</title>
<date>1998</date>
<location>Mannheim, Germany.</location>
<contexts>
<context position="7664" citStr="(1998)" startWordPosition="1152" endWordPosition="1152">dark-blue dunkel+blau dark blue g. kennenlernen get to know kennen+lernen know learn h. gr¨osstenteils in most instances gr¨ossten+teils largest partly Compound modifiers often have a special form, such as the addition of an “s” to the base form of Regierung in Example (1a). We will refer to these form variants as compounding forms. Table 1 exemplifies the type of operations used in Swedish, German, and Danish to form compounding forms. There are many more alternative compounding forms in Swedish and German than in Danish. For an overview of the possible compounding forms in German see Langer (1998) or K¨urschner (2003), in Danish see K¨urschner (2003), and in Swedish see Thorell (1981) or Stymne and Holmqvist (2008). Some compounding forms coincide with paradigmatic forms, such as German Jahres that can also be genitive, and Stadien that can also be plural, from Table 1. There are different views on whether to treat these forms as paradigmatic forms or as compounding forms. We follow Langer (1998) in viewing them as compounding forms, because they often do not correspond to plural or possessive semantics. Many individual compound modifiers have more than one possible compounding form. I</context>
<context position="75666" citStr="(1998)" startWordPosition="11988" endWordPosition="11988"> we performed an intrinsic evaluation of several splitting methods, and compared it to translation results using the same splitting strategies. In this experiment we used German Europarl. We used a modified version of the splitting strategy of Koehn and Knight (2003) as a basis for the comparison, and applied it to all nouns, adjectives, adverbs, and verbs of minimum length six characters into parts of minimum three characters, by allowing all splits where the parts were found in a corpus, and were tagged as content words. Parts were allowed to be modified by all compound suffixes from Langer (1998). The best splitting option, which can be no split, was chosen based on the arithmetic mean of the corpus frequencies of the parts. We also imposed the restriction on the compound head that its part-of-speech tag needs to be the same as for the full word. We call this system arith. In other variants of splitting strategies, one feature at the time was changed, based on the arith system: geom – using the geometric mean instead of the arithmetic mean eager – choosing the split with the highest number of parts, instead of using the arithmetic mean part4 – limiting the length of compound parts to </context>
</contexts>
<marker>1998</marker>
<rawString>Institut f¨ur Deutsche Sprache. 1998. Rechtschreibreform (Aktualisierte Ausgabe). IDS Sprachreport, Extra-Ausgabe Dezember 1998. Mannheim, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit X,</booktitle>
<pages>79--86</pages>
<location>Phuket.</location>
<contexts>
<context position="52604" citStr="Koehn 2005" startWordPosition="8261" endWordPosition="8262"> affect translation quality. Here we do not vary representation scheme or merging. • Section 8.5 reports results from varying the merging process. In particular we compare the novel sequence labeling method with the heuristic methods. Here we do not vary representation scheme or splitting. • In Section 8.6 we apply the overall best strategies to corpora of different sizes and to out-of-domain data. 8.1 Experimental Set-up We performed experiments on translation from English into German, Swedish, and Danish, all of which have closed compounds. We tested all experimental conditions on Europarl (Koehn 2005) for translation from English to German. We also give contrasting results on English–Swedish Europarl and for an automotive corpus for translation from English to Swedish and Danish. The automotive corpus was gathered from translation memory data. The two corpora are quite different. The automotive corpus is from a limited domain, and of a homogeneous nature, whereas Europarl is more diverse, and tends to have a more complex language than the automotive corpus. Table 4 summarizes the sizes for the corpora that we used in Sections 8.2–8.5. The German test set is the test2007 set from the WMT 20</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Koehn, Philipp. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of MT Summit X, pages 79–86, Phuket.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Abhishek Arun</author>
<author>Hieu Hoang</author>
</authors>
<title>Towards better machine translation quality for the German–English language pairs.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>139--142</pages>
<location>Columbus, OH.</location>
<marker>Koehn, Arun, Hoang, 2008</marker>
<rawString>Koehn, Philipp, Abhishek Arun, and Hieu Hoang. 2008. Towards better machine translation quality for the German–English language pairs. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 139–142, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>868--876</pages>
<location>Prague.</location>
<contexts>
<context position="18117" citStr="Koehn and Hoang 2007" startWordPosition="2769" endWordPosition="2772">plit morphs, but not when splitting all morphs. They reported that there were problems with the order of morphs in the output. Badr, Zbib, and Glass (2008) reported results for translation from English to Arabic, where they used a combination merging method 1072 Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT where forms were picked from the corpus for known combinations of morphs and words, and generated based on handwritten recombination rules otherwise, which led to improvements over the baseline. They also used the morphs+POS as factors in a factored translation model (Koehn and Hoang 2007) where surface forms were generated from this information; this gave a small improvement on a large corpus, but at the cost of high runtime. El Kholy and Habash (2010) extended the merging scheme of Badr, Zbib, and Glass (2008) by using the conditional probability and a language model score to pick the best known merging option. They showed a small effect of this on an MT task, even though this strategy was the best option in an intrinsic evaluation based on human reference translations. Compound splitting has been addressed in many articles, as a separate task (Schiller 2005) or targeted for </context>
<context position="22339" citStr="Koehn and Hoang 2007" startWordPosition="3408" endWordPosition="3411"> be merged into compounds, and choose the correct compounding form for the compound modifiers. The first problem can become hopelessly difficult if the translation did not put components nicely side by side and in the correct order. Preliminary to merging, then, the problem of coalescence needs to be addressed, that is, translations where compound elements are correctly positioned should be promoted. Figure 1 gives an overview of the translation process with compound processing. Compounds are split before training the translation system, and merged after translation. We use factored decoding (Koehn and Hoang 2007), where features other than just surface words can be used by the system. In our case we tag the data with parts of speech and use a factored model with POS-tags on the target side, which allows us to have a POS-sequence model, beside the standard language model. This configuration has a very small overhead compared with decoding without factors. As we show, using part-of-speech tags on the target side helps to improve the coalescence of compound parts, and can be used to guide the merging process. For the tuning step there are two options: either we can split the development set and tune with</context>
<context position="54447" citStr="Koehn and Hoang 2007" startWordPosition="8553" endWordPosition="8556">e not to use this approach since it would be very time consuming and thus impractical. 5 http://www.statmt.org/wmt08. 1085 Computational Linguistics Volume 39, Number 4 Table 4 Overview of the experimental settings for the experiments in Sections 8.2–8.5. Name Europarl German Europarl Swedish Auto Swedish Auto Danish Corpus Europarl Europarl Automotive Automotive Languages En→De En→Sv En→Sv En→Da Training sentences 701,157 701,157 329,090 168,047 Avg. target sentence length 20.5 19.4 9.3 9.2 Dev sentences 500 500 2,000 1,000 Test sentences 2,000 2,000 1,000 1,000 We used factored translation (Koehn and Hoang 2007) in our experiments, with both surface words and part-of-speech tags on the target side, with a sequence model on parts-of-speech. For part-of-speech tagging we used TreeTagger (Schmid 1994) for German, an in-house hidden Markov model tagger based on Cutting et al. (1992) for Danish and Swedish, and for Swedish also the Granska tagger (Carlberger and Kann 1999). For the majority of experiments we used the Moses decoder (Koehn et al. 2007), which is a standard phrase-based statistical decoder, which allows factored decoding. For word alignment, Giza++ (Och and Ney 2003) was used and for languag</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Koehn, Philipp and Hieu Hoang. 2007. Factored translation models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 868–876, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL, Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="54889" citStr="Koehn et al. 2007" startWordPosition="8624" endWordPosition="8627">8,047 Avg. target sentence length 20.5 19.4 9.3 9.2 Dev sentences 500 500 2,000 1,000 Test sentences 2,000 2,000 1,000 1,000 We used factored translation (Koehn and Hoang 2007) in our experiments, with both surface words and part-of-speech tags on the target side, with a sequence model on parts-of-speech. For part-of-speech tagging we used TreeTagger (Schmid 1994) for German, an in-house hidden Markov model tagger based on Cutting et al. (1992) for Danish and Swedish, and for Swedish also the Granska tagger (Carlberger and Kann 1999). For the majority of experiments we used the Moses decoder (Koehn et al. 2007), which is a standard phrase-based statistical decoder, which allows factored decoding. For word alignment, Giza++ (Och and Ney 2003) was used and for language modeling we used SRILM (Stolcke 2002). For parameter optimization we used minimum error rate training (Och 2003). For each experiment we ran minimum error rate training three times in order to reduce the effect of optimizer instability, and report the average result and standard deviation. In the merging experiments based on sequence labeling we used the CRF++ toolkit.6 For the merging experiment with sequence labeling, we used the Matr</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL, Demo and Poster Sessions, pages 177–180, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the EACL,</booktitle>
<pages>187--193</pages>
<location>Budapest.</location>
<contexts>
<context position="3479" citStr="Koehn and Knight 2003" startWordPosition="508" endWordPosition="511"> the desired compounds are missing in the training data or that they have not been aligned correctly. When a compound is the idiomatic word choice in the translation, systems often produce separate words, genitive or other alternative constructions, or translate only one part of the compound. For an SMT system to cope with the productivity of the phenomenon, any effective strategy should be able to correctly process compounds that have never been seen in the training data as such, although possibly their components have, either in isolation or within a different compound. Previous work (e.g., Koehn and Knight 2003) has shown that compound splitting improves translation from compounding languages into English. In this article we explore several aspects of the less-researched area of compound treatment for translation into such languages, using three Germanic languages (German, Swedish, and Danish) as examples.1 The assumption is that splitting compounds will also improve translation for this translation direction and lead to more natural translations. The strategy we adopt is to split compounds in the training data, and to merge them in the translation output. Our overall goal is to improve translation q</context>
<context position="11857" citStr="Koehn and Knight 2003" startWordPosition="1771" endWordPosition="1774">ds that occur only once in a text (Carlberger et al. 2005). The most common type of compound is the noun+noun compound, which makes up 62% of the compounds in the German news corpus of Baroni, Matiasek, and Trost. 3. Related Work The problems arising from differences in compounding strategies in translation from German into English have been addressed by several authors. The most common architecture for translation from German is to split compounds in a preprocessing step prior to training and translation using some automatic method, which has been suggested both for SMT (Nießen and Ney 2000; Koehn and Knight 2003; Popovi´c, Stein, and Ney 2006; Holmqvist, Stymne, and Ahrenberg 2007) and example-based MT (Brown 2002). German compounds are split into their component parts in a preprocessing step and the translation model is then trained between modified German and English. At translation time, the German source text is also run through a compound splitter. In the studies cited here, only one splitting option is given as input to the decoder, which can be problematic in case the splitting is wrong, or if any of the parts are unknown. In Dyer (2009) several splitting options were given to the decoder in t</context>
<context position="19007" citStr="Koehn and Knight 2003" startWordPosition="2909" endWordPosition="2912">a language model score to pick the best known merging option. They showed a small effect of this on an MT task, even though this strategy was the best option in an intrinsic evaluation based on human reference translations. Compound splitting has been addressed in many articles, as a separate task (Schiller 2005) or targeted for applications such as information retrieval (Holz and Biemann 2008), speech recognition (Larson et al. 2000), grammar checking (Sj¨obergh and Kann 2004), lexicon acquisition (Kokkinakis 2001), word prediction (Baroni, Matiasek, and Trost 2002), and machine translation (Koehn and Knight 2003; Dyer 2009; Fritzinger and Fraser 2010; Macherey et al. 2011). The most successful strategies that address compound processing for MT apply compound splitting as a preprocessing step before training the translation models. Koehn and Knight (2003) presented an empirical splitting algorithm targeted at SMT from German to English. They split words in all possible places, and considered a splitting option valid if all its parts had been seen as words in a monolingual corpus. They allowed the addition of -s or -es at all splitting points. If there were several valid splitting options they chose on</context>
<context position="20355" citStr="Koehn and Knight (2003)" startWordPosition="3119" endWordPosition="3122">ting algorithms intrinsically on a gold standard of manually split noun phrases and on machine translation of noun phrases. The best results for PBSMT were achieved by using either the geometric mean, or the highest number of splits. There were no correlations between translation results and the intrinsic evaluation. Several other researchers have also explored compound splitting for translation from German to English. Nießen and Ney (2000) used a morpho-syntactic analyzer for splitting German compounds prior to translation. Popovi´c, Stein, and Ney (2006) used the geometric mean version from Koehn and Knight (2003) as well as the morphosyntactic algorithm from Nießen and Ney (2000) for splitting, with similar positive results for both options. Fritzinger and Fraser (2010) combined linguistic analysis with corpus-driven scoring and showed an improvement compared to using only a corpus-driven approach. Macherey et al. (2011) described a corpus-driven method that learns compounding form transformations of a language in addition to just compound splitting, and showed an improvement for translation from several languages into English compared to a baseline without compound treatment. Dyer (2010) suggested a </context>
<context position="23687" citStr="Koehn and Knight (2003)" startWordPosition="3631" endWordPosition="3634">ancedda, and Ahrenberg Generation of Compound Words in SMT compounds, or we can merge compounds in the translation output, before performing the optimization. We found empirically that we got the best results using the second approach, of performing compound merging during the tuning process. When we tuned on split texts, the results were more unstable, and especially the number of words in the translation output varied substantially. We thus use merging also during the tuning process in all experiments, as shown in Figure 1. 5. Compound Splitting Our method for compound splitting is based on Koehn and Knight (2003) and Stymne (2008). For each word all possible segmentations are explored, with the restrictions that all parts must have at least three characters, and the last part, the compound head, must have the same part-of-speech tag as the word itself. Hyphens are treated as additions to compound modifiers, just as +s or +e. Segmentations are scored with the arithmetic mean of frequencies for each part in the training corpus and the segmentation with the highest score is chosen. We have investigated several variants of the basic method and their effects on compound translation. The variants investigat</context>
<context position="75327" citStr="Koehn and Knight (2003)" startWordPosition="11927" endWordPosition="11930">successful. On Europarl the EPOS-tagset was the most successful, whereas the RPOS-tagset and count features could help for the automotive domain. 8.4 Influence of Splitting Strategies In the following experiments we investigated how compound splitting strategies of different quality influenced the suggested compound processing method. In order to do this we performed an intrinsic evaluation of several splitting methods, and compared it to translation results using the same splitting strategies. In this experiment we used German Europarl. We used a modified version of the splitting strategy of Koehn and Knight (2003) as a basis for the comparison, and applied it to all nouns, adjectives, adverbs, and verbs of minimum length six characters into parts of minimum three characters, by allowing all splits where the parts were found in a corpus, and were tagged as content words. Parts were allowed to be modified by all compound suffixes from Langer (1998). The best splitting option, which can be no split, was chosen based on the arithmetic mean of the corpus frequencies of the parts. We also imposed the restriction on the compound head that its part-of-speech tag needs to be the same as for the full word. We ca</context>
<context position="76971" citStr="Koehn and Knight (2003)" startWordPosition="12203" endWordPosition="12206">mmon – only allowing the common compound suffixes -s, -es, -n, -nen anypos – not using part-of-speech tags The corpus frequencies were gathered from the target side of the training corpus. We compared the systems with compound splitting with a factored baseline system without any compound processing. To measure the success of the different compound splitting algorithms we performed an intrinsic evaluation. We used the gold standard from Stymne (2008), created by manually annotating the first 5,000 words of the test text for one-to-one correspondence with the English reference text, similar to Koehn and Knight (2003). A one-toone correspondence occurs when the words in a German compound are translated as separate words in English. In addition there can be inserted function words. As an example, Medienfreiheit is in one-to-one correspondence with freedom of the media, since the two German parts Medien and Freiheit corresponds to two separate words, media and freedom. The two function words of, the are ignored. Out of the 5,000 words 174 were compounds in one-to-one correspondence with English. The result of the 1093 Computational Linguistics Volume 39, Number 4 one-to-one evaluation is shown in Table 16. T</context>
<context position="81577" citStr="Koehn and Knight (2003)" startWordPosition="12965" endWordPosition="12968">m 18.9 (0.1)* 5.78 (0.02) 26.5 (0.2)** eager 18.8 (0.1)** 5.80 (0.01) 26.6 (0.1) part4 18.8 (0.1)** 5.79 (0.01) 26.5 (0.1)* max2 18.8 (0.1) 5.80 (0.01) 26.6 (0.1) common 18.9 (0.1) 5.78 (0.01) 26.6 (0.1) anypos 18.9 (0.1) 5.75 (0.01)** 26.6 (0.1) arith, max2, and common systems are on par with the baseline on all metrics, with no significant differences, whereas the other systems are worse than the baseline on at least one metric. On the intrinsic evaluation there were quite large differences between the systems, which are not found on the translation task. This is similar to previous work by Koehn and Knight (2003) for translation in the other direction, where systems similar to the eager and geom system performed similarly on a translation task, while the eager system was much worse on their intrinsic evaluation. In our evaluation only the eager system is significantly worse than any other system on Bleu, where it is worse than the baseline and the arith system. The arith system overall is competitive with both the baseline and the other split systems, despite the relatively low results on the intrinsic evaluation. Overall, it seems that the translation task is not very sensitive to the quality of the </context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Koehn, Philipp and Kevin Knight. 2003. Empirical methods for compound splitting. In Proceedings of the 10th Conference of the EACL, pages 187–193, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitros Kokkinakis</author>
</authors>
<title>A Framework for the Acquisition of Lexical Knowledge: Description and Applications.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>G¨oteborg University, Sweden.</institution>
<contexts>
<context position="18907" citStr="Kokkinakis 2001" startWordPosition="2897" endWordPosition="2898">ed the merging scheme of Badr, Zbib, and Glass (2008) by using the conditional probability and a language model score to pick the best known merging option. They showed a small effect of this on an MT task, even though this strategy was the best option in an intrinsic evaluation based on human reference translations. Compound splitting has been addressed in many articles, as a separate task (Schiller 2005) or targeted for applications such as information retrieval (Holz and Biemann 2008), speech recognition (Larson et al. 2000), grammar checking (Sj¨obergh and Kann 2004), lexicon acquisition (Kokkinakis 2001), word prediction (Baroni, Matiasek, and Trost 2002), and machine translation (Koehn and Knight 2003; Dyer 2009; Fritzinger and Fraser 2010; Macherey et al. 2011). The most successful strategies that address compound processing for MT apply compound splitting as a preprocessing step before training the translation models. Koehn and Knight (2003) presented an empirical splitting algorithm targeted at SMT from German to English. They split words in all possible places, and considered a splitting option valid if all its parts had been seen as words in a monolingual corpus. They allowed the additi</context>
</contexts>
<marker>Kokkinakis, 2001</marker>
<rawString>Kokkinakis, Dimitros. 2001. A Framework for the Acquisition of Lexical Knowledge: Description and Applications. Ph.D. thesis, G¨oteborg University, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian K¨urschner</author>
</authors>
<title>Von Volk-s-musik und Sport-ø-geist im Lemming-ø-land – Af folk-e-musik og sport-s-˚and i lemming-e-landet:</title>
<date>2003</date>
<booktitle>Fugenelemente im Deutschen und D¨anischen – eine kontrastive Studie zu einem Grenzfall der Morphologie. Master’s thesis,</booktitle>
<location>Albert-Ludwigs-Universit¨at, Freiburg, Germany.</location>
<marker>K¨urschner, 2003</marker>
<rawString>K¨urschner, Sebastian. 2003. Von Volk-s-musik und Sport-ø-geist im Lemming-ø-land – Af folk-e-musik og sport-s-˚and i lemming-e-landet: Fugenelemente im Deutschen und D¨anischen – eine kontrastive Studie zu einem Grenzfall der Morphologie. Master’s thesis, Albert-Ludwigs-Universit¨at, Freiburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<location>Williamstown, MA.</location>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, John, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning, pages 282–289, Williamstown, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Langer</author>
</authors>
<title>Zur Morphologie und Semantik von Nominalkomposita.</title>
<date>1998</date>
<booktitle>In Tagungsband der 4. Konferenz zur Verarbeitung nat¨urlicher Sprache (KONVENS),</booktitle>
<pages>83--97</pages>
<location>Bonn.</location>
<contexts>
<context position="7664" citStr="Langer (1998)" startWordPosition="1151" endWordPosition="1152">elblau dark-blue dunkel+blau dark blue g. kennenlernen get to know kennen+lernen know learn h. gr¨osstenteils in most instances gr¨ossten+teils largest partly Compound modifiers often have a special form, such as the addition of an “s” to the base form of Regierung in Example (1a). We will refer to these form variants as compounding forms. Table 1 exemplifies the type of operations used in Swedish, German, and Danish to form compounding forms. There are many more alternative compounding forms in Swedish and German than in Danish. For an overview of the possible compounding forms in German see Langer (1998) or K¨urschner (2003), in Danish see K¨urschner (2003), and in Swedish see Thorell (1981) or Stymne and Holmqvist (2008). Some compounding forms coincide with paradigmatic forms, such as German Jahres that can also be genitive, and Stadien that can also be plural, from Table 1. There are different views on whether to treat these forms as paradigmatic forms or as compounding forms. We follow Langer (1998) in viewing them as compounding forms, because they often do not correspond to plural or possessive semantics. Many individual compound modifiers have more than one possible compounding form. I</context>
<context position="75666" citStr="Langer (1998)" startWordPosition="11987" endWordPosition="11988">do this we performed an intrinsic evaluation of several splitting methods, and compared it to translation results using the same splitting strategies. In this experiment we used German Europarl. We used a modified version of the splitting strategy of Koehn and Knight (2003) as a basis for the comparison, and applied it to all nouns, adjectives, adverbs, and verbs of minimum length six characters into parts of minimum three characters, by allowing all splits where the parts were found in a corpus, and were tagged as content words. Parts were allowed to be modified by all compound suffixes from Langer (1998). The best splitting option, which can be no split, was chosen based on the arithmetic mean of the corpus frequencies of the parts. We also imposed the restriction on the compound head that its part-of-speech tag needs to be the same as for the full word. We call this system arith. In other variants of splitting strategies, one feature at the time was changed, based on the arith system: geom – using the geometric mean instead of the arithmetic mean eager – choosing the split with the highest number of parts, instead of using the arithmetic mean part4 – limiting the length of compound parts to </context>
</contexts>
<marker>Langer, 1998</marker>
<rawString>Langer, Stefan. 1998. Zur Morphologie und Semantik von Nominalkomposita. In Tagungsband der 4. Konferenz zur Verarbeitung nat¨urlicher Sprache (KONVENS), pages 83–97, Bonn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Larson</author>
<author>Daniel Willett</author>
<author>Joachim K¨ohler</author>
<author>Gerhard Rigoll</author>
</authors>
<title>Compound splitting and lexical unit recombination for improved performance of a speech recognition system for German parliamentary speeches.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth International Conference on Spoken Language Processing,</booktitle>
<volume>3</volume>
<pages>945--948</pages>
<location>Beijing.</location>
<marker>Larson, Willett, K¨ohler, Rigoll, 2000</marker>
<rawString>Larson, Martha, Daniel Willett, Joachim K¨ohler, and Gerhard Rigoll. 2000. Compound splitting and lexical unit recombination for improved performance of a speech recognition system for German parliamentary speeches. In Proceedings of the Sixth International Conference on Spoken Language Processing, volume 3, pages 945–948, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>228--231</pages>
<location>Prague.</location>
<contexts>
<context position="56175" citStr="Lavie and Agarwal 2007" startWordPosition="8821" endWordPosition="8824">x is a phrasebased decoder that allows discontiguous phrases, and parameter optimization based on gradient descent for smoothed NIST. We extended the original Matrax decoder with factored decoding on the target side. Compounds were split before training using the corpus-based method described in Section 5. Except for the experiments comparing different compound merging methods, we used the POS-match merging algorithm developed by us. We report results on three metrics: Bleu (Papineni et al. 2002), NIST (Doddington 2002), and Meteor. For Meteor we use the version tuned on adequacy and fluency (Lavie and Agarwal 2007) for German, and the original version with default weights (Banerjee and Lavie 2005) for Swedish and Danish, since there is no tuned version for those languages. For Bleu and Meteor we use the %-notation. Significance testing was performed using approximate randomization (Riezler and Maxwell 2005), with 10,000 iterations, on output based on three optimizer runs, as recommended by Clark et al. (2011). 8.2 General Effects of the Compound Processing Strategy One effect of compound splitting is that the number of word types is reduced. Tables 5 and 6 show the number of types (number of unique word</context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Lavie, Alon and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228–231, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Macherey</author>
<author>Andrew Dai</author>
<author>David Talbot</author>
<author>Ashok Popat</author>
<author>Franz Och</author>
</authors>
<title>Language-independent compound splitting with morphological operations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the ACL: Human Language Technologies,</booktitle>
<pages>1--395</pages>
<location>Portland, OR.</location>
<contexts>
<context position="19069" citStr="Macherey et al. 2011" startWordPosition="2919" endWordPosition="2922">They showed a small effect of this on an MT task, even though this strategy was the best option in an intrinsic evaluation based on human reference translations. Compound splitting has been addressed in many articles, as a separate task (Schiller 2005) or targeted for applications such as information retrieval (Holz and Biemann 2008), speech recognition (Larson et al. 2000), grammar checking (Sj¨obergh and Kann 2004), lexicon acquisition (Kokkinakis 2001), word prediction (Baroni, Matiasek, and Trost 2002), and machine translation (Koehn and Knight 2003; Dyer 2009; Fritzinger and Fraser 2010; Macherey et al. 2011). The most successful strategies that address compound processing for MT apply compound splitting as a preprocessing step before training the translation models. Koehn and Knight (2003) presented an empirical splitting algorithm targeted at SMT from German to English. They split words in all possible places, and considered a splitting option valid if all its parts had been seen as words in a monolingual corpus. They allowed the addition of -s or -es at all splitting points. If there were several valid splitting options they chose one based on the number of splits, the geometric mean of part fr</context>
<context position="20669" citStr="Macherey et al. (2011)" startWordPosition="3166" endWordPosition="3169">on. Several other researchers have also explored compound splitting for translation from German to English. Nießen and Ney (2000) used a morpho-syntactic analyzer for splitting German compounds prior to translation. Popovi´c, Stein, and Ney (2006) used the geometric mean version from Koehn and Knight (2003) as well as the morphosyntactic algorithm from Nießen and Ney (2000) for splitting, with similar positive results for both options. Fritzinger and Fraser (2010) combined linguistic analysis with corpus-driven scoring and showed an improvement compared to using only a corpus-driven approach. Macherey et al. (2011) described a corpus-driven method that learns compounding form transformations of a language in addition to just compound splitting, and showed an improvement for translation from several languages into English compared to a baseline without compound treatment. Dyer (2010) suggested a compound splitting method based on sequence labeling, which gave good results for lattice-based translation from German. 4. Compound Translation For translation into a compounding language, we adopt the compound processing strategy suggested by Popovi´c, Stein, and Ney (2006). The process is: 1. Split compounds o</context>
</contexts>
<marker>Macherey, Dai, Talbot, Popat, Och, 2011</marker>
<rawString>Macherey, Klaus, Andrew Dai, David Talbot, Ashok Popat, and Franz Och. 2011. Language-independent compound splitting with morphological operations. In Proceedings of the 49th Annual Meeting of the ACL: Human Language Technologies, pages 1,395–1,404, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Hermann Ney</author>
</authors>
<title>Improving SMT quality with morpho-syntactic analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>1--081</pages>
<contexts>
<context position="11834" citStr="Nießen and Ney 2000" startWordPosition="1767" endWordPosition="1770">x words, that is, words that occur only once in a text (Carlberger et al. 2005). The most common type of compound is the noun+noun compound, which makes up 62% of the compounds in the German news corpus of Baroni, Matiasek, and Trost. 3. Related Work The problems arising from differences in compounding strategies in translation from German into English have been addressed by several authors. The most common architecture for translation from German is to split compounds in a preprocessing step prior to training and translation using some automatic method, which has been suggested both for SMT (Nießen and Ney 2000; Koehn and Knight 2003; Popovi´c, Stein, and Ney 2006; Holmqvist, Stymne, and Ahrenberg 2007) and example-based MT (Brown 2002). German compounds are split into their component parts in a preprocessing step and the translation model is then trained between modified German and English. At translation time, the German source text is also run through a compound splitter. In the studies cited here, only one splitting option is given as input to the decoder, which can be problematic in case the splitting is wrong, or if any of the parts are unknown. In Dyer (2009) several splitting options were gi</context>
<context position="20176" citStr="Nießen and Ney (2000)" startWordPosition="3093" endWordPosition="3096">e were several valid splitting options they chose one based on the number of splits, the geometric mean of part frequencies, or based on alignment data. They evaluated the splitting algorithms intrinsically on a gold standard of manually split noun phrases and on machine translation of noun phrases. The best results for PBSMT were achieved by using either the geometric mean, or the highest number of splits. There were no correlations between translation results and the intrinsic evaluation. Several other researchers have also explored compound splitting for translation from German to English. Nießen and Ney (2000) used a morpho-syntactic analyzer for splitting German compounds prior to translation. Popovi´c, Stein, and Ney (2006) used the geometric mean version from Koehn and Knight (2003) as well as the morphosyntactic algorithm from Nießen and Ney (2000) for splitting, with similar positive results for both options. Fritzinger and Fraser (2010) combined linguistic analysis with corpus-driven scoring and showed an improvement compared to using only a corpus-driven approach. Macherey et al. (2011) described a corpus-driven method that learns compounding form transformations of a language in addition to</context>
</contexts>
<marker>Nießen, Ney, 2000</marker>
<rawString>Nießen, Sonja and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In Proceedings of the 18th International Conference on Computational Linguistics, pages 1,081–1,085, Saarbr¨ucken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical machine translation with scarce resources using morpho-syntactic information.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<marker>Nießen, Ney, 2004</marker>
<rawString>Nießen, Sonja and Hermann Ney. 2004. Statistical machine translation with scarce resources using morpho-syntactic information. Computational Linguistics, 30(2):181–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the ACL,</booktitle>
<pages>160--167</pages>
<location>Sapporo.</location>
<contexts>
<context position="55161" citStr="Och 2003" startWordPosition="8667" endWordPosition="8668">model on parts-of-speech. For part-of-speech tagging we used TreeTagger (Schmid 1994) for German, an in-house hidden Markov model tagger based on Cutting et al. (1992) for Danish and Swedish, and for Swedish also the Granska tagger (Carlberger and Kann 1999). For the majority of experiments we used the Moses decoder (Koehn et al. 2007), which is a standard phrase-based statistical decoder, which allows factored decoding. For word alignment, Giza++ (Och and Ney 2003) was used and for language modeling we used SRILM (Stolcke 2002). For parameter optimization we used minimum error rate training (Och 2003). For each experiment we ran minimum error rate training three times in order to reduce the effect of optimizer instability, and report the average result and standard deviation. In the merging experiments based on sequence labeling we used the CRF++ toolkit.6 For the merging experiment with sequence labeling, we used the Matrax decoder (Simard et al. 2005) on the automotive corpus. Matrax is a phrasebased decoder that allows discontiguous phrases, and parameter optimization based on gradient descent for smoothed NIST. We extended the original Matrax decoder with factored decoding on the targe</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, Franz Josef. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 42nd Annual Meeting of the ACL, pages 160–167, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="55022" citStr="Och and Ney 2003" startWordPosition="8643" endWordPosition="8646">actored translation (Koehn and Hoang 2007) in our experiments, with both surface words and part-of-speech tags on the target side, with a sequence model on parts-of-speech. For part-of-speech tagging we used TreeTagger (Schmid 1994) for German, an in-house hidden Markov model tagger based on Cutting et al. (1992) for Danish and Swedish, and for Swedish also the Granska tagger (Carlberger and Kann 1999). For the majority of experiments we used the Moses decoder (Koehn et al. 2007), which is a standard phrase-based statistical decoder, which allows factored decoding. For word alignment, Giza++ (Och and Ney 2003) was used and for language modeling we used SRILM (Stolcke 2002). For parameter optimization we used minimum error rate training (Och 2003). For each experiment we ran minimum error rate training three times in order to reduce the effect of optimizer instability, and report the average result and standard deviation. In the merging experiments based on sequence labeling we used the CRF++ toolkit.6 For the merging experiment with sequence labeling, we used the Matrax decoder (Simard et al. 2005) on the automotive corpus. Matrax is a phrasebased decoder that allows discontiguous phrases, and para</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="56053" citStr="Papineni et al. 2002" startWordPosition="8801" endWordPosition="8804">rging experiment with sequence labeling, we used the Matrax decoder (Simard et al. 2005) on the automotive corpus. Matrax is a phrasebased decoder that allows discontiguous phrases, and parameter optimization based on gradient descent for smoothed NIST. We extended the original Matrax decoder with factored decoding on the target side. Compounds were split before training using the corpus-based method described in Section 5. Except for the experiments comparing different compound merging methods, we used the POS-match merging algorithm developed by us. We report results on three metrics: Bleu (Papineni et al. 2002), NIST (Doddington 2002), and Meteor. For Meteor we use the version tuned on adequacy and fluency (Lavie and Agarwal 2007) for German, and the original version with default weights (Banerjee and Lavie 2005) for Swedish and Danish, since there is no tuned version for those languages. For Bleu and Meteor we use the %-notation. Significance testing was performed using approximate randomization (Riezler and Maxwell 2005), with 10,000 iterations, on output based on three optimizer runs, as recommended by Clark et al. (2011). 8.2 General Effects of the Compound Processing Strategy One effect of comp</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the ACL, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical machine translation with a small amount of bilingual training data.</title>
<date>2006</date>
<booktitle>In 5th LREC SALTMIL Workshop on Minority Languages,</booktitle>
<pages>25--29</pages>
<location>Genoa.</location>
<marker>Popovi´c, Ney, 2006</marker>
<rawString>Popovi´c, Maja and Hermann Ney. 2006. Statistical machine translation with a small amount of bilingual training data. In 5th LREC SALTMIL Workshop on Minority Languages, pages 25–29, Genoa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Daniel Stein</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical machine translation of German compound words.</title>
<date>2006</date>
<booktitle>In Proceedings of FinTAL – 5th International Conference on Natural Language Processing,</booktitle>
<pages>616--624</pages>
<location>Turku.</location>
<marker>Popovi´c, Stein, Ney, 2006</marker>
<rawString>Popovi´c, Maja, Daniel Stein, and Hermann Ney. 2006. Statistical machine translation of German compound words. In Proceedings of FinTAL – 5th International Conference on Natural Language Processing, pages 616–624, Turku.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of IEEE,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="46087" citStr="Rabiner 1989" startWordPosition="7263" endWordPosition="7264">s in an unrestricted way. As discussed, in fact, the list-based heuristics can only form compounds that were observed in the training data or in some suitable monolingual corpus, and are thus not productive. The POS-match heuristic is more flexible, but is still limited in that it can only form 1082 Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT a compound if a modifying element (non-head) has been observed and tagged as such in the training data. The array of sequence labeling algorithms potentially suitable to our problem is fairly broad, including hidden Markov models (Rabiner 1989), conditional random fields (CRFs) (Lafferty, McCallum, and Pereira 2001), Semi-CRFs (Sarawagi and Cohen 2004), structured perceptrons (Collins 2002), structured support vector machines (Tsochantaridis et al. 2005), Max-Margin Markov networks (Taskar, Guestrin, and Koller 2003), and more. Because the focus of this work is on the application rather than on a comparison among alternative structured learning approaches, we limited ourselves to a single implementation. Considering its good scaling capabilities, capability to handle strongly redundant and overlapping features, and widespread recogn</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner, Lawrence R. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of IEEE, 77(2):257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at ACL’05,</booktitle>
<pages>57--64</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="56473" citStr="Riezler and Maxwell 2005" startWordPosition="8866" endWordPosition="8869"> Section 5. Except for the experiments comparing different compound merging methods, we used the POS-match merging algorithm developed by us. We report results on three metrics: Bleu (Papineni et al. 2002), NIST (Doddington 2002), and Meteor. For Meteor we use the version tuned on adequacy and fluency (Lavie and Agarwal 2007) for German, and the original version with default weights (Banerjee and Lavie 2005) for Swedish and Danish, since there is no tuned version for those languages. For Bleu and Meteor we use the %-notation. Significance testing was performed using approximate randomization (Riezler and Maxwell 2005), with 10,000 iterations, on output based on three optimizer runs, as recommended by Clark et al. (2011). 8.2 General Effects of the Compound Processing Strategy One effect of compound splitting is that the number of word types is reduced. Tables 5 and 6 show the number of types (number of unique words) and tokens (total number of words) and the type/token ratio for Europarl in the different representation schemes. The type count and the type/token ratio in the baseline system are much higher in 6 http://crfpp.sourceforge.net/. 1086 Stymne, Cancedda, and Ahrenberg Generation of Compound Words </context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Riezler, Stefan and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at ACL’05, pages 57–64, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semi-Markov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems 17 (NIPS),</booktitle>
<pages>1--185</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="46197" citStr="Sarawagi and Cohen 2004" startWordPosition="7275" endWordPosition="7278">that were observed in the training data or in some suitable monolingual corpus, and are thus not productive. The POS-match heuristic is more flexible, but is still limited in that it can only form 1082 Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT a compound if a modifying element (non-head) has been observed and tagged as such in the training data. The array of sequence labeling algorithms potentially suitable to our problem is fairly broad, including hidden Markov models (Rabiner 1989), conditional random fields (CRFs) (Lafferty, McCallum, and Pereira 2001), Semi-CRFs (Sarawagi and Cohen 2004), structured perceptrons (Collins 2002), structured support vector machines (Tsochantaridis et al. 2005), Max-Margin Markov networks (Taskar, Guestrin, and Koller 2003), and more. Because the focus of this work is on the application rather than on a comparison among alternative structured learning approaches, we limited ourselves to a single implementation. Considering its good scaling capabilities, capability to handle strongly redundant and overlapping features, and widespread recognition in the NLP community, we chose to use CRFs. 7.3.1 Features. Each sequence item (i.e., each separation po</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sarawagi, Sunita and William W. Cohen. 2004. Semi-Markov conditional random fields for information extraction. In Advances in Neural Information Processing Systems 17 (NIPS), pages 1,185–1,192, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Schiller</author>
</authors>
<title>German compound analysis with WFSC.</title>
<date>2005</date>
<booktitle>In Proceedings of the Finite State Methods and Natural Language Processing,</booktitle>
<pages>239--246</pages>
<location>Helsinki.</location>
<contexts>
<context position="10714" citStr="Schiller 2005" startWordPosition="1577" endWordPosition="1578">t) br¨odrak¨arlek (brotherly love) SV ”-er/+ra broder+k¨arlek (brother love) børnesko (children’s shoe) DA ”+e barn+sko (child shoe) 1070 Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT (3) SV tullagstiftning customs legislation tull+lagstiftning custom legislation DE Zelllinie cell line Zell+Linie cell line Compounding is common and productive; new compounds can be readily formed and understood. This is confirmed in a number of corpus studies. In German, compounds have been shown to make up 5–7% of tokens and 43–47% of types in news text (Baroni, Matiasek, and Trost 2002; Schiller 2005). If function words are removed, an even higher number of the tokens are compounds; in both Swedish and German 10% of the content words in a news text have been found to be compounds (Hedlund 2002). That compounding is productive means that it is likely that a high number of compounds have a very low frequency in texts. Baroni, Matiasek, and Trost (2002) found that 83% of the compounds in a large German news corpus occur less than five times. In Swedish, compounds are the most common type of hapax words, that is, words that occur only once in a text (Carlberger et al. 2005). The most common ty</context>
<context position="18700" citStr="Schiller 2005" startWordPosition="2869" endWordPosition="2870">on model (Koehn and Hoang 2007) where surface forms were generated from this information; this gave a small improvement on a large corpus, but at the cost of high runtime. El Kholy and Habash (2010) extended the merging scheme of Badr, Zbib, and Glass (2008) by using the conditional probability and a language model score to pick the best known merging option. They showed a small effect of this on an MT task, even though this strategy was the best option in an intrinsic evaluation based on human reference translations. Compound splitting has been addressed in many articles, as a separate task (Schiller 2005) or targeted for applications such as information retrieval (Holz and Biemann 2008), speech recognition (Larson et al. 2000), grammar checking (Sj¨obergh and Kann 2004), lexicon acquisition (Kokkinakis 2001), word prediction (Baroni, Matiasek, and Trost 2002), and machine translation (Koehn and Knight 2003; Dyer 2009; Fritzinger and Fraser 2010; Macherey et al. 2011). The most successful strategies that address compound processing for MT apply compound splitting as a preprocessing step before training the translation models. Koehn and Knight (2003) presented an empirical splitting algorithm ta</context>
</contexts>
<marker>Schiller, 2005</marker>
<rawString>Schiller, Anne. 2005. German compound analysis with WFSC. In Proceedings of the Finite State Methods and Natural Language Processing, pages 239–246, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester.</location>
<contexts>
<context position="54637" citStr="Schmid 1994" startWordPosition="8583" endWordPosition="8584">mental settings for the experiments in Sections 8.2–8.5. Name Europarl German Europarl Swedish Auto Swedish Auto Danish Corpus Europarl Europarl Automotive Automotive Languages En→De En→Sv En→Sv En→Da Training sentences 701,157 701,157 329,090 168,047 Avg. target sentence length 20.5 19.4 9.3 9.2 Dev sentences 500 500 2,000 1,000 Test sentences 2,000 2,000 1,000 1,000 We used factored translation (Koehn and Hoang 2007) in our experiments, with both surface words and part-of-speech tags on the target side, with a sequence model on parts-of-speech. For part-of-speech tagging we used TreeTagger (Schmid 1994) for German, an in-house hidden Markov model tagger based on Cutting et al. (1992) for Danish and Swedish, and for Swedish also the Granska tagger (Carlberger and Kann 1999). For the majority of experiments we used the Moses decoder (Koehn et al. 2007), which is a standard phrase-based statistical decoder, which allows factored decoding. For word alignment, Giza++ (Och and Ney 2003) was used and for language modeling we used SRILM (Stolcke 2002). For parameter optimization we used minimum error rate training (Och 2003). For each experiment we ran minimum error rate training three times in orde</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid, Helmut. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, pages 44–49, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Florian Laws</author>
</authors>
<title>Estimation of conditional probabilities with decision trees and an application to fine-grained POS tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>777--784</pages>
<location>Manchester.</location>
<contexts>
<context position="106688" citStr="Schmid and Laws 2008" startWordPosition="17015" endWordPosition="17018">tical machine transition, year 2008 and 2010, which we will refer to as WMT08 and WMT10.7 The corpora used are presented in Table 25. In these experiments we also investigated the effects of in- and out-of-domain data. For WMT08 we trained and tuned on only Europarl data, and tested both in-domain, on Europarl and out-of-domain, on News. For WMT10 we trained on a mix of data, and tested on News. For these experiments we used morphologically enriched POS-tagsets. For WMT08 we used a commercial dependency parser for the morphology (Tapanainen and J¨arvinen 1997), and for WMT10 we used RFTagger (Schmid and Laws 2008). For the systems with split compounds we used the EPOS-tagset based on morphological tags. We used the POS-match merging algorithm. Table 26 shows the results on the WMT08 evaluation. On Europarl the system with compound processing and an EPOS-model is significantly better than both baselines on NIST, and on the News domain it is significantly better than both baselines on Bleu. On the other metrics the EPOS system is on par with the baseline for both domains. The differences between the two baselines are mostly small. Overall, the scores are much lower on the out-of-domain data. Table 27 sho</context>
</contexts>
<marker>Schmid, Laws, 2008</marker>
<rawString>Schmid, Helmut and Florian Laws. 2008. Estimation of conditional probabilities with decision trees and an application to fine-grained POS tagging. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 777–784, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Nicola Cancedda</author>
<author>Bruno Cavestro</author>
<author>Marc Dymetman</author>
<author>Eric Gaussier</author>
<author>Cyril Goutte</author>
<author>Kenji Yamada</author>
<author>Philippe Langlais</author>
<author>Arne Mauser</author>
</authors>
<title>Translating with non-contiguous phrases.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>755--762</pages>
<location>Vancouver.</location>
<contexts>
<context position="55520" citStr="Simard et al. 2005" startWordPosition="8722" endWordPosition="8725"> standard phrase-based statistical decoder, which allows factored decoding. For word alignment, Giza++ (Och and Ney 2003) was used and for language modeling we used SRILM (Stolcke 2002). For parameter optimization we used minimum error rate training (Och 2003). For each experiment we ran minimum error rate training three times in order to reduce the effect of optimizer instability, and report the average result and standard deviation. In the merging experiments based on sequence labeling we used the CRF++ toolkit.6 For the merging experiment with sequence labeling, we used the Matrax decoder (Simard et al. 2005) on the automotive corpus. Matrax is a phrasebased decoder that allows discontiguous phrases, and parameter optimization based on gradient descent for smoothed NIST. We extended the original Matrax decoder with factored decoding on the target side. Compounds were split before training using the corpus-based method described in Section 5. Except for the experiments comparing different compound merging methods, we used the POS-match merging algorithm developed by us. We report results on three metrics: Bleu (Papineni et al. 2002), NIST (Doddington 2002), and Meteor. For Meteor we use the version</context>
</contexts>
<marker>Simard, Cancedda, Cavestro, Dymetman, Gaussier, Goutte, Yamada, Langlais, Mauser, 2005</marker>
<rawString>Simard, Michel, Nicola Cancedda, Bruno Cavestro, Marc Dymetman, Eric Gaussier, Cyril Goutte, Kenji Yamada, Philippe Langlais, and Arne Mauser. 2005. Translating with non-contiguous phrases. In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing, pages 755–762, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonas Sj¨obergh</author>
<author>Viggo Kann</author>
</authors>
<title>Finding the correct interpretation of Swedish compounds, a statistical approach.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04),</booktitle>
<pages>899--902</pages>
<location>Lisbon.</location>
<marker>Sj¨obergh, Kann, 2004</marker>
<rawString>Sj¨obergh, Jonas and Viggo Kann. 2004. Finding the correct interpretation of Swedish compounds, a statistical approach. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04), pages 899–902, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the Seventh International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="55086" citStr="Stolcke 2002" startWordPosition="8656" endWordPosition="8657">both surface words and part-of-speech tags on the target side, with a sequence model on parts-of-speech. For part-of-speech tagging we used TreeTagger (Schmid 1994) for German, an in-house hidden Markov model tagger based on Cutting et al. (1992) for Danish and Swedish, and for Swedish also the Granska tagger (Carlberger and Kann 1999). For the majority of experiments we used the Moses decoder (Koehn et al. 2007), which is a standard phrase-based statistical decoder, which allows factored decoding. For word alignment, Giza++ (Och and Ney 2003) was used and for language modeling we used SRILM (Stolcke 2002). For parameter optimization we used minimum error rate training (Och 2003). For each experiment we ran minimum error rate training three times in order to reduce the effect of optimizer instability, and report the average result and standard deviation. In the merging experiments based on sequence labeling we used the CRF++ toolkit.6 For the merging experiment with sequence labeling, we used the Matrax decoder (Simard et al. 2005) on the automotive corpus. Matrax is a phrasebased decoder that allows discontiguous phrases, and parameter optimization based on gradient descent for smoothed NIST. </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, Andreas. 2002. SRILM—an extensible language modeling toolkit. In Proceedings of the Seventh International Conference on Spoken Language Processing, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
</authors>
<title>German compounds in factored statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of GoTAL – 6th International Conference on Natural Language Processing,</booktitle>
<pages>464--475</pages>
<contexts>
<context position="5382" citStr="Stymne (2008)" startWordPosition="797" endWordPosition="798">in the methods for compound splitting and merging. The rest of the article is structured as follows. Section 2 gives an overview of compound formation in the three target languages used in this work. Section 3 reports related work on compound processing for machine translation. Section 4 describes the compound processing strategy we use and Section 5 describes compound splitting. Section 6 addresses compound coalescence, followed by compound merging in Section 7. In Section 8 we present experimental results and in Section 9 we state our conclusions. 1 This work is a synthesis and extension of Stymne (2008); Stymne and Holmqvist (2008); Stymne, Holmqvist, and Ahrenberg (2008); Stymne (2009); and Stymne and Cancedda (2011). 1068 Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT 2. Closed Compounds in German, Swedish, and Danish Compounds in German, Swedish, and Danish are generally closed, written without word boundaries, as exemplified for German in Example (1). Compounds can be made up of two (1a) or more (1b) parts where parts may also be coordinated (1c). In a few cases compounds are written with a hyphen (1d), often when one of the parts is a proper name or an abbreviation.</context>
<context position="23705" citStr="Stymne (2008)" startWordPosition="3636" endWordPosition="3637">ation of Compound Words in SMT compounds, or we can merge compounds in the translation output, before performing the optimization. We found empirically that we got the best results using the second approach, of performing compound merging during the tuning process. When we tuned on split texts, the results were more unstable, and especially the number of words in the translation output varied substantially. We thus use merging also during the tuning process in all experiments, as shown in Figure 1. 5. Compound Splitting Our method for compound splitting is based on Koehn and Knight (2003) and Stymne (2008). For each word all possible segmentations are explored, with the restrictions that all parts must have at least three characters, and the last part, the compound head, must have the same part-of-speech tag as the word itself. Hyphens are treated as additions to compound modifiers, just as +s or +e. Segmentations are scored with the arithmetic mean of frequencies for each part in the training corpus and the segmentation with the highest score is chosen. We have investigated several variants of the basic method and their effects on compound translation. The variants investigated are: • Using ge</context>
<context position="76802" citStr="Stymne (2008)" startWordPosition="12178" endWordPosition="12179">f using the arithmetic mean part4 – limiting the length of compound parts to four characters max2 – limiting the maximum number of parts per compound to two common – only allowing the common compound suffixes -s, -es, -n, -nen anypos – not using part-of-speech tags The corpus frequencies were gathered from the target side of the training corpus. We compared the systems with compound splitting with a factored baseline system without any compound processing. To measure the success of the different compound splitting algorithms we performed an intrinsic evaluation. We used the gold standard from Stymne (2008), created by manually annotating the first 5,000 words of the test text for one-to-one correspondence with the English reference text, similar to Koehn and Knight (2003). A one-toone correspondence occurs when the words in a German compound are translated as separate words in English. In addition there can be inserted function words. As an example, Medienfreiheit is in one-to-one correspondence with freedom of the media, since the two German parts Medien and Freiheit corresponds to two separate words, media and freedom. The two function words of, the are ignored. Out of the 5,000 words 174 wer</context>
<context position="82257" citStr="Stymne 2008" startWordPosition="13079" endWordPosition="13080">e eager and geom system performed similarly on a translation task, while the eager system was much worse on their intrinsic evaluation. In our evaluation only the eager system is significantly worse than any other system on Bleu, where it is worse than the baseline and the arith system. The arith system overall is competitive with both the baseline and the other split systems, despite the relatively low results on the intrinsic evaluation. Overall, it seems that the translation task is not very sensitive to the quality of the splitting strategy. As in previous research (Koehn and Knight 2003; Stymne 2008) there are no clear relations between the intrinsic evaluation and the MT evaluation. Both systems with low intrinsic accuracy (such as anypos) and with high accuracy (such as geom) tend to be worse than other systems on at least some MT metrics, even though the differences are small. We thus think that for the task of translation into compounding languages, the MT performance cannot be predicted based on intrinsic evaluations. In a previous similar study using a smaller corpus (Stymne 2008), we found somewhat bigger differences between the systems, and in that study most of the systems with s</context>
</contexts>
<marker>Stymne, 2008</marker>
<rawString>Stymne, Sara. 2008. German compounds in factored statistical machine translation. In Proceedings of GoTAL – 6th International Conference on Natural Language Processing, pages 464–475, Gothenburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
</authors>
<title>A comparison of merging strategies for translation of German compounds.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Student Research Workshop,</booktitle>
<pages>61--69</pages>
<location>Athens.</location>
<contexts>
<context position="5467" citStr="Stymne (2009)" startWordPosition="808" endWordPosition="809">red as follows. Section 2 gives an overview of compound formation in the three target languages used in this work. Section 3 reports related work on compound processing for machine translation. Section 4 describes the compound processing strategy we use and Section 5 describes compound splitting. Section 6 addresses compound coalescence, followed by compound merging in Section 7. In Section 8 we present experimental results and in Section 9 we state our conclusions. 1 This work is a synthesis and extension of Stymne (2008); Stymne and Holmqvist (2008); Stymne, Holmqvist, and Ahrenberg (2008); Stymne (2009); and Stymne and Cancedda (2011). 1068 Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT 2. Closed Compounds in German, Swedish, and Danish Compounds in German, Swedish, and Danish are generally closed, written without word boundaries, as exemplified for German in Example (1). Compounds can be made up of two (1a) or more (1b) parts where parts may also be coordinated (1c). In a few cases compounds are written with a hyphen (1d), often when one of the parts is a proper name or an abbreviation. Most compounds are nouns (1a–1e), but they can also be adjectives (1f), verbs (1g), </context>
</contexts>
<marker>Stymne, 2009</marker>
<rawString>Stymne, Sara. 2009. A comparison of merging strategies for translation of German compounds. In Proceedings of the EACL 2009 Student Research Workshop, pages 61–69, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Nicola Cancedda</author>
</authors>
<title>Productive generation of compound words in statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>250--260</pages>
<location>Edinburgh.</location>
<contexts>
<context position="5499" citStr="Stymne and Cancedda (2011)" startWordPosition="811" endWordPosition="814">tion 2 gives an overview of compound formation in the three target languages used in this work. Section 3 reports related work on compound processing for machine translation. Section 4 describes the compound processing strategy we use and Section 5 describes compound splitting. Section 6 addresses compound coalescence, followed by compound merging in Section 7. In Section 8 we present experimental results and in Section 9 we state our conclusions. 1 This work is a synthesis and extension of Stymne (2008); Stymne and Holmqvist (2008); Stymne, Holmqvist, and Ahrenberg (2008); Stymne (2009); and Stymne and Cancedda (2011). 1068 Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT 2. Closed Compounds in German, Swedish, and Danish Compounds in German, Swedish, and Danish are generally closed, written without word boundaries, as exemplified for German in Example (1). Compounds can be made up of two (1a) or more (1b) parts where parts may also be coordinated (1c). In a few cases compounds are written with a hyphen (1d), often when one of the parts is a proper name or an abbreviation. Most compounds are nouns (1a–1e), but they can also be adjectives (1f), verbs (1g), and adverbs (1h). English transl</context>
<context position="102152" citStr="Stymne and Cancedda 2011" startWordPosition="16291" endWordPosition="16294"> compounds). The differences between the systems on MT metrics are generally small and mostly not significant, as could be expected, since the differences were small on the intrinsic evaluation. For German Europarl only 10–739 out of 29,786 words are actually different between any pair of systems. The only noticeable differences between the systems are that the list system is worse than the other systems for Danish and German. The list strategy is competitive for Swedish automotive, which is much bigger than Danish, and in one of our previous experiments, with a large Swedish Europarl corpus (Stymne and Cancedda 2011), confirming our intuition that the list strategy tends to work better with large corpora. Even though we do not see much effect on MT metrics, we still think the small differences on the intrinsic evaluation are important. Compound merging is a postprocessing process, which operates directly on the actual translation results. This is quite different from compound splitting, as described in Section 8.4, which is performed as part of the preprocessing and where we cannot be sure on the impact of the preprocessing on the translation process. The sequence labeler has the advantage over the heuris</context>
</contexts>
<marker>Stymne, Cancedda, 2011</marker>
<rawString>Stymne, Sara and Nicola Cancedda. 2011. Productive generation of compound words in statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 250–260, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Maria Holmqvist</author>
</authors>
<title>Processing of Swedish compounds for phrase-based statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Annual Conference of the European Association for Machine Translation,</booktitle>
<pages>180--189</pages>
<location>Hamburg.</location>
<contexts>
<context position="5411" citStr="Stymne and Holmqvist (2008)" startWordPosition="799" endWordPosition="802">for compound splitting and merging. The rest of the article is structured as follows. Section 2 gives an overview of compound formation in the three target languages used in this work. Section 3 reports related work on compound processing for machine translation. Section 4 describes the compound processing strategy we use and Section 5 describes compound splitting. Section 6 addresses compound coalescence, followed by compound merging in Section 7. In Section 8 we present experimental results and in Section 9 we state our conclusions. 1 This work is a synthesis and extension of Stymne (2008); Stymne and Holmqvist (2008); Stymne, Holmqvist, and Ahrenberg (2008); Stymne (2009); and Stymne and Cancedda (2011). 1068 Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT 2. Closed Compounds in German, Swedish, and Danish Compounds in German, Swedish, and Danish are generally closed, written without word boundaries, as exemplified for German in Example (1). Compounds can be made up of two (1a) or more (1b) parts where parts may also be coordinated (1c). In a few cases compounds are written with a hyphen (1d), often when one of the parts is a proper name or an abbreviation. Most compounds are nouns (1a</context>
<context position="7784" citStr="Stymne and Holmqvist (2008)" startWordPosition="1168" endWordPosition="1171">ils in most instances gr¨ossten+teils largest partly Compound modifiers often have a special form, such as the addition of an “s” to the base form of Regierung in Example (1a). We will refer to these form variants as compounding forms. Table 1 exemplifies the type of operations used in Swedish, German, and Danish to form compounding forms. There are many more alternative compounding forms in Swedish and German than in Danish. For an overview of the possible compounding forms in German see Langer (1998) or K¨urschner (2003), in Danish see K¨urschner (2003), and in Swedish see Thorell (1981) or Stymne and Holmqvist (2008). Some compounding forms coincide with paradigmatic forms, such as German Jahres that can also be genitive, and Stadien that can also be plural, from Table 1. There are different views on whether to treat these forms as paradigmatic forms or as compounding forms. We follow Langer (1998) in viewing them as compounding forms, because they often do not correspond to plural or possessive semantics. Many individual compound modifiers have more than one possible compounding form. In Example (2) we provide examples of several possible forms of the modifier Kind (child) in German compounds. 1069 Compu</context>
</contexts>
<marker>Stymne, Holmqvist, 2008</marker>
<rawString>Stymne, Sara and Maria Holmqvist. 2008. Processing of Swedish compounds for phrase-based statistical machine translation. In Proceedings of the 12th Annual Conference of the European Association for Machine Translation, pages 180–189, Hamburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Maria Holmqvist</author>
<author>Lars Ahrenberg</author>
</authors>
<title>Effects of morphological analysis in translation between German and English.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>135--138</pages>
<location>Columbus, OH.</location>
<marker>Stymne, Holmqvist, Ahrenberg, 2008</marker>
<rawString>Stymne, Sara, Maria Holmqvist, and Lars Ahrenberg. 2008. Effects of morphological analysis in translation between German and English. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 135–138, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paso Tapanainen</author>
<author>Timo J¨arvinen</author>
</authors>
<title>A nonprojective dependency parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<pages>64--71</pages>
<location>Washington, DC.</location>
<marker>Tapanainen, J¨arvinen, 1997</marker>
<rawString>Tapanainen, Paso and Timo J¨arvinen. 1997. A nonprojective dependency parser. In Proceedings of the 5th Conference on Applied Natural Language Processing, pages 64–71, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Carlos Guestrin</author>
<author>Daphne Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 16 (NIPS),</booktitle>
<pages>25--32</pages>
<location>Vancouver.</location>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>Taskar, Ben, Carlos Guestrin, and Daphne Koller. 2003. Max-margin Markov networks. In Advances in Neural Information Processing Systems 16 (NIPS), pages 25–32, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olof 1981 Thorell</author>
</authors>
<title>Svensk ordbildningsl¨ara. Esselte Studium,</title>
<date></date>
<location>Stockholm,</location>
<marker>Thorell, </marker>
<rawString>Thorell, Olof.1981. Svensk ordbildningsl¨ara. Esselte Studium, Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thorsten Joachims</author>
<author>Thomas Hofmann</author>
<author>Altun Yasemin</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1</pages>
<contexts>
<context position="46301" citStr="Tsochantaridis et al. 2005" startWordPosition="7287" endWordPosition="7290">uctive. The POS-match heuristic is more flexible, but is still limited in that it can only form 1082 Stymne, Cancedda, and Ahrenberg Generation of Compound Words in SMT a compound if a modifying element (non-head) has been observed and tagged as such in the training data. The array of sequence labeling algorithms potentially suitable to our problem is fairly broad, including hidden Markov models (Rabiner 1989), conditional random fields (CRFs) (Lafferty, McCallum, and Pereira 2001), Semi-CRFs (Sarawagi and Cohen 2004), structured perceptrons (Collins 2002), structured support vector machines (Tsochantaridis et al. 2005), Max-Margin Markov networks (Taskar, Guestrin, and Koller 2003), and more. Because the focus of this work is on the application rather than on a comparison among alternative structured learning approaches, we limited ourselves to a single implementation. Considering its good scaling capabilities, capability to handle strongly redundant and overlapping features, and widespread recognition in the NLP community, we chose to use CRFs. 7.3.1 Features. Each sequence item (i.e., each separation point between words) is represented by means of a vector of features. Our aim was to include features repr</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Yasemin, 2005</marker>
<rawString>Tsochantaridis, Ioannis, Thorsten Joachims, Thomas Hofmann, and Altun Yasemin. 2005. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:1,453–1,484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sami Virpioja</author>
<author>Jaako J V¨ayrynen</author>
<author>Mathias Creutz</author>
<author>Markus Sadeniemi</author>
</authors>
<title>Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner.</title>
<date>2007</date>
<marker>Virpioja, V¨ayrynen, Creutz, Sadeniemi, 2007</marker>
<rawString>Virpioja, Sami, Jaako J. V¨ayrynen, Mathias Creutz, and Markus Sadeniemi. 2007. Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of MT Summit XI,</booktitle>
<pages>491--498</pages>
<location>Copenhagen.</location>
<marker></marker>
<rawString>In Proceedings of MT Summit XI, pages 491–498, Copenhagen.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>