<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.051724">
<title confidence="0.997913">
Monolingual Alignment by Edit Rate Computation
on Sentential Paraphrase Pairs
</title>
<author confidence="0.748388">
Houda Bouamor Aur´elien Max Anne Vilnat
</author>
<affiliation confidence="0.664067">
LIMSI-CNRS
</affiliation>
<address confidence="0.6655995">
Univ. Paris Sud
Orsay, France
</address>
<email confidence="0.999277">
{firstname.lastname}@limsi.fr
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996478166666667">
In this paper, we present a novel way of tack-
ling the monolingual alignment problem on
pairs of sentential paraphrases by means of
edit rate computation. In order to inform the
edit rate, information in the form of subsenten-
tial paraphrases is provided by a range of tech-
niques built for different purposes. We show
that the tunable TER-PLUS metric from Ma-
chine Translation evaluation can achieve good
performance on this task and that it can effec-
tively exploit information coming from com-
plementary sources.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999793886792453">
The acquisition of subsentential paraphrases has at-
tracted a lot of attention recently (Madnani and Dorr,
2010). Techniques are usually developed for extract-
ing paraphrase candidates from specific types of cor-
pora, including monolingual parallel corpora (Barzi-
lay and McKeown, 2001), monolingual comparable
corpora (Del´eger and Zweigenbaum, 2009), bilin-
gual parallel corpora (Bannard and Callison-Burch,
2005), and edit histories of multi-authored text (Max
and Wisniewski, 2010). These approaches face two
main issues, which correspond to the typical mea-
sures of precision, or how appropriate the extracted
paraphrases are, and of recall, or how many of the
paraphrases present in a given corpus can be found
effectively. To start with, both measures are often
hard to compute in practice, as 1) the definition of
what makes an acceptable paraphrase pair is still
a research question, and 2) it is often impractical
to extract a complete set of acceptable paraphrases
from most resources. Second, as regards the pre-
cision of paraphrase acquisition techniques in par-
ticular, it is notable that most works on paraphrase
acquisition are not based on direct observation of
larger paraphrase pairs. Even monolingual corpora
obtained by pairing very closely related texts such as
news headlines on the same topic and from the same
time frame (Dolan et al., 2004) often contain unre-
lated segments that should not be aligned to form a
subsentential paraphrase pair. Using bilingual cor-
pora to acquire paraphrases indirectly by pivoting
through other languages is faced, in particular, with
the issue of phrase polysemy, both in the source and
in the pivot languages.
It has previously been noted that highly parallel
monolingual corpora, typically obtained via mul-
tiple translation into the same language, consti-
tute the most appropriate type of corpus for ex-
tracting high quality paraphrases, in spite of their
rareness (Barzilay and McKeown, 2001; Cohn et
al., 2008; Bouamor et al., 2010). We build on this
claim here to propose an original approach for the
task of subsentential alignment based on the compu-
tation of a minimum edit rate between two sentential
paraphrases. More precisely, we concentrate on the
alignment of atomic paraphrase pairs (Cohn et al.,
2008), where the words from both paraphrases are
aligned as a whole to the words of the other para-
phrase, as opposed to composite paraphrase pairs
obtained by joining together adjacent paraphrase
pairs or possibly adding unaligned words. Figure 1
provides examples of atomic paraphrase pairs de-
rived from a word alignment between two English
sentential paraphrases.
</bodyText>
<page confidence="0.98939">
395
</page>
<note confidence="0.7783445">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 395–400,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.995279111111111">
China
will
continue
implementing
the
financial
opening
up
policy
</figure>
<figureCaption confidence="0.992388666666666">
Figure 1: Reference alignments for a pair of English
sentential paraphrases and their associated list of atomic
paraphrase pairs extracted from them. Note that identity
pairs (e.g. China H China) will never be considered in
this work and will not be taken into account for evalua-
tion.
</figureCaption>
<bodyText confidence="0.992462964912281">
The remainder of this paper is organized as fol-
lows. We first briefly describe in section 2 how we
apply edit rate computation to the task of atomic
paraphrase alignment, and we explain in section 3
how we can inform such a technique with paraphrase
candidates extracted by additional techniques. We
present our experiments and discuss their results in
section 4 and conclude in section 5.
2 Edit rate for paraphrase alignment
TER-PLUS (Translation Edit Rate Plus) (Snover et
al., 2010) is a score designed for evaluation of Ma-
chine Translation (MT) output. Its typical use takes
a system hypothesis to compute an optimal set of
word edits that can transform it into some existing
reference translation. Edit types include exact word
matching, word insertion and deletion, block move-
ment of contiguous words (computed as an approx-
imation), as well as variants substitution through
stemming, synonym or paraphrase matching. Each
edit type is parameterized by at least one weight
which can be optimized using e.g. hill climbing.
TER-PLUS is therefore a tunable metric. We will
henceforth design as TERMT the TER metric (basi-
cally, without variants matching) optimized for cor-
relation with human judgment of accuracy in MT
evaluation, which is to date one of the most used
metrics for this task.
While this metric was not designed explicitely for
the acquisition of word alignments, it produces as a
by-product of its approximate search a list of align-
ments involving either individual words or phrases,
potentially fitting with the previous definition of
atomic paraphrase pairs. When applying it on a
MT system hypothesis and a reference translation,
it computes how much effort would be needed to
obtain the reference from the hypothesis, possibly
independently of the appropriateness of the align-
ments produced. However, if we consider instead
a pair of sentential paraphrases, it can be used to
reveal what subsentential units can be aligned. Of
course, this relies on information that will often go
beyond simple exact word matching. This is where
the capability of exploiting paraphrase matching can
come into play: TER-PLUS can exploit a table of
paraphrase pairs, and defines the cost of a phrase
substitution as “a function of the probability of the
paraphrase and the number of edits needed to align
the two phrases without the use of phrase substitu-
tions”. Intuitively, the more parallel two sentential
paraphrases are, the more atomic paraphrase pairs
will be reliably found, and the easier it will be for
TER-PLUS to correctly identify the remaining pairs.
But in the general case, and considering less appar-
ently parallel sentence pairs, its work can be facil-
itated by the incorporation of candidate paraphrase
pairs in its paraphrase table. We consider this possi-
ble type of hybridation in the next section.
</bodyText>
<sectionHeader confidence="0.8365625" genericHeader="method">
3 Informing edit rate computation with
other techniques
</sectionHeader>
<bodyText confidence="0.999878692307692">
In this article, we use three baseline techniques
for paraphrase pair acquisition, which we will only
briefly introduce (see (Bouamor et al., 2010) for
more details). As explained previously, we want to
evaluate whether and how their candidate paraphrase
pairs can be used to improve paraphrase acquisition
on sentential paraphrases using TER-PLUS. We se-
lected these three techniques for the complementar-
ity of types of information that they use: statistical
word alignment without a priori linguistic knowl-
edge, symbolic expression of linguistic variation ex-
ploiting a priori linguistic knowledge, and syntactic
similarity.
</bodyText>
<figure confidence="0.9956373">
China
will
carry
on
open
financial
policy
continuer-carry on
financial opening
ups--open financial
</figure>
<page confidence="0.994837">
396
</page>
<bodyText confidence="0.998857795918367">
Statistical Word Alignment The GIZA++
tool (Och and Ney, 2004) computes statistical word
alignment models of increasing complexity from
parallel corpora. While originally developped in the
bilingual context of Machine Translation, nothing
prevents building such models on monolingual
corpora. However, in order to build reliable models
it is necessary to use enough training material
including minimal redundancy of words. To this
end, we will be using monolingual corpora made
up of multiply-translated sentences, allowing us to
provide GIZA++ with all possible sentence pairs
to improve the quality of its word alignments (note
that following common practice we used symetrized
alignments from the alignments in both directions).
This constitutes an advantage for this technique that
the following techniques working on each sentence
pair independently do not have.
Symbolic expression of linguistic variation The
FASTR tool (Jacquemin, 1999) was designed to spot
term variants in large corpora. Variants are de-
scribed through metarules expressing how the mor-
phosyntactic structure of a term variant can be de-
rived from a given term by means of regular ex-
pressions on word categories. Paradigmatic varia-
tion can also be expressed by defining constraints
between words to force them to belong to the same
morphological or semantic family, both constraints
relying on preexisting repertoires available for En-
glish and French. To compute candidate paraphrase
pairs using FASTR, we first consider all the phrases
from the first sentence and search for variants in the
other sentence, do the reverse process and take the
intersection of the two sets.
Syntactic similarity The algorithm introduced
by Pang et al. (2003) takes two sentences as in-
put and merges them by top-down syntactic fusion
guided by compatible syntactic substructure. A
lexical blocking mechanism prevents sentence con-
stituents from fusionning when there is evidence of
the presence of a word in another constituent of one
of the sentence. We use the Berkeley Probabilistic
parser (Petrov and Klein, 2007) to obtain syntac-
tic trees for English and its Bonsai adaptation for
French (Candito et al., 2010). Because this process
is highly sensitive to syntactic parse errors, we use
k-best parses (with k = 3 in our experiments) and
retain the most compact fusion from any pair of can-
didate parses.
</bodyText>
<sectionHeader confidence="0.996991" genericHeader="evaluation">
4 Experiments and discussion
</sectionHeader>
<bodyText confidence="0.999081375">
We used the methodology described by Cohn et al.
(2008) for constructing evaluation corpora and as-
sessing the performance of various techniques on the
task of paraphrase acquisition. In a nutshell, pairs of
sentential paraphrases are hand-aligned and define a
set of reference atomic paraphrase pairs at the level
of words or blocks or words, denoted as Ratom, and
also a set of reference composite paraphrase pairs
obtained by joining adjacent atomic paraphrase pairs
(up to a given length), denoted as R. Techniques
output word alignments from which atomic candi-
date paraphrase pairs, denoted as Hatom, as well as
composite paraphrase pairs, denoted as H, can be
extracted. The usual measures of precision, recall
and f-measure can then be defined in the following
way:
</bodyText>
<equation confidence="0.993991">
|Hatom ∩ R ||H ∩ Ratom |2pr
p = |Hatom |r = |Ratom |f1 = p + r
</equation>
<bodyText confidence="0.957660130434783">
To evaluate our individual techniques and their
use by the tunable TER-PLUS technique (hence-
forth TERP), we measured results on two different
corpora in French and English. In each case, a held-
out development corpus of 150 paraphrase pairs was
used for tuning the TERP hybrid systems towards
precision (→ p), recall (→ r), or F-measure (→
f1).1 All techniques were evaluated on the same test
set consisting of 375 paraphrase pairs. For English,
we used the MTC corpus described in (Cohn et al.,
2008), which consists of multiply-translated Chi-
nese sentences into English, with an average lexical
overlap2 of 65.91% (all tokens) and 63.95% (content
words only). We used as our reference set both the
alignments marked as “Sure” and “Possible”. For
French, we used the CESTA corpus of news articles3
obtained by translating into French from various lan-
guages with an average lexical overlap of 79.63%
(all tokens) and 78.19% (content words only). These
climbing was used for tuning as in (Snover et al.,
2010), with uniform weights and 100 random restarts.
compute the percentage of lexical overlap be-
tween the vocabularies of two sentences
</bodyText>
<footnote confidence="0.973316166666667">
and S2 as
1Hill
2We
S1
:|S1∩S2|/min(|S1|,|S2|)
3http://www.elda.org/article125.html
</footnote>
<page confidence="0.991367">
397
</page>
<table confidence="0.998553454545455">
Individual techniques Hybrid systems (TERPpara+X)
Giza++ Fastr Pang TMT → p TERPpara → f1 → p +G → f1 → p +F → f1 → p +P → f1 +G + F + P
G F P → r → r → r → r → p → r → f1
French French
p 28.99 52.48 62.50 25.66 31.35 30.26 31.43 41.99 30.55 41.14 36.74 29.65 34.84 54.49 20.94 33.89 42.27 27.06 42.80
r 45.98 8.59 8.65 41.15 44.22 44.60 44.10 35.88 45.67 35.25 40.96 43.85 44.41 13.61 40.40 40.46 31.36 44.10 31.61
f1 35.56 14.77 15.20 25.66 36.69 36.05 36.70 38.70 36.61 37.97 38.74 35.38 39.05 21.78 27.58 36.88 36.01 33.54 36.37
English English
p 18.28 33.02 36.66 20.41 31.19 19.14 19.35 26.89 19.85 21.25 41.57 20.81 22.51 31.32 18.02 18.92 29.45 16.81 29.42
r 14.63 5.41 2.23 17.37 2.31 19.38 19.69 11.92 18.47 17.10 6.94 21.02 20.28 3.41 18.94 16.44 13.57 19.30 16.35
f1 16.25 9.30 4.21 18.77 4.31 19.26 19.52 16.52 19.14 18.95 11.91 20.92 21.33 6.15 18.47 17.59 18.58 17.96 21.02
</table>
<figureCaption confidence="0.9970655">
Figure 2: Results on the test set on French and English for the individual techniques and TERP hybrid systems.
Column headers of the form “→ c” indicate that TERP was tuned on criterion c.
</figureCaption>
<bodyText confidence="0.999935295081968">
figures reveal that the French corpus tends to contain
more literal translations, possibly due to the original
languages of the sentences, which are closer to the
target language than Chinese is to English. We used
the YAWAT (Germann, 2008) interactive alignment
tool and measure inter-annotator agreement over a
subset and found it to be similar to the value reported
by Cohn et al. (2008) for English.
Results for all individual techniques in the two
languages are given on Figure 2. We first note that
all techniques fared better on the French corpus than
on the English corpus. This can certainly be ex-
plained by the fact that the former results from more
literal translations, which are consequently easier to
word-align.
TERMT (i.e. TER tuned for Machine Transla-
tion evaluation) performs significantly worse on all
metrics for both languages than our tuned TERP ex-
periments, revealing that the two tasks have differ-
ent objectives. The two linguistically-aware tech-
niques, FASTR and PANG, have a very strong pre-
cision on the more parallel French corpus, and also
on the English corpus to a lesser extent, but fail to
achieve a high recall (note, in particular, that they
do not attempt to report preferentially atomic para-
phrase pairs). GIZA++ and TERPpara perform in
the same range, with acceptable precision and re-
call, TERPpara performing overall better, with e.g. a
1.14 advantage on f-measure on French and 3.27 on
English. Recall that TERP works independently on
each paraphrase pair, while GIZA++ makes use of
artificial repetitions of paraphrases of the same sen-
tence.
Figure 3 gives an indication of how well each
technique performs depending on the difficulty of
the task, which we estimate here as the value
(1 − TER(para1, para2)), whose low values cor-
respond to sentences which are costly to trans-
form into the other using TER. Not surprisingly,
TERPpara and GIZA++, and PANG to a lesser ex-
tent, perform better on “more parallel” sentential
paraphrase pairs. Conversely, FASTR is not affected
by the degree of parallelism between sentences, and
manages to extract synonyms and more generally
term variants, at any level of difficulty.
We have further tested 4 hybrid configurations
by providing TERPpara with the output of the other
individual techniques and of their union, the latter
simply obtained by taking paraphrase pairs output
by at least one of these techniques. On French,
where individual techniques achieve good perfor-
mance, any hybridation improves the F-measure
over both TERPpara and the technique used, the best
performance, using FASTR, corresponding to an im-
provement of respectively +2.35 and +24.28 over
TERPpara and FASTR. Taking the union of all tech-
niques does not yield additional gains: this might
be explained by the fact that incorrect predictions
are proportionnally more present and consequently
have a greater impact when combining techniques
without weighting them, possibly at the level of each
</bodyText>
<page confidence="0.992221">
398
</page>
<figure confidence="0.999313166666666">
100
90
80
70
F-measure
60
50
40
30
20
10
0
TERpParaF1
Giza++
Fastr
Pang
&lt;0.1 &lt;0.2 &lt;0.3 &lt;0.4 &lt;0.5 &lt;0.6 &lt;0.7 &lt;0.8 &lt;0.9
F-measure
TERpParaF1
Giza++
Fastr
Pang
70
&lt;0.1 &lt;0.2 &lt;0.3 &lt;0.4 &lt;0.5 &lt;0.6 &lt;0.7 &lt;0.8 &lt;0.9
100
90
80
60
50
40
30
20
10
0
Difficulty (1-TER) Difficulty (1-TER)
(a) French (b) English
</figure>
<figureCaption confidence="0.91448675">
Figure 3: F-measure values for our 4 individual techniques on French and English depending on the complexity of
paraphrase pairs measured with the (1-TER) formula. Note that each value corresponds to the average of F-measure
values for test examples falling in a given difficulty range, and that all ranges do not necessarily contain the same
number of examples.
</figureCaption>
<bodyText confidence="0.999123625">
prediction.4 Successful hybridation on English seem
harder to obtain, which may be partly attributed to
the poor quality of the individual techniques relative
to TERPpara. We however note anew an improve-
ment over TERPpara of +1.81 when using FASTR.
This confirms that some types of linguistic equiva-
lences cannot be captured using edit rate computa-
tion alone, even on this type of corpus.
</bodyText>
<sectionHeader confidence="0.990866" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999633744186047">
In this article, we have described the use of edit rate
computation for paraphrase alignment at the sub-
sentential level from sentential paraphrases and the
possibility of informing this search with paraphrase
candidates coming from other techniques. Our ex-
periments have shown that in some circumstances
some techniques have a good complementarity and
manage to improve results significantly. We are
currently studying hard-to-align subsentential para-
phrases from the type of corpora we used in order to
get a better understanding of the types of knowledge
required to improve automatic acquisition of these
units.
4Indeed, measuring the precision on the union yields a poor
performance of 23.96, but with the highest achievable value of
50.56 for recall. Similarly, the maximum value for precision
with a good recall can be obtained by taking the intersection of
the results of TERPpara and GIZA++, which yields a value of
60.39.
Our future work also includes the acquisition of
paraphrase patterns (e.g. (Zhao et al., 2008)) to gen-
eralize the acquired equivalence units to more con-
texts, which could be both used in applications and
to attempt improving further paraphrase acquisition
techniques. Integrating the use of patterns within an
edit rate computation technique will however raise
new difficulties.
We are finally also in the process of conducting
a careful study of the characteristics of the para-
phrase pairs that each technique can extract with
high confidence, so that we can improve our hybri-
dation experiments by considering confidence val-
ues at the paraphrase level using Machine Learning.
This way, we may be able to use an edit rate com-
putation algorithm such as TER-PLUS as a more
efficient system combiner for paraphrase extraction
methods than what was proposed here. A poten-
tial application of this would be an alternative pro-
posal to the paraphrase evaluation metric PARAMET-
RIC (Callison-Burch et al., 2008), where individual
techniques, outputing word alignments or not, could
be evaluated from the ability of the informated edit
rate technique to use correct equivalence units.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999890333333333">
This work was partly funded by a grant from LIMSI.
The authors wish to thank the anonymous reviewers
for their useful comments and suggestions.
</bodyText>
<page confidence="0.998549">
399
</page>
<sectionHeader confidence="0.996404" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99995325">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, Ann Arbor, USA.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of ACL, Toulouse, France.
Houda Bouamor, Aur´elien Max, and Anne Vilnat. 2010.
Comparison of Paraphrase Acquisition Techniques on
Sentential Paraphrases. In Proceedings of IceTAL, Re-
jkavik, Iceland.
Chris Callison-Burch, Trevor Cohn, and Mirella Lapata.
2008. Parametric: An automatic evaluation metric for
paraphrasing. In Proceedings of COLING, Manch-
ester, UK.
Marie Candito, Benoit Crabb´e, and Pascal Denis. 2010.
Statistical French dependency parsing: treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4).
Louise Del´eger and Pierre Zweigenbaum. 2009. Extract-
ing lay paraphrases of specialized expressions from
monolingual comparable medical corpora. In Pro-
ceedings of the 2nd Workshop on Building and Using
Comparable Corpora: from Parallel to Non-parallel
Corpora, Singapore.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of Coling 2004, pages 350–356, Geneva,
Switzerland.
Ulrich Germann. 2008. Yawat : Yet Another Word
Alignment Tool. In Proceedings of the ACL-08: HLT
Demo Session, Columbus, USA.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of ACL, pages 341–348, College Park, USA.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of Data-
Driven Methods. Computational Linguistics, 36(3).
Aur´elien Max and Guillaume Wisniewski. 2010. Min-
ing Naturally-occurring Corrections and Paraphrases
from Wikipedia’s Revision History. In Proceedings of
LREC, Valletta, Malta.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4).
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignement of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL-HLT, Edmonton, Canada.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL-
HLT, Rochester, USA.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3).
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase Pat-
terns from Bilingual Corpora. In Proceedings ofACL-
HLT, Columbus, USA.
</reference>
<page confidence="0.997283">
400
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.759543">
<title confidence="0.9980625">Monolingual Alignment by Edit Rate on Sentential Paraphrase Pairs</title>
<author confidence="0.927655">Houda Bouamor Aur´elien Max Anne Vilnat</author>
<affiliation confidence="0.985103">Univ. Paris</affiliation>
<address confidence="0.845006">Orsay,</address>
<abstract confidence="0.998034230769231">In this paper, we present a novel way of tackling the monolingual alignment problem on pairs of sentential paraphrases by means of edit rate computation. In order to inform the edit rate, information in the form of subsentential paraphrases is provided by a range of techniques built for different purposes. We show that the tunable TER-PLUS metric from Machine Translation evaluation can achieve good performance on this task and that it can effectively exploit information coming from complementary sources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with Bilingual Parallel Corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="1133" citStr="Bannard and Callison-Burch, 2005" startWordPosition="163" endWordPosition="166">oses. We show that the tunable TER-PLUS metric from Machine Translation evaluation can achieve good performance on this task and that it can effectively exploit information coming from complementary sources. 1 Introduction The acquisition of subsentential paraphrases has attracted a lot of attention recently (Madnani and Dorr, 2010). Techniques are usually developed for extracting paraphrase candidates from specific types of corpora, including monolingual parallel corpora (Barzilay and McKeown, 2001), monolingual comparable corpora (Del´eger and Zweigenbaum, 2009), bilingual parallel corpora (Bannard and Callison-Burch, 2005), and edit histories of multi-authored text (Max and Wisniewski, 2010). These approaches face two main issues, which correspond to the typical measures of precision, or how appropriate the extracted paraphrases are, and of recall, or how many of the paraphrases present in a given corpus can be found effectively. To start with, both measures are often hard to compute in practice, as 1) the definition of what makes an acceptable paraphrase pair is still a research question, and 2) it is often impractical to extract a complete set of acceptable paraphrases from most resources. Second, as regards </context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. In Proceedings of ACL, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="1005" citStr="Barzilay and McKeown, 2001" startWordPosition="147" endWordPosition="151">t rate, information in the form of subsentential paraphrases is provided by a range of techniques built for different purposes. We show that the tunable TER-PLUS metric from Machine Translation evaluation can achieve good performance on this task and that it can effectively exploit information coming from complementary sources. 1 Introduction The acquisition of subsentential paraphrases has attracted a lot of attention recently (Madnani and Dorr, 2010). Techniques are usually developed for extracting paraphrase candidates from specific types of corpora, including monolingual parallel corpora (Barzilay and McKeown, 2001), monolingual comparable corpora (Del´eger and Zweigenbaum, 2009), bilingual parallel corpora (Bannard and Callison-Burch, 2005), and edit histories of multi-authored text (Max and Wisniewski, 2010). These approaches face two main issues, which correspond to the typical measures of precision, or how appropriate the extracted paraphrases are, and of recall, or how many of the paraphrases present in a given corpus can be found effectively. To start with, both measures are often hard to compute in practice, as 1) the definition of what makes an acceptable paraphrase pair is still a research quest</context>
<context position="2665" citStr="Barzilay and McKeown, 2001" startWordPosition="409" endWordPosition="412">m the same time frame (Dolan et al., 2004) often contain unrelated segments that should not be aligned to form a subsentential paraphrase pair. Using bilingual corpora to acquire paraphrases indirectly by pivoting through other languages is faced, in particular, with the issue of phrase polysemy, both in the source and in the pivot languages. It has previously been noted that highly parallel monolingual corpora, typically obtained via multiple translation into the same language, constitute the most appropriate type of corpus for extracting high quality paraphrases, in spite of their rareness (Barzilay and McKeown, 2001; Cohn et al., 2008; Bouamor et al., 2010). We build on this claim here to propose an original approach for the task of subsentential alignment based on the computation of a minimum edit rate between two sentential paraphrases. More precisely, we concentrate on the alignment of atomic paraphrase pairs (Cohn et al., 2008), where the words from both paraphrases are aligned as a whole to the words of the other paraphrase, as opposed to composite paraphrase pairs obtained by joining together adjacent paraphrase pairs or possibly adding unaligned words. Figure 1 provides examples of atomic paraphra</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of ACL, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Houda Bouamor</author>
<author>Aur´elien Max</author>
<author>Anne Vilnat</author>
</authors>
<title>Comparison of Paraphrase Acquisition Techniques on Sentential Paraphrases.</title>
<date>2010</date>
<booktitle>In Proceedings of IceTAL,</booktitle>
<location>Rejkavik, Iceland.</location>
<contexts>
<context position="2707" citStr="Bouamor et al., 2010" startWordPosition="417" endWordPosition="420">n contain unrelated segments that should not be aligned to form a subsentential paraphrase pair. Using bilingual corpora to acquire paraphrases indirectly by pivoting through other languages is faced, in particular, with the issue of phrase polysemy, both in the source and in the pivot languages. It has previously been noted that highly parallel monolingual corpora, typically obtained via multiple translation into the same language, constitute the most appropriate type of corpus for extracting high quality paraphrases, in spite of their rareness (Barzilay and McKeown, 2001; Cohn et al., 2008; Bouamor et al., 2010). We build on this claim here to propose an original approach for the task of subsentential alignment based on the computation of a minimum edit rate between two sentential paraphrases. More precisely, we concentrate on the alignment of atomic paraphrase pairs (Cohn et al., 2008), where the words from both paraphrases are aligned as a whole to the words of the other paraphrase, as opposed to composite paraphrase pairs obtained by joining together adjacent paraphrase pairs or possibly adding unaligned words. Figure 1 provides examples of atomic paraphrase pairs derived from a word alignment bet</context>
<context position="6927" citStr="Bouamor et al., 2010" startWordPosition="1089" endWordPosition="1092">ntential paraphrases are, the more atomic paraphrase pairs will be reliably found, and the easier it will be for TER-PLUS to correctly identify the remaining pairs. But in the general case, and considering less apparently parallel sentence pairs, its work can be facilitated by the incorporation of candidate paraphrase pairs in its paraphrase table. We consider this possible type of hybridation in the next section. 3 Informing edit rate computation with other techniques In this article, we use three baseline techniques for paraphrase pair acquisition, which we will only briefly introduce (see (Bouamor et al., 2010) for more details). As explained previously, we want to evaluate whether and how their candidate paraphrase pairs can be used to improve paraphrase acquisition on sentential paraphrases using TER-PLUS. We selected these three techniques for the complementarity of types of information that they use: statistical word alignment without a priori linguistic knowledge, symbolic expression of linguistic variation exploiting a priori linguistic knowledge, and syntactic similarity. China will carry on open financial policy continuer-carry on financial opening ups--open financial 396 Statistical Word Al</context>
</contexts>
<marker>Bouamor, Max, Vilnat, 2010</marker>
<rawString>Houda Bouamor, Aur´elien Max, and Anne Vilnat. 2010. Comparison of Paraphrase Acquisition Techniques on Sentential Paraphrases. In Proceedings of IceTAL, Rejkavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Parametric: An automatic evaluation metric for paraphrasing.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Manchester, UK.</location>
<marker>Callison-Burch, Cohn, Lapata, 2008</marker>
<rawString>Chris Callison-Burch, Trevor Cohn, and Mirella Lapata. 2008. Parametric: An automatic evaluation metric for paraphrasing. In Proceedings of COLING, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Benoit Crabb´e</author>
<author>Pascal Denis</author>
</authors>
<title>Statistical French dependency parsing: treebank conversion and first results.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Valletta,</location>
<marker>Candito, Crabb´e, Denis, 2010</marker>
<rawString>Marie Candito, Benoit Crabb´e, and Pascal Denis. 2010. Statistical French dependency parsing: treebank conversion and first results. In Proceedings of LREC, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Chris Callison-Burch</author>
<author>Mirella Lapata</author>
</authors>
<title>Constructing corpora for the development and evaluation of paraphrase systems.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="2684" citStr="Cohn et al., 2008" startWordPosition="413" endWordPosition="416"> et al., 2004) often contain unrelated segments that should not be aligned to form a subsentential paraphrase pair. Using bilingual corpora to acquire paraphrases indirectly by pivoting through other languages is faced, in particular, with the issue of phrase polysemy, both in the source and in the pivot languages. It has previously been noted that highly parallel monolingual corpora, typically obtained via multiple translation into the same language, constitute the most appropriate type of corpus for extracting high quality paraphrases, in spite of their rareness (Barzilay and McKeown, 2001; Cohn et al., 2008; Bouamor et al., 2010). We build on this claim here to propose an original approach for the task of subsentential alignment based on the computation of a minimum edit rate between two sentential paraphrases. More precisely, we concentrate on the alignment of atomic paraphrase pairs (Cohn et al., 2008), where the words from both paraphrases are aligned as a whole to the words of the other paraphrase, as opposed to composite paraphrase pairs obtained by joining together adjacent paraphrase pairs or possibly adding unaligned words. Figure 1 provides examples of atomic paraphrase pairs derived fr</context>
<context position="9955" citStr="Cohn et al. (2008)" startWordPosition="1554" endWordPosition="1557">ture. A lexical blocking mechanism prevents sentence constituents from fusionning when there is evidence of the presence of a word in another constituent of one of the sentence. We use the Berkeley Probabilistic parser (Petrov and Klein, 2007) to obtain syntactic trees for English and its Bonsai adaptation for French (Candito et al., 2010). Because this process is highly sensitive to syntactic parse errors, we use k-best parses (with k = 3 in our experiments) and retain the most compact fusion from any pair of candidate parses. 4 Experiments and discussion We used the methodology described by Cohn et al. (2008) for constructing evaluation corpora and assessing the performance of various techniques on the task of paraphrase acquisition. In a nutshell, pairs of sentential paraphrases are hand-aligned and define a set of reference atomic paraphrase pairs at the level of words or blocks or words, denoted as Ratom, and also a set of reference composite paraphrase pairs obtained by joining adjacent atomic paraphrase pairs (up to a given length), denoted as R. Techniques output word alignments from which atomic candidate paraphrase pairs, denoted as Hatom, as well as composite paraphrase pairs, denoted as </context>
<context position="11235" citStr="Cohn et al., 2008" startWordPosition="1771" endWordPosition="1774">nd f-measure can then be defined in the following way: |Hatom ∩ R ||H ∩ Ratom |2pr p = |Hatom |r = |Ratom |f1 = p + r To evaluate our individual techniques and their use by the tunable TER-PLUS technique (henceforth TERP), we measured results on two different corpora in French and English. In each case, a heldout development corpus of 150 paraphrase pairs was used for tuning the TERP hybrid systems towards precision (→ p), recall (→ r), or F-measure (→ f1).1 All techniques were evaluated on the same test set consisting of 375 paraphrase pairs. For English, we used the MTC corpus described in (Cohn et al., 2008), which consists of multiply-translated Chinese sentences into English, with an average lexical overlap2 of 65.91% (all tokens) and 63.95% (content words only). We used as our reference set both the alignments marked as “Sure” and “Possible”. For French, we used the CESTA corpus of news articles3 obtained by translating into French from various languages with an average lexical overlap of 79.63% (all tokens) and 78.19% (content words only). These climbing was used for tuning as in (Snover et al., 2010), with uniform weights and 100 random restarts. compute the percentage of lexical overlap bet</context>
<context position="13430" citStr="Cohn et al. (2008)" startWordPosition="2155" endWordPosition="2158">.33 6.15 18.47 17.59 18.58 17.96 21.02 Figure 2: Results on the test set on French and English for the individual techniques and TERP hybrid systems. Column headers of the form “→ c” indicate that TERP was tuned on criterion c. figures reveal that the French corpus tends to contain more literal translations, possibly due to the original languages of the sentences, which are closer to the target language than Chinese is to English. We used the YAWAT (Germann, 2008) interactive alignment tool and measure inter-annotator agreement over a subset and found it to be similar to the value reported by Cohn et al. (2008) for English. Results for all individual techniques in the two languages are given on Figure 2. We first note that all techniques fared better on the French corpus than on the English corpus. This can certainly be explained by the fact that the former results from more literal translations, which are consequently easier to word-align. TERMT (i.e. TER tuned for Machine Translation evaluation) performs significantly worse on all metrics for both languages than our tuned TERP experiments, revealing that the two tasks have different objectives. The two linguistically-aware techniques, FASTR and PA</context>
</contexts>
<marker>Cohn, Callison-Burch, Lapata, 2008</marker>
<rawString>Trevor Cohn, Chris Callison-Burch, and Mirella Lapata. 2008. Constructing corpora for the development and evaluation of paraphrase systems. Computational Linguistics, 34(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louise Del´eger</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Extracting lay paraphrases of specialized expressions from monolingual comparable medical corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2nd Workshop on Building and Using Comparable Corpora: from Parallel to Non-parallel Corpora,</booktitle>
<marker>Del´eger, Zweigenbaum, 2009</marker>
<rawString>Louise Del´eger and Pierre Zweigenbaum. 2009. Extracting lay paraphrases of specialized expressions from monolingual comparable medical corpora. In Proceedings of the 2nd Workshop on Building and Using Comparable Corpora: from Parallel to Non-parallel Corpora, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>350--356</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2081" citStr="Dolan et al., 2004" startWordPosition="318" endWordPosition="321">both measures are often hard to compute in practice, as 1) the definition of what makes an acceptable paraphrase pair is still a research question, and 2) it is often impractical to extract a complete set of acceptable paraphrases from most resources. Second, as regards the precision of paraphrase acquisition techniques in particular, it is notable that most works on paraphrase acquisition are not based on direct observation of larger paraphrase pairs. Even monolingual corpora obtained by pairing very closely related texts such as news headlines on the same topic and from the same time frame (Dolan et al., 2004) often contain unrelated segments that should not be aligned to form a subsentential paraphrase pair. Using bilingual corpora to acquire paraphrases indirectly by pivoting through other languages is faced, in particular, with the issue of phrase polysemy, both in the source and in the pivot languages. It has previously been noted that highly parallel monolingual corpora, typically obtained via multiple translation into the same language, constitute the most appropriate type of corpus for extracting high quality paraphrases, in spite of their rareness (Barzilay and McKeown, 2001; Cohn et al., 2</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of Coling 2004, pages 350–356, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
</authors>
<title>Yawat : Yet Another Word Alignment Tool.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08: HLT Demo Session,</booktitle>
<location>Columbus, USA.</location>
<contexts>
<context position="13280" citStr="Germann, 2008" startWordPosition="2132" endWordPosition="2133">9 11.92 18.47 17.10 6.94 21.02 20.28 3.41 18.94 16.44 13.57 19.30 16.35 f1 16.25 9.30 4.21 18.77 4.31 19.26 19.52 16.52 19.14 18.95 11.91 20.92 21.33 6.15 18.47 17.59 18.58 17.96 21.02 Figure 2: Results on the test set on French and English for the individual techniques and TERP hybrid systems. Column headers of the form “→ c” indicate that TERP was tuned on criterion c. figures reveal that the French corpus tends to contain more literal translations, possibly due to the original languages of the sentences, which are closer to the target language than Chinese is to English. We used the YAWAT (Germann, 2008) interactive alignment tool and measure inter-annotator agreement over a subset and found it to be similar to the value reported by Cohn et al. (2008) for English. Results for all individual techniques in the two languages are given on Figure 2. We first note that all techniques fared better on the French corpus than on the English corpus. This can certainly be explained by the fact that the former results from more literal translations, which are consequently easier to word-align. TERMT (i.e. TER tuned for Machine Translation evaluation) performs significantly worse on all metrics for both la</context>
</contexts>
<marker>Germann, 2008</marker>
<rawString>Ulrich Germann. 2008. Yawat : Yet Another Word Alignment Tool. In Proceedings of the ACL-08: HLT Demo Session, Columbus, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Jacquemin</author>
</authors>
<title>Syntagmatic and paradigmatic representations of term variation.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>341--348</pages>
<location>College Park, USA.</location>
<contexts>
<context position="8452" citStr="Jacquemin, 1999" startWordPosition="1310" endWordPosition="1311"> models it is necessary to use enough training material including minimal redundancy of words. To this end, we will be using monolingual corpora made up of multiply-translated sentences, allowing us to provide GIZA++ with all possible sentence pairs to improve the quality of its word alignments (note that following common practice we used symetrized alignments from the alignments in both directions). This constitutes an advantage for this technique that the following techniques working on each sentence pair independently do not have. Symbolic expression of linguistic variation The FASTR tool (Jacquemin, 1999) was designed to spot term variants in large corpora. Variants are described through metarules expressing how the morphosyntactic structure of a term variant can be derived from a given term by means of regular expressions on word categories. Paradigmatic variation can also be expressed by defining constraints between words to force them to belong to the same morphological or semantic family, both constraints relying on preexisting repertoires available for English and French. To compute candidate paraphrase pairs using FASTR, we first consider all the phrases from the first sentence and searc</context>
</contexts>
<marker>Jacquemin, 1999</marker>
<rawString>Christian Jacquemin. 1999. Syntagmatic and paradigmatic representations of term variation. In Proceedings of ACL, pages 341–348, College Park, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Generating Phrasal and Sentential Paraphrases: A Survey of DataDriven Methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="834" citStr="Madnani and Dorr, 2010" startWordPosition="124" endWordPosition="127">, we present a novel way of tackling the monolingual alignment problem on pairs of sentential paraphrases by means of edit rate computation. In order to inform the edit rate, information in the form of subsentential paraphrases is provided by a range of techniques built for different purposes. We show that the tunable TER-PLUS metric from Machine Translation evaluation can achieve good performance on this task and that it can effectively exploit information coming from complementary sources. 1 Introduction The acquisition of subsentential paraphrases has attracted a lot of attention recently (Madnani and Dorr, 2010). Techniques are usually developed for extracting paraphrase candidates from specific types of corpora, including monolingual parallel corpora (Barzilay and McKeown, 2001), monolingual comparable corpora (Del´eger and Zweigenbaum, 2009), bilingual parallel corpora (Bannard and Callison-Burch, 2005), and edit histories of multi-authored text (Max and Wisniewski, 2010). These approaches face two main issues, which correspond to the typical measures of precision, or how appropriate the extracted paraphrases are, and of recall, or how many of the paraphrases present in a given corpus can be found </context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie J. Dorr. 2010. Generating Phrasal and Sentential Paraphrases: A Survey of DataDriven Methods. Computational Linguistics, 36(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aur´elien Max</author>
<author>Guillaume Wisniewski</author>
</authors>
<title>Mining Naturally-occurring Corrections and Paraphrases from Wikipedia’s Revision History.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Valletta,</location>
<contexts>
<context position="1203" citStr="Max and Wisniewski, 2010" startWordPosition="173" endWordPosition="176">tion can achieve good performance on this task and that it can effectively exploit information coming from complementary sources. 1 Introduction The acquisition of subsentential paraphrases has attracted a lot of attention recently (Madnani and Dorr, 2010). Techniques are usually developed for extracting paraphrase candidates from specific types of corpora, including monolingual parallel corpora (Barzilay and McKeown, 2001), monolingual comparable corpora (Del´eger and Zweigenbaum, 2009), bilingual parallel corpora (Bannard and Callison-Burch, 2005), and edit histories of multi-authored text (Max and Wisniewski, 2010). These approaches face two main issues, which correspond to the typical measures of precision, or how appropriate the extracted paraphrases are, and of recall, or how many of the paraphrases present in a given corpus can be found effectively. To start with, both measures are often hard to compute in practice, as 1) the definition of what makes an acceptable paraphrase pair is still a research question, and 2) it is often impractical to extract a complete set of acceptable paraphrases from most resources. Second, as regards the precision of paraphrase acquisition techniques in particular, it i</context>
</contexts>
<marker>Max, Wisniewski, 2010</marker>
<rawString>Aur´elien Max and Guillaume Wisniewski. 2010. Mining Naturally-occurring Corrections and Paraphrases from Wikipedia’s Revision History. In Proceedings of LREC, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Herman Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="7570" citStr="Och and Ney, 2004" startWordPosition="1182" endWordPosition="1185">plained previously, we want to evaluate whether and how their candidate paraphrase pairs can be used to improve paraphrase acquisition on sentential paraphrases using TER-PLUS. We selected these three techniques for the complementarity of types of information that they use: statistical word alignment without a priori linguistic knowledge, symbolic expression of linguistic variation exploiting a priori linguistic knowledge, and syntactic similarity. China will carry on open financial policy continuer-carry on financial opening ups--open financial 396 Statistical Word Alignment The GIZA++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. While originally developped in the bilingual context of Machine Translation, nothing prevents building such models on monolingual corpora. However, in order to build reliable models it is necessary to use enough training material including minimal redundancy of words. To this end, we will be using monolingual corpora made up of multiply-translated sentences, allowing us to provide GIZA++ with all possible sentence pairs to improve the quality of its word alignments (note that following common practice w</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Herman Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based alignement of multiple translations: Extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="9223" citStr="Pang et al. (2003)" startWordPosition="1433" endWordPosition="1436">iant can be derived from a given term by means of regular expressions on word categories. Paradigmatic variation can also be expressed by defining constraints between words to force them to belong to the same morphological or semantic family, both constraints relying on preexisting repertoires available for English and French. To compute candidate paraphrase pairs using FASTR, we first consider all the phrases from the first sentence and search for variants in the other sentence, do the reverse process and take the intersection of the two sets. Syntactic similarity The algorithm introduced by Pang et al. (2003) takes two sentences as input and merges them by top-down syntactic fusion guided by compatible syntactic substructure. A lexical blocking mechanism prevents sentence constituents from fusionning when there is evidence of the presence of a word in another constituent of one of the sentence. We use the Berkeley Probabilistic parser (Petrov and Klein, 2007) to obtain syntactic trees for English and its Bonsai adaptation for French (Candito et al., 2010). Because this process is highly sensitive to syntactic parse errors, we use k-best parses (with k = 3 in our experiments) and retain the most co</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based alignement of multiple translations: Extracting paraphrases and generating new sentences. In Proceedings of NAACL-HLT, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACLHLT,</booktitle>
<location>Rochester, USA.</location>
<contexts>
<context position="9580" citStr="Petrov and Klein, 2007" startWordPosition="1489" endWordPosition="1492">raphrase pairs using FASTR, we first consider all the phrases from the first sentence and search for variants in the other sentence, do the reverse process and take the intersection of the two sets. Syntactic similarity The algorithm introduced by Pang et al. (2003) takes two sentences as input and merges them by top-down syntactic fusion guided by compatible syntactic substructure. A lexical blocking mechanism prevents sentence constituents from fusionning when there is evidence of the presence of a word in another constituent of one of the sentence. We use the Berkeley Probabilistic parser (Petrov and Klein, 2007) to obtain syntactic trees for English and its Bonsai adaptation for French (Candito et al., 2010). Because this process is highly sensitive to syntactic parse errors, we use k-best parses (with k = 3 in our experiments) and retain the most compact fusion from any pair of candidate parses. 4 Experiments and discussion We used the methodology described by Cohn et al. (2008) for constructing evaluation corpora and assessing the performance of various techniques on the task of paraphrase acquisition. In a nutshell, pairs of sentential paraphrases are hand-aligned and define a set of reference ato</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of NAACLHLT, Rochester, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>TER-Plus: paraphrase, semantic, and alignment enhancements to Translation Edit Rate.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="4389" citStr="Snover et al., 2010" startWordPosition="681" endWordPosition="684">ote that identity pairs (e.g. China H China) will never be considered in this work and will not be taken into account for evaluation. The remainder of this paper is organized as follows. We first briefly describe in section 2 how we apply edit rate computation to the task of atomic paraphrase alignment, and we explain in section 3 how we can inform such a technique with paraphrase candidates extracted by additional techniques. We present our experiments and discuss their results in section 4 and conclude in section 5. 2 Edit rate for paraphrase alignment TER-PLUS (Translation Edit Rate Plus) (Snover et al., 2010) is a score designed for evaluation of Machine Translation (MT) output. Its typical use takes a system hypothesis to compute an optimal set of word edits that can transform it into some existing reference translation. Edit types include exact word matching, word insertion and deletion, block movement of contiguous words (computed as an approximation), as well as variants substitution through stemming, synonym or paraphrase matching. Each edit type is parameterized by at least one weight which can be optimized using e.g. hill climbing. TER-PLUS is therefore a tunable metric. We will henceforth </context>
<context position="11742" citStr="Snover et al., 2010" startWordPosition="1853" endWordPosition="1856">test set consisting of 375 paraphrase pairs. For English, we used the MTC corpus described in (Cohn et al., 2008), which consists of multiply-translated Chinese sentences into English, with an average lexical overlap2 of 65.91% (all tokens) and 63.95% (content words only). We used as our reference set both the alignments marked as “Sure” and “Possible”. For French, we used the CESTA corpus of news articles3 obtained by translating into French from various languages with an average lexical overlap of 79.63% (all tokens) and 78.19% (content words only). These climbing was used for tuning as in (Snover et al., 2010), with uniform weights and 100 random restarts. compute the percentage of lexical overlap between the vocabularies of two sentences and S2 as 1Hill 2We S1 :|S1∩S2|/min(|S1|,|S2|) 3http://www.elda.org/article125.html 397 Individual techniques Hybrid systems (TERPpara+X) Giza++ Fastr Pang TMT → p TERPpara → f1 → p +G → f1 → p +F → f1 → p +P → f1 +G + F + P G F P → r → r → r → r → p → r → f1 French French p 28.99 52.48 62.50 25.66 31.35 30.26 31.43 41.99 30.55 41.14 36.74 29.65 34.84 54.49 20.94 33.89 42.27 27.06 42.80 r 45.98 8.59 8.65 41.15 44.22 44.60 44.10 35.88 45.67 35.25 40.96 43.85 44.41 </context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2010</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and Richard Schwartz. 2010. TER-Plus: paraphrase, semantic, and alignment enhancements to Translation Edit Rate. Machine Translation, 23(2-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora.</title>
<date>2008</date>
<booktitle>In Proceedings ofACLHLT,</booktitle>
<location>Columbus, USA.</location>
<contexts>
<context position="18091" citStr="Zhao et al., 2008" startWordPosition="2912" endWordPosition="2915">ly studying hard-to-align subsentential paraphrases from the type of corpora we used in order to get a better understanding of the types of knowledge required to improve automatic acquisition of these units. 4Indeed, measuring the precision on the union yields a poor performance of 23.96, but with the highest achievable value of 50.56 for recall. Similarly, the maximum value for precision with a good recall can be obtained by taking the intersection of the results of TERPpara and GIZA++, which yields a value of 60.39. Our future work also includes the acquisition of paraphrase patterns (e.g. (Zhao et al., 2008)) to generalize the acquired equivalence units to more contexts, which could be both used in applications and to attempt improving further paraphrase acquisition techniques. Integrating the use of patterns within an edit rate computation technique will however raise new difficulties. We are finally also in the process of conducting a careful study of the characteristics of the paraphrase pairs that each technique can extract with high confidence, so that we can improve our hybridation experiments by considering confidence values at the paraphrase level using Machine Learning. This way, we may </context>
</contexts>
<marker>Zhao, Wang, Liu, Li, 2008</marker>
<rawString>Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li. 2008. Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora. In Proceedings ofACLHLT, Columbus, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>