<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000139">
<title confidence="0.995091">
Understanding the Semantic Structure of Noun Phrase Queries
</title>
<author confidence="0.989178">
Xiao Li
</author>
<affiliation confidence="0.959021">
Microsoft Research
</affiliation>
<address confidence="0.934254">
One Microsoft Way
Redmond, WA 98052 USA
</address>
<email confidence="0.998394">
xiaol@microsoft.com
</email>
<sectionHeader confidence="0.994774" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902733333333">
Determining the semantic intent of web
queries not only involves identifying their
semantic class, which is a primary focus
of previous works, but also understanding
their semantic structure. In this work, we
formally define the semantic structure of
noun phrase queries as comprised of intent
heads and intent modifiers. We present
methods that automatically identify these
constituents as well as their semantic roles
based on Markov and semi-Markov con-
ditional random fields. We show that the
use of semantic features and syntactic fea-
tures significantly contribute to improving
the understanding performance.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998172892857143">
Web queries can be considered as implicit ques-
tions or commands, in that they are performed ei-
ther to find information on the web or to initiate
interaction with web services. Web users, how-
ever, rarely express their intent in full language.
For example, to find out “what are the movies of
2010 in which johnny depp stars”, a user may sim-
ply query “johnny depp movies 2010”. Today’s
search engines, generally speaking, are based on
matching such keywords against web documents
and ranking relevant results using sophisticated
features and algorithms.
As search engine technologies evolve, it is in-
creasingly believed that search will be shifting
away from “ten blue links” toward understanding
intent and serving objects. This trend has been
largely driven by an increasing amount of struc-
tured and semi-structured data made available to
search engines, such as relational databases and
semantically annotated web documents. Search-
ing over such data sources, in many cases, can
offer more relevant and essential results com-
pared with merely returning web pages that con-
tain query keywords. Table 1 shows a simplified
view of a structured data source, where each row
represents a movie object. Consider the query
“johnny depp movies 2010”. It is possible to re-
trieve a set of movie objects from Table 1 that
satisfy the constraints Year = 2010 and Cast E)
Johnny Depp. This would deliver direct answers to
the query rather than having the user sort through
list of keyword results.
In no small part, the success of such an ap-
proach relies on robust understanding of query in-
tent. Most previous works in this area focus on
query intent classification (Shen et al., 2006; Li
et al., 2008b; Arguello et al., 2009). Indeed, the
intent class information is crucial in determining
if a query can be answered by any structured data
sources and, if so, by which one. In this work, we
go one step further and study the semantic struc-
ture of a query, i.e., individual constituents of a
query and their semantic roles. In particular, we
focus on noun phrase queries. A key contribution
of this work is that we formally define query se-
mantic structure as comprised of intent heads (IH)
and intent modifiers (IM), e.g.,
[IM:Title alice in wonderland] [IM:Year 2010] [IH cast]
It is determined that “cast” is an IH of the above
query, representing the essential information the
user intends to obtain. Furthermore, there are two
IMs, “alice in wonderland” and “2010”, serving as
filters of the information the user receives.
Identifying the semantic structure of queries can
be beneficial to information retrieval. Knowing
the semantic role of each query constituent, we
</bodyText>
<page confidence="0.947946">
1337
</page>
<note confidence="0.970042375">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1337–1345,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
Title Year Genre Director Cast Review
Precious 2009 Drama Lee Daniels Gabby Sidibe, Mo’Nique,.. .
2012 2009 Action, Sci Fi Roland Emmerich John Cusack, Chiwetel Ejiofor,.. .
Avatar 2009 Action, Sci Fi James Cameron Sam Worthington, Zoe Saldana,.. .
The Rum Diary 2010 Adventure, Drama Bruce Robinson Johnny Depp,Giovanni Ribisi,.. .
Alice in Wonderland 2010 Adventure, Family Tim Burton Mia Wasikowska, Johnny Depp,.. .
</note>
<tableCaption confidence="0.999101">
Table 1: A simplified view of a structured data source for the Movie domain.
</tableCaption>
<bodyText confidence="0.999956461538462">
can reformulate the query into a structured form
or reweight different query constituents for struc-
tured data retrieval (Robertson et al., 2004; Kim
et al., 2009; Paparizos et al., 2009). Alternatively,
the knowledge of IHs, IMs and semantic labels of
IMs may be used as additional evidence in a learn-
ing to rank framework (Burges et al., 2005).
A second contribution of this work is to present
methods that automatically extract the semantic
structure of noun phrase queries, i.e., IHs, IMs
and the semantic labels of IMs. In particular, we
investigate the use of transition, lexical, semantic
and syntactic features. The semantic features can
be constructed from structured data sources or by
mining query logs, while the syntactic features can
be obtained by readily-available syntactic analy-
sis tools. We compare the roles of these features
in two discriminative models, Markov and semi-
Markov conditional random fields. The second
model is especially interesting to us since in our
task it is beneficial to use features that measure
segment-level characteristics. Finally, we evaluate
our proposed models and features on manually-
annotated query sets from three domains, while
our techniques are general enough to be applied
to many other domains.
</bodyText>
<sectionHeader confidence="0.999828" genericHeader="introduction">
2 Related Works
</sectionHeader>
<subsectionHeader confidence="0.993946">
2.1 Query intent understanding
</subsectionHeader>
<bodyText confidence="0.999978114285714">
As mentioned in the introduction, previous works
on query intent understanding have largely fo-
cused on classification, i.e., automatically map-
ping queries into semantic classes (Shen et al.,
2006; Li et al., 2008b; Arguello et al., 2009).
There are relatively few published works on un-
derstanding the semantic structure of web queries.
The most relevant ones are on the problem of
query tagging, i.e., assigning semantic labels to
query terms (Li et al., 2009; Manshadi and Li,
2009). For example, in “canon powershot sd850
camera silver”, the word “canon” should be tagged
as Brand. In particular, Li et al. leveraged click-
through data and a database to automatically de-
rive training data for learning a CRF-based tagger.
Manshadi and Li developed a hybrid, generative
grammar model for a similar task. Both works are
closely related to one aspect of our work, which
is to assign semantic labels to IMs. A key differ-
ence is that they do not conceptually distinguish
between IHs and IMs.
On the other hand, there have been a series of
research studies related to IH identification (Pasca
and Durme, 2007; Pasca and Durme, 2008). Their
methods aim at extracting attribute names, such
as cost and side effect for the concept Drug, from
documents and query logs in a weakly-supervised
learning framework. When used in the context
of web queries, attribute names usually serve as
IHs. In fact, one immediate application of their
research is to understand web queries that request
factual information of some concepts, e.g. “asiprin
cost” and “aspirin side effect”. Their framework,
however, does not consider the identification and
categorization of IMs (attribute values).
</bodyText>
<subsectionHeader confidence="0.999839">
2.2 Question answering
</subsectionHeader>
<bodyText confidence="0.999980571428572">
Query intent understanding is analogous to ques-
tion understanding for question answering (QA)
systems. Many web queries can be viewed as the
keyword-based counterparts of natural language
questions. For example, the query “california na-
tional” and “national parks califorina” both imply
the question “What are the national parks in Cali-
fornia?”. In particular, a number of works investi-
gated the importance of head noun extraction in
understanding what-type questions (Metzler and
Croft, 2005; Li et al., 2008a). To extract head
nouns, they applied syntax-based rules using the
information obtained from part-of-speech (POS)
tagging and deep parsing. As questions posed
in natural language tend to have strong syntactic
structures, such an approach was demonstrated to
be accurate in identifying head nouns.
In identifying IHs in noun phrase queries, how-
ever, direct syntactic analysis is unlikely to be as
effective. This is because syntactic structures are
in general less pronounced in web queries. In this
</bodyText>
<page confidence="0.988193">
1338
</page>
<bodyText confidence="0.999432333333333">
work, we propose to use POS tagging and parsing
outputs as features, in addition to other features, in
extracting the semantic structure of web queries.
</bodyText>
<subsectionHeader confidence="0.993298">
2.3 Information extraction
</subsectionHeader>
<bodyText confidence="0.999972230769231">
Finally, there exist large bodies of work on infor-
mation extraction using models based on Markov
and semi-Markov CRFs (Lafferty et al., 2001;
Sarawagi and Cohen, 2004), and in particular for
the task of named entity recognition (McCallum
and Li, 2003).
The problem studied in this work is concerned
with identifying more generic “semantic roles” of
the constituents in noun phrase queries. While
some IM categories belong to named entities such
as IM:Director for the intent class Movie, there
can be semantic labels that are not named entities
such as IH and IM:Genre (again for Movie).
</bodyText>
<sectionHeader confidence="0.98628" genericHeader="method">
3 Query Semantic Structure
</sectionHeader>
<bodyText confidence="0.9999279">
Unlike database query languages such as SQL,
web queries are usually formulated as sequences
of words without explicit structures. This makes
web queries difficult to interpret by computers.
For example, should the query “aspirin side effect”
be interpreted as “the side effect of aspirin” or “the
aspirin of side effect”? Before trying to build mod-
els that can automatically makes such decisions,
we first need to understand what constitute the se-
mantic structure of a noun phrase query.
</bodyText>
<subsectionHeader confidence="0.993638">
3.1 Definition
</subsectionHeader>
<bodyText confidence="0.9998306">
We let C denote a set of query intent classes that
represent semantic concepts such as Movie, Prod-
uct and Drug. The query constituents introduced
below are all defined w.r.t. the intent class of a
query, c E C, which is assumed to be known.
</bodyText>
<subsectionHeader confidence="0.634088">
Intent head
</subsectionHeader>
<bodyText confidence="0.977903441176471">
An intent head (IH) is a query segment that cor-
responds to an attribute name of an intent class.
For example, the IH of the query “alice in won-
derland 2010 cast” is “cast”, which is an attribute
name of Movie. By issuing the query, the user in-
tends to find out the values of the IH (i.e., cast). A
query can have multiple IHs, e.g., “movie avatar
director and cast”. More importantly, there can
be queries without an explicit IH. For example,
“movie avatar” does not contain any segment that
corresponds to an attribute name of Movie. Such a
query, however, does have an implicit intent which
is to obtain general information about the movie.
Intent modifier
In contrast, an intent modifier (IM) is a query seg-
ment that corresponds to an attribute value (of
some attribute name). The role of IMs is to impos-
ing constraints on the attributes of an intent class.
For example, there are two constraints implied in
the query “alice in wonderland 2010 cast”: (1) the
Title of the movie is “alice in wonderland”; and
(2) the Year of the movie is “2010”. Interestingly,
the user does not explicitly specify the attribute
names, i.e., Title and Year, in this query. Such
information, however, can be inferred given do-
main knowledge. In fact, one important goal of
this work is to identify the semantic labels of IMs,
i.e., the attribute names they implicitly refer to. We
use A, to denote the set of IM semantic labels for
the intent class c.
Other
Additionally, there can be query segments that do
not play any semantic roles, which we refer to as
Other.
</bodyText>
<subsectionHeader confidence="0.999972">
3.2 Syntactic analysis
</subsectionHeader>
<bodyText confidence="0.999996964285714">
The notion of IHs and IMs in this work is closely
related to that of linguistic head nouns and modi-
fiers for noun phrases. In many cases, the IHs of
noun phrase queries are exactly the head nouns in
the linguistic sense. Exceptions mostly occur in
queries without explicit IHs, e.g., “movie avatar”
in which the head noun “avatar” serves as an IM
instead. Due to the strong resemblance, it is inter-
esting to see if IHs can be identified by extracting
linguistic head nouns from queries based on syn-
tactic analysis. To this end, we apply the follow-
ing heuristics for head noun extraction. We first
run a POS-tagger and a chunker jointly on each
query, where the POS-tagger/chunker is based on
an HMM system trained on English Penn Tree-
bank (Gao et al., 2001). We then mark the right
most NP chunk before any prepositional phrase
or adjective clause, and apply the NP head rules
(Collins, 1999) to the marked NP chunk.
The main problem with this approach, however,
is that a readily-available POS tagger or chunker is
usually trained on natural language sentences and
thus is unlikely to produce accurate results on web
queries. As shown in (Barr et al., 2008), the lexi-
cal category distribution of web queries is dramat-
ically different from that of natural languages. For
example, prepositions and subordinating conjunc-
tions, which are strong indicators of the syntactic
</bodyText>
<page confidence="0.974409">
1339
</page>
<bodyText confidence="0.999930714285714">
structure in natural languages, are often missing in
web queries. Moreover, unlike most natural lan-
guages that follow the linear-order principle, web
queries can have relatively free word orders (al-
though some orders may occur more often than
others statistically). These factors make it diffi-
cult to produce reliable syntactic analysis outputs.
Consequently, the head nouns and hence the IHs
extracted therefrom are likely to be error-prone, as
will be shown by our experiments in Section 6.3.
Although a POS tagger and a chunker may not
work well on queries, their output can be used as
features for learning statistical models for seman-
tic structure extraction, which we introduce next.
</bodyText>
<sectionHeader confidence="0.995365" genericHeader="method">
4 Models
</sectionHeader>
<bodyText confidence="0.999975222222222">
This section presents two statistical models for se-
mantic understanding of noun phrase queries. As-
suming that the intent class c ∈ C of a query is
known, we cast the problem of extracting the se-
mantic structure of the query into a joint segmen-
tation/classification problem. At a high level, we
would like to identify query segments that corre-
spond to IHs, IMs and Others. Furthermore, for
each IM segment, we would like to assign a se-
mantic label, denoted by IM:a, a ∈ Ac, indicating
which attribute name it refers to. In other words,
our label set consists of Y = {IH, {IM:a}aE��,
Other}.
Formally, we let x = (x1, x2,... , xM) denote
an input query of length M. To avoid confusion,
we use i to represent the index of a word token
and j to represent the index of a segment in the
following text. Our goal is to obtain
</bodyText>
<equation confidence="0.989783">
s* = argmax p(s|c, x) (1)
S
</equation>
<bodyText confidence="0.999961642857143">
where s = (s1, s2, ... , sN) denotes a query seg-
mentation as well as a classification of all seg-
ments. Each segment sj is represented by a tu-
ple (uj, vj, yj). Here uj and vj are the indices of
the starting and ending word tokens respectively;
yj ∈ Y is a label indicating the semantic role of
s. We further augment the segment sequence with
two special segments: Start and End, represented
by s0 and sN+1 respectively. For notional simplic-
ity, we assume that the intent class is given and
use p(s|x) as a shorthand for p(s|c, x), but keep in
mind that the label space and hence the parameter
space is class-dependent. Now we introduce two
methods of modeling p(s|x).
</bodyText>
<subsectionHeader confidence="0.989966">
4.1 CRFs
</subsectionHeader>
<bodyText confidence="0.999567666666667">
One natural approach to extracting the semantic
structure of queries is to use linear-chain CRFs
(Lafferty et al., 2001). They model the con-
ditional probability of a label sequence given
the input, where the labels, denoted as y =
(y1, y2,. .. , yM), yi ∈ Y, have a one-to-one cor-
respondence with the word tokens in the input.
Using linear-chain CRFs, we aim to find the la-
bel sequence that maximizes
</bodyText>
<equation confidence="0.78425775">
�M+11:
1
pλ(y|x) = Zλ(x) exp λ · f(yi_1, yi, x, i) .
i=1
The partition function
is a normalization
is a weight vector and
x) is
</equation>
<bodyText confidence="0.986421823529412">
a vector of feature functions referred to as a fea-
ture vector. The features used in CRFs will be de-
scribed in Section 5.
Given manually-labeled queries, we estimate
that maximizes the conditional likelihood of train-
ing data while regularizing model parameters. The
learned model is then used to predict the label se-
quence yfor future input sequences x. To obtain s
in Equation (1), we simply concatenate the maxi-
mum number of consecutive word tokens that have
the same label and treat the resulting sequence as a
segment. By doing this, we implicitly assume that
there are no two adjacent segments with the same
label in the true segment sequence. Although this
assumption is not always correct in practice, we
consider it a reasonable approximation given what
we empiri
</bodyText>
<equation confidence="0.937828125">
Zλ(x)
λ
f(yi_1,yi,
λ
cally observed in our training data.
4.2 Semi-Markov CRFs
N+1
sj, x) (3)
</equation>
<bodyText confidence="0.9339205">
In this case, the features
sj, x) are de-
fined on segments instead of on word tokens.
More precisely, they are of the function form
yj, x, uj, vj). It is easy to see that by
imposing a constraint
= vi, the model is
reduced to standard linear-chain CRFs. Semi-
Markov CRFs make Markov assumptions at the
segment level, thereby naturally offering mean
</bodyText>
<equation confidence="0.983798777777778">
1 1:
p(s|x) = Zλ(x) expλ·f(sj_1,
j=1
f(sj_1,
f(yj_1,
ui
s to
(2)
factor.
</equation>
<bodyText confidence="0.917038">
In contrast to standard CRFs, semi-Markov CRFs
directly model the segmentation of an input se-
quence as well as a classification of the segments
(Sarawagi an
d Cohen, 2004), i.e.,
</bodyText>
<page confidence="0.673657">
1340
</page>
<table confidence="0.992997272727273">
CRF features
A1: Transition S(yi−1 = a)S(yi = b) transiting from state a to b
S(xi = w)S(yi = b)
S(xi E Wc)S(yi = b)
S(xi−1:i E Wc)S(yi = b)
S(POS(xi) = z)S(yi = b)
A2: Lexical current word is w
A3: Semantic current word occurs in lexicon G
A4: Semantic current bigram occurs in lexicon G
A5: Syntactic POS tag of the current word is z
Semi-Markov CRF features
</table>
<equation confidence="0.998786571428571">
S(yj−1 = a)S(yj = b)
S(xuj:vj = w)S(yj = b)
S(xuj:vj 3 w)S(yj = b)
S(xuj:vj E G)S(yj = b)
s(xuj:vj, l)S(yj = b)
S(POS(xuj:vj) = z)S(yj = b)
S(Chunk(xuj:vj) = c)S(yj = b)
</equation>
<figure confidence="0.9527383125">
max
!Ec
Transiting from state a to b
Current segment is w
Current segment contains word w
Current segment is an element in lexicon G
The max similarity between the segment and elements in G
Current segment’s POS sequence is z
Current segment is a chunk with phrase type c
B1: Transition
B2: Lexical
B3: Lexical
B4: Semantic
B5: Semantic
B6: Syntactic
B7: Syntactic
</figure>
<tableCaption confidence="0.79715925">
Table 2: A summary of feature types in CRFs and segmental CRFs for query understanding. We assume
that the state label is b in all features and omit this in the feature descriptions.
incorporate segment-level features, as will be pre-
sented in Section 5.
</tableCaption>
<sectionHeader confidence="0.99896" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.9984984">
In this work, we explore the use of transition, lexi-
cal, semantic and syntactic features in Markov and
semi-Markov CRFs. The mathematical expression
of these features are summarized in Table 2 with
details described as follows.
</bodyText>
<subsectionHeader confidence="0.985527">
5.1 Transition features
</subsectionHeader>
<bodyText confidence="0.9998934">
Transition features, i.e., A1 and B1 in Table 2,
capture state transition patterns between adjacent
word tokens in CRFs, and between adjacent seg-
ments in semi-Markov CRFs. We only use first-
order transition features in this work.
</bodyText>
<subsectionHeader confidence="0.998043">
5.2 Lexical features
</subsectionHeader>
<bodyText confidence="0.999992428571429">
In CRFs, a lexical feature (A2) is implemented as
a binary function that indicates whether a specific
word co-occurs with a state label. The set of words
to be considered in this work are those observed
in the training data. We can also generalize this
type of features from words to n-grams. In other
words, instead of inspecting the word identity at
the current position, we inspect the n-gram iden-
tity by applying a window of length n centered at
the current position.
Since feature functions are defined on segments
in semi-Markov CRFs, we create B2 that indicates
whether the phrase in a hypothesized query seg-
ment co-occurs with a state label. Here the set of
phrase identities are extracted from the query seg-
ments in the training data. Furthermore, we create
another type of lexical feature, B3, which is acti-
vated when a specific word occurs in a hypothe-
sized query segment. The use of B3 would favor
unseen words being included in adjacent segments
rather than to be isolated as separate segments.
</bodyText>
<subsectionHeader confidence="0.991085">
5.3 Semantic features
</subsectionHeader>
<bodyText confidence="0.9999646">
Models relying on lexical features may require
very large amounts of training data to produce
accurate prediction performance, as the feature
space is in general large and sparse. To make our
model generalize better, we create semantic fea-
tures based on what we call lexicons. A lexicon,
denoted as L, is a cluster of semantically-related
words/phrases. For example, a cluster of movie
titles or director names can be such a lexicon. Be-
fore describing how such lexicons are generated
for our task, we first introduce the forms of the
semantic features assuming the availability of the
lexicons.
We let L denote a lexicon, and WL denote the
set of n-grams extracted from L. For CRFs, we
create a binary function that indicates whether any
n-gram in WL co-occurs with a state label, with
n = 1, 2 for A3, A4 respectively. For both A3
and A4, the number of such semantic features is
equal to the number of lexicons multiplied by the
number of state labels.
The same source of semantic knowledge can be
conveniently incorporated in semi-Markov CRFs.
One set of semantic features (B4) inspect whether
the phrase of a hypothesized query segment
matches any element in a given lexicon. A sec-
ond set of semantic features (B5) relax the exact
match constraints made by B4, and take as the fea-
ture value the maximum “similarity” between the
query segment and all lexicon elements. The fol-
</bodyText>
<page confidence="0.963299">
1341
</page>
<bodyText confidence="0.91878">
lowing similarity function is used in this work,
</bodyText>
<equation confidence="0.84877">
s(x,,,:,,,, l) = 1 − Lev(x,,;:,;, l)/|l |(4)
</equation>
<bodyText confidence="0.9999884">
where Lev represents the Levenshtein distance.
Notice that we normalize the Levenshtein distance
by the length of the lexicon element, as we em-
pirically found it performing better compared with
normalizing by the length of the segment. In com-
puting the maximum similarity, we first retrieve a
set of lexicon elements with a positive tf-idf co-
sine distance with the segment; we then evaluate
Equation (4) for each retrieved element and find
the one with the maximum similarity score.
</bodyText>
<subsectionHeader confidence="0.832805">
Lexicon generation
</subsectionHeader>
<bodyText confidence="0.999979690476191">
To create the semantic features described above,
we generate two types of lexicons leveraging
databases and query logs for each intent class.
The first type of lexicon is an IH lexicon com-
prised of a list of attribute names for the intent
class, e.g., “box office” and “review” for the intent
class Movie. One easy way of composing such a
list is by aggregating the column names in the cor-
responding database such as Table 1. However,
this approach may result in low coverage on IHs
for some domains. Moreover, many database col-
umn names, such as Title, are unlikely to appear as
IHs in queries. Inspired by Pasca and Van Durme
(2007), we apply a bootstrapping algorithm that
automatically learns attribute names for an intent
class from query logs. The key difference from
their work is that we create templates that consist
of semantic labels at the segment level from train-
ing data. For example, “alice in wonderland 2010
cast” is labeled as “IM:Title IM:Year IH”, and thus
“IM:Title + IM:Year + #” is used as a template. We
select the most frequent templates (top 2 in this
work) from training data and use them to discover
new IH phrases from the query log.
Secondly, we have a set IM lexicons, each com-
prised of a list of attribute values of an attribute
name in A, We exploit internal resources to gen-
erate such lexicons. For example, the lexicon for
IM:Title (in Movie) is a list of movie titles gener-
ated by aggregating the values in the Title column
of a movie database. Similarly, the lexicon for
IM:Employee (in Job) is a list of employee names
extracted from a job listing database. Note that
a substantial amount of research effort has been
dedicated to automatic lexicon acquisition from
the Web (Pantel and Pennacchiotti, 2006; Pennac-
chiotti and Pantel, 2009). These techniques can be
used in expanding the semantic lexicons for IMs
when database resources are not available. But we
do not use such techniques in our work since the
lexicons extracted from databases in general have
good precision and coverage.
</bodyText>
<subsectionHeader confidence="0.995264">
5.4 Syntactic features
</subsectionHeader>
<bodyText confidence="0.999985074074074">
As mentioned in Section 3.2, web queries often
lack syntactic cues and do not necessarily follow
the linear order principle. Consequently, applying
syntactic analysis such as POS tagging or chunk-
ing using models trained on natural language cor-
pora is unlikely to give accurate results on web
queries, as supported by our experimental evi-
dence in Section 6.3. It may be beneficial, how-
ever, to use syntactic analysis results as additional
evidence in learning.
To this end, we generate a sequence of POS tags
for a given query, and use the co-occurrence of
POS tag identities and state labels as syntactic fea-
tures (A5) for CRFs.
For semi-Markov CRFs, we instead examine
the POS tag sequence of the corresponding phrase
in a query segment. Again their identities are com-
bined with state labels to create syntactic features
B6. Furthermore, since it is natural to incorporate
segment-level features in semi-Markov CRFs, we
can directly use the output of a syntactic chunker.
To be precise, if a query segment is determined by
the chunker to be a chunk, we use the indicator of
the phrase type of the chunk (e.g., NP, PP) com-
bined with a state label as the feature, denoted by
B7 in the Table. Such features are not activated if
a query segment is determined not to be a chunk.
</bodyText>
<sectionHeader confidence="0.99145" genericHeader="method">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.992894">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.999792076923077">
To evaluate our proposed models and features, we
collected queries from three domains, Movie, Job
and National Park, and had them manually anno-
tated. The annotation was given on both segmen-
tation of the queries and classification of the seg-
ments according to the label sets defined in Ta-
ble 3. There are 1000/496 samples in the train-
ing/test set for the Movie domain, 600/366 for the
Job domain and 491/185 for the National Park do-
main. In evaluation, we report the test-set perfor-
mance in each domain as well as the average per-
formance (weighted by their respectively test-set
size) over all domains.
</bodyText>
<page confidence="0.972346">
1342
</page>
<table confidence="0.998592416666667">
Movie Job National Park
IH trailer, box office IH listing, salary IH lodging, calendar
IM:Award oscar best picture IM:Category engineering IM:Category national forest
IM:Cast johnny depp IM:City las vegas IM:City page
IM:Character michael corleone IM:County orange IM:Country us
IM:Category tv series IM:Employer walmart IM:Name yosemite
IM:Country american IM:Level entry level IM:POI volcano
IM:Director steven spielberg IM:Salary high-paying IM:Rating best
IM:Genre action IM:State florida IM:State flordia
IM:Rating best IM:Type full time
IM:Title the godfather
Other the, in, that Other the, in, that Other the, in, that
</table>
<tableCaption confidence="0.9843635">
Table 3: Label sets and their respective query segment examples for the intent class Movie, Job and
National Park.
</tableCaption>
<subsectionHeader confidence="0.998976">
6.2 Metrics
</subsectionHeader>
<bodyText confidence="0.999918941176471">
There are two evaluation metrics used in our work:
segment F1 and sentence accuracy (Acc). The
first metric is computed based on precision and re-
call at the segment level. Specifically, let us as-
sume that the true segment sequence of a query
is s = (s1, s2, ... , sN), and the decoded segment
sequence is s0 = (s01, s02, ... , s0 ). We say that
s0� is a true positive if s0� E s. The precision
and recall, then, are measured as the total num-
ber of true positives divided by the total num-
ber of decoded and true segments respectively.
We report the F1-measure which is computed as
2 · prec · recall/(prec + recall).
Secondly, a sentence is correct if all decoded
segments are true positives. Sentence accuracy is
measured by the total number of correct sentences
divided by the total number of sentences.
</bodyText>
<subsectionHeader confidence="0.876301">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999912428571429">
We start with models that incorporate first-order
transition features which are standard for both
Markov and semi-Markov CRFs. We then exper-
iment with lexical features, semantic features and
syntactic features for both models. Table 4 and
Table 5 give a summarization of all experimental
results.
</bodyText>
<subsectionHeader confidence="0.808551">
Lexical features
</subsectionHeader>
<bodyText confidence="0.999990444444445">
The first experiment we did is to evaluate the per-
formance of lexical features (combined with tran-
sition features). This involves the use of A2 in Ta-
ble 2 for CRFs, and B2 and B3 for semi-Markov
CRFs. Note that adding B3, i.e., indicators of
whether a query segment contains a word iden-
tity, gave an absolute 7.0%/3.2% gain in sentence
accuracy and segment F1 on average, as shown
in the row B1-B3 in Table 5. For both A2 and
B3, we also tried extending the features based on
word IDs to those based on n-gram IDs, where
n = 1, 2, 3. This greatly increased the number of
lexical features but did not improve learning per-
formance, most likely due to the limited amounts
of training data coupled with the sparsity of such
features. In general, lexical features do not gener-
alize well to the test data, which accounts for the
relatively poor performance of both models.
</bodyText>
<subsectionHeader confidence="0.57658">
Semantic features
</subsectionHeader>
<bodyText confidence="0.999994">
We created IM lexicons from three in-house
databases on Movie, Job and National Parks.
Some lexicons, e.g., IM:State, are shared across
domains. Regarding IH lexicons, we applied the
bootstrapping algorithm described in Section 5.3
to a 1-month query log of Bing. We selected the
most frequent 57 and 131 phrases to form the IH
lexicons for Movie and National Park respectively.
We do not have an IH lexicon for Job as the at-
tribute names in that domain are much fewer and
are well covered by training set examples.
We implemented A3 and A4 for CRFs, which
are based on the n-gram sets created from lex-
icons; and B4 and B5 for semi-Markov CRFs,
which are based on exact and fuzzy match with
lexicon items. As shown in Table 4 and 5, drastic
increases in sentence accuracies and F1-measures
were observed for both models.
</bodyText>
<subsectionHeader confidence="0.630627">
Syntactic features
</subsectionHeader>
<bodyText confidence="0.99999375">
As shown in the row A1-A5 in Table 4, combined
with all other features, the syntactic features (A5)
built upon POS tags boosted the CRF model per-
formance. Table 6 listed the most dominant pos-
itive and negative features based on POS tags for
Movie (features for the other two domains are not
reported due to space limit). We can see that
many of these features make intuitive sense. For
</bodyText>
<page confidence="0.901835">
1343
</page>
<table confidence="0.991387714285714">
Movie Job National Park Average
Features Acc F1 Acc F1 Acc F1 Acc F1
A1,A2: Tran + Lex 59.9 75.8 65.6 84.7 61.6 75.6 62.1 78.9
A1-A3: Tran + Lex + Sem 67.9 80.2 70.8 87.4 70.5 80.8 69.4 82.8
A1-A4: Tran + Lex + Sem 72.4 83.5 72.4 89.7 71.1 82.3 72.2 85.0
A1-A5: Tran + Lex + Sem + Syn 74.4 84.8 75.1 89.4 75.1 85.4 74.8 86.5
A2-A5: Lex + Sem + Syn 64.9 78.8 68.1 81.1 64.8 83.7 65.4 81.0
</table>
<tableCaption confidence="0.977534">
Table 4: Sentence accuracy (Acc) and segment F1 (F1) using CRFs with different features.
</tableCaption>
<table confidence="0.999703111111111">
Movie Job National Park Average
Features Acc F1 Acc F1 Acc F1 Acc F1
B1,B2: Tran + Lex 53.4 71.6 59.6 83.8 60.0 77.3 56.7 76.9
B1-B3: Tran + Lex 61.3 77.7 65.9 85.9 66.0 80.7 63.7 80.1
B1-B4: Tran + Lex + Sem 73.8 83.6 76.0 89.7 74.6 85.3 74.7 86.1
B1-B5: Tran + Lex + Sem 75.0 84.3 76.5 89.7 76.8 86.8 75.8 86.6
B1-B6: Tran + Lex + Sem + Syn 75.8 84.3 76.2 89.7 76.8 87.2 76.1 86.7
B1-B5,B7: Tran + Lex + Sem + Syn 75.6 84.1 76.0 89.3 76.8 86.8 75.9 86.4
B2-B6:Lex + Sem + Syn 72.0 82.0 73.2 87.9 76.5 89.3 73.8 85.6
</table>
<tableCaption confidence="0.999609">
Table 5: Sentence accuracy (Acc) and segment F1 (F1) using semi-Markov CRFs with different features.
</tableCaption>
<bodyText confidence="0.999595">
example, IN (preposition or subordinating con-
junction) is a strong indicator of Other, while TO
and IM:Date usually do not co-occur. Some fea-
tures, however, may appear less “correct”. This
is largely due to the inaccurate output of the POS
tagger. For example, a large number of actor
names were mis-tagged as RB, resulting in a high
positive weight of the feature (RB, IM:Cast).
Table 6: Syntactic features with the largest posi-
tive/negative weights in the CRF model for Movie
Similarly, we added segment-level POS tag fea-
tures (B6) to semi-Markov CRFs, which lead to
the best overall results as shown by the highlighted
numbers in Table 5. Again many of the dominant
features are consistent with our intuition. For ex-
ample, the most positive feature for Movie is (CD
JJS, IM:Rating) (e.g. 100 best). When syntactic
features based on chunking results (B7) are used
instead of B6, the performance is not as good.
</bodyText>
<sectionHeader confidence="0.444876" genericHeader="method">
Transition features
</sectionHeader>
<bodyText confidence="0.992975913043478">
In addition, it is interesting to see the importance
of transition features in both models. Since web
queries do not generally follow the linear order
principle, is it helpful to incorporate transition fea-
tures in learning? To answer this question, we
dropped the transition features from the best sys-
tems, corresponding to the last rows in Table 4
and 5. This resulted in substantial degradations
in performance. One intuitive explanation is that
although web queries are relatively “order-free”,
statistically speaking, some orders are much more
likely to occur than others. This makes it benefi-
cial to use transition features.
Comparison to syntactic analysis
Finally, we conduct a simple experiment by using
the heuristics described in Section 3.2 in extract-
ing IHs from queries. The precision and recall of
IHs averaged over all 3 domains are 50.4% and
32.8% respectively. The precision and recall num-
bers from our best model-based system, i.e., B1-
B6 in Table 5, are 89.9% and 84.6% respectively,
which are significantly better than those based on
pure syntactic analysis.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999050266666667">
In this work, we make the first attempt to define
the semantic structure of noun phrase queries. We
propose statistical methods to automatically ex-
tract IHs, IMs and the semantic labels of IMs us-
ing a variety of features. Experiments show the ef-
fectiveness of semantic features and syntactic fea-
tures in both Markov and semi-Markov CRF mod-
els. In the future, it would be useful to explore
other approaches to automatic lexicon discovery
to improve the quality or to increase the coverage
of both IH and IM lexicons, and to systematically
evaluate their impact on query understanding per-
formance.
The author would like to thank Hisami Suzuki
and Jianfeng Gao for useful discussions.
</bodyText>
<table confidence="0.9798698">
Positive Negative
(IN, Other), (TO, IM:Date)
(VBD, Other) (IN, IM:Cast)
(CD, IM:Date) (CD, IH)
(RB, IM:Cast) (IN, IM:Character)
</table>
<page confidence="0.987156">
1344
</page>
<sectionHeader confidence="0.993705" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999709676470589">
Jaime Arguello, Fernando Diaz, Jamie Callan, and
Jean-Francois Crespo. 2009. Sources of evidence
for vertical selection. In SIGIR’09: Proceedings of
the 32st Annual International ACM SIGIR confer-
ence on Research and Development in Information
Retrieval.
Cory Barr, Rosie Jones, and Moira Regelson. 2008.
The linguistic structure of English web-search
queries. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1021–1030.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
ICML’05: Proceedings of the 22nd international
conference on Machine learning, pages 89–96.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun,
Ming Zhou, and Chang-Ning Huang. 2001. Im-
proving query translation for CLIR using statistical
models. In SIGIR’01: Proceedings of the 24th An-
nual International ACM SIGIR conference on Re-
search and Development in Information Retrieval.
Jinyoung Kim, Xiaobing Xue, and Bruce Croft. 2009.
A probabilistic retrieval model for semistructured
data. In ECIR’09: Proceedings of the 31st Euro-
pean Conference on Information Retrieval, pages
228–239.
John Lafferty, Andrew McCallum, and Ferdando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning, pages 282–289.
Fangtao Li, Xian Zhang, Jinhui Yuan, and Xiaoyan
Zhu. 2008a. Classifying what-type questions by
head noun tagging. In COLING’08: Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 481–488.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2008b. Learn-
ing query intent from regularized click graph. In
SIGIR’08: Proceedings of the 31st Annual Interna-
tional ACM SIGIR conference on Research and De-
velopment in Information Retrieval, July.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-
ing structured information from user queries with
semi-supervised conditional random fields. In SI-
GIR’09: Proceedings of the 32st Annual Interna-
tional ACM SIGIR conference on Research and De-
velopment in Information Retrieva.
Mehdi Manshadi and Xiao Li. 2009. Semantic tagging
of web search queries. In Proceedings of the 47th
Annual Meeting of the ACL and the 4th IJCNLP of
the AFNLP.
Andrew McCallum and Wei Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003, pages 188–
191.
Donald Metzler and Bruce Croft. 2005. Analysis of
statistical question classification for fact-based ques-
tions. Jounral ofInformation Retrieval, 8(3).
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally har-vesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguis-tics and the 44th annual meeting of
the ACL, pages 113–120.
Stelios Paparizos, Alexandros Ntoulas, John Shafer,
and Rakesh Agrawal. 2009. Answering web queries
using structured data sources. In Proceedings of the
35th SIGMOD international conference on Manage-
ment of data.
Marius Pasca and Benjamin Van Durme. 2007. What
you seek is what you get: Extraction of class at-
tributes from query logs. In IJCAI’07: Proceedings
of the 20th International Joint Conference on Artifi-
cial Intelligence.
Marius Pasca and Benjamin Van Durme. 2008.
Weakly-supervised acquisition of open-domain
classes and class attributes from web documents and
query logs. In Proceedings ofACL-08: HLT.
Marco Pennacchiotti and Patrick Pantel. 2009. Entity
extraction via ensemble semantics. In EMNLP’09:
Proceedings of Conference on Empirical Methods in
Natural Language Processing, pages 238–247.
Stephen Robertson, Hugo Zaragoza, and Michael Tay-
lor. 2004. Simple BM25 extension to multiple
weighted fields. In CIKM’04: Proceedings of the
thirteenth ACM international conference on Infor-
mation and knowledge management, pages 42–49.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
Markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems (NIPS’04).
Dou Shen, Jian-Tao Sun, Qiang Yang, and Zheng Chen.
2006. Building bridges for web query classification.
In SIGIR’06: Proceedings of the 29th Annual Inter-
national ACM SIGIR conference on research and de-
velopment in information retrieval, pages 131–138.
</reference>
<page confidence="0.992595">
1345
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978960">
<title confidence="0.999963">Understanding the Semantic Structure of Noun Phrase Queries</title>
<author confidence="0.999733">Xiao Li</author>
<affiliation confidence="0.999933">Microsoft Research</affiliation>
<address confidence="0.99433">One Microsoft Way Redmond, WA 98052 USA</address>
<email confidence="0.999851">xiaol@microsoft.com</email>
<abstract confidence="0.999384125">Determining the semantic intent of web queries not only involves identifying their semantic class, which is a primary focus of previous works, but also understanding their semantic structure. In this work, we formally define the semantic structure of phrase queries as comprised of We present methods that automatically identify these constituents as well as their semantic roles based on Markov and semi-Markov conditional random fields. We show that the use of semantic features and syntactic features significantly contribute to improving the understanding performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jaime Arguello</author>
<author>Fernando Diaz</author>
<author>Jamie Callan</author>
<author>Jean-Francois Crespo</author>
</authors>
<title>Sources of evidence for vertical selection.</title>
<date>2009</date>
<booktitle>In SIGIR’09: Proceedings of the 32st Annual International ACM SIGIR conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="2494" citStr="Arguello et al., 2009" startWordPosition="397" endWordPosition="400">s. Table 1 shows a simplified view of a structured data source, where each row represents a movie object. Consider the query “johnny depp movies 2010”. It is possible to retrieve a set of movie objects from Table 1 that satisfy the constraints Year = 2010 and Cast E) Johnny Depp. This would deliver direct answers to the query rather than having the user sort through list of keyword results. In no small part, the success of such an approach relies on robust understanding of query intent. Most previous works in this area focus on query intent classification (Shen et al., 2006; Li et al., 2008b; Arguello et al., 2009). Indeed, the intent class information is crucial in determining if a query can be answered by any structured data sources and, if so, by which one. In this work, we go one step further and study the semantic structure of a query, i.e., individual constituents of a query and their semantic roles. In particular, we focus on noun phrase queries. A key contribution of this work is that we formally define query semantic structure as comprised of intent heads (IH) and intent modifiers (IM), e.g., [IM:Title alice in wonderland] [IM:Year 2010] [IH cast] It is determined that “cast” is an IH of the ab</context>
<context position="5667" citStr="Arguello et al., 2009" startWordPosition="897" endWordPosition="900">om fields. The second model is especially interesting to us since in our task it is beneficial to use features that measure segment-level characteristics. Finally, we evaluate our proposed models and features on manuallyannotated query sets from three domains, while our techniques are general enough to be applied to many other domains. 2 Related Works 2.1 Query intent understanding As mentioned in the introduction, previous works on query intent understanding have largely focused on classification, i.e., automatically mapping queries into semantic classes (Shen et al., 2006; Li et al., 2008b; Arguello et al., 2009). There are relatively few published works on understanding the semantic structure of web queries. The most relevant ones are on the problem of query tagging, i.e., assigning semantic labels to query terms (Li et al., 2009; Manshadi and Li, 2009). For example, in “canon powershot sd850 camera silver”, the word “canon” should be tagged as Brand. In particular, Li et al. leveraged clickthrough data and a database to automatically derive training data for learning a CRF-based tagger. Manshadi and Li developed a hybrid, generative grammar model for a similar task. Both works are closely related to</context>
</contexts>
<marker>Arguello, Diaz, Callan, Crespo, 2009</marker>
<rawString>Jaime Arguello, Fernando Diaz, Jamie Callan, and Jean-Francois Crespo. 2009. Sources of evidence for vertical selection. In SIGIR’09: Proceedings of the 32st Annual International ACM SIGIR conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cory Barr</author>
<author>Rosie Jones</author>
<author>Moira Regelson</author>
</authors>
<title>The linguistic structure of English web-search queries.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1021--1030</pages>
<contexts>
<context position="12419" citStr="Barr et al., 2008" startWordPosition="2025" endWordPosition="2028"> following heuristics for head noun extraction. We first run a POS-tagger and a chunker jointly on each query, where the POS-tagger/chunker is based on an HMM system trained on English Penn Treebank (Gao et al., 2001). We then mark the right most NP chunk before any prepositional phrase or adjective clause, and apply the NP head rules (Collins, 1999) to the marked NP chunk. The main problem with this approach, however, is that a readily-available POS tagger or chunker is usually trained on natural language sentences and thus is unlikely to produce accurate results on web queries. As shown in (Barr et al., 2008), the lexical category distribution of web queries is dramatically different from that of natural languages. For example, prepositions and subordinating conjunctions, which are strong indicators of the syntactic 1339 structure in natural languages, are often missing in web queries. Moreover, unlike most natural languages that follow the linear-order principle, web queries can have relatively free word orders (although some orders may occur more often than others statistically). These factors make it difficult to produce reliable syntactic analysis outputs. Consequently, the head nouns and henc</context>
</contexts>
<marker>Barr, Jones, Regelson, 2008</marker>
<rawString>Cory Barr, Rosie Jones, and Moira Regelson. 2008. The linguistic structure of English web-search queries. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1021–1030.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Burges</author>
<author>Tal Shaked</author>
<author>Erin Renshaw</author>
<author>Ari Lazier</author>
<author>Matt Deeds</author>
<author>Nicole Hamilton</author>
<author>Greg Hullender</author>
</authors>
<title>Learning to rank using gradient descent.</title>
<date>2005</date>
<booktitle>In ICML’05: Proceedings of the 22nd international conference on Machine learning,</booktitle>
<pages>89--96</pages>
<contexts>
<context position="4476" citStr="Burges et al., 2005" startWordPosition="714" endWordPosition="717">rthington, Zoe Saldana,.. . The Rum Diary 2010 Adventure, Drama Bruce Robinson Johnny Depp,Giovanni Ribisi,.. . Alice in Wonderland 2010 Adventure, Family Tim Burton Mia Wasikowska, Johnny Depp,.. . Table 1: A simplified view of a structured data source for the Movie domain. can reformulate the query into a structured form or reweight different query constituents for structured data retrieval (Robertson et al., 2004; Kim et al., 2009; Paparizos et al., 2009). Alternatively, the knowledge of IHs, IMs and semantic labels of IMs may be used as additional evidence in a learning to rank framework (Burges et al., 2005). A second contribution of this work is to present methods that automatically extract the semantic structure of noun phrase queries, i.e., IHs, IMs and the semantic labels of IMs. In particular, we investigate the use of transition, lexical, semantic and syntactic features. The semantic features can be constructed from structured data sources or by mining query logs, while the syntactic features can be obtained by readily-available syntactic analysis tools. We compare the roles of these features in two discriminative models, Markov and semiMarkov conditional random fields. The second model is </context>
</contexts>
<marker>Burges, Shaked, Renshaw, Lazier, Deeds, Hamilton, Hullender, 2005</marker>
<rawString>Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In ICML’05: Proceedings of the 22nd international conference on Machine learning, pages 89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="12153" citStr="Collins, 1999" startWordPosition="1982" endWordPosition="1983">e.g., “movie avatar” in which the head noun “avatar” serves as an IM instead. Due to the strong resemblance, it is interesting to see if IHs can be identified by extracting linguistic head nouns from queries based on syntactic analysis. To this end, we apply the following heuristics for head noun extraction. We first run a POS-tagger and a chunker jointly on each query, where the POS-tagger/chunker is based on an HMM system trained on English Penn Treebank (Gao et al., 2001). We then mark the right most NP chunk before any prepositional phrase or adjective clause, and apply the NP head rules (Collins, 1999) to the marked NP chunk. The main problem with this approach, however, is that a readily-available POS tagger or chunker is usually trained on natural language sentences and thus is unlikely to produce accurate results on web queries. As shown in (Barr et al., 2008), the lexical category distribution of web queries is dramatically different from that of natural languages. For example, prepositions and subordinating conjunctions, which are strong indicators of the syntactic 1339 structure in natural languages, are often missing in web queries. Moreover, unlike most natural languages that follow</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Jian-Yun Nie</author>
<author>Jian Zhang</author>
<author>Endong Xun</author>
<author>Ming Zhou</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Improving query translation for CLIR using statistical models.</title>
<date>2001</date>
<booktitle>In SIGIR’01: Proceedings of the 24th Annual International ACM SIGIR conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="12018" citStr="Gao et al., 2001" startWordPosition="1957" endWordPosition="1960">e IHs of noun phrase queries are exactly the head nouns in the linguistic sense. Exceptions mostly occur in queries without explicit IHs, e.g., “movie avatar” in which the head noun “avatar” serves as an IM instead. Due to the strong resemblance, it is interesting to see if IHs can be identified by extracting linguistic head nouns from queries based on syntactic analysis. To this end, we apply the following heuristics for head noun extraction. We first run a POS-tagger and a chunker jointly on each query, where the POS-tagger/chunker is based on an HMM system trained on English Penn Treebank (Gao et al., 2001). We then mark the right most NP chunk before any prepositional phrase or adjective clause, and apply the NP head rules (Collins, 1999) to the marked NP chunk. The main problem with this approach, however, is that a readily-available POS tagger or chunker is usually trained on natural language sentences and thus is unlikely to produce accurate results on web queries. As shown in (Barr et al., 2008), the lexical category distribution of web queries is dramatically different from that of natural languages. For example, prepositions and subordinating conjunctions, which are strong indicators of t</context>
</contexts>
<marker>Gao, Nie, Zhang, Xun, Zhou, Huang, 2001</marker>
<rawString>Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun, Ming Zhou, and Chang-Ning Huang. 2001. Improving query translation for CLIR using statistical models. In SIGIR’01: Proceedings of the 24th Annual International ACM SIGIR conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinyoung Kim</author>
<author>Xiaobing Xue</author>
<author>Bruce Croft</author>
</authors>
<title>A probabilistic retrieval model for semistructured data. In</title>
<date>2009</date>
<booktitle>ECIR’09: Proceedings of the 31st European Conference on Information Retrieval,</booktitle>
<pages>228--239</pages>
<contexts>
<context position="4293" citStr="Kim et al., 2009" startWordPosition="682" endWordPosition="685">ious 2009 Drama Lee Daniels Gabby Sidibe, Mo’Nique,.. . 2012 2009 Action, Sci Fi Roland Emmerich John Cusack, Chiwetel Ejiofor,.. . Avatar 2009 Action, Sci Fi James Cameron Sam Worthington, Zoe Saldana,.. . The Rum Diary 2010 Adventure, Drama Bruce Robinson Johnny Depp,Giovanni Ribisi,.. . Alice in Wonderland 2010 Adventure, Family Tim Burton Mia Wasikowska, Johnny Depp,.. . Table 1: A simplified view of a structured data source for the Movie domain. can reformulate the query into a structured form or reweight different query constituents for structured data retrieval (Robertson et al., 2004; Kim et al., 2009; Paparizos et al., 2009). Alternatively, the knowledge of IHs, IMs and semantic labels of IMs may be used as additional evidence in a learning to rank framework (Burges et al., 2005). A second contribution of this work is to present methods that automatically extract the semantic structure of noun phrase queries, i.e., IHs, IMs and the semantic labels of IMs. In particular, we investigate the use of transition, lexical, semantic and syntactic features. The semantic features can be constructed from structured data sources or by mining query logs, while the syntactic features can be obtained by</context>
</contexts>
<marker>Kim, Xue, Croft, 2009</marker>
<rawString>Jinyoung Kim, Xiaobing Xue, and Bruce Croft. 2009. A probabilistic retrieval model for semistructured data. In ECIR’09: Proceedings of the 31st European Conference on Information Retrieval, pages 228–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Ferdando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="8458" citStr="Lafferty et al., 2001" startWordPosition="1338" endWordPosition="1341">actic structures, such an approach was demonstrated to be accurate in identifying head nouns. In identifying IHs in noun phrase queries, however, direct syntactic analysis is unlikely to be as effective. This is because syntactic structures are in general less pronounced in web queries. In this 1338 work, we propose to use POS tagging and parsing outputs as features, in addition to other features, in extracting the semantic structure of web queries. 2.3 Information extraction Finally, there exist large bodies of work on information extraction using models based on Markov and semi-Markov CRFs (Lafferty et al., 2001; Sarawagi and Cohen, 2004), and in particular for the task of named entity recognition (McCallum and Li, 2003). The problem studied in this work is concerned with identifying more generic “semantic roles” of the constituents in noun phrase queries. While some IM categories belong to named entities such as IM:Director for the intent class Movie, there can be semantic labels that are not named entities such as IH and IM:Genre (again for Movie). 3 Query Semantic Structure Unlike database query languages such as SQL, web queries are usually formulated as sequences of words without explicit struct</context>
<context position="14978" citStr="Lafferty et al., 2001" startWordPosition="2475" endWordPosition="2478">are the indices of the starting and ending word tokens respectively; yj ∈ Y is a label indicating the semantic role of s. We further augment the segment sequence with two special segments: Start and End, represented by s0 and sN+1 respectively. For notional simplicity, we assume that the intent class is given and use p(s|x) as a shorthand for p(s|c, x), but keep in mind that the label space and hence the parameter space is class-dependent. Now we introduce two methods of modeling p(s|x). 4.1 CRFs One natural approach to extracting the semantic structure of queries is to use linear-chain CRFs (Lafferty et al., 2001). They model the conditional probability of a label sequence given the input, where the labels, denoted as y = (y1, y2,. .. , yM), yi ∈ Y, have a one-to-one correspondence with the word tokens in the input. Using linear-chain CRFs, we aim to find the label sequence that maximizes �M+11: 1 pλ(y|x) = Zλ(x) exp λ · f(yi_1, yi, x, i) . i=1 The partition function is a normalization is a weight vector and x) is a vector of feature functions referred to as a feature vector. The features used in CRFs will be described in Section 5. Given manually-labeled queries, we estimate that maximizes the conditi</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Ferdando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Xian Zhang</author>
<author>Jinhui Yuan</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Classifying what-type questions by head noun tagging.</title>
<date>2008</date>
<booktitle>In COLING’08: Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>481--488</pages>
<contexts>
<context position="2469" citStr="Li et al., 2008" startWordPosition="393" endWordPosition="396">tain query keywords. Table 1 shows a simplified view of a structured data source, where each row represents a movie object. Consider the query “johnny depp movies 2010”. It is possible to retrieve a set of movie objects from Table 1 that satisfy the constraints Year = 2010 and Cast E) Johnny Depp. This would deliver direct answers to the query rather than having the user sort through list of keyword results. In no small part, the success of such an approach relies on robust understanding of query intent. Most previous works in this area focus on query intent classification (Shen et al., 2006; Li et al., 2008b; Arguello et al., 2009). Indeed, the intent class information is crucial in determining if a query can be answered by any structured data sources and, if so, by which one. In this work, we go one step further and study the semantic structure of a query, i.e., individual constituents of a query and their semantic roles. In particular, we focus on noun phrase queries. A key contribution of this work is that we formally define query semantic structure as comprised of intent heads (IH) and intent modifiers (IM), e.g., [IM:Title alice in wonderland] [IM:Year 2010] [IH cast] It is determined that </context>
<context position="5642" citStr="Li et al., 2008" startWordPosition="893" endWordPosition="896">v conditional random fields. The second model is especially interesting to us since in our task it is beneficial to use features that measure segment-level characteristics. Finally, we evaluate our proposed models and features on manuallyannotated query sets from three domains, while our techniques are general enough to be applied to many other domains. 2 Related Works 2.1 Query intent understanding As mentioned in the introduction, previous works on query intent understanding have largely focused on classification, i.e., automatically mapping queries into semantic classes (Shen et al., 2006; Li et al., 2008b; Arguello et al., 2009). There are relatively few published works on understanding the semantic structure of web queries. The most relevant ones are on the problem of query tagging, i.e., assigning semantic labels to query terms (Li et al., 2009; Manshadi and Li, 2009). For example, in “canon powershot sd850 camera silver”, the word “canon” should be tagged as Brand. In particular, Li et al. leveraged clickthrough data and a database to automatically derive training data for learning a CRF-based tagger. Manshadi and Li developed a hybrid, generative grammar model for a similar task. Both wor</context>
<context position="7632" citStr="Li et al., 2008" startWordPosition="1210" endWordPosition="1213">does not consider the identification and categorization of IMs (attribute values). 2.2 Question answering Query intent understanding is analogous to question understanding for question answering (QA) systems. Many web queries can be viewed as the keyword-based counterparts of natural language questions. For example, the query “california national” and “national parks califorina” both imply the question “What are the national parks in California?”. In particular, a number of works investigated the importance of head noun extraction in understanding what-type questions (Metzler and Croft, 2005; Li et al., 2008a). To extract head nouns, they applied syntax-based rules using the information obtained from part-of-speech (POS) tagging and deep parsing. As questions posed in natural language tend to have strong syntactic structures, such an approach was demonstrated to be accurate in identifying head nouns. In identifying IHs in noun phrase queries, however, direct syntactic analysis is unlikely to be as effective. This is because syntactic structures are in general less pronounced in web queries. In this 1338 work, we propose to use POS tagging and parsing outputs as features, in addition to other feat</context>
</contexts>
<marker>Li, Zhang, Yuan, Zhu, 2008</marker>
<rawString>Fangtao Li, Xian Zhang, Jinhui Yuan, and Xiaoyan Zhu. 2008a. Classifying what-type questions by head noun tagging. In COLING’08: Proceedings of the 22nd International Conference on Computational Linguistics, pages 481–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Li</author>
<author>Ye-Yi Wang</author>
<author>Alex Acero</author>
</authors>
<title>Learning query intent from regularized click graph.</title>
<date>2008</date>
<booktitle>In SIGIR’08: Proceedings of the 31st Annual International ACM SIGIR conference on Research and Development in Information Retrieval,</booktitle>
<contexts>
<context position="2469" citStr="Li et al., 2008" startWordPosition="393" endWordPosition="396">tain query keywords. Table 1 shows a simplified view of a structured data source, where each row represents a movie object. Consider the query “johnny depp movies 2010”. It is possible to retrieve a set of movie objects from Table 1 that satisfy the constraints Year = 2010 and Cast E) Johnny Depp. This would deliver direct answers to the query rather than having the user sort through list of keyword results. In no small part, the success of such an approach relies on robust understanding of query intent. Most previous works in this area focus on query intent classification (Shen et al., 2006; Li et al., 2008b; Arguello et al., 2009). Indeed, the intent class information is crucial in determining if a query can be answered by any structured data sources and, if so, by which one. In this work, we go one step further and study the semantic structure of a query, i.e., individual constituents of a query and their semantic roles. In particular, we focus on noun phrase queries. A key contribution of this work is that we formally define query semantic structure as comprised of intent heads (IH) and intent modifiers (IM), e.g., [IM:Title alice in wonderland] [IM:Year 2010] [IH cast] It is determined that </context>
<context position="5642" citStr="Li et al., 2008" startWordPosition="893" endWordPosition="896">v conditional random fields. The second model is especially interesting to us since in our task it is beneficial to use features that measure segment-level characteristics. Finally, we evaluate our proposed models and features on manuallyannotated query sets from three domains, while our techniques are general enough to be applied to many other domains. 2 Related Works 2.1 Query intent understanding As mentioned in the introduction, previous works on query intent understanding have largely focused on classification, i.e., automatically mapping queries into semantic classes (Shen et al., 2006; Li et al., 2008b; Arguello et al., 2009). There are relatively few published works on understanding the semantic structure of web queries. The most relevant ones are on the problem of query tagging, i.e., assigning semantic labels to query terms (Li et al., 2009; Manshadi and Li, 2009). For example, in “canon powershot sd850 camera silver”, the word “canon” should be tagged as Brand. In particular, Li et al. leveraged clickthrough data and a database to automatically derive training data for learning a CRF-based tagger. Manshadi and Li developed a hybrid, generative grammar model for a similar task. Both wor</context>
<context position="7632" citStr="Li et al., 2008" startWordPosition="1210" endWordPosition="1213">does not consider the identification and categorization of IMs (attribute values). 2.2 Question answering Query intent understanding is analogous to question understanding for question answering (QA) systems. Many web queries can be viewed as the keyword-based counterparts of natural language questions. For example, the query “california national” and “national parks califorina” both imply the question “What are the national parks in California?”. In particular, a number of works investigated the importance of head noun extraction in understanding what-type questions (Metzler and Croft, 2005; Li et al., 2008a). To extract head nouns, they applied syntax-based rules using the information obtained from part-of-speech (POS) tagging and deep parsing. As questions posed in natural language tend to have strong syntactic structures, such an approach was demonstrated to be accurate in identifying head nouns. In identifying IHs in noun phrase queries, however, direct syntactic analysis is unlikely to be as effective. This is because syntactic structures are in general less pronounced in web queries. In this 1338 work, we propose to use POS tagging and parsing outputs as features, in addition to other feat</context>
</contexts>
<marker>Li, Wang, Acero, 2008</marker>
<rawString>Xiao Li, Ye-Yi Wang, and Alex Acero. 2008b. Learning query intent from regularized click graph. In SIGIR’08: Proceedings of the 31st Annual International ACM SIGIR conference on Research and Development in Information Retrieval, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Li</author>
<author>Ye-Yi Wang</author>
<author>Alex Acero</author>
</authors>
<title>Extracting structured information from user queries with semi-supervised conditional random fields.</title>
<date>2009</date>
<booktitle>In SIGIR’09: Proceedings of the 32st Annual International ACM SIGIR conference on Research and Development in Information Retrieva.</booktitle>
<contexts>
<context position="5889" citStr="Li et al., 2009" startWordPosition="934" endWordPosition="937"> query sets from three domains, while our techniques are general enough to be applied to many other domains. 2 Related Works 2.1 Query intent understanding As mentioned in the introduction, previous works on query intent understanding have largely focused on classification, i.e., automatically mapping queries into semantic classes (Shen et al., 2006; Li et al., 2008b; Arguello et al., 2009). There are relatively few published works on understanding the semantic structure of web queries. The most relevant ones are on the problem of query tagging, i.e., assigning semantic labels to query terms (Li et al., 2009; Manshadi and Li, 2009). For example, in “canon powershot sd850 camera silver”, the word “canon” should be tagged as Brand. In particular, Li et al. leveraged clickthrough data and a database to automatically derive training data for learning a CRF-based tagger. Manshadi and Li developed a hybrid, generative grammar model for a similar task. Both works are closely related to one aspect of our work, which is to assign semantic labels to IMs. A key difference is that they do not conceptually distinguish between IHs and IMs. On the other hand, there have been a series of research studies related</context>
</contexts>
<marker>Li, Wang, Acero, 2009</marker>
<rawString>Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extracting structured information from user queries with semi-supervised conditional random fields. In SIGIR’09: Proceedings of the 32st Annual International ACM SIGIR conference on Research and Development in Information Retrieva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehdi Manshadi</author>
<author>Xiao Li</author>
</authors>
<title>Semantic tagging of web search queries.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP.</booktitle>
<contexts>
<context position="5913" citStr="Manshadi and Li, 2009" startWordPosition="938" endWordPosition="941">three domains, while our techniques are general enough to be applied to many other domains. 2 Related Works 2.1 Query intent understanding As mentioned in the introduction, previous works on query intent understanding have largely focused on classification, i.e., automatically mapping queries into semantic classes (Shen et al., 2006; Li et al., 2008b; Arguello et al., 2009). There are relatively few published works on understanding the semantic structure of web queries. The most relevant ones are on the problem of query tagging, i.e., assigning semantic labels to query terms (Li et al., 2009; Manshadi and Li, 2009). For example, in “canon powershot sd850 camera silver”, the word “canon” should be tagged as Brand. In particular, Li et al. leveraged clickthrough data and a database to automatically derive training data for learning a CRF-based tagger. Manshadi and Li developed a hybrid, generative grammar model for a similar task. Both works are closely related to one aspect of our work, which is to assign semantic labels to IMs. A key difference is that they do not conceptually distinguish between IHs and IMs. On the other hand, there have been a series of research studies related to IH identification (P</context>
</contexts>
<marker>Manshadi, Li, 2009</marker>
<rawString>Mehdi Manshadi and Xiao Li. 2009. Semantic tagging of web search queries. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proceedings of the seventh conference on Natural language learning at HLT-NAACL</booktitle>
<pages>188--191</pages>
<contexts>
<context position="8569" citStr="McCallum and Li, 2003" startWordPosition="1356" endWordPosition="1359">Hs in noun phrase queries, however, direct syntactic analysis is unlikely to be as effective. This is because syntactic structures are in general less pronounced in web queries. In this 1338 work, we propose to use POS tagging and parsing outputs as features, in addition to other features, in extracting the semantic structure of web queries. 2.3 Information extraction Finally, there exist large bodies of work on information extraction using models based on Markov and semi-Markov CRFs (Lafferty et al., 2001; Sarawagi and Cohen, 2004), and in particular for the task of named entity recognition (McCallum and Li, 2003). The problem studied in this work is concerned with identifying more generic “semantic roles” of the constituents in noun phrase queries. While some IM categories belong to named entities such as IM:Director for the intent class Movie, there can be semantic labels that are not named entities such as IH and IM:Genre (again for Movie). 3 Query Semantic Structure Unlike database query languages such as SQL, web queries are usually formulated as sequences of words without explicit structures. This makes web queries difficult to interpret by computers. For example, should the query “aspirin side e</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003, pages 188– 191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>Bruce Croft</author>
</authors>
<title>Analysis of statistical question classification for fact-based questions.</title>
<date>2005</date>
<journal>Jounral ofInformation Retrieval,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="7615" citStr="Metzler and Croft, 2005" startWordPosition="1206" endWordPosition="1209">heir framework, however, does not consider the identification and categorization of IMs (attribute values). 2.2 Question answering Query intent understanding is analogous to question understanding for question answering (QA) systems. Many web queries can be viewed as the keyword-based counterparts of natural language questions. For example, the query “california national” and “national parks califorina” both imply the question “What are the national parks in California?”. In particular, a number of works investigated the importance of head noun extraction in understanding what-type questions (Metzler and Croft, 2005; Li et al., 2008a). To extract head nouns, they applied syntax-based rules using the information obtained from part-of-speech (POS) tagging and deep parsing. As questions posed in natural language tend to have strong syntactic structures, such an approach was demonstrated to be accurate in identifying head nouns. In identifying IHs in noun phrase queries, however, direct syntactic analysis is unlikely to be as effective. This is because syntactic structures are in general less pronounced in web queries. In this 1338 work, we propose to use POS tagging and parsing outputs as features, in addit</context>
</contexts>
<marker>Metzler, Croft, 2005</marker>
<rawString>Donald Metzler and Bruce Croft. 2005. Analysis of statistical question classification for fact-based questions. Jounral ofInformation Retrieval, 8(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically har-vesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguis-tics and the 44th annual meeting of the ACL,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="23270" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="3920" endWordPosition="3923">hem to discover new IH phrases from the query log. Secondly, we have a set IM lexicons, each comprised of a list of attribute values of an attribute name in A, We exploit internal resources to generate such lexicons. For example, the lexicon for IM:Title (in Movie) is a list of movie titles generated by aggregating the values in the Title column of a movie database. Similarly, the lexicon for IM:Employee (in Job) is a list of employee names extracted from a job listing database. Note that a substantial amount of research effort has been dedicated to automatic lexicon acquisition from the Web (Pantel and Pennacchiotti, 2006; Pennacchiotti and Pantel, 2009). These techniques can be used in expanding the semantic lexicons for IMs when database resources are not available. But we do not use such techniques in our work since the lexicons extracted from databases in general have good precision and coverage. 5.4 Syntactic features As mentioned in Section 3.2, web queries often lack syntactic cues and do not necessarily follow the linear order principle. Consequently, applying syntactic analysis such as POS tagging or chunking using models trained on natural language corpora is unlikely to give accurate results on web </context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically har-vesting semantic relations. In Proceedings of the 21st International Conference on Computational Linguis-tics and the 44th annual meeting of the ACL, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stelios Paparizos</author>
<author>Alexandros Ntoulas</author>
<author>John Shafer</author>
<author>Rakesh Agrawal</author>
</authors>
<title>Answering web queries using structured data sources.</title>
<date>2009</date>
<booktitle>In Proceedings of the 35th SIGMOD international conference on Management of data.</booktitle>
<contexts>
<context position="4318" citStr="Paparizos et al., 2009" startWordPosition="686" endWordPosition="689">e Daniels Gabby Sidibe, Mo’Nique,.. . 2012 2009 Action, Sci Fi Roland Emmerich John Cusack, Chiwetel Ejiofor,.. . Avatar 2009 Action, Sci Fi James Cameron Sam Worthington, Zoe Saldana,.. . The Rum Diary 2010 Adventure, Drama Bruce Robinson Johnny Depp,Giovanni Ribisi,.. . Alice in Wonderland 2010 Adventure, Family Tim Burton Mia Wasikowska, Johnny Depp,.. . Table 1: A simplified view of a structured data source for the Movie domain. can reformulate the query into a structured form or reweight different query constituents for structured data retrieval (Robertson et al., 2004; Kim et al., 2009; Paparizos et al., 2009). Alternatively, the knowledge of IHs, IMs and semantic labels of IMs may be used as additional evidence in a learning to rank framework (Burges et al., 2005). A second contribution of this work is to present methods that automatically extract the semantic structure of noun phrase queries, i.e., IHs, IMs and the semantic labels of IMs. In particular, we investigate the use of transition, lexical, semantic and syntactic features. The semantic features can be constructed from structured data sources or by mining query logs, while the syntactic features can be obtained by readily-available syntac</context>
</contexts>
<marker>Paparizos, Ntoulas, Shafer, Agrawal, 2009</marker>
<rawString>Stelios Paparizos, Alexandros Ntoulas, John Shafer, and Rakesh Agrawal. 2009. Answering web queries using structured data sources. In Proceedings of the 35th SIGMOD international conference on Management of data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
<author>Benjamin Van Durme</author>
</authors>
<title>What you seek is what you get: Extraction of class attributes from query logs.</title>
<date>2007</date>
<booktitle>In IJCAI’07: Proceedings of the 20th International Joint Conference on Artificial Intelligence.</booktitle>
<marker>Pasca, Van Durme, 2007</marker>
<rawString>Marius Pasca and Benjamin Van Durme. 2007. What you seek is what you get: Extraction of class attributes from query logs. In IJCAI’07: Proceedings of the 20th International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08:</booktitle>
<publisher>HLT.</publisher>
<marker>Pasca, Van Durme, 2008</marker>
<rawString>Marius Pasca and Benjamin Van Durme. 2008. Weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs. In Proceedings ofACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Patrick Pantel</author>
</authors>
<title>Entity extraction via ensemble semantics.</title>
<date>2009</date>
<booktitle>In EMNLP’09: Proceedings of Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="23303" citStr="Pennacchiotti and Pantel, 2009" startWordPosition="3924" endWordPosition="3928">rom the query log. Secondly, we have a set IM lexicons, each comprised of a list of attribute values of an attribute name in A, We exploit internal resources to generate such lexicons. For example, the lexicon for IM:Title (in Movie) is a list of movie titles generated by aggregating the values in the Title column of a movie database. Similarly, the lexicon for IM:Employee (in Job) is a list of employee names extracted from a job listing database. Note that a substantial amount of research effort has been dedicated to automatic lexicon acquisition from the Web (Pantel and Pennacchiotti, 2006; Pennacchiotti and Pantel, 2009). These techniques can be used in expanding the semantic lexicons for IMs when database resources are not available. But we do not use such techniques in our work since the lexicons extracted from databases in general have good precision and coverage. 5.4 Syntactic features As mentioned in Section 3.2, web queries often lack syntactic cues and do not necessarily follow the linear order principle. Consequently, applying syntactic analysis such as POS tagging or chunking using models trained on natural language corpora is unlikely to give accurate results on web queries, as supported by our expe</context>
</contexts>
<marker>Pennacchiotti, Pantel, 2009</marker>
<rawString>Marco Pennacchiotti and Patrick Pantel. 2009. Entity extraction via ensemble semantics. In EMNLP’09: Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Robertson</author>
<author>Hugo Zaragoza</author>
<author>Michael Taylor</author>
</authors>
<title>Simple BM25 extension to multiple weighted fields.</title>
<date>2004</date>
<booktitle>In CIKM’04: Proceedings of the thirteenth ACM international conference on Information and knowledge management,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="4275" citStr="Robertson et al., 2004" startWordPosition="678" endWordPosition="681">irector Cast Review Precious 2009 Drama Lee Daniels Gabby Sidibe, Mo’Nique,.. . 2012 2009 Action, Sci Fi Roland Emmerich John Cusack, Chiwetel Ejiofor,.. . Avatar 2009 Action, Sci Fi James Cameron Sam Worthington, Zoe Saldana,.. . The Rum Diary 2010 Adventure, Drama Bruce Robinson Johnny Depp,Giovanni Ribisi,.. . Alice in Wonderland 2010 Adventure, Family Tim Burton Mia Wasikowska, Johnny Depp,.. . Table 1: A simplified view of a structured data source for the Movie domain. can reformulate the query into a structured form or reweight different query constituents for structured data retrieval (Robertson et al., 2004; Kim et al., 2009; Paparizos et al., 2009). Alternatively, the knowledge of IHs, IMs and semantic labels of IMs may be used as additional evidence in a learning to rank framework (Burges et al., 2005). A second contribution of this work is to present methods that automatically extract the semantic structure of noun phrase queries, i.e., IHs, IMs and the semantic labels of IMs. In particular, we investigate the use of transition, lexical, semantic and syntactic features. The semantic features can be constructed from structured data sources or by mining query logs, while the syntactic features </context>
</contexts>
<marker>Robertson, Zaragoza, Taylor, 2004</marker>
<rawString>Stephen Robertson, Hugo Zaragoza, and Michael Taylor. 2004. Simple BM25 extension to multiple weighted fields. In CIKM’04: Proceedings of the thirteenth ACM international conference on Information and knowledge management, pages 42–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>SemiMarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS’04).</booktitle>
<contexts>
<context position="8485" citStr="Sarawagi and Cohen, 2004" startWordPosition="1342" endWordPosition="1345">an approach was demonstrated to be accurate in identifying head nouns. In identifying IHs in noun phrase queries, however, direct syntactic analysis is unlikely to be as effective. This is because syntactic structures are in general less pronounced in web queries. In this 1338 work, we propose to use POS tagging and parsing outputs as features, in addition to other features, in extracting the semantic structure of web queries. 2.3 Information extraction Finally, there exist large bodies of work on information extraction using models based on Markov and semi-Markov CRFs (Lafferty et al., 2001; Sarawagi and Cohen, 2004), and in particular for the task of named entity recognition (McCallum and Li, 2003). The problem studied in this work is concerned with identifying more generic “semantic roles” of the constituents in noun phrase queries. While some IM categories belong to named entities such as IM:Director for the intent class Movie, there can be semantic labels that are not named entities such as IH and IM:Genre (again for Movie). 3 Query Semantic Structure Unlike database query languages such as SQL, web queries are usually formulated as sequences of words without explicit structures. This makes web querie</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2004. SemiMarkov conditional random fields for information extraction. In Advances in Neural Information Processing Systems (NIPS’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Jian-Tao Sun</author>
<author>Qiang Yang</author>
<author>Zheng Chen</author>
</authors>
<title>Building bridges for web query classification.</title>
<date>2006</date>
<booktitle>In SIGIR’06: Proceedings of the 29th Annual International ACM SIGIR conference on</booktitle>
<pages>131--138</pages>
<contexts>
<context position="2452" citStr="Shen et al., 2006" startWordPosition="389" endWordPosition="392"> web pages that contain query keywords. Table 1 shows a simplified view of a structured data source, where each row represents a movie object. Consider the query “johnny depp movies 2010”. It is possible to retrieve a set of movie objects from Table 1 that satisfy the constraints Year = 2010 and Cast E) Johnny Depp. This would deliver direct answers to the query rather than having the user sort through list of keyword results. In no small part, the success of such an approach relies on robust understanding of query intent. Most previous works in this area focus on query intent classification (Shen et al., 2006; Li et al., 2008b; Arguello et al., 2009). Indeed, the intent class information is crucial in determining if a query can be answered by any structured data sources and, if so, by which one. In this work, we go one step further and study the semantic structure of a query, i.e., individual constituents of a query and their semantic roles. In particular, we focus on noun phrase queries. A key contribution of this work is that we formally define query semantic structure as comprised of intent heads (IH) and intent modifiers (IM), e.g., [IM:Title alice in wonderland] [IM:Year 2010] [IH cast] It is</context>
<context position="5625" citStr="Shen et al., 2006" startWordPosition="889" endWordPosition="892">arkov and semiMarkov conditional random fields. The second model is especially interesting to us since in our task it is beneficial to use features that measure segment-level characteristics. Finally, we evaluate our proposed models and features on manuallyannotated query sets from three domains, while our techniques are general enough to be applied to many other domains. 2 Related Works 2.1 Query intent understanding As mentioned in the introduction, previous works on query intent understanding have largely focused on classification, i.e., automatically mapping queries into semantic classes (Shen et al., 2006; Li et al., 2008b; Arguello et al., 2009). There are relatively few published works on understanding the semantic structure of web queries. The most relevant ones are on the problem of query tagging, i.e., assigning semantic labels to query terms (Li et al., 2009; Manshadi and Li, 2009). For example, in “canon powershot sd850 camera silver”, the word “canon” should be tagged as Brand. In particular, Li et al. leveraged clickthrough data and a database to automatically derive training data for learning a CRF-based tagger. Manshadi and Li developed a hybrid, generative grammar model for a simil</context>
</contexts>
<marker>Shen, Sun, Yang, Chen, 2006</marker>
<rawString>Dou Shen, Jian-Tao Sun, Qiang Yang, and Zheng Chen. 2006. Building bridges for web query classification. In SIGIR’06: Proceedings of the 29th Annual International ACM SIGIR conference on research and development in information retrieval, pages 131–138.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>