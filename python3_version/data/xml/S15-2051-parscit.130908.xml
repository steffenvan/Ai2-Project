<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002610">
<title confidence="0.793676">
SemEval-2015 Task 14: Analysis of Clinical Text
</title>
<author confidence="0.660999">
No´emie Elhadad♣, Sameer Pradhan†, Sharon Lipsky Gorman♣,
</author>
<affiliation confidence="0.69738125">
Suresh Manandhar♦, Wendy Chapman♠, Guergana Savova†♣ Columbia University, USA
† Boston Children’s Hospital, USA
♦ University of York, UK
♠ University of Utah, USA
</affiliation>
<email confidence="0.996936">
noemie.elhadad@columbia.edu, guergana.savova@childrens.harvard.edu
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957107142857">
We describe two tasks—named entity recog-
nition (Task 1) and template slot filling (Task
2)—for clinical texts. The tasks leverage an-
notations from the ShARe corpus, which con-
sists of clinical notes with annotated men-
tions disorders, along with their normaliza-
tion to a medical terminology and eight addi-
tional attributes. The purpose of these tasks
was to identify advances in clinical named en-
tity recognition and establish the state of the
art in disorder template slot filling. Task 2
consisted of two subtasks: template slot fill-
ing given gold-standard disorder spans (Task
2a) and end-to-end disorder span identifica-
tion together with template slot filling (Task
2b). For Task 1 (disorder span detection and
normalization), 16 teams participated. The
best system yielded a strict F1-score of 75.7,
with a precision of 78.3 and recall of 73.2.
For Task 2a (template slot filling given gold-
standard disorder spans), six teams partici-
pated. The best system yielded a combined
overall weighted accuracy for slot filling of
88.6. For Task 2b (disorder recognition and
template slot filling), nine teams participated.
The best system yielded a combined relaxed F
(for span detection) and overall weighted ac-
curacy of 80.8.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961487179487">
Patient records are abundant with reports, narratives,
discussions, and updates about patients. This un-
structured part of the record is dense with mentions
of clinical entities, such as conditions, anatomical
sites, medications, and procedures. Identifying the
different entities discussed in a patient record, their
status towards the patient, and how they relate to
each other is one of the core tasks of clinical natural
language processing. Indeed, with robust systems
to extract such mentions, along with their associated
attributes in the text (e.g., presence of negation for
a given entity mention), several high-level applica-
tions can be developed such as information extrac-
tion, question answering, and summarization.
In biomedicine, there are rich lexicons that can
be leveraged for the task of named entity recogni-
tion and entity linking or normalization. The Uni-
fied Medical Language System (UMLS) represents
over 130 lexicons/thesauri with terms from a va-
riety of languages. The UMLS Metathesaurus in-
tegrates standard resources such as SNOMED-CT,
ICD9, and RxNORM that are used worldwide in
clinical care, public health, and epidemiology. In
addition, the UMLS also provides a semantic net-
work in which every concept in the Metathesaurus is
represented by its Concept Unique Identifier (CUI)
and is semantically typed (Bodenreider and McCray,
2003).
The SemEval-2015 Task 14, Analysis of Clinical
Text is the newest iteration in a series of community
challenges organized around the tasks of named en-
tity recognition for clinical texts. In SemEval-2014
Task 7 (Pradhan et al., 2014) and previous challenge
2013 (Pradhan et al., 2013), we had focused on the
task of named entity recognition for disorder men-
tions in clinical texs, along with normalization to
UMLS CUIs. This year, we shift focus on the task
of identifying a series of attributes describing a dis-
order mention. Like for previous challenges, we use
</bodyText>
<page confidence="0.991936">
303
</page>
<bodyText confidence="0.868254333333333">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 303–310,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
the ShARe corpus1 and introduce a new set of anno-
tations for disorder attributes.
In the remainder of this paper, we describe the
dataset and the annotations provided to the task par-
ticipants, the subtasks comprising the overall task,
and the results of the teams that participated along
with notable approaches in their systems.
</bodyText>
<sectionHeader confidence="0.990523" genericHeader="introduction">
2 Dataset
</sectionHeader>
<table confidence="0.996436">
Train Dev Test
Notes 298 133 100
Words 182K 153K 109K
</table>
<tableCaption confidence="0.991541">
Table 1: Notes, words, and disorder distributions in the
training, development, and testing sets.
</tableCaption>
<bodyText confidence="0.999707407407407">
The dataset used is the ShARe corpus (Pradhan et
al., 2015). As a whole, it consists of 531 deidentified
clinical notes (a mix of discharge summaries and ra-
diology reports) selected from the MIMIC II clinical
database version 2.5 (Saeed et al., 2002). Part of the
ShARe corpus was released as part of Semeval 2014
Task 7. In fact, to enable meaningful comparisons
of systems performance across years, the 2015 Se-
mEval training set combines the 2014 training and
development sets, while the 2015 SemEval devel-
opment set consists of the 2014 test set. The 2015
test set is a previously unseen set of clinical notes
from the ShARe corpus. Table 2 provides descrip-
tive statistics about the different sets. In addition
to the ShARe corpus annotations, task participants
were provided with a large set of unlabeled deiden-
tified clinical notes, also from MIMIC II (400,000+
notes).
The ShARe corpus contains gold-standard anno-
tations of disorder mentions and a set of attributes, as
described in Table 2. We refer to the nine attributes
as a disorder template. The annotation schema for
the template was derived from the established clini-
cal element model2. The complete guidelines for the
ShARe annotations are available on the ShARe web-
site3. Here, we provide a few examples to illustrate
what each attribute captures.
</bodyText>
<footnote confidence="0.996923666666667">
1share.healthnlp.org
2www.clinicalelement.com
3share.healthnlp.org
</footnote>
<table confidence="0.9997779375">
Train Dev
Disorder mentions 11,144 7,967
CUI=CUI-less 30% 24%
CUI 70% 76%
Unique CUIs 1,352 1,139
Negation = yes 19.6% 20.1%
Negation = no 80.4% 79.9%
Subject = patient 99.2% 98.4%
Subject = family member &lt;1% 1.4%
Subject = other &lt;1% &lt;1%
Subject = donor other &lt;1% 0%
Uncertainty = yes 8.9% 5.9%
Uncertainty = no 91.1% 94.1%
Course = changed &lt;1% &lt;1%
Course = resolved &lt;1% &lt;1%
Course = worsened &lt; 1% &lt;1%
Course = improved &lt; 1% 1%
Course = decreased 1.6% &lt;1%
Course = increased 2% 1.7%
Course = unmarked 94.1% 95.2%
Severity = slight 1.1% &lt;1%
Severity = severe 3.5% 2.6%
Severity = moderate 5.9% 2.3%
Severity = unmarked 89.49% 94.2%
Conditional = true 4.9% 6.2%
Conditional = false 95.1% 93.8%
Generic = true &lt;1% 1%
Generic = false 99.1 99%
Body Location = CUI 55.3% 44.7%
Body Location = null 44.4% 54.6%
Body Location = CUI-less &lt;1% &lt;1%
Unique BL CUIs 734 511
</table>
<tableCaption confidence="0.995878">
Table 3: Distribution of different attribute values in the
training and testing sets.
</tableCaption>
<listItem confidence="0.999111933333333">
• In the statement “patient denies numbness,” the
disorder numbness has an associated negation at-
tribute set to “yes.”
• In the sentence “son has schizophrenia”, the dis-
order schizophrenia has a subject attribute set to
“family member.”
• The sentence “Evaluation of MI.” contains a dis-
order (MI) with the uncertainty attribute set to
“yes”.
• An example of disorder with a non-default course
attribute can be found in the sentence “The cough
got worse over the next two weeks.”, where its
value is “worsened.”
• The severity attribute is set to “slight” in “He has
slight bleeding.”
</listItem>
<page confidence="0.994613">
304
</page>
<table confidence="0.999861785714286">
Slot Description Possible Values
CUI CUI; indicates normalized disorder CUI, CUI-less
NEG Negation; indicates whether disorder is negated no∗, yes
SUB Subject; indicates who experiences the disorder patient∗, null, other, family member,
donor family member, donor other
UNC Uncertainty; indicates presence of doubt about the disorder no∗, yes
COU Course; indicates progress or decline of the disorder unmarked∗, changed, increased, de-
creased, improved, worsened, resolved
SEV Severity; indicates how severe the disorder is unmarked∗, slight, moderate, severe
CND Conditional; indicates conditional existence of disorder un- false∗, true
der specific circumstances
GEN Generic; indicates a generic mention of a disorder false∗, true
BL Body Location; represents normalized CUI of body loca- null∗, CUI, CUI-less
tion(s) associated with disorder
</table>
<tableCaption confidence="0.987812">
Table 2: Disorder attributes and their possible values. Default values are indicated with an *.
</tableCaption>
<listItem confidence="0.936189333333333">
• In the sentence “Pt should come back if any rash
occurs,” the disorder rash has a conditional at-
tribute with value “true.”
• In the sentence “Patient has a facial rash”, the
body location associated with the disorder “facial
rash” is “face” with CUI C0015450. Note that the
body location does not have to be a substring of
the disorder mention, even though in this example
it is.
</listItem>
<bodyText confidence="0.999848090909091">
The ShARe corpus was annotated following a rig-
orous process. Annotators were professional coders
who trained for the specific task of ShARe annota-
tions. The annotation process consisted of a double
annotation step followed by an adjudication phase.
For all annotations, in addition to all the values for
the attributes, their corresponding character spans in
the text were recorded and are available as part of
the ShARe annotations. Table 3 shows the distri-
bution of the different attributes in the training and
development sets.
</bodyText>
<sectionHeader confidence="0.995479" genericHeader="method">
3 Tasks
</sectionHeader>
<bodyText confidence="0.9957995">
The Analysis of Clinical Text Task is split into two
tasks, one on named entity recognition, and one on
template slot filling for the named entities. Partici-
pants were able to submit to either or both tasks.
</bodyText>
<subsectionHeader confidence="0.995212">
3.1 Task 1: Disorder Identification
</subsectionHeader>
<bodyText confidence="0.988345176470588">
For task 1, disorder identification, the goal is to rec-
ognize the span of a disorder mention in input clin-
ical text and to normalize the disorder to a unique
CUI in the UMLS/SNOMED-CT terminology. The
UMLS/SNOMED-CT terminology is defined as the
set of CUIs in the UMLS, but restricted to concepts
that are included in the SNOMED-CT terminology.
Participants were free to use any publicly avail-
able resources, such as UMLS, WordNet, and
Wikipedia, as well as the large corpus of un-
annotated clinical notes.
The following are examples of input/output for
Task 1.
1 In “The rhythm appears to be atrial fibrillation.”
the span “atrial fibrillation” is the gold-standard
disorder, and its normalization is CUI C0004238
(preferred term atrial fibrillation). This is a
2 In “The left atrium is moderately dilated.” the dis-
order span is discontiguous: “left atrium...dilated”
and its normalization is CUI C0344720 (preferred
term left atrial dilatation).
3 In “53 year old man s/p fall from ladder.” the
disorder is “fall from ladder” and is normalized
to C0337212 (preferred term accidental fall from
ladder).
Example 1 represents the easiest cases. Example
2 represents instances of disorders as listed in the
UMLS that are best mapped to discontiguous men-
tions. In Example 3, one has to infer that the
description is a synonym of the UMLS preferred
term. Finally, in some cases, a disorder mention
is present, but there is no good equivalent CUI in
UMLS/SNOMED-CT. The disorder is then normal-
ized to “CUI-less”.
</bodyText>
<page confidence="0.99532">
305
</page>
<subsectionHeader confidence="0.990042">
3.2 Task 2: Disorder Slot Filling
</subsectionHeader>
<bodyText confidence="0.999980666666667">
This task focuses on identifying the normalized
value for the nine attributes described above: the
CUI of the disorder (very much like in Task 1), nega-
tion indicator, subject, uncertainty indicator, course,
severity, conditional, generic indicator, and body lo-
cation.
We describe Task 2 as a slot-filling task: given a
disorder mention (either provided by gold-standard
or identified automatically) in a clinical note, iden-
tify the normalized value of the nine slots. Note that
there are two aspects to slot filling: cues in the text
and normalized value. In this task, we focus on nor-
malized value and ignore cue detection.
To understand the state of the art for this new task,
we considered two subtasks. In both cases, given a
disorder span, participants are asked to identify the
nine attributes related to the disorder. In Task 2a,
the gold-standard disorder span(s) are provided as
input. In Task 2b, no gold-standard information is
provided; systems must recognize spans for disorder
mentions and fill in the value of the nine attributes.
</bodyText>
<sectionHeader confidence="0.999801" genericHeader="method">
4 Evaluation Metrics
</sectionHeader>
<subsectionHeader confidence="0.999167">
4.1 Task 1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9980095">
Evaluation for Task 1 is reported according to a F-
score, that captures both the disorder span recogni-
tion and the CUI normalization steps. We compute
two versions of the F-score:
</bodyText>
<listItem confidence="0.9812498">
• Strict F-score: a predicted mention is considered
a true positive if (i) the character span of the dis-
order is exactly the same as for the gold-standard
mention; and (ii) the predicted CUI is correct. The
predicted disorder is considered a false positive if
the span is incorrect or the CUI is incorrect.
• Relaxed F-score: a predicted mention is a true
positive if (i) there is any word overlap between
the predicted mention span and the gold-standard
span (both in the case of contiguous and discon-
</listItem>
<bodyText confidence="0.89503875">
tiguous spans); and (ii) the predicted CUI is cor-
rect. The predicted mention is a false positive if
the span shares no words with the gold-standard
span or the CUI is incorrect.
Thus, given, Dtp, the number of true positives
disorder mentions, Dfp, the number of false posi-
tive disorder mentions, and Dfn, the number of false
negative disorder mentions
</bodyText>
<equation confidence="0.9870963">
Dtp
Precision = P = (1)
Dtp+Dfp
Dtp
Recall = R = (2)
Dtp + Dfn
2 × P × R
F =
(3)
P + R
</equation>
<subsectionHeader confidence="0.958248">
4.2 Task 2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999995066666667">
We introduce a variety of evaluation metrics, which
capture different aspects of the task of disorder tem-
plate slot filling. Overall, for Task 2a, we reported
average unweighted accuracy, weighted accuracy,
and per-slot weighted accuracy for each of the nine
slots. For Task 2b, we report the same metrics, and
in addition report relaxed F for span identification.
We now describe per-disorder evaluation met-
rics, and then describe the overall evaluation metrics
which provide aggregated system assessment. Given
the K slots (s1, ..., sK) to fill (in our task the nine
different slots), each slot sk has nk possible normal-
ized values (sik)i ∈ 1..nk. For a given disorder, its
gold-standard value for slot sk is denoted gsk, and
its predicted value is denoted psk.
</bodyText>
<subsubsectionHeader confidence="0.932249">
4.2.1 Per-Disorder Evaluation Metrics
</subsubsectionHeader>
<bodyText confidence="0.842523">
Per-disorder unweighted accuracy The un-
weighted accuracy represents the ability of a system
to identify all the slot values for a given disorder.
The per-disorder unweighted accuracy is simply
defined as:
</bodyText>
<equation confidence="0.986804">
EKk=1 I(gsk, psk)
K
</equation>
<bodyText confidence="0.99542425">
where I is the identity function: I(x, y) = 1 if x = y
and 0 otherwise.
Per-disorder weighted accuracy The weighted
per-disorder accuracy takes into account the preva-
lence of different values for each of the slots. This
metric captures how good a system is at identifying
rare values of different slots. The weights are thus
defined as follows:
</bodyText>
<listItem confidence="0.990181285714286">
• The CUI slot’s weight is set to 1, for all CUI val-
ues.
• The body location slot’s weight is defined as
weight(NULL) = 1-prevalence(NULL), and the
weight for any non-NULL value (including CUI-
less) is set to weight(CUI) = 1-prevalence(body
location with a non-NULL value).
</listItem>
<page confidence="0.954535">
306
</page>
<listItem confidence="0.969114666666667">
• For each other slot sk, we define nk weights
weight(sik) (one for each of its possible normal-
ized values) as follows:
</listItem>
<equation confidence="0.831096">
Vi E 1..nk, weight(sik) = 1 − prevalence(sik)
</equation>
<bodyText confidence="0.999940714285714">
where prevalence(sik) is the prevalence of value
si k in the overall corpus(training, development, and
testing sets). The weights are such that highly preva-
lent values have smaller weights and rare values
have bigger weight.
Thus, weighted per-disorder accuracy is defined
as
</bodyText>
<equation confidence="0.999204333333333">
EEKk=1 weight(gsk) * I(gsk,psk) (4)
K
k=1 weight(gsk)
</equation>
<bodyText confidence="0.99975375">
where, like above, gsk is the gold-standard value of
slot sk and psk is the predicted value of slot sk, and
I is the identity function: I(x, y) = 1 if x = y and 0
otherwise.
</bodyText>
<subsectionHeader confidence="0.785271">
4.2.2 Overall Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999131">
Weighted and Unweighted Accuracy. Armed
with the per-disorder unweighted and weighted ac-
curacy scores, we can compute an average across all
true-positive disorders. For task 2a, the disorders are
provided, so they are all true positive, but for task 2b,
it is important to note that we only consider the true-
positive disorders to compute the overall accuracy.
</bodyText>
<equation confidence="0.770684">
Accuracy = E#tPper disorder acc(tpi) (5)
#tp
</equation>
<bodyText confidence="0.999110142857143">
Per-Slot Accuracy. Per-slot accuracy are useful in
assessing the ability of a system to fill in a particu-
lar slot. For each slot, an average per-slot accuracy
is defined as the accuracy for each true-positive dis-
order to recognize the value for that particular slot
across the true-positive spans. Thus, for slot sk, the
per-slot accuracy is:
</bodyText>
<equation confidence="0.989311">
E#tp
i=1 weight(gsi,k) * I(gsi,k, psi,k) (6)
E#tP weight(gsi,k)
</equation>
<bodyText confidence="0.998739">
where for each true-positive span there is a gold-
standard value gsi,k and a predicted value psi,k for
slot sk.
</bodyText>
<figureCaption confidence="0.998371">
Figure 1: Task 1 results.
</figureCaption>
<bodyText confidence="0.999709">
Disorder Span Identification. This overall met-
ric is only meaningful for Task 2b, where the sys-
tem has to identify disorders prior to filling in their
templates. Like in Task 1, we report an F-score met-
ric to assess how good the system is at identifying
disorder span. Note that unlike in Task 1, this F
score does not consider CUI normalization, as this
is captured through the accuracy in the template fill-
ing task. Thus, a true disorder span is defined as
any overalp with a gold-stand disorder span. In the
case of several predicted spans that overlap with a
gold-standard span, then only one of them is chosen
to be true positive (the longest ones), and the other
predicted spans are considered false positives.
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.976378">
5.1 Task 1
</subsectionHeader>
<bodyText confidence="0.998931">
16 teams participated in Task 1. Strict and relaxed
precision, recall, and F metrics are reported in Fig-
ure 1. We relied on the strict F to rank different sub-
missions. The best system from team ezDI reported
</bodyText>
<page confidence="0.995744">
307
</page>
<bodyText confidence="0.991448368421053">
75.7 strict F, also reporting the highest relaxed F
(78.7) (Pathak et al., 2015).
For disorder span recognition, most teams used
a CRF-based approach. Features explored included
traditional NER features: lexical (bag of words
and bigrams, orthographic features), syntactic fea-
tures derived from either part-of-speech and phrase
chunking information or dependency parsing, and
domain features (note type and section headers of
clinical note). Lookup to dictionary (either UMLS
or customized lexicon of disorders) was an essential
feature for performance. To leverage further these
lexicons, for instance, Xu and colleagues (Xu et al.,
2015) implemented a vector-space model similar-
ity computation to known disorders as an additional
feature in their appraoch.
The best-performing teams made use of the large
unannotated corpus of clinical notes provided in the
challenge (Pathak et al., 2015; Leal et al., 2015; Xu
et al., 2015). Teams explored the use of Brown clus-
ters (Brown et al., 1992) and word embeddings (Col-
lobert et al., 2011). Pathak and colleagues (Pathak et
al., 2015) note that word2vec (Mikolov et al., 2013)
did not yield satisfactory results. Instead, they report
better results clustering sentences in the unannotated
texts based on their sequence of part-of-speech tags,
and using the clusters as feature in the CRF.
Teams continued to explore approaches for rec-
ognizing discontiguous entities. Pathak and col-
leagues (Pathak et al., 2015), for instance, built a
specialized SVM-based classifier for that purpose.
For CUI normalization, the best performing teams
focused on augmenting existing dictionaries with
lists of unambiguous abbreviations (Leal et al.,
2015) or by pre-processing UMLS and breaking
down existing lexical variants to account for high
paraphrasing power of disorder terms (Pathak et al.,
2015).
</bodyText>
<subsectionHeader confidence="0.97923">
5.2 Task 2
</subsectionHeader>
<bodyText confidence="0.999959148148148">
Six teams participated in Task 2a. Evaluation met-
rics are reported in Figure 2. We relied on the
Weighted Accuracy (WA) to rank the teams (high-
lighted in the Figure is F*WA, but since in Task
2a gold-standard disorders are provided, F is 1).
The best system (team UTH-CCB) yielded a WA of
88.6 (Xu et al., 2015).
For Task 2b, nine teams participated. Evaluation
metrics are reported in Figure 3. We relied on the
combination of F score for disorder span identifica-
tion and Weighted Accuracy for template filling to
rank the teams (F*WA in the figure). The best sys-
tem (team UTH-CCB) yielded a F*WA of 80.8.
Approaches to template filling focused on build-
ing classifiers for each attribute. Specialized lex-
icons of trigger terms for each attribute (e.g., list
of negation terms) along with distance to disorder
spans was a helpful feature. Overall, like in Task
1, a range of feature types from lexical to syntactic
proved useful in the template filling task.
The per-slot accuracies (columns BL, CUI, CND,
COU, GEN, NEG, SEV, SUB, and UNC in Figures 2
and 3) indicate that overall some attributes are eas-
ier to recognize than others. Body Location, per-
haps not surprisingly, was the most difficult after
CUI normalization, in part because it also requires
a normalization to an anatomical site.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999977166666667">
In this task, we introduced a new version of the
ShARe corpus, with annotations of disorders and a
wide set of disorder attributes. The biggest improve-
ments in the task of disorder recognition (both span
identification and CUI normalization) come from
leveraging large amounts of unannotated texts and
using word embeddings as additional feature in the
task. The detection of discontiguous disorder seems
to still be an open challenge for the community,
however.
The new task of template filling (identifying nine
attributes for a given disorder) was met with enthu-
siasm by the participating teams. We introduced a
variety of evaluation metrics to capture the differ-
ent aspects of the task. Different approaches show
that while some attributes are harder to identify than
other, overall the best performing teams achieved
excellent results.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99895575">
This work was supported by the Shared Annotated
Resources (ShARe) project NIH R01 GM090187.
We greatly appreciate the hard work of our program
committee members and the ShARe annotators.
</bodyText>
<page confidence="0.99778">
308
</page>
<figureCaption confidence="0.998729">
Figure 2: Task 2a results.
</figureCaption>
<sectionHeader confidence="0.998442" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996640509803921">
Olivier Bodenreider and Alexa T McCray. 2003. Explor-
ing semantic groups through visual approaches. Jour-
nal of biomedical informatics, 36(6):414–432.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J Della Pietra, and Jenifer C Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional linguistics, 18(4):467–479.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493–
2537.
Andr´e Leal, Bruno Martins, and Francisco Couto. 2015.
ULisboa: Semeval 2015 - task 14 analysis of clinical
text: Recognition and normalization of medical con-
cepts. In Proceedings of SemEval-2015.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representations
of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems,
pages 3111–3119.
Parth Pathak, Pinal Patel, Vishal Panchal, Sagar Soni,
Kinjal Dani, Narayan Choudhary, and Amrish Patel.
2015. ezDI: A semi-supervised nlp system for clinical
narrative analysis. In Proceedings of SemEval-2015.
Sameer Pradhan, Noemie Elhadad, Brett R South, David
Martinez, Lee Christensen, Amy Vogel, Hanna Suomi-
nen, Wendy Chapman, and Guergana Savova. 2013.
Task 1: Share/clef ehealth evaluation lab 2013. In On-
line Working Notes of CLEF, page 230.
Sameer Pradhan, No´emie Elhadad, Wendy Chapman,
Suresh Manandhar, and Guergana Savova. 2014.
Semeval-2014 task 7: Analysis of clinical text. In Pro-
ceedings of the 8th International Workshop on Seman-
tic Evaluation (SemEval 2014), pages 54–62.
Sameer Pradhan, No´emie Elhadad, Brett R South, David
Martinez, Lee Christensen, Amy Vogel, Hanna Suomi-
nen, Wendy W Chapman, and Guergana Savova.
2015. Evaluating the state of the art in disorder
recognition and normalization of the clinical narrative.
Journal of the American Medical Informatics Associa-
tion, 22(1):143–154.
Mohammed Saeed, C Lieu, G Raber, and RG Mark.
2002. Mimic II: a massive temporal ICU patient
database to support research in intelligent patient mon-
itoring. In Computers in Cardiology, 2002, pages
641–644. IEEE.
Jun Xu, Yaoyun Zhang, Jingqi Wang, Yonghui Wu, Min
Jian, Ergin Soysal, and Hua Xu. 2015. UTH-CCB:
The participation of the SemEval 2015 challenge - task
14. In Proceedings of SemEval-2015.
</reference>
<page confidence="0.999266">
309
</page>
<figureCaption confidence="0.998523">
Figure 3: Task 2b results.
</figureCaption>
<page confidence="0.994931">
310
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.621873">
<title confidence="0.960619">SemEval-2015 Task 14: Analysis of Clinical Text</title>
<author confidence="0.994633">Sameer Sharon Lipsky</author>
<affiliation confidence="0.996969">Wendy Guergana University,</affiliation>
<address confidence="0.964802">Children’s Hospital, USA</address>
<affiliation confidence="0.999111">University of York,</affiliation>
<address confidence="0.998353">of Utah, USA</address>
<abstract confidence="0.977164965517242">We describe two tasks—named entity recognition (Task 1) and template slot filling (Task 2)—for clinical texts. The tasks leverage annotations from the ShARe corpus, which consists of clinical notes with annotated mentions disorders, along with their normalization to a medical terminology and eight additional attributes. The purpose of these tasks was to identify advances in clinical named entity recognition and establish the state of the art in disorder template slot filling. Task 2 consisted of two subtasks: template slot filling given gold-standard disorder spans (Task 2a) and end-to-end disorder span identification together with template slot filling (Task 2b). For Task 1 (disorder span detection and normalization), 16 teams participated. The best system yielded a strict F1-score of 75.7, with a precision of 78.3 and recall of 73.2. For Task 2a (template slot filling given goldstandard disorder spans), six teams participated. The best system yielded a combined overall weighted accuracy for slot filling of 88.6. For Task 2b (disorder recognition and template slot filling), nine teams participated. The best system yielded a combined relaxed F (for span detection) and overall weighted accuracy of 80.8.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Olivier Bodenreider</author>
<author>Alexa T McCray</author>
</authors>
<title>Exploring semantic groups through visual approaches.</title>
<date>2003</date>
<journal>Journal of biomedical informatics,</journal>
<volume>36</volume>
<issue>6</issue>
<contexts>
<context position="2943" citStr="Bodenreider and McCray, 2003" startWordPosition="440" endWordPosition="443">omedicine, there are rich lexicons that can be leveraged for the task of named entity recognition and entity linking or normalization. The Unified Medical Language System (UMLS) represents over 130 lexicons/thesauri with terms from a variety of languages. The UMLS Metathesaurus integrates standard resources such as SNOMED-CT, ICD9, and RxNORM that are used worldwide in clinical care, public health, and epidemiology. In addition, the UMLS also provides a semantic network in which every concept in the Metathesaurus is represented by its Concept Unique Identifier (CUI) and is semantically typed (Bodenreider and McCray, 2003). The SemEval-2015 Task 14, Analysis of Clinical Text is the newest iteration in a series of community challenges organized around the tasks of named entity recognition for clinical texts. In SemEval-2014 Task 7 (Pradhan et al., 2014) and previous challenge 2013 (Pradhan et al., 2013), we had focused on the task of named entity recognition for disorder mentions in clinical texs, along with normalization to UMLS CUIs. This year, we shift focus on the task of identifying a series of attributes describing a disorder mention. Like for previous challenges, we use 303 Proceedings of the 9th Internat</context>
</contexts>
<marker>Bodenreider, McCray, 2003</marker>
<rawString>Olivier Bodenreider and Alexa T McCray. 2003. Exploring semantic groups through visual approaches. Journal of biomedical informatics, 36(6):414–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="18242" citStr="Brown et al., 1992" startWordPosition="2974" endWordPosition="2977"> and domain features (note type and section headers of clinical note). Lookup to dictionary (either UMLS or customized lexicon of disorders) was an essential feature for performance. To leverage further these lexicons, for instance, Xu and colleagues (Xu et al., 2015) implemented a vector-space model similarity computation to known disorders as an additional feature in their appraoch. The best-performing teams made use of the large unannotated corpus of clinical notes provided in the challenge (Pathak et al., 2015; Leal et al., 2015; Xu et al., 2015). Teams explored the use of Brown clusters (Brown et al., 1992) and word embeddings (Collobert et al., 2011). Pathak and colleagues (Pathak et al., 2015) note that word2vec (Mikolov et al., 2013) did not yield satisfactory results. Instead, they report better results clustering sentences in the unannotated texts based on their sequence of part-of-speech tags, and using the clusters as feature in the CRF. Teams continued to explore approaches for recognizing discontiguous entities. Pathak and colleagues (Pathak et al., 2015), for instance, built a specialized SVM-based classifier for that purpose. For CUI normalization, the best performing teams focused on</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Classbased n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2537</pages>
<contexts>
<context position="18287" citStr="Collobert et al., 2011" startWordPosition="2981" endWordPosition="2985">on headers of clinical note). Lookup to dictionary (either UMLS or customized lexicon of disorders) was an essential feature for performance. To leverage further these lexicons, for instance, Xu and colleagues (Xu et al., 2015) implemented a vector-space model similarity computation to known disorders as an additional feature in their appraoch. The best-performing teams made use of the large unannotated corpus of clinical notes provided in the challenge (Pathak et al., 2015; Leal et al., 2015; Xu et al., 2015). Teams explored the use of Brown clusters (Brown et al., 1992) and word embeddings (Collobert et al., 2011). Pathak and colleagues (Pathak et al., 2015) note that word2vec (Mikolov et al., 2013) did not yield satisfactory results. Instead, they report better results clustering sentences in the unannotated texts based on their sequence of part-of-speech tags, and using the clusters as feature in the CRF. Teams continued to explore approaches for recognizing discontiguous entities. Pathak and colleagues (Pathak et al., 2015), for instance, built a specialized SVM-based classifier for that purpose. For CUI normalization, the best performing teams focused on augmenting existing dictionaries with lists </context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493– 2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Leal</author>
<author>Bruno Martins</author>
<author>Francisco Couto</author>
</authors>
<title>ULisboa: Semeval</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval-2015.</booktitle>
<contexts>
<context position="18161" citStr="Leal et al., 2015" startWordPosition="2958" endWordPosition="2961">om either part-of-speech and phrase chunking information or dependency parsing, and domain features (note type and section headers of clinical note). Lookup to dictionary (either UMLS or customized lexicon of disorders) was an essential feature for performance. To leverage further these lexicons, for instance, Xu and colleagues (Xu et al., 2015) implemented a vector-space model similarity computation to known disorders as an additional feature in their appraoch. The best-performing teams made use of the large unannotated corpus of clinical notes provided in the challenge (Pathak et al., 2015; Leal et al., 2015; Xu et al., 2015). Teams explored the use of Brown clusters (Brown et al., 1992) and word embeddings (Collobert et al., 2011). Pathak and colleagues (Pathak et al., 2015) note that word2vec (Mikolov et al., 2013) did not yield satisfactory results. Instead, they report better results clustering sentences in the unannotated texts based on their sequence of part-of-speech tags, and using the clusters as feature in the CRF. Teams continued to explore approaches for recognizing discontiguous entities. Pathak and colleagues (Pathak et al., 2015), for instance, built a specialized SVM-based classif</context>
</contexts>
<marker>Leal, Martins, Couto, 2015</marker>
<rawString>Andr´e Leal, Bruno Martins, and Francisco Couto. 2015. ULisboa: Semeval 2015 - task 14 analysis of clinical text: Recognition and normalization of medical concepts. In Proceedings of SemEval-2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="18374" citStr="Mikolov et al., 2013" startWordPosition="2996" endWordPosition="2999">disorders) was an essential feature for performance. To leverage further these lexicons, for instance, Xu and colleagues (Xu et al., 2015) implemented a vector-space model similarity computation to known disorders as an additional feature in their appraoch. The best-performing teams made use of the large unannotated corpus of clinical notes provided in the challenge (Pathak et al., 2015; Leal et al., 2015; Xu et al., 2015). Teams explored the use of Brown clusters (Brown et al., 1992) and word embeddings (Collobert et al., 2011). Pathak and colleagues (Pathak et al., 2015) note that word2vec (Mikolov et al., 2013) did not yield satisfactory results. Instead, they report better results clustering sentences in the unannotated texts based on their sequence of part-of-speech tags, and using the clusters as feature in the CRF. Teams continued to explore approaches for recognizing discontiguous entities. Pathak and colleagues (Pathak et al., 2015), for instance, built a specialized SVM-based classifier for that purpose. For CUI normalization, the best performing teams focused on augmenting existing dictionaries with lists of unambiguous abbreviations (Leal et al., 2015) or by pre-processing UMLS and breaking</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parth Pathak</author>
<author>Pinal Patel</author>
<author>Vishal Panchal</author>
<author>Sagar Soni</author>
<author>Kinjal Dani</author>
<author>Narayan Choudhary</author>
<author>Amrish Patel</author>
</authors>
<title>ezDI: A semi-supervised nlp system for clinical narrative analysis.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval-2015.</booktitle>
<contexts>
<context position="17332" citStr="Pathak et al., 2015" startWordPosition="2837" endWordPosition="2840">g task. Thus, a true disorder span is defined as any overalp with a gold-stand disorder span. In the case of several predicted spans that overlap with a gold-standard span, then only one of them is chosen to be true positive (the longest ones), and the other predicted spans are considered false positives. 5 Results 5.1 Task 1 16 teams participated in Task 1. Strict and relaxed precision, recall, and F metrics are reported in Figure 1. We relied on the strict F to rank different submissions. The best system from team ezDI reported 307 75.7 strict F, also reporting the highest relaxed F (78.7) (Pathak et al., 2015). For disorder span recognition, most teams used a CRF-based approach. Features explored included traditional NER features: lexical (bag of words and bigrams, orthographic features), syntactic features derived from either part-of-speech and phrase chunking information or dependency parsing, and domain features (note type and section headers of clinical note). Lookup to dictionary (either UMLS or customized lexicon of disorders) was an essential feature for performance. To leverage further these lexicons, for instance, Xu and colleagues (Xu et al., 2015) implemented a vector-space model similar</context>
<context position="18708" citStr="Pathak et al., 2015" startWordPosition="3046" endWordPosition="3049">l notes provided in the challenge (Pathak et al., 2015; Leal et al., 2015; Xu et al., 2015). Teams explored the use of Brown clusters (Brown et al., 1992) and word embeddings (Collobert et al., 2011). Pathak and colleagues (Pathak et al., 2015) note that word2vec (Mikolov et al., 2013) did not yield satisfactory results. Instead, they report better results clustering sentences in the unannotated texts based on their sequence of part-of-speech tags, and using the clusters as feature in the CRF. Teams continued to explore approaches for recognizing discontiguous entities. Pathak and colleagues (Pathak et al., 2015), for instance, built a specialized SVM-based classifier for that purpose. For CUI normalization, the best performing teams focused on augmenting existing dictionaries with lists of unambiguous abbreviations (Leal et al., 2015) or by pre-processing UMLS and breaking down existing lexical variants to account for high paraphrasing power of disorder terms (Pathak et al., 2015). 5.2 Task 2 Six teams participated in Task 2a. Evaluation metrics are reported in Figure 2. We relied on the Weighted Accuracy (WA) to rank the teams (highlighted in the Figure is F*WA, but since in Task 2a gold-standard di</context>
</contexts>
<marker>Pathak, Patel, Panchal, Soni, Dani, Choudhary, Patel, 2015</marker>
<rawString>Parth Pathak, Pinal Patel, Vishal Panchal, Sagar Soni, Kinjal Dani, Narayan Choudhary, and Amrish Patel. 2015. ezDI: A semi-supervised nlp system for clinical narrative analysis. In Proceedings of SemEval-2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Noemie Elhadad</author>
<author>Brett R South</author>
<author>David Martinez</author>
<author>Lee Christensen</author>
<author>Amy Vogel</author>
<author>Hanna Suominen</author>
<author>Wendy Chapman</author>
<author>Guergana Savova</author>
</authors>
<title>Task 1: Share/clef ehealth evaluation lab 2013. In Online Working Notes of CLEF,</title>
<date>2013</date>
<pages>230</pages>
<contexts>
<context position="3228" citStr="Pradhan et al., 2013" startWordPosition="486" endWordPosition="489">ndard resources such as SNOMED-CT, ICD9, and RxNORM that are used worldwide in clinical care, public health, and epidemiology. In addition, the UMLS also provides a semantic network in which every concept in the Metathesaurus is represented by its Concept Unique Identifier (CUI) and is semantically typed (Bodenreider and McCray, 2003). The SemEval-2015 Task 14, Analysis of Clinical Text is the newest iteration in a series of community challenges organized around the tasks of named entity recognition for clinical texts. In SemEval-2014 Task 7 (Pradhan et al., 2014) and previous challenge 2013 (Pradhan et al., 2013), we had focused on the task of named entity recognition for disorder mentions in clinical texs, along with normalization to UMLS CUIs. This year, we shift focus on the task of identifying a series of attributes describing a disorder mention. Like for previous challenges, we use 303 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 303–310, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics the ShARe corpus1 and introduce a new set of annotations for disorder attributes. In the remainder of this paper, we describe the dat</context>
</contexts>
<marker>Pradhan, Elhadad, South, Martinez, Christensen, Vogel, Suominen, Chapman, Savova, 2013</marker>
<rawString>Sameer Pradhan, Noemie Elhadad, Brett R South, David Martinez, Lee Christensen, Amy Vogel, Hanna Suominen, Wendy Chapman, and Guergana Savova. 2013. Task 1: Share/clef ehealth evaluation lab 2013. In Online Working Notes of CLEF, page 230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
</authors>
<title>No´emie Elhadad, Wendy Chapman, Suresh Manandhar, and Guergana Savova.</title>
<date>2014</date>
<marker>Pradhan, 2014</marker>
<rawString>Sameer Pradhan, No´emie Elhadad, Wendy Chapman, Suresh Manandhar, and Guergana Savova. 2014.</rawString>
</citation>
<citation valid="true">
<title>Semeval-2014 task 7: Analysis of clinical text.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>54--62</pages>
<marker>2014</marker>
<rawString>Semeval-2014 task 7: Analysis of clinical text. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 54–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>No´emie Elhadad</author>
<author>Brett R South</author>
<author>David Martinez</author>
<author>Lee Christensen</author>
<author>Amy Vogel</author>
<author>Hanna Suominen</author>
<author>Wendy W Chapman</author>
<author>Guergana Savova</author>
</authors>
<title>Evaluating the state of the art in disorder recognition and normalization of the clinical narrative.</title>
<date>2015</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="4246" citStr="Pradhan et al., 2015" startWordPosition="650" endWordPosition="653">ado, June 4-5, 2015. c�2015 Association for Computational Linguistics the ShARe corpus1 and introduce a new set of annotations for disorder attributes. In the remainder of this paper, we describe the dataset and the annotations provided to the task participants, the subtasks comprising the overall task, and the results of the teams that participated along with notable approaches in their systems. 2 Dataset Train Dev Test Notes 298 133 100 Words 182K 153K 109K Table 1: Notes, words, and disorder distributions in the training, development, and testing sets. The dataset used is the ShARe corpus (Pradhan et al., 2015). As a whole, it consists of 531 deidentified clinical notes (a mix of discharge summaries and radiology reports) selected from the MIMIC II clinical database version 2.5 (Saeed et al., 2002). Part of the ShARe corpus was released as part of Semeval 2014 Task 7. In fact, to enable meaningful comparisons of systems performance across years, the 2015 SemEval training set combines the 2014 training and development sets, while the 2015 SemEval development set consists of the 2014 test set. The 2015 test set is a previously unseen set of clinical notes from the ShARe corpus. Table 2 provides descri</context>
</contexts>
<marker>Pradhan, Elhadad, South, Martinez, Christensen, Vogel, Suominen, Chapman, Savova, 2015</marker>
<rawString>Sameer Pradhan, No´emie Elhadad, Brett R South, David Martinez, Lee Christensen, Amy Vogel, Hanna Suominen, Wendy W Chapman, and Guergana Savova. 2015. Evaluating the state of the art in disorder recognition and normalization of the clinical narrative. Journal of the American Medical Informatics Association, 22(1):143–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Saeed</author>
<author>C Lieu</author>
<author>G Raber</author>
<author>RG Mark</author>
</authors>
<title>Mimic II: a massive temporal ICU patient database to support research in intelligent patient monitoring.</title>
<date>2002</date>
<booktitle>In Computers in Cardiology,</booktitle>
<pages>641--644</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4437" citStr="Saeed et al., 2002" startWordPosition="682" endWordPosition="685">be the dataset and the annotations provided to the task participants, the subtasks comprising the overall task, and the results of the teams that participated along with notable approaches in their systems. 2 Dataset Train Dev Test Notes 298 133 100 Words 182K 153K 109K Table 1: Notes, words, and disorder distributions in the training, development, and testing sets. The dataset used is the ShARe corpus (Pradhan et al., 2015). As a whole, it consists of 531 deidentified clinical notes (a mix of discharge summaries and radiology reports) selected from the MIMIC II clinical database version 2.5 (Saeed et al., 2002). Part of the ShARe corpus was released as part of Semeval 2014 Task 7. In fact, to enable meaningful comparisons of systems performance across years, the 2015 SemEval training set combines the 2014 training and development sets, while the 2015 SemEval development set consists of the 2014 test set. The 2015 test set is a previously unseen set of clinical notes from the ShARe corpus. Table 2 provides descriptive statistics about the different sets. In addition to the ShARe corpus annotations, task participants were provided with a large set of unlabeled deidentified clinical notes, also from MI</context>
</contexts>
<marker>Saeed, Lieu, Raber, Mark, 2002</marker>
<rawString>Mohammed Saeed, C Lieu, G Raber, and RG Mark. 2002. Mimic II: a massive temporal ICU patient database to support research in intelligent patient monitoring. In Computers in Cardiology, 2002, pages 641–644. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Xu</author>
<author>Yaoyun Zhang</author>
<author>Jingqi Wang</author>
<author>Yonghui Wu</author>
<author>Min Jian</author>
<author>Ergin Soysal</author>
<author>Hua Xu</author>
</authors>
<title>challenge - task 14.</title>
<date>2015</date>
<booktitle>UTH-CCB: The participation of the SemEval</booktitle>
<contexts>
<context position="17891" citStr="Xu et al., 2015" startWordPosition="2916" endWordPosition="2919">orting the highest relaxed F (78.7) (Pathak et al., 2015). For disorder span recognition, most teams used a CRF-based approach. Features explored included traditional NER features: lexical (bag of words and bigrams, orthographic features), syntactic features derived from either part-of-speech and phrase chunking information or dependency parsing, and domain features (note type and section headers of clinical note). Lookup to dictionary (either UMLS or customized lexicon of disorders) was an essential feature for performance. To leverage further these lexicons, for instance, Xu and colleagues (Xu et al., 2015) implemented a vector-space model similarity computation to known disorders as an additional feature in their appraoch. The best-performing teams made use of the large unannotated corpus of clinical notes provided in the challenge (Pathak et al., 2015; Leal et al., 2015; Xu et al., 2015). Teams explored the use of Brown clusters (Brown et al., 1992) and word embeddings (Collobert et al., 2011). Pathak and colleagues (Pathak et al., 2015) note that word2vec (Mikolov et al., 2013) did not yield satisfactory results. Instead, they report better results clustering sentences in the unannotated text</context>
<context position="19408" citStr="Xu et al., 2015" startWordPosition="3161" endWordPosition="3164">ormalization, the best performing teams focused on augmenting existing dictionaries with lists of unambiguous abbreviations (Leal et al., 2015) or by pre-processing UMLS and breaking down existing lexical variants to account for high paraphrasing power of disorder terms (Pathak et al., 2015). 5.2 Task 2 Six teams participated in Task 2a. Evaluation metrics are reported in Figure 2. We relied on the Weighted Accuracy (WA) to rank the teams (highlighted in the Figure is F*WA, but since in Task 2a gold-standard disorders are provided, F is 1). The best system (team UTH-CCB) yielded a WA of 88.6 (Xu et al., 2015). For Task 2b, nine teams participated. Evaluation metrics are reported in Figure 3. We relied on the combination of F score for disorder span identification and Weighted Accuracy for template filling to rank the teams (F*WA in the figure). The best system (team UTH-CCB) yielded a F*WA of 80.8. Approaches to template filling focused on building classifiers for each attribute. Specialized lexicons of trigger terms for each attribute (e.g., list of negation terms) along with distance to disorder spans was a helpful feature. Overall, like in Task 1, a range of feature types from lexical to syntac</context>
</contexts>
<marker>Xu, Zhang, Wang, Wu, Jian, Soysal, Xu, 2015</marker>
<rawString>Jun Xu, Yaoyun Zhang, Jingqi Wang, Yonghui Wu, Min Jian, Ergin Soysal, and Hua Xu. 2015. UTH-CCB: The participation of the SemEval 2015 challenge - task 14. In Proceedings of SemEval-2015.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>