<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9448485">
Utterance Segmentation Using Combined Approach
Based on Bi-directional N-gram and Maximum Entropy
</title>
<author confidence="0.954978">
Ding Liu
</author>
<affiliation confidence="0.879462333333333">
National Laboratory of Pattern Recognition
Institute of Automation
Chinese Academy of Sciences
</affiliation>
<address confidence="0.616175">
Beijing 100080, China.
</address>
<email confidence="0.896767">
dliu@nlpr.ia.ac.cn
</email>
<author confidence="0.705001">
Chengqing Zong
</author>
<affiliation confidence="0.656325666666667">
National Laboratory of Pattern Recognition
Institute of Automation
Chinese Academy of Sciences
</affiliation>
<address confidence="0.471471">
Beijing 100080, China.
</address>
<email confidence="0.797297">
cqzong@nlpr.ia.ac.cn
</email>
<note confidence="0.791878">
Output (text
or speech)
</note>
<sectionHeader confidence="0.931233" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999792227272727">
This paper proposes a new approach to
segmentation of utterances into sentences
using a new linguistic model based upon
Maximum-entropy-weighted Bi-
directional N-grams. The usual N-gram
algorithm searches for sentence bounda-
ries in a text from left to right only. Thus
a candidate sentence boundary in the text
is evaluated mainly with respect to its left
context, without fully considering its right
context. Using this approach, utterances
are often divided into incomplete sen-
tences or fragments. In order to make use
of both the right and left contexts of can-
didate sentence boundaries, we propose a
new linguistic modeling approach based
on Maximum-entropy-weighted Bi-
directional N-grams. Experimental results
indicate that the new approach signifi-
cantly outperforms the usual N-gram al-
gorithm for segmenting both Chinese and
English utterances.
</bodyText>
<sectionHeader confidence="0.992494" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999850166666667">
Due to the improvement of speech recognition
technology, spoken language user interfaces, spo-
ken dialogue systems, and speech translation sys-
tems are no longer only laboratory dreams.
Roughly speaking, such systems have the structure
shown in Figure 1.
</bodyText>
<note confidence="0.543966333333333">
Language
analysis and
generation
</note>
<figureCaption confidence="0.983332">
Figure 1. System with speech input.
</figureCaption>
<bodyText confidence="0.983274244444444">
In these systems, the language analysis module
takes the output of speech recognition as its input,
representing the current utterance exactly as pro-
nounced, without any punctuation symbols mark-
ing the boundaries of sentences. Here is an
example: ��YW2&apos;�,*00 9 ����#��
_9*2&apos;&apos;A#2&apos;9_f/ 913 V91Y7 . (this way please
please take this elevator to the ninth floor the floor
attendant will meet you at your elevator entrance
there and show you to room 913.) As the example
shows, it will be difficult for a text analysis module
to parse the input if the utterance is not segmented.
Further, the output utterance from the speech rec-
ognizer usually contains wrongly recognized
words or noise words. Thus it is crucial to segment
the utterance before further language processing.
We believe that accurate segmentation can greatly
improve the performance of language analysis
modules.
Stevenson et al. have demonstrated the difficul-
ties of text segmentation through an experiment in
which six people, educated to at least the Bache-
lor’s degree level, were required to segment into
sentences broadcast transcripts from which all
punctuation symbols had been removed. The ex-
perimental results show that humans do not always
agree on the insertion of punctuation symbols, and
that their segmentation performance is not very
good (Stevenson and Gaizauskas, 2000). Thus it is
a great challenge for computers to perform the task
Speech
recognition
Input speech
automatically. To solve this problem, many meth-
ods have been proposed, which can be roughly
classified into two categories. One approach is
based on simple acoustic criteria, such as non-
speech intervals (e.g. pauses), pitch and energy.
We can call this approach acoustic segmentation.
The other approach, which can be called linguistic
segmentation, is based on linguistic clues, includ-
ing lexical knowledge, syntactic structure, seman-
tic information etc. Acoustic segmentation can not
always work well, because utterance boundaries do
not always correspond to acoustic criteria. For ex-
</bodyText>
<note confidence="0.666313">
ample: 21 &lt;pause&gt;7W%7&lt;pause&gt;VXf *A1Y7
Z-)9-fl#&lt;pause&gt;�,O&lt;pause&gt;�,X�%7LI&apos;1T. Since
</note>
<bodyText confidence="0.993786222222222">
the simple acoustic criteria are inadequate, linguis-
tic clues play an indispensable role in utterance
segmentation, and many methods relying on them
have been proposed.
This paper proposes a new approach to linguis-
tic segmentation using a Maximum-entropy-
weighted Bi-directional N-gram-based algorithm
(MEBN). To evaluate the performance of MEBN,
we conducted experiments in both Chinese and
English. All the results show that MEBN outper-
forms the normal N-gram algorithm. The remain-
der of this paper will focus on description of our
new approach for linguistic segmentation. In Sec-
tion 2, some related work on utterance segmenta-
tion is briefly reviewed, and our motivations are
described. Section 3 describes MEBN in detail.
The experimental results are presented in Section 4.
Finally, Section 5 gives our conclusion.
</bodyText>
<sectionHeader confidence="0.974566" genericHeader="introduction">
2 Related Work and Our Motivations
2.1 Related Work
</sectionHeader>
<bodyText confidence="0.999075186915888">
Stolcke et al. (1998, 1996) proposed an approach
to detection of sentence boundaries and disfluency
locations in speech transcribed by an automatic
recognizer, based on a combination of prosodic
cues modeled by decision trees and N-gram lan-
guage models. Their N-gram language model is
mainly based on part of speech, and retains some
words which are particularly relevant to segmenta-
tion. Of course, most part-of-speech taggers re-
quire sentence boundaries to be pre-determined; so
to require the use of part-of-speech information in
utterance segmentation would risk circularity. Cet-
tolo et al.’s (1998) approach to sentence boundary
detection is somewhat similar to Stolcke et al.’s.
They applied word-based N-gram language models
to utterance segmentation, and then combined
them with prosodic models. Compared with N-
gram language models, their combined models
achieved an improvement of 0.5% and 2.3% in
precision and recall respectively.
Beeferman et al. (1998) used the CYBERPUNC
system to add intra-sentence punctuation (espe-
cially commas) to the output of an automatic
speech recognition (ASR) system. They claim that,
since commas are the most frequently used punc-
tuation symbols, their correct insertion is by far the
most helpful addition for making texts legible.
CYBERPUNC augmented a standard trigram
speech recognition model with lexical information
concerning commas, and achieved a precision of
75.6% and a recall of 65.6% when testing on 2,317
sentences from the Wall Street Journal.
Gotoh et al. (1998) applied a simple non-speech
interval model to detect sentence boundaries in
English broadcast speech transcripts. They com-
pared their results with those of N-gram language
models and found theirs far superior. However,
broadcast speech transcripts are not really spoken
language, but something more like spoken written
language. Further, radio broadcasters speak for-
mally, so that their reading pauses match sentence
boundaries quite well. It is thus understandable that
the simple non-speech interval model outperforms
the N-gram language model under these conditions;
but segmentation of natural utterances is quite dif-
ferent.
Zong et al. (2003) proposed an approach to ut-
terance segmentation aiming at improving the per-
formance of spoken language translation (SLT)
systems. Their method is based on rules which are
oriented toward key word detection, template
matching, and syntactic analysis. Since this ap-
proach is intended to facilitate translation of Chi-
nese-to-English SLT systems, it rewrites long
sentences as several simple units. Once again,
these results cannot be regarded as general-purpose
utterance segmentation. Furuse et al. (1998) simi-
larly propose an input-splitting method for translat-
ing spoken language which includes many long or
ill-formed expressions. The method splits an input
into well-balanced translation units, using a seman-
tic dictionary.
Ramaswamy et al. (1998) applied a maximum
entropy approach to the detection of command
boundaries in a conversational natural language
user interface. They considered as their features
words and their distances to potential boundaries.
They posited 400 feature functions, and trained
their weights using 3000 commands. The system
then achieved a precision of 98.2% in a test set of
1900 commands. However, command sentences
for conversational natural language user interfaces
contain much smaller vocabularies and simpler
structures than the sentences of natural spoken lan-
guage. In any case, this method has been very
helpful to us in designing our own approach to ut-
terance segmentation.
There are several additional approaches which are
not designed for utterance segmentation but which
can nevertheless provide useful ideas. For example,
Reynar et al. (1997) proposed an approach to the
disambiguation of punctuation marks. They con-
sidered only the first word to the left and right of
any potential sentence boundary, and claimed that
examining wider context was not beneficial. The
features they considered included the candidate’s
prefix and suffix; the presence of particular charac-
ters in the prefix or suffix; whether the candidate
was honorific (e.g. Mr., Dr.); and whether the can-
didate was a corporate designator (e.g. Corp.). The
system was tested on the Brown Corpus, and
achieved a precision of 98.8%. Elsewhere, Nakano
et al. (1999) proposed a method for incrementally
understanding user utterances whose semantic
boundaries were unknown. The method operated
by incrementally finding plausible sequences of
utterances that play crucial roles in the task execu-
tion of dialogues, and by utilizing beam search to
deal with the ambiguity of boundaries and with
syntactic and semantic ambiguities. Though the
method does not require utterance segmentation
before discourse processing, it employs special
rule tables for discontinuation of significant utter-
ance boundaries. Such rule tables are not easy to
maintain, and experimental results have demon-
strated only that the method outperformed the
method assuming pauses to be semantic boundaries.
</bodyText>
<subsectionHeader confidence="0.998748">
2.2 Our motivations
</subsectionHeader>
<bodyText confidence="0.999993023809524">
Though numerous methods for utterance segmen-
tation have been proposed, many problems remain
unsolved.
One remaining problem relates to the language
model. The N-gram model evaluates candidate
sentence boundaries mainly according to their left
context, and has achieved reasonably good results,
but it can’t take into account the distant right con-
text to the candidate. This is the reason that N-
gram methods often wrongly divide some long
sentences into halves or multiple segments. For
example:小王病了一个星期. The N-gram method
is likely to insert a boundary mark between “了”
and “一”, which corresponds to our everyday im-
pression that, if reading from the left and not
considering several more words to the right of the
current word, we will probably consider “小王病
了” as a whole sentence. However, we find that, if
we search the sentence boundaries from right to
left, such errors can be effectively avoided. In the
present example, we won’t consider “一个星期”
as a whole sentence, and the search will be contin-
ued until the word “小” is encountered. Accord-
ingly, in order to avoid segmentation errors made
by the normal N-gram method, we propose a re-
verse N-gram segmentation method (RN) which
does seek sentence boundaries from right to left.
Further, we simply integrate the two N-gram
methods and propose a bi-directional N-gram
method (BN), which takes into account both the
left and the right context of a candidate segmenta-
tion site. Since the relative usefulness or signifi-
cance of the two N-gram methods varies
depending on the context, we propose a method of
weighting them appropriately, using parameters
generated by a maximum entropy method which
takes as its features information about words in the
context. This is our Maximum-Entropy-Weighted
Bi-directional N-gram-based segmentation method.
We hope MEBN can retain the correct segments
discovered by the usual N-gram algorithm, yet ef-
fectively skip the wrong segments.
</bodyText>
<sectionHeader confidence="0.991891" genericHeader="method">
3 Maximum-Entropy-Weighted Bi-
</sectionHeader>
<subsectionHeader confidence="0.73387275">
directional N-gram-based Segmentation
Method
3.1 Normal N-gram Algorithm (NN) for Ut-
terance Segmentation
</subsectionHeader>
<bodyText confidence="0.993356">
Assuming that W1W2 ... Wm (where m is a natural
number) is a word sequence, we consider it as an n
order Markov chain, in which the word
Wi (1 &lt;_ i &lt;_ m) is predicted by the n-1 words to its
</bodyText>
<equation confidence="0.961875166666667">
left. Here is the corresponding formula:
P W W W W
(  |... ) (  |... )
= i
P W W W
i 1 2 i − 1 i −n+1 i−1
</equation>
<bodyText confidence="0.994243">
From this conditional probability formula for a
word, we can derive the probability of a word se-
</bodyText>
<equation confidence="0.976864818181818">
quence W1 W2 ... Wi :
P WW W P WW W P W WW W
( .. . ) ( ... ) (  |... )
= ×
1 2 i 1 2 i −1 i 1 2 i−1
Integrating the two formulas above, we get:
P WW W P WW W P W W W
( . . . ) ( ... ) (  |... )
= ×
1 2 i 1 2 i − 1 i i n
− +1 i− 1
</equation>
<bodyText confidence="0.96143075">
Let us use SB to indicate a sentence boundary
and add it to the word sequence. The value of
P(W1W2 ... W iSBW i+1) and P ( W 1 W 2 ... Wi Wi+ 1 ) will
determine whether a specific word
Wi (1 ≤ i ≤ m) is the final word of a sentence. We
say Wi is the final word of a sentence if and only
if P( W1 W2 ... W iSBW i +1) &gt; P(W1W2 ... Wi Wi+1) .
Taking the trigram as our example and consid-
ering the two cases where Wi-1 is and is not the
final word of a sentence, P(W1W2 ... WiSBWi+1)
and P(W1W2...Wi Wi+1) is computed respectively
by the following two formulas:
</bodyText>
<equation confidence="0.995683952380953">
P WW W SBW P WW SBW P SB SBW P W W SB
( ... ) ( ... ) (  |) (  |)
= × ×
1 2 i +
i 1 1 2 i i i + 1 i
.. . ) (  |) (  |)
W W P SB W W P W W SB
× ×
− 1 i+1 i
i i
−1 i i
P WW WW P WW SBW P W SBW
( ... ) ( ... ) (  |)
= ×
1 2 i i + 1 1 2 i i + 1 i
×
.. . ) (  |)
W W P W W W
−1
i i
−1 i +1 i i
</equation>
<bodyText confidence="0.999402666666667">
In the normal N-gram method, the above iterative
formulas are computed to search the sentence
boundaries from W1 to Wm.
</bodyText>
<subsectionHeader confidence="0.995515">
3.2 Reverse N-gram Algorithm (RN) for Ut-
terance Segmentation
</subsectionHeader>
<bodyText confidence="0.99979025">
In the reverse N-gram segmentation method, we
take the word sequence W1W2 ... Wm as a reverse
Markov chain in which Wi (1≤ i≤ m) is predicted
by the n-1 words to its right. That is:
</bodyText>
<equation confidence="0.995857">
P W W W W
(  |... ) (  |...
i + = i
P W W W
i m m − 1 1 i + n − 1 i + 1
</equation>
<bodyText confidence="0.987256333333333">
As in the N-gram algorithm, we compute the
occurring probability of word sequence
W1W2. . .Wm using the formula:
</bodyText>
<equation confidence="0.954295090909091">
P W W W P W W W P W W W W
( .. . ) ( ... ) (  |... )
=
m m − 1 i −1 i + ×
m m 1 i m m − 1 i+1
Then the iterative computation
formula is:
P W W W P W W W P W W W
( . . . ) ( ... ) (  |... )
= + ×
m m − 1 i m m − 1 i 1 i i + n − 1 i +1
</equation>
<bodyText confidence="0.9773355">
By adding SB to the word sequence, we say Wi
is the final word of a sentence if and only if
</bodyText>
<equation confidence="0.994061928571429">
P(WmWm−1 ... W i +1SBWi) &gt; P(WmWm−1 ... W i +1Wi) .
Similar to NN, P(WmWm−1 ... W i +1SBW i ) and
P(WmWm−1 ... W i +1Wi) are computed as follows in
the trigram:
= × ×
i + 1 i m m − 1 i + 1 i + 1 i i + 1
.. . W SBW P W W SBW P SB SBW P W W SB
) ( ... ) (  |) (  |)
+ P W W W W P SB W W P W W SB
( ... ) ( |
× ) (  |)
×
m m − 1 i i
+ 2 1
+ i i
+ 2 1
+ i i + 1
×
i + 1 i i +1
.. . ) (  |)
SBW P W SBW
+P W W W W P W W W
( ... ) ( |
× )
m m − 1 i i
+2 +1 i i i
+2 1
+
</equation>
<bodyText confidence="0.995409">
In contrast to the normal N-gram segmentation
method, we compute the above iterative formulas
to seek sentence boundaries from Wm to W1.
</bodyText>
<subsectionHeader confidence="0.9903955">
3.3 Bi-directional N-gram Algorithm for Ut-
terance Segmentation
</subsectionHeader>
<bodyText confidence="0.999085882352941">
From the iterative formulas of the normal N-gram
algorithm and the reverse N-gram algorithm, we
can see that the normal N-gram method recognizes
a candidate sentence boundary location mainly
according to its left context, while the reverse N-
gram method mainly depends on its right context.
Theoretically at least, it is reasonable to suppose
that, if we synthetically consider both the left and
the right context by integrating the NN and the RN,
the overall segmentation accuracy will be im-
proved.
Considering the word sequence W1W2 ... Wm , the
candidate sites for sentence boundaries may be
found between W1 and W2 , between W2 and
W3, ..., or between Wm −1 and Wm. The number of
candidate sites is thus m-1. We number those m-1
candidate sites 1, 2 ... m-1 in succession, and we
</bodyText>
<equation confidence="0.955745">
use Pis (i) (1≤ i≤ m −1) and
Pno (i) (1 ≤ i ≤ m −1) respectively to indicate the
</equation>
<bodyText confidence="0.99581825">
probability that the current site i really is, or is not,
a sentence boundary. Thus, to compute the word
sequence segmentation, we must compute Pis (i)
and Pno (i) for each of the m-1 candidate sites. In
the bi-directional BN, we compute Pis (i) and
Pno (i) by combining the NN results and RN re-
sults. The combination is described by the follow-
ing formulas:
</bodyText>
<equation confidence="0.885642764705882">
( ) =P i P i
( )× ( )
is NN is RN
_
( )
i P
= ()
i P
× ( )
i
BN no NN no RN
_
+
WW
1 2
P(
+
P(
WW
1 2
)
PW W
( m m −1
P W W
( m m − 1
..
.W i
+ 1
Wi)
= P W W
( m m − 1
P i
is BN
Pno
</equation>
<bodyText confidence="0.915473166666667">
where Pis _NN(i) , Pno _NN(i) denote the probabili-
ties calculated by NN which correspond to
P(W1 W2 ... W iSBW i+1) and P(W1W2 ...Wi Wi+1) in
section 3.1 respectively and Pis _ RN(i) , Pno _RN (i)
denote the probabilities calculated by RN which
correspond to P(WmWm−1 ... W i +1SBW i ) and
</bodyText>
<equation confidence="0.719670333333333">
P(WmWm−1 ... W i+1Wi) in section 3.2 respectively.
We say there exits a sentence boundary at site i
(1≤ i≤ m −1) if and only if Pis _BN (i) &gt; Pno _BN (i) .
</equation>
<subsectionHeader confidence="0.942753">
3.4 Maximum Entropy Approach for Utter-
ance Segmentation
</subsectionHeader>
<bodyText confidence="0.999950857142857">
In this section, we explain our maximum-entropy-
based model for utterance segmentation. That is,
we estimate the joint probability distribution of the
candidate sites and their surrounding words. Since
we consider information concerning the lexical
context to be useful, we define the feature func-
tions for our maximum method as follows:
</bodyText>
<equation confidence="0.982076166666667">
1 if include Suffix c S
( ( ( ), ) &amp; &amp;b ==
j 1)
f b c
2 1 ( , ) =
j  0 else
</equation>
<bodyText confidence="0.983931318181818">
Sj denotes a sequence of one or more words
which we can call the Matching String. (Note that
Sj may contain the sentence boundary mark ‘SB’.)
The candidate c’s state is denoted by b, where b=1
indicates that c is a sentence boundary and b=0
indicates that it is not a boundary. Prefix(c) de-
notes all the word sequences ending with c (that is,
c&apos;s left context plus c) and Suffix(c) denotes all the
word sequences beginning with c (in other words,
c plus its right context). For example: in the utter-
ance: 去&lt;c1&gt;机&lt;c2&gt;场&lt;c3&gt;怎&lt;c4&gt;么&lt;c5&gt;走,
‘场’, ‘机场’, and ‘去机场’ are c3’s Prefix, while
‘怎’ , ‘怎么’and ‘怎么走’ are c3’s Suffix. The
value of function include(Pr efix(c), Sj) is true
when word sequence Sj is one of c’s Prefixes, and
the value of function include(Suffix(c), Sj) is
true when Sj is one of c’s Suffixes.
Corresponding to the four feature functions
f j 1 0 (b,c),f j1 1(b,c), f j20(b,c),f j2 1(b,c) are the
four parameters αj 1 0 , α j 1 1, α j 20 , α j 2 1 . Thus the
joint probability distribution of the candidate sites
and their surrounding contexts is given by:
</bodyText>
<equation confidence="0.996053363636364">
( , ) f b c ( , ) f b c
( , )
10 j11 ( , )
j f b c j21
j20
P c b
( , )= k f b c
π 1 ( )
10 11 20 21
∏ = α ×α ×α ×α
j j j j j
</equation>
<bodyText confidence="0.999544818181818">
where k is the total number of the Matching Strings
and π is a parameter set to make P(c,1) and P(c,0)
sum to 1. The unknown parameters
αj 1 0, αj 1 1, αj 20, αj 2 1 are chosen to maximize the
likelihood of the training data using the General-
ized Iterative Scaling (Darroch and Ratcliff, 1972)
algorithm. In the maximum entropy approach, we
say that a candidate site is a sentence boundary if
and only if P(c, 1) &gt; P(c, 0). (At this point, we can
anticipate a technical problem with the maximum
approach to utterance segmentation. When a
Matching String contains SB, we cannot know
whether it belongs to the Prefixes or Suffixes of
the candidate site until the left and right contexts of
the candidate site have been segmented. Thus if the
segmentation proceeds from left to right, the lexi-
cal information in the right context of the current
candidate site will always remain uncertain. Like-
wise, if it proceeds from right to left, the informa-
tion in the left context of the current candidate site
remains uncertain. The next subsection will de-
scribe a pragmatic solution to this problem.)
</bodyText>
<subsectionHeader confidence="0.384501">
3.5 Maximum-Entropy-Weighted Bi-
</subsectionHeader>
<bodyText confidence="0.911425333333333">
directional N-gram Algorithm for Utter-
ance Segmentation
In the bi-directional N-gram based algorithm, we
have considered the left-to-right N-gram algorithm
and the right-to-left algorithm as having the same
significance. Actually, however, they should be
assigned differing weights, depending on the lexi-
cal contexts. The combination formulas are as fol-
lows:
</bodyText>
<equation confidence="0.954387307692308">
( )=W C P
( )× ( )
i W C P
× ( ) × ( )
i
n is i is NN r is i is RN
(i)W C P
= ( ) × ( )
i W C P
× ( ) × ( )
i
n no i no NN r no i no RN
Wn _is (Ci) , Wn _no (Ci) , Wr _is (Ci) , Wr _no (Ci )
</equation>
<bodyText confidence="0.996295">
are the functions of the context surrounding candi-
date site i which denotes the weights of
</bodyText>
<equation confidence="0.66435">
Pis
NN
,Pno
NN
,Pis
RN
and Pno
RN
re-
spectively. Assuming that the weights of
_
(i)
_
(i)
_
(i)
_
(i)
Pis _NN (i)
</equation>
<bodyText confidence="0.6841915">
and Pno _NN (i) depend upon the context to the left
of the candidate site, and that the weights of
</bodyText>
<equation confidence="0.751516428571429">
1 if include
fj10 (b,c) = 0)
fj1 1 (b, c) =  0 (include (Pr efix (c), Sj) &amp; &amp;b == else 1)
1 if include Suffix c S
fj20 (b, c) = 0)
( (Pr efix c S
( ), ) &amp; &amp; b ==
</equation>
<figure confidence="0.961796111111111">
j

0 else
( ( ( ), ) &amp; &amp; b ==
j

0 else
P i
is
Pno
We evaluate site i as a sentence boundary if and
only if Pis _ MEBN ( i) &gt; Pno _ MEBN ( i) .
4 Experiment
Pis_RN(i) and Pno_RN(i) depend on the context to
the right of the candidate site, the weight functions
can be rewritten as:
Wn _is (LeftCi) , Wn _no (LeftCi) , Wr _is (RightCi) ,
Wr no (RightCi) . It is reasonable to assume that as
</figure>
<page confidence="0.755792">
_
</page>
<bodyText confidence="0.850668">
the joint probability P(LeftCi,i = SB) rises,
Pis NN(i) will increase in significance. (The joint
</bodyText>
<page confidence="0.724261">
_
</page>
<bodyText confidence="0.953989">
probability in question is the probability of the cur-
rent candidate’s left context, taken together with
the probability that the candidate is a sentence
boundary.) Therefore the value of Wn _is (LeftCi)
is given by Wn is LeftCi = P LeftCi i = SB .
_ ( ) ( , )
Similarly we can give the formulas for comput-
ing Wn _no (LeftCi) , Wr _is (RightCi) , and
Wr no (RightCi) as follows:
</bodyText>
<equation confidence="0.951037733333333">
_
Wn no(LeftCi) = P(LeftCi,i!= SB )
_
Wr is RightC i P RightC i i SB
( ) (
= , =
_
W RightC P RightC i SB
( ) (
= , ! =
r no i i
_
We can easily get the values of
P( LeftCi, i = SB) ,P( LeftCi, i!= SB) ,
P(RightCi, i = SB) , and P(RightCi, i!= SB)
</equation>
<bodyText confidence="0.7849715">
using the method described in the maximum en-
tropy approach section. For example:
</bodyText>
<equation confidence="0.992554818181818">
j 1
P LeftC i SB
( , ) π α f
= = k
∏ =
i j 1 j 11
P LeftC i SB
( , ! ) π α
= = k
i ∏ =
j 1 j
</equation>
<bodyText confidence="0.9996355">
As mentioned in last subsection, we need seg-
mented contexts for maximum entropy approach.
Since the maximum entropy parameters for MEBN
algorithm are used as modifying NN and RN, we
just estimate the joint probability of the candidate
and its surrounding contexts based upon the seg-
ments by NN and RN. Using NLeftCi indicate the
left context to the candidate i which has been seg-
mented by NN algorithm and RRightCi indicate the
right context to i which has been segmented by RN,
the combination probability computing formulas
for MEBN are as follows:
</bodyText>
<figure confidence="0.804043411764706">
)
)
i
1 (1, )
10
× P(RRightC i , i=SB ) × Pis RN (i)
_
_
10
(0,
i)
fj
i
i=SB
,
PisBN (i)=P
ME
</figure>
<equation confidence="0.963180173913043">
) × Pis NN (i)
(NLeftC
=
= P NLeftC i
( i,
) × P ( )
i
no NN
P ( )
i
no MEBN
_
SB
_
!
i
) × P ( )
i
no RN
× P(RRightC
i SB
! =
,
</equation>
<subsectionHeader confidence="0.999459">
4.1 Model Training
</subsectionHeader>
<bodyText confidence="0.999764590909091">
Our models are trained on both Chinese and Eng-
lish corpora, which cover the domains of hotel res-
ervation, flight booking, traffic information,
sightseeing, daily life and so on. We replaced the
full stops with “SB” and removed all other punc-
tuation marks in the training corpora. Since in most
actual systems part of speech information cannot
be accessed before determining the sentence
boundaries, we use Chinese characters and English
words without POS tags as the units of our N-gram
models. Trigram and reverse trigram probabilities
are estimated based on the processed training cor-
pus by using Modified Kneser-Ney Smoothing
(Chen and Goodman, 1998). As to the maximum
entropy model, the Matching Strings are chosen as
all the word sequences occurring in the training
corpus whose length is no more than 3 words. The
unknown parameters corresponding to the feature
functions are generated based on the training cor-
pus using the Generalized Iterative Scaling algo-
rithm. Table 1 gives an overview of the training
corpus.
</bodyText>
<table confidence="0.9938244">
Corpus SIZE SB Num- Average Length
ber of Sentence
Chinese 4.02MB 148967 8 Chinese charac-
ters
English 4.49MB 149311 6 words
</table>
<tableCaption confidence="0.999818">
Table 1. Overview of the Training Corpus.
</tableCaption>
<subsectionHeader confidence="0.998731">
4.2 Testing Results
</subsectionHeader>
<bodyText confidence="0.9145386">
We test our methods using open corpora which are
also limited to the domains mentioned above. All
punctuation marks are removed from the test cor-
pora. An overview of the test corpus appears in
table 2.
</bodyText>
<table confidence="0.9923846">
Corpus SIZE SB Average Length
Number of Sentence
Chinese 412KB 12032 10 Chinese char-
acters
English 391KB 10518 7 words
</table>
<tableCaption confidence="0.999717">
Table 2. Overview of the Testing Corpus.
</tableCaption>
<bodyText confidence="0.969675272727273">
We have implemented four segmentation algo-
rithms using NN, RN, BN and MEBN respectively.
If we use “RightNum” to denote the number of
right segmentations, “WrongNum” denote the
number of wrong segmentations, and “TotalNum”
to denote the number of segmentations in the
original testing corpus, the precision (P) can be
computed using the formula
P=RightNum/(RightNum+WrongNum), the recall
(R) is computed as R=RightNum/TotalNum, and
2× ×
</bodyText>
<table confidence="0.934790125">
P R
the F-Score is computed as F-Score =
Methods Total Right Wrong Preci- Recall F-Score
Num Num Num sion
NN 12032 10167 2638 79.4% 84.5% 81.9%
RN 12032 10396 2615 79.9% 86.4% 83.0%
BN 12032 10528 2249 82.4% 87.5% 84.9%
MEBN 12032 10348 1587 86.7% 86.0% 86.3%
</table>
<tableCaption confidence="0.9766575">
Table 3. Experimental Results for Chinese Utter-
ance Segmentation.
</tableCaption>
<table confidence="0.999783666666667">
Methods Total Right Wrong Preci- Recall F-Score
Num Num Num sion
NN 10518 8730 3164 73.4% 83.0% 77.9%
RN 10518 9014 3351 72.9% 85.7% 78.8%
BN 10518 9056 3019 75.0% 86.1% 80.2%
MEBN 10518 8929 2403 78.8% 84.9% 81.7%
</table>
<tableCaption confidence="0.9770215">
Table 4. Experimental Results for English Utter-
ance Segmentation.
</tableCaption>
<bodyText confidence="0.99988325">
From the result tables it is clear that RN, BN, and
MEBN all outperforms the normal N-gram algo-
rithm in the F-score for both Chinese and English
utterance segmentation. MEBN achieved the best
performance which improves the precision by
7.3% and the recall by 1.5% in the Chinese ex-
periment, and improves the precision by 5.4% and
the recall by 1.9% in the English experiment.
</bodyText>
<subsectionHeader confidence="0.999764">
4.3 Result analysis
</subsectionHeader>
<bodyText confidence="0.998628631578947">
MEBN was proposed in order to maintain the cor-
rect segments of the normal N-gram algorithm
while skipping the wrong segments. In order to see
whether our original intention has been realized,
we compared the segments as determined by RN
with those determined by NN, compare the seg-
ments found by BN with those of NN and then
compare the segments found by MEBN with those
of NN. For RN, BN and MEBN, suppose TN de-
notes the number of total segmentations, CON de-
notes the number of correct segmentations
overlapping with those found by NN; SWN de-
notes the number of wrong NN segmentations
which were skipped; WNON denotes the number
of wrong segmentations not overlapping with those
of NN; and CNON denotes the number of segmen-
tations which were correct but did not overlap with
those of NN. The statistical results are listed in
Table 5 and Table 6.
</bodyText>
<table confidence="0.939374">
Methods TN CON SWN WNON CNON
RN 13011 9525 1098 1077 870
BN 12777 9906 753 355 622
MEBN 11935 9646 1274 223 678
</table>
<tableCaption confidence="0.939007">
Table 5. Chinese Utterance Segmentation Results
Comparison.
</tableCaption>
<table confidence="0.9513295">
Methods TN CON SWN WNON CNON
RN 12365 8223 1077 1271 792
BN 12075 8565 640 488 491
MEBN 11332 8370 1247 486 559
</table>
<tableCaption confidence="0.7421265">
Table 6. English Utterance Segmentation Results
Comparison.
</tableCaption>
<bodyText confidence="0.968194857142857">
Focusing upon the Chinese results, we can see that
RN skips 1098 incorrect segments found by NN,
and has 9525 correct segments in common with
those of NN. It verifies our supposition that RN
can effectively avoid some errors made by NN.
But because at the same time RN brings in 1077
new errors, RN doesn’t improve much in precision.
BN skips 753 incorrect segments and brings in 355
new segmentation errors; has 9906 correct seg-
ments in common with those of NN and brings in
622 new correct segments. So by equally integrat-
ing NN and RN, BN on one hand finds more cor-
rect segments, on the other hand brings in less
wrong segments than NN. But in skipping incor-
rect segments by NN, BN still performs worse than
RN, showing that it only exerts the error skipping
ability of RN to some extent. As for MEBN, it
skips 1274 incorrect segments and at the same time
brings in only 223 new incorrect segments. Addi-
tionally it maintains 9646 correct segments in com-
mon with those of NN and brings in 678 new
correct segments. In recall MEBN performs a little
worse than BN, but in precision it achieves a much
better performance than BN, showing that modi-
fied by the maximum entropy weights, MEBN
makes use of the error skipping ability of RN more
effectively. Further, in skipping wrong segments
by NN, MEBN even outperforms RN, which indi-
cates the weights we set on NN and RN not only
act as modifying parameters, but also have direct
beneficial affection on utterance segmentation.
.
P+R
The testing results are described in Table 3 and
Table 4.
</bodyText>
<sectionHeader confidence="0.998162" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999890538461538">
This paper proposes a reverse N-gram algorithm, a
bi-directional N-gram algorithm and a Maximum-
entropy-weighted Bi-directional N-gram algorithm
for utterance segmentation. The experimental re-
sults for both Chinese and English utterance seg-
mentation show that MEBN significantly
outperforms the usual N-gram algorithm. This is
because MEBN takes into account both the left and
right contexts of candidate sites: it integrates the
left-to-right N-gram algorithm and the right-to-left
N-gram algorithm with appropriate weights, using
clues on the sites’ lexical context, as modeled by
maximum entropy.
</bodyText>
<sectionHeader confidence="0.993964" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99989425">
This work is sponsored by the Natural Sciences
Foundation of China under grant No.60175012, as
well as supported by the National Key Fundamen-
tal Research Program (the 973 Program) of China
under the grant G1998030504.
The authors are very grateful to Dr. Mark Selig-
man for his very useful suggestions and his very
careful proofreading.
</bodyText>
<sectionHeader confidence="0.998476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999943430555556">
Beeferman D., A. Berger, and J. Lafferty. 1998.
CYBERPUNC: A lightweight punctuation annotation
system for speech. In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, Seattle, WA. pp. 689-692.
Beeferman D., A. Berger, and J. Lafferty. 1999. Statisti-
cal models for text segmentation. Machine Learning
34, pp 177-210.
Berger A., S. Della Pietra, and V. Della Pietra. 1996. A
Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics, 22(1), pp. 39-
71.
Cettolo M. and D. Falavigna. 1998. Automatic
Detection of Semantic Boundaries Based on Acoustic
and Lexical Knowledge. ICSLP 1998, pp. 1551-1554.
Chen S. F. and J. Goodman. 1998. An empirical study
of smoothing techniques for language modeling. Tech-
nical Report TR-10-98, Center for Research in Com-
puting Technology, Harvard University. pp.243-255.
Darroch J. N. and D. Ratcliff. 1972. Generalized Itera-
tive Scaling for Log-Linear Models. The Annals of
Mathematical Statistics, 43(5), pp. 1470-1480.
Furuse O., S. Yamada, and K. Yamamoto. 1998. Split-
ting Long or Ill-formed Input for Robust Spoken-
language Translation. COLING-ACL 1998, pp. 421-
427.
Gotoh Y. and S. Renals. 2000. Sentence Boundary De-
tection in Broadcast Speech Transcripts. In Proc. In-
ternational Workshop on Automatic Speech
Recognition, pp. 228-235.
Nakano M., N. Miyazaki, J. Hirasawa, K. Dohsaka, and
T. Kawabata. 1999. Understanding Unsegmented User
Utterances in Real-Time Spoken Dialogue Systems.
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-99), Col-
lege Park, MD, USA, pp. 200-207.
Ramaswamy N. G. and J. Kleindienst. 1998. Automatic
Identification of Command Boundaries in a Conversa-
tional Natural Language User Interface. ICSLP 1998.
pp. 401-404.
Reynar J. and A. Ratnaparkhi. 1997. A maximum en-
tropy approach to identifying sentence boundaries. In
Proceedings of the 5th Conference on Applications of
Natural Language Processing (ANLP), Washington
DC, pp. 16-19.
Seligman M. 2000. Nine Issues in Speech Translation.
In Machine Translation, 15, pp. 149-185.
Stevenson M. and R. Gaizauskas. 2000. Experiments on
sentence boundary detection. In Proceedings of the
Sixth Conference on Applied Natural Language Proc-
essing and the First Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pp. 24-30.
Stolcke A. and E. Shriberg. 1996. Automatic linguistic
segmentation of conversational speech. Proc. Intl.
Conf. on Spoken Language Processing, Philadelphia,
PA, vol. 2, pp. 1005-1008.
Stolcke A., E. Shriberg, R. Bates, M. Ostendorf, D.
Hakkani, M. Plauche, G. Tur, and Y. Lu. 1998. Auto-
matic Detection of Sentence Boundaries and Disfluen-
cies based on Recognized Words. Proc. Intl. Conf. on
Spoken Language Processing, Sydney, Australia, vol.
5, pp. 2247-2250.
Zong, C. and F. Ren. 2003. Chinese Utterance Segmen-
tation in Spoken Language translation. In Proceedings
of the 4th international conference on intelligent text
processing and Computational Linguistics (CICLing),
Mexico, Feb 16-22. pp. 516-525.
Zhou Y. 2001. Utterance Segmentation Based on Deci-
sion Tree. Proceedings of the 6th National joint Con-
ference on Computational Linguistics, Taiyuan, China,
pp. 246-252.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.047002">
<title confidence="0.852640666666667">Utterance Segmentation Using Combined Based on Bi-directional N-gram and Maximum Entropy Ding</title>
<affiliation confidence="0.824591666666667">National Laboratory of Pattern Institute of Chinese Academy of</affiliation>
<address confidence="0.817578">Beijing 100080,</address>
<email confidence="0.776829">dliu@nlpr.ia.ac.cn</email>
<note confidence="0.51662725">Chengqing National Laboratory of Pattern Institute of Chinese Academy of Beijing 100080, China. cqzong@nlpr.ia.ac.cn Output (text or speech)</note>
<abstract confidence="0.991519818181818">This paper proposes a new approach to segmentation of utterances into sentences using a new linguistic model based upon directional N-grams. The usual N-gram algorithm searches for sentence boundaries in a text from left to right only. Thus a candidate sentence boundary in the text is evaluated mainly with respect to its left context, without fully considering its right context. Using this approach, utterances are often divided into incomplete sentences or fragments. In order to make use of both the right and left contexts of candidate sentence boundaries, we propose a new linguistic modeling approach based on Maximum-entropy-weighted Bidirectional N-grams. Experimental results indicate that the new approach significantly outperforms the usual N-gram algorithm for segmenting both Chinese and English utterances.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>CYBERPUNC: A lightweight punctuation annotation system for speech.</title>
<date>1998</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>689--692</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="5566" citStr="Beeferman et al. (1998)" startWordPosition="827" endWordPosition="830">ich are particularly relevant to segmentation. Of course, most part-of-speech taggers require sentence boundaries to be pre-determined; so to require the use of part-of-speech information in utterance segmentation would risk circularity. Cettolo et al.’s (1998) approach to sentence boundary detection is somewhat similar to Stolcke et al.’s. They applied word-based N-gram language models to utterance segmentation, and then combined them with prosodic models. Compared with Ngram language models, their combined models achieved an improvement of 0.5% and 2.3% in precision and recall respectively. Beeferman et al. (1998) used the CYBERPUNC system to add intra-sentence punctuation (especially commas) to the output of an automatic speech recognition (ASR) system. They claim that, since commas are the most frequently used punctuation symbols, their correct insertion is by far the most helpful addition for making texts legible. CYBERPUNC augmented a standard trigram speech recognition model with lexical information concerning commas, and achieved a precision of 75.6% and a recall of 65.6% when testing on 2,317 sentences from the Wall Street Journal. Gotoh et al. (1998) applied a simple non-speech interval model t</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1998</marker>
<rawString>Beeferman D., A. Berger, and J. Lafferty. 1998. CYBERPUNC: A lightweight punctuation annotation system for speech. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, Seattle, WA. pp. 689-692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<journal>Machine Learning</journal>
<volume>34</volume>
<pages>177--210</pages>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Beeferman D., A. Berger, and J. Lafferty. 1999. Statistical models for text segmentation. Machine Learning 34, pp 177-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>39--71</pages>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger A., S. Della Pietra, and V. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1), pp. 39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cettolo</author>
<author>D Falavigna</author>
</authors>
<date>1998</date>
<booktitle>Automatic Detection of Semantic Boundaries Based on Acoustic and Lexical Knowledge. ICSLP</booktitle>
<pages>1551--1554</pages>
<marker>Cettolo, Falavigna, 1998</marker>
<rawString>Cettolo M. and D. Falavigna. 1998. Automatic Detection of Semantic Boundaries Based on Acoustic and Lexical Knowledge. ICSLP 1998, pp. 1551-1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<pages>243--255</pages>
<institution>Center for Research in Computing Technology, Harvard University.</institution>
<contexts>
<context position="23117" citStr="Chen and Goodman, 1998" startWordPosition="4352" endWordPosition="4355"> Chinese and English corpora, which cover the domains of hotel reservation, flight booking, traffic information, sightseeing, daily life and so on. We replaced the full stops with “SB” and removed all other punctuation marks in the training corpora. Since in most actual systems part of speech information cannot be accessed before determining the sentence boundaries, we use Chinese characters and English words without POS tags as the units of our N-gram models. Trigram and reverse trigram probabilities are estimated based on the processed training corpus by using Modified Kneser-Ney Smoothing (Chen and Goodman, 1998). As to the maximum entropy model, the Matching Strings are chosen as all the word sequences occurring in the training corpus whose length is no more than 3 words. The unknown parameters corresponding to the feature functions are generated based on the training corpus using the Generalized Iterative Scaling algorithm. Table 1 gives an overview of the training corpus. Corpus SIZE SB Num- Average Length ber of Sentence Chinese 4.02MB 148967 8 Chinese characters English 4.49MB 149311 6 words Table 1. Overview of the Training Corpus. 4.2 Testing Results We test our methods using open corpora which</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Chen S. F. and J. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Center for Research in Computing Technology, Harvard University. pp.243-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized Iterative Scaling for Log-Linear Models.</title>
<date>1972</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>43</volume>
<issue>5</issue>
<pages>1470--1480</pages>
<contexts>
<context position="18529" citStr="Darroch and Ratcliff, 1972" startWordPosition="3398" endWordPosition="3401">b,c),f j1 1(b,c), f j20(b,c),f j2 1(b,c) are the four parameters αj 1 0 , α j 1 1, α j 20 , α j 2 1 . Thus the joint probability distribution of the candidate sites and their surrounding contexts is given by: ( , ) f b c ( , ) f b c ( , ) 10 j11 ( , ) j f b c j21 j20 P c b ( , )= k f b c π 1 ( ) 10 11 20 21 ∏ = α ×α ×α ×α j j j j j where k is the total number of the Matching Strings and π is a parameter set to make P(c,1) and P(c,0) sum to 1. The unknown parameters αj 1 0, αj 1 1, αj 20, αj 2 1 are chosen to maximize the likelihood of the training data using the Generalized Iterative Scaling (Darroch and Ratcliff, 1972) algorithm. In the maximum entropy approach, we say that a candidate site is a sentence boundary if and only if P(c, 1) &gt; P(c, 0). (At this point, we can anticipate a technical problem with the maximum approach to utterance segmentation. When a Matching String contains SB, we cannot know whether it belongs to the Prefixes or Suffixes of the candidate site until the left and right contexts of the candidate site have been segmented. Thus if the segmentation proceeds from left to right, the lexical information in the right context of the current candidate site will always remain uncertain. Likewi</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>Darroch J. N. and D. Ratcliff. 1972. Generalized Iterative Scaling for Log-Linear Models. The Annals of Mathematical Statistics, 43(5), pp. 1470-1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Furuse</author>
<author>S Yamada</author>
<author>K Yamamoto</author>
</authors>
<title>Splitting Long or Ill-formed Input for Robust Spokenlanguage Translation. COLING-ACL</title>
<date>1998</date>
<pages>421--427</pages>
<contexts>
<context position="7272" citStr="Furuse et al. (1998)" startWordPosition="1081" endWordPosition="1084"> language model under these conditions; but segmentation of natural utterances is quite different. Zong et al. (2003) proposed an approach to utterance segmentation aiming at improving the performance of spoken language translation (SLT) systems. Their method is based on rules which are oriented toward key word detection, template matching, and syntactic analysis. Since this approach is intended to facilitate translation of Chinese-to-English SLT systems, it rewrites long sentences as several simple units. Once again, these results cannot be regarded as general-purpose utterance segmentation. Furuse et al. (1998) similarly propose an input-splitting method for translating spoken language which includes many long or ill-formed expressions. The method splits an input into well-balanced translation units, using a semantic dictionary. Ramaswamy et al. (1998) applied a maximum entropy approach to the detection of command boundaries in a conversational natural language user interface. They considered as their features words and their distances to potential boundaries. They posited 400 feature functions, and trained their weights using 3000 commands. The system then achieved a precision of 98.2% in a test se</context>
</contexts>
<marker>Furuse, Yamada, Yamamoto, 1998</marker>
<rawString>Furuse O., S. Yamada, and K. Yamamoto. 1998. Splitting Long or Ill-formed Input for Robust Spokenlanguage Translation. COLING-ACL 1998, pp. 421-427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gotoh</author>
<author>S Renals</author>
</authors>
<title>Sentence Boundary Detection in Broadcast Speech Transcripts.</title>
<date>2000</date>
<booktitle>In Proc. International Workshop on Automatic Speech Recognition,</booktitle>
<pages>228--235</pages>
<marker>Gotoh, Renals, 2000</marker>
<rawString>Gotoh Y. and S. Renals. 2000. Sentence Boundary Detection in Broadcast Speech Transcripts. In Proc. International Workshop on Automatic Speech Recognition, pp. 228-235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nakano</author>
<author>N Miyazaki</author>
<author>J Hirasawa</author>
<author>K Dohsaka</author>
<author>T Kawabata</author>
</authors>
<title>Understanding Unsegmented User Utterances in Real-Time Spoken Dialogue Systems.</title>
<date>1999</date>
<booktitle>Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-99),</booktitle>
<pages>200--207</pages>
<location>College Park, MD, USA,</location>
<contexts>
<context position="8943" citStr="Nakano et al. (1999)" startWordPosition="1337" endWordPosition="1340"> For example, Reynar et al. (1997) proposed an approach to the disambiguation of punctuation marks. They considered only the first word to the left and right of any potential sentence boundary, and claimed that examining wider context was not beneficial. The features they considered included the candidate’s prefix and suffix; the presence of particular characters in the prefix or suffix; whether the candidate was honorific (e.g. Mr., Dr.); and whether the candidate was a corporate designator (e.g. Corp.). The system was tested on the Brown Corpus, and achieved a precision of 98.8%. Elsewhere, Nakano et al. (1999) proposed a method for incrementally understanding user utterances whose semantic boundaries were unknown. The method operated by incrementally finding plausible sequences of utterances that play crucial roles in the task execution of dialogues, and by utilizing beam search to deal with the ambiguity of boundaries and with syntactic and semantic ambiguities. Though the method does not require utterance segmentation before discourse processing, it employs special rule tables for discontinuation of significant utterance boundaries. Such rule tables are not easy to maintain, and experimental resu</context>
</contexts>
<marker>Nakano, Miyazaki, Hirasawa, Dohsaka, Kawabata, 1999</marker>
<rawString>Nakano M., N. Miyazaki, J. Hirasawa, K. Dohsaka, and T. Kawabata. 1999. Understanding Unsegmented User Utterances in Real-Time Spoken Dialogue Systems. Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-99), College Park, MD, USA, pp. 200-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N G Ramaswamy</author>
<author>J Kleindienst</author>
</authors>
<title>Automatic Identification of Command Boundaries in a Conversational Natural Language User Interface. ICSLP</title>
<date>1998</date>
<pages>401--404</pages>
<marker>Ramaswamy, Kleindienst, 1998</marker>
<rawString>Ramaswamy N. G. and J. Kleindienst. 1998. Automatic Identification of Command Boundaries in a Conversational Natural Language User Interface. ICSLP 1998. pp. 401-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reynar</author>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applications of Natural Language Processing (ANLP), Washington DC,</booktitle>
<pages>16--19</pages>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Reynar J. and A. Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the 5th Conference on Applications of Natural Language Processing (ANLP), Washington DC, pp. 16-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Seligman</author>
</authors>
<title>Nine Issues in Speech Translation.</title>
<date>2000</date>
<journal>In Machine Translation,</journal>
<volume>15</volume>
<pages>149--185</pages>
<marker>Seligman, 2000</marker>
<rawString>Seligman M. 2000. Nine Issues in Speech Translation. In Machine Translation, 15, pp. 149-185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>R Gaizauskas</author>
</authors>
<title>Experiments on sentence boundary detection.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Conference on Applied Natural Language Processing and the First Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>24--30</pages>
<contexts>
<context position="2965" citStr="Stevenson and Gaizauskas, 2000" startWordPosition="439" endWordPosition="442">ent the utterance before further language processing. We believe that accurate segmentation can greatly improve the performance of language analysis modules. Stevenson et al. have demonstrated the difficulties of text segmentation through an experiment in which six people, educated to at least the Bachelor’s degree level, were required to segment into sentences broadcast transcripts from which all punctuation symbols had been removed. The experimental results show that humans do not always agree on the insertion of punctuation symbols, and that their segmentation performance is not very good (Stevenson and Gaizauskas, 2000). Thus it is a great challenge for computers to perform the task Speech recognition Input speech automatically. To solve this problem, many methods have been proposed, which can be roughly classified into two categories. One approach is based on simple acoustic criteria, such as nonspeech intervals (e.g. pauses), pitch and energy. We can call this approach acoustic segmentation. The other approach, which can be called linguistic segmentation, is based on linguistic clues, including lexical knowledge, syntactic structure, semantic information etc. Acoustic segmentation can not always work well,</context>
</contexts>
<marker>Stevenson, Gaizauskas, 2000</marker>
<rawString>Stevenson M. and R. Gaizauskas. 2000. Experiments on sentence boundary detection. In Proceedings of the Sixth Conference on Applied Natural Language Processing and the First Conference of the North American Chapter of the Association for Computational Linguistics, pp. 24-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>E Shriberg</author>
</authors>
<title>Automatic linguistic segmentation of conversational speech.</title>
<date>1996</date>
<booktitle>Proc. Intl. Conf. on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>1005--1008</pages>
<location>Philadelphia, PA,</location>
<marker>Stolcke, Shriberg, 1996</marker>
<rawString>Stolcke A. and E. Shriberg. 1996. Automatic linguistic segmentation of conversational speech. Proc. Intl. Conf. on Spoken Language Processing, Philadelphia, PA, vol. 2, pp. 1005-1008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>M Ostendorf</author>
<author>D Hakkani</author>
<author>M Plauche</author>
<author>G Tur</author>
<author>Y Lu</author>
</authors>
<title>Automatic Detection of Sentence Boundaries and Disfluencies based on Recognized Words.</title>
<date>1998</date>
<booktitle>Proc. Intl. Conf. on Spoken Language Processing,</booktitle>
<volume>5</volume>
<pages>2247--2250</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="4621" citStr="Stolcke et al. (1998" startWordPosition="686" endWordPosition="689">tional N-gram-based algorithm (MEBN). To evaluate the performance of MEBN, we conducted experiments in both Chinese and English. All the results show that MEBN outperforms the normal N-gram algorithm. The remainder of this paper will focus on description of our new approach for linguistic segmentation. In Section 2, some related work on utterance segmentation is briefly reviewed, and our motivations are described. Section 3 describes MEBN in detail. The experimental results are presented in Section 4. Finally, Section 5 gives our conclusion. 2 Related Work and Our Motivations 2.1 Related Work Stolcke et al. (1998, 1996) proposed an approach to detection of sentence boundaries and disfluency locations in speech transcribed by an automatic recognizer, based on a combination of prosodic cues modeled by decision trees and N-gram language models. Their N-gram language model is mainly based on part of speech, and retains some words which are particularly relevant to segmentation. Of course, most part-of-speech taggers require sentence boundaries to be pre-determined; so to require the use of part-of-speech information in utterance segmentation would risk circularity. Cettolo et al.’s (1998) approach to sent</context>
</contexts>
<marker>Stolcke, Shriberg, Bates, Ostendorf, Hakkani, Plauche, Tur, Lu, 1998</marker>
<rawString>Stolcke A., E. Shriberg, R. Bates, M. Ostendorf, D. Hakkani, M. Plauche, G. Tur, and Y. Lu. 1998. Automatic Detection of Sentence Boundaries and Disfluencies based on Recognized Words. Proc. Intl. Conf. on Spoken Language Processing, Sydney, Australia, vol. 5, pp. 2247-2250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zong</author>
<author>F Ren</author>
</authors>
<title>Chinese Utterance Segmentation in Spoken Language translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th international conference on intelligent text processing and Computational Linguistics (CICLing),</booktitle>
<pages>516--525</pages>
<location>Mexico,</location>
<marker>Zong, Ren, 2003</marker>
<rawString>Zong, C. and F. Ren. 2003. Chinese Utterance Segmentation in Spoken Language translation. In Proceedings of the 4th international conference on intelligent text processing and Computational Linguistics (CICLing), Mexico, Feb 16-22. pp. 516-525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhou</author>
</authors>
<title>Utterance Segmentation Based on Decision Tree.</title>
<date>2001</date>
<booktitle>Proceedings of the 6th National joint Conference on Computational Linguistics,</booktitle>
<pages>246--252</pages>
<location>Taiyuan, China,</location>
<marker>Zhou, 2001</marker>
<rawString>Zhou Y. 2001. Utterance Segmentation Based on Decision Tree. Proceedings of the 6th National joint Conference on Computational Linguistics, Taiyuan, China, pp. 246-252.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>