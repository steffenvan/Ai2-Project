<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997167">
Bilingual Co-Training for Monolingual Hyponymy-Relation Acquisition
</title>
<author confidence="0.995117">
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Torisawa
</author>
<affiliation confidence="0.987883">
Language Infrastructure Group, MASTAR Project,
National Institute of Information and Communications Technology (NICT)
</affiliation>
<address confidence="0.879868">
3-5 Hikaridai Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
</address>
<email confidence="0.999347">
{rovellia,uchimoto,torisawa}@nict.go.jp
</email>
<sectionHeader confidence="0.993908" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977222222222">
This paper proposes a novel framework
called bilingual co-training for a large-
scale, accurate acquisition method for
monolingual semantic knowledge. In
this framework, we combine the indepen-
dent processes of monolingual semantic-
knowledge acquisition for two languages
using bilingual resources to boost perfor-
mance. We apply this framework to large-
scale hyponymy-relation acquisition from
Wikipedia. Experimental results show
that our approach improved the F-measure
by 3.6–10.3%. We also show that bilin-
gual co-training enables us to build classi-
fiers for two languages in tandem with the
same combined amount of data as required
for training a single classifier in isolation
while achieving superior performance.
</bodyText>
<sectionHeader confidence="0.99002" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.990588488372093">
Acquiring and accumulating semantic knowledge
are crucial steps for developing high-level NLP
applications such as question answering, although
it remains difficult to acquire a large amount of
highly accurate semantic knowledge. This pa-
per proposes a novel framework for a large-scale,
accurate acquisition method for monolingual se-
mantic knowledge, especially for semantic rela-
tions between nominals such as hyponymy and
meronymy. We call the framework bilingual co-
training.
The acquisition of semantic relations between
nominals can be seen as a classification task of se-
mantic relations – to determine whether two nom-
inals hold a particular semantic relation (Girju et
al., 2007). Supervised learning methods, which
have often been applied to this classification task,
have shown promising results. In those methods,
however, a large amount of training data is usually
required to obtain high performance, and the high
costs of preparing training data have always been
a bottleneck.
Our research on bilingual co-training sprang
from a very simple idea: perhaps training data in a
language can be enlarged without much cost if we
translate training data in another language and add
the translation to the training data in the original
language. We also noticed that it may be possi-
ble to further enlarge the training data by trans-
lating the reliable part of the classification results
in another language. Since the learning settings
(feature sets, feature values, training data, corpora,
and so on) are usually different in two languages,
the reliable part in one language may be over-
lapped by an unreliable part in another language.
Adding the translated part of the classification re-
sults to the training data will improve the classifi-
cation results in the unreliable part. This process
can also be repeated by swapping the languages,
as illustrated in Figure 1. Actually, this is nothing
other than a bilingual version of co-training (Blum
and Mitchell, 1998).
Language 1 Language 2
</bodyText>
<figureCaption confidence="0.999651">
Figure 1: Concept of bilingual co-training
</figureCaption>
<bodyText confidence="0.999771333333333">
Let us show an example in our current task:
hyponymy-relation acquisition from Wikipedia.
Our original approach for this task was super-
</bodyText>
<figure confidence="0.988278617647059">
..... .....
Training Training
Further Enlarged
Training Data
for Language 1
Further Enlarged
Training Data
for Language 2
Training
Enlarged
Training Data
for Language 1
Classifier
Translate
reliable parts of
classification
results
Enlarged
Training Data
for Language 2
Classifier
Training
Iteration
Manually Prepared
Training Data
for Language 1
Classifier Classifier
Translate
Training reliable parts of Training
classification
results
Manually Prepared
Training Data
for Language 2
</figure>
<page confidence="0.980931">
432
</page>
<note confidence="0.999615">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 432–440,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.996379754098361">
vised learning based on the approach proposed by
Sumida et al. (2008), which was only applied for
Japanese and achieved around 80% in F-measure.
In their approach, a common substring in a hyper-
nym and a hyponym is assumed to be one strong
clue for recognizing that the two words constitute
a hyponymy relation. For example, recognizing a
proper hyponymy relation between two Japanese
words, N# (kouso meaning enzyme) and bH7K33
�N# (kasuibunkaikouso meaning hydrolase), is
relatively easy because they share a common suf-
fix: kouso. On the other hand, judging whether
their English translations (enzyme and hydrolase)
have a hyponymy relation is probably more dif-
ficult since they do not share any substrings. A
classifier for Japanese will regard the hyponymy
relation as valid with high confidence, while a
classifier for English may not be so positive. In
this case, we can compensate for the weak part of
the English classifier by adding the English trans-
lation of the Japanese hyponymy relation, which
was recognized with high confidence, to the En-
glish training data.
In addition, if we repeat this process by swap-
ping English and Japanese, further improvement
may be possible. Furthermore, the reliable parts
that are automatically produced by a classifier can
be larger than manually tailored training data. If
this is the case, the effect of adding the transla-
tion to the training data can be quite large, and the
same level of effect may not be achievable by a
reasonable amount of labor for preparing the train-
ing data. This is the whole idea.
Through a series of experiments, this paper
shows that the above idea is valid at least for one
task: large-scale monolingual hyponymy-relation
acquisition from English and Japanese Wikipedia.
Experimental results showed that our method
based on bilingual co-training improved the per-
formance of monolingual hyponymy-relation ac-
quisition about 3.6–10.3% in the F-measure.
Bilingual co-training also enables us to build clas-
sifiers for two languages in tandem with the same
combined amount of data as would be required
for training a single classifier in isolation while
achieving superior performance.
People probably expect that a key factor in the
success of this bilingual co-training is how to
translate the training data. We actually did transla-
tion by a simple look-up procedure in the existing
translation dictionaries without any machine trans-
lation systems or disambiguation processes. De-
spite this simple approach, we obtained consistent
improvement in our task using various translation
dictionaries.
This paper is organized as follows. Section 2
presents bilingual co-training, and Section 3 pre-
cisely describes our system. Section 4 describes
our experiments and presents results. Section 5
discusses related work. Conclusions are drawn
and future work is mentioned in Section 6.
</bodyText>
<sectionHeader confidence="0.985958" genericHeader="introduction">
2 Bilingual Co-Training
</sectionHeader>
<bodyText confidence="0.999180307692308">
Let 5 and T be two different languages, and let
CL be a set of class labels to be obtained as a re-
sult of learning/classification. To simplify the dis-
cussion, we assume that a class label is binary; i.e.,
the classification results are “yes” or “no.” Thus,
CL = {yes, no}. Also, we denote the set of all
nonnegative real numbers by R+.
Assume X = XS U XT is a set of instances in
languages 5 and T to be classified. In the con-
text of a hyponymy-relation acquisition task, the
instances are pairs of nominals. Then we assume
that classifier c assigns class label cl in CL and
confidence value r for assigning the label, i.e.,
c(x) = (x, cl, r), where x E X, cl E CL, and
r E R+. Note that we used support vector ma-
chines (SVMs) in our experiments and (the abso-
lute value of) the distance between a sample and
the hyperplane determined by the SVMs was used
as confidence value r. The training data are de-
noted by L C X xCL, and we denote the learning
by function LEARN; if classifier c is trained by
training data L, then c = LEARN(L). Particu-
larly, we denote the training sets for 5 and T that
are manually prepared by LS and LT, respectively.
Also, bilingual instance dictionary DBI is defined
as the translation pairs of instances in XS and XT.
Thus, DBI = {(s, t)} C XS x XT. In the case
of hyponymy-relation acquisition in English and
Japanese, (s, t) E DBI could be (s=(enzyme, hy-
drolase), t=( (meaning enzyme), )1[171311
# (meaning hydrolase))).
Our bilingual co-training is given in Figure 2. In
the initial stage, c0 S and c0T are learned with manu-
ally labeled instances LS and LT (lines 2–5). Then
cs and cT are applied to classify instances in XS
and XT (lines 6–7). Denote CRs as a set of the
classification results of cs on instances XS that is
not in Ls and is registered in DBI. Lines 10–18
describe a way of selecting from CRs newly la-
</bodyText>
<page confidence="0.968602">
433
</page>
<equation confidence="0.947040909090909">
1: i = 0
2: L0S = LS; L0T = LT
3: repeat
4: ciS := LEARN(LiS)
5: ciT := LEARN(LiT)
6: CRiS := {ciS(xS)IxS E XS,
bcl (xS, cl) E/ LiS, ]xT (xS, xT) E DBI}
7: CRiT := {ciT(xT)IxT E XT,
bcl (xT, cl) E/ LiT, ]xS (xS, xT) E DBI}
8: L(i+1)
S := LiS
</equation>
<listItem confidence="0.74505225">
9: L(i+1)T := Li T
10: for each (xS, clS, rS) E TopN(CRiS) do
11: for each xT such that (xS, xT) E DBI
and (xT, clT, rT) E CRiT do
</listItem>
<equation confidence="0.8161008">
12: if rS &gt; θ then
13: if rT &lt; θ or clS = clT then
L(i+1)
T := L(i+1)
14: T U {(xT, clS)}
</equation>
<listItem confidence="0.872029888888889">
15: end if
16: end if
17: end for
18: end for
19: for each (xT, clT, rT) E TopN(CRiT) do
20: for each xS such that (xS, xT) E DBI
and (xS, clS, rS) E CRiS do
21: if rT &gt; θ then
22: if rS &lt; θ or clS = clT then
</listItem>
<equation confidence="0.8284125">
L(i+1)
S := L(i+1)
</equation>
<listItem confidence="0.592370285714286">
23: S U {(xS,clT)}
24: end if
25: end if
26: end for
27: end for
28: i = i + 1
29: until a fixed number of iterations is reached
</listItem>
<figureCaption confidence="0.998877">
Figure 2: Pseudo-code of bilingual co-training
</figureCaption>
<bodyText confidence="0.999767263157895">
beled instances to be added to a new training set
in T. TopN(CRiS) is a set of ci S(x), whose rS
is top-N highest in CRiS. (In our experiments,
N = 900.) During the selection, ciS acts as a
teacher and ciT as a student. The teacher instructs
his student in the class label of xT, which is actu-
ally a translation of xS by bilingual instance dic-
tionary DBI, through clS only if he can do it with
a certain level of confidence, say rS &gt; θ, and
if one of two other condition meets (rT &lt; θ or
clS = clT). clS = clT is a condition to avoid
problems, especially when the student also has a
certain level of confidence in his opinion on a class
label but disagrees with the teacher: rT &gt; θ and
clS =� clT. In that case, the teacher does nothing
and ignores the instance. Condition rT &lt; θ en-
ables the teacher to instruct his student in the class
label of xT in spite of their disagreement in a class
label. If every condition is satisfied, (xT, clS) is
added to existing labeled instances L(i+1)
T .The
roles are reversed in lines 19–27 so that ciT be-
comes a teacher and ciS a student.
Similar to co-training (Blum and Mitchell,
1998), one classifier seeks another’s opinion to se-
lect new labeled instances. One main difference
between co-training and bilingual co-training is
the space of instances: co-training is based on dif-
ferent features of the same instances, and bilin-
gual co-training is based on different spaces of in-
stances divided by languages. Since some of the
instances in different spaces are connected by a
bilingual instance dictionary, they seem to be in
the same space. Another big difference lies in
the role of the two classifiers. The two classifiers
in co-training work on the same task, but those
in bilingual co-training do the same type of task
rather than the same task.
</bodyText>
<sectionHeader confidence="0.9073895" genericHeader="method">
3 Acquisition of Hyponymy Relations
from Wikipedia
</sectionHeader>
<bodyText confidence="0.996441666666667">
Our system, which acquires hyponymy relations
from Wikipedia based on bilingual co-training,
is described in Figure 3. The following three
main parts are described in this section: candidate
extraction, hyponymy-relation classification, and
bilingual instance dictionary construction.
</bodyText>
<figureCaption confidence="0.993756">
Figure 3: System architecture
</figureCaption>
<subsectionHeader confidence="0.999107">
3.1 Candidate Extraction
</subsectionHeader>
<bodyText confidence="0.954587333333333">
We follow Sumida et al. (2008) to extract
hyponymy-relation candidates from English and
Japanese Wikipedia. A layout structure is chosen
</bodyText>
<figure confidence="0.999372866666667">
Classifier in E
Wikipedia
Articles in E
Bilingual Co-Training
Labeled
instances
Hyponymy-relation
candidate extraction
Candidates
in E
Newly labeled
instances for E
Bilingual instance dictionary
Unlabeled Unlabeled
instances in E instances in J
Acquisition of
translation dictionary
Translation
dictionary
Newly labeled
instances for J
Hyponymy-relation
candidate extraction
Candidates
in J
Classifier in J
Wikipedia
Articles in J
Labeled
instances
</figure>
<page confidence="0.555396">
434
</page>
<figure confidence="0.996775285714286">
Tiger
Range
Taxonomy
Subspecies
Bengal tiger
Malayan tiger
Siberian tiger
</figure>
<figureCaption confidence="0.887856333333333">
(b) Tree structure of
Figure 4(a)
Figure 4: Wikipedia article and its layout structure
</figureCaption>
<bodyText confidence="0.995844083333333">
as a source of hyponymy relations because it can
provide a huge amount of them (Sumida et al.,
2008; Sumida and Torisawa, 2008)1, and recog-
nition of the layout structure is easy regardless of
languages. Every English and Japanese Wikipedia
article was transformed into a tree structure like
Figure 4, where layout items title, (sub)section
headings, and list items in an article were used
as nodes in a tree structure. Sumida et al. (2008)
found that some pairs consisting of a node and one
of its descendants constituted a proper hyponymy
relation (e.g., (TIGER, SIBERIAN TIGER)), and
this could be a knowledge source of hyponymy
relation acquisition. A hyponymy-relation candi-
date is then extracted from the tree structure by re-
garding a node as a hypernym candidate and all
its subordinate nodes as hyponym candidates of
the hypernym candidate (e.g., (TIGER, TAXON-
OMY) and (TIGER, SIBERIAN TIGER) from Fig-
ure 4). 39 M English hyponymy-relation candi-
dates and 10 M Japanese ones were extracted from
Wikipedia. These candidates are classified into
proper hyponymy relations and others by using the
classifiers described below.
</bodyText>
<subsectionHeader confidence="0.995096">
3.2 Hyponymy-Relation Classification
</subsectionHeader>
<bodyText confidence="0.999967444444444">
We use SVMs (Vapnik, 1995) as classifiers for
the classification of the hyponymy relations on the
hyponymy-relation candidates. Let hyper be a hy-
pernym candidate, hypo be a hyper’s hyponym
candidate, and (hyper, hypo) be a hyponymy-
relation candidate. The lexical, structure-based,
and infobox-based features of (hyper, hypo) in Ta-
ble 1 are used for building English and Japanese
classifiers. Note that 5F3–5F5 and IF were not
</bodyText>
<footnote confidence="0.67763875">
1Sumida et al. (2008) reported that they obtained 171 K,
420 K, and 1.48 M hyponymy relations from a definition sen-
tence, a category system, and a layout structure in Japanese
Wikipedia, respectively.
</footnote>
<bodyText confidence="0.997630836734694">
used in Sumida et al. (2008) but LF1–LF5 and
5F1–5F2 are the same as their feature set.
Let us provide an overview of the feature
sets used in Sumida et al. (2008). See Sum-
ida et al. (2008) for more details. Lexical fea-
tures LF1–LF5 are used to recognize the lexi-
cal evidence encoded in hyper and hypo for hy-
ponymy relations. For example, (hyper,hypo) is
often a proper hyponymy relation if hyper and
hypo share the same head morpheme or word.
In LF1 and LF2, such information is provided
along with the words/morphemes and the parts of
speech of hyper and hypo, which can be multi-
word/morpheme nouns. TagChunk (Daum´e III et
al., 2005) for English and MeCab (MeCab, 2008)
for Japanese were used to provide the lexical fea-
tures. Several simple lexical patterns2 were also
applied to hyponymy-relation candidates. For ex-
ample, “List of artists” is converted into “artists”
by lexical pattern “list of X.” Hyponymy-relation
candidates whose hypernym candidate matches
such a lexical pattern are likely to be valid (e.g.,
(List of artists, Leonardo da Vinci)). We use LF4
for dealing with these cases. If a typical or fre-
quently used section heading in a Wikipedia arti-
cle, such as “History” or “References,” is used as
a hyponym candidate in a hyponymy-relation can-
didate, the hyponymy-relation candidate is usually
not a hyponymy relation. LF5 is used to recognize
these hyponymy-relation candidates.
Structure-based features are related to the
tree structure of Wikipedia articles from which
hyponymy-relation candidate (hyper,hypo) is ex-
tracted. 5F1 provides the distance between hyper
and hypo in the tree structure. 5F2 represents the
type of layout items from which hyper and hypo
are originated. These are the feature sets used in
Sumida et al. (2008).
We also added some new items to the above
feature sets. 5F3 represents the types of tree
nodes including root, leaf, and others. For exam-
ple, (hyper,hypo) is seldom a hyponymy relation
if hyper is from a root node (or title) and hypo
is from a hyper’s child node (or section head-
ings). 5F4 and 5F5 represent the structural con-
texts of hyper and hypo in a tree structure. They
can provide evidence related to similar hyponymy-
relation candidates in the structural contexts.
An infobox-based feature, IF, is based on a
</bodyText>
<footnote confidence="0.8132845">
2We used the same Japanese lexical patterns in Sumida et
al. (2008) to build English lexical patterns with them.
</footnote>
<figure confidence="0.5983425">
(a) Layout structure
of article TIGER
</figure>
<page confidence="0.865671">
435
</page>
<table confidence="0.42874575">
Type Description Example
LF, Morphemes/words hyper: tiger*, hypo: Siberian, hypo: tiger*
LF2 POS of morphemes/words hyper: NN*, hypo: NP, hypo: NN*
LF3 hyper and hypo, themselves hyper: Tiger, hypo: Siberian tiger
LF4 Used lexical patterns hyper: “List of X”, hypo: “Notable X”
LF5 Typical section headings hyper: History, hypo: Reference
SF, Distance between hyper and hypo 3
SF2 Type of layout items hyper: title, hypo: bulleted list
SF3 Type of tree nodes hyper: root node, hypo: leaf node
SF4 LF, and LF3 of hypo’s parent node LF3:Subspecies
SF5 LF, and LF3 of hyper’s child node LF3: Taxonomy
IF Semantic properties of hyper and hypo hyper: (taxobox,species), hypo: (taxobox,name)
</table>
<tableCaption confidence="0.87953">
Table 1: Feature type and its value. ∗ in LF, and LF2 represent the head morpheme/word and its POS.
Except those in LF4 and LF5, examples are derived from (TIGER, SIBERIAN TIGER) in Figure 4.
</tableCaption>
<bodyText confidence="0.999811181818182">
Wikipedia infobox, a special kind of template, that
describes a tabular summary of an article subject
expressed by attribute-value pairs. An attribute
type coupled with the infobox name to which it
belongs provides the semantic properties of its
value that enable us to easily understand what
the attribute value means (Auer and Lehmann,
2007; Wu and Weld, 2007). For example, in-
fobox template City Japan in Wikipedia article
Kyoto contains several attribute-value pairs such
as “Mayor=Daisaku Kadokawa” as attribute=its
value. What Daisaku Kadokawa, the attribute
value of mayor in the example, represents is hard
to understand alone if we lack knowledge, but
its attribute type, mayor, gives a clue–Daisaku
Kadokawa is a mayor related to Kyoto. These
semantic properties enable us to discover seman-
tic evidence for hyponymy relations. We ex-
tract triples (infobox name, attribute type, attribute
value) from the Wikipedia infoboxes and encode
such information related to hyper and hypo in our
feature set IF.3
</bodyText>
<subsectionHeader confidence="0.8304975">
3.3 Bilingual Instance Dictionary
Construction
</subsectionHeader>
<bodyText confidence="0.636306">
Multilingual versions of Wikipedia articles are
connected by cross-language links and usually
have titles that are bilinguals of each other (Erd-
mann et al., 2008). English and Japanese articles
connected by a cross-language link are extracted
from Wikipedia, and their titles are regarded as
translation pairs4. The translation pairs between
3We obtained 1.6 M object-attribute-value triples in
Japanese and 5.9 M in English.
</bodyText>
<page confidence="0.64187">
4197 K translation pairs were extracted.
</page>
<bodyText confidence="0.9995526">
English and Japanese terms are used for building
bilingual instance dictionary DBI for hyponymy-
relation acquisition, where DBI is composed of
translation pairs between English and Japanese
hyponymy-relation candidates5.
</bodyText>
<sectionHeader confidence="0.999585" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999941">
We used the MAY 2008 version of English
Wikipedia and the JUNE 2008 version of
Japanese Wikipedia for our experiments. 24,000
hyponymy-relation candidates, randomly selected
in both languages, were manually checked to build
training, development, and test sets6. Around
8,000 hyponymy relations were found in the man-
ually checked data for both languages7. 20,000 of
the manually checked data were used as a train-
ing set for training the initial classifier. The rest
were equally divided into development and test
sets. The development set was used to select the
optimal parameters in bilingual co-training and the
test set was used to evaluate our system.
We used TinySVM (TinySVM, 2002) with a
polynomial kernel of degree 2 as a classifier. The
maximum iteration number in the bilingual co-
training was set as 100. Two parameters, 0 and
TopN, were selected through experiments on the
development set. 0 = 1 and TopN=900 showed
</bodyText>
<footnote confidence="0.998224444444444">
5We also used redirection links in English and Japanese
Wikipedia for recognizing the variations of terms when we
built a bilingual instance dictionary with Wikipedia cross-
language links.
6It took about two or three months to check them in each
language.
7Regarding a hyponymy relation as a positive sample and
the others as a negative sample for training SVMs, “positive
sample:negative sample” was about 8,000:16,000=1:2
</footnote>
<page confidence="0.999078">
436
</page>
<bodyText confidence="0.9999586875">
the best performance and were used as the optimal
parameter in the following experiments.
We conducted three experiments to show ef-
fects of bilingual co-training, training data size,
and bilingual instance dictionaries. In the first two
experiments, we experimented with a bilingual in-
stance dictionary derived from Wikipedia cross-
language links. Comparison among systems based
on three different bilingual instance dictionaries is
shown in the third experiment.
Precision (P), recall (R), and F1-measure (F1),
as in Eq (1), were used as the evaluation measures,
where Rel represents a set of manually checked
hyponymy relations and HRbyS represents a set
of hyponymy-relation candidates classified as hy-
ponymy relations by the system:
</bodyText>
<equation confidence="0.999938666666667">
P = jRel n HRbySj/jHRbySj (1)
R = jRel n HRbySj/jRelj
F1 = 2 x (P x R)/(P + R)
</equation>
<subsectionHeader confidence="0.938744">
4.1 Effect of Bilingual Co-Training
</subsectionHeader>
<table confidence="0.998349833333333">
ENGLISH JAPANESE
P R F1 P R F1
SYT 78.5 63.8 70.4 75.0 77.4 76.1
INIT 77.9 67.4 72.2 74.5 78.5 76.6
TRAN 76.8 70.3 73.4 76.7 79.3 78.0
BICO 78.0 83.7 80.7 78.3 85.2 81.6
</table>
<tableCaption confidence="0.999657">
Table 2: Performance of different systems (%)
</tableCaption>
<bodyText confidence="0.999645363636364">
Table 2 shows the comparison results of the four
systems. SYT represents the Sumida et al. (2008)
system that we implemented and tested with the
same data as ours. INIT is a system based on ini-
tial classifier c0 in bilingual co-training. We trans-
lated training data in one language by using our
bilingual instance dictionary and added the trans-
lation to the existing training data in the other
language like bilingual co-training did. The size
of the English and Japanese training data reached
20,729 and 20,486. We trained initial classifier c0
with the new training data. TRAN is a system
based on the classifier. BICO is a system based
on bilingual co-training.
For Japanese, SYT showed worse performance
than that reported in Sumida et al. (2008), proba-
bly due to the difference in training data size (ours
is 20,000 and Sumida et al. (2008) was 29,900).
The size of the test data was also different – ours
is 2,000 and Sumida et al. (2008) was 1,000.
Comparison between INIT and SYT shows the
effect of SF3–SF5 and IF, newly introduced
feature types, in hyponymy-relation classification.
INIT consistently outperformed SYT, although the
difference was merely around 0.5–1.8% in F1.
BICO showed significant performance im-
provement (around 3.6–10.3% in F1) over SYT,
INIT, and TRAN regardless of the language. Com-
parison between TRAN and BICO showed that
bilingual co-training is useful for enlarging the
training data and that the performance gain by
bilingual co-training cannot be achieved by sim-
ply translating the existing training data.
</bodyText>
<figure confidence="0.647452">
Training Data (103)
</figure>
<figureCaption confidence="0.901602">
Figure 5: F1 curves based on the increase of train-
ing data size during bilingual co-training
</figureCaption>
<bodyText confidence="0.934714428571429">
Figure 5 shows F1 curves based on the size
of the training data including those manually tai-
lored and automatically obtained through bilin-
gual co-training. The curve starts from 20,000 and
ends around 55,000 in Japanese and 62,000 in En-
glish. As the training data size increases, the F1
curves tend to go upward in both languages. This
indicates that the two classifiers cooperate well
to boost their performance through bilingual co-
training.
We recognized 5.4 M English and 2.41 M
Japanese hyponymy relations from the classifi-
cation results of BICO on all hyponymy-relation
candidates in both languages.
</bodyText>
<subsectionHeader confidence="0.999323">
4.2 Effect of Training Data Size
</subsectionHeader>
<bodyText confidence="0.9999115">
We performed two tests to investigate the effect of
the training data size on bilingual co-training. The
first test posed the following question: “If we build
2n training samples by hand and the building cost
is the same in both languages, which is better from
the monolingual aspects: 2n monolingual training
samples or n bilingual training samples?” Table 3
and Figure 6 show the results.
</bodyText>
<figure confidence="0.998530555555555">
20 25 30 35 40 45 50 55 60
F1
81
79
77
75
73
English
Japanese
</figure>
<page confidence="0.99721">
437
</page>
<bodyText confidence="0.998632214285714">
In INIT-E and INIT-J, a classifier in each lan-
guage, which was trained with 2n monolingual
training samples, did not learn through bilingual
co-training. In BICO-E and BICO-J, bilingual co-
training was applied to the initial classifiers trained
with n training samples in both languages. As
shown in Table 3, BICO, with half the size of the
training samples used in INIT, always performed
better than INIT in both languages. This indicates
that bilingual co-training enables us to build clas-
sifiers for two languages in tandem with the same
combined amount of data as required for training
a single classifier in isolation while achieving su-
perior performance.
</bodyText>
<subsectionHeader confidence="0.496258">
Training Data Size
</subsectionHeader>
<figureCaption confidence="0.959188">
Figure 6: F1 based on training data size:
with/without bilingual co-training
</figureCaption>
<table confidence="0.9986804">
n 2n n
INIT-E INIT-J BICO-E BICO-J
2500 67.3 72.3 70.5 73.0
5000 69.2 74.3 74.6 76.9
10000 72.2 76.6 76.9 78.6
</table>
<tableCaption confidence="0.980832">
Table 3: F1 based on training data size:
with/without bilingual co-training (%)
</tableCaption>
<bodyText confidence="0.9981973125">
The second test asked: “Can we always im-
prove performance through bilingual co-training
with one strong and one weak classifier?” If the
answer is yes, then we can apply our framework
to acquisition of hyponymy-relations in other lan-
guages, i.e., German and French, without much
effort for preparing a large amount of training
data, because our strong classifier in English or
Japanese can boost the performance of a weak
classifier in other languages.
To answer the question, we tested the perfor-
mance of classifiers by using all training data
(20,000) for a strong classifier and by changing the
training data size of the other from 1,000 to 15,000
({1,000, 5,000, 10,000, 15,000}) for a weak clas-
sifier.
</bodyText>
<table confidence="0.999682">
INIT-E BICO-E INIT-J BICO-J
1,000 72.2 79.6 64.0 72.7
5,000 72.2 79.6 73.1 75.3
10,000 72.2 79.8 74.3 79.0
15,000 72.2 80.4 77.0 80.1
</table>
<tableCaption confidence="0.984086">
Table 4: F1 based on training data size: when En-
glish classifier is strong one
</tableCaption>
<table confidence="0.999943">
INIT-E BICO-E INIT-J BICO-J
1,000 60.3 69.7 76.6 79.3
5,000 67.3 74.6 76.6 79.6
10,000 69.2 77.7 76.6 80.1
15,000 71.0 79.3 76.6 80.6
</table>
<tableCaption confidence="0.9385295">
Table 5: F1 based on training data size: when
Japanese classifier is strong one
</tableCaption>
<bodyText confidence="0.999268416666667">
Tables 4 and 5 show the results, where “INIT”
represents a system based on the initial classifier
in each language and “BICO” represents a sys-
tem based on bilingual co-training. The results
were encouraging because the classifiers showed
better performance than their initial ones in every
setting. In other words, a strong classifier always
taught a weak classifier well, and the strong one
also got help from the weak one, regardless of the
size of the training data with which the weaker one
learned. The test showed that bilingual co-training
can work well if we have one strong classifier.
</bodyText>
<subsectionHeader confidence="0.999804">
4.3 Effect of Bilingual Instance Dictionaries
</subsectionHeader>
<bodyText confidence="0.9999868">
We tested our method with different bilingual in-
stance dictionaries to investigate their effect. We
built bilingual instance dictionaries based on dif-
ferent translation dictionaries whose translation
entries came from different domains (i.e., gen-
eral domain, technical domain, and Wikipedia)
and had a different degree of translation ambigu-
ity. In Table 6, D1 and D2 correspond to sys-
tems based on a bilingual instance dictionary de-
rived from two handcrafted translation dictionar-
ies, EDICT (Breen, 2008) (a general-domain dic-
tionary) and “The Japan Science and Technology
Agency Dictionary,” (a translation dictionary for
technical terms) respectively. D3, which is the
same as BICO in Table 2, is based on a bilingual
</bodyText>
<figure confidence="0.987449666666666">
2500 5000 7500 10000 15000 20000
F1
81
79
77
75
73
71
69
67
65
INIT-E
INIT-J
BICO-E
BICO-J
</figure>
<page confidence="0.993751">
438
</page>
<table confidence="0.999441666666667">
DIC F1 DIC STATISTICS
TYPE
E J ENTRY E2J J2E
D1 α=5 76.5 78.4 588K 1.80 1.77
D1 ALL 75.0 77.2 990K 7.17 2.52
D2 α=5 76.9 78.5 667K 1.89 1.55
D2 ALL 77.0 77.9 750K 3.05 1.71
D3 α=5 80.7 81.6 197K 1.03 1.02
D3 ALL 80.7 81.6 197K 1.03 1.02
</table>
<bodyText confidence="0.998269176470588">
instance dictionary derived from Wikipedia. EN-
TRY represents the number of translation dictio-
nary entries used for building a bilingual instance
dictionary. E2J (or J2E) represents the average
translation ambiguities of English (or Japanese)
terms in the entries. To show the effect of these
translation ambiguities, we used each dictionary
under two different conditions, α=5 and ALL. α=5
represents the condition where only translation en-
tries with less than five translation ambiguities are
used; ALL represents no restriction on translation
ambiguities.
Table 6: Effect of different bilingual instance dic-
tionaries
The results showed that D3 was the best and
that the performances of the others were sim-
ilar to each other. The differences in the F1
scores between α=5 and ALL were relatively small
within the same system triggered by translation
ambiguities. The performance gap between D3
and the other systems might explain the fact that
both hyponymy-relation candidates and the trans-
lation dictionary used in D3 were extracted from
the same dataset (i.e., Wikipedia), and thus the
bilingual instance dictionary built with the trans-
lation dictionary in D3 had better coverage of
the Wikipedia entries consisting of hyponymy-
relation candidates than the other bilingual in-
stance dictionaries. Although D1 and D2 showed
lower performance than D3, the experimental re-
sults showed that bilingual co-training was always
effective no matter which dictionary was used
(Note that F1 of INIT in Table 2 was 72.2 in En-
glish and 76.6 in Japanese.)
</bodyText>
<sectionHeader confidence="0.999977" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999979151515152">
Li and Li (2002) proposed bilingual bootstrapping
for word translation disambiguation. Similar to
bilingual co-training, classifiers for two languages
cooperated in learning with bilingual resources in
bilingual bootstrapping. However, the two clas-
sifiers in bilingual bootstrapping were for a bilin-
gual task but did different tasks from the monolin-
gual viewpoint. A classifier in each language is for
word sense disambiguation, where a class label (or
word sense) is different based on the languages.
On the contrary, classifiers in bilingual co-training
cooperate in doing the same type of tasks.
Bilingual resources have been used for mono-
lingual tasks including verb classification and
noun phrase semantic interpolation (Merlo et al.,
2002; Girju, 2006). However, unlike ours, their fo-
cus was limited to bilingual features for one mono-
lingual classifier based on supervised learning.
Recently, there has been increased interest in se-
mantic relation acquisition from corpora. Some
regarded Wikipedia as the corpora and applied
hand-crafted or machine-learned rules to acquire
semantic relations (Herbelot and Copestake, 2006;
Kazama and Torisawa, 2007; Ruiz-casado et al.,
2005; Nastase and Strube, 2008; Sumida et al.,
2008; Suchanek et al., 2007). Several researchers
who participated in SemEval-07 (Girju et al.,
2007) proposed methods for the classification of
semantic relations between simple nominals in
English sentences. However, the previous work
seldom considered the bilingual aspect of seman-
tic relations in the acquisition of monolingual se-
mantic relations.
</bodyText>
<sectionHeader confidence="0.999578" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99999495">
We proposed a bilingual co-training approach and
applied it to hyponymy-relation acquisition from
Wikipedia. Experiments showed that bilingual
co-training is effective for improving the perfor-
mance of classifiers in both languages. We fur-
ther showed that bilingual co-training enables us
to build classifiers for two languages in tandem,
outperforming classifiers trained individually for
each language while requiring no more training
data in total than a single classifier trained in iso-
lation.
We showed that bilingual co-training is also
helpful for boosting the performance of a weak
classifier in one language with the help of a strong
classifier in the other language without lowering
the performance of either classifier. This indicates
that the framework can reduce the cost of prepar-
ing training data in new languages with the help of
our English and Japanese strong classifiers. Our
future work focuses on this issue.
</bodyText>
<page confidence="0.999086">
439
</page>
<sectionHeader confidence="0.995844" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998935477272727">
S¨oren Auer and Jens Lehmann. 2007. What have
Innsbruck and Leipzig in common? Extracting se-
mantics from wiki content. In Proc. of the 4th
European Semantic Web Conference (ESWC 2007),
pages 503–517. Springer.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT’
98: Proceedings of the eleventh annual conference
on Computational learning theory, pages 92–100.
Jim Breen. 2008. EDICT Japanese/English dictionary
file, The Electronic Dictionary Research and Devel-
opment Group, Monash University.
Hal Daum´e III, John Langford, and Daniel Marcu.
2005. Search-based structured prediction as classi-
fication. In Proc. ofNIPS Workshop on Advances in
Structured Learningfor Text and Speech Processing,
Whistler, Canada.
Maike Erdmann, Kotaro Nakayama, Takahiro Hara,
and Shojiro Nishio. 2008. A bilingual dictionary
extracted from the Wikipedia link structure. In Proc.
ofDASFAA, pages 686–689.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic re-
lations between nominals. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 13–18.
Roxana Girju. 2006. Out-of-context noun phrase se-
mantic interpretation with cross-linguistic evidence.
In CIKM ’06: Proceedings of the 15th ACM inter-
national conference on Information and knowledge
management, pages 268–276.
Aurelie Herbelot and Ann Copestake. 2006. Acquir-
ing ontological relationships from Wikipedia using
RMRS. In Proc. of the ISWC 2006 Workshop on
Web Content Mining with Human Language Tech-
nologies.
Jun’ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proc. of Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 698–707.
Cong Li and Hang Li. 2002. Word translation disam-
biguation using bilingual bootstrapping. In Proc. of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 343–351.
MeCab. 2008. MeCab: Yet another part-of-speech
and morphological analyzer. http://mecab.
sourceforge.net/.
Paola Merlo, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. 2002. A multilingual paradigm
for automatic verb classification. In Proc. of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 207–214.
Vivi Nastase and Michael Strube. 2008. Decoding
Wikipedia categories for knowledge acquisition. In
Proc. ofAAAI 08, pages 1219–1224.
Maria Ruiz-casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic extraction of semantic
relationships for Wordnet by means of pattern learn-
ing from Wikipedia. In Proc. of NLDB, pages 67–
79. Springer Verlag.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proc. of the 16th international conference
on World Wide Web, pages 697–706.
Asuka Sumida and Kentaro Torisawa. 2008. Hack-
ing Wikipedia for hyponymy relation acquisition. In
Proc. of the Third International Joint Conference
on Natural Language Processing (IJCNLP), pages
883–888, January.
Asuka Sumida, Naoki Yoshinaga, and Kentaro Tori-
sawa. 2008. Boosting precision and recall of hy-
ponymy relation acquisition from hierarchical lay-
outs in Wikipedia. In Proceedings of the 6th In-
ternational Conference on Language Resources and
Evaluation.
TinySVM. 2002. http://chasen.org/˜taku/
software/TinySVM.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying Wikipedia. In CIKM ’07: Proceedings
of the sixteenth ACM conference on Conference on
information and knowledge management, pages 41–
50.
</reference>
<page confidence="0.998149">
440
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998803">Bilingual Co-Training for Monolingual Hyponymy-Relation Acquisition</title>
<author confidence="0.989382">Jong-Hoon Oh</author>
<author confidence="0.989382">Kiyotaka Uchimoto</author>
<author confidence="0.989382">Kentaro Torisawa</author>
<affiliation confidence="0.995631">Language Infrastructure Group, MASTAR Project, National Institute of Information and Communications Technology (NICT)</affiliation>
<address confidence="0.924421">3-5 Hikaridai Seika-cho, Soraku-gun, Kyoto 619-0289 Japan</address>
<abstract confidence="0.997076597014925">This paper proposes a novel framework co-training a largescale, accurate acquisition method for knowledge. In this framework, we combine the independent processes of monolingual semanticknowledge acquisition for two languages using bilingual resources to boost performance. We apply this framework to largescale hyponymy-relation acquisition from Wikipedia. Experimental results show that our approach improved the F-measure by 3.6–10.3%. We also show that bilingual co-training enables us to build classifiers for two languages in tandem with the same combined amount of data as required for training a single classifier in isolation while achieving superior performance. 1 Motivation Acquiring and accumulating semantic knowledge are crucial steps for developing high-level NLP applications such as question answering, although it remains difficult to acquire a large amount of highly accurate semantic knowledge. This paper proposes a novel framework for a large-scale, accurate acquisition method for monolingual semantic knowledge, especially for semantic relations between nominals such as hyponymy and We call the framework co- The acquisition of semantic relations between nominals can be seen as a classification task of semantic relations – to determine whether two nominals hold a particular semantic relation (Girju et al., 2007). Supervised learning methods, which have often been applied to this classification task, have shown promising results. In those methods, however, a large amount of training data is usually required to obtain high performance, and the high costs of preparing training data have always been a bottleneck. Our research on bilingual co-training sprang from a very simple idea: perhaps training data in a language can be enlarged without much cost if we translate training data in another language and add the translation to the training data in the original language. We also noticed that it may be possible to further enlarge the training data by translating the reliable part of the classification results in another language. Since the learning settings (feature sets, feature values, training data, corpora, and so on) are usually different in two languages, the reliable part in one language may be overlapped by an unreliable part in another language. Adding the translated part of the classification results to the training data will improve the classification results in the unreliable part. This process can also be repeated by swapping the languages, as illustrated in Figure 1. Actually, this is nothing other than a bilingual version of co-training (Blum and Mitchell, 1998). Language 1 Language 2 1: Concept of co-training Let us show an example in our current task: hyponymy-relation acquisition from Wikipedia. original approach for this task was super- ..... .....</abstract>
<title confidence="0.97628475">Training Training Further Enlarged Training Data for Language 1 Further Enlarged Training Data for Language 2 Training Enlarged Training Data for Language 1 Classifier</title>
<abstract confidence="0.8461166">Translate reliable parts of classification results Enlarged</abstract>
<title confidence="0.8626378">Training Data for Language 2 Classifier Training Iteration Manually Prepared Training Data for Language 1 Classifier Classifier Translate Training reliable parts of Training classification results Manually Prepared Training Data</title>
<abstract confidence="0.99056352406417">for Language 2 432 of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the pages 432–440, Singapore, 2-7 August 2009. ACL and AFNLP vised learning based on the approach proposed by Sumida et al. (2008), which was only applied for Japanese and achieved around 80% in F-measure. In their approach, a common substring in a hypernym and a hyponym is assumed to be one strong clue for recognizing that the two words constitute a hyponymy relation. For example, recognizing a proper hyponymy relation between two Japanese and is relatively easy because they share a common suf- On the other hand, judging whether English translations have a hyponymy relation is probably more difficult since they do not share any substrings. A classifier for Japanese will regard the hyponymy relation as valid with high confidence, while a classifier for English may not be so positive. In this case, we can compensate for the weak part of the English classifier by adding the English translation of the Japanese hyponymy relation, which was recognized with high confidence, to the English training data. In addition, if we repeat this process by swapping English and Japanese, further improvement may be possible. Furthermore, the reliable parts that are automatically produced by a classifier can be larger than manually tailored training data. If this is the case, the effect of adding the translation to the training data can be quite large, and the same level of effect may not be achievable by a reasonable amount of labor for preparing the training data. This is the whole idea. Through a series of experiments, this paper shows that the above idea is valid at least for one task: large-scale monolingual hyponymy-relation acquisition from English and Japanese Wikipedia. Experimental results showed that our method based on bilingual co-training improved the performance of monolingual hyponymy-relation acquisition about 3.6–10.3% in the F-measure. Bilingual co-training also enables us to build classifiers for two languages in tandem with the same combined amount of data as would be required for training a single classifier in isolation while achieving superior performance. People probably expect that a key factor in the success of this bilingual co-training is how to translate the training data. We actually did translation by a simple look-up procedure in the existing translation dictionaries without any machine translation systems or disambiguation processes. Despite this simple approach, we obtained consistent improvement in our task using various translation dictionaries. This paper is organized as follows. Section 2 presents bilingual co-training, and Section 3 precisely describes our system. Section 4 describes our experiments and presents results. Section 5 discusses related work. Conclusions are drawn and future work is mentioned in Section 6. 2 Bilingual Co-Training two different languages, and let a set of class labels to be obtained as a result of learning/classification. To simplify the discussion, we assume that a class label is binary; i.e., the classification results are “yes” or “no.” Thus, Also, we denote the set of all real numbers by a set of instances in be classified. In the context of a hyponymy-relation acquisition task, the instances are pairs of nominals. Then we assume classifier class label value assigning the label, i.e., = cl, where and Note that we used support vector machines (SVMs) in our experiments and (the absolute value of) the distance between a sample and the hyperplane determined by the SVMs was used confidence value The training data are deby and we denote the learning function if classifier trained by data then Particuwe denote the training sets for manually prepared by respectively. bilingual instance dictionary defined the translation pairs of instances in C In the case of hyponymy-relation acquisition in English and be hyt=( (meaning Our bilingual co-training is given in Figure 2. In initial stage, learned with manulabeled instances 2–5). Then applied to classify instances in 6–7). Denote a set of the results of instances is in is registered in Lines 10–18 a way of selecting from la- 433 i 0 2: = = 3: repeat 4: := 5: := 6: := 7: := 8: 9: T for for that do if θ if θ T 15: end if 16: end if 17: end for 18: end for for for that do if θ if θ S 24: end if 25: end if 26: end for 27: end for i 1 until fixed number of iterations is reached 2: Pseudo-code of co-training beled instances to be added to a new training set a set of whose top-N highest in (In our experiments, During the selection, acts as a and as a student. The teacher instructs student in the class label of which is actua translation of bilingual instance dicthrough if he can do it with certain level of confidence, say and one of two other condition meets θ a condition to avoid problems, especially when the student also has a certain level of confidence in his opinion on a class but disagrees with the teacher: θ In that case, the teacher does nothing ignores the instance. Condition θ enables the teacher to instruct his student in the class of spite of their disagreement in a class If every condition is satisfied, to existing labeled instances are reversed in lines 19–27 so that bea teacher and a student. Similar to co-training (Blum and Mitchell, 1998), one classifier seeks another’s opinion to select new labeled instances. One main difference between co-training and bilingual co-training is the space of instances: co-training is based on different features of the same instances, and bilingual co-training is based on different spaces of instances divided by languages. Since some of the instances in different spaces are connected by a bilingual instance dictionary, they seem to be in the same space. Another big difference lies in the role of the two classifiers. The two classifiers in co-training work on the same task, but those bilingual co-training do same type of rather than the same task. 3 Acquisition of Hyponymy Relations from Wikipedia Our system, which acquires hyponymy relations from Wikipedia based on bilingual co-training, is described in Figure 3. The following three main parts are described in this section: candidate extraction, hyponymy-relation classification, and bilingual instance dictionary construction. Figure 3: System architecture 3.1 Candidate Extraction We follow Sumida et al. (2008) to extract hyponymy-relation candidates from English and Japanese Wikipedia. A layout structure is chosen</abstract>
<title confidence="0.834558066666667">Classifier in E Wikipedia Articles in E Bilingual Co-Training Labeled instances Hyponymy-relation candidate extraction Candidates in E Newly labeled instances for E Bilingual instance dictionary Unlabeled Unlabeled instances in E instances in J</title>
<abstract confidence="0.965996902097902">Acquisition of translation dictionary Translation dictionary Newly labeled instances for J Hyponymy-relation candidate extraction Candidates in J Classifier in J Wikipedia Articles in J Labeled instances 434 Tiger Range Taxonomy Subspecies Bengal tiger Malayan tiger Siberian tiger (b) Tree structure of Figure 4(a) Figure 4: Wikipedia article and its layout structure as a source of hyponymy relations because it can provide a huge amount of them (Sumida et al., Sumida and Torisawa, and recognition of the layout structure is easy regardless of languages. Every English and Japanese Wikipedia article was transformed into a tree structure like 4, where layout items and items an article were used as nodes in a tree structure. Sumida et al. (2008) found that some pairs consisting of a node and one of its descendants constituted a proper hyponymy (e.g., and this could be a knowledge source of hyponymy relation acquisition. A hyponymy-relation candidate is then extracted from the tree structure by regarding a node as a hypernym candidate and all its subordinate nodes as hyponym candidates of hypernym candidate (e.g., and from Figure 4). 39 M English hyponymy-relation candidates and 10 M Japanese ones were extracted from Wikipedia. These candidates are classified into proper hyponymy relations and others by using the classifiers described below. 3.2 Hyponymy-Relation Classification We use SVMs (Vapnik, 1995) as classifiers for the classification of the hyponymy relations on the candidates. Let a hycandidate, a hyponym and a hyponymyrelation candidate. The lexical, structure-based, infobox-based features of in Table 1 are used for building English and Japanese Note that not et al. (2008) reported that they obtained 171 K, 420 K, and 1.48 M hyponymy relations from a definition sentence, a category system, and a layout structure in Japanese Wikipedia, respectively. in Sumida et al. (2008) but the same as their feature set. Let us provide an overview of the feature sets used in Sumida et al. (2008). See Sumida et al. (2008) for more details. Lexical feaused to recognize the lexievidence encoded in hyrelations. For example, is a proper hyponymy relation if the same head morpheme or word. such information is provided along with the words/morphemes and the parts of of which can be multiword/morpheme nouns. TagChunk (Daum´e III et al., 2005) for English and MeCab (MeCab, 2008) for Japanese were used to provide the lexical fea- Several simple lexical were also applied to hyponymy-relation candidates. For example, “List of artists” is converted into “artists” by lexical pattern “list of X.” Hyponymy-relation candidates whose hypernym candidate matches such a lexical pattern are likely to be valid (e.g., of artists, Leonardo da Vinci)). We use for dealing with these cases. If a typical or frequently used section heading in a Wikipedia article, such as “History” or “References,” is used as a hyponym candidate in a hyponymy-relation candidate, the hyponymy-relation candidate is usually a hyponymy relation. used to recognize these hyponymy-relation candidates. Structure-based features are related to the tree structure of Wikipedia articles from which candidate is exthe distance between the tree structure. the of layout items from which are originated. These are the feature sets used in Sumida et al. (2008). We also added some new items to the above sets. the types of tree nodes including root, leaf, and others. For examis seldom a hyponymy relation from a root node (or title) and from a child node (or section headthe structural conof a tree structure. They can provide evidence related to similar hyponymyrelation candidates in the structural contexts. infobox-based feature, is based on a used the same Japanese lexical patterns in Sumida et al. (2008) to build English lexical patterns with them. (a) Layout structure article 435 Type Description Example hyper: hypo: Siberian, hypo: of morphemes/words hyper: hypo: NP, hypo: themselves hyper: Tiger, hypo: Siberian tiger lexical patterns hyper: “List of X”, hypo: “Notable X” section headings hyper: History, hypo: Reference between of layout items hyper: title, hypo: bulleted list of tree nodes hyper: root node, hypo: leaf node parent node child node Taxonomy properties of (taxobox,species), hypo: (taxobox,name) 1: Feature type and its value. the head morpheme/word and its POS. those in examples are derived from in Figure 4. Wikipedia infobox, a special kind of template, that describes a tabular summary of an article subject expressed by attribute-value pairs. An attribute type coupled with the infobox name to which it belongs provides the semantic properties of its value that enable us to easily understand what the attribute value means (Auer and Lehmann, 2007; Wu and Weld, 2007). For example, intemplate Japan Wikipedia article several attribute-value pairs such “Mayor=Daisaku Kadokawa” as What the attribute of the example, represents is hard to understand alone if we lack knowledge, but attribute type, gives a a to These semantic properties enable us to discover semantic evidence for hyponymy relations. We extriples name, attribute type, attribute from the Wikipedia infoboxes and encode information related to our set 3.3 Bilingual Instance Dictionary Construction Multilingual versions of Wikipedia articles are connected by cross-language links and usually have titles that are bilinguals of each other (Erdmann et al., 2008). English and Japanese articles connected by a cross-language link are extracted from Wikipedia, and their titles are regarded as The translation pairs between obtained 1.6 M object-attribute-value triples in Japanese and 5.9 M in English. K translation pairs were extracted. English and Japanese terms are used for building instance dictionary hyponymyacquisition, where composed of translation pairs between English and Japanese 4 Experiments used the version of English and the version of Japanese Wikipedia for our experiments. 24,000 hyponymy-relation candidates, randomly selected in both languages, were manually checked to build development, and test Around 8,000 hyponymy relations were found in the manchecked data for both 20,000 of the manually checked data were used as a training set for training the initial classifier. The rest were equally divided into development and test sets. The development set was used to select the optimal parameters in bilingual co-training and the test set was used to evaluate our system. We used TinySVM (TinySVM, 2002) with a polynomial kernel of degree 2 as a classifier. The maximum iteration number in the bilingual cowas set as 100. Two parameters, were selected through experiments on the set. 1 showed also used redirection links in English and Japanese Wikipedia for recognizing the variations of terms when we built a bilingual instance dictionary with Wikipedia crosslanguage links. took about two or three months to check them in each language. a hyponymy relation as a positive sample and the others as a negative sample for training SVMs, “positive sample:negative sample” was about 8,000:16,000=1:2 436 the best performance and were used as the optimal parameter in the following experiments. We conducted three experiments to show effects of bilingual co-training, training data size, and bilingual instance dictionaries. In the first two experiments, we experimented with a bilingual instance dictionary derived from Wikipedia crosslanguage links. Comparison among systems based on three different bilingual instance dictionaries is shown in the third experiment. recall and as in Eq (1), were used as the evaluation measures, a set of manually checked relations and a set of hyponymy-relation candidates classified as hyponymy relations by the system: n HRbySj/jHRbySj n HRbySj/jRelj 2 x 4.1 Effect of Bilingual Co-Training P R P R SYT 78.5 63.8 70.4 75.0 77.4 76.1 INIT 77.9 67.4 72.2 74.5 78.5 76.6 TRAN 76.8 70.3 73.4 76.7 79.3 78.0 BICO 78.0 83.7 80.7 78.3 85.2 81.6 Table 2: Performance of different systems (%) Table 2 shows the comparison results of the four systems. SYT represents the Sumida et al. (2008) system that we implemented and tested with the same data as ours. INIT is a system based on iniclassifier in bilingual co-training. We translated training data in one language by using our bilingual instance dictionary and added the translation to the existing training data in the other language like bilingual co-training did. The size of the English and Japanese training data reached and 20,486. We trained initial classifier with the new training data. TRAN is a system based on the classifier. BICO is a system based on bilingual co-training. For Japanese, SYT showed worse performance than that reported in Sumida et al. (2008), probably due to the difference in training data size (ours is 20,000 and Sumida et al. (2008) was 29,900). The size of the test data was also different – ours is 2,000 and Sumida et al. (2008) was 1,000. Comparison between INIT and SYT shows the of newly introduced feature types, in hyponymy-relation classification. INIT consistently outperformed SYT, although the was merely around 0.5–1.8% in BICO showed significant performance im- (around 3.6–10.3% in over SYT, INIT, and TRAN regardless of the language. Comparison between TRAN and BICO showed that bilingual co-training is useful for enlarging the training data and that the performance gain by bilingual co-training cannot be achieved by simply translating the existing training data. Data 5: based on the increase of training data size during bilingual co-training 5 shows based on the size of the training data including those manually tailored and automatically obtained through bilingual co-training. The curve starts from 20,000 and ends around 55,000 in Japanese and 62,000 in En- As the training data size increases, the curves tend to go upward in both languages. This indicates that the two classifiers cooperate well to boost their performance through bilingual cotraining. We recognized 5.4 M English and 2.41 M Japanese hyponymy relations from the classification results of BICO on all hyponymy-relation candidates in both languages. 4.2 Effect of Training Data Size We performed two tests to investigate the effect of the training data size on bilingual co-training. The first test posed the following question: “If we build samples by hand and the building cost is the same in both languages, which is better from monolingual aspects: training or training samples?” Table 3 and Figure 6 show the results.</abstract>
<phone confidence="0.705847">20 25 30 35 40 45 50 55 60</phone>
<note confidence="0.6083175">81 79 77 75 73 English Japanese 437</note>
<abstract confidence="0.997383388888889">In INIT-E and INIT-J, a classifier in each lanwhich was trained with training samples, did not learn through bilingual co-training. In BICO-E and BICO-J, bilingual cotraining was applied to the initial classifiers trained samples in both languages. As shown in Table 3, BICO, with half the size of the training samples used in INIT, always performed better than INIT in both languages. This indicates that bilingual co-training enables us to build classifiers for two languages in tandem with the same combined amount of data as required for training a single classifier in isolation while achieving superior performance. Training Data Size 6: on training data size: with/without bilingual co-training n 2n n</abstract>
<affiliation confidence="0.777262">INIT-E INIT-J BICO-E BICO-J</affiliation>
<address confidence="0.728754">2500 67.3 72.3 70.5 73.0 5000 69.2 74.3 74.6 76.9</address>
<phone confidence="0.588082">10000 72.2 76.6 76.9 78.6</phone>
<abstract confidence="0.894723666666666">3: on training data size: with/without bilingual co-training (%) The second test asked: “Can we always improve performance through bilingual co-training with one strong and one weak classifier?” If the answer is yes, then we can apply our framework to acquisition of hyponymy-relations in other languages, i.e., German and French, without much effort for preparing a large amount of training data, because our strong classifier in English or Japanese can boost the performance of a weak classifier in other languages. To answer the question, we tested the performance of classifiers by using all training data (20,000) for a strong classifier and by changing the training data size of the other from 1,000 to 15,000 5,000, 10,000, for a weak classifier. INIT-E BICO-E INIT-J BICO-J 1,000 72.2 79.6 64.0 72.7 5,000 72.2 79.6 73.1 75.3 10,000 72.2 79.8 74.3 79.0 15,000 72.2 80.4 77.0 80.1 4: on training data size: when English classifier is strong one INIT-E BICO-E INIT-J BICO-J 1,000 60.3 69.7 76.6 79.3 5,000 67.3 74.6 76.6 79.6 10,000 69.2 77.7 76.6 80.1 15,000 71.0 79.3 76.6 80.6 5: on training data size: when Japanese classifier is strong one Tables 4 and 5 show the results, where “INIT” represents a system based on the initial classifier in each language and “BICO” represents a system based on bilingual co-training. The results were encouraging because the classifiers showed better performance than their initial ones in every setting. In other words, a strong classifier always taught a weak classifier well, and the strong one also got help from the weak one, regardless of the size of the training data with which the weaker one learned. The test showed that bilingual co-training can work well if we have one strong classifier. 4.3 Effect of Bilingual Instance Dictionaries We tested our method with different bilingual instance dictionaries to investigate their effect. We built bilingual instance dictionaries based on different translation dictionaries whose translation entries came from different domains (i.e., general domain, technical domain, and Wikipedia) and had a different degree of translation ambiguity. In Table 6, D1 and D2 correspond to systems based on a bilingual instance dictionary derived from two handcrafted translation dictionaries, EDICT (Breen, 2008) (a general-domain dictionary) and “The Japan Science and Technology Agency Dictionary,” (a translation dictionary for technical terms) respectively. D3, which is the same as BICO in Table 2, is based on a bilingual</abstract>
<phone confidence="0.385064">2500 5000 7500 10000 15000 20000</phone>
<note confidence="0.690671157894737">81 79 77 75 73 71 69 67 65 INIT-E INIT-J BICO-E BICO-J 438 DIC E J E2J J2E D1 76.5 78.4 588K 1.80 1.77 D1 75.0 77.2 990K 7.17 2.52 D2 76.9 78.5 667K 1.89 1.55</note>
<phone confidence="0.311943">D2 77.0 77.9 750K 3.05 1.71</phone>
<abstract confidence="0.960631285714285">D3 80.7 81.6 197K 1.03 1.02 D3 80.7 81.6 197K 1.03 1.02 dictionary derived from Wikipedia. the number of translation dictionary entries used for building a bilingual instance dictionary. E2J (or J2E) represents the average translation ambiguities of English (or Japanese) terms in the entries. To show the effect of these translation ambiguities, we used each dictionary two different conditions, and represents the condition where only translation entries with less than five translation ambiguities are no restriction on translation ambiguities. Table 6: Effect of different bilingual instance dictionaries The results showed that D3 was the best and that the performances of the others were simto each other. The differences in the between and relatively small within the same system triggered by translation ambiguities. The performance gap between D3 and the other systems might explain the fact that both hyponymy-relation candidates and the translation dictionary used in D3 were extracted from the same dataset (i.e., Wikipedia), and thus the bilingual instance dictionary built with the translation dictionary in D3 had better coverage of the Wikipedia entries consisting of hyponymyrelation candidates than the other bilingual instance dictionaries. Although D1 and D2 showed lower performance than D3, the experimental results showed that bilingual co-training was always effective no matter which dictionary was used that INIT in Table 2 was 72.2 in English and 76.6 in Japanese.) 5 Related Work Li and Li (2002) proposed bilingual bootstrapping for word translation disambiguation. Similar to bilingual co-training, classifiers for two languages cooperated in learning with bilingual resources in bilingual bootstrapping. However, the two classifiers in bilingual bootstrapping were for a bilingual task but did different tasks from the monolingual viewpoint. A classifier in each language is for word sense disambiguation, where a class label (or word sense) is different based on the languages. On the contrary, classifiers in bilingual co-training cooperate in doing the same type of tasks. Bilingual resources have been used for monolingual tasks including verb classification and noun phrase semantic interpolation (Merlo et al., 2002; Girju, 2006). However, unlike ours, their focus was limited to bilingual features for one monolingual classifier based on supervised learning. Recently, there has been increased interest in semantic relation acquisition from corpora. Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007; Ruiz-casado et al., 2005; Nastase and Strube, 2008; Sumida et al., 2008; Suchanek et al., 2007). Several researchers who participated in SemEval-07 (Girju et al., 2007) proposed methods for the classification of semantic relations between simple nominals in English sentences. However, the previous work seldom considered the bilingual aspect of semantic relations in the acquisition of monolingual semantic relations. 6 Conclusion We proposed a bilingual co-training approach and applied it to hyponymy-relation acquisition from Wikipedia. Experiments showed that bilingual co-training is effective for improving the performance of classifiers in both languages. We further showed that bilingual co-training enables us to build classifiers for two languages in tandem, outperforming classifiers trained individually for each language while requiring no more training data in total than a single classifier trained in isolation. We showed that bilingual co-training is also helpful for boosting the performance of a weak classifier in one language with the help of a strong classifier in the other language without lowering the performance of either classifier. This indicates that the framework can reduce the cost of preparing training data in new languages with the help of our English and Japanese strong classifiers. Our future work focuses on this issue. 439 References S¨oren Auer and Jens Lehmann. 2007. What have Innsbruck and Leipzig in common? Extracting sefrom wiki content. In of the 4th Semantic Web Conference (ESWC pages 503–517. Springer.</abstract>
<note confidence="0.470764555555556">Avrim Blum and Tom Mitchell. 1998. Combining laand unlabeled data with co-training. In 98: Proceedings of the eleventh annual conference Computational learning pages 92–100. Jim Breen. 2008. EDICT Japanese/English dictionary file, The Electronic Dictionary Research and Development Group, Monash University. Hal Daum´e III, John Langford, and Daniel Marcu. 2005. Search-based structured prediction as classi- In ofNIPS Workshop on Advances in Learningfor Text and Speech Whistler, Canada. Maike Erdmann, Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio. 2008. A bilingual dictionary from the Wikipedia link structure. In pages 686–689. Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007.</note>
<abstract confidence="0.955725821428571">Semeval-2007 task 04: Classification of semantic rebetween nominals. In of the Fourth International Workshop on Semantic Evaluations pages 13–18. Roxana Girju. 2006. Out-of-context noun phrase semantic interpretation with cross-linguistic evidence. ’06: Proceedings of the 15th ACM international conference on Information and knowledge pages 268–276. Aurelie Herbelot and Ann Copestake. 2006. Acquiring ontological relationships from Wikipedia using In of the ISWC 2006 Workshop on Web Content Mining with Human Language Tech- Jun’ichi Kazama and Kentaro Torisawa. 2007. Exploiting Wikipedia as external knowledge for named recognition. In of Joint Conference on Empirical Methods in Natural Language Processand Computational Natural Language pages 698–707. Cong Li and Hang Li. 2002. Word translation disamusing bilingual bootstrapping. In of the 40th Annual Meeting of the Association for Compages 343–351. MeCab. 2008. MeCab: Yet another part-of-speech morphological analyzer. Paola Merlo, Suzanne Stevenson, Vivian Tsang, and Gianluca Allaria. 2002. A multilingual paradigm automatic verb classification. In of the</abstract>
<note confidence="0.638995384615385">40th Annual Meeting of the Association for Compupages 207–214. Vivi Nastase and Michael Strube. 2008. Decoding Wikipedia categories for knowledge acquisition. In ofAAAI pages 1219–1224. Maria Ruiz-casado, Enrique Alfonseca, and Pablo Castells. 2005. Automatic extraction of semantic relationships for Wordnet by means of pattern learnfrom Wikipedia. In of pages 67– 79. Springer Verlag. Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A Core of Semantic Knowl- In of the 16th international conference World Wide pages 697–706. Asuka Sumida and Kentaro Torisawa. 2008. Hacking Wikipedia for hyponymy relation acquisition. In Proc. of the Third International Joint Conference Natural Language Processing pages 883–888, January. Asuka Sumida, Naoki Yoshinaga, and Kentaro Torisawa. 2008. Boosting precision and recall of hyponymy relation acquisition from hierarchical layin Wikipedia. In of the 6th International Conference on Language Resources and 2002. N. Vapnik. 1995. nature of statistical</note>
<affiliation confidence="0.458188">Springer-Verlag New York, Inc.,</affiliation>
<address confidence="0.936847">New York, NY, USA.</address>
<note confidence="0.7584855">Fei Wu and Daniel S. Weld. 2007. Autonomously se- Wikipedia. In ’07: Proceedings of the sixteenth ACM conference on Conference on and knowledge pages 41– 50. 440</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Jens Lehmann</author>
</authors>
<title>What have Innsbruck and Leipzig in common? Extracting semantics from wiki content.</title>
<date>2007</date>
<booktitle>In Proc. of the 4th European Semantic Web Conference (ESWC</booktitle>
<pages>503--517</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="17850" citStr="Auer and Lehmann, 2007" startWordPosition="2960" endWordPosition="2963">IF Semantic properties of hyper and hypo hyper: (taxobox,species), hypo: (taxobox,name) Table 1: Feature type and its value. ∗ in LF, and LF2 represent the head morpheme/word and its POS. Except those in LF4 and LF5, examples are derived from (TIGER, SIBERIAN TIGER) in Figure 4. Wikipedia infobox, a special kind of template, that describes a tabular summary of an article subject expressed by attribute-value pairs. An attribute type coupled with the infobox name to which it belongs provides the semantic properties of its value that enable us to easily understand what the attribute value means (Auer and Lehmann, 2007; Wu and Weld, 2007). For example, infobox template City Japan in Wikipedia article Kyoto contains several attribute-value pairs such as “Mayor=Daisaku Kadokawa” as attribute=its value. What Daisaku Kadokawa, the attribute value of mayor in the example, represents is hard to understand alone if we lack knowledge, but its attribute type, mayor, gives a clue–Daisaku Kadokawa is a mayor related to Kyoto. These semantic properties enable us to discover semantic evidence for hyponymy relations. We extract triples (infobox name, attribute type, attribute value) from the Wikipedia infoboxes and encod</context>
</contexts>
<marker>Auer, Lehmann, 2007</marker>
<rawString>S¨oren Auer and Jens Lehmann. 2007. What have Innsbruck and Leipzig in common? Extracting semantics from wiki content. In Proc. of the 4th European Semantic Web Conference (ESWC 2007), pages 503–517. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT’ 98: Proceedings of the eleventh annual conference on Computational learning theory,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="3041" citStr="Blum and Mitchell, 1998" startWordPosition="446" endWordPosition="449">e reliable part of the classification results in another language. Since the learning settings (feature sets, feature values, training data, corpora, and so on) are usually different in two languages, the reliable part in one language may be overlapped by an unreliable part in another language. Adding the translated part of the classification results to the training data will improve the classification results in the unreliable part. This process can also be repeated by swapping the languages, as illustrated in Figure 1. Actually, this is nothing other than a bilingual version of co-training (Blum and Mitchell, 1998). Language 1 Language 2 Figure 1: Concept of bilingual co-training Let us show an example in our current task: hyponymy-relation acquisition from Wikipedia. Our original approach for this task was super..... ..... Training Training Further Enlarged Training Data for Language 1 Further Enlarged Training Data for Language 2 Training Enlarged Training Data for Language 1 Classifier Translate reliable parts of classification results Enlarged Training Data for Language 2 Classifier Training Iteration Manually Prepared Training Data for Language 1 Classifier Classifier Translate Training reliable pa</context>
<context position="10602" citStr="Blum and Mitchell, 1998" startWordPosition="1807" endWordPosition="1810">lS = clT is a condition to avoid problems, especially when the student also has a certain level of confidence in his opinion on a class label but disagrees with the teacher: rT &gt; θ and clS =� clT. In that case, the teacher does nothing and ignores the instance. Condition rT &lt; θ enables the teacher to instruct his student in the class label of xT in spite of their disagreement in a class label. If every condition is satisfied, (xT, clS) is added to existing labeled instances L(i+1) T .The roles are reversed in lines 19–27 so that ciT becomes a teacher and ciS a student. Similar to co-training (Blum and Mitchell, 1998), one classifier seeks another’s opinion to select new labeled instances. One main difference between co-training and bilingual co-training is the space of instances: co-training is based on different features of the same instances, and bilingual co-training is based on different spaces of instances divided by languages. Since some of the instances in different spaces are connected by a bilingual instance dictionary, they seem to be in the same space. Another big difference lies in the role of the two classifiers. The two classifiers in co-training work on the same task, but those in bilingual</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT’ 98: Proceedings of the eleventh annual conference on Computational learning theory, pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Breen</author>
</authors>
<title>EDICT Japanese/English dictionary file, The Electronic Dictionary Research</title>
<date>2008</date>
<institution>and Development Group, Monash University.</institution>
<contexts>
<context position="27668" citStr="Breen, 2008" startWordPosition="4555" endWordPosition="4556">l co-training can work well if we have one strong classifier. 4.3 Effect of Bilingual Instance Dictionaries We tested our method with different bilingual instance dictionaries to investigate their effect. We built bilingual instance dictionaries based on different translation dictionaries whose translation entries came from different domains (i.e., general domain, technical domain, and Wikipedia) and had a different degree of translation ambiguity. In Table 6, D1 and D2 correspond to systems based on a bilingual instance dictionary derived from two handcrafted translation dictionaries, EDICT (Breen, 2008) (a general-domain dictionary) and “The Japan Science and Technology Agency Dictionary,” (a translation dictionary for technical terms) respectively. D3, which is the same as BICO in Table 2, is based on a bilingual 2500 5000 7500 10000 15000 20000 F1 81 79 77 75 73 71 69 67 65 INIT-E INIT-J BICO-E BICO-J 438 DIC F1 DIC STATISTICS TYPE E J ENTRY E2J J2E D1 α=5 76.5 78.4 588K 1.80 1.77 D1 ALL 75.0 77.2 990K 7.17 2.52 D2 α=5 76.9 78.5 667K 1.89 1.55 D2 ALL 77.0 77.9 750K 3.05 1.71 D3 α=5 80.7 81.6 197K 1.03 1.02 D3 ALL 80.7 81.6 197K 1.03 1.02 instance dictionary derived from Wikipedia. ENTRY re</context>
</contexts>
<marker>Breen, 2008</marker>
<rawString>Jim Breen. 2008. EDICT Japanese/English dictionary file, The Electronic Dictionary Research and Development Group, Monash University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e John Langford</author>
<author>Daniel Marcu</author>
</authors>
<title>Search-based structured prediction as classification.</title>
<date>2005</date>
<booktitle>In Proc. ofNIPS Workshop on Advances in Structured Learningfor Text and Speech Processing,</booktitle>
<location>Whistler, Canada.</location>
<marker>Langford, Marcu, 2005</marker>
<rawString>Hal Daum´e III, John Langford, and Daniel Marcu. 2005. Search-based structured prediction as classification. In Proc. ofNIPS Workshop on Advances in Structured Learningfor Text and Speech Processing, Whistler, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maike Erdmann</author>
<author>Kotaro Nakayama</author>
<author>Takahiro Hara</author>
<author>Shojiro Nishio</author>
</authors>
<title>A bilingual dictionary extracted from the Wikipedia link structure.</title>
<date>2008</date>
<booktitle>In Proc. ofDASFAA,</booktitle>
<pages>686--689</pages>
<contexts>
<context position="18728" citStr="Erdmann et al., 2008" startWordPosition="3092" endWordPosition="3096">resents is hard to understand alone if we lack knowledge, but its attribute type, mayor, gives a clue–Daisaku Kadokawa is a mayor related to Kyoto. These semantic properties enable us to discover semantic evidence for hyponymy relations. We extract triples (infobox name, attribute type, attribute value) from the Wikipedia infoboxes and encode such information related to hyper and hypo in our feature set IF.3 3.3 Bilingual Instance Dictionary Construction Multilingual versions of Wikipedia articles are connected by cross-language links and usually have titles that are bilinguals of each other (Erdmann et al., 2008). English and Japanese articles connected by a cross-language link are extracted from Wikipedia, and their titles are regarded as translation pairs4. The translation pairs between 3We obtained 1.6 M object-attribute-value triples in Japanese and 5.9 M in English. 4197 K translation pairs were extracted. English and Japanese terms are used for building bilingual instance dictionary DBI for hyponymyrelation acquisition, where DBI is composed of translation pairs between English and Japanese hyponymy-relation candidates5. 4 Experiments We used the MAY 2008 version of English Wikipedia and the JUN</context>
</contexts>
<marker>Erdmann, Nakayama, Hara, Nishio, 2008</marker>
<rawString>Maike Erdmann, Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio. 2008. A bilingual dictionary extracted from the Wikipedia link structure. In Proc. ofDASFAA, pages 686–689.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>Semeval-2007 task 04: Classification of semantic relations between nominals.</title>
<date>2007</date>
<booktitle>In Proc. of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>13--18</pages>
<contexts>
<context position="1758" citStr="Girju et al., 2007" startWordPosition="239" endWordPosition="242">for developing high-level NLP applications such as question answering, although it remains difficult to acquire a large amount of highly accurate semantic knowledge. This paper proposes a novel framework for a large-scale, accurate acquisition method for monolingual semantic knowledge, especially for semantic relations between nominals such as hyponymy and meronymy. We call the framework bilingual cotraining. The acquisition of semantic relations between nominals can be seen as a classification task of semantic relations – to determine whether two nominals hold a particular semantic relation (Girju et al., 2007). Supervised learning methods, which have often been applied to this classification task, have shown promising results. In those methods, however, a large amount of training data is usually required to obtain high performance, and the high costs of preparing training data have always been a bottleneck. Our research on bilingual co-training sprang from a very simple idea: perhaps training data in a language can be enlarged without much cost if we translate training data in another language and add the translation to the training data in the original language. We also noticed that it may be poss</context>
<context position="31099" citStr="Girju et al., 2007" startWordPosition="5099" endWordPosition="5102">ntic interpolation (Merlo et al., 2002; Girju, 2006). However, unlike ours, their focus was limited to bilingual features for one monolingual classifier based on supervised learning. Recently, there has been increased interest in semantic relation acquisition from corpora. Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007; Ruiz-casado et al., 2005; Nastase and Strube, 2008; Sumida et al., 2008; Suchanek et al., 2007). Several researchers who participated in SemEval-07 (Girju et al., 2007) proposed methods for the classification of semantic relations between simple nominals in English sentences. However, the previous work seldom considered the bilingual aspect of semantic relations in the acquisition of monolingual semantic relations. 6 Conclusion We proposed a bilingual co-training approach and applied it to hyponymy-relation acquisition from Wikipedia. Experiments showed that bilingual co-training is effective for improving the performance of classifiers in both languages. We further showed that bilingual co-training enables us to build classifiers for two languages in tandem</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. Semeval-2007 task 04: Classification of semantic relations between nominals. In Proc. of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 13–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<title>Out-of-context noun phrase semantic interpretation with cross-linguistic evidence.</title>
<date>2006</date>
<booktitle>In CIKM ’06: Proceedings of the 15th ACM international conference on Information and knowledge management,</booktitle>
<pages>268--276</pages>
<contexts>
<context position="30532" citStr="Girju, 2006" startWordPosition="5018" endWordPosition="5019">ages cooperated in learning with bilingual resources in bilingual bootstrapping. However, the two classifiers in bilingual bootstrapping were for a bilingual task but did different tasks from the monolingual viewpoint. A classifier in each language is for word sense disambiguation, where a class label (or word sense) is different based on the languages. On the contrary, classifiers in bilingual co-training cooperate in doing the same type of tasks. Bilingual resources have been used for monolingual tasks including verb classification and noun phrase semantic interpolation (Merlo et al., 2002; Girju, 2006). However, unlike ours, their focus was limited to bilingual features for one monolingual classifier based on supervised learning. Recently, there has been increased interest in semantic relation acquisition from corpora. Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007; Ruiz-casado et al., 2005; Nastase and Strube, 2008; Sumida et al., 2008; Suchanek et al., 2007). Several researchers who participated in SemEval-07 (Girju et al., 2007) proposed methods for the classif</context>
</contexts>
<marker>Girju, 2006</marker>
<rawString>Roxana Girju. 2006. Out-of-context noun phrase semantic interpretation with cross-linguistic evidence. In CIKM ’06: Proceedings of the 15th ACM international conference on Information and knowledge management, pages 268–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aurelie Herbelot</author>
<author>Ann Copestake</author>
</authors>
<title>Acquiring ontological relationships from Wikipedia using RMRS.</title>
<date>2006</date>
<booktitle>In Proc. of the ISWC</booktitle>
<contexts>
<context position="30902" citStr="Herbelot and Copestake, 2006" startWordPosition="5069" endWordPosition="5072">. On the contrary, classifiers in bilingual co-training cooperate in doing the same type of tasks. Bilingual resources have been used for monolingual tasks including verb classification and noun phrase semantic interpolation (Merlo et al., 2002; Girju, 2006). However, unlike ours, their focus was limited to bilingual features for one monolingual classifier based on supervised learning. Recently, there has been increased interest in semantic relation acquisition from corpora. Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007; Ruiz-casado et al., 2005; Nastase and Strube, 2008; Sumida et al., 2008; Suchanek et al., 2007). Several researchers who participated in SemEval-07 (Girju et al., 2007) proposed methods for the classification of semantic relations between simple nominals in English sentences. However, the previous work seldom considered the bilingual aspect of semantic relations in the acquisition of monolingual semantic relations. 6 Conclusion We proposed a bilingual co-training approach and applied it to hyponymy-relation acquisition from Wikipedia. Experiments showed that biling</context>
</contexts>
<marker>Herbelot, Copestake, 2006</marker>
<rawString>Aurelie Herbelot and Ann Copestake. 2006. Acquiring ontological relationships from Wikipedia using RMRS. In Proc. of the ISWC 2006 Workshop on Web Content Mining with Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Exploiting Wikipedia as external knowledge for named entity recognition.</title>
<date>2007</date>
<booktitle>In Proc. of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>698--707</pages>
<contexts>
<context position="30929" citStr="Kazama and Torisawa, 2007" startWordPosition="5073" endWordPosition="5076"> in bilingual co-training cooperate in doing the same type of tasks. Bilingual resources have been used for monolingual tasks including verb classification and noun phrase semantic interpolation (Merlo et al., 2002; Girju, 2006). However, unlike ours, their focus was limited to bilingual features for one monolingual classifier based on supervised learning. Recently, there has been increased interest in semantic relation acquisition from corpora. Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007; Ruiz-casado et al., 2005; Nastase and Strube, 2008; Sumida et al., 2008; Suchanek et al., 2007). Several researchers who participated in SemEval-07 (Girju et al., 2007) proposed methods for the classification of semantic relations between simple nominals in English sentences. However, the previous work seldom considered the bilingual aspect of semantic relations in the acquisition of monolingual semantic relations. 6 Conclusion We proposed a bilingual co-training approach and applied it to hyponymy-relation acquisition from Wikipedia. Experiments showed that bilingual co-training is effectiv</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2007. Exploiting Wikipedia as external knowledge for named entity recognition. In Proc. of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 698–707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cong Li</author>
<author>Hang Li</author>
</authors>
<title>Word translation disambiguation using bilingual bootstrapping.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>343--351</pages>
<contexts>
<context position="29790" citStr="Li and Li (2002)" startWordPosition="4908" endWordPosition="4911">relation candidates and the translation dictionary used in D3 were extracted from the same dataset (i.e., Wikipedia), and thus the bilingual instance dictionary built with the translation dictionary in D3 had better coverage of the Wikipedia entries consisting of hyponymyrelation candidates than the other bilingual instance dictionaries. Although D1 and D2 showed lower performance than D3, the experimental results showed that bilingual co-training was always effective no matter which dictionary was used (Note that F1 of INIT in Table 2 was 72.2 in English and 76.6 in Japanese.) 5 Related Work Li and Li (2002) proposed bilingual bootstrapping for word translation disambiguation. Similar to bilingual co-training, classifiers for two languages cooperated in learning with bilingual resources in bilingual bootstrapping. However, the two classifiers in bilingual bootstrapping were for a bilingual task but did different tasks from the monolingual viewpoint. A classifier in each language is for word sense disambiguation, where a class label (or word sense) is different based on the languages. On the contrary, classifiers in bilingual co-training cooperate in doing the same type of tasks. Bilingual resourc</context>
</contexts>
<marker>Li, Li, 2002</marker>
<rawString>Cong Li and Hang Li. 2002. Word translation disambiguation using bilingual bootstrapping. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics, pages 343–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MeCab</author>
</authors>
<title>MeCab: Yet another part-of-speech and morphological analyzer.</title>
<date>2008</date>
<note>http://mecab. sourceforge.net/.</note>
<contexts>
<context position="14873" citStr="MeCab, 2008" startWordPosition="2479" endWordPosition="2480"> same as their feature set. Let us provide an overview of the feature sets used in Sumida et al. (2008). See Sumida et al. (2008) for more details. Lexical features LF1–LF5 are used to recognize the lexical evidence encoded in hyper and hypo for hyponymy relations. For example, (hyper,hypo) is often a proper hyponymy relation if hyper and hypo share the same head morpheme or word. In LF1 and LF2, such information is provided along with the words/morphemes and the parts of speech of hyper and hypo, which can be multiword/morpheme nouns. TagChunk (Daum´e III et al., 2005) for English and MeCab (MeCab, 2008) for Japanese were used to provide the lexical features. Several simple lexical patterns2 were also applied to hyponymy-relation candidates. For example, “List of artists” is converted into “artists” by lexical pattern “list of X.” Hyponymy-relation candidates whose hypernym candidate matches such a lexical pattern are likely to be valid (e.g., (List of artists, Leonardo da Vinci)). We use LF4 for dealing with these cases. If a typical or frequently used section heading in a Wikipedia article, such as “History” or “References,” is used as a hyponym candidate in a hyponymy-relation candidate, t</context>
</contexts>
<marker>MeCab, 2008</marker>
<rawString>MeCab. 2008. MeCab: Yet another part-of-speech and morphological analyzer. http://mecab. sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Suzanne Stevenson</author>
<author>Vivian Tsang</author>
<author>Gianluca Allaria</author>
</authors>
<title>A multilingual paradigm for automatic verb classification.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>207--214</pages>
<contexts>
<context position="30518" citStr="Merlo et al., 2002" startWordPosition="5014" endWordPosition="5017">ifiers for two languages cooperated in learning with bilingual resources in bilingual bootstrapping. However, the two classifiers in bilingual bootstrapping were for a bilingual task but did different tasks from the monolingual viewpoint. A classifier in each language is for word sense disambiguation, where a class label (or word sense) is different based on the languages. On the contrary, classifiers in bilingual co-training cooperate in doing the same type of tasks. Bilingual resources have been used for monolingual tasks including verb classification and noun phrase semantic interpolation (Merlo et al., 2002; Girju, 2006). However, unlike ours, their focus was limited to bilingual features for one monolingual classifier based on supervised learning. Recently, there has been increased interest in semantic relation acquisition from corpora. Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007; Ruiz-casado et al., 2005; Nastase and Strube, 2008; Sumida et al., 2008; Suchanek et al., 2007). Several researchers who participated in SemEval-07 (Girju et al., 2007) proposed methods f</context>
</contexts>
<marker>Merlo, Stevenson, Tsang, Allaria, 2002</marker>
<rawString>Paola Merlo, Suzanne Stevenson, Vivian Tsang, and Gianluca Allaria. 2002. A multilingual paradigm for automatic verb classification. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics, pages 207–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Michael Strube</author>
</authors>
<title>Decoding Wikipedia categories for knowledge acquisition.</title>
<date>2008</date>
<booktitle>In Proc. ofAAAI 08,</booktitle>
<pages>1219--1224</pages>
<contexts>
<context position="30981" citStr="Nastase and Strube, 2008" startWordPosition="5081" endWordPosition="5084"> type of tasks. Bilingual resources have been used for monolingual tasks including verb classification and noun phrase semantic interpolation (Merlo et al., 2002; Girju, 2006). However, unlike ours, their focus was limited to bilingual features for one monolingual classifier based on supervised learning. Recently, there has been increased interest in semantic relation acquisition from corpora. Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007; Ruiz-casado et al., 2005; Nastase and Strube, 2008; Sumida et al., 2008; Suchanek et al., 2007). Several researchers who participated in SemEval-07 (Girju et al., 2007) proposed methods for the classification of semantic relations between simple nominals in English sentences. However, the previous work seldom considered the bilingual aspect of semantic relations in the acquisition of monolingual semantic relations. 6 Conclusion We proposed a bilingual co-training approach and applied it to hyponymy-relation acquisition from Wikipedia. Experiments showed that bilingual co-training is effective for improving the performance of classifiers in bo</context>
</contexts>
<marker>Nastase, Strube, 2008</marker>
<rawString>Vivi Nastase and Michael Strube. 2008. Decoding Wikipedia categories for knowledge acquisition. In Proc. ofAAAI 08, pages 1219–1224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Ruiz-casado</author>
<author>Enrique Alfonseca</author>
<author>Pablo Castells</author>
</authors>
<title>Automatic extraction of semantic relationships for Wordnet by means of pattern learning from Wikipedia.</title>
<date>2005</date>
<booktitle>In Proc. of NLDB,</booktitle>
<pages>67--79</pages>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="30955" citStr="Ruiz-casado et al., 2005" startWordPosition="5077" endWordPosition="5080">ooperate in doing the same type of tasks. Bilingual resources have been used for monolingual tasks including verb classification and noun phrase semantic interpolation (Merlo et al., 2002; Girju, 2006). However, unlike ours, their focus was limited to bilingual features for one monolingual classifier based on supervised learning. Recently, there has been increased interest in semantic relation acquisition from corpora. Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007; Ruiz-casado et al., 2005; Nastase and Strube, 2008; Sumida et al., 2008; Suchanek et al., 2007). Several researchers who participated in SemEval-07 (Girju et al., 2007) proposed methods for the classification of semantic relations between simple nominals in English sentences. However, the previous work seldom considered the bilingual aspect of semantic relations in the acquisition of monolingual semantic relations. 6 Conclusion We proposed a bilingual co-training approach and applied it to hyponymy-relation acquisition from Wikipedia. Experiments showed that bilingual co-training is effective for improving the perfor</context>
</contexts>
<marker>Ruiz-casado, Alfonseca, Castells, 2005</marker>
<rawString>Maria Ruiz-casado, Enrique Alfonseca, and Pablo Castells. 2005. Automatic extraction of semantic relationships for Wordnet by means of pattern learning from Wikipedia. In Proc. of NLDB, pages 67– 79. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A Core of Semantic Knowledge.</title>
<date>2007</date>
<booktitle>In Proc. of the 16th international conference on World Wide Web,</booktitle>
<pages>697--706</pages>
<contexts>
<context position="31026" citStr="Suchanek et al., 2007" startWordPosition="5089" endWordPosition="5092">sed for monolingual tasks including verb classification and noun phrase semantic interpolation (Merlo et al., 2002; Girju, 2006). However, unlike ours, their focus was limited to bilingual features for one monolingual classifier based on supervised learning. Recently, there has been increased interest in semantic relation acquisition from corpora. Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007; Ruiz-casado et al., 2005; Nastase and Strube, 2008; Sumida et al., 2008; Suchanek et al., 2007). Several researchers who participated in SemEval-07 (Girju et al., 2007) proposed methods for the classification of semantic relations between simple nominals in English sentences. However, the previous work seldom considered the bilingual aspect of semantic relations in the acquisition of monolingual semantic relations. 6 Conclusion We proposed a bilingual co-training approach and applied it to hyponymy-relation acquisition from Wikipedia. Experiments showed that bilingual co-training is effective for improving the performance of classifiers in both languages. We further showed that bilingua</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A Core of Semantic Knowledge. In Proc. of the 16th international conference on World Wide Web, pages 697–706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asuka Sumida</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Hacking Wikipedia for hyponymy relation acquisition.</title>
<date>2008</date>
<booktitle>In Proc. of the Third International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>883--888</pages>
<contexts>
<context position="12536" citStr="Sumida and Torisawa, 2008" startWordPosition="2094" endWordPosition="2097">ion Candidates in E Newly labeled instances for E Bilingual instance dictionary Unlabeled Unlabeled instances in E instances in J Acquisition of translation dictionary Translation dictionary Newly labeled instances for J Hyponymy-relation candidate extraction Candidates in J Classifier in J Wikipedia Articles in J Labeled instances 434 Tiger Range Taxonomy Subspecies Bengal tiger Malayan tiger Siberian tiger (b) Tree structure of Figure 4(a) Figure 4: Wikipedia article and its layout structure as a source of hyponymy relations because it can provide a huge amount of them (Sumida et al., 2008; Sumida and Torisawa, 2008)1, and recognition of the layout structure is easy regardless of languages. Every English and Japanese Wikipedia article was transformed into a tree structure like Figure 4, where layout items title, (sub)section headings, and list items in an article were used as nodes in a tree structure. Sumida et al. (2008) found that some pairs consisting of a node and one of its descendants constituted a proper hyponymy relation (e.g., (TIGER, SIBERIAN TIGER)), and this could be a knowledge source of hyponymy relation acquisition. A hyponymy-relation candidate is then extracted from the tree structure by</context>
</contexts>
<marker>Sumida, Torisawa, 2008</marker>
<rawString>Asuka Sumida and Kentaro Torisawa. 2008. Hacking Wikipedia for hyponymy relation acquisition. In Proc. of the Third International Joint Conference on Natural Language Processing (IJCNLP), pages 883–888, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asuka Sumida</author>
<author>Naoki Yoshinaga</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Boosting precision and recall of hyponymy relation acquisition from hierarchical layouts in Wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="3955" citStr="Sumida et al. (2008)" startWordPosition="582" endWordPosition="585"> Training Data for Language 2 Training Enlarged Training Data for Language 1 Classifier Translate reliable parts of classification results Enlarged Training Data for Language 2 Classifier Training Iteration Manually Prepared Training Data for Language 1 Classifier Classifier Translate Training reliable parts of Training classification results Manually Prepared Training Data for Language 2 432 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 432–440, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP vised learning based on the approach proposed by Sumida et al. (2008), which was only applied for Japanese and achieved around 80% in F-measure. In their approach, a common substring in a hypernym and a hyponym is assumed to be one strong clue for recognizing that the two words constitute a hyponymy relation. For example, recognizing a proper hyponymy relation between two Japanese words, N# (kouso meaning enzyme) and bH7K33 �N# (kasuibunkaikouso meaning hydrolase), is relatively easy because they share a common suffix: kouso. On the other hand, judging whether their English translations (enzyme and hydrolase) have a hyponymy relation is probably more difficult </context>
<context position="11688" citStr="Sumida et al. (2008)" startWordPosition="1973" endWordPosition="1976">g difference lies in the role of the two classifiers. The two classifiers in co-training work on the same task, but those in bilingual co-training do the same type of task rather than the same task. 3 Acquisition of Hyponymy Relations from Wikipedia Our system, which acquires hyponymy relations from Wikipedia based on bilingual co-training, is described in Figure 3. The following three main parts are described in this section: candidate extraction, hyponymy-relation classification, and bilingual instance dictionary construction. Figure 3: System architecture 3.1 Candidate Extraction We follow Sumida et al. (2008) to extract hyponymy-relation candidates from English and Japanese Wikipedia. A layout structure is chosen Classifier in E Wikipedia Articles in E Bilingual Co-Training Labeled instances Hyponymy-relation candidate extraction Candidates in E Newly labeled instances for E Bilingual instance dictionary Unlabeled Unlabeled instances in E instances in J Acquisition of translation dictionary Translation dictionary Newly labeled instances for J Hyponymy-relation candidate extraction Candidates in J Classifier in J Wikipedia Articles in J Labeled instances 434 Tiger Range Taxonomy Subspecies Bengal t</context>
<context position="14021" citStr="Sumida et al. (2008)" startWordPosition="2327" endWordPosition="2330">ted from Wikipedia. These candidates are classified into proper hyponymy relations and others by using the classifiers described below. 3.2 Hyponymy-Relation Classification We use SVMs (Vapnik, 1995) as classifiers for the classification of the hyponymy relations on the hyponymy-relation candidates. Let hyper be a hypernym candidate, hypo be a hyper’s hyponym candidate, and (hyper, hypo) be a hyponymyrelation candidate. The lexical, structure-based, and infobox-based features of (hyper, hypo) in Table 1 are used for building English and Japanese classifiers. Note that 5F3–5F5 and IF were not 1Sumida et al. (2008) reported that they obtained 171 K, 420 K, and 1.48 M hyponymy relations from a definition sentence, a category system, and a layout structure in Japanese Wikipedia, respectively. used in Sumida et al. (2008) but LF1–LF5 and 5F1–5F2 are the same as their feature set. Let us provide an overview of the feature sets used in Sumida et al. (2008). See Sumida et al. (2008) for more details. Lexical features LF1–LF5 are used to recognize the lexical evidence encoded in hyper and hypo for hyponymy relations. For example, (hyper,hypo) is often a proper hyponymy relation if hyper and hypo share the same</context>
<context position="15957" citStr="Sumida et al. (2008)" startWordPosition="2646" endWordPosition="2649">on heading in a Wikipedia article, such as “History” or “References,” is used as a hyponym candidate in a hyponymy-relation candidate, the hyponymy-relation candidate is usually not a hyponymy relation. LF5 is used to recognize these hyponymy-relation candidates. Structure-based features are related to the tree structure of Wikipedia articles from which hyponymy-relation candidate (hyper,hypo) is extracted. 5F1 provides the distance between hyper and hypo in the tree structure. 5F2 represents the type of layout items from which hyper and hypo are originated. These are the feature sets used in Sumida et al. (2008). We also added some new items to the above feature sets. 5F3 represents the types of tree nodes including root, leaf, and others. For example, (hyper,hypo) is seldom a hyponymy relation if hyper is from a root node (or title) and hypo is from a hyper’s child node (or section headings). 5F4 and 5F5 represent the structural contexts of hyper and hypo in a tree structure. They can provide evidence related to similar hyponymyrelation candidates in the structural contexts. An infobox-based feature, IF, is based on a 2We used the same Japanese lexical patterns in Sumida et al. (2008) to build Engli</context>
<context position="21785" citStr="Sumida et al. (2008)" startWordPosition="3580" endWordPosition="3583">aluation measures, where Rel represents a set of manually checked hyponymy relations and HRbyS represents a set of hyponymy-relation candidates classified as hyponymy relations by the system: P = jRel n HRbySj/jHRbySj (1) R = jRel n HRbySj/jRelj F1 = 2 x (P x R)/(P + R) 4.1 Effect of Bilingual Co-Training ENGLISH JAPANESE P R F1 P R F1 SYT 78.5 63.8 70.4 75.0 77.4 76.1 INIT 77.9 67.4 72.2 74.5 78.5 76.6 TRAN 76.8 70.3 73.4 76.7 79.3 78.0 BICO 78.0 83.7 80.7 78.3 85.2 81.6 Table 2: Performance of different systems (%) Table 2 shows the comparison results of the four systems. SYT represents the Sumida et al. (2008) system that we implemented and tested with the same data as ours. INIT is a system based on initial classifier c0 in bilingual co-training. We translated training data in one language by using our bilingual instance dictionary and added the translation to the existing training data in the other language like bilingual co-training did. The size of the English and Japanese training data reached 20,729 and 20,486. We trained initial classifier c0 with the new training data. TRAN is a system based on the classifier. BICO is a system based on bilingual co-training. For Japanese, SYT showed worse p</context>
<context position="31002" citStr="Sumida et al., 2008" startWordPosition="5085" endWordPosition="5088">resources have been used for monolingual tasks including verb classification and noun phrase semantic interpolation (Merlo et al., 2002; Girju, 2006). However, unlike ours, their focus was limited to bilingual features for one monolingual classifier based on supervised learning. Recently, there has been increased interest in semantic relation acquisition from corpora. Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007; Ruiz-casado et al., 2005; Nastase and Strube, 2008; Sumida et al., 2008; Suchanek et al., 2007). Several researchers who participated in SemEval-07 (Girju et al., 2007) proposed methods for the classification of semantic relations between simple nominals in English sentences. However, the previous work seldom considered the bilingual aspect of semantic relations in the acquisition of monolingual semantic relations. 6 Conclusion We proposed a bilingual co-training approach and applied it to hyponymy-relation acquisition from Wikipedia. Experiments showed that bilingual co-training is effective for improving the performance of classifiers in both languages. We furt</context>
</contexts>
<marker>Sumida, Yoshinaga, Torisawa, 2008</marker>
<rawString>Asuka Sumida, Naoki Yoshinaga, and Kentaro Torisawa. 2008. Boosting precision and recall of hyponymy relation acquisition from hierarchical layouts in Wikipedia. In Proceedings of the 6th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TinySVM</author>
</authors>
<date>2002</date>
<note>http://chasen.org/˜taku/ software/TinySVM.</note>
<contexts>
<context position="19954" citStr="TinySVM, 2002" startWordPosition="3279" endWordPosition="3280">on of Japanese Wikipedia for our experiments. 24,000 hyponymy-relation candidates, randomly selected in both languages, were manually checked to build training, development, and test sets6. Around 8,000 hyponymy relations were found in the manually checked data for both languages7. 20,000 of the manually checked data were used as a training set for training the initial classifier. The rest were equally divided into development and test sets. The development set was used to select the optimal parameters in bilingual co-training and the test set was used to evaluate our system. We used TinySVM (TinySVM, 2002) with a polynomial kernel of degree 2 as a classifier. The maximum iteration number in the bilingual cotraining was set as 100. Two parameters, 0 and TopN, were selected through experiments on the development set. 0 = 1 and TopN=900 showed 5We also used redirection links in English and Japanese Wikipedia for recognizing the variations of terms when we built a bilingual instance dictionary with Wikipedia crosslanguage links. 6It took about two or three months to check them in each language. 7Regarding a hyponymy relation as a positive sample and the others as a negative sample for training SVMs</context>
</contexts>
<marker>TinySVM, 2002</marker>
<rawString>TinySVM. 2002. http://chasen.org/˜taku/ software/TinySVM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>1995</date>
<publisher>Springer-Verlag</publisher>
<location>New York,</location>
<contexts>
<context position="13600" citStr="Vapnik, 1995" startWordPosition="2263" endWordPosition="2264">)), and this could be a knowledge source of hyponymy relation acquisition. A hyponymy-relation candidate is then extracted from the tree structure by regarding a node as a hypernym candidate and all its subordinate nodes as hyponym candidates of the hypernym candidate (e.g., (TIGER, TAXONOMY) and (TIGER, SIBERIAN TIGER) from Figure 4). 39 M English hyponymy-relation candidates and 10 M Japanese ones were extracted from Wikipedia. These candidates are classified into proper hyponymy relations and others by using the classifiers described below. 3.2 Hyponymy-Relation Classification We use SVMs (Vapnik, 1995) as classifiers for the classification of the hyponymy relations on the hyponymy-relation candidates. Let hyper be a hypernym candidate, hypo be a hyper’s hyponym candidate, and (hyper, hypo) be a hyponymyrelation candidate. The lexical, structure-based, and infobox-based features of (hyper, hypo) in Table 1 are used for building English and Japanese classifiers. Note that 5F3–5F5 and IF were not 1Sumida et al. (2008) reported that they obtained 171 K, 420 K, and 1.48 M hyponymy relations from a definition sentence, a category system, and a layout structure in Japanese Wikipedia, respectively.</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The nature of statistical learning theory. Springer-Verlag New York, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Autonomously semantifying Wikipedia.</title>
<date>2007</date>
<booktitle>In CIKM ’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>41--50</pages>
<contexts>
<context position="17870" citStr="Wu and Weld, 2007" startWordPosition="2964" endWordPosition="2967">f hyper and hypo hyper: (taxobox,species), hypo: (taxobox,name) Table 1: Feature type and its value. ∗ in LF, and LF2 represent the head morpheme/word and its POS. Except those in LF4 and LF5, examples are derived from (TIGER, SIBERIAN TIGER) in Figure 4. Wikipedia infobox, a special kind of template, that describes a tabular summary of an article subject expressed by attribute-value pairs. An attribute type coupled with the infobox name to which it belongs provides the semantic properties of its value that enable us to easily understand what the attribute value means (Auer and Lehmann, 2007; Wu and Weld, 2007). For example, infobox template City Japan in Wikipedia article Kyoto contains several attribute-value pairs such as “Mayor=Daisaku Kadokawa” as attribute=its value. What Daisaku Kadokawa, the attribute value of mayor in the example, represents is hard to understand alone if we lack knowledge, but its attribute type, mayor, gives a clue–Daisaku Kadokawa is a mayor related to Kyoto. These semantic properties enable us to discover semantic evidence for hyponymy relations. We extract triples (infobox name, attribute type, attribute value) from the Wikipedia infoboxes and encode such information r</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel S. Weld. 2007. Autonomously semantifying Wikipedia. In CIKM ’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 41– 50.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>