<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005129">
<title confidence="0.997707">
Applying Pairwise Ranked Optimisation to Improve the Interpolation of
Translation Models
</title>
<author confidence="0.998337">
Barry Haddow
</author>
<affiliation confidence="0.998883">
University of Edinburgh
</affiliation>
<address confidence="0.873809">
Scotland
</address>
<email confidence="0.998356">
bhaddow@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.998555" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996910153846154">
In Statistical Machine Translation we often
have to combine different sources of parallel
training data to build a good system. One way
of doing this is to build separate translation
models from each data set and linearly inter-
polate them, and to date the main method for
optimising the interpolation weights is to min-
imise the model perplexity on a heldout set. In
this work, rather than optimising for this indi-
rect measure, we directly optimise for BLEU
on the tuning set and show improvements in
average performance over two data sets and 8
language pairs.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99978976923077">
Statistical Machine Translation (SMT) requires
large quantities of parallel training data in order to
produce high quality translation systems. This train-
ing data, however, is often scarce and must be drawn
from whatever sources are available. If these data
sources differ systematically from each other, and/or
from the test data, then the problem of combining
these disparate data sets to create the best possible
translation system is known as domain adaptation.
One approach to domain adaptation is to build
separate models for each training domain, then
weight them to create a system tuned to the test do-
main. In SMT, a successful approach to building do-
main specific language models is to build one from
each corpus, then linearly interpolate them, choos-
ing weights that minimise the perplexity on a suit-
able heldout set of in-domain data. This method
has been applied by many authors (e.g. (Koehn and
Schroeder, 2007)), and is implemented in popular
language modelling tools like IRSTLM (Federico et
al., 2008) and SRILM (Stolcke, 2002).
Similar interpolation techniques have been devel-
oped for translation model interpolation (Foster et
al., 2010; Sennrich, 2012) for phrase-based systems
but have not been as widely adopted, perhaps be-
cause the efficacy of the methods is not as clear-
cut. In this previous work, the authors used stan-
dard phrase extraction heuristics to extract phrases
from a heldout set of parallel sentences, then tuned
the translation model (i.e. the phrase table) inter-
polation weights to minimise the perplexity of the
interpolated model on this set of extracted phrases.
In this paper, we try to improve on this perplexity
optimisation of phrase table interpolation weights by
addressing two of its shortcomings. The first prob-
lem is that the perplexity is not well defined because
of the differing coverage of the phrase tables, and
their partial coverage of the phrases extracted from
the heldout set. Secondly, perplexity may not corre-
late with the performance of the final SMT system.
So, instead of optimising the interpolation
weights for the indirect goal of translation model
perplexity, we optimise them directly for transla-
tion performance. We do this by incorporating these
weights into SMT tuning using a modified version of
Pairwise Ranked Optimisation (PRO) (Hopkins and
May, 2011).
In experiments on two different domain adapta-
tion problems and 8 language pairs, we show that
our method achieves comparable or improved per-
formance, when compared to the perplexity minimi-
sation method. This is an encouraging result as it
</bodyText>
<page confidence="0.979159">
342
</page>
<subsectionHeader confidence="0.295372">
Proceedings of NAACL-HLT 2013, pages 342–347,
</subsectionHeader>
<bodyText confidence="0.940447833333333">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
shows that PRO can be adapted to optimise transla-
tion parameters other than those in the standard lin-
ear model.
to simultaneously optimise such non-linear parame-
ters.
</bodyText>
<sectionHeader confidence="0.9919615" genericHeader="method">
2 Optimising Phrase Table Interpolation
Weights
</sectionHeader>
<subsectionHeader confidence="0.999255">
2.1 Previous Approaches
</subsectionHeader>
<bodyText confidence="0.999918470588235">
In the work of Foster and Kuhn (2007), linear inter-
polation weights were derived from different mea-
sures of distance between the training corpora, but
this was not found to be successful. Optimising the
weights to minimise perplexity, as described in the
introduction, was found by later authors to be more
useful (Foster et al., 2010; Sennrich, 2012), gener-
ally showing small improvements over the default
approach of concatenating all training data.
An alternative approach is to use log-linear inter-
polation, so that the interpolation weights can be
easily optimised in tuning (Koehn and Schroeder,
2007; Bertoldi and Federico, 2009; Banerjee et al.,
2011). However, this effectively multiplies the prob-
abilities across phrase tables, which does not seem
appropriate, especially for phrases absent from 1 ta-
ble.
</bodyText>
<subsectionHeader confidence="0.99966">
2.2 Tuning SMT Systems
</subsectionHeader>
<bodyText confidence="0.999840545454546">
The standard SMT model scores translation hy-
potheses as a linear combination of features. The
model score of a hypothesis e is then defined to
be w · h(e, f, a) where w is a weight vector, and
h(e, f, a) a vector of feature functions defined over
source sentences (f), hypotheses, and their align-
ments (a). The weights are normally optimised
(tuned) to maximise BLEU on a heldout set (the tun-
ing set).
The most popular algorithm for this weight op-
timisation is the line-search based MERT (Och,
2003), but recently other algorithms that support
more features, such as PRO (Hopkins and May,
2011) or MIRA-based algorithms (Watanabe et al.,
2007; Chiang et al., 2008; Cherry and Foster, 2012),
have been introduced. All these algorithms assume
that the model score is a linear function of the pa-
rameters w. However since the phrase table prob-
abilities enter the score function in log form, if
these probabilities are a linear interpolation, then the
model score is not a linear function of the interpola-
tion weights. We will show that PRO can be used
</bodyText>
<subsectionHeader confidence="0.999111">
2.3 Pairwise Ranked Optimisation
</subsectionHeader>
<bodyText confidence="0.999955928571428">
PRO is a batch tuning algorithm in the sense that
there is an outer loop which repeatedly decodes a
small (1000-2000 sentence) tuning set and passes
the n-best lists from this tuning set to the core al-
gorithm (also known as the inner loop). The core
algorithm samples pairs of hypotheses from the n-
best lists (according to a specific procedure), and
uses these samples to optimise the weight vector w.
The core algorithm in PRO will now be explained
in more detail. Suppose that the N sampled hypoth-
esis pairs (xαi , xβi ) are indexed by i and have corre-
sponding feature vectors pairs (hαi , hβi ). If the gain
of a given hypothesis (we use smoothed sentence
BLEU) is given by the function g(x), then we define
</bodyText>
<equation confidence="0.9846">
yi by yi ≡ sgn(g(xαi ) − g(xβi )) (1)
</equation>
<bodyText confidence="0.998478">
For weights w, and hypothesis pair (xαi , xβi ), the
(model) score difference Δswi is given by:
</bodyText>
<equation confidence="0.969697333333333">
� �
Δsw i ≡ sw(xα i ) − sw(xβ i ) ≡ w · hα i − hβ (2)
i
</equation>
<bodyText confidence="0.998966333333333">
Then the core PRO algorithm updates the weight
vector to w* by solving the following optimisation
problem:
</bodyText>
<equation confidence="0.9967805">
log (σ (yiΔsi
w)) (3)
</equation>
<bodyText confidence="0.999978083333333">
where σ(x) is the standard sigmoid function. The
derivative of the function can be computed easily,
and the optimisation problem can be solved with
standard numerical optimisation algorithms such as
L-BFGS (Byrd et al., 1995). PRO is normally im-
plemented by converting each sample to a training
example for a 2 class maximum entropy classifier,
with the feature values set to Δhi and the responses
set to the yi, whereupon the log-likelihood is the ob-
jective given in Equation (3). As in maximum en-
tropy modeling, it is usual to add a Gaussian prior to
the objective (3) in PRO training.
</bodyText>
<subsectionHeader confidence="0.990536">
2.4 Extending PRO for Mixture Models
</subsectionHeader>
<bodyText confidence="0.9999815">
We now show how to apply the PRO tuning algo-
rithm of the previous subsection to simultaneously
</bodyText>
<equation confidence="0.98149225">
N
w* = arg max
w
i=1
</equation>
<page confidence="0.979741">
343
</page>
<bodyText confidence="0.999543133333333">
optimise the weights of the translation system, and
the interpolation weights.
In the standard phrase-based model, some of the
features are derived from logs of phrase translation
probabilities. If the phrase table is actually a linear
interpolation of two (or more) phrase tables, then
we can consider these features as also being func-
tions of the interpolation weights. The interpola-
tion weights then enter the score differences IAswi �
via the phrase features, and we can jointly optimise
the objective in Equation (3) for translation model
weights and interpolation weights.
To make this more concrete, suppose that the fea-
ture vector consists of m phrase table features and
n − m other features1
</bodyText>
<equation confidence="0.997663">
h = (log(p1), ... , log(pm), hm+1, ... hn) (4)
</equation>
<bodyText confidence="0.9982715">
where each pj is an interpolation of two probability
distributions pjA and pjB. So, pj - AjpjA+(1−Aj)pjB
with 0 &lt; Aj &lt; 1. Defining A - (A1 ... Am), the
optimisation problem is then:
</bodyText>
<equation confidence="0.982759666666667">
( ( //
(w�, ��) = arg max(w,�) EN i=1 log � yiAs(w,�)
i
</equation>
<bodyText confidence="0.910363833333333">
(5)
where the sum is over the sampled hypothesis pairs
and the A indicates the difference between the
model scores of the two hypotheses in the pair, as
before. The model score s(w,�)
i is given by
</bodyText>
<equation confidence="0.9979098">
( ( //
wj � log Ajpj Ai + (1 − Aj)pj Bi)
n
+ wjhji (6)
j=m+1
</equation>
<bodyText confidence="0.999680888888889">
where w - (wi ... wn). A Gaussian regularisa-
tion term is added to the objective, as it was for
PRO. By replacing the core algorithm of PRO with
the optimisation above, the interpolation weights
can be trained simultaneously with the other model
weights.
Actually, the above explanation contains a simpli-
fication, in that it shows the phrase features interpo-
lated at sentence level. In reality the phrase features
</bodyText>
<footnote confidence="0.600889">
1Since the phrase penalty feature is a constant across phrase
pairs it is not interpolated, and so is classed with the the “other”
features. The lexical scores, although not actually probabilities,
are interpolated.
</footnote>
<bodyText confidence="0.999786">
are interpolated at the phrase level, then combined to
give the sentence level feature value. This makes the
definition of the objective more complex than that
shown above, but still optimisable using bounded L-
BFGS.
</bodyText>
<sectionHeader confidence="0.999887" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999955">
3.1 Corpus and Baselines
</subsectionHeader>
<bodyText confidence="0.999980513513514">
We ran experiments with data from the WMT shared
tasks (Callison-Burch et al., 2007; Callison-Burch et
al., 2012), as well as OpenSubtitles data2 released by
the OPUS project (Tiedemann, 2009).
The experiments targeted both the news-
commentary (nc) and OpenSubtitles (st) domains,
with nc-devtest2007 and nc-test2007
for tuning and testing in the nc domain, respec-
tively, and corresponding 2000 sentence tuning
and test sets selected from the st data. The news-
commentary v7 corpus and a 200k sentence corpus
selected from the remaining st data were used as
in-domain training data for the respective domains,
with europarl v7 (ep) used as out-of-domain train-
ing data in both cases. The language pairs we tested
were the WMT language pairs for nc (English (en)
to and from Spanish (es), German (de), French (fr)
and Czech (cs)), with Dutch (nl) substituted for de
in the st experiments.
To build phrase-based translation systems, we
used the standard Moses (Koehn et al., 2007) train-
ing pipeline, in particular employing the usual 5
phrase features – forward and backward phrase
probabilities, forward and backward lexical scores
and a phrase penalty. The 5-gram Kneser-Ney
smoothed language models were trained by SRILM
(Stolcke, 2002), with KenLM (Heafield, 2011) used
at runtime. The language model is always a linear
interpolation of models estimated on the in- and out-
of-domain corpora, with weights tuned by SRILM’s
perplexity minimisation3. All experiments were run
three times with BLEU scores averaged, as recom-
mended by Clark et al. (2011). Performance was
evaluated using case-insensitive BLEU (Papineni et
al., 2002), as implemented in Moses.
The baseline systems were tuned using the Moses
version of PRO, a reimplementation of the original
</bodyText>
<footnote confidence="0.965854333333333">
2www.opensubtitles.org
3Our method could also be applied to language model inter-
polation but we chose to focus on phrase tables in this paper.
</footnote>
<equation confidence="0.3272455">
�m
j=1
</equation>
<page confidence="0.994419">
344
</page>
<bodyText confidence="0.999903076923077">
algorithm using the sampling scheme recommended
by Hopkins and May. We ran 15 iterations of PRO,
choosing the weights that maximised BLEU on the
tuning set. For the PRO training of the interpo-
lated models, we used the same sampling scheme,
with optimisation of the model weights and interpo-
lation weights implemented in Python using scipy4.
The implementation is available in Moses, in the
contrib/promix directory.
The phrase table interpolation and perplexity-
based minimisation of interpolation weights used
the code accompanying Sennrich (2012), also avail-
able in Moses.
</bodyText>
<subsectionHeader confidence="0.730553">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.998801708333333">
For each of the two test sets (nc and st), we com-
pare four different translation systems (three base-
line systems, and our new interpolation method):
in Phrase and reordering tables were built from just
the in-domain data.
joint Phrase and reordering tables were built from
the in- and out-of-domain data, concatenated.
perp Separate phrase tables built on in- and out-of-
domain data, interpolated using perplexity min-
imisation. The reordering table is as for joint.
pro-mix As perp, but interpolation weights opti-
mised using our modified PRO algorithm.
So the two interpolated models (perp and pro-mix)
are the same as joint except that their 4 non-constant
phrase features are interpolated across the two sep-
arate phrase tables. Note that the language models
are the same across all four systems.
The results of this comparison over the 8 language
pairs are shown in Figure 1, and summarised in Ta-
ble 1, which shows the mean BLEU change relative
to the in system. It can be seen that the pro-mix
method presented here is out-performing the per-
plexity optimisation on the nc data set, and perform-
ing similarly on the st data set.
</bodyText>
<table confidence="0.937607333333333">
joint perp pro-mix
nc +0.18 +0.44 +0.91
st -0.04 +0.55 +0.48
</table>
<tableCaption confidence="0.9473575">
Table 1: Mean BLEU relative to in system for each
data set. System names as in Figure 1
</tableCaption>
<footnote confidence="0.973176">
4www.scipy.org
</footnote>
<sectionHeader confidence="0.929684" genericHeader="conclusions">
4 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.9999178">
The results show that the pro-mix method is a vi-
able way of tuning systems built with interpolated
phrase tables, and performs better than the current
perplexity minimisation method on one of two data
sets used in experiments. On the other data set (st),
the out-of-domain data makes much less difference
to the system performance in general, most proba-
bly because the difference between the in and out-
of-domain data sets in much larger (Haddow and
Koehn, 2012). Whilst the differences between pro-
mix and perplexity minimisation are not large on the
nc test set (about +0.5 BLEU) the results have been
demonstrated to apply across many language pairs.
The advantage of the pro-mix method over other
approaches is that it directly optimises the mea-
sure that we are interested in, rather than optimising
an intermediate measure and hoping that translation
performance improves. In this work we optimise for
BLEU, but the same method could easily be used to
optimise for any sentence-level translation metric.
</bodyText>
<sectionHeader confidence="0.988905" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999568">
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement 288769 (ACCEPT).
</bodyText>
<sectionHeader confidence="0.994994" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99915335">
Pratyush Banerjee, Sudip K. Naskar, Johann Roturier,
Andy Way, and Josef van Genabith. 2011. Domain
Adaptation in Statistical Machine Translation of User-
Forum Data using Component Level Mixture Mod-
elling. In Proceedings of MT Summit.
Nicola Bertoldi and Marcello Federico. 2009. Domain
Adaptation for Statistical Machine Translation from
Monolingual Resources. In Proceedings of WMT.
R. H. Byrd, P. Lu, and J. Nocedal. 1995. A limited
memory algorithm for bound constrained optimiza-
tion. SIAM Journal on Scientific and Statistical Com-
puting, 16(5):1190–1208.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136–158, Prague, Czech Republic, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
</reference>
<page confidence="0.995521">
345
</page>
<reference confidence="0.996566782608696">
Findings of the 2012 Workshop on Statistical Machine
Translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10–
51, Montr´eal, Canada, June. Association for Compu-
tational Linguistics.
Colin Cherry and George Foster. 2012. Batch Tuning
Strategies for Statistical Machine Translation. In Pro-
ceedings of NAACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online Large-Margin Training of Syntactic and Struc-
tural Translation Features. In Proceedings of EMNLP.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.
2011. Better hypothesis testing for statistical machine
translation: Controlling for optimizer instability. In
Proceedings of ACL.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an Open Source Toolkit for Handling
Large Scale Language Models. In Proceedings of In-
terspeech, Brisbane, Australie.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
128–135, Prague, Czech Republic, June. Association
for Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative Instance Weighting for Domain Adap-
tation in Statistical Machine Translation. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 451–459, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Barry Haddow and Philipp Koehn. 2012. Analysing
the Effect of Out-of-Domain Data on SMT Systems.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 422–432, Montr´eal,
Canada, June. Association for Computational Linguis-
tics.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187–197, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1352–1362, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in Domain Adaptation for Statistical Machine Transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224–227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the ACL Demo Sessions, pages 177–180,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Franz J. Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings of
ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Rico Sennrich. 2012. Perplexity Minimization for Trans-
lation Model Domain Adaptation in Statistical Ma-
chine Translation. In Proceedings of EACL.
Andreas Stolcke. 2002. SRILM – An Extensible Lan-
guage Modeling Toolkit. In Proc. Intl. Conf. on Spo-
ken Language Processing, vol. 2, pages 901–904.
J¨org Tiedemann. 2009. News from OPUS - A Collection
of Multilingual Parallel Corpora with Tools and Inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova, and
R. Mitkov, editors, Recent Advances in Natural Lan-
guage Processing (vol V), pages 237–248. John Ben-
jamins, Amsterdam/Philadelphia.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online Large-Margin Training for Sta-
tistical Machine Translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764–
773, Prague, Czech Republic, June. Association for
Computational Linguistics.
</reference>
<page confidence="0.995622">
346
</page>
<figure confidence="0.996698846153846">
News Commentary
Bleu
0 10 20 30
perp
pro−mix
in
joint
cs−en en−cs de−en en−de fr−en en−fr es−en en−es
in perp Open Subtitles
joint pro−mix
Bleu
0 5 10 15 20 25
cs−en en−cs nl−en en−nl fr−en en−fr es−en en−es
</figure>
<figureCaption confidence="0.973722333333333">
Figure 1: Comparison of the performance (BLEU) on in-domain data, of our pro-mix interpolation weight
tuning method with three baselines: in using just in-domain parallel training data training; joint also using
europarl data; and perp using perplexity minimisation to interpolate in-domain and europarl data.
</figureCaption>
<page confidence="0.994476">
347
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956312">
<title confidence="0.9982465">Applying Pairwise Ranked Optimisation to Improve the Interpolation Translation Models</title>
<author confidence="0.993871">Barry</author>
<affiliation confidence="0.996982">University of</affiliation>
<email confidence="0.991204">bhaddow@inf.ed.ac.uk</email>
<abstract confidence="0.998266857142857">In Statistical Machine Translation we often have to combine different sources of parallel training data to build a good system. One way of doing this is to build separate translation models from each data set and linearly interpolate them, and to date the main method for optimising the interpolation weights is to minimise the model perplexity on a heldout set. In this work, rather than optimising for this indimeasure, we directly optimise for on the tuning set and show improvements in average performance over two data sets and 8 language pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pratyush Banerjee</author>
<author>Sudip K Naskar</author>
<author>Johann Roturier</author>
<author>Andy Way</author>
<author>Josef van Genabith</author>
</authors>
<title>Domain Adaptation in Statistical Machine Translation of UserForum Data using Component Level Mixture Modelling.</title>
<date>2011</date>
<booktitle>In Proceedings of MT Summit.</booktitle>
<marker>Banerjee, Naskar, Roturier, Way, van Genabith, 2011</marker>
<rawString>Pratyush Banerjee, Sudip K. Naskar, Johann Roturier, Andy Way, and Josef van Genabith. 2011. Domain Adaptation in Statistical Machine Translation of UserForum Data using Component Level Mixture Modelling. In Proceedings of MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain Adaptation for Statistical Machine Translation from Monolingual Resources.</title>
<date>2009</date>
<booktitle>In Proceedings of WMT.</booktitle>
<contexts>
<context position="4317" citStr="Bertoldi and Federico, 2009" startWordPosition="676" endWordPosition="679">ork of Foster and Kuhn (2007), linear interpolation weights were derived from different measures of distance between the training corpora, but this was not found to be successful. Optimising the weights to minimise perplexity, as described in the introduction, was found by later authors to be more useful (Foster et al., 2010; Sennrich, 2012), generally showing small improvements over the default approach of concatenating all training data. An alternative approach is to use log-linear interpolation, so that the interpolation weights can be easily optimised in tuning (Koehn and Schroeder, 2007; Bertoldi and Federico, 2009; Banerjee et al., 2011). However, this effectively multiplies the probabilities across phrase tables, which does not seem appropriate, especially for phrases absent from 1 table. 2.2 Tuning SMT Systems The standard SMT model scores translation hypotheses as a linear combination of features. The model score of a hypothesis e is then defined to be w · h(e, f, a) where w is a weight vector, and h(e, f, a) a vector of feature functions defined over source sentences (f), hypotheses, and their alignments (a). The weights are normally optimised (tuned) to maximise BLEU on a heldout set (the tuning s</context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico. 2009. Domain Adaptation for Statistical Machine Translation from Monolingual Resources. In Proceedings of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R H Byrd</author>
<author>P Lu</author>
<author>J Nocedal</author>
</authors>
<title>A limited memory algorithm for bound constrained optimization.</title>
<date>1995</date>
<journal>SIAM Journal on Scientific and Statistical Computing,</journal>
<volume>16</volume>
<issue>5</issue>
<contexts>
<context position="6856" citStr="Byrd et al., 1995" startWordPosition="1131" endWordPosition="1134">use smoothed sentence BLEU) is given by the function g(x), then we define yi by yi ≡ sgn(g(xαi ) − g(xβi )) (1) For weights w, and hypothesis pair (xαi , xβi ), the (model) score difference Δswi is given by: � � Δsw i ≡ sw(xα i ) − sw(xβ i ) ≡ w · hα i − hβ (2) i Then the core PRO algorithm updates the weight vector to w* by solving the following optimisation problem: log (σ (yiΔsi w)) (3) where σ(x) is the standard sigmoid function. The derivative of the function can be computed easily, and the optimisation problem can be solved with standard numerical optimisation algorithms such as L-BFGS (Byrd et al., 1995). PRO is normally implemented by converting each sample to a training example for a 2 class maximum entropy classifier, with the feature values set to Δhi and the responses set to the yi, whereupon the log-likelihood is the objective given in Equation (3). As in maximum entropy modeling, it is usual to add a Gaussian prior to the objective (3) in PRO training. 2.4 Extending PRO for Mixture Models We now show how to apply the PRO tuning algorithm of the previous subsection to simultaneously N w* = arg max w i=1 343 optimise the weights of the translation system, and the interpolation weights. I</context>
</contexts>
<marker>Byrd, Lu, Nocedal, 1995</marker>
<rawString>R. H. Byrd, P. Lu, and J. Nocedal. 1995. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific and Statistical Computing, 16(5):1190–1208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>136--158</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="9588" citStr="Callison-Burch et al., 2007" startWordPosition="1611" endWordPosition="1614"> features interpolated at sentence level. In reality the phrase features 1Since the phrase penalty feature is a constant across phrase pairs it is not interpolated, and so is classed with the the “other” features. The lexical scores, although not actually probabilities, are interpolated. are interpolated at the phrase level, then combined to give the sentence level feature value. This makes the definition of the objective more complex than that shown above, but still optimisable using bounded LBFGS. 3 Experiments 3.1 Corpus and Baselines We ran experiments with data from the WMT shared tasks (Callison-Burch et al., 2007; Callison-Burch et al., 2012), as well as OpenSubtitles data2 released by the OPUS project (Tiedemann, 2009). The experiments targeted both the newscommentary (nc) and OpenSubtitles (st) domains, with nc-devtest2007 and nc-test2007 for tuning and testing in the nc domain, respectively, and corresponding 2000 sentence tuning and test sets selected from the st data. The newscommentary v7 corpus and a 200k sentence corpus selected from the remaining st data were used as in-domain training data for the respective domains, with europarl v7 (ep) used as out-of-domain training data in both cases. Th</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (meta-) evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 136–158, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<contexts>
<context position="9618" citStr="Callison-Burch et al., 2012" startWordPosition="1615" endWordPosition="1618">tence level. In reality the phrase features 1Since the phrase penalty feature is a constant across phrase pairs it is not interpolated, and so is classed with the the “other” features. The lexical scores, although not actually probabilities, are interpolated. are interpolated at the phrase level, then combined to give the sentence level feature value. This makes the definition of the objective more complex than that shown above, but still optimisable using bounded LBFGS. 3 Experiments 3.1 Corpus and Baselines We ran experiments with data from the WMT shared tasks (Callison-Burch et al., 2007; Callison-Burch et al., 2012), as well as OpenSubtitles data2 released by the OPUS project (Tiedemann, 2009). The experiments targeted both the newscommentary (nc) and OpenSubtitles (st) domains, with nc-devtest2007 and nc-test2007 for tuning and testing in the nc domain, respectively, and corresponding 2000 sentence tuning and test sets selected from the st data. The newscommentary v7 corpus and a 200k sentence corpus selected from the remaining st data were used as in-domain training data for the respective domains, with europarl v7 (ep) used as out-of-domain training data in both cases. The language pairs we tested wer</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012.</rawString>
</citation>
<citation valid="true">
<title>Findings of the 2012 Workshop on Statistical Machine Translation.</title>
<date></date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<marker></marker>
<rawString>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10– 51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch Tuning Strategies for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="5209" citStr="Cherry and Foster, 2012" startWordPosition="827" endWordPosition="830">combination of features. The model score of a hypothesis e is then defined to be w · h(e, f, a) where w is a weight vector, and h(e, f, a) a vector of feature functions defined over source sentences (f), hypotheses, and their alignments (a). The weights are normally optimised (tuned) to maximise BLEU on a heldout set (the tuning set). The most popular algorithm for this weight optimisation is the line-search based MERT (Och, 2003), but recently other algorithms that support more features, such as PRO (Hopkins and May, 2011) or MIRA-based algorithms (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012), have been introduced. All these algorithms assume that the model score is a linear function of the parameters w. However since the phrase table probabilities enter the score function in log form, if these probabilities are a linear interpolation, then the model score is not a linear function of the interpolation weights. We will show that PRO can be used 2.3 Pairwise Ranked Optimisation PRO is a batch tuning algorithm in the sense that there is an outer loop which repeatedly decodes a small (1000-2000 sentence) tuning set and passes the n-best lists from this tuning set to the core algorithm</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online Large-Margin Training of Syntactic and Structural Translation Features.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5183" citStr="Chiang et al., 2008" startWordPosition="823" endWordPosition="826">potheses as a linear combination of features. The model score of a hypothesis e is then defined to be w · h(e, f, a) where w is a weight vector, and h(e, f, a) a vector of feature functions defined over source sentences (f), hypotheses, and their alignments (a). The weights are normally optimised (tuned) to maximise BLEU on a heldout set (the tuning set). The most popular algorithm for this weight optimisation is the line-search based MERT (Och, 2003), but recently other algorithms that support more features, such as PRO (Hopkins and May, 2011) or MIRA-based algorithms (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012), have been introduced. All these algorithms assume that the model score is a linear function of the parameters w. However since the phrase table probabilities enter the score function in log form, if these probabilities are a linear interpolation, then the model score is not a linear function of the interpolation weights. We will show that PRO can be used 2.3 Pairwise Ranked Optimisation PRO is a batch tuning algorithm in the sense that there is an outer loop which repeatedly decodes a small (1000-2000 sentence) tuning set and passes the n-best lists from this tuning</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online Large-Margin Training of Syntactic and Structural Translation Features. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="11052" citStr="Clark et al. (2011)" startWordPosition="1843" endWordPosition="1846">we used the standard Moses (Koehn et al., 2007) training pipeline, in particular employing the usual 5 phrase features – forward and backward phrase probabilities, forward and backward lexical scores and a phrase penalty. The 5-gram Kneser-Ney smoothed language models were trained by SRILM (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. The language model is always a linear interpolation of models estimated on the in- and outof-domain corpora, with weights tuned by SRILM’s perplexity minimisation3. All experiments were run three times with BLEU scores averaged, as recommended by Clark et al. (2011). Performance was evaluated using case-insensitive BLEU (Papineni et al., 2002), as implemented in Moses. The baseline systems were tuned using the Moses version of PRO, a reimplementation of the original 2www.opensubtitles.org 3Our method could also be applied to language model interpolation but we chose to focus on phrase tables in this paper. �m j=1 344 algorithm using the sampling scheme recommended by Hopkins and May. We ran 15 iterations of PRO, choosing the weights that maximised BLEU on the tuning set. For the PRO training of the interpolated models, we used the same sampling scheme, w</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
</authors>
<title>IRSTLM: an Open Source Toolkit for Handling Large Scale Language Models.</title>
<date>2008</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<location>Brisbane, Australie.</location>
<contexts>
<context position="1759" citStr="Federico et al., 2008" startWordPosition="277" endWordPosition="280">e the best possible translation system is known as domain adaptation. One approach to domain adaptation is to build separate models for each training domain, then weight them to create a system tuned to the test domain. In SMT, a successful approach to building domain specific language models is to build one from each corpus, then linearly interpolate them, choosing weights that minimise the perplexity on a suitable heldout set of in-domain data. This method has been applied by many authors (e.g. (Koehn and Schroeder, 2007)), and is implemented in popular language modelling tools like IRSTLM (Federico et al., 2008) and SRILM (Stolcke, 2002). Similar interpolation techniques have been developed for translation model interpolation (Foster et al., 2010; Sennrich, 2012) for phrase-based systems but have not been as widely adopted, perhaps because the efficacy of the methods is not as clearcut. In this previous work, the authors used standard phrase extraction heuristics to extract phrases from a heldout set of parallel sentences, then tuned the translation model (i.e. the phrase table) interpolation weights to minimise the perplexity of the interpolated model on this set of extracted phrases. In this paper,</context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, 2008</marker>
<rawString>Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an Open Source Toolkit for Handling Large Scale Language Models. In Proceedings of Interspeech, Brisbane, Australie.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixture-model adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>128--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3719" citStr="Foster and Kuhn (2007)" startWordPosition="584" endWordPosition="587">erent domain adaptation problems and 8 language pairs, we show that our method achieves comparable or improved performance, when compared to the perplexity minimisation method. This is an encouraging result as it 342 Proceedings of NAACL-HLT 2013, pages 342–347, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics shows that PRO can be adapted to optimise translation parameters other than those in the standard linear model. to simultaneously optimise such non-linear parameters. 2 Optimising Phrase Table Interpolation Weights 2.1 Previous Approaches In the work of Foster and Kuhn (2007), linear interpolation weights were derived from different measures of distance between the training corpora, but this was not found to be successful. Optimising the weights to minimise perplexity, as described in the introduction, was found by later authors to be more useful (Foster et al., 2010; Sennrich, 2012), generally showing small improvements over the default approach of concatenating all training data. An alternative approach is to use log-linear interpolation, so that the interpolation weights can be easily optimised in tuning (Koehn and Schroeder, 2007; Bertoldi and Federico, 2009; </context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixture-model adaptation for SMT. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 128–135, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>451--459</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="1896" citStr="Foster et al., 2010" startWordPosition="296" endWordPosition="299"> training domain, then weight them to create a system tuned to the test domain. In SMT, a successful approach to building domain specific language models is to build one from each corpus, then linearly interpolate them, choosing weights that minimise the perplexity on a suitable heldout set of in-domain data. This method has been applied by many authors (e.g. (Koehn and Schroeder, 2007)), and is implemented in popular language modelling tools like IRSTLM (Federico et al., 2008) and SRILM (Stolcke, 2002). Similar interpolation techniques have been developed for translation model interpolation (Foster et al., 2010; Sennrich, 2012) for phrase-based systems but have not been as widely adopted, perhaps because the efficacy of the methods is not as clearcut. In this previous work, the authors used standard phrase extraction heuristics to extract phrases from a heldout set of parallel sentences, then tuned the translation model (i.e. the phrase table) interpolation weights to minimise the perplexity of the interpolated model on this set of extracted phrases. In this paper, we try to improve on this perplexity optimisation of phrase table interpolation weights by addressing two of its shortcomings. The first</context>
<context position="4016" citStr="Foster et al., 2010" startWordPosition="632" endWordPosition="635">013 Association for Computational Linguistics shows that PRO can be adapted to optimise translation parameters other than those in the standard linear model. to simultaneously optimise such non-linear parameters. 2 Optimising Phrase Table Interpolation Weights 2.1 Previous Approaches In the work of Foster and Kuhn (2007), linear interpolation weights were derived from different measures of distance between the training corpora, but this was not found to be successful. Optimising the weights to minimise perplexity, as described in the introduction, was found by later authors to be more useful (Foster et al., 2010; Sennrich, 2012), generally showing small improvements over the default approach of concatenating all training data. An alternative approach is to use log-linear interpolation, so that the interpolation weights can be easily optimised in tuning (Koehn and Schroeder, 2007; Bertoldi and Federico, 2009; Banerjee et al., 2011). However, this effectively multiplies the probabilities across phrase tables, which does not seem appropriate, especially for phrases absent from 1 table. 2.2 Tuning SMT Systems The standard SMT model scores translation hypotheses as a linear combination of features. The mo</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 451–459, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>Analysing the Effect of Out-of-Domain Data on SMT Systems.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>422--432</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="13780" citStr="Haddow and Koehn, 2012" startWordPosition="2292" endWordPosition="2295">44 +0.91 st -0.04 +0.55 +0.48 Table 1: Mean BLEU relative to in system for each data set. System names as in Figure 1 4www.scipy.org 4 Discussion and Conclusions The results show that the pro-mix method is a viable way of tuning systems built with interpolated phrase tables, and performs better than the current perplexity minimisation method on one of two data sets used in experiments. On the other data set (st), the out-of-domain data makes much less difference to the system performance in general, most probably because the difference between the in and outof-domain data sets in much larger (Haddow and Koehn, 2012). Whilst the differences between promix and perplexity minimisation are not large on the nc test set (about +0.5 BLEU) the results have been demonstrated to apply across many language pairs. The advantage of the pro-mix method over other approaches is that it directly optimises the measure that we are interested in, rather than optimising an intermediate measure and hoping that translation performance improves. In this work we optimise for BLEU, but the same method could easily be used to optimise for any sentence-level translation metric. Acknowledgments The research leading to these results </context>
</contexts>
<marker>Haddow, Koehn, 2012</marker>
<rawString>Barry Haddow and Philipp Koehn. 2012. Analysing the Effect of Out-of-Domain Data on SMT Systems. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 422–432, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and Smaller Language Model Queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="10768" citStr="Heafield, 2011" startWordPosition="1799" endWordPosition="1800">ain training data in both cases. The language pairs we tested were the WMT language pairs for nc (English (en) to and from Spanish (es), German (de), French (fr) and Czech (cs)), with Dutch (nl) substituted for de in the st experiments. To build phrase-based translation systems, we used the standard Moses (Koehn et al., 2007) training pipeline, in particular employing the usual 5 phrase features – forward and backward phrase probabilities, forward and backward lexical scores and a phrase penalty. The 5-gram Kneser-Ney smoothed language models were trained by SRILM (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. The language model is always a linear interpolation of models estimated on the in- and outof-domain corpora, with weights tuned by SRILM’s perplexity minimisation3. All experiments were run three times with BLEU scores averaged, as recommended by Clark et al. (2011). Performance was evaluated using case-insensitive BLEU (Papineni et al., 2002), as implemented in Moses. The baseline systems were tuned using the Moses version of PRO, a reimplementation of the original 2www.opensubtitles.org 3Our method could also be applied to language model interpolation but we chose to focus </context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as Ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="3069" citStr="Hopkins and May, 2011" startWordPosition="484" endWordPosition="487"> by addressing two of its shortcomings. The first problem is that the perplexity is not well defined because of the differing coverage of the phrase tables, and their partial coverage of the phrases extracted from the heldout set. Secondly, perplexity may not correlate with the performance of the final SMT system. So, instead of optimising the interpolation weights for the indirect goal of translation model perplexity, we optimise them directly for translation performance. We do this by incorporating these weights into SMT tuning using a modified version of Pairwise Ranked Optimisation (PRO) (Hopkins and May, 2011). In experiments on two different domain adaptation problems and 8 language pairs, we show that our method achieves comparable or improved performance, when compared to the perplexity minimisation method. This is an encouraging result as it 342 Proceedings of NAACL-HLT 2013, pages 342–347, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics shows that PRO can be adapted to optimise translation parameters other than those in the standard linear model. to simultaneously optimise such non-linear parameters. 2 Optimising Phrase Table Interpolation Weights 2.1 Previou</context>
<context position="5114" citStr="Hopkins and May, 2011" startWordPosition="812" endWordPosition="815">ble. 2.2 Tuning SMT Systems The standard SMT model scores translation hypotheses as a linear combination of features. The model score of a hypothesis e is then defined to be w · h(e, f, a) where w is a weight vector, and h(e, f, a) a vector of feature functions defined over source sentences (f), hypotheses, and their alignments (a). The weights are normally optimised (tuned) to maximise BLEU on a heldout set (the tuning set). The most popular algorithm for this weight optimisation is the line-search based MERT (Och, 2003), but recently other algorithms that support more features, such as PRO (Hopkins and May, 2011) or MIRA-based algorithms (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012), have been introduced. All these algorithms assume that the model score is a linear function of the parameters w. However since the phrase table probabilities enter the score function in log form, if these probabilities are a linear interpolation, then the model score is not a linear function of the interpolation weights. We will show that PRO can be used 2.3 Pairwise Ranked Optimisation PRO is a batch tuning algorithm in the sense that there is an outer loop which repeatedly decodes a small (1000-2</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as Ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in Domain Adaptation for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>224--227</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1666" citStr="Koehn and Schroeder, 2007" startWordPosition="263" endWordPosition="266">ther, and/or from the test data, then the problem of combining these disparate data sets to create the best possible translation system is known as domain adaptation. One approach to domain adaptation is to build separate models for each training domain, then weight them to create a system tuned to the test domain. In SMT, a successful approach to building domain specific language models is to build one from each corpus, then linearly interpolate them, choosing weights that minimise the perplexity on a suitable heldout set of in-domain data. This method has been applied by many authors (e.g. (Koehn and Schroeder, 2007)), and is implemented in popular language modelling tools like IRSTLM (Federico et al., 2008) and SRILM (Stolcke, 2002). Similar interpolation techniques have been developed for translation model interpolation (Foster et al., 2010; Sennrich, 2012) for phrase-based systems but have not been as widely adopted, perhaps because the efficacy of the methods is not as clearcut. In this previous work, the authors used standard phrase extraction heuristics to extract phrases from a heldout set of parallel sentences, then tuned the translation model (i.e. the phrase table) interpolation weights to minim</context>
<context position="4288" citStr="Koehn and Schroeder, 2007" startWordPosition="672" endWordPosition="675">revious Approaches In the work of Foster and Kuhn (2007), linear interpolation weights were derived from different measures of distance between the training corpora, but this was not found to be successful. Optimising the weights to minimise perplexity, as described in the introduction, was found by later authors to be more useful (Foster et al., 2010; Sennrich, 2012), generally showing small improvements over the default approach of concatenating all training data. An alternative approach is to use log-linear interpolation, so that the interpolation weights can be easily optimised in tuning (Koehn and Schroeder, 2007; Bertoldi and Federico, 2009; Banerjee et al., 2011). However, this effectively multiplies the probabilities across phrase tables, which does not seem appropriate, especially for phrases absent from 1 table. 2.2 Tuning SMT Systems The standard SMT model scores translation hypotheses as a linear combination of features. The model score of a hypothesis e is then defined to be w · h(e, f, a) where w is a weight vector, and h(e, f, a) a vector of feature functions defined over source sentences (f), hypotheses, and their alignments (a). The weights are normally optimised (tuned) to maximise BLEU o</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in Domain Adaptation for Statistical Machine Translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 224–227, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Demo Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Alexandra</location>
<contexts>
<context position="10480" citStr="Koehn et al., 2007" startWordPosition="1755" endWordPosition="1758"> respectively, and corresponding 2000 sentence tuning and test sets selected from the st data. The newscommentary v7 corpus and a 200k sentence corpus selected from the remaining st data were used as in-domain training data for the respective domains, with europarl v7 (ep) used as out-of-domain training data in both cases. The language pairs we tested were the WMT language pairs for nc (English (en) to and from Spanish (es), German (de), French (fr) and Czech (cs)), with Dutch (nl) substituted for de in the st experiments. To build phrase-based translation systems, we used the standard Moses (Koehn et al., 2007) training pipeline, in particular employing the usual 5 phrase features – forward and backward phrase probabilities, forward and backward lexical scores and a phrase penalty. The 5-gram Kneser-Ney smoothed language models were trained by SRILM (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. The language model is always a linear interpolation of models estimated on the in- and outof-domain corpora, with weights tuned by SRILM’s perplexity minimisation3. All experiments were run three times with BLEU scores averaged, as recommended by Clark et al. (2011). Performance was evaluated </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the ACL Demo Sessions, pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5019" citStr="Och, 2003" startWordPosition="799" endWordPosition="800">se tables, which does not seem appropriate, especially for phrases absent from 1 table. 2.2 Tuning SMT Systems The standard SMT model scores translation hypotheses as a linear combination of features. The model score of a hypothesis e is then defined to be w · h(e, f, a) where w is a weight vector, and h(e, f, a) a vector of feature functions defined over source sentences (f), hypotheses, and their alignments (a). The weights are normally optimised (tuned) to maximise BLEU on a heldout set (the tuning set). The most popular algorithm for this weight optimisation is the line-search based MERT (Och, 2003), but recently other algorithms that support more features, such as PRO (Hopkins and May, 2011) or MIRA-based algorithms (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012), have been introduced. All these algorithms assume that the model score is a linear function of the parameters w. However since the phrase table probabilities enter the score function in log form, if these probabilities are a linear interpolation, then the model score is not a linear function of the interpolation weights. We will show that PRO can be used 2.3 Pairwise Ranked Optimisation PRO is a batch tun</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="11131" citStr="Papineni et al., 2002" startWordPosition="1853" endWordPosition="1856">ular employing the usual 5 phrase features – forward and backward phrase probabilities, forward and backward lexical scores and a phrase penalty. The 5-gram Kneser-Ney smoothed language models were trained by SRILM (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. The language model is always a linear interpolation of models estimated on the in- and outof-domain corpora, with weights tuned by SRILM’s perplexity minimisation3. All experiments were run three times with BLEU scores averaged, as recommended by Clark et al. (2011). Performance was evaluated using case-insensitive BLEU (Papineni et al., 2002), as implemented in Moses. The baseline systems were tuned using the Moses version of PRO, a reimplementation of the original 2www.opensubtitles.org 3Our method could also be applied to language model interpolation but we chose to focus on phrase tables in this paper. �m j=1 344 algorithm using the sampling scheme recommended by Hopkins and May. We ran 15 iterations of PRO, choosing the weights that maximised BLEU on the tuning set. For the PRO training of the interpolated models, we used the same sampling scheme, with optimisation of the model weights and interpolation weights implemented in </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Perplexity Minimization for Translation Model Domain Adaptation in Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="1913" citStr="Sennrich, 2012" startWordPosition="300" endWordPosition="301">n weight them to create a system tuned to the test domain. In SMT, a successful approach to building domain specific language models is to build one from each corpus, then linearly interpolate them, choosing weights that minimise the perplexity on a suitable heldout set of in-domain data. This method has been applied by many authors (e.g. (Koehn and Schroeder, 2007)), and is implemented in popular language modelling tools like IRSTLM (Federico et al., 2008) and SRILM (Stolcke, 2002). Similar interpolation techniques have been developed for translation model interpolation (Foster et al., 2010; Sennrich, 2012) for phrase-based systems but have not been as widely adopted, perhaps because the efficacy of the methods is not as clearcut. In this previous work, the authors used standard phrase extraction heuristics to extract phrases from a heldout set of parallel sentences, then tuned the translation model (i.e. the phrase table) interpolation weights to minimise the perplexity of the interpolated model on this set of extracted phrases. In this paper, we try to improve on this perplexity optimisation of phrase table interpolation weights by addressing two of its shortcomings. The first problem is that </context>
<context position="4033" citStr="Sennrich, 2012" startWordPosition="636" endWordPosition="637">omputational Linguistics shows that PRO can be adapted to optimise translation parameters other than those in the standard linear model. to simultaneously optimise such non-linear parameters. 2 Optimising Phrase Table Interpolation Weights 2.1 Previous Approaches In the work of Foster and Kuhn (2007), linear interpolation weights were derived from different measures of distance between the training corpora, but this was not found to be successful. Optimising the weights to minimise perplexity, as described in the introduction, was found by later authors to be more useful (Foster et al., 2010; Sennrich, 2012), generally showing small improvements over the default approach of concatenating all training data. An alternative approach is to use log-linear interpolation, so that the interpolation weights can be easily optimised in tuning (Koehn and Schroeder, 2007; Bertoldi and Federico, 2009; Banerjee et al., 2011). However, this effectively multiplies the probabilities across phrase tables, which does not seem appropriate, especially for phrases absent from 1 table. 2.2 Tuning SMT Systems The standard SMT model scores translation hypotheses as a linear combination of features. The model score of a hy</context>
<context position="11958" citStr="Sennrich (2012)" startWordPosition="1984" endWordPosition="1985">on but we chose to focus on phrase tables in this paper. �m j=1 344 algorithm using the sampling scheme recommended by Hopkins and May. We ran 15 iterations of PRO, choosing the weights that maximised BLEU on the tuning set. For the PRO training of the interpolated models, we used the same sampling scheme, with optimisation of the model weights and interpolation weights implemented in Python using scipy4. The implementation is available in Moses, in the contrib/promix directory. The phrase table interpolation and perplexitybased minimisation of interpolation weights used the code accompanying Sennrich (2012), also available in Moses. 3.2 Results For each of the two test sets (nc and st), we compare four different translation systems (three baseline systems, and our new interpolation method): in Phrase and reordering tables were built from just the in-domain data. joint Phrase and reordering tables were built from the in- and out-of-domain data, concatenated. perp Separate phrase tables built on in- and out-ofdomain data, interpolated using perplexity minimisation. The reordering table is as for joint. pro-mix As perp, but interpolation weights optimised using our modified PRO algorithm. So the tw</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012. Perplexity Minimization for Translation Model Domain Adaptation in Statistical Machine Translation. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Intl. Conf. on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="1785" citStr="Stolcke, 2002" startWordPosition="283" endWordPosition="284">stem is known as domain adaptation. One approach to domain adaptation is to build separate models for each training domain, then weight them to create a system tuned to the test domain. In SMT, a successful approach to building domain specific language models is to build one from each corpus, then linearly interpolate them, choosing weights that minimise the perplexity on a suitable heldout set of in-domain data. This method has been applied by many authors (e.g. (Koehn and Schroeder, 2007)), and is implemented in popular language modelling tools like IRSTLM (Federico et al., 2008) and SRILM (Stolcke, 2002). Similar interpolation techniques have been developed for translation model interpolation (Foster et al., 2010; Sennrich, 2012) for phrase-based systems but have not been as widely adopted, perhaps because the efficacy of the methods is not as clearcut. In this previous work, the authors used standard phrase extraction heuristics to extract phrases from a heldout set of parallel sentences, then tuned the translation model (i.e. the phrase table) interpolation weights to minimise the perplexity of the interpolated model on this set of extracted phrases. In this paper, we try to improve on this</context>
<context position="10739" citStr="Stolcke, 2002" startWordPosition="1795" endWordPosition="1796">l v7 (ep) used as out-of-domain training data in both cases. The language pairs we tested were the WMT language pairs for nc (English (en) to and from Spanish (es), German (de), French (fr) and Czech (cs)), with Dutch (nl) substituted for de in the st experiments. To build phrase-based translation systems, we used the standard Moses (Koehn et al., 2007) training pipeline, in particular employing the usual 5 phrase features – forward and backward phrase probabilities, forward and backward lexical scores and a phrase penalty. The 5-gram Kneser-Ney smoothed language models were trained by SRILM (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. The language model is always a linear interpolation of models estimated on the in- and outof-domain corpora, with weights tuned by SRILM’s perplexity minimisation3. All experiments were run three times with BLEU scores averaged, as recommended by Clark et al. (2011). Performance was evaluated using case-insensitive BLEU (Papineni et al., 2002), as implemented in Moses. The baseline systems were tuned using the Moses version of PRO, a reimplementation of the original 2www.opensubtitles.org 3Our method could also be applied to language model interpo</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proc. Intl. Conf. on Spoken Language Processing, vol. 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing (vol V),</booktitle>
<pages>237--248</pages>
<editor>In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors,</editor>
<publisher>John Benjamins, Amsterdam/Philadelphia.</publisher>
<contexts>
<context position="9697" citStr="Tiedemann, 2009" startWordPosition="1629" endWordPosition="1630"> across phrase pairs it is not interpolated, and so is classed with the the “other” features. The lexical scores, although not actually probabilities, are interpolated. are interpolated at the phrase level, then combined to give the sentence level feature value. This makes the definition of the objective more complex than that shown above, but still optimisable using bounded LBFGS. 3 Experiments 3.1 Corpus and Baselines We ran experiments with data from the WMT shared tasks (Callison-Burch et al., 2007; Callison-Burch et al., 2012), as well as OpenSubtitles data2 released by the OPUS project (Tiedemann, 2009). The experiments targeted both the newscommentary (nc) and OpenSubtitles (st) domains, with nc-devtest2007 and nc-test2007 for tuning and testing in the nc domain, respectively, and corresponding 2000 sentence tuning and test sets selected from the st data. The newscommentary v7 corpus and a 200k sentence corpus selected from the remaining st data were used as in-domain training data for the respective domains, with europarl v7 (ep) used as out-of-domain training data in both cases. The language pairs we tested were the WMT language pairs for nc (English (en) to and from Spanish (es), German </context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>J¨org Tiedemann. 2009. News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces. In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing (vol V), pages 237–248. John Benjamins, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online Large-Margin Training for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>764--773</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5162" citStr="Watanabe et al., 2007" startWordPosition="819" endWordPosition="822">l scores translation hypotheses as a linear combination of features. The model score of a hypothesis e is then defined to be w · h(e, f, a) where w is a weight vector, and h(e, f, a) a vector of feature functions defined over source sentences (f), hypotheses, and their alignments (a). The weights are normally optimised (tuned) to maximise BLEU on a heldout set (the tuning set). The most popular algorithm for this weight optimisation is the line-search based MERT (Och, 2003), but recently other algorithms that support more features, such as PRO (Hopkins and May, 2011) or MIRA-based algorithms (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012), have been introduced. All these algorithms assume that the model score is a linear function of the parameters w. However since the phrase table probabilities enter the score function in log form, if these probabilities are a linear interpolation, then the model score is not a linear function of the interpolation weights. We will show that PRO can be used 2.3 Pairwise Ranked Optimisation PRO is a batch tuning algorithm in the sense that there is an outer loop which repeatedly decodes a small (1000-2000 sentence) tuning set and passes the n-best l</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online Large-Margin Training for Statistical Machine Translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 764– 773, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>