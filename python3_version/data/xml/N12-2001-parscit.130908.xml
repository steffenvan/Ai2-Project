<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.117255">
<title confidence="0.979157">
Finding the Right Supervisor: Expert-Finding in a University Domain
</title>
<author confidence="0.996956">
Fawaz Alarfaj, Udo Kruschwitz, David Hunter and Chris Fox
</author>
<affiliation confidence="0.9976275">
School of Computer Science and Electronic Engineering
University of Essex
</affiliation>
<address confidence="0.97418">
Colchester, CO4 3SQ, UK
</address>
<email confidence="0.980946">
{falarf, udo, dkhunter, foxcj}@essex.ac.uk
</email>
<sectionHeader confidence="0.995231" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999785933333333">
Effective knowledge management is a key fac-
tor in the development and success of any or-
ganisation. Many different methods have been
devised to address this need. Applying these
methods to identify the experts within an or-
ganisation has attracted a lot of attention. We
look at one such problem that arises within
universities on a daily basis but has attracted
little attention in the literature, namely the
problem of a searcher who is trying to iden-
tify a potential PhD supervisor, or, from the
perspective of the university’s research office,
to allocate a PhD application to a suitable su-
pervisor. We reduce this problem to identify-
ing a ranked list of experts for a given query
(representing a research area).
We report on experiments to find experts in a
university domain using two different meth-
ods to extract a ranked list of candidates:
a database-driven method and a data-driven
method. The first one is based on a fixed list
of experts (e.g. all members of academic staff)
while the second method is based on auto-
matic Named-Entity Recognition (NER). We
use a graded weighting based on proximity be-
tween query and candidate name to rank the
list of candidates. As a baseline, we use a
system that ranks candidates simply based on
frequency of occurrence within the top docu-
ments.
</bodyText>
<sectionHeader confidence="0.999333" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9993475">
The knowledge and expertise of individuals are sig-
nificant resources for organisation. Managing this
</bodyText>
<page confidence="0.845823">
1
</page>
<bodyText confidence="0.9999655">
intangible resource effectively and efficiently con-
stitutes an essential and very important task (Non-
aka and Takeuchi, 1995; Law and Ngai, 2008). Ap-
proaching experts is the primary and most direct way
of utilising their knowledge (Yang and Huh, 2008;
Li et al., 2011). Therefore, it is important to have a
means of locating the right experts within organisa-
tions. The expert-finding task can be categorised as
an information retrieval task similar to a web search,
but where the results are people rather than docu-
ments. An expert-finding system allows users to in-
put a query, and it returns a ranked list of experts.
Here we look at a university context. We start
with a real-world problem which is to identify a list
of experts within an academic environment, e.g. a
university intranet. The research reported here is
based on an empirical study of a simple but effective
method in which a system that applies the concept of
expert-finding has been designed and implemented.
The proposed system will contribute to provide an
expert-search service to all of the university’s stake-
holders.
Expert-finding systems require two main re-
sources in order to function: a list of candidates and
a collection of data from which the evidence of ex-
pertise can be extracted. We present two approaches
to address this problem, a database-driven and a
data-driven method using NER. The main differ-
ence between the two methods is the way in which
the candidates’ list is constructed. In the database
method, the candidates are simply selected from a
known list of experts, e.g. the university’s academic
staff. In the NER method, the candidates are ex-
tracted automatically from the pages returned by an
</bodyText>
<note confidence="0.380122">
Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 1–6,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999958727272727">
underlying search engine. This method promises to
be more useful for finding experts from a wider (and
possibly more up-to-date) range of candidates. Both
methods apply the same ranking function(s), as will
be discussed below.
This paper will survey related work in Section 2
and introduce the expert-finding task in a university
domain in Section 3. The process of ranking experts
will be discussed in Section 4. The evaluation will
be described in Section 4, followed by a discussion
of the experiment’s results in Section 5.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99994006097561">
The expert-finding task addresses the problem of re-
trieving a ranked list of people who are knowledge-
able about a given topic. This task has found its
place in the commercial environment as early as the
1980’s, as discussed in Maybury (2006); however,
there was very limited academic research on finding
and ranking experts until the introduction of the en-
terprise track at the 2005 Text REtrieval Conference
(TREC) (Craswell et al., 2005).
When expert-finding we must know the experts’
profiles, These profiles may be generated manually
or automatically. Manually created profiles may be
problematic. If, for example, experts enter their own
information, they may exaggerate or downplay their
expertise. In addition, any changes of expertise for
any expert requires a manual update to the expert’s
profile. Thus incurring high maintenance costs. An
example of manually generated profiles is the work
of Dumais and Nielsen (1992). Although their sys-
tem automatically assigns submitted manuscripts to
reviewers, the profiles of the reviewers or experts are
created manually.
The alternative is to generate the profiles automat-
ically, for example by extracting relevant informa-
tion from a document collection. The assumption is
that individuals will tend to be expert in the topics
of documents with which they are associated. Ex-
perts can be associated with the documents in which
they are mentioned (Craswell et al., 2001) or with
e-mails they have sent or received (Balog and de Ri-
jke, 2006a; Campbell et al., 2003; Dom et al., 2003).
They can also be associated with their home pages
or CVs (Maybury et al., 2001), and with documents
they have written (Maybury et al., 2001; Becerra-
Fernandez, 2000). Finally, some researchers use
search logs to associate experts with the web pages
they have visited (Wang et al., 2002; Macdonald and
White, 2009).
After associating candidate experts with one or
more of the kinds of textual evidence mentioned
above, the next step is to find and rank candidates
based on a user query. Many methods have been
proposed to perform this task. Craswell et al. (2001)
create virtual documents for each candidate (or em-
ployee). These virtual documents are simply con-
catenated texts of all documents from the corpus as-
sociated with a particular candidate. Afterwards,
the system indexes and processes queries for the
employee’s documents. The results would show a
list of experts based on the ten best matching em-
ployee documents. Liu et al. (2005) have applied
expert-search in the context of a community-based
question-answering service. Based on a virtual doc-
ument approach, their work applied three language
models: the query likelihood model, the relevance
model and the cluster-based language model. They
concluded that combining language models can en-
hance the retrieval performance.
Two principal approaches recognised for expert-
finding can be found in the literature. Both were
first proposed by Balog et al. (2006b). The mod-
els are called the candidate model and the doc-
ument model, or Model 1 and Model 2, respec-
tively. Different names have been used for the
two methods. Fang and Zhai (2007) refer to them
as ‘Candidate Generation Models and Topic Gen-
eration Models’. Petkova and Croft (2006) call
them the ‘Query-Dependent Model’ and the ‘Query-
Independent Model’. The main difference between
the models is that the candidate-based approaches
(Model 1) build a textual representation of candi-
date experts, and then rank the candidates based on
the given query, whereas the document-based ap-
proaches (Model 2) first find documents that are rel-
evant to the query, and then locate the associated ex-
perts in these documents.
Balog et al. (2006b) have compared the two mod-
els and concluded that Model 2 outperforms Model
1 on all measures (for this reason, we will adopt
Model 2).
As Model 2 proved to be more efficient, it formed
the basis of many other expert-search systems (Fang
</bodyText>
<page confidence="0.962554">
2
</page>
<bodyText confidence="0.999962972222223">
and Zhai, 2007; Petkova and Croft, 2007; Yao
et al., 2008). Fang and Zhai developed a mixture
model using proximity-based document representa-
tion. This model makes it possible to put different
weights on different representations of a candidate
expert (Fang and Zhai, 2007). Another mixture of
personal and global language models was proposed
by Serdyukov and Hiemstra (2008). They combined
two criteria for personal expertise in the final rank-
ing: the probability of generation of the query by the
personal language model and a prior probability of
candidate experts that expresses their level of activ-
ity in the important discussions on the query topic.
Zhu et al. (2010) claimed that earlier language
models did not consider document features. They
proposed an approach that incorporates: internal
document structure; document URLs; page rank;
anchor texts; and multiple levels of association be-
tween experts and topics.
All of the proposed frameworks assume that the
more documents associated with a candidate that
score highly with respect to a query, the more likely
the candidate is to have relevant expertise for that
query. Macdonald and Ounis (2008) developed a
different approach, called the Voting Model. In their
model, candidate experts are ranked first by consid-
ering a ranking of documents with respect to the
users’ query. Then, using the candidate profiles,
votes from the ranked documents are converted into
votes for candidates.
There have been attempts to tackle the expert-
finding problem using social networks. This has
mainly been investigated from two directions. The
first direction uses graph-based measures on social
networks to produce a ranking of experts (Campbell
et al., 2003; Dom et al., 2003). The second direc-
tion assumes similarities among the neighbours in a
social network and defines a smoothing procedure
to rank experts (Karimzadehgan et al., 2009; Zhang
et al., 2007).
Some have argued that it is not enough to find ex-
perts by looking only at the queries’ without tak-
ing the users into consideration. They claim that
there are several factors that may play a role in
decisions concerning which experts to recommend.
Some of these factors are the users’ expertise level,
social proximity and physical proximity (Borgatti
and Cross, 2003; McDonald and Ackerman, 1998;
Shami et al., 2008). McDonald and Ackerman
(1998) emphasised the importance of the accessi-
bility of the expert. They argued that people usu-
ally prefer to contact the experts who are physically
or organisationally close to them. Moreover, Shami
et al. (2008) found that people prefer to contact ex-
perts they know, even when they could potentially
receive more information from other experts who are
located outside their social network.
Woudstra and van den Hooff (2008) identified a
number of factors in selecting experts that are re-
lated to quality and accessibility. They argued that
the process of choosing which candidate expert to
contact might differ depending on the specific situa-
tion.
Hofmann et al. (2010) showed that many of these
factors can be modelled. They claimed that integrat-
ing them with retrieval models can improve retrieval
performance. Smirnova and Balog (2011) provided
a user-oriented model for expert-finding where they
placed an emphasis on the social distance between
the user and the expert. They considered a number
of social graphs based on organisational hierarchy,
geographical location and collaboration.
</bodyText>
<sectionHeader confidence="0.976145" genericHeader="method">
3 Expert-Finding in a University
</sectionHeader>
<bodyText confidence="0.9999931875">
In any higher educational institution, finding an ap-
propriate supervisor is a critical task for research
students, a task that can be very time consuming,
especially if academics describe their work using
terms that a student is not familiar with. A searcher
may build up a picture of who is likely to have
the relevant expertise by looking for university aca-
demic staff who have written numerous documents
about the general topic, who have authored docu-
ments exactly related to the topic, or who list the
topic as one of their research interest areas. Au-
tomating this process will not only help research stu-
dents find the most suitable supervisors, but it also
allow the university to allocate applications to super-
visors, and help researchers find other people inter-
ested in the particular topics.
</bodyText>
<subsectionHeader confidence="0.998282">
3.1 Method
</subsectionHeader>
<bodyText confidence="0.9997925">
The two approaches we apply, database-driven, and
data-driven using NER1 are illustrated in Figure 1.
</bodyText>
<footnote confidence="0.970198">
1We use OpenNLP to identify named entities.
</footnote>
<page confidence="0.993584">
3
</page>
<figureCaption confidence="0.999958">
Figure 1: System Architecture.
</figureCaption>
<bodyText confidence="0.999217571428571">
times the candidate’s name appears in the document
d(cc). Then it calculates the candidate metric cm by
dividing the candidate count d(cc) by the number of
tokens in the document d(nt).
Equation 1 defines the metric, where cm is the
final candidate’s metric for all documents and n is
the number of documents.
</bodyText>
<subsectionHeader confidence="0.998896">
3.3 Our Approach
</subsectionHeader>
<bodyText confidence="0.999654153846154">
Our approach takes into account the proximity be-
tween query terms and candidate names in the
matching documents in the form of a distance
weight. This measure will adds a distance weight
value to the main candidate’s metric that was gener-
ated earlier. Similar approaches have been proposed
in the literature for different expert search applica-
tions Lu et al. (2006); Cao et al. (2005). The dis-
tance weight will be higher whenever the name ap-
pears closer to the query term, within a +/- 10 word
window.
We experiment with two different formulae. The
first formula is as follows:
</bodyText>
<figure confidence="0.997098305555555">
Candidate Rank
For Page
Candidate Rank
For Page
Display Experts
Update
Candidate Rank
Named-Entity Recognition
Method
Candidate List
Named-Entity
Recognition
Query Search API URLs URL Filter Filtered URLs
Normalized
Web Page
String
Candidate
Evaluator
HTML Parser
Parsed Html
Page
Normalizer
Normalized
Web Page
String
Database Method
Web Page
String
Candidate
Evaluator
DB Connection
Page Reader
Candidate
Extractor
Candidate
List
</figure>
<equation confidence="0.953645272727273">
d(cc) (1)
d(nt)
n
X
d=1
cm =
Xn
i=1
cm1 =
Xm
j=1
</equation>
<bodyText confidence="0.999954944444445">
The main difference between the two methods is the
way in which the candidates’ list is constructed. We
argue that each method has its advantages. In the
database method, the candidates are simply the uni-
versity’s academic staff. This avoids giving results
unrelated to the university. It would be appropriate if
the aim is to find the experts from among the univer-
sity academics. In the data-driven method, the can-
didates are extracted from the pages returned by the
underlying search engine. The experts found by this
method are not necessarily university staff. They
could be former academics, PhD students, visiting
professors, or newly appointed staff.
Both methods apply the same ranking functions,
one baseline function which is purely based on fre-
quency and one which takes into account proximity
of query terms with matches of potential candidates
in the retrieved set of documents.
</bodyText>
<subsectionHeader confidence="0.999446">
3.2 The Baseline Approach
</subsectionHeader>
<bodyText confidence="0.997715333333333">
The baseline we chose for ranking candidates is the
frequency of appearance of names in the top twenty
retrieved documents. The system counts how many
</bodyText>
<equation confidence="0.9916748">
~dij
1 if dij &lt; 10
(cm + ), αij =
� � αij 0 Otherwise
(2)
</equation>
<bodyText confidence="0.999995833333333">
where n is the number of times the candidate’s name
has been found in the matching documents, m is the
number of times the (full) query has been identified,
and dij is the distance between the name position
and query position (Q has been set empirically to 3).
The second formula is:
</bodyText>
<equation confidence="0.99184325">
� dij
1 if dij &lt; 10
(cm + cmαt3 ), αij = 0 Otherwise
(3)
</equation>
<bodyText confidence="0.999559285714286">
This equation is designed to return a smaller value
as the distance x increases, and to give the candidate
with lower frequency a higher weight.
In both cases, candidates are ranked according to
the final score and displayed in order so that the can-
didates who are most likely to be experts are dis-
played at the top of the list.
</bodyText>
<sectionHeader confidence="0.997796" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999643">
As with any IR system, evaluation can be difficult.
In the given context one might argue that precision
</bodyText>
<equation confidence="0.9915908">
Xn
i=1
cm2 =
Xm
j=1
</equation>
<page confidence="0.983116">
4
</page>
<bodyText confidence="0.9957048125">
is more important than recall. In any case, recall can
be difficult to measure precisely. To address these is-
sues we approximate a gold standard as follows. We
selected one school within the university for which
a page of research topics with corresponding aca-
demics exists In this experiment we take this map-
ping as a complete set of correct matches. In this
page, there are 371 topics (i.e. potential queries) di-
vided among 28 more general research topics. Each
topic/query is associated with one or more of the
school’s academic staff. It is presumed that those
names belong to experts on the corresponding top-
ics.
Table 1 illustrates some general topics with the
number of (sub)topic they contain. Table 2 list some
of the topics.
</bodyText>
<table confidence="0.9988275">
Topic N
Analogue and Digital Systems Architectures 2
Artificial Intelligence 26
Audio 12
Brain Computer Interface 18
Computational Finance Economics and Management 1
Computational Intelligence 10
. . . . . .
</table>
<tableCaption confidence="0.998856">
Table 1: Distribution of topics - N denotes the number of
topics for the corresponding general topic area.
</tableCaption>
<table confidence="0.945835875">
High-Speed Lasers And Photodetectors
Human Behaviour And The Psychology
Human Motion Tracking
Human-Centred Robotics
Hybrid Heuristics
Hybrid Intelligent Systems Which Include Neuro-Fuzzy Systems
Hypercomplex Algebras And Fourier Transforms
Hypercomplex Fourier Transforms And Filters
</table>
<tableCaption confidence="0.997312">
Table 2: Some topics/queries
</tableCaption>
<bodyText confidence="0.99987425">
The measure used to test the system is recall at
the following values 13, 5, 7, 10, 15, 201. We
also measure Mean Average Precision at rank 20
(MAP@20).
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999700916666667">
Table 3 shows the system results where BL is the
baseline result. There are two main findings. First
of all, the database-driven approach outperforms
the data-driven approach. Secondly, our approach
which applies a grading of results based on prox-
imity between queries and potential expert names
significantly outperforms the baseline approach that
only considers frequency, that is true for both formu-
lae we apply when ranking the results (using paired
t-tests applied to MAP with p&lt;0.0001). However,
the differences between cm1 and cm2 tend not to be
significantly different.
</bodyText>
<table confidence="0.999644666666667">
BL cm1 cm2
NER DB NER DB NER DB
R@3 0.47 0.48 0.49 0.76 0.58 0.79
R@5 0.56 0.60 0.58 0.83 0.68 0.86
R@7 0.61 0.64 0.62 0.87 0.72 0.88
R@10 0.65 0.69 0.68 0.89 0.78 0.90
R@15 0.69 0.72 0.74 0.91 0.80 0.91
R@20 0.71 0.75 0.76 0.92 0.82 0.93
MAP 0.20 0.28 0.50 0.61 0.52 0.66
</table>
<tableCaption confidence="0.999513">
Table 3: Performance Measures
</tableCaption>
<bodyText confidence="0.9998122">
It is perhaps important to mention that our data is
fairly clean. More noise would make the creation of
relational database more difficult. In that case the
data-driven approach may become more appropri-
ate.
</bodyText>
<sectionHeader confidence="0.999231" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999961043478261">
The main objective of this work was to explore
expert-finding in a university domain, an area that
has to the best of our knowledge so far attracted
little attention in the literature. The main finding
is that a database-driven approach (utilising a fixed
set of known experts) outperforms a data-driven ap-
proach which is based on automatic named-entity
recognition. Furthermore, exploiting proximity be-
tween query and candidate outperforms a straight
frequency measure.
There are a number of directions for future work.
For example, modelling the user background and
interests could increase the system’s effectiveness.
Some more realistic end-user studies could be used
to evaluate the systems. Consideration could be
given to term dependence and positional models as
in Metzler and Croft (2005), which might improve
our proximity-based scoring function. Finaly, our
gold standard collection penalises a data-driven ap-
proach, which might offer a broader range of ex-
perts. We will continue this line of work using
both technical evaluation measures as well as user-
focused evaluations.
</bodyText>
<page confidence="0.991184">
5
</page>
<sectionHeader confidence="0.990068" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999632269565217">
K. Balog and M. de Rijke. Finding experts and their details in e-mail
corpora. In Proceedings of the 15th international conference on
World Wide Web, pages 1035–1036. ACM, 2006a.
K. Balog, L. Azzopardi, and M. de Rijke. Formal models for expert
finding in enterprise corpora. In Proceedings of the 29th annual
international ACM SIGIR conference on Research and development
in information retrieval, pages 43–50. ACM, 2006b.
I. Becerra-Fernandez. Facilitating the online search of experts at NASA
using expert seeker people-finder. In Proceedings of the 3rd Interna-
tional Conference on Practical Aspects of Knowledge Management
(PAKM), Basel, Switzerland, 2000.
S.P. Borgatti and R. Cross. A relational view of information seeking and
learning in social networks. Management science, 49(4):432–445,
2003.
C.S. Campbell, P.P. Maglio, A. Cozzi, and B. Dom. Expertise identifica-
tion using email communications. In Proceedings of the twelfth in-
ternational conference on Information and knowledge management,
pages 528–531. ACM, 2003.
Y. Cao, J. Liu, S. Bao, and H. Li. Research on expert search at enterprise
track of trec 2005. In 14th Text Retrieval Conference (TREC 2005),
2005.
N. Craswell, D. Hawking, A.M. Vercoustre, and P. Wilkins. P@ noptic
expert: Searching for experts not just for documents. In Ausweb
Poster Proceedings, Queensland, Australia, 2001.
N. Craswell, A.P. de Vries, and I. Soboroff. Overview of the TREC-
2005 enterprise track. In TREC 2005 Conference Notebook, pages
199–205, 2005.
B. Dom, I. Eiron, A. Cozzi, and Y. Zhang. Graph-based ranking algo-
rithms for e-mail expertise analysis. In Proceedings of the 8th ACM
SIGMOD workshop on Research issues in data mining and knowl-
edge discovery, pages 42–48. ACM, 2003.
S.T. Dumais and J. Nielsen. Automating the assignment of submitted
manuscripts to reviewers. In Proceedings of the 15th annual inter-
national ACM SIGIR conference on Research and development in
information retrieval, pages 233–244. ACM, 1992.
H. Fang and C.X. Zhai. Probabilistic models for expert finding. Ad-
vances in Information Retrieval, pages 418–430, 2007.
K. Hofmann, K. Balog, T. Bogers, and M. de Rijke. Contextual fac-
tors for finding similar experts. Journal of the American Society for
Information Science and Technology, 61(5):994–1014, 2010.
M. Karimzadehgan, R. White, and M. Richardson. Enhancing expert
finding using organizational hierarchies. Advances in Information
Retrieval, pages 177–188, 2009.
C. Law and E. Ngai. An empirical study of the effects of knowledge
sharing and learning behaviors on firm performance. Expert Systems
with Applications, 34(4):2342–2349, 2008.
M. Li, L. Liu, and C. Li. An approach to expert recommendation based
on fuzzy linguistic method and fuzzy text classification in knowl-
edge management systems. Expert Systems with Applications, 38
(7):8586–8596, 2011.
X. Liu, W.B. Croft, and M. Koll. Finding experts in community-based
question-answering services. In Proceedings of the 14th ACM in-
ternational conference on Information and knowledge management,
pages 315–316. ACM, 2005.
W. Lu, S. Robertson, A. MacFarlane, and H. Zhao. Window-based
enterprise expert search. In Proceeddings of the 15th Text REtrieval
Conference (TREC 2006). NIST, 2006.
C. Macdonald and I. Ounis. Voting techniques for expert search. Knowl-
edge and information systems, 16(3):259–280, 2008.
C. Macdonald and R.W. White. Usefulness of click-through data in
expert search. In SIGIR, volume 9, pages 816–817, 2009.
M. Maybury, R. D’Amore, and D. House. Expert finding for collabo-
rative virtual environments. Communications of the ACM, 44(12):
55–56, 2001.
M.T. Maybury. Expert finding systems. MITRE Center for Integrated
Intelligence Systems Bedford, Massachusetts, USA, 2006.
D.W. McDonald and M.S. Ackerman. Just talk to me: a field study
of expertise location. In Proceedings of the 1998 ACM conference
on Computer supported cooperative work, pages 315–324. ACM,
1998.
D. Metzler and W.B. Croft. A markov random field model for term de-
pendencies. In Proceedings of the 28th annual international ACM
SIGIR conference on Research and development in information re-
trieval, pages 472–479. ACM, 2005.
I. Nonaka and H. Takeuchi. The knowledge creating company: How
Japanese The knowledge creating company: How Japanese compa-
nies create the dynamics of innovation. Oxford University Press,
New York, 1995.
D. Petkova and W.B. Croft. Hierarchical language models for ex-
pert finding in enterprise corpora. In Tools with Artificial Intel-
ligence, 2006. ICTAI’06. 18th IEEE International Conference on,
pages 599–608. IEEE, 2006.
D. Petkova and W.B. Croft. Proximity-based document representation
for named entity retrieval. In Proceedings of the sixteenth ACM con-
ference on Conference on information and knowledge management,
pages 731–740. ACM, 2007.
P. Serdyukov and D. Hiemstra. Modeling documents as mixtures of
persons for expert finding. Advances in Information Retrieval, pages
309–320, 2008.
N.S. Shami, K. Ehrlich, and D.R. Millen. Pick me: link selection in
expertise search results. In Proceeding of the twenty-sixth annual
SIGCHI conference on Human factors in computing systems, pages
1089–1092. ACM, 2008.
E. Smirnova and K. Balog. A user-oriented model for expert finding.
Advances in Information Retrieval, pages 580–592, 2011.
J. Wang, Z. Chen, L. Tao, W.Y. Ma, and L. Wenyin. Ranking user’s
relevance to a topic through link analysis on web logs. In Proceed-
ings of the 4th international workshop on Web information and data
management, pages 49–54. ACM, 2002.
L. Woudstra and B. van den Hooff. Inside the source selection pro-
cess: Selection criteria for human information sources. Information
Processing &amp; Management, 44(3):1267–1278, 2008.
K. Yang and S. Huh. Automatic expert identification using a text au-
tomatic expert identification using a text categorization technique in
knowledge management systems. Expert Systems with Expert Sys-
tems with Applications, 34(2):1445–1455, 2008.
J. Yao, J. Xu, and J. Niu. Using role determination and expert min-
ing in the enterprise environment. In Proceedings of the 2008 Text
REtrieval Conference (TREC 2008), 2008.
J. Zhang, J. Tang, and J. Li. Expert finding in a social network. Ad-
vances in Databases: Concepts, Systems and Applications, pages
1066–1069, 2007.
J. Zhu, X. Huang, D. Song, and S. R¨uger. Integrating multiple doc-
ument features in language models for expert finding. Knowledge
and Information Systems, 23(1):29–54, 2010.
</reference>
<page confidence="0.998763">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.556085">
<title confidence="0.999332">Finding the Right Supervisor: Expert-Finding in a University Domain</title>
<author confidence="0.850439">Fawaz Alarfaj</author>
<author confidence="0.850439">Udo Kruschwitz</author>
<author confidence="0.850439">David Hunter</author>
<author confidence="0.850439">Chris</author>
<affiliation confidence="0.994534">School of Computer Science and Electronic University of</affiliation>
<address confidence="0.816337">Colchester, CO4 3SQ,</address>
<email confidence="0.968585">udo,dkhunter,</email>
<abstract confidence="0.994152774193548">Effective knowledge management is a key factor in the development and success of any organisation. Many different methods have been devised to address this need. Applying these methods to identify the experts within an organisation has attracted a lot of attention. We look at one such problem that arises within universities on a daily basis but has attracted little attention in the literature, namely the problem of a searcher who is trying to identify a potential PhD supervisor, or, from the perspective of the university’s research office, to allocate a PhD application to a suitable supervisor. We reduce this problem to identifying a ranked list of experts for a given query (representing a research area). We report on experiments to find experts in a university domain using two different methods to extract a ranked list of candidates: a database-driven method and a data-driven method. The first one is based on a fixed list of experts (e.g. all members of academic staff) while the second method is based on automatic Named-Entity Recognition (NER). We use a graded weighting based on proximity between query and candidate name to rank the list of candidates. As a baseline, we use a system that ranks candidates simply based on frequency of occurrence within the top documents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Balog</author>
<author>M de Rijke</author>
</authors>
<title>Finding experts and their details in e-mail corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web,</booktitle>
<pages>1035--1036</pages>
<publisher>ACM,</publisher>
<marker>Balog, de Rijke, 2006</marker>
<rawString>K. Balog and M. de Rijke. Finding experts and their details in e-mail corpora. In Proceedings of the 15th international conference on World Wide Web, pages 1035–1036. ACM, 2006a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Balog</author>
<author>L Azzopardi</author>
<author>M de Rijke</author>
</authors>
<title>Formal models for expert finding in enterprise corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>43--50</pages>
<publisher>ACM,</publisher>
<marker>Balog, Azzopardi, de Rijke, 2006</marker>
<rawString>K. Balog, L. Azzopardi, and M. de Rijke. Formal models for expert finding in enterprise corpora. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 43–50. ACM, 2006b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Becerra-Fernandez</author>
</authors>
<title>Facilitating the online search of experts at NASA using expert seeker people-finder.</title>
<date>2000</date>
<booktitle>In Proceedings of the 3rd International Conference on Practical Aspects of Knowledge Management (PAKM),</booktitle>
<location>Basel, Switzerland,</location>
<marker>Becerra-Fernandez, 2000</marker>
<rawString>I. Becerra-Fernandez. Facilitating the online search of experts at NASA using expert seeker people-finder. In Proceedings of the 3rd International Conference on Practical Aspects of Knowledge Management (PAKM), Basel, Switzerland, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Borgatti</author>
<author>R Cross</author>
</authors>
<title>A relational view of information seeking and learning in social networks.</title>
<date>2003</date>
<journal>Management science,</journal>
<volume>49</volume>
<issue>4</issue>
<contexts>
<context position="10269" citStr="Borgatti and Cross, 2003" startWordPosition="1661" endWordPosition="1664">to produce a ranking of experts (Campbell et al., 2003; Dom et al., 2003). The second direction assumes similarities among the neighbours in a social network and defines a smoothing procedure to rank experts (Karimzadehgan et al., 2009; Zhang et al., 2007). Some have argued that it is not enough to find experts by looking only at the queries’ without taking the users into consideration. They claim that there are several factors that may play a role in decisions concerning which experts to recommend. Some of these factors are the users’ expertise level, social proximity and physical proximity (Borgatti and Cross, 2003; McDonald and Ackerman, 1998; Shami et al., 2008). McDonald and Ackerman (1998) emphasised the importance of the accessibility of the expert. They argued that people usually prefer to contact the experts who are physically or organisationally close to them. Moreover, Shami et al. (2008) found that people prefer to contact experts they know, even when they could potentially receive more information from other experts who are located outside their social network. Woudstra and van den Hooff (2008) identified a number of factors in selecting experts that are related to quality and accessibility. </context>
</contexts>
<marker>Borgatti, Cross, 2003</marker>
<rawString>S.P. Borgatti and R. Cross. A relational view of information seeking and learning in social networks. Management science, 49(4):432–445, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Campbell</author>
<author>P P Maglio</author>
<author>A Cozzi</author>
<author>B Dom</author>
</authors>
<title>Expertise identification using email communications.</title>
<date>2003</date>
<booktitle>In Proceedings of the twelfth international conference on Information and knowledge management,</booktitle>
<pages>528--531</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="5587" citStr="Campbell et al., 2003" startWordPosition="903" endWordPosition="906">he work of Dumais and Nielsen (1992). Although their system automatically assigns submitted manuscripts to reviewers, the profiles of the reviewers or experts are created manually. The alternative is to generate the profiles automatically, for example by extracting relevant information from a document collection. The assumption is that individuals will tend to be expert in the topics of documents with which they are associated. Experts can be associated with the documents in which they are mentioned (Craswell et al., 2001) or with e-mails they have sent or received (Balog and de Rijke, 2006a; Campbell et al., 2003; Dom et al., 2003). They can also be associated with their home pages or CVs (Maybury et al., 2001), and with documents they have written (Maybury et al., 2001; BecerraFernandez, 2000). Finally, some researchers use search logs to associate experts with the web pages they have visited (Wang et al., 2002; Macdonald and White, 2009). After associating candidate experts with one or more of the kinds of textual evidence mentioned above, the next step is to find and rank candidates based on a user query. Many methods have been proposed to perform this task. Craswell et al. (2001) create virtual do</context>
<context position="9699" citStr="Campbell et al., 2003" startWordPosition="1566" endWordPosition="1569"> is to have relevant expertise for that query. Macdonald and Ounis (2008) developed a different approach, called the Voting Model. In their model, candidate experts are ranked first by considering a ranking of documents with respect to the users’ query. Then, using the candidate profiles, votes from the ranked documents are converted into votes for candidates. There have been attempts to tackle the expertfinding problem using social networks. This has mainly been investigated from two directions. The first direction uses graph-based measures on social networks to produce a ranking of experts (Campbell et al., 2003; Dom et al., 2003). The second direction assumes similarities among the neighbours in a social network and defines a smoothing procedure to rank experts (Karimzadehgan et al., 2009; Zhang et al., 2007). Some have argued that it is not enough to find experts by looking only at the queries’ without taking the users into consideration. They claim that there are several factors that may play a role in decisions concerning which experts to recommend. Some of these factors are the users’ expertise level, social proximity and physical proximity (Borgatti and Cross, 2003; McDonald and Ackerman, 1998;</context>
</contexts>
<marker>Campbell, Maglio, Cozzi, Dom, 2003</marker>
<rawString>C.S. Campbell, P.P. Maglio, A. Cozzi, and B. Dom. Expertise identification using email communications. In Proceedings of the twelfth international conference on Information and knowledge management, pages 528–531. ACM, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Cao</author>
<author>J Liu</author>
<author>S Bao</author>
<author>H Li</author>
</authors>
<title>Research on expert search at enterprise track of trec</title>
<date>2005</date>
<booktitle>In 14th Text Retrieval Conference (TREC</booktitle>
<contexts>
<context position="13173" citStr="Cao et al. (2005)" startWordPosition="2132" endWordPosition="2135">ric cm by dividing the candidate count d(cc) by the number of tokens in the document d(nt). Equation 1 defines the metric, where cm is the final candidate’s metric for all documents and n is the number of documents. 3.3 Our Approach Our approach takes into account the proximity between query terms and candidate names in the matching documents in the form of a distance weight. This measure will adds a distance weight value to the main candidate’s metric that was generated earlier. Similar approaches have been proposed in the literature for different expert search applications Lu et al. (2006); Cao et al. (2005). The distance weight will be higher whenever the name appears closer to the query term, within a +/- 10 word window. We experiment with two different formulae. The first formula is as follows: Candidate Rank For Page Candidate Rank For Page Display Experts Update Candidate Rank Named-Entity Recognition Method Candidate List Named-Entity Recognition Query Search API URLs URL Filter Filtered URLs Normalized Web Page String Candidate Evaluator HTML Parser Parsed Html Page Normalizer Normalized Web Page String Database Method Web Page String Candidate Evaluator DB Connection Page Reader Candidate</context>
</contexts>
<marker>Cao, Liu, Bao, Li, 2005</marker>
<rawString>Y. Cao, J. Liu, S. Bao, and H. Li. Research on expert search at enterprise track of trec 2005. In 14th Text Retrieval Conference (TREC 2005), 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Craswell</author>
<author>D Hawking</author>
<author>A M Vercoustre</author>
<author>P Wilkins</author>
</authors>
<title>P@ noptic expert: Searching for experts not just for documents.</title>
<date>2001</date>
<booktitle>In Ausweb Poster Proceedings,</booktitle>
<location>Queensland, Australia,</location>
<contexts>
<context position="5494" citStr="Craswell et al., 2001" startWordPosition="885" endWordPosition="888">profile. Thus incurring high maintenance costs. An example of manually generated profiles is the work of Dumais and Nielsen (1992). Although their system automatically assigns submitted manuscripts to reviewers, the profiles of the reviewers or experts are created manually. The alternative is to generate the profiles automatically, for example by extracting relevant information from a document collection. The assumption is that individuals will tend to be expert in the topics of documents with which they are associated. Experts can be associated with the documents in which they are mentioned (Craswell et al., 2001) or with e-mails they have sent or received (Balog and de Rijke, 2006a; Campbell et al., 2003; Dom et al., 2003). They can also be associated with their home pages or CVs (Maybury et al., 2001), and with documents they have written (Maybury et al., 2001; BecerraFernandez, 2000). Finally, some researchers use search logs to associate experts with the web pages they have visited (Wang et al., 2002; Macdonald and White, 2009). After associating candidate experts with one or more of the kinds of textual evidence mentioned above, the next step is to find and rank candidates based on a user query. M</context>
</contexts>
<marker>Craswell, Hawking, Vercoustre, Wilkins, 2001</marker>
<rawString>N. Craswell, D. Hawking, A.M. Vercoustre, and P. Wilkins. P@ noptic expert: Searching for experts not just for documents. In Ausweb Poster Proceedings, Queensland, Australia, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Craswell</author>
<author>A P de Vries</author>
<author>I Soboroff</author>
</authors>
<title>Overview of the TREC2005 enterprise track.</title>
<date>2005</date>
<booktitle>In TREC 2005 Conference Notebook,</booktitle>
<pages>199--205</pages>
<marker>Craswell, de Vries, Soboroff, 2005</marker>
<rawString>N. Craswell, A.P. de Vries, and I. Soboroff. Overview of the TREC2005 enterprise track. In TREC 2005 Conference Notebook, pages 199–205, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dom</author>
<author>I Eiron</author>
<author>A Cozzi</author>
<author>Y Zhang</author>
</authors>
<title>Graph-based ranking algorithms for e-mail expertise analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery,</booktitle>
<pages>42--48</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="5606" citStr="Dom et al., 2003" startWordPosition="907" endWordPosition="910">ielsen (1992). Although their system automatically assigns submitted manuscripts to reviewers, the profiles of the reviewers or experts are created manually. The alternative is to generate the profiles automatically, for example by extracting relevant information from a document collection. The assumption is that individuals will tend to be expert in the topics of documents with which they are associated. Experts can be associated with the documents in which they are mentioned (Craswell et al., 2001) or with e-mails they have sent or received (Balog and de Rijke, 2006a; Campbell et al., 2003; Dom et al., 2003). They can also be associated with their home pages or CVs (Maybury et al., 2001), and with documents they have written (Maybury et al., 2001; BecerraFernandez, 2000). Finally, some researchers use search logs to associate experts with the web pages they have visited (Wang et al., 2002; Macdonald and White, 2009). After associating candidate experts with one or more of the kinds of textual evidence mentioned above, the next step is to find and rank candidates based on a user query. Many methods have been proposed to perform this task. Craswell et al. (2001) create virtual documents for each ca</context>
<context position="9718" citStr="Dom et al., 2003" startWordPosition="1570" endWordPosition="1573">pertise for that query. Macdonald and Ounis (2008) developed a different approach, called the Voting Model. In their model, candidate experts are ranked first by considering a ranking of documents with respect to the users’ query. Then, using the candidate profiles, votes from the ranked documents are converted into votes for candidates. There have been attempts to tackle the expertfinding problem using social networks. This has mainly been investigated from two directions. The first direction uses graph-based measures on social networks to produce a ranking of experts (Campbell et al., 2003; Dom et al., 2003). The second direction assumes similarities among the neighbours in a social network and defines a smoothing procedure to rank experts (Karimzadehgan et al., 2009; Zhang et al., 2007). Some have argued that it is not enough to find experts by looking only at the queries’ without taking the users into consideration. They claim that there are several factors that may play a role in decisions concerning which experts to recommend. Some of these factors are the users’ expertise level, social proximity and physical proximity (Borgatti and Cross, 2003; McDonald and Ackerman, 1998; Shami et al., 2008</context>
</contexts>
<marker>Dom, Eiron, Cozzi, Zhang, 2003</marker>
<rawString>B. Dom, I. Eiron, A. Cozzi, and Y. Zhang. Graph-based ranking algorithms for e-mail expertise analysis. In Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery, pages 42–48. ACM, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S T Dumais</author>
<author>J Nielsen</author>
</authors>
<title>Automating the assignment of submitted manuscripts to reviewers.</title>
<date>1992</date>
<booktitle>In Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>233--244</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="5002" citStr="Dumais and Nielsen (1992)" startWordPosition="808" endWordPosition="811">and ranking experts until the introduction of the enterprise track at the 2005 Text REtrieval Conference (TREC) (Craswell et al., 2005). When expert-finding we must know the experts’ profiles, These profiles may be generated manually or automatically. Manually created profiles may be problematic. If, for example, experts enter their own information, they may exaggerate or downplay their expertise. In addition, any changes of expertise for any expert requires a manual update to the expert’s profile. Thus incurring high maintenance costs. An example of manually generated profiles is the work of Dumais and Nielsen (1992). Although their system automatically assigns submitted manuscripts to reviewers, the profiles of the reviewers or experts are created manually. The alternative is to generate the profiles automatically, for example by extracting relevant information from a document collection. The assumption is that individuals will tend to be expert in the topics of documents with which they are associated. Experts can be associated with the documents in which they are mentioned (Craswell et al., 2001) or with e-mails they have sent or received (Balog and de Rijke, 2006a; Campbell et al., 2003; Dom et al., 2</context>
</contexts>
<marker>Dumais, Nielsen, 1992</marker>
<rawString>S.T. Dumais and J. Nielsen. Automating the assignment of submitted manuscripts to reviewers. In Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval, pages 233–244. ACM, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Fang</author>
<author>C X Zhai</author>
</authors>
<title>Probabilistic models for expert finding.</title>
<date>2007</date>
<booktitle>Advances in Information Retrieval,</booktitle>
<pages>418--430</pages>
<contexts>
<context position="7203" citStr="Fang and Zhai (2007)" startWordPosition="1167" endWordPosition="1170">context of a community-based question-answering service. Based on a virtual document approach, their work applied three language models: the query likelihood model, the relevance model and the cluster-based language model. They concluded that combining language models can enhance the retrieval performance. Two principal approaches recognised for expertfinding can be found in the literature. Both were first proposed by Balog et al. (2006b). The models are called the candidate model and the document model, or Model 1 and Model 2, respectively. Different names have been used for the two methods. Fang and Zhai (2007) refer to them as ‘Candidate Generation Models and Topic Generation Models’. Petkova and Croft (2006) call them the ‘Query-Dependent Model’ and the ‘QueryIndependent Model’. The main difference between the models is that the candidate-based approaches (Model 1) build a textual representation of candidate experts, and then rank the candidates based on the given query, whereas the document-based approaches (Model 2) first find documents that are relevant to the query, and then locate the associated experts in these documents. Balog et al. (2006b) have compared the two models and concluded that M</context>
</contexts>
<marker>Fang, Zhai, 2007</marker>
<rawString>H. Fang and C.X. Zhai. Probabilistic models for expert finding. Advances in Information Retrieval, pages 418–430, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hofmann</author>
<author>K Balog</author>
<author>T Bogers</author>
<author>M de Rijke</author>
</authors>
<title>Contextual factors for finding similar experts.</title>
<date>2010</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>61</volume>
<issue>5</issue>
<marker>Hofmann, Balog, Bogers, de Rijke, 2010</marker>
<rawString>K. Hofmann, K. Balog, T. Bogers, and M. de Rijke. Contextual factors for finding similar experts. Journal of the American Society for Information Science and Technology, 61(5):994–1014, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Karimzadehgan</author>
<author>R White</author>
<author>M Richardson</author>
</authors>
<title>Enhancing expert finding using organizational hierarchies.</title>
<date>2009</date>
<booktitle>Advances in Information Retrieval,</booktitle>
<pages>177--188</pages>
<contexts>
<context position="9880" citStr="Karimzadehgan et al., 2009" startWordPosition="1595" endWordPosition="1598">first by considering a ranking of documents with respect to the users’ query. Then, using the candidate profiles, votes from the ranked documents are converted into votes for candidates. There have been attempts to tackle the expertfinding problem using social networks. This has mainly been investigated from two directions. The first direction uses graph-based measures on social networks to produce a ranking of experts (Campbell et al., 2003; Dom et al., 2003). The second direction assumes similarities among the neighbours in a social network and defines a smoothing procedure to rank experts (Karimzadehgan et al., 2009; Zhang et al., 2007). Some have argued that it is not enough to find experts by looking only at the queries’ without taking the users into consideration. They claim that there are several factors that may play a role in decisions concerning which experts to recommend. Some of these factors are the users’ expertise level, social proximity and physical proximity (Borgatti and Cross, 2003; McDonald and Ackerman, 1998; Shami et al., 2008). McDonald and Ackerman (1998) emphasised the importance of the accessibility of the expert. They argued that people usually prefer to contact the experts who ar</context>
</contexts>
<marker>Karimzadehgan, White, Richardson, 2009</marker>
<rawString>M. Karimzadehgan, R. White, and M. Richardson. Enhancing expert finding using organizational hierarchies. Advances in Information Retrieval, pages 177–188, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Law</author>
<author>E Ngai</author>
</authors>
<title>An empirical study of the effects of knowledge sharing and learning behaviors on firm performance. Expert Systems with Applications,</title>
<date>2008</date>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="1830" citStr="Law and Ngai, 2008" startWordPosition="293" endWordPosition="296">t of experts (e.g. all members of academic staff) while the second method is based on automatic Named-Entity Recognition (NER). We use a graded weighting based on proximity between query and candidate name to rank the list of candidates. As a baseline, we use a system that ranks candidates simply based on frequency of occurrence within the top documents. 1 Introduction The knowledge and expertise of individuals are significant resources for organisation. Managing this 1 intangible resource effectively and efficiently constitutes an essential and very important task (Nonaka and Takeuchi, 1995; Law and Ngai, 2008). Approaching experts is the primary and most direct way of utilising their knowledge (Yang and Huh, 2008; Li et al., 2011). Therefore, it is important to have a means of locating the right experts within organisations. The expert-finding task can be categorised as an information retrieval task similar to a web search, but where the results are people rather than documents. An expert-finding system allows users to input a query, and it returns a ranked list of experts. Here we look at a university context. We start with a real-world problem which is to identify a list of experts within an acad</context>
</contexts>
<marker>Law, Ngai, 2008</marker>
<rawString>C. Law and E. Ngai. An empirical study of the effects of knowledge sharing and learning behaviors on firm performance. Expert Systems with Applications, 34(4):2342–2349, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Li</author>
<author>L Liu</author>
<author>C Li</author>
</authors>
<title>An approach to expert recommendation based on fuzzy linguistic method and fuzzy text classification in knowledge management systems.</title>
<date>2011</date>
<journal>Expert Systems with Applications,</journal>
<volume>38</volume>
<pages>7--8586</pages>
<contexts>
<context position="1953" citStr="Li et al., 2011" startWordPosition="315" endWordPosition="318">. We use a graded weighting based on proximity between query and candidate name to rank the list of candidates. As a baseline, we use a system that ranks candidates simply based on frequency of occurrence within the top documents. 1 Introduction The knowledge and expertise of individuals are significant resources for organisation. Managing this 1 intangible resource effectively and efficiently constitutes an essential and very important task (Nonaka and Takeuchi, 1995; Law and Ngai, 2008). Approaching experts is the primary and most direct way of utilising their knowledge (Yang and Huh, 2008; Li et al., 2011). Therefore, it is important to have a means of locating the right experts within organisations. The expert-finding task can be categorised as an information retrieval task similar to a web search, but where the results are people rather than documents. An expert-finding system allows users to input a query, and it returns a ranked list of experts. Here we look at a university context. We start with a real-world problem which is to identify a list of experts within an academic environment, e.g. a university intranet. The research reported here is based on an empirical study of a simple but eff</context>
</contexts>
<marker>Li, Liu, Li, 2011</marker>
<rawString>M. Li, L. Liu, and C. Li. An approach to expert recommendation based on fuzzy linguistic method and fuzzy text classification in knowledge management systems. Expert Systems with Applications, 38 (7):8586–8596, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Liu</author>
<author>W B Croft</author>
<author>M Koll</author>
</authors>
<title>Finding experts in community-based question-answering services.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM international conference on Information and knowledge management,</booktitle>
<pages>315--316</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="6548" citStr="Liu et al. (2005)" startWordPosition="1063" endWordPosition="1066">ociating candidate experts with one or more of the kinds of textual evidence mentioned above, the next step is to find and rank candidates based on a user query. Many methods have been proposed to perform this task. Craswell et al. (2001) create virtual documents for each candidate (or employee). These virtual documents are simply concatenated texts of all documents from the corpus associated with a particular candidate. Afterwards, the system indexes and processes queries for the employee’s documents. The results would show a list of experts based on the ten best matching employee documents. Liu et al. (2005) have applied expert-search in the context of a community-based question-answering service. Based on a virtual document approach, their work applied three language models: the query likelihood model, the relevance model and the cluster-based language model. They concluded that combining language models can enhance the retrieval performance. Two principal approaches recognised for expertfinding can be found in the literature. Both were first proposed by Balog et al. (2006b). The models are called the candidate model and the document model, or Model 1 and Model 2, respectively. Different names h</context>
</contexts>
<marker>Liu, Croft, Koll, 2005</marker>
<rawString>X. Liu, W.B. Croft, and M. Koll. Finding experts in community-based question-answering services. In Proceedings of the 14th ACM international conference on Information and knowledge management, pages 315–316. ACM, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lu</author>
<author>S Robertson</author>
<author>A MacFarlane</author>
<author>H Zhao</author>
</authors>
<title>Window-based enterprise expert search.</title>
<date>2006</date>
<booktitle>In Proceeddings of the 15th Text REtrieval Conference (TREC</booktitle>
<publisher>NIST,</publisher>
<contexts>
<context position="13154" citStr="Lu et al. (2006)" startWordPosition="2128" endWordPosition="2131"> the candidate metric cm by dividing the candidate count d(cc) by the number of tokens in the document d(nt). Equation 1 defines the metric, where cm is the final candidate’s metric for all documents and n is the number of documents. 3.3 Our Approach Our approach takes into account the proximity between query terms and candidate names in the matching documents in the form of a distance weight. This measure will adds a distance weight value to the main candidate’s metric that was generated earlier. Similar approaches have been proposed in the literature for different expert search applications Lu et al. (2006); Cao et al. (2005). The distance weight will be higher whenever the name appears closer to the query term, within a +/- 10 word window. We experiment with two different formulae. The first formula is as follows: Candidate Rank For Page Candidate Rank For Page Display Experts Update Candidate Rank Named-Entity Recognition Method Candidate List Named-Entity Recognition Query Search API URLs URL Filter Filtered URLs Normalized Web Page String Candidate Evaluator HTML Parser Parsed Html Page Normalizer Normalized Web Page String Database Method Web Page String Candidate Evaluator DB Connection Pa</context>
</contexts>
<marker>Lu, Robertson, MacFarlane, Zhao, 2006</marker>
<rawString>W. Lu, S. Robertson, A. MacFarlane, and H. Zhao. Window-based enterprise expert search. In Proceeddings of the 15th Text REtrieval Conference (TREC 2006). NIST, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Macdonald</author>
<author>I Ounis</author>
</authors>
<title>Voting techniques for expert search. Knowledge and information systems,</title>
<date>2008</date>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="9151" citStr="Macdonald and Ounis (2008)" startWordPosition="1481" endWordPosition="1484">r probability of candidate experts that expresses their level of activity in the important discussions on the query topic. Zhu et al. (2010) claimed that earlier language models did not consider document features. They proposed an approach that incorporates: internal document structure; document URLs; page rank; anchor texts; and multiple levels of association between experts and topics. All of the proposed frameworks assume that the more documents associated with a candidate that score highly with respect to a query, the more likely the candidate is to have relevant expertise for that query. Macdonald and Ounis (2008) developed a different approach, called the Voting Model. In their model, candidate experts are ranked first by considering a ranking of documents with respect to the users’ query. Then, using the candidate profiles, votes from the ranked documents are converted into votes for candidates. There have been attempts to tackle the expertfinding problem using social networks. This has mainly been investigated from two directions. The first direction uses graph-based measures on social networks to produce a ranking of experts (Campbell et al., 2003; Dom et al., 2003). The second direction assumes si</context>
</contexts>
<marker>Macdonald, Ounis, 2008</marker>
<rawString>C. Macdonald and I. Ounis. Voting techniques for expert search. Knowledge and information systems, 16(3):259–280, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Macdonald</author>
<author>R W White</author>
</authors>
<title>Usefulness of click-through data in expert search.</title>
<date>2009</date>
<booktitle>In SIGIR,</booktitle>
<volume>9</volume>
<pages>816--817</pages>
<contexts>
<context position="5920" citStr="Macdonald and White, 2009" startWordPosition="959" endWordPosition="962">n is that individuals will tend to be expert in the topics of documents with which they are associated. Experts can be associated with the documents in which they are mentioned (Craswell et al., 2001) or with e-mails they have sent or received (Balog and de Rijke, 2006a; Campbell et al., 2003; Dom et al., 2003). They can also be associated with their home pages or CVs (Maybury et al., 2001), and with documents they have written (Maybury et al., 2001; BecerraFernandez, 2000). Finally, some researchers use search logs to associate experts with the web pages they have visited (Wang et al., 2002; Macdonald and White, 2009). After associating candidate experts with one or more of the kinds of textual evidence mentioned above, the next step is to find and rank candidates based on a user query. Many methods have been proposed to perform this task. Craswell et al. (2001) create virtual documents for each candidate (or employee). These virtual documents are simply concatenated texts of all documents from the corpus associated with a particular candidate. Afterwards, the system indexes and processes queries for the employee’s documents. The results would show a list of experts based on the ten best matching employee </context>
</contexts>
<marker>Macdonald, White, 2009</marker>
<rawString>C. Macdonald and R.W. White. Usefulness of click-through data in expert search. In SIGIR, volume 9, pages 816–817, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maybury</author>
<author>R D’Amore</author>
<author>D House</author>
</authors>
<title>Expert finding for collaborative virtual environments.</title>
<date>2001</date>
<journal>Communications of the ACM,</journal>
<volume>44</volume>
<issue>12</issue>
<pages>55--56</pages>
<marker>Maybury, D’Amore, House, 2001</marker>
<rawString>M. Maybury, R. D’Amore, and D. House. Expert finding for collaborative virtual environments. Communications of the ACM, 44(12): 55–56, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Maybury</author>
</authors>
<title>Expert finding systems. MITRE Center for Integrated Intelligence Systems</title>
<date>2006</date>
<location>Bedford, Massachusetts, USA,</location>
<contexts>
<context position="4314" citStr="Maybury (2006)" startWordPosition="706" endWordPosition="707">he same ranking function(s), as will be discussed below. This paper will survey related work in Section 2 and introduce the expert-finding task in a university domain in Section 3. The process of ranking experts will be discussed in Section 4. The evaluation will be described in Section 4, followed by a discussion of the experiment’s results in Section 5. 2 Related Work The expert-finding task addresses the problem of retrieving a ranked list of people who are knowledgeable about a given topic. This task has found its place in the commercial environment as early as the 1980’s, as discussed in Maybury (2006); however, there was very limited academic research on finding and ranking experts until the introduction of the enterprise track at the 2005 Text REtrieval Conference (TREC) (Craswell et al., 2005). When expert-finding we must know the experts’ profiles, These profiles may be generated manually or automatically. Manually created profiles may be problematic. If, for example, experts enter their own information, they may exaggerate or downplay their expertise. In addition, any changes of expertise for any expert requires a manual update to the expert’s profile. Thus incurring high maintenance c</context>
</contexts>
<marker>Maybury, 2006</marker>
<rawString>M.T. Maybury. Expert finding systems. MITRE Center for Integrated Intelligence Systems Bedford, Massachusetts, USA, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D W McDonald</author>
<author>M S Ackerman</author>
</authors>
<title>Just talk to me: a field study of expertise location.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1998 ACM conference on Computer supported cooperative work,</booktitle>
<pages>315--324</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="10298" citStr="McDonald and Ackerman, 1998" startWordPosition="1665" endWordPosition="1668">perts (Campbell et al., 2003; Dom et al., 2003). The second direction assumes similarities among the neighbours in a social network and defines a smoothing procedure to rank experts (Karimzadehgan et al., 2009; Zhang et al., 2007). Some have argued that it is not enough to find experts by looking only at the queries’ without taking the users into consideration. They claim that there are several factors that may play a role in decisions concerning which experts to recommend. Some of these factors are the users’ expertise level, social proximity and physical proximity (Borgatti and Cross, 2003; McDonald and Ackerman, 1998; Shami et al., 2008). McDonald and Ackerman (1998) emphasised the importance of the accessibility of the expert. They argued that people usually prefer to contact the experts who are physically or organisationally close to them. Moreover, Shami et al. (2008) found that people prefer to contact experts they know, even when they could potentially receive more information from other experts who are located outside their social network. Woudstra and van den Hooff (2008) identified a number of factors in selecting experts that are related to quality and accessibility. They argued that the process </context>
</contexts>
<marker>McDonald, Ackerman, 1998</marker>
<rawString>D.W. McDonald and M.S. Ackerman. Just talk to me: a field study of expertise location. In Proceedings of the 1998 ACM conference on Computer supported cooperative work, pages 315–324. ACM, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Metzler</author>
<author>W B Croft</author>
</authors>
<title>A markov random field model for term dependencies.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>472--479</pages>
<publisher>ACM,</publisher>
<marker>Metzler, Croft, 2005</marker>
<rawString>D. Metzler and W.B. Croft. A markov random field model for term dependencies. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 472–479. ACM, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Nonaka</author>
<author>H Takeuchi</author>
</authors>
<title>The knowledge creating company: How Japanese The knowledge creating company: How Japanese companies create the dynamics of innovation.</title>
<date>1995</date>
<publisher>Oxford University Press,</publisher>
<location>New York,</location>
<contexts>
<context position="1809" citStr="Nonaka and Takeuchi, 1995" startWordPosition="288" endWordPosition="292">one is based on a fixed list of experts (e.g. all members of academic staff) while the second method is based on automatic Named-Entity Recognition (NER). We use a graded weighting based on proximity between query and candidate name to rank the list of candidates. As a baseline, we use a system that ranks candidates simply based on frequency of occurrence within the top documents. 1 Introduction The knowledge and expertise of individuals are significant resources for organisation. Managing this 1 intangible resource effectively and efficiently constitutes an essential and very important task (Nonaka and Takeuchi, 1995; Law and Ngai, 2008). Approaching experts is the primary and most direct way of utilising their knowledge (Yang and Huh, 2008; Li et al., 2011). Therefore, it is important to have a means of locating the right experts within organisations. The expert-finding task can be categorised as an information retrieval task similar to a web search, but where the results are people rather than documents. An expert-finding system allows users to input a query, and it returns a ranked list of experts. Here we look at a university context. We start with a real-world problem which is to identify a list of e</context>
</contexts>
<marker>Nonaka, Takeuchi, 1995</marker>
<rawString>I. Nonaka and H. Takeuchi. The knowledge creating company: How Japanese The knowledge creating company: How Japanese companies create the dynamics of innovation. Oxford University Press, New York, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Petkova</author>
<author>W B Croft</author>
</authors>
<title>Hierarchical language models for expert finding in enterprise corpora.</title>
<date>2006</date>
<booktitle>In Tools with Artificial Intelligence, 2006. ICTAI’06. 18th IEEE International Conference on,</booktitle>
<pages>599--608</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="7304" citStr="Petkova and Croft (2006)" startWordPosition="1183" endWordPosition="1186">eir work applied three language models: the query likelihood model, the relevance model and the cluster-based language model. They concluded that combining language models can enhance the retrieval performance. Two principal approaches recognised for expertfinding can be found in the literature. Both were first proposed by Balog et al. (2006b). The models are called the candidate model and the document model, or Model 1 and Model 2, respectively. Different names have been used for the two methods. Fang and Zhai (2007) refer to them as ‘Candidate Generation Models and Topic Generation Models’. Petkova and Croft (2006) call them the ‘Query-Dependent Model’ and the ‘QueryIndependent Model’. The main difference between the models is that the candidate-based approaches (Model 1) build a textual representation of candidate experts, and then rank the candidates based on the given query, whereas the document-based approaches (Model 2) first find documents that are relevant to the query, and then locate the associated experts in these documents. Balog et al. (2006b) have compared the two models and concluded that Model 2 outperforms Model 1 on all measures (for this reason, we will adopt Model 2). As Model 2 prove</context>
</contexts>
<marker>Petkova, Croft, 2006</marker>
<rawString>D. Petkova and W.B. Croft. Hierarchical language models for expert finding in enterprise corpora. In Tools with Artificial Intelligence, 2006. ICTAI’06. 18th IEEE International Conference on, pages 599–608. IEEE, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Petkova</author>
<author>W B Croft</author>
</authors>
<title>Proximity-based document representation for named entity retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>731--740</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="8031" citStr="Petkova and Croft, 2007" startWordPosition="1306" endWordPosition="1309"> models is that the candidate-based approaches (Model 1) build a textual representation of candidate experts, and then rank the candidates based on the given query, whereas the document-based approaches (Model 2) first find documents that are relevant to the query, and then locate the associated experts in these documents. Balog et al. (2006b) have compared the two models and concluded that Model 2 outperforms Model 1 on all measures (for this reason, we will adopt Model 2). As Model 2 proved to be more efficient, it formed the basis of many other expert-search systems (Fang 2 and Zhai, 2007; Petkova and Croft, 2007; Yao et al., 2008). Fang and Zhai developed a mixture model using proximity-based document representation. This model makes it possible to put different weights on different representations of a candidate expert (Fang and Zhai, 2007). Another mixture of personal and global language models was proposed by Serdyukov and Hiemstra (2008). They combined two criteria for personal expertise in the final ranking: the probability of generation of the query by the personal language model and a prior probability of candidate experts that expresses their level of activity in the important discussions on </context>
</contexts>
<marker>Petkova, Croft, 2007</marker>
<rawString>D. Petkova and W.B. Croft. Proximity-based document representation for named entity retrieval. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 731–740. ACM, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Serdyukov</author>
<author>D Hiemstra</author>
</authors>
<title>Modeling documents as mixtures of persons for expert finding.</title>
<date>2008</date>
<booktitle>Advances in Information Retrieval,</booktitle>
<pages>309--320</pages>
<contexts>
<context position="8367" citStr="Serdyukov and Hiemstra (2008)" startWordPosition="1357" endWordPosition="1360"> et al. (2006b) have compared the two models and concluded that Model 2 outperforms Model 1 on all measures (for this reason, we will adopt Model 2). As Model 2 proved to be more efficient, it formed the basis of many other expert-search systems (Fang 2 and Zhai, 2007; Petkova and Croft, 2007; Yao et al., 2008). Fang and Zhai developed a mixture model using proximity-based document representation. This model makes it possible to put different weights on different representations of a candidate expert (Fang and Zhai, 2007). Another mixture of personal and global language models was proposed by Serdyukov and Hiemstra (2008). They combined two criteria for personal expertise in the final ranking: the probability of generation of the query by the personal language model and a prior probability of candidate experts that expresses their level of activity in the important discussions on the query topic. Zhu et al. (2010) claimed that earlier language models did not consider document features. They proposed an approach that incorporates: internal document structure; document URLs; page rank; anchor texts; and multiple levels of association between experts and topics. All of the proposed frameworks assume that the more</context>
</contexts>
<marker>Serdyukov, Hiemstra, 2008</marker>
<rawString>P. Serdyukov and D. Hiemstra. Modeling documents as mixtures of persons for expert finding. Advances in Information Retrieval, pages 309–320, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N S Shami</author>
<author>K Ehrlich</author>
<author>D R Millen</author>
</authors>
<title>Pick me: link selection in expertise search results.</title>
<date>2008</date>
<booktitle>In Proceeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>1089--1092</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="10319" citStr="Shami et al., 2008" startWordPosition="1669" endWordPosition="1672"> Dom et al., 2003). The second direction assumes similarities among the neighbours in a social network and defines a smoothing procedure to rank experts (Karimzadehgan et al., 2009; Zhang et al., 2007). Some have argued that it is not enough to find experts by looking only at the queries’ without taking the users into consideration. They claim that there are several factors that may play a role in decisions concerning which experts to recommend. Some of these factors are the users’ expertise level, social proximity and physical proximity (Borgatti and Cross, 2003; McDonald and Ackerman, 1998; Shami et al., 2008). McDonald and Ackerman (1998) emphasised the importance of the accessibility of the expert. They argued that people usually prefer to contact the experts who are physically or organisationally close to them. Moreover, Shami et al. (2008) found that people prefer to contact experts they know, even when they could potentially receive more information from other experts who are located outside their social network. Woudstra and van den Hooff (2008) identified a number of factors in selecting experts that are related to quality and accessibility. They argued that the process of choosing which can</context>
</contexts>
<marker>Shami, Ehrlich, Millen, 2008</marker>
<rawString>N.S. Shami, K. Ehrlich, and D.R. Millen. Pick me: link selection in expertise search results. In Proceeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems, pages 1089–1092. ACM, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Smirnova</author>
<author>K Balog</author>
</authors>
<title>A user-oriented model for expert finding.</title>
<date>2011</date>
<booktitle>Advances in Information Retrieval,</booktitle>
<pages>580--592</pages>
<contexts>
<context position="11184" citStr="Smirnova and Balog (2011)" startWordPosition="1807" endWordPosition="1810">ound that people prefer to contact experts they know, even when they could potentially receive more information from other experts who are located outside their social network. Woudstra and van den Hooff (2008) identified a number of factors in selecting experts that are related to quality and accessibility. They argued that the process of choosing which candidate expert to contact might differ depending on the specific situation. Hofmann et al. (2010) showed that many of these factors can be modelled. They claimed that integrating them with retrieval models can improve retrieval performance. Smirnova and Balog (2011) provided a user-oriented model for expert-finding where they placed an emphasis on the social distance between the user and the expert. They considered a number of social graphs based on organisational hierarchy, geographical location and collaboration. 3 Expert-Finding in a University In any higher educational institution, finding an appropriate supervisor is a critical task for research students, a task that can be very time consuming, especially if academics describe their work using terms that a student is not familiar with. A searcher may build up a picture of who is likely to have the r</context>
</contexts>
<marker>Smirnova, Balog, 2011</marker>
<rawString>E. Smirnova and K. Balog. A user-oriented model for expert finding. Advances in Information Retrieval, pages 580–592, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wang</author>
<author>Z Chen</author>
<author>L Tao</author>
<author>W Y Ma</author>
<author>L Wenyin</author>
</authors>
<title>Ranking user’s relevance to a topic through link analysis on web logs.</title>
<date>2002</date>
<booktitle>In Proceedings of the 4th international workshop on Web information and data management,</booktitle>
<pages>49--54</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="5892" citStr="Wang et al., 2002" startWordPosition="955" endWordPosition="958">tion. The assumption is that individuals will tend to be expert in the topics of documents with which they are associated. Experts can be associated with the documents in which they are mentioned (Craswell et al., 2001) or with e-mails they have sent or received (Balog and de Rijke, 2006a; Campbell et al., 2003; Dom et al., 2003). They can also be associated with their home pages or CVs (Maybury et al., 2001), and with documents they have written (Maybury et al., 2001; BecerraFernandez, 2000). Finally, some researchers use search logs to associate experts with the web pages they have visited (Wang et al., 2002; Macdonald and White, 2009). After associating candidate experts with one or more of the kinds of textual evidence mentioned above, the next step is to find and rank candidates based on a user query. Many methods have been proposed to perform this task. Craswell et al. (2001) create virtual documents for each candidate (or employee). These virtual documents are simply concatenated texts of all documents from the corpus associated with a particular candidate. Afterwards, the system indexes and processes queries for the employee’s documents. The results would show a list of experts based on the</context>
</contexts>
<marker>Wang, Chen, Tao, Ma, Wenyin, 2002</marker>
<rawString>J. Wang, Z. Chen, L. Tao, W.Y. Ma, and L. Wenyin. Ranking user’s relevance to a topic through link analysis on web logs. In Proceedings of the 4th international workshop on Web information and data management, pages 49–54. ACM, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Woudstra</author>
<author>B van den Hooff</author>
</authors>
<title>Inside the source selection process: Selection criteria for human information sources.</title>
<date>2008</date>
<journal>Information Processing &amp; Management,</journal>
<volume>44</volume>
<issue>3</issue>
<marker>Woudstra, van den Hooff, 2008</marker>
<rawString>L. Woudstra and B. van den Hooff. Inside the source selection process: Selection criteria for human information sources. Information Processing &amp; Management, 44(3):1267–1278, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yang</author>
<author>S Huh</author>
</authors>
<title>Automatic expert identification using a text automatic expert identification using a text categorization technique in knowledge management systems.</title>
<date>2008</date>
<booktitle>Expert Systems with Expert Systems with Applications,</booktitle>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="1935" citStr="Yang and Huh, 2008" startWordPosition="311" endWordPosition="314">ty Recognition (NER). We use a graded weighting based on proximity between query and candidate name to rank the list of candidates. As a baseline, we use a system that ranks candidates simply based on frequency of occurrence within the top documents. 1 Introduction The knowledge and expertise of individuals are significant resources for organisation. Managing this 1 intangible resource effectively and efficiently constitutes an essential and very important task (Nonaka and Takeuchi, 1995; Law and Ngai, 2008). Approaching experts is the primary and most direct way of utilising their knowledge (Yang and Huh, 2008; Li et al., 2011). Therefore, it is important to have a means of locating the right experts within organisations. The expert-finding task can be categorised as an information retrieval task similar to a web search, but where the results are people rather than documents. An expert-finding system allows users to input a query, and it returns a ranked list of experts. Here we look at a university context. We start with a real-world problem which is to identify a list of experts within an academic environment, e.g. a university intranet. The research reported here is based on an empirical study o</context>
</contexts>
<marker>Yang, Huh, 2008</marker>
<rawString>K. Yang and S. Huh. Automatic expert identification using a text automatic expert identification using a text categorization technique in knowledge management systems. Expert Systems with Expert Systems with Applications, 34(2):1445–1455, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yao</author>
<author>J Xu</author>
<author>J Niu</author>
</authors>
<title>Using role determination and expert mining in the enterprise environment.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Text REtrieval Conference (TREC</booktitle>
<contexts>
<context position="8050" citStr="Yao et al., 2008" startWordPosition="1310" endWordPosition="1313">date-based approaches (Model 1) build a textual representation of candidate experts, and then rank the candidates based on the given query, whereas the document-based approaches (Model 2) first find documents that are relevant to the query, and then locate the associated experts in these documents. Balog et al. (2006b) have compared the two models and concluded that Model 2 outperforms Model 1 on all measures (for this reason, we will adopt Model 2). As Model 2 proved to be more efficient, it formed the basis of many other expert-search systems (Fang 2 and Zhai, 2007; Petkova and Croft, 2007; Yao et al., 2008). Fang and Zhai developed a mixture model using proximity-based document representation. This model makes it possible to put different weights on different representations of a candidate expert (Fang and Zhai, 2007). Another mixture of personal and global language models was proposed by Serdyukov and Hiemstra (2008). They combined two criteria for personal expertise in the final ranking: the probability of generation of the query by the personal language model and a prior probability of candidate experts that expresses their level of activity in the important discussions on the query topic. Zh</context>
</contexts>
<marker>Yao, Xu, Niu, 2008</marker>
<rawString>J. Yao, J. Xu, and J. Niu. Using role determination and expert mining in the enterprise environment. In Proceedings of the 2008 Text REtrieval Conference (TREC 2008), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhang</author>
<author>J Tang</author>
<author>J Li</author>
</authors>
<title>Expert finding in a social network. Advances in Databases: Concepts, Systems and Applications,</title>
<date>2007</date>
<pages>1066--1069</pages>
<contexts>
<context position="9901" citStr="Zhang et al., 2007" startWordPosition="1599" endWordPosition="1602">ng of documents with respect to the users’ query. Then, using the candidate profiles, votes from the ranked documents are converted into votes for candidates. There have been attempts to tackle the expertfinding problem using social networks. This has mainly been investigated from two directions. The first direction uses graph-based measures on social networks to produce a ranking of experts (Campbell et al., 2003; Dom et al., 2003). The second direction assumes similarities among the neighbours in a social network and defines a smoothing procedure to rank experts (Karimzadehgan et al., 2009; Zhang et al., 2007). Some have argued that it is not enough to find experts by looking only at the queries’ without taking the users into consideration. They claim that there are several factors that may play a role in decisions concerning which experts to recommend. Some of these factors are the users’ expertise level, social proximity and physical proximity (Borgatti and Cross, 2003; McDonald and Ackerman, 1998; Shami et al., 2008). McDonald and Ackerman (1998) emphasised the importance of the accessibility of the expert. They argued that people usually prefer to contact the experts who are physically or organ</context>
</contexts>
<marker>Zhang, Tang, Li, 2007</marker>
<rawString>J. Zhang, J. Tang, and J. Li. Expert finding in a social network. Advances in Databases: Concepts, Systems and Applications, pages 1066–1069, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhu</author>
<author>X Huang</author>
<author>D Song</author>
<author>S R¨uger</author>
</authors>
<title>Integrating multiple document features in language models for expert finding.</title>
<date>2010</date>
<journal>Knowledge and Information Systems,</journal>
<volume>23</volume>
<issue>1</issue>
<marker>Zhu, Huang, Song, R¨uger, 2010</marker>
<rawString>J. Zhu, X. Huang, D. Song, and S. R¨uger. Integrating multiple document features in language models for expert finding. Knowledge and Information Systems, 23(1):29–54, 2010.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>