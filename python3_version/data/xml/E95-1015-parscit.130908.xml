<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.7331815">
The Problem of Computing the Most Probable Tree in
Data-Oriented Parsing and Stochastic Tree Grammars
</title>
<author confidence="0.974881">
Rens Bod
</author>
<affiliation confidence="0.997171666666667">
Institute for Logic, Language and Computation
Department of Computational Linguistics
University of Amsterdam
</affiliation>
<address confidence="0.92557">
Spuistraat 134, 1012 VB Amsterdam
The Netherlands
</address>
<email confidence="0.974392">
rens @mars.let.uva.nl
</email>
<sectionHeader confidence="0.993533" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945107142857">
We deal with the question as to whether there
exists a polynomial time algorithm for computing
the most probable parse tree of a sentence generated
by a data-oriented parsing (DOP) model. (Scha,
1990; Bod, 1992, 1993a). Therefore we describe
DOP as a stochastic tree-substitution grammar
(STSG). In STSG, a tree can be generated by
exponentially many derivations involving different
elementary trees. The probability of a tree is equal
to the sum of the probabilities of all its
derivations.
We show that in STSG, in contrast with
stochastic context-free grammar, the Viterbi
algorithm cannot be used for computing a most
probable tree of a string. We propose a simple
modification of Viterbi which allows by means of
a &amp;quot;select-random&amp;quot; search to estimate the most
probable tree of a string in polynomial time.
Experiments with DOP on ATIS show that
only in 68% of the cases, the most probable
derivation of a string generates the most probable
tree of that string. Therefore, the parse accuracy
obtained by the most probable trees (96%) is
dramatically higher than the parse accuracy obtained
by the most probable derivations (65%).
It is still an open question whether the
most probable tree of a string can be
deterministically computed in polynomial time.
</bodyText>
<sectionHeader confidence="0.995672" genericHeader="method">
1 Data-Oriented Parsing
</sectionHeader>
<bodyText confidence="0.985107611111111">
A Data-Oriented Parsing model (Scha, 1990; Bod,
1992, 1993a) is characterized by a corpus of analyzed
language utterances, together with a set of operations
that combine sub-analyses from the corpus into new
analyses. We will limit ourselves in this paper to
corpora with purely syntactic annotations. For the
semantic dimension of DOP, the reader is referred to
(van den Berg et al., 1994). Consider the imaginary
example corpus consisting of only two trees in figure
1. We will assume one operation for combining
subtrees. This operation is called &amp;quot;composition&amp;quot;, and
is indicated by the infix operator The composition
of t and u, tou, yields a copy of t in which its
leftmost nontenninal leaf node has been identified
with the root nodeof u (i.e., u is substituted on the
leftmost nontenninal leaf node of t). For reasons of
simplicity we will write in the following (tou)ov as:
to u ov.
</bodyText>
<figureCaption confidence="0.997473">
Figure 1. Example corpus of two trees.
</figureCaption>
<bodyText confidence="0.915292">
Now the (ambiguous) sentence &amp;quot;She displayed the
dress on the table&amp;quot; can be parsed by combining
subtrees from the corpus. For instance:
</bodyText>
<figure confidence="0.9763236">
0 NP
NP VP the dress
I on the table
she vp PP
V NP
displayed
NP VP
I
she vp PP
A
</figure>
<figureCaption confidence="0.773172333333333">
displayed the dress on the table
Figure 2. Derivation and parse tree for &amp;quot;She displayed the
dress on the table&amp;quot;
</figureCaption>
<figure confidence="0.994471785714286">
IA
she V NP
wanted /\
NP PP
AA
the dress P ANP
I
on the rack
NP
I 7.VP
she vp pp
&amp;quot;
A IA
displayed the block on the table
</figure>
<page confidence="0.997048">
104
</page>
<bodyText confidence="0.998089">
As the reader may easily ascertain, a different
derivation may yield a different parse tree. However, a
different derivation may also very well yield the same
parse tree; for instance:
</bodyText>
<note confidence="0.5504992">
NP VP 0 NP
she V NP
the dress
displayed
VP PP
</note>
<figure confidence="0.853490666666667">
p NP
I
on the table
</figure>
<figureCaption confidence="0.97517">
Figure 3. Different derivation generating the same parse
</figureCaption>
<figure confidence="0.47254">
tree for &amp;quot;She displayed the dress on the table&amp;quot;
or
</figure>
<figureCaption confidence="0.6941435">
Figure 4. Another derivation generating the same parse
tree for &amp;quot;She displayed the dress on the table&amp;quot;
</figureCaption>
<bodyText confidence="0.999967857142857">
Thus, a parse tree can have several derivations
involving different subtrees. Using the corpus for our
stochastic estimations, we estimate the probability of
substituting a certain subtree on a specific node as the
probability of selecting this subtree among all
subtrees in the corpus that could be substituted on
that node.&apos; The probability of a derivation can be
computed as the product of the probabilities of the
substitutions that it involves. The probability of a
parse tree is equal to the probability that any of its
derivations occurs, which is the sum of the
probabilities of all derivations of that parse tree.
Finally, the probability of a word string is equal to
the sum of the probabilities of all its parse trees.
</bodyText>
<sectionHeader confidence="0.786549" genericHeader="method">
2 DOP as a Stochastic Tree-
</sectionHeader>
<subsectionHeader confidence="0.983082">
Substitution Grammar
</subsectionHeader>
<bodyText confidence="0.999222857142857">
In order to deal with the problem of computing the
most probable parse tree of a string, it is convenient
to describe DOP as a &amp;quot;Stochastic Tree-Substitution
Grammar&amp;quot; (STSG). STSG can be seen as a
generalization over DOP, where the elementary trees
of STSG are the subtrees of DOP, and the
probabilities of the elementary trees are the
</bodyText>
<footnote confidence="0.584319">
1Very small frequencies are smoothed by Good-Turing.
</footnote>
<bodyText confidence="0.684657">
substitution-probabilities of the corresponding
subtrees of DOP (Bod, 1993c).
</bodyText>
<equation confidence="0.542354">
A Stochastic Tree-Substitution Grammar G is a five-
tuple &lt;VN, V, S. R, P&gt; where
VN is a finite set of nonterminal symbols.
VT is a finite set of terminal symbols.
S E VN is the distinguished symbol.
</equation>
<bodyText confidence="0.97313375">
R is a finite set of elementary trees whose top nodes
and interior nodes are labeled by nonterminal symbols
and whose yield nodes are labeled by terminal or
nonterminal symbols.
P is a function which assigns to every elementary tree
te R a probability p(t). For a tree t with a root a,
p(t) is interpreted as the probability of substituting t
on a. We require, therefore, that 0 &lt; p(t) 1 and
</bodyText>
<equation confidence="0.462113">
Itroot(t).--a p(t) = 1.
</equation>
<bodyText confidence="0.999654666666667">
If t/ and t2 are trees such that the leftmost
nonterminal yield node of t/ is equal to the root of t2,
then t/ ot2 is the tree that results from substituting t2
for this leftmost nonterminal yield node in t1. The
partial function o is called leftmost substitution. For
reasons of conciseness we will use the term
substitution for leftmost substitution.
A leftmost derivation generated by an STSG G
is a tuple of trees &lt;//,...,tn&gt; such that ti,...,tn are
elements of R, the root of t/ is labeled by S and the
yield of tio...ot, is labeled by terminal symbols. The
set of leftmost derivations generated by G is thus
</bodyText>
<equation confidence="0.94118">
given by Derivations(G) = 1&lt;t1,...,tn&gt; I E R
root(ti) = S A yie/d(t/....otn) e VT+ . For
</equation>
<bodyText confidence="0.99380872">
convenience we will use the term derivation for
leftmost derivation. A derivation &lt;ti,...,tn&gt; is called
a derivation of tree T, iff t/....otn = T. A derivation
&lt;t/.....t&gt; is called a derivation of string s, iff
= s. The probability of a derivation
tn&gt; is defined as p(t 1) • ... • p(t,).
A parse tree generated by an STSG G is a tree
T such that there is a derivation &lt; t1,...,tn &gt; E
Derivations(G) for which t/.... .t, = T. The set of
parse trees, or tree language, generated by G is given
by Parses(G) = {TI 3 &lt;t1,...,tn&gt; e Derivations(G) :
tio....t, = T). For reasons of conciseness we will
often use the terms parse or tree for a parse tree. A
parse whose yield is equal to string s, is called a parse
of S. The probability of a parse is defined as the sum
of the probabilities of all its derivations.
A string generated by an STSG G is an
element of VT+ such that there is a parse generated by
G whose yield is equal to the string. The set of
strings, or string language, generated by G is given
by Strings(G) = {s I 3 T: Te Parses(G) A s =
yield(T)). The probability of a string is defined as the
sum of the probabilities of all its parses. This means
that the probability of a string is also equal to the
sum of the probabilities of all its derivations.
</bodyText>
<figure confidence="0.926258785714286">
VP
VP PP
A A
V NP p NP
displayed
NP
she
the dress
I /\
on the table
105
3 Computing a most probable For the input string abcd, the following derivation
parse tree in STSG
forest is then obtained:
</figure>
<bodyText confidence="0.999814875">
In order to deal with the problem of computing the
most probable parse tree of a sentence, we will
distinguish between parsing and disambiguation. By
parsing we mean the creation of a parse forest for an
input sentence. By disambiguation we mean the
selection of the most probable parse2 from the forest.
The creation of a parse forest is an intermediate step
for computing the most probable parse.
</bodyText>
<subsectionHeader confidence="0.998222">
3.1 Parsing
</subsectionHeader>
<bodyText confidence="0.99759155">
From the way STSG combines elementary trees by
means of substitution, it follows that an input
sentence can be parsed by the same algorithms as
(S)CFGs. Every elementary tree t is used as a
context-free rewrite rule root(t) --&gt; yield(t). Given a
chart parsing algorithm, an input sentence of length n
can be parsed in n3 time.
In order to obtain a chart-like forest for a
sentence parsed in STSG, we need to label the well-
formed substrings in the chart not only with the
syntactic categories of that substring but with the full
elementary trees t that correspond to the use of the
derived rules root(t) --&gt;yield(t). Note that in a chart-
like forest generated by an STSG, different derivations
that generate a same tree do not collapse. We will
therefore talk about a derivation forest generated by an
STSG (cf. Sima&apos;an et al., 1994).
The following formal example illustrates
what a derivation forest of a string may look like. In
the example, we leave out the probabilities, which are
needed only in the disambiguation process. The visual
representation comes from (Kay, 1980): every entry
(ij) in the chart is indicated by an edge and spans the
words between the i-th and the j-th position of a
sentence. Every edge is labeled with the elementary
trees that denote the underlying phrase. The example-
STSG consists of the following elementary trees:
2 Although theoretically there can be more than one
most probable parse for a sentence, in practice a system
that employs a non-trivial treebank tends to generate
exactly one most probable parse for a given input
sentence.
Note that different derivations in the forest generate
the same tree. By exhaustively unpacking the forest,
four different derivations generating two different trees
are obtained. We may ask whether we can pack the
forest by collapsing spurious derivations.
Unfortunately, no efficient procedure is known that
accomplishes this (remember that there can be
exponentially many derivations for one tree).
</bodyText>
<subsectionHeader confidence="0.997782">
3.2 Disambiguation
</subsectionHeader>
<bodyText confidence="0.981547379310345">
Cubic time parsing does not guarantee cubic time
disambiguation, as a sentence may have exponentially
many parses and any such parse may have
exponentially many derivations. Therefore, in order to
find the most probable parse of a sentence, it is not
efficient to compare the probabilities of the parses by
exhaustively unpacking the chart. Even for
determining the probability of one parse, it is not
efficient to add the probabilities of all derivations of
that parse.
3.2.1 Viterbi optimization is not feasible
for finding the most probable parse
There exists a heuristic optimization algorithm,
known as Viterbi optimization, which selects on the
basis of an SCFG the most probable derivation of a
sentence in cubic time (Viterbi, 1967; Fujisaki et al.,
1989; Jelinek et al., 1990). In STSG, however, the
most probable derivation does not necessarily generate
the most probable parse, as the probability of a parse
is defined as the sum of the probabilities of all its
derivations. Thus, there is an important question as to
whether we can adapt the Viterbi algorithm for finding
the most probable parse.
To understand the difficulty of the problem,
we look in more detail at the Viterbi algorithm. The
basic idea of the Viterbi algorithm is the early
pruning of low probability subderivations in a
bottom-up fashion. Two different subderivations of
the same part of the sentence and whose resulting
</bodyText>
<figureCaption confidence="0.991523">
Figure 5. Elementary trees of an example-STSG
</figureCaption>
<figure confidence="0.9997814">
! Aa g
A (\ I
a b c d
Ac A A ,(iB A
A
A C
A
A
&apos;A &apos;A.i.
b b • • .d
</figure>
<figureCaption confidence="0.995765">
Figure 6. Derivation forest for abed
</figureCaption>
<page confidence="0.984615">
106
</page>
<bodyText confidence="0.999859555555556">
subparses have the same root can both be developed
(if at all) to derivations of the whole sentence in the
same ways. Therefore, if one of these two
subderivations has a lower probability, then it can be
eliminated. This is illustrated by a formal example in
figure 7. Suppose that during bottom-up parsing of
the string abcd the following two subderivations dl
and d2 have been generated for the substring abc.
(Actually represented are their resulting subparses.)
</bodyText>
<figureCaption confidence="0.981305">
Figure 7. Two subparses for the string abcd
</figureCaption>
<bodyText confidence="0.999959125">
If the probability of dl is higher than the probability
of d2, we can eliminate d2 if we are only interested in
finding the most probable derivation of abcd. But if
we are interested in finding the most probable parse of
abcd (generated by STSG), we are not allowed to
eliminate d2. This can be seen by the following.
Suppose that we have the additional elementary tree
given in figure 8.
</bodyText>
<figure confidence="0.947179333333333">
Ac
A
a
</figure>
<figureCaption confidence="0.999995">
Figure 8. Elementary tree.
</figureCaption>
<bodyText confidence="0.984640743589744">
This elementary tree may be developed to the same
tree that can be developed by d2, but not to the tree
that can be developed by dl. And since the probability
of a parse tree is equal to the sum of the probabilities
of all its derivations, it is still possible that d2
contributes to the generation of the most probable
parse. Therefore we are not allowed to eliminate d2.
This counter-example does not prove that
there is no heuristic optimization that allows
polynomial time selection of the most probable parse.
But it makes clear that a &amp;quot;select-best&amp;quot; search, as
accomplished by Viterbi, is not adequate for finding
the most probable parse in STSG. So far, it is
unknown whether the problem of finding the most
probable parse in a deterministic way is inherently
exponential or not (cf. Sima&apos;an et al., 1994). One
should of course ask how often in practice the most
probable derivation produces the most probable parse,
but this can only be answered by means of
experiments on real life corpora. Experiments on the
ATIS corpus (see session 4) show that only in 68%
of the cases the most probable derivation of a sentence
generates the most probable parse of that sentence.
Moreover, the parse accuracy obtained by the most
probable parse is dramatically higher than the parse
accuracy obtained by the parse generated by the most
probable derivation.
3.2.2 Estimating the most probable parse
by Monte Carlo search
We will leave it as an open question whether the most
probable parse can be deterministically derived in
polynomial time. Here we will ask whether there
exists a polynomial time approximation procedure
that estimates the most probable parse with an
estimation error that can be made arbitrarily small.
We have seen that a &amp;quot;select-best&amp;quot; search, as
accomplished by Viterbi, can be used for finding the
most probable derivation but not for finding the most
probable parse. If we apply instead of a select-best
search, a &amp;quot;select-random&amp;quot; search, we can generate a
random derivation. By iteratively generating a large
number of random derivations we can estimate the
most probable parse as the parse which results most
often from these random derivations (since the
probability of a parse is the probability that any of its
derivations occurs). The most probable parse can be
estimated as accurately as desired by making the
number of random samples as large as desired.
According to the Law of Large Numbers, the most
often generated parse converges to the most probable
parse. Methods that estimate the probability of an
event by taking random samples are known as Monte
Carlo methods (Hammersley &amp; Handscomb, 1964).3
The selection of a random derivation is
accomplished in a bottom-up fashion analogous to
Viterbi. Instead of selecting the most probable
subderivation at each node-sharing in the chart, a
random subderivation is selected (i.e. sampled) at each
node-sharing (that is, a subderivation that has n times
as large a probability as another subderivation should
also have n times as large a chance to be chosen as
this other subderivation). Once sampled at the S-node,
the random derivation of the whole sentence can be
retrieved by tracing back the choices made at each
node-sharing. Of course, we may postpone sampling
until the S-node, such that we sample directly from
the distribution of all S-derivations. But this would
take exponential time, since there may be
exponentially many derivations for the whole
sentence. By sampling bottom-up at every node where
ambiguity appears, the maximum number of different
subderivations at each node-sharing is bounded to a
constant (the total number of rules of that node), and
therefore the time complexity of generating a random
derivation of an input sentence is equal to the time
complexity of finding the most probable derivation,
0(n3). This is exemplified by the following
algorithm.
</bodyText>
<footnote confidence="0.997859142857143">
3 Note that Monte Carlo estimation of the most probable
parse is more reliable than the estimation of the most
probable parse by generating the n most probable
derivations by Viterbi, since it might be that the most
probable parse is exclusively generated by many low
probable derivations. The Monte Carlo method is
guaranteed to converge to the most probable parse.
</footnote>
<page confidence="0.996877">
107
</page>
<bodyText confidence="0.992011347826087">
Sampling a random derivation from a derivation forest
Given a derivation forest, of a sentence of n words,
consisting of labeled entries (i,j) that span the words
between the i-th and the j-th position of the sentence.
Every entry is labeled with linked elementary trees,
together with their probabilities, that constitute
subderivations of the underlying subsentence.
Sampling a derivation from the chart consists of
choosing at every labeled entry (bottom-up, breadth-
first) a random subderivation of each root-node:
for k := 1 to n do
for i := 0 to n-k do
for chart-entry (i,i+k) do
for each root-node X do
select4 a random subderivation of root X
eliminate the other subderivations
We now have an algorithm that selects a random
derivation from a derivation forest. Converting this
derivation into a parse tree gives a first estimation for
the most probable parse. Since one random sample is
not a reliable estimate, we sample a large number of
random derivations and see which parse is generated
most frequently. This is exemplified by the following
algorithm. (Note that we might also estimate the
most probable derivation by random sampling,
namely by counting which derivation is sampled most
often; however, the most probable derivation can be
more effectively generated by Viterbi.)
Estimating the most probable parse (MPP)
Given a derivation forest for an input sentence:
repeat until the MPP converges
sample a random derivation from the forest
store the parse generated by the random derivation
MPP := the most frequently occurring parse
There is an important question as to how long the
convergence of the most probable parse may take. Is
there a tractable upper bound on the number of
derivations that have to be sampled from the forest
before stability in the top of the parse distribution
occurs? The answer is yes: the worst case time
complexity of achieving a maximum estimation error
E by means of random sampling is 0(E-2),
independently of the probability distribution. This is a
classical result from sampling theory (cf. Hammersley
and Handscomb, 1964), and follows directly from
Chebyshev&apos;s inequality. In practice, it means that the
</bodyText>
<subsubsectionHeader confidence="0.220869">
4 Let ( (el, pi) , (e2, p2) , , (en, pn) ) be a probability
</subsubsectionHeader>
<bodyText confidence="0.998377155172414">
distribution of events e1, e2, ..., en; an event e i is said to
be randomly selected iff its probability of being selected
is equal to pi. In order to allow for &amp;quot;direct sampling&amp;quot;, one
must convert the probability distribution into a
corresponding sample space for which holds that the
frequency of occurrence fi of each event e i is a positive
integer equal to Npi, where N is the size of the sample
space.
error e is inversely proportional to the square-root of
the number of random samples N and therefore, to
reduce e by a factor of k, the number of samples N
needs to be increased k2-fold. In practical experiments
(see §4), we will limit the number of samples to a
pre-determined, sufficiently large bound N.
What is the theoretical worst case time
complexity of parsing and disambiguation together?
That is, given an STSG and an input sentence, what
is the maximal time cost of finding the most probable
parse of a sentence? If we use a CKY-parser, the
creation of a derivation forest for a sentence of n
words takes 0(n3) time. Taking also into account the
size G of an STSG (defined as the sum of the lengths
of the yields of all its elementary trees), the time
complexity of creating a derivation forest is
proportional to Gn3 . The time complexity of
disambiguation is both proportional to the cost of
sampling a derivation, i.e. Gn3, and to the cost of the
convergence by means of iteration, which is C2. This
means that the time complexity of disambiguation is
given by 0(Gn3e-2). The total time complexity of
parsing and disambiguation is equal to 0(Gn3) +
0(Gn3 e-2) = 0(Gn3 e-2). Thus, there exists a tractable
procedure that estimates the most probable parse of an
input sentence.
Notice that although the Monte Carlo
disambiguation algorithm estimates the most
probable parse of a sentence in polynomial time, it is
not in the class of polynomial time decidable
algorithms. The Monte Carlo algorithm cannot decide
in polynomial time what is the most probable parse;
it can only make the error-probability of the estimated
most probable parse arbitrarily small. As such, the
Monte Carlo algorithm is a probabilistic algorithm
belonging to the class of Bounded error Probabilistic
Polynomial time (BPP) algorithms.
We hypothesize that Monte Carlo
disambiguation is also relevant for other stochastic
grammars. It turns out that all stochastic extensions
of CFGs that are stochastically richer than SCFG
need exponential time algorithms for finding a most
probable parse tree (cf. Briscoe &amp; Carroll, 1992;
Black et al., 1993; Magerman &amp; Weir, 1992; Schabes
&amp; Waters, 1993). To our knowledge, it has never
been studied whether there exist BPP-algorithms for
these models. Alhough it is beyond the scope of our
research, we conjecture that there exists a Monte
Carlo disambiguation algorithm for at least Stochastic
Tree-Adjoining Grammar (Schabes, 1992).
</bodyText>
<subsectionHeader confidence="0.936349">
3.2.3 Psychological relevance of Monte
Carlo disambiguation
</subsectionHeader>
<bodyText confidence="0.999925666666667">
As has been noted, an important difference between
the Viterbi algorithm and the Monte Carlo algorithm
is, that with the latter we never have 100%
confidence. In our opinion, this should not be seen as
a disadvantage. In fact, absolute confidence about the
most probable parse does not have any significance,
as the probability assigned to a parse is already an
estimation of its actual probability. One may ask as
to whether Monte Carlo is appropriate for modeling
</bodyText>
<page confidence="0.998374">
108
</page>
<bodyText confidence="0.992606666666667">
human sentence perception. The following lists some
properties of Monte Carlo disambiguation that may
be of psychological interest:
</bodyText>
<listItem confidence="0.734344833333333">
1. As mentioned above, Monte Carlo never provides
100% confidence about the best analysis. This
corresponds to the psychological observation that
people never have absolute confidence about their
interpretation of an ambiguous sentence.
2. Although conceptually Monte Carlo uses the total
</listItem>
<bodyText confidence="0.856648625">
space of possible analyses, it tends to sample only the
most likely ones. Very unlikely analyses may only be
sampled after considerable time, but it is not
guaranteed that all analyses are found in finite time.
This matches with experiments on human sentence
perception where very implausible analyses are only
perceived with great difficulty and after considerable
time.
</bodyText>
<listItem confidence="0.6167108125">
3. Monte Carlo does not necessarily give the same
results for different sequences of samples, especially if
different analyses in the top of the distribution are
almost equally likely. In the case there is more than
one most probable analysis, Monte Carlo does not
converge to one analysis but keeps alternating,
however large the number of samples is made. In
experiments with human sentence perception, it has
often been shown that different analyses can be
perceived for one sentence. And in case these analyses
are equally plausible, people perceive so-called
fluctuation effects. This fluctuation phenomenon is
also well-known in the perception of ambiguous
visual patterns.
4. Monte Carlo can be made parallel in a very
straightforward way: N samples can be computed by
</listItem>
<bodyText confidence="0.736713857142857">
N processing units, where equal outputs are
reinforced. The more processing units are employed,
the better the estimation. However, since the number
of processing units is finite, there is never absolute
confidence. This has some similarity with the Parallel
Distributed Processing paradigm for human (language)
processing (Rumelhart &amp; McClelland, 1986).
</bodyText>
<sectionHeader confidence="0.997983" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9998554">
In this section, we report on experiments with an
implementation of DOP that parses and disambiguates
part-of-speech strings. In (Bod, 1995) it is shown how
DOP is extended to parse word strings that possibly
contain unknown words.
</bodyText>
<subsectionHeader confidence="0.98452">
4.1 The test environment
</subsectionHeader>
<bodyText confidence="0.99999705882353">
For our experiments, we used a manually corrected
version of the Air Travel Information System (ATIS)
spoken language corpus (Hemphill et al., 1990)
annotated in the Pennsylvania Treebank (Marcus et
al., 1993). We employed the &amp;quot;blind testing&amp;quot; method,
dividing the corpus into a 90% training set and a 10%
test set by randomly selecting sentences. The 675
trees from the training set were converted into their
subtrees together with their relative frequencies,
yielding roughly 4* i05 different subtrees. The 75
part-of-speech sequences from the test set served as
input strings that were parsed and disambiguated using
the subtrees from the training set. As motivated in
(Bod, 1993b), we use the notion of parse accuracy as
our accuracy metric, defined as the percentage of the
test strings for which the most probable parse is
identical to the parse in the test set.
</bodyText>
<subsectionHeader confidence="0.996463">
4.2 Accuracy as a function of subtree-depth
</subsectionHeader>
<bodyText confidence="0.997764666666667">
It is one of the most essential features of DOP, that
arbitrarily large subtrees are taken into consideration
to estimate the probability of a parse. In order to test
the usefulness of this feature, we performed different
experiments constraining the depth of the subtrees.
The following table shows the results of seven
experiments for different maximum depths of the
training set subtrees. The accuracy refers to the parse
accuracy at 400 randomly sampled parses, and is
rounded off to the nearest integer. The CPU time
refers to the average CPU time per string employed
by a Spark II.
</bodyText>
<table confidence="0.996636111111111">
depth of parse CPU time
subtrees accuracy (hours)
1 52% . 04 h
&lt;2 87 % .21 h
&lt;3 92 % .72 h
&lt;4 93% 1.6h
&lt;5 93 % 1.9 h
&lt;6 95 % 2.2 h
unbounded 96 % 3.5 h
</table>
<tableCaption confidence="0.999892">
Table 1. Parse results on the ATIS corpus
</tableCaption>
<bodyText confidence="0.999979909090909">
The table shows a dramatic increase in parse accuracy
when enlarging the maximum depth of the subtrees
from 1 to 2. (Remember that for depth one, DOP is
equivalent to a stochastic context-free grammar.) The
accuracy keeps increasing, at a slower rate, when the
depth is enlarged further. The highest accuracy is
obtained by using all subtrees from the training set:
72 out of the 75 sentences from the test set are parsed
correctly. Thus, the accuracy increases if larger
subtrees are used, though the CPU time increases
considerably as well.
</bodyText>
<subsectionHeader confidence="0.968338">
4.3 Does the most probable derivation
</subsectionHeader>
<bodyText confidence="0.998914466666667">
generate the most probable parse?
Another important feature of DOP is that the
probability of a resulting parse tree is computed as the
sum of the probabilities of all its derivations.
Although the most probable parse of a sentence is not
necessarily generated by the most probable derivation
of that sentence, there is a question as to how often
these two coincide. In order to study this, we also
calculated the derivation accuracy, defined as the
percentage of the test strings for which the parse
generated by the most probable derviation is identical
to the parse in the test set. The following table shows
the derivation accuracy against the parse accuracy for
the 75 test set strings from the ATIS corpus, using
different maximum depths for the corpus subtrees.
</bodyText>
<page confidence="0.990691">
109
</page>
<figure confidence="0.996972452380953">
depth of
subtrees
1
&lt;2
&lt;3
&lt;4
&lt;5
&lt;6
unbounded
derivation
accuracy
52%
47%
49%
57%
60%
65%
65%
parse
accuracy
52%
87%
92%
93%
93%
95%
96%
(S (NP
(VP
*)
VB/Arrange
(NP (NP DT/the NN/flight NN/code)
(PP IN/of
(NP (NP DT/the NN/flight)
(PP (PP IN/from
(NP NP/Denver))
(PP TO/to
(NP NP/Dallas
NP/Worth)))
(PP IN/in
(NP (VP VBG/descending)
NN/order))))))
</figure>
<tableCaption confidence="0.977626">
Table 2. Derivation accuracy vs. parse accuracy
</tableCaption>
<bodyText confidence="0.9999545">
The table shows that the derivation accuracy is equal
to the parse accuracy if the depth of the subtrees is
constrained to 1. This is not surprising, as for depth
1, DOP is equivalent with SCFG where every parse is
generated by exactly one derivation. What is
remarkable, is, that the derivation accuracy decreases if
the depth of the subtrees is enlarged to 2. If the depth
is enlarged further, the derivation accuracy increases
again. The highest derivation accuracy is obtained by
using all subtrees from the corpus (65%), but remains
far behind the highest parse accuracy (96%). From
this table we conclude that if we.are interested in the
most probable analysis of a string we must not look
at the probability of the process of achieving that
analysis but at the probability of the result of that
process.
</bodyText>
<subsectionHeader confidence="0.944815">
4.4 The significance of once-occurring
</subsectionHeader>
<bodyText confidence="0.91656575">
subtrees
There is an important question as to whether we can
reduce the &amp;quot;grammar constant&amp;quot; of DOP by eliminating
very infrequent subtrees, without affecting the parse
accuracy. In order to study this question, we start with
a test result. Consider the test set sentence &amp;quot;Arrange
the flight code of the flight from Denver
to Dallas Worth in descending order, which
has the following parse in the test set:
(PP IN/in
(NP (VP VBG/descending)
NN/order)))
The corresponding p-o-s sequence of this sentence is
the test set string &amp;quot;VB DT NN NN IN DT NN IN NP
TO NP NP IN VBG NN&amp;quot;. At subtree-depth 2, the
following most probable parse was estimated for this
string (where for reasons of readability the words are
added to the p-o-s tags):
In this parse, we see that the prepositional phrase &amp;quot;in
descending order&amp;quot; is incorrectly attached to the NP
&amp;quot;the flight&amp;quot; instead of to the verb &amp;quot;arrange&amp;quot;. This
wrong attachment may be explained by the high
relative frequencies of the following subtrees of depth
2 (that appear in structures of sentences like &amp;quot;Show me
the transportation from SFO to downtown San
Francisco in August&amp;quot;, where the PP &amp;quot;in August&amp;quot;
is attached to the NP &amp;quot;the transportation&amp;quot;, and
not to the verb &amp;quot;show&amp;quot;):
</bodyText>
<table confidence="0.5133574">
NP NP NP NP
PP PP PP
PP IN PP
NP PP IN
NP
</table>
<bodyText confidence="0.994852666666667">
Only if the maximum depth was enlarged to 4,
subtrees like the following were available, which led
to the estimation of the correct tree.
</bodyText>
<table confidence="0.771888666666667">
VP VB
NP NP
PP
PP IN
NP VP VBG
NN
</table>
<bodyText confidence="0.999945583333333">
It is interesting to note that this subtree occurs only
once in the training set. Nevertheless, it induces the
correct parsing of the test string. This seems to
contradict the fact that probabilities based on sparse
data are not reliable. Since many large subtrees are
once-occurring events (hapaxes), there seems to be a
preference in DOP for an occurrence-based approach if
enough context is provided: large subtrees, even if
they occur once, tend to contribute to the generation
of the correct parse, since they provide much
contextual information. Although these subtrees have
low probabilities, they tend to induce the correct parse
because fewer subtrees are needed to construct a parse.
Additional experiments seemed to confirm
this hypothesis. Throwing away all hapaxes yielded
an accuracy of 92%, which is a decrease of 4%.
Distinguishing between small and large hapaxes,
showed that the accuracy was not affected by
eliminating the hapaxes of depth 1 (however, as an
advantage, the convergence seemed to get slightly
faster). Eliminating hapaxes larger than depth 1,
decreased the accuracy. The following table shows the
parse accuracy after eliminating once-occurring
subtrees of different maximum depths.
</bodyText>
<figure confidence="0.92217595">
(S (NP *)
(VP VB/Arrange
(NP (NP DT/the NN/flight NN/code)
(PP IN/of
(NP (NP DT/the NN/flight)
(PP (PP IN/from
(NP NP/Denver))
(PP TO/to
(NP NP/Dallas
NP/worth))))))
110
depth of parse
hapaxes accuracy
1 96%
&lt;2 95%
&lt;3 95%
&lt;4 93%
&lt;5 92%
&lt;6 92%
unbounded 92%
</figure>
<tableCaption confidence="0.854092">
Table 3. Parse accuracy after eliminating once-occurring
subtrees
</tableCaption>
<sectionHeader confidence="0.691723" genericHeader="conclusions">
Conclusions
</sectionHeader>
<bodyText confidence="0.999983375">
We have shown that in DOP and STSG the Viterbi
algorithm cannot be used for computing a most
probable tree of a string. We developed a modification
of Viterbi which allows by means of an iterative
Monte Carlo search to estimate the most probable tree
of a string in polynomial time. Experiments on ATIS
showed that only in 68% of the cases, the most
probable derivation of a string generates the most
probable tree of that string, and that the parse accuracy
is dramatically higher than the derivation accuracy.
We conjectured that the Monte Carlo algorithm can
also be applied to other stochastic grammars for
computing the most probable tree of a string. The
question as to whether the most probable tree of a
string can also be deterministically derived in
polynomial time is still unsolved.
</bodyText>
<sectionHeader confidence="0.998436" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999122">
The author is indebted to Remko Scha for valuable
comments on an earlier version of this paper, and to
Khalil Sima&apos;an for useful discussions.
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999907576923077">
M. van den Berg, R. Bod &amp; R. Scha, 1994. &amp;quot;A
Corpus-Based Approach to Semantic Interpretation&amp;quot;,
Proceedings Ninth Amsterdam Colloquium,
Amsterdam.
E. Black, R. Garside and G. Leech, 1993.
Statistically-Driven Computer Grammars of English:
The IBM/Lancaster Approach, Rodopi: Amsterdam-
Atlanta.
R. Bod, 1992. &amp;quot;A Computational Model of
Language Performance: Data Oriented Parsing&amp;quot;,
Proceedings COLING&apos;92, Nantes.
R. Bod, 1993a. &amp;quot;Using an Annotated Corpus as a
Stochastic Grammar&amp;quot;, Proceedings European Chapter
fo the ACL&apos;93, Utrecht.
R. Bod, 1993b. &amp;quot;Monte Carlo Parsing&amp;quot;,
Proceedings Third International Workshop on Parsing
Technologies, Tilburg/Durbuy.
R. Bod, 1993c. &amp;quot;Data Oriented Parsing as a General
Framework for Stochastic Language Processing&amp;quot;, in:
K.Sikkel &amp; A. Nijholt (eds.), Parsing Natural
Language, TWLT6, Twente University.
R. Bod, 1995. Enriching Linguistics with
Statistics: Performance Models of Natural Language.
PhD-thesis, University of Amsterdam (forthcoming).
T. Briscoe and J. Carroll, 1993. &amp;quot;Generalized
Probabilistic LR Parsing of Natural Language
(Corpora) with Unification-Based Grammars&amp;quot;,
Computational Linguistics 19(1), 25-59.
T. Fujisalci, F. Jelinek, J. Cocke, E. Black and T.
Nishino, 1989. &amp;quot;A Probabilistic Method for Sentence
Disambiguation&amp;quot;, Proceedings 1st Int. Workshop on
Parsing Technologies, Pittsburgh.
J.M. Hammersley and D.C. Handscomb, 1964.
Monte Carlo Methods, Chapman and Hall, London.
C.T. Hemphill, J.J. Godfrey and G.R. Doddington,
1990. &amp;quot;The ATIS spoken language systems pilot
corpus&amp;quot;. Proceedings DARPA Speech and Natural
Language Workshop, Hidden Valley, Morgan
Kaufmann.
F. Jelinek, J.D. Lafferty and R.L. Mercer, 1990.
Basic Methods of Probabilistic Context Free
Grammars, Technical Report IBM RC 16374
(#72684), Yorktown Heights.
M. Kay, 1980. Algorithmic Schemata and Data
Structures in Syntactic Processing. Report CSL-80-
12, Xerox PARC, Palo Alto, Ca.
D. Magerman and C. Weir, 1992. &amp;quot;Efficiency,
Robustness and Accuracy in Picky Chart Parsing&amp;quot;,
Proceedings ACL&apos;92, Newark, Delaware.
M. Marcus, B. Santorini and M. Marcinkiewicz,
1993. &amp;quot;Building a Large Annotated Corpus of
English: the Penn Treebank&amp;quot;, Computational
Linguistics 19(2).
D. Rumelhart and J. McClelland, 1986. Parallel
Distributed Processing, The MIT Press, Cambridge,
Mass.
R. Scha, 1990. &amp;quot;Language Theory and Language
Technology; Competence and Performance&amp;quot; (in
Dutch), in Q.A.M. de Kort &amp; G.L.J. Leerdam (eds.),
Computertoepassingen in de Neerlandistiek, Almere:
Landelijke Vereniging van Neerlandici (LVVN-
jaarboek).
Y. Schabes, 1992. &amp;quot;Stochastic Lexicalized Tree-
Adjoining Grammars&amp;quot;, Proceedings COLING&apos;92,
Nantes.
Y. Schabes and R. Waters, 1993. &amp;quot;Stochastic
Lexicalized Context Free Grammars&amp;quot;, Proceedings
Third International Workshop on Parsing
Technologies, Tilburg/Durbuy.
K. Sima&apos;an, R. Bod, S. Krauwer and R. Scha,
1994. &amp;quot;Efficient Disambiguation by means of
Stochastic Tree Substitution Grammars&amp;quot;, Proceedings
International Conference on New Methods in
Language Processing, UMIST, Manchester.
A. Viterbi, 1967. &amp;quot;Error bounds for convolutional
codes and an asymptotically optimum decoding
algorithm&amp;quot;, IEEE Trans. Information Theory, IT-13,
260-269.
</reference>
<page confidence="0.998799">
111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997921">The Problem of Computing the Most Probable Tree in Data-Oriented Parsing and Stochastic Tree Grammars</title>
<author confidence="0.849272">Rens Bod</author>
<affiliation confidence="0.956807">Institute for Logic, Language and Computation Department of Computational Linguistics University of Amsterdam</affiliation>
<address confidence="0.9846115">Spuistraat 134, 1012 VB Amsterdam The Netherlands</address>
<email confidence="0.919941">rens@mars.let.uva.nl</email>
<abstract confidence="0.995713552941177">We deal with the question as to whether there exists a polynomial time algorithm for computing the most probable parse tree of a sentence generated by a data-oriented parsing (DOP) model. (Scha, 1990; Bod, 1992, 1993a). Therefore we describe DOP as a stochastic tree-substitution grammar (STSG). In STSG, a tree can be generated by exponentially many derivations involving different elementary trees. The probability of a tree is equal to the sum of the probabilities of all its derivations. We show that in STSG, in contrast with stochastic context-free grammar, the Viterbi algorithm cannot be used for computing a most probable tree of a string. We propose a simple modification of Viterbi which allows by means of to most probable tree of a string in polynomial time. Experiments with DOP on ATIS show that only in 68% of the cases, the most probable derivation of a string generates the most probable tree of that string. Therefore, the parse accuracy obtained by the most probable trees (96%) is dramatically higher than the parse accuracy obtained by the most probable derivations (65%). It is still an open question whether the most probable tree of a string can be in polynomial time. 1 Data-Oriented Parsing A Data-Oriented Parsing model (Scha, 1990; Bod, 1992, 1993a) is characterized by a corpus of analyzed language utterances, together with a set of operations that combine sub-analyses from the corpus into new analyses. We will limit ourselves in this paper to corpora with purely syntactic annotations. For the semantic dimension of DOP, the reader is referred to (van den Berg et al., 1994). Consider the imaginary example corpus consisting of only two trees in figure 1. We will assume one operation for combining subtrees. This operation is called &amp;quot;composition&amp;quot;, and is indicated by the infix operator The composition tou, a copy of which its leftmost nontenninal leaf node has been identified the root nodeof the nontenninal leaf node of reasons of we will write in the following Figure 1. Example corpus of two trees. the (ambiguous) sentence displayed the on the table&amp;quot; be parsed by combining subtrees from the corpus. For instance: 0 NP NP VP the dress the table she vp PP V NP displayed NP VP I she vp PP A displayed the dress on the table 2. Derivation and parse tree for displayed the dress on the table&amp;quot; IA she V NP wanted /\ NP PP AA dress P I on the rack NP 104 As the reader may easily ascertain, a different derivation may yield a different parse tree. However, a different derivation may also very well yield the same parse tree; for instance: NP she VP 0 NP V NP the dress displayed VP PP p NP I on the table Different derivation generating the same parse for displayed the dress on the table&amp;quot; or 4. derivation generating the same parse for displayed the dress on the table&amp;quot; Thus, a parse tree can have several derivations involving different subtrees. Using the corpus for our stochastic estimations, we estimate the probability of substituting a certain subtree on a specific node as the probability of selecting this subtree among all subtrees in the corpus that could be substituted on that node.&apos; The probability of a derivation can be computed as the product of the probabilities of the substitutions that it involves. The probability of a parse tree is equal to the probability that any of its derivations occurs, which is the sum of the probabilities of all derivations of that parse tree. Finally, the probability of a word string is equal to the sum of the probabilities of all its parse trees. DOP as a Stochastic Tree- Substitution Grammar In order to deal with the problem of computing the most probable parse tree of a string, it is convenient to describe DOP as a &amp;quot;Stochastic Tree-Substitution Grammar&amp;quot; (STSG). STSG can be seen as a generalization over DOP, where the elementary trees of STSG are the subtrees of DOP, and the probabilities of the elementary trees are the small frequencies are smoothed by substitution-probabilities of the corresponding subtrees of DOP (Bod, 1993c). Grammar G a five- V, S. is a finite set of nonterminal symbols. is a finite set of terminal symbols. is the distinguished symbol. a finite set of elementary trees whose top nodes and interior nodes are labeled by nonterminal symbols and whose yield nodes are labeled by terminal or nonterminal symbols. a function which assigns to every elementary tree R a probability a tree a root interpreted as the probability of substituting require, therefore, that 0 &lt; and = 1. trees such that the yield node equal to the ot2 the tree that results from substituting this leftmost nonterminal yield node in function ois called substitution. reasons of conciseness we will use the term substitution for leftmost substitution. derivation by an STSG a tuple of trees that are of root of labeled by the of labeled by terminal symbols. The of leftmost derivations generated by thus by = I R = S A . convenience we will use the term derivation for derivation. A derivation called derivation of tree = derivation called a derivation of string s. probability of a derivation defined as 1) • ... • p(t,). tree by an STSG a tree that there is a derivation &lt; &gt; which .t, set of trees, or language, by given = 3 : = T). reasons of conciseness we will use the terms a parse tree. A whose yield is equal to string called a parse probability of a parse is defined as the sum of the probabilities of all its derivations. by an STSG an of such that there is a parse generated by G whose yield is equal to the string. The set of or language, by given = {s 3 Te Parses(G) = probability of a string is defined as the sum of the probabilities of all its parses. This means that the probability of a string is also equal to the sum of the probabilities of all its derivations.</abstract>
<title confidence="0.72456875">VP VP PP A A NP NP</title>
<abstract confidence="0.997038622222223">displayed NP she the dress I /\ on the table 105 Computing a most probable the input string abcd, the following derivation parse tree in STSG forest is then obtained: In order to deal with the problem of computing the most probable parse tree of a sentence, we will distinguish between parsing and disambiguation. By parsing we mean the creation of a parse forest for an input sentence. By disambiguation we mean the of the most probable from the forest. The creation of a parse forest is an intermediate step for computing the most probable parse. 3.1 Parsing From the way STSG combines elementary trees by means of substitution, it follows that an input sentence can be parsed by the same algorithms as Every elementary tree used as a rewrite rule --&gt; yield(t). a parsing algorithm, an input sentence of length be parsed in time. In order to obtain a chart-like forest for a sentence parsed in STSG, we need to label the wellformed substrings in the chart not only with the syntactic categories of that substring but with the full trees correspond to the use of the rules --&gt;yield(t). that in a chartlike forest generated by an STSG, different derivations that generate a same tree do not collapse. We will talk about a forest by an STSG (cf. Sima&apos;an et al., 1994). The following formal example illustrates what a derivation forest of a string may look like. In the example, we leave out the probabilities, which are needed only in the disambiguation process. The visual representation comes from (Kay, 1980): every entry the chart is indicated by an edge and spans the words between the i-th and the j-th position of a sentence. Every edge is labeled with the elementary trees that denote the underlying phrase. The example- STSG consists of the following elementary trees: 2Although theoretically there can be more than one most probable parse for a sentence, in practice a system that employs a non-trivial treebank tends to generate exactly one most probable parse for a given input sentence. Note that different derivations in the forest generate the same tree. By exhaustively unpacking the forest, four different derivations generating two different trees are obtained. We may ask whether we can pack the forest by collapsing spurious derivations. Unfortunately, no efficient procedure is known that accomplishes this (remember that there can be exponentially many derivations for one tree). 3.2 Disambiguation Cubic time parsing does not guarantee cubic time disambiguation, as a sentence may have exponentially many parses and any such parse may have exponentially many derivations. Therefore, in order to find the most probable parse of a sentence, it is not efficient to compare the probabilities of the parses by exhaustively unpacking the chart. Even for determining the probability of one parse, it is not efficient to add the probabilities of all derivations of that parse. 3.2.1 Viterbi optimization is not feasible for finding the most probable parse There exists a heuristic optimization algorithm, known as Viterbi optimization, which selects on the basis of an SCFG the most probable derivation of a sentence in cubic time (Viterbi, 1967; Fujisaki et al., 1989; Jelinek et al., 1990). In STSG, however, the most probable derivation does not necessarily generate the most probable parse, as the probability of a parse is defined as the sum of the probabilities of all its derivations. Thus, there is an important question as to whether we can adapt the Viterbi algorithm for finding the most probable parse. To understand the difficulty of the problem, we look in more detail at the Viterbi algorithm. The basic idea of the Viterbi algorithm is the early pruning of low probability subderivations in a bottom-up fashion. Two different subderivations of the same part of the sentence and whose resulting Figure 5. Elementary trees of an example-STSG</abstract>
<title confidence="0.880792">g A (\ I a b c d A A A A A C A A</title>
<abstract confidence="0.995418838785047">b b • • .d 6. Derivation forest for 106 subparses have the same root can both be developed (if at all) to derivations of the whole sentence in the same ways. Therefore, if one of these two subderivations has a lower probability, then it can be eliminated. This is illustrated by a formal example in 7. Suppose that parsing of string following two subderivations been generated for the substring (Actually represented are their resulting subparses.) 7. Two subparses for the string the probability of higher than the probability can eliminate we are only interested in the most probable derivationof if are interested in finding the most probable parseof by STSG), we are not allowed to can be seen by the following. Suppose that we have the additional elementary tree given in figure 8. A a Figure 8. Elementary tree. This elementary tree may be developed to the same that can be developed by not to the tree can be developed by since the probability of a parse tree is equal to the sum of the probabilities all its derivations, it is still possible that contributes to the generation of the most probable Therefore we are not allowed to eliminate This counter-example does not prove that there is no heuristic optimization that allows polynomial time selection of the most probable parse. it makes clear that a as accomplished by Viterbi, is not adequate for finding the most probable parse in STSG. So far, it is unknown whether the problem of finding the most probable parse in a deterministic way is inherently exponential or not (cf. Sima&apos;an et al., 1994). One should of course ask how often in practice the most probable derivation produces the most probable parse, but this can only be answered by means of experiments on real life corpora. Experiments on the ATIS corpus (see session 4) show that only in 68% of the cases the most probable derivation of a sentence generates the most probable parse of that sentence. Moreover, the parse accuracy obtained by the most probable parse is dramatically higher than the parse accuracy obtained by the parse generated by the most probable derivation. 3.2.2 Estimating the most probable parse by Monte Carlo search will leave it an open question whether the most probable parse can be deterministically derived in polynomial time. Here we will ask whether there exists a polynomial time approximation procedure the probable parse with an estimation error that can be made arbitrarily small. have seen that a as accomplished by Viterbi, can be used for finding the most probable derivation but not for finding the most probable parse. If we apply instead of a select-best a we can generate a random derivation. By iteratively generating a large number of random derivations we can estimate the most probable parse as the parse which results most often from these random derivations (since the probability of a parse is the probability that any of its derivations occurs). The most probable parse can be estimated as accurately as desired by making the number of random samples as large as desired. According to the Law of Large Numbers, the most often generated parse converges to the most probable parse. Methods that estimate the probability of an event by taking random samples are known as Monte methods (Hammersley &amp; Handscomb, The selection of a random derivation is accomplished in a bottom-up fashion analogous to Viterbi. Instead of selecting the most probable subderivation at each node-sharing in the chart, a random subderivation is selected (i.e. sampled) at each (that is, a subderivation that has as large a probability as another subderivation should have as large a chance to be chosen as this other subderivation). Once sampled at the S-node, the random derivation of the whole sentence can be retrieved by tracing back the choices made at each node-sharing. Of course, we may postpone sampling until the S-node, such that we sample directly from the distribution of all S-derivations. But this would take exponential time, since there may be exponentially many derivations for the whole sentence. By sampling bottom-up at every node where ambiguity appears, the maximum number of different subderivations at each node-sharing is bounded to a constant (the total number of rules of that node), and therefore the time complexity of generating a random derivation of an input sentence is equal to the time complexity of finding the most probable derivation, is exemplified by the following algorithm. 3Note that Monte Carlo estimation of the most probable parse is more reliable than the estimation of the most probable parse by generating the n most probable derivations by Viterbi, since it might be that the most probable parse is exclusively generated by many low probable derivations. The Monte Carlo method is guaranteed to converge to the most probable parse. 107 Sampling a random derivation from a derivation forest a derivation forest, of a sentence of of labeled entries span the words between the i-th and the j-th position of the sentence. Every entry is labeled with linked elementary trees, together with their probabilities, that constitute subderivations of the underlying subsentence. Sampling a derivation from the chart consists of choosing at every labeled entry (bottom-up, breadthfirst) a random subderivation of each root-node: for k := 1 to n do for i := 0 to n-k do for chart-entry (i,i+k) do for each root-node X do a random subderivation of root X eliminate the other subderivations We now have an algorithm that selects a random derivation from a derivation forest. Converting this derivation into a parse tree gives a first estimation for the most probable parse. Since one random sample is not a reliable estimate, we sample a large number of random derivations and see which parse is generated most frequently. This is exemplified by the following algorithm. (Note that we might also estimate the probable random sampling, namely by counting which derivation is sampled most often; however, the most probable derivation can be more effectively generated by Viterbi.) Estimating the most probable parse (MPP) Given a derivation forest for an input sentence: repeat until the MPP converges sample a random derivation from the forest store the parse generated by the random derivation MPP := the most frequently occurring parse There is an important question as to how long the convergence of the most probable parse may take. Is there a tractable upper bound on the number of derivations that have to be sampled from the forest before stability in the top of the parse distribution occurs? The answer is yes: the worst case time complexity of achieving a maximum estimation error means of random sampling is independently of the probability distribution. This is a classical result from sampling theory (cf. Hammersley and Handscomb, 1964), and follows directly from Chebyshev&apos;s inequality. In practice, it means that the 4Let ( , (e2, , , ) a probability of events e2, ..., event iis said to be randomly selected iff its probability of being selected equal to order to allow for &amp;quot;direct sampling&amp;quot;, one must convert the probability distribution into a corresponding sample space for which holds that the of occurrence of each event iis positive equal to the size of the sample space. inversely proportional to the square-root of number of random samples therefore, to a factor of number of samples to be increased In practical experiments (see §4), we will limit the number of samples to a sufficiently large bound What is the theoretical worst case time complexity of parsing and disambiguation together? That is, given an STSG and an input sentence, what is the maximal time cost of finding the most probable parse of a sentence? If we use a CKY-parser, the creation of a derivation forest for a sentence of n takes Taking also into account the size G of an STSG (defined as the sum of the lengths of the yields of all its elementary trees), the time complexity of creating a derivation forest is to . time complexity of disambiguation is both proportional to the cost of a derivation, i.e. to the cost of the by means of iteration, which is means that the time complexity of disambiguation is by complexity of and disambiguation is equal to + = there exists a tractable procedure that estimates the most probable parse of an input sentence. Notice that although the Monte Carlo disambiguation algorithm estimates the most probable parse of a sentence in polynomial time, it is not in the class of polynomial time decidable The Monte Carlo algorithm cannot in polynomial time what is the most probable parse; it can only make the error-probability of the estimated most probable parse arbitrarily small. As such, the Monte Carlo algorithm is a probabilistic algorithm belonging to the class of Bounded error Probabilistic time We hypothesize that Monte Carlo disambiguation is also relevant for other stochastic grammars. It turns out that all stochastic extensions of CFGs that are stochastically richer than SCFG need exponential time algorithms for finding a most probable parse tree (cf. Briscoe &amp; Carroll, 1992; Black et al., 1993; Magerman &amp; Weir, 1992; Schabes &amp; Waters, 1993). To our knowledge, it has never been studied whether there exist BPP-algorithms for these models. Alhough it is beyond the scope of our research, we conjecture that there exists a Monte Carlo disambiguation algorithm for at least Stochastic Tree-Adjoining Grammar (Schabes, 1992). 3.2.3 Psychological relevance of Monte Carlo disambiguation As has been noted, an important difference between the Viterbi algorithm and the Monte Carlo algorithm is, that with the latter we never have 100% confidence. In our opinion, this should not be seen as a disadvantage. In fact, absolute confidence about the most probable parse does not have any significance, as the probability assigned to a parse is already an estimation of its actual probability. One may ask as to whether Monte Carlo is appropriate for modeling 108 human sentence perception. The following lists some properties of Monte Carlo disambiguation that may be of psychological interest: 1. As mentioned above, Monte Carlo never provides 100% confidence about the best analysis. This corresponds to the psychological observation that people never have absolute confidence about their interpretation of an ambiguous sentence. 2. Although conceptually Monte Carlo uses the total space of possible analyses, it tends to sample only the most likely ones. Very unlikely analyses may only be sampled after considerable time, but it is not that are found in finite time. This matches with experiments on human sentence perception where very implausible analyses are only perceived with great difficulty and after considerable time. 3. Monte Carlo does not necessarily give the same results for different sequences of samples, especially if different analyses in the top of the distribution are almost equally likely. In the case there is more than one most probable analysis, Monte Carlo does not converge to one analysis but keeps alternating, however large the number of samples is made. In experiments with human sentence perception, it has often been shown that different analyses can be perceived for one sentence. And in case these analyses are equally plausible, people perceive so-called fluctuation effects. This fluctuation phenomenon is also well-known in the perception of ambiguous visual patterns. 4. Monte Carlo can be made parallel in a very way: can be computed by units, where equal outputs are reinforced. The more processing units are employed, the better the estimation. However, since the number of processing units is finite, there is never absolute confidence. This has some similarity with the Parallel Distributed Processing paradigm for human (language) processing (Rumelhart &amp; McClelland, 1986). 4 Experiments In this section, we report on experiments with an implementation of DOP that parses and disambiguates part-of-speech strings. In (Bod, 1995) it is shown how DOP is extended to parse word strings that possibly contain unknown words. 4.1 The test environment For our experiments, we used a manually corrected version of the Air Travel Information System (ATIS) spoken language corpus (Hemphill et al., 1990) annotated in the Pennsylvania Treebank (Marcus et al., 1993). We employed the &amp;quot;blind testing&amp;quot; method, dividing the corpus into a 90% training set and a 10% test set by randomly selecting sentences. The 675 trees from the training set were converted into their subtrees together with their relative frequencies, roughly 4*different subtrees. The 75 part-of-speech sequences from the test set served as input strings that were parsed and disambiguated using the subtrees from the training set. As motivated in 1993b), we use the notion of accuracy our accuracy metric, defined as the percentage of the test strings for which the most probable parse is the parse in the test set. 4.2 Accuracy as a function of subtree-depth It is one of the most essential features of DOP, that arbitrarily large subtrees are taken into consideration to estimate the probability of a parse. In order to test the usefulness of this feature, we performed different constraining the the subtrees. The following table shows the results of seven experiments for different maximum depths of the training set subtrees. The accuracy refers to the parse accuracy at 400 randomly sampled parses, and is rounded off to the nearest integer. The CPU time refers to the average CPU time per string employed by a Spark II. depth of parse CPU time subtrees accuracy (hours) 1 52% . 04 h &lt;2 87 % .21 h &lt;3 92 % .72 h &lt;4 93% 1.6h &lt;5 93 % 1.9 h &lt;6 95 % 2.2 h unbounded 96 % 3.5 h Table 1. Parse results on the ATIS corpus The table shows a dramatic increase in parse accuracy when enlarging the maximum depth of the subtrees from 1 to 2. (Remember that for depth one, DOP is equivalent to a stochastic context-free grammar.) The accuracy keeps increasing, at a slower rate, when the depth is enlarged further. The highest accuracy is obtained by using all subtrees from the training set: 72 out of the 75 sentences from the test set are parsed correctly. Thus, the accuracy increases if larger subtrees are used, though the CPU time increases considerably as well. 4.3 Does the most probable derivation generate the most probable parse? Another important feature of DOP is that the probability of a resulting parse tree is computed as the sum of the probabilities of all its derivations. Although the most probable parse of a sentence is not necessarily generated by the most probable derivation of that sentence, there is a question as to how often these two coincide. In order to study this, we also the accuracy, as the percentage of the test strings for which the parse generated by the most probable derviation is identical to the parse in the test set. The following table shows the derivation accuracy against the parse accuracy for the 75 test set strings from the ATIS corpus, using different maximum depths for the corpus subtrees. 109 depth of 1 &lt;2 &lt;3 &lt;4 &lt;5 &lt;6 unbounded derivation accuracy 52% 47% 49% 57% 60% 65% 65% parse accuracy 52% 87% 92% 93% 93% 95% 96% (VP *) VB/Arrange (NP (NP DT/the NN/flight NN/code) (PP IN/of (NP (NP DT/the NN/flight) (PP (PP IN/from (NP NP/Denver)) (PP TO/to (NP NP/Dallas NP/Worth))) (PP IN/in (NP (VP VBG/descending) NN/order)))))) Table 2. Derivation accuracy vs. parse accuracy The table shows that the derivation accuracy is equal to the parse accuracy if the depth of the subtrees is constrained to 1. This is not surprising, as for depth 1, DOP is equivalent with SCFG where every parse is generated by exactly one derivation. What is remarkable, is, that the derivation accuracy decreases if the depth of the subtrees is enlarged to 2. If the depth is enlarged further, the derivation accuracy increases again. The highest derivation accuracy is obtained by using all subtrees from the corpus (65%), but remains far behind the highest parse accuracy (96%). From this table we conclude that if we.are interested in the most probable analysis of a string we must not look the probability of the achieving that but at the probability of the that process. The of once-occurring subtrees an important question as to whether we can reduce the &amp;quot;grammar constant&amp;quot; of DOP by eliminating very infrequent subtrees, without affecting the parse accuracy. In order to study this question, we start with test result. Consider the test set sentence the flight code of the flight from Denver Dallas Worth in descending order, has the following parse in the test set: (PP IN/in (NP (VP VBG/descending) NN/order))) The corresponding p-o-s sequence of this sentence is test set string DT NN NN IN DT NN IN NP NP NP IN VBG NN&amp;quot;. subtree-depth 2, the following most probable parse was estimated for this string (where for reasons of readability the words are added to the p-o-s tags): In this parse, we see that the prepositional phrase &amp;quot;in order&amp;quot; incorrectly attached to the NP flight&amp;quot; of to the verb wrong attachment may be explained by the high relative frequencies of the following subtrees of depth (that appear in structures of sentences like me the transportation from SFO to downtown San the PP &amp;quot;in attached to NP transportation&amp;quot;, not to the verb &amp;quot;show&amp;quot;):</abstract>
<title confidence="0.895996">NP NP NP NP PP PP PP PP IN PP NP PP IN NP</title>
<abstract confidence="0.960987666666667">Only if the maximum depth was enlarged to 4, subtrees like the following were available, which led to the estimation of the correct tree.</abstract>
<title confidence="0.918662333333333">VP VB NP NP PP PP IN NP VP VBG NN</title>
<abstract confidence="0.990783611940299">It is interesting to note that this subtree occurs only once in the training set. Nevertheless, it induces the correct parsing of the test string. This seems to contradict the fact that probabilities based on sparse data are not reliable. Since many large subtrees are once-occurring events (hapaxes), there seems to be a preference in DOP for an occurrence-based approach if enough context is provided: large subtrees, even if they occur once, tend to contribute to the generation of the correct parse, since they provide much contextual information. Although these subtrees have low probabilities, they tend to induce the correct parse because fewer subtrees are needed to construct a parse. Additional experiments seemed to confirm this hypothesis. Throwing away all hapaxes yielded an accuracy of 92%, which is a decrease of 4%. Distinguishing between small and large hapaxes, showed that the accuracy was not affected by eliminating the hapaxes of depth 1 (however, as an advantage, the convergence seemed to get slightly faster). Eliminating hapaxes larger than depth 1, decreased the accuracy. The following table shows the parse accuracy after eliminating once-occurring subtrees of different maximum depths. (S (NP *) (VP VB/Arrange (NP (NP DT/the NN/flight NN/code) (PP IN/of (NP (NP DT/the NN/flight) (PP (PP IN/from (NP NP/Denver)) (PP TO/to (NP NP/Dallas NP/worth)))))) 110 depth of parse hapaxes accuracy 1 96% &lt;2 95% &lt;3 95% &lt;4 93% &lt;5 92% &lt;6 92% unbounded 92% Table 3. Parse accuracy after eliminating once-occurring subtrees Conclusions We have shown that in DOP and STSG the Viterbi algorithm cannot be used for computing a most probable tree of a string. We developed a modification of Viterbi which allows by means of an iterative Carlo search to most probable tree of a string in polynomial time. Experiments on ATIS showed that only in 68% of the cases, the most probable derivation of a string generates the most probable tree of that string, and that the parse accuracy is dramatically higher than the derivation accuracy. We conjectured that the Monte Carlo algorithm can also be applied to other stochastic grammars for computing the most probable tree of a string. The question as to whether the most probable tree of a can also be in polynomial time is still unsolved. Acknowledgments The author is indebted to Remko Scha for valuable comments on an earlier version of this paper, and to Khalil Sima&apos;an for useful discussions.</abstract>
<note confidence="0.944567">References M. van den Berg, R. Bod &amp; R. Scha, 1994. &amp;quot;A Corpus-Based Approach to Semantic Interpretation&amp;quot;, Proceedings Ninth Amsterdam Colloquium, Amsterdam. E. Black, R. Garside and G. Leech, 1993. Statistically-Driven Computer Grammars of English: IBM/Lancaster Approach, Amsterdam- Atlanta. R. Bod, 1992. &amp;quot;A Computational Model of Language Performance: Data Oriented Parsing&amp;quot;, COLING&apos;92, R. Bod, 1993a. &amp;quot;Using an Annotated Corpus as a Grammar&amp;quot;, European Chapter the ACL&apos;93, R. Bod, 1993b. &amp;quot;Monte Carlo Parsing&amp;quot;, Proceedings Third International Workshop on Parsing R. Bod, 1993c. &amp;quot;Data Oriented Parsing as a General Framework for Stochastic Language Processing&amp;quot;, in: &amp; A. Nijholt (eds.), Natural</note>
<affiliation confidence="0.834444">Twente University.</affiliation>
<address confidence="0.76984">Bod, 1995. Linguistics with</address>
<note confidence="0.798204860465116">Statistics: Performance Models of Natural Language. PhD-thesis, University of Amsterdam (forthcoming). T. Briscoe and J. Carroll, 1993. &amp;quot;Generalized Probabilistic LR Parsing of Natural Language (Corpora) with Unification-Based Grammars&amp;quot;, Linguistics T. Fujisalci, F. Jelinek, J. Cocke, E. Black and T. Nishino, 1989. &amp;quot;A Probabilistic Method for Sentence 1st Int. Workshop on Technologies, J.M. Hammersley and D.C. Handscomb, 1964. Carlo Methods, and Hall, London. C.T. Hemphill, J.J. Godfrey and G.R. Doddington, 1990. &amp;quot;The ATIS spoken language systems pilot DARPA Speech and Natural Workshop, Valley, Morgan Kaufmann. F. Jelinek, J.D. Lafferty and R.L. Mercer, 1990. Basic Methods of Probabilistic Context Free Report IBM RC 16374 (#72684), Yorktown Heights. Kay, 1980. Schemata and Data in Syntactic Processing. 12, Xerox PARC, Palo Alto, Ca. D. Magerman and C. Weir, 1992. &amp;quot;Efficiency, Robustness and Accuracy in Picky Chart Parsing&amp;quot;, ACL&apos;92, Delaware. M. Marcus, B. Santorini and M. Marcinkiewicz, 1993. &amp;quot;Building a Large Annotated Corpus of the Penn Treebank&amp;quot;, Rumelhart and J. McClelland, 1986. Processing, MIT Press, Cambridge, Mass. R. Scha, 1990. &amp;quot;Language Theory and Language Technology; Competence and Performance&amp;quot; (in Dutch), in Q.A.M. de Kort &amp; G.L.J. Leerdam (eds.), in de Neerlandistiek, Landelijke Vereniging van Neerlandici (LVVNjaarboek). Y. Schabes, 1992. &amp;quot;Stochastic Lexicalized Tree- Grammars&amp;quot;, COLING&apos;92, Nantes. Y. Schabes and R. Waters, 1993. &amp;quot;Stochastic</note>
<title confidence="0.8223875">Context Free Grammars&amp;quot;, Third International Workshop on Parsing</title>
<author confidence="0.794031">K Sima&apos;an</author>
<author confidence="0.794031">R Bod</author>
<author confidence="0.794031">S Krauwer</author>
<author confidence="0.794031">R Scha</author>
<note confidence="0.502096888888889">1994. &amp;quot;Efficient Disambiguation by means of Tree Substitution Grammars&amp;quot;, International Conference on New Methods in Processing, Manchester. A. Viterbi, 1967. &amp;quot;Error bounds for convolutional codes and an asymptotically optimum decoding Trans. Information Theory, 260-269. 111</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M van den Berg</author>
<author>R Bod</author>
<author>R Scha</author>
</authors>
<title>A Corpus-Based Approach to Semantic Interpretation&amp;quot;,</title>
<date>1994</date>
<booktitle>Proceedings Ninth Amsterdam Colloquium,</booktitle>
<location>Amsterdam.</location>
<marker>van den Berg, Bod, Scha, 1994</marker>
<rawString>M. van den Berg, R. Bod &amp; R. Scha, 1994. &amp;quot;A Corpus-Based Approach to Semantic Interpretation&amp;quot;, Proceedings Ninth Amsterdam Colloquium, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>R Garside</author>
<author>G Leech</author>
</authors>
<title>Statistically-Driven Computer Grammars of English: The IBM/Lancaster Approach,</title>
<date>1993</date>
<location>Rodopi: AmsterdamAtlanta.</location>
<contexts>
<context position="21326" citStr="Black et al., 1993" startWordPosition="3637" endWordPosition="3640">not decide in polynomial time what is the most probable parse; it can only make the error-probability of the estimated most probable parse arbitrarily small. As such, the Monte Carlo algorithm is a probabilistic algorithm belonging to the class of Bounded error Probabilistic Polynomial time (BPP) algorithms. We hypothesize that Monte Carlo disambiguation is also relevant for other stochastic grammars. It turns out that all stochastic extensions of CFGs that are stochastically richer than SCFG need exponential time algorithms for finding a most probable parse tree (cf. Briscoe &amp; Carroll, 1992; Black et al., 1993; Magerman &amp; Weir, 1992; Schabes &amp; Waters, 1993). To our knowledge, it has never been studied whether there exist BPP-algorithms for these models. Alhough it is beyond the scope of our research, we conjecture that there exists a Monte Carlo disambiguation algorithm for at least Stochastic Tree-Adjoining Grammar (Schabes, 1992). 3.2.3 Psychological relevance of Monte Carlo disambiguation As has been noted, an important difference between the Viterbi algorithm and the Monte Carlo algorithm is, that with the latter we never have 100% confidence. In our opinion, this should not be seen as a disadv</context>
</contexts>
<marker>Black, Garside, Leech, 1993</marker>
<rawString>E. Black, R. Garside and G. Leech, 1993. Statistically-Driven Computer Grammars of English: The IBM/Lancaster Approach, Rodopi: AmsterdamAtlanta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>A Computational Model of Language Performance: Data Oriented Parsing&amp;quot;,</title>
<date>1992</date>
<booktitle>Proceedings COLING&apos;92,</booktitle>
<location>Nantes.</location>
<contexts>
<context position="1637" citStr="Bod, 1992" startWordPosition="254" endWordPosition="255">t-random&amp;quot; search to estimate the most probable tree of a string in polynomial time. Experiments with DOP on ATIS show that only in 68% of the cases, the most probable derivation of a string generates the most probable tree of that string. Therefore, the parse accuracy obtained by the most probable trees (96%) is dramatically higher than the parse accuracy obtained by the most probable derivations (65%). It is still an open question whether the most probable tree of a string can be deterministically computed in polynomial time. 1 Data-Oriented Parsing A Data-Oriented Parsing model (Scha, 1990; Bod, 1992, 1993a) is characterized by a corpus of analyzed language utterances, together with a set of operations that combine sub-analyses from the corpus into new analyses. We will limit ourselves in this paper to corpora with purely syntactic annotations. For the semantic dimension of DOP, the reader is referred to (van den Berg et al., 1994). Consider the imaginary example corpus consisting of only two trees in figure 1. We will assume one operation for combining subtrees. This operation is called &amp;quot;composition&amp;quot;, and is indicated by the infix operator The composition of t and u, tou, yields a copy o</context>
</contexts>
<marker>Bod, 1992</marker>
<rawString>R. Bod, 1992. &amp;quot;A Computational Model of Language Performance: Data Oriented Parsing&amp;quot;, Proceedings COLING&apos;92, Nantes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Using an Annotated Corpus as a Stochastic Grammar&amp;quot;,</title>
<date>1993</date>
<booktitle>Proceedings European Chapter fo the ACL&apos;93,</booktitle>
<location>Utrecht.</location>
<contexts>
<context position="4668" citStr="Bod, 1993" startWordPosition="788" endWordPosition="789">, the probability of a word string is equal to the sum of the probabilities of all its parse trees. 2 DOP as a Stochastic TreeSubstitution Grammar In order to deal with the problem of computing the most probable parse tree of a string, it is convenient to describe DOP as a &amp;quot;Stochastic Tree-Substitution Grammar&amp;quot; (STSG). STSG can be seen as a generalization over DOP, where the elementary trees of STSG are the subtrees of DOP, and the probabilities of the elementary trees are the 1Very small frequencies are smoothed by Good-Turing. substitution-probabilities of the corresponding subtrees of DOP (Bod, 1993c). A Stochastic Tree-Substitution Grammar G is a fivetuple &lt;VN, V, S. R, P&gt; where VN is a finite set of nonterminal symbols. VT is a finite set of terminal symbols. S E VN is the distinguished symbol. R is a finite set of elementary trees whose top nodes and interior nodes are labeled by nonterminal symbols and whose yield nodes are labeled by terminal or nonterminal symbols. P is a function which assigns to every elementary tree te R a probability p(t). For a tree t with a root a, p(t) is interpreted as the probability of substituting t on a. We require, therefore, that 0 &lt; p(t) 1 and Itroot</context>
<context position="25062" citStr="Bod, 1993" startWordPosition="4215" endWordPosition="4216">vel Information System (ATIS) spoken language corpus (Hemphill et al., 1990) annotated in the Pennsylvania Treebank (Marcus et al., 1993). We employed the &amp;quot;blind testing&amp;quot; method, dividing the corpus into a 90% training set and a 10% test set by randomly selecting sentences. The 675 trees from the training set were converted into their subtrees together with their relative frequencies, yielding roughly 4* i05 different subtrees. The 75 part-of-speech sequences from the test set served as input strings that were parsed and disambiguated using the subtrees from the training set. As motivated in (Bod, 1993b), we use the notion of parse accuracy as our accuracy metric, defined as the percentage of the test strings for which the most probable parse is identical to the parse in the test set. 4.2 Accuracy as a function of subtree-depth It is one of the most essential features of DOP, that arbitrarily large subtrees are taken into consideration to estimate the probability of a parse. In order to test the usefulness of this feature, we performed different experiments constraining the depth of the subtrees. The following table shows the results of seven experiments for different maximum depths of the </context>
</contexts>
<marker>Bod, 1993</marker>
<rawString>R. Bod, 1993a. &amp;quot;Using an Annotated Corpus as a Stochastic Grammar&amp;quot;, Proceedings European Chapter fo the ACL&apos;93, Utrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Monte Carlo Parsing&amp;quot;,</title>
<date>1993</date>
<booktitle>Proceedings Third International Workshop on Parsing Technologies,</booktitle>
<location>Tilburg/Durbuy.</location>
<contexts>
<context position="4668" citStr="Bod, 1993" startWordPosition="788" endWordPosition="789">, the probability of a word string is equal to the sum of the probabilities of all its parse trees. 2 DOP as a Stochastic TreeSubstitution Grammar In order to deal with the problem of computing the most probable parse tree of a string, it is convenient to describe DOP as a &amp;quot;Stochastic Tree-Substitution Grammar&amp;quot; (STSG). STSG can be seen as a generalization over DOP, where the elementary trees of STSG are the subtrees of DOP, and the probabilities of the elementary trees are the 1Very small frequencies are smoothed by Good-Turing. substitution-probabilities of the corresponding subtrees of DOP (Bod, 1993c). A Stochastic Tree-Substitution Grammar G is a fivetuple &lt;VN, V, S. R, P&gt; where VN is a finite set of nonterminal symbols. VT is a finite set of terminal symbols. S E VN is the distinguished symbol. R is a finite set of elementary trees whose top nodes and interior nodes are labeled by nonterminal symbols and whose yield nodes are labeled by terminal or nonterminal symbols. P is a function which assigns to every elementary tree te R a probability p(t). For a tree t with a root a, p(t) is interpreted as the probability of substituting t on a. We require, therefore, that 0 &lt; p(t) 1 and Itroot</context>
<context position="25062" citStr="Bod, 1993" startWordPosition="4215" endWordPosition="4216">vel Information System (ATIS) spoken language corpus (Hemphill et al., 1990) annotated in the Pennsylvania Treebank (Marcus et al., 1993). We employed the &amp;quot;blind testing&amp;quot; method, dividing the corpus into a 90% training set and a 10% test set by randomly selecting sentences. The 675 trees from the training set were converted into their subtrees together with their relative frequencies, yielding roughly 4* i05 different subtrees. The 75 part-of-speech sequences from the test set served as input strings that were parsed and disambiguated using the subtrees from the training set. As motivated in (Bod, 1993b), we use the notion of parse accuracy as our accuracy metric, defined as the percentage of the test strings for which the most probable parse is identical to the parse in the test set. 4.2 Accuracy as a function of subtree-depth It is one of the most essential features of DOP, that arbitrarily large subtrees are taken into consideration to estimate the probability of a parse. In order to test the usefulness of this feature, we performed different experiments constraining the depth of the subtrees. The following table shows the results of seven experiments for different maximum depths of the </context>
</contexts>
<marker>Bod, 1993</marker>
<rawString>R. Bod, 1993b. &amp;quot;Monte Carlo Parsing&amp;quot;, Proceedings Third International Workshop on Parsing Technologies, Tilburg/Durbuy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Data Oriented Parsing as a General Framework for Stochastic Language Processing&amp;quot;,</title>
<date>1993</date>
<booktitle>Parsing Natural Language, TWLT6,</booktitle>
<editor>in: K.Sikkel &amp; A. Nijholt (eds.),</editor>
<institution>Twente University.</institution>
<contexts>
<context position="4668" citStr="Bod, 1993" startWordPosition="788" endWordPosition="789">, the probability of a word string is equal to the sum of the probabilities of all its parse trees. 2 DOP as a Stochastic TreeSubstitution Grammar In order to deal with the problem of computing the most probable parse tree of a string, it is convenient to describe DOP as a &amp;quot;Stochastic Tree-Substitution Grammar&amp;quot; (STSG). STSG can be seen as a generalization over DOP, where the elementary trees of STSG are the subtrees of DOP, and the probabilities of the elementary trees are the 1Very small frequencies are smoothed by Good-Turing. substitution-probabilities of the corresponding subtrees of DOP (Bod, 1993c). A Stochastic Tree-Substitution Grammar G is a fivetuple &lt;VN, V, S. R, P&gt; where VN is a finite set of nonterminal symbols. VT is a finite set of terminal symbols. S E VN is the distinguished symbol. R is a finite set of elementary trees whose top nodes and interior nodes are labeled by nonterminal symbols and whose yield nodes are labeled by terminal or nonterminal symbols. P is a function which assigns to every elementary tree te R a probability p(t). For a tree t with a root a, p(t) is interpreted as the probability of substituting t on a. We require, therefore, that 0 &lt; p(t) 1 and Itroot</context>
<context position="25062" citStr="Bod, 1993" startWordPosition="4215" endWordPosition="4216">vel Information System (ATIS) spoken language corpus (Hemphill et al., 1990) annotated in the Pennsylvania Treebank (Marcus et al., 1993). We employed the &amp;quot;blind testing&amp;quot; method, dividing the corpus into a 90% training set and a 10% test set by randomly selecting sentences. The 675 trees from the training set were converted into their subtrees together with their relative frequencies, yielding roughly 4* i05 different subtrees. The 75 part-of-speech sequences from the test set served as input strings that were parsed and disambiguated using the subtrees from the training set. As motivated in (Bod, 1993b), we use the notion of parse accuracy as our accuracy metric, defined as the percentage of the test strings for which the most probable parse is identical to the parse in the test set. 4.2 Accuracy as a function of subtree-depth It is one of the most essential features of DOP, that arbitrarily large subtrees are taken into consideration to estimate the probability of a parse. In order to test the usefulness of this feature, we performed different experiments constraining the depth of the subtrees. The following table shows the results of seven experiments for different maximum depths of the </context>
</contexts>
<marker>Bod, 1993</marker>
<rawString>R. Bod, 1993c. &amp;quot;Data Oriented Parsing as a General Framework for Stochastic Language Processing&amp;quot;, in: K.Sikkel &amp; A. Nijholt (eds.), Parsing Natural Language, TWLT6, Twente University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Enriching Linguistics with Statistics: Performance Models of Natural Language. PhD-thesis,</title>
<date>1995</date>
<institution>University of Amsterdam (forthcoming).</institution>
<contexts>
<context position="24264" citStr="Bod, 1995" startWordPosition="4088" endWordPosition="4089">rns. 4. Monte Carlo can be made parallel in a very straightforward way: N samples can be computed by N processing units, where equal outputs are reinforced. The more processing units are employed, the better the estimation. However, since the number of processing units is finite, there is never absolute confidence. This has some similarity with the Parallel Distributed Processing paradigm for human (language) processing (Rumelhart &amp; McClelland, 1986). 4 Experiments In this section, we report on experiments with an implementation of DOP that parses and disambiguates part-of-speech strings. In (Bod, 1995) it is shown how DOP is extended to parse word strings that possibly contain unknown words. 4.1 The test environment For our experiments, we used a manually corrected version of the Air Travel Information System (ATIS) spoken language corpus (Hemphill et al., 1990) annotated in the Pennsylvania Treebank (Marcus et al., 1993). We employed the &amp;quot;blind testing&amp;quot; method, dividing the corpus into a 90% training set and a 10% test set by randomly selecting sentences. The 675 trees from the training set were converted into their subtrees together with their relative frequencies, yielding roughly 4* i05</context>
</contexts>
<marker>Bod, 1995</marker>
<rawString>R. Bod, 1995. Enriching Linguistics with Statistics: Performance Models of Natural Language. PhD-thesis, University of Amsterdam (forthcoming).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalized Probabilistic LR Parsing of Natural Language (Corpora) with Unification-Based Grammars&amp;quot;,</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>1</issue>
<pages>25--59</pages>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>T. Briscoe and J. Carroll, 1993. &amp;quot;Generalized Probabilistic LR Parsing of Natural Language (Corpora) with Unification-Based Grammars&amp;quot;, Computational Linguistics 19(1), 25-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fujisalci</author>
<author>F Jelinek</author>
<author>J Cocke</author>
<author>E Black</author>
<author>T Nishino</author>
</authors>
<title>A Probabilistic Method for Sentence Disambiguation&amp;quot;,</title>
<date>1989</date>
<booktitle>Proceedings 1st Int. Workshop on Parsing Technologies,</booktitle>
<location>Pittsburgh.</location>
<marker>Fujisalci, Jelinek, Cocke, Black, Nishino, 1989</marker>
<rawString>T. Fujisalci, F. Jelinek, J. Cocke, E. Black and T. Nishino, 1989. &amp;quot;A Probabilistic Method for Sentence Disambiguation&amp;quot;, Proceedings 1st Int. Workshop on Parsing Technologies, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Hammersley</author>
<author>D C Handscomb</author>
</authors>
<title>Monte Carlo Methods,</title>
<date>1964</date>
<publisher>Chapman and Hall,</publisher>
<location>London.</location>
<contexts>
<context position="18645" citStr="Hammersley and Handscomb, 1964" startWordPosition="3179" endWordPosition="3182"> the forest store the parse generated by the random derivation MPP := the most frequently occurring parse There is an important question as to how long the convergence of the most probable parse may take. Is there a tractable upper bound on the number of derivations that have to be sampled from the forest before stability in the top of the parse distribution occurs? The answer is yes: the worst case time complexity of achieving a maximum estimation error E by means of random sampling is 0(E-2), independently of the probability distribution. This is a classical result from sampling theory (cf. Hammersley and Handscomb, 1964), and follows directly from Chebyshev&apos;s inequality. In practice, it means that the 4 Let ( (el, pi) , (e2, p2) , , (en, pn) ) be a probability distribution of events e1, e2, ..., en; an event e i is said to be randomly selected iff its probability of being selected is equal to pi. In order to allow for &amp;quot;direct sampling&amp;quot;, one must convert the probability distribution into a corresponding sample space for which holds that the frequency of occurrence fi of each event e i is a positive integer equal to Npi, where N is the size of the sample space. error e is inversely proportional to the square-ro</context>
<context position="14966" citStr="Hammersley &amp; Handscomb, 1964" startWordPosition="2585" endWordPosition="2588">teratively generating a large number of random derivations we can estimate the most probable parse as the parse which results most often from these random derivations (since the probability of a parse is the probability that any of its derivations occurs). The most probable parse can be estimated as accurately as desired by making the number of random samples as large as desired. According to the Law of Large Numbers, the most often generated parse converges to the most probable parse. Methods that estimate the probability of an event by taking random samples are known as Monte Carlo methods (Hammersley &amp; Handscomb, 1964).3 The selection of a random derivation is accomplished in a bottom-up fashion analogous to Viterbi. Instead of selecting the most probable subderivation at each node-sharing in the chart, a random subderivation is selected (i.e. sampled) at each node-sharing (that is, a subderivation that has n times as large a probability as another subderivation should also have n times as large a chance to be chosen as this other subderivation). Once sampled at the S-node, the random derivation of the whole sentence can be retrieved by tracing back the choices made at each node-sharing. Of course, we may p</context>
</contexts>
<marker>Hammersley, Handscomb, 1964</marker>
<rawString>J.M. Hammersley and D.C. Handscomb, 1964. Monte Carlo Methods, Chapman and Hall, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Hemphill</author>
<author>J J Godfrey</author>
<author>G R Doddington</author>
</authors>
<title>The ATIS spoken language systems pilot corpus&amp;quot;.</title>
<date>1990</date>
<booktitle>Proceedings DARPA Speech and Natural Language Workshop,</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Hidden Valley,</location>
<contexts>
<context position="24529" citStr="Hemphill et al., 1990" startWordPosition="4129" endWordPosition="4132">processing units is finite, there is never absolute confidence. This has some similarity with the Parallel Distributed Processing paradigm for human (language) processing (Rumelhart &amp; McClelland, 1986). 4 Experiments In this section, we report on experiments with an implementation of DOP that parses and disambiguates part-of-speech strings. In (Bod, 1995) it is shown how DOP is extended to parse word strings that possibly contain unknown words. 4.1 The test environment For our experiments, we used a manually corrected version of the Air Travel Information System (ATIS) spoken language corpus (Hemphill et al., 1990) annotated in the Pennsylvania Treebank (Marcus et al., 1993). We employed the &amp;quot;blind testing&amp;quot; method, dividing the corpus into a 90% training set and a 10% test set by randomly selecting sentences. The 675 trees from the training set were converted into their subtrees together with their relative frequencies, yielding roughly 4* i05 different subtrees. The 75 part-of-speech sequences from the test set served as input strings that were parsed and disambiguated using the subtrees from the training set. As motivated in (Bod, 1993b), we use the notion of parse accuracy as our accuracy metric, def</context>
</contexts>
<marker>Hemphill, Godfrey, Doddington, 1990</marker>
<rawString>C.T. Hemphill, J.J. Godfrey and G.R. Doddington, 1990. &amp;quot;The ATIS spoken language systems pilot corpus&amp;quot;. Proceedings DARPA Speech and Natural Language Workshop, Hidden Valley, Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
</authors>
<title>Basic Methods of Probabilistic Context Free Grammars,</title>
<date>1990</date>
<tech>Technical Report IBM RC 16374 (#72684),</tech>
<location>Yorktown Heights.</location>
<contexts>
<context position="10634" citStr="Jelinek et al., 1990" startWordPosition="1840" endWordPosition="1843">ivations. Therefore, in order to find the most probable parse of a sentence, it is not efficient to compare the probabilities of the parses by exhaustively unpacking the chart. Even for determining the probability of one parse, it is not efficient to add the probabilities of all derivations of that parse. 3.2.1 Viterbi optimization is not feasible for finding the most probable parse There exists a heuristic optimization algorithm, known as Viterbi optimization, which selects on the basis of an SCFG the most probable derivation of a sentence in cubic time (Viterbi, 1967; Fujisaki et al., 1989; Jelinek et al., 1990). In STSG, however, the most probable derivation does not necessarily generate the most probable parse, as the probability of a parse is defined as the sum of the probabilities of all its derivations. Thus, there is an important question as to whether we can adapt the Viterbi algorithm for finding the most probable parse. To understand the difficulty of the problem, we look in more detail at the Viterbi algorithm. The basic idea of the Viterbi algorithm is the early pruning of low probability subderivations in a bottom-up fashion. Two different subderivations of the same part of the sentence a</context>
</contexts>
<marker>Jelinek, Lafferty, Mercer, 1990</marker>
<rawString>F. Jelinek, J.D. Lafferty and R.L. Mercer, 1990. Basic Methods of Probabilistic Context Free Grammars, Technical Report IBM RC 16374 (#72684), Yorktown Heights.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Algorithmic Schemata and Data Structures in Syntactic Processing. Report CSL-80-12, Xerox PARC,</title>
<date>1980</date>
<location>Palo Alto, Ca.</location>
<contexts>
<context position="8932" citStr="Kay, 1980" startWordPosition="1572" endWordPosition="1573">h the syntactic categories of that substring but with the full elementary trees t that correspond to the use of the derived rules root(t) --&gt;yield(t). Note that in a chartlike forest generated by an STSG, different derivations that generate a same tree do not collapse. We will therefore talk about a derivation forest generated by an STSG (cf. Sima&apos;an et al., 1994). The following formal example illustrates what a derivation forest of a string may look like. In the example, we leave out the probabilities, which are needed only in the disambiguation process. The visual representation comes from (Kay, 1980): every entry (ij) in the chart is indicated by an edge and spans the words between the i-th and the j-th position of a sentence. Every edge is labeled with the elementary trees that denote the underlying phrase. The exampleSTSG consists of the following elementary trees: 2 Although theoretically there can be more than one most probable parse for a sentence, in practice a system that employs a non-trivial treebank tends to generate exactly one most probable parse for a given input sentence. Note that different derivations in the forest generate the same tree. By exhaustively unpacking the fore</context>
</contexts>
<marker>Kay, 1980</marker>
<rawString>M. Kay, 1980. Algorithmic Schemata and Data Structures in Syntactic Processing. Report CSL-80-12, Xerox PARC, Palo Alto, Ca.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Magerman</author>
<author>C Weir</author>
</authors>
<title>Efficiency, Robustness and Accuracy in Picky Chart Parsing&amp;quot;,</title>
<date>1992</date>
<booktitle>Proceedings ACL&apos;92,</booktitle>
<location>Newark, Delaware.</location>
<contexts>
<context position="21349" citStr="Magerman &amp; Weir, 1992" startWordPosition="3641" endWordPosition="3644">mial time what is the most probable parse; it can only make the error-probability of the estimated most probable parse arbitrarily small. As such, the Monte Carlo algorithm is a probabilistic algorithm belonging to the class of Bounded error Probabilistic Polynomial time (BPP) algorithms. We hypothesize that Monte Carlo disambiguation is also relevant for other stochastic grammars. It turns out that all stochastic extensions of CFGs that are stochastically richer than SCFG need exponential time algorithms for finding a most probable parse tree (cf. Briscoe &amp; Carroll, 1992; Black et al., 1993; Magerman &amp; Weir, 1992; Schabes &amp; Waters, 1993). To our knowledge, it has never been studied whether there exist BPP-algorithms for these models. Alhough it is beyond the scope of our research, we conjecture that there exists a Monte Carlo disambiguation algorithm for at least Stochastic Tree-Adjoining Grammar (Schabes, 1992). 3.2.3 Psychological relevance of Monte Carlo disambiguation As has been noted, an important difference between the Viterbi algorithm and the Monte Carlo algorithm is, that with the latter we never have 100% confidence. In our opinion, this should not be seen as a disadvantage. In fact, absolu</context>
</contexts>
<marker>Magerman, Weir, 1992</marker>
<rawString>D. Magerman and C. Weir, 1992. &amp;quot;Efficiency, Robustness and Accuracy in Picky Chart Parsing&amp;quot;, Proceedings ACL&apos;92, Newark, Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: the Penn Treebank&amp;quot;,</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="24590" citStr="Marcus et al., 1993" startWordPosition="4138" endWordPosition="4141"> This has some similarity with the Parallel Distributed Processing paradigm for human (language) processing (Rumelhart &amp; McClelland, 1986). 4 Experiments In this section, we report on experiments with an implementation of DOP that parses and disambiguates part-of-speech strings. In (Bod, 1995) it is shown how DOP is extended to parse word strings that possibly contain unknown words. 4.1 The test environment For our experiments, we used a manually corrected version of the Air Travel Information System (ATIS) spoken language corpus (Hemphill et al., 1990) annotated in the Pennsylvania Treebank (Marcus et al., 1993). We employed the &amp;quot;blind testing&amp;quot; method, dividing the corpus into a 90% training set and a 10% test set by randomly selecting sentences. The 675 trees from the training set were converted into their subtrees together with their relative frequencies, yielding roughly 4* i05 different subtrees. The 75 part-of-speech sequences from the test set served as input strings that were parsed and disambiguated using the subtrees from the training set. As motivated in (Bod, 1993b), we use the notion of parse accuracy as our accuracy metric, defined as the percentage of the test strings for which the most</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini and M. Marcinkiewicz, 1993. &amp;quot;Building a Large Annotated Corpus of English: the Penn Treebank&amp;quot;, Computational Linguistics 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Rumelhart</author>
<author>J McClelland</author>
</authors>
<title>Parallel Distributed Processing,</title>
<date>1986</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="24108" citStr="Rumelhart &amp; McClelland, 1986" startWordPosition="4063" endWordPosition="4066">these analyses are equally plausible, people perceive so-called fluctuation effects. This fluctuation phenomenon is also well-known in the perception of ambiguous visual patterns. 4. Monte Carlo can be made parallel in a very straightforward way: N samples can be computed by N processing units, where equal outputs are reinforced. The more processing units are employed, the better the estimation. However, since the number of processing units is finite, there is never absolute confidence. This has some similarity with the Parallel Distributed Processing paradigm for human (language) processing (Rumelhart &amp; McClelland, 1986). 4 Experiments In this section, we report on experiments with an implementation of DOP that parses and disambiguates part-of-speech strings. In (Bod, 1995) it is shown how DOP is extended to parse word strings that possibly contain unknown words. 4.1 The test environment For our experiments, we used a manually corrected version of the Air Travel Information System (ATIS) spoken language corpus (Hemphill et al., 1990) annotated in the Pennsylvania Treebank (Marcus et al., 1993). We employed the &amp;quot;blind testing&amp;quot; method, dividing the corpus into a 90% training set and a 10% test set by randomly s</context>
</contexts>
<marker>Rumelhart, McClelland, 1986</marker>
<rawString>D. Rumelhart and J. McClelland, 1986. Parallel Distributed Processing, The MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Scha</author>
</authors>
<title>Language Theory and Language Technology; Competence and Performance&amp;quot;</title>
<date>1990</date>
<booktitle>Computertoepassingen in de Neerlandistiek, Almere: Landelijke Vereniging</booktitle>
<editor>(in Dutch), in Q.A.M. de Kort &amp; G.L.J. Leerdam (eds.),</editor>
<contexts>
<context position="1626" citStr="Scha, 1990" startWordPosition="252" endWordPosition="253"> of a &amp;quot;select-random&amp;quot; search to estimate the most probable tree of a string in polynomial time. Experiments with DOP on ATIS show that only in 68% of the cases, the most probable derivation of a string generates the most probable tree of that string. Therefore, the parse accuracy obtained by the most probable trees (96%) is dramatically higher than the parse accuracy obtained by the most probable derivations (65%). It is still an open question whether the most probable tree of a string can be deterministically computed in polynomial time. 1 Data-Oriented Parsing A Data-Oriented Parsing model (Scha, 1990; Bod, 1992, 1993a) is characterized by a corpus of analyzed language utterances, together with a set of operations that combine sub-analyses from the corpus into new analyses. We will limit ourselves in this paper to corpora with purely syntactic annotations. For the semantic dimension of DOP, the reader is referred to (van den Berg et al., 1994). Consider the imaginary example corpus consisting of only two trees in figure 1. We will assume one operation for combining subtrees. This operation is called &amp;quot;composition&amp;quot;, and is indicated by the infix operator The composition of t and u, tou, yiel</context>
</contexts>
<marker>Scha, 1990</marker>
<rawString>R. Scha, 1990. &amp;quot;Language Theory and Language Technology; Competence and Performance&amp;quot; (in Dutch), in Q.A.M. de Kort &amp; G.L.J. Leerdam (eds.), Computertoepassingen in de Neerlandistiek, Almere: Landelijke Vereniging van Neerlandici (LVVNjaarboek).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
</authors>
<title>Stochastic Lexicalized TreeAdjoining Grammars&amp;quot;,</title>
<date>1992</date>
<booktitle>Proceedings COLING&apos;92,</booktitle>
<location>Nantes.</location>
<contexts>
<context position="21654" citStr="Schabes, 1992" startWordPosition="3689" endWordPosition="3690">nte Carlo disambiguation is also relevant for other stochastic grammars. It turns out that all stochastic extensions of CFGs that are stochastically richer than SCFG need exponential time algorithms for finding a most probable parse tree (cf. Briscoe &amp; Carroll, 1992; Black et al., 1993; Magerman &amp; Weir, 1992; Schabes &amp; Waters, 1993). To our knowledge, it has never been studied whether there exist BPP-algorithms for these models. Alhough it is beyond the scope of our research, we conjecture that there exists a Monte Carlo disambiguation algorithm for at least Stochastic Tree-Adjoining Grammar (Schabes, 1992). 3.2.3 Psychological relevance of Monte Carlo disambiguation As has been noted, an important difference between the Viterbi algorithm and the Monte Carlo algorithm is, that with the latter we never have 100% confidence. In our opinion, this should not be seen as a disadvantage. In fact, absolute confidence about the most probable parse does not have any significance, as the probability assigned to a parse is already an estimation of its actual probability. One may ask as to whether Monte Carlo is appropriate for modeling 108 human sentence perception. The following lists some properties of Mo</context>
</contexts>
<marker>Schabes, 1992</marker>
<rawString>Y. Schabes, 1992. &amp;quot;Stochastic Lexicalized TreeAdjoining Grammars&amp;quot;, Proceedings COLING&apos;92, Nantes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
<author>R Waters</author>
</authors>
<title>Stochastic Lexicalized Context Free Grammars&amp;quot;,</title>
<date>1993</date>
<booktitle>Proceedings Third International Workshop on Parsing Technologies,</booktitle>
<location>Tilburg/Durbuy.</location>
<contexts>
<context position="21374" citStr="Schabes &amp; Waters, 1993" startWordPosition="3645" endWordPosition="3648">ost probable parse; it can only make the error-probability of the estimated most probable parse arbitrarily small. As such, the Monte Carlo algorithm is a probabilistic algorithm belonging to the class of Bounded error Probabilistic Polynomial time (BPP) algorithms. We hypothesize that Monte Carlo disambiguation is also relevant for other stochastic grammars. It turns out that all stochastic extensions of CFGs that are stochastically richer than SCFG need exponential time algorithms for finding a most probable parse tree (cf. Briscoe &amp; Carroll, 1992; Black et al., 1993; Magerman &amp; Weir, 1992; Schabes &amp; Waters, 1993). To our knowledge, it has never been studied whether there exist BPP-algorithms for these models. Alhough it is beyond the scope of our research, we conjecture that there exists a Monte Carlo disambiguation algorithm for at least Stochastic Tree-Adjoining Grammar (Schabes, 1992). 3.2.3 Psychological relevance of Monte Carlo disambiguation As has been noted, an important difference between the Viterbi algorithm and the Monte Carlo algorithm is, that with the latter we never have 100% confidence. In our opinion, this should not be seen as a disadvantage. In fact, absolute confidence about the m</context>
</contexts>
<marker>Schabes, Waters, 1993</marker>
<rawString>Y. Schabes and R. Waters, 1993. &amp;quot;Stochastic Lexicalized Context Free Grammars&amp;quot;, Proceedings Third International Workshop on Parsing Technologies, Tilburg/Durbuy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima&apos;an</author>
<author>R Bod</author>
<author>S Krauwer</author>
<author>R Scha</author>
</authors>
<title>Efficient Disambiguation by means of Stochastic Tree Substitution Grammars&amp;quot;,</title>
<date>1994</date>
<booktitle>Proceedings International Conference on New Methods in Language Processing, UMIST,</booktitle>
<location>Manchester.</location>
<contexts>
<context position="8688" citStr="Sima&apos;an et al., 1994" startWordPosition="1532" endWordPosition="1535"> rule root(t) --&gt; yield(t). Given a chart parsing algorithm, an input sentence of length n can be parsed in n3 time. In order to obtain a chart-like forest for a sentence parsed in STSG, we need to label the wellformed substrings in the chart not only with the syntactic categories of that substring but with the full elementary trees t that correspond to the use of the derived rules root(t) --&gt;yield(t). Note that in a chartlike forest generated by an STSG, different derivations that generate a same tree do not collapse. We will therefore talk about a derivation forest generated by an STSG (cf. Sima&apos;an et al., 1994). The following formal example illustrates what a derivation forest of a string may look like. In the example, we leave out the probabilities, which are needed only in the disambiguation process. The visual representation comes from (Kay, 1980): every entry (ij) in the chart is indicated by an edge and spans the words between the i-th and the j-th position of a sentence. Every edge is labeled with the elementary trees that denote the underlying phrase. The exampleSTSG consists of the following elementary trees: 2 Although theoretically there can be more than one most probable parse for a sente</context>
<context position="13150" citStr="Sima&apos;an et al., 1994" startWordPosition="2288" endWordPosition="2291"> sum of the probabilities of all its derivations, it is still possible that d2 contributes to the generation of the most probable parse. Therefore we are not allowed to eliminate d2. This counter-example does not prove that there is no heuristic optimization that allows polynomial time selection of the most probable parse. But it makes clear that a &amp;quot;select-best&amp;quot; search, as accomplished by Viterbi, is not adequate for finding the most probable parse in STSG. So far, it is unknown whether the problem of finding the most probable parse in a deterministic way is inherently exponential or not (cf. Sima&apos;an et al., 1994). One should of course ask how often in practice the most probable derivation produces the most probable parse, but this can only be answered by means of experiments on real life corpora. Experiments on the ATIS corpus (see session 4) show that only in 68% of the cases the most probable derivation of a sentence generates the most probable parse of that sentence. Moreover, the parse accuracy obtained by the most probable parse is dramatically higher than the parse accuracy obtained by the parse generated by the most probable derivation. 3.2.2 Estimating the most probable parse by Monte Carlo se</context>
</contexts>
<marker>Sima&apos;an, Bod, Krauwer, Scha, 1994</marker>
<rawString>K. Sima&apos;an, R. Bod, S. Krauwer and R. Scha, 1994. &amp;quot;Efficient Disambiguation by means of Stochastic Tree Substitution Grammars&amp;quot;, Proceedings International Conference on New Methods in Language Processing, UMIST, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimum decoding algorithm&amp;quot;,</title>
<date>1967</date>
<journal>IEEE Trans. Information Theory,</journal>
<volume>13</volume>
<pages>260--269</pages>
<contexts>
<context position="10588" citStr="Viterbi, 1967" startWordPosition="1834" endWordPosition="1835"> parse may have exponentially many derivations. Therefore, in order to find the most probable parse of a sentence, it is not efficient to compare the probabilities of the parses by exhaustively unpacking the chart. Even for determining the probability of one parse, it is not efficient to add the probabilities of all derivations of that parse. 3.2.1 Viterbi optimization is not feasible for finding the most probable parse There exists a heuristic optimization algorithm, known as Viterbi optimization, which selects on the basis of an SCFG the most probable derivation of a sentence in cubic time (Viterbi, 1967; Fujisaki et al., 1989; Jelinek et al., 1990). In STSG, however, the most probable derivation does not necessarily generate the most probable parse, as the probability of a parse is defined as the sum of the probabilities of all its derivations. Thus, there is an important question as to whether we can adapt the Viterbi algorithm for finding the most probable parse. To understand the difficulty of the problem, we look in more detail at the Viterbi algorithm. The basic idea of the Viterbi algorithm is the early pruning of low probability subderivations in a bottom-up fashion. Two different sub</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>A. Viterbi, 1967. &amp;quot;Error bounds for convolutional codes and an asymptotically optimum decoding algorithm&amp;quot;, IEEE Trans. Information Theory, IT-13, 260-269.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>