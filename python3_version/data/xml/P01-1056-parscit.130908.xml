<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004504">
<title confidence="0.987135">
Evaluating a Trainable Sentence Planner for a Spoken Dialogue System
</title>
<author confidence="0.956657">
Owen Rambow Monica Rogati Marilyn A. Walker
</author>
<affiliation confidence="0.7292655">
AT&amp;T Labs – Research Carnegie Mellon University AT&amp;T Labs – Research
Florham Park, NJ, USA Pittsburgh, PA, USA Florham Park, NJ, USA
</affiliation>
<email confidence="0.996826">
rambow@research.att.com mrogati+@cs.cmu.edu walker@research.att.com
</email>
<sectionHeader confidence="0.993854" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999805904761905">
Techniques for automatically training
modules of a natural language gener-
ator have recently been proposed, but
a fundamental concern is whether the
quality of utterances produced with
trainable components can compete with
hand-crafted template-based or rule-
based approaches. In this paper We ex-
perimentally evaluate a trainable sen-
tence planner for a spoken dialogue sys-
tem by eliciting subjective human judg-
ments. In order to perform an ex-
haustive comparison, we also evaluate
a hand-crafted template-based genera-
tion component, two rule-based sen-
tence planners, and two baseline sen-
tence planners. We show that the train-
able sentence planner performs better
than the rule-based systems and the
baselines, and as well as the hand-
crafted system.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99989723">
The past several years have seen a large increase
in commercial dialog systems. These systems
typically use system-initiative dialog strategies,
with system utterances highly scripted for style
and register and recorded by voice talent. How-
ever several factors argue against the continued
use of these simple techniques for producing the
system side of the conversation. First, text-to-
speech has improved to the point of being a vi-
able alternative to pre-recorded prompts. Second,
there is a perceived need for spoken dialog sys-
tems to be more flexible and support user initia-
tive, but this requires greater flexibility in utter-
ance generation. Finally, systems to support com-
plex planning are being developed, which will re-
quire more sophisticated output.
As we move away from systems with pre-
recorded prompts, there are two possible ap-
proaches to producing system utterances. The
first is template-based generation, where ut-
terances are produced from hand-crafted string
templates. Most current research systems use
template-based generation because it is concep-
tually straightforward. However, while little or
no linguistic training is needed to write templates,
it is a tedious and time-consuming task: one or
more templates must be written for each combi-
nation of goals and discourse contexts, and lin-
guistic issues such as subject-verb agreement and
determiner-noun agreement must be repeatedly
encoded for each template. Furthermore, main-
tenance of the collection of templates becomes a
software engineering problem as the complexity
of the dialog system increases.1
The second approach is natural language gen-
eration (NLG), which customarily divides the
generation process into three modules (Rambow
and Korelsky, 1992): (1) Text Planning, (2) Sen-
tence Planning, and (3) Surface Realization. In
this paper, we discuss only sentence planning; the
role of the sentence planner is to choose abstract
lexico-structural resources for a text plan, where
a text plan encodes the communicative goals for
an utterance (and, sometimes, their rhetorical
structure). In general, NLG promises portability
across application domains and dialog situations
by focusing on the development of rules for each
generation module that are general and domain-
&apos;Although we are not aware of any software engineering
studies of template development and maintenance, this claim
is supported by abundant anecdotal evidence.
independent. However, the quality of the output
for a particular domain, or a particular situation
in a dialog, may be inferior to that of a template-
based system without considerable investment in
domain-specific rules or domain-tuning of gen-
eral rules. Furthermore, since rule-based systems
use sophisticated linguistic representations, this
handcrafting requires linguistic knowledge.
Recently, several approaches for automatically
training modules of an NLG system have been
proposed (Langkilde and Knight, 1998; Mel-
lish et al., 1998; Walker, 2000). These hold
the promise that the complex step of customiz-
ing NLG systems by hand can be automated,
while avoiding the need for tedious hand-crafting
of templates. While the engineering benefits of
trainable approaches appear obvious, it is unclear
whether the utterance quality is high enough.
In (Walker et al., 2001) we propose a new
model of sentence planning called SPOT. In
SPOT, the sentence planner is automatically
trained, using feedback from two human judges,
to choose the best from among different options
for realizing a set of communicative goals. In
(Walker et al., 2001), we evaluate the perfor-
mance of the learning component of SPOT, and
show that SPOT learns to select sentence plans
that are highly rated by the two human judges.
While this evaluation shows that SPOT has in-
deed learned from the human judges, it does not
show that using only two human judgments is
sufficient to produce more broadly acceptable re-
sults, nor does it show that SPOT performs as well
as optimized hand-crafted template or rule-based
systems. In this paper we address these questions.
Because SPOT is trained on data from a work-
ing system, we can directly compare SPOT to the
hand-crafted, template-based generation compo-
nent of the current system. In order to perform an
exhaustive comparison, we also implemented two
rule-based and two baseline sentence-planners.
One baseline simply produces a single sentence
for each communicative goal. Another baseline
randomly makes decisions about how to combine
communicative goals into sentences. We directly
compare these different approaches in an evalua-
tion experiment in which 60 human subjects rate
each system’s output on a scale of 1 to 5.
The experimental design is described in section
</bodyText>
<note confidence="0.5818">
System1: Welcome.... What airport would you like to fly
</note>
<tableCaption confidence="0.68917025">
out of?
User2: I need to go to Dallas.
System3: Flying to Dallas. What departure airport was
that?
</tableCaption>
<note confidence="0.702846">
User4: from Newark on September the 1st.
System5: Flying from Newark to Dallas, Leaving on the
1st of September, And what time did you want
to leave?
</note>
<figureCaption confidence="0.999763">
Figure 1: A dialog with AMELIA
</figureCaption>
<bodyText confidence="0.9979015">
2. The sentence planners used in the evaluation
are described in section 3. In section 4, we present
our results. We show that the trainable sentence
planner performs better than both rule-based sys-
tems and as well as the hand-crafted template-
based system. These four systems outperform the
baseline sentence planners. Section 5 summarizes
our results and discusses related and future work.
</bodyText>
<sectionHeader confidence="0.969904" genericHeader="method">
2 Experimental Context and Design
</sectionHeader>
<bodyText confidence="0.99991959375">
Our research concerns developing and evaluat-
ing a portable generation component for a mixed-
initiative travel planning system, AMELIA, de-
veloped at AT&amp;T Labs as part of DARPA Com-
municator. Consider the required generation ca-
pabilities of AMELIA, as illustrated in Figure 1.
Utterance System1 requests information about
the caller’s departure airport, but in User2, the
caller takes the initiative to provide information
about her destination. In System3, the system’s
goal is to implicitly confirm the destination (be-
cause of the possibility of error in the speech
recognition component), and request information
(for the second time) of the caller’s departure air-
port. This combination of communicative goals
arises dynamically in the dialog because the sys-
tem supports user initiative, and requires differ-
ent capabilities for generation than if the system
could only understand the direct answer to the
question that it asked in System1.
In User4, the caller provides this information
but takes the initiative to provide the month and
day of travel. Given the system’s dialog strategy,
the communicative goals for its next turn are to
implicitly confirm all the information that the user
has provided so far, i.e. the departure and desti-
nation cities and the month and day information,
as well as to request information about the time
of travel. The system’s representation of its com-
municative goals for System5 is in Figure 2. As
before, this combination of communicative goals
arises in response to the user’s initiative.
</bodyText>
<equation confidence="0.7246446">
implicit-confirm(orig-city:NEWARK)
implicit-confirm(dest-city:DALLAS)
implicit-confirm(month:9)
implicit-confirm(day-number:1)
request(depart-time:whatever)
</equation>
<figureCaption confidence="0.894161">
Figure 2: The text plan (communicative goals) for
System5 in Figure 1
</figureCaption>
<bodyText confidence="0.8842305">
Like most working research spoken dialog
systems, AMELIA uses hand-crafted, template-
based generation. Its output is created by choos-
ing string templates for each elementary speech
act, using a large choice function which depends
on the type of speech act and various context con-
ditions. Values of template variables (such as ori-
gin and destination cities) are instantiated by the
dialog manager. The string templates for all the
speech acts of a turn are heuristically ordered and
then appended to produce the output. In order to
produce output that is not highly redundant, string
templates must be written for every possible com-
bination of speech acts in a text plan. We refer to
the output generated by AMELIA using this ap-
proach as the TEMPLATE output.
System Realization
TEMPLATE Flying from Newark to Dallas, Leaving on
the 1st of September, And what time did
you want to leave?
SPoT What time would you like to travel on
September the 1st to Dallas from Newark?
</bodyText>
<table confidence="0.8617187">
RBS (Rule- What time would you like to travel on
Based) September the 1st to Dallas from Newark?
ICF (Rule- What time would you like to fly on
Based) September the 1st to Dallas from Newark?
RANDOM Leaving in September. Leaving on the
1st. What time would you, traveling from
Newark to Dallas, like to leave?
NOAGG Leaving on the 1. Leaving in September.
Going to Dallas. Leaving from Newark.
What time would you like to leave?
</table>
<figureCaption confidence="0.923116666666667">
Figure 3: Sample outputs for System5 of Figure 1
for each type of generation system used in the
evaluation experiment.
</figureCaption>
<bodyText confidence="0.999981739130435">
We perform an evaluation using human sub-
jects who judged the TEMPLATE output of
AMELIA against five NLG-based approaches:
SPOT, two rule-based approaches, and two base-
lines. We describe them in Section 3. An exam-
ple output for the text plan in Figure 2 for each
system is in Figure 3. The experiment required
human subjects to read 5 dialogs of real inter-
actions with AMELIA. At 20 points over the 5
dialogs, AMELIA’s actual utterance (TEMPLATE)
is augmented with a set of variants; each set of
variants included a representative generated by
SPOT, and representatives of the four compari-
son sentence planners. At times two or more of
these variants coincided, in which case sentences
were not repeated and fewer than six sentences
were presented to the subjects. The subjects rated
each variation on a 5-point Likert scale, by stating
the degree to which they agreed with the state-
ment The system’s utterance is easy to under-
stand, well-formed, and appropriate to the dialog
context. Sixty colleagues not involved in this re-
search completed the experiment.
</bodyText>
<sectionHeader confidence="0.948467" genericHeader="method">
3 Sentence Planning Systems
</sectionHeader>
<bodyText confidence="0.9999475">
This section describes the five sentence planners
that we compare. SPOT, the two rule-based
systems, and the two baseline sentence planners
are all NLG based sentence planners. In Sec-
tion 3.1, we describe the shared representations
of the NLG based sentence planners. Section 3.2
describes the baselines, RANDOM and NOAGG.
Section 3.3 describes SPOT. Section 3.4 de-
scribes the rule-based sentence planners, RBS
and ICF.
</bodyText>
<subsectionHeader confidence="0.999833">
3.1 Aggregation in Sentence Planning
</subsectionHeader>
<bodyText confidence="0.998821625">
In all of the NLG sentence planners, each speech
act is assigned a canonical lexico-structural rep-
resentation (called a DSyntS – Deep Syntactic
Structure (Melˇcuk, 1988)). We exclude issues of
lexical choice from this study, and restrict our at-
tention to the question of how elementary struc-
tures for separate elementary speech acts are as-
sembled into extended discourse. The basis of all
the NLG systems is a set of clause-combining op-
erations that incrementally transform a list of el-
ementary predicate-argument representations (the
DSyntSs corresponding to the elementary speech
acts of a single text plan) into a list of lexico-
structural representations of one or more sen-
tences, that are sent to a surface realizer. We uti-
lize the RealPro Surface realizer with all of the
</bodyText>
<table confidence="0.999388076923077">
Rule Arg 1 Arg 2 Result
MERGE You are leaving from Newark. You are leaving at 5 You are leaving at 5 from Newark
SOFT-MERGE You are leaving from Newark You are going to Dallas You are traveling from Newark to
Dallas
CONJUNCTION You are leaving from Newark. You are going to Dallas. You are leaving from Newark and
you are going to Dallas.
RELATIVE- Your flight leaves at 5. Your flight arrives at 9. Your flight, which leaves at 5, ar-
CLAUSE rives at 9.
ADJECTIVE Your flight leaves at 5. Your flight is nonstop. Your nonstop flight leaves at 5.
PERIOD You are leaving from Newark. You are going to Dallas. You are leaving from Newark.
You are going to Dallas
RANDOM CUE- What time would yo like to n/a Now, what time would you like to
WORD leave? leave?
</table>
<figureCaption confidence="0.997722">
Figure 4: List of clause combining operations with examples
</figureCaption>
<bodyText confidence="0.994406722222222">
sentence planners (Lavoie and Rambow, 1997).
DSyntSs are combined using the operations ex-
emplified in Figure 4. The result of applying the
operations is a sentence plan tree (or sp-tree
for short), which is a binary tree with leaves la-
beled by all the elementary speech acts from the
input text plan, and with its interior nodes la-
beled with clause-combining operations. As an
example, Figure 5 shows the sp-tree for utterance
System5 in Figure 1. Node soft-merge-general
merges an implicit-confirmation of the destina-
tion city and the origin city. The row labelled
SOFT-MERGE in Figure 4 shows the result when
Args 1 and 2 are implicit confirmations of the ori-
gin and destination. See (Walker et al., 2001) for
more detail on the sp-tree. The experimental sen-
tence planners described below vary how the sp-
tree is constructed.
</bodyText>
<figure confidence="0.443906">
imp−confirm(dest−city) imp−confirm(orig−city)
</figure>
<figureCaption confidence="0.9864805">
Figure 5: A Sentence Plan Tree for Utterance Sys-
tem 5 in Dialog D1
</figureCaption>
<subsectionHeader confidence="0.997764">
3.2 Baseline Sentence Planners
</subsectionHeader>
<bodyText confidence="0.999772034482759">
In one obvious baseline system the sp-tree is con-
structed by applying only the PERIOD operation:
each elementary speech act is realized as its own
sentence. This baseline, NOAGG, was suggested
by Hovy and Wanner (1996). For NOAGG, we
order the communicative acts from the text plan
as follows: implicit confirms precede explicit
confirms precede requests. Figure 3 includes a
NOAGG output for the text plan in Figure 2.
A second possible baseline sentence planner
simply applies combination rules randomly ac-
cording to a hand-crafted probability distribution
based on preferences for operations such as the
MERGE family over CONJUNCTION and PERIOD.
In order to be able to generate the resulting sen-
tence plan tree, we exclude certain combinations,
such as generating anything other than a PERIOD
above a node labeled PERIOD in a sentence plan.
The resulting sentence planner we refer to as
RANDOM. Figure 3 includes a RANDOM output
for the text plan in Figure 2.
In order to construct a more complex, and
hopefully better, sentence planner, we need to en-
code constraints on the application of, and order-
ing of, the operations. It is here that the remaining
approaches differ. In the first approach, SPOT,
we learn constraints from training material; in the
second approach, rule-based, we construct con-
straints by hand.
</bodyText>
<subsectionHeader confidence="0.980096">
3.3 SPoT: A Trainable Sentence Planner
</subsectionHeader>
<bodyText confidence="0.99971825">
For the sentence planner SPOT, we reconceptu-
alize sentence planning as consisting of two dis-
tinct phases as in Figure 6. In the first phase, the
sentence-plan-generator (SPG) randomly gener-
ates up to twenty possible sentence plans for a
given text-plan input. For this phase we use the
RANDOM sentence-planner. In the second phase,
the sentence-plan-ranker (SPR) ranks the sample
</bodyText>
<figure confidence="0.992634777777778">
imp−confirm(month) request(time) soft−merge−general
3
1
soft−merge−general
imp−confirm(day)
2
soft−merge
soft−merge−general
Text Plan Chosen sp−tree with associated DSyntS
</figure>
<figureCaption confidence="0.999991">
Figure 6: Architecture of SPoT
</figureCaption>
<bodyText confidence="0.999679588235294">
sentence plans, and then selects the top-ranked
output to input to the surface realizer. The SPR
is automatically trained by applying RankBoost
(Freund et al., 1998) to learn ranking rules from
training data. The training data was assembled
by using RANDOM to randomly generate up to
20 realizations for 100 turns; two human judges
then ranked each of these realizations (using the
setup described in Section 2). Over 3,000 fea-
tures were discovered from the generated trees
by routines that encode structural and lexical as-
pects of the sp-trees and the DSyntS. RankBoost
identified the features that contribute most to a
realization’s ranking. The SPR uses these rules
to rank alternative sp-trees, and then selects the
top-ranked output as input to the surface realizer.
Walker et al. (2001) describe SPOT in detail.
</bodyText>
<subsectionHeader confidence="0.994894">
3.4 Two Rule-Based Sentence Planners
</subsectionHeader>
<bodyText confidence="0.999946375">
It has not been the object of our research to con-
struct a rule-based sentence planner by hand, be
it domain-independent or optimized for our do-
main. Our goal was to compare the SPOT sen-
tence planner with a representative rule-based
system. We decided against using an existing off-
the-shelf rule-based system, since it would be too
complex a task to port it to our application. In-
stead, we constructed two reasonably representa-
tive rule-based sentence planners. This task was
made easier by the fact that we could reuse much
of the work done for SPOT, in particular the data
structure of the sp-tree and the implementation of
the clause-combining operations. We developed
the two systems by applying heuristics for pro-
ducing good output, such as preferences for ag-
gregation. They differ only in the initial ordering
of the communicative acts in the input text plan.
In the first rule-based system, RBS (for “Rule-
Based System”), we order the speech acts with
explicit confirms first, then requests, then implicit
confirms. Note that explicit confirms and requests
do not co-occur in our data set. The second rule-
based system is identical, except that implicit con-
firms come first rather than last. This system we
call ICF (for “Rule-based System with Implicit
Confirms First”).
In the initial step of both RBS and ICF,
we take the two leftmost members of the text
plan and try to combine them using the follow-
ing preference ranking of the combination op-
erations: ADJECTIVE, the MERGEs, CONJUNC-
TION, RELATIVE-CLAUSE, PERIOD. The first
operation to succeed is chosen. This yields a bi-
nary sp-tree with three nodes, which becomes the
current sp-tree. As long as the root node of
the current sp-tree is not a PERIOD, we iterate
through the list of remaining speech acts on the
ordered text plan, combining each one with the
current sp-tree using the preference-ranked opera-
tions as just described. The result of each iteration
step is a binary, left-branching sp-tree. However,
if the root node of the current sp-tree is a PERIOD,
we start a new current sp-tree, as in the initial step
described above. When the text plan has been ex-
hausted, all partial sp-trees (all of which except
for the last one are rooted in PERIOD) are com-
bined in a left-branching tree using PERIOD. Cue
words are added as follows: (1) The cue word
now is attached to utterances beginning a new
subtask; (2) The cue word and is attached to ut-
terances continuing a subtask; (3) The cue words
alright or okay are attached to utterances contain-
ing implicit confirmations. Figure 3 includes an
RBS and an ICF output for the text plan in Fig-
ure 2. In this case ICF and RBS differ only in
the verb chosen as a more general verb during the
SOFT-MERGE operation.
We illustrate the RBS procedure with an ex-
ample for which ICF works similarly. For RBS,
the text plan in Figure 2 is ordered so that the re-
quest is first. For the request, a DSyntS is cho-
sen that can be paraphrased as What time would
you like to leave?. Then, the first implicit-confirm
</bodyText>
<figure confidence="0.991399375">
Sp−trees with associated DSyntSs
★ Sentence Planner ★
❁
SPG
✍
✍ ✍
.
.
SPR
✍
.
✍
Dialog
System
RealPro
Realizer
</figure>
<bodyText confidence="0.999937735294118">
is translated by lookup into a DSyntS which on
its own could generate Leaving in September.
We first try the ADJECTIVE aggregation opera-
tion, but since neither tree is a predicative ad-
jective, this fails. We then try the MERGE fam-
ily. MERGE-GENERAL succeeds, since the tree
for the request has an embedded node labeled
leave. The resulting DSyntS can be paraphrased
as What time would you like to leave in Septem-
ber?, and is attached to the new root node of
the resulting sp-tree. The root node is labeled
MERGE-GENERAL, and its two daughters are the
two speech acts. The implicit-confirm of the
day is added in a similar manner (adding an-
other left-branching node to the sp-tree), yielding
a DSyntS that can be paraphrased as What time
would you like to leave on September the 1st? (us-
ing some special-case attachment for dates within
MERGE). We now try and add the DSyntS for
the implicit-confirm, whose DSyntS might gener-
ate Going to Dallas. Here, we again cannot use
ADJECTIVE, nor can we use MERGE or MERGE-
GENERAL, since the verbs are not identical. In-
stead, we use SOFT-MERGE-GENERAL, which
identifies the leave node with the go root node of
the DSyntS of the implicit-confirm. When soft-
merging leave with go, fly is chosen as a general-
ization, resulting in a DSyntS that can be gener-
ated as What time would you like to fly on Septem-
ber the 1st to Dallas?. The sp-tree has added a
layer but is still left-branching. Finally, the last
implicit-confirm is added to yield a DSyntS that
is realized as What time would you like to fly on
September the 1st to Dallas from Newark?.
</bodyText>
<sectionHeader confidence="0.990619" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999736384615385">
All 60 subjects completed the experiment in a half
hour or less. The experiment resulted in a total
of 1200 judgements for each of the systems be-
ing compared, since each subject judged 20 ut-
terances by each system. We first discuss overall
differences among the different systems and then
make comparisons among the four different types
of systems: (1) TEMPLATE, (2) SPOT, (3) two
rule-based systems, and (4) two baseline systems.
All statistically significant results discussed here
had p values of less than .01.
We first examined whether differences in hu-
man ratings (score) were predictable from the
</bodyText>
<table confidence="0.985286">
System Min Max Mean S.D.
TEMPLATE 1 5 3.9 1.1
SPoT 1 5 3.9 1.3
RBS 1 5 3.4 1.4
ICF 1 5 3.5 1.4
No Aggregation 1 5 3.0 1.2
Random 1 5 2.7 1.4
</table>
<figureCaption confidence="0.981524">
Figure 7: Summary of Overall Results for all Sys-
tems Evaluated
</figureCaption>
<bodyText confidence="0.970819722222222">
type of system that produced the utterance be-
ing rated. A one-way ANOVA with system as the
independent variable and score as the dependent
variable showed that there were significant differ-
ences in score as a function of system. The overall
differences are summarized in Figure 7.
As Figure 7 indicates, some system outputs re-
ceived more consistent scores than others, e.g.
the standard deviation for TEMPLATE was much
smaller than RANDOM. The ranking of the sys-
tems by average score is TEMPLATE, SPOT, ICF,
RBS, NOAGG, and RANDOM. Posthoc compar-
isons of the scores of individual pairs of systems
using the adjusted Bonferroni statistic revealed
several different groupings.2
The highest ranking systems were TEMPLATE
and SPOT, whose ratings were not statistically
significantly different from one another. This
shows that it is possible to match the quality of a
hand-crafted system with a trainable one, which
should be more portable, more general and re-
quire less overall engineering effort.
The next group of systems were the two rule-
based systems, ICF and RBS, which were not
statistically different from one another. However
SPOT was statistically better than both of these
systems (p .01). Figure 8 shows that SPOT
got more high rankings than either of the rule-
based systems. In a sense this may not be that
surprising, because as Hovy and Wanner (1996)
point out, it is difficult to construct a rule-based
sentence planner that handles all the rule interac-
tions in a reasonable way. Features that SPoT’s
SPR uses allow SPOT to be sensitive to particular
discourse configurations or lexical collocations.
In order to encode these in a rule-based sentence
</bodyText>
<footnote confidence="0.958000666666667">
2The adjusted Bonferroni statistic guards against acci-
dentally finding differences between systems when making
multiple comparisons among systems.
</footnote>
<bodyText confidence="0.999963294117647">
planner, one would first have to discover these
constraints and then determine a way of enforc-
ing them. However the SPR simply learns that
a particular configuration is less preferred, result-
ing in a small decrement in ranking for the cor-
responding sp-tree. This flexibility of increment-
ing or decrementing a particular sp-tree by a small
amount may in the end allow it to be more sensi-
tive to small distinctions than a rule-based system.
Along with the TEMPLATE and RULE-BASED
systems, SPOT also scored better than the base-
line systems NOAGG and RANDOM. This is also
somewhat to be expected, since the baseline sys-
tems were intended to be the simplest systems
constructable. However it would have been a pos-
sible outcome for SPOT to not be different than
either system, e.g. if the sp-trees produced by
RANDOM were all equally good, or if the ag-
gregation rules that SPOT learned produced out-
put less readable than NOAGG. Figure 8 shows
that the distributions of scores for SPOT vs. the
baseline systems are very different, with SPOT
skewed towards higher scores.
Interestingly NOAGG also scored better than
RANDOM (p .01), and the standard deviation
of its scores was smaller (see Figure 7). Remem-
ber that RANDOM’s sp-trees often resulted in ar-
bitrarily ordering the speech acts in the output.
While NOAGG produced redundant utterances, it
placed the initiative taking speech act at the end of
the utterance in its most natural position, possibly
resulting in a preference for NOAGG over RAN-
DOM. Another reason to prefer NOAGG could be
its predictability.
</bodyText>
<sectionHeader confidence="0.997859" genericHeader="evaluation">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999976080645162">
Other work has also explored automatically train-
ing modules of a generator (Langkilde and
Knight, 1998; Mellish et al., 1998; Walker, 2000).
However, to our knowledge, this is the first re-
ported experimental comparison of a trainable
technique that shows that the quality of system
utterances produced with trainable components
can compete with hand-crafted or rule-based tech-
niques. The results validate our methodology;
SPOT outperforms two representative rule-based
sentence planners, and performs as well as the
hand-crafted TEMPLATE system, but is more eas-
ily and quickly tuned to a new domain: the train-
ing materials for the SPOT sentence planner can
be collected from subjective judgements from a
small number of judges with little or no linguistic
knowledge.
Previous work on evaluation of natural lan-
guage generation has utilized three different ap-
proaches to evaluation (Mellish and Dale, 1998).
The first approach is a subjective evaluation
methodology such as we use here, where human
subjects rate NLG outputs produced by different
sources (Lester and Porter, 1997). Other work has
evaluated template-based spoken dialog genera-
tion with a task-based approach, i.e. the genera-
tor is evaluated with a metric such as task com-
pletion or user satisfaction after dialog comple-
tion (Walker, 2000). This approach can work
well when the task only involves one or two ex-
changes, when the choices have large effects over
the whole dialog, or the choices vary the con-
tent of the utterance. Because sentence plan-
ning choices realize the same content and only
affect the current utterance, we believed it impor-
tant to get local feedback. A final approach fo-
cuses on subproblems of natural language gener-
ation such as the generation of referring expres-
sions. For this type of problem it is possible to
evaluate the generator by the degree to which it
matches human performance (Yeh and Mellish,
1997). When evaluating sentence planning, this
approach doesn’t make sense because many dif-
ferent realizations may be equally good.
However, this experiment did not show that
trainable sentence planners produce, in general,
better-quality output than template-based or rule-
based sentence planners. That would be im-
possible: given the nature of template and rule-
based systems, any quality standard for the output
can be met given sufficient person-hours, elapsed
time, and software engineering acumen. Our prin-
cipal goal, rather, is to show that the quality of the
TEMPLATE output, for a currently operational dia-
log system whose template-based output compo-
nent was developed, expanded, and refined over
about 18 months, can be achieved using a train-
able system, for which the necessary training data
was collected in three person-days. Furthermore,
we wished to show that a representative rule-
based system based on current literature, without
massive domain-tuning, cannot achieve the same
</bodyText>
<figure confidence="0.997518375">
Number of plans with that score or more
1200
1000
800
600
400
AMELIA
SPOT
IC
RB
NOAGG
RAN
200
0
1 1.5 2 2.5 3 3.5 4 4.5 5
Score
</figure>
<figureCaption confidence="0.999975">
Figure 8: Chart comparing distribution of human ratings for SPOT, RBS, ICF, NOAGG and RANDOM.
</figureCaption>
<bodyText confidence="0.9989825">
level of quality. In future work, we hope to extend
SPoT and integrate it into AMELIA.
</bodyText>
<sectionHeader confidence="0.99954" genericHeader="conclusions">
6 Acknowledments
</sectionHeader>
<bodyText confidence="0.997237">
This work was partially funded by DARPA under
contract MDA972-99-3-0003.
</bodyText>
<sectionHeader confidence="0.999188" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999874068181818">
Y. Freund, R. Iyer, R. E. Schapire, and Y.Singer. 1998.
An efficient boosting algorithm for combining pref-
erences. In Machine Learning: Proc. of the Fif-
teenth International Conference.
E.H. Hovy and L. Wanner. 1996. Managing sentence
planning requirements. In Proc. of the ECAI’96
Workshop Gaps and Bridges: New Directions in
Planning and Natural Language Generation.
I. Langkilde and K. Knight. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In Proc.
of COLING-ACL.
Benoit Lavoie and Owen Rambow. 1997. A fast and
portable realizer for text generation systems. In
Proc. of the Third Conference on Applied Natural
Language Processing, ANLP97, pages 265–268.
J. Lester and B. Porter. 1997. Developing and em-
pirically evaluating robust explanation generators:
The knight experiments. Computational Linguis-
tics, 23-1:65–103.
C. Mellish and R. Dale. 1998. Evaluation in the
context of natural language generation. Computer
Speech and Language, 12(3).
C. Mellish, A. Knott, J. Oberlander, and M.
O’Donnell. 1998. Experiments using stochas-
tic search for text planning. In Proc. of Interna-
tional Conference on Natural Language Genera-
tion, pages 97–108.
I. A. Melˇcuk. 1988. Dependency Syntax: Theory and
Practice. SUNY, Albany, New York.
O. Rambow and T. Korelsky. 1992. Applied text
generation. In Proc. of the Third Conference on
Applied Natural Language Processing, ANLP92,
pages 40–47.
M. Walker, O. Rambow, and M. Rogati. 2001. Spot:
A trainable sentence planner. In Proc. of the North
American Meeting of the Association for Computa-
tional Linguistics.
M. A. Walker. 2000. An application of reinforcement
learning to dialogue strategy selection in a spoken
dialogue system for email. Journal ofArtificial In-
telligence Research, 12:387–416.
C.L. Yeh and C. Mellish. 1997. An empirical study
on the generation of anaphora in chinese. Compu-
tational Linguistics, 23-1:169–190.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.947502">
<title confidence="0.999992">Evaluating a Trainable Sentence Planner for a Spoken Dialogue System</title>
<author confidence="0.999984">Owen Rambow Monica Rogati Marilyn A Walker</author>
<affiliation confidence="0.999081">AT&amp;T Labs – Research Carnegie Mellon University AT&amp;T Labs – Research</affiliation>
<address confidence="0.998148">Florham Park, NJ, USA Pittsburgh, PA, USA Florham Park, NJ, USA</address>
<email confidence="0.999889">rambow@research.att.commrogati+@cs.cmu.eduwalker@research.att.com</email>
<abstract confidence="0.997726">Techniques for automatically training modules of a natural language generator have recently been proposed, but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rulebased approaches. In this paper We experimentally evaluate a trainable sentence planner for a spoken dialogue system by eliciting subjective human judgments. In order to perform an exhaustive comparison, we also evaluate a hand-crafted template-based generation component, two rule-based sentence planners, and two baseline sentence planners. We show that the trainable sentence planner performs better than the rule-based systems and the baselines, and as well as the handcrafted system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Iyer</author>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>1998</date>
<booktitle>In Machine Learning: Proc. of the Fifteenth International Conference.</booktitle>
<contexts>
<context position="16019" citStr="Freund et al., 1998" startWordPosition="2540" endWordPosition="2543">first phase, the sentence-plan-generator (SPG) randomly generates up to twenty possible sentence plans for a given text-plan input. For this phase we use the RANDOM sentence-planner. In the second phase, the sentence-plan-ranker (SPR) ranks the sample imp−confirm(month) request(time) soft−merge−general 3 1 soft−merge−general imp−confirm(day) 2 soft−merge soft−merge−general Text Plan Chosen sp−tree with associated DSyntS Figure 6: Architecture of SPoT sentence plans, and then selects the top-ranked output to input to the surface realizer. The SPR is automatically trained by applying RankBoost (Freund et al., 1998) to learn ranking rules from training data. The training data was assembled by using RANDOM to randomly generate up to 20 realizations for 100 turns; two human judges then ranked each of these realizations (using the setup described in Section 2). Over 3,000 features were discovered from the generated trees by routines that encode structural and lexical aspects of the sp-trees and the DSyntS. RankBoost identified the features that contribute most to a realization’s ranking. The SPR uses these rules to rank alternative sp-trees, and then selects the top-ranked output as input to the surface rea</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 1998</marker>
<rawString>Y. Freund, R. Iyer, R. E. Schapire, and Y.Singer. 1998. An efficient boosting algorithm for combining preferences. In Machine Learning: Proc. of the Fifteenth International Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Hovy</author>
<author>L Wanner</author>
</authors>
<title>Managing sentence planning requirements.</title>
<date>1996</date>
<booktitle>In Proc. of the ECAI’96 Workshop Gaps and Bridges: New Directions in Planning and Natural Language Generation.</booktitle>
<contexts>
<context position="14130" citStr="Hovy and Wanner (1996)" startWordPosition="2247" endWordPosition="2250">OFT-MERGE in Figure 4 shows the result when Args 1 and 2 are implicit confirmations of the origin and destination. See (Walker et al., 2001) for more detail on the sp-tree. The experimental sentence planners described below vary how the sptree is constructed. imp−confirm(dest−city) imp−confirm(orig−city) Figure 5: A Sentence Plan Tree for Utterance System 5 in Dialog D1 3.2 Baseline Sentence Planners In one obvious baseline system the sp-tree is constructed by applying only the PERIOD operation: each elementary speech act is realized as its own sentence. This baseline, NOAGG, was suggested by Hovy and Wanner (1996). For NOAGG, we order the communicative acts from the text plan as follows: implicit confirms precede explicit confirms precede requests. Figure 3 includes a NOAGG output for the text plan in Figure 2. A second possible baseline sentence planner simply applies combination rules randomly according to a hand-crafted probability distribution based on preferences for operations such as the MERGE family over CONJUNCTION and PERIOD. In order to be able to generate the resulting sentence plan tree, we exclude certain combinations, such as generating anything other than a PERIOD above a node labeled P</context>
<context position="23603" citStr="Hovy and Wanner (1996)" startWordPosition="3855" endWordPosition="3858">ngs were not statistically significantly different from one another. This shows that it is possible to match the quality of a hand-crafted system with a trainable one, which should be more portable, more general and require less overall engineering effort. The next group of systems were the two rulebased systems, ICF and RBS, which were not statistically different from one another. However SPOT was statistically better than both of these systems (p .01). Figure 8 shows that SPOT got more high rankings than either of the rulebased systems. In a sense this may not be that surprising, because as Hovy and Wanner (1996) point out, it is difficult to construct a rule-based sentence planner that handles all the rule interactions in a reasonable way. Features that SPoT’s SPR uses allow SPOT to be sensitive to particular discourse configurations or lexical collocations. In order to encode these in a rule-based sentence 2The adjusted Bonferroni statistic guards against accidentally finding differences between systems when making multiple comparisons among systems. planner, one would first have to discover these constraints and then determine a way of enforcing them. However the SPR simply learns that a particular</context>
</contexts>
<marker>Hovy, Wanner, 1996</marker>
<rawString>E.H. Hovy and L. Wanner. 1996. Managing sentence planning requirements. In Proc. of the ECAI’96 Workshop Gaps and Bridges: New Directions in Planning and Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="4001" citStr="Langkilde and Knight, 1998" startWordPosition="589" endWordPosition="592">eering studies of template development and maintenance, this claim is supported by abundant anecdotal evidence. independent. However, the quality of the output for a particular domain, or a particular situation in a dialog, may be inferior to that of a templatebased system without considerable investment in domain-specific rules or domain-tuning of general rules. Furthermore, since rule-based systems use sophisticated linguistic representations, this handcrafting requires linguistic knowledge. Recently, several approaches for automatically training modules of an NLG system have been proposed (Langkilde and Knight, 1998; Mellish et al., 1998; Walker, 2000). These hold the promise that the complex step of customizing NLG systems by hand can be automated, while avoiding the need for tedious hand-crafting of templates. While the engineering benefits of trainable approaches appear obvious, it is unclear whether the utterance quality is high enough. In (Walker et al., 2001) we propose a new model of sentence planning called SPOT. In SPOT, the sentence planner is automatically trained, using feedback from two human judges, to choose the best from among different options for realizing a set of communicative goals. </context>
<context position="25740" citStr="Langkilde and Knight, 1998" startWordPosition="4204" endWordPosition="4207">. Interestingly NOAGG also scored better than RANDOM (p .01), and the standard deviation of its scores was smaller (see Figure 7). Remember that RANDOM’s sp-trees often resulted in arbitrarily ordering the speech acts in the output. While NOAGG produced redundant utterances, it placed the initiative taking speech act at the end of the utterance in its most natural position, possibly resulting in a preference for NOAGG over RANDOM. Another reason to prefer NOAGG could be its predictability. 5 Discussion and Future Work Other work has also explored automatically training modules of a generator (Langkilde and Knight, 1998; Mellish et al., 1998; Walker, 2000). However, to our knowledge, this is the first reported experimental comparison of a trainable technique that shows that the quality of system utterances produced with trainable components can compete with hand-crafted or rule-based techniques. The results validate our methodology; SPOT outperforms two representative rule-based sentence planners, and performs as well as the hand-crafted TEMPLATE system, but is more easily and quickly tuned to a new domain: the training materials for the SPOT sentence planner can be collected from subjective judgements from </context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and K. Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Lavoie</author>
<author>Owen Rambow</author>
</authors>
<title>A fast and portable realizer for text generation systems.</title>
<date>1997</date>
<booktitle>In Proc. of the Third Conference on Applied Natural Language Processing, ANLP97,</booktitle>
<pages>265--268</pages>
<contexts>
<context position="12984" citStr="Lavoie and Rambow, 1997" startWordPosition="2058" endWordPosition="2061">re leaving from Newark. You are going to Dallas. You are leaving from Newark and you are going to Dallas. RELATIVE- Your flight leaves at 5. Your flight arrives at 9. Your flight, which leaves at 5, arCLAUSE rives at 9. ADJECTIVE Your flight leaves at 5. Your flight is nonstop. Your nonstop flight leaves at 5. PERIOD You are leaving from Newark. You are going to Dallas. You are leaving from Newark. You are going to Dallas RANDOM CUE- What time would yo like to n/a Now, what time would you like to WORD leave? leave? Figure 4: List of clause combining operations with examples sentence planners (Lavoie and Rambow, 1997). DSyntSs are combined using the operations exemplified in Figure 4. The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts from the input text plan, and with its interior nodes labeled with clause-combining operations. As an example, Figure 5 shows the sp-tree for utterance System5 in Figure 1. Node soft-merge-general merges an implicit-confirmation of the destination city and the origin city. The row labelled SOFT-MERGE in Figure 4 shows the result when Args 1 and 2 are implicit confir</context>
</contexts>
<marker>Lavoie, Rambow, 1997</marker>
<rawString>Benoit Lavoie and Owen Rambow. 1997. A fast and portable realizer for text generation systems. In Proc. of the Third Conference on Applied Natural Language Processing, ANLP97, pages 265–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lester</author>
<author>B Porter</author>
</authors>
<title>Developing and empirically evaluating robust explanation generators: The knight experiments.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--1</pages>
<contexts>
<context position="26716" citStr="Lester and Porter, 1997" startWordPosition="4354" endWordPosition="4357">ive rule-based sentence planners, and performs as well as the hand-crafted TEMPLATE system, but is more easily and quickly tuned to a new domain: the training materials for the SPOT sentence planner can be collected from subjective judgements from a small number of judges with little or no linguistic knowledge. Previous work on evaluation of natural language generation has utilized three different approaches to evaluation (Mellish and Dale, 1998). The first approach is a subjective evaluation methodology such as we use here, where human subjects rate NLG outputs produced by different sources (Lester and Porter, 1997). Other work has evaluated template-based spoken dialog generation with a task-based approach, i.e. the generator is evaluated with a metric such as task completion or user satisfaction after dialog completion (Walker, 2000). This approach can work well when the task only involves one or two exchanges, when the choices have large effects over the whole dialog, or the choices vary the content of the utterance. Because sentence planning choices realize the same content and only affect the current utterance, we believed it important to get local feedback. A final approach focuses on subproblems o</context>
</contexts>
<marker>Lester, Porter, 1997</marker>
<rawString>J. Lester and B. Porter. 1997. Developing and empirically evaluating robust explanation generators: The knight experiments. Computational Linguistics, 23-1:65–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mellish</author>
<author>R Dale</author>
</authors>
<title>Evaluation in the context of natural language generation.</title>
<date>1998</date>
<journal>Computer Speech and Language,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="26542" citStr="Mellish and Dale, 1998" startWordPosition="4327" endWordPosition="4330"> utterances produced with trainable components can compete with hand-crafted or rule-based techniques. The results validate our methodology; SPOT outperforms two representative rule-based sentence planners, and performs as well as the hand-crafted TEMPLATE system, but is more easily and quickly tuned to a new domain: the training materials for the SPOT sentence planner can be collected from subjective judgements from a small number of judges with little or no linguistic knowledge. Previous work on evaluation of natural language generation has utilized three different approaches to evaluation (Mellish and Dale, 1998). The first approach is a subjective evaluation methodology such as we use here, where human subjects rate NLG outputs produced by different sources (Lester and Porter, 1997). Other work has evaluated template-based spoken dialog generation with a task-based approach, i.e. the generator is evaluated with a metric such as task completion or user satisfaction after dialog completion (Walker, 2000). This approach can work well when the task only involves one or two exchanges, when the choices have large effects over the whole dialog, or the choices vary the content of the utterance. Because sente</context>
</contexts>
<marker>Mellish, Dale, 1998</marker>
<rawString>C. Mellish and R. Dale. 1998. Evaluation in the context of natural language generation. Computer Speech and Language, 12(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mellish</author>
<author>A Knott</author>
<author>J Oberlander</author>
<author>M O’Donnell</author>
</authors>
<title>Experiments using stochastic search for text planning.</title>
<date>1998</date>
<booktitle>In Proc. of International Conference on Natural Language Generation,</booktitle>
<pages>97--108</pages>
<marker>Mellish, Knott, Oberlander, O’Donnell, 1998</marker>
<rawString>C. Mellish, A. Knott, J. Oberlander, and M. O’Donnell. 1998. Experiments using stochastic search for text planning. In Proc. of International Conference on Natural Language Generation, pages 97–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Melˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<location>SUNY, Albany, New York.</location>
<marker>Melˇcuk, 1988</marker>
<rawString>I. A. Melˇcuk. 1988. Dependency Syntax: Theory and Practice. SUNY, Albany, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Rambow</author>
<author>T Korelsky</author>
</authors>
<title>Applied text generation.</title>
<date>1992</date>
<booktitle>In Proc. of the Third Conference on Applied Natural Language Processing, ANLP92,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="2811" citStr="Rambow and Korelsky, 1992" startWordPosition="417" endWordPosition="420">e little or no linguistic training is needed to write templates, it is a tedious and time-consuming task: one or more templates must be written for each combination of goals and discourse contexts, and linguistic issues such as subject-verb agreement and determiner-noun agreement must be repeatedly encoded for each template. Furthermore, maintenance of the collection of templates becomes a software engineering problem as the complexity of the dialog system increases.1 The second approach is natural language generation (NLG), which customarily divides the generation process into three modules (Rambow and Korelsky, 1992): (1) Text Planning, (2) Sentence Planning, and (3) Surface Realization. In this paper, we discuss only sentence planning; the role of the sentence planner is to choose abstract lexico-structural resources for a text plan, where a text plan encodes the communicative goals for an utterance (and, sometimes, their rhetorical structure). In general, NLG promises portability across application domains and dialog situations by focusing on the development of rules for each generation module that are general and domain&apos;Although we are not aware of any software engineering studies of template developme</context>
</contexts>
<marker>Rambow, Korelsky, 1992</marker>
<rawString>O. Rambow and T. Korelsky. 1992. Applied text generation. In Proc. of the Third Conference on Applied Natural Language Processing, ANLP92, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>O Rambow</author>
<author>M Rogati</author>
</authors>
<title>Spot: A trainable sentence planner.</title>
<date>2001</date>
<booktitle>In Proc. of the North American Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4357" citStr="Walker et al., 2001" startWordPosition="647" endWordPosition="650">es. Furthermore, since rule-based systems use sophisticated linguistic representations, this handcrafting requires linguistic knowledge. Recently, several approaches for automatically training modules of an NLG system have been proposed (Langkilde and Knight, 1998; Mellish et al., 1998; Walker, 2000). These hold the promise that the complex step of customizing NLG systems by hand can be automated, while avoiding the need for tedious hand-crafting of templates. While the engineering benefits of trainable approaches appear obvious, it is unclear whether the utterance quality is high enough. In (Walker et al., 2001) we propose a new model of sentence planning called SPOT. In SPOT, the sentence planner is automatically trained, using feedback from two human judges, to choose the best from among different options for realizing a set of communicative goals. In (Walker et al., 2001), we evaluate the performance of the learning component of SPOT, and show that SPOT learns to select sentence plans that are highly rated by the two human judges. While this evaluation shows that SPOT has indeed learned from the human judges, it does not show that using only two human judgments is sufficient to produce more broadl</context>
<context position="13648" citStr="Walker et al., 2001" startWordPosition="2171" endWordPosition="2174">exemplified in Figure 4. The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts from the input text plan, and with its interior nodes labeled with clause-combining operations. As an example, Figure 5 shows the sp-tree for utterance System5 in Figure 1. Node soft-merge-general merges an implicit-confirmation of the destination city and the origin city. The row labelled SOFT-MERGE in Figure 4 shows the result when Args 1 and 2 are implicit confirmations of the origin and destination. See (Walker et al., 2001) for more detail on the sp-tree. The experimental sentence planners described below vary how the sptree is constructed. imp−confirm(dest−city) imp−confirm(orig−city) Figure 5: A Sentence Plan Tree for Utterance System 5 in Dialog D1 3.2 Baseline Sentence Planners In one obvious baseline system the sp-tree is constructed by applying only the PERIOD operation: each elementary speech act is realized as its own sentence. This baseline, NOAGG, was suggested by Hovy and Wanner (1996). For NOAGG, we order the communicative acts from the text plan as follows: implicit confirms precede explicit confirm</context>
<context position="16646" citStr="Walker et al. (2001)" startWordPosition="2642" endWordPosition="2645">rn ranking rules from training data. The training data was assembled by using RANDOM to randomly generate up to 20 realizations for 100 turns; two human judges then ranked each of these realizations (using the setup described in Section 2). Over 3,000 features were discovered from the generated trees by routines that encode structural and lexical aspects of the sp-trees and the DSyntS. RankBoost identified the features that contribute most to a realization’s ranking. The SPR uses these rules to rank alternative sp-trees, and then selects the top-ranked output as input to the surface realizer. Walker et al. (2001) describe SPOT in detail. 3.4 Two Rule-Based Sentence Planners It has not been the object of our research to construct a rule-based sentence planner by hand, be it domain-independent or optimized for our domain. Our goal was to compare the SPOT sentence planner with a representative rule-based system. We decided against using an existing offthe-shelf rule-based system, since it would be too complex a task to port it to our application. Instead, we constructed two reasonably representative rule-based sentence planners. This task was made easier by the fact that we could reuse much of the work d</context>
</contexts>
<marker>Walker, Rambow, Rogati, 2001</marker>
<rawString>M. Walker, O. Rambow, and M. Rogati. 2001. Spot: A trainable sentence planner. In Proc. of the North American Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
</authors>
<title>An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email.</title>
<date>2000</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>12--387</pages>
<contexts>
<context position="4038" citStr="Walker, 2000" startWordPosition="598" endWordPosition="599">ance, this claim is supported by abundant anecdotal evidence. independent. However, the quality of the output for a particular domain, or a particular situation in a dialog, may be inferior to that of a templatebased system without considerable investment in domain-specific rules or domain-tuning of general rules. Furthermore, since rule-based systems use sophisticated linguistic representations, this handcrafting requires linguistic knowledge. Recently, several approaches for automatically training modules of an NLG system have been proposed (Langkilde and Knight, 1998; Mellish et al., 1998; Walker, 2000). These hold the promise that the complex step of customizing NLG systems by hand can be automated, while avoiding the need for tedious hand-crafting of templates. While the engineering benefits of trainable approaches appear obvious, it is unclear whether the utterance quality is high enough. In (Walker et al., 2001) we propose a new model of sentence planning called SPOT. In SPOT, the sentence planner is automatically trained, using feedback from two human judges, to choose the best from among different options for realizing a set of communicative goals. In (Walker et al., 2001), we evaluate</context>
<context position="25777" citStr="Walker, 2000" startWordPosition="4212" endWordPosition="4213">OM (p .01), and the standard deviation of its scores was smaller (see Figure 7). Remember that RANDOM’s sp-trees often resulted in arbitrarily ordering the speech acts in the output. While NOAGG produced redundant utterances, it placed the initiative taking speech act at the end of the utterance in its most natural position, possibly resulting in a preference for NOAGG over RANDOM. Another reason to prefer NOAGG could be its predictability. 5 Discussion and Future Work Other work has also explored automatically training modules of a generator (Langkilde and Knight, 1998; Mellish et al., 1998; Walker, 2000). However, to our knowledge, this is the first reported experimental comparison of a trainable technique that shows that the quality of system utterances produced with trainable components can compete with hand-crafted or rule-based techniques. The results validate our methodology; SPOT outperforms two representative rule-based sentence planners, and performs as well as the hand-crafted TEMPLATE system, but is more easily and quickly tuned to a new domain: the training materials for the SPOT sentence planner can be collected from subjective judgements from a small number of judges with little </context>
</contexts>
<marker>Walker, 2000</marker>
<rawString>M. A. Walker. 2000. An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email. Journal ofArtificial Intelligence Research, 12:387–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Yeh</author>
<author>C Mellish</author>
</authors>
<title>An empirical study on the generation of anaphora in chinese.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--1</pages>
<contexts>
<context position="27536" citStr="Yeh and Mellish, 1997" startWordPosition="4494" endWordPosition="4497"> completion (Walker, 2000). This approach can work well when the task only involves one or two exchanges, when the choices have large effects over the whole dialog, or the choices vary the content of the utterance. Because sentence planning choices realize the same content and only affect the current utterance, we believed it important to get local feedback. A final approach focuses on subproblems of natural language generation such as the generation of referring expressions. For this type of problem it is possible to evaluate the generator by the degree to which it matches human performance (Yeh and Mellish, 1997). When evaluating sentence planning, this approach doesn’t make sense because many different realizations may be equally good. However, this experiment did not show that trainable sentence planners produce, in general, better-quality output than template-based or rulebased sentence planners. That would be impossible: given the nature of template and rulebased systems, any quality standard for the output can be met given sufficient person-hours, elapsed time, and software engineering acumen. Our principal goal, rather, is to show that the quality of the TEMPLATE output, for a currently operatio</context>
</contexts>
<marker>Yeh, Mellish, 1997</marker>
<rawString>C.L. Yeh and C. Mellish. 1997. An empirical study on the generation of anaphora in chinese. Computational Linguistics, 23-1:169–190.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>