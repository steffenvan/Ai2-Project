<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000569">
<title confidence="0.913183">
Query log analysis with GALATEAS LangLog
</title>
<author confidence="0.706231">
Marco Trevisan and Luca Dini
</author>
<note confidence="0.580839">
CELI
</note>
<email confidence="0.8499315">
trevisan@celi.it
dini@celi.it
</email>
<author confidence="0.943161">
Igor Barsanti
</author>
<affiliation confidence="0.768728">
Gonetwork
</affiliation>
<email confidence="0.991253">
i.barsanti@gonetwork.it
</email>
<author confidence="0.82295">
Eduard Barbu
</author>
<affiliation confidence="0.742106">
Universit`a di Trento
</affiliation>
<email confidence="0.965125">
eduard.barbu@unitn.it
</email>
<author confidence="0.946809">
Nikolaos Lagos
</author>
<affiliation confidence="0.910091">
Xerox Research Centre Europe
</affiliation>
<email confidence="0.980317">
Nikolaos.Lagos@xrce.xerox.com
</email>
<author confidence="0.946054">
Fr´ed´erique Segond and Mathieu Rhulmann
</author>
<affiliation confidence="0.757353">
Objet Direct
</affiliation>
<email confidence="0.9614455">
fsegond@objetdirect.com
mruhlmann@objetdirect.com
</email>
<author confidence="0.480173">
Ed Vald
Bridgeman Art Library
</author>
<email confidence="0.981819">
ed.vald@bridgemanart.co.uk
</email>
<sectionHeader confidence="0.995226" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999723384615384">
This article describes GALATEAS
LangLog, a system performing Search Log
Analysis. LangLog illustrates how NLP
technologies can be a powerful support
tool for market research even when the
source of information is a collection of
queries each one consisting of few words.
We push the standard Search Log Analysis
forward taking into account the semantics
of the queries. The main innovation of
LangLog is the implementation of two
highly customizable components that
cluster and classify the queries in the log.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968288461538">
Transaction logs become increasingly important
for studying the user interaction with systems
like Web Searching Engines, Digital Libraries, In-
tranet Servers and others (Jansen, 2006). Var-
ious service providers keep log files recording
the user interaction with the searching engines.
Transaction logs are useful to understand the user
search strategy but also to improve query sugges-
tions (Wen and Zhang, 2003) and to enhance
the retrieval quality of search engines (Joachims,
2002). The process of analyzing the transaction
logs to understand the user behaviour and to as-
sess the system performance is known as Transac-
tion Log Analysis (TLA). Transaction Log Anal-
ysis is concerned with the analysis of both brows-
ing and searching activity inside a website. The
analysis of transaction logs that focuses on search
activity only is known as Search Log Analysis
(SLA). According to Jansen (2008) both TLA
and SLA have three stages: data collection, data
preparation and data analysis. In the data collec-
tion stage one collects data describing the user
interaction with the system. Data preparation is
the process of loading the collected data in a re-
lational database. The data loaded in the database
gives a transaction log representation independent
of the particular log syntax. In the final stage
the data prepared at the previous step is analyzed.
One may notice that the traditional three levels
log analyses give a syntactic view of the infor-
mation in the logs. Counting terms, measuring
the logical complexity of queries or the simple
procedures that associate queries with the ses-
sions in no way accesses the semantics of queries.
LangLog system addreses the semantic problem
performing clustering and classification for real
query logs. Clustering the queries in the logs al-
lows the identification of meaningful groups of
queries. Classifying the queries according to a
relevant list of categories permits the assessment
of how well the searching engine meets the user
needs. In addition the LangLog system address
problems like automatic language identification,
Name Entity Recognition, and automatic query
translation. The rest of the paper is organized
as follows: the next section briefly reviews some
systems performing SLA. Then we present the
data sources the architecture and the analysis pro-
cess of the LangLog system. The conclusion sec-
tion concludes the article summarizing the work
and presenting some new possible enhancements
of the LangLog.
</bodyText>
<page confidence="0.99034">
87
</page>
<note confidence="0.7182615">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 87–91,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.998421" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999965736842105">
The information in the log files is useful in many
ways, but its extraction raises many challenges
and issues. Facca and Lanzi (2005) offer a sur-
vey of the topic. There are several commercial
systems to extract and analyze this information,
such as Adobe web analytics1, SAS Web Analyt-
ics2, Infor Epiphany3, IBM SPSS4. These prod-
ucts are often part of a customer relation manage-
ment (CRM) system. None of those showcases
include any form of linguistic processing. On the
other hand, Web queries have been the subject
of linguistic analysis, to improve the performance
of information retrieval systems. For example, a
study (Monz and de Rijke, 2002) experimented
with shallow morphological analysis, another (Li
et al., 2006) analyzed queries to remove spelling
mistakes. These works encourage our belief that
linguistic analysis could be beneficial for Web log
analysis systems.
</bodyText>
<sectionHeader confidence="0.984374" genericHeader="method">
3 Data sources
</sectionHeader>
<bodyText confidence="0.999978181818182">
LangLog requires the following information from
the Web logs: the time of the interaction, the
query, click-through information and possibly
more. LangLog processes log files which con-
form to the W3C extended log format. No other
formats are supported. The system prototype is
based on query logs spanning one month of inter-
actions recorded at the Bridgeman Art Library5.
Bridgeman Art library contains a large repository
of images coming from 8000 collections and rep-
resenting more than 29.000 artists.
</bodyText>
<sectionHeader confidence="0.99896" genericHeader="method">
4 Analyses
</sectionHeader>
<bodyText confidence="0.99991">
LangLog organizes the search log data into units
called queries and hits. In a typical search-
ing scenario a user submits a query to the con-
tent provider’s site-searching engine and clicks
on some (or none) of the search results. From
now on we will refer to a clicked item as a hit,
and we will refer to the text typed by the user as
the query. This information alone is valuable to
the content provider because it allows to discover
</bodyText>
<footnote confidence="0.9999662">
1http://www.omniture.com/en/products/analytics
2http://www.sas.com/solutions/webanalytics/index.html
3http://www.infor.com
4http://www-01.ibm.com/software/analytics/spss/
5http://www.bridgemanart.com
</footnote>
<bodyText confidence="0.9978706">
which queries were served with results that satis-
fied the user, and which queries were not.
LangLog extracts queries and hits from the log
files, and performs the following analyses on the
queries:
</bodyText>
<listItem confidence="0.999884">
• language identification
• tokenization and lemmatization
• named entity recognition
• classification
• cluster analysis
</listItem>
<bodyText confidence="0.999752162162163">
Language information may help the content
provider decide whether to translate the content
into new languages.
Lemmatization is especially important in lan-
guages like German and Italian that have a rich
morphology. Frequency statistics of keywords
help understand what users want, but they are bi-
ased towards items associated with words with
lesser ortographic and morpho-syntactic varia-
tion. For example, two thousand queries for
”trousers”, one thousand queries for ”handbag”
and another thousand queries for ”handbags”
means that handbags are twice as popular as
trousers, although statistics based on raw words
would say otherwise.
Named entities extraction helps the content
provider for the same reasons lemmatization does.
Named entities are especially important because
they identify real-world items that the content
provider can relate to, while lemmas less often do
so. The name entities and the most important con-
cepts can be linked afterwards with resources like
Wikipedia which offer a rich specification of their
properties.
Both classification and clustering allow the
content provider to understand what kind of the
users look for and how this information is targeted
by means of queries.
Classification consists of classifying queries
into categories drawn from a classification
schema. When the schema used to classify
is different from the schema used in the con-
tent provider’s website, classification may provide
hints as to what kind of queries are not matched
by items in the website. In a similar way, cluster
analysis can be used to identify new market seg-
ments or new trends in the user’s behaviour. Clus-
</bodyText>
<page confidence="0.994818">
88
</page>
<bodyText confidence="0.9999693">
ter analysis provide more flexybility than classifi-
cation, but the information it produces is less pre-
cise. Many trials and errors may be necessary be-
fore finding interesting results. One hopes that the
final clustering solution will give insights into the
patterns of users’ searches. For example an on-
line book store may discover that one cluster con-
tains many software-related terms, altough none
of those terms is popular enough to be noticeable
in the statistics.
</bodyText>
<sectionHeader confidence="0.993644" genericHeader="method">
5 Architecture
</sectionHeader>
<bodyText confidence="0.99978925">
LangLog consists of three subsystems: log ac-
quisition, log analysis, log disclosure. Periodi-
cally the log acquisition subsystem gathers new
data which it passes to the log analyses compo-
nent. The results of the analyses are then available
through the log disclosure subsystem.
Log acquisition deals with the acquisition and
normalization and anonymization of the data con-
tained in the content provider’s log files. The
data flows from the content provider’s servers to
LangLog’s central database. This process is car-
ried out by a series of Pentaho Data Integration6
procedures.
Log analysis deals with the anaysis of the data.
The analyses proper are executed by NLP systems
provided by third parties and accessible as Web
services. LangLog uses NLP Web services for
language identification, morpho-syntactic analy-
sis, named entity recognition, classification and
clustering. The analyses are stored in the database
along with the original data.
Log disclosure is actually a collection of inde-
pendent systems that allow the content providers
to access their information and the analyses. Log
disclosure systems are also concerned with access
control and protection of privacy. The content
provider can access the output of LangLog using
AWStats, QlikView, or JPivot.
</bodyText>
<listItem confidence="0.995292857142857">
• AWStats7 is a widely used log analysis sys-
tem for websites. The logs gathered from the
websites are parsed by AWStats, which gen-
erates a complete report about visitors, vis-
its duration, visitor’s countries and other data
to disclose useful information about the visi-
tor’s behavior.
</listItem>
<footnote confidence="0.9998155">
6http://kettle.pentaho.com
7http://awstats.sourceforge.net
</footnote>
<listItem confidence="0.96997">
• QlikView8 is a business intelligence (BI)
platform. A BI platform provides histori-
cal, current, and predictive views of busi-
ness operations. Usually such tools are used
by companies to have a clear view of their
business over time. In LangLog, QlickView
does not display sales or costs evolution over
time. Instead, it displays queries on the con-
tent provider’s website over time. A dash-
board with many elements (input selections,
tables, charts, etc.) provides a wide range of
tools to visualize the data.
• JPivot9 is a front-end for Mondrian. Mon-
drian10 is an Online Analytical Processing
(OLAP) engine, a system capable of han-
dling and analyzing large quantities of data.
</listItem>
<bodyText confidence="0.960044923076923">
JPivot allows the user to explore the output
of LangLog, by slicing the data along many
dimensions. JPivot allows the user to display
charts, export results to Microsoft Excel or
CSV, and use custom OLAP MDX queries.
Log analysis deals with the anaysis of the data.
The analyses proper are executed by NLP systems
provided by third parties and accessible as Web
services. LangLog uses NLP Web services for
language identification, morpho-syntactic analy-
sis, named entity recognition, classification and
clustering. The analyses are stored in the database
along with the original data.
</bodyText>
<subsectionHeader confidence="0.985346">
5.1 Language Identification
</subsectionHeader>
<bodyText confidence="0.9870156">
The system uses a language identification sys-
tem (Bosca and Dini, 2010) which offers language
identification for English, French, Italian, Span-
ish, Polish and German. The system uses four
different strategies:
</bodyText>
<listItem confidence="0.9993561">
• N-gram character models: uses the distance
between the character based models of the
input and of a reference corpus for the lan-
guage (Wikipedia).
• Word frequency: looks up the frequency of
the words in the query with respect to a ref-
erence corpus for the language.
• Function words: searches for particles
highly connoting a specific language (such
as prepositions, conjunctions).
</listItem>
<footnote confidence="0.999973666666667">
8http://www.qlikview.com
9http://jpivot.sourceforge.net
10http://mondrian.pentaho.com
</footnote>
<page confidence="0.999494">
89
</page>
<listItem confidence="0.993317">
• Prior knowledge: provides a default guess
based on a set of hypothesis and heuristics
like region/browser language.
</listItem>
<subsectionHeader confidence="0.99574">
5.2 Lemmatization
</subsectionHeader>
<bodyText confidence="0.999954125">
To perform lemmatization, Langlog uses general-
purpose morpho-syntactic analysers based on the
Xerox Incremental Parser (XIP), a deep robust
syntactic parser (Ait-Mokhtar et al., 2002). The
system has been adapted with domain-specific
part of speech disambiguation grammar rules, ac-
cording to the results a linguistic study of the de-
velopment corpus.
</bodyText>
<subsectionHeader confidence="0.974951">
5.3 Named entity recognition
</subsectionHeader>
<bodyText confidence="0.999992705882353">
LangLog uses the Xerox named entity recogni-
tion web service (Brun and Ehrmann, 2009) for
English and French. XIP includes also a named
entity detection component, based on a combina-
tion of lexical information and hand-crafted con-
textual rules. For example, the named entity
recognition system was adapted to handle titles
of portraits, which were frequent in our dataset.
While for other NLP tasks LangLog uses the same
system for every content provider, named entity
recognition is a task that produces better analyses
when it is tailored to the domain of the content.
Because LangLog uses a NER Web service, it is
easy to replace the default NER system with a dif-
ferent one. So if the content provider is interested
in the development of a NER system tailored for
a specific domain, LangLog can accomodate this.
</bodyText>
<subsectionHeader confidence="0.995755">
5.4 Clustering
</subsectionHeader>
<bodyText confidence="0.998392666666667">
We developed two clustering systems: one per-
forms hierarchical clustering, another performs
soft clustering.
</bodyText>
<listItem confidence="0.894526">
• CLUTO: the hierarchical clustering system
</listItem>
<bodyText confidence="0.931448285714286">
relies on CLUTO411, a clustering toolkit.
To understand the main ideas CLUTO is
based on one might consult Zhao and
Karypis (2002). The clustering process pro-
ceeds as follows. First, the set of queries to
be clustered is partitioned in k groups where
k is the number of desired clusters. To do
so, the system uses a partitional clustering
algorithm which finds the k-way clustering
solution making repeated bisections. Then
11http://glaros.dtc.umn.edu/gkhome/views/cluto
the system arranges the clusters in a hierar-
chy by successively merging the most similar
clusters in a tree.
</bodyText>
<listItem confidence="0.9329365">
• MALLET: the soft clustering system we
developed relies on MALLET (McCallum,
2002), a Latent Dirichlet Allocation (LDA)
toolkit (Steyvers and Griffiths, 2007).
</listItem>
<bodyText confidence="0.999778111111111">
Our MALLET-based system considers that
each query is a document and builds a topic
model describing the documents. The result-
ing topics are the clusters. Each query is as-
sociated with each topic according to a cer-
tain strenght. Unlike the system based on
CLUTO, this system produces soft clusters,
i.e. each query may belong to more than one
cluster.
</bodyText>
<subsectionHeader confidence="0.957288">
5.5 Classification
</subsectionHeader>
<bodyText confidence="0.999954923076923">
LangLog allows the same query to be classified
many times using different classification schemas
and different classification strategies. The result
of the classification of an input query is always a
map that assigns each category a weight, where
the higher the weight, the more likely the query
belongs to the category. If NER performs bet-
ter when tailored to a specific domain, classifi-
cation is a task that is hardly useful without any
customization. We need a different classification
schema for each content provider. We developed
two classification system: an unsupervised sys-
tem and a supervised one.
</bodyText>
<listItem confidence="0.85975">
• Unsupervised: this system does not require
</listItem>
<bodyText confidence="0.984695222222222">
any training data nor any domain-specific
corpus. The output weight of each category
is computed as the cosine similarity between
the vector models of the most representa-
tive Wikipedia article for the category and
the collection of Wikipedia articles most rel-
evant to the input query. Our evaluation in
the KDD-Cup 2005 dataset results in 19.14
precision and 22.22 F-measure. For com-
parison, the state of the art in the competi-
tion achieved a 46.1 F-measure. Our system
could not achieve a similar score because it
is unsupervised, and therefore it cannot make
use of the KDD-Cup training dataset. In ad-
dition, it uses only the query to perform clas-
sification, whereas KDD-Cup systems were
also able to access the result sets associated
to the queries.
</bodyText>
<page confidence="0.989161">
90
</page>
<listItem confidence="0.966312">
• Supervised: this system is based on the
</listItem>
<bodyText confidence="0.9939055625">
Weka framework. Therefore it can use any
machine learning algorithm implemented in
Weka. It uses features derived from the
queries and from Bridgeman metadata. We
trained a Naive Bayes classifier on a set of
15.000 queries annotated with 55 categories
and hits and obtained a F-measure of 0.26.
The results obtained for the classification
are encouraging but not yet at the level of
the state of the art. The main reason for
this is the use of only in-house meta-data in
the feature computation. In the future we
will improve both components by providing
them with features from large resources like
Wikipedia or exploiting the results returned
by Web Searching engines.
</bodyText>
<sectionHeader confidence="0.999523" genericHeader="method">
6 Demonstration
</sectionHeader>
<bodyText confidence="0.972273">
Our demonstration presents:
</bodyText>
<listItem confidence="0.933249727272727">
• The setting of our case study: the Bridgeman
Art Library website, a typical user search,
and what is recorded in the log file.
• The conceptual model of the results of the
analyses: search episodes, queries, lemmas,
named entities, classification, clustering.
• The data flow across the parts of the system,
from content provider’s servers to the front-
end through databases, NLP Web services
and data marts.
• The result of the analyses via QlikView.
</listItem>
<sectionHeader confidence="0.996968" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998936923076923">
In this paper we presented the LangLog system,
a customizable system for analyzing query logs.
The LangLog performs language identification,
lemmatization, NER, classification and clustering
for query logs. We tested the LangLog system on
queries in Bridgeman Library Art. In the future
we will test the system on query logs in differ-
ent domains (e.g. pharmaceutical, hardware and
software, etc.) thus increasing the coverage and
the significance of the results. Moreover we will
incorporate in our system the session information
which should increase the precision of both clus-
tering and classification components.
</bodyText>
<sectionHeader confidence="0.996289" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998746705882353">
Salah Ait-Mokhtar, Jean-Pierre Chanod and Claude
Roux 2002. Robustness Beyond Shallowness: In-
cremental Deep Parsing. Journal of Natural Lan-
guage Engineering 8, 2-3, 121-144.
Alessio Bosca and Luca Dini. 2010. Language Identi-
fication Strategies for Cross Language Information
Retrieval. CLEF 2010 Working Notes.
C. Brun and M. Ehrmann. 2007. Adaptation of
a Named Entity Recognition System for the ES-
TER 2 Evaluation Campaign. In proceedings of
the IEEE International Conference on Natural Lan-
guage Processing and Knowledge Engineering.
F. M. Facca and P. L. Lanzi. 2005. Mining interesting
knowledge from weblogs: a survey. Data Knowl.
Eng. 53(3):225241.
Jansen, B. J. 2006. Search log analysis: What is it;
what’s been done; how to do it. Library and Infor-
mation Science Research 28(3):407-432.
Jansen, B. J. 2008. The methodology of search log
analysis. In B. J. Jansen, A. Spink and I. Taksa (eds)
Handbook of Web log analysis 100-123. Hershey,
PA: IGI.
Joachims T. 2002. Optimizing search engines us-
ing clickthrough data. In proceedings of the 8th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining 133-142.
M. Li, Y. Zhang, M. Zhu, and M. Zhou. 2006. Ex-
ploring distributional similarity based models for
query spelling correction. In proceedings of In ACL
06: the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting of
the ACL 10251032, 2006.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
C. Monz and M. de Rijke. 2002. Shallow Morpholog-
ical Analysis in Monolingual Information Retrieval
for Dutch, German and Italian. In Proceedings of
CLEF 2001. Springer
M. Steyvers and T. Griffiths. 2007. Probabilistic
Topic Models. In T. Landauer, D McNamara, S.
Dennis and W. Kintsch (eds), Handbook of Latent
Semantic Analysis, Psychology Press.
J. R. Wen and H.J. Zhang 2003. Query Clustering
in the Web Context. In Wu, Xiong and Shekhar
(eds) Information Retrieval and Clustering 195-
226. Kluwer Academic Publishers.
Y. Zhao and G. Karypis. 2002. Evaluation of hierar-
chical clustering algorithms for document datasets.
In proceedings of the ACM Conference on Informa-
tion and Knowledge Management.
</reference>
<page confidence="0.999151">
91
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.091417">
<title confidence="0.999585">Query log analysis with GALATEAS LangLog</title>
<author confidence="0.937634">Trevisan</author>
<email confidence="0.970774">dini@celi.it</email>
<author confidence="0.5492365">Igor Gonetwork</author>
<email confidence="0.974813">i.barsanti@gonetwork.it</email>
<affiliation confidence="0.857958">Eduard Universit`a di</affiliation>
<email confidence="0.969587">eduard.barbu@unitn.it</email>
<author confidence="0.899838">Nikolaos Lagos</author>
<affiliation confidence="0.99203">Xerox Research Centre</affiliation>
<email confidence="0.972834">Nikolaos.Lagos@xrce.xerox.com</email>
<note confidence="0.4605785">Segond Objet</note>
<email confidence="0.989608">mruhlmann@objetdirect.com</email>
<author confidence="0.9934785">Ed Vald Bridgeman Art</author>
<email confidence="0.996705">ed.vald@bridgemanart.co.uk</email>
<abstract confidence="0.9966015">This article describes GALATEAS LangLog, a system performing Search Log Analysis. LangLog illustrates how NLP technologies can be a powerful support tool for market research even when the source of information is a collection of queries each one consisting of few words. We push the standard Search Log Analysis forward taking into account the semantics of the queries. The main innovation of LangLog is the implementation of two highly customizable components that cluster and classify the queries in the log.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Salah Ait-Mokhtar</author>
</authors>
<title>Jean-Pierre Chanod and Claude Roux 2002. Robustness Beyond Shallowness: Incremental Deep Parsing.</title>
<journal>Journal of Natural Language Engineering</journal>
<volume>8</volume>
<pages>2--3</pages>
<marker>Ait-Mokhtar, </marker>
<rawString>Salah Ait-Mokhtar, Jean-Pierre Chanod and Claude Roux 2002. Robustness Beyond Shallowness: Incremental Deep Parsing. Journal of Natural Language Engineering 8, 2-3, 121-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessio Bosca</author>
<author>Luca Dini</author>
</authors>
<title>Language Identification Strategies for Cross Language Information Retrieval.</title>
<date>2010</date>
<publisher>CLEF</publisher>
<contexts>
<context position="11124" citStr="Bosca and Dini, 2010" startWordPosition="1698" endWordPosition="1701">cing the data along many dimensions. JPivot allows the user to display charts, export results to Microsoft Excel or CSV, and use custom OLAP MDX queries. Log analysis deals with the anaysis of the data. The analyses proper are executed by NLP systems provided by third parties and accessible as Web services. LangLog uses NLP Web services for language identification, morpho-syntactic analysis, named entity recognition, classification and clustering. The analyses are stored in the database along with the original data. 5.1 Language Identification The system uses a language identification system (Bosca and Dini, 2010) which offers language identification for English, French, Italian, Spanish, Polish and German. The system uses four different strategies: • N-gram character models: uses the distance between the character based models of the input and of a reference corpus for the language (Wikipedia). • Word frequency: looks up the frequency of the words in the query with respect to a reference corpus for the language. • Function words: searches for particles highly connoting a specific language (such as prepositions, conjunctions). 8http://www.qlikview.com 9http://jpivot.sourceforge.net 10http://mondrian.pe</context>
</contexts>
<marker>Bosca, Dini, 2010</marker>
<rawString>Alessio Bosca and Luca Dini. 2010. Language Identification Strategies for Cross Language Information Retrieval. CLEF 2010 Working Notes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brun</author>
<author>M Ehrmann</author>
</authors>
<title>Adaptation of a Named Entity Recognition System for the ESTER 2 Evaluation Campaign.</title>
<date>2007</date>
<booktitle>In proceedings of the IEEE International Conference on Natural Language Processing and Knowledge Engineering.</booktitle>
<marker>Brun, Ehrmann, 2007</marker>
<rawString>C. Brun and M. Ehrmann. 2007. Adaptation of a Named Entity Recognition System for the ESTER 2 Evaluation Campaign. In proceedings of the IEEE International Conference on Natural Language Processing and Knowledge Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Facca</author>
<author>P L Lanzi</author>
</authors>
<title>Mining interesting knowledge from weblogs: a survey.</title>
<date>2005</date>
<journal>Data Knowl. Eng.</journal>
<volume>53</volume>
<issue>3</issue>
<contexts>
<context position="3805" citStr="Facca and Lanzi (2005)" startWordPosition="568" endWordPosition="571">reviews some systems performing SLA. Then we present the data sources the architecture and the analysis process of the LangLog system. The conclusion section concludes the article summarizing the work and presenting some new possible enhancements of the LangLog. 87 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 87–91, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics 2 Related work The information in the log files is useful in many ways, but its extraction raises many challenges and issues. Facca and Lanzi (2005) offer a survey of the topic. There are several commercial systems to extract and analyze this information, such as Adobe web analytics1, SAS Web Analytics2, Infor Epiphany3, IBM SPSS4. These products are often part of a customer relation management (CRM) system. None of those showcases include any form of linguistic processing. On the other hand, Web queries have been the subject of linguistic analysis, to improve the performance of information retrieval systems. For example, a study (Monz and de Rijke, 2002) experimented with shallow morphological analysis, another (Li et al., 2006) analyzed</context>
</contexts>
<marker>Facca, Lanzi, 2005</marker>
<rawString>F. M. Facca and P. L. Lanzi. 2005. Mining interesting knowledge from weblogs: a survey. Data Knowl. Eng. 53(3):225241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Jansen</author>
</authors>
<title>Search log analysis: What is it; what’s been done; how to do it.</title>
<date>2006</date>
<journal>Library and Information Science Research</journal>
<pages>28--3</pages>
<contexts>
<context position="1163" citStr="Jansen, 2006" startWordPosition="152" endWordPosition="153">rates how NLP technologies can be a powerful support tool for market research even when the source of information is a collection of queries each one consisting of few words. We push the standard Search Log Analysis forward taking into account the semantics of the queries. The main innovation of LangLog is the implementation of two highly customizable components that cluster and classify the queries in the log. 1 Introduction Transaction logs become increasingly important for studying the user interaction with systems like Web Searching Engines, Digital Libraries, Intranet Servers and others (Jansen, 2006). Various service providers keep log files recording the user interaction with the searching engines. Transaction logs are useful to understand the user search strategy but also to improve query suggestions (Wen and Zhang, 2003) and to enhance the retrieval quality of search engines (Joachims, 2002). The process of analyzing the transaction logs to understand the user behaviour and to assess the system performance is known as Transaction Log Analysis (TLA). Transaction Log Analysis is concerned with the analysis of both browsing and searching activity inside a website. The analysis of transact</context>
</contexts>
<marker>Jansen, 2006</marker>
<rawString>Jansen, B. J. 2006. Search log analysis: What is it; what’s been done; how to do it. Library and Information Science Research 28(3):407-432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Jansen</author>
</authors>
<title>The methodology of search log analysis. In</title>
<date>2008</date>
<publisher>IGI.</publisher>
<location>Hershey, PA:</location>
<contexts>
<context position="1874" citStr="Jansen (2008)" startWordPosition="267" endWordPosition="268">es. Transaction logs are useful to understand the user search strategy but also to improve query suggestions (Wen and Zhang, 2003) and to enhance the retrieval quality of search engines (Joachims, 2002). The process of analyzing the transaction logs to understand the user behaviour and to assess the system performance is known as Transaction Log Analysis (TLA). Transaction Log Analysis is concerned with the analysis of both browsing and searching activity inside a website. The analysis of transaction logs that focuses on search activity only is known as Search Log Analysis (SLA). According to Jansen (2008) both TLA and SLA have three stages: data collection, data preparation and data analysis. In the data collection stage one collects data describing the user interaction with the system. Data preparation is the process of loading the collected data in a relational database. The data loaded in the database gives a transaction log representation independent of the particular log syntax. In the final stage the data prepared at the previous step is analyzed. One may notice that the traditional three levels log analyses give a syntactic view of the information in the logs. Counting terms, measuring </context>
</contexts>
<marker>Jansen, 2008</marker>
<rawString>Jansen, B. J. 2008. The methodology of search log analysis. In B. J. Jansen, A. Spink and I. Taksa (eds) Handbook of Web log analysis 100-123. Hershey, PA: IGI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In proceedings of the 8th ACM SIGKDD international conference on Knowledge discovery and data mining</booktitle>
<pages>133--142</pages>
<contexts>
<context position="1463" citStr="Joachims, 2002" startWordPosition="199" endWordPosition="200">angLog is the implementation of two highly customizable components that cluster and classify the queries in the log. 1 Introduction Transaction logs become increasingly important for studying the user interaction with systems like Web Searching Engines, Digital Libraries, Intranet Servers and others (Jansen, 2006). Various service providers keep log files recording the user interaction with the searching engines. Transaction logs are useful to understand the user search strategy but also to improve query suggestions (Wen and Zhang, 2003) and to enhance the retrieval quality of search engines (Joachims, 2002). The process of analyzing the transaction logs to understand the user behaviour and to assess the system performance is known as Transaction Log Analysis (TLA). Transaction Log Analysis is concerned with the analysis of both browsing and searching activity inside a website. The analysis of transaction logs that focuses on search activity only is known as Search Log Analysis (SLA). According to Jansen (2008) both TLA and SLA have three stages: data collection, data preparation and data analysis. In the data collection stage one collects data describing the user interaction with the system. Dat</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Joachims T. 2002. Optimizing search engines using clickthrough data. In proceedings of the 8th ACM SIGKDD international conference on Knowledge discovery and data mining 133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Li</author>
<author>Y Zhang</author>
<author>M Zhu</author>
<author>M Zhou</author>
</authors>
<title>Exploring distributional similarity based models for query spelling correction.</title>
<date>2006</date>
<booktitle>In proceedings of In ACL 06: the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL 10251032,</booktitle>
<contexts>
<context position="4396" citStr="Li et al., 2006" startWordPosition="663" endWordPosition="666">s. Facca and Lanzi (2005) offer a survey of the topic. There are several commercial systems to extract and analyze this information, such as Adobe web analytics1, SAS Web Analytics2, Infor Epiphany3, IBM SPSS4. These products are often part of a customer relation management (CRM) system. None of those showcases include any form of linguistic processing. On the other hand, Web queries have been the subject of linguistic analysis, to improve the performance of information retrieval systems. For example, a study (Monz and de Rijke, 2002) experimented with shallow morphological analysis, another (Li et al., 2006) analyzed queries to remove spelling mistakes. These works encourage our belief that linguistic analysis could be beneficial for Web log analysis systems. 3 Data sources LangLog requires the following information from the Web logs: the time of the interaction, the query, click-through information and possibly more. LangLog processes log files which conform to the W3C extended log format. No other formats are supported. The system prototype is based on query logs spanning one month of interactions recorded at the Bridgeman Art Library5. Bridgeman Art library contains a large repository of image</context>
</contexts>
<marker>Li, Zhang, Zhu, Zhou, 2006</marker>
<rawString>M. Li, Y. Zhang, M. Zhu, and M. Zhou. 2006. Exploring distributional similarity based models for query spelling correction. In proceedings of In ACL 06: the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL 10251032, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<location>http://mallet.cs.umass.edu.</location>
<contexts>
<context position="13897" citStr="McCallum, 2002" startWordPosition="2123" endWordPosition="2124">erstand the main ideas CLUTO is based on one might consult Zhao and Karypis (2002). The clustering process proceeds as follows. First, the set of queries to be clustered is partitioned in k groups where k is the number of desired clusters. To do so, the system uses a partitional clustering algorithm which finds the k-way clustering solution making repeated bisections. Then 11http://glaros.dtc.umn.edu/gkhome/views/cluto the system arranges the clusters in a hierarchy by successively merging the most similar clusters in a tree. • MALLET: the soft clustering system we developed relies on MALLET (McCallum, 2002), a Latent Dirichlet Allocation (LDA) toolkit (Steyvers and Griffiths, 2007). Our MALLET-based system considers that each query is a document and builds a topic model describing the documents. The resulting topics are the clusters. Each query is associated with each topic according to a certain strenght. Unlike the system based on CLUTO, this system produces soft clusters, i.e. each query may belong to more than one cluster. 5.5 Classification LangLog allows the same query to be classified many times using different classification schemas and different classification strategies. The result of </context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Monz</author>
<author>M de Rijke</author>
</authors>
<title>Shallow Morphological Analysis in Monolingual Information Retrieval for Dutch, German and Italian.</title>
<date>2002</date>
<booktitle>In Proceedings of CLEF</booktitle>
<publisher>Springer</publisher>
<marker>Monz, de Rijke, 2002</marker>
<rawString>C. Monz and M. de Rijke. 2002. Shallow Morphological Analysis in Monolingual Information Retrieval for Dutch, German and Italian. In Proceedings of CLEF 2001. Springer</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steyvers</author>
<author>T Griffiths</author>
</authors>
<title>Probabilistic Topic Models. In</title>
<date>2007</date>
<publisher>Psychology Press.</publisher>
<contexts>
<context position="13973" citStr="Steyvers and Griffiths, 2007" startWordPosition="2131" endWordPosition="2134">o and Karypis (2002). The clustering process proceeds as follows. First, the set of queries to be clustered is partitioned in k groups where k is the number of desired clusters. To do so, the system uses a partitional clustering algorithm which finds the k-way clustering solution making repeated bisections. Then 11http://glaros.dtc.umn.edu/gkhome/views/cluto the system arranges the clusters in a hierarchy by successively merging the most similar clusters in a tree. • MALLET: the soft clustering system we developed relies on MALLET (McCallum, 2002), a Latent Dirichlet Allocation (LDA) toolkit (Steyvers and Griffiths, 2007). Our MALLET-based system considers that each query is a document and builds a topic model describing the documents. The resulting topics are the clusters. Each query is associated with each topic according to a certain strenght. Unlike the system based on CLUTO, this system produces soft clusters, i.e. each query may belong to more than one cluster. 5.5 Classification LangLog allows the same query to be classified many times using different classification schemas and different classification strategies. The result of the classification of an input query is always a map that assigns each categ</context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>M. Steyvers and T. Griffiths. 2007. Probabilistic Topic Models. In T. Landauer, D McNamara, S. Dennis and W. Kintsch (eds), Handbook of Latent Semantic Analysis, Psychology Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Wen</author>
<author>H J Zhang</author>
</authors>
<title>Query Clustering in the Web Context.</title>
<date>2003</date>
<booktitle>In Wu, Xiong and Shekhar (eds) Information Retrieval and Clustering</booktitle>
<pages>195--226</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1391" citStr="Wen and Zhang, 2003" startWordPosition="186" endWordPosition="189">rd taking into account the semantics of the queries. The main innovation of LangLog is the implementation of two highly customizable components that cluster and classify the queries in the log. 1 Introduction Transaction logs become increasingly important for studying the user interaction with systems like Web Searching Engines, Digital Libraries, Intranet Servers and others (Jansen, 2006). Various service providers keep log files recording the user interaction with the searching engines. Transaction logs are useful to understand the user search strategy but also to improve query suggestions (Wen and Zhang, 2003) and to enhance the retrieval quality of search engines (Joachims, 2002). The process of analyzing the transaction logs to understand the user behaviour and to assess the system performance is known as Transaction Log Analysis (TLA). Transaction Log Analysis is concerned with the analysis of both browsing and searching activity inside a website. The analysis of transaction logs that focuses on search activity only is known as Search Log Analysis (SLA). According to Jansen (2008) both TLA and SLA have three stages: data collection, data preparation and data analysis. In the data collection stag</context>
</contexts>
<marker>Wen, Zhang, 2003</marker>
<rawString>J. R. Wen and H.J. Zhang 2003. Query Clustering in the Web Context. In Wu, Xiong and Shekhar (eds) Information Retrieval and Clustering 195-226. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhao</author>
<author>G Karypis</author>
</authors>
<title>Evaluation of hierarchical clustering algorithms for document datasets.</title>
<date>2002</date>
<booktitle>In proceedings of the ACM Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="13364" citStr="Zhao and Karypis (2002)" startWordPosition="2040" endWordPosition="2043"> that produces better analyses when it is tailored to the domain of the content. Because LangLog uses a NER Web service, it is easy to replace the default NER system with a different one. So if the content provider is interested in the development of a NER system tailored for a specific domain, LangLog can accomodate this. 5.4 Clustering We developed two clustering systems: one performs hierarchical clustering, another performs soft clustering. • CLUTO: the hierarchical clustering system relies on CLUTO411, a clustering toolkit. To understand the main ideas CLUTO is based on one might consult Zhao and Karypis (2002). The clustering process proceeds as follows. First, the set of queries to be clustered is partitioned in k groups where k is the number of desired clusters. To do so, the system uses a partitional clustering algorithm which finds the k-way clustering solution making repeated bisections. Then 11http://glaros.dtc.umn.edu/gkhome/views/cluto the system arranges the clusters in a hierarchy by successively merging the most similar clusters in a tree. • MALLET: the soft clustering system we developed relies on MALLET (McCallum, 2002), a Latent Dirichlet Allocation (LDA) toolkit (Steyvers and Griffit</context>
</contexts>
<marker>Zhao, Karypis, 2002</marker>
<rawString>Y. Zhao and G. Karypis. 2002. Evaluation of hierarchical clustering algorithms for document datasets. In proceedings of the ACM Conference on Information and Knowledge Management.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>