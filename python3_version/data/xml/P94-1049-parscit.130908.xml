<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010389">
<title confidence="0.981017">
Integration Of Visual Inter-word Constraints And
Linguistic Knowledge In Degraded Text Recognition
</title>
<author confidence="0.998452">
Tao Hong
</author>
<affiliation confidence="0.999568666666667">
Center of Excellence for Document Analysis and Recognition
Department of Computer Science
State University of New York at Buffalo, Buffalo, NY 14260
</affiliation>
<email confidence="0.797218">
taohonecs.buffalo.edu
</email>
<sectionHeader confidence="0.99542" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998062">
Degraded text recognition is a difficult task. Given a
noisy text image, a word recognizer can be applied to
generate several candidates for each word image. High-
level knowledge sources can then be used to select a
decision from the candidate set for each word image.
In this paper, we propose that visual inter-word con-
straints can be used to facilitate candidate selection.
Visual inter-word constraints provide a way to link word
images inside the text page, and to interpret them sys-
tematically.
</bodyText>
<sectionHeader confidence="0.975512" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.994853161290322">
The objective of visual text recognition is to transform
an arbitrary image of text into its symbolic equivalent
correctly. Recent technical advances in the area of doc-
ument recognition have made automatic text recogni-
tion a viable alternative to manual key entry. Given a
high quality text page, a commercial document recog-
nition system can recognize the words on the page at
a high correct rate. However, given a degraded text
page, such as a multiple-generation photocopy or fac-
simile, performance usually drops abruptly([1]).
Given a degraded text image, word images can be ex-
tracted after layout analysis. A word image from a de-
graded text page may have touching characters, broken
characters, distorted or blurred characters, which may
make the word image difficult to recognize accurately.
After character recognition and correction based on dic-
tionary look-up, a word recognizer will provide one or
more word candidates for each word image. Figure 1
lists the word candidate sets for the sentence, &amp;quot;Please
fill in the application form.&amp;quot; Each word candidate has
a confidence score, but the score may not be reliable
because of noise in the image. The correct word candi-
date is usually in the candidate set, but may not be the
candidate with the highest confidence score. Instead of
simply picking up the word candidate with the high-
est recognition score, which may make the correct rate
quite low, we need to find a method which can select a
candidate for each word image so that the correct rate
can be as high as possible.
Contextual information and high-level knowledge can
be used to select a decision word for each word image
</bodyText>
<table confidence="0.991997090909091">
1 2 3 4 5 6 7
Please fin in tire application farm !
0.90 0.33 0.30 0.80 0.90 0.35
Fleece fill In toe applicators form
0.05 0.30 0.28 0.10 0.05 0.30
Pierce flu lo lire acquisition forth
0.02 0.21 0.25 0.05 0.03 0.20
Fierce flit ill the duplication foam
0.02 0.10 0.13 0.03 0.01 0.11
Pieces till Io Ike implication force
0.01 0.06 0.04 0.02 0.01 0.04
</table>
<figureCaption confidence="0.9671605">
Figure 1: Candidate Sets for the Sentence: &amp;quot;Please fill
in the application form!&amp;quot;
</figureCaption>
<bodyText confidence="0.999930379310345">
in its context. Currently, there are two approaches,
the statistical approach and the structural approach,
towards the problem of candidate selection. In the sta-
tistical approach, language models, such as a Hidden
Markov Model and word collocation can be utilized for
candidate selection ([2, 4, 5]). In the structural ap-
proach, lattice parsing techniques have been developed
for candidate selection([3, 7]).
The contextual constraints considered in a statisti-
cal language model, such as word collocation, are local
constraints. For a word image, a candidate will be se-
lected according to the candidate information from its
neighboring word images in a fixed window size. The
window size is usually set as one or two. In the lattice
parsing method, a grammar is used to select a candi-
date for each word image inside a sentence so that the
sequence of those selected candidates form a grammat-
ical and meaningful sentence. For example, consider
the sentence &amp;quot;Please fill in the application form&amp;quot;. We
assume all words except the word &amp;quot;form&amp;quot; have been
recognized correctly and the candidate set for the word
&amp;quot;form&amp;quot; is { farm, form, forth, foam, forth } (see the
second sentence in Figure 2). The candidate &amp;quot;form&amp;quot;
can be selected easily because the collocation between
&amp;quot;application&amp;quot; and &amp;quot;form&amp;quot; is strong and the resulting
sentence is grammatical.
The contextual information inside a small window or
inside a sentence sometimes may not be enough to select
a candidate correctly. For example, consider the sen-
</bodyText>
<page confidence="0.968018">
328
</page>
<bodyText confidence="0.913217">
Sentence 1
1 2 3 4 5 6 7 8 9 10
This farm is almost the same as that one
form
forth
foam
force
Sentence 2
11 12 13 14 15 16 17
Please fill in the application farm !
form
forth
foam
force
</bodyText>
<figureCaption confidence="0.9981265">
Figure 2: Word candidates of two example sen-
tences(word images 2 and 16 are similar)
</figureCaption>
<bodyText confidence="0.994605333333333">
skill; it Ls tiologicany.based. Language
is something we (11*born,n3owing
how • to 1121.0- W. • Yet9I-1 by pc7thasis that
There biokkic—al underpinnings to
human linguistic ability does not ex-
plain everything. .There may indeed
</bodyText>
<figure confidence="0.8968285">
co
versa] elements. All,knowt languages
0
sh(are)certain. organtzabonal principles.
</figure>
<figureCaption confidence="0.999861">
Figure 3: Part of text page with three sentences
</figureCaption>
<bodyText confidence="0.995547777777778">
tence &amp;quot;This form is almost the same as that one&amp;quot; (see
the first sentence in Figure 2). Word image 16 has five
candidates: { farm, form, forth, foam, forth }. After
lattice parsing, the candidate &amp;quot;forth&amp;quot; will be removed
because it does not fit the context. But it is difficult
to select a candidate from &amp;quot;farm&amp;quot;, &amp;quot;form&amp;quot; &amp;quot;foam&amp;quot; and
&amp;quot;force&amp;quot; because each of them makes the sentence gram-
matical and meaningful. In such a case, more contex-
tual constraints are needed to distinguish the remaining
candidates and to select the correct one.
Let&apos;s further assume that the sentences in Figure 2
are from the same text. By image matching, we know
word images 2 and 16 are visually similar. If two word
images are almost the same, they must be the same
word. Therefore, same candidates must be selected for
word image 2 and word image 16, After &amp;quot;form&amp;quot; is chosen
for image 16 it can also be chosen as the decision for
image 2.
</bodyText>
<subsectionHeader confidence="0.724288">
Possible Relation s between W1, and W2
</subsectionHeader>
<bodyText confidence="0.821681">
type at symbolic level at image level
</bodyText>
<equation confidence="0.8932945">
1 W1 = W2 rz,&apos; W2
2 14/2 = x• W1 • 17 W1 r•Ze, subimage_o f (W2)
</equation>
<figure confidence="0.590707666666667">
3 pre f ix _o f (WO = le ft_part_o f (WO &apos;Aa,
pre f ix _o f (W2) le ft_part_o f (W2)
4 su f fix _o f (WO = right_part_o f (WO r-z•-•
su f f ix _o f (W2) right_part_o f (W2)
5 su f f ix _o f (WO = right_part_o f (WO
pre f tx_o f (W2) le f t_part_o f (W2)
</figure>
<tableCaption confidence="0.666528">
Note 1: means approximately image matching;
Note 2: &amp;quot;.&amp;quot; means concatenation.
Table 1: Possible Inter-word Relations
</tableCaption>
<subsectionHeader confidence="0.841802">
Visual Inter-Word Relations
</subsectionHeader>
<bodyText confidence="0.999880473684211">
A visual inter-word relation can be defined between two
word images if they share the same pattern at the image
level. There are five types of visual inter-word relations
listed in the right part of Table 1. Figure 3 is a part of
a scanned text image in which a small number of word
relations are circled to demonstrate the abundance of
inter-word relations defined above even in such a small
fragment of a real text page. Word images 2 and 8 are
almost the same. Word image 9 matches the left part
of word image I quite well. Word image 5 matches a
part of the image 6, and so on.
Visual inter-word relations can be computed by ap-
plying simple image matching techniques. They can be
calculated in clean text images, as well as in highly de-
graded text images, because the word images, due to
their relatively large size, are tolerant to noise ([6]).
Visual inter-word relations can be used as constraints
in the process of word image interpretation, especially
for candidate selection. It is not surprising that word
relations at the image level are highly consistent with
word relations at the symbolic level(see Table 1). If two
words hold a relation at the symbolic level and they are
written in the same font and size, their word images
should keep the same relation at the image level. And
also, if two word images hold a relation at the image
level, the truth values of the word images should have
the same relation at the symbolic level. In Figure 3,
word images 2 and 8 must be recognized as the same
word because they can match each other; the identity
of word image 5 must be a sub-string of the identity of
word image 6 because word image 5 can match with a
part of word image 6; and so on.
Visual inter-word constraints provide us a way to link
word images inside a text page, and to interpret them
systematically. The research discussed in this paper in-
tegrates visual inter-word constraints with a statistical
language model and a lattice parser to improve the per-
formance of candidate selection.
</bodyText>
<page confidence="0.996651">
329
</page>
<table confidence="0.999316363636364">
Article Number Word Candidate Selection
Of Recognition
Words Result
Using No Using
Constraints Constraints
A06 2213 53.8% 83.1% 88.5%
G02 2267 67.7% 83.8% 87.8%
J42 2269 54.5% 83.6% 89.5%
NO1 2313 57.3% 82.7% 87.1%
R07 2340 52.2% 82.6% 88.1%
Total 11402 57.1% 83.1% 88.2%
</table>
<tableCaption confidence="0.995107">
Table 2: Comparison Of Candidate Selection Results
</tableCaption>
<subsectionHeader confidence="0.982281">
Current Status of Work
</subsectionHeader>
<bodyText confidence="0.999992720930233">
A word-collocation-based relaxation algorithm and
a probabilistic lattice chart parser have been de-
signed for word candidate selection in degraded text
recognition([3, 4]). The relaxation algorithm runs iter-
atively. In each iteration, the confidence score of each
candidate is adjusted based on its current confidence
and its collocation scores with the currently most pre-
ferred candidates for its neighboring word images. Re-
laxation ends when all candidates reach a stable state.
For each word image, those candidates with a low con-
fidence score will be removed from the candidate sets.
Then, the probabilistic lattice chart parser will be ap-
plied to the reduced candidate sets to select the can-
didates that appear in the most preferred parse trees
built by the parser. There can be different strategies to
use visual inter-word constraints inside the relaxation
algorithm and the lattice parser. One of the strategies
we are exploiting is to re-evaluate the top candidates
for the related word images after each iteration of re-
laxation or after lattice parsing. If they hold the same
relation at the symbolic level, the confidence scores of
the candidates will be increased. Otherwise, the images
with a low confidence score will follow the decision of
the images with a high confidence score.
Five articles from the Brown Corpus were chosen ran-
domly as testing samples. They are A06, G02,142, NO1
and R07, each with about 2,000 words. Given a word
image, our word recognizer generates its top10 candi-
dates from a dictionary with 70,000 different entries.
In preliminary experiments, we exploit only the type-1
relation listed in Table 1. After clustering word im-
ages by image matching, similar images will be in the
same cluster. Any two images from the same cluster
hold the type-1 relation. Word collocation data were
trained from the Penn Treebank and the Brown Cor-
pus except for the five testing samples. Table 2 shows
results of candidate selection with and without using
visual inter-word constraints. The topl correct rate for
candidate lists generated by a word recognizer is as low
as 57.1%, Without using visual inter-word constraints,
the correct rate of candidate selection by relaxation and
lattice parsing is 83.1%. After using visual inter-word
constraints, the correct rate becomes 88.2%.
</bodyText>
<subsectionHeader confidence="0.562979">
Conclusions and Future Directions
</subsectionHeader>
<bodyText confidence="0.999991428571429">
Integration of natural language processing and image
processing is a new area of interest in document anal-
ysis. Word candidate selection is a problem we are
faced with in degraded text recognition, as well as in
handwriting recognition. Statistical language models
and lattice parsers have been designed for the prob-
lem. Visual inter-word constraints in a text page can
be used with linguistic knowledge sources to facilitate
candidate selection. Preliminary experimental results
show that the performance of candidate selection is im-
proved significantly although only one inter-word rela-
tion was used. The next step is to fully integrate visual
inter-word constraints and linguistic knowledge sources
in the relaxation algorithm and the lattice parser.
</bodyText>
<sectionHeader confidence="0.999022" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999844">
I would like to thank Jonathan J. Hull for his support
and his helpful comments on drafts of this paper.
</bodyText>
<sectionHeader confidence="0.99921" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99992628125">
[1] Henry S. Baird, &amp;quot;Document Image Defect Models
and Their Uses,&amp;quot; in Proceedings of the Second In-
ternational Conference on Document Analysis and
Recognition ICDAR-93, Tsukuba, Japan, October
20-22, 1993, pp. 62-67.
[2] Kenneth Ward Church and Patrick Hanks, &amp;quot;Word
Association Norms, Mutual Information, and Lexi-
cography,&amp;quot; Computational Linguistics, Vol. 16, No.
1, pp. 22-29, 1990.
[3] Tao Hong and Jonathan J. Hull, &amp;quot;Text Recognition
Enhancement with a Probabilistic Lattice Chart
Parser,&amp;quot; in Proceedings of the Second International
Conference on Document Analysis and Recognition
ICDAR-93, Tsukuba, Japan, October 20-22, 1993.
[4] Tao Hong and Jonathan J. Hull, &amp;quot;Degraded Text
Recognition Using Word Collocation,&amp;quot; in Pro-
ceedings of IS&amp;T/SPIE Symposium on Document
Recognition, San Jose, CA, February 6-10, 1994.
[5] Jonathan J. Hull, &amp;quot;A Hidden Markov Model for
Language Syntax in Text Recognition,&amp;quot; in Pro-
ceedings of 11th IAPR International Conference on
Pattern Recognition, The Hague, The Netherlands,
pp.124-12&apos;7, 1992.
[6] Siamak Khoubyari and Jonathan J. Hull, &amp;quot;Keyword
Location in Noisy Document Image,&amp;quot; in Proceed-
ings of the Second Annual Symposium on Docu-
ment Analysis and Information Retrieval, Las Ve-
gas, Nevada, pp. 217-231, April 26-28, 1993.
[7] Masaru Tomita, &amp;quot;An Efficient Word Lattice Pars-
ing Algorithm for Continuous Speech Recognition,&amp;quot;
in Proceedings of the International Conference on
Acoustic, Speech and Signal Processing, 1986.
</reference>
<page confidence="0.998387">
330
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.685527">
<title confidence="0.998278">Integration Of Visual Inter-word Constraints And Linguistic Knowledge In Degraded Text Recognition</title>
<author confidence="0.999915">Tao Hong</author>
<affiliation confidence="0.957955">Center of Excellence for Document Analysis and Recognition Department of Computer Science State University of New York at Buffalo, Buffalo, NY 14260</affiliation>
<email confidence="0.997062">taohonecs.buffalo.edu</email>
<abstract confidence="0.980678272727273">Degraded text recognition is a difficult task. Given a noisy text image, a word recognizer can be applied to generate several candidates for each word image. Highlevel knowledge sources can then be used to select a decision from the candidate set for each word image. In this paper, we propose that visual inter-word constraints can be used to facilitate candidate selection. Visual inter-word constraints provide a way to link word images inside the text page, and to interpret them systematically.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Henry S Baird</author>
</authors>
<title>Document Image Defect Models and Their Uses,&amp;quot;</title>
<date>1993</date>
<booktitle>in Proceedings of the Second International Conference on Document Analysis and Recognition ICDAR-93,</booktitle>
<pages>62--67</pages>
<location>Tsukuba, Japan,</location>
<contexts>
<context position="1328" citStr="[1]" startWordPosition="205" endWordPosition="205">es inside the text page, and to interpret them systematically. Introduction The objective of visual text recognition is to transform an arbitrary image of text into its symbolic equivalent correctly. Recent technical advances in the area of document recognition have made automatic text recognition a viable alternative to manual key entry. Given a high quality text page, a commercial document recognition system can recognize the words on the page at a high correct rate. However, given a degraded text page, such as a multiple-generation photocopy or facsimile, performance usually drops abruptly([1]). Given a degraded text image, word images can be extracted after layout analysis. A word image from a degraded text page may have touching characters, broken characters, distorted or blurred characters, which may make the word image difficult to recognize accurately. After character recognition and correction based on dictionary look-up, a word recognizer will provide one or more word candidates for each word image. Figure 1 lists the word candidate sets for the sentence, &amp;quot;Please fill in the application form.&amp;quot; Each word candidate has a confidence score, but the score may not be reliable beca</context>
</contexts>
<marker>[1]</marker>
<rawString>Henry S. Baird, &amp;quot;Document Image Defect Models and Their Uses,&amp;quot; in Proceedings of the Second International Conference on Document Analysis and Recognition ICDAR-93, Tsukuba, Japan, October 20-22, 1993, pp. 62-67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word Association Norms, Mutual Information, and Lexicography,&amp;quot;</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<pages>22--29</pages>
<contexts>
<context position="3168" citStr="[2, 4, 5]" startWordPosition="520" endWordPosition="522">05 0.30 0.28 0.10 0.05 0.30 Pierce flu lo lire acquisition forth 0.02 0.21 0.25 0.05 0.03 0.20 Fierce flit ill the duplication foam 0.02 0.10 0.13 0.03 0.01 0.11 Pieces till Io Ike implication force 0.01 0.06 0.04 0.02 0.01 0.04 Figure 1: Candidate Sets for the Sentence: &amp;quot;Please fill in the application form!&amp;quot; in its context. Currently, there are two approaches, the statistical approach and the structural approach, towards the problem of candidate selection. In the statistical approach, language models, such as a Hidden Markov Model and word collocation can be utilized for candidate selection ([2, 4, 5]). In the structural approach, lattice parsing techniques have been developed for candidate selection([3, 7]). The contextual constraints considered in a statistical language model, such as word collocation, are local constraints. For a word image, a candidate will be selected according to the candidate information from its neighboring word images in a fixed window size. The window size is usually set as one or two. In the lattice parsing method, a grammar is used to select a candidate for each word image inside a sentence so that the sequence of those selected candidates form a grammatical an</context>
</contexts>
<marker>[2]</marker>
<rawString>Kenneth Ward Church and Patrick Hanks, &amp;quot;Word Association Norms, Mutual Information, and Lexicography,&amp;quot; Computational Linguistics, Vol. 16, No. 1, pp. 22-29, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Hong</author>
<author>Jonathan J Hull</author>
</authors>
<title>Text Recognition Enhancement with a Probabilistic Lattice Chart Parser,&amp;quot;</title>
<date>1993</date>
<booktitle>in Proceedings of the Second International Conference on Document Analysis and Recognition ICDAR-93,</booktitle>
<location>Tsukuba, Japan,</location>
<contexts>
<context position="3276" citStr="[3, 7]" startWordPosition="536" endWordPosition="537">the duplication foam 0.02 0.10 0.13 0.03 0.01 0.11 Pieces till Io Ike implication force 0.01 0.06 0.04 0.02 0.01 0.04 Figure 1: Candidate Sets for the Sentence: &amp;quot;Please fill in the application form!&amp;quot; in its context. Currently, there are two approaches, the statistical approach and the structural approach, towards the problem of candidate selection. In the statistical approach, language models, such as a Hidden Markov Model and word collocation can be utilized for candidate selection ([2, 4, 5]). In the structural approach, lattice parsing techniques have been developed for candidate selection([3, 7]). The contextual constraints considered in a statistical language model, such as word collocation, are local constraints. For a word image, a candidate will be selected according to the candidate information from its neighboring word images in a fixed window size. The window size is usually set as one or two. In the lattice parsing method, a grammar is used to select a candidate for each word image inside a sentence so that the sequence of those selected candidates form a grammatical and meaningful sentence. For example, consider the sentence &amp;quot;Please fill in the application form&amp;quot;. We assume a</context>
<context position="8992" citStr="[3, 4]" startWordPosition="1555" endWordPosition="1556">anguage model and a lattice parser to improve the performance of candidate selection. 329 Article Number Word Candidate Selection Of Recognition Words Result Using No Using Constraints Constraints A06 2213 53.8% 83.1% 88.5% G02 2267 67.7% 83.8% 87.8% J42 2269 54.5% 83.6% 89.5% NO1 2313 57.3% 82.7% 87.1% R07 2340 52.2% 82.6% 88.1% Total 11402 57.1% 83.1% 88.2% Table 2: Comparison Of Candidate Selection Results Current Status of Work A word-collocation-based relaxation algorithm and a probabilistic lattice chart parser have been designed for word candidate selection in degraded text recognition([3, 4]). The relaxation algorithm runs iteratively. In each iteration, the confidence score of each candidate is adjusted based on its current confidence and its collocation scores with the currently most preferred candidates for its neighboring word images. Relaxation ends when all candidates reach a stable state. For each word image, those candidates with a low confidence score will be removed from the candidate sets. Then, the probabilistic lattice chart parser will be applied to the reduced candidate sets to select the candidates that appear in the most preferred parse trees built by the parser.</context>
</contexts>
<marker>[3]</marker>
<rawString>Tao Hong and Jonathan J. Hull, &amp;quot;Text Recognition Enhancement with a Probabilistic Lattice Chart Parser,&amp;quot; in Proceedings of the Second International Conference on Document Analysis and Recognition ICDAR-93, Tsukuba, Japan, October 20-22, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Hong</author>
<author>Jonathan J Hull</author>
</authors>
<title>Degraded Text Recognition Using Word Collocation,&amp;quot;</title>
<date>1994</date>
<booktitle>in Proceedings of IS&amp;T/SPIE Symposium on Document Recognition,</booktitle>
<location>San Jose, CA,</location>
<contexts>
<context position="3168" citStr="[2, 4, 5]" startWordPosition="520" endWordPosition="522">05 0.30 0.28 0.10 0.05 0.30 Pierce flu lo lire acquisition forth 0.02 0.21 0.25 0.05 0.03 0.20 Fierce flit ill the duplication foam 0.02 0.10 0.13 0.03 0.01 0.11 Pieces till Io Ike implication force 0.01 0.06 0.04 0.02 0.01 0.04 Figure 1: Candidate Sets for the Sentence: &amp;quot;Please fill in the application form!&amp;quot; in its context. Currently, there are two approaches, the statistical approach and the structural approach, towards the problem of candidate selection. In the statistical approach, language models, such as a Hidden Markov Model and word collocation can be utilized for candidate selection ([2, 4, 5]). In the structural approach, lattice parsing techniques have been developed for candidate selection([3, 7]). The contextual constraints considered in a statistical language model, such as word collocation, are local constraints. For a word image, a candidate will be selected according to the candidate information from its neighboring word images in a fixed window size. The window size is usually set as one or two. In the lattice parsing method, a grammar is used to select a candidate for each word image inside a sentence so that the sequence of those selected candidates form a grammatical an</context>
<context position="8992" citStr="[3, 4]" startWordPosition="1555" endWordPosition="1556">anguage model and a lattice parser to improve the performance of candidate selection. 329 Article Number Word Candidate Selection Of Recognition Words Result Using No Using Constraints Constraints A06 2213 53.8% 83.1% 88.5% G02 2267 67.7% 83.8% 87.8% J42 2269 54.5% 83.6% 89.5% NO1 2313 57.3% 82.7% 87.1% R07 2340 52.2% 82.6% 88.1% Total 11402 57.1% 83.1% 88.2% Table 2: Comparison Of Candidate Selection Results Current Status of Work A word-collocation-based relaxation algorithm and a probabilistic lattice chart parser have been designed for word candidate selection in degraded text recognition([3, 4]). The relaxation algorithm runs iteratively. In each iteration, the confidence score of each candidate is adjusted based on its current confidence and its collocation scores with the currently most preferred candidates for its neighboring word images. Relaxation ends when all candidates reach a stable state. For each word image, those candidates with a low confidence score will be removed from the candidate sets. Then, the probabilistic lattice chart parser will be applied to the reduced candidate sets to select the candidates that appear in the most preferred parse trees built by the parser.</context>
</contexts>
<marker>[4]</marker>
<rawString>Tao Hong and Jonathan J. Hull, &amp;quot;Degraded Text Recognition Using Word Collocation,&amp;quot; in Proceedings of IS&amp;T/SPIE Symposium on Document Recognition, San Jose, CA, February 6-10, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan J Hull</author>
</authors>
<title>A Hidden Markov Model for Language Syntax in Text Recognition,&amp;quot;</title>
<date>1992</date>
<booktitle>in Proceedings of 11th IAPR International Conference on Pattern Recognition, The Hague, The Netherlands,</booktitle>
<pages>124--12</pages>
<contexts>
<context position="3168" citStr="[2, 4, 5]" startWordPosition="520" endWordPosition="522">05 0.30 0.28 0.10 0.05 0.30 Pierce flu lo lire acquisition forth 0.02 0.21 0.25 0.05 0.03 0.20 Fierce flit ill the duplication foam 0.02 0.10 0.13 0.03 0.01 0.11 Pieces till Io Ike implication force 0.01 0.06 0.04 0.02 0.01 0.04 Figure 1: Candidate Sets for the Sentence: &amp;quot;Please fill in the application form!&amp;quot; in its context. Currently, there are two approaches, the statistical approach and the structural approach, towards the problem of candidate selection. In the statistical approach, language models, such as a Hidden Markov Model and word collocation can be utilized for candidate selection ([2, 4, 5]). In the structural approach, lattice parsing techniques have been developed for candidate selection([3, 7]). The contextual constraints considered in a statistical language model, such as word collocation, are local constraints. For a word image, a candidate will be selected according to the candidate information from its neighboring word images in a fixed window size. The window size is usually set as one or two. In the lattice parsing method, a grammar is used to select a candidate for each word image inside a sentence so that the sequence of those selected candidates form a grammatical an</context>
</contexts>
<marker>[5]</marker>
<rawString>Jonathan J. Hull, &amp;quot;A Hidden Markov Model for Language Syntax in Text Recognition,&amp;quot; in Proceedings of 11th IAPR International Conference on Pattern Recognition, The Hague, The Netherlands, pp.124-12&apos;7, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siamak Khoubyari</author>
<author>Jonathan J Hull</author>
</authors>
<title>Keyword Location in Noisy Document Image,&amp;quot;</title>
<date>1993</date>
<booktitle>in Proceedings of the Second Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>217--231</pages>
<location>Las Vegas, Nevada,</location>
<contexts>
<context position="7308" citStr="[6]" startWordPosition="1272" endWordPosition="1272"> image in which a small number of word relations are circled to demonstrate the abundance of inter-word relations defined above even in such a small fragment of a real text page. Word images 2 and 8 are almost the same. Word image 9 matches the left part of word image I quite well. Word image 5 matches a part of the image 6, and so on. Visual inter-word relations can be computed by applying simple image matching techniques. They can be calculated in clean text images, as well as in highly degraded text images, because the word images, due to their relatively large size, are tolerant to noise ([6]). Visual inter-word relations can be used as constraints in the process of word image interpretation, especially for candidate selection. It is not surprising that word relations at the image level are highly consistent with word relations at the symbolic level(see Table 1). If two words hold a relation at the symbolic level and they are written in the same font and size, their word images should keep the same relation at the image level. And also, if two word images hold a relation at the image level, the truth values of the word images should have the same relation at the symbolic level. In</context>
</contexts>
<marker>[6]</marker>
<rawString>Siamak Khoubyari and Jonathan J. Hull, &amp;quot;Keyword Location in Noisy Document Image,&amp;quot; in Proceedings of the Second Annual Symposium on Document Analysis and Information Retrieval, Las Vegas, Nevada, pp. 217-231, April 26-28, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>An Efficient Word Lattice Parsing Algorithm for Continuous Speech Recognition,&amp;quot;</title>
<date>1986</date>
<booktitle>in Proceedings of the International Conference on Acoustic, Speech and Signal Processing,</booktitle>
<contexts>
<context position="3276" citStr="[3, 7]" startWordPosition="536" endWordPosition="537">the duplication foam 0.02 0.10 0.13 0.03 0.01 0.11 Pieces till Io Ike implication force 0.01 0.06 0.04 0.02 0.01 0.04 Figure 1: Candidate Sets for the Sentence: &amp;quot;Please fill in the application form!&amp;quot; in its context. Currently, there are two approaches, the statistical approach and the structural approach, towards the problem of candidate selection. In the statistical approach, language models, such as a Hidden Markov Model and word collocation can be utilized for candidate selection ([2, 4, 5]). In the structural approach, lattice parsing techniques have been developed for candidate selection([3, 7]). The contextual constraints considered in a statistical language model, such as word collocation, are local constraints. For a word image, a candidate will be selected according to the candidate information from its neighboring word images in a fixed window size. The window size is usually set as one or two. In the lattice parsing method, a grammar is used to select a candidate for each word image inside a sentence so that the sequence of those selected candidates form a grammatical and meaningful sentence. For example, consider the sentence &amp;quot;Please fill in the application form&amp;quot;. We assume a</context>
</contexts>
<marker>[7]</marker>
<rawString>Masaru Tomita, &amp;quot;An Efficient Word Lattice Parsing Algorithm for Continuous Speech Recognition,&amp;quot; in Proceedings of the International Conference on Acoustic, Speech and Signal Processing, 1986.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>