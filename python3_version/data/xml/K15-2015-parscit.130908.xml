<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004388">
<title confidence="0.97351">
Hybrid Approach to PDTB-styled Discourse Parsing for CoNLL-2015
</title>
<author confidence="0.919901">
Yasuhisa Yoshida, Katsuhiko Hayashi, Tsutomu Hirao and Masaaki Nagata
</author>
<affiliation confidence="0.771077">
NTT Communication Science Laboratories, NTT Corporation
</affiliation>
<address confidence="0.79121">
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
</address>
<email confidence="0.72905">
{yoshida.y,hayashi.katsuhiko,
hirao.tsutomu,nagata.masaaki}@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.99376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974315789474">
This paper describes our end-to-end
PDTB-styled discourse parser for the
CoNLL-2015 shared task. We employed a
machine learning-based approach to iden-
tify discourse relation between text spans
for both explicit and implicit relations and
employed a rule-based approach to ex-
tract arguments of the discourse relations.
In particular, we focus on improving the
implicit discourse relation identification.
First, we extract adjacent pairs of sen-
tences that have some discourse relation-
ships by exploiting a two-class classifier
from an entire document. Second, we
assign sense labels for them by utilizing
a multiple-class classifier. Our system
achieved a 0.316 overall F-score for the
development set, 0.249 for the testset and
0.157 for the blind testset.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999859870967742">
In this paper, we describe our end-to-end PDTB-
styled discourse parsing system for CoNLL-2015.
Our system is an extension of Ziheng et al.’s dis-
course parser (Ziheng et al., 2014). Our explicit
connective-argument structure parser consists of
three modules: (1) a connective classifier that clas-
sifies connective candidates into discourse con-
nective or not, (2) an argument position classi-
fier that classifies whether Arg1 and the discourse
connective co-occur in the same sentence or not.
(3) a rule-based argument extraction that extracts
both Arg1 and Arg2 using rules derived from
a syntactic tree. The implicit parser consists of
two modules: (1) argument pair identification that
finds the pair of adjacent sentences that have some
discourse relation, (2) sense labeler assigning the
role of the discourse relation between the sen-
tences.
In addition, we introduce a new evaluation mea-
sure for argument extraction. Since exact match-
ing between arguments used in “scorer.py” pro-
vided by the organizers of CoNLL-2015 is too
strict, we introduce relaxed matching for the task.
The evaluation metric measures how close argu-
ments provided by the system are to the gold argu-
ments.
The evaluation results provided by the CoNLL-
2015 official scorer show that our system achieved
5th rank in the Arg1 extractor, 6th rank in the
Arg2 extractor, 4th rank in the Arg1&amp;Arg2 ex-
tractor, and 8th rank in overall performance.
</bodyText>
<sectionHeader confidence="0.994299" genericHeader="introduction">
2 Explicit Connective-Argument
Identification
</sectionHeader>
<bodyText confidence="0.999817714285714">
The explicit connective-argument parser consists
of three steps. First, we identify discourse con-
nectives for an entire document. Second, we de-
termine whether Arg1 is contained in the same
sentence that includes the discourse connective.
Third, we assign a sense label for each discourse
connective.
</bodyText>
<subsectionHeader confidence="0.984717">
2.1 Connective Classification
</subsectionHeader>
<bodyText confidence="0.999629">
The connective classifier classifies ambiguous
connective candidates such as “and” into discourse
connective or not. We exploit lexical features and
features obtained from parse trees by extending
(Ziheng et al., 2014). Note that connective can-
didates were extracted from the PYTHON script
“conn head mapper.py” provided by the organiz-
ers of CoNLL-2015. Features that we utilized are
shown in Table 1.
We trained the classifier by using SVM with
second-order polynomial kernel.
</bodyText>
<subsectionHeader confidence="0.999209">
2.2 Argument Position Classification
</subsectionHeader>
<bodyText confidence="0.997337">
By following (Ziheng et al., 2014), we imple-
mented an argument position classifier that clas-
sifies the location of the arguments of arbitrary
</bodyText>
<page confidence="0.991489">
95
</page>
<note confidence="0.909259">
Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 95–99,
Beijing, China, July 26-31, 2015. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.86207825">
Type Features
Cs, POSs, {Wordu}, {POSu}
Path(Cs,root), Parent(Cs), depth(Cs),
RightSib(Cs), LeftSib(Cs)
</table>
<tableCaption confidence="0.9026305">
u = s − 5,..., s − 1, s + 1,..., s + 5
Table 1: Features used in connective classifier
</tableCaption>
<bodyText confidence="0.999938545454546">
discourse connective into “same sentence” (SS)
or “previous sentence” (PS). SS indicates both
Arg1 and Arg2 are located in the same sentence
that contains the discourse connective. PS indi-
cates Arg1 is located in the sentence previous to
that containing both the discourse connective and
Arg2. We utilized context features in Table 1 and
the position of the connective Cs: start, middle, or
end.
We also trained the classifier by using SVM
with second-order polynomial kernel.
</bodyText>
<subsectionHeader confidence="0.998374">
2.3 Sense Classification
</subsectionHeader>
<bodyText confidence="0.998893">
We assign majority sense f* for each discourse
connective Cs as follows:
</bodyText>
<equation confidence="0.935004">
f* = arg maxℓCLfreq(Cs,f). (1)
</equation>
<bodyText confidence="0.992052">
L is a set of sense labels used in training data
and freq returns the frequency of co-occurrences
of the discourse connective and sense label.
</bodyText>
<sectionHeader confidence="0.9830105" genericHeader="method">
3 Implicit Connective-Argument
Relationship Identification
</sectionHeader>
<bodyText confidence="0.999908428571429">
The implicit parser consists of two steps. First is
the argument identification step. In this step, we
examine whether an adjacent sentence pair in the
same paragraph has a discourse relation or not.
Second is the sense classification step. Given a
pair of sentences, we classify it into a predefined
sense label.
</bodyText>
<subsectionHeader confidence="0.999731">
3.1 Argument Position Identification
</subsectionHeader>
<bodyText confidence="0.997826857142857">
In the argument identification step, following
Ghosh et al. (2011), the identifier examines all ad-
jacent sentence pairs within each paragraph. For
each pair of sentences (Si, Si+1), we identify the
existence of a discourse relation. To identify the
existence of the relation (binary classification), we
used SVM with the following features.
</bodyText>
<listItem confidence="0.999348142857143">
• First unigram, last unigram, and first trigram
of Si and Si+1.
• Si (or Si+1) contains modality words or not.
• Word pairs (wi, wi+1) E Si x Si+1
• Brown cluster pairs feature defined in Ruther-
ford and Xue (2014)
• Sentence-to-sentence discourse dependency
</listItem>
<bodyText confidence="0.815974285714286">
tree features including existence of depen-
dency edges and rhetorical relation labels.
Discourse dependency trees are defined in Li
et al. (Li et al., 2014).
If the identifier identifies that a pair of sentences
(Si, Si+1) has the discourse relation, we heuristi-
cally regard Si as Arg1 and Si+1 as Arg2.
</bodyText>
<subsectionHeader confidence="0.999075">
3.2 Sense Classification
</subsectionHeader>
<bodyText confidence="0.999993055555556">
In the sense classification step, we classify the
discourse relation between a pair of sentences
(Si, Si+1) into five senses: “Expansion”, “Con-
tingency”, “Temporal”, “Comparison”, and “En-
tRel”. To classify the sense of a pair of sentences,
we used multi-class SVM. We used the same fea-
tures described in the argument position identifi-
cation step. To increase the number of training
data, we used the (inter-sentential) explicit train-
ing data as the additional training data (Ruther-
ford and Xue, 2015). We removed a connective
from each instance in the explicit training data and
treated them as implicit training data. The accu-
racy of classification into five senses is still low be-
cause the distribution of the senses is imbalanced.
Following Rutherford and Xue (2014), we resam-
pled the instances in the training data of sense clas-
sification to balance the distribution of the senses.
</bodyText>
<sectionHeader confidence="0.982865" genericHeader="method">
4 Argument Extractor
</sectionHeader>
<bodyText confidence="0.99998875">
We utilized two rule-based argument extractors.
One extracts both Arg1 and Arg2 from the same
sentence (SS). The other extracts Arg1 and Arg2
from adjacent sentences respectively (PS).
</bodyText>
<subsectionHeader confidence="0.940965">
4.1 SS Cases
4.1.1 Subordinating Conjunctions
</subsectionHeader>
<bodyText confidence="0.9996805">
We adopted Dinesh et al. (2005)’s tree subtrac-
tion method for subordinating conjunctions. This
method takes a constituent parse tree as an input
and detects argument spans as follows:
</bodyText>
<listItem confidence="0.9834052">
(1) set a node variable x to the last word of the
target connective,
(2) set x to the parent node of x and repeat until
x has label SBAR or S and set a node variable
Arg2 to the node of x,
</listItem>
<figure confidence="0.547625">
Context
Parse Tree
</figure>
<page confidence="0.716572">
96
</page>
<table confidence="0.999906285714286">
P Dev F P Test F P Blind F
R R R
Con. .924 .857 .889 .918 .866 .891 .925 .353 .510
Arg1 .658 .549 .599 .719 .584 .644 .638 .330 .435
Arg2 .768 .640 .698 .587 .477 .526 .765 .395 .521
Arg1&amp;Arg2 .566 .471 .514 .488 .397 .438 .522 .269 .356
Overall .348 .290 .316 .279 .226 .249 .230 .119 .157
</table>
<tableCaption confidence="0.984341">
Table 2: Official evaluation results.
</tableCaption>
<listItem confidence="0.9986355">
(3) set x to the parent node of x and repeat again
until x has label SBAR or S and set a node
variable Arg1 to the current node of x,
(4) consider span(Arg2) as the span of argu-
ment2 and span(Arg1)\span(Arg2) as that
of argument1, where span(·) is a function
mapping a node · to a set of words dominated
by the node.
</listItem>
<subsectionHeader confidence="0.64574">
4.1.2 Coordinating Conjunctions
</subsectionHeader>
<bodyText confidence="0.971395666666667">
For coordinating conjunctions, we also define a
rule-based method that works on a constituent
tree:
</bodyText>
<listItem confidence="0.930893857142857">
(1) set a node variable x to the last word of the
target connective,
(2) set a node variable y to x and x to the parent
node of x, and repeat while the leftmost word
in span(x) is equal to that in span(y), and
after the process, add y and the more right
child nodes of x into a set Arg2 set,
(3-1) if a node labeled with S or SBAR is contained
in the set of the more right child nodes of x
than y, set a node variable Arg1 to the node,
(3-2) otherwise, set x to the parent node x and re-
peat until x has label SBAR or S, and set a
node variable Arg1 to the node of x,
(4) consider union span(Arg2 set)
</listItem>
<bodyText confidence="0.9712152">
as the span of argument2 and
span(Arg1) \ union span(Arg2 set)
as that of argument1, where union span(·)
is a function mapping a node set · to the
union of each word set span(Arg2) for
</bodyText>
<sectionHeader confidence="0.386211" genericHeader="method">
Arg2 E Arg2 set.
</sectionHeader>
<subsectionHeader confidence="0.6897105">
4.1.3 Discourse Adverbials &amp; Implicit
Argument Structures
</subsectionHeader>
<bodyText confidence="0.99943075">
We did not treat the discourse adverbial
connective-argument and inter-sentential implicit
argument structures because their frequencies are
not high in the training data.
</bodyText>
<subsectionHeader confidence="0.970593">
4.2 PS Cases
</subsectionHeader>
<bodyText confidence="0.987968">
In the PS cases, our rule-based extraction method
is very simple and has only two processes: (1) re-
move sentence end symbols such as . ! ?. and
(2) remove brace expressions enclosed in sentence
start and end brackets like “”. This method repeats
(1) and (2) until unchanged.
</bodyText>
<sectionHeader confidence="0.973927" genericHeader="evaluation">
5 Evaluation Results
</sectionHeader>
<bodyText confidence="0.999987620689655">
Table 2 shows the official evaluation results. From
the results, explicit connective identification and
the Arg2 extractor performed well, but perfor-
mance of the Arg1 extractor and sense classifi-
cation was not very good. Thus, the overall per-
formance is significantly degraded. Table 3 shows
the official evaluation results for explicit relations.
Compared with the testset, the accuracies for the
blind testset drastically dropped. This is because
our programs might failed to identify some con-
nectives. Table 4 shows the official evaluation
results for implicit relations. Among the partici-
pants, our implicit parser performed well (1st rank
in the Arg1&amp;Arg2 extractor and 2nd rank in the
overall performance). Previous study like Ghosh
et al. (2011) jointly extracted the argument and
classified the sense with a single classifier. Our
system performed well since we split our system
into the argument extractor and the sense classi-
fier.
“scorer.py” employs exact matching for argu-
ment extraction, and when the span of the ar-
gument provided by systems exactly matches
the span of the human annotated argument, the
scorer evaluates the system’s tuples. However,
the boundaries of human annotated arguments are
blurry. The span of the argument may differ
from the span annotated by another human. Thus,
we evaluate our argument extractor with relaxed
</bodyText>
<page confidence="0.999261">
97
</page>
<table confidence="0.999883285714286">
P Dev F P Test F P Blind F
R R R
Con. .924 .857 .889 .918 .866 .891 .924 .353 .510
Arg1 .578 .537 .557 .475 .448 .461 .509 .194 .281
Arg2 .749 .696 .722 .705 .664 .684 .689 .263 .380
Arg1&amp;Arg2 .498 .462 .479 .400 .377 .388 .392 .149 .216
Overall .447 .415 .430 .355 .335 .345 .307 .117 .169
</table>
<tableCaption confidence="0.983609">
Table 3: Official evaluation results for explicit relations.
</tableCaption>
<table confidence="0.999947666666667">
P Dev F P Test F P Blind F
R R R
Arg1 .729 .546 .625 .708 .491 .579 .692 .438 .537
Arg2 .788 .589 .675 .733 .509 .601 .804 .508 .623
Arg1&amp;Arg2 .641 .480 .549 .596 .413 .488 .588 .372 .456
Overall .237 .177 .203 .184 .128 .151 .191 .121 .148
</table>
<tableCaption confidence="0.995535">
Table 4: Official evaluation results for implicit relations.
</tableCaption>
<figure confidence="0.7514205">
1 0.9 0.8 0.7 0.6 0.5
Threshold
</figure>
<figureCaption confidence="0.9813715">
Figure 1: Evaluation results with relaxed match-
ing.
</figureCaption>
<bodyText confidence="0.977006">
matching. We compute token-based arg-Fscore
between the system argument and the gold argu-
ment that is defined as follows:
</bodyText>
<equation confidence="0.9820402">
Prec. = |As ∩ Ag|
|As |� (2)
Rec. = |As ∩ Ag|
|Ag |� (3)
arg-Fscore =
</equation>
<bodyText confidence="0.999958928571429">
As indicates a set of tokenIDs obtained from
the system argument. Ag indicates a set of to-
kenIDs obtained from the gold argument. Then,
we regard the system argument that has a cer-
tain threshold arg-Fscore as the correct argument.
Figure 1 shows evaluation results with thresholds
from 1.0 to 0.5. When we set the threshold to
0.5, Arg1&amp;Arg2 Fscore achieved 0.7. This im-
plies that our system can detect most of the correct
positions of both explicit and implicit connectives
but can not extract the correct span of the argu-
ments. Moreover, overall performance is still low
because of error caused by the sense classification
modules.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999919571428571">
In this paper, we presented our PDTB-styled full
discourse parser for CoNLL-2015. We extended
the work by (Ziheng et al., 2014). The experi-
mental resulted show that our performed well on
explicit connective identification and Arg1 extrac-
tion, but not on Arg2 extraction and sense classi-
fication.
</bodyText>
<sectionHeader confidence="0.998497" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996466307692308">
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. 2005.
Attribution and the (non-) alignment of syntactic and
discourse arguments of connectives. In Proceed-
ings of the Workshop on Frontiers in Corpus Anno-
tations II: Pie in the Sky, pages 29–36. Association
for Computational Linguistics.
Sucheta Ghosh, Sara Tonelli, Giuseppe Riccardi, and
Richard Johansson. 2011. End-to-end discourse
parser evaluation. In Fifth IEEE International
Conference on Semantic Computing (ICSC), 2011;
September 18-21, 2011; Palo Alto, United States,
pages 169–172.
</reference>
<figure confidence="0.998248266666667">
Fscore
0.9
0.8
0.7
0.6
0.5
0.4
1
Arg1
Arg2
Arg1&amp;Arg2
Overall
� (4)
2 ∗ Prec. ∗ Rec.
Prec. + Rec.
</figure>
<page confidence="0.996831">
98
</page>
<reference confidence="0.999035904761905">
Sujian Li, Liang Wang, Ziqiang Cao, and Wenjie Li.
2014. Text-level discourse dependency parsing. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 25–35, Baltimore, Maryland,
June. Association for Computational Linguistics.
Attapol Rutherford and Nianwen Xue. 2014. Dis-
covering implicit discourse relations through brown
cluster pair representation and coreference patterns.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 645–654, Gothenburg, Sweden,
April. Association for Computational Linguistics.
Attapol Rutherford and Nianwen Xue. 2015. Improv-
ing the inference of implicit discourse relations via
classifying explicit discourse connectives. In Proc.
of the 2015 NAACL. Association for Computational
Linguistics.
Lin Ziheng, Ng Hwee Tou, and Kan Min-Yen. 2014. A
PDTB-styled End-to-End Discourse Parser. Natural
Language Engineering, 20:151–184.
</reference>
<page confidence="0.998969">
99
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.497821">
<title confidence="0.73678">Hybrid Approach to PDTB-styled Discourse Parsing for CoNLL-2015</title>
<author confidence="0.905992">Katsuhiko Hayashi Yoshida</author>
<author confidence="0.905992">Tsutomu Hirao</author>
<affiliation confidence="0.968764">NTT Communication Science Laboratories, NTT</affiliation>
<address confidence="0.932748">2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237,</address>
<abstract confidence="0.98514065">This paper describes our end-to-end PDTB-styled discourse parser for the CoNLL-2015 shared task. We employed a machine learning-based approach to identify discourse relation between text spans for both explicit and implicit relations and employed a rule-based approach to extract arguments of the discourse relations. In particular, we focus on improving the implicit discourse relation identification. First, we extract adjacent pairs of sentences that have some discourse relationships by exploiting a two-class classifier from an entire document. Second, we assign sense labels for them by utilizing a multiple-class classifier. Our system achieved a 0.316 overall F-score for the development set, 0.249 for the testset and 0.157 for the blind testset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>Attribution and the (non-) alignment of syntactic and discourse arguments of connectives.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky,</booktitle>
<pages>29--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7195" citStr="Dinesh et al. (2005)" startWordPosition="1099" endWordPosition="1102">he explicit training data and treated them as implicit training data. The accuracy of classification into five senses is still low because the distribution of the senses is imbalanced. Following Rutherford and Xue (2014), we resampled the instances in the training data of sense classification to balance the distribution of the senses. 4 Argument Extractor We utilized two rule-based argument extractors. One extracts both Arg1 and Arg2 from the same sentence (SS). The other extracts Arg1 and Arg2 from adjacent sentences respectively (PS). 4.1 SS Cases 4.1.1 Subordinating Conjunctions We adopted Dinesh et al. (2005)’s tree subtraction method for subordinating conjunctions. This method takes a constituent parse tree as an input and detects argument spans as follows: (1) set a node variable x to the last word of the target connective, (2) set x to the parent node of x and repeat until x has label SBAR or S and set a node variable Arg2 to the node of x, Context Parse Tree 96 P Dev F P Test F P Blind F R R R Con. .924 .857 .889 .918 .866 .891 .925 .353 .510 Arg1 .658 .549 .599 .719 .584 .644 .638 .330 .435 Arg2 .768 .640 .698 .587 .477 .526 .765 .395 .521 Arg1&amp;Arg2 .566 .471 .514 .488 .397 .438 .522 .269 .35</context>
</contexts>
<marker>Dinesh, Lee, Miltsakaki, Prasad, Joshi, Webber, 2005</marker>
<rawString>Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bonnie Webber. 2005. Attribution and the (non-) alignment of syntactic and discourse arguments of connectives. In Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky, pages 29–36. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sucheta Ghosh</author>
<author>Sara Tonelli</author>
<author>Giuseppe Riccardi</author>
<author>Richard Johansson</author>
</authors>
<title>End-to-end discourse parser evaluation.</title>
<date>2011</date>
<booktitle>In Fifth IEEE International Conference on Semantic Computing (ICSC),</booktitle>
<pages>169--172</pages>
<contexts>
<context position="5164" citStr="Ghosh et al. (2011)" startWordPosition="770" endWordPosition="773">L is a set of sense labels used in training data and freq returns the frequency of co-occurrences of the discourse connective and sense label. 3 Implicit Connective-Argument Relationship Identification The implicit parser consists of two steps. First is the argument identification step. In this step, we examine whether an adjacent sentence pair in the same paragraph has a discourse relation or not. Second is the sense classification step. Given a pair of sentences, we classify it into a predefined sense label. 3.1 Argument Position Identification In the argument identification step, following Ghosh et al. (2011), the identifier examines all adjacent sentence pairs within each paragraph. For each pair of sentences (Si, Si+1), we identify the existence of a discourse relation. To identify the existence of the relation (binary classification), we used SVM with the following features. • First unigram, last unigram, and first trigram of Si and Si+1. • Si (or Si+1) contains modality words or not. • Word pairs (wi, wi+1) E Si x Si+1 • Brown cluster pairs feature defined in Rutherford and Xue (2014) • Sentence-to-sentence discourse dependency tree features including existence of dependency edges and rhetoric</context>
<context position="10431" citStr="Ghosh et al. (2011)" startWordPosition="1687" endWordPosition="1690">well, but performance of the Arg1 extractor and sense classification was not very good. Thus, the overall performance is significantly degraded. Table 3 shows the official evaluation results for explicit relations. Compared with the testset, the accuracies for the blind testset drastically dropped. This is because our programs might failed to identify some connectives. Table 4 shows the official evaluation results for implicit relations. Among the participants, our implicit parser performed well (1st rank in the Arg1&amp;Arg2 extractor and 2nd rank in the overall performance). Previous study like Ghosh et al. (2011) jointly extracted the argument and classified the sense with a single classifier. Our system performed well since we split our system into the argument extractor and the sense classifier. “scorer.py” employs exact matching for argument extraction, and when the span of the argument provided by systems exactly matches the span of the human annotated argument, the scorer evaluates the system’s tuples. However, the boundaries of human annotated arguments are blurry. The span of the argument may differ from the span annotated by another human. Thus, we evaluate our argument extractor with relaxed </context>
</contexts>
<marker>Ghosh, Tonelli, Riccardi, Johansson, 2011</marker>
<rawString>Sucheta Ghosh, Sara Tonelli, Giuseppe Riccardi, and Richard Johansson. 2011. End-to-end discourse parser evaluation. In Fifth IEEE International Conference on Semantic Computing (ICSC), 2011; September 18-21, 2011; Palo Alto, United States, pages 169–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujian Li</author>
<author>Liang Wang</author>
<author>Ziqiang Cao</author>
<author>Wenjie Li</author>
</authors>
<title>Text-level discourse dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>25--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="5853" citStr="Li et al., 2014" startWordPosition="883" endWordPosition="886">ph. For each pair of sentences (Si, Si+1), we identify the existence of a discourse relation. To identify the existence of the relation (binary classification), we used SVM with the following features. • First unigram, last unigram, and first trigram of Si and Si+1. • Si (or Si+1) contains modality words or not. • Word pairs (wi, wi+1) E Si x Si+1 • Brown cluster pairs feature defined in Rutherford and Xue (2014) • Sentence-to-sentence discourse dependency tree features including existence of dependency edges and rhetorical relation labels. Discourse dependency trees are defined in Li et al. (Li et al., 2014). If the identifier identifies that a pair of sentences (Si, Si+1) has the discourse relation, we heuristically regard Si as Arg1 and Si+1 as Arg2. 3.2 Sense Classification In the sense classification step, we classify the discourse relation between a pair of sentences (Si, Si+1) into five senses: “Expansion”, “Contingency”, “Temporal”, “Comparison”, and “EntRel”. To classify the sense of a pair of sentences, we used multi-class SVM. We used the same features described in the argument position identification step. To increase the number of training data, we used the (inter-sentential) explicit</context>
</contexts>
<marker>Li, Wang, Cao, Li, 2014</marker>
<rawString>Sujian Li, Liang Wang, Ziqiang Cao, and Wenjie Li. 2014. Text-level discourse dependency parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25–35, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Attapol Rutherford</author>
<author>Nianwen Xue</author>
</authors>
<title>Discovering implicit discourse relations through brown cluster pair representation and coreference patterns.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>645--654</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="5653" citStr="Rutherford and Xue (2014)" startWordPosition="853" endWordPosition="857">t into a predefined sense label. 3.1 Argument Position Identification In the argument identification step, following Ghosh et al. (2011), the identifier examines all adjacent sentence pairs within each paragraph. For each pair of sentences (Si, Si+1), we identify the existence of a discourse relation. To identify the existence of the relation (binary classification), we used SVM with the following features. • First unigram, last unigram, and first trigram of Si and Si+1. • Si (or Si+1) contains modality words or not. • Word pairs (wi, wi+1) E Si x Si+1 • Brown cluster pairs feature defined in Rutherford and Xue (2014) • Sentence-to-sentence discourse dependency tree features including existence of dependency edges and rhetorical relation labels. Discourse dependency trees are defined in Li et al. (Li et al., 2014). If the identifier identifies that a pair of sentences (Si, Si+1) has the discourse relation, we heuristically regard Si as Arg1 and Si+1 as Arg2. 3.2 Sense Classification In the sense classification step, we classify the discourse relation between a pair of sentences (Si, Si+1) into five senses: “Expansion”, “Contingency”, “Temporal”, “Comparison”, and “EntRel”. To classify the sense of a pair o</context>
</contexts>
<marker>Rutherford, Xue, 2014</marker>
<rawString>Attapol Rutherford and Nianwen Xue. 2014. Discovering implicit discourse relations through brown cluster pair representation and coreference patterns. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 645–654, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Attapol Rutherford</author>
<author>Nianwen Xue</author>
</authors>
<title>Improving the inference of implicit discourse relations via classifying explicit discourse connectives.</title>
<date>2015</date>
<booktitle>In Proc. of the 2015 NAACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6526" citStr="Rutherford and Xue, 2015" startWordPosition="991" endWordPosition="995">ntences (Si, Si+1) has the discourse relation, we heuristically regard Si as Arg1 and Si+1 as Arg2. 3.2 Sense Classification In the sense classification step, we classify the discourse relation between a pair of sentences (Si, Si+1) into five senses: “Expansion”, “Contingency”, “Temporal”, “Comparison”, and “EntRel”. To classify the sense of a pair of sentences, we used multi-class SVM. We used the same features described in the argument position identification step. To increase the number of training data, we used the (inter-sentential) explicit training data as the additional training data (Rutherford and Xue, 2015). We removed a connective from each instance in the explicit training data and treated them as implicit training data. The accuracy of classification into five senses is still low because the distribution of the senses is imbalanced. Following Rutherford and Xue (2014), we resampled the instances in the training data of sense classification to balance the distribution of the senses. 4 Argument Extractor We utilized two rule-based argument extractors. One extracts both Arg1 and Arg2 from the same sentence (SS). The other extracts Arg1 and Arg2 from adjacent sentences respectively (PS). 4.1 SS C</context>
</contexts>
<marker>Rutherford, Xue, 2015</marker>
<rawString>Attapol Rutherford and Nianwen Xue. 2015. Improving the inference of implicit discourse relations via classifying explicit discourse connectives. In Proc. of the 2015 NAACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Ziheng</author>
<author>Ng Hwee Tou</author>
<author>Kan Min-Yen</author>
</authors>
<title>A PDTB-styled End-to-End Discourse Parser. Natural Language Engineering,</title>
<date>2014</date>
<pages>20--151</pages>
<contexts>
<context position="1284" citStr="Ziheng et al., 2014" startWordPosition="174" endWordPosition="177">ar, we focus on improving the implicit discourse relation identification. First, we extract adjacent pairs of sentences that have some discourse relationships by exploiting a two-class classifier from an entire document. Second, we assign sense labels for them by utilizing a multiple-class classifier. Our system achieved a 0.316 overall F-score for the development set, 0.249 for the testset and 0.157 for the blind testset. 1 Introduction In this paper, we describe our end-to-end PDTBstyled discourse parsing system for CoNLL-2015. Our system is an extension of Ziheng et al.’s discourse parser (Ziheng et al., 2014). Our explicit connective-argument structure parser consists of three modules: (1) a connective classifier that classifies connective candidates into discourse connective or not, (2) an argument position classifier that classifies whether Arg1 and the discourse connective co-occur in the same sentence or not. (3) a rule-based argument extraction that extracts both Arg1 and Arg2 using rules derived from a syntactic tree. The implicit parser consists of two modules: (1) argument pair identification that finds the pair of adjacent sentences that have some discourse relation, (2) sense labeler ass</context>
<context position="3112" citStr="Ziheng et al., 2014" startWordPosition="453" endWordPosition="456">k in overall performance. 2 Explicit Connective-Argument Identification The explicit connective-argument parser consists of three steps. First, we identify discourse connectives for an entire document. Second, we determine whether Arg1 is contained in the same sentence that includes the discourse connective. Third, we assign a sense label for each discourse connective. 2.1 Connective Classification The connective classifier classifies ambiguous connective candidates such as “and” into discourse connective or not. We exploit lexical features and features obtained from parse trees by extending (Ziheng et al., 2014). Note that connective candidates were extracted from the PYTHON script “conn head mapper.py” provided by the organizers of CoNLL-2015. Features that we utilized are shown in Table 1. We trained the classifier by using SVM with second-order polynomial kernel. 2.2 Argument Position Classification By following (Ziheng et al., 2014), we implemented an argument position classifier that classifies the location of the arguments of arbitrary 95 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 95–99, Beijing, China, July 26-31, 2015. c�2014 Associ</context>
</contexts>
<marker>Ziheng, Tou, Min-Yen, 2014</marker>
<rawString>Lin Ziheng, Ng Hwee Tou, and Kan Min-Yen. 2014. A PDTB-styled End-to-End Discourse Parser. Natural Language Engineering, 20:151–184.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>