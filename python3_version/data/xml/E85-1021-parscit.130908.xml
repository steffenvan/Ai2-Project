<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000059">
<note confidence="0.3524946">
DESIGN AND IMPLEMENTATION OF A LEXICAL DATA BASE
Eric Wehrli
Department of Linguistics
U.C.L.A.
405 Hilgard Ave, Los Angeles, CA 90024
</note>
<sectionHeader confidence="0.542592" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999632615384615">
This paper is concerned with the
specifications and the implementation of a
particular concept of word-based lexicon to be
used for large natural language processing systems
such as machine translation systems, and compares
it with the morpheme-based conception of the
lexicon traditionally assumed in computational
linguistics.
It will be argued that, although less
concise, a relational word-based lexicon is
superior to-a morpheme-based lexicon from a
theoretical, computational and also practical
viewpoint.
</bodyText>
<sectionHeader confidence="0.989863" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.99974775">
It has been traditionally assumed by
computational linguists and particularly by
designers of large natural language processing
systems such as machine translation systems that
the lexicon should be limited to lexical
information that cannot be derived by rules.
According to this view, a lexicon consists of a
list of basic morphemes along with irregular or
unpredictable words.
In this paper, I would like to reexamine this
traditional view of the lexicon and point out some
of the problems it faces which seriously question
the general adequacy of this model for natural
language processing.
As a trade-off between the often conflicting
linguistic, computational and also practical
considerations, an alternative conception of the
lexicon will be discussed, largely based on
Jackendoff&apos;s (1975) proposal. According to this
view, lexical entries are fully-specified but
related to one another. First developed for a
French parser (cf. Wehrli, 1984), this model has
been adopted for an English parser in development,
as well as for the prototype of a French-English
translation system.
This paper is organized as follows: the first
section addresses the general issue of what
constitutes a lexical entry as well as the
question of the relation between lexicon and
morphology from the point of view of both
theoretical linguistics and computational
linguistics. Section 2 discusses the relational
word-based model of the lexicon and the role
morphology is assigned in this model. Finally, it
spells out some of the details of the
implementation of this model.
</bodyText>
<sectionHeader confidence="0.983122" genericHeader="method">
OVERVIEW OF THE PROBLEM
</sectionHeader>
<bodyText confidence="0.999968384615385">
One of the well-known characteristic features
of natural languages is the size and the
complexity of their lexicons. This is in sharp
constrast with artificial languages, which
typically have small lexicons, in most cases made
up of simple, unambiguous lexical items. Not only
do natural languages have a huge number of lexical
elements -- no matter what precise definition of
this latter term one chooses -- but these lexical
elements can furthermore (i) be ambiguous in
several ways (ii) have a non-trivial internal
structure, or (iii) be part of compounds or
idiomatic expressions, as illustrated in (1)-(4):
</bodyText>
<listItem confidence="0.732419125">
(1) ambiguous words:
can, fly, bank, pen, race, etc.
(2) internal structure:
use-ful-ness, mis-understand-ing, lake-s,
tri-ed
(3) compounds:
milkman, moonlight, etc.
(4) idiomatic expressions:
</listItem>
<bodyText confidence="0.9896176875">
to kick the bucket, by and large,
to pull someone&apos;s leg, etc.
In fact, the notion of word, itself, is not
all that clear, as numerous linguists --
theoreticians and/or computational linguists --
have acknowledged. Thus, to take an example from
the computational linguistics literature, Kay
(1977) notes:
&amp;quot;In common usage, the term word refers
sometimes to sequences of letters that
can be bounded by spaces or punctuation
marks in a text. According to this view,
run, runs, running and ran are
different words. But common usage also
allows these to count as instances of
the same word because they belong to the
</bodyText>
<page confidence="0.997913">
146
</page>
<bodyText confidence="0.999950866141733">
same paradigm in English accidence and
are listed in the same entry in the
dictionary.&amp;quot;
Some of these problems, as well as the
general question of what constitutes a lexical
entry, whether or not lexical items should be
related to one another, etc. have been much
debated over the last 10 or 15 years within the
framework of generative grammar. Considered as a
relatively minor appendix of the phrase-structure
rule component in the early days of generative
grammar, the lexicon became little by little an
autonomous component of the grammar with its own
specific formalism -- lexical entries as matrices
of features, as advocated by Chomsky (1965).
Finally, it also acquired specific types of rules,
the so-called word formation rules (cf. Halle,
1973; Aronoff, 1976; Lieber, 1980; Selkirk, 1983,
and others), and lexical redundancy rules (cf.
Jackendoff, 1975; Bresnan, 1977).
By and large, there seems to be widespread
agreement among linguists that the lexicon should
be viewed as the repository of all the
idiosyncratic properties of the lexical items of a
language (phonological, morphological, syntactic,
semantic, etc.). This agreement quickly
disappears, however, when it comes to defining
what constitutes a lexical item, or, to put it
slightly differently, what the lexicon is a list
of, and how should it be organized.
Among the many proposals discussed in the
linguistic literature, I will consider two
radically opposed views that I shall call the
morpheme-bayed and the word-based conceptions of
the lexicon .
The morpheme-based lexicon corresponds to the
traditional derivational view of the lexicon,
shared by the structuralist school, many of the
generative linguists and virtually all the
computational linguists. According to this option,
only non-derived morphemes are actually listed in
the lexicon, complex words being derived by means
of morphological rules. In contrast, in a
word-based lexicon a la Jackendoff, all the words
(simple and complex) are listed as independent
lexical entries, derivational as well as
inflectional relons being expressed by means of
redundancy rules
The crucial distinction between these two
views of the lexicon has to do with the role of
morphology. The morpheme-based conception of the
lexicon advocates a dynamic view of morphology,
i.e. a conception according to which &amp;quot;words are
generated each time anew&amp;quot; (Hoekstra et al. 1980).
This view contrasts with the static conception of
morphology assumed in Jackendoff&apos;s word-based
theory of the lexicon.
Interestingly enough, with the exception of
some (usually very small) systems with no
morphology at all, all the lexicons in
computational linguistic projects seem to assume a
dynamic conception of morphology.
The no-morphology option, which can be viewed
as an extreme version of the word-based lexicon
mentioned above modulo the redundancy rules, has
been adopted mostly for convenience by researchers
working on parsers for languages fairly
uninteresting from the point of view of
morphology, e.g. English. It has the non-trivial
merit of reducing the lexical analysis to a simple
dictionary look-up. Since all flectional forms of
a given word are listed independently, all the
orthographic words must be present in the lexicon.
Thus, this option presents the double advantage of
being simple and efficient. The price to pay is
fairly high, though, in the sense that the
resulting lexicon displays an enormous amount of
redundancy: lexical information relevant for a
whole class of morphologically related words has
to be duplicated for every member of the class.
This duplication of information, in turn, makes
the task of updating and/or deleting lexical
entries much more complex than it should be.
This option is more seriously flawed than
just being redundant and space-greedy, though. By
ignoring the obvious fact that words in natural
languag es do have some internal structure, may
belong to declension or conjugation classes, but
above all that different orthographical words may
in fact realize the same grammatical word in
different syntactic environments it fails to be
descriptively adequate. Interestingly enough, this
inadequacy turns out to have serious consequences.
Consider, for example, the case of a translation
system. Because a lexicon of this exhaustive list
type has no way of representing a notion such as
&amp;quot;lexeme&amp;quot;, it lacks the proper level for lexical
transfer. Thus, if been, was, were, am and be are
treated as independant words, what should be their
translation, say in French, especially if we
assume that the French lexicon is organized on the
same model? The point is straightforward: there is
no way one can give translation equivalents for
orthographic words. Lexical transfer can only be
made at the more abstract level of lexeme. The
choice of a particular orthographic word to
realize this lexeme is strictly language
dependent. In the previous example, assuming that,
say, were is to be translated as a form of the
verbe etre, the choice of the correct flectional
form will be governed by various factors and
properties of the French sentence. In other words,
a transfer lexicon must state the fact that the
verb to be is translated in French by etre, rather
than the lower level fact that under some
circumstances were is translated by etaient.
The problems caused by the size and the
complexity of natural language lexicons, as well
as the basic inadequacy of the &amp;quot;no morphology&amp;quot;
option just described, have been long acknowledged
by computational linguists, in particular by those
involved in the development of large-scale
application programs such as machine translation.
It is thus hardly surprising that some version of
the morpheme-based lexicon has been the option
common to all large natural language systems.
There is no doubt that restricting the lexicon to
</bodyText>
<page confidence="0.990988">
147
</page>
<bodyText confidence="0.9909154">
basic morphemes and deriving all complex words as
well as all the inflected forms by morphological
rules, reduces substantially the size of the
lexicon. This was indeed a crucial issue not so
long ago, when computer memory was scarce and
expensive.
There are, however, numerous problems --
linguistic, computational as well as practical --
with the morpheme-based conception of the lexicon.
Its inadequacy from a theoretical linguistic point
of view has been discussed abundantly in the
&amp;quot;lexicalist&amp;quot; literature. See in particular Chomsky
(1970), Halle (1973) and Jackendoff (1975). Some
of the linguistic problems are summarized below,
along with some mentions of computational as well
as practical problems inherent to this approach.
First of all, from a conceptual point of
view, the adoption of a derivational model of
morphology suggests that the derivation of a word
is very similar, as a process, to the derivation
of a sentence. Such a view, however, fails to
recognize some fundamental distinctions between
the syntax of words and the syntax of sentences,
for instance regarding creativity. Whereas the
vast majority of the words we use are fixed
expressions that we have heard before, exactly the
opposite is true of sentences: most sentences we
hear are likely to be novel to us.
Also, given a morpheme-based lexicon, the
morphological analysis creates readings of words
that do not exist, such as strawberry understood
as a compund of the morphemes straw and berry.
This is far from being an isolate case, examples
like the following are not hard to find:
(5)a. comput-er
</bodyText>
<listItem confidence="0.9658055">
b. trans-mission
c. under-stand
d. re-ply
e. hard-ly
</listItem>
<bodyText confidence="0.999179651162791">
The problem with these words is that they are
morphologically composed of two or more morphemes,
but their meaning is not derivable from the
meaning of these morphemes. Notice that listing
these words as such in the lexicon is not
sufficient. The morphological analysis will still
apply, creating an additional reading on the basis
of the meaning of its parts. To block this process
requires an ad hoc feature, i.e. a specific
feature saying that this word should not be
analysed any further.
Generally speaking, the morpheme-based
lexicon along with its word formation rules, i.e.
the rules that govern the combination of morphemes
is bound to generate far more words (or readings
of words) than what really exists in a particular
language. It is clearly the case that only a
strict subset of the possible combination of
morphemes are actually realized. To put it
differently, it confuses the notion of potential
word for a language with the notion of actual
word4.
This point was already noticed in Halle
(1973), who suggested that in addition to the list
of morphemes and the word formation rules which
characterize the set of possible words, there must
exist a list of actual words which functions as a
filter on the output of word formation rules. This
filter, in other words, accounts for the
difference between potential words and actual
words.
The idiosyncratic behaviour of lexical items
has been further stressed in &amp;quot;Remarks on
Nominalization&amp;quot; where Chomsky convincingly argues
that the meaning of derived nominals, such as
those in (6), cannot be derived by rules fr on the
meaning of its constitutive morphemes. Given the
fact that derivational morphology is semantically
irregular it should not be handled in the syntax.
Chomsky concludes that derived nominals must be
listed as such in the lexicon, the relation
between verb and nominals beeing captured by
lexical redundancy rules.
</bodyText>
<listItem confidence="0.88804225">
(6)a. revolve revolution
b. marry marriage
c. do deed
d. act action
</listItem>
<bodyText confidence="0.999908648648649">
It should be noticed that the somewhat
erratic and unpredictable morphological relations
are not restricted to the domain of what is
traditionally called derivation. As Halle points
out (p. 6), the whole range of exceptional
behaviour observed with derivation can be found
with inflection. Halle gives examples of
accidental gaps such as defective paradigms,
phonological irregularity (accentuation of Russian
nouns) and idiosyncratic meaning.
From a computational point of view,&apos; a
morpheme-based lexicon has few merits beyond the
fact that it is comparatively small in size. In
the generation process as well as in the analysis
process the lack of clear distinction between
possible and actual words makes it unreliable --
i.e. one can never be sure that its output is
correct. Also, since a large number of
morphological rules must systematically be applied
to every single word to make sure that all
possible readings of each word is taken into
consideration, lexical analysis based on such
conceptions of the lexicon are bound to be fairly
inefficient. Over the years, increasingly
sophisticated morphological parsers have been
designed, the best examples being Kay&apos;s (1977),
Karttunen (1983) and Koskeniemmi (1983a,b), but
not surprisingly, the efficiency of such systems
remain well below the simple dictionary lookup .
Also, this model has the dubious property
that the retrieval of an irregular form
necessitates less computation than the retrieval
of a regular form. This is so because unlike
regular forms that have to be created/analyzed
each time they are used, irregular forms are
listed as such in the lexicon. Hence, they can
simply be looked up.
</bodyText>
<page confidence="0.992831">
148
</page>
<bodyText confidence="0.997398301587302">
This rapid and necessarily incomplete
overview of the organization of the lexicon and
the role of morphology in theoretical and
computational linguistics has emphasized two basic
types of requirements: the linguistic requirements
which have to do with descriptive adequacy of the
model, and the computational requirements which
has to do with the efficiency of the process of
lexical analysis or generation. In particular, we
argued that a lexicon consisting of the list of
all the inflected forms without any morphology
fails to meet the first requirement, i.e.
linguistic adequacy. It was also pointed out that
such a model lacks the abstract lexical level
which is relevant, for instance, for lexical
transfer in translation systems. Although clearly
superior to what we called the &amp;quot;no morphology&amp;quot;
system, the traditional morpheme-based model runs
into numerous problems with respect to both
linguistic and computational requirements.
A third type of considerations which are
often overlooked in academical discussions, but
turns out to be of primary importance for any
&amp;quot;real life&amp;quot; system involving a large lexical data
base is what I would call &amp;quot;practical requirements&amp;quot;
and has to do with the complexity of the task of
creating a lexical entry. It can roughly be viewed
as a measure of the time it takes to create a new
lexical entry, and of the amount of linguistic
knowledge that is required to achieve this task.
The relevance of these practical requirements
becomes more and more evident as large natural
language processing systems are being developed.
For instance, a translation system -- or any other
type of natural language processing program that
must be able to handle very large amounts of text
-- necessitates dictionaries of substantial size,
of the order of at least tens of thousands of
entries, perhaps even more than 100,000 lexical
entries. Needless to say the task of creating as
well as the one of updating such huge databases
represents an astronomical investment in terms of
human resources which cannot be overestimated.
Whether it takes an average of, say 3 minutes, to
enter a new lexical entry or 30 minutes may not be
all that important as long as we are considering
lexicons of a few hundred words. It may be the
difference between feasible agd not feasible when
it comes to very big databases .
Another important practical issue is the
level of linguistic knowledge that is required
from the user. Systems which require little
technical knowledge are to be preferred to those
requiring an extensive amount of linguistic
background, everything else being equal. It should
be clear, in this respect, that morpheme-based
Lexicons tend to require more linguistic knowledge
from the user than a word-based lexicon, since the
user has to specify (i) what the morphological
structure of the word is (ii) to what extent the
meaning of the word is or is not derived from the
meaning of its parts, (iii) what
morphophonological rules apply in the derivation
of this word.
A RELATIONAL WORD-BASED LEXICON
The traditional view in computational
linguistics is to assume some version of the
morpheme-based lexicon, coupled with a
morphological analyzer/generator. Thus it is
assumed that a dynamic morphological process takes
place both in the analysis and in the generation
of words (i.e. orthographical words). Each time a
word is read or heard, it is decomposed into its
atomic constituents and each time it is produced
it has t9 be re-created from its atomic
constituents .
As I pointed out earlier, I don&apos;t see any
compelling evidence supporting this view other
than the simplicity argument. Crucial for this
argument, then, is the assumption that the
complexity measure is just a measure of the length
of the lexicon, i.e. the sum of the symbols
contained in the lexicon.
One cannot exclude, though, more
sophisticated ways to mesure the complexity of the
lexicon. Jackendoff (1975:640) suggests an
alternative complexity measure based on
&amp;quot;independent information content&amp;quot;. Intuitively,
the idea is that redundant information that is
predictable by the existence gf a redundancy rule
does not count as independent .
Assumimg a strict lexicalist framework a la
Jackendoff, we developed a word-based lexical
database dubbed relational word-based lexicon
(RWL). Essentially, the RWL model is a list-type
lexicon with cross references. All the words of
the language are listed in such a lexicon and have
independent lexical entries. The morphological
relations between two or more lexical entries are
captured by a complex network of relations. The
basic idea underlying this organization is to
factor out properties shared by several lexical
entries.
To take a simple example, all the
morphological forms of the English verb run have a
lexical entry. Hence, run, runs, ran and running
are listed independently in the lexicon. At the
same time, however, these four lexical entries are
to be related in some way to express the fact that
they are morphologically related, i.e. they belong
to the same paradigm. In turns, this has the
further advantage of providing a clear definition
of the &amp;quot;lexeme&amp;quot;, the abstract lexical unit which
is relevant, for instance, for lexical transfer,
as will be pointed out below.
In contrast with the common use in
computational linguistics, gin this model
morphology is essentially static . By interpreting
morphology as relations within the lexical
database rather than as a process, we shift some
complexity from the parsing algorithm to the
lexical data structures. Whether or not this shift
is justified from a linguistic point of view is an
open question, and I have nothing to say about it
here. From a computational point of view, though,
this shift has rather interesting consequences.
</bodyText>
<page confidence="0.998135">
149
</page>
<bodyText confidence="0.999838916666667">
First of all, it drastically simplifies the
task of lexical analysis (or generation), making
it a deterministic process -- as opposed to a
necessarily non-deterministic morphological
parser. In fact, it makes lexical analysis rather
trivial, equating it with a fairly simple database
query. It follows that the process of retrieving
an irregular word is identical to the process of
retrieving a regular word. The distinction between
regular morphological forms and exceptional ones
has no effect on the lexical analysis, i.e. on
processing. Rather, it affects the complexity
measure of the lexicon.
Also, in sharp contrast to what happens with
a derivational conception of morphology, in our
model, the morphological complexity of a language
has very little effect on the efficiency of
lexical analysis, which seems essentially correct:
speakers of morphologically complex languages do
not seem to require significantly more time to
parse individual words than speakers of, say,
English.
A partial implementation of this relational
word-based model of the lexicon has been realized
for the parser for French described in Wehrli
(1984). This section describes some of the
features of this implementation. Only inflection
has been implemented, so far. Some aspects of
derivational morphology should be added in the
near future.
In this implementation, lexical entries are
composed of three distinct kinds of objects
referred to as words, morpho-syntactic elements
and lexemes, cf. figure 1. A word is simply a
string of characters, or what is sometimes called
an orthographic word. It is linked to a set of
morpho-syntactic elements, each one of them
specifying a particular grammatical reading of the
word. A morpho-syntactic element is a just a
particular set of grammatical features such as
category, gender, number, person, case, etc. A
lexeme contains all the information shared by all
the flectional forms of a given lexical item. The
lexeme is defined as a set of syntactic and
semantic features shared by one or several
morpho-syntactic elements. Roughly speaking, it
contains the kind of information one expect to
find in a standard dictionary entry.
</bodyText>
<figure confidence="0.536174833333333">
Words Morpho-syntactic elements
Lexemes
est
est
`east&apos;
.N, sg.
</figure>
<bodyText confidence="0.945395666666667">
V, 3rd sg. pres.
Adv, inter. prtc. I
Adv, inter. prt I
ete
V, past part.
etre N, sg.
V, inf.
sommes V. 1st pl. ores.
ete
</bodyText>
<figure confidence="0.982294818181818">
&apos;summer&apos;
etre
&apos;being&apos;
&apos;Are
&apos;to be&apos;
etre (aux.)
â€¢ &apos;to be
est-ce que I
est-ce qu&apos;
somme
&apos;amount&apos;
</figure>
<figureCaption confidence="0.872517">
suis V. 1st sg. pres.
suivre
to follow&apos;
V. 1-2 sg. ores.
Figure 1: Structure of the lexicon
</figureCaption>
<page confidence="0.99335">
150
</page>
<bodyText confidence="0.999900792000001">
In relational terms, fully-specified lexical
entries are broken into three different relations.
The full set of information belonging to a lexical
entry can be obtained by intersecting the three
relations.
The following example illustrates the
structure of the lexical data base and the
respective roles of words, morpho-syntactic
elements and lexemes. In French, suis is
ambiguous. It is the first person singular present
tense of the verb etre (&apos;to be&apos;), which, as in
English, is both a verb and an auxiliary. But suis
is also the first and second person singular
present tense of the verb suivre (&apos;to follow&apos;).
This information is represented as follows: the
lexicon has a word (in the technical sense, i.e. a
string of characters) suis associated with two
morpho-syntactic elements. The first
morpho-syntactic element which bears the features
[+V, 1st, sg, present] is linked to a list of two
lexemes. One of them contains all the general
properties of the verb etre, the other one the
information corresponding to the auxiliary reading
of etre. As for the second morpho-syntactic
element, it bears the features [+V, lst-2nd, sg,
present] and it is related to the lexeme
containing the syntactic and semantic features
characterizing the verb suivre.
Such an organization allows for a substantial
reduction of redundancy. All the different
morphological forms of etre, i.e. over 25
different words are ultimately linked to 2 lexemes
(verbal and auxiliary readings). Thus, information
about subcategorization, selectional restrictions,
etc. is specified only once rather than 25 times
or more. Naturally, this concentration of the
information also simplifies the updating
procedure. Also, as we pointed out above, this
structure provides a clear definition of &amp;quot;lexeme&amp;quot;,
the abstract lexical representation, which is the
level of representation relevant for transfer in
translation systems.
Figure 1, above, illustrates the structure of
the lexical database. Boxes stand for the
different items (words, morphosyntactic elements,
lexemes) and arrows represent the relations
between these items. Notice that not all
morphosyntactic elements are associated with some
lexemes. In fact, there is a lexeme level only for
those categories which display morphological
variation, i.e. nouns, adjectives, verbs and
determiners.
The arrow between the words est and est-ce
que expresses the fact that the string est occurs
at the initial of the compound est-ce que. This is
the way compounds are dealt with in this lexicon.
The compound clair de lune (&apos;moonlight&apos;) is listed
as an independent word -- along with its
associated morphosyntactic elements and lexemes --
related to the word clair. The function of this
relation is to signal to the analyzer that the
word clair is also the first segment of a
compound.
Consider the vertical arrow between the
lexeme corresponding to the verbal reading of etre
(&apos;to be&apos;) and the lexeme corresponding to the
auxiliary reading of etre. It expresses the fact
that a given morphosyntactic element may have
several distinct readings (in this case the verbal
reading and the auxiliary reading). Thus,
morphosyntactic elements can be related not just
to one lexeme, but to a list of lexemes.
The role of morphology in Jackendoff&apos;s system
is double. First, the redundancy rules have a
static role, which is to describe morphological
patterns in the language, and thus to account for
word-structure. In addition to this primary role,
morphology also assumes a secondary role, in the
sense that it can be used to produce new words or
to analyze words that are not present in the
lexicon. In this respect, Jackendoff (1975:668)
notes, &amp;quot;lexical redundacy rules are learned form
generalizations observed in already known lexical
items. Once learned, they make it easier to learn
new lexical items&amp;quot;. In other words, redundancy
rules can also function as word f8rmation rules
and, hence, have a dynamic function .
In our implementation of the relational
word-based lexicon, morphology has also a double
function. On the one hand, morphological relations
are embedded in the structure of the database
itself and, roughly, correspond to Jackendoff&apos;s
redundancy rules in their static role. On the
other hand, morphological rules are considered as
&amp;quot;learning rules&amp;quot;, i.e. as devices which facilitate
the acquisition of the paradigm of the inflected
forms of a new lexeme. As such, morphological
rules apply when a new word is entered in the
lexicon. Their role is to help and assist the user
in his/her task of entering new lexical entries.
For example, if the infinitival form of a verb is
entered, the morphological rules are used to
create all the inflected forms, in an interactive
session. So, for instance, the system first
considers the verb to be morphologically regular.
If so, that is if the user confirms this
hypothesis, the system generates all the inflected
forms without further assistance. If the answer is
no, the system will try another hypothesis,
looking for subregularities.
Our relational word-based lexicon was first
implemented on a relational database system on a
VAX-780. However, for efficiency reasons, it was
transfered to a more conventional system using
indexed sequential and direct access files. In its
present implementation, on a VAX-750, words and
morphosyntactic elements are stored in indexed
sequential files, lexemes in direct access files.
In other words, the lexicon is entirely stored in
external files, which can be expanded, practically
without affecting the efficiency of the system. A
set of menu-oriented procedures allow the user to
interact with the lexical data base, to either
insert, delete, update or just visualize words and
their lexical specifications.
</bodyText>
<page confidence="0.997805">
151
</page>
<sectionHeader confidence="0.989222" genericHeader="conclusions">
CONCLUSION
</sectionHeader>
<bodyText confidence="0.999975219512195">
Several important issues have been discussed
in this paper, regarding the structure and the
function of the lexicon, as well as the role of
morphology. We first pointed out the important
role of morphology and showed that it cannot be
dispensed with, even in processing systems with no
particular psychological claim. Hence, an
exhaustive list of all the orthographic forms of
English words cannot stand for an adequate lexicon
of English.
Turning then to what appears to be the
traditional conception of morphology in
computational linguistics, we showed that a
morpheme-based lexicon, along with a derivational
morphological component faces a variety of serious
problems, including its inability to distinguish
actual words from potential words, its inability
to express partial morphological or semantic
relations, as well as its inherent inefficiency
and often lack of reliability.
The success of this traditional conception of
the lexicon in computational linguistics must
probably be attributed to its relative
conciseness. However, alternative ways to evaluate
the complexity of lexical entries, i.e.
Jackendoff&apos;s independent information content, as
well as the emergence of cheap and abundant memory
have drastically modify this state of affair, and
open new perspectives more in line with current
research in theoretical linguistics.
To the traditional view, we opposed a
relational word-based lexicon, along the lines of
Jackendoff&apos;s (1975) proposal, where morphology can
be viewed, in part, as relations among lexical
entries. Simple words, complex words, compounds,
etc., are all listed in our lexicon. But lexical
entries which belong to a same paradigm are
related to the same lexeme. Rather than deriving
or analyzing words each time they are used,
morphological rules only serve when a new word
occurs.
</bodyText>
<sectionHeader confidence="0.796818" genericHeader="acknowledgments">
FOOTNOTES
</sectionHeader>
<bodyText confidence="0.9310003">
1. One might think of compromises between these
two options, such as, for instance, the
stem-based lexicon argued for in Anderson
(1982), where lexical entries consists of stems
rather than morphemes, and an independent
morphological component is responsible for the
derivation of inflectional forms.
Aronoff&apos;s (1976) proposal can also be viewed as
a compromise solution. See footnote 2.
2. It should be pointed out that other word-based
theories have been proposed. For instance,
Aronoff (1976) argues for a word-based lexicon
where only words which are atomic or exceptional
in one way or another are entered in the
lexicon.
3. In this paper, I will simply consider
inflectional morphology as the adunction to
words of affixes which only modify features such
as tense, person, number, gender, case, etc. as
in read-s, read-ing, book-s. Derivational
morphology, on the other hand, deals with the
addition of affixes which can modify the meaning
of the word, and very often its categorial
status, e.g. use-ful, use-ful-ness, hard-lv.
4. Potential words are words that are well-formed
with respect to word formation rules, whereas
the actual words are the those potential words
that are realized in this language. To give an
example, both arrival and arrivation are
potential English words, but only the second
happens to be an actual English word.
5. For instance, Koskeniemmi (1983b) mentions an
average of 100 milliseconds per words on a
DEC-20.
6. This figure is indeed very conservative. Slocum
(1982:8) reports that the cost of writing a
dictionary entry for the TAUM-Aviation project
was estimated at 3.75 man-hours...
7. This conception is yet another example of the
&amp;quot;historicist approach&amp;quot; typical of classical
transformational generative grammar, which
assumes that synchronic processes recapitulates
many of the diachronic developments.
8. The following is an approximation of how
independent information can be measu red:
&amp;quot;(Information measure)
Given a fully specified leixcal entry W to be
introduced into the lexicon, the independent
information it adds to the lexicon is
(a) the information that W exists in the
lexicon, i.e. that W is a word of the
language; plus
(b) all the information in W which cannot be
predicted by the existence of some
redundancy rule R which permits W to be
partially described in terms of information
already in the lexicon; plus
(c) the cost of referring to the redundancy
rule R.
9. It will be argued below that morphology has a
secondary role, which is to facilitate the
acquisition of new words.
10. In the conclusion of his &amp;quot;Prolegomena&amp;quot; Halle
also mentions the possibility th at word
formation rules be used when the speaker hears
an unfamiliar word or when he uses a word freely
invented.
11. From a psychological point of view, it could
also be argued that morphology facilitates
memorization.
</bodyText>
<page confidence="0.997615">
152
</page>
<sectionHeader confidence="0.99292" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.993495625">
Anderson, S. R. (1982). &amp;quot;Where is morphology?&amp;quot;,
Linguistic Inquiry.
Aronoff, M. (1976). Word Fromation in Generative
Grammar, Linguistic Inquiry Monograph One,
MIT Press.
Bresnan, J. (1977). &amp;quot;A realistic transformational
grammar&amp;quot;, in Halle, M., J. Bresnan and G.A.
Miller (eds.) Linguistic Theory and
Psychological Reality, MIT Press.
Chomsky, N. (1957). Syntactic Structures, Mouton.
Chomsky, N. (1965). Aspects of the Theory of
Syntax, MIT Press.
Chomsky, N. (1970). &amp;quot;Remarks on nominalization&amp;quot;,
Studies on Semantics in Generative Grammar,
Mouton.
Halle, M. (1973). &amp;quot;Prolegomena to a theory of word
formation&amp;quot;, Linguistic Inquiry, 4.1. pp.
3-16.
Hoekstra, T., H. van der Hulst and M. Moortgat
(1983). Lexical Grammar, Foris.
Jackendoff, R. (1975). &amp;quot;Morphological and semantic
regularities in the lexicon&amp;quot;, Language 51.3,
pp. 639-671.
Karttunen, L. (1983). &amp;quot;IIMMO: A general
morphological processor&amp;quot;. Texas Linguistic
Forum, No. 22, pp. 165-228.
Kay, M. (1977). &amp;quot;Morphological and syntactic
analysis&amp;quot;, in A. Zampoli (ed.) Linguistic
Structures Processing, North-Holland.
Koskenniemi, K. (1983a). Two-Level Morphology: A
General Computational Model For Word-Form
Recognition And Production, Publications No
11, Uaiversity of Helsinki.
Koskenniemi, K. (1983b). &amp;quot;Two-Level Model for
Morphological Analysis&amp;quot;, Proceedings of the
Eighth International Joint Conference on
Artificial Intelligence, pp. 683-685, William
Kaufmann, Inc.
Lieber, R. (1980). On the Organization of the
Lexicon, Ph.D. Dissertation, MIT.
</reference>
<bodyText confidence="0.797452888888889">
Selkirk, E. (1982). The Syntax of Words.
Linguistic Inquiry Monograph Seven, MIT
Press.
Slocum, J. (1981). &amp;quot;Machine translation: its
history, current status and future
prospects&amp;quot;, mimeo, University of Texas.
Wehrli, E.,(1984). &amp;quot;A Government-Binding parser
for French&amp;quot;, working paper no 48,
ISSCO-Geneva University.
</bodyText>
<page confidence="0.998878">
153
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.889058">
<title confidence="0.996367">DESIGN AND IMPLEMENTATION OF A LEXICAL DATA BASE</title>
<author confidence="0.999996">Eric Wehrli</author>
<affiliation confidence="0.9824915">Department of Linguistics U.C.L.A.</affiliation>
<address confidence="0.998488">405 Hilgard Ave, Los Angeles, CA 90024</address>
<abstract confidence="0.994621928571429">This paper is concerned with the specifications and the implementation of a particular concept of word-based lexicon to be used for large natural language processing systems such as machine translation systems, and compares it with the morpheme-based conception of the lexicon traditionally assumed in computational linguistics. It will be argued that, although less concise, a relational word-based lexicon is morpheme-based lexicon from a theoretical, computational and also practical viewpoint.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S R Anderson</author>
</authors>
<title>Where is morphology?&amp;quot;, Linguistic Inquiry.</title>
<date>1982</date>
<contexts>
<context position="30755" citStr="Anderson (1982)" startWordPosition="4858" endWordPosition="4859">traditional view, we opposed a relational word-based lexicon, along the lines of Jackendoff&apos;s (1975) proposal, where morphology can be viewed, in part, as relations among lexical entries. Simple words, complex words, compounds, etc., are all listed in our lexicon. But lexical entries which belong to a same paradigm are related to the same lexeme. Rather than deriving or analyzing words each time they are used, morphological rules only serve when a new word occurs. FOOTNOTES 1. One might think of compromises between these two options, such as, for instance, the stem-based lexicon argued for in Anderson (1982), where lexical entries consists of stems rather than morphemes, and an independent morphological component is responsible for the derivation of inflectional forms. Aronoff&apos;s (1976) proposal can also be viewed as a compromise solution. See footnote 2. 2. It should be pointed out that other word-based theories have been proposed. For instance, Aronoff (1976) argues for a word-based lexicon where only words which are atomic or exceptional in one way or another are entered in the lexicon. 3. In this paper, I will simply consider inflectional morphology as the adunction to words of affixes which o</context>
</contexts>
<marker>Anderson, 1982</marker>
<rawString>Anderson, S. R. (1982). &amp;quot;Where is morphology?&amp;quot;, Linguistic Inquiry.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Aronoff</author>
</authors>
<title>Word Fromation in Generative Grammar, Linguistic Inquiry Monograph One,</title>
<date>1976</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4441" citStr="Aronoff, 1976" startWordPosition="687" endWordPosition="688">nstitutes a lexical entry, whether or not lexical items should be related to one another, etc. have been much debated over the last 10 or 15 years within the framework of generative grammar. Considered as a relatively minor appendix of the phrase-structure rule component in the early days of generative grammar, the lexicon became little by little an autonomous component of the grammar with its own specific formalism -- lexical entries as matrices of features, as advocated by Chomsky (1965). Finally, it also acquired specific types of rules, the so-called word formation rules (cf. Halle, 1973; Aronoff, 1976; Lieber, 1980; Selkirk, 1983, and others), and lexical redundancy rules (cf. Jackendoff, 1975; Bresnan, 1977). By and large, there seems to be widespread agreement among linguists that the lexicon should be viewed as the repository of all the idiosyncratic properties of the lexical items of a language (phonological, morphological, syntactic, semantic, etc.). This agreement quickly disappears, however, when it comes to defining what constitutes a lexical item, or, to put it slightly differently, what the lexicon is a list of, and how should it be organized. Among the many proposals discussed i</context>
<context position="31114" citStr="Aronoff (1976)" startWordPosition="4911" endWordPosition="4912"> deriving or analyzing words each time they are used, morphological rules only serve when a new word occurs. FOOTNOTES 1. One might think of compromises between these two options, such as, for instance, the stem-based lexicon argued for in Anderson (1982), where lexical entries consists of stems rather than morphemes, and an independent morphological component is responsible for the derivation of inflectional forms. Aronoff&apos;s (1976) proposal can also be viewed as a compromise solution. See footnote 2. 2. It should be pointed out that other word-based theories have been proposed. For instance, Aronoff (1976) argues for a word-based lexicon where only words which are atomic or exceptional in one way or another are entered in the lexicon. 3. In this paper, I will simply consider inflectional morphology as the adunction to words of affixes which only modify features such as tense, person, number, gender, case, etc. as in read-s, read-ing, book-s. Derivational morphology, on the other hand, deals with the addition of affixes which can modify the meaning of the word, and very often its categorial status, e.g. use-ful, use-ful-ness, hard-lv. 4. Potential words are words that are well-formed with respec</context>
</contexts>
<marker>Aronoff, 1976</marker>
<rawString>Aronoff, M. (1976). Word Fromation in Generative Grammar, Linguistic Inquiry Monograph One, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bresnan</author>
</authors>
<title>A realistic transformational grammar&amp;quot;,</title>
<date>1977</date>
<booktitle>Linguistic Theory and Psychological Reality,</booktitle>
<editor>in Halle, M., J. Bresnan and G.A. Miller (eds.)</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4551" citStr="Bresnan, 1977" startWordPosition="702" endWordPosition="703"> debated over the last 10 or 15 years within the framework of generative grammar. Considered as a relatively minor appendix of the phrase-structure rule component in the early days of generative grammar, the lexicon became little by little an autonomous component of the grammar with its own specific formalism -- lexical entries as matrices of features, as advocated by Chomsky (1965). Finally, it also acquired specific types of rules, the so-called word formation rules (cf. Halle, 1973; Aronoff, 1976; Lieber, 1980; Selkirk, 1983, and others), and lexical redundancy rules (cf. Jackendoff, 1975; Bresnan, 1977). By and large, there seems to be widespread agreement among linguists that the lexicon should be viewed as the repository of all the idiosyncratic properties of the lexical items of a language (phonological, morphological, syntactic, semantic, etc.). This agreement quickly disappears, however, when it comes to defining what constitutes a lexical item, or, to put it slightly differently, what the lexicon is a list of, and how should it be organized. Among the many proposals discussed in the linguistic literature, I will consider two radically opposed views that I shall call the morpheme-bayed </context>
</contexts>
<marker>Bresnan, 1977</marker>
<rawString>Bresnan, J. (1977). &amp;quot;A realistic transformational grammar&amp;quot;, in Halle, M., J. Bresnan and G.A. Miller (eds.) Linguistic Theory and Psychological Reality, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Syntactic Structures,</title>
<date>1957</date>
<location>Mouton.</location>
<marker>Chomsky, 1957</marker>
<rawString>Chomsky, N. (1957). Syntactic Structures, Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax,</title>
<date>1965</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4322" citStr="Chomsky (1965)" startWordPosition="669" endWordPosition="670"> and are listed in the same entry in the dictionary.&amp;quot; Some of these problems, as well as the general question of what constitutes a lexical entry, whether or not lexical items should be related to one another, etc. have been much debated over the last 10 or 15 years within the framework of generative grammar. Considered as a relatively minor appendix of the phrase-structure rule component in the early days of generative grammar, the lexicon became little by little an autonomous component of the grammar with its own specific formalism -- lexical entries as matrices of features, as advocated by Chomsky (1965). Finally, it also acquired specific types of rules, the so-called word formation rules (cf. Halle, 1973; Aronoff, 1976; Lieber, 1980; Selkirk, 1983, and others), and lexical redundancy rules (cf. Jackendoff, 1975; Bresnan, 1977). By and large, there seems to be widespread agreement among linguists that the lexicon should be viewed as the repository of all the idiosyncratic properties of the lexical items of a language (phonological, morphological, syntactic, semantic, etc.). This agreement quickly disappears, however, when it comes to defining what constitutes a lexical item, or, to put it sl</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Chomsky, N. (1965). Aspects of the Theory of Syntax, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Remarks on nominalization&amp;quot;,</title>
<date>1970</date>
<booktitle>Studies on Semantics in Generative Grammar,</booktitle>
<location>Mouton.</location>
<contexts>
<context position="10043" citStr="Chomsky (1970)" startWordPosition="1562" endWordPosition="1563">nguage systems. There is no doubt that restricting the lexicon to 147 basic morphemes and deriving all complex words as well as all the inflected forms by morphological rules, reduces substantially the size of the lexicon. This was indeed a crucial issue not so long ago, when computer memory was scarce and expensive. There are, however, numerous problems -- linguistic, computational as well as practical -- with the morpheme-based conception of the lexicon. Its inadequacy from a theoretical linguistic point of view has been discussed abundantly in the &amp;quot;lexicalist&amp;quot; literature. See in particular Chomsky (1970), Halle (1973) and Jackendoff (1975). Some of the linguistic problems are summarized below, along with some mentions of computational as well as practical problems inherent to this approach. First of all, from a conceptual point of view, the adoption of a derivational model of morphology suggests that the derivation of a word is very similar, as a process, to the derivation of a sentence. Such a view, however, fails to recognize some fundamental distinctions between the syntax of words and the syntax of sentences, for instance regarding creativity. Whereas the vast majority of the words we use</context>
</contexts>
<marker>Chomsky, 1970</marker>
<rawString>Chomsky, N. (1970). &amp;quot;Remarks on nominalization&amp;quot;, Studies on Semantics in Generative Grammar, Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Halle</author>
</authors>
<title>Prolegomena to a theory of word formation&amp;quot;,</title>
<date>1973</date>
<journal>Linguistic Inquiry,</journal>
<volume>4</volume>
<pages>3--16</pages>
<contexts>
<context position="4426" citStr="Halle, 1973" startWordPosition="685" endWordPosition="686">on of what constitutes a lexical entry, whether or not lexical items should be related to one another, etc. have been much debated over the last 10 or 15 years within the framework of generative grammar. Considered as a relatively minor appendix of the phrase-structure rule component in the early days of generative grammar, the lexicon became little by little an autonomous component of the grammar with its own specific formalism -- lexical entries as matrices of features, as advocated by Chomsky (1965). Finally, it also acquired specific types of rules, the so-called word formation rules (cf. Halle, 1973; Aronoff, 1976; Lieber, 1980; Selkirk, 1983, and others), and lexical redundancy rules (cf. Jackendoff, 1975; Bresnan, 1977). By and large, there seems to be widespread agreement among linguists that the lexicon should be viewed as the repository of all the idiosyncratic properties of the lexical items of a language (phonological, morphological, syntactic, semantic, etc.). This agreement quickly disappears, however, when it comes to defining what constitutes a lexical item, or, to put it slightly differently, what the lexicon is a list of, and how should it be organized. Among the many propos</context>
<context position="10057" citStr="Halle (1973)" startWordPosition="1564" endWordPosition="1565">There is no doubt that restricting the lexicon to 147 basic morphemes and deriving all complex words as well as all the inflected forms by morphological rules, reduces substantially the size of the lexicon. This was indeed a crucial issue not so long ago, when computer memory was scarce and expensive. There are, however, numerous problems -- linguistic, computational as well as practical -- with the morpheme-based conception of the lexicon. Its inadequacy from a theoretical linguistic point of view has been discussed abundantly in the &amp;quot;lexicalist&amp;quot; literature. See in particular Chomsky (1970), Halle (1973) and Jackendoff (1975). Some of the linguistic problems are summarized below, along with some mentions of computational as well as practical problems inherent to this approach. First of all, from a conceptual point of view, the adoption of a derivational model of morphology suggests that the derivation of a word is very similar, as a process, to the derivation of a sentence. Such a view, however, fails to recognize some fundamental distinctions between the syntax of words and the syntax of sentences, for instance regarding creativity. Whereas the vast majority of the words we use are fixed exp</context>
<context position="12148" citStr="Halle (1973)" startWordPosition="1910" endWordPosition="1911"> feature, i.e. a specific feature saying that this word should not be analysed any further. Generally speaking, the morpheme-based lexicon along with its word formation rules, i.e. the rules that govern the combination of morphemes is bound to generate far more words (or readings of words) than what really exists in a particular language. It is clearly the case that only a strict subset of the possible combination of morphemes are actually realized. To put it differently, it confuses the notion of potential word for a language with the notion of actual word4. This point was already noticed in Halle (1973), who suggested that in addition to the list of morphemes and the word formation rules which characterize the set of possible words, there must exist a list of actual words which functions as a filter on the output of word formation rules. This filter, in other words, accounts for the difference between potential words and actual words. The idiosyncratic behaviour of lexical items has been further stressed in &amp;quot;Remarks on Nominalization&amp;quot; where Chomsky convincingly argues that the meaning of derived nominals, such as those in (6), cannot be derived by rules fr on the meaning of its constitutive </context>
</contexts>
<marker>Halle, 1973</marker>
<rawString>Halle, M. (1973). &amp;quot;Prolegomena to a theory of word formation&amp;quot;, Linguistic Inquiry, 4.1. pp. 3-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hoekstra</author>
<author>H van der Hulst</author>
<author>M Moortgat</author>
</authors>
<title>Lexical Grammar,</title>
<date>1983</date>
<location>Foris.</location>
<marker>Hoekstra, van der Hulst, Moortgat, 1983</marker>
<rawString>Hoekstra, T., H. van der Hulst and M. Moortgat (1983). Lexical Grammar, Foris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>Morphological and semantic regularities in the lexicon&amp;quot;,</title>
<date>1975</date>
<journal>Language</journal>
<volume>51</volume>
<pages>639--671</pages>
<contexts>
<context position="4535" citStr="Jackendoff, 1975" startWordPosition="700" endWordPosition="701">tc. have been much debated over the last 10 or 15 years within the framework of generative grammar. Considered as a relatively minor appendix of the phrase-structure rule component in the early days of generative grammar, the lexicon became little by little an autonomous component of the grammar with its own specific formalism -- lexical entries as matrices of features, as advocated by Chomsky (1965). Finally, it also acquired specific types of rules, the so-called word formation rules (cf. Halle, 1973; Aronoff, 1976; Lieber, 1980; Selkirk, 1983, and others), and lexical redundancy rules (cf. Jackendoff, 1975; Bresnan, 1977). By and large, there seems to be widespread agreement among linguists that the lexicon should be viewed as the repository of all the idiosyncratic properties of the lexical items of a language (phonological, morphological, syntactic, semantic, etc.). This agreement quickly disappears, however, when it comes to defining what constitutes a lexical item, or, to put it slightly differently, what the lexicon is a list of, and how should it be organized. Among the many proposals discussed in the linguistic literature, I will consider two radically opposed views that I shall call the</context>
<context position="10079" citStr="Jackendoff (1975)" startWordPosition="1567" endWordPosition="1568"> that restricting the lexicon to 147 basic morphemes and deriving all complex words as well as all the inflected forms by morphological rules, reduces substantially the size of the lexicon. This was indeed a crucial issue not so long ago, when computer memory was scarce and expensive. There are, however, numerous problems -- linguistic, computational as well as practical -- with the morpheme-based conception of the lexicon. Its inadequacy from a theoretical linguistic point of view has been discussed abundantly in the &amp;quot;lexicalist&amp;quot; literature. See in particular Chomsky (1970), Halle (1973) and Jackendoff (1975). Some of the linguistic problems are summarized below, along with some mentions of computational as well as practical problems inherent to this approach. First of all, from a conceptual point of view, the adoption of a derivational model of morphology suggests that the derivation of a word is very similar, as a process, to the derivation of a sentence. Such a view, however, fails to recognize some fundamental distinctions between the syntax of words and the syntax of sentences, for instance regarding creativity. Whereas the vast majority of the words we use are fixed expressions that we have </context>
<context position="18675" citStr="Jackendoff (1975" startWordPosition="2959" endWordPosition="2960">ion of words (i.e. orthographical words). Each time a word is read or heard, it is decomposed into its atomic constituents and each time it is produced it has t9 be re-created from its atomic constituents . As I pointed out earlier, I don&apos;t see any compelling evidence supporting this view other than the simplicity argument. Crucial for this argument, then, is the assumption that the complexity measure is just a measure of the length of the lexicon, i.e. the sum of the symbols contained in the lexicon. One cannot exclude, though, more sophisticated ways to mesure the complexity of the lexicon. Jackendoff (1975:640) suggests an alternative complexity measure based on &amp;quot;independent information content&amp;quot;. Intuitively, the idea is that redundant information that is predictable by the existence gf a redundancy rule does not count as independent . Assumimg a strict lexicalist framework a la Jackendoff, we developed a word-based lexical database dubbed relational word-based lexicon (RWL). Essentially, the RWL model is a list-type lexicon with cross references. All the words of the language are listed in such a lexicon and have independent lexical entries. The morphological relations between two or more lexi</context>
<context position="26696" citStr="Jackendoff (1975" startWordPosition="4234" endWordPosition="4235">y have several distinct readings (in this case the verbal reading and the auxiliary reading). Thus, morphosyntactic elements can be related not just to one lexeme, but to a list of lexemes. The role of morphology in Jackendoff&apos;s system is double. First, the redundancy rules have a static role, which is to describe morphological patterns in the language, and thus to account for word-structure. In addition to this primary role, morphology also assumes a secondary role, in the sense that it can be used to produce new words or to analyze words that are not present in the lexicon. In this respect, Jackendoff (1975:668) notes, &amp;quot;lexical redundacy rules are learned form generalizations observed in already known lexical items. Once learned, they make it easier to learn new lexical items&amp;quot;. In other words, redundancy rules can also function as word f8rmation rules and, hence, have a dynamic function . In our implementation of the relational word-based lexicon, morphology has also a double function. On the one hand, morphological relations are embedded in the structure of the database itself and, roughly, correspond to Jackendoff&apos;s redundancy rules in their static role. On the other hand, morphological rules </context>
</contexts>
<marker>Jackendoff, 1975</marker>
<rawString>Jackendoff, R. (1975). &amp;quot;Morphological and semantic regularities in the lexicon&amp;quot;, Language 51.3, pp. 639-671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Karttunen</author>
</authors>
<title>IIMMO: A general morphological processor&amp;quot;.</title>
<date>1983</date>
<journal>Texas Linguistic Forum,</journal>
<volume>22</volume>
<pages>165--228</pages>
<contexts>
<context position="14297" citStr="Karttunen (1983)" startWordPosition="2247" endWordPosition="2248">he generation process as well as in the analysis process the lack of clear distinction between possible and actual words makes it unreliable -- i.e. one can never be sure that its output is correct. Also, since a large number of morphological rules must systematically be applied to every single word to make sure that all possible readings of each word is taken into consideration, lexical analysis based on such conceptions of the lexicon are bound to be fairly inefficient. Over the years, increasingly sophisticated morphological parsers have been designed, the best examples being Kay&apos;s (1977), Karttunen (1983) and Koskeniemmi (1983a,b), but not surprisingly, the efficiency of such systems remain well below the simple dictionary lookup . Also, this model has the dubious property that the retrieval of an irregular form necessitates less computation than the retrieval of a regular form. This is so because unlike regular forms that have to be created/analyzed each time they are used, irregular forms are listed as such in the lexicon. Hence, they can simply be looked up. 148 This rapid and necessarily incomplete overview of the organization of the lexicon and the role of morphology in theoretical and co</context>
</contexts>
<marker>Karttunen, 1983</marker>
<rawString>Karttunen, L. (1983). &amp;quot;IIMMO: A general morphological processor&amp;quot;. Texas Linguistic Forum, No. 22, pp. 165-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Morphological and syntactic analysis&amp;quot;,</title>
<date>1977</date>
<booktitle>Linguistic Structures Processing,</booktitle>
<editor>in A. Zampoli (ed.)</editor>
<publisher>North-Holland.</publisher>
<contexts>
<context position="3354" citStr="Kay (1977)" startWordPosition="505" endWordPosition="506">ave a non-trivial internal structure, or (iii) be part of compounds or idiomatic expressions, as illustrated in (1)-(4): (1) ambiguous words: can, fly, bank, pen, race, etc. (2) internal structure: use-ful-ness, mis-understand-ing, lake-s, tri-ed (3) compounds: milkman, moonlight, etc. (4) idiomatic expressions: to kick the bucket, by and large, to pull someone&apos;s leg, etc. In fact, the notion of word, itself, is not all that clear, as numerous linguists -- theoreticians and/or computational linguists -- have acknowledged. Thus, to take an example from the computational linguistics literature, Kay (1977) notes: &amp;quot;In common usage, the term word refers sometimes to sequences of letters that can be bounded by spaces or punctuation marks in a text. According to this view, run, runs, running and ran are different words. But common usage also allows these to count as instances of the same word because they belong to the 146 same paradigm in English accidence and are listed in the same entry in the dictionary.&amp;quot; Some of these problems, as well as the general question of what constitutes a lexical entry, whether or not lexical items should be related to one another, etc. have been much debated over the</context>
</contexts>
<marker>Kay, 1977</marker>
<rawString>Kay, M. (1977). &amp;quot;Morphological and syntactic analysis&amp;quot;, in A. Zampoli (ed.) Linguistic Structures Processing, North-Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Koskenniemi</author>
</authors>
<title>Two-Level Morphology: A General Computational Model For Word-Form Recognition And Production, Publications No 11, Uaiversity of Helsinki.</title>
<date>1983</date>
<marker>Koskenniemi, 1983</marker>
<rawString>Koskenniemi, K. (1983a). Two-Level Morphology: A General Computational Model For Word-Form Recognition And Production, Publications No 11, Uaiversity of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Koskenniemi</author>
</authors>
<title>Two-Level Model for Morphological Analysis&amp;quot;,</title>
<date>1983</date>
<booktitle>Proceedings of the Eighth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>683--685</pages>
<publisher>Kaufmann, Inc.</publisher>
<location>William</location>
<marker>Koskenniemi, 1983</marker>
<rawString>Koskenniemi, K. (1983b). &amp;quot;Two-Level Model for Morphological Analysis&amp;quot;, Proceedings of the Eighth International Joint Conference on Artificial Intelligence, pp. 683-685, William Kaufmann, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lieber</author>
</authors>
<title>On the Organization of the Lexicon,</title>
<date>1980</date>
<location>Ph.D. Dissertation, MIT.</location>
<contexts>
<context position="4455" citStr="Lieber, 1980" startWordPosition="689" endWordPosition="690">ical entry, whether or not lexical items should be related to one another, etc. have been much debated over the last 10 or 15 years within the framework of generative grammar. Considered as a relatively minor appendix of the phrase-structure rule component in the early days of generative grammar, the lexicon became little by little an autonomous component of the grammar with its own specific formalism -- lexical entries as matrices of features, as advocated by Chomsky (1965). Finally, it also acquired specific types of rules, the so-called word formation rules (cf. Halle, 1973; Aronoff, 1976; Lieber, 1980; Selkirk, 1983, and others), and lexical redundancy rules (cf. Jackendoff, 1975; Bresnan, 1977). By and large, there seems to be widespread agreement among linguists that the lexicon should be viewed as the repository of all the idiosyncratic properties of the lexical items of a language (phonological, morphological, syntactic, semantic, etc.). This agreement quickly disappears, however, when it comes to defining what constitutes a lexical item, or, to put it slightly differently, what the lexicon is a list of, and how should it be organized. Among the many proposals discussed in the linguist</context>
</contexts>
<marker>Lieber, 1980</marker>
<rawString>Lieber, R. (1980). On the Organization of the Lexicon, Ph.D. Dissertation, MIT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>