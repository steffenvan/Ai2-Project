<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.045921">
<title confidence="0.998257">
Introduction of a new paraphrase generation tool
based on Monte-Carlo sampling
</title>
<author confidence="0.942545">
Jonathan Chevelu&apos;,2 Thomas Lavergne Yves Lepage&apos; Thierry Moudenc2
</author>
<listItem confidence="0.5451055">
(1) GREYC, université de Caen Basse-Normandie
(2) Orange Labs; 2, avenue Pierre Marzin, 22307 Lannion
</listItem>
<email confidence="0.8208745">
{jonathan.chevelu,thierry.moudenc}@orange-ftgroup.com,
thomas.lavergne@reveurs.org, yves.lepage@info.unicaen.fr
</email>
<sectionHeader confidence="0.993553" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997708">
We propose a new specifically designed
method for paraphrase generation based
on Monte-Carlo sampling and show how
this algorithm is suitable for its task.
Moreover, the basic algorithm presented
here leaves a lot of opportunities for fu-
ture improvement. In particular, our algo-
rithm does not constraint the scoring func-
tion in opposite to Iiterbi based decoders.
It is now possible to use some global fea-
tures in paraphrase scoring functions. This
algorithm opens new outlooks for para-
phrase generation and other natural lan-
guage processing applications like statis-
tical machine translation.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998825">
A paraphrase generation system is a program
which, given a source sentence, produces a differ-
ent sentence with almost the same meaning.
Paraphrase generation is useful in applications
to choose between different forms to keep the
most appropriate one. For instance, automatic
summary can be seen as a particular paraphrasing
task (Barzilay and Lee, 2003) with the aim of se-
lecting the shortest paraphrase.
Paraphrases can also be used to improve natu-
ral language processing (NLP) systems. (Callison-
Burch et al., 2006) improved machine translations
by augmenting the coverage of patterns that can
be translated. Similarly, (Sekine, 2005) improved
information retrieval based on pattern recognition
by introducing paraphrase generation.
In order to produce paraphrases, a promising
approach is to see the paraphrase generation prob-
lem as a translation problem, where the target lan-
guage is the same as the source language (Quirk et
al., 2004; Bannard and Callison-Burch, 2005).
A problem that has drawn less attention is the
generation step which corresponds to the decoding
step in SMT. Most paraphrase generation tools use
some standard SMT decoding algorithms (Quirk et
al., 2004) or some off-the-shelf decoding tools like
MOSES (Koehn et al., 2007). The goal of a de-
coder is to find the best path in the lattice produced
from a paraphrase table. This is basically achieved
by using dynamic programming and especially the
Iiterbi algorithm associated with beam searching.
However decoding algorithms were designed
for translation, not for paraphrase generation. Al-
though left-to-right decoding is justified for trans-
lation, it may not be necessary for paraphrase
generation. A paraphrase generation tool usually
starts with a sentence which may be very similar to
some potential solution. In other words, there is no
need to &amp;quot;translate&amp;quot; all of the sentences. Moreover,
decoding may not be suitable for non-contiguous
transformation rules.
In addition, dynamic programming imposes an
incremental scoring function to evaluate the qual-
ity of each hypothesis. For instance, it cannot cap-
ture some scattered syntactical dependencies. Im-
proving on this major issue is a key point to im-
prove paraphrase generation systems.
This paper first presents an alternative to decod-
ing that is based on transformation rule application
in section 2. In section 3 we propose a paraphrase
generation method for this paradigm based on an
algorithm used in two-player games. Section 4
briefly explain experimental context and its asso-
ciated protocol for evaluation of the proposed sys-
tem. We compare the proposed algorithm with a
baseline system in section 5. Finally, in section 6,
we point to future research tracks to improve para-
phrase generation tools.
</bodyText>
<sectionHeader confidence="0.868461" genericHeader="method">
2 Statistical paraphrase generation using
transformation rules
</sectionHeader>
<bodyText confidence="0.999966333333333">
The paraphrase generation problem can be seen as
an exploration problem. We seek the best para-
phrase according to a scoring function in a space
</bodyText>
<page confidence="0.984186">
249
</page>
<note confidence="0.9256485">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 249–252,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999979848484849">
to search by applying successive transformations.
This space is composed of states connected by ac-
tions. An action is a transformation rule with a
place where it applies in the sentence. States are a
sentence with a set of possible actions. Applying
an action in a given state consists in transforming
the sentence of the state and removing all rules that
are no more applicable. In our framework, each
state, except the root, can be a final state. This
is modelised by adding a stop rule as a particular
action. We impose the constraint that any trans-
formed part of the source sentence cannot be trans-
formed anymore.
This paradigm is more approriate for paraphrase
generation than the standard SMT approach in re-
spect to several points: there is no need for left-
to-right decoding because a transformation can be
applied anywhere without order; there is no need
to transform the whole of a sentence because each
state is a final state; there is no need to keep the
identity transformation for each phrase in the para-
phrase table; the only domain knowledge needed
is a generative model and a scoring function for
final states; it is possible to mix different genera-
tive models because a statistical paraphrase table,
an analogical solver and a paraphrase memory for
instance; there is no constraint on the scoring func-
tion because it only scores final states.
Note that the branching factor with a paraphrase
table can be around thousand actions per states
which makes the generation problem a difficult
computational problem. Hence we need an effi-
cient generation algorithm.
</bodyText>
<sectionHeader confidence="0.969834" genericHeader="method">
3 Monte-Carlo based Paraphrase
Generation
</sectionHeader>
<bodyText confidence="0.999213327272727">
UCT (Kocsis and Szepesvári, 2006) (Upper Con-
fidence bound applied to Tree) is a Monte-Carlo
planning algorithm that have some interesting
properties: it grows the search tree non-uniformly
and favours the most promising sequences, with-
out pruning branch; it can deal with high branch-
ing factor; it is an any-time algorithm and returns
best solution found so far when interrupted; it does
not require expert domain knowledge to evaluate
states. These properties make it ideally suited for
games with high branching factor and for which
there is no strong evaluation function.
For the same reasons, this algorithm sounds in-
teresting for paraphrase generation. In particular,
it does not put constraint on the scoring function.
We propose a variation of the UCT algorithm for
paraphrase generation named MCPG for Monte-
Carlo based Paraphrase Generation.
The main part of the algorithm is the sampling
step. An episode of this step is a sequence of states
and actions, s1, a1, s2, a2, ..., sT, from the root
state to a final state. During an episode construc-
tion, there are two ways to select the action ai to
perfom from a state si.
If the current state was already explored in a
previous episode, the action is selected accord-
ing to a compromise between exploration and ex-
ploitation. This compromise is computed using
the UCB-Tunned formula (Auer et al., 2001) as-
sociated with the RAVE heuristic (Gelly and Sil-
ver, 2007). If the current state is explored for
the first time, its score is estimated using Monte-
Carlo sampling. In other word, to complete the
episode, the actions ai, ai+1, ... , aT−1, aT are se-
lected randomly until a stop rule is drawn.
At the end of each episode, a reward is com-
puted for the final state sT using a scoring func-
tion and the value of each (state, action) pair of the
episode is updated. Then, the algorithm computes
an other episode with the new values.
Periodically, the sampling step is stopped and
the best action at the root state is selected. This
action is then definitely applied and a sampling
is restarted from the new root state. The action
sequence is built incrementally and selected af-
ter being enough sampled. For our experiments,
we have chosen to stop sampling regularly after a
fixed amount η of episodes.
Our main adaptation of the original algorithm
is in the (state, action) value updating procedure.
Since the goal of the algorithm is to maximise a
scoring function, we use the maximum reachable
score from a state as value instead of the score ex-
pectation. This algorithm suits the paradigm pro-
posed for paraphrase generation.
</bodyText>
<sectionHeader confidence="0.99816" genericHeader="method">
4 Experimental context
</sectionHeader>
<bodyText confidence="0.999916666666667">
This section describes the experimental context
and the methodology followed to evaluate our sta-
tistical paraphrase generation tool.
</bodyText>
<subsectionHeader confidence="0.965134">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.99964275">
For the experiment reported in section 5, we use
one of the largest, multi-lingual, freely available
aligned corpus, Europarl (Koehn, 2005). It con-
sists of European parliament debates. We choose
</bodyText>
<page confidence="0.971637">
250
</page>
<bodyText confidence="0.999957888888889">
French as the language for paraphrases and En-
glish as the pivot language. For this pair of lan-
guages, the corpus consists of 1, 487, 459 French
sentences aligned with 1, 461,429 English sen-
tences. Note that the sentences in this corpus
are long, with an average length of 30 words per
French sentence and 27.1 for English. We ran-
domly extracted 100 French sentences as a test
corpus.
</bodyText>
<subsectionHeader confidence="0.996396">
4.2 Language model and paraphrase table
</subsectionHeader>
<bodyText confidence="0.9998215">
Paraphrase generation tools based on SMT meth-
ods need a language model and a paraphrase table.
Both are computed on a training corpus.
The language models we use are n-gram lan-
guage models with back-off. We use SRILM (Stol-
cke, 2002) with its default parameters for this pur-
pose. The length of the n-grams is five.
To build a paraphrase table, we use the con-
struction method via a pivot language proposed
in (Bannard and Callison-Burch, 2005).
Three heuristics are used to prune the para-
phrase table. The first heuristic prunes any entry
in the paraphrase table composed of tokens with a
probability lower than a threshold E. The second,
called pruning pivot heuristic, consists in deleting
all pivot clusters larger than a threshold τ. The
last heuristic keeps only the κ most probable para-
phrases for each source phrase in the final para-
phrase table. For this study, we empirically fix
E= 10−5, τ = 200 and κ = 10.
</bodyText>
<subsectionHeader confidence="0.994039">
4.3 Evaluation Protocol
</subsectionHeader>
<bodyText confidence="0.999956714285714">
We developed a dedicated website to allow the hu-
man judges with some flexibility in workplaces
and evaluation periods. We retain the principle of
the two-step evaluation, common in the machine
translation domain and already used for para-
phrase evaluation (Bannard and Callison-Burch,
2005).
The question asked to the human evaluator for
the syntactic task is: Is the following sentence in
good French? The question asked to the human
evaluator for the semantic task is: Do the following
two sentences express the same thing?
In our experiments, each paraphrase was evalu-
ated by two native French evaluators.
</bodyText>
<sectionHeader confidence="0.789873" genericHeader="method">
5 Comparison with a SMT decoder
</sectionHeader>
<bodyText confidence="0.999946076923077">
In order to validate our algorithm for paraphrase
generation, we compare it with an off-the-shelf
SMT decoder.
We use the MOSES decoder (Koehn et al., 2007)
as a baseline. The MOSES scoring function is
set by four weighting factors αΦ, αLM, αD, αW.
Conventionally, these four weights are adjusted
during a tuning step on a training corpus. The
tuning step is inappropriate for paraphrase because
there is no such tuning corpus available. We em-
pirically set αΦ = 1, αLM = 1, αD = 10 and
αW = 0. Hence, the scoring function (or reward
function for MCPG) is equivalent to:
</bodyText>
<equation confidence="0.996116">
R(f0|f,I) = p(f0) × Φ(f|f0,I)
</equation>
<bodyText confidence="0.99974325">
where f and f0 are the source and target sen-
tences, I a segmentation in phrases of f, p(f0)
the language model score and Φ(f|f0, I) =
Hi∈I p(fi|f0i) the paraphrase table score.
The MCPG algorithm needs two parameters.
One is the number of episodes η done before se-
lecting the best action at root state. The other is
k, an equivalence parameter which balances the
exploration/exploitation compromise (Auer et al.,
2001). We empirically set η = 1, 000, 000 and
k = 1, 000.
For our algorithm, note that identity paraphrase
probabilities are biased: for each phrase it is
equal to the probability of the most probable para-
phrase. Moreover, as the source sentence is the
best meaning preserved &amp;quot;paraphrase&amp;quot;, a sentence
cannot have a better score. Hence, we use a
slightly different scoring function:
Note that for this model, there is no need to know
the identity transformations probability for un-
changed part of the sentence.
Results are presented in Table 1. The Kappa
statistics associated with the results are 0.84, 0.64
and 0.59 which are usually considered as a &amp;quot;per-
fect&amp;quot;, &amp;quot;substantial&amp;quot; and &amp;quot;moderate&amp;quot; agreement.
Results are close to evaluations from the base-
line system. The main differences are from Kappa
statistics which are lower for the MOSES system
evaluation. Judges changed between the two ex-
periments. We may wonder whether an evaluation
with only two judges is reliable. This points to the
ambiguity of any paraphrase definition.
</bodyText>
<equation confidence="0.512349666666667">
R(f0|f,I) = min ⎛ p(f0) T7 p(fi|f0i) 1 ⎞
⎜ ⎜ ⎝ p(f)i∈I p(fi|fi) ⎠ ⎟ ⎟
fi6=f&apos;i
</equation>
<page confidence="0.848571">
251
</page>
<table confidence="0.96278325">
System MOSES MCPG
Well formed (Kappa) 64 %(0.57) 63 %(0.84)
Meaning preserved (Kappa) 58 %(0.48) 55 %(0.64)
Well formed and meaning preserved (Kappa) 50 %(0.54) 49 %(0.59)
</table>
<tableCaption confidence="0.8199855">
Table 1: Results of paraphrases evaluation for 100 sentences in French using English as the pivot lan-
guage. Comparison between the baseline system MOSES and our algorithm MCPG.
</tableCaption>
<bodyText confidence="0.997818">
By doing this experiment, we have shown that
our algorithm with a biased paraphrase table is
state-of-the-art to generate paraphrases.
</bodyText>
<sectionHeader confidence="0.994374" genericHeader="conclusions">
6 Conclusions and further research
</sectionHeader>
<bodyText confidence="0.99998046875">
In this paper, we have proposed a different
paradigm and a new algorithm in NLP field
adapted for statistical paraphrases generation.
This method, based on large graph exploration by
Monte-Carlo sampling, produces results compa-
rable with state-of-the-art paraphrase generation
tools based on SMT decoders.
The algorithm structure is flexible and generic
enough to easily work with discontinous patterns.
It is also possible to mix various transformation
methods to increase paraphrase variability.
The rate of ill-formed paraphrase is high at
37%. The result analysis suggests an involvement
of the non-preservation of the original meaning
when a paraphrase is evaluated ill-formed. Al-
though the mesure is not statistically significant
because the test corpus is too small, the same trend
is also observed in other experiments. Improv-
ing on the language model issue is a key point to
improve paraphrase generation systems. Our al-
gorithm can work with unconstraint scoring func-
tions, in particular, there is no need for the scor-
ing function to be incremental as for Viterbi based
decoders. We are working to add, in the scoring
function, a linguistic knowledge based analyzer to
solve this problem.
Because MCPG is based on a different paradigm,
its output scores cannot be directly compared to
MOSES scores. In order to prove the optimisa-
tion qualities of MCPG versus state-of-the-art de-
coders, we are transforming our paraphrase gener-
ation tool into a translation tool.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999891306122449">
P. Auer, N. Cesa-Bianchi, and C. Gentile. 2001. Adap-
tive and self-confident on-line learning algorithms.
Machine Learning.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Annual
Meeting of ACL, pages 597–604, Morristown, NJ,
USA. Association for Computational Linguistics.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In HLT-NAACL
2003: Main Proceedings, pages 16–23.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In HLT-NAACL 2006: Main
Proceedings, pages 17–24, Morristown, NJ, USA.
Association for Computational Linguistics.
Sylvain Gelly and David Silver. 2007. Combining on-
line and offline knowledge in UCT. In 24th Interna-
tional Conference on Machine Learning (ICML’07),
pages 273–280, June.
Levente Kocsis and Csaba Szepesvári. 2006. Bandit
based monte-carlo planning. In 17th European Con-
ference on Machine Learning, (ECML’06), pages
282–293, September.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Annual Meeting of ACL, Demonstra-
tion Session, pages 177–180, June.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit.
Chris Quirk, Chris Brockett, and Bill Dolan. 2004.
Monolingual machine translation for paraphrase
generation. In Dekang Lin and Dekai Wu, edi-
tors, the 2004 Conference on Empirical Methods
in Natural Language Processing, pages 142–149.,
Barcelona, Spain, 25-26 July. Association for Com-
putational Linguistics.
Satoshi Sekine. 2005. Automatic paraphrase discov-
ery based on context and keywords between ne pairs.
In Proceedings of International Workshop on Para-
phrase (IWP2005).
Andreas Stolcke. 2002. Srilm – an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing.
</reference>
<page confidence="0.997431">
252
</page>
</variant>
</algorithm>

<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Auer</author>
<author>N Cesa-Bianchi</author>
<author>C Gentile</author>
</authors>
<title>Adaptive and self-confident on-line learning algorithms.</title>
<date>2001</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="7028" citStr="Auer et al., 2001" startWordPosition="1104" endWordPosition="1107">ariation of the UCT algorithm for paraphrase generation named MCPG for MonteCarlo based Paraphrase Generation. The main part of the algorithm is the sampling step. An episode of this step is a sequence of states and actions, s1, a1, s2, a2, ..., sT, from the root state to a final state. During an episode construction, there are two ways to select the action ai to perfom from a state si. If the current state was already explored in a previous episode, the action is selected according to a compromise between exploration and exploitation. This compromise is computed using the UCB-Tunned formula (Auer et al., 2001) associated with the RAVE heuristic (Gelly and Silver, 2007). If the current state is explored for the first time, its score is estimated using MonteCarlo sampling. In other word, to complete the episode, the actions ai, ai+1, ... , aT−1, aT are selected randomly until a stop rule is drawn. At the end of each episode, a reward is computed for the final state sT using a scoring function and the value of each (state, action) pair of the episode is updated. Then, the algorithm computes an other episode with the new values. Periodically, the sampling step is stopped and the best action at the root</context>
<context position="11635" citStr="Auer et al., 2001" startWordPosition="1893" endWordPosition="1896">e is no such tuning corpus available. We empirically set αΦ = 1, αLM = 1, αD = 10 and αW = 0. Hence, the scoring function (or reward function for MCPG) is equivalent to: R(f0|f,I) = p(f0) × Φ(f|f0,I) where f and f0 are the source and target sentences, I a segmentation in phrases of f, p(f0) the language model score and Φ(f|f0, I) = Hi∈I p(fi|f0i) the paraphrase table score. The MCPG algorithm needs two parameters. One is the number of episodes η done before selecting the best action at root state. The other is k, an equivalence parameter which balances the exploration/exploitation compromise (Auer et al., 2001). We empirically set η = 1, 000, 000 and k = 1, 000. For our algorithm, note that identity paraphrase probabilities are biased: for each phrase it is equal to the probability of the most probable paraphrase. Moreover, as the source sentence is the best meaning preserved &amp;quot;paraphrase&amp;quot;, a sentence cannot have a better score. Hence, we use a slightly different scoring function: Note that for this model, there is no need to know the identity transformations probability for unchanged part of the sentence. Results are presented in Table 1. The Kappa statistics associated with the results are 0.84, 0.</context>
</contexts>
<marker>Auer, Cesa-Bianchi, Gentile, 2001</marker>
<rawString>P. Auer, N. Cesa-Bianchi, and C. Gentile. 2001. Adaptive and self-confident on-line learning algorithms. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Annual Meeting of ACL,</booktitle>
<pages>597--604</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1950" citStr="Bannard and Callison-Burch, 2005" startWordPosition="277" endWordPosition="280">nd Lee, 2003) with the aim of selecting the shortest paraphrase. Paraphrases can also be used to improve natural language processing (NLP) systems. (CallisonBurch et al., 2006) improved machine translations by augmenting the coverage of patterns that can be translated. Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation. In order to produce paraphrases, a promising approach is to see the paraphrase generation problem as a translation problem, where the target language is the same as the source language (Quirk et al., 2004; Bannard and Callison-Burch, 2005). A problem that has drawn less attention is the generation step which corresponds to the decoding step in SMT. Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like MOSES (Koehn et al., 2007). The goal of a decoder is to find the best path in the lattice produced from a paraphrase table. This is basically achieved by using dynamic programming and especially the Iiterbi algorithm associated with beam searching. However decoding algorithms were designed for translation, not for paraphrase generation. Although le</context>
<context position="9478" citStr="Bannard and Callison-Burch, 2005" startWordPosition="1525" endWordPosition="1528">pus are long, with an average length of 30 words per French sentence and 27.1 for English. We randomly extracted 100 French sentences as a test corpus. 4.2 Language model and paraphrase table Paraphrase generation tools based on SMT methods need a language model and a paraphrase table. Both are computed on a training corpus. The language models we use are n-gram language models with back-off. We use SRILM (Stolcke, 2002) with its default parameters for this purpose. The length of the n-grams is five. To build a paraphrase table, we use the construction method via a pivot language proposed in (Bannard and Callison-Burch, 2005). Three heuristics are used to prune the paraphrase table. The first heuristic prunes any entry in the paraphrase table composed of tokens with a probability lower than a threshold E. The second, called pruning pivot heuristic, consists in deleting all pivot clusters larger than a threshold τ. The last heuristic keeps only the κ most probable paraphrases for each source phrase in the final paraphrase table. For this study, we empirically fix E= 10−5, τ = 200 and κ = 10. 4.3 Evaluation Protocol We developed a dedicated website to allow the human judges with some flexibility in workplaces and ev</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Annual Meeting of ACL, pages 597–604, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In HLT-NAACL 2003: Main Proceedings,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="1330" citStr="Barzilay and Lee, 2003" startWordPosition="183" endWordPosition="186"> Iiterbi based decoders. It is now possible to use some global features in paraphrase scoring functions. This algorithm opens new outlooks for paraphrase generation and other natural language processing applications like statistical machine translation. 1 Introduction A paraphrase generation system is a program which, given a source sentence, produces a different sentence with almost the same meaning. Paraphrase generation is useful in applications to choose between different forms to keep the most appropriate one. For instance, automatic summary can be seen as a particular paraphrasing task (Barzilay and Lee, 2003) with the aim of selecting the shortest paraphrase. Paraphrases can also be used to improve natural language processing (NLP) systems. (CallisonBurch et al., 2006) improved machine translations by augmenting the coverage of patterns that can be translated. Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation. In order to produce paraphrases, a promising approach is to see the paraphrase generation problem as a translation problem, where the target language is the same as the source language (Quirk et al., 2004; Bannard and C</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiple-sequence alignment. In HLT-NAACL 2003: Main Proceedings, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In HLT-NAACL 2006: Main Proceedings,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In HLT-NAACL 2006: Main Proceedings, pages 17–24, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Gelly</author>
<author>David Silver</author>
</authors>
<title>Combining online and offline knowledge in UCT.</title>
<date>2007</date>
<booktitle>In 24th International Conference on Machine Learning (ICML’07),</booktitle>
<pages>273--280</pages>
<contexts>
<context position="7088" citStr="Gelly and Silver, 2007" startWordPosition="1114" endWordPosition="1118"> named MCPG for MonteCarlo based Paraphrase Generation. The main part of the algorithm is the sampling step. An episode of this step is a sequence of states and actions, s1, a1, s2, a2, ..., sT, from the root state to a final state. During an episode construction, there are two ways to select the action ai to perfom from a state si. If the current state was already explored in a previous episode, the action is selected according to a compromise between exploration and exploitation. This compromise is computed using the UCB-Tunned formula (Auer et al., 2001) associated with the RAVE heuristic (Gelly and Silver, 2007). If the current state is explored for the first time, its score is estimated using MonteCarlo sampling. In other word, to complete the episode, the actions ai, ai+1, ... , aT−1, aT are selected randomly until a stop rule is drawn. At the end of each episode, a reward is computed for the final state sT using a scoring function and the value of each (state, action) pair of the episode is updated. Then, the algorithm computes an other episode with the new values. Periodically, the sampling step is stopped and the best action at the root state is selected. This action is then definitely applied a</context>
</contexts>
<marker>Gelly, Silver, 2007</marker>
<rawString>Sylvain Gelly and David Silver. 2007. Combining online and offline knowledge in UCT. In 24th International Conference on Machine Learning (ICML’07), pages 273–280, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Levente Kocsis</author>
<author>Csaba Szepesvári</author>
</authors>
<title>Bandit based monte-carlo planning.</title>
<date>2006</date>
<booktitle>In 17th European Conference on Machine Learning, (ECML’06),</booktitle>
<pages>282--293</pages>
<contexts>
<context position="5704" citStr="Kocsis and Szepesvári, 2006" startWordPosition="883" endWordPosition="886"> table; the only domain knowledge needed is a generative model and a scoring function for final states; it is possible to mix different generative models because a statistical paraphrase table, an analogical solver and a paraphrase memory for instance; there is no constraint on the scoring function because it only scores final states. Note that the branching factor with a paraphrase table can be around thousand actions per states which makes the generation problem a difficult computational problem. Hence we need an efficient generation algorithm. 3 Monte-Carlo based Paraphrase Generation UCT (Kocsis and Szepesvári, 2006) (Upper Confidence bound applied to Tree) is a Monte-Carlo planning algorithm that have some interesting properties: it grows the search tree non-uniformly and favours the most promising sequences, without pruning branch; it can deal with high branching factor; it is an any-time algorithm and returns best solution found so far when interrupted; it does not require expert domain knowledge to evaluate states. These properties make it ideally suited for games with high branching factor and for which there is no strong evaluation function. For the same reasons, this algorithm sounds interesting fo</context>
</contexts>
<marker>Kocsis, Szepesvári, 2006</marker>
<rawString>Levente Kocsis and Csaba Szepesvári. 2006. Bandit based monte-carlo planning. In 17th European Conference on Machine Learning, (ECML’06), pages 282–293, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch Mayne</author>
<author>Christopher Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Annual Meeting of ACL, Demonstration Session,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="2226" citStr="Koehn et al., 2007" startWordPosition="321" endWordPosition="324">2005) improved information retrieval based on pattern recognition by introducing paraphrase generation. In order to produce paraphrases, a promising approach is to see the paraphrase generation problem as a translation problem, where the target language is the same as the source language (Quirk et al., 2004; Bannard and Callison-Burch, 2005). A problem that has drawn less attention is the generation step which corresponds to the decoding step in SMT. Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like MOSES (Koehn et al., 2007). The goal of a decoder is to find the best path in the lattice produced from a paraphrase table. This is basically achieved by using dynamic programming and especially the Iiterbi algorithm associated with beam searching. However decoding algorithms were designed for translation, not for paraphrase generation. Although left-to-right decoding is justified for translation, it may not be necessary for paraphrase generation. A paraphrase generation tool usually starts with a sentence which may be very similar to some potential solution. In other words, there is no need to &amp;quot;translate&amp;quot; all of the s</context>
<context position="10773" citStr="Koehn et al., 2007" startWordPosition="1741" endWordPosition="1744"> in the machine translation domain and already used for paraphrase evaluation (Bannard and Callison-Burch, 2005). The question asked to the human evaluator for the syntactic task is: Is the following sentence in good French? The question asked to the human evaluator for the semantic task is: Do the following two sentences express the same thing? In our experiments, each paraphrase was evaluated by two native French evaluators. 5 Comparison with a SMT decoder In order to validate our algorithm for paraphrase generation, we compare it with an off-the-shelf SMT decoder. We use the MOSES decoder (Koehn et al., 2007) as a baseline. The MOSES scoring function is set by four weighting factors αΦ, αLM, αD, αW. Conventionally, these four weights are adjusted during a tuning step on a training corpus. The tuning step is inappropriate for paraphrase because there is no such tuning corpus available. We empirically set αΦ = 1, αLM = 1, αD = 10 and αW = 0. Hence, the scoring function (or reward function for MCPG) is equivalent to: R(f0|f,I) = p(f0) × Φ(f|f0,I) where f and f0 are the source and target sentences, I a segmentation in phrases of f, p(f0) the language model score and Φ(f|f0, I) = Hi∈I p(fi|f0i) the par</context>
</contexts>
<marker>Koehn, Hoang, Mayne, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne, Christopher Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Annual Meeting of ACL, Demonstration Session, pages 177–180, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit.</booktitle>
<contexts>
<context position="8553" citStr="Koehn, 2005" startWordPosition="1365" endWordPosition="1366">n of the original algorithm is in the (state, action) value updating procedure. Since the goal of the algorithm is to maximise a scoring function, we use the maximum reachable score from a state as value instead of the score expectation. This algorithm suits the paradigm proposed for paraphrase generation. 4 Experimental context This section describes the experimental context and the methodology followed to evaluate our statistical paraphrase generation tool. 4.1 Data For the experiment reported in section 5, we use one of the largest, multi-lingual, freely available aligned corpus, Europarl (Koehn, 2005). It consists of European parliament debates. We choose 250 French as the language for paraphrases and English as the pivot language. For this pair of languages, the corpus consists of 1, 487, 459 French sentences aligned with 1, 461,429 English sentences. Note that the sentences in this corpus are long, with an average length of 30 words per French sentence and 27.1 for English. We randomly extracted 100 French sentences as a test corpus. 4.2 Language model and paraphrase table Paraphrase generation tools based on SMT methods need a language model and a paraphrase table. Both are computed on </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>Bill Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>142--149</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona,</location>
<contexts>
<context position="1915" citStr="Quirk et al., 2004" startWordPosition="273" endWordPosition="276">ing task (Barzilay and Lee, 2003) with the aim of selecting the shortest paraphrase. Paraphrases can also be used to improve natural language processing (NLP) systems. (CallisonBurch et al., 2006) improved machine translations by augmenting the coverage of patterns that can be translated. Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation. In order to produce paraphrases, a promising approach is to see the paraphrase generation problem as a translation problem, where the target language is the same as the source language (Quirk et al., 2004; Bannard and Callison-Burch, 2005). A problem that has drawn less attention is the generation step which corresponds to the decoding step in SMT. Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like MOSES (Koehn et al., 2007). The goal of a decoder is to find the best path in the lattice produced from a paraphrase table. This is basically achieved by using dynamic programming and especially the Iiterbi algorithm associated with beam searching. However decoding algorithms were designed for translation, not for</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and Bill Dolan. 2004. Monolingual machine translation for paraphrase generation. In Dekang Lin and Dekai Wu, editors, the 2004 Conference on Empirical Methods in Natural Language Processing, pages 142–149., Barcelona, Spain, 25-26 July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>Automatic paraphrase discovery based on context and keywords between ne pairs.</title>
<date>2005</date>
<booktitle>In Proceedings of International Workshop on Paraphrase (IWP2005).</booktitle>
<contexts>
<context position="1612" citStr="Sekine, 2005" startWordPosition="228" endWordPosition="229"> system is a program which, given a source sentence, produces a different sentence with almost the same meaning. Paraphrase generation is useful in applications to choose between different forms to keep the most appropriate one. For instance, automatic summary can be seen as a particular paraphrasing task (Barzilay and Lee, 2003) with the aim of selecting the shortest paraphrase. Paraphrases can also be used to improve natural language processing (NLP) systems. (CallisonBurch et al., 2006) improved machine translations by augmenting the coverage of patterns that can be translated. Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation. In order to produce paraphrases, a promising approach is to see the paraphrase generation problem as a translation problem, where the target language is the same as the source language (Quirk et al., 2004; Bannard and Callison-Burch, 2005). A problem that has drawn less attention is the generation step which corresponds to the decoding step in SMT. Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like MOSES (Koehn</context>
</contexts>
<marker>Sekine, 2005</marker>
<rawString>Satoshi Sekine. 2005. Automatic paraphrase discovery based on context and keywords between ne pairs. In Proceedings of International Workshop on Paraphrase (IWP2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="9269" citStr="Stolcke, 2002" startWordPosition="1490" endWordPosition="1492"> English as the pivot language. For this pair of languages, the corpus consists of 1, 487, 459 French sentences aligned with 1, 461,429 English sentences. Note that the sentences in this corpus are long, with an average length of 30 words per French sentence and 27.1 for English. We randomly extracted 100 French sentences as a test corpus. 4.2 Language model and paraphrase table Paraphrase generation tools based on SMT methods need a language model and a paraphrase table. Both are computed on a training corpus. The language models we use are n-gram language models with back-off. We use SRILM (Stolcke, 2002) with its default parameters for this purpose. The length of the n-grams is five. To build a paraphrase table, we use the construction method via a pivot language proposed in (Bannard and Callison-Burch, 2005). Three heuristics are used to prune the paraphrase table. The first heuristic prunes any entry in the paraphrase table composed of tokens with a probability lower than a threshold E. The second, called pruning pivot heuristic, consists in deleting all pivot clusters larger than a threshold τ. The last heuristic keeps only the κ most probable paraphrases for each source phrase in the fina</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm – an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>