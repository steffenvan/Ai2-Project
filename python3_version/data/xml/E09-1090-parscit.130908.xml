<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005542">
<title confidence="0.991889">
Fast Full Parsing by Linear-Chain Conditional Random Fields
</title>
<author confidence="0.997128">
Yoshimasa Tsuruokatt Jun’ichi Tsujiitt* Sophia Ananiadoutt
</author>
<affiliation confidence="0.964151333333333">
t School of Computer Science, University of Manchester, UK
t National Centre for Text Mining (NaCTeM), UK
* Department of Computer Science, University of Tokyo, Japan
</affiliation>
<email confidence="0.99752">
{yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.uk
</email>
<sectionHeader confidence="0.997372" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999939866666667">
This paper presents a chunking-based dis-
criminative approach to full parsing. We
convert the task of full parsing into a series
of chunking tasks and apply a conditional
random field (CRF) model to each level
of chunking. The probability of an en-
tire parse tree is computed as the product
of the probabilities of individual chunk-
ing results. The parsing is performed in a
bottom-up manner and the best derivation
is efficiently obtained by using a depth-
first search algorithm. Experimental re-
sults demonstrate that this simple parsing
framework produces a fast and reasonably
accurate parser.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954517241379">
Full parsing analyzes the phrase structure of a sen-
tence and provides useful input for many kinds
of high-level natural language processing such as
summarization (Knight and Marcu, 2000), pro-
noun resolution (Yang et al., 2006), and infor-
mation extraction (Miyao et al., 2008). One of
the major obstacles that discourage the use of full
parsing in large-scale natural language process-
ing applications is its computational cost. For ex-
ample, the MEDLINE corpus, a collection of ab-
stracts of biomedical papers, consists of 70 million
sentences and would require more than two years
of processing time if the parser needs one second
to process a sentence.
Generative models based on lexicalized PCFGs
enjoyed great success as the machine learning
framework for full parsing (Collins, 1999; Char-
niak, 2000), but recently discriminative models
attract more attention due to their superior accu-
racy (Charniak and Johnson, 2005; Huang, 2008)
and adaptability to new grammars and languages
(Buchholz and Marsi, 2006).
A traditional approach to discriminative full
parsing is to convert a full parsing task into a series
of classification problems. Ratnaparkhi (1997)
performs full parsing in a bottom-up and left-to-
right manner and uses a maximum entropy clas-
sifier to make decisions to construct individual
phrases. Sagae and Lavie (2006) use the shift-
reduce parsing framework and a maximum en-
tropy model for local classification to decide pars-
ing actions. These approaches are often called
history-based approaches.
A more recent approach to discriminative full
parsing is to treat the task as a single structured
prediction problem. Finkel et al. (2008) incor-
porated rich local features into a tree CRF model
and built a competitive parser. Huang (2008) pro-
posed to use a parse forest to incorporate non-local
features. They used a perceptron algorithm to op-
timize the weights of the features and achieved
state-of-the-art accuracy. Petrov and Klein (2008)
introduced latent variables in tree CRFs and pro-
posed a caching mechanism to speed up the com-
putation.
In general, the latter whole-sentence ap-
proaches give better accuracy than history-based
approaches because they can better trade off deci-
sions made in different parts in a parse tree. How-
ever, the whole-sentence approaches tend to re-
quire a large computational cost both in training
and parsing. In contrast, history-based approaches
are less computationally intensive and usually pro-
duce fast parsers.
In this paper, we present a history-based parser
using CRFs, by treating the task of full parsing as
a series of chunking problems where it recognizes
chunks in a flat input sequence. We use the linear-
</bodyText>
<note confidence="0.922557">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 790–798,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.984442">
790
</page>
<figure confidence="0.967120714285714">
QP
VBN NN VBD DT JJ CD CD NNS .
NP
VP
NP VBD NP .
Estimated volume was a light 2.4 million ounces .
volume was ounces .
</figure>
<figureCaption confidence="0.9999235">
Figure 1: Chunking, the first (base) level.
Figure 3: Chunking, the 3rd level.
</figureCaption>
<figure confidence="0.95995">
NP
NP VBD DT JJ QP NNS .
volume was a light million ounces .
</figure>
<figureCaption confidence="0.903404">
Figure 2: Chunking, the 2nd level.
</figureCaption>
<figure confidence="0.997153">
S
NP VP .
volume was .
</figure>
<figureCaption confidence="0.99985">
Figure 4: Chunking, the 4th level.
</figureCaption>
<bodyText confidence="0.999632428571429">
chain CRF model to perform chunking.
Although our parsing model falls into the cat-
egory of history-based approaches, it is one step
closer to the whole-sentence approaches because
the parser uses a whole-sequence model (i.e.
CRFs) for individual chunking tasks. In other
words, our parser could be located somewhere
between traditional history-based approaches and
whole-sentence approaches. One of our motiva-
tions for this work was that our parsing model
may achieve a better balance between accuracy
and speed than existing parsers.
It is also worth mentioning that our approach is
similar in spirit to supertagging for parsing with
lexicalized grammar formalisms such as CCG and
HPSG (Clark and Curran, 2004; Ninomiya et al.,
2006), in which significant speed-ups for parsing
time are achieved.
In this paper, we show that our approach is in-
deed appealing in that the parser runs very fast
and gives competitive accuracy. We evaluate our
parser on the standard data set for parsing exper-
iments (i.e. the Penn Treebank) and compare it
with existing approaches to full parsing.
This paper is organized as follows. Section 2
presents the overall chunk parsing strategy. Sec-
tion 3 describes the CRF model used to perform
individual chunking steps. Section 4 describes the
depth-first algorithm for finding the best derivation
of a parse tree. The part-of-speech tagger used in
the parser is described in section 5. Experimen-
tal results on the Penn Treebank corpus are pro-
vided in Section 6. Section 7 discusses possible
improvements and extensions of our work. Sec-
tion 8 offers some concluding remarks.
</bodyText>
<sectionHeader confidence="0.978435" genericHeader="introduction">
2 Full Parsing by Chunking
</sectionHeader>
<bodyText confidence="0.999886225806451">
This section describes the parsing framework em-
ployed in this work.
The parsing process is conceptually very sim-
ple. The parser first performs chunking by iden-
tifying base phrases, and converts the identified
phrases to non-terminal symbols. It then performs
chunking for the updated sequence and converts
the newly recognized phrases into non-terminal
symbols. The parser repeats this process until the
whole sequence is chunked as a sentence
Figures 1 to 4 show an example of a parsing pro-
cess by this framework. In the first (base) level,
the chunker identifies two base phrases, (NP Es-
timated volume) and (QP 2.4 million), and re-
places each phrase with its non-terminal symbol
and head1. In the second level, the chunker iden-
tifies a noun phrase, (NP a light million ounces),
and converts it into NP. This process is repeated
until the whole sentence is chunked at the fourth
level. The full parse tree is recovered from the
chunking history in a straightforward way.
This idea of converting full parsing into a se-
ries of chunking tasks is not new by any means—
the history of this kind of approach dates back to
1950s (Joshi and Hopely, 1996). More recently,
Brants (1999) used a cascaded Markov model to
parse German text. Tjong Kim Sang (2001) used
the IOB tagging method to represent chunks and
memory-based learning, and achieved an f-score
of 80.49 on the WSJ corpus. Tsuruoka and Tsu-
jii (2005) improved upon their approach by using
</bodyText>
<footnote confidence="0.896233">
1The head word is identified by using the head-
percolation table (Magerman, 1995).
</footnote>
<page confidence="0.994852">
791
</page>
<figure confidence="0.992532333333333">
5000
4000
# sentences
3000
2000
1000
0
0 5 10 15 20 25 30
Height
</figure>
<subsectionHeader confidence="0.992823">
3.1 Linear Chain CRFs
</subsectionHeader>
<bodyText confidence="0.995826333333333">
A linear chain CRF defines a single log-linear
probabilistic distribution over all possible tag se-
quences y for the input sequence x:
</bodyText>
<equation confidence="0.99904725">
T K
1
XXZ(x) exp λkfk(t, yt, yt−1, x),
t=1 k=1
</equation>
<bodyText confidence="0.99913425">
where fk(t, yt, yt−1, x) is typically a binary func-
tion indicating the presence of feature k, λk is the
weight of the feature, and Z(X) is a normalization
function:
</bodyText>
<equation confidence="0.894755">
p(y|x) =
</equation>
<figureCaption confidence="0.996398666666667">
Figure 5: Distribution of tree height in WSJ sec- X exp T K λkfk(t, yt, yt−1, x).
tions 2-21. Z(x) = X X
y t=1 k=1
</figureCaption>
<bodyText confidence="0.993444333333333">
a maximum entropy classifier and achieved an f-
score of 85.9. However, there is still a large gap
between the accuracy of chunking-based parsers
and that of widely-used practical parsers such as
Collins parser and Charniak parser (Collins, 1999;
Charniak, 2000).
</bodyText>
<subsectionHeader confidence="0.999226">
2.1 Heights of Trees
</subsectionHeader>
<bodyText confidence="0.9999713">
A natural question about this parsing framework is
how many levels of chunking are usually needed to
parse a sentence. We examined the distribution of
the heights of the trees in sections 2-21 of the Wall
Street Journal (WSJ) corpus. The result is shown
in Figure 5. Most of the sentences have less than
20 levels. The average was 10.0, which means we
need to perform, on average, 10 chunking tasks to
obtain a full parse tree for a sentence if the parsing
is performed in a deterministic manner.
</bodyText>
<sectionHeader confidence="0.702601" genericHeader="method">
3 Chunking with CRFs
</sectionHeader>
<bodyText confidence="0.98637125">
The accuracy of chunk parsing is highly depen-
dent on the accuracy of each level of chunking.
This section describes our approach to the chunk-
ing task.
A common approach to the chunking problem
is to convert the problem into a sequence tagging
task by using the “BIO” (B for beginning, I for
inside, and O for outside) representation. For ex-
ample, the chunking process given in Figure 1 is
expressed as the following BIO sequences.
B-NP I-NP O O O B-QP I-QP O O
This representation enables us to use the linear-
chain CRF model to perform chunking, since the
task is simply assigning appropriate labels to a se-
quence.
This model allows us to define features on states
and edges combined with surface observations.
The weights of the features are determined in
such a way that they maximize the conditional log-
likelihood of the training data:
</bodyText>
<equation confidence="0.998083">
Ga = XN log p(y(i)|x(i)) + R(λ),
i=1
</equation>
<bodyText confidence="0.998931666666667">
where R(λ) is introduced for the purpose of regu-
larization which prevents the model from overfit-
ting the training data. The L1 or L2 norm is com-
monly used in statistical natural language process-
ing (Gao et al., 2007). We used L1-regularization,
which is defined as
</bodyText>
<equation confidence="0.89466625">
XK
1
R(λ) = C
k=1
</equation>
<bodyText confidence="0.9996636">
where C is the meta-parameter that controls the
degree of regularization. We used the OWL-QN
algorithm (Andrew and Gao, 2007) to obtain the
parameters that maximize the L1-regularized con-
ditional log-likelihood.
</bodyText>
<subsectionHeader confidence="0.987478">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.976760363636364">
Table 1 shows the features used in chunking for
the base level. Since the task is basically identical
to shallow parsing by CRFs, we follow the feature
sets used in the previous work by Sha and Pereira
(2003). We use unigrams, bigrams, and trigrams
of part-of-speech (POS) tags and words.
The difference between our CRF chunker and
that in (Sha and Pereira, 2003) is that we could
not use second-order CRF models, hence we could
not use trigram features on the BIO states. We
|λk|,
</bodyText>
<page confidence="0.974415">
792
</page>
<table confidence="0.999450833333333">
Symbol Unigrams s−2, s−1, s0, s+1, s+2
Symbol Bigrams s−2s−1, s−1s0, s0s+1, s+1s+2
Symbol Trigrams s−3s−2s−1, s−2s−1s0, s−1s0s+1, s0s+1s+2, s+1s+2s+3
Word Unigrams h−2, h−1, h0, h+1, h+2
Word Bigrams h−2h−1, h−1h0, h0h+1, h+1h+2
Word Trigrams h−1h0h+1
</table>
<tableCaption confidence="0.998156">
Table 1: Feature templates used in the base level chunking. s represents a terminal symbol (i.e. POS tag)
</tableCaption>
<bodyText confidence="0.984334304347826">
and the subscript represents a relative position. h represents a word.
found that using second order CRFs in our task
was very difficult because of the computational
cost. Recall that the computational cost for CRFs
is quadratic to the number of possible states. In
our task, we need to consider the states for all non-
terminal symbols, whereas their work is only con-
cerned with noun phrases.
Table 2 shows feature templates used in the non-
base levels of chunking. In the non-base levels of
chunking, we can use a richer set of features than
the base-level chunking because the chunker has
access to the information about the partial trees
that have been already created. In addition to the
features listed in Table 1, the chunker looks into
the daughters of the current non-terminal sym-
bol and use them as features. It also uses the
words and POS tags around the edges of the re-
gion covered by the current non-terminal symbol.
We also added a special feature to better capture
PP-attachment. The chunker looks at the head of
the second daughter of the prepositional phrase to
incorporate the semantic head of the phrase.
</bodyText>
<sectionHeader confidence="0.993733" genericHeader="method">
4 Searching for the Best Parse
</sectionHeader>
<bodyText confidence="0.999582333333333">
The probability for an entire parse tree is com-
puted as the product of the probabilities output by
the individual CRF chunkers:
</bodyText>
<equation confidence="0.952236">
h
score = ri p(yi|Xi), (1)
i=0
</equation>
<bodyText confidence="0.999977959183674">
where i is the level of chunking and h is the height
of the tree. The task of full parsing is then to
choose the series of chunking results that maxi-
mizes this probability.
It should be noted that there are cases where
different derivations (chunking histories) lead to
the same parse tree (i.e. phrase structure). Strictly
speaking, therefore, what we describe here as the
probability of a parse tree is actually the proba-
bility of a single derivation. The probabilities of
the derivations should then be marginalized over
to produce the probability of a parse tree, but in
this paper we ignore this effect and simply focus
only on the best derivation.
We use a depth-first search algorithm to find the
highest probability derivation. Figure 6 shows the
algorithm in pseudo-code. The parsing process is
implemented with a recursive function. In each
level of chunking, the recursive function first in-
vokes a CRF chunker to obtain chunking hypothe-
ses for the given sequence. For each hypothesis
whose probability is high enough to have possibil-
ity of constituting the best derivation, the function
calls itself with the sequence updated by the hy-
pothesis. The parsing process is performed in a
bottom up manner and this recursive process ter-
minates if the whole sequence is chunked as a sen-
tence.
To extract multiple chunking hypotheses from
the CRF chunker, we use a branch-and-bound
algorithm rather than the A* search algorithm,
which is perhaps more commonly used in previous
studies. We do not give pseudo code, but the ba-
sic idea is as follows. It first performs the forward
Viterbi algorithm to obtain the best sequence, stor-
ing the upper bounds that are used for pruning in
branch-and-bound. It then performs a branch-and-
bound algorithm in a backward manner to retrieve
possible candidate sequences whose probabilities
are greater than the given threshold. Unlike A*
search, this method is memory efficient because it
is performed in a depth-first manner and does not
require priority queues for keeping uncompleted
hypotheses.
It is straightforward to introduce beam search in
this search algorithm—we simply limit the num-
ber of hypotheses generated by the CRF chunker.
We examine how the width of the beam affects the
parsing performance in the experiments.
</bodyText>
<page confidence="0.990177">
793
</page>
<table confidence="0.999914181818182">
Symbol Unigrams s−2, s−1, s0, s+1, s+2
Symbol Bigrams s−2s−1, s−1s0, s0s+1, s+1s+2
Symbol Trigrams s−3s−2s−1, s−2s−1s0, s−1s0s+1, s0s+1s+2, s+1s+2s+3
Head Unigrams h−2, h−1, h0, h+1, h+2
Head Bigrams h−2h−1, h−1h0, h0h+1, h+1h+2
Head Trigrams h−1h0h+1
Symbol &amp; Daughters s0d01, ... s0d0m
Symbol &amp; Word/POS context s0wj−1, s0pj−1, s0wk+1 , s0pk+1
Symbol &amp; Words on the edges s0wj, s0wk
Freshness whether s0 has been created in the level just below
PP-attachment h−1h0m02 (only when s0 = PP)
</table>
<tableCaption confidence="0.9607796">
Table 2: Feature templates used in the upper level chunking. s represents a non-terminal symbol. h
represents a head percolated from the bottom for each symbol. d0i is the ith daughter of s0. wj is the
first word in the range covered by s0. wj−1 is the word preceding wj. wk is the last word in the range
covered by s0. wk+1 is the word following wk. p represents POS tags. m02 represents the head of the
second daughter of s0.
</tableCaption>
<table confidence="0.9999168">
Word Unigram w−2, w−1, w0, w+1, wi+2
Word Bigram w−1w0, w0w+1, w−1w+1
Prefix, Suffix prefixes of w0
suffixes of w0
(up to length 10)
Character features w0 has a hyphen
w0 has a number
w0 has a capital letter
w0 is all capital
Normalized word N(w0)
</table>
<tableCaption confidence="0.980031333333333">
Table 3: Feature templates used in the POS tagger.
w represents a word and the subscript represents a
relative position.
</tableCaption>
<sectionHeader confidence="0.990772" genericHeader="method">
5 Part-of-Speech Tagging
</sectionHeader>
<bodyText confidence="0.99999325">
We use the CRF model also for POS tagging.
The CRF-based POS tagger is incorporated in the
parser in exactly the same way as the other lay-
ers of chunking. In other words, the POS tagging
process is treated like the bottom layer of chunk-
ing, so the parser considers multiple probabilistic
hypotheses output by the tagger in the search al-
gorithm described in the previous section.
</bodyText>
<subsectionHeader confidence="0.699208">
5.1 Features
</subsectionHeader>
<bodyText confidence="0.999925777777778">
Table 3 shows the feature templates used in the
POS tagger. Most of them are standard features
commonly used in POS tagging for English. We
used unigrams and bigrams of neighboring words,
prefixes and suffixes of the current word, and some
characteristics of the word. We also normalized
the current word by lowering capital letters and
converting all the numerals into ‘#’, and used the
normalized word as a feature.
</bodyText>
<sectionHeader confidence="0.999654" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999976157894737">
We ran parsing experiments using the Wall Street
Journal corpus. Sections 2-21 were used as the
training data. Section 22 was used as the devel-
opment data, with which we tuned the feature set
and parameters for learning and parsing. Section
23 was reserved for the final accuracy report.
The training data for the CRF chunkers were
created by converting each parse tree in the train-
ing data into a list of chunking sequences like
the ones presented in Figures 1 to 4. We trained
three CRF models, i.e., the POS tagging model,
the base chunking model, and the non-base chunk-
ing model. The training took about two days on a
single CPU.
We used the evalb script provided by Sekine and
Collins for evaluating the labeled recall/precision
of the parser outputs2. All experiments were car-
ried out on a server with 2.2 GHz AMD Opteron
processors and 16GB memory.
</bodyText>
<subsectionHeader confidence="0.999549">
6.1 Chunking Performance
</subsectionHeader>
<bodyText confidence="0.99991425">
First, we describe the accuracy of individual
chunking processes. Table 4 shows the results
for the ten most frequently occurring symbols on
the development data. Noun phrases (NP) are the
</bodyText>
<footnote confidence="0.9988465">
2The script is available at http://nlp.cs.nyu.edu/evalb/. We
used the parameter file “COLLINS.prm”.
</footnote>
<page confidence="0.994667">
794
</page>
<listItem confidence="0.993007565217391">
1: procedure PARSESENTENCE(x)
2: PARSE(x, 1, 0)
4: function PARSE(x, p, q)
5: if x is chunked as a complete sentence
6: return p
7: H +- PERFORMCHUNKING(x, q/p)
8: for h E H in descending order of their
probabilities do
9: r +- p x h.probability
10: if r &gt; q then
11: x′ +- UPDATESEQUENCE(x, h)
12: s +- PARSE(x′, r, q)
13: if s &gt; q then
14: q +- s
15: return q
17: function PERFORMCHUNKING(x, t)
18: perform chunking with a CRF chunker and
19: return a set of chunking hypotheses whose
20: probabilities are greater than t.
22: function UPDATESEQUENCE(x, h)
23: update sequence x according to chunking
24: hypothesis h and return the updated
25: sequence.
</listItem>
<table confidence="0.999820384615385">
Symbol # Samples Recall Prec. F-score
NP 317,597 94.79 94.16 94.47
VP 76,281 91.46 91.98 91.72
PP 66,979 92.84 92.61 92.72
S 33,739 91.48 90.64 91.06
ADVP 21,686 84.25 85.86 85.05
ADJP 14,422 77.27 78.46 77.86
QP 14,308 89.43 91.16 90.28
SBAR 11,603 96.42 96.97 96.69
WHNP 8,827 95.54 97.50 96.51
PRT 3,391 95.72 90.52 93.05
: : : : :
all 579,253 92.63 92.62 92.63
</table>
<tableCaption confidence="0.982343">
Table 4: Chunking performance (section 22, all
sentences).
</tableCaption>
<table confidence="0.998786571428571">
Beam Recall Prec. F-score Time (sec)
1 86.72 87.83 87.27 16
2 88.50 88.85 88.67 41
3 88.69 89.08 88.88 61
4 88.72 89.13 88.92 92
5 88.73 89.14 88.93 119
10 88.68 89.19 88.93 179
</table>
<tableCaption confidence="0.982186">
Table 5: Beam width and parsing performance
</tableCaption>
<figureCaption confidence="0.714138714285714">
(section 22, all sentences).
Figure 6: Searching for the best parse with a
depth-first search algorithm. This pseudo-code il-
lustrates how to find the highest probability parse,
but in the real implementation, the function needs
to keep track of chunking histories as well as prob-
abilities.
</figureCaption>
<bodyText confidence="0.998813727272727">
most common symbol and consist of 55% of all
phrases. The accuracy of noun phrases recognition
was relatively high, but it may be useful to design
special features for this particular type of phrase,
considering the dominance of noun phrases in the
corpus. Although not directly comparable, Sha
and Pereira (2003) report almost the same level
of accuracy (94.38%) on noun phrase recognition,
using a much smaller training set. We attribute
their superior performance mainly to the use of
second-order features on state transitions. Table 4
also suggests that adverb phrases (ADVP) and ad-
jective phrases (ADJP) are more difficult to recog-
nize than other types of phrases, which coincides
with the result reported in (Collins, 1999).
It should be noted that the performance reported
in this table was evaluated using the gold standard
sequences as the input to the CRF chunkers. In the
real parsing process, the chunkers have to use the
output from the previous (one level below) chun-
ker, so the quality of the input is not as good as
that used in this evaluation.
</bodyText>
<subsectionHeader confidence="0.999952">
6.2 Parsing Performance
</subsectionHeader>
<bodyText confidence="0.999956529411764">
Next, we present the actual parsing performance.
The first set of experiments concerns the relation-
ship between the width of beam and the parsing
performance. Table 5 shows the results obtained
on the development data. We varied the width of
the beam from 1 to 10. The beam width of 1 cor-
responds to deterministic parsing. Somewhat un-
expectedly, the parsing accuracy did not drop sig-
nificantly even when we reduced the beam width
to a very small number such as 2 or 3.
One of the interesting findings was that re-
call scores were consistently lower than precision
scores throughout all experiments. A possible rea-
son is that, since the score of a parse is defined
as the product of all chunking probabilities, the
parser could prefer a parse tree that consists of
a small number of chunk layers. This may stem
</bodyText>
<page confidence="0.996176">
795
</page>
<bodyText confidence="0.9999614">
from the history-based model’s inability of prop-
erly trading off decisions made by different chun-
kers.
Overall, the parsing speed was very high. The
deterministic version (beam width = 1) parsed
1700 sentences in 16 seconds, which means that
the parser needed only 10 msec to parse one sen-
tence. The parsing speed decreases as we increase
the beam width.
The parser was also memory efficient. Thanks
to L1 regularization, the training process did not
result in many non-zero feature weights. The num-
bers of non-zero weight features were 58,505 (for
the base chunker), 263,889 (for the non-base chun-
ker), and 42,201 (for the POS tagger). The parser
required only 14MB of memory to run.
There was little accuracy difference between the
beam width of 4 and 5, so we adopted the beam
width of 4 for the final accuracy report on the test
data.
</bodyText>
<subsectionHeader confidence="0.999943">
6.3 Comparison with Previous Work
</subsectionHeader>
<bodyText confidence="0.9999924">
Table 6 shows the performance of our parser on
the test data and summarizes the results of previ-
ous work. Our parser achieved an f-score of 88.4
on the test data, which is comparable to the accu-
racy achieved by recent discriminative approaches
such as Finkel et al. (2008) and Petrov &amp; Klein
(2008), but is not as high as the state-of-the-art
accuracy achieved by the parsers that can incor-
porate global features such as Huang (2008) and
Charniak &amp; Johnson (2005). Our parser was more
accurate than traditional history-based approaches
such as Sagae &amp; Lavie (2006) and Ratnaparkhi
(1997), and was significantly better than previous
cascaded chunking approaches such as Tsuruoka
&amp; Tsujii (2005) and Tjong Kim Sang (2001).
Although the comparison presented in the table
is not perfectly fair because of the differences in
hardware platforms, the results show that our pars-
ing model is a promising addition to the parsing
frameworks for building a fast and accurate parser.
</bodyText>
<sectionHeader confidence="0.998517" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999972229166667">
One of the obvious ways to improve the accuracy
of our parser is to improve the accuracy of in-
dividual CRF models. As mentioned earlier, we
were not able to use second-order features on state
transitions, which would have been very useful,
due to the problem of computational cost. Incre-
mental feature selection methods such as grafting
(Perkins et al., 2003) may help us to incorporate
such higher-order features, but the problem of de-
creased efficiency of dynamic programming in the
CRF would probably need to be addressed.
In this work, we treated the chunking problem
as a sequence labeling problem by using the BIO
representation for the chunks. However, semi-
Markov conditional random fields (semi-CRFs)
can directly handle the chunking problem by
considering all possible combinations of subse-
quences of arbitrary length (Sarawagi and Cohen,
2004). Semi-CRFs allow one to use a richer set
of features than CRFs, so the use of semi-CRFs
in our parsing framework should lead to improved
accuracy. Moreover, semi-CRFs would allow us to
incorporate some useful restrictions in producing
chunking hypotheses. For example, we could nat-
urally incorporate the restriction that every chunk
has to contain at least one symbol that has just
been created in the previous level3. It is hard for
the normal CRF model to incorporate such restric-
tions.
Introducing latent variables into the CRF model
may be another promising approach. This is the
main idea of Petrov and Klein (2008), which sig-
nificantly improved parsing accuracy.
A totally different approach to improving the
accuracy of our parser is to use the idea of “self-
training” described in (McClosky et al., 2006).
The basic idea is to create a larger set of training
data by applying an accurate parser (e.g. rerank-
ing parser) to a large amount of raw text. We can
then use the automatically created treebank as the
additional training data for our parser. This ap-
proach suggests that accurate (but slow) parsers
and fast (but not-so-accurate) parsers can actually
help each other.
Also, since it is not difficult to extend our parser
to produce N-best parsing hypotheses, one could
build a fast reranking parser by using the parser as
the base (hypotheses generating) parser.
</bodyText>
<sectionHeader confidence="0.999061" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999834">
Although the idea of treating full parsing as a se-
ries of chunking problems has a long history, there
has not been a competitive parser based on this
parsing framework. In this paper, we have demon-
strated that the framework actually enables us to
</bodyText>
<footnote confidence="0.998929">
3For example, the sequence VBD DT JJ in Figure 2 can-
not be a chunk in the current level because it would have been
already chunked in the previous level if it were.
</footnote>
<page confidence="0.988047">
796
</page>
<table confidence="0.999917307692308">
Recall Precision F-score Time (min)
This work (deterministic) 86.3 87.5 86.9 0.5
This work (search, beam width = 4) 88.2 88.7 88.4 1.7
Huang (2008) 91.7 Unk
Finkel et al. (2008) 87.8 88.2 88.0 &gt;250*
Petrov &amp; Klein (2008) 88.3 3*
Sagae &amp; Lavie (2006) 87.8 88.1 87.9 17
Charniak &amp; Johnson (2005) 90.6 91.3 91.0 Unk
Tsuruoka &amp; Tsujii (2005) 85.0 86.8 85.9 2
Collins (1999) 88.1 88.3 88.2 39**
Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk
Charniak (2000) 89.6 89.5 89.5 23**
Ratnaparkhi (1997) 86.3 87.5 86.9 Unk
</table>
<tableCaption confidence="0.994376">
Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the
</tableCaption>
<bodyText confidence="0.972522052631579">
training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the
parsers.
build a competitive parser if we use CRF mod-
els for each level of chunking and a depth-first
search algorithm to search for the highest proba-
bility parse.
Like other discriminative learning approaches,
one of the advantages of our parser is its general-
ity. The design of our parser is very generic, and
the features used in our parser are not particularly
specific to the Penn Treebank. We expect it to be
straightforward to adapt the parser to other projec-
tive grammars and languages.
This parsing framework should be useful when
one needs to process a large amount of text or
when real time processing is required, in which
the parsing speed is of top priority. In the deter-
ministic setting, our parser only needed about 10
msec to parse a sentence.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999859285714286">
This work described in this paper has been
funded by the Biotechnology and Biological Sci-
ences Research Council (BBSRC; BB/E004431/1)
and the European BOOTStrep project (FP6 -
028099). The research team is hosted by the
JISC/BBSRC/EPSRC sponsored National Centre
for Text Mining.
</bodyText>
<sectionHeader confidence="0.999642" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999749472222222">
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In Pro-
ceedings ofICML, pages 33–40.
Thorsten Brants. 1999. Cascaded markov models. In
Proceedings of EACL.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149–164.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings ofACL, pages 173–180.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL 2000,
pages 132–139.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of COLING 2004, pages 282–
288.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings ofACL-
08:HLT, pages 959–967.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings ofACL, pages
824–831.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08:HLT, pages 586–594.
Aravind K. Joshi and Phil Hopely. 1996. A parser
from antiquity. Natural Language Engineering,
2(4):291–294.
</reference>
<page confidence="0.970059">
797
</page>
<reference confidence="0.999777979591837">
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings ofAAAI/IAAI, pages 703–710.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL, pages
276–283.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings ofHLT-NAACL.
Yusuke Miyao, Rune Saetre, Kenji Sage, Takuya Mat-
suzaki, and Jun’ichi Tsujii. 2008. Task-oriented
evaluation of syntactic parsers and their representa-
tions. In Proceedings ofACL-08:HLT, pages 46–54.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of EMNLP 2006,
pages 155–163.
Simon Perkins, Kevin Lacker, and James Theiler.
2003. Grafting: fast, incremental feature selection
by gradient descent in function space. The Journal
ofMachine Learning Research, 3:1333–1356.
Slav Petrov and Dan Klein. 2008. Discriminative
log-linear grammars with latent variables. In Ad-
vances in Neural Information Processing Systems 20
(NIPS), pages 1153–1160.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proceedings of EMNLP 1997, pages 1–10.
Kenji Sagae and Alon Lavie. 2006. A best-first proba-
bilistic shift-reduce parser. In Proceedings of COL-
ING/ACL, pages 691–698.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Proceedings ofNIPS.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL.
Erik Tjong Kim Sang. 2001. Transforming a chunker
to a parser. In J. Veenstra W. Daelemans, K. Sima‘an
and J. Zavrel, editors, Computational Linguistics in
the Netherlands 2000, pages 177–188. Rodopi.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Chunk
parsing revisited. In Proceedings of IWPT, pages
133–140.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured
syntactic features. In Proceedings of COLING/ACL,
pages 41–48.
</reference>
<page confidence="0.997048">
798
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.308887">
<title confidence="0.999824">Fast Full Parsing by Linear-Chain Conditional Random Fields</title>
<author confidence="0.995798">Jun’ichi Sophia</author>
<affiliation confidence="0.985854">tSchool of Computer Science, University of Manchester, UK</affiliation>
<address confidence="0.4955825">tNational Centre for Text Mining (NaCTeM), UK of Computer Science, University of Tokyo, Japan</address>
<abstract confidence="0.9992536875">This paper presents a chunking-based discriminative approach to full parsing. We convert the task of full parsing into a series of chunking tasks and apply a conditional random field (CRF) model to each level of chunking. The probability of an entire parse tree is computed as the product of the probabilities of individual chunking results. The parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depthfirst search algorithm. Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Jianfeng Gao</author>
</authors>
<title>Scalable training of L1-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="9959" citStr="Andrew and Gao, 2007" startWordPosition="1660" endWordPosition="1663">s and edges combined with surface observations. The weights of the features are determined in such a way that they maximize the conditional loglikelihood of the training data: Ga = XN log p(y(i)|x(i)) + R(λ), i=1 where R(λ) is introduced for the purpose of regularization which prevents the model from overfitting the training data. The L1 or L2 norm is commonly used in statistical natural language processing (Gao et al., 2007). We used L1-regularization, which is defined as XK 1 R(λ) = C k=1 where C is the meta-parameter that controls the degree of regularization. We used the OWL-QN algorithm (Andrew and Gao, 2007) to obtain the parameters that maximize the L1-regularized conditional log-likelihood. 3.2 Features Table 1 shows the features used in chunking for the base level. Since the task is basically identical to shallow parsing by CRFs, we follow the feature sets used in the previous work by Sha and Pereira (2003). We use unigrams, bigrams, and trigrams of part-of-speech (POS) tags and words. The difference between our CRF chunker and that in (Sha and Pereira, 2003) is that we could not use second-order CRF models, hence we could not use trigram features on the BIO states. We |λk|, 792 Symbol Unigram</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Galen Andrew and Jianfeng Gao. 2007. Scalable training of L1-regularized log-linear models. In Proceedings ofICML, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Cascaded markov models.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="6965" citStr="Brants (1999)" startWordPosition="1126" endWordPosition="1127"> two base phrases, (NP Estimated volume) and (QP 2.4 million), and replaces each phrase with its non-terminal symbol and head1. In the second level, the chunker identifies a noun phrase, (NP a light million ounces), and converts it into NP. This process is repeated until the whole sentence is chunked at the fourth level. The full parse tree is recovered from the chunking history in a straightforward way. This idea of converting full parsing into a series of chunking tasks is not new by any means— the history of this kind of approach dates back to 1950s (Joshi and Hopely, 1996). More recently, Brants (1999) used a cascaded Markov model to parse German text. Tjong Kim Sang (2001) used the IOB tagging method to represent chunks and memory-based learning, and achieved an f-score of 80.49 on the WSJ corpus. Tsuruoka and Tsujii (2005) improved upon their approach by using 1The head word is identified by using the headpercolation table (Magerman, 1995). 791 5000 4000 # sentences 3000 2000 1000 0 0 5 10 15 20 25 30 Height 3.1 Linear Chain CRFs A linear chain CRF defines a single log-linear probabilistic distribution over all possible tag sequences y for the input sequence x: T K 1 XXZ(x) exp λkfk(t, yt</context>
</contexts>
<marker>Brants, 1999</marker>
<rawString>Thorsten Brants. 1999. Cascaded markov models. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X,</booktitle>
<pages>149--164</pages>
<contexts>
<context position="1974" citStr="Buchholz and Marsi, 2006" startWordPosition="294" endWordPosition="297">sing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their superior accuracy (Charniak and Johnson, 2005; Huang, 2008) and adaptability to new grammars and languages (Buchholz and Marsi, 2006). A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach to discriminative full parsing is to treat the task as a single structured </context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL-X, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="1886" citStr="Charniak and Johnson, 2005" startWordPosition="281" endWordPosition="284"> obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their superior accuracy (Charniak and Johnson, 2005; Huang, 2008) and adaptability to new grammars and languages (Buchholz and Marsi, 2006). A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more rec</context>
<context position="22625" citStr="Charniak &amp; Johnson (2005)" startWordPosition="3824" endWordPosition="3827">difference between the beam width of 4 and 5, so we adopted the beam width of 4 for the final accuracy report on the test data. 6.3 Comparison with Previous Work Table 6 shows the performance of our parser on the test data and summarizes the results of previous work. Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al. (2008) and Petrov &amp; Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak &amp; Johnson (2005). Our parser was more accurate than traditional history-based approaches such as Sagae &amp; Lavie (2006) and Ratnaparkhi (1997), and was significantly better than previous cascaded chunking approaches such as Tsuruoka &amp; Tsujii (2005) and Tjong Kim Sang (2001). Although the comparison presented in the table is not perfectly fair because of the differences in hardware platforms, the results show that our parsing model is a promising addition to the parsing frameworks for building a fast and accurate parser. 7 Discussion One of the obvious ways to improve the accuracy of our parser is to improve the</context>
<context position="26098" citStr="Charniak &amp; Johnson (2005)" startWordPosition="4404" endWordPosition="4407">ng history, there has not been a competitive parser based on this parsing framework. In this paper, we have demonstrated that the framework actually enables us to 3For example, the sequence VBD DT JJ in Figure 2 cannot be a chunk in the current level because it would have been already chunked in the previous level if it were. 796 Recall Precision F-score Time (min) This work (deterministic) 86.3 87.5 86.9 0.5 This work (search, beam width = 4) 88.2 88.7 88.4 1.7 Huang (2008) 91.7 Unk Finkel et al. (2008) 87.8 88.2 88.0 &gt;250* Petrov &amp; Klein (2008) 88.3 3* Sagae &amp; Lavie (2006) 87.8 88.1 87.9 17 Charniak &amp; Johnson (2005) 90.6 91.3 91.0 Unk Tsuruoka &amp; Tsujii (2005) 85.0 86.8 85.9 2 Collins (1999) 88.1 88.3 88.2 39** Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk Charniak (2000) 89.6 89.5 89.5 23** Ratnaparkhi (1997) 86.3 87.5 86.9 Unk Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the parsers. build a competitive parser if we use CRF models for each level of chunking and a depth-first search algorithm to search for the highest probability parse. Like other discriminative </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings ofACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1768" citStr="Charniak, 2000" startWordPosition="265" endWordPosition="267">, pronoun resolution (Yang et al., 2006), and information extraction (Miyao et al., 2008). One of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their superior accuracy (Charniak and Johnson, 2005; Huang, 2008) and adaptability to new grammars and languages (Buchholz and Marsi, 2006). A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for </context>
<context position="8134" citStr="Charniak, 2000" startWordPosition="1337" endWordPosition="1338"> input sequence x: T K 1 XXZ(x) exp λkfk(t, yt, yt−1, x), t=1 k=1 where fk(t, yt, yt−1, x) is typically a binary function indicating the presence of feature k, λk is the weight of the feature, and Z(X) is a normalization function: p(y|x) = Figure 5: Distribution of tree height in WSJ sec- X exp T K λkfk(t, yt, yt−1, x). tions 2-21. Z(x) = X X y t=1 k=1 a maximum entropy classifier and achieved an fscore of 85.9. However, there is still a large gap between the accuracy of chunking-based parsers and that of widely-used practical parsers such as Collins parser and Charniak parser (Collins, 1999; Charniak, 2000). 2.1 Heights of Trees A natural question about this parsing framework is how many levels of chunking are usually needed to parse a sentence. We examined the distribution of the heights of the trees in sections 2-21 of the Wall Street Journal (WSJ) corpus. The result is shown in Figure 5. Most of the sentences have less than 20 levels. The average was 10.0, which means we need to perform, on average, 10 chunking tasks to obtain a full parse tree for a sentence if the parsing is performed in a deterministic manner. 3 Chunking with CRFs The accuracy of chunk parsing is highly dependent on the ac</context>
<context position="26251" citStr="Charniak (2000)" startWordPosition="4434" endWordPosition="4435">3For example, the sequence VBD DT JJ in Figure 2 cannot be a chunk in the current level because it would have been already chunked in the previous level if it were. 796 Recall Precision F-score Time (min) This work (deterministic) 86.3 87.5 86.9 0.5 This work (search, beam width = 4) 88.2 88.7 88.4 1.7 Huang (2008) 91.7 Unk Finkel et al. (2008) 87.8 88.2 88.0 &gt;250* Petrov &amp; Klein (2008) 88.3 3* Sagae &amp; Lavie (2006) 87.8 88.1 87.9 17 Charniak &amp; Johnson (2005) 90.6 91.3 91.0 Unk Tsuruoka &amp; Tsujii (2005) 85.0 86.8 85.9 2 Collins (1999) 88.1 88.3 88.2 39** Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk Charniak (2000) 89.6 89.5 89.5 23** Ratnaparkhi (1997) 86.3 87.5 86.9 Unk Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the parsers. build a competitive parser if we use CRF models for each level of chunking and a depth-first search algorithm to search for the highest probability parse. Like other discriminative learning approaches, one of the advantages of our parser is its generality. The design of our parser is very generic, and the features used in our parser</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of NAACL 2000, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 2004,</booktitle>
<pages>282--288</pages>
<contexts>
<context position="4869" citStr="Clark and Curran, 2004" startWordPosition="772" endWordPosition="775">of history-based approaches, it is one step closer to the whole-sentence approaches because the parser uses a whole-sequence model (i.e. CRFs) for individual chunking tasks. In other words, our parser could be located somewhere between traditional history-based approaches and whole-sentence approaches. One of our motivations for this work was that our parsing model may achieve a better balance between accuracy and speed than existing parsers. It is also worth mentioning that our approach is similar in spirit to supertagging for parsing with lexicalized grammar formalisms such as CCG and HPSG (Clark and Curran, 2004; Ninomiya et al., 2006), in which significant speed-ups for parsing time are achieved. In this paper, we show that our approach is indeed appealing in that the parser runs very fast and gives competitive accuracy. We evaluate our parser on the standard data set for parsing experiments (i.e. the Penn Treebank) and compare it with existing approaches to full parsing. This paper is organized as follows. Section 2 presents the overall chunk parsing strategy. Section 3 describes the CRF model used to perform individual chunking steps. Section 4 describes the depth-first algorithm for finding the b</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. The importance of supertagging for wide-coverage CCG parsing. In Proceedings of COLING 2004, pages 282– 288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1751" citStr="Collins, 1999" startWordPosition="263" endWordPosition="264">nd Marcu, 2000), pronoun resolution (Yang et al., 2006), and information extraction (Miyao et al., 2008). One of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their superior accuracy (Charniak and Johnson, 2005; Huang, 2008) and adaptability to new grammars and languages (Buchholz and Marsi, 2006). A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum e</context>
<context position="8117" citStr="Collins, 1999" startWordPosition="1335" endWordPosition="1336">ences y for the input sequence x: T K 1 XXZ(x) exp λkfk(t, yt, yt−1, x), t=1 k=1 where fk(t, yt, yt−1, x) is typically a binary function indicating the presence of feature k, λk is the weight of the feature, and Z(X) is a normalization function: p(y|x) = Figure 5: Distribution of tree height in WSJ sec- X exp T K λkfk(t, yt, yt−1, x). tions 2-21. Z(x) = X X y t=1 k=1 a maximum entropy classifier and achieved an fscore of 85.9. However, there is still a large gap between the accuracy of chunking-based parsers and that of widely-used practical parsers such as Collins parser and Charniak parser (Collins, 1999; Charniak, 2000). 2.1 Heights of Trees A natural question about this parsing framework is how many levels of chunking are usually needed to parse a sentence. We examined the distribution of the heights of the trees in sections 2-21 of the Wall Street Journal (WSJ) corpus. The result is shown in Figure 5. Most of the sentences have less than 20 levels. The average was 10.0, which means we need to perform, on average, 10 chunking tasks to obtain a full parse tree for a sentence if the parsing is performed in a deterministic manner. 3 Chunking with CRFs The accuracy of chunk parsing is highly de</context>
<context position="20119" citStr="Collins, 1999" startWordPosition="3386" endWordPosition="3387">but it may be useful to design special features for this particular type of phrase, considering the dominance of noun phrases in the corpus. Although not directly comparable, Sha and Pereira (2003) report almost the same level of accuracy (94.38%) on noun phrase recognition, using a much smaller training set. We attribute their superior performance mainly to the use of second-order features on state transitions. Table 4 also suggests that adverb phrases (ADVP) and adjective phrases (ADJP) are more difficult to recognize than other types of phrases, which coincides with the result reported in (Collins, 1999). It should be noted that the performance reported in this table was evaluated using the gold standard sequences as the input to the CRF chunkers. In the real parsing process, the chunkers have to use the output from the previous (one level below) chunker, so the quality of the input is not as good as that used in this evaluation. 6.2 Parsing Performance Next, we present the actual parsing performance. The first set of experiments concerns the relationship between the width of beam and the parsing performance. Table 5 shows the results obtained on the development data. We varied the width of t</context>
<context position="26174" citStr="Collins (1999)" startWordPosition="4420" endWordPosition="4421"> this paper, we have demonstrated that the framework actually enables us to 3For example, the sequence VBD DT JJ in Figure 2 cannot be a chunk in the current level because it would have been already chunked in the previous level if it were. 796 Recall Precision F-score Time (min) This work (deterministic) 86.3 87.5 86.9 0.5 This work (search, beam width = 4) 88.2 88.7 88.4 1.7 Huang (2008) 91.7 Unk Finkel et al. (2008) 87.8 88.2 88.0 &gt;250* Petrov &amp; Klein (2008) 88.3 3* Sagae &amp; Lavie (2006) 87.8 88.1 87.9 17 Charniak &amp; Johnson (2005) 90.6 91.3 91.0 Unk Tsuruoka &amp; Tsujii (2005) 85.0 86.8 85.9 2 Collins (1999) 88.1 88.3 88.2 39** Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk Charniak (2000) 89.6 89.5 89.5 23** Ratnaparkhi (1997) 86.3 87.5 86.9 Unk Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the parsers. build a competitive parser if we use CRF models for each level of chunking and a depth-first search algorithm to search for the highest probability parse. Like other discriminative learning approaches, one of the advantages of our parser is its generality. </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Alex Kleeman</author>
<author>Christopher D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL08:HLT,</booktitle>
<pages>959--967</pages>
<contexts>
<context position="2614" citStr="Finkel et al. (2008)" startWordPosition="394" endWordPosition="397">oach to discriminative full parsing is to convert a full parsing task into a series of classification problems. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem. Finkel et al. (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Huang (2008) proposed to use a parse forest to incorporate non-local features. They used a perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy. Petrov and Klein (2008) introduced latent variables in tree CRFs and proposed a caching mechanism to speed up the computation. In general, the latter whole-sentence approaches give better accuracy than history-based approaches because they can better trade off decisions made in different parts in a parse tree. However,</context>
<context position="22434" citStr="Finkel et al. (2008)" startWordPosition="3791" endWordPosition="3794">atures were 58,505 (for the base chunker), 263,889 (for the non-base chunker), and 42,201 (for the POS tagger). The parser required only 14MB of memory to run. There was little accuracy difference between the beam width of 4 and 5, so we adopted the beam width of 4 for the final accuracy report on the test data. 6.3 Comparison with Previous Work Table 6 shows the performance of our parser on the test data and summarizes the results of previous work. Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al. (2008) and Petrov &amp; Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak &amp; Johnson (2005). Our parser was more accurate than traditional history-based approaches such as Sagae &amp; Lavie (2006) and Ratnaparkhi (1997), and was significantly better than previous cascaded chunking approaches such as Tsuruoka &amp; Tsujii (2005) and Tjong Kim Sang (2001). Although the comparison presented in the table is not perfectly fair because of the differences in hardware platforms, the results show that our parsin</context>
<context position="25982" citStr="Finkel et al. (2008)" startWordPosition="4382" endWordPosition="4385">ting) parser. 8 Conclusion Although the idea of treating full parsing as a series of chunking problems has a long history, there has not been a competitive parser based on this parsing framework. In this paper, we have demonstrated that the framework actually enables us to 3For example, the sequence VBD DT JJ in Figure 2 cannot be a chunk in the current level because it would have been already chunked in the previous level if it were. 796 Recall Precision F-score Time (min) This work (deterministic) 86.3 87.5 86.9 0.5 This work (search, beam width = 4) 88.2 88.7 88.4 1.7 Huang (2008) 91.7 Unk Finkel et al. (2008) 87.8 88.2 88.0 &gt;250* Petrov &amp; Klein (2008) 88.3 3* Sagae &amp; Lavie (2006) 87.8 88.1 87.9 17 Charniak &amp; Johnson (2005) 90.6 91.3 91.0 Unk Tsuruoka &amp; Tsujii (2005) 85.0 86.8 85.9 2 Collins (1999) 88.1 88.3 88.2 39** Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk Charniak (2000) 89.6 89.5 89.5 23** Ratnaparkhi (1997) 86.3 87.5 86.9 Unk Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the parsers. build a competitive parser if we use CRF models for each level of</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional random field parsing. In Proceedings ofACL08:HLT, pages 959–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Galen Andrew</author>
<author>Mark Johnson</author>
<author>Kristina Toutanova</author>
</authors>
<title>A comparative study of parameter estimation methods for statistical natural language processing.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>824--831</pages>
<contexts>
<context position="9767" citStr="Gao et al., 2007" startWordPosition="1627" endWordPosition="1630">ation enables us to use the linearchain CRF model to perform chunking, since the task is simply assigning appropriate labels to a sequence. This model allows us to define features on states and edges combined with surface observations. The weights of the features are determined in such a way that they maximize the conditional loglikelihood of the training data: Ga = XN log p(y(i)|x(i)) + R(λ), i=1 where R(λ) is introduced for the purpose of regularization which prevents the model from overfitting the training data. The L1 or L2 norm is commonly used in statistical natural language processing (Gao et al., 2007). We used L1-regularization, which is defined as XK 1 R(λ) = C k=1 where C is the meta-parameter that controls the degree of regularization. We used the OWL-QN algorithm (Andrew and Gao, 2007) to obtain the parameters that maximize the L1-regularized conditional log-likelihood. 3.2 Features Table 1 shows the features used in chunking for the base level. Since the task is basically identical to shallow parsing by CRFs, we follow the feature sets used in the previous work by Sha and Pereira (2003). We use unigrams, bigrams, and trigrams of part-of-speech (POS) tags and words. The difference betw</context>
</contexts>
<marker>Gao, Andrew, Johnson, Toutanova, 2007</marker>
<rawString>Jianfeng Gao, Galen Andrew, Mark Johnson, and Kristina Toutanova. 2007. A comparative study of parameter estimation methods for statistical natural language processing. In Proceedings ofACL, pages 824–831.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08:HLT,</booktitle>
<pages>586--594</pages>
<contexts>
<context position="1900" citStr="Huang, 2008" startWordPosition="285" endWordPosition="286">he use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their superior accuracy (Charniak and Johnson, 2005; Huang, 2008) and adaptability to new grammars and languages (Buchholz and Marsi, 2006). A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach t</context>
<context position="22595" citStr="Huang (2008)" startWordPosition="3821" endWordPosition="3822"> little accuracy difference between the beam width of 4 and 5, so we adopted the beam width of 4 for the final accuracy report on the test data. 6.3 Comparison with Previous Work Table 6 shows the performance of our parser on the test data and summarizes the results of previous work. Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al. (2008) and Petrov &amp; Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak &amp; Johnson (2005). Our parser was more accurate than traditional history-based approaches such as Sagae &amp; Lavie (2006) and Ratnaparkhi (1997), and was significantly better than previous cascaded chunking approaches such as Tsuruoka &amp; Tsujii (2005) and Tjong Kim Sang (2001). Although the comparison presented in the table is not perfectly fair because of the differences in hardware platforms, the results show that our parsing model is a promising addition to the parsing frameworks for building a fast and accurate parser. 7 Discussion One of the obvious ways to improve the accuracy o</context>
<context position="25952" citStr="Huang (2008)" startWordPosition="4378" endWordPosition="4379">ase (hypotheses generating) parser. 8 Conclusion Although the idea of treating full parsing as a series of chunking problems has a long history, there has not been a competitive parser based on this parsing framework. In this paper, we have demonstrated that the framework actually enables us to 3For example, the sequence VBD DT JJ in Figure 2 cannot be a chunk in the current level because it would have been already chunked in the previous level if it were. 796 Recall Precision F-score Time (min) This work (deterministic) 86.3 87.5 86.9 0.5 This work (search, beam width = 4) 88.2 88.7 88.4 1.7 Huang (2008) 91.7 Unk Finkel et al. (2008) 87.8 88.2 88.0 &gt;250* Petrov &amp; Klein (2008) 88.3 3* Sagae &amp; Lavie (2006) 87.8 88.1 87.9 17 Charniak &amp; Johnson (2005) 90.6 91.3 91.0 Unk Tsuruoka &amp; Tsujii (2005) 85.0 86.8 85.9 2 Collins (1999) 88.1 88.3 88.2 39** Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk Charniak (2000) 89.6 89.5 89.5 23** Ratnaparkhi (1997) 86.3 87.5 86.9 Unk Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the parsers. build a competitive parser if we us</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL-08:HLT, pages 586–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Phil Hopely</author>
</authors>
<title>A parser from antiquity.</title>
<date>1996</date>
<journal>Natural Language Engineering,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="6935" citStr="Joshi and Hopely, 1996" startWordPosition="1120" endWordPosition="1123">rst (base) level, the chunker identifies two base phrases, (NP Estimated volume) and (QP 2.4 million), and replaces each phrase with its non-terminal symbol and head1. In the second level, the chunker identifies a noun phrase, (NP a light million ounces), and converts it into NP. This process is repeated until the whole sentence is chunked at the fourth level. The full parse tree is recovered from the chunking history in a straightforward way. This idea of converting full parsing into a series of chunking tasks is not new by any means— the history of this kind of approach dates back to 1950s (Joshi and Hopely, 1996). More recently, Brants (1999) used a cascaded Markov model to parse German text. Tjong Kim Sang (2001) used the IOB tagging method to represent chunks and memory-based learning, and achieved an f-score of 80.49 on the WSJ corpus. Tsuruoka and Tsujii (2005) improved upon their approach by using 1The head word is identified by using the headpercolation table (Magerman, 1995). 791 5000 4000 # sentences 3000 2000 1000 0 0 5 10 15 20 25 30 Height 3.1 Linear Chain CRFs A linear chain CRF defines a single log-linear probabilistic distribution over all possible tag sequences y for the input sequence </context>
</contexts>
<marker>Joshi, Hopely, 1996</marker>
<rawString>Aravind K. Joshi and Phil Hopely. 1996. A parser from antiquity. Natural Language Engineering, 2(4):291–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticsbased summarization - step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proceedings ofAAAI/IAAI,</booktitle>
<pages>703--710</pages>
<contexts>
<context position="1153" citStr="Knight and Marcu, 2000" startWordPosition="165" endWordPosition="168">andom field (CRF) model to each level of chunking. The probability of an entire parse tree is computed as the product of the probabilities of individual chunking results. The parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depthfirst search algorithm. Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser. 1 Introduction Full parsing analyzes the phrase structure of a sentence and provides useful input for many kinds of high-level natural language processing such as summarization (Knight and Marcu, 2000), pronoun resolution (Yang et al., 2006), and information extraction (Miyao et al., 2008). One of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; </context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization - step one: Sentence compression. In Proceedings ofAAAI/IAAI, pages 703–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>276--283</pages>
<contexts>
<context position="7311" citStr="Magerman, 1995" startWordPosition="1184" endWordPosition="1185">s recovered from the chunking history in a straightforward way. This idea of converting full parsing into a series of chunking tasks is not new by any means— the history of this kind of approach dates back to 1950s (Joshi and Hopely, 1996). More recently, Brants (1999) used a cascaded Markov model to parse German text. Tjong Kim Sang (2001) used the IOB tagging method to represent chunks and memory-based learning, and achieved an f-score of 80.49 on the WSJ corpus. Tsuruoka and Tsujii (2005) improved upon their approach by using 1The head word is identified by using the headpercolation table (Magerman, 1995). 791 5000 4000 # sentences 3000 2000 1000 0 0 5 10 15 20 25 30 Height 3.1 Linear Chain CRFs A linear chain CRF defines a single log-linear probabilistic distribution over all possible tag sequences y for the input sequence x: T K 1 XXZ(x) exp λkfk(t, yt, yt−1, x), t=1 k=1 where fk(t, yt, yt−1, x) is typically a binary function indicating the presence of feature k, λk is the weight of the feature, and Z(X) is a normalization function: p(y|x) = Figure 5: Distribution of tree height in WSJ sec- X exp T K λkfk(t, yt, yt−1, x). tions 2-21. Z(x) = X X y t=1 k=1 a maximum entropy classifier and achi</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical decision-tree models for parsing. In Proceedings of ACL, pages 276–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="24811" citStr="McClosky et al., 2006" startWordPosition="4176" endWordPosition="4179">rporate some useful restrictions in producing chunking hypotheses. For example, we could naturally incorporate the restriction that every chunk has to contain at least one symbol that has just been created in the previous level3. It is hard for the normal CRF model to incorporate such restrictions. Introducing latent variables into the CRF model may be another promising approach. This is the main idea of Petrov and Klein (2008), which significantly improved parsing accuracy. A totally different approach to improving the accuracy of our parser is to use the idea of “selftraining” described in (McClosky et al., 2006). The basic idea is to create a larger set of training data by applying an accurate parser (e.g. reranking parser) to a large amount of raw text. We can then use the automatically created treebank as the additional training data for our parser. This approach suggests that accurate (but slow) parsers and fast (but not-so-accurate) parsers can actually help each other. Also, since it is not difficult to extend our parser to produce N-best parsing hypotheses, one could build a fast reranking parser by using the parser as the base (hypotheses generating) parser. 8 Conclusion Although the idea of t</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Rune Saetre</author>
<author>Kenji Sage</author>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Task-oriented evaluation of syntactic parsers and their representations.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08:HLT,</booktitle>
<pages>46--54</pages>
<contexts>
<context position="1242" citStr="Miyao et al., 2008" startWordPosition="180" endWordPosition="183"> computed as the product of the probabilities of individual chunking results. The parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depthfirst search algorithm. Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser. 1 Introduction Full parsing analyzes the phrase structure of a sentence and provides useful input for many kinds of high-level natural language processing such as summarization (Knight and Marcu, 2000), pronoun resolution (Yang et al., 2006), and information extraction (Miyao et al., 2008). One of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their s</context>
</contexts>
<marker>Miyao, Saetre, Sage, Matsuzaki, Tsujii, 2008</marker>
<rawString>Yusuke Miyao, Rune Saetre, Kenji Sage, Takuya Matsuzaki, and Jun’ichi Tsujii. 2008. Task-oriented evaluation of syntactic parsers and their representations. In Proceedings ofACL-08:HLT, pages 46–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Takuya Matsuzaki</author>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Extremely lexicalized models for accurate and fast HPSG parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>155--163</pages>
<contexts>
<context position="4893" citStr="Ninomiya et al., 2006" startWordPosition="776" endWordPosition="779">hes, it is one step closer to the whole-sentence approaches because the parser uses a whole-sequence model (i.e. CRFs) for individual chunking tasks. In other words, our parser could be located somewhere between traditional history-based approaches and whole-sentence approaches. One of our motivations for this work was that our parsing model may achieve a better balance between accuracy and speed than existing parsers. It is also worth mentioning that our approach is similar in spirit to supertagging for parsing with lexicalized grammar formalisms such as CCG and HPSG (Clark and Curran, 2004; Ninomiya et al., 2006), in which significant speed-ups for parsing time are achieved. In this paper, we show that our approach is indeed appealing in that the parser runs very fast and gives competitive accuracy. We evaluate our parser on the standard data set for parsing experiments (i.e. the Penn Treebank) and compare it with existing approaches to full parsing. This paper is organized as follows. Section 2 presents the overall chunk parsing strategy. Section 3 describes the CRF model used to perform individual chunking steps. Section 4 describes the depth-first algorithm for finding the best derivation of a pars</context>
</contexts>
<marker>Ninomiya, Matsuzaki, Tsuruoka, Miyao, Tsujii, 2006</marker>
<rawString>Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006. Extremely lexicalized models for accurate and fast HPSG parsing. In Proceedings of EMNLP 2006, pages 155–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Perkins</author>
<author>Kevin Lacker</author>
<author>James Theiler</author>
</authors>
<title>Grafting: fast, incremental feature selection by gradient descent in function space.</title>
<date>2003</date>
<journal>The Journal ofMachine Learning Research,</journal>
<pages>3--1333</pages>
<contexts>
<context position="23505" citStr="Perkins et al., 2003" startWordPosition="3966" endWordPosition="3969">lthough the comparison presented in the table is not perfectly fair because of the differences in hardware platforms, the results show that our parsing model is a promising addition to the parsing frameworks for building a fast and accurate parser. 7 Discussion One of the obvious ways to improve the accuracy of our parser is to improve the accuracy of individual CRF models. As mentioned earlier, we were not able to use second-order features on state transitions, which would have been very useful, due to the problem of computational cost. Incremental feature selection methods such as grafting (Perkins et al., 2003) may help us to incorporate such higher-order features, but the problem of decreased efficiency of dynamic programming in the CRF would probably need to be addressed. In this work, we treated the chunking problem as a sequence labeling problem by using the BIO representation for the chunks. However, semiMarkov conditional random fields (semi-CRFs) can directly handle the chunking problem by considering all possible combinations of subsequences of arbitrary length (Sarawagi and Cohen, 2004). Semi-CRFs allow one to use a richer set of features than CRFs, so the use of semi-CRFs in our parsing fr</context>
</contexts>
<marker>Perkins, Lacker, Theiler, 2003</marker>
<rawString>Simon Perkins, Kevin Lacker, and James Theiler. 2003. Grafting: fast, incremental feature selection by gradient descent in function space. The Journal ofMachine Learning Research, 3:1333–1356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative log-linear grammars with latent variables.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 20 (NIPS),</booktitle>
<pages>1153--1160</pages>
<contexts>
<context position="2917" citStr="Petrov and Klein (2008)" startWordPosition="443" endWordPosition="446">06) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem. Finkel et al. (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Huang (2008) proposed to use a parse forest to incorporate non-local features. They used a perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy. Petrov and Klein (2008) introduced latent variables in tree CRFs and proposed a caching mechanism to speed up the computation. In general, the latter whole-sentence approaches give better accuracy than history-based approaches because they can better trade off decisions made in different parts in a parse tree. However, the whole-sentence approaches tend to require a large computational cost both in training and parsing. In contrast, history-based approaches are less computationally intensive and usually produce fast parsers. In this paper, we present a history-based parser using CRFs, by treating the task of full pa</context>
<context position="24620" citStr="Petrov and Klein (2008)" startWordPosition="4145" endWordPosition="4148">4). Semi-CRFs allow one to use a richer set of features than CRFs, so the use of semi-CRFs in our parsing framework should lead to improved accuracy. Moreover, semi-CRFs would allow us to incorporate some useful restrictions in producing chunking hypotheses. For example, we could naturally incorporate the restriction that every chunk has to contain at least one symbol that has just been created in the previous level3. It is hard for the normal CRF model to incorporate such restrictions. Introducing latent variables into the CRF model may be another promising approach. This is the main idea of Petrov and Klein (2008), which significantly improved parsing accuracy. A totally different approach to improving the accuracy of our parser is to use the idea of “selftraining” described in (McClosky et al., 2006). The basic idea is to create a larger set of training data by applying an accurate parser (e.g. reranking parser) to a large amount of raw text. We can then use the automatically created treebank as the additional training data for our parser. This approach suggests that accurate (but slow) parsers and fast (but not-so-accurate) parsers can actually help each other. Also, since it is not difficult to exte</context>
<context position="22460" citStr="Petrov &amp; Klein (2008)" startWordPosition="3796" endWordPosition="3799">he base chunker), 263,889 (for the non-base chunker), and 42,201 (for the POS tagger). The parser required only 14MB of memory to run. There was little accuracy difference between the beam width of 4 and 5, so we adopted the beam width of 4 for the final accuracy report on the test data. 6.3 Comparison with Previous Work Table 6 shows the performance of our parser on the test data and summarizes the results of previous work. Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al. (2008) and Petrov &amp; Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak &amp; Johnson (2005). Our parser was more accurate than traditional history-based approaches such as Sagae &amp; Lavie (2006) and Ratnaparkhi (1997), and was significantly better than previous cascaded chunking approaches such as Tsuruoka &amp; Tsujii (2005) and Tjong Kim Sang (2001). Although the comparison presented in the table is not perfectly fair because of the differences in hardware platforms, the results show that our parsing model is a promising add</context>
<context position="26025" citStr="Petrov &amp; Klein (2008)" startWordPosition="4390" endWordPosition="4393">ea of treating full parsing as a series of chunking problems has a long history, there has not been a competitive parser based on this parsing framework. In this paper, we have demonstrated that the framework actually enables us to 3For example, the sequence VBD DT JJ in Figure 2 cannot be a chunk in the current level because it would have been already chunked in the previous level if it were. 796 Recall Precision F-score Time (min) This work (deterministic) 86.3 87.5 86.9 0.5 This work (search, beam width = 4) 88.2 88.7 88.4 1.7 Huang (2008) 91.7 Unk Finkel et al. (2008) 87.8 88.2 88.0 &gt;250* Petrov &amp; Klein (2008) 88.3 3* Sagae &amp; Lavie (2006) 87.8 88.1 87.9 17 Charniak &amp; Johnson (2005) 90.6 91.3 91.0 Unk Tsuruoka &amp; Tsujii (2005) 85.0 86.8 85.9 2 Collins (1999) 88.1 88.3 88.2 39** Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk Charniak (2000) 89.6 89.5 89.5 23** Ratnaparkhi (1997) 86.3 87.5 86.9 Unk Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the parsers. build a competitive parser if we use CRF models for each level of chunking and a depth-first search algorith</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Slav Petrov and Dan Klein. 2008. Discriminative log-linear grammars with latent variables. In Advances in Neural Information Processing Systems 20 (NIPS), pages 1153–1160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>1--10</pages>
<contexts>
<context position="2124" citStr="Ratnaparkhi (1997)" startWordPosition="318" endWordPosition="319">s and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their superior accuracy (Charniak and Johnson, 2005; Huang, 2008) and adaptability to new grammars and languages (Buchholz and Marsi, 2006). A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem. Finkel et al. (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Huang (2008) proposed </context>
<context position="22749" citStr="Ratnaparkhi (1997)" startWordPosition="3844" endWordPosition="3845"> Comparison with Previous Work Table 6 shows the performance of our parser on the test data and summarizes the results of previous work. Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al. (2008) and Petrov &amp; Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak &amp; Johnson (2005). Our parser was more accurate than traditional history-based approaches such as Sagae &amp; Lavie (2006) and Ratnaparkhi (1997), and was significantly better than previous cascaded chunking approaches such as Tsuruoka &amp; Tsujii (2005) and Tjong Kim Sang (2001). Although the comparison presented in the table is not perfectly fair because of the differences in hardware platforms, the results show that our parsing model is a promising addition to the parsing frameworks for building a fast and accurate parser. 7 Discussion One of the obvious ways to improve the accuracy of our parser is to improve the accuracy of individual CRF models. As mentioned earlier, we were not able to use second-order features on state transitions</context>
<context position="26290" citStr="Ratnaparkhi (1997)" startWordPosition="4440" endWordPosition="4441"> in Figure 2 cannot be a chunk in the current level because it would have been already chunked in the previous level if it were. 796 Recall Precision F-score Time (min) This work (deterministic) 86.3 87.5 86.9 0.5 This work (search, beam width = 4) 88.2 88.7 88.4 1.7 Huang (2008) 91.7 Unk Finkel et al. (2008) 87.8 88.2 88.0 &gt;250* Petrov &amp; Klein (2008) 88.3 3* Sagae &amp; Lavie (2006) 87.8 88.1 87.9 17 Charniak &amp; Johnson (2005) 90.6 91.3 91.0 Unk Tsuruoka &amp; Tsujii (2005) 85.0 86.8 85.9 2 Collins (1999) 88.1 88.3 88.2 39** Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk Charniak (2000) 89.6 89.5 89.5 23** Ratnaparkhi (1997) 86.3 87.5 86.9 Unk Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the parsers. build a competitive parser if we use CRF models for each level of chunking and a depth-first search algorithm to search for the highest probability parse. Like other discriminative learning approaches, one of the advantages of our parser is its generality. The design of our parser is very generic, and the features used in our parser are not particularly specific to the P</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of EMNLP 1997, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A best-first probabilistic shift-reduce parser.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>691--698</pages>
<contexts>
<context position="2297" citStr="Sagae and Lavie (2006)" startWordPosition="344" endWordPosition="347">at success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their superior accuracy (Charniak and Johnson, 2005; Huang, 2008) and adaptability to new grammars and languages (Buchholz and Marsi, 2006). A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem. Finkel et al. (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Huang (2008) proposed to use a parse forest to incorporate non-local features. They used a perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy. Pet</context>
<context position="26464" citStr="Sagae and Lavie, 2006" startWordPosition="4468" endWordPosition="4471">his work (deterministic) 86.3 87.5 86.9 0.5 This work (search, beam width = 4) 88.2 88.7 88.4 1.7 Huang (2008) 91.7 Unk Finkel et al. (2008) 87.8 88.2 88.0 &gt;250* Petrov &amp; Klein (2008) 88.3 3* Sagae &amp; Lavie (2006) 87.8 88.1 87.9 17 Charniak &amp; Johnson (2005) 90.6 91.3 91.0 Unk Tsuruoka &amp; Tsujii (2005) 85.0 86.8 85.9 2 Collins (1999) 88.1 88.3 88.2 39** Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk Charniak (2000) 89.6 89.5 89.5 23** Ratnaparkhi (1997) 86.3 87.5 86.9 Unk Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the parsers. build a competitive parser if we use CRF models for each level of chunking and a depth-first search algorithm to search for the highest probability parse. Like other discriminative learning approaches, one of the advantages of our parser is its generality. The design of our parser is very generic, and the features used in our parser are not particularly specific to the Penn Treebank. We expect it to be straightforward to adapt the parser to other projective grammars and languages. This parsing framework should be useful when one needs to pro</context>
<context position="22726" citStr="Sagae &amp; Lavie (2006)" startWordPosition="3839" endWordPosition="3842">ort on the test data. 6.3 Comparison with Previous Work Table 6 shows the performance of our parser on the test data and summarizes the results of previous work. Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al. (2008) and Petrov &amp; Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak &amp; Johnson (2005). Our parser was more accurate than traditional history-based approaches such as Sagae &amp; Lavie (2006) and Ratnaparkhi (1997), and was significantly better than previous cascaded chunking approaches such as Tsuruoka &amp; Tsujii (2005) and Tjong Kim Sang (2001). Although the comparison presented in the table is not perfectly fair because of the differences in hardware platforms, the results show that our parsing model is a promising addition to the parsing frameworks for building a fast and accurate parser. 7 Discussion One of the obvious ways to improve the accuracy of our parser is to improve the accuracy of individual CRF models. As mentioned earlier, we were not able to use second-order featur</context>
<context position="26054" citStr="Sagae &amp; Lavie (2006)" startWordPosition="4396" endWordPosition="4399"> a series of chunking problems has a long history, there has not been a competitive parser based on this parsing framework. In this paper, we have demonstrated that the framework actually enables us to 3For example, the sequence VBD DT JJ in Figure 2 cannot be a chunk in the current level because it would have been already chunked in the previous level if it were. 796 Recall Precision F-score Time (min) This work (deterministic) 86.3 87.5 86.9 0.5 This work (search, beam width = 4) 88.2 88.7 88.4 1.7 Huang (2008) 91.7 Unk Finkel et al. (2008) 87.8 88.2 88.0 &gt;250* Petrov &amp; Klein (2008) 88.3 3* Sagae &amp; Lavie (2006) 87.8 88.1 87.9 17 Charniak &amp; Johnson (2005) 90.6 91.3 91.0 Unk Tsuruoka &amp; Tsujii (2005) 85.0 86.8 85.9 2 Collins (1999) 88.1 88.3 88.2 39** Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk Charniak (2000) 89.6 89.5 89.5 23** Ratnaparkhi (1997) 86.3 87.5 86.9 Unk Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the parsers. build a competitive parser if we use CRF models for each level of chunking and a depth-first search algorithm to search for the highest p</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. A best-first probabilistic shift-reduce parser. In Proceedings of COLING/ACL, pages 691–698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings ofNIPS.</booktitle>
<contexts>
<context position="23999" citStr="Sarawagi and Cohen, 2004" startWordPosition="4042" endWordPosition="4045">ery useful, due to the problem of computational cost. Incremental feature selection methods such as grafting (Perkins et al., 2003) may help us to incorporate such higher-order features, but the problem of decreased efficiency of dynamic programming in the CRF would probably need to be addressed. In this work, we treated the chunking problem as a sequence labeling problem by using the BIO representation for the chunks. However, semiMarkov conditional random fields (semi-CRFs) can directly handle the chunking problem by considering all possible combinations of subsequences of arbitrary length (Sarawagi and Cohen, 2004). Semi-CRFs allow one to use a richer set of features than CRFs, so the use of semi-CRFs in our parsing framework should lead to improved accuracy. Moreover, semi-CRFs would allow us to incorporate some useful restrictions in producing chunking hypotheses. For example, we could naturally incorporate the restriction that every chunk has to contain at least one symbol that has just been created in the previous level3. It is hard for the normal CRF model to incorporate such restrictions. Introducing latent variables into the CRF model may be another promising approach. This is the main idea of Pe</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2004. Semimarkov conditional random fields for information extraction. In Proceedings ofNIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="10267" citStr="Sha and Pereira (2003)" startWordPosition="1711" endWordPosition="1714">itting the training data. The L1 or L2 norm is commonly used in statistical natural language processing (Gao et al., 2007). We used L1-regularization, which is defined as XK 1 R(λ) = C k=1 where C is the meta-parameter that controls the degree of regularization. We used the OWL-QN algorithm (Andrew and Gao, 2007) to obtain the parameters that maximize the L1-regularized conditional log-likelihood. 3.2 Features Table 1 shows the features used in chunking for the base level. Since the task is basically identical to shallow parsing by CRFs, we follow the feature sets used in the previous work by Sha and Pereira (2003). We use unigrams, bigrams, and trigrams of part-of-speech (POS) tags and words. The difference between our CRF chunker and that in (Sha and Pereira, 2003) is that we could not use second-order CRF models, hence we could not use trigram features on the BIO states. We |λk|, 792 Symbol Unigrams s−2, s−1, s0, s+1, s+2 Symbol Bigrams s−2s−1, s−1s0, s0s+1, s+1s+2 Symbol Trigrams s−3s−2s−1, s−2s−1s0, s−1s0s+1, s0s+1s+2, s+1s+2s+3 Word Unigrams h−2, h−1, h0, h+1, h+2 Word Bigrams h−2h−1, h−1h0, h0h+1, h+1h+2 Word Trigrams h−1h0h+1 Table 1: Feature templates used in the base level chunking. s represen</context>
<context position="19702" citStr="Sha and Pereira (2003)" startWordPosition="3318" endWordPosition="3321">arsing performance (section 22, all sentences). Figure 6: Searching for the best parse with a depth-first search algorithm. This pseudo-code illustrates how to find the highest probability parse, but in the real implementation, the function needs to keep track of chunking histories as well as probabilities. most common symbol and consist of 55% of all phrases. The accuracy of noun phrases recognition was relatively high, but it may be useful to design special features for this particular type of phrase, considering the dominance of noun phrases in the corpus. Although not directly comparable, Sha and Pereira (2003) report almost the same level of accuracy (94.38%) on noun phrase recognition, using a much smaller training set. We attribute their superior performance mainly to the use of second-order features on state transitions. Table 4 also suggests that adverb phrases (ADVP) and adjective phrases (ADJP) are more difficult to recognize than other types of phrases, which coincides with the result reported in (Collins, 1999). It should be noted that the performance reported in this table was evaluated using the gold standard sequences as the input to the CRF chunkers. In the real parsing process, the chu</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
</authors>
<title>Transforming a chunker to a parser.</title>
<date>2001</date>
<booktitle>Computational Linguistics in the Netherlands</booktitle>
<pages>177--188</pages>
<editor>In J. Veenstra W. Daelemans, K. Sima‘an and J. Zavrel, editors,</editor>
<publisher>Rodopi.</publisher>
<contexts>
<context position="7038" citStr="Sang (2001)" startWordPosition="1139" endWordPosition="1140"> each phrase with its non-terminal symbol and head1. In the second level, the chunker identifies a noun phrase, (NP a light million ounces), and converts it into NP. This process is repeated until the whole sentence is chunked at the fourth level. The full parse tree is recovered from the chunking history in a straightforward way. This idea of converting full parsing into a series of chunking tasks is not new by any means— the history of this kind of approach dates back to 1950s (Joshi and Hopely, 1996). More recently, Brants (1999) used a cascaded Markov model to parse German text. Tjong Kim Sang (2001) used the IOB tagging method to represent chunks and memory-based learning, and achieved an f-score of 80.49 on the WSJ corpus. Tsuruoka and Tsujii (2005) improved upon their approach by using 1The head word is identified by using the headpercolation table (Magerman, 1995). 791 5000 4000 # sentences 3000 2000 1000 0 0 5 10 15 20 25 30 Height 3.1 Linear Chain CRFs A linear chain CRF defines a single log-linear probabilistic distribution over all possible tag sequences y for the input sequence x: T K 1 XXZ(x) exp λkfk(t, yt, yt−1, x), t=1 k=1 where fk(t, yt, yt−1, x) is typically a binary functi</context>
<context position="22881" citStr="Sang (2001)" startWordPosition="3864" endWordPosition="3865">r parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al. (2008) and Petrov &amp; Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak &amp; Johnson (2005). Our parser was more accurate than traditional history-based approaches such as Sagae &amp; Lavie (2006) and Ratnaparkhi (1997), and was significantly better than previous cascaded chunking approaches such as Tsuruoka &amp; Tsujii (2005) and Tjong Kim Sang (2001). Although the comparison presented in the table is not perfectly fair because of the differences in hardware platforms, the results show that our parsing model is a promising addition to the parsing frameworks for building a fast and accurate parser. 7 Discussion One of the obvious ways to improve the accuracy of our parser is to improve the accuracy of individual CRF models. As mentioned earlier, we were not able to use second-order features on state transitions, which would have been very useful, due to the problem of computational cost. Incremental feature selection methods such as graftin</context>
<context position="26216" citStr="Sang (2001)" startWordPosition="4428" endWordPosition="4429">amework actually enables us to 3For example, the sequence VBD DT JJ in Figure 2 cannot be a chunk in the current level because it would have been already chunked in the previous level if it were. 796 Recall Precision F-score Time (min) This work (deterministic) 86.3 87.5 86.9 0.5 This work (search, beam width = 4) 88.2 88.7 88.4 1.7 Huang (2008) 91.7 Unk Finkel et al. (2008) 87.8 88.2 88.0 &gt;250* Petrov &amp; Klein (2008) 88.3 3* Sagae &amp; Lavie (2006) 87.8 88.1 87.9 17 Charniak &amp; Johnson (2005) 90.6 91.3 91.0 Unk Tsuruoka &amp; Tsujii (2005) 85.0 86.8 85.9 2 Collins (1999) 88.1 88.3 88.2 39** Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk Charniak (2000) 89.6 89.5 89.5 23** Ratnaparkhi (1997) 86.3 87.5 86.9 Unk Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the parsers. build a competitive parser if we use CRF models for each level of chunking and a depth-first search algorithm to search for the highest probability parse. Like other discriminative learning approaches, one of the advantages of our parser is its generality. The design of our parser is very generic, </context>
</contexts>
<marker>Sang, 2001</marker>
<rawString>Erik Tjong Kim Sang. 2001. Transforming a chunker to a parser. In J. Veenstra W. Daelemans, K. Sima‘an and J. Zavrel, editors, Computational Linguistics in the Netherlands 2000, pages 177–188. Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Chunk parsing revisited.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>133--140</pages>
<contexts>
<context position="7192" citStr="Tsuruoka and Tsujii (2005)" startWordPosition="1162" endWordPosition="1166">, and converts it into NP. This process is repeated until the whole sentence is chunked at the fourth level. The full parse tree is recovered from the chunking history in a straightforward way. This idea of converting full parsing into a series of chunking tasks is not new by any means— the history of this kind of approach dates back to 1950s (Joshi and Hopely, 1996). More recently, Brants (1999) used a cascaded Markov model to parse German text. Tjong Kim Sang (2001) used the IOB tagging method to represent chunks and memory-based learning, and achieved an f-score of 80.49 on the WSJ corpus. Tsuruoka and Tsujii (2005) improved upon their approach by using 1The head word is identified by using the headpercolation table (Magerman, 1995). 791 5000 4000 # sentences 3000 2000 1000 0 0 5 10 15 20 25 30 Height 3.1 Linear Chain CRFs A linear chain CRF defines a single log-linear probabilistic distribution over all possible tag sequences y for the input sequence x: T K 1 XXZ(x) exp λkfk(t, yt, yt−1, x), t=1 k=1 where fk(t, yt, yt−1, x) is typically a binary function indicating the presence of feature k, λk is the weight of the feature, and Z(X) is a normalization function: p(y|x) = Figure 5: Distribution of tree he</context>
<context position="22855" citStr="Tsuruoka &amp; Tsujii (2005)" startWordPosition="3857" endWordPosition="3860">arizes the results of previous work. Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al. (2008) and Petrov &amp; Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak &amp; Johnson (2005). Our parser was more accurate than traditional history-based approaches such as Sagae &amp; Lavie (2006) and Ratnaparkhi (1997), and was significantly better than previous cascaded chunking approaches such as Tsuruoka &amp; Tsujii (2005) and Tjong Kim Sang (2001). Although the comparison presented in the table is not perfectly fair because of the differences in hardware platforms, the results show that our parsing model is a promising addition to the parsing frameworks for building a fast and accurate parser. 7 Discussion One of the obvious ways to improve the accuracy of our parser is to improve the accuracy of individual CRF models. As mentioned earlier, we were not able to use second-order features on state transitions, which would have been very useful, due to the problem of computational cost. Incremental feature selecti</context>
<context position="26142" citStr="Tsuruoka &amp; Tsujii (2005)" startWordPosition="4412" endWordPosition="4415">parser based on this parsing framework. In this paper, we have demonstrated that the framework actually enables us to 3For example, the sequence VBD DT JJ in Figure 2 cannot be a chunk in the current level because it would have been already chunked in the previous level if it were. 796 Recall Precision F-score Time (min) This work (deterministic) 86.3 87.5 86.9 0.5 This work (search, beam width = 4) 88.2 88.7 88.4 1.7 Huang (2008) 91.7 Unk Finkel et al. (2008) 87.8 88.2 88.0 &gt;250* Petrov &amp; Klein (2008) 88.3 3* Sagae &amp; Lavie (2006) 87.8 88.1 87.9 17 Charniak &amp; Johnson (2005) 90.6 91.3 91.0 Unk Tsuruoka &amp; Tsujii (2005) 85.0 86.8 85.9 2 Collins (1999) 88.1 88.3 88.2 39** Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk Charniak (2000) 89.6 89.5 89.5 23** Ratnaparkhi (1997) 86.3 87.5 86.9 Unk Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the parsers. build a competitive parser if we use CRF models for each level of chunking and a depth-first search algorithm to search for the highest probability parse. Like other discriminative learning approaches, one of the advantages o</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Chunk parsing revisited. In Proceedings of IWPT, pages 133–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Kernel-based pronoun resolution with structured syntactic features.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="1193" citStr="Yang et al., 2006" startWordPosition="172" endWordPosition="175">king. The probability of an entire parse tree is computed as the product of the probabilities of individual chunking results. The parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depthfirst search algorithm. Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser. 1 Introduction Full parsing analyzes the phrase structure of a sentence and provides useful input for many kinds of high-level natural language processing such as summarization (Knight and Marcu, 2000), pronoun resolution (Yang et al., 2006), and information extraction (Miyao et al., 2008). One of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discrimina</context>
</contexts>
<marker>Yang, Su, Tan, 2006</marker>
<rawString>Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006. Kernel-based pronoun resolution with structured syntactic features. In Proceedings of COLING/ACL, pages 41–48.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>