<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.998392">
Painless Unsupervised Learning with Features
</title>
<author confidence="0.996404">
Taylor Berg-Kirkpatrick Alexandre Bouchard-Cˆot´e John DeNero Dan Klein
</author>
<affiliation confidence="0.997419">
Computer Science Division
University of California at Berkeley
</affiliation>
<email confidence="0.967676">
{tberg, bouchard, denero, klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.997079" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975157894737">
We show how features can easily be added
to standard generative models for unsuper-
vised learning, without requiring complex
new training methods. In particular, each
component multinomial of a generative model
can be turned into a miniature logistic regres-
sion model if feature locality permits. The in-
tuitive EM algorithm still applies, but with a
gradient-based M-step familiar from discrim-
inative training of logistic regression mod-
els. We apply this technique to part-of-speech
induction, grammar induction, word align-
ment, and word segmentation, incorporating
a few linguistically-motivated features into
the standard generative model for each task.
These feature-enhanced models each outper-
form their basic counterparts by a substantial
margin, and even compete with and surpass
more complex state-of-the-art models.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999875421052632">
Unsupervised learning methods have been increas-
ingly successful in recent NLP research. The rea-
sons are varied: increased supplies of unlabeled
data, improved understanding of modeling methods,
additional choices of optimization algorithms, and,
perhaps most importantly for the present work, in-
corporation of richer domain knowledge into struc-
tured models. Unfortunately, that knowledge has
generally been encoded in the form of conditional
independence structure, which means that injecting
it is both tricky (because the connection between
independence and knowledge is subtle) and time-
consuming (because new structure often necessitates
new inference algorithms).
In this paper, we present a range of experiments
wherein we improve existing unsupervised models
by declaratively adding richer features. In particu-
lar, we parameterize the local multinomials of exist-
ing generative models using features, in a way which
does not require complex new machinery but which
still provides substantial flexibility. In the feature-
engineering paradigm, one can worry less about the
backbone structure and instead use hand-designed
features to declaratively inject domain knowledge
into a model. While feature engineering has his-
torically been associated with discriminative, super-
vised learning settings, we argue that it can and
should be applied more broadly to the unsupervised
setting.
The idea of using features in unsupervised learn-
ing is neither new nor even controversial. Many
top unsupervised results use feature-based mod-
els (Smith and Eisner, 2005; Haghighi and Klein,
2006). However, such approaches have presented
their own barriers, from challenging normalization
problems, to neighborhood design, to the need for
complex optimization procedures. As a result, most
work still focuses on the stable and intuitive ap-
proach of using the EM algorithm to optimize data
likelihood in locally normalized, generative models.
The primary contribution of this paper is to
demonstrate the clear empirical success of a sim-
ple and accessible approach to unsupervised learn-
ing with features, which can be optimized by us-
ing standard NLP building blocks. We consider
the same generative, locally-normalized models that
dominate past work on a range of tasks. However,
we follow Chen (2003), Bisani and Ney (2008), and
Bouchard-Cˆot´e et al. (2008), and allow each com-
ponent multinomial of the model to be a miniature
multi-class logistic regression model. In this case,
the EM algorithm still applies with the E-step un-
changed. The M-step involves gradient-based train-
ing familiar from standard supervised logistic re-
gression (i.e., maximum entropy models). By inte-
grating these two familiar learning techniques, we
add features to unsupervised models without any
</bodyText>
<page confidence="0.930446">
582
</page>
<note confidence="0.869273">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 582–590,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999126294117647">
specialized learning or inference.
A second contribution of this work is to show that
further gains can be achieved by directly optimiz-
ing data likelihood with LBFGS (Liu et al., 1989).
This alternative optimization procedure requires no
additional machinery beyond what EM uses. This
approach is still very simple to implement, and we
found that it empirically outperforms EM.
This paper is largely empirical; the underlying op-
timization techniques are known, even if the overall
approach will be novel to many readers. As an em-
pirical demonstration, our results span an array of
unsupervised learning tasks: part-of-speech induc-
tion, grammar induction, word alignment, and word
segmentation. In each task, we show that declaring a
few linguistically motivated feature templates yields
state-of-the-art results.
</bodyText>
<sectionHeader confidence="0.997697" genericHeader="introduction">
2 Models
</sectionHeader>
<bodyText confidence="0.999894166666667">
We start by explaining our feature-enhanced model
for part-of-speech (POS) induction. This particular
example illustrates our approach to adding features
to unsupervised models in a well-known NLP task.
We then explain how the technique applies more
generally.
</bodyText>
<subsectionHeader confidence="0.994559">
2.1 Example: Part-of-Speech Induction
</subsectionHeader>
<bodyText confidence="0.9999708125">
POS induction consists of labeling words in text
with POS tags. A hidden Markov model (HMM) is a
standard model for this task, used in both a frequen-
tist setting (Merialdo, 1994; Elworthy, 1994) and in
a Bayesian setting (Goldwater and Griffiths, 2007;
Johnson, 2007).
A POS HMM generates a sequence of words in
order. In each generation step, an observed word
emission yi and a hidden successor POS tag zi+1 are
generated independently, conditioned on the current
POS tag zi . This process continues until an absorb-
ing stop state is generated by the transition model.
There are two types of conditional distributions in
the model—emission and transition probabilities—
that are both multinomial probability distributions.
The joint likelihood factors into these distributions:
</bodyText>
<equation confidence="0.999328">
Pe(Y=Y,Z=Z)= Pe(Z1 = z1) ·
|z|
Pe(Yi = yi|Zi = zi) · Pe(Zi+1 = zi+1|Zi = zi)
</equation>
<bodyText confidence="0.98075532">
The emission distribution Pe(Yi = yi|Zi = zi) is
parameterized by conditional probabilities θy,z,EMIT
for each word y given tag z. Alternatively, we can
express this emission distribution as the output of a
logistic regression model, replacing the explicit con-
ditional probability table by a logistic function pa-
rameterized by weights and features:
exp hw, f(y, z, EMIT)i
Ey′ exp hw, f(y′, z, EMIT)i
This feature-based logistic expression is equivalent
to the flat multinomial in the case that the feature
function f(y, z, EMIT) consists of all indicator fea-
tures on tuples (y, z, EMIT), which we call BASIC
features. The equivalence follows by setting weight
wy,z,EMIT = log(θy,z,EMIT).1 This formulation is
known as the natural parameterization of the multi-
nomial distribution.
In order to enhance this emission distribution, we
include coarse features in f(y, z, EMIT), in addi-
tion to the BASIC features. Crucially, these features
can be active across multiple (y, z) values. In this
way, the model can abstract general patterns, such
as a POS tag co-occurring with an inflectional mor-
pheme. We discuss specific POS features in Sec-
tion 4.
</bodyText>
<subsectionHeader confidence="0.998102">
2.2 General Directed Models
</subsectionHeader>
<bodyText confidence="0.99911775">
Like the HMM, all of the models we propose are
based on locally normalized generative decisions
that condition on some context. In general, let X =
(Z, Y) denote the sequence of generation steps (ran-
dom variables) where Z contains all hidden random
variables and Y contains all observed random vari-
ables. The joint probability of this directed model
factors as:
</bodyText>
<equation confidence="0.9968105">
Pw(X = X) = fl Pw (Xi = xi��X&amp;quot;�i� = x&amp;quot;�i� ) ,
i∈I
</equation>
<bodyText confidence="0.995226">
where X,r(i) denotes the parents of Xi and I is the
index set of the variables in X.
In the models that we use, each factor in the above
expression is the output of a local logistic regression
</bodyText>
<footnote confidence="0.9471975">
1As long as no transition or emission probabilities are equal
to zero. When zeros are present, for instance to model that an
absorbing stop state can only transition to itself, it is often possi-
ble to absorb these zeros into a base measure. All the arguments
in this paper carry with a structured base measure; we drop it for
simplicity.
</footnote>
<equation confidence="0.838665">
fl
i=1
θy,z,EMIT(w) =
</equation>
<page confidence="0.955682">
583
</page>
<bodyText confidence="0.91729">
model parameterized by w:
</bodyText>
<equation confidence="0.98837825">
��X�(i) = c� = exp(w, f(d, c, t))
Pw �Xi = d
E
d′ exp(w, f(d′, c, t))
</equation>
<bodyText confidence="0.999010583333333">
Above, d is the generative decision value for Xi
picked by the model, c is the conditioning context
tuple of values for the parents of Xi, and t is the
type of decision being made. For instance, the POS
HMM has two types of decisions: transitions and
emissions. In the emission model, the type t is EMIT,
the decision d is a word and the context c is a tag.
The denominator normalizes the factor to be a prob-
ability distribution over decisions.
The objective function we derive from this model
is the marginal likelihood of the observations y,
along with a regularization term:
</bodyText>
<equation confidence="0.9767505">
L(w) = log Pw(Y = y) — n||w||2 (1)
2
</equation>
<bodyText confidence="0.999933833333333">
This model has two advantages over the more preva-
lent form of a feature-rich unsupervised model, the
globally normalized Markov random field.2 First,
as we explain in Section 3, optimizing our objec-
tive does not require computing expectations over
the joint distribution. In the case of the POS HMM,
for example, we do not need to enumerate an in-
finite sum of products of potentials when optimiz-
ing, in contrast to Haghighi and Klein (2006). Sec-
ond, we found that locally normalized models em-
pirically outperform their globally normalized coun-
terparts, despite their efficiency and simplicity.
</bodyText>
<sectionHeader confidence="0.999616" genericHeader="method">
3 Optimization
</sectionHeader>
<subsectionHeader confidence="0.999991">
3.1 Optimizing with Expectation Maximization
</subsectionHeader>
<bodyText confidence="0.999946714285714">
In this section, we describe the EM algorithm ap-
plied to our feature-rich, locally normalized models.
For models parameterized by standard multinomi-
als, EM optimizes L(0) = log Pe(Y = y) (Demp-
ster et al., 1977). The E-step computes expected
counts for each tuple of decision d, context c, and
multinomial type t:
</bodyText>
<equation confidence="0.970528666666667">
L� Y = yJ
ed,c,t �E� ✶(Xi =d, X�(i) =c, t) (2)
i∈I
</equation>
<bodyText confidence="0.8279795">
2The locally normalized model class is actually equivalent
to its globally normalized counterpart when the former meets
the following three conditions: (1) The graphical model is a
directed tree. (2) The BASIC features are included in f. (3) We
do not include regularization in the model (κ = 0). This follows
from Smith and Johnson (2007).
These expected counts are then normalized in the
M-step to re-estimate 0:
</bodyText>
<equation confidence="0.8518255">
Bd,c,t +— Ed′ ed′,c,t
ed,c,t
</equation>
<bodyText confidence="0.999806857142857">
Normalizing expected counts in this way maximizes
the expected complete log likelihood with respect to
the current model parameters.
EM can likewise optimize L(w) for our locally
normalized models with logistic parameterizations.
The E-step first precomputes multinomial parame-
ters from w for each decision, context, and type:
</bodyText>
<equation confidence="0.909231">
exp(w, f(d, c, t))
Bd,c,t(w) � Ed′ exp(w, f(d′, c, t))
</equation>
<bodyText confidence="0.9976788">
Then, expected counts e are computed accord-
ing to Equation 2. In the case of POS induction,
expected counts are computed with the forward-
backward algorithm in both the standard and logistic
parameterizations. The only change is that the con-
ditional probabilities 0 are now functions of w.
The M-step changes more substantially, but still
relies on canonical NLP learning methods. We wish
to choose w to optimize the regularized expected
complete log likelihood:
</bodyText>
<equation confidence="0.905424">
f(w, e) = � ed,c,t log Bd,c,t(w) — n||w||2 2 (3)
d,c,t
</equation>
<bodyText confidence="0.999897666666667">
We optimize this objective via a gradient-based
search algorithm like LBFGS. The gradient with re-
spect to w takes the form
</bodyText>
<equation confidence="0.99383925">
Vf(w,e) = � ed,c,t · Δd,c,t(w) — 2n · w (4)
d,c,t
�Δd,c,t(w) = f(d, c, t) � Bd′,c,t(w)f(d′, c, t)
d′
</equation>
<bodyText confidence="0.999819181818182">
This gradient matches that of regularized logis-
tic regression in a supervised model: the differ-
ence Δ between the observed and expected features,
summed over every decision and context. In the su-
pervised case, we would observe the count of occur-
rences of (d, c, t), but in the unsupervised M-step,
we instead substitute expected counts ed,c,t.
This gradient-based M-step is an iterative proce-
dure. For each different value of w considered dur-
ing the search, we must recompute 0(w), which re-
quires computation in proportion to the size of the
</bodyText>
<page confidence="0.990528">
584
</page>
<bodyText confidence="0.999957">
parameter space. However, e stays fixed throughout
the M-step. Algorithm 1 outlines EM in its entirety.
The subroutine climb(·, ·, ·) represents a generic op-
timization step such as an LBFGS iteration.
</bodyText>
<figure confidence="0.96124">
Algorithm 1 Feature-enhanced EM
repeat
Compute expected counts e D Eq. 2
repeat
Compute ℓ(w, e) D Eq. 3
Compute Vℓ(w, e) D Eq. 4
w +— climb(w, ℓ(w, e), Vℓ(w, e))
until convergence
until convergence
</figure>
<subsectionHeader confidence="0.99747">
3.2 Direct Marginal Likelihood Optimization
</subsectionHeader>
<bodyText confidence="0.860900052631579">
Another approach to optimizing Equation 1 is to
compute the gradient of the log marginal likelihood
directly (Salakhutdinov et al., 2003). The gradient
turns out to have the same form as Equation 4, with
the key difference that ed,,,t is recomputed for every
different value of w. Algorithm 2 outlines the proce-
dure. Justification for this algorithm appears in the
Appendix.
Algorithm 2 Feature-enhanced direct gradient
repeat
Compute expected counts e D Eq. 2
Compute L(w) D Eq. 1
Compute Vℓ(w, e) D Eq. 4
w +— climb(w, L(w), Vℓ(w, e))
until convergence
In practice, we find that this optimization ap-
proach leads to higher task accuracy for several
models. However, in cases where computing ed,,,t
is expensive, EM can be a more efficient alternative.
</bodyText>
<sectionHeader confidence="0.997556" genericHeader="method">
4 Part-of-Speech Induction
</sectionHeader>
<bodyText confidence="0.999947714285714">
We now describe experiments that demonstrate the
effectiveness of locally normalized logistic models.
We first use the bigram HMM described in Sec-
tion 2.1 for POS induction, which has two types of
multinomials. For type EMIT, the decisions d are
words and contexts c are tags. For type TRANS, the
decisions and contexts are both tags.
</bodyText>
<subsectionHeader confidence="0.956232">
4.1 POS Induction Features
</subsectionHeader>
<bodyText confidence="0.999638375">
We use the same set of features used by Haghighi
and Klein (2006) in their baseline globally normal-
ized Markov random field (MRF) model. These are
all coarse features on emission contexts that activate
for words with certain orthographic properties. We
use only the BASIC features for transitions. For
an emission with word y and tag z, we use the
following feature templates:
</bodyText>
<table confidence="0.597437333333333">
BASIC: I(y = ·, z = ·)
CONTAINS-DIGIT: Check if y contains digit and conjoin
with z:
I(containsDigit(y) = ·, z = ·)
CONTAINS-HYPHEN: I(containsHyphen(x) = ·, z = ·)
INITIAL-CAP: Check if the first letter of y is
capitalized: I(isCap(y) = ·, z = ·)
N-GRAM: Indicator functions for character n-
grams of up to length 3 present in y.
</table>
<subsectionHeader confidence="0.911356">
4.2 POS Induction Data and Evaluation
</subsectionHeader>
<bodyText confidence="0.999995866666667">
We train and test on the entire WSJ tag corpus (Mar-
cus et al., 1993). We attempt the most difficult ver-
sion of this task where the only information our sys-
tem can make use of is the unlabeled text itself. In
particular, we do not make use of a tagging dictio-
nary. We use 45 tag clusters, the number of POS tags
that appear in the WSJ corpus. There is an identifi-
ability issue when evaluating inferred tags. In or-
der to measure accuracy on the hand-labeled corpus,
we map each cluster to the tag that gives the highest
accuracy, the many-1 evaluation approach (Johnson,
2007). We run all POS induction models for 1000
iterations, with 10 random initializations. The mean
and standard deviation of many-1 accuracy appears
in Table 1.
</bodyText>
<subsectionHeader confidence="0.990638">
4.3 POS Induction Results
</subsectionHeader>
<bodyText confidence="0.999739090909091">
We compare our model to the basic HMM and a bi-
gram version of the feature-enhanced MRF model of
Haghighi and Klein (2006). Using EM, we achieve
a many-1 accuracy of 68.1. This outperforms the
basic HMM baseline by a 5.0 margin. The same
model, trained using the direct gradient approach,
achieves a many-1 accuracy of 75.5, outperforming
the basic HMM baseline by a margin of 12.4. These
results show that the direct gradient approach can of-
fer additional boosts in performance when used with
a feature-enhanced model. We also outperform the
</bodyText>
<page confidence="0.993417">
585
</page>
<bodyText confidence="0.999326166666667">
globally normalized MRF, which uses the same set
of features and which we train using a direct gradi-
ent approach.
To the best of our knowledge, our system achieves
the best performance to date on the WSJ corpus for
totally unsupervised POS tagging.3
</bodyText>
<sectionHeader confidence="0.99771" genericHeader="method">
5 Grammar Induction
</sectionHeader>
<bodyText confidence="0.9998934">
We next apply our technique to a grammar induction
task: the unsupervised learning of dependency parse
trees via the dependency model with valence (DMV)
(Klein and Manning, 2004). A dependency parse is
a directed tree over tokens in a sentence. Each edge
of the tree specifies a directed dependency from a
head token to a dependent, or argument token. Thus,
the number of dependencies in a parse is exactly the
number of tokens in the sentence, not counting the
artificial root token.
</bodyText>
<subsectionHeader confidence="0.588121">
5.1 Dependency Model with Valence
</subsectionHeader>
<bodyText confidence="0.92820212">
The DMV defines a probability distribution over de-
pendency parse trees. In this head-outward attach-
ment model, a parse and the word tokens are derived
together through a recursive generative process. For
each token generated so far, starting with the root, a
set of left dependents is generated, followed by a set
of right dependents.
There are two types of multinomial distributions
in this model. The Bernoulli STOP probabilities
Bd,c,STOP capture the valence of a particular head. For
this type, the decision d is whether or not to stop
generating arguments, and the context c contains the
current head h, direction 6 and adjacency adj. If
a head’s stop probability is high, it will be encour-
aged to accept few arguments. The ATTACH multi-
nomial probability distributions Bd,c,ATTACH capture
attachment preferences of heads. For this type, a de-
cision d is an argument token a, and the context c
consists of a head h and a direction 6.
We take the same approach as previous work
(Klein and Manning, 2004; Cohen and Smith, 2009)
and use gold POS tags in place of words.
3Haghighi and Klein (2006) achieve higher accuracies by
making use of labeled prototypes. We do not use any external
information.
</bodyText>
<subsectionHeader confidence="0.954502">
5.2 Grammar Induction Features
</subsectionHeader>
<bodyText confidence="0.8413819">
One way to inject knowledge into a dependency
model is to encode the similarity between the vari-
ous morphological variants of nouns and verbs. We
encode this similarity by incorporating features into
both the STOP and the ATTACH probabilities. The
attachment features appear below; the stop feature
templates are similar and are therefore omitted.
BASIC: I(a = ·, h = ·, S = ·)
NOUN: Generalize the morphological variants of
nouns by using isNoun(·):
</bodyText>
<equation confidence="0.997984">
I(a = ·, isNoun(h) = ·, S = ·)
I(isNoun(a) = ·, h = ·, S = ·)
I(isNoun(a) = ·, isNoun(h) = ·, S = ·)
</equation>
<bodyText confidence="0.986156333333333">
VERB: Same as above, generalizing verbs instead
of nouns by using isVerb(·)
NOUN-VERB: Same as above, generalizing with
</bodyText>
<equation confidence="0.884828">
isVerbOrNoun(·) = isVerb(·)∨isNoun(·)
</equation>
<bodyText confidence="0.98042025">
BACK-OFF: We add versions of all other features that
ignore direction or adjacency.
While the model has the expressive power to al-
low specific morphological variants to have their
own behaviors, the existence of coarse features en-
courages uniform analyses, which in turn gives bet-
ter accuracies.
Cohen and Smith’s (2009) method has similar
characteristics. They add a shared logistic-normal
prior (SLN) to the DMV in order to tie multinomial
parameters across related derivation events. They
achieve their best results by only tying parame-
ters between different multinomials when the cor-
responding contexts are headed by nouns and verbs.
This observation motivates the features we choose to
incorporate into the DMV.
</bodyText>
<subsectionHeader confidence="0.994158">
5.3 Grammar Induction Data and Evaluation
</subsectionHeader>
<bodyText confidence="0.999943923076923">
For our English experiments we train and report di-
rected attachment accuracy on portions of the WSJ
corpus. We work with a standard, reduced version of
WSJ, WSJ10, that contains only sentences of length
10 or less after punctuation has been removed. We
train on sections 2-21, and use section 22 as a de-
velopment set. We report accuracy on section 23.
These are the same training, development, and test
sets used by Cohen and Smith (2009). The regular-
ization parameter (n) is tuned on the development
set to maximize accuracy.
For our Chinese experiments, we use the same
corpus and training/test split as Cohen and Smith
</bodyText>
<page confidence="0.996656">
586
</page>
<bodyText confidence="0.999854111111111">
(2009). We train on sections 1-270 of the Penn Chi-
nese Treebank (Xue et al., 2002), similarly reduced
(CTB10). We test on sections 271-300 of CTB10,
and use sections 400-454 as a development set.
The DMV is known to be sensitive to initializa-
tion. We use the deterministic harmonic initializer
from Klein and Manning (2004). We ran each op-
timization procedure for 100 iterations. The results
are reported in Table 1.
</bodyText>
<subsectionHeader confidence="0.966829">
5.4 Grammar Induction Results
</subsectionHeader>
<bodyText confidence="0.999992083333333">
We are able to outperform Cohen and Smith’s (2009)
best system, which requires a more complicated
variational inference method, on both English and
Chinese data sets. Their system achieves an accu-
racy of 61.3 for English and an accuracy of 51.9 for
Chinese.4 Our feature-enhanced model, trained us-
ing the direct gradient approach, achieves an accu-
racy of 63.0 for English, and an accuracy of 53.6 for
Chinese. To our knowledge, our method for feature-
based dependency parse induction outperforms all
existing methods that make the same set of condi-
tional independence assumptions as the DMV.
</bodyText>
<sectionHeader confidence="0.988357" genericHeader="method">
6 Word Alignment
</sectionHeader>
<bodyText confidence="0.999934714285714">
Word alignment is a core machine learning com-
ponent of statistical machine translation systems,
and one of the few NLP tasks that is dominantly
solved using unsupervised techniques. The pur-
pose of word alignment models is to induce a cor-
respondence between the words of a sentence and
the words of its translation.
</bodyText>
<subsectionHeader confidence="0.998196">
6.1 Word Alignment Models
</subsectionHeader>
<bodyText confidence="0.999734375">
We consider two classic generative alignment mod-
els that are both used heavily today, IBM Model 1
(Brown et al., 1994) and the HMM alignment model
(Ney and Vogel, 1996). These models generate a
hidden alignment vector z and an observed foreign
sentence y, all conditioned on an observed English
sentence e. The likelihood of both models takes the
form:
</bodyText>
<equation confidence="0.9435585">
P (y, z|e) = 11 p(zj = i|zj−1) - θyj,ei,ALIGN
j
</equation>
<footnote confidence="0.873609333333333">
4Using additional bilingual data, Cohen and Smith (2009)
achieve an accuracy of 62.0 for English, and an accuracy of
52.0 for Chinese, still below our results.
</footnote>
<table confidence="0.999946923076923">
Model Inference Reg Eval
POS Induction Many-1
Basic-HMM EM – 63.1 (1.3)
Feature-MRF LBFGS 0.1 59.6 (6.9)
Feature-HMM EM 1.0 68.1 (1.7)
LBFGS 1.0 75.5 (1.1)
Grammar Induction Dir
0 Basic-DMV EM – 47.8
Feature-DMV EM 0.05 48.3
LBFGS 10.0 63.0
(Cohen and Smith, 2009) 61.3
0 Basic-DMV EM – 42.5
B1 Feature-DMV EM 1.0 49.9
WSJ 5.0 53.6
CT LBFGS
(Cohen and Smith, 2009) 51.9
Word Alignment AER
Basic-Model 1 EM – 38.0
C Feature-Model 1 EM – 35.6
Basic-HMM EM – 33.8
NI Feature-HMM EM – 30.0
Word Segmentation F1
Basic-Unigram EM – 76.9 (0.1)
R Feature-Unigram EM 0.2 84.5 (0.5)
B LBFGS 0.2 88.0 (0.1)
(Johnson and Goldwater, 2009) 87
</table>
<tableCaption confidence="0.72201225">
Table 1: Locally normalized feature-based models outperform
all proposed baselines for all four tasks. LBFGS outperformed
EM in all cases where the algorithm was sufficiently fast to run.
Details of each experiment appear in the main text.
</tableCaption>
<bodyText confidence="0.9997854">
The distortion term p(zj = i|zj−1) is uniform in
Model 1, and Markovian in the HMM. See Liang et
al. (2006) for details on the specific variant of the
distortion model of the HMM that we used. We use
these standard distortion models in both the baseline
and feature-enhanced word alignment systems.
The bilexical emission model θy,e,ALIGN differen-
tiates our feature-enhanced system from the base-
line system. In the former, the emission model is a
standard conditional multinomial that represents the
probability that decision word y is generated from
context word e, while in our system, the emission
model is re-parameterized as a logistic regression
model and feature-enhanced.
Many supervised feature-based alignment models
have been developed. In fact, this logistic parame-
terization of the HMM has been proposed before and
yielded alignment improvements, but was trained
using supervised estimation techniques (Varea et al.,
2002).5 However, most full translation systems to-
</bodyText>
<footnote confidence="0.996555333333333">
5Varea et al. (2002) describes unsupervised EM optimiza-
tion with logistic regression models at a high level—their dy-
namic training approach—but provides no experiments.
</footnote>
<page confidence="0.995892">
587
</page>
<bodyText confidence="0.99987625">
day rely on unsupervised learning so that the models
may be applied easily to many language pairs. Our
approach provides efficient and consistent unsuper-
vised estimation for feature-rich alignment models.
</bodyText>
<subsectionHeader confidence="0.999387">
6.2 Word Alignment Features
</subsectionHeader>
<bodyText confidence="0.9993384">
The BASIC features on pairs of lexical items
provide strong baseline performance. We add
coarse features to the model in order to inject
prior knowledge and tie together lexical items with
similar characteristics.
</bodyText>
<construct confidence="0.716364666666667">
BASIC: I(e = ·, y = ·)
EDIT-DISTANCE: I(dist(y, e) = ·)
DICTIONARY: I((y, e) ∈ D) for dictionary D.
STEM: I(stem(e) = ·, y = ·) for Porterstemmer.
PREFIX: I(prefix(e) = ·, y = ·) for prefixes of
length 4.
</construct>
<bodyText confidence="0.864156">
CHARACTER: I(e = ·, charAt(y, i) = ·) for index i in
the Chinese word.
These features correspond to several common
augmentations of word alignment models, such as
adding dictionary priors and truncating long words,
but here we integrate them all coherently into a sin-
gle model.
</bodyText>
<subsectionHeader confidence="0.999807">
6.3 Word Alignment Data and Evaluation
</subsectionHeader>
<bodyText confidence="0.999972730769231">
We evaluate on the standard hand-aligned portion
of the NIST 2002 Chinese-English development set
(Ayan et al., 2005). The set is annotated with sure S
and possible P alignments. We measure alignment
quality using alignment error rate (AER) (Och and
Ney, 2000).
We train the models on 10,000 sentences of FBIS
Chinese-English newswire. This is not a large-scale
experiment, but large enough to be relevant for low-
resource languages. LBFGS experiments are not
provided because computing expectations in these
models is too computationally intensive to run for
many iterations. Hence, EM training is a more ap-
propriate optimization approach: computing the M-
step gradient requires only summing over word type
pairs, while the marginal likelihood gradient needed
for LBFGS requires summing over training sentence
alignments. The final alignments, in both the base-
line and the feature-enhanced models, are computed
by training the generative models in both directions,
combining the result with hard union competitive
thresholding (DeNero and Klein, 2007), and us-
ing agreement training for the HMM (Liang et al.,
2006). The combination of these techniques yields
a state-of-the-art unsupervised baseline for Chinese-
English.
</bodyText>
<subsectionHeader confidence="0.997979">
6.4 Word Alignment Results
</subsectionHeader>
<bodyText confidence="0.999941285714286">
For both IBM Model 1 and the HMM alignment
model, EM training with feature-enhanced models
outperforms the standard multinomial models, by
2.4 and 3.8 AER respectively.6 As expected, large
positive weights are assigned to both the dictionary
and edit distance features. Stem and character fea-
tures also contribute to the performance gain.
</bodyText>
<sectionHeader confidence="0.942262" genericHeader="method">
7 Word Segmentation
</sectionHeader>
<bodyText confidence="0.999909846153846">
Finally, we show that it is possible to improve upon
the simple and effective word segmentation model
presented in Liang and Klein (2009) by adding
phonological features. Unsupervised word segmen-
tation is the task of identifying word boundaries in
sentences where spaces have been removed. For a
sequence of characters y = (y1, ..., yn), a segmen-
tation is a sequence of segments z = (z1, ..., z|z|)
such that z is a partition of y and each zi is a con-
tiguous subsequence of y. Unsupervised models for
this task infer word boundaries from corpora of sen-
tences of characters without ever seeing examples of
well-formed words.
</bodyText>
<subsectionHeader confidence="0.998295">
7.1 Unigram Double-Exponential Model
</subsectionHeader>
<bodyText confidence="0.998081266666667">
Liang and Klein’s (2009) unigram double-
exponential model corresponds to a simple
derivational process where sentences of characters
x are generated a word at a time, drawn from a
multinomial over all possible strings θz,SEGMENT.
For this type, there is no context and the decision is
the particular string generated. In order to avoid the
degenerate MLE that assigns mass only to single
segment sentences it is helpful to independently
generate a length for each segment from a fixed
distribution. Liang and Klein (2009) constrain in-
dividual segments to have maximum length 10 and
generate lengths from the following distribution:
θl,LENGTH = exp(−l1.6) when 1 &lt; l &lt; 10. Their
model is deficient since it is possible to generate
</bodyText>
<footnote confidence="0.991799">
6The best published results for this dataset are supervised,
and trained on 17 times more data (Haghighi et al., 2009).
</footnote>
<page confidence="0.994861">
588
</page>
<bodyText confidence="0.999144">
lengths that are inconsistent with the actual lengths
of the generated segments. The likelihood equation
is given by:
</bodyText>
<equation confidence="0.892208">
P(Y = y,Z = z) =
[(1 − ESTOP) Ez{,SEGMENT exp(−|&apos;7&apos;2|1.s)]
</equation>
<subsectionHeader confidence="0.999808">
7.2 Segmentation Data and Evaluation
</subsectionHeader>
<bodyText confidence="0.9999719">
We train and test on the phonetic version of the
Bernstein-Ratner corpus (1987). This is the same
set-up used by Liang and Klein (2009), Goldwater
et al. (2006), and Johnson and Goldwater (2009).
This corpus consists of 9790 child-directed utter-
ances transcribed using a phonetic representation.
We measure segment F1 score on the entire corpus.
We run all word segmentation models for 300 iter-
ations with 10 random initializations and report the
mean and standard deviation of F1 in Table 1.
</bodyText>
<subsectionHeader confidence="0.998148">
7.3 Segmentation Features
</subsectionHeader>
<bodyText confidence="0.9989535">
The SEGMENT multinomial is the important distri-
bution in this model. We use the following features:
</bodyText>
<construct confidence="0.5602076">
BASIC: 1(z = ·)
LENGTH: 1(length(z) = ·)
NUMBER-VOWELS: 1(numVowels(z) = ·)
PHONO-CLASS-PREF: 1(prefix(coarsePhonemes(z)) = ·)
PHONO-CLASS-PREF: 1(suffix(coarsePhonemes(z)) = ·)
</construct>
<bodyText confidence="0.999962625">
The phonological class prefix and suffix features
project each phoneme of a string to a coarser class
and then take prefix and suffix indicators on the
string of projected characters. We include two ver-
sions of these features that use projections with dif-
ferent levels of coarseness. The goal of these fea-
tures is to help the model learn general phonetic
shapes that correspond to well-formed word bound-
aries.
As is the case in general for our method, the
feature-enhanced unigram model still respects the
conditional independence assumptions that the stan-
dard unigram model makes, and inference is still
performed using a simple dynamic program to com-
pute expected sufficient statistics, which are just seg-
ment counts.
</bodyText>
<subsectionHeader confidence="0.998386">
7.4 Segmentation Results
</subsectionHeader>
<bodyText confidence="0.999981636363636">
To our knowledge our system achieves the best per-
formance to date on the Bernstein-Ratner corpus,
with an F1 of 88.0. It is substantially simpler than
the non-parametric Bayesian models proposed by
Johnson et al. (2007), which require sampling pro-
cedures to perform inference and achieve an F1 of
87 (Johnson and Goldwater, 2009). Similar to our
other results, the direct gradient approach outper-
forms EM for feature-enhanced models, and both
approaches outperform the baseline, which achieves
an F1 of 76.9.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999962545454546">
We have shown that simple, locally normalized
models can effectively incorporate features into un-
supervised models. These enriched models can
be easily optimized using standard NLP build-
ing blocks. Beyond the four tasks explored in
this paper—POS tagging, DMV grammar induc-
tion, word alignment, and word segmentation—the
method can be applied to many other tasks, for ex-
ample grounded semantics, unsupervised PCFG in-
duction, document clustering, and anaphora resolu-
tion.
</bodyText>
<sectionHeader confidence="0.997364" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999412333333333">
We thank Percy Liang for making his word segmen-
tation code available to us, and the anonymous re-
viewers for their comments.
</bodyText>
<sectionHeader confidence="0.93787" genericHeader="references">
Appendix: Optimization
</sectionHeader>
<bodyText confidence="0.917229625">
In this section, we derive the gradient of the log marginal likeli-
hood needed for the direct gradient approach. Let w0 be the cur-
rent weights in Algorithm 2 and e = e(w0) be the expectations
under these weights as computed in Equation 2. In order to jus-
tify Algorithm 2, we need to prove that ∇L(w0) = ∇ℓ(w0, e).
We use the following simple lemma: if φ, ψ are real-valued
functions such that: (1) φ(w0) = ψ(w0) for some w0; (2)
φ(w) ≤ ψ(w) on an open set containing w0; and (3), φ and ψ
are differentiable at w0; then ∇ψ(w0) = ∇φ(w0).
We set ψ(w) = L(w) and φ(w) = ℓ(w, e)−E. Pw0(Z =
z|Y = y) log Pw0(Z = z|Y = y). If we can show that ψ, φ
satisfy the conditions of the lemma we are done since the second
term of φ depends on w0, but not on w.
Property (3) can be easily checked, and property (2) follows
from Jensen’s inequality. Finally, property (1) follows from
Lemma 2 of Neal and Hinton (1998).
</bodyText>
<figure confidence="0.993014">
71.|
H
8=1
ESTOP
</figure>
<page confidence="0.994704">
589
</page>
<sectionHeader confidence="0.993353" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999888970588235">
N. F. Ayan, B. Dorr, and C. Monz. 2005. Combining
word alignments using neural networks. In Empirical
Methods in Natural Language Processing.
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. K. Nelson and A. van Kleeck.
M. Bisani and H. Ney. 2008. Joint-sequence models for
grapheme-to-phoneme conversion.
A. Bouchard-Cˆot´e, P. Liang, D. Klein, and T. L. Griffiths.
2008. A probabilistic approach to language change.
In Neural Information Processing Systems.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1994. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics.
S. F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Eurospeech.
S. B. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsuper-
vised grammar induction. In North American Chapter
of the Association for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological).
J. DeNero and D. Klein. 2007. Tailoring word align-
ments to syntactic machine translation. In Association
for Computational Linguistics.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Association for Computational Lin-
guistics.
S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Association for Computational Linguistics.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006.
Contextual dependencies in unsupervised word seg-
mentation. In International Conference on Computa-
tional Linguistics/Association for Computational Lin-
guistics.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Association for Computa-
tional Linguistics.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Association for Computational Linguistics.
M. Johnson and S. Goldwater. 2009. Improving non-
parametric Bayesian inference: Experiments on unsu-
pervised word segmentation with adaptor grammars.
In North American Chapter of the Association for
Computational Linguistics.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Adaptor grammars: a framework for specifying com-
positional nonparametric Bayesian models. In Neural
Information Processing Systems.
M. Johnson. 2007. Why doesnt EM find good HMM
POS-taggers? In Empirical Methods in Natural Lan-
guage Processing/Computational Natural Language
Learning.
D. Klein and C. D. Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependency
and constituency. In Association for Computational
Linguistics.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In North American Chapter of the As-
sociation for Computational Linguistics.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In North American Chapter of the Associ-
ation for Computational Linguistics.
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the penn treebank. Computational Linguistics.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics.
R. Neal and G. E. Hinton. 1998. A view of the EM
algorithm that justifies incremental, sparse, and other
variants. In Learning in Graphical Models. Kluwer
Academic Publishers.
H. Ney and S. Vogel. 1996. HMM-based word alignment
in statistical translation. In International Conference
on Computational Linguistics.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Association for Computational Lin-
guistics.
R. Salakhutdinov, S. Roweis, and Z. Ghahramani. 2003.
Optimization with EM and expectation-conjugate-
gradient. In International Conference on Machine
Learning.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In As-
sociation for Computational Linguistics.
N. A. Smith and M. Johnson. 2007. Weighted and prob-
abilistic context-free grammars are equally expressive.
Computational Linguistics.
I. G. Varea, F. J. Och, H. Ney, and F. Casacuberta. 2002.
Refined lexicon models for statistical machine transla-
tion using a maximum entropy approach. In Associa-
tion for Computational Linguistics.
N. Xue, F-D Chiou, and M. Palmer. 2002. Building a
large-scale annotated Chinese corpus. In International
Conference on Computational Linguistics.
</reference>
<page confidence="0.997348">
590
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.748482">
<title confidence="0.999938">Painless Unsupervised Learning with Features</title>
<author confidence="0.999453">Taylor Berg-Kirkpatrick Alexandre Bouchard-Cˆot´e John DeNero Dan</author>
<affiliation confidence="0.995408">Computer Science University of California at</affiliation>
<email confidence="0.764367">bouchard,denero,</email>
<abstract confidence="0.9994443">We show how features can easily be added to standard generative models for unsupervised learning, without requiring complex new training methods. In particular, each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits. The intuitive EM algorithm still applies, but with a gradient-based M-step familiar from discriminative training of logistic regression models. We apply this technique to part-of-speech induction, grammar induction, word alignment, and word segmentation, incorporating a few linguistically-motivated features into the standard generative model for each task. These feature-enhanced models each outperform their basic counterparts by a substantial margin, and even compete with and surpass more complex state-of-the-art models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N F Ayan</author>
<author>B Dorr</author>
<author>C Monz</author>
</authors>
<title>Combining word alignments using neural networks.</title>
<date>2005</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="24804" citStr="Ayan et al., 2005" startWordPosition="4046" endWordPosition="4049">) EDIT-DISTANCE: I(dist(y, e) = ·) DICTIONARY: I((y, e) ∈ D) for dictionary D. STEM: I(stem(e) = ·, y = ·) for Porterstemmer. PREFIX: I(prefix(e) = ·, y = ·) for prefixes of length 4. CHARACTER: I(e = ·, charAt(y, i) = ·) for index i in the Chinese word. These features correspond to several common augmentations of word alignment models, such as adding dictionary priors and truncating long words, but here we integrate them all coherently into a single model. 6.3 Word Alignment Data and Evaluation We evaluate on the standard hand-aligned portion of the NIST 2002 Chinese-English development set (Ayan et al., 2005). The set is annotated with sure S and possible P alignments. We measure alignment quality using alignment error rate (AER) (Och and Ney, 2000). We train the models on 10,000 sentences of FBIS Chinese-English newswire. This is not a large-scale experiment, but large enough to be relevant for lowresource languages. LBFGS experiments are not provided because computing expectations in these models is too computationally intensive to run for many iterations. Hence, EM training is a more appropriate optimization approach: computing the Mstep gradient requires only summing over word type pairs, whil</context>
</contexts>
<marker>Ayan, Dorr, Monz, 2005</marker>
<rawString>N. F. Ayan, B. Dorr, and C. Monz. 2005. Combining word alignments using neural networks. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bernstein-Ratner</author>
</authors>
<title>The phonology of parentchild speech.</title>
<date>1987</date>
<journal>K. Nelson</journal>
<marker>Bernstein-Ratner, 1987</marker>
<rawString>N. Bernstein-Ratner. 1987. The phonology of parentchild speech. K. Nelson and A. van Kleeck.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bisani</author>
<author>H Ney</author>
</authors>
<title>Joint-sequence models for grapheme-to-phoneme conversion.</title>
<date>2008</date>
<contexts>
<context position="3381" citStr="Bisani and Ney (2008)" startWordPosition="483" endWordPosition="486"> neighborhood design, to the need for complex optimization procedures. As a result, most work still focuses on the stable and intuitive approach of using the EM algorithm to optimize data likelihood in locally normalized, generative models. The primary contribution of this paper is to demonstrate the clear empirical success of a simple and accessible approach to unsupervised learning with features, which can be optimized by using standard NLP building blocks. We consider the same generative, locally-normalized models that dominate past work on a range of tasks. However, we follow Chen (2003), Bisani and Ney (2008), and Bouchard-Cˆot´e et al. (2008), and allow each component multinomial of the model to be a miniature multi-class logistic regression model. In this case, the EM algorithm still applies with the E-step unchanged. The M-step involves gradient-based training familiar from standard supervised logistic regression (i.e., maximum entropy models). By integrating these two familiar learning techniques, we add features to unsupervised models without any 582 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 582–590, Los Angeles, California, June 2</context>
</contexts>
<marker>Bisani, Ney, 2008</marker>
<rawString>M. Bisani and H. Ney. 2008. Joint-sequence models for grapheme-to-phoneme conversion.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bouchard-Cˆot´e</author>
<author>P Liang</author>
<author>D Klein</author>
<author>T L Griffiths</author>
</authors>
<title>A probabilistic approach to language change.</title>
<date>2008</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<marker>Bouchard-Cˆot´e, Liang, Klein, Griffiths, 2008</marker>
<rawString>A. Bouchard-Cˆot´e, P. Liang, D. Klein, and T. L. Griffiths. 2008. A probabilistic approach to language change. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics.</title>
<date>1994</date>
<contexts>
<context position="21254" citStr="Brown et al., 1994" startWordPosition="3466" endWordPosition="3469">rebased dependency parse induction outperforms all existing methods that make the same set of conditional independence assumptions as the DMV. 6 Word Alignment Word alignment is a core machine learning component of statistical machine translation systems, and one of the few NLP tasks that is dominantly solved using unsupervised techniques. The purpose of word alignment models is to induce a correspondence between the words of a sentence and the words of its translation. 6.1 Word Alignment Models We consider two classic generative alignment models that are both used heavily today, IBM Model 1 (Brown et al., 1994) and the HMM alignment model (Ney and Vogel, 1996). These models generate a hidden alignment vector z and an observed foreign sentence y, all conditioned on an observed English sentence e. The likelihood of both models takes the form: P (y, z|e) = 11 p(zj = i|zj−1) - θyj,ei,ALIGN j 4Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. Model Inference Reg Eval POS Induction Many-1 Basic-HMM EM – 63.1 (1.3) Feature-MRF LBFGS 0.1 59.6 (6.9) Feature-HMM EM 1.0 68.1 (1.7) LBFGS 1.0 75.5 (1.1) G</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1994</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1994. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
</authors>
<title>Conditional and joint models for grapheme-to-phoneme conversion.</title>
<date>2003</date>
<booktitle>In Eurospeech.</booktitle>
<contexts>
<context position="3358" citStr="Chen (2003)" startWordPosition="481" endWordPosition="482"> problems, to neighborhood design, to the need for complex optimization procedures. As a result, most work still focuses on the stable and intuitive approach of using the EM algorithm to optimize data likelihood in locally normalized, generative models. The primary contribution of this paper is to demonstrate the clear empirical success of a simple and accessible approach to unsupervised learning with features, which can be optimized by using standard NLP building blocks. We consider the same generative, locally-normalized models that dominate past work on a range of tasks. However, we follow Chen (2003), Bisani and Ney (2008), and Bouchard-Cˆot´e et al. (2008), and allow each component multinomial of the model to be a miniature multi-class logistic regression model. In this case, the EM algorithm still applies with the E-step unchanged. The M-step involves gradient-based training familiar from standard supervised logistic regression (i.e., maximum entropy models). By integrating these two familiar learning techniques, we add features to unsupervised models without any 582 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 582–590, Los Ange</context>
</contexts>
<marker>Chen, 2003</marker>
<rawString>S. F. Chen. 2003. Conditional and joint models for grapheme-to-phoneme conversion. In Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="17442" citStr="Cohen and Smith, 2009" startWordPosition="2834" endWordPosition="2837">noulli STOP probabilities Bd,c,STOP capture the valence of a particular head. For this type, the decision d is whether or not to stop generating arguments, and the context c contains the current head h, direction 6 and adjacency adj. If a head’s stop probability is high, it will be encouraged to accept few arguments. The ATTACH multinomial probability distributions Bd,c,ATTACH capture attachment preferences of heads. For this type, a decision d is an argument token a, and the context c consists of a head h and a direction 6. We take the same approach as previous work (Klein and Manning, 2004; Cohen and Smith, 2009) and use gold POS tags in place of words. 3Haghighi and Klein (2006) achieve higher accuracies by making use of labeled prototypes. We do not use any external information. 5.2 Grammar Induction Features One way to inject knowledge into a dependency model is to encode the similarity between the various morphological variants of nouns and verbs. We encode this similarity by incorporating features into both the STOP and the ATTACH probabilities. The attachment features appear below; the stop feature templates are similar and are therefore omitted. BASIC: I(a = ·, h = ·, S = ·) NOUN: Generalize th</context>
<context position="19552" citStr="Cohen and Smith (2009)" startWordPosition="3183" endWordPosition="3186">when the corresponding contexts are headed by nouns and verbs. This observation motivates the features we choose to incorporate into the DMV. 5.3 Grammar Induction Data and Evaluation For our English experiments we train and report directed attachment accuracy on portions of the WSJ corpus. We work with a standard, reduced version of WSJ, WSJ10, that contains only sentences of length 10 or less after punctuation has been removed. We train on sections 2-21, and use section 22 as a development set. We report accuracy on section 23. These are the same training, development, and test sets used by Cohen and Smith (2009). The regularization parameter (n) is tuned on the development set to maximize accuracy. For our Chinese experiments, we use the same corpus and training/test split as Cohen and Smith 586 (2009). We train on sections 1-270 of the Penn Chinese Treebank (Xue et al., 2002), similarly reduced (CTB10). We test on sections 271-300 of CTB10, and use sections 400-454 as a development set. The DMV is known to be sensitive to initialization. We use the deterministic harmonic initializer from Klein and Manning (2004). We ran each optimization procedure for 100 iterations. The results are reported in Tabl</context>
<context position="21593" citStr="Cohen and Smith (2009)" startWordPosition="3524" endWordPosition="3527">ques. The purpose of word alignment models is to induce a correspondence between the words of a sentence and the words of its translation. 6.1 Word Alignment Models We consider two classic generative alignment models that are both used heavily today, IBM Model 1 (Brown et al., 1994) and the HMM alignment model (Ney and Vogel, 1996). These models generate a hidden alignment vector z and an observed foreign sentence y, all conditioned on an observed English sentence e. The likelihood of both models takes the form: P (y, z|e) = 11 p(zj = i|zj−1) - θyj,ei,ALIGN j 4Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. Model Inference Reg Eval POS Induction Many-1 Basic-HMM EM – 63.1 (1.3) Feature-MRF LBFGS 0.1 59.6 (6.9) Feature-HMM EM 1.0 68.1 (1.7) LBFGS 1.0 75.5 (1.1) Grammar Induction Dir 0 Basic-DMV EM – 47.8 Feature-DMV EM 0.05 48.3 LBFGS 10.0 63.0 (Cohen and Smith, 2009) 61.3 0 Basic-DMV EM – 42.5 B1 Feature-DMV EM 1.0 49.9 WSJ 5.0 53.6 CT LBFGS (Cohen and Smith, 2009) 51.9 Word Alignment AER Basic-Model 1 EM – 38.0 C Feature-Model 1 EM – 35.6 Basic-HMM EM – 33.8 NI Feature-HMM EM – 30.0 Word Segme</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological).</journal>
<contexts>
<context position="9725" citStr="Dempster et al., 1977" startWordPosition="1524" endWordPosition="1528"> over the joint distribution. In the case of the POS HMM, for example, we do not need to enumerate an infinite sum of products of potentials when optimizing, in contrast to Haghighi and Klein (2006). Second, we found that locally normalized models empirically outperform their globally normalized counterparts, despite their efficiency and simplicity. 3 Optimization 3.1 Optimizing with Expectation Maximization In this section, we describe the EM algorithm applied to our feature-rich, locally normalized models. For models parameterized by standard multinomials, EM optimizes L(0) = log Pe(Y = y) (Dempster et al., 1977). The E-step computes expected counts for each tuple of decision d, context c, and multinomial type t: L� Y = yJ ed,c,t �E� ✶(Xi =d, X�(i) =c, t) (2) i∈I 2The locally normalized model class is actually equivalent to its globally normalized counterpart when the former meets the following three conditions: (1) The graphical model is a directed tree. (2) The BASIC features are included in f. (3) We do not include regularization in the model (κ = 0). This follows from Smith and Johnson (2007). These expected counts are then normalized in the M-step to re-estimate 0: Bd,c,t +— Ed′ ed′,c,t ed,c,t No</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Tailoring word alignments to syntactic machine translation.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="25737" citStr="DeNero and Klein, 2007" startWordPosition="4187" endWordPosition="4190">ages. LBFGS experiments are not provided because computing expectations in these models is too computationally intensive to run for many iterations. Hence, EM training is a more appropriate optimization approach: computing the Mstep gradient requires only summing over word type pairs, while the marginal likelihood gradient needed for LBFGS requires summing over training sentence alignments. The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al., 2006). The combination of these techniques yields a state-of-the-art unsupervised baseline for ChineseEnglish. 6.4 Word Alignment Results For both IBM Model 1 and the HMM alignment model, EM training with feature-enhanced models outperforms the standard multinomial models, by 2.4 and 3.8 AER respectively.6 As expected, large positive weights are assigned to both the dictionary and edit distance features. Stem and character features also contribute to the performance gain. 7 Word Segmentation Finally, we show that it is possible to impro</context>
</contexts>
<marker>DeNero, Klein, 2007</marker>
<rawString>J. DeNero and D. Klein. 2007. Tailoring word alignments to syntactic machine translation. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Does Baum-Welch re-estimation help taggers?</title>
<date>1994</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5350" citStr="Elworthy, 1994" startWordPosition="780" endWordPosition="781">h task, we show that declaring a few linguistically motivated feature templates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions: Pe(Y=Y,Z=Z)= Pe(</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>D. Elworthy. 1994. Does Baum-Welch re-estimation help taggers? In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T L Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5407" citStr="Goldwater and Griffiths, 2007" startWordPosition="787" endWordPosition="790">stically motivated feature templates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions: Pe(Y=Y,Z=Z)= Pe(Z1 = z1) · |z| Pe(Yi = yi|Zi = zi) · Pe(Zi+1 = zi+1|Zi = </context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T L Griffiths</author>
<author>M Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In International Conference on Computational Linguistics/Association for Computational Linguistics.</booktitle>
<contexts>
<context position="28180" citStr="Goldwater et al. (2006)" startWordPosition="4585" endWordPosition="4588">he following distribution: θl,LENGTH = exp(−l1.6) when 1 &lt; l &lt; 10. Their model is deficient since it is possible to generate 6The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al., 2009). 588 lengths that are inconsistent with the actual lengths of the generated segments. The likelihood equation is given by: P(Y = y,Z = z) = [(1 − ESTOP) Ez{,SEGMENT exp(−|&apos;7&apos;2|1.s)] 7.2 Segmentation Data and Evaluation We train and test on the phonetic version of the Bernstein-Ratner corpus (1987). This is the same set-up used by Liang and Klein (2009), Goldwater et al. (2006), and Johnson and Goldwater (2009). This corpus consists of 9790 child-directed utterances transcribed using a phonetic representation. We measure segment F1 score on the entire corpus. We run all word segmentation models for 300 iterations with 10 random initializations and report the mean and standard deviation of F1 in Table 1. 7.3 Segmentation Features The SEGMENT multinomial is the important distribution in this model. We use the following features: BASIC: 1(z = ·) LENGTH: 1(length(z) = ·) NUMBER-VOWELS: 1(numVowels(z) = ·) PHONO-CLASS-PREF: 1(prefix(coarsePhonemes(z)) = ·) PHONO-CLASS-PR</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>S. Goldwater, T. L. Griffiths, and M. Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In International Conference on Computational Linguistics/Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Prototype-driven learning for sequence models. In Association for Computational Linguistics.</title>
<date>2006</date>
<contexts>
<context position="2655" citStr="Haghighi and Klein, 2006" startWordPosition="371" endWordPosition="374">inery but which still provides substantial flexibility. In the featureengineering paradigm, one can worry less about the backbone structure and instead use hand-designed features to declaratively inject domain knowledge into a model. While feature engineering has historically been associated with discriminative, supervised learning settings, we argue that it can and should be applied more broadly to the unsupervised setting. The idea of using features in unsupervised learning is neither new nor even controversial. Many top unsupervised results use feature-based models (Smith and Eisner, 2005; Haghighi and Klein, 2006). However, such approaches have presented their own barriers, from challenging normalization problems, to neighborhood design, to the need for complex optimization procedures. As a result, most work still focuses on the stable and intuitive approach of using the EM algorithm to optimize data likelihood in locally normalized, generative models. The primary contribution of this paper is to demonstrate the clear empirical success of a simple and accessible approach to unsupervised learning with features, which can be optimized by using standard NLP building blocks. We consider the same generative</context>
<context position="9301" citStr="Haghighi and Klein (2006)" startWordPosition="1461" endWordPosition="1464">ver decisions. The objective function we derive from this model is the marginal likelihood of the observations y, along with a regularization term: L(w) = log Pw(Y = y) — n||w||2 (1) 2 This model has two advantages over the more prevalent form of a feature-rich unsupervised model, the globally normalized Markov random field.2 First, as we explain in Section 3, optimizing our objective does not require computing expectations over the joint distribution. In the case of the POS HMM, for example, we do not need to enumerate an infinite sum of products of potentials when optimizing, in contrast to Haghighi and Klein (2006). Second, we found that locally normalized models empirically outperform their globally normalized counterparts, despite their efficiency and simplicity. 3 Optimization 3.1 Optimizing with Expectation Maximization In this section, we describe the EM algorithm applied to our feature-rich, locally normalized models. For models parameterized by standard multinomials, EM optimizes L(0) = log Pe(Y = y) (Dempster et al., 1977). The E-step computes expected counts for each tuple of decision d, context c, and multinomial type t: L� Y = yJ ed,c,t �E� ✶(Xi =d, X�(i) =c, t) (2) i∈I 2The locally normalize</context>
<context position="13642" citStr="Haghighi and Klein (2006)" startWordPosition="2181" endWordPosition="2184">his optimization approach leads to higher task accuracy for several models. However, in cases where computing ed,,,t is expensive, EM can be a more efficient alternative. 4 Part-of-Speech Induction We now describe experiments that demonstrate the effectiveness of locally normalized logistic models. We first use the bigram HMM described in Section 2.1 for POS induction, which has two types of multinomials. For type EMIT, the decisions d are words and contexts c are tags. For type TRANS, the decisions and contexts are both tags. 4.1 POS Induction Features We use the same set of features used by Haghighi and Klein (2006) in their baseline globally normalized Markov random field (MRF) model. These are all coarse features on emission contexts that activate for words with certain orthographic properties. We use only the BASIC features for transitions. For an emission with word y and tag z, we use the following feature templates: BASIC: I(y = ·, z = ·) CONTAINS-DIGIT: Check if y contains digit and conjoin with z: I(containsDigit(y) = ·, z = ·) CONTAINS-HYPHEN: I(containsHyphen(x) = ·, z = ·) INITIAL-CAP: Check if the first letter of y is capitalized: I(isCap(y) = ·, z = ·) N-GRAM: Indicator functions for characte</context>
<context position="15200" citStr="Haghighi and Klein (2006)" startWordPosition="2457" endWordPosition="2460">ctionary. We use 45 tag clusters, the number of POS tags that appear in the WSJ corpus. There is an identifiability issue when evaluating inferred tags. In order to measure accuracy on the hand-labeled corpus, we map each cluster to the tag that gives the highest accuracy, the many-1 evaluation approach (Johnson, 2007). We run all POS induction models for 1000 iterations, with 10 random initializations. The mean and standard deviation of many-1 accuracy appears in Table 1. 4.3 POS Induction Results We compare our model to the basic HMM and a bigram version of the feature-enhanced MRF model of Haghighi and Klein (2006). Using EM, we achieve a many-1 accuracy of 68.1. This outperforms the basic HMM baseline by a 5.0 margin. The same model, trained using the direct gradient approach, achieves a many-1 accuracy of 75.5, outperforming the basic HMM baseline by a margin of 12.4. These results show that the direct gradient approach can offer additional boosts in performance when used with a feature-enhanced model. We also outperform the 585 globally normalized MRF, which uses the same set of features and which we train using a direct gradient approach. To the best of our knowledge, our system achieves the best pe</context>
<context position="17510" citStr="Haghighi and Klein (2006)" startWordPosition="2847" endWordPosition="2850">icular head. For this type, the decision d is whether or not to stop generating arguments, and the context c contains the current head h, direction 6 and adjacency adj. If a head’s stop probability is high, it will be encouraged to accept few arguments. The ATTACH multinomial probability distributions Bd,c,ATTACH capture attachment preferences of heads. For this type, a decision d is an argument token a, and the context c consists of a head h and a direction 6. We take the same approach as previous work (Klein and Manning, 2004; Cohen and Smith, 2009) and use gold POS tags in place of words. 3Haghighi and Klein (2006) achieve higher accuracies by making use of labeled prototypes. We do not use any external information. 5.2 Grammar Induction Features One way to inject knowledge into a dependency model is to encode the similarity between the various morphological variants of nouns and verbs. We encode this similarity by incorporating features into both the STOP and the ATTACH probabilities. The attachment features appear below; the stop feature templates are similar and are therefore omitted. BASIC: I(a = ·, h = ·, S = ·) NOUN: Generalize the morphological variants of nouns by using isNoun(·): I(a = ·, isNou</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>A. Haghighi and D. Klein. 2006. Prototype-driven learning for sequence models. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>J Blitzer</author>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Better word alignments with supervised ITG models.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="27800" citStr="Haghighi et al., 2009" startWordPosition="4522" endWordPosition="4525">NT. For this type, there is no context and the decision is the particular string generated. In order to avoid the degenerate MLE that assigns mass only to single segment sentences it is helpful to independently generate a length for each segment from a fixed distribution. Liang and Klein (2009) constrain individual segments to have maximum length 10 and generate lengths from the following distribution: θl,LENGTH = exp(−l1.6) when 1 &lt; l &lt; 10. Their model is deficient since it is possible to generate 6The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al., 2009). 588 lengths that are inconsistent with the actual lengths of the generated segments. The likelihood equation is given by: P(Y = y,Z = z) = [(1 − ESTOP) Ez{,SEGMENT exp(−|&apos;7&apos;2|1.s)] 7.2 Segmentation Data and Evaluation We train and test on the phonetic version of the Bernstein-Ratner corpus (1987). This is the same set-up used by Liang and Klein (2009), Goldwater et al. (2006), and Johnson and Goldwater (2009). This corpus consists of 9790 child-directed utterances transcribed using a phonetic representation. We measure segment F1 score on the entire corpus. We run all word segmentation model</context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009. Better word alignments with supervised ITG models. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>S Goldwater</author>
</authors>
<title>Improving nonparametric Bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="22322" citStr="Johnson and Goldwater, 2009" startWordPosition="3654" endWordPosition="3657">s. Model Inference Reg Eval POS Induction Many-1 Basic-HMM EM – 63.1 (1.3) Feature-MRF LBFGS 0.1 59.6 (6.9) Feature-HMM EM 1.0 68.1 (1.7) LBFGS 1.0 75.5 (1.1) Grammar Induction Dir 0 Basic-DMV EM – 47.8 Feature-DMV EM 0.05 48.3 LBFGS 10.0 63.0 (Cohen and Smith, 2009) 61.3 0 Basic-DMV EM – 42.5 B1 Feature-DMV EM 1.0 49.9 WSJ 5.0 53.6 CT LBFGS (Cohen and Smith, 2009) 51.9 Word Alignment AER Basic-Model 1 EM – 38.0 C Feature-Model 1 EM – 35.6 Basic-HMM EM – 33.8 NI Feature-HMM EM – 30.0 Word Segmentation F1 Basic-Unigram EM – 76.9 (0.1) R Feature-Unigram EM 0.2 84.5 (0.5) B LBFGS 0.2 88.0 (0.1) (Johnson and Goldwater, 2009) 87 Table 1: Locally normalized feature-based models outperform all proposed baselines for all four tasks. LBFGS outperformed EM in all cases where the algorithm was sufficiently fast to run. Details of each experiment appear in the main text. The distortion term p(zj = i|zj−1) is uniform in Model 1, and Markovian in the HMM. See Liang et al. (2006) for details on the specific variant of the distortion model of the HMM that we used. We use these standard distortion models in both the baseline and feature-enhanced word alignment systems. The bilexical emission model θy,e,ALIGN differentiates ou</context>
<context position="28214" citStr="Johnson and Goldwater (2009)" startWordPosition="4590" endWordPosition="4593">,LENGTH = exp(−l1.6) when 1 &lt; l &lt; 10. Their model is deficient since it is possible to generate 6The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al., 2009). 588 lengths that are inconsistent with the actual lengths of the generated segments. The likelihood equation is given by: P(Y = y,Z = z) = [(1 − ESTOP) Ez{,SEGMENT exp(−|&apos;7&apos;2|1.s)] 7.2 Segmentation Data and Evaluation We train and test on the phonetic version of the Bernstein-Ratner corpus (1987). This is the same set-up used by Liang and Klein (2009), Goldwater et al. (2006), and Johnson and Goldwater (2009). This corpus consists of 9790 child-directed utterances transcribed using a phonetic representation. We measure segment F1 score on the entire corpus. We run all word segmentation models for 300 iterations with 10 random initializations and report the mean and standard deviation of F1 in Table 1. 7.3 Segmentation Features The SEGMENT multinomial is the important distribution in this model. We use the following features: BASIC: 1(z = ·) LENGTH: 1(length(z) = ·) NUMBER-VOWELS: 1(numVowels(z) = ·) PHONO-CLASS-PREF: 1(prefix(coarsePhonemes(z)) = ·) PHONO-CLASS-PREF: 1(suffix(coarsePhonemes(z)) = </context>
<context position="29891" citStr="Johnson and Goldwater, 2009" startWordPosition="4851" endWordPosition="4854">thod, the feature-enhanced unigram model still respects the conditional independence assumptions that the standard unigram model makes, and inference is still performed using a simple dynamic program to compute expected sufficient statistics, which are just segment counts. 7.4 Segmentation Results To our knowledge our system achieves the best performance to date on the Bernstein-Ratner corpus, with an F1 of 88.0. It is substantially simpler than the non-parametric Bayesian models proposed by Johnson et al. (2007), which require sampling procedures to perform inference and achieve an F1 of 87 (Johnson and Goldwater, 2009). Similar to our other results, the direct gradient approach outperforms EM for feature-enhanced models, and both approaches outperform the baseline, which achieves an F1 of 76.9. 8 Conclusion We have shown that simple, locally normalized models can effectively incorporate features into unsupervised models. These enriched models can be easily optimized using standard NLP building blocks. Beyond the four tasks explored in this paper—POS tagging, DMV grammar induction, word alignment, and word segmentation—the method can be applied to many other tasks, for example grounded semantics, unsupervise</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>M. Johnson and S. Goldwater. 2009. Improving nonparametric Bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars. In North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>T L Griffiths</author>
<author>S Goldwater</author>
</authors>
<title>Adaptor grammars: a framework for specifying compositional nonparametric Bayesian models.</title>
<date>2007</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<contexts>
<context position="29781" citStr="Johnson et al. (2007)" startWordPosition="4833" endWordPosition="4836">al phonetic shapes that correspond to well-formed word boundaries. As is the case in general for our method, the feature-enhanced unigram model still respects the conditional independence assumptions that the standard unigram model makes, and inference is still performed using a simple dynamic program to compute expected sufficient statistics, which are just segment counts. 7.4 Segmentation Results To our knowledge our system achieves the best performance to date on the Bernstein-Ratner corpus, with an F1 of 88.0. It is substantially simpler than the non-parametric Bayesian models proposed by Johnson et al. (2007), which require sampling procedures to perform inference and achieve an F1 of 87 (Johnson and Goldwater, 2009). Similar to our other results, the direct gradient approach outperforms EM for feature-enhanced models, and both approaches outperform the baseline, which achieves an F1 of 76.9. 8 Conclusion We have shown that simple, locally normalized models can effectively incorporate features into unsupervised models. These enriched models can be easily optimized using standard NLP building blocks. Beyond the four tasks explored in this paper—POS tagging, DMV grammar induction, word alignment, an</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>M. Johnson, T. L. Griffiths, and S. Goldwater. 2007. Adaptor grammars: a framework for specifying compositional nonparametric Bayesian models. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Why doesnt EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Empirical Methods in Natural Language Processing/Computational Natural Language Learning.</booktitle>
<contexts>
<context position="5423" citStr="Johnson, 2007" startWordPosition="791" endWordPosition="792">lates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions: Pe(Y=Y,Z=Z)= Pe(Z1 = z1) · |z| Pe(Yi = yi|Zi = zi) · Pe(Zi+1 = zi+1|Zi = zi) The emission</context>
<context position="10218" citStr="Johnson (2007)" startWordPosition="1613" endWordPosition="1614">ized models. For models parameterized by standard multinomials, EM optimizes L(0) = log Pe(Y = y) (Dempster et al., 1977). The E-step computes expected counts for each tuple of decision d, context c, and multinomial type t: L� Y = yJ ed,c,t �E� ✶(Xi =d, X�(i) =c, t) (2) i∈I 2The locally normalized model class is actually equivalent to its globally normalized counterpart when the former meets the following three conditions: (1) The graphical model is a directed tree. (2) The BASIC features are included in f. (3) We do not include regularization in the model (κ = 0). This follows from Smith and Johnson (2007). These expected counts are then normalized in the M-step to re-estimate 0: Bd,c,t +— Ed′ ed′,c,t ed,c,t Normalizing expected counts in this way maximizes the expected complete log likelihood with respect to the current model parameters. EM can likewise optimize L(w) for our locally normalized models with logistic parameterizations. The E-step first precomputes multinomial parameters from w for each decision, context, and type: exp(w, f(d, c, t)) Bd,c,t(w) � Ed′ exp(w, f(d′, c, t)) Then, expected counts e are computed according to Equation 2. In the case of POS induction, expected counts are c</context>
<context position="14895" citStr="Johnson, 2007" startWordPosition="2407" endWordPosition="2408">. 4.2 POS Induction Data and Evaluation We train and test on the entire WSJ tag corpus (Marcus et al., 1993). We attempt the most difficult version of this task where the only information our system can make use of is the unlabeled text itself. In particular, we do not make use of a tagging dictionary. We use 45 tag clusters, the number of POS tags that appear in the WSJ corpus. There is an identifiability issue when evaluating inferred tags. In order to measure accuracy on the hand-labeled corpus, we map each cluster to the tag that gives the highest accuracy, the many-1 evaluation approach (Johnson, 2007). We run all POS induction models for 1000 iterations, with 10 random initializations. The mean and standard deviation of many-1 accuracy appears in Table 1. 4.3 POS Induction Results We compare our model to the basic HMM and a bigram version of the feature-enhanced MRF model of Haghighi and Klein (2006). Using EM, we achieve a many-1 accuracy of 68.1. This outperforms the basic HMM baseline by a 5.0 margin. The same model, trained using the direct gradient approach, achieves a many-1 accuracy of 75.5, outperforming the basic HMM baseline by a margin of 12.4. These results show that the direct</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>M. Johnson. 2007. Why doesnt EM find good HMM POS-taggers? In Empirical Methods in Natural Language Processing/Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="16073" citStr="Klein and Manning, 2004" startWordPosition="2601" endWordPosition="2604">in of 12.4. These results show that the direct gradient approach can offer additional boosts in performance when used with a feature-enhanced model. We also outperform the 585 globally normalized MRF, which uses the same set of features and which we train using a direct gradient approach. To the best of our knowledge, our system achieves the best performance to date on the WSJ corpus for totally unsupervised POS tagging.3 5 Grammar Induction We next apply our technique to a grammar induction task: the unsupervised learning of dependency parse trees via the dependency model with valence (DMV) (Klein and Manning, 2004). A dependency parse is a directed tree over tokens in a sentence. Each edge of the tree specifies a directed dependency from a head token to a dependent, or argument token. Thus, the number of dependencies in a parse is exactly the number of tokens in the sentence, not counting the artificial root token. 5.1 Dependency Model with Valence The DMV defines a probability distribution over dependency parse trees. In this head-outward attachment model, a parse and the word tokens are derived together through a recursive generative process. For each token generated so far, starting with the root, a </context>
<context position="17418" citStr="Klein and Manning, 2004" startWordPosition="2830" endWordPosition="2833">ns in this model. The Bernoulli STOP probabilities Bd,c,STOP capture the valence of a particular head. For this type, the decision d is whether or not to stop generating arguments, and the context c contains the current head h, direction 6 and adjacency adj. If a head’s stop probability is high, it will be encouraged to accept few arguments. The ATTACH multinomial probability distributions Bd,c,ATTACH capture attachment preferences of heads. For this type, a decision d is an argument token a, and the context c consists of a head h and a direction 6. We take the same approach as previous work (Klein and Manning, 2004; Cohen and Smith, 2009) and use gold POS tags in place of words. 3Haghighi and Klein (2006) achieve higher accuracies by making use of labeled prototypes. We do not use any external information. 5.2 Grammar Induction Features One way to inject knowledge into a dependency model is to encode the similarity between the various morphological variants of nouns and verbs. We encode this similarity by incorporating features into both the STOP and the ATTACH probabilities. The attachment features appear below; the stop feature templates are similar and are therefore omitted. BASIC: I(a = ·, h = ·, S </context>
<context position="20063" citStr="Klein and Manning (2004)" startWordPosition="3269" endWordPosition="3272">rt accuracy on section 23. These are the same training, development, and test sets used by Cohen and Smith (2009). The regularization parameter (n) is tuned on the development set to maximize accuracy. For our Chinese experiments, we use the same corpus and training/test split as Cohen and Smith 586 (2009). We train on sections 1-270 of the Penn Chinese Treebank (Xue et al., 2002), similarly reduced (CTB10). We test on sections 271-300 of CTB10, and use sections 400-454 as a development set. The DMV is known to be sensitive to initialization. We use the deterministic harmonic initializer from Klein and Manning (2004). We ran each optimization procedure for 100 iterations. The results are reported in Table 1. 5.4 Grammar Induction Results We are able to outperform Cohen and Smith’s (2009) best system, which requires a more complicated variational inference method, on both English and Chinese data sets. Their system achieves an accuracy of 61.3 for English and an accuracy of 51.9 for Chinese.4 Our feature-enhanced model, trained using the direct gradient approach, achieves an accuracy of 63.0 for English, and an accuracy of 53.6 for Chinese. To our knowledge, our method for featurebased dependency parse ind</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>Online EM for unsupervised models.</title>
<date>2009</date>
<journal>In North American Chapter of the Association for Computational Linguistics.</journal>
<contexts>
<context position="26429" citStr="Liang and Klein (2009)" startWordPosition="4294" endWordPosition="4297">ombination of these techniques yields a state-of-the-art unsupervised baseline for ChineseEnglish. 6.4 Word Alignment Results For both IBM Model 1 and the HMM alignment model, EM training with feature-enhanced models outperforms the standard multinomial models, by 2.4 and 3.8 AER respectively.6 As expected, large positive weights are assigned to both the dictionary and edit distance features. Stem and character features also contribute to the performance gain. 7 Word Segmentation Finally, we show that it is possible to improve upon the simple and effective word segmentation model presented in Liang and Klein (2009) by adding phonological features. Unsupervised word segmentation is the task of identifying word boundaries in sentences where spaces have been removed. For a sequence of characters y = (y1, ..., yn), a segmentation is a sequence of segments z = (z1, ..., z|z|) such that z is a partition of y and each zi is a contiguous subsequence of y. Unsupervised models for this task infer word boundaries from corpora of sentences of characters without ever seeing examples of well-formed words. 7.1 Unigram Double-Exponential Model Liang and Klein’s (2009) unigram doubleexponential model corresponds to a si</context>
<context position="28155" citStr="Liang and Klein (2009)" startWordPosition="4581" endWordPosition="4584"> generate lengths from the following distribution: θl,LENGTH = exp(−l1.6) when 1 &lt; l &lt; 10. Their model is deficient since it is possible to generate 6The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al., 2009). 588 lengths that are inconsistent with the actual lengths of the generated segments. The likelihood equation is given by: P(Y = y,Z = z) = [(1 − ESTOP) Ez{,SEGMENT exp(−|&apos;7&apos;2|1.s)] 7.2 Segmentation Data and Evaluation We train and test on the phonetic version of the Bernstein-Ratner corpus (1987). This is the same set-up used by Liang and Klein (2009), Goldwater et al. (2006), and Johnson and Goldwater (2009). This corpus consists of 9790 child-directed utterances transcribed using a phonetic representation. We measure segment F1 score on the entire corpus. We run all word segmentation models for 300 iterations with 10 random initializations and report the mean and standard deviation of F1 in Table 1. 7.3 Segmentation Features The SEGMENT multinomial is the important distribution in this model. We use the following features: BASIC: 1(z = ·) LENGTH: 1(length(z) = ·) NUMBER-VOWELS: 1(numVowels(z) = ·) PHONO-CLASS-PREF: 1(prefix(coarsePhoneme</context>
</contexts>
<marker>Liang, Klein, 2009</marker>
<rawString>P. Liang and D. Klein. 2009. Online EM for unsupervised models. In North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="22673" citStr="Liang et al. (2006)" startWordPosition="3713" endWordPosition="3716">, 2009) 51.9 Word Alignment AER Basic-Model 1 EM – 38.0 C Feature-Model 1 EM – 35.6 Basic-HMM EM – 33.8 NI Feature-HMM EM – 30.0 Word Segmentation F1 Basic-Unigram EM – 76.9 (0.1) R Feature-Unigram EM 0.2 84.5 (0.5) B LBFGS 0.2 88.0 (0.1) (Johnson and Goldwater, 2009) 87 Table 1: Locally normalized feature-based models outperform all proposed baselines for all four tasks. LBFGS outperformed EM in all cases where the algorithm was sufficiently fast to run. Details of each experiment appear in the main text. The distortion term p(zj = i|zj−1) is uniform in Model 1, and Markovian in the HMM. See Liang et al. (2006) for details on the specific variant of the distortion model of the HMM that we used. We use these standard distortion models in both the baseline and feature-enhanced word alignment systems. The bilexical emission model θy,e,ALIGN differentiates our feature-enhanced system from the baseline system. In the former, the emission model is a standard conditional multinomial that represents the probability that decision word y is generated from context word e, while in our system, the emission model is re-parameterized as a logistic regression model and feature-enhanced. Many supervised feature-bas</context>
<context position="25800" citStr="Liang et al., 2006" startWordPosition="4199" endWordPosition="4202">ions in these models is too computationally intensive to run for many iterations. Hence, EM training is a more appropriate optimization approach: computing the Mstep gradient requires only summing over word type pairs, while the marginal likelihood gradient needed for LBFGS requires summing over training sentence alignments. The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al., 2006). The combination of these techniques yields a state-of-the-art unsupervised baseline for ChineseEnglish. 6.4 Word Alignment Results For both IBM Model 1 and the HMM alignment model, EM training with feature-enhanced models outperforms the standard multinomial models, by 2.4 and 3.8 AER respectively.6 As expected, large positive weights are assigned to both the dictionary and edit distance features. Stem and character features also contribute to the performance gain. 7 Word Segmentation Finally, we show that it is possible to improve upon the simple and effective word segmentation model presen</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agreement. In North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
<author>C Dong</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming.</booktitle>
<contexts>
<context position="4219" citStr="Liu et al., 1989" startWordPosition="610" endWordPosition="613">-step involves gradient-based training familiar from standard supervised logistic regression (i.e., maximum entropy models). By integrating these two familiar learning techniques, we add features to unsupervised models without any 582 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 582–590, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics specialized learning or inference. A second contribution of this work is to show that further gains can be achieved by directly optimizing data likelihood with LBFGS (Liu et al., 1989). This alternative optimization procedure requires no additional machinery beyond what EM uses. This approach is still very simple to implement, and we found that it empirically outperforms EM. This paper is largely empirical; the underlying optimization techniques are known, even if the overall approach will be novel to many readers. As an empirical demonstration, our results span an array of unsupervised learning tasks: part-of-speech induction, grammar induction, word alignment, and word segmentation. In each task, we show that declaring a few linguistically motivated feature templates yiel</context>
</contexts>
<marker>Liu, Nocedal, Dong, 1989</marker>
<rawString>D. C. Liu, J. Nocedal, and C. Dong. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the penn treebank. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="14389" citStr="Marcus et al., 1993" startWordPosition="2312" endWordPosition="2316"> activate for words with certain orthographic properties. We use only the BASIC features for transitions. For an emission with word y and tag z, we use the following feature templates: BASIC: I(y = ·, z = ·) CONTAINS-DIGIT: Check if y contains digit and conjoin with z: I(containsDigit(y) = ·, z = ·) CONTAINS-HYPHEN: I(containsHyphen(x) = ·, z = ·) INITIAL-CAP: Check if the first letter of y is capitalized: I(isCap(y) = ·, z = ·) N-GRAM: Indicator functions for character ngrams of up to length 3 present in y. 4.2 POS Induction Data and Evaluation We train and test on the entire WSJ tag corpus (Marcus et al., 1993). We attempt the most difficult version of this task where the only information our system can make use of is the unlabeled text itself. In particular, we do not make use of a tagging dictionary. We use 45 tag clusters, the number of POS tags that appear in the WSJ corpus. There is an identifiability issue when evaluating inferred tags. In order to measure accuracy on the hand-labeled corpus, we map each cluster to the tag that gives the highest accuracy, the many-1 evaluation approach (Johnson, 2007). We run all POS induction models for 1000 iterations, with 10 random initializations. The mea</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: the penn treebank. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model. Computational Linguistics.</title>
<date>1994</date>
<contexts>
<context position="5333" citStr="Merialdo, 1994" startWordPosition="778" endWordPosition="779">entation. In each task, we show that declaring a few linguistically motivated feature templates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions:</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Neal</author>
<author>G E Hinton</author>
</authors>
<title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
<date>1998</date>
<booktitle>In Learning in Graphical Models.</booktitle>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Neal, Hinton, 1998</marker>
<rawString>R. Neal and G. E. Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In Learning in Graphical Models. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>S Vogel</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="21304" citStr="Ney and Vogel, 1996" startWordPosition="3475" endWordPosition="3478">l existing methods that make the same set of conditional independence assumptions as the DMV. 6 Word Alignment Word alignment is a core machine learning component of statistical machine translation systems, and one of the few NLP tasks that is dominantly solved using unsupervised techniques. The purpose of word alignment models is to induce a correspondence between the words of a sentence and the words of its translation. 6.1 Word Alignment Models We consider two classic generative alignment models that are both used heavily today, IBM Model 1 (Brown et al., 1994) and the HMM alignment model (Ney and Vogel, 1996). These models generate a hidden alignment vector z and an observed foreign sentence y, all conditioned on an observed English sentence e. The likelihood of both models takes the form: P (y, z|e) = 11 p(zj = i|zj−1) - θyj,ei,ALIGN j 4Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. Model Inference Reg Eval POS Induction Many-1 Basic-HMM EM – 63.1 (1.3) Feature-MRF LBFGS 0.1 59.6 (6.9) Feature-HMM EM 1.0 68.1 (1.7) LBFGS 1.0 75.5 (1.1) Grammar Induction Dir 0 Basic-DMV EM – 47.8 Feature</context>
</contexts>
<marker>Ney, Vogel, 1996</marker>
<rawString>H. Ney and S. Vogel. 1996. HMM-based word alignment in statistical translation. In International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="24947" citStr="Och and Ney, 2000" startWordPosition="4070" endWordPosition="4073">e) = ·, y = ·) for prefixes of length 4. CHARACTER: I(e = ·, charAt(y, i) = ·) for index i in the Chinese word. These features correspond to several common augmentations of word alignment models, such as adding dictionary priors and truncating long words, but here we integrate them all coherently into a single model. 6.3 Word Alignment Data and Evaluation We evaluate on the standard hand-aligned portion of the NIST 2002 Chinese-English development set (Ayan et al., 2005). The set is annotated with sure S and possible P alignments. We measure alignment quality using alignment error rate (AER) (Och and Ney, 2000). We train the models on 10,000 sentences of FBIS Chinese-English newswire. This is not a large-scale experiment, but large enough to be relevant for lowresource languages. LBFGS experiments are not provided because computing expectations in these models is too computationally intensive to run for many iterations. Hence, EM training is a more appropriate optimization approach: computing the Mstep gradient requires only summing over word type pairs, while the marginal likelihood gradient needed for LBFGS requires summing over training sentence alignments. The final alignments, in both the basel</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. Improved statistical alignment models. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Salakhutdinov</author>
<author>S Roweis</author>
<author>Z Ghahramani</author>
</authors>
<title>Optimization with EM and expectation-conjugategradient.</title>
<date>2003</date>
<booktitle>In International Conference on Machine Learning.</booktitle>
<contexts>
<context position="12572" citStr="Salakhutdinov et al., 2003" startWordPosition="2001" endWordPosition="2004">mputation in proportion to the size of the 584 parameter space. However, e stays fixed throughout the M-step. Algorithm 1 outlines EM in its entirety. The subroutine climb(·, ·, ·) represents a generic optimization step such as an LBFGS iteration. Algorithm 1 Feature-enhanced EM repeat Compute expected counts e D Eq. 2 repeat Compute ℓ(w, e) D Eq. 3 Compute Vℓ(w, e) D Eq. 4 w +— climb(w, ℓ(w, e), Vℓ(w, e)) until convergence until convergence 3.2 Direct Marginal Likelihood Optimization Another approach to optimizing Equation 1 is to compute the gradient of the log marginal likelihood directly (Salakhutdinov et al., 2003). The gradient turns out to have the same form as Equation 4, with the key difference that ed,,,t is recomputed for every different value of w. Algorithm 2 outlines the procedure. Justification for this algorithm appears in the Appendix. Algorithm 2 Feature-enhanced direct gradient repeat Compute expected counts e D Eq. 2 Compute L(w) D Eq. 1 Compute Vℓ(w, e) D Eq. 4 w +— climb(w, L(w), Vℓ(w, e)) until convergence In practice, we find that this optimization approach leads to higher task accuracy for several models. However, in cases where computing ed,,,t is expensive, EM can be a more efficie</context>
</contexts>
<marker>Salakhutdinov, Roweis, Ghahramani, 2003</marker>
<rawString>R. Salakhutdinov, S. Roweis, and Z. Ghahramani. 2003. Optimization with EM and expectation-conjugategradient. In International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data. In Association for Computational Linguistics.</title>
<date>2005</date>
<contexts>
<context position="2628" citStr="Smith and Eisner, 2005" startWordPosition="367" endWordPosition="370">require complex new machinery but which still provides substantial flexibility. In the featureengineering paradigm, one can worry less about the backbone structure and instead use hand-designed features to declaratively inject domain knowledge into a model. While feature engineering has historically been associated with discriminative, supervised learning settings, we argue that it can and should be applied more broadly to the unsupervised setting. The idea of using features in unsupervised learning is neither new nor even controversial. Many top unsupervised results use feature-based models (Smith and Eisner, 2005; Haghighi and Klein, 2006). However, such approaches have presented their own barriers, from challenging normalization problems, to neighborhood design, to the need for complex optimization procedures. As a result, most work still focuses on the stable and intuitive approach of using the EM algorithm to optimize data likelihood in locally normalized, generative models. The primary contribution of this paper is to demonstrate the clear empirical success of a simple and accessible approach to unsupervised learning with features, which can be optimized by using standard NLP building blocks. We c</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. A. Smith and J. Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>M Johnson</author>
</authors>
<title>Weighted and probabilistic context-free grammars are equally expressive. Computational Linguistics.</title>
<date>2007</date>
<contexts>
<context position="10218" citStr="Smith and Johnson (2007)" startWordPosition="1611" endWordPosition="1614">lly normalized models. For models parameterized by standard multinomials, EM optimizes L(0) = log Pe(Y = y) (Dempster et al., 1977). The E-step computes expected counts for each tuple of decision d, context c, and multinomial type t: L� Y = yJ ed,c,t �E� ✶(Xi =d, X�(i) =c, t) (2) i∈I 2The locally normalized model class is actually equivalent to its globally normalized counterpart when the former meets the following three conditions: (1) The graphical model is a directed tree. (2) The BASIC features are included in f. (3) We do not include regularization in the model (κ = 0). This follows from Smith and Johnson (2007). These expected counts are then normalized in the M-step to re-estimate 0: Bd,c,t +— Ed′ ed′,c,t ed,c,t Normalizing expected counts in this way maximizes the expected complete log likelihood with respect to the current model parameters. EM can likewise optimize L(w) for our locally normalized models with logistic parameterizations. The E-step first precomputes multinomial parameters from w for each decision, context, and type: exp(w, f(d, c, t)) Bd,c,t(w) � Ed′ exp(w, f(d′, c, t)) Then, expected counts e are computed according to Equation 2. In the case of POS induction, expected counts are c</context>
</contexts>
<marker>Smith, Johnson, 2007</marker>
<rawString>N. A. Smith and M. Johnson. 2007. Weighted and probabilistic context-free grammars are equally expressive. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I G Varea</author>
<author>F J Och</author>
<author>H Ney</author>
<author>F Casacuberta</author>
</authors>
<title>Refined lexicon models for statistical machine translation using a maximum entropy approach. In Association for Computational Linguistics.</title>
<date>2002</date>
<contexts>
<context position="23501" citStr="Varea et al., 2002" startWordPosition="3836" endWordPosition="3839">ssion model θy,e,ALIGN differentiates our feature-enhanced system from the baseline system. In the former, the emission model is a standard conditional multinomial that represents the probability that decision word y is generated from context word e, while in our system, the emission model is re-parameterized as a logistic regression model and feature-enhanced. Many supervised feature-based alignment models have been developed. In fact, this logistic parameterization of the HMM has been proposed before and yielded alignment improvements, but was trained using supervised estimation techniques (Varea et al., 2002).5 However, most full translation systems to5Varea et al. (2002) describes unsupervised EM optimization with logistic regression models at a high level—their dynamic training approach—but provides no experiments. 587 day rely on unsupervised learning so that the models may be applied easily to many language pairs. Our approach provides efficient and consistent unsupervised estimation for feature-rich alignment models. 6.2 Word Alignment Features The BASIC features on pairs of lexical items provide strong baseline performance. We add coarse features to the model in order to inject prior knowled</context>
</contexts>
<marker>Varea, Och, Ney, Casacuberta, 2002</marker>
<rawString>I. G. Varea, F. J. Och, H. Ney, and F. Casacuberta. 2002. Refined lexicon models for statistical machine translation using a maximum entropy approach. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F-D Chiou</author>
<author>M Palmer</author>
</authors>
<title>Building a large-scale annotated Chinese corpus.</title>
<date>2002</date>
<booktitle>In International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="19822" citStr="Xue et al., 2002" startWordPosition="3230" endWordPosition="3233"> the WSJ corpus. We work with a standard, reduced version of WSJ, WSJ10, that contains only sentences of length 10 or less after punctuation has been removed. We train on sections 2-21, and use section 22 as a development set. We report accuracy on section 23. These are the same training, development, and test sets used by Cohen and Smith (2009). The regularization parameter (n) is tuned on the development set to maximize accuracy. For our Chinese experiments, we use the same corpus and training/test split as Cohen and Smith 586 (2009). We train on sections 1-270 of the Penn Chinese Treebank (Xue et al., 2002), similarly reduced (CTB10). We test on sections 271-300 of CTB10, and use sections 400-454 as a development set. The DMV is known to be sensitive to initialization. We use the deterministic harmonic initializer from Klein and Manning (2004). We ran each optimization procedure for 100 iterations. The results are reported in Table 1. 5.4 Grammar Induction Results We are able to outperform Cohen and Smith’s (2009) best system, which requires a more complicated variational inference method, on both English and Chinese data sets. Their system achieves an accuracy of 61.3 for English and an accurac</context>
</contexts>
<marker>Xue, Chiou, Palmer, 2002</marker>
<rawString>N. Xue, F-D Chiou, and M. Palmer. 2002. Building a large-scale annotated Chinese corpus. In International Conference on Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>