<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000978">
<title confidence="0.996099">
Approximate Searching for Distributional Similarity
</title>
<author confidence="0.995167">
James Gorman and James R. Curran
</author>
<affiliation confidence="0.9952105">
School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.674149">
NSW 2006, Australia
</address>
<email confidence="0.99869">
{jgorman2,james}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.996657" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999893307692308">
Distributional similarity requires large
volumes of data to accurately represent
infrequent words. However, the nearest-
neighbour approach to finding synonyms
suffers from poor scalability. The Spa-
tial Approximation Sample Hierarchy
(SASH), proposed by Houle (2003b), is
a data structure for approximate nearest-
neighbour queries that balances the effi-
ciency/approximation trade-off. We have
intergrated this into an existing distribu-
tional similarity system, tripling efficiency
with a minor accuracy penalty.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919117647059">
With the development of WordNet (Fellbaum, 1998)
and large electronic thesauri, information from lex-
ical semantic resources is regularly used to solve
NLP problems. These problems include collocation
discovery (Pearce, 2001), smoothing and estimation
(Brown et al., 1992; Clark and Weir, 2001) and ques-
tion answering (Pasca and Harabagiu, 2001).
Unfortunately, these resources are expensive and
time-consuming to create manually, and tend to suf-
fer from problems of bias, inconsistency, and limited
coverage. In addition, lexicographers cannot keep
up with constantly evolving language use and can-
not afford to build new resources for the many sub-
domains that NLP techniques are being applied to.
There is a clear need for methods to extract lexical
semantic resources automatically or tools that assist
in their manual creation and maintenance.
</bodyText>
<page confidence="0.993602">
97
</page>
<bodyText confidence="0.99980955882353">
Much of the existing work on automatically ex-
tracting resources is based on the distributional hy-
pothesis that similar words appear in similar con-
texts. Existing approaches differ primarily in their
definition of “context”, e.g. the surrounding words
or the entire document, and their choice of distance
metric for calculating similarity between the vector
of contexts representing each term. Finding syn-
onyms using distributional similarity involves per-
forming a nearest-neighbour search over the context
vectors for each term. This is very computation-
ally intensive and scales according to the vocabulary
size and the number of contexts for each term. Cur-
ran and Moens (2002b) have demonstrated that dra-
matically increasing the quantity of text used to ex-
tract contexts significantly improves synonym qual-
ity. Unfortunately, this also increases the vocabulary
size and the number of contexts for each term, mak-
ing the use of huge datasets infeasible.
There have been many data structures and ap-
proximation algorithms proposed to reduce the com-
putational complexity of nearest-neighbour search
(Ch´avez et al., 2001). Many of these approaches re-
duce the search space by using clustering techniques
to generate an index of near-neighbours. We use the
Spacial Approximation Sample Hierarchy (SASH)
data structure developed by Houle (2003b) as it al-
lows more control over the efficiency-approximation
trade-off than other approximation methods.
This paper describes integrating the SASH into
an existing distributional similarity system (Cur-
ran, 2004). We show that replacing the nearest-
neighbour search improves efficiency by a factor of
three with only a minor accuracy penalty.
</bodyText>
<note confidence="0.9930975">
Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 97–104,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.931662" genericHeader="introduction">
2 Distributional Similarity
</sectionHeader>
<bodyText confidence="0.999847666666667">
Distributional similarity systems can be separated
into two components. The first component extracts
the contexts from raw text and compiles them into a
statistical description of the contexts each term ap-
pears in. The second component performs nearest-
neighbour search or clustering to determine which
terms are similar, based on distance calculations be-
tween their context vectors. The approach used in
this paper follows Curran (2004).
</bodyText>
<subsectionHeader confidence="0.992547">
2.1 Extraction Method
</subsectionHeader>
<bodyText confidence="0.999980571428571">
A context relation is defined as a tuple (w, r, w′)
where w is a term, which occurs in some grammati-
cal relation r with another word w′ in some sentence.
We refer to the tuple (r, w′) as an attribute of w. For
example, (dog, diect-obj, walk) indicates that dog
was the direct object of walk in a sentence.
Context extraction begins with a Maximum En-
tropy POS tagger and chunker (Ratnaparkhi, 1996).
The Grefenstette (1994) relation extractor produces
context relations that are then lemmatised using the
Minnen et al. (2000) morphological analyser. The
relations for each term are collected together and
counted, producing a context vector of attributes and
their frequencies in the corpus.
</bodyText>
<subsectionHeader confidence="0.998223">
2.2 Measures and Weights
</subsectionHeader>
<bodyText confidence="0.999883714285714">
Both nearest-neighbour and cluster analysis meth-
ods require a distance measure that calculates the
similarity between context vectors. Curran (2004)
decomposes this measure into measure and weight
functions. The measure function calculates the sim-
ilarity between two weighted context vectors and the
weight function calculates a weight from the raw fre-
quency information for each context relation.
The SASH requires a distance measure that pre-
serves metric space (see Section 4.1). For these ex-
periments we use the JACCARD (1) measure and the
TTEST (2) weight, as Curran and Moens (2002a)
found them to have the best performance in their
comparison /of many distance measures.
</bodyText>
<equation confidence="0.989513333333333">
1(r,w′) min(Wgt(wm, ∗r, ∗w′), wgt(Wn, ∗r, ∗w′))
max/W (/W * ∗w′), tw
m, r, wg(n, ∗r, ∗w′ 1
�(r,w′) l �l))
p(w, r, w′) − p(∗, r, w′)p(w, ∗, ∗) (2)
yp(∗, r, w′)p(w, ∗, ∗)
</equation>
<sectionHeader confidence="0.978552" genericHeader="method">
3 Nearest-neighbour search
</sectionHeader>
<bodyText confidence="0.999954111111111">
The simplest algorithm for finding synonyms is
nearest-neighbour search, which involves pairwise
vector comparison of the target term with every term
in the vocabulary. Given an n term vocabulary and
up to m attributes for each term, the asymptotic time
complexity of nearest-neighbour search is O(n2m).
This is very expensive with even a moderate vocab-
ulary and small attribute vectors making the use of
huge datasets infeasible.
</bodyText>
<subsectionHeader confidence="0.996004">
3.1 Heuristic
</subsectionHeader>
<bodyText confidence="0.999929">
Using cutoff to remove low frequency terms can sig-
nificantly reduce the value of n. In these experi-
ments, we used a cutoff of 5. However, a solution
is still needed to reduce the factor m. Unfortunately,
reducing m by eliminating low frequency contexts
has a significant impact on the quality of the results.
Curran and Moens (2002a) propose an initial
heuristic comparison to reduce the number of
full O(m) vector comparisons. They introduce a
bounded vector (length k) of canonical attributes,
selected from the full vector, to represent the
term. The selected attributes are the most strongly
weighted verb attributes: Curran and Moens chose
these relations as they generally constrain the se-
mantics of the term more and partake in fewer id-
iomatic collocations.
If a pair of terms share at least one canonical
attribute then a full similarity comparison is per-
formed, otherwise the terms are not considered sim-
ilar. If a maximum of p positive results are returned,
our complexity becomes O(n2k+npm), which, since
k is constant, is O(n2 + npm).
</bodyText>
<sectionHeader confidence="0.988708" genericHeader="method">
4 The SASH
</sectionHeader>
<bodyText confidence="0.999676909090909">
The SASH approximates a nearest-neighbour search
by pre-computing some of the near-neighbours of
each node (terms in our case). It is arranged as a
multi-leveled pyramid, where each node is linked
to its (approximate) near-neighbours on the levels
above and below. This produces multiple paths be-
tween nodes, allowing the SASH to shape itself to
the data set (Houle, 2003a). This graph is searched
by finding the near-neighbours of the target node
at each level. The following description is adapted
from Houle (2003b).
</bodyText>
<page confidence="0.99878">
98
</page>
<figure confidence="0.999687571428571">
A
1
B C D
E F G H
I J
5
K L
</figure>
<figureCaption confidence="0.964269">
Figure 1: A SASH, where p = 2, c = 3 and k = 2
</figureCaption>
<figure confidence="0.945191">
2
3
4
</figure>
<subsectionHeader confidence="0.996109">
4.1 Metric Spaces
</subsectionHeader>
<bodyText confidence="0.999913571428572">
The SASH organises nodes that can be measured in
metric space. Although it is not necessary for the
SASH to work, only in this space can performance
be guaranteed. Our meaures produce a metric-like
space for the terms derived from large datasets.
A domain D is a metric space if there exists a
function dist : D x D —� R&gt;0 such that:
</bodyText>
<listItem confidence="0.9974986">
1. dist(p, q) &gt; 0 b p, q E D (non-negativity)
2. dist(p, q) = 0 iff p = q b p, q E D (equality)
3. dist(p, q) = dist(q, p) b p, q E D (symmetry)
4. dist(p, q) + dist(q, r) &gt; dist(p, r)
b p, q, r E D (triangle inequality)
</listItem>
<bodyText confidence="0.9954693">
We invert the similarity measure to produce a dis-
tance, resulting in condition 2 not being satisfied
since dist(p, p) = x, x &gt; 0. For most measures x
is constant, so dist(p, q) &gt; dist(p, p) if p # q and p
and q do not occur in exactly the same contexts. For
some measures, e.g. DICE, dist(p, p) &gt; dist(p, q),
that is, p is closer to q than it is to itself. These do
not preserve metric space in any way, so cannot be
used with the SASH.
Ch´avez et al. (2001) divides condition 2 into:
</bodyText>
<listItem confidence="0.992258333333333">
5. dist(p, p) = 0 b p E D (reflexivity)
6. dist(p,q) &gt; 0 iff p # q b p,q E D
(strict positiveness)
</listItem>
<bodyText confidence="0.9913965">
If strict positiveness is not satisfied the space is
called pseudometric. In theory, our measures do not
satisfy this condition, however in practice most large
datasets will satisfy this condition.
</bodyText>
<subsectionHeader confidence="0.992124">
4.2 Structure
</subsectionHeader>
<bodyText confidence="0.996908">
The SASH is a directed, edge-weighted graph with
the following properties:
</bodyText>
<listItem confidence="0.95153">
• Each term corresponds to a unique node.
• The nodes are arranged into a hierarchy of lev-
</listItem>
<bodyText confidence="0.6082082">
els, with the bottom level containing n2 nodes
and the top containing a single root node. Each
level, except the top, will contain half as many
nodes as the level below. These are numbered
from 1 (top) to h.
</bodyText>
<listItem confidence="0.989749">
• Edges between nodes are linked from consecu-
tive levels. Each node will have at most p par-
ent nodes in the level above, and c child nodes
in the level below.
• Every node must have at least one parent so that
all nodes are reachable from the root.
</listItem>
<bodyText confidence="0.795227">
Figure 1 shows a SASH which will be used below.
</bodyText>
<subsectionHeader confidence="0.997381">
4.3 Construction
</subsectionHeader>
<bodyText confidence="0.999899928571429">
The SASH is constructed iteratively by finding the
nearest parents in the level above. The nodes are
first randomly distributed to reduce any clustering
effects. They are then split into the levels described
above, with level h having n2 nodes, level 2 at most c
nodes and level 1 having a single root node.
The root node has all nodes at level 2 as children
and each node at level 2 has the root as its sole par-
ent. Then for each node in each level i from 3 to
h, we find the set of p nearest parent nodes in level
(i − 1). The node then asks that parent if it can be
a child. As only the closest c nodes can be children
of a node, it may be the case that a requested parent
rejects a child.
</bodyText>
<page confidence="0.994817">
99
</page>
<table confidence="0.9992826">
DIST c LOAD TIME
RANDOM 16 21.0hr
RANDOM 64 15.6hr
RANDOM 128 21.1hr
FOLD1500 16 50.2hr
FOLD1500 64 33.4hr
FOLD1500 128 25.7hr
SORT 16 75.5hr
SORT 64 23.8hr
SORT 128 33.8hr
</table>
<tableCaption confidence="0.999789">
Table 1: Load time distributions and values of c
</tableCaption>
<bodyText confidence="0.796747833333333">
If a child is left without any parents it is said to be
orphaned. Any orphaned nodes must now find the
closest node in the above level that has fewer than
c children. Once all nodes have at least one parent,
we move to the next level. This proceeds iteratively
through the levels.
</bodyText>
<subsectionHeader confidence="0.993972">
4.4 Search
</subsectionHeader>
<bodyText confidence="0.991371">
Searching the SASH is also performed iteratively. To
find the k nearest neighbours of a node q, we first
find the k nearest neighbours at each level. At level 1
we take the single root node to be nearest. Then, for
each level after, we find the k nearest unique children
of the nodes found in the level above. When the
last level has been searched, we return the closest k
nodes from all the sets of near neighbours returned.
In Figure 1, the filled nodes demonstrate a search
for the near-neighbours of some node q, using k = 2.
Our search begins with the root node A. As we are
using k = 2, we must find the two nearest children of
A using our similarity measure. In this case, C and
D are closer than B. We now find the closest two
children of C and D. E is not checked as it is only
a child of B. All other nodes are checked, including
F and G, which are shared as children by B and C.
From this level we chose G and H. We then consider
the fourth and fifth levels similarly.
At this point we now have the list of near nodes
A, C, D, G, H, I, J, K and L. From this we chose
the two nodes closest to q: H and I marked in black.
These are returned as the near-neighbours of q.
k can also be varied at each level to force a larger
number of elements to be tested at the base of the
SASH using, for instance, the equation:
</bodyText>
<equation confidence="0.988198">
1− h−i 1
ki = max{ k log2 n , 2 pc } (3)
</equation>
<bodyText confidence="0.974998">
We use this geometric function in our experiments.
</bodyText>
<subsectionHeader confidence="0.894206">
4.5 Complexity
</subsectionHeader>
<bodyText confidence="0.919446375">
When measuring the time complexity, we consider
the number of distance measurements as these dom-
inate the computation. If we do not consider the
problem of assigning parents to orphans, for n
nodes, p parents per child, at most c children per
parent and a search returning k elements, the loose
upper bounds are:
SASH construction
</bodyText>
<equation confidence="0.995153">
pcn log2 n (4)
Approx. k-NN query (uniform)
ck log2 n (5)
Approx. k-NN query (geometric)
log2 n (6)
</equation>
<bodyText confidence="0.999942">
Since the average number of children per node is
approximately 2p, practical complexities can be de-
rived using c = 2p.
In Houle’s experiments, typically less than 5% of
computation time was spent assigning parents to or-
phans, even for relatively small c. In some of our
experiments we found that low values of c produced
significantly worse load times that for higher values,
but this was highly dependant on the distribution of
nodes. Table 1 shows this with respect to several
distributions and values of c.
</bodyText>
<sectionHeader confidence="0.998549" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.998357888888889">
The simplest method of evaluation is direct com-
parison of the extracted synonyms with a manually-
created gold standard (Grefenstette, 1994). How-
ever, on small corpora, rare direct matches provide
limited information for evaluation, and thesaurus
coverage is a problem. Our evaluation uses a com-
bination of three electronic thesauri: the Macquarie
(Bernard, 1990), Roget’s (Roget, 1911) and Moby
(Ward, 1996) thesauri.
</bodyText>
<equation confidence="0.59341075">
pc2
+
−1 2
1
k1+log2 n
1
k
log2 n
</equation>
<page confidence="0.875603">
100
</page>
<bodyText confidence="0.9998285">
With this gold standard in place, it is possible
to use precision and recall measures to evaluate the
quality of the extracted thesaurus. To help overcome
the problems of direct comparisons we use several
measures of system performance: direct matches
(DIRECT), inverse rank (INVR), and precision of the
top n synonyms (P(n)), for n = 1, 5 and 10.
INVR is the sum of the inverse rank of each
matching synonym, e.g. matching synonyms at
ranks 3, 5 and 28 give an inverse rank score of
</bodyText>
<equation confidence="0.7061365">
3 + 1
1 5 + 128, and with at most 100 synonyms, the max-
</equation>
<bodyText confidence="0.998645714285714">
imum INVR score is 5.187. Precision of the top n is
the percentage of matching synonyms in the top n
extracted synonyms.
The same 70 single-word nouns were used for the
evaluation as in Curran and Moens (2002a). These
were chosen randomly from WordNet such that they
covered a range over the following properties:
</bodyText>
<sectionHeader confidence="0.419869" genericHeader="method">
InvR
</sectionHeader>
<bodyText confidence="0.993092833333333">
frequency Penn Treebank and BNC frequencies;
number of senses WordNet and Macquarie senses;
specificity depth in the WordNet hierarchy;
concreteness distribution across WordNet subtrees.
For each of these terms, the closest 100 terms and
their similarity score were extracted.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999346842105263">
The contexts were extracted from the non-speech
portion of the British National Corpus (Burnard,
1995). All experiments used the JACCARD measure
function, the TTEST weight function and a cutoff
frequency of 5. The SASH was constructed using the
geometric equation for ki described in Section 4.4.
When the heuristic was applied, the TTESTLOG
weight function was used with a canonical set size
of 100 and a maximum frequency cutoff of 10,000.
The values 4–16, 8–32, 16–64, and 32–128 were
chosen for p and c. This gives a range of branch-
ing factors to test the balance between sparseness,
where there is potential for erroneous fragmentation
of large clusters, and bushiness, where more tests
must be made to find near children. The c = 4p re-
lationship is derived from the simple hashing rule
of thumb that says that a hash table should have
roughly twice the size required to store all its ele-
ments (Houle, 2003b).
</bodyText>
<table confidence="0.952244571428572">
DIST FREQUENCY # RELATIONS
Mean Median Mean Median
RANDOM 342 18 126 13
FOLD500 915 865.5 500 500
FOLD1000 2155 1970.5 1001 1001.5
FOLD1500 3656 3444 1506 1510.5
SORT 44753 37937.5 8290 7583.5
</table>
<figure confidence="0.844299">
0 1000 2000 3000 4000 5000 6000 7000
Avg Search Time (ms)
</figure>
<figureCaption confidence="0.99923">
Figure 2: INVR against average search time
</figureCaption>
<bodyText confidence="0.999976833333333">
Our initial experiments showed that the random
distribution of nodes (RANDOM) in SASH construc-
tion caused the nearest-neighbour approximation to
be very inaccurate for distributional similarity. Al-
though the speed was improved by two orders of
magnitude when c = 16, it achieved only 13% of the
INVR of the naive implementation. The best RAN-
DOM result was less than three times faster then the
naive solution and only 60% INVR.
In accordance with Zipf’s law the majority of
terms have very low frequencies. Similarity mea-
surements made against these low frequency terms
are less reliable, as accuracy increases with the num-
ber of relations and their frequencies (Curran and
Moens, 2002b). This led to the idea that ordering
the nodes by frequency before generating the SASH
would improve accuracy.
The SASH was then generated with the highest
frequency terms were near the root so that the initial
search paths would be more accurate. This has the
unfortunate side-effect of slowing search by up to
four times because comparisons with high frequency
terms take longer than with low frequency terms as
they have a larger number of relations.
</bodyText>
<figure confidence="0.967443428571429">
Table 2: Top
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
3 SASH level averages with c = 128
random
fold1500
sort
</figure>
<page confidence="0.98888">
101
</page>
<table confidence="0.999826375">
DIST c DIRECT P(1) P(5) P(10) INVR SEARCH TIME
NAIVE 2.83 49% 41% 32% 1.43 12217ms
RANDOM 16 0.17 9% 6% 3% 0.18 13% 120ms
RANDOM 64 1.09 30% 21% 15% 0.72 50% 1388ms
RANDOM 128 1.53 31% 24% 20% 0.86 60% 4488ms
SORT 16 1.51 33% 25% 20% 0.90 63% 490ms
SORT 64 2.55 47% 38% 31% 1.34 94% 2197ms
SORT 128 2.81 49% 41% 33% 1.43 100% 6960ms
</table>
<tableCaption confidence="0.999955">
Table 3: Evaluation of different random and fully sorted distributions
</tableCaption>
<bodyText confidence="0.999954105263158">
This led to updating our original frequency order-
ing idea by recognising that we did not need the most
accurately comparable terms at the top of the SASH,
only more accurately comparable terms than those
randomly selected.
As a first attempt, we constructed SASHs with fre-
quency orderings that were folded about a chosen
number of relations M. For each term, if its num-
ber of relations mi was greater than M, it was given
a new ranking based on the score M2
mi. In this way,
very high and very low frequency terms were pushed
away from the root. The folding points this was
tested for were 500, 1000 and 1500. There are many
other node organising schemes we are yet to explore.
The frequency distributions over the top three lev-
els for each ordering scheme are shown in Table 2.
Zipf’s law results in a large difference between the
mean and median frequency values in the RANDOM
results: most of the nodes have low frequency, but
some high frequency results push the mean up. The
four-fold reduction in efficiency for SORT (see Ta-
ble 3) is a result of the mean number of relations
being over 65 times that of RANDOM.
Experiments covering the full set of permutations
of these parameters were run, with and without the
heuristic applied. In the cases where the heuristic
rejected pairs of terms, the SASH treated the rejected
pairs as being as infinitely far apart. In addition, the
brute force solutions were generated with (NAIVE
HEURISTIC) and without (NAIVE) the heuristic.
We have assumed that all weights and measures
introduce similar distribution properties into the
SASH, so that the best weight and measure when per-
forming a brute-force search will also produce the
best results when combined with the SASH. Future
experiments will explore SASH behaviour with other
similarity measures.
</bodyText>
<sectionHeader confidence="0.999615" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.9998382">
Table 3 presents the results for the initial experi-
ments. SORT was consistently more accurate than
RANDOM, and when c = 128, performed as well as
NAIVE for all evaluation measures except for direct
matches. Both SASH solutions outperformed NAIVE
in efficiency.
The trade-off between efficiency and approxima-
tion accuracy is evident in these results. The most
efficient result is 100 times faster than NAIVE, but
only 13% accurate on INVR, with 6% of direct
matches. The most accurate result is 100% accu-
rate on INVR, with 99% of direct matches, but is
less than twice as fast.
Table 4 shows the trade-off for folded distribu-
tions. The least accurate FOLD500 result is 30%
accurate but 50 times faster than NAIVE, while the
most accurate is 87% but less than two times faster.
The least accurate FOLD1500 result is 43% accurate
but 71 times faster than NAIVE, while the most ac-
curate is 101% and two and half times faster. These
results show the impact of moving high frequency
terms away from the root.
Figure 2 plots the trade-off using search time and
INVR at c = 16, 32, 64 and 128. For c = 16 every
SASH has very poor accuracy. By c = 64 their ac-
curacy has improved dramatically, but their search
time also increased somewhat. At c = 128, there
is only a small improvement in accuracy, coinciding
with a large increase in search time. The best trade-
off between efficiency and approximation accuracy
occurs at the knee of the curve where c = 64.
When c = 128 both SORT and FOLD1500 perform
as well as, or slightly outperform NAIVE on some
evaluation measures. These evaluation measures in-
volve the rank of correct synonyms, so if the SASH
</bodyText>
<page confidence="0.991329">
102
</page>
<table confidence="0.9998397">
DIST c DIRECT P(1) P(5) P(10) INVR SEARCH TIME
FOLD500 16 0.53 23% 11% 8% 0.43 30% 243ms
FOLD500 64 1.69 49% 29% 23% 1.09 76% 2880ms
FOLD500 128 2.29 50% 35% 27% 1.25 87% 6848ms
FOLD1000 16 0.61 29% 14% 9% 0.51 35% 228ms
FOLD1000 64 2.07 49% 36% 26% 1.21 84% 3192ms
FOLD1000 128 2.57 50% 39% 31% 1.40 98% 4330ms
FOLD1500 16 0.90 30% 17% 13% 0.62 43% 171ms
FOLD1500 64 2.36 57% 39% 30% 1.36 95% 3193ms
FOLD1500 128 2.67 53% 42% 32% 1.44 101% 4739ms
</table>
<tableCaption confidence="0.999894">
Table 4: Evaluation of folded distributions
</tableCaption>
<bodyText confidence="0.999966320000001">
approximation was to fail to find some incorrectly
proposed synonyms ranked above some other cor-
rect synonyms, those correct synonyms would have
their ranking pushed up. In this way, the approxima-
tion can potentially outperform the original nearest-
neighbour algorithm.
From Tables 3 and 4 we also see that as the value
of c increases, so does the accuracy across all of
the experiments. This is because as c increases the
number of paths between nodes increases and we
have a solution closer to a true nearest-neighbour
search, that is, there are more ways of finding the
true nearest-neighbour nodes.
Table 5 presents the results of combining the
canonical attributes heuristic (see Section 3.1) with
the SASH approximation. This NAIVE HEURISTIC is
14 times faster than NAIVE and 97% accurate, with
96% of direct matches. The combination has com-
parable accuracy and is much more efficient than the
best of the SASH solutions. The best heuristic SASH
results used the SORT ordering with c = 16, which
was 37 times faster than NAIVE and 2.5 times faster
than NAIVE HEURISTIC. Its performance was statis-
tically indistinguishable from NAIVE HEURISTIC.
Using the heuristic changes the impact of the
number of children c on the SASH performance char-
acteristics. It seems that beyond c = 16 the only
significant effect is to reduce the efficiency (often to
slower than NAIVE HEURISTIC).
The heuristic interacts in an interesting way with
the ordering of the nodes in the SASH. This is most
obvious with the RANDOM results. The RANDOM
heuristic INVR results are eight times better than the
full RANDOM results. Similar, though less dramatic,
results are seen with other orderings. It appears that
using the heuristic changes the clustering of nearest-
neighbours within the SASH so that better matching
paths are chosen and more noisy matches are elimi-
nated entirely by the heuristic.
It may seem that there are no major advantages
to using the SASH with the already efficient heuris-
tic matching method. However, our experiments
have used small canonical attribute vectors (maxi-
mum length 100). Increasing the canonical vector
size allows us to increase the accuracy of heuristic
solutions at the cost of efficiency. Using a SASH so-
lution would offset some of this efficiency penalty.
This has the potential for a solution that is more than
an order of magnitude faster than NAIVE and is al-
most as accurate.
</bodyText>
<sectionHeader confidence="0.997654" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999475625">
We have integrated a nearest-neighbour approxima-
tion data structure, the Spacial Approximation Sam-
ple Hierarchy (SASH), with a state-of-the-art distri-
butional similarity system. In the process we have
extended the original SASH construction algorithms
(Houle, 2003b) to deal with the non-uniform distri-
bution of words within semantic space.
We intend to test other similarity measures and
node ordering strategies, including a more linguistic
analysis using WordNet, and further explore the in-
teraction between the canonical vector heuristic and
the SASH. The larger 300 word evaluation set used
by Curran (2004) will be used, and combined with a
more detailed analyis. Finally, we plan to optimise
our SASH implementation so that it is comparable
with the highly optimised nearest-neighbour code.
</bodyText>
<page confidence="0.997823">
103
</page>
<table confidence="0.999847181818182">
DIST c DIRECT P(1) P(5) P(10) INVR SEARCH TIME
NAIVE HEURISTIC 2.72 49% 40% 32% 1.40 827ms
RANDOM 16 2.61 50% 40% 31% 1.39 99% 388ms
RANDOM 64 2.72 49% 40% 32% 1.40 100% 1254ms
RANDOM 128 2.71 49% 40% 32% 1.40 100% 1231ms
FOLD1500 16 2.53 49% 40% 31% 1.36 97% 363ms
FOLD1500 64 2.72 49% 40% 32% 1.40 100% 900ms
FOLD1500 128 2.72 49% 40% 32% 1.40 100% 974ms
SORT 16 2.78 49% 40% 32% 1.41 100% 323ms
SORT 64 2.73 49% 40% 32% 1.40 100% 1238ms
SORT 128 2.73 49% 40% 32% 1.40 100% 1049ms
</table>
<tableCaption confidence="0.999838">
Table 5: Evaluation of different distributions using the approximation
</tableCaption>
<bodyText confidence="0.998592333333333">
The result is distributional similarity calculated
three times faster than existing systems with only a
minor accuracy penalty.
</bodyText>
<sectionHeader confidence="0.997971" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99715325">
We would like to thank the anonymous reviewers
for their helpful feedback and corrections. This
work has been supported by the Australian Research
Council under Discovery Project DP0453131.
</bodyText>
<sectionHeader confidence="0.998626" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999891950819672">
John R. L. Bernard, editor. 1990. The Macquarie Encyclopedic
Thesaurus. The Macquarie Library, Sydney, Australia.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jen-
nifer C. Lai, and Robert L. Mercer. 1992. Class-based n-
gram models of natural language. Computational Linguis-
tics, 18(4):467–479, December.
Lou Burnard, editor. 1995. Users Reference Guide British Na-
tional Corpus Version 1.0. Oxford University Computing
Services.
Edgar Ch´avez, Gonzalo Navarro, Ricardo Baeza-Yates, and
Jos´e L. Marroquin. 2001. Searching in metric spaces. ACM
Computing Surveys, 33(3):273–321, September.
Stephen Clark and David Weir. 2001. Class-based probability
estimation using a semantic hierarchy. In Proceedings of the
Second Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 95–102, Pitts-
burgh, PA USA, 2–7 June.
James R. Curran and Marc Moens. 2002a. Improvements
in automatic thesaurus extraction. In Proceedings of the
Workshop of the ACL Special Interest Group on the Lexicon
(SIGLEX), pages 59–66, Philadelphia, USA, 12 July.
James R. Curran and Marc Moens. 2002b. Scaling context
space. In Proceedings of the 40th annual meeting of the
Association for Computational Linguistics, pages 231–238,
Philadelphia, USA, 7–12 July.
James R. Curran. 2004. From Distributional to Semantic Simi-
larity. Ph.D. thesis, University of Edinburgh.
Christiane Fellbaum, editor. 1998. WordNet: an electronic lex-
ical database. The MIT Press, Cambridge, MA USA.
Gregory Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers, Boston,
USA.
Michael E. Houle. 2003a. Navigating massive data sets via lo-
cal clustering. In Proceedings of the ninth ACM SIGKDD
international conference on Knowledge discovery and data
mining, pages 547–552, Washington, DC, USA, 24–27 Au-
gust.
Michael E. Houle. 2003b. SASH: a saptial approximation
sample hierarchy for similarity search. Technical Report
RT0517, IBM Reasearch, Tokyo Research Laboratory, Yam-
ato Kanagawa, Japan, March.
Guido Minnen, John Carroll, and Darren Pearce. 2000. Ro-
bust applied morphological generation. In Proceedings of
the First International Natural Language Generation Con-
ference, pages 201–208, Mitzpe Ramon, Israel, 12–16 June.
Marius Pasca and Sanda Harabagiu. 2001. The informative
role of wordnet in open-domain question answering. In Pro-
ceedings of the Workshop on WordNet and Other Lexical
Resources: Applications, Extensions and Customizations,
pages 138–143, Pittsburgh, PA USA, 2–7 June.
Darren Pearce. 2001. Synonymy in collocation extraction. In
Proceedings of the Workshop on WordNet and Other Lex-
ical Resources: Applications, Extensions and Customiza-
tions, pages 41–46, Pittsburgh, PA USA, 2–7 June.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages 133–
142, 17–18 May.
Peter Roget. 1911. Thesaurus of English words and phrases.
Longmans, Green and Co., London, UK.
Grady Ward. 1996. Moby Thesaurus. Moby Project.
</reference>
<page confidence="0.998786">
104
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.499227">
<title confidence="0.998451">Approximate Searching for Distributional Similarity</title>
<author confidence="0.999721">R Gorman</author>
<affiliation confidence="0.9868975">School of Information University of</affiliation>
<address confidence="0.514497">NSW 2006,</address>
<abstract confidence="0.997657928571429">Distributional similarity requires large volumes of data to accurately represent infrequent words. However, the nearestneighbour approach to finding synonyms suffers from poor scalability. The Spatial Approximation Sample Hierarchy proposed by Houle (2003b), is a data structure for approximate nearestneighbour queries that balances the efficiency/approximation trade-off. We have intergrated this into an existing distributional similarity system, tripling efficiency with a minor accuracy penalty.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John R L Bernard</author>
<author>editor</author>
</authors>
<date>1990</date>
<booktitle>The Macquarie Encyclopedic Thesaurus. The Macquarie Library,</booktitle>
<location>Sydney, Australia.</location>
<marker>Bernard, editor, 1990</marker>
<rawString>John R. L. Bernard, editor. 1990. The Macquarie Encyclopedic Thesaurus. The Macquarie Library, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based ngram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="996" citStr="Brown et al., 1992" startWordPosition="127" endWordPosition="130">scalability. The Spatial Approximation Sample Hierarchy (SASH), proposed by Houle (2003b), is a data structure for approximate nearestneighbour queries that balances the efficiency/approximation trade-off. We have intergrated this into an existing distributional similarity system, tripling efficiency with a minor accuracy penalty. 1 Introduction With the development of WordNet (Fellbaum, 1998) and large electronic thesauri, information from lexical semantic resources is regularly used to solve NLP problems. These problems include collocation discovery (Pearce, 2001), smoothing and estimation (Brown et al., 1992; Clark and Weir, 2001) and question answering (Pasca and Harabagiu, 2001). Unfortunately, these resources are expensive and time-consuming to create manually, and tend to suffer from problems of bias, inconsistency, and limited coverage. In addition, lexicographers cannot keep up with constantly evolving language use and cannot afford to build new resources for the many subdomains that NLP techniques are being applied to. There is a clear need for methods to extract lexical semantic resources automatically or tools that assist in their manual creation and maintenance. 97 Much of the existing </context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based ngram models of natural language. Computational Linguistics, 18(4):467–479, December.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Users Reference Guide British National Corpus Version 1.0.</booktitle>
<editor>Lou Burnard, editor.</editor>
<institution>Oxford University Computing Services.</institution>
<marker>1995</marker>
<rawString>Lou Burnard, editor. 1995. Users Reference Guide British National Corpus Version 1.0. Oxford University Computing Services.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edgar Ch´avez</author>
<author>Gonzalo Navarro</author>
<author>Ricardo Baeza-Yates</author>
<author>Jos´e L Marroquin</author>
</authors>
<title>Searching in metric spaces.</title>
<date>2001</date>
<journal>ACM Computing Surveys,</journal>
<volume>33</volume>
<issue>3</issue>
<marker>Ch´avez, Navarro, Baeza-Yates, Marroquin, 2001</marker>
<rawString>Edgar Ch´avez, Gonzalo Navarro, Ricardo Baeza-Yates, and Jos´e L. Marroquin. 2001. Searching in metric spaces. ACM Computing Surveys, 33(3):273–321, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>95--102</pages>
<location>Pittsburgh, PA USA,</location>
<contexts>
<context position="1019" citStr="Clark and Weir, 2001" startWordPosition="131" endWordPosition="134">tial Approximation Sample Hierarchy (SASH), proposed by Houle (2003b), is a data structure for approximate nearestneighbour queries that balances the efficiency/approximation trade-off. We have intergrated this into an existing distributional similarity system, tripling efficiency with a minor accuracy penalty. 1 Introduction With the development of WordNet (Fellbaum, 1998) and large electronic thesauri, information from lexical semantic resources is regularly used to solve NLP problems. These problems include collocation discovery (Pearce, 2001), smoothing and estimation (Brown et al., 1992; Clark and Weir, 2001) and question answering (Pasca and Harabagiu, 2001). Unfortunately, these resources are expensive and time-consuming to create manually, and tend to suffer from problems of bias, inconsistency, and limited coverage. In addition, lexicographers cannot keep up with constantly evolving language use and cannot afford to build new resources for the many subdomains that NLP techniques are being applied to. There is a clear need for methods to extract lexical semantic resources automatically or tools that assist in their manual creation and maintenance. 97 Much of the existing work on automatically e</context>
</contexts>
<marker>Clark, Weir, 2001</marker>
<rawString>Stephen Clark and David Weir. 2001. Class-based probability estimation using a semantic hierarchy. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics, pages 95–102, Pittsburgh, PA USA, 2–7 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Marc Moens</author>
</authors>
<title>Improvements in automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon (SIGLEX),</booktitle>
<pages>59--66</pages>
<location>Philadelphia, USA,</location>
<contexts>
<context position="2251" citStr="Curran and Moens (2002" startWordPosition="320" endWordPosition="324">resources is based on the distributional hypothesis that similar words appear in similar contexts. Existing approaches differ primarily in their definition of “context”, e.g. the surrounding words or the entire document, and their choice of distance metric for calculating similarity between the vector of contexts representing each term. Finding synonyms using distributional similarity involves performing a nearest-neighbour search over the context vectors for each term. This is very computationally intensive and scales according to the vocabulary size and the number of contexts for each term. Curran and Moens (2002b) have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. Unfortunately, this also increases the vocabulary size and the number of contexts for each term, making the use of huge datasets infeasible. There have been many data structures and approximation algorithms proposed to reduce the computational complexity of nearest-neighbour search (Ch´avez et al., 2001). Many of these approaches reduce the search space by using clustering techniques to generate an index of near-neighbours. We use the Spacial Approximation Sam</context>
<context position="5197" citStr="Curran and Moens (2002" startWordPosition="774" endWordPosition="777">he corpus. 2.2 Measures and Weights Both nearest-neighbour and cluster analysis methods require a distance measure that calculates the similarity between context vectors. Curran (2004) decomposes this measure into measure and weight functions. The measure function calculates the similarity between two weighted context vectors and the weight function calculates a weight from the raw frequency information for each context relation. The SASH requires a distance measure that preserves metric space (see Section 4.1). For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison /of many distance measures. 1(r,w′) min(Wgt(wm, ∗r, ∗w′), wgt(Wn, ∗r, ∗w′)) max/W (/W * ∗w′), tw m, r, wg(n, ∗r, ∗w′ 1 �(r,w′) l �l)) p(w, r, w′) − p(∗, r, w′)p(w, ∗, ∗) (2) yp(∗, r, w′)p(w, ∗, ∗) 3 Nearest-neighbour search The simplest algorithm for finding synonyms is nearest-neighbour search, which involves pairwise vector comparison of the target term with every term in the vocabulary. Given an n term vocabulary and up to m attributes for each term, the asymptotic time complexity of nearest-neighbour search is O(n2m). This is v</context>
<context position="14443" citStr="Curran and Moens (2002" startWordPosition="2503" endWordPosition="2506">d thesaurus. To help overcome the problems of direct comparisons we use several measures of system performance: direct matches (DIRECT), inverse rank (INVR), and precision of the top n synonyms (P(n)), for n = 1, 5 and 10. INVR is the sum of the inverse rank of each matching synonym, e.g. matching synonyms at ranks 3, 5 and 28 give an inverse rank score of 3 + 1 1 5 + 128, and with at most 100 synonyms, the maximum INVR score is 5.187. Precision of the top n is the percentage of matching synonyms in the top n extracted synonyms. The same 70 single-word nouns were used for the evaluation as in Curran and Moens (2002a). These were chosen randomly from WordNet such that they covered a range over the following properties: InvR frequency Penn Treebank and BNC frequencies; number of senses WordNet and Macquarie senses; specificity depth in the WordNet hierarchy; concreteness distribution across WordNet subtrees. For each of these terms, the closest 100 terms and their similarity score were extracted. 6 Experiments The contexts were extracted from the non-speech portion of the British National Corpus (Burnard, 1995). All experiments used the JACCARD measure function, the TTEST weight function and a cutoff freq</context>
<context position="16739" citStr="Curran and Moens, 2002" startWordPosition="2881" endWordPosition="2884">nodes (RANDOM) in SASH construction caused the nearest-neighbour approximation to be very inaccurate for distributional similarity. Although the speed was improved by two orders of magnitude when c = 16, it achieved only 13% of the INVR of the naive implementation. The best RANDOM result was less than three times faster then the naive solution and only 60% INVR. In accordance with Zipf’s law the majority of terms have very low frequencies. Similarity measurements made against these low frequency terms are less reliable, as accuracy increases with the number of relations and their frequencies (Curran and Moens, 2002b). This led to the idea that ordering the nodes by frequency before generating the SASH would improve accuracy. The SASH was then generated with the highest frequency terms were near the root so that the initial search paths would be more accurate. This has the unfortunate side-effect of slowing search by up to four times because comparisons with high frequency terms take longer than with low frequency terms as they have a larger number of relations. Table 2: Top 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 3 SASH level averages with c = 128 random fold1500 sort 101 DIST c DIRECT P(1) P(5) P(10) INVR SEAR</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James R. Curran and Marc Moens. 2002a. Improvements in automatic thesaurus extraction. In Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon (SIGLEX), pages 59–66, Philadelphia, USA, 12 July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Marc Moens</author>
</authors>
<title>Scaling context space.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>231--238</pages>
<location>Philadelphia, USA, 7–12</location>
<contexts>
<context position="2251" citStr="Curran and Moens (2002" startWordPosition="320" endWordPosition="324">resources is based on the distributional hypothesis that similar words appear in similar contexts. Existing approaches differ primarily in their definition of “context”, e.g. the surrounding words or the entire document, and their choice of distance metric for calculating similarity between the vector of contexts representing each term. Finding synonyms using distributional similarity involves performing a nearest-neighbour search over the context vectors for each term. This is very computationally intensive and scales according to the vocabulary size and the number of contexts for each term. Curran and Moens (2002b) have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. Unfortunately, this also increases the vocabulary size and the number of contexts for each term, making the use of huge datasets infeasible. There have been many data structures and approximation algorithms proposed to reduce the computational complexity of nearest-neighbour search (Ch´avez et al., 2001). Many of these approaches reduce the search space by using clustering techniques to generate an index of near-neighbours. We use the Spacial Approximation Sam</context>
<context position="5197" citStr="Curran and Moens (2002" startWordPosition="774" endWordPosition="777">he corpus. 2.2 Measures and Weights Both nearest-neighbour and cluster analysis methods require a distance measure that calculates the similarity between context vectors. Curran (2004) decomposes this measure into measure and weight functions. The measure function calculates the similarity between two weighted context vectors and the weight function calculates a weight from the raw frequency information for each context relation. The SASH requires a distance measure that preserves metric space (see Section 4.1). For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison /of many distance measures. 1(r,w′) min(Wgt(wm, ∗r, ∗w′), wgt(Wn, ∗r, ∗w′)) max/W (/W * ∗w′), tw m, r, wg(n, ∗r, ∗w′ 1 �(r,w′) l �l)) p(w, r, w′) − p(∗, r, w′)p(w, ∗, ∗) (2) yp(∗, r, w′)p(w, ∗, ∗) 3 Nearest-neighbour search The simplest algorithm for finding synonyms is nearest-neighbour search, which involves pairwise vector comparison of the target term with every term in the vocabulary. Given an n term vocabulary and up to m attributes for each term, the asymptotic time complexity of nearest-neighbour search is O(n2m). This is v</context>
<context position="14443" citStr="Curran and Moens (2002" startWordPosition="2503" endWordPosition="2506">d thesaurus. To help overcome the problems of direct comparisons we use several measures of system performance: direct matches (DIRECT), inverse rank (INVR), and precision of the top n synonyms (P(n)), for n = 1, 5 and 10. INVR is the sum of the inverse rank of each matching synonym, e.g. matching synonyms at ranks 3, 5 and 28 give an inverse rank score of 3 + 1 1 5 + 128, and with at most 100 synonyms, the maximum INVR score is 5.187. Precision of the top n is the percentage of matching synonyms in the top n extracted synonyms. The same 70 single-word nouns were used for the evaluation as in Curran and Moens (2002a). These were chosen randomly from WordNet such that they covered a range over the following properties: InvR frequency Penn Treebank and BNC frequencies; number of senses WordNet and Macquarie senses; specificity depth in the WordNet hierarchy; concreteness distribution across WordNet subtrees. For each of these terms, the closest 100 terms and their similarity score were extracted. 6 Experiments The contexts were extracted from the non-speech portion of the British National Corpus (Burnard, 1995). All experiments used the JACCARD measure function, the TTEST weight function and a cutoff freq</context>
<context position="16739" citStr="Curran and Moens, 2002" startWordPosition="2881" endWordPosition="2884">nodes (RANDOM) in SASH construction caused the nearest-neighbour approximation to be very inaccurate for distributional similarity. Although the speed was improved by two orders of magnitude when c = 16, it achieved only 13% of the INVR of the naive implementation. The best RANDOM result was less than three times faster then the naive solution and only 60% INVR. In accordance with Zipf’s law the majority of terms have very low frequencies. Similarity measurements made against these low frequency terms are less reliable, as accuracy increases with the number of relations and their frequencies (Curran and Moens, 2002b). This led to the idea that ordering the nodes by frequency before generating the SASH would improve accuracy. The SASH was then generated with the highest frequency terms were near the root so that the initial search paths would be more accurate. This has the unfortunate side-effect of slowing search by up to four times because comparisons with high frequency terms take longer than with low frequency terms as they have a larger number of relations. Table 2: Top 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 3 SASH level averages with c = 128 random fold1500 sort 101 DIST c DIRECT P(1) P(5) P(10) INVR SEAR</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James R. Curran and Marc Moens. 2002b. Scaling context space. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 231–238, Philadelphia, USA, 7–12 July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="3124" citStr="Curran, 2004" startWordPosition="451" endWordPosition="453">sets infeasible. There have been many data structures and approximation algorithms proposed to reduce the computational complexity of nearest-neighbour search (Ch´avez et al., 2001). Many of these approaches reduce the search space by using clustering techniques to generate an index of near-neighbours. We use the Spacial Approximation Sample Hierarchy (SASH) data structure developed by Houle (2003b) as it allows more control over the efficiency-approximation trade-off than other approximation methods. This paper describes integrating the SASH into an existing distributional similarity system (Curran, 2004). We show that replacing the nearestneighbour search improves efficiency by a factor of three with only a minor accuracy penalty. Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 97–104, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics 2 Distributional Similarity Distributional similarity systems can be separated into two components. The first component extracts the contexts from raw text and compiles them into a statistical description of the contexts each term appears in. The second component performs nearestneighbour search or clustering to det</context>
<context position="4759" citStr="Curran (2004)" startWordPosition="705" endWordPosition="706"> that dog was the direct object of walk in a sentence. Context extraction begins with a Maximum Entropy POS tagger and chunker (Ratnaparkhi, 1996). The Grefenstette (1994) relation extractor produces context relations that are then lemmatised using the Minnen et al. (2000) morphological analyser. The relations for each term are collected together and counted, producing a context vector of attributes and their frequencies in the corpus. 2.2 Measures and Weights Both nearest-neighbour and cluster analysis methods require a distance measure that calculates the similarity between context vectors. Curran (2004) decomposes this measure into measure and weight functions. The measure function calculates the similarity between two weighted context vectors and the weight function calculates a weight from the raw frequency information for each context relation. The SASH requires a distance measure that preserves metric space (see Section 4.1). For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison /of many distance measures. 1(r,w′) min(Wgt(wm, ∗r, ∗w′), wgt(Wn, ∗r, ∗w′)) max/W (/W * ∗w′), tw m,</context>
<context position="24672" citStr="Curran (2004)" startWordPosition="4254" endWordPosition="4255"> We have integrated a nearest-neighbour approximation data structure, the Spacial Approximation Sample Hierarchy (SASH), with a state-of-the-art distributional similarity system. In the process we have extended the original SASH construction algorithms (Houle, 2003b) to deal with the non-uniform distribution of words within semantic space. We intend to test other similarity measures and node ordering strategies, including a more linguistic analysis using WordNet, and further explore the interaction between the canonical vector heuristic and the SASH. The larger 300 word evaluation set used by Curran (2004) will be used, and combined with a more detailed analyis. Finally, we plan to optimise our SASH implementation so that it is comparable with the highly optimised nearest-neighbour code. 103 DIST c DIRECT P(1) P(5) P(10) INVR SEARCH TIME NAIVE HEURISTIC 2.72 49% 40% 32% 1.40 827ms RANDOM 16 2.61 50% 40% 31% 1.39 99% 388ms RANDOM 64 2.72 49% 40% 32% 1.40 100% 1254ms RANDOM 128 2.71 49% 40% 32% 1.40 100% 1231ms FOLD1500 16 2.53 49% 40% 31% 1.36 97% 363ms FOLD1500 64 2.72 49% 40% 32% 1.40 100% 900ms FOLD1500 128 2.72 49% 40% 32% 1.40 100% 974ms SORT 16 2.78 49% 40% 32% 1.41 100% 323ms SORT 64 2.73</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James R. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA USA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: an electronic lexical database. The MIT Press, Cambridge, MA USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston, USA.</location>
<contexts>
<context position="4317" citStr="Grefenstette (1994)" startWordPosition="642" endWordPosition="643">earch or clustering to determine which terms are similar, based on distance calculations between their context vectors. The approach used in this paper follows Curran (2004). 2.1 Extraction Method A context relation is defined as a tuple (w, r, w′) where w is a term, which occurs in some grammatical relation r with another word w′ in some sentence. We refer to the tuple (r, w′) as an attribute of w. For example, (dog, diect-obj, walk) indicates that dog was the direct object of walk in a sentence. Context extraction begins with a Maximum Entropy POS tagger and chunker (Ratnaparkhi, 1996). The Grefenstette (1994) relation extractor produces context relations that are then lemmatised using the Minnen et al. (2000) morphological analyser. The relations for each term are collected together and counted, producing a context vector of attributes and their frequencies in the corpus. 2.2 Measures and Weights Both nearest-neighbour and cluster analysis methods require a distance measure that calculates the similarity between context vectors. Curran (2004) decomposes this measure into measure and weight functions. The measure function calculates the similarity between two weighted context vectors and the weight</context>
<context position="13379" citStr="Grefenstette, 1994" startWordPosition="2313" endWordPosition="2314">ode is approximately 2p, practical complexities can be derived using c = 2p. In Houle’s experiments, typically less than 5% of computation time was spent assigning parents to orphans, even for relatively small c. In some of our experiments we found that low values of c produced significantly worse load times that for higher values, but this was highly dependant on the distribution of nodes. Table 1 shows this with respect to several distributions and values of c. 5 Evaluation The simplest method of evaluation is direct comparison of the extracted synonyms with a manuallycreated gold standard (Grefenstette, 1994). However, on small corpora, rare direct matches provide limited information for evaluation, and thesaurus coverage is a problem. Our evaluation uses a combination of three electronic thesauri: the Macquarie (Bernard, 1990), Roget’s (Roget, 1911) and Moby (Ward, 1996) thesauri. pc2 + −1 2 1 k1+log2 n 1 k log2 n 100 With this gold standard in place, it is possible to use precision and recall measures to evaluate the quality of the extracted thesaurus. To help overcome the problems of direct comparisons we use several measures of system performance: direct matches (DIRECT), inverse rank (INVR), </context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Houle</author>
</authors>
<title>Navigating massive data sets via local clustering.</title>
<date>2003</date>
<booktitle>In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>547--552</pages>
<location>Washington, DC, USA,</location>
<contexts>
<context position="2911" citStr="Houle (2003" startWordPosition="423" endWordPosition="424"> the quantity of text used to extract contexts significantly improves synonym quality. Unfortunately, this also increases the vocabulary size and the number of contexts for each term, making the use of huge datasets infeasible. There have been many data structures and approximation algorithms proposed to reduce the computational complexity of nearest-neighbour search (Ch´avez et al., 2001). Many of these approaches reduce the search space by using clustering techniques to generate an index of near-neighbours. We use the Spacial Approximation Sample Hierarchy (SASH) data structure developed by Houle (2003b) as it allows more control over the efficiency-approximation trade-off than other approximation methods. This paper describes integrating the SASH into an existing distributional similarity system (Curran, 2004). We show that replacing the nearestneighbour search improves efficiency by a factor of three with only a minor accuracy penalty. Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 97–104, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics 2 Distributional Similarity Distributional similarity systems can be separated into two components. The </context>
<context position="7358" citStr="Houle, 2003" startWordPosition="1135" endWordPosition="1136">bute then a full similarity comparison is performed, otherwise the terms are not considered similar. If a maximum of p positive results are returned, our complexity becomes O(n2k+npm), which, since k is constant, is O(n2 + npm). 4 The SASH The SASH approximates a nearest-neighbour search by pre-computing some of the near-neighbours of each node (terms in our case). It is arranged as a multi-leveled pyramid, where each node is linked to its (approximate) near-neighbours on the levels above and below. This produces multiple paths between nodes, allowing the SASH to shape itself to the data set (Houle, 2003a). This graph is searched by finding the near-neighbours of the target node at each level. The following description is adapted from Houle (2003b). 98 A 1 B C D E F G H I J 5 K L Figure 1: A SASH, where p = 2, c = 3 and k = 2 2 3 4 4.1 Metric Spaces The SASH organises nodes that can be measured in metric space. Although it is not necessary for the SASH to work, only in this space can performance be guaranteed. Our meaures produce a metric-like space for the terms derived from large datasets. A domain D is a metric space if there exists a function dist : D x D —� R&gt;0 such that: 1. dist(p, q) &gt;</context>
<context position="15756" citStr="Houle, 2003" startWordPosition="2719" endWordPosition="2720">n the heuristic was applied, the TTESTLOG weight function was used with a canonical set size of 100 and a maximum frequency cutoff of 10,000. The values 4–16, 8–32, 16–64, and 32–128 were chosen for p and c. This gives a range of branching factors to test the balance between sparseness, where there is potential for erroneous fragmentation of large clusters, and bushiness, where more tests must be made to find near children. The c = 4p relationship is derived from the simple hashing rule of thumb that says that a hash table should have roughly twice the size required to store all its elements (Houle, 2003b). DIST FREQUENCY # RELATIONS Mean Median Mean Median RANDOM 342 18 126 13 FOLD500 915 865.5 500 500 FOLD1000 2155 1970.5 1001 1001.5 FOLD1500 3656 3444 1506 1510.5 SORT 44753 37937.5 8290 7583.5 0 1000 2000 3000 4000 5000 6000 7000 Avg Search Time (ms) Figure 2: INVR against average search time Our initial experiments showed that the random distribution of nodes (RANDOM) in SASH construction caused the nearest-neighbour approximation to be very inaccurate for distributional similarity. Although the speed was improved by two orders of magnitude when c = 16, it achieved only 13% of the INVR of</context>
<context position="24324" citStr="Houle, 2003" startWordPosition="4200" endWordPosition="4201">maximum length 100). Increasing the canonical vector size allows us to increase the accuracy of heuristic solutions at the cost of efficiency. Using a SASH solution would offset some of this efficiency penalty. This has the potential for a solution that is more than an order of magnitude faster than NAIVE and is almost as accurate. 8 Conclusion We have integrated a nearest-neighbour approximation data structure, the Spacial Approximation Sample Hierarchy (SASH), with a state-of-the-art distributional similarity system. In the process we have extended the original SASH construction algorithms (Houle, 2003b) to deal with the non-uniform distribution of words within semantic space. We intend to test other similarity measures and node ordering strategies, including a more linguistic analysis using WordNet, and further explore the interaction between the canonical vector heuristic and the SASH. The larger 300 word evaluation set used by Curran (2004) will be used, and combined with a more detailed analyis. Finally, we plan to optimise our SASH implementation so that it is comparable with the highly optimised nearest-neighbour code. 103 DIST c DIRECT P(1) P(5) P(10) INVR SEARCH TIME NAIVE HEURISTIC</context>
</contexts>
<marker>Houle, 2003</marker>
<rawString>Michael E. Houle. 2003a. Navigating massive data sets via local clustering. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 547–552, Washington, DC, USA, 24–27 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Houle</author>
</authors>
<title>SASH: a saptial approximation sample hierarchy for similarity search.</title>
<date>2003</date>
<tech>Technical Report RT0517,</tech>
<institution>IBM Reasearch, Tokyo Research Laboratory,</institution>
<location>Yamato Kanagawa, Japan,</location>
<contexts>
<context position="2911" citStr="Houle (2003" startWordPosition="423" endWordPosition="424"> the quantity of text used to extract contexts significantly improves synonym quality. Unfortunately, this also increases the vocabulary size and the number of contexts for each term, making the use of huge datasets infeasible. There have been many data structures and approximation algorithms proposed to reduce the computational complexity of nearest-neighbour search (Ch´avez et al., 2001). Many of these approaches reduce the search space by using clustering techniques to generate an index of near-neighbours. We use the Spacial Approximation Sample Hierarchy (SASH) data structure developed by Houle (2003b) as it allows more control over the efficiency-approximation trade-off than other approximation methods. This paper describes integrating the SASH into an existing distributional similarity system (Curran, 2004). We show that replacing the nearestneighbour search improves efficiency by a factor of three with only a minor accuracy penalty. Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 97–104, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics 2 Distributional Similarity Distributional similarity systems can be separated into two components. The </context>
<context position="7358" citStr="Houle, 2003" startWordPosition="1135" endWordPosition="1136">bute then a full similarity comparison is performed, otherwise the terms are not considered similar. If a maximum of p positive results are returned, our complexity becomes O(n2k+npm), which, since k is constant, is O(n2 + npm). 4 The SASH The SASH approximates a nearest-neighbour search by pre-computing some of the near-neighbours of each node (terms in our case). It is arranged as a multi-leveled pyramid, where each node is linked to its (approximate) near-neighbours on the levels above and below. This produces multiple paths between nodes, allowing the SASH to shape itself to the data set (Houle, 2003a). This graph is searched by finding the near-neighbours of the target node at each level. The following description is adapted from Houle (2003b). 98 A 1 B C D E F G H I J 5 K L Figure 1: A SASH, where p = 2, c = 3 and k = 2 2 3 4 4.1 Metric Spaces The SASH organises nodes that can be measured in metric space. Although it is not necessary for the SASH to work, only in this space can performance be guaranteed. Our meaures produce a metric-like space for the terms derived from large datasets. A domain D is a metric space if there exists a function dist : D x D —� R&gt;0 such that: 1. dist(p, q) &gt;</context>
<context position="15756" citStr="Houle, 2003" startWordPosition="2719" endWordPosition="2720">n the heuristic was applied, the TTESTLOG weight function was used with a canonical set size of 100 and a maximum frequency cutoff of 10,000. The values 4–16, 8–32, 16–64, and 32–128 were chosen for p and c. This gives a range of branching factors to test the balance between sparseness, where there is potential for erroneous fragmentation of large clusters, and bushiness, where more tests must be made to find near children. The c = 4p relationship is derived from the simple hashing rule of thumb that says that a hash table should have roughly twice the size required to store all its elements (Houle, 2003b). DIST FREQUENCY # RELATIONS Mean Median Mean Median RANDOM 342 18 126 13 FOLD500 915 865.5 500 500 FOLD1000 2155 1970.5 1001 1001.5 FOLD1500 3656 3444 1506 1510.5 SORT 44753 37937.5 8290 7583.5 0 1000 2000 3000 4000 5000 6000 7000 Avg Search Time (ms) Figure 2: INVR against average search time Our initial experiments showed that the random distribution of nodes (RANDOM) in SASH construction caused the nearest-neighbour approximation to be very inaccurate for distributional similarity. Although the speed was improved by two orders of magnitude when c = 16, it achieved only 13% of the INVR of</context>
<context position="24324" citStr="Houle, 2003" startWordPosition="4200" endWordPosition="4201">maximum length 100). Increasing the canonical vector size allows us to increase the accuracy of heuristic solutions at the cost of efficiency. Using a SASH solution would offset some of this efficiency penalty. This has the potential for a solution that is more than an order of magnitude faster than NAIVE and is almost as accurate. 8 Conclusion We have integrated a nearest-neighbour approximation data structure, the Spacial Approximation Sample Hierarchy (SASH), with a state-of-the-art distributional similarity system. In the process we have extended the original SASH construction algorithms (Houle, 2003b) to deal with the non-uniform distribution of words within semantic space. We intend to test other similarity measures and node ordering strategies, including a more linguistic analysis using WordNet, and further explore the interaction between the canonical vector heuristic and the SASH. The larger 300 word evaluation set used by Curran (2004) will be used, and combined with a more detailed analyis. Finally, we plan to optimise our SASH implementation so that it is comparable with the highly optimised nearest-neighbour code. 103 DIST c DIRECT P(1) P(5) P(10) INVR SEARCH TIME NAIVE HEURISTIC</context>
</contexts>
<marker>Houle, 2003</marker>
<rawString>Michael E. Houle. 2003b. SASH: a saptial approximation sample hierarchy for similarity search. Technical Report RT0517, IBM Reasearch, Tokyo Research Laboratory, Yamato Kanagawa, Japan, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Robust applied morphological generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the First International Natural Language Generation Conference,</booktitle>
<pages>201--208</pages>
<location>Mitzpe Ramon,</location>
<contexts>
<context position="4419" citStr="Minnen et al. (2000)" startWordPosition="655" endWordPosition="658">r context vectors. The approach used in this paper follows Curran (2004). 2.1 Extraction Method A context relation is defined as a tuple (w, r, w′) where w is a term, which occurs in some grammatical relation r with another word w′ in some sentence. We refer to the tuple (r, w′) as an attribute of w. For example, (dog, diect-obj, walk) indicates that dog was the direct object of walk in a sentence. Context extraction begins with a Maximum Entropy POS tagger and chunker (Ratnaparkhi, 1996). The Grefenstette (1994) relation extractor produces context relations that are then lemmatised using the Minnen et al. (2000) morphological analyser. The relations for each term are collected together and counted, producing a context vector of attributes and their frequencies in the corpus. 2.2 Measures and Weights Both nearest-neighbour and cluster analysis methods require a distance measure that calculates the similarity between context vectors. Curran (2004) decomposes this measure into measure and weight functions. The measure function calculates the similarity between two weighted context vectors and the weight function calculates a weight from the raw frequency information for each context relation. The SASH r</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2000</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2000. Robust applied morphological generation. In Proceedings of the First International Natural Language Generation Conference, pages 201–208, Mitzpe Ramon, Israel, 12–16 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
<author>Sanda Harabagiu</author>
</authors>
<title>The informative role of wordnet in open-domain question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations,</booktitle>
<pages>138--143</pages>
<location>Pittsburgh, PA USA,</location>
<contexts>
<context position="1070" citStr="Pasca and Harabagiu, 2001" startWordPosition="139" endWordPosition="142">oposed by Houle (2003b), is a data structure for approximate nearestneighbour queries that balances the efficiency/approximation trade-off. We have intergrated this into an existing distributional similarity system, tripling efficiency with a minor accuracy penalty. 1 Introduction With the development of WordNet (Fellbaum, 1998) and large electronic thesauri, information from lexical semantic resources is regularly used to solve NLP problems. These problems include collocation discovery (Pearce, 2001), smoothing and estimation (Brown et al., 1992; Clark and Weir, 2001) and question answering (Pasca and Harabagiu, 2001). Unfortunately, these resources are expensive and time-consuming to create manually, and tend to suffer from problems of bias, inconsistency, and limited coverage. In addition, lexicographers cannot keep up with constantly evolving language use and cannot afford to build new resources for the many subdomains that NLP techniques are being applied to. There is a clear need for methods to extract lexical semantic resources automatically or tools that assist in their manual creation and maintenance. 97 Much of the existing work on automatically extracting resources is based on the distributional </context>
</contexts>
<marker>Pasca, Harabagiu, 2001</marker>
<rawString>Marius Pasca and Sanda Harabagiu. 2001. The informative role of wordnet in open-domain question answering. In Proceedings of the Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, pages 138–143, Pittsburgh, PA USA, 2–7 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darren Pearce</author>
</authors>
<title>Synonymy in collocation extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations,</booktitle>
<pages>41--46</pages>
<location>Pittsburgh, PA USA,</location>
<contexts>
<context position="950" citStr="Pearce, 2001" startWordPosition="122" endWordPosition="123">ch to finding synonyms suffers from poor scalability. The Spatial Approximation Sample Hierarchy (SASH), proposed by Houle (2003b), is a data structure for approximate nearestneighbour queries that balances the efficiency/approximation trade-off. We have intergrated this into an existing distributional similarity system, tripling efficiency with a minor accuracy penalty. 1 Introduction With the development of WordNet (Fellbaum, 1998) and large electronic thesauri, information from lexical semantic resources is regularly used to solve NLP problems. These problems include collocation discovery (Pearce, 2001), smoothing and estimation (Brown et al., 1992; Clark and Weir, 2001) and question answering (Pasca and Harabagiu, 2001). Unfortunately, these resources are expensive and time-consuming to create manually, and tend to suffer from problems of bias, inconsistency, and limited coverage. In addition, lexicographers cannot keep up with constantly evolving language use and cannot afford to build new resources for the many subdomains that NLP techniques are being applied to. There is a clear need for methods to extract lexical semantic resources automatically or tools that assist in their manual crea</context>
</contexts>
<marker>Pearce, 2001</marker>
<rawString>Darren Pearce. 2001. Synonymy in collocation extraction. In Proceedings of the Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, pages 41–46, Pittsburgh, PA USA, 2–7 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-ofspeech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="4292" citStr="Ratnaparkhi, 1996" startWordPosition="639" endWordPosition="640">forms nearestneighbour search or clustering to determine which terms are similar, based on distance calculations between their context vectors. The approach used in this paper follows Curran (2004). 2.1 Extraction Method A context relation is defined as a tuple (w, r, w′) where w is a term, which occurs in some grammatical relation r with another word w′ in some sentence. We refer to the tuple (r, w′) as an attribute of w. For example, (dog, diect-obj, walk) indicates that dog was the direct object of walk in a sentence. Context extraction begins with a Maximum Entropy POS tagger and chunker (Ratnaparkhi, 1996). The Grefenstette (1994) relation extractor produces context relations that are then lemmatised using the Minnen et al. (2000) morphological analyser. The relations for each term are collected together and counted, producing a context vector of attributes and their frequencies in the corpus. 2.2 Measures and Weights Both nearest-neighbour and cluster analysis methods require a distance measure that calculates the similarity between context vectors. Curran (2004) decomposes this measure into measure and weight functions. The measure function calculates the similarity between two weighted conte</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy part-ofspeech tagger. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133– 142, 17–18 May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Roget</author>
</authors>
<title>Thesaurus of English words and phrases. Longmans,</title>
<date>1911</date>
<location>Green and Co., London, UK.</location>
<contexts>
<context position="13625" citStr="Roget, 1911" startWordPosition="2349" endWordPosition="2350"> low values of c produced significantly worse load times that for higher values, but this was highly dependant on the distribution of nodes. Table 1 shows this with respect to several distributions and values of c. 5 Evaluation The simplest method of evaluation is direct comparison of the extracted synonyms with a manuallycreated gold standard (Grefenstette, 1994). However, on small corpora, rare direct matches provide limited information for evaluation, and thesaurus coverage is a problem. Our evaluation uses a combination of three electronic thesauri: the Macquarie (Bernard, 1990), Roget’s (Roget, 1911) and Moby (Ward, 1996) thesauri. pc2 + −1 2 1 k1+log2 n 1 k log2 n 100 With this gold standard in place, it is possible to use precision and recall measures to evaluate the quality of the extracted thesaurus. To help overcome the problems of direct comparisons we use several measures of system performance: direct matches (DIRECT), inverse rank (INVR), and precision of the top n synonyms (P(n)), for n = 1, 5 and 10. INVR is the sum of the inverse rank of each matching synonym, e.g. matching synonyms at ranks 3, 5 and 28 give an inverse rank score of 3 + 1 1 5 + 128, and with at most 100 synonym</context>
</contexts>
<marker>Roget, 1911</marker>
<rawString>Peter Roget. 1911. Thesaurus of English words and phrases. Longmans, Green and Co., London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grady Ward</author>
</authors>
<title>Moby Thesaurus. Moby Project.</title>
<date>1996</date>
<contexts>
<context position="13647" citStr="Ward, 1996" startWordPosition="2353" endWordPosition="2354">ed significantly worse load times that for higher values, but this was highly dependant on the distribution of nodes. Table 1 shows this with respect to several distributions and values of c. 5 Evaluation The simplest method of evaluation is direct comparison of the extracted synonyms with a manuallycreated gold standard (Grefenstette, 1994). However, on small corpora, rare direct matches provide limited information for evaluation, and thesaurus coverage is a problem. Our evaluation uses a combination of three electronic thesauri: the Macquarie (Bernard, 1990), Roget’s (Roget, 1911) and Moby (Ward, 1996) thesauri. pc2 + −1 2 1 k1+log2 n 1 k log2 n 100 With this gold standard in place, it is possible to use precision and recall measures to evaluate the quality of the extracted thesaurus. To help overcome the problems of direct comparisons we use several measures of system performance: direct matches (DIRECT), inverse rank (INVR), and precision of the top n synonyms (P(n)), for n = 1, 5 and 10. INVR is the sum of the inverse rank of each matching synonym, e.g. matching synonyms at ranks 3, 5 and 28 give an inverse rank score of 3 + 1 1 5 + 128, and with at most 100 synonyms, the maximum INVR sc</context>
</contexts>
<marker>Ward, 1996</marker>
<rawString>Grady Ward. 1996. Moby Thesaurus. Moby Project.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>