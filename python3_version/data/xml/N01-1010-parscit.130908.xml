<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000043">
<title confidence="0.937269">
Tree-cut and A Lexicon based on Systematic Polysemy
</title>
<author confidence="0.994634">
Noriko Tomuro
</author>
<affiliation confidence="0.997463">
DePaul University
School of Computer Science, Telecommunications and Information Systems
</affiliation>
<address confidence="0.840772">
243 S. Wabash Ave.
Chicago, IL 60604
</address>
<email confidence="0.998245">
tomuro@cs.depaul.edu
</email>
<sectionHeader confidence="0.980081" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999562125">
This paper describes a lexicon organized around sys-
tematic polysemy: a set of word senses that are
related in systematic and predictable ways. The
lexicon is derived by a fully automatic extraction
method which utilizes a clustering technique called
tree-cut. We compare our lexicon to WordNet
cousins, and the inter-annotator disagreement ob-
served between WordNet Semcor and DSO corpora.
</bodyText>
<sectionHeader confidence="0.99551" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941914285715">
In recent years, the granularity of word senses
for computational lexicons has been discussed fre-
quently in Lexical Semantics (for example, (Kilgar-
riff, 1998a; Palmer, 1998)). This issue emerged as a
prominent problem after previous studies and ex-
ercises in Word Sense Disambiguation (WSD) re-
ported that, when fine-grained sense definitions such
as those in WordNet (Miller, 1990) were used, en-
tries became very similar and indistinguishable to
human annotators, thereby causing disagreement on
correct tags (Kilgarriff, 1998b; Veronis, 1998; Ng et
al., 1999). In addition to WSD, the selection of sense
inventories is fundamentally critical in other Natural
Language Processing (NLP) tasks such as Informa-
tion Extraction (IE) and Machine Translation (MT),
as well as in Information Retrieval (IR), since the
difference in the correct sense assignments affects re-
call, precision and other evaluation measures.
In response to this, several approaches have been
proposed which group fine-grained word senses in
various ways to derive coarse-grained sense groups.
Some approaches utilize an abstraction hierarchy de-
fined in a dictionary (Kilgarriff, 1998b), while others
utilize surface syntactic patterns of the functional
structures (such as predicate-argument structure for
verbs) of words (Palmer, 1998). Also, the current
version of WordNet (1.6) encodes groupings of sim-
ilar/related word senses (or synsets) by a relation
called cousin.
Another approach to grouping word senses is to
utilize a linguistic phenomenon called systematic
polysemy: a set of word senses that are related in sys-
tematic and predictable ways.&apos; For example, ANIMAL
and MEAT meanings of the word &amp;quot;chicken&amp;quot; are re-
lated because chicken as meat refers to the flesh of
a chicken as a bird that is used for food.2 This rela-
tion is systematic, since many ANIMAL words such as
&amp;quot;duck&amp;quot; and &amp;quot;lamb&amp;quot; have a MEAT meaning. Another
example is the relation QUANTITY-PROCESS observed
in nouns such as &amp;quot;increase&amp;quot; and &amp;quot;supply&amp;quot;.
Sense grouping based on systematic polysemy is
lexico-semantically motivated in that it expresses
general human knowledge about the relatedness of
word meanings. Such sense groupings have advan-
tages compared to other approaches. First, related
senses of a word often exist simultaneously in a
discourse (for example the QUANTITY and PROCESS
meanings of &amp;quot;increase&amp;quot; above). Thus, systematic
polysemy can be effectively used in WSD (and WSD
evaluation) to accept multiple or alternative sense
tags (Buitelaar, personal communication). Second,
many systematic relations are observed between
senses which belong to different semantic categories.
So if a lexicon is defined by a collection of sepa-
rate trees/hierarchies (such as the case of Word-
Net), systematic polysemy can express similarity be-
tween senses that are not hierarchically proximate.
Third, by explicitly representing (inter-)relations be-
tween senses, a lexicon based on systematic poly-
semy can facilitate semantic inferences. Thus it is
useful in knowledge-intensive NLP tasks such as dis-
course analysis, IE and MT. More recently, (Gonzalo
et al., 2000) also discusses potential usefulness of sys-
tematic polysemy for clustering word senses for IR.
However, extracting systematic relations from
large sense inventories is a difficult task. Most of-
ten, this procedure is done manually. For example,
WordNet cousin relations were identified manually
by the WordNet lexicographers. A similar effort was
also made in the EuroWordnet project (Vossen et
</bodyText>
<footnote confidence="0.967517142857143">
&apos;Systematic polysemy (in the sense we use in this paper) is
also referred to as regular polysemy (Apresjan, 1973) or logical
polysemy (Pustejovsky, 1995).
2Note that systematic polysemy should be contrasted
with homonymy, which refers to words which have more
than one unrelated sense (e.g. FINANCIAL INSTITUTION and
SLOPING LAND meanings of the word &amp;quot;bank&amp;quot;).
</footnote>
<bodyText confidence="0.999268217391304">
al., 1999). The problem is not only that manual
inspection of a large, complex lexicon is very time-
consuming, it is also prone to inconsistencies.
In this paper, we describes a lexicon organized
around systematic polysemy. The lexicon is derived
by a fully automatic extraction method which uti-
lizes a clustering technique called tree-cut (Li and
Abe, 1998). In our previous work (Tomuro, 2000),
we applied this method to a small subset of Word-
Net nouns and showed potential applicability. In the
current work, we applied the method to all nouns
and verbs in WordNet, and built a lexicon in which
word senses are partitioned by systematic polysemy.
We report results of comparing our lexicon with the
WordNet cousins as well as the inter-annotator dis-
agreement observed between two semantically an-
notated corpora: WordNet Semcor (Landes et al.,
1998) and DSO (Ng and Lee, 1996). The results are
quite promising: our extraction method discovered
89% of the WordNet cousins, and the sense parti-
tions in our lexicon yielded better K values (Car-
letta, 1996) than arbitrary sense groupings on the
agreement data.
</bodyText>
<sectionHeader confidence="0.687484" genericHeader="introduction">
2 The Tree-cut Technique
</sectionHeader>
<bodyText confidence="0.999956333333333">
The tree-cut technique is an unsupervised learning
technique which partitions data items organized in a
tree structure into mutually-disjoint clusters. It was
originally proposed in (Li and Abe, 1998), and then
adopted in our previous method for automatically
extracting systematic polysemy (Tomuro, 2000). In
this section, we give a brief summary of this tree-cut
technique using examples from (Li and Abe, 1998)&apos;s
original work.
</bodyText>
<subsectionHeader confidence="0.813294">
2.1 Tree-cut Models
</subsectionHeader>
<bodyText confidence="0.99987045">
The tree-cut technique is applied to data items that
are organized in a structure called a thesaurus tree.
A thesaurus tree is a hierarchically organized lexicon
where leaf nodes encode lexical data (i.e., words) and
internal nodes represent abstract semantic classes.
A tree-cut is a partition of a thesaurus tree. It is
a list of internal/leaf nodes in the tree, and each
node represents a set of all leaf nodes in a subtree
rooted by the node. Such a set is also considered as a
cluster.3 Clusters in a tree-cut exhaustively cover all
leaf nodes of the tree, and they are mutually disjoint.
For instance, Figure 1 shows an example thesaurus
tree and one possible tree-cut [AIRCRAFT, ball, kite,
puzzle], which is indicated by a thick curve in the
figure. There are also four other possible tree-cuts
for this tree: [airplane, helicopter, ball, kite, puzzle],
[airplane, helicopter, TOY], [AIRCRAFT, TOY] and
[ARTIFACT].
In (Li and Abe, 1998), the tree-cut technique
was applied to the problem of acquiring general-
</bodyText>
<footnote confidence="0.388127">
3A leaf node is also a cluster whose cardinality is 1.
</footnote>
<bodyText confidence="0.999387272727273">
ized case frame patterns from a corpus. Thus, each
node/word in the tree received as its value the num-
ber of instances where the word occurred as a case
role (subject, object etc.) of a given verb. Then the
acquisition of a generalized case frame was viewed as
a problem of selecting the best tree-cut model that
estimates the true probability distribution, given a
sample corpus data.
Formally, a tree-cut model M is a pair consisting
of a tree-cut F and a probability parameter vector
O of the same length,
</bodyText>
<equation confidence="0.992774">
M = (F, ©) (1)
</equation>
<bodyText confidence="0.910889">
where F and O are:
</bodyText>
<equation confidence="0.999681">
F = [C1, ••,Ck],E) = [P(C1), ••, P(Ck)] (2)
</equation>
<bodyText confidence="0.997639384615384">
where Ci (1 G i G k) is a cluster in the tree-
cut, P(Ci) is the probability of a cluster Ci, and
Pki=1 P(Ci) = 1. Note that P(C) is the prob-
ability of cluster C = {n1, ••, nm } as a whole,
that is, P(C) = Pj=1 P(nj). For example, sup-
pose a corpus contains 10 instances of verb-object
relation for the verb &amp;quot;fly&amp;quot;, and the frequencies
of object nouns n, denoted f(n), are as follows:
f(airplane) = 5, f(helicopter) = 3, f(ball) =
0, f(kite) = 2, f(puzzle) = 0. Then, the set of tree-
cut models for the example thesaurus tree shown in
Figure 1 includes ([airplane, helicopter, TOY], [.5,
.3, .2]) and ([AIRCRAFT, TOY], [.8, .2]).
</bodyText>
<subsectionHeader confidence="0.994831">
2.2 The MDL Principle
</subsectionHeader>
<bodyText confidence="0.9998">
To select the best tree-cut model, (Li and Abe, 1998)
uses the Minimal Description Length (MDL). The
MDL is a principle of data compression in Informa-
tion Theory which states that, for a given dataset,
the best model is the one which requires the min-
imum length (often measured in bits) to encode
the model (the model description length) and the
data (the data description length) (Rissanen, 1978).
Thus, the MDL principle captures the trade-off be-
tween the simplicity of a model, which is measured
by the number of clusters in a tree-cut, and the good-
ness of fit to the data, which is measured by the
estimation accuracy of the probability distribution.
The calculation of the description length for a
tree-cut model is as follows. Given a thesaurus tree
T and a sample S consisting of the case frame in-
stances, the total description length L(M, S) for a
tree-cut model M = (F, O) is
</bodyText>
<equation confidence="0.952159">
L(M, S) = L(F) + L(01F) + L(S1F, o) (3)
</equation>
<bodyText confidence="0.999319833333333">
where L(F) is the model description length, L(01F)
is the parameter description length (explained
shortly), and L(S1F, O) is the data description
length. Note that L(F) + L(01F) essentially corre-
sponds to the usual notion of the model description
length.
</bodyText>
<figure confidence="0.897031214285714">
Γ L(Θ|Γ) L(S|Γ,Θ) L(M,S)
[A] 1.66 11.60 13.26
[AC,TOY] 3.32 14.34 17.66
[ap,heli,TOY] 4.98 14.44 19.42
[AC,ball,kite,puz] 6.64 4.96 11.60
[ap,hel,ball,kite,puz] 8.31 5.06 13.37
frequency
ARTIFACT
0.8
AIRCRAFT TOY
0.2 0.0
airplane helicopter ball kite puzzle
5 3 0 2 0
0.0
</figure>
<figureCaption confidence="0.999997">
Figure 1: The MDL lengths and the final tree-cut
</figureCaption>
<bodyText confidence="0.9886965">
Each length in L(M; S) is calculated as follows.4
The model description length L(F) is
</bodyText>
<equation confidence="0.996743">
L(F) = log21G1 (4)
</equation>
<bodyText confidence="0.999962375">
where G is the set of all cuts in T, and 1G1 denotes
the size of G. This value is a constant for all mod-
els, thus it is omitted in the calculation of the total
length.
The parameter description length L(O117) indi-
cates the complexity of the model. It is the length
required to encode the probability distribution of the
clusters in the tree-cut F. It is calculated as
</bodyText>
<equation confidence="0.9943865">
k
L(O1�) = 2 x log21S1 (5)
</equation>
<bodyText confidence="0.99987425">
where k is the length of O, and 1S1 is the size of S.
Finally, the data description length L(S1F; O) is
the length required to encode the whole sample data.
It is calculated as
</bodyText>
<equation confidence="0.9877285">
L(S1F; O) = — log2P(n) (6)
nES
</equation>
<bodyText confidence="0.801118">
where, for each n E C and each C E F,
</bodyText>
<equation confidence="0.924584">
P(n) = P(C) and P(C) = f(S1) (7)
</equation>
<bodyText confidence="0.9982828">
Note the equation (7) essentially computes the Max-
imum Likelihood Estimate (MLE) for all n.5
A table in Figure 1 shows the MDL lengths for all
five tree-cut models. The best model is the one with
the tree-cut [AIRCRAFT, ball, kite, puzzle].
</bodyText>
<sectionHeader confidence="0.949224" genericHeader="method">
3 Clustering Systematic Polysemy
</sectionHeader>
<bodyText confidence="0.8535776">
Using the tree-cut technique described above, our
previous work (Tomuro, 2000) extracted systematic
polysemy from WordNet. In this section, we give a
summary of this method, and describe the cluster
pairs obtained by the method.
4For justification and detailed explanation of these formu-
las, see (Li and Abe, 1998).
5 In our previous work, we used entropy instead of NILE.
That is because the lexicon represents true population, not
samples; thus there is no additional data to estimate.
</bodyText>
<subsectionHeader confidence="0.997491">
3.1 Extraction Method
</subsectionHeader>
<bodyText confidence="0.999714045454546">
In our previous work, systematically related word
senses are derived as binary cluster pairs, by apply-
ing the extraction procedure to a combination of two
WordNet (sub)trees. This process is done in the fol-
lowing three steps. In the first step, all leaf nodes
of the two trees are assigned a value of either 1, if
a node/word appears in both trees, or 0 otherwise.6
In the second step, the tree-cut technique is applied
to each tree separately, and two tree-cuts (or sets of
clusters) are obtained. To search the best tree-cut
for a tree (i.e., the model which requires the mini-
mum total description length), a greedy algorithm
called Find-MDL described in (Li and Abe, 1998)
is used to speed up the search. Finally in the third
step, clusters in those two tree-cuts are matched up,
and the pairs which have substantial overlap (more
than three overlapping words) are selected as sys-
tematic polysemies.
Figure 2 shows parts of the final tree-cuts for the
ARTIFACT and MEASURE classes. Note in the figure,
bold letters indicate words which are polysemous in
the two trees (i.e., assigned a value 1).
</bodyText>
<subsectionHeader confidence="0.99922">
3.2 Modification
</subsectionHeader>
<bodyText confidence="0.999976214285714">
In the current work, we made a minor modification
to the extraction method described above, by re-
moving nodes that are assigned a value 0 from the
trees. The purpose was to make the tree-cut tech-
nique less sensitive to the structure of a tree and
produce more specific clusters defined at deeper lev-
els.&apos; The MDL principle inherently penalizes a com-
plex tree-cut by assigning a long parameter length.
Therefore, shorter tree-cuts partitioned at abstract
levels are often preferred. This causes a problem
when the tree is bushy, which is the case with Word-
Net trees. Indeed, many tree-cut clusters obtained
in our previous work were from nodes at depth 1
(counting the root as depth 0) — around 88% (122
</bodyText>
<footnote confidence="0.971182714285714">
6Prior to this, each WordNet (sub)tree is transformed into
a thesaurus tree, since WordNet tree is a graph rather than a
tree, and internal nodes as well as leaf nodes carry data. In
the transformation, all internal nodes in a WordNet tree are
copied as leaf nodes, and shared subtrees are duplicated.
7Removing nodes with 0 is also warranted since we are not
estimating values for those nodes (as explained in footnote 5).
</footnote>
<figure confidence="0.998016111111111">
ARTIFACT
yard
MEASURE
foot
IMPLEMENT
0.1
STRUCTURE INSTRUMEN-
TALITY
0.53
ARTICLE
TABLEWARE
UTENSIL
mixer porcelain
ROD
foot
knot
spoon
spoon
dish
plate
bottle bucket
DEVICE
CONTAINER
VESSEL
base building
0.02
bucket
knot
0.36
DEFINITE
QUANTITY
0.33
INDEFINITE
QUANTITY
0.12
0.07
bit block
ounce
CONTAINERFUL
load
morning
flash
quarter
bottle
spoon
mile
yard
foot
LINEAR
MEASURE
TIME
PERIOD
LINEAR
UNIT
</figure>
<figureCaption confidence="0.99959">
Figure 2: Parts of the final tree-cuts for ARTIFACT and MEASURE
</figureCaption>
<tableCaption confidence="0.998399">
Table 1: Automatically Extracted Cluster Pairs
</tableCaption>
<table confidence="0.999265">
Category Basic Underspecified Cluster
classes classes pairs
Nouns 24 99 2,377
Verbs 10 59 1,710
Total 34 158 4,077
</table>
<bodyText confidence="0.996850571428571">
out of total 138 clusters) obtained for 5 combinations
of WordNet noun trees. Note that we did not allow
a cluster at the root of a tree; thus, depth 1 is the
highest level for any cluster. After the modification
above, the proportion of depth 1 clusters decreased
to 49% (169 out of total 343 clusters) for the same
tree combinations.
</bodyText>
<subsectionHeader confidence="0.993292">
3.3 Extracted Cluster Pairs
</subsectionHeader>
<bodyText confidence="0.999943184210526">
We applied the modified method described above to
all nouns and verbs in WordNet. We first parti-
tioned words in the two categories into basic classes.
A basic class is an abstract semantic concept, and
it corresponds to a (sub)tree in the WordNet hier-
archies. We chose 24 basic classes for nouns and 10
basic classes for verbs, from WordNet Top categories
for nouns and lexicographers&apos; file names for verbs
respectively. Those basic classes exhaustively cover
all words in the two categories encoded in Word-
Net. For example, basic classes for nouns include
ARTIFACT, SUBSTANCE and LOCATION, while basic
classes for verbs include CHANGE, MOTION and STATE.
For each part-of-speech category, we applied our
extraction method to all combinations of two ba-
sic classes. Here, a combined class, for instance
ARTIFACT-SUBSTANCE, represents an underspecified
semantic class. We obtained 2,377 cluster pairs in
99 underspecified classes for nouns, and 1,710 cluster
pairs in 59 underspecified classes for verbs. Table 1
shows a summary of the number of basic and under-
specified classes and cluster pairs extracted by our
method.
Although the results vary among category combi-
nations, the accuracy (precision) of the derived clus-
ter pairs was rather low: 50 to 60% on average, based
on our manual inspection using around 5% randomly
chosen samples.&apos; This means our automatic method
over-generates possible relations. We speculate that
this is because in general, there are many homony-
mous relations that are &apos;systematic&apos; in the English
language. For example, in the ARTIFACT-GROUP
class, a pair [LUMBER, SOCIAL GROUP] was extracted.
Words which are common in the two clusters are
&amp;quot;picket&amp;quot;, &amp;quot;board&amp;quot; and &amp;quot;stock&amp;quot;. Since there are
enough number of such words (for our purpose), our
automatic method could not differentiate them from
true systematic polysemy.
</bodyText>
<sectionHeader confidence="0.5968235" genericHeader="method">
4 Evaluation: Comparison with
WordNet Cousins
</sectionHeader>
<bodyText confidence="0.998353698113208">
To test our automatic extraction method, we com-
pared the cluster pairs derived by our method to
WordNet cousins. The cousin relation is relatively
new in WordNet, and the coverage is still incom-
plete. Currently a total of 194 unique relations are
encoded. A cousin relation in WordNet is defined
between two synsets, and it indicates that senses of
a word that appear in both of the (sub)trees rooted
by those synsets are related.&apos; The cousins were man-
&apos;Note that the relatedness between clusters was deter-
mined solely by our subjective judgement. That is because
there is no existing large-scale lexicon which encodes related
senses completely for all words in the lexicon. (Note that
WordNet cousin relation is encoded only for some words).
Although the distinction between related vs. unrelated mean-
ings is sometimes unclear, systematicity of the related senses
among words is quite intuitive and has been well studied in
Lexical Semantics (for example, (Apresjan, 1973; Nunberg,
1995; Copestake and Briscoe, 1995)). A comparison with
WordNet cousin is discussed in the next section 4.
9Actually, cousin is one of the three relations which in-
dicate the grouping of related senses of a word. Others are
sister and twin. In this paper, we use cousin to refer to all
relations listed in &amp;quot;cousin.tps&amp;quot; file (available in a WordNet
distribution).
ually identified by the WordNet lexicographers.
To compare the automatically derived cluster
pairs to WordNet cousins, we used the hypernym-
hyponym relation in the trees, instead of the number
or ratio of the overlapping words. This is because
the levels at which the cousin relations are defined
differ quite widely, from depth 0 to depth 6, thus the
number of polysemous words covered in each cousin
relation significantly varies. Therefore, it was diffi-
cult to decide on an appropriate threshold value for
either criteria.
Using the hypernym-hyponym relation, we
checked, for each cousin relation, whether there was
at least one cluster pair that subsumed or was sub-
sumed by the cousin. More specifically, for a cousin
relation defined between nodes c1 and c2 in trees
T1 and T2 respectively and a cluster pair defined
between nodes r1 and r2 in the same trees, we de-
cided on the correspondence if c1 is a hypernym or
hyponym of r1, and c2 is a hypernym or hyponym
r2 at the same time.
Based on this criteria, we obtained a result indi-
cating that 173 out of the 194 cousin relations had
corresponding cluster pairs. This makes the recall
ratio 89%, which we consider to be quite high.
In addition to the WordNet cousins, our auto-
matic extraction method discovered several interest-
ing relations. Table 2 shows some examples.
</bodyText>
<sectionHeader confidence="0.989108" genericHeader="method">
5 A Lexicon based on Systematic
Relations
</sectionHeader>
<bodyText confidence="0.999918190476191">
Using the extracted cluster pairs, we partitioned
word senses for all nouns and verbs in WordNet, and
produced a lexicon. Recall from the previous section
that our cluster pairs are generated for all possible
binary combinations of basic classes, thus one sense
could appear in more than one cluster pair. For ex-
ample, Table 3 shows the cluster pairs (and a set of
senses covered by each pair, which we call a sense
cover) extracted for the noun &amp;quot;table&amp;quot; (which has 6
senses in WordNet). Also as we have mentioned ear-
lier in section accuracy-result, our cluster pairs con-
tain many false positives ones. For those reasons, we
took a conservative approach, by disallowing transi-
tivity of cluster pairs.
To partition senses of a word, we first assign each
sense cover a value which we call a connectedness. It
is defined as follows. For a given word w which has n
senses, let S be the set of all sense covers generated
for w. Let cij denote the number of sense covers in
which sense i (si) and sense j (sj) occurred together
in S (where cii = 0 for all 1 &lt; i &lt; n), and dij =
</bodyText>
<footnote confidence="0.565119666666667">
I:n cik+ckj where k =�i, , c 0 C 0
k=1 C ik k&apos; ,
and C = Pi;j cij. A connectedness of a sense cover
</footnote>
<page confidence="0.34039">
sc E S, denoted CNsc, where sc = (sl; ::; sm) (1 &lt;
</page>
<tableCaption confidence="0.998718">
Table 3: Extracted Relations for &amp;quot;table&amp;quot;
</tableCaption>
<table confidence="0.992305222222222">
Sense Cover Cluster Pair CN
(1 4) [ARRANGEMENT, NAT OBJ] 1.143
(1 5) [ARRANGEMENT, SOC GROUP] 1.143
(2 3) [FURNITURE] 4.429
(2 3 [FURNITURE, NAT OBJ] 7.429
(2 3 [FURNITURE, SOC GROUP] 7.714
(2 3 [FURNITURE, FOOD] 7.429
(4 5) [NAT OBJ, SOC GROUP] 1.429
(5 6) [SOC GROUP, FOOD] 1.286
</table>
<equation confidence="0.991575">
l &lt; m &lt; n) is defined as:
m m
CNsc = X cij + dij (8)
i=l j=1
</equation>
<bodyText confidence="0.9982616">
Intuitively, cij represents the weight of a direct re-
lation, and dij represents the weight of an indirect
relation between any two senses i and j. The idea
behind this connectedness measure is to favor sense
covers that have strong intra-relations. This mea-
sure also effectively takes into account a one-level
transitivity in dij. As an example, the connectedness
of (2 3 4) is the summation of c23; c34; c24; d23; d34
and d24. Here, c23 = 4 because sense 2 and 3 co-
occur in four sense covers, and c34 = c24 = 1. Also,
</bodyText>
<equation confidence="0.9993535">
d23 = (c24+c43)+(c25+c53)+(c26+c63) = 2+2+2 = :429
C 14
</equation>
<bodyText confidence="0.999771785714286">
(omitting cases where either or both cik and ckj are
zero), and similarly d34 = :5 and d24 = :5. Thus,
CN(234) = 4 + 1 + 1 +:429 +:5 +:5 = 7:429. Table 3
shows the connectedness values for all sense covers
for &amp;quot;table&amp;quot;.
Then, we partition the senses by selecting a set of
non-overlapping sense covers which maximizes the
total connectedness value. So in the example above,
the set {(1 4),(2 3 5)} yields the maximum con-
nectedness. Finally, senses that are not covered by
any sense covers are taken as singletons, and added
to the final sense partition. So the sense partition
for &amp;quot;table&amp;quot; becomes {(1 4),(2 3 5),(6)}.
Table 4 shows the comparison between Word-
Net and our new lexicon. As you can see,
our lexicon contains much less ambiguity: the
ratio of monosemous words increased from 84%
(88,650/105,461ti.84) to 92% (96,964/105,461ti.92),
and the average number of senses for polysemous
words decreased from 2.73 to 2.52 for nouns, and
from 3.57 to 2.82 for verbs.
As a note, our lexicon is similar to CORELEX
(Buitelaar, 1998) (or CORELEX-II presented in
(Buitelaar, 2000)), in that both lexicons share the
same motivation. However, our lexicon differs from
CORELEX in that CORELEX looks at all senses of
a word and groups words that have the same sense
distribution pattern, whereas our lexicon groups
</bodyText>
<tableCaption confidence="0.994146">
Table 2: Examples of Automatically Extracted Systematic Polysemy
</tableCaption>
<table confidence="0.999277875">
Underspecified Class Cluster Pair Common Words
ACTION-LOCATION [ACTION, POINT] &amp;quot;drop&amp;quot;, &amp;quot;circle&amp;quot;, &amp;quot;intersection&amp;quot;, &amp;quot;dig&amp;quot;,
&amp;quot;crossing&amp;quot;, &amp;quot;bull&apos;s eye&amp;quot;
ARTIFACT-GROUP [STRUCTURE, PEOPLE] &amp;quot;house&amp;quot;, &amp;quot;convent&amp;quot;, &amp;quot;market&amp;quot;, &amp;quot;center&amp;quot;
ARTIFACT-SUBSTANCE [FABRIC, CHEMICAL COMPOUND] &amp;quot;acetate&amp;quot;, &amp;quot;nylon&amp;quot;, &amp;quot;acrylic&amp;quot;, &amp;quot;polyester&amp;quot;
COMMUNICATION-PERSON [VOICE, SINGER] &amp;quot;soprano&amp;quot;, &amp;quot;alto&amp;quot;, &amp;quot;tenor&amp;quot;, &amp;quot;baritone&amp;quot;
[WRITING, RELIGIOUS PERSON] &amp;quot;John&amp;quot;, &amp;quot;Matthew&amp;quot;, &amp;quot;Jonah&amp;quot;, &amp;quot;Joshua&amp;quot;,
&amp;quot;Jeremiah&amp;quot;
</table>
<tableCaption confidence="0.99606">
Table 4: WordNet vs. the New Lexicon
</tableCaption>
<table confidence="0.998868583333333">
Category WordNet New
Nouns Monosemous 82,892 88,977
Polysemous 12,243 6,158
Total words 95,135 95,135
Ave # senses 2.73 2.52
Verbs Monosemous 5,758 7,987
Polysemous 4,568 2,339
Total words 10,326 10,326
Ave # senses 3.57 2.82
Total Monosemous 88,650 96,964
Polysemous 16,811 8,497
Total words 105,461 105,461
</table>
<bodyText confidence="0.85655775">
word senses that have the same systematic relation.
Thus, our lexicon represents systematic polysemy at
a finer level than CORELEX, by pinpointing related
senses within each word.
</bodyText>
<sectionHeader confidence="0.9710515" genericHeader="method">
6 Evaluation: Inter-annotator
Disagreement
</sectionHeader>
<bodyText confidence="0.971250476190476">
To test if the sense partitions in our lexicon con-
stitute an appropriate (or useful) level of granular-
ity, we applied it to the inter-annotator disagree-
ment observed in two semantically annotated cor-
pora: WordNet Semcor (Landes et al., 1998) and
DSO (Ng and Lee, 1996). The agreement between
those corpora is previously studied in (Ng et al.,
1999). In our current work, we first re-produced
their agreement data, then used our sense partitions
to see whether or not they yield a better agreement.
In this experiment, we extracted 28,772 sen-
tences/instances for 191 words (consisting of 121
nouns and 70 verbs) tagged in the intersection of
the two corpora. This constitutes the base data set.
Table 5 shows the breakdown of the number of in-
stances where tags agreed and disagreed.10 As you
10Note that the numbers reported in (Ng et al., 1999) are
slightly more than the ones reported in this paper. For in-
stance, the number of sentences in the intersected corpus re-
ported in (Ng et al., 1999) is 30,315. We speculate the dis-
crepancies are due to the different sentence alignment meth-
</bodyText>
<tableCaption confidence="0.998501">
Table 5: Agreement between Semcor and DSO
</tableCaption>
<table confidence="0.998819">
Category Agree Disagree Total Ave. K
Nouns 6,528 5,815 12,343 .268
Verbs 7,408 9,021 16,429 .260
Total 13,936 14,836 28,772 .264
(%) (48.4) (51.6) (100.0)
</table>
<bodyText confidence="0.99918248">
can see, the agreement is not very high: only around
48%.11
This low agreement ratio is also reflected in a mea-
sure called the K statistic (Carletta, 1996; Bruce and
Wiebe, 1998; Ng et al., 1999). K measure takes into
account chance agreement, thus better representing
the state of disagreement. A K value is calculated
for each word, on a confusion matrix where rows
represent the senses assigned by judge 1 (DSO) and
columns represent the senses assigned by judge 2
(Semcor). Table 6 shows an example matrix for the
noun &amp;quot;table&amp;quot;.
A K value for a word is calculated as follows. We
use the notation and formula used in (Bruce and
Wiebe, 1998). Let nib denote the number of in-
stances where the judge 1 assigned sense i and the
judge 2 assigned sense j to the same instance, and
ni+ and n+i denote the marginal totals of rows and
columns respectively. The formula is:
where Pii = nii n++ (i.e., proportion of nii, the number
of instances where both judges agreed on sense i, to
the total instances), Pi+ = ni+ n++ and P+i = n+i n++ .
The K value is 1.0 when the agreement is perfect
(i.e., values in the off-diagonal cells are all 0, that
is, Ei Pii = 1), or 0 when the agreement is purely
</bodyText>
<footnote confidence="0.9134268">
ods used in the experiments.
11(Ng et al., 1999) reports a higher agreement of 57%. We
speculate the discrepancy might be from the version of Word-
Net senses used in DSO, which was slightly different from the
standard delivery version (as noted in (Ng et al., 1999)).
</footnote>
<equation confidence="0.816762071428571">
Ei Pii —Ei Pi+P+i
k =
(9)
1 — Ei Pi+P+i
Table 6: Confusion Matrix for the noun &amp;quot;table&amp;quot; (K = .611)
Judge 2 (Semcor)
1 2 3 4 5 6 Total
43 0 0 0 0 0
6 17 3 0 0 0
0 0 0 0 0 0
1 0 0 0 0 0
0 0 0 0 0 0
2 2 1 0 0 0
43 (= n1+)
</equation>
<figure confidence="0.9334125">
26 (= n2+)
0 (= n3+)
1 (= n4+)
0 (= n5+)
5 (= n6+)
75
Total 52 19 4 0 0 0
(= n+1) (= n+2) (= n+3) (= n+4) (= n+5) (= n+6) (= n++)
1
2
Judge 1 3
(DSO) 4
5
6
</figure>
<tableCaption confidence="0.580124">
Table 7: Reduced Matrix for &amp;quot;table&amp;quot; (K = .699) Table 8: Our Lexicon vs. Random Partitions
</tableCaption>
<figure confidence="0.953868095238095">
1,4 1,4 2,3,5
2,3,5
6
Total
44 0
6 20
2 3
52 23
6 Total
0
44
0
0
26
5
0 75
Category Total Our Lexicon Random
Ave. K Ave. K
Nouns 10,980 .247 .217
Verbs 14,392 .283 .262
Total 25,372 .260 .233
</figure>
<bodyText confidence="0.9989847">
by chance (i.e., values in a row (or column) are uni-
formly distributed across rows (or columns), that is,
Pii = Pi+P+i for all 1 G_ i G_ M, where M is the
number of rows/columns). K also takes a negative
value when there is a systematic disagreement be-
tween the two judges (e.g., some values in the diago-
nal cells are 0, that is, Pii = 0 for some i). Normally,
K &gt; .8 is considered a good agreement (Carletta,
1996).
By using the formula above, the average K for the
191 words was .264, as shown in Table 5.12 This
means the agreement between Semcor and DSO is
quite low.
We selected the same 191 words from our lexicon,
and used their sense partitions to reduce the size of
the confusion matrices. For each word, we computed
the K for the reduced matrix, and compared it with
the K for a random sense grouping of the same parti-
tion pattern.13 For example, the partition pattern of
J(1 4),(2 3 5),(6)1 for &amp;quot;table&amp;quot; mentioned earlier
(where Table 7 shows its reduced matrix) is a multi-
nomial combination (2 3 1). The K value for a ran-
dom grouping is obtained by generating 5,000 ran-
dom partitions which have the same pattern as the
corresponding sense partition in our lexicon, then
taking the mean of their K&apos;s. Then we measured the
possible increase in K by our lexicon by taking the
difference between the paired K values for all words
(i.e., Kw by our sense partition - Kw by random par-
tition, for a word w), and performed a significance
</bodyText>
<footnote confidence="0.77504125">
12(Ng et al. 1999)&apos;s result is slightly higher: r, = .317.
13For this comparison, we excluded 23 words whose sense
partitions consisted of only 1 sense cover. This is reflected in
the total number of instances in Table 8.
</footnote>
<bodyText confidence="0.999891551724138">
test, with a null hypothesis that there was no signif-
icant increase. The result showed that the P-values
were 4.17 and 2.65 for nouns and verbs respectively,
which were both statistically significant. Therefore,
the null hypothesis was rejected, and we concluded
that there was a significant increase in K by using
our lexicon.
As a note, the average K&apos;s for the 191 words from
our lexicon and their corresponding random parti-
tions were .260 and .233 respectively. Those values
are in fact lower than that for the original WordNet
lexicon. There are two major reasons for this. First,
in general, combining any arbitrary senses does not
always increase K. In the given formula 9, K actually
decreases when the increase in Ei Pii (i.e., the diag-
onal sum) in the reduced matrix is less than the in-
crease in Ei Pi+P+i (i.e., the marginal product sum)
by some factor.14 This situation typically happens
when senses combined are well distinguished in the
original matrix, in the sense that, for senses i and j,
nij and nji are 0 or very small (relative to the total
frequency). Second, some systematic relations are in
fact easily distinguishable. Senses in such relations
often denote different objects in a context, for in-
stance ANIMAL and MEAT senses of &amp;quot;chicken&amp;quot;. Since
our lexicon groups those senses together, the K&apos;s for
the reduce matrices decrease for the reason we men-
tioned above. Table 8 shows the breakdown of the
average K for our lexicon and random groupings.
</bodyText>
<footnote confidence="0.963860666666667">
14This is because F_i Pi+P+i is subtracted in both the nu-
merator and the denominator in the r, formula. Note that
both F_i Pii and F_i Pi+P+i always increase when any ar-
bitrary senses are combined. The factor mentioned here is
1-Pi Pii
1-Pi Pi+P+i
</footnote>
<sectionHeader confidence="0.850633" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999860541666667">
As we reported in previous sections, our tree-cut
extraction method discovered 89% of the Word-
Net cousins. Although the precision was rela-
tively low (50-60%), this is an encouraging re-
sult. As for the lexicon, our sense partitions con-
sistently yielded better K values than arbitrary
sense groupings. We consider these results to
be quite promising. Our data is available at
www.depaul.edu/-ntomuro/research/naacl-01.html.
It is significant to note that cluster pairs and sense
partitions derived in this work are domain indepen-
dent. Such information is useful in broad-domain
applications, or as a background lexicon (Kilgarriff,
1997) in domain specific applications or text catego-
rization and IR tasks. For those tasks, we anticipate
that our extraction methods may be useful in deriv-
ing characteristics of the domains or given corpus,
as well as customizing the lexical resource. This is
our next future research.
For other future work, we plan to investigate an
automatic way of detecting and filtering unrelated
relations. We are also planning to compare our sense
partitions with the systematic disagreement ob-
tained by (Wiebe, et al., 1998)&apos;s automatic classifier.
</bodyText>
<sectionHeader confidence="0.994857" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999828666666667">
The author wishes to thank Steve Lytinen at
DePaul University and the anonymous reviewers for
very useful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.998276" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999692819277108">
Apresjan, J. (1973). Regular Polysemy. Linguistics,
(142).
Bruce, R. and Wiebe, J. (1998). Word-sense Dis-
tinguishability and Inter-coder Agreement. In
Proceedings of the COLING/ACL-98, Montreal,
Canada.
Buitelaar, P. (1998). CORELEX: Systematic Poly-
semy and Underspecification. Ph.D. dissertation,
Department of Computer Science, Brandeis Uni-
versity.
Buitelaar, P. (2000). Reducing Lexical Semantic
Complexity with Systematic Polysemous Classes
and Underspecification. In Proceedings of the
ANLP/NAACL-00 Workshop on Syntactic and
Semantic Complexity in Natural Language Pro-
cessing, Seattle, WA.
Carletta, J. (1996). Assessing Agreement on Clas-
sification Tasks: The Kappa Statistic, Computa-
tional Linguistics, 22(2).
Copestake, A. and Briscoe, T. (1995). Semi-
productive Polysemy and Sense Extension. Jour-
nal of Semantics, 12.
Gonzalo, J., Chugur, I. and Verdejo, F. (2000).
Sense Clusters for Information Retrieval: Evi-
dence from Semcor and the InterLingual Index.
In Proceedings of the ACL-2000 Workshop on
Word Senses and Multilinguality, Hong-Kong.
Kilgarriff, A. (1997). Foreground and Background
Lexicons and Word Sense Disambiguation for In-
formation Extraction. In Proceedings of the In-
ternational Workshop on Lexically Driven Infor-
mation Extraction.
Kilgarriff, A. (1998a). SENSEVAL: An Exercise
in Evaluating Word Sense Disambiguation Pro-
grams. In Proceedings of the LREC.
Kilgarriff, A. (1998b). Inter-tagger Agreement. In
Advanced Papers of the SENSEVAL Workshop,
Sussex, UK.
Landes, S., Leacock, C. and Tengi, R. (1998).
Building Semantic Concordance. In WordNet:
An Electronic Lexical Database, The MIT Press.
Li, H. and Abe, N. (1998). Generalizing Case
Frames Using a Thesaurus and the MDL Prin-
ciple, Computational Linguistics, 24(2).
Miller, G. (eds.) (1990). WORDNET: An Online
Lexical Database. International Journal of Lex-
icography, 3(4).
Ng, H.T., and Lee, H.B. (1996). Integrating Mul-
tiple Knowledge Sources to Disambiguate Word
Sense. In Proceedings of the ACL-96, Santa Cruz,
CA.
Ng, H.T., Lim, C. and Foo, S. (1999). A Case
Study on Inter-Annotator Agreement for Word
Sense Disambiguation. In Proceedings of the
ACL SIGLEX Workshop on Standardizing Lexi-
cal Resources, College Park, MD.
Nunberg, G. (1995). Transfers of Meaning. Journal
of Semantics, 12.
Palmer, M. (1998). Are Wordnet sense distinctions
appropriate for computational lexicons? In Ad-
vanced Papers of the SENSEVAL Workshop, Sus-
sex, UK.
Pustejovsky, J. (1995). The Generative Lexicon,
The MIT Press.
Rissanen, J. (1978). Modeling by Shortest Data
Description. Automatic, 14.
Tomuro, N. (2000). Automatic Extraction of Sys-
tematic Polysemy Using Tree-cut. In Proceedings
of the ANLP/NAACL-00 Workshop on Syntactic
and Semantic Complexity in Natural Language
Processing, Seattle, WA.
Veronis, J. (1998). A Study of Polysemy Judge-
ments and Inter-annotator Agreement. In Ad-
vanced Papers of the SENSEVAL Workshop, Sus-
sex, UK.
Vossen, P., Peters, W. and Gonzalo, J. (1999). To-
wards a Universal Index of Meaning. In Proceed-
ings of the ACL SIGLEX Workshop on Standard-
izing Lexical Resources, College Park, MD.
Wiebe, J., Bruce, R. and O&apos;Hara, T. (1999). De-
velopment and Use of a Gold-Standard Data Set
for Subjectivity Classifications. In Proceedings of
the ACL-99, College Park, MD.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.495077">
<title confidence="0.838478">Tree-cut and A Lexicon based on Systematic Polysemy Noriko</title>
<author confidence="0.869048">DePaul</author>
<affiliation confidence="0.999121">School of Computer Science, Telecommunications and Information</affiliation>
<address confidence="0.962182">243 S. Wabash Chicago, IL</address>
<email confidence="0.99905">tomuro@cs.depaul.edu</email>
<abstract confidence="0.979923222222222">paper describes a lexicon organized around sysa set of word senses that are related in systematic and predictable ways. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called We compare our lexicon to WordNet and the inter-annotator disagreement observed between WordNet Semcor and DSO corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Apresjan</author>
</authors>
<title>Regular Polysemy. Linguistics,</title>
<date>1973</date>
<contexts>
<context position="4230" citStr="Apresjan, 1973" startWordPosition="634" endWordPosition="635">n knowledge-intensive NLP tasks such as discourse analysis, IE and MT. More recently, (Gonzalo et al., 2000) also discusses potential usefulness of systematic polysemy for clustering word senses for IR. However, extracting systematic relations from large sense inventories is a difficult task. Most often, this procedure is done manually. For example, WordNet cousin relations were identified manually by the WordNet lexicographers. A similar effort was also made in the EuroWordnet project (Vossen et &apos;Systematic polysemy (in the sense we use in this paper) is also referred to as regular polysemy (Apresjan, 1973) or logical polysemy (Pustejovsky, 1995). 2Note that systematic polysemy should be contrasted with homonymy, which refers to words which have more than one unrelated sense (e.g. FINANCIAL INSTITUTION and SLOPING LAND meanings of the word &amp;quot;bank&amp;quot;). al., 1999). The problem is not only that manual inspection of a large, complex lexicon is very timeconsuming, it is also prone to inconsistencies. In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1</context>
<context position="17526" citStr="Apresjan, 1973" startWordPosition="2879" endWordPosition="2880">hat appear in both of the (sub)trees rooted by those synsets are related.&apos; The cousins were man&apos;Note that the relatedness between clusters was determined solely by our subjective judgement. That is because there is no existing large-scale lexicon which encodes related senses completely for all words in the lexicon. (Note that WordNet cousin relation is encoded only for some words). Although the distinction between related vs. unrelated meanings is sometimes unclear, systematicity of the related senses among words is quite intuitive and has been well studied in Lexical Semantics (for example, (Apresjan, 1973; Nunberg, 1995; Copestake and Briscoe, 1995)). A comparison with WordNet cousin is discussed in the next section 4. 9Actually, cousin is one of the three relations which indicate the grouping of related senses of a word. Others are sister and twin. In this paper, we use cousin to refer to all relations listed in &amp;quot;cousin.tps&amp;quot; file (available in a WordNet distribution). ually identified by the WordNet lexicographers. To compare the automatically derived cluster pairs to WordNet cousins, we used the hypernymhyponym relation in the trees, instead of the number or ratio of the overlapping words. T</context>
</contexts>
<marker>Apresjan, 1973</marker>
<rawString>Apresjan, J. (1973). Regular Polysemy. Linguistics, (142).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bruce</author>
<author>J Wiebe</author>
</authors>
<title>Word-sense Distinguishability and Inter-coder Agreement.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING/ACL-98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="25308" citStr="Bruce and Wiebe, 1998" startWordPosition="4204" endWordPosition="4207"> 1999) are slightly more than the ones reported in this paper. For instance, the number of sentences in the intersected corpus reported in (Ng et al., 1999) is 30,315. We speculate the discrepancies are due to the different sentence alignment methTable 5: Agreement between Semcor and DSO Category Agree Disagree Total Ave. K Nouns 6,528 5,815 12,343 .268 Verbs 7,408 9,021 16,429 .260 Total 13,936 14,836 28,772 .264 (%) (48.4) (51.6) (100.0) can see, the agreement is not very high: only around 48%.11 This low agreement ratio is also reflected in a measure called the K statistic (Carletta, 1996; Bruce and Wiebe, 1998; Ng et al., 1999). K measure takes into account chance agreement, thus better representing the state of disagreement. A K value is calculated for each word, on a confusion matrix where rows represent the senses assigned by judge 1 (DSO) and columns represent the senses assigned by judge 2 (Semcor). Table 6 shows an example matrix for the noun &amp;quot;table&amp;quot;. A K value for a word is calculated as follows. We use the notation and formula used in (Bruce and Wiebe, 1998). Let nib denote the number of instances where the judge 1 assigned sense i and the judge 2 assigned sense j to the same instance, and </context>
</contexts>
<marker>Bruce, Wiebe, 1998</marker>
<rawString>Bruce, R. and Wiebe, J. (1998). Word-sense Distinguishability and Inter-coder Agreement. In Proceedings of the COLING/ACL-98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
</authors>
<title>CORELEX: Systematic Polysemy and Underspecification.</title>
<date>1998</date>
<tech>Ph.D. dissertation,</tech>
<institution>Department of Computer Science, Brandeis University.</institution>
<contexts>
<context position="22476" citStr="Buitelaar, 1998" startWordPosition="3773" endWordPosition="3774">m connectedness. Finally, senses that are not covered by any sense covers are taken as singletons, and added to the final sense partition. So the sense partition for &amp;quot;table&amp;quot; becomes {(1 4),(2 3 5),(6)}. Table 4 shows the comparison between WordNet and our new lexicon. As you can see, our lexicon contains much less ambiguity: the ratio of monosemous words increased from 84% (88,650/105,461ti.84) to 92% (96,964/105,461ti.92), and the average number of senses for polysemous words decreased from 2.73 to 2.52 for nouns, and from 3.57 to 2.82 for verbs. As a note, our lexicon is similar to CORELEX (Buitelaar, 1998) (or CORELEX-II presented in (Buitelaar, 2000)), in that both lexicons share the same motivation. However, our lexicon differs from CORELEX in that CORELEX looks at all senses of a word and groups words that have the same sense distribution pattern, whereas our lexicon groups Table 2: Examples of Automatically Extracted Systematic Polysemy Underspecified Class Cluster Pair Common Words ACTION-LOCATION [ACTION, POINT] &amp;quot;drop&amp;quot;, &amp;quot;circle&amp;quot;, &amp;quot;intersection&amp;quot;, &amp;quot;dig&amp;quot;, &amp;quot;crossing&amp;quot;, &amp;quot;bull&apos;s eye&amp;quot; ARTIFACT-GROUP [STRUCTURE, PEOPLE] &amp;quot;house&amp;quot;, &amp;quot;convent&amp;quot;, &amp;quot;market&amp;quot;, &amp;quot;center&amp;quot; ARTIFACT-SUBSTANCE [FABRIC, CHEMICAL CO</context>
</contexts>
<marker>Buitelaar, 1998</marker>
<rawString>Buitelaar, P. (1998). CORELEX: Systematic Polysemy and Underspecification. Ph.D. dissertation, Department of Computer Science, Brandeis University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
</authors>
<title>Reducing Lexical Semantic Complexity with Systematic Polysemous Classes and Underspecification.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP/NAACL-00 Workshop on Syntactic and Semantic Complexity in Natural Language Processing,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="22522" citStr="Buitelaar, 2000" startWordPosition="3779" endWordPosition="3780">covered by any sense covers are taken as singletons, and added to the final sense partition. So the sense partition for &amp;quot;table&amp;quot; becomes {(1 4),(2 3 5),(6)}. Table 4 shows the comparison between WordNet and our new lexicon. As you can see, our lexicon contains much less ambiguity: the ratio of monosemous words increased from 84% (88,650/105,461ti.84) to 92% (96,964/105,461ti.92), and the average number of senses for polysemous words decreased from 2.73 to 2.52 for nouns, and from 3.57 to 2.82 for verbs. As a note, our lexicon is similar to CORELEX (Buitelaar, 1998) (or CORELEX-II presented in (Buitelaar, 2000)), in that both lexicons share the same motivation. However, our lexicon differs from CORELEX in that CORELEX looks at all senses of a word and groups words that have the same sense distribution pattern, whereas our lexicon groups Table 2: Examples of Automatically Extracted Systematic Polysemy Underspecified Class Cluster Pair Common Words ACTION-LOCATION [ACTION, POINT] &amp;quot;drop&amp;quot;, &amp;quot;circle&amp;quot;, &amp;quot;intersection&amp;quot;, &amp;quot;dig&amp;quot;, &amp;quot;crossing&amp;quot;, &amp;quot;bull&apos;s eye&amp;quot; ARTIFACT-GROUP [STRUCTURE, PEOPLE] &amp;quot;house&amp;quot;, &amp;quot;convent&amp;quot;, &amp;quot;market&amp;quot;, &amp;quot;center&amp;quot; ARTIFACT-SUBSTANCE [FABRIC, CHEMICAL COMPOUND] &amp;quot;acetate&amp;quot;, &amp;quot;nylon&amp;quot;, &amp;quot;acrylic&amp;quot;, &amp;quot;polyes</context>
</contexts>
<marker>Buitelaar, 2000</marker>
<rawString>Buitelaar, P. (2000). Reducing Lexical Semantic Complexity with Systematic Polysemous Classes and Underspecification. In Proceedings of the ANLP/NAACL-00 Workshop on Syntactic and Semantic Complexity in Natural Language Processing, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing Agreement on Classification Tasks: The Kappa Statistic,</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="5530" citStr="Carletta, 1996" startWordPosition="843" endWordPosition="845">f WordNet nouns and showed potential applicability. In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996). The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better K values (Carletta, 1996) than arbitrary sense groupings on the agreement data. 2 The Tree-cut Technique The tree-cut technique is an unsupervised learning technique which partitions data items organized in a tree structure into mutually-disjoint clusters. It was originally proposed in (Li and Abe, 1998), and then adopted in our previous method for automatically extracting systematic polysemy (Tomuro, 2000). In this section, we give a brief summary of this tree-cut technique using examples from (Li and Abe, 1998)&apos;s original work. 2.1 Tree-cut Models The tree-cut technique is applied to data items that are organized in</context>
<context position="25285" citStr="Carletta, 1996" startWordPosition="4202" endWordPosition="4203">d in (Ng et al., 1999) are slightly more than the ones reported in this paper. For instance, the number of sentences in the intersected corpus reported in (Ng et al., 1999) is 30,315. We speculate the discrepancies are due to the different sentence alignment methTable 5: Agreement between Semcor and DSO Category Agree Disagree Total Ave. K Nouns 6,528 5,815 12,343 .268 Verbs 7,408 9,021 16,429 .260 Total 13,936 14,836 28,772 .264 (%) (48.4) (51.6) (100.0) can see, the agreement is not very high: only around 48%.11 This low agreement ratio is also reflected in a measure called the K statistic (Carletta, 1996; Bruce and Wiebe, 1998; Ng et al., 1999). K measure takes into account chance agreement, thus better representing the state of disagreement. A K value is calculated for each word, on a confusion matrix where rows represent the senses assigned by judge 1 (DSO) and columns represent the senses assigned by judge 2 (Semcor). Table 6 shows an example matrix for the noun &amp;quot;table&amp;quot;. A K value for a word is calculated as follows. We use the notation and formula used in (Bruce and Wiebe, 1998). Let nib denote the number of instances where the judge 1 assigned sense i and the judge 2 assigned sense j to </context>
<context position="27657" citStr="Carletta, 1996" startWordPosition="4703" endWordPosition="4704">titions 1,4 1,4 2,3,5 2,3,5 6 Total 44 0 6 20 2 3 52 23 6 Total 0 44 0 0 26 5 0 75 Category Total Our Lexicon Random Ave. K Ave. K Nouns 10,980 .247 .217 Verbs 14,392 .283 .262 Total 25,372 .260 .233 by chance (i.e., values in a row (or column) are uniformly distributed across rows (or columns), that is, Pii = Pi+P+i for all 1 G_ i G_ M, where M is the number of rows/columns). K also takes a negative value when there is a systematic disagreement between the two judges (e.g., some values in the diagonal cells are 0, that is, Pii = 0 for some i). Normally, K &gt; .8 is considered a good agreement (Carletta, 1996). By using the formula above, the average K for the 191 words was .264, as shown in Table 5.12 This means the agreement between Semcor and DSO is quite low. We selected the same 191 words from our lexicon, and used their sense partitions to reduce the size of the confusion matrices. For each word, we computed the K for the reduced matrix, and compared it with the K for a random sense grouping of the same partition pattern.13 For example, the partition pattern of J(1 4),(2 3 5),(6)1 for &amp;quot;table&amp;quot; mentioned earlier (where Table 7 shows its reduced matrix) is a multinomial combination (2 3 1). The </context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Carletta, J. (1996). Assessing Agreement on Classification Tasks: The Kappa Statistic, Computational Linguistics, 22(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>T Briscoe</author>
</authors>
<title>Semiproductive Polysemy and Sense Extension.</title>
<date>1995</date>
<journal>Journal of Semantics,</journal>
<volume>12</volume>
<contexts>
<context position="17571" citStr="Copestake and Briscoe, 1995" startWordPosition="2883" endWordPosition="2886">trees rooted by those synsets are related.&apos; The cousins were man&apos;Note that the relatedness between clusters was determined solely by our subjective judgement. That is because there is no existing large-scale lexicon which encodes related senses completely for all words in the lexicon. (Note that WordNet cousin relation is encoded only for some words). Although the distinction between related vs. unrelated meanings is sometimes unclear, systematicity of the related senses among words is quite intuitive and has been well studied in Lexical Semantics (for example, (Apresjan, 1973; Nunberg, 1995; Copestake and Briscoe, 1995)). A comparison with WordNet cousin is discussed in the next section 4. 9Actually, cousin is one of the three relations which indicate the grouping of related senses of a word. Others are sister and twin. In this paper, we use cousin to refer to all relations listed in &amp;quot;cousin.tps&amp;quot; file (available in a WordNet distribution). ually identified by the WordNet lexicographers. To compare the automatically derived cluster pairs to WordNet cousins, we used the hypernymhyponym relation in the trees, instead of the number or ratio of the overlapping words. This is because the levels at which the cousin</context>
</contexts>
<marker>Copestake, Briscoe, 1995</marker>
<rawString>Copestake, A. and Briscoe, T. (1995). Semiproductive Polysemy and Sense Extension. Journal of Semantics, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gonzalo</author>
<author>I Chugur</author>
<author>F Verdejo</author>
</authors>
<title>Sense Clusters for Information Retrieval: Evidence from Semcor and the InterLingual Index.</title>
<date>2000</date>
<contexts>
<context position="3723" citStr="Gonzalo et al., 2000" startWordPosition="555" endWordPosition="558">ags (Buitelaar, personal communication). Second, many systematic relations are observed between senses which belong to different semantic categories. So if a lexicon is defined by a collection of separate trees/hierarchies (such as the case of WordNet), systematic polysemy can express similarity between senses that are not hierarchically proximate. Third, by explicitly representing (inter-)relations between senses, a lexicon based on systematic polysemy can facilitate semantic inferences. Thus it is useful in knowledge-intensive NLP tasks such as discourse analysis, IE and MT. More recently, (Gonzalo et al., 2000) also discusses potential usefulness of systematic polysemy for clustering word senses for IR. However, extracting systematic relations from large sense inventories is a difficult task. Most often, this procedure is done manually. For example, WordNet cousin relations were identified manually by the WordNet lexicographers. A similar effort was also made in the EuroWordnet project (Vossen et &apos;Systematic polysemy (in the sense we use in this paper) is also referred to as regular polysemy (Apresjan, 1973) or logical polysemy (Pustejovsky, 1995). 2Note that systematic polysemy should be contrasted</context>
</contexts>
<marker>Gonzalo, Chugur, Verdejo, 2000</marker>
<rawString>Gonzalo, J., Chugur, I. and Verdejo, F. (2000). Sense Clusters for Information Retrieval: Evidence from Semcor and the InterLingual Index.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the ACL-2000 Workshop on Word Senses and Multilinguality, Hong-Kong.</booktitle>
<marker></marker>
<rawString>In Proceedings of the ACL-2000 Workshop on Word Senses and Multilinguality, Hong-Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>Foreground and Background Lexicons and Word Sense Disambiguation for Information Extraction.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Workshop on Lexically Driven Information Extraction.</booktitle>
<contexts>
<context position="31287" citStr="Kilgarriff, 1997" startWordPosition="5325" endWordPosition="5326">previous sections, our tree-cut extraction method discovered 89% of the WordNet cousins. Although the precision was relatively low (50-60%), this is an encouraging result. As for the lexicon, our sense partitions consistently yielded better K values than arbitrary sense groupings. We consider these results to be quite promising. Our data is available at www.depaul.edu/-ntomuro/research/naacl-01.html. It is significant to note that cluster pairs and sense partitions derived in this work are domain independent. Such information is useful in broad-domain applications, or as a background lexicon (Kilgarriff, 1997) in domain specific applications or text categorization and IR tasks. For those tasks, we anticipate that our extraction methods may be useful in deriving characteristics of the domains or given corpus, as well as customizing the lexical resource. This is our next future research. For other future work, we plan to investigate an automatic way of detecting and filtering unrelated relations. We are also planning to compare our sense partitions with the systematic disagreement obtained by (Wiebe, et al., 1998)&apos;s automatic classifier. Acknowledgments The author wishes to thank Steve Lytinen at DeP</context>
</contexts>
<marker>Kilgarriff, 1997</marker>
<rawString>Kilgarriff, A. (1997). Foreground and Background Lexicons and Word Sense Disambiguation for Information Extraction. In Proceedings of the International Workshop on Lexically Driven Information Extraction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>SENSEVAL: An Exercise in Evaluating Word Sense Disambiguation Programs.</title>
<date>1998</date>
<booktitle>In Proceedings of the LREC.</booktitle>
<contexts>
<context position="783" citStr="Kilgarriff, 1998" startWordPosition="111" endWordPosition="113">h Ave. Chicago, IL 60604 tomuro@cs.depaul.edu Abstract This paper describes a lexicon organized around systematic polysemy: a set of word senses that are related in systematic and predictable ways. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut. We compare our lexicon to WordNet cousins, and the inter-annotator disagreement observed between WordNet Semcor and DSO corpora. 1 Introduction In recent years, the granularity of word senses for computational lexicons has been discussed frequently in Lexical Semantics (for example, (Kilgarriff, 1998a; Palmer, 1998)). This issue emerged as a prominent problem after previous studies and exercises in Word Sense Disambiguation (WSD) reported that, when fine-grained sense definitions such as those in WordNet (Miller, 1990) were used, entries became very similar and indistinguishable to human annotators, thereby causing disagreement on correct tags (Kilgarriff, 1998b; Veronis, 1998; Ng et al., 1999). In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), </context>
</contexts>
<marker>Kilgarriff, 1998</marker>
<rawString>Kilgarriff, A. (1998a). SENSEVAL: An Exercise in Evaluating Word Sense Disambiguation Programs. In Proceedings of the LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>Inter-tagger Agreement.</title>
<date>1998</date>
<booktitle>In Advanced Papers of the SENSEVAL Workshop,</booktitle>
<location>Sussex, UK.</location>
<contexts>
<context position="783" citStr="Kilgarriff, 1998" startWordPosition="111" endWordPosition="113">h Ave. Chicago, IL 60604 tomuro@cs.depaul.edu Abstract This paper describes a lexicon organized around systematic polysemy: a set of word senses that are related in systematic and predictable ways. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut. We compare our lexicon to WordNet cousins, and the inter-annotator disagreement observed between WordNet Semcor and DSO corpora. 1 Introduction In recent years, the granularity of word senses for computational lexicons has been discussed frequently in Lexical Semantics (for example, (Kilgarriff, 1998a; Palmer, 1998)). This issue emerged as a prominent problem after previous studies and exercises in Word Sense Disambiguation (WSD) reported that, when fine-grained sense definitions such as those in WordNet (Miller, 1990) were used, entries became very similar and indistinguishable to human annotators, thereby causing disagreement on correct tags (Kilgarriff, 1998b; Veronis, 1998; Ng et al., 1999). In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), </context>
</contexts>
<marker>Kilgarriff, 1998</marker>
<rawString>Kilgarriff, A. (1998b). Inter-tagger Agreement. In Advanced Papers of the SENSEVAL Workshop, Sussex, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Landes</author>
<author>C Leacock</author>
<author>R Tengi</author>
</authors>
<title>Building Semantic Concordance. In WordNet: An Electronic Lexical Database,</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5327" citStr="Landes et al., 1998" startWordPosition="807" endWordPosition="810">icon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work (Tomuro, 2000), we applied this method to a small subset of WordNet nouns and showed potential applicability. In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996). The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better K values (Carletta, 1996) than arbitrary sense groupings on the agreement data. 2 The Tree-cut Technique The tree-cut technique is an unsupervised learning technique which partitions data items organized in a tree structure into mutually-disjoint clusters. It was originally proposed in (Li and Abe, 1998), and then adopted in our previous method for automatically extracting systematic polysemy (Tomuro, 2000). In this se</context>
<context position="24090" citStr="Landes et al., 1998" startWordPosition="3993" endWordPosition="3996"> Polysemous 4,568 2,339 Total words 10,326 10,326 Ave # senses 3.57 2.82 Total Monosemous 88,650 96,964 Polysemous 16,811 8,497 Total words 105,461 105,461 word senses that have the same systematic relation. Thus, our lexicon represents systematic polysemy at a finer level than CORELEX, by pinpointing related senses within each word. 6 Evaluation: Inter-annotator Disagreement To test if the sense partitions in our lexicon constitute an appropriate (or useful) level of granularity, we applied it to the inter-annotator disagreement observed in two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996). The agreement between those corpora is previously studied in (Ng et al., 1999). In our current work, we first re-produced their agreement data, then used our sense partitions to see whether or not they yield a better agreement. In this experiment, we extracted 28,772 sentences/instances for 191 words (consisting of 121 nouns and 70 verbs) tagged in the intersection of the two corpora. This constitutes the base data set. Table 5 shows the breakdown of the number of instances where tags agreed and disagreed.10 As you 10Note that the numbers reported in (Ng et al., 19</context>
</contexts>
<marker>Landes, Leacock, Tengi, 1998</marker>
<rawString>Landes, S., Leacock, C. and Tengi, R. (1998). Building Semantic Concordance. In WordNet: An Electronic Lexical Database, The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Generalizing Case Frames Using a Thesaurus and the MDL Principle,</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="4834" citStr="Li and Abe, 1998" startWordPosition="726" endWordPosition="729">resjan, 1973) or logical polysemy (Pustejovsky, 1995). 2Note that systematic polysemy should be contrasted with homonymy, which refers to words which have more than one unrelated sense (e.g. FINANCIAL INSTITUTION and SLOPING LAND meanings of the word &amp;quot;bank&amp;quot;). al., 1999). The problem is not only that manual inspection of a large, complex lexicon is very timeconsuming, it is also prone to inconsistencies. In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work (Tomuro, 2000), we applied this method to a small subset of WordNet nouns and showed potential applicability. In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996). The results are quite promising: our extraction method discovered 89% of the W</context>
<context position="7005" citStr="Li and Abe, 1998" startWordPosition="1078" endWordPosition="1081">t of internal/leaf nodes in the tree, and each node represents a set of all leaf nodes in a subtree rooted by the node. Such a set is also considered as a cluster.3 Clusters in a tree-cut exhaustively cover all leaf nodes of the tree, and they are mutually disjoint. For instance, Figure 1 shows an example thesaurus tree and one possible tree-cut [AIRCRAFT, ball, kite, puzzle], which is indicated by a thick curve in the figure. There are also four other possible tree-cuts for this tree: [airplane, helicopter, ball, kite, puzzle], [airplane, helicopter, TOY], [AIRCRAFT, TOY] and [ARTIFACT]. In (Li and Abe, 1998), the tree-cut technique was applied to the problem of acquiring general3A leaf node is also a cluster whose cardinality is 1. ized case frame patterns from a corpus. Thus, each node/word in the tree received as its value the number of instances where the word occurred as a case role (subject, object etc.) of a given verb. Then the acquisition of a generalized case frame was viewed as a problem of selecting the best tree-cut model that estimates the true probability distribution, given a sample corpus data. Formally, a tree-cut model M is a pair consisting of a tree-cut F and a probability par</context>
<context position="8419" citStr="Li and Abe, 1998" startWordPosition="1345" endWordPosition="1348">ster Ci, and Pki=1 P(Ci) = 1. Note that P(C) is the probability of cluster C = {n1, ••, nm } as a whole, that is, P(C) = Pj=1 P(nj). For example, suppose a corpus contains 10 instances of verb-object relation for the verb &amp;quot;fly&amp;quot;, and the frequencies of object nouns n, denoted f(n), are as follows: f(airplane) = 5, f(helicopter) = 3, f(ball) = 0, f(kite) = 2, f(puzzle) = 0. Then, the set of treecut models for the example thesaurus tree shown in Figure 1 includes ([airplane, helicopter, TOY], [.5, .3, .2]) and ([AIRCRAFT, TOY], [.8, .2]). 2.2 The MDL Principle To select the best tree-cut model, (Li and Abe, 1998) uses the Minimal Description Length (MDL). The MDL is a principle of data compression in Information Theory which states that, for a given dataset, the best model is the one which requires the minimum length (often measured in bits) to encode the model (the model description length) and the data (the data description length) (Rissanen, 1978). Thus, the MDL principle captures the trade-off between the simplicity of a model, which is measured by the number of clusters in a tree-cut, and the goodness of fit to the data, which is measured by the estimation accuracy of the probability distribution</context>
<context position="11235" citStr="Li and Abe, 1998" startWordPosition="1845" endWordPosition="1848"> = P(C) and P(C) = f(S1) (7) Note the equation (7) essentially computes the Maximum Likelihood Estimate (MLE) for all n.5 A table in Figure 1 shows the MDL lengths for all five tree-cut models. The best model is the one with the tree-cut [AIRCRAFT, ball, kite, puzzle]. 3 Clustering Systematic Polysemy Using the tree-cut technique described above, our previous work (Tomuro, 2000) extracted systematic polysemy from WordNet. In this section, we give a summary of this method, and describe the cluster pairs obtained by the method. 4For justification and detailed explanation of these formulas, see (Li and Abe, 1998). 5 In our previous work, we used entropy instead of NILE. That is because the lexicon represents true population, not samples; thus there is no additional data to estimate. 3.1 Extraction Method In our previous work, systematically related word senses are derived as binary cluster pairs, by applying the extraction procedure to a combination of two WordNet (sub)trees. This process is done in the following three steps. In the first step, all leaf nodes of the two trees are assigned a value of either 1, if a node/word appears in both trees, or 0 otherwise.6 In the second step, the tree-cut techn</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Li, H. and Abe, N. (1998). Generalizing Case Frames Using a Thesaurus and the MDL Principle, Computational Linguistics, 24(2).</rawString>
</citation>
<citation valid="true">
<title>WORDNET: An Online Lexical Database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<editor>Miller, G. (eds.)</editor>
<marker>1990</marker>
<rawString>Miller, G. (eds.) (1990). WORDNET: An Online Lexical Database. International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>H B Lee</author>
</authors>
<title>Integrating Multiple Knowledge Sources to Disambiguate Word Sense.</title>
<date>1996</date>
<booktitle>In Proceedings of the ACL-96,</booktitle>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="5354" citStr="Ng and Lee, 1996" startWordPosition="813" endWordPosition="816">omatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work (Tomuro, 2000), we applied this method to a small subset of WordNet nouns and showed potential applicability. In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996). The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better K values (Carletta, 1996) than arbitrary sense groupings on the agreement data. 2 The Tree-cut Technique The tree-cut technique is an unsupervised learning technique which partitions data items organized in a tree structure into mutually-disjoint clusters. It was originally proposed in (Li and Abe, 1998), and then adopted in our previous method for automatically extracting systematic polysemy (Tomuro, 2000). In this section, we give a brief summ</context>
<context position="24117" citStr="Ng and Lee, 1996" startWordPosition="3999" endWordPosition="4002">words 10,326 10,326 Ave # senses 3.57 2.82 Total Monosemous 88,650 96,964 Polysemous 16,811 8,497 Total words 105,461 105,461 word senses that have the same systematic relation. Thus, our lexicon represents systematic polysemy at a finer level than CORELEX, by pinpointing related senses within each word. 6 Evaluation: Inter-annotator Disagreement To test if the sense partitions in our lexicon constitute an appropriate (or useful) level of granularity, we applied it to the inter-annotator disagreement observed in two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996). The agreement between those corpora is previously studied in (Ng et al., 1999). In our current work, we first re-produced their agreement data, then used our sense partitions to see whether or not they yield a better agreement. In this experiment, we extracted 28,772 sentences/instances for 191 words (consisting of 121 nouns and 70 verbs) tagged in the intersection of the two corpora. This constitutes the base data set. Table 5 shows the breakdown of the number of instances where tags agreed and disagreed.10 As you 10Note that the numbers reported in (Ng et al., 1999) are slightly more than </context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Ng, H.T., and Lee, H.B. (1996). Integrating Multiple Knowledge Sources to Disambiguate Word Sense. In Proceedings of the ACL-96, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>C Lim</author>
<author>S Foo</author>
</authors>
<title>A Case Study on Inter-Annotator Agreement for Word Sense Disambiguation.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL SIGLEX Workshop on Standardizing Lexical Resources,</booktitle>
<location>College Park, MD.</location>
<contexts>
<context position="1185" citStr="Ng et al., 1999" startWordPosition="170" endWordPosition="173">d between WordNet Semcor and DSO corpora. 1 Introduction In recent years, the granularity of word senses for computational lexicons has been discussed frequently in Lexical Semantics (for example, (Kilgarriff, 1998a; Palmer, 1998)). This issue emerged as a prominent problem after previous studies and exercises in Word Sense Disambiguation (WSD) reported that, when fine-grained sense definitions such as those in WordNet (Miller, 1990) were used, entries became very similar and indistinguishable to human annotators, thereby causing disagreement on correct tags (Kilgarriff, 1998b; Veronis, 1998; Ng et al., 1999). In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), as well as in Information Retrieval (IR), since the difference in the correct sense assignments affects recall, precision and other evaluation measures. In response to this, several approaches have been proposed which group fine-grained word senses in various ways to derive coarse-grained sense groups. Some approaches utilize an abstraction hierarchy defined in a dictionary (Kilgarriff, 1998b), whil</context>
<context position="24197" citStr="Ng et al., 1999" startWordPosition="4012" endWordPosition="4015">ous 16,811 8,497 Total words 105,461 105,461 word senses that have the same systematic relation. Thus, our lexicon represents systematic polysemy at a finer level than CORELEX, by pinpointing related senses within each word. 6 Evaluation: Inter-annotator Disagreement To test if the sense partitions in our lexicon constitute an appropriate (or useful) level of granularity, we applied it to the inter-annotator disagreement observed in two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996). The agreement between those corpora is previously studied in (Ng et al., 1999). In our current work, we first re-produced their agreement data, then used our sense partitions to see whether or not they yield a better agreement. In this experiment, we extracted 28,772 sentences/instances for 191 words (consisting of 121 nouns and 70 verbs) tagged in the intersection of the two corpora. This constitutes the base data set. Table 5 shows the breakdown of the number of instances where tags agreed and disagreed.10 As you 10Note that the numbers reported in (Ng et al., 1999) are slightly more than the ones reported in this paper. For instance, the number of sentences in the in</context>
<context position="26367" citStr="Ng et al., 1999" startWordPosition="4403" endWordPosition="4406">Bruce and Wiebe, 1998). Let nib denote the number of instances where the judge 1 assigned sense i and the judge 2 assigned sense j to the same instance, and ni+ and n+i denote the marginal totals of rows and columns respectively. The formula is: where Pii = nii n++ (i.e., proportion of nii, the number of instances where both judges agreed on sense i, to the total instances), Pi+ = ni+ n++ and P+i = n+i n++ . The K value is 1.0 when the agreement is perfect (i.e., values in the off-diagonal cells are all 0, that is, Ei Pii = 1), or 0 when the agreement is purely ods used in the experiments. 11(Ng et al., 1999) reports a higher agreement of 57%. We speculate the discrepancy might be from the version of WordNet senses used in DSO, which was slightly different from the standard delivery version (as noted in (Ng et al., 1999)). Ei Pii —Ei Pi+P+i k = (9) 1 — Ei Pi+P+i Table 6: Confusion Matrix for the noun &amp;quot;table&amp;quot; (K = .611) Judge 2 (Semcor) 1 2 3 4 5 6 Total 43 0 0 0 0 0 6 17 3 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 2 1 0 0 0 43 (= n1+) 26 (= n2+) 0 (= n3+) 1 (= n4+) 0 (= n5+) 5 (= n6+) 75 Total 52 19 4 0 0 0 (= n+1) (= n+2) (= n+3) (= n+4) (= n+5) (= n+6) (= n++) 1 2 Judge 1 3 (DSO) 4 5 6 Table 7</context>
<context position="28700" citStr="Ng et al. 1999" startWordPosition="4893" endWordPosition="4896"> For example, the partition pattern of J(1 4),(2 3 5),(6)1 for &amp;quot;table&amp;quot; mentioned earlier (where Table 7 shows its reduced matrix) is a multinomial combination (2 3 1). The K value for a random grouping is obtained by generating 5,000 random partitions which have the same pattern as the corresponding sense partition in our lexicon, then taking the mean of their K&apos;s. Then we measured the possible increase in K by our lexicon by taking the difference between the paired K values for all words (i.e., Kw by our sense partition - Kw by random partition, for a word w), and performed a significance 12(Ng et al. 1999)&apos;s result is slightly higher: r, = .317. 13For this comparison, we excluded 23 words whose sense partitions consisted of only 1 sense cover. This is reflected in the total number of instances in Table 8. test, with a null hypothesis that there was no significant increase. The result showed that the P-values were 4.17 and 2.65 for nouns and verbs respectively, which were both statistically significant. Therefore, the null hypothesis was rejected, and we concluded that there was a significant increase in K by using our lexicon. As a note, the average K&apos;s for the 191 words from our lexicon and th</context>
</contexts>
<marker>Ng, Lim, Foo, 1999</marker>
<rawString>Ng, H.T., Lim, C. and Foo, S. (1999). A Case Study on Inter-Annotator Agreement for Word Sense Disambiguation. In Proceedings of the ACL SIGLEX Workshop on Standardizing Lexical Resources, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Nunberg</author>
</authors>
<title>Transfers of Meaning.</title>
<date>1995</date>
<journal>Journal of Semantics,</journal>
<volume>12</volume>
<contexts>
<context position="17541" citStr="Nunberg, 1995" startWordPosition="2881" endWordPosition="2882">th of the (sub)trees rooted by those synsets are related.&apos; The cousins were man&apos;Note that the relatedness between clusters was determined solely by our subjective judgement. That is because there is no existing large-scale lexicon which encodes related senses completely for all words in the lexicon. (Note that WordNet cousin relation is encoded only for some words). Although the distinction between related vs. unrelated meanings is sometimes unclear, systematicity of the related senses among words is quite intuitive and has been well studied in Lexical Semantics (for example, (Apresjan, 1973; Nunberg, 1995; Copestake and Briscoe, 1995)). A comparison with WordNet cousin is discussed in the next section 4. 9Actually, cousin is one of the three relations which indicate the grouping of related senses of a word. Others are sister and twin. In this paper, we use cousin to refer to all relations listed in &amp;quot;cousin.tps&amp;quot; file (available in a WordNet distribution). ually identified by the WordNet lexicographers. To compare the automatically derived cluster pairs to WordNet cousins, we used the hypernymhyponym relation in the trees, instead of the number or ratio of the overlapping words. This is because </context>
</contexts>
<marker>Nunberg, 1995</marker>
<rawString>Nunberg, G. (1995). Transfers of Meaning. Journal of Semantics, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
</authors>
<title>Are Wordnet sense distinctions appropriate for computational lexicons?</title>
<date>1998</date>
<booktitle>In Advanced Papers of the SENSEVAL Workshop,</booktitle>
<location>Sussex, UK.</location>
<contexts>
<context position="799" citStr="Palmer, 1998" startWordPosition="114" endWordPosition="115">60604 tomuro@cs.depaul.edu Abstract This paper describes a lexicon organized around systematic polysemy: a set of word senses that are related in systematic and predictable ways. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut. We compare our lexicon to WordNet cousins, and the inter-annotator disagreement observed between WordNet Semcor and DSO corpora. 1 Introduction In recent years, the granularity of word senses for computational lexicons has been discussed frequently in Lexical Semantics (for example, (Kilgarriff, 1998a; Palmer, 1998)). This issue emerged as a prominent problem after previous studies and exercises in Word Sense Disambiguation (WSD) reported that, when fine-grained sense definitions such as those in WordNet (Miller, 1990) were used, entries became very similar and indistinguishable to human annotators, thereby causing disagreement on correct tags (Kilgarriff, 1998b; Veronis, 1998; Ng et al., 1999). In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), as well as in In</context>
</contexts>
<marker>Palmer, 1998</marker>
<rawString>Palmer, M. (1998). Are Wordnet sense distinctions appropriate for computational lexicons? In Advanced Papers of the SENSEVAL Workshop, Sussex, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>The Generative Lexicon,</title>
<date>1995</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="4270" citStr="Pustejovsky, 1995" startWordPosition="639" endWordPosition="640">as discourse analysis, IE and MT. More recently, (Gonzalo et al., 2000) also discusses potential usefulness of systematic polysemy for clustering word senses for IR. However, extracting systematic relations from large sense inventories is a difficult task. Most often, this procedure is done manually. For example, WordNet cousin relations were identified manually by the WordNet lexicographers. A similar effort was also made in the EuroWordnet project (Vossen et &apos;Systematic polysemy (in the sense we use in this paper) is also referred to as regular polysemy (Apresjan, 1973) or logical polysemy (Pustejovsky, 1995). 2Note that systematic polysemy should be contrasted with homonymy, which refers to words which have more than one unrelated sense (e.g. FINANCIAL INSTITUTION and SLOPING LAND meanings of the word &amp;quot;bank&amp;quot;). al., 1999). The problem is not only that manual inspection of a large, complex lexicon is very timeconsuming, it is also prone to inconsistencies. In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work (Tomuro, 2000</context>
</contexts>
<marker>Pustejovsky, 1995</marker>
<rawString>Pustejovsky, J. (1995). The Generative Lexicon, The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Modeling by Shortest Data Description.</title>
<date>1978</date>
<journal>Automatic,</journal>
<volume>14</volume>
<contexts>
<context position="8763" citStr="Rissanen, 1978" startWordPosition="1405" endWordPosition="1406"> f(kite) = 2, f(puzzle) = 0. Then, the set of treecut models for the example thesaurus tree shown in Figure 1 includes ([airplane, helicopter, TOY], [.5, .3, .2]) and ([AIRCRAFT, TOY], [.8, .2]). 2.2 The MDL Principle To select the best tree-cut model, (Li and Abe, 1998) uses the Minimal Description Length (MDL). The MDL is a principle of data compression in Information Theory which states that, for a given dataset, the best model is the one which requires the minimum length (often measured in bits) to encode the model (the model description length) and the data (the data description length) (Rissanen, 1978). Thus, the MDL principle captures the trade-off between the simplicity of a model, which is measured by the number of clusters in a tree-cut, and the goodness of fit to the data, which is measured by the estimation accuracy of the probability distribution. The calculation of the description length for a tree-cut model is as follows. Given a thesaurus tree T and a sample S consisting of the case frame instances, the total description length L(M, S) for a tree-cut model M = (F, O) is L(M, S) = L(F) + L(01F) + L(S1F, o) (3) where L(F) is the model description length, L(01F) is the parameter desc</context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>Rissanen, J. (1978). Modeling by Shortest Data Description. Automatic, 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Tomuro</author>
</authors>
<title>Automatic Extraction of Systematic Polysemy Using Tree-cut.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP/NAACL-00 Workshop on Syntactic and Semantic Complexity in Natural Language Processing,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="4871" citStr="Tomuro, 2000" startWordPosition="734" endWordPosition="735">ovsky, 1995). 2Note that systematic polysemy should be contrasted with homonymy, which refers to words which have more than one unrelated sense (e.g. FINANCIAL INSTITUTION and SLOPING LAND meanings of the word &amp;quot;bank&amp;quot;). al., 1999). The problem is not only that manual inspection of a large, complex lexicon is very timeconsuming, it is also prone to inconsistencies. In this paper, we describes a lexicon organized around systematic polysemy. The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998). In our previous work (Tomuro, 2000), we applied this method to a small subset of WordNet nouns and showed potential applicability. In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy. We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996). The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partiti</context>
<context position="10999" citStr="Tomuro, 2000" startWordPosition="1809" endWordPosition="1810">gth of O, and 1S1 is the size of S. Finally, the data description length L(S1F; O) is the length required to encode the whole sample data. It is calculated as L(S1F; O) = — log2P(n) (6) nES where, for each n E C and each C E F, P(n) = P(C) and P(C) = f(S1) (7) Note the equation (7) essentially computes the Maximum Likelihood Estimate (MLE) for all n.5 A table in Figure 1 shows the MDL lengths for all five tree-cut models. The best model is the one with the tree-cut [AIRCRAFT, ball, kite, puzzle]. 3 Clustering Systematic Polysemy Using the tree-cut technique described above, our previous work (Tomuro, 2000) extracted systematic polysemy from WordNet. In this section, we give a summary of this method, and describe the cluster pairs obtained by the method. 4For justification and detailed explanation of these formulas, see (Li and Abe, 1998). 5 In our previous work, we used entropy instead of NILE. That is because the lexicon represents true population, not samples; thus there is no additional data to estimate. 3.1 Extraction Method In our previous work, systematically related word senses are derived as binary cluster pairs, by applying the extraction procedure to a combination of two WordNet (sub)</context>
</contexts>
<marker>Tomuro, 2000</marker>
<rawString>Tomuro, N. (2000). Automatic Extraction of Systematic Polysemy Using Tree-cut. In Proceedings of the ANLP/NAACL-00 Workshop on Syntactic and Semantic Complexity in Natural Language Processing, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veronis</author>
</authors>
<title>A Study of Polysemy Judgements and Inter-annotator Agreement.</title>
<date>1998</date>
<booktitle>In Advanced Papers of the SENSEVAL Workshop,</booktitle>
<location>Sussex, UK.</location>
<contexts>
<context position="1167" citStr="Veronis, 1998" startWordPosition="168" endWordPosition="169">reement observed between WordNet Semcor and DSO corpora. 1 Introduction In recent years, the granularity of word senses for computational lexicons has been discussed frequently in Lexical Semantics (for example, (Kilgarriff, 1998a; Palmer, 1998)). This issue emerged as a prominent problem after previous studies and exercises in Word Sense Disambiguation (WSD) reported that, when fine-grained sense definitions such as those in WordNet (Miller, 1990) were used, entries became very similar and indistinguishable to human annotators, thereby causing disagreement on correct tags (Kilgarriff, 1998b; Veronis, 1998; Ng et al., 1999). In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), as well as in Information Retrieval (IR), since the difference in the correct sense assignments affects recall, precision and other evaluation measures. In response to this, several approaches have been proposed which group fine-grained word senses in various ways to derive coarse-grained sense groups. Some approaches utilize an abstraction hierarchy defined in a dictionary (Kilgar</context>
</contexts>
<marker>Veronis, 1998</marker>
<rawString>Veronis, J. (1998). A Study of Polysemy Judgements and Inter-annotator Agreement. In Advanced Papers of the SENSEVAL Workshop, Sussex, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
<author>W Peters</author>
<author>J Gonzalo</author>
</authors>
<title>Towards a Universal Index of Meaning.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL SIGLEX Workshop on Standardizing Lexical Resources,</booktitle>
<location>College Park, MD.</location>
<marker>Vossen, Peters, Gonzalo, 1999</marker>
<rawString>Vossen, P., Peters, W. and Gonzalo, J. (1999). Towards a Universal Index of Meaning. In Proceedings of the ACL SIGLEX Workshop on Standardizing Lexical Resources, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>R Bruce</author>
<author>T O&apos;Hara</author>
</authors>
<title>Development and Use of a Gold-Standard Data Set for Subjectivity Classifications.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL-99,</booktitle>
<location>College Park, MD.</location>
<marker>Wiebe, Bruce, O&apos;Hara, 1999</marker>
<rawString>Wiebe, J., Bruce, R. and O&apos;Hara, T. (1999). Development and Use of a Gold-Standard Data Set for Subjectivity Classifications. In Proceedings of the ACL-99, College Park, MD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>