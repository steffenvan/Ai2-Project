<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006085">
<title confidence="0.6208105">
Book Reviews
Statistical Methods for Speech Recognition
</title>
<author confidence="0.848131">
Frederick Jelinek
</author>
<affiliation confidence="0.453378">
(The Johns Hopkins University)
</affiliation>
<figureCaption confidence="0.6882055">
Cambridge, MA: The MIT Press
(Language, speech, and communication
series), 1997, xvii+283 pp; hardbound,
ISBN 0-262-10066-5, $35.00
</figureCaption>
<figure confidence="0.8991375">
Reviewed by
Eric Neufeld
</figure>
<subsubsectionHeader confidence="0.590462">
University of Saskatchewan
</subsubsectionHeader>
<bodyText confidence="0.999671935483871">
Current practitioners in the area of speech recognition who are familiar with the ap-
proach of Jelinek and others will find this a compact, concise, and useful overview
of the state of the art in statistical approaches to speech recognition. Readers already
familiar with Rabiner and Juang (1993) will find it an excellent companion volume.
Computational linguists will also find this book to be engaging reading. For one
thing, Jelinek&apos;s lucid and well-organized survey might well have been written with
the mathematical backgrounds of computational linguists and computer scientists in
mind. Apart from a smattering of probability, the reader needs only some basic discrete
mathematics.
This is partly because Jelinek does not discuss signal processing, the crucial first
step in speech recognition, where continuous vectors representing waveforms are
matched with discrete phonetic symbols. Some may think this omission surprising,
but from Jelinek&apos;s quick sketch of the vector quantization algorithm, it seems reason-
able to assume that although signal processing is critical (&amp;quot;bad [signal] processing
means loss of information: there is less of it to extract&amp;quot;), developments in that disci-
pline and advances in statistical aspects of speech recognition can proceed relatively
independently of each other. (Readers wishing to explore signal processing in the
context of speech recognition might consult Rabiner and Juang [1993].)
Thus, Jelinek initially concentrates on the problem of building a speech recognizer
that can be trained to construct the most probable hypothesis (a string of English text
originating in a speaker&apos;s mind) that explains a perceived string of phonetic symbols
reaching the hearer&apos;s ear. These strings of phonetic symbols are outputs of an acoustic
processor: finite sequences of discrete symbols from a finite phonetic alphabet such as
might be used in a dictionary.
In his introduction, Jelinek writes &amp;quot;I am fascinated by the idea that while system
structure and parametrization should come from intuitive understanding of the pro-
cess, the parameter values are best extracted from the data.&amp;quot; The speech recognizers
that he describes use the assumption that natural language at the phonetic and se-
mantic levels is an output-generating Markov process and use the outputs (sounds)
to infer the text that produced it. This, of course, is the celebrated method of hidden
Markov models (HMMs), and while it seems to oversimplify the structure of language,
</bodyText>
<page confidence="0.974821">
297
</page>
<note confidence="0.626934">
Computational Linguistics Volume 25, Number 2
</note>
<bodyText confidence="0.999956971428571">
this method gives the recognizer the ability to exploit context to resolve ambiguities.
(Some speech recognizers use neural net models; Jelinek does not discuss these.)
Although by now most people in CL know about HMMs and understand in broad
terms how they are used, many would be surprised to see how remarkable a first
cut at a solution they provide. Even when the English text in the speaker&apos;s mind
is pronounced idiosyncratically and convolved with ambient noise before it reaches
the hearer, contextual relationships can still provide the (computer) hearer with suf-
ficient clues to decode the message. (As a side note, there may also be cognitive or
psychological validity to this approach: Saffran, Aslin, and Newport [1996] suggest
that segmentation of words from fluent speech can be accomplished by 8-month-old
infants solely on the basis of the statistical relationships between neighboring speech
sounds.)
The first half of the book presents the basic algorithms needed to build and
parametrize an HMM-based speech recognizer. Jelinek devotes a chapter to the Viterbi
algorithm and another to hypothesis search on trees. He also gives the Baum-Welch
(forward-backward) algorithm that is guaranteed to improve any initial estimate of pa-
rameters (though it may converge to a local maximum). He also describes the method
of optimal linear smoothing that ensures that sequences not seen during training are
not assigned zero probabilities when encountered after training. A larger training set
does not solve this problem because no training set can contain every possible se-
quence that may eventually occur in practice.
The last half of the book refines the ideas of the first half. It requires more math-
ematical knowledge from the reader and Jelinek provides a chapter of required back-
ground. He discusses the EM algorithm from which the Baum-Welch algorithm can be
derived and which allows the generalization of HMMs that output vectors of normally
distributed real numbers. This lets a speech recognizer exploit waveform information
that vector quantization loses. He also discusses the problem of coarticulation, the in-
fluence of phones on one another. For example, the waveform for i in king more closely
resembles the o in moves than the i in bishop. This is due to the influence of the nasal
ng in king and the m in moves. The conflict therefore can be resolved by considering
the phonetic context, i.e., the most probable sequence of phones.
The lifetime of experience that Jelinek brings this presentation provides a great
foundation upon which AT and CL practitioners can build. Recently, Zweig and Rus-
sell (1998) report successes using dynamic Bayesian nets, a natural generalization of
HMMs, to handle the coarticulation problem in isolated-word speech recognition.
</bodyText>
<note confidence="0.668916">
References 13 December 1996, pages 1,926-1,928.
</note>
<reference confidence="0.794477272727273">
Rabiner, Lawrence R. and Biing-Hwang Zweig, Geoffrey and Stuart Russell. Speech
Juang. 1993. Fundamentals of Speech recognition with dynamic Bayesian
Recognition, Prentice-Hall, Englewood networks. Proceedings of the Fifteenth
Cliffs, NJ. National Conference on Artificial Intelligence
Saffran, Jenny R., Richard N. Aslin, and (AAAI-98), Madison, Wisconsin: AAAI
Elissa L. Newport. 1996. Statistical learning Press, pages 173-180.
by 8-month-old infants. Science, 274(5294),
Eric Neufeld is interested in probabilistic approaches to artificial intelligence, including natu-
ral language processing. With Greg Adams, he has written several papers on HMM-based
approaches to part-of-speech tagging of text. Neufeld&apos;s address is Department of Computer
Science, University of Saskatchewan, Saskatoon, SK, Canada S7H 3A8; e-mail: eric@cs.usask.ca
</reference>
<page confidence="0.996705">
298
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.296117">
<title confidence="0.997414">Book Reviews Statistical Methods for Speech Recognition</title>
<author confidence="0.99999">Frederick Jelinek</author>
<affiliation confidence="0.998052">(The Johns Hopkins University)</affiliation>
<address confidence="0.975216">Cambridge, MA: The MIT Press</address>
<keyword confidence="0.474883">(Language, speech, and communication series), 1997, xvii+283 pp; hardbound,</keyword>
<note confidence="0.97368">ISBN 0-262-10066-5, $35.00 Reviewed by</note>
<author confidence="0.999831">Eric Neufeld</author>
<affiliation confidence="0.995827">University of Saskatchewan</affiliation>
<abstract confidence="0.997619458333333">Current practitioners in the area of speech recognition who are familiar with the approach of Jelinek and others will find this a compact, concise, and useful overview of the state of the art in statistical approaches to speech recognition. Readers already familiar with Rabiner and Juang (1993) will find it an excellent companion volume. Computational linguists will also find this book to be engaging reading. For one thing, Jelinek&apos;s lucid and well-organized survey might well have been written with the mathematical backgrounds of computational linguists and computer scientists in mind. Apart from a smattering of probability, the reader needs only some basic discrete mathematics. This is partly because Jelinek does not discuss signal processing, the crucial first step in speech recognition, where continuous vectors representing waveforms are matched with discrete phonetic symbols. Some may think this omission surprising, but from Jelinek&apos;s quick sketch of the vector quantization algorithm, it seems reasonable to assume that although signal processing is critical (&amp;quot;bad [signal] processing means loss of information: there is less of it to extract&amp;quot;), developments in that discipline and advances in statistical aspects of speech recognition can proceed relatively independently of each other. (Readers wishing to explore signal processing in the context of speech recognition might consult Rabiner and Juang [1993].) Thus, Jelinek initially concentrates on the problem of building a speech recognizer that can be trained to construct the most probable hypothesis (a string of English text originating in a speaker&apos;s mind) that explains a perceived string of phonetic symbols reaching the hearer&apos;s ear. These strings of phonetic symbols are outputs of an acoustic processor: finite sequences of discrete symbols from a finite phonetic alphabet such as might be used in a dictionary.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>