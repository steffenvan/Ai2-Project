<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.088523">
<title confidence="0.979324">
Incremental Adaptation of Speech-to-Speech Translation
</title>
<author confidence="0.9956385">
Nguyen Bach, Roger Hsiao, Matthias Eck, Paisarn Charoenpornsawat, Stephan Vogel,
Tanja Schultz, Ian Lane, Alex Waibel and Alan W. Black
</author>
<affiliation confidence="0.929786666666667">
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.979992">
{nbach, wrhsiao, matteck, paisarn, stephan.vogel, tanja, ianlane, ahw, awb}@cs.cmu.edu
</email>
<sectionHeader confidence="0.995507" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996033133333333">
In building practical two-way speech-to-speech
translation systems the end user will always wish
to use the system in an environment different from
the original training data. As with all speech sys-
tems, it is important to allow the system to adapt
to the actual usage situations. This paper investi-
gates how a speech-to-speech translation system can
adapt day-to-day from collected data on day one to
improve performance on day two. The platform is
the CMU Iraqi-English portable two-way speech-
to-speech system as developed under the DARPA
TransTac program. We show how machine transla-
tion, speech recognition and overall system perfor-
mance can be improved on day 2 after adapting from
day 1 in both a supervised and unsupervised way.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9986426">
As speech-to-speech translation systems move from the
laboratory into field deployment, we quickly see that mis-
match in training data with field use can degrade the per-
formance of the system. Retraining based on field us-
age is a common technique used in all speech systems
to improve performance. In the case of speech-to-speech
translation we would particularly like to be able to adapt
the system based on its usage automatically without hav-
ing to ship data back to the laboratory for retraining. This
paper investigates the scenario of a two-day event. We
wish to improve the system for the second day based on
the data collected on the first day.
Our system is designed for eyes-free use and hence
provides no graphical user interface. This allows the user
to concentrate on his surrounding environment during an
operation. The system only provides audio control and
feedback. Additionally the system operates on a push-to-
talk method. Previously the system (Hsiao et al., 2006;
Bach et al., 2007) needed 2 buttons to operate, one for the
English speaker and the other one for the Iraqi speaker.
</bodyText>
<figureCaption confidence="0.999501">
Figure 1: The users interact with the system
</figureCaption>
<bodyText confidence="0.999967470588236">
To make the system easier and faster to use, we propose
to use a single button which can be controlled by the En-
glish speaker. We mounted a microphone and a Wii re-
mote controller together as shown in 1.
Since the Wii controller has an accelerometer which
can be used to detect the orientation of the controller, this
feature can be applied to identify who is speaking. When
the English speaker points towards himself, the system
will switch to English-Iraqi translation. However, when
the Wii is pointed towards somebody else, the system will
switch to Iraqi-English translation. In addition, we attach
a light on the Wii controller providing visual feedback.
This can inform an Iraqi speaker when to start speaking.
The overall system is composed of five major compo-
nents: two automatic speech recognition (ASR) systems,
a bidirectional statistical machine translation (SMT) sys-
tem and two text-to-speech (TTS) systems.
</bodyText>
<sectionHeader confidence="0.955043" genericHeader="method">
2 Data Scenario
</sectionHeader>
<bodyText confidence="0.99994025">
The standard data that is available for the TransTac
project was collected by recording human interpreter
mediated dialogs between war fighters and Iraqi native
speakers in various scenarios. The dialog partners were
aware that the data was being collected for training ma-
chine based translation devices, but would often talk di-
rectly to the human interpreter rather than pretending it
was an automatic device. This means that the dialog
</bodyText>
<page confidence="0.996506">
149
</page>
<note confidence="0.358787">
Proceedings of NAACL HLT 2009: Short Papers, pages 149–152,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999897025">
partners soon ignored the recording equipment and used
a mostly natural language, using informal pronunciation
and longer sentences with more disfluencies than we find
in machine mediated translation dialogs.
Most users mismatch their language when they com-
municate using an automatic speech-to-speech transla-
tion system. They often switch to a clearer pronuncia-
tion and use shorter and simpler sentences with less dis-
fluency. This change could have a significant impact on
speech recognition and machine translation performance
if a system was originally trained on data from the inter-
preter mediated dialogs.
For this reason, additional data was collected during
the TransTac meeting in June of 2008. This data was
collected with dialog partners using the speech-to-speech
translation systems from 4 developer participants in the
TransTac program. The dialog partners were given a de-
scription of the specific scenario in form of a rough script
and had to speak their sentences into the translation sys-
tems. The dialog partners were not asked to actually react
to the potentially incorrect translations but just followed
the script, ignoring the output of the translation system.
This has the effect that the dialog partners are no longer
talking to a human interpreter, but to a machine, press-
ing push-to-talk buttons etc. and will change their speech
patterns accordingly.
The data was collected over two days, with around 2
hours of actual speech per day. This data was transcribed
and translated, resulting in 864 and 824 utterance pairs
on day 1 and 2, respectively.
of having transcribers provide accurate transcription ref-
erences for day 1 data, and see how it may improve the
performance on day 2. We compare unigram, bigram and
trigram LMs for adaptation. Since the amount of day 1
data is much smaller than the whole training set and we
do not assume transcription of day 1 is always available,
the interpolation weight is chosen of be 0.9 for the orig-
inal trigram LM and 0.1 for the new LM built from the
day 1 data. The WER of baseline ASR system on day 1
is 32.0%.
</bodyText>
<table confidence="0.5953545">
Base 1-g hypo 2-g hypo 3-g hypo 1-g ref 2-g ref 3-g ref
31.3 30.9 31.2 31.1 30.6 30.5 30.4
</table>
<tableCaption confidence="0.996415">
Table 1: Iraqi ASR’s WER on day 2 using different adaptation
schemes for day 1 data
</tableCaption>
<bodyText confidence="0.99832225">
The results in Table 1 show that the ASR benefits from
LM adaptation. Adapting day 1 data can slightly improve
the performance of day 2. The improvement is larger
when day 1 transcript is available which is expected. The
result also shows that the unigram LM is the most robust
model for adaptation as it works reasonably well when
transcripts are not available, whereas bigram and trigram
LM are more sensitive to the ASR errors made on day 1.
</bodyText>
<table confidence="0.97546825">
Day 1 Day 2
No ASR adaptation 29.39 27.41
Unsupervised ASR adaptation 31.55 27.66
Supervised ASR adaptation 32.19 27.65
</table>
<tableCaption confidence="0.99858">
Table 2: Impact of ASR adaptation to SMT
</tableCaption>
<sectionHeader confidence="0.989844" genericHeader="method">
3 ASR LM Adaptation
</sectionHeader>
<bodyText confidence="0.999966064516129">
This section describes the Iraqi ASR system and how we
perform LM adaptation on the day 1 data to improve ASR
performance on day 2. The CMU Iraqi ASR system is
trained with around 350 hours of audio data collected un-
der the TransTac program. The acoustic model is speaker
independent but incremental unsupervised MLLR adap-
tation is performed to improve recognition. The acous-
tic model has 6000 codebooks and each codebook has
at most 64 Gaussian mixtures determined by merge-and-
split training. Semi-tied covariance and boosted MMI
discriminative training is performed to improve the model
(Povey et al., 2009). The features for the acoustic model
is the standard 39-dimension MFCC and we concatenate
adjacent 15 frames and perform LDA to reduce the di-
mension to 42 for the final feature vectors. The language
model of the ASR system is a trigram LM trained on the
audio transcripts with around three million words with
Kneser-Ney smoothing (Stolcke, 2002).
To perform LM adaptation for the ASR system, we use
the ASR hypotheses from day 1 to build a LM. This LM
is then interpolated with the original trigram LM to pro-
duce an adapted LM for day 2. We also evaluate the effect
Table 2 shows the impact of ASR adaptation on the
performance of the translation system in BLEU (Papineni
et al., 2002). In these experiments we only performed
adaptation on ASR and still using the baseline SMT com-
ponent. There is no obvious difference between unsuper-
vised and supervised ASR adaptation on performance of
SMT on day 2. However, we can see that the difference
in WER on day 2 of unsupervised and supervised ASR
adaptation is relatively small.
</bodyText>
<sectionHeader confidence="0.996779" genericHeader="method">
4 SMT Adaptation
</sectionHeader>
<bodyText confidence="0.999789363636364">
The Iraqi-English SMT system is trained with around
650K sentence pairs collected under the TransTac pro-
gram. We used PESA phrase extraction (Vogel, 2005)
and a suffix array language model (Zhang and Vogel,
2005). To adapt SMT components one approach is to op-
timize LM interpolation weights by minimizing perplex-
ity of the 1-best translation output (Bulyko et al., 2007).
Related work including (Eck et al., 2004) attempts to use
information retrieval to select training sentences similar
to those in the test set. To adapt the SMT components
we use a domain-specific LM on top of the background
</bodyText>
<page confidence="0.990194">
150
</page>
<bodyText confidence="0.998947">
language models. This approach is similar to the work
in (Chen et al., 2008). sThe adaptation framework is 1)
create a domain-specific LM via an n-best list of day 1
machine translation hypothesis, or day 1 translation ref-
erences; 2) re-tune the translation system on day 1 via
minimum error rate training (MERT) (Venugopal and Vo-
gel, 2005).
</bodyText>
<table confidence="0.995839">
Use Day 1 Day 2
Baseline 29.39 27.41
500 Best 1gramLM 29.18 27.23
MT Hypos 2gramLM 29.53 27.50
3gramLM 29.36 27.23
</table>
<tableCaption confidence="0.99998">
Table 3: Performance in BLEU of unsupervised adaptation.
</tableCaption>
<bodyText confidence="0.999527363636364">
The first question we would like to address is whether
our adaptation obtains improvements via an unsupervised
manner. We take day 1 baseline ASR hypothesis and use
the baseline SMT to get the MT hypothesis and a 500-
best list. We train a domain LM using the 500-best list
and use the MT hypotheses as the reference in MERT. We
treat day 1 as a development set and day 2 as an unseen
test set. In Table 3 we compare the performance of four
systems: the baseline which does not have any adaptation
steps; and 3 adapted systems using unigram, bigram and
trigram LMs build from 500-best MT hypotheses.
</bodyText>
<table confidence="0.999719428571429">
Use Day 1 Day 2
Baseline (no tune) 29.39 27.41
Baseline (tune) 29.49 27.30
500 Best 1gramLM 30.27 28.29
MT Hypos 2gramLM 30.39 28.30
3gramLM 28.36 24.64
MT Ref 1gramLM MT Ref 30.53 28.35
</table>
<tableCaption confidence="0.999966">
Table 4: Performance in BLEU of supervised adaptation.
</tableCaption>
<bodyText confidence="0.999889642857143">
Experimental results from unsupervised adaptation did
not show consistent improvements but suggest we may
obtain gains via supervised adaptation. In supervised
adaptation, we assume we have day 1 translation refer-
ences. The references are used in MERT. In Table 4 we
show performances of two additional systems which are
the baseline system without adaptation but tuned toward
day 1, and the adapted system which used day 1 trans-
lation references to train a unigram LM (1gramLM MT
Ref). The unigram and bigram LMs from 500-best and
unigram LM from MT day 1 references perform rela-
tively similar on day 2. Using a trigram 500-best LM
returned a large degradation and this LM is sensitive to
the translation errors on day1
</bodyText>
<sectionHeader confidence="0.990709" genericHeader="method">
5 Joint Adaptation
</sectionHeader>
<bodyText confidence="0.989166">
In Sections 3 and 4 we saw that individual adaptation
helps ASR to reduce WER and SMT to increase BLEU
</bodyText>
<table confidence="0.990841">
ASR SMT Day 1 Day 2
No adaptation No adaptation 29.39 27.41
Unsupervised ASR 1gramLM 500-Best 32.07 28.65
adaptation with MT Hypo
1gramLM ASR hypo 1gramLM MT Ref 31.76 28.83
Supervised ASR 1gramLM 500-Best 32.48 28.59
adaptation with MT Hypo
1gramLM transcription 1gramLM MT Ref 32.68 28.60
</table>
<tableCaption confidence="0.999946">
Table 5: Performance in BLEU of joint adaptation.
</tableCaption>
<bodyText confidence="0.999934125">
score. The next step in validating the adaptation frame-
work was to check if the joint adaptation of ASR and
SMT on day 1 data will lead to improvements on day
2. Table 5 shows the combination of ASR and SMT
adaptation methods. Improvements are obtained by us-
ing both ASR and SMT adaptation. Joint adaptation con-
sistently gained more than one BLEU point improvement
on day 2. Our best system is unsupervised ASR adapta-
tion via 1gramLM of ASR day 1 transcription coupled
with supervised SMT adaptation via 1gramLM of day
1 translation references. An interesting result is that to
have a better result on day 2 our approach only requires
translation references on day 1. We selected 1gramLM
of 500-best MT hypotheses to conduct the experiments
since there is no significant difference between 1gramLM
and 2gramLM on day 2 as showed in Table 3.
</bodyText>
<sectionHeader confidence="0.974863" genericHeader="method">
6 Selective Adaptation
</sectionHeader>
<bodyText confidence="0.9999634">
The previous results indicate that we require human
translation references on day 1 data to get improved per-
formance on day 2. However, our goal is to make a better
system on day 2 but try to minimize human efforts on day
1. Therefore, we raise two questions: 1) Can we still ob-
tain improvements by not using all of day 1 data? and 2)
Can we obtain more improvements?
To answer these questions we performed oracle exper-
iments when we take the translation hypotheses on day
1 of the baseline SMT and compare them with transla-
tion references, then select sentences which have BLEU
scores higher than a threshold. The subset of day 1 sen-
tences is used to perform supervised adaptation in a sim-
ilar way showed in section 5. These experiments also
simulate the situation when we have a perfect confidence
score for machine translation hypothesis selection. Table
6 shows results when we use various portions of day 1 to
perform adaptation. By using day 1 sentences which have
smoothed sentence BLEU scores higher than 10 or 20 we
have very close performance with adaptation by using all
day 1 data. The results also show that by using 416 sen-
tences which have sentence BLEU score higher than 40
on day 1, our adapted translation components outperform
the baseline. Performance starts degrading after 50. Ex-
perimental results lead to the answer for question 1) that
</bodyText>
<page confidence="0.995259">
151
</page>
<bodyText confidence="0.9996234">
by using less day 1 data our adapted translation compo-
nents still obtain improvements compare with the base-
line, and 2) we did not see that using less data will lead
us to a better performance compare with using all day 1
data.
</bodyText>
<table confidence="0.999826444444444">
No. sents Day 1 Day 2
Baseline 29.39 27.41
&gt; 0 864 30.27 28.29
&gt; 10 797 31.15 28.27
&gt; 20 747 30.81 28.24
&gt; 30 585 30.04 27.71
&gt; 40 416 29.72 27.65
&gt; 50 296 30.06 27.04
Correct 98 29.18 27.19
</table>
<tableCaption confidence="0.998974">
Table 6: Performance in BLEU of selective adaptation
</tableCaption>
<figureCaption confidence="0.987523">
Figure 2: Summarization of adaptation performances
</figureCaption>
<sectionHeader confidence="0.998824" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999979166666667">
This work clearly shows that improvement is possible us-
ing collected data for adaptation. The overall picture is
shown in Figure 2. However this result is only based on
one such data set, it would be useful to do such adaptation
over multiple days. The best results however still require
producing translation references, notably ASR transcrip-
tions do not seem to help, but may still be required in the
process of generating translation references. We wish to
further investigate automatic adaptation based on implicit
confidence scores, or even active participation of the user
e.g. by marking bad utterance which could be excluded
from the adaptation.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998819">
This work is in part supported by the US DARPA under the TransTac
(Spoken Language Communication and Translation System for Tactical
Use) program. Any opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the authors and do not
necessarily reflect the views of DARPA. We would also like to thank
Cepstral LLC and Mobile Technologies LLC, for support of some of
the lower level software components.
</bodyText>
<sectionHeader confidence="0.989866" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999869625">
Nguyen Bach, Matthias Eck, Paisarn Charoenpornsawat, Thilo
Khler, Sebastian Stker, ThuyLinh Nguyen, Roger Hsiao,
Alex Waibel, Stephan Vogel, Tanja Schultz, and Alan Black.
2007. The CMU TransTac 2007 Eyes-free and Hands-free
Two-way Speech-to-Speech Translation System. In Proc.
of the International Workshop on Spoken Language Trans-
lation, Trento, Italy.
Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, Long
Nguyen, and John Makhoul. 2007. Language Model Adap-
tation in Machine Translation from Speech. In Proc. of Int.
Conf. on Acoustics, Speech and Signal Processing, Honolulu,
Hawaii, USA.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li. 2008.
Exploiting n-best hypotheses for smt self-enhancement. In
Proceedings ofACL-08: HLT, Short Papers, pages 157–160,
Columbus, Ohio, USA, June.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004. Lan-
guage model adaptation for statistical machine translation
based on information retrieval. In Proc. LREC’04, Lisbon,
Portugal.
Roger Hsiao, Ashish Venugopal, Thilo Kohler, Ying Zhang,
Paisarn Charoenpornsawat, Andreas Zollmann, Stephan Vo-
gel, Alan W Black, Tanja Schultz, and Alex Waibel. 2006.
Optimizing Components for Handheld Two-way Speech
Translation for an English-Iraqi Arabic System. In Proc. of
Interspeech, Pittsburgh, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: A method for automatic evaluation of
machine translation. In Proceedings of ACL’02, pages 311–
318, Philadelphia, PA, July.
Daniel Povey, Dimitri Kanevsky, Brian Kingsbury, Bhu vana
Ramabhadran, George Saon, and Karthik Visweswariah.
2009. Boosted MMI for model and feature-space discrim-
inative training. In Proc. ofInt. Conf. on Acoustics, Speech
and Signal Processing, Las Vegas, USA.
Andreas Stolcke. 2002. SRILM – An extensible language mod-
eling toolkit. In Proc. Intl. Conf. on Spoken Language Pro-
cessing, volume 2, pages 901–904, Denver.
Ashish Venugopal and Stephan Vogel. 2005. Considerations
in maximum mutual information and minimum classification
error training for statistical machine translation. In Proceed-
ings ofEAMT-05, Budapest, Hungary.
Stephan Vogel. 2005. Pesa: Phrase pair extraction as sentence
splitting. In Proc. ofMT SUMMIT X, Phuket, Thailand.
Ying Zhang and Stephan Vogel. 2005. An efficient phrase-to-
phrase alignment model for arbitrarily long phrase and large
corpora. In Proceedings of EAMT’05, Budapest, Hungary,
May. The European Association for Machine Translation.
</reference>
<page confidence="0.998128">
152
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.244280">
<title confidence="0.999174">Incremental Adaptation of Speech-to-Speech Translation</title>
<author confidence="0.956296">Nguyen Bach</author>
<author confidence="0.956296">Roger Hsiao</author>
<author confidence="0.956296">Matthias Eck</author>
<author confidence="0.956296">Paisarn Charoenpornsawat</author>
<author confidence="0.956296">Stephan</author>
<affiliation confidence="0.784874">Tanja Schultz, Ian Lane, Alex Waibel and Alan W. InterACT, Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.994167">Pittsburgh, PA 15213,</address>
<email confidence="0.940144">wrhsiao,matteck,paisarn,stephan.vogel,tanja,ianlane,ahw,</email>
<abstract confidence="0.975403875">In building practical two-way speech-to-speech translation systems the end user will always wish to use the system in an environment different from the original training data. As with all speech systems, it is important to allow the system to adapt to the actual usage situations. This paper investigates how a speech-to-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two. The platform is the CMU Iraqi-English portable two-way speechto-speech system as developed under the DARPA TransTac program. We show how machine translation, speech recognition and overall system performance can be improved on day 2 after adapting from day 1 in both a supervised and unsupervised way.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>Matthias Eck</author>
<author>Paisarn Charoenpornsawat</author>
<author>Thilo Khler</author>
<author>Sebastian Stker</author>
<author>ThuyLinh Nguyen</author>
<author>Roger Hsiao</author>
<author>Alex Waibel</author>
<author>Stephan Vogel</author>
<author>Tanja Schultz</author>
<author>Alan Black</author>
</authors>
<title>Eyes-free and Hands-free Two-way Speech-to-Speech Translation System.</title>
<date>2007</date>
<booktitle>The CMU TransTac</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="2133" citStr="Bach et al., 2007" startWordPosition="329" endWordPosition="332">adapt the system based on its usage automatically without having to ship data back to the laboratory for retraining. This paper investigates the scenario of a two-day event. We wish to improve the system for the second day based on the data collected on the first day. Our system is designed for eyes-free use and hence provides no graphical user interface. This allows the user to concentrate on his surrounding environment during an operation. The system only provides audio control and feedback. Additionally the system operates on a push-totalk method. Previously the system (Hsiao et al., 2006; Bach et al., 2007) needed 2 buttons to operate, one for the English speaker and the other one for the Iraqi speaker. Figure 1: The users interact with the system To make the system easier and faster to use, we propose to use a single button which can be controlled by the English speaker. We mounted a microphone and a Wii remote controller together as shown in 1. Since the Wii controller has an accelerometer which can be used to detect the orientation of the controller, this feature can be applied to identify who is speaking. When the English speaker points towards himself, the system will switch to English-Iraq</context>
</contexts>
<marker>Bach, Eck, Charoenpornsawat, Khler, Stker, Nguyen, Hsiao, Waibel, Vogel, Schultz, Black, 2007</marker>
<rawString>Nguyen Bach, Matthias Eck, Paisarn Charoenpornsawat, Thilo Khler, Sebastian Stker, ThuyLinh Nguyen, Roger Hsiao, Alex Waibel, Stephan Vogel, Tanja Schultz, and Alan Black. 2007. The CMU TransTac 2007 Eyes-free and Hands-free Two-way Speech-to-Speech Translation System. In Proc. of the International Workshop on Spoken Language Translation, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Bulyko</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>Long Nguyen</author>
<author>John Makhoul</author>
</authors>
<title>Language Model Adaptation in Machine Translation from Speech.</title>
<date>2007</date>
<booktitle>In Proc. of Int. Conf. on Acoustics, Speech and Signal Processing,</booktitle>
<location>Honolulu, Hawaii, USA.</location>
<contexts>
<context position="8697" citStr="Bulyko et al., 2007" startWordPosition="1429" endWordPosition="1432">e is no obvious difference between unsupervised and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small. 4 SMT Adaptation The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one approach is to optimize LM interpolation weights by minimizing perplexity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background 150 language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 translation references; 2) re-tune the translation system on day 1 via minimum error rate training (MERT) (Venugopal and Vogel, 2005). Use Day 1 Day 2 Baseline 29.</context>
</contexts>
<marker>Bulyko, Matsoukas, Schwartz, Nguyen, Makhoul, 2007</marker>
<rawString>Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, Long Nguyen, and John Makhoul. 2007. Language Model Adaptation in Machine Translation from Speech. In Proc. of Int. Conf. on Acoustics, Speech and Signal Processing, Honolulu, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>Exploiting n-best hypotheses for smt self-enhancement.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT, Short Papers,</booktitle>
<pages>157--160</pages>
<location>Columbus, Ohio, USA,</location>
<contexts>
<context position="9003" citStr="Chen et al., 2008" startWordPosition="1482" endWordPosition="1485">entence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one approach is to optimize LM interpolation weights by minimizing perplexity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background 150 language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 translation references; 2) re-tune the translation system on day 1 via minimum error rate training (MERT) (Venugopal and Vogel, 2005). Use Day 1 Day 2 Baseline 29.39 27.41 500 Best 1gramLM 29.18 27.23 MT Hypos 2gramLM 29.53 27.50 3gramLM 29.36 27.23 Table 3: Performance in BLEU of unsupervised adaptation. The first question we would like to address is whether our adaptation obtains improvements via an unsupervised manner. We take day 1 baseline ASR hypothesis and u</context>
</contexts>
<marker>Chen, Zhang, Aw, Li, 2008</marker>
<rawString>Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li. 2008. Exploiting n-best hypotheses for smt self-enhancement. In Proceedings ofACL-08: HLT, Short Papers, pages 157–160, Columbus, Ohio, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Language model adaptation for statistical machine translation based on information retrieval.</title>
<date>2004</date>
<booktitle>In Proc. LREC’04,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="8740" citStr="Eck et al., 2004" startWordPosition="1436" endWordPosition="1439">d and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small. 4 SMT Adaptation The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one approach is to optimize LM interpolation weights by minimizing perplexity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background 150 language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 translation references; 2) re-tune the translation system on day 1 via minimum error rate training (MERT) (Venugopal and Vogel, 2005). Use Day 1 Day 2 Baseline 29.39 27.41 500 Best 1gramLM 29.18 27.23 MT Hy</context>
</contexts>
<marker>Eck, Vogel, Waibel, 2004</marker>
<rawString>Matthias Eck, Stephan Vogel, and Alex Waibel. 2004. Language model adaptation for statistical machine translation based on information retrieval. In Proc. LREC’04, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Hsiao</author>
<author>Ashish Venugopal</author>
<author>Thilo Kohler</author>
<author>Ying Zhang</author>
<author>Paisarn Charoenpornsawat</author>
<author>Andreas Zollmann</author>
<author>Stephan Vogel</author>
<author>Alan W Black</author>
<author>Tanja Schultz</author>
<author>Alex Waibel</author>
</authors>
<title>Optimizing Components for Handheld Two-way Speech Translation for an English-Iraqi Arabic System. In</title>
<date>2006</date>
<booktitle>Proc. of Interspeech,</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="2113" citStr="Hsiao et al., 2006" startWordPosition="325" endWordPosition="328"> like to be able to adapt the system based on its usage automatically without having to ship data back to the laboratory for retraining. This paper investigates the scenario of a two-day event. We wish to improve the system for the second day based on the data collected on the first day. Our system is designed for eyes-free use and hence provides no graphical user interface. This allows the user to concentrate on his surrounding environment during an operation. The system only provides audio control and feedback. Additionally the system operates on a push-totalk method. Previously the system (Hsiao et al., 2006; Bach et al., 2007) needed 2 buttons to operate, one for the English speaker and the other one for the Iraqi speaker. Figure 1: The users interact with the system To make the system easier and faster to use, we propose to use a single button which can be controlled by the English speaker. We mounted a microphone and a Wii remote controller together as shown in 1. Since the Wii controller has an accelerometer which can be used to detect the orientation of the controller, this feature can be applied to identify who is speaking. When the English speaker points towards himself, the system will sw</context>
</contexts>
<marker>Hsiao, Venugopal, Kohler, Zhang, Charoenpornsawat, Zollmann, Vogel, Black, Schultz, Waibel, 2006</marker>
<rawString>Roger Hsiao, Ashish Venugopal, Thilo Kohler, Ying Zhang, Paisarn Charoenpornsawat, Andreas Zollmann, Stephan Vogel, Alan W Black, Tanja Schultz, and Alex Waibel. 2006. Optimizing Components for Handheld Two-way Speech Translation for an English-Iraqi Arabic System. In Proc. of Interspeech, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="7970" citStr="Papineni et al., 2002" startWordPosition="1309" endWordPosition="1312">on MFCC and we concatenate adjacent 15 frames and perform LDA to reduce the dimension to 42 for the final feature vectors. The language model of the ASR system is a trigram LM trained on the audio transcripts with around three million words with Kneser-Ney smoothing (Stolcke, 2002). To perform LM adaptation for the ASR system, we use the ASR hypotheses from day 1 to build a LM. This LM is then interpolated with the original trigram LM to produce an adapted LM for day 2. We also evaluate the effect Table 2 shows the impact of ASR adaptation on the performance of the translation system in BLEU (Papineni et al., 2002). In these experiments we only performed adaptation on ASR and still using the baseline SMT component. There is no obvious difference between unsupervised and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small. 4 SMT Adaptation The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one a</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of ACL’02, pages 311– 318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Povey</author>
<author>Dimitri Kanevsky</author>
<author>Brian Kingsbury</author>
<author>Bhu vana Ramabhadran</author>
<author>George Saon</author>
<author>Karthik Visweswariah</author>
</authors>
<title>Boosted MMI for model and feature-space discriminative training.</title>
<date>2009</date>
<booktitle>In Proc. ofInt. Conf. on Acoustics, Speech and Signal Processing,</booktitle>
<location>Las Vegas, USA.</location>
<contexts>
<context position="7284" citStr="Povey et al., 2009" startWordPosition="1186" endWordPosition="1189">daptation This section describes the Iraqi ASR system and how we perform LM adaptation on the day 1 data to improve ASR performance on day 2. The CMU Iraqi ASR system is trained with around 350 hours of audio data collected under the TransTac program. The acoustic model is speaker independent but incremental unsupervised MLLR adaptation is performed to improve recognition. The acoustic model has 6000 codebooks and each codebook has at most 64 Gaussian mixtures determined by merge-andsplit training. Semi-tied covariance and boosted MMI discriminative training is performed to improve the model (Povey et al., 2009). The features for the acoustic model is the standard 39-dimension MFCC and we concatenate adjacent 15 frames and perform LDA to reduce the dimension to 42 for the final feature vectors. The language model of the ASR system is a trigram LM trained on the audio transcripts with around three million words with Kneser-Ney smoothing (Stolcke, 2002). To perform LM adaptation for the ASR system, we use the ASR hypotheses from day 1 to build a LM. This LM is then interpolated with the original trigram LM to produce an adapted LM for day 2. We also evaluate the effect Table 2 shows the impact of ASR a</context>
</contexts>
<marker>Povey, Kanevsky, Kingsbury, Ramabhadran, Saon, Visweswariah, 2009</marker>
<rawString>Daniel Povey, Dimitri Kanevsky, Brian Kingsbury, Bhu vana Ramabhadran, George Saon, and Karthik Visweswariah. 2009. Boosted MMI for model and feature-space discriminative training. In Proc. ofInt. Conf. on Acoustics, Speech and Signal Processing, Las Vegas, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Intl. Conf. on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver.</location>
<contexts>
<context position="7630" citStr="Stolcke, 2002" startWordPosition="1246" endWordPosition="1247"> to improve recognition. The acoustic model has 6000 codebooks and each codebook has at most 64 Gaussian mixtures determined by merge-andsplit training. Semi-tied covariance and boosted MMI discriminative training is performed to improve the model (Povey et al., 2009). The features for the acoustic model is the standard 39-dimension MFCC and we concatenate adjacent 15 frames and perform LDA to reduce the dimension to 42 for the final feature vectors. The language model of the ASR system is a trigram LM trained on the audio transcripts with around three million words with Kneser-Ney smoothing (Stolcke, 2002). To perform LM adaptation for the ASR system, we use the ASR hypotheses from day 1 to build a LM. This LM is then interpolated with the original trigram LM to produce an adapted LM for day 2. We also evaluate the effect Table 2 shows the impact of ASR adaptation on the performance of the translation system in BLEU (Papineni et al., 2002). In these experiments we only performed adaptation on ASR and still using the baseline SMT component. There is no obvious difference between unsupervised and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An extensible language modeling toolkit. In Proc. Intl. Conf. on Spoken Language Processing, volume 2, pages 901–904, Denver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Stephan Vogel</author>
</authors>
<title>Considerations in maximum mutual information and minimum classification error training for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofEAMT-05,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="9267" citStr="Venugopal and Vogel, 2005" startWordPosition="1525" endWordPosition="1529">y of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background 150 language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 translation references; 2) re-tune the translation system on day 1 via minimum error rate training (MERT) (Venugopal and Vogel, 2005). Use Day 1 Day 2 Baseline 29.39 27.41 500 Best 1gramLM 29.18 27.23 MT Hypos 2gramLM 29.53 27.50 3gramLM 29.36 27.23 Table 3: Performance in BLEU of unsupervised adaptation. The first question we would like to address is whether our adaptation obtains improvements via an unsupervised manner. We take day 1 baseline ASR hypothesis and use the baseline SMT to get the MT hypothesis and a 500- best list. We train a domain LM using the 500-best list and use the MT hypotheses as the reference in MERT. We treat day 1 as a development set and day 2 as an unseen test set. In Table 3 we compare the perfo</context>
</contexts>
<marker>Venugopal, Vogel, 2005</marker>
<rawString>Ashish Venugopal and Stephan Vogel. 2005. Considerations in maximum mutual information and minimum classification error training for statistical machine translation. In Proceedings ofEAMT-05, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
</authors>
<title>Pesa: Phrase pair extraction as sentence splitting.</title>
<date>2005</date>
<booktitle>In Proc. ofMT SUMMIT X,</booktitle>
<location>Phuket, Thailand.</location>
<contexts>
<context position="8481" citStr="Vogel, 2005" startWordPosition="1395" endWordPosition="1396">impact of ASR adaptation on the performance of the translation system in BLEU (Papineni et al., 2002). In these experiments we only performed adaptation on ASR and still using the baseline SMT component. There is no obvious difference between unsupervised and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small. 4 SMT Adaptation The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one approach is to optimize LM interpolation weights by minimizing perplexity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background 150 language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best li</context>
</contexts>
<marker>Vogel, 2005</marker>
<rawString>Stephan Vogel. 2005. Pesa: Phrase pair extraction as sentence splitting. In Proc. ofMT SUMMIT X, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
</authors>
<title>An efficient phrase-tophrase alignment model for arbitrarily long phrase and large corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of EAMT’05,</booktitle>
<location>Budapest, Hungary,</location>
<contexts>
<context position="8539" citStr="Zhang and Vogel, 2005" startWordPosition="1403" endWordPosition="1406">he translation system in BLEU (Papineni et al., 2002). In these experiments we only performed adaptation on ASR and still using the baseline SMT component. There is no obvious difference between unsupervised and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small. 4 SMT Adaptation The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one approach is to optimize LM interpolation weights by minimizing perplexity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background 150 language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 trans</context>
</contexts>
<marker>Zhang, Vogel, 2005</marker>
<rawString>Ying Zhang and Stephan Vogel. 2005. An efficient phrase-tophrase alignment model for arbitrarily long phrase and large corpora. In Proceedings of EAMT’05, Budapest, Hungary, May. The European Association for Machine Translation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>