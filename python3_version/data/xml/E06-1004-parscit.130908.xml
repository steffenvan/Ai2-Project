<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.787208">
Computational Complexity of Statistical Machine Translation
Raghavendra Udupa U.
IBM India Research Lab
New Delhi
India
</note>
<email confidence="0.80148">
uraghave@in.ibm.com
</email>
<author confidence="0.749229">
Hemanta K. Maji
</author>
<affiliation confidence="0.9231595">
Dept. of Computer Science
University of Illinois at Urbana-Champaigne
</affiliation>
<email confidence="0.973866">
hemanta.maji@gmail.com
</email>
<sectionHeader confidence="0.984077" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9984365">
In this paper we study a set of prob-
lems that are of considerable importance
to Statistical Machine Translation (SMT)
but which have not been addressed satis-
factorily by the SMT research community.
Over the last decade, a variety of SMT
algorithms have been built and empiri-
cally tested whereas little is known about
the computational complexity of some of
the fundamental problems of SMT. Our
work aims at providing useful insights into
the the computational complexity of those
problems. We prove that while IBM Mod-
els 1-2 are conceptually and computation-
ally simple, computations involving the
higher (and more useful) models are hard.
Since it is unlikely that there exists a poly-
nomial time solution for any of these hard
problems (unless P = NP and P#P =
P), our results highlight and justify the
need for developing polynomial time ap-
proximations for these computations. We
also discuss some practical ways of deal-
ing with complexity.
</bodyText>
<sectionHeader confidence="0.992318" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999604518518519">
Statistical Machine Translation is a data driven
machine translation technique which uses proba-
bilistic models of natural language for automatic
translation (Brown et al., 1993), (Al-Onaizan et
al., 1999). The parameters of the models are
estimated by iterative maximum-likelihood train-
ing on a large parallel corpus of natural language
texts using the EM algorithm (Brown et al., 1993).
The models are then used to decode, i.e. trans-
late texts from the source language to the target
language 1 (Tillman, 2001), (Wang, 1997), (Ger-
mann et al., 2003), (Udupa et al., 2004). The
models are independent of the language pair and
therefore, can be used to build a translation sys-
tem for any language pair as long as a parallel
corpus of texts is available for training. Increas-
ingly, parallel corpora are becoming available
for many language pairs and SMT systems have
been built for French-English, German-English,
Arabic-English, Chinese-English, Hindi-English
and other language pairs (Brown et al., 1993), (Al-
Onaizan et al., 1999), (Udupa, 2004).
In SMT, every English sentence e is considered
as a translation of a given French sentence f with
probability Pr (f|e). Therefore, the problem of
translating f can be viewed as a problem of finding
the most probable translation of f:
</bodyText>
<equation confidence="0.8509725">
Pr(f|e)P(e).
(1)
</equation>
<bodyText confidence="0.913845">
The probability distributions Pr(f|e) and
Pr(e) are known as translation model and lan-
guage model respectively. In the classic work on
SMT, Brown and his colleagues at IBM introduced
the notion of alignment between a sentence f and
its translation e and used it in the development of
translation models (Brown et al., 1993). An align-
ment between f = f1 ... fm and e = e1 ... el
is a many-to-one mapping a : 11, ... , m} —*
10, ... , l}. Thus, an alignment a between f and e
associates the french word fj to the English word
2
eaj . The number of words of f mapped to ei by
a is called the fertility of ei and is denoted by Oi.
Since Pr(f|e) = Ea Pr(f, a|e), equation 1 can
</bodyText>
<note confidence="0.8874088">
1In this paper, we use French and English as the prototyp-
ical examples of source and target languages respectively.
2e0 is a special word called the null word and is used to
account for those words in f that are not connected by a to
any of the words of e.
</note>
<figure confidence="0.921306142857143">
e∗ = argmax Pr(e|f) = argmax
e e
25
be rewritten as follows:
e* = argmax E Pr(f, a|e)Pr(e). (2)
e a
• Relaxed Decoding
</figure>
<bodyText confidence="0.972582923076923">
Given the model parameters and a sentence f,
determine the most probable translation and
alignment pair for f.
Brown and his colleagues developed a series
of 5 translation models which have become to be
known in the field of machine translation as IBM
models. For a detailed introduction to IBM trans-
lation models, please see (Brown et al., 1993). In
practice, models 3-5 are known to give good re-
sults and models 1-2 are used to seed the EM it-
erations of the higher models. IBM model 3 is
the prototypical translation model and it models
Pr(f, a|e) as follows:
</bodyText>
<equation confidence="0.99732">
P (f,a|e) = n (o0  |E&apos;=7 0i) r1Z=1 n (oi|ei) oi!
X r1mj=1 t (fj|eaj) X Hj:aj=,40 d(j|i,m,l)
</equation>
<tableCaption confidence="0.978165">
Table 1: IBM Model 3
</tableCaption>
<bodyText confidence="0.997083">
Here, n(o|e) is the fertility model, t(f|e) is
the lexicon model and d(j|i, m, l) is the distortion
model.
The computational tasks involving IBM Models
are the following:
</bodyText>
<listItem confidence="0.720236">
• Viterbi Alignment
</listItem>
<bodyText confidence="0.986399666666667">
Given the model parameters and a sentence
pair (f, e), determine the most probable
alignment between f and e.
</bodyText>
<equation confidence="0.865081">
a* = argmax P(f, a|e)
a
</equation>
<listItem confidence="0.766453">
• Expectation Evaluation
</listItem>
<bodyText confidence="0.99683175">
This forms the core of model training via the
EM algorithm. Please see Section 2.3 for
a description of the computational task in-
volved in the EM iterations.
</bodyText>
<listItem confidence="0.741033">
• Conditional Probability
</listItem>
<bodyText confidence="0.852915">
Given the model parameters and a sentence
pair (f, e), compute P(f|e).
</bodyText>
<equation confidence="0.9580335">
P(f|e) = � P(f, a|e)
a
</equation>
<listItem confidence="0.774245">
• Exact Decoding
</listItem>
<bodyText confidence="0.999433">
Given the model parameters and a sentence f,
determine the most probable translation of f.
</bodyText>
<equation confidence="0.996286">
P(f, a|e) P(e)
(e*, a*) = argmax P(f, a|e) P(e)
(e,a)
</equation>
<bodyText confidence="0.981595047619048">
Viterbi Alignment computation finds applica-
tions not only in SMT but also in other areas
of Natural Language Processing (Wang, 1998),
(Marcu, 2002). Expectation Evaluation is the
soul of parameter estimation (Brown et al., 1993),
(Al-Onaizan et al., 1999). Conditional Proba-
bility computation is important in experimentally
studying the concentration of the probability mass
around the Viterbi alignment, i.e. in determining
the goodness of the Viterbi alignment in compar-
ison to the rest of the alignments. Decoding is
an integral component of all SMT systems (Wang,
1997), (Tillman, 2000), (Och et al., 2001), (Ger-
mann et al., 2003), (Udupa et al., 2004). Exact
Decoding is the original decoding problem as de-
fined in (Brown et al., 1993) and Relaxed Decod-
ing is the relaxation of the decoding problem typ-
ically used in practice.
While several heuristics have been developed
by practitioners of SMT for the computational
tasks involving IBM models, not much is known
about the computational complexity of these tasks.
In their seminal paper on SMT, Brown and his col-
leagues highlighted the problems we face as we go
from IBM Models 1-2 to 3-5(Brown et al., 1993)
3:
“As we progress from Model 1 to Model 5, eval-
uating the expectations that gives us counts be-
comes increasingly difficult. In Models 3 and 4,
we must be content with approximate EM itera-
tions because it is not feasible to carry out sums
over all possible alignments for these models. In
practice, we are never sure that we have found the
Viterbi alignment”.
However, neither their work nor the subsequent
research in SMT studied the computational com-
plexity of these fundamental problems with the
exception of the Decoding problem. In (Knight,
1999) it was proved that the Exact Decoding prob-
lem is NP-Hard when the language model is a bi-
gram model.
Our results may be summarized as follows:
</bodyText>
<equation confidence="0.708761666666667">
3The emphasis is ours.
e* = argmax
e
�
a
26
</equation>
<bodyText confidence="0.997236">
lies in the class #P, where p(.) is a polynomial.
Given functions f, g : E* —* N, we say that g is
polynomial-time Turing reducible to f (i.e. g GT
f) if there is a Turing machine with an oracle for
f that computes g in time polynomial in the size
of the input. Similarly, we say that f is #P-Hard,
if every function in #P can be polynomial time
Turing reduced to f. If f is #P-Hard and is in
#P, then we say that f is #P-Complete.
</bodyText>
<subsectionHeader confidence="0.986274">
2.1 Viterbi Alignment Computation
</subsectionHeader>
<bodyText confidence="0.9983625">
VITERBI-3 is defined as follows. Given the para-
meters of IBM Model 3 and a sentence pair (f, e),
compute the most probable alignment a* betwen f
and e:
</bodyText>
<equation confidence="0.688875">
a* = argmax P(f, a|e).
a
</equation>
<subsectionHeader confidence="0.993523">
2.2 Conditional Probability Computation
</subsectionHeader>
<bodyText confidence="0.977941">
PROBABILITY-3 is defined as follows. Given
the parameters of IBM Model 3, and a sen-
tence pair (f, e), compute the probability
P (f|e) = Pa P (f, a|e).
</bodyText>
<subsectionHeader confidence="0.9988">
2.3 Expectation Evaluation in EM Iterations
</subsectionHeader>
<bodyText confidence="0.9999574">
(f, e)-COUNT-3, (O, e)-COUNT-3, (j, i, m, l)-
COUNT-3, 0-COUNT-3, and 1-COUNT-3 are de-
fined respectively as follows. Given the parame-
ters of IBM Model 3, and a sentence pair (f, e),
compute the following 4:
</bodyText>
<equation confidence="0.983156">
X
c(f|e; f, e) =
a
X
c(O|e;f,e) =
a
X
c(j|i, m, l; f, e) =
a
c(0; f, e) = X P(a|f, e)(m − 2O0), and
a
c(1; f, e) = X P(a|f, e)O0.
a
</equation>
<subsectionHeader confidence="0.987463">
2.4 Decoding
</subsectionHeader>
<bodyText confidence="0.6517232">
E-DECODING-3 and R-DECODING-3 are defined
as follows. Given the parameters of IBM Model 3,
4As the counts are normalized in the EM iteration, we can
replace P(alf, e) by P(f, ale) in the Expectation Evaluation
tasks.
</bodyText>
<figure confidence="0.681798444444445">
1. Viterbi Alignment computation is NP-Hard
for IBM Models 3, 4, and 5.
2. Expectation Evaluation in EM Iterations is
#P-Complete for IBM Models 3, 4, and 5.
3. Conditional Probability computation is
#P-Complete for IBM Models 3, 4, and 5.
4. Exact Decoding is #P-Hard for IBM Mod-
els 3, 4, and 5.
5. Relaxed Decoding is NP-Hard for IBM
</figure>
<bodyText confidence="0.975262933333333">
Models 3, 4, and 5.
Note that our results for decoding are sharper
than that of (Knight, 1999). Firstly, we show that
Exact Decoding is #P-Hard for IBM Models 3-5
and not just NP-Hard. Secondly, we show that
Relaxed Decoding is NP-Hard for Models 3-5
even when the language model is a uniform dis-
tribution.
The rest of the paper is organized as follows.
We formally define all the problems discussed in
the paper (Section 2). Next, we take up each of the
problems discussed in this section and derive the
stated result for them (Section 3). After this, we
discuss the implications of our results (Section 4)
and suggest future directions (Section 5).
</bodyText>
<sectionHeader confidence="0.409279" genericHeader="method">
2 Problem Definition
</sectionHeader>
<bodyText confidence="0.998010555555556">
Consider the functions f,g : E* —* 10, 11. We
say that g GmP f (g is polynomial-time many-one
reducible to f), if there exists a polynomial time
reduction r(.) such that g(x) = f(r(x)) for all
input instances x E E*. This means that given a
machine to evaluate f(.) in polynomial time, there
exists a machine that can evaluate g(.) in polyno-
mial time. We say a function f is NP-Hard, if all
functions in NP are polynomial-time many-one
reducible to f. In addition, if f E NP, then we
say that f is NP-Complete.
Also relevant to our work are counting func-
tions that answer queries such as “how many com-
putation paths exist for accepting a particular in-
stance of input?” Let w be a witness for the ac-
ceptance of an input instance x and χ(x, w) be
a polynomial time witness checking function (i.e.
χ(x, w) E P). The function f : E* —* N such that
</bodyText>
<equation confidence="0.958087111111111">
f(x) = X χ(x, w)
WEE∗
|W|&lt;_P(|X|)
P(a|f, e) X s(f, fj)s(e, eaj),
j
P(a|f, e) X s(O, Oi)s(e, ei),
i
P(a|f, e)s(i, aj),
27
</equation>
<bodyText confidence="0.999789333333333">
and a sentence f, compute its most probable trans-
lation according to the following equations respec-
tively.
</bodyText>
<equation confidence="0.98972025">
e* = argmax
e
(e*, a*) = argmax
(e,a)
</equation>
<subsectionHeader confidence="0.732758">
2.5 SETCOVER
</subsectionHeader>
<bodyText confidence="0.999728571428571">
Given a collection of sets C = {S1, ... , Sl} and
a set X ⊆ ∪li=1Si, find the minimum cardinality
subset C&apos; of C such that every element in X be-
longs to at least one member of C&apos;.
SETCOVER is a well-known NP-Complete
problem. If SETCOVER ≤mp f, then f is NP-
Hard.
</bodyText>
<subsectionHeader confidence="0.724645">
2.6 PERMANENT
</subsectionHeader>
<bodyText confidence="0.999302777777778">
Proof: We give a polynomial time many-one
reduction from SETCOVER to VITERBI-3. Given
a collection of sets C = {S1, ... , Sl} and a set
X ⊆ ∪li=1Si, we create an instance of VITERBI-3
as follows:
For each set Si ∈ C, we create a word ei (1 ≤ i ≤
l). Similarly, for each element vj ∈ X we create
a word fj (1 ≤ j ≤ |X |= m). We set the model
parameters as follows:
</bodyText>
<equation confidence="0.680611714285714">
t (fj|ei) =
� 1
2 if O
n (O|e) = �! =60
1 if O = 0
d(j|i,m,l) = 1.
Now consider the sentences e =
e1 ... el and f = f1 ... fm.
� P(f, a|e) P(e)
a
P(f, a|e) P(e).
�
1 if vj ∈ Si
0 otherwise
</equation>
<bodyText confidence="0.978719">
Given a matrix M = [Mj,i]nxn whose entries are
either 0 or 1, compute the following:
</bodyText>
<equation confidence="0.9847455">
perm(M) = Eπ Hnj=1 Mj,πj where π is a per-
mutation of 1, ... , n.
</equation>
<bodyText confidence="0.995529">
This problem is the same as that of counting the
number of perfect matchings in a bipartite graph
and is known to be #P-Complete (?). If PERMA-
NENT ≤T f, then f is #P-Hard.
</bodyText>
<sectionHeader confidence="0.745209" genericHeader="method">
2.7 COMPAREPERMANENTS
</sectionHeader>
<equation confidence="0.994719071428571">
l
Oi �n(Oi |ei) Oi!
i=1
O0|
P (f, a|e) = n
�l
i=1
m
× H t (fj|eaj) H d (j|i, m, l)
j=1 j: aj=,40
1
�l
i=1
21−δ(φi,0)
</equation>
<bodyText confidence="0.997298857142857">
Given two matrices A = [Aj,i]nxn and B =
[Bj,i]nxn whose entries are either 0 or 1, determine
which of them has a larger permanent. PERMA-
NENT is known to be Turing reducible to COM-
PAREPERMANENTS (Jerrum, 2005) and therefore,
if COMPAREPERMANENTS ≤T f, then f is #P-
Hard.
</bodyText>
<sectionHeader confidence="0.979438" genericHeader="method">
3 Main Results
</sectionHeader>
<bodyText confidence="0.999920125">
In this section, we present the main reductions
for the problems with Model 3 as the translation
model. Our reductions can be easily carried over
to Models 4−5 with minor modifications. In order
to keep the presentation of the main ideas simple,
we let the lexicon, distortion, and fertility models
to be any non-negative functions and not just prob-
ability distributions in our reductions.
</bodyText>
<subsectionHeader confidence="0.990688">
3.1 VITERBI-3
</subsectionHeader>
<bodyText confidence="0.97181275">
We show that VITERBI-3 is NP-Hard.
Lemma 1 SETCOVER ≤mp VITERBI-3.
We can construct a cover for X from the output
of VITERBI-3 by defining C&apos; = {Si|Oi &gt; 0}. We
note that P (f, a|e) = Hni=1 21−δ(φi,0) = 2−|C�|.
1
Therefore, Viterbi alignment results in the mini-
mum cover for X.
</bodyText>
<subsectionHeader confidence="0.994469">
3.2 PROBABILITY-3
</subsectionHeader>
<bodyText confidence="0.999531">
We show that PROBABILITY-3 is #P-Complete.
We begin by proving the following:
</bodyText>
<table confidence="0.5726494">
Lemma 2 PERMANENT ≤T PROBABILITY-3.
Proof: Given a 0,1-matrix M =
[Mj, i]nxn, we define f = f1 ... fn and e =
e1 ... en where each ei and fj is distinct and set
the Model 3 parameters as follows:
</table>
<equation confidence="0.9170506">
�
1 if Mj,i = 1
t (fj|ei) =
0 otherwise
�
1 if O = 1
n (O|e) =
0 otherwise
d(j|i,n,n) = 1.
28
</equation>
<bodyText confidence="0.977836">
Clearly, with the above parameter setting,
P (f, a|e) = Hnj=1 Mj, aj if a is a permutation
and 0 otherwise. Therefore,
</bodyText>
<figure confidence="0.569594416666667">
1:
P (f|e) =
a
1: =
a is a permutation
Thus, by construction, PROBABILITY-3 com-
putes perm (M). Besides, the construction con-
serves the number of witnesses. Hence, PERMA-
NENT ≤T PROBABILITY-3.
We now prove that
Lemma 3 PROBABILITY-3 is in #P.
Proof: Let (f, e) be the input to
</figure>
<bodyText confidence="0.997416533333333">
PROBABILITY-3. Let m and l be the lengths
of f and e respectively. With each alignment
a = (a1, a2, ... , am) we associate a unique num-
ber na = a1a2 ... am in base l + 1. Clearly,
0 ≤ na ≤ (l + 1)m − 1. Let w be the binary
encoding of na. Conversely, with every binary
string w we can associate an alignment a if the
value of w is in the range 0, ... , (l + 1)m − 1. It
requires O (m log (l + 1)) bits to encode an align-
ment. Thus, given an alignment we can compute
its encoding and given the encoding we can com-
pute the corresponding alignment in time polyno-
mial in l and m. Similarly, given an encoding we
can compute P (f, a|e) in time polynomial in l and
m. Now, if p(.) is a polynomial, then function
</bodyText>
<equation confidence="0.996442666666667">
f (f, e) = 1: P (f, a|e)
wE10,11∗
|w|&lt;p(|(f, e)|)
</equation>
<bodyText confidence="0.991814714285714">
is in #P. Choose p (x) = dx log2 (x + 1)e.
Clearly, all alignments can be encoded using at
most p ( |(f, e) |) bits. Therefore, if (f, e) com-
putes P (f|e) and hence, PROBABILITY-3 is in
#P.
It follows immediately from Lemma 2 and
Lemma 3 that
</bodyText>
<equation confidence="0.8579352">
Theorem 1 PROBABILITY-3 is #P-Complete.
3.3 (f, e)-COUNT-3
Lemma 4 PERMANENT ≤T (f, e)-COUNT-3.
Proof: The proof is similar to that of
Lemma 2. Let f = f1 f2 . . . fn fˆand e =
e1 e2 ... en ˆe. We set the translation model para-
meters as follows:
{ 1 if f = fj, e = ei and Mj,i = 1
1 if f = fˆand e = eˆ
0 otherwise.
</equation>
<bodyText confidence="0.999449333333333">
The rest of the parameters are set as in Lemma 2.
Let A be the set of alignments a, such that an+1 =
n + 1 and an1 is a permutation of 1, 2, ... , n. Now,
</bodyText>
<equation confidence="0.989443428571429">
� � 1:
c ˆf|ˆe; f, e =
a
ˆf, fj)δ(ˆe, eaj)
1: = P (f, a|e)
aEA
Mj, aj = perm (M) .
</equation>
<bodyText confidence="0.974303125">
Therefore, PERMANENT ≤T COUNT-3.
Lemma 5 (f, e)-COUNT-3 is in #P.
Proof: The proof is essentially the same as
that of Lemma 3. Note that given an encoding w,
P (f, a|e) Emj=1 δ (fj, f) δ (eaj, e) can be evalu-
ated in time polynomial in |(f, e)|.
Hence, from Lemma 4 and Lemma 5, it follows
that
</bodyText>
<construct confidence="0.69002475">
Theorem 2 (f, e)-COUNT-3 is #P-Complete.
3.4 (j, i, m, l)-COUNT-3
Lemma 6 PERMANENT ≤T (j, i, m, l)-COUNT-
3.
</construct>
<bodyText confidence="0.8351175">
Proof: We proceed as in the
proof of Lemma 4 with some modifica-
</bodyText>
<equation confidence="0.731257">
tions. Let e = e1 ... ei_1ˆeei ... en and
f = f1 ... fj_1 ˆffj ... fn. The parameters
</equation>
<bodyText confidence="0.992648">
are set as in Lemma 4. Let A be the set of
alignments, a, such that a is a permutation of
1, 2, ... , (n + 1) and aj = i. Observe that
P (f, a|e) is non-zero only for the alignments in
A. It follows immediately that with these para-
meter settings, c(j|i, n, n; f, e) = perm (M) .
</bodyText>
<equation confidence="0.93020608">
Lemma 7 (j, i, m, l)-COUNT-3 is in #P.
Proof: Similar to the proof of Lemma 5.
Theorem 3 (j, i, m, l)-COUNT-3 is #P-
Complete.
n
H Mj, aj = perm (M)
j=1
t (f|e) =
P (f, a|e)
P (f, a|e) n+11: δ( ˆf, fj)δ(ˆe, eaj)
j=1
1: = P (f, a|e) n+11: δ(
aEA j=1
1: =
aEA
n
H
j=1
29
3.5 (0, e)-COUNT-3
Lemma 8 PERMANENT ≤T (0, e)-COUNT-3.
Proof: Let e = e1 ... enˆe and f =
z } |{
k
fˆ ... ˆ
</equation>
<bodyText confidence="0.4933165">
f. Let A be the set of alignments
for which an1 is a permutation of 1, 2, ... , n and
</bodyText>
<equation confidence="0.992062">
k
z } |{
an+k
n+1 =
1 if 0 = 1 and e =6eˆ
1 if 0 = k and e = eˆ
0 otherwise.
</equation>
<bodyText confidence="0.857161142857143">
The rest of the parameters are set as in Lemma 4.
Note that P (f, a|e) is non-zero only for the align-
ments in A. It follows immediately that with these
parameter settings, c(k|ˆe; f, e) = perm (M) .
Lemma 9 (0, e)-COUNT-3 is in #P.
Proof: Similar to the proof of Lemma 5.
Theorem 4 (0, e)-COUNT-3 is #P-Complete.
</bodyText>
<subsectionHeader confidence="0.270757">
3.6 0-COUNT-3
</subsectionHeader>
<bodyText confidence="0.6123975">
Lemma 10 PERMANENT ≤T 0-COUNT-3.
Proof: Let e = e1 ... en and f = f1 ... fn
Let A be the set of alignments, a, such that an1 is
a permutation of 1, ... , n and an+1 = 0. We set
</bodyText>
<equation confidence="0.97329">
1 if f = fj, e = ei and Mj, i = 1
1 if f = fˆ and e =NULL
0 otherwise.
</equation>
<bodyText confidence="0.976241">
The rest of the parameters are set as in Lemma 4.
It is easy to see that with these settings, c(0;f,e)
</bodyText>
<equation confidence="0.883789666666667">
(n−2) =
perm (M) .
Lemma 11 0-COUNT-3 is in #P.
</equation>
<bodyText confidence="0.8944705">
Proof: Similar to the proof of Lemma 5.
Theorem 5 0-COUNT-3 is #P-Complete.
</bodyText>
<subsectionHeader confidence="0.331185">
3.7 1-COUNT-3
</subsectionHeader>
<bodyText confidence="0.499307666666667">
Lemma 12 PERMANENT ≤T 1-COUNT-3.
Proof: We set the parameters as in
Lemma 10. It follows immediately that
</bodyText>
<equation confidence="0.6229305">
c(1; f, e) = perm (M) .
Lemma 13 1-COUNT-3 is in #P.
</equation>
<bodyText confidence="0.611929">
Proof: Similar to the proof of Lemma 5.
Theorem 6 1-COUNT-3 is #P-Complete.
</bodyText>
<note confidence="0.44430475">
3.8 E-DECODING-3
Lemma 14 COMPAREPERMANENTS ≤T E-
DECODING-3
Proof: Let M and N be the two 0-1 matri-
</note>
<equation confidence="0.917519625">
ces. Let f = f1f2 ... fn, e(1) = e(1)
1e(1)
2 ... e(1)
n
and e(2) = e(2)
1 e(2) 2... e(2)
n . Further, let e(1) and
e(2) have no words in common and each word
</equation>
<bodyText confidence="0.915459857142857">
appears exactly once. By setting the bigram lan-
guage model probabilities of the bigrams that oc-
cur in e(1) and e(2) to 1 and all other bigram prob-
abilities to 0, we can ensure that the only trans-
lations considered by E-DECODING-3 are indeed
e(1) and e(2) and P (e(1)) = P (e(2)) = 1. We
then set
</bodyText>
<equation confidence="0.9981308">
1 if f = fj, e = e(1)
i and Mj,i = 1
1 if f = fj, e = e(2)
i and Nj,i = 1
0 otherwise
n (0|e) = 1 0 =
( 0 otherwise
1
d(j|i,n,n) = 1.
Now, P (f|e(1)) = perm (M), and P (f|e(2)) =
</equation>
<bodyText confidence="0.836304666666667">
perm (N). Therefore, given the output of E-
DECODING-3 we can find out which of M and
N has a larger permanent.
</bodyText>
<note confidence="0.50437">
Hence E-DECODING-3 is #P − Hard.
</note>
<table confidence="0.3837694">
3.9 R-DECODING-3
Lemma 15 SETCOVER ≤mp R-DECODING-3
Proof: Given an instance of SETCOVER, we
set the parameters as in the proof of Lemma 1 with
the following modification:
</table>
<equation confidence="0.972390666666667">
(n (0|e) = 0 otherwise.
2φ! if 0 &gt; 0
1
</equation>
<bodyText confidence="0.99994475">
Let e be the optimal translation obtained by solv-
ing R-DECODING-3. As the language model is
uniform, the exact order of the words in e is not
important. Now, we observe that:
</bodyText>
<listItem confidence="0.885116">
• e contains words only from the set
</listItem>
<bodyText confidence="0.923971">
{e1, e2, ... , el}. This is because, there can-
not be any zero fertility word as n (0|e) = 0
and the only words that can have a non-zero
fertility are from {e1, e2, ... , el} due to the
way we have set the lexicon parameters.
• No word occurs more than once in e. Assume
on the contrary that the word ei occurs k &gt; 1
</bodyText>
<figure confidence="0.694674375">
f1 ... fn
(n + 1) ... (n + 1) . We set
⎧
⎨⎪
⎪⎩
n (0|e) =
ˆf.
⎧
⎨⎪
⎪⎩
t (f|e) =
⎧
⎨⎪
⎪⎩
t (f|e) =
30
</figure>
<bodyText confidence="0.9985037">
times in e. Replace these k occurrences by
only one occurrence of ez and connect all the
words connected to them to this word. This
would increase the score of e by a factor of
2k−1 &gt; 1 contradicting the assumption on
the optimality of e.
As a result, the only candidates for e are subsets of
le1, e2, ... , el} in any order. It is now straight for-
ward to verify that a minimum set cover can be re-
covered from e as shown in the proof of Lemma 1.
</bodyText>
<sectionHeader confidence="0.745777" genericHeader="method">
3.10 IBM Models 4 and 5
</sectionHeader>
<bodyText confidence="0.926236">
The reductions are for Model 3 can be easily ex-
tended to Models 4 and 5. Thus, we have the fol-
lowing:
</bodyText>
<construct confidence="0.826834454545455">
Theorem 7 Viterbi Alignment computation is
NP-Hard for IBM Models 3 − 5.
Theorem 8 Expectation Evaluation in the EM
Steps is #P-Complete for IBM Models 3 − 5.
Theorem 9 Conditional Probability computation
is #P-Complete for IBM Models 3 − 5.
Theorem 10 Exact Decoding is #P-Hard for
IBM Models 3 − 5.
Theorem 11 Relaxed Decoding is NP-Hard for
IBM Models 3 − 5 even when the language model
is a uniform distribution.
</construct>
<sectionHeader confidence="0.998126" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999779564516129">
Our results answer several open questions on the
computation of Viterbi Alignment and Expectation
Evaluation. Unless P = NP and P#P = P,
there can be no polynomial time algorithms for
either of these problems. The evaluation of ex-
pectations becomes increasingly difficult as we go
from IBM Models 1-2 to Models 3-5 exactly be-
cause the problem is #P-Complete for the latter
models. There cannot be any trick for IBM Mod-
els 3-5 that would help us carry out the sums over
all possible alignments exactly. There cannot exist
a closed form expression (whose representation is
polynomial in the size of the input) for P (f|e) and
the counts in the EM iterations for Models 3-5.
It should be noted that the computation of
Viterbi Alignment and Expectation Evaluation is
easy for Models 1-2. What makes these computa-
tions hard for Models 3-5? To answer this ques-
tion, we observe that Models 1-2 lack explicit fer-
tility model unlike Models 3-5. In the former mod-
els, fertility probabilities are determined by the
lexicon and alignment models. Whereas, in Mod-
els 3-5, the fertility model is independent of the
lexicon and alignment models. It is precisely this
freedom that makes computations on Models 3-5
harder than the computations on Models 1-2.
There are three different ways of dealing with
the computational barrier posed by our problems.
The first of these is to develop a restricted fertil-
ity model that permits polynomial time computa-
tions. It remains to be found what kind of parame-
terized distributions are suitable for this purpose.
The second approach is to develop provably good
approximation algorithms for these problems as is
done with many NP-Hard and #P-Hard prob-
lems. Provably good approximation algorithms
exist for several covering problems including Set
Cover and Vertex Cover. Viterbi Alignment is itself
a special type of covering problem and it remains
to be seen whether some of the techniques devel-
oped for covering algorithms are useful for finding
good approximations to Viterbi Alignment. Sim-
ilarly, there exist several techniques for approxi-
mating the permanent of a matrix. It needs to be
explored if some of these ideas can be adapted for
Expectation Evaluation.
As the third approach to deal with complex-
ity, we can approximate the space of all possi-
ble (l + 1)m alignments by an exponentially large
subspace. To be useful such large subspaces
should also admit optimal polynomial time al-
gorithms for the problems we have discussed in
this paper. This is exactly the approach taken
by (Udupa, 2005) for solving the decoding and
Viterbi alignment problems. They show that very
efficient polynomial time algorithms can be de-
veloped for both Decoding and Viterbi Alignment
problems. Not only the algorithms are prov-
ably superior in a computational complexity sense,
(Udupa, 2005) are also able to get substantial im-
provements in BLEU and NIST scores over the
Greedy decoder.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99977175">
IBM models 3-5 are widely used in SMT. The
computational tasks discussed in this work form
the backbone of all SMT systems that use IBM
models. We believe that our results on the compu-
tational complexity of the tasks in SMT will result
in a better understanding of these tasks from a the-
oretical perspective. We also believe that our re-
sults may help in the design of effective heuristics
</bodyText>
<page confidence="0.972576">
31
</page>
<bodyText confidence="0.999904214285714">
for some of these tasks. A theoretical analysis of
the commonly employed heuristics will also be of
interest.
An open question in SMT is whether there ex-
ists closed form expressions (whose representation
is polynomial in the size of the input) for P (f|e)
and the counts in the EM iterations for models 3-5
(Brown et al., 1993). For models 1-2, closed form
expressions exist for P (f|e) and the counts in the
EM iterations for models 3-5. Our results show
that there cannot exist a closed form expression
(whose representation is polynomial in the size of
the input) for P(fIe) and the counts in the EM
iterations for Models 3-5 unless P = NP.
</bodyText>
<sectionHeader confidence="0.99434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999478">
K. Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. Computational
Linguistics.
Brown, P. et al: 1993. The Mathematics of Machine
Translation: Parameter Estimation. Computational
Linguistics, 2(19):263–311.
Al-Onaizan, Y. et al. 1999. Statistical Machine Trans-
lation: Final Report. JHU Workshop Final Report.
R. Udupa, and T. Faruquie. 2004. An English-Hindi
Statistical Machine Translation System. Proceed-
ings of the 1st IJCNLP.
Y. Wang, and A. Waibel. 1998. Modeling with Struc-
tures in Statistical Machine Translation. Proceed-
ings of the 36th ACL.
D. Marcu and W. Wong. 2002. A Phrase-Based, Joint
Probability Model for Statistical Machine Transla-
tion. Proceedings of the EMNLP.
L. Valiant. 1979. The complexity of computing the
permanent. Theoretical Computer Science, 8:189–
201.
M. Jerrum. 2005. Personal communication.
C. Tillman. 2001. Word Re-ordering and Dynamic
Programming based Search Algorithm for Statistical
Machine Translation. Ph.D. Thesis, University of
Technology Aachen 42–45.
Y. Wang and A. Waibel. 2001. Decoding algorithm in
statistical machine translation. Proceedings of the
35th ACL 366–372.
C. Tillman and H. Ney. 2000. Word reordering and
DP-based search in statistical machine translation.
Proceedings of the 18th COLING 850–856.
F. Och, N. Ueffing, and H. Ney. 2000. An efficient A*
search algorithm for statistical machine translation.
Proceedings of the ACL 2001 Workshop on Data-
Driven Methods in Machine Translation 55–62.
U. Germann et al. 2003. Fast Decoding and Optimal
Decoding for Machine Translation. Artificial Intel-
ligence.
R. Udupa, H. Maji, and T. Faruquie. 2004. An Al-
gorithmic Framework for the Decoding Problem in
Statistical Machine Translation. Proceedings of the
20th COLING.
R. Udupa and H. Maji. 2005. Theory of Alignment
Generators and Applications to Statistical Machine
Translation. Proceedings of the 19th IJCAI.
</reference>
<page confidence="0.944821">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.664151">
<title confidence="0.99375">Computational Complexity of Statistical Machine Translation</title>
<author confidence="0.961242">Raghavendra Udupa U</author>
<affiliation confidence="0.916097">IBM India Research Lab New Delhi India</affiliation>
<email confidence="0.999457">uraghave@in.ibm.com</email>
<author confidence="0.999441">Hemanta K Maji</author>
<affiliation confidence="0.999807">Dept. of Computer Science University of Illinois at Urbana-Champaigne</affiliation>
<email confidence="0.998712">hemanta.maji@gmail.com</email>
<abstract confidence="0.99494992">In this paper we study a set of problems that are of considerable importance to Statistical Machine Translation (SMT) but which have not been addressed satisfactorily by the SMT research community. Over the last decade, a variety of SMT algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental problems of SMT. Our work aims at providing useful insights into the the computational complexity of those problems. We prove that while IBM Models 1-2 are conceptually and computationally simple, computations involving the higher (and more useful) models are hard. Since it is unlikely that there exists a polynomial time solution for any of these hard (unless = NP our results highlight and justify the need for developing polynomial time approximations for these computations. We also discuss some practical ways of dealing with complexity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Knight</author>
</authors>
<title>Decoding Complexity in WordReplacement Translation Models. Computational Linguistics.</title>
<date>1999</date>
<contexts>
<context position="6740" citStr="Knight, 1999" startWordPosition="1149" endWordPosition="1150"> face as we go from IBM Models 1-2 to 3-5(Brown et al., 1993) 3: “As we progress from Model 1 to Model 5, evaluating the expectations that gives us counts becomes increasingly difficult. In Models 3 and 4, we must be content with approximate EM iterations because it is not feasible to carry out sums over all possible alignments for these models. In practice, we are never sure that we have found the Viterbi alignment”. However, neither their work nor the subsequent research in SMT studied the computational complexity of these fundamental problems with the exception of the Decoding problem. In (Knight, 1999) it was proved that the Exact Decoding problem is NP-Hard when the language model is a bigram model. Our results may be summarized as follows: 3The emphasis is ours. e* = argmax e � a 26 lies in the class #P, where p(.) is a polynomial. Given functions f, g : E* —* N, we say that g is polynomial-time Turing reducible to f (i.e. g GT f) if there is a Turing machine with an oracle for f that computes g in time polynomial in the size of the input. Similarly, we say that f is #P-Hard, if every function in #P can be polynomial time Turing reduced to f. If f is #P-Hard and is in #P, then we say that</context>
<context position="8804" citStr="Knight, 1999" startWordPosition="1545" endWordPosition="1546">defined as follows. Given the parameters of IBM Model 3, 4As the counts are normalized in the EM iteration, we can replace P(alf, e) by P(f, ale) in the Expectation Evaluation tasks. 1. Viterbi Alignment computation is NP-Hard for IBM Models 3, 4, and 5. 2. Expectation Evaluation in EM Iterations is #P-Complete for IBM Models 3, 4, and 5. 3. Conditional Probability computation is #P-Complete for IBM Models 3, 4, and 5. 4. Exact Decoding is #P-Hard for IBM Models 3, 4, and 5. 5. Relaxed Decoding is NP-Hard for IBM Models 3, 4, and 5. Note that our results for decoding are sharper than that of (Knight, 1999). Firstly, we show that Exact Decoding is #P-Hard for IBM Models 3-5 and not just NP-Hard. Secondly, we show that Relaxed Decoding is NP-Hard for Models 3-5 even when the language model is a uniform distribution. The rest of the paper is organized as follows. We formally define all the problems discussed in the paper (Section 2). Next, we take up each of the problems discussed in this section and derive the stated result for them (Section 3). After this, we discuss the implications of our results (Section 4) and suggest future directions (Section 5). 2 Problem Definition Consider the functions</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>K. Knight. 1999. Decoding Complexity in WordReplacement Translation Models. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
</authors>
<title>The Mathematics of Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>2</volume>
<issue>19</issue>
<marker>Brown, 1993</marker>
<rawString>Brown, P. et al: 1993. The Mathematics of Machine Translation: Parameter Estimation. Computational Linguistics, 2(19):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
</authors>
<title>Statistical Machine Translation: Final Report.</title>
<date>1999</date>
<tech>JHU Workshop Final Report.</tech>
<marker>Al-Onaizan, 1999</marker>
<rawString>Al-Onaizan, Y. et al. 1999. Statistical Machine Translation: Final Report. JHU Workshop Final Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>T Faruquie</author>
</authors>
<title>An English-Hindi Statistical Machine Translation System.</title>
<date>2004</date>
<booktitle>Proceedings of the 1st IJCNLP.</booktitle>
<marker>Udupa, Faruquie, 2004</marker>
<rawString>R. Udupa, and T. Faruquie. 2004. An English-Hindi Statistical Machine Translation System. Proceedings of the 1st IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>A Waibel</author>
</authors>
<date>1998</date>
<booktitle>Modeling with Structures in Statistical Machine Translation. Proceedings of the 36th ACL.</booktitle>
<marker>Wang, Waibel, 1998</marker>
<rawString>Y. Wang, and A. Waibel. 1998. Modeling with Structures in Statistical Machine Translation. Proceedings of the 36th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wong</author>
</authors>
<title>A Phrase-Based, Joint Probability Model for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>Proceedings of the EMNLP.</booktitle>
<marker>Marcu, Wong, 2002</marker>
<rawString>D. Marcu and W. Wong. 2002. A Phrase-Based, Joint Probability Model for Statistical Machine Translation. Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Valiant</author>
</authors>
<title>The complexity of computing the permanent.</title>
<date>1979</date>
<journal>Theoretical Computer Science,</journal>
<tech>Personal communication.</tech>
<volume>8</volume>
<marker>Valiant, 1979</marker>
<rawString>L. Valiant. 1979. The complexity of computing the permanent. Theoretical Computer Science, 8:189– 201. M. Jerrum. 2005. Personal communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillman</author>
</authors>
<title>Word Re-ordering and Dynamic Programming based Search Algorithm for Statistical Machine Translation.</title>
<date>2001</date>
<tech>Ph.D. Thesis,</tech>
<pages>42--45</pages>
<institution>University of Technology Aachen</institution>
<contexts>
<context position="1725" citStr="Tillman, 2001" startWordPosition="268" endWordPosition="269"> for these computations. We also discuss some practical ways of dealing with complexity. 1 Introduction Statistical Machine Translation is a data driven machine translation technique which uses probabilistic models of natural language for automatic translation (Brown et al., 1993), (Al-Onaizan et al., 1999). The parameters of the models are estimated by iterative maximum-likelihood training on a large parallel corpus of natural language texts using the EM algorithm (Brown et al., 1993). The models are then used to decode, i.e. translate texts from the source language to the target language 1 (Tillman, 2001), (Wang, 1997), (Germann et al., 2003), (Udupa et al., 2004). The models are independent of the language pair and therefore, can be used to build a translation system for any language pair as long as a parallel corpus of texts is available for training. Increasingly, parallel corpora are becoming available for many language pairs and SMT systems have been built for French-English, German-English, Arabic-English, Chinese-English, Hindi-English and other language pairs (Brown et al., 1993), (AlOnaizan et al., 1999), (Udupa, 2004). In SMT, every English sentence e is considered as a translation o</context>
</contexts>
<marker>Tillman, 2001</marker>
<rawString>C. Tillman. 2001. Word Re-ordering and Dynamic Programming based Search Algorithm for Statistical Machine Translation. Ph.D. Thesis, University of Technology Aachen 42–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>A Waibel</author>
</authors>
<title>Decoding algorithm in statistical machine translation.</title>
<date>2001</date>
<booktitle>Proceedings of the 35th ACL</booktitle>
<pages>366--372</pages>
<marker>Wang, Waibel, 2001</marker>
<rawString>Y. Wang and A. Waibel. 2001. Decoding algorithm in statistical machine translation. Proceedings of the 35th ACL 366–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillman</author>
<author>H Ney</author>
</authors>
<title>Word reordering and DP-based search in statistical machine translation.</title>
<date>2000</date>
<booktitle>Proceedings of the 18th COLING</booktitle>
<pages>850--856</pages>
<marker>Tillman, Ney, 2000</marker>
<rawString>C. Tillman and H. Ney. 2000. Word reordering and DP-based search in statistical machine translation. Proceedings of the 18th COLING 850–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>N Ueffing</author>
<author>H Ney</author>
</authors>
<title>An efficient A* search algorithm for statistical machine translation.</title>
<date>2000</date>
<booktitle>Proceedings of the ACL 2001 Workshop on DataDriven Methods in Machine Translation</booktitle>
<pages>55--62</pages>
<marker>Och, Ueffing, Ney, 2000</marker>
<rawString>F. Och, N. Ueffing, and H. Ney. 2000. An efficient A* search algorithm for statistical machine translation. Proceedings of the ACL 2001 Workshop on DataDriven Methods in Machine Translation 55–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Germann</author>
</authors>
<title>Fast Decoding and Optimal Decoding for Machine Translation.</title>
<date>2003</date>
<journal>Artificial Intelligence.</journal>
<marker>Germann, 2003</marker>
<rawString>U. Germann et al. 2003. Fast Decoding and Optimal Decoding for Machine Translation. Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>H Maji</author>
<author>T Faruquie</author>
</authors>
<title>An Algorithmic Framework for the Decoding Problem in Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>Proceedings of the 20th COLING.</booktitle>
<contexts>
<context position="1785" citStr="Udupa et al., 2004" startWordPosition="277" endWordPosition="280"> ways of dealing with complexity. 1 Introduction Statistical Machine Translation is a data driven machine translation technique which uses probabilistic models of natural language for automatic translation (Brown et al., 1993), (Al-Onaizan et al., 1999). The parameters of the models are estimated by iterative maximum-likelihood training on a large parallel corpus of natural language texts using the EM algorithm (Brown et al., 1993). The models are then used to decode, i.e. translate texts from the source language to the target language 1 (Tillman, 2001), (Wang, 1997), (Germann et al., 2003), (Udupa et al., 2004). The models are independent of the language pair and therefore, can be used to build a translation system for any language pair as long as a parallel corpus of texts is available for training. Increasingly, parallel corpora are becoming available for many language pairs and SMT systems have been built for French-English, German-English, Arabic-English, Chinese-English, Hindi-English and other language pairs (Brown et al., 1993), (AlOnaizan et al., 1999), (Udupa, 2004). In SMT, every English sentence e is considered as a translation of a given French sentence f with probability Pr (f|e). There</context>
<context position="5680" citStr="Udupa et al., 2004" startWordPosition="966" endWordPosition="969">pplications not only in SMT but also in other areas of Natural Language Processing (Wang, 1998), (Marcu, 2002). Expectation Evaluation is the soul of parameter estimation (Brown et al., 1993), (Al-Onaizan et al., 1999). Conditional Probability computation is important in experimentally studying the concentration of the probability mass around the Viterbi alignment, i.e. in determining the goodness of the Viterbi alignment in comparison to the rest of the alignments. Decoding is an integral component of all SMT systems (Wang, 1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004). Exact Decoding is the original decoding problem as defined in (Brown et al., 1993) and Relaxed Decoding is the relaxation of the decoding problem typically used in practice. While several heuristics have been developed by practitioners of SMT for the computational tasks involving IBM models, not much is known about the computational complexity of these tasks. In their seminal paper on SMT, Brown and his colleagues highlighted the problems we face as we go from IBM Models 1-2 to 3-5(Brown et al., 1993) 3: “As we progress from Model 1 to Model 5, evaluating the expectations that gives us count</context>
</contexts>
<marker>Udupa, Maji, Faruquie, 2004</marker>
<rawString>R. Udupa, H. Maji, and T. Faruquie. 2004. An Algorithmic Framework for the Decoding Problem in Statistical Machine Translation. Proceedings of the 20th COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>H Maji</author>
</authors>
<date>2005</date>
<booktitle>Theory of Alignment Generators and Applications to Statistical Machine Translation. Proceedings of the 19th IJCAI.</booktitle>
<marker>Udupa, Maji, 2005</marker>
<rawString>R. Udupa and H. Maji. 2005. Theory of Alignment Generators and Applications to Statistical Machine Translation. Proceedings of the 19th IJCAI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>