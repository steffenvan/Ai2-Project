<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006006">
<title confidence="0.923172">
METER: MEasuring TExt Reuse
</title>
<author confidence="0.997197">
Paul Clough and Robert Gaizauskas and Scott S.L. Piao and Yorick Wilks
</author>
<affiliation confidence="0.9982255">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.930697">
Regent Court, 211 Portobello Street,
Sheffield, England, S1 4DP
</address>
<email confidence="0.999457">
{initial.surname@dcs.shef.ac.uk}
</email>
<sectionHeader confidence="0.995672" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999770148148148">
In this paper we present results from
the METER (MEasuring TExt Reuse)
project whose aim is to explore issues
pertaining to text reuse and derivation,
especially in the context of newspapers
using newswire sources. Although the
reuse of text by journalists has been
studied in linguistics, we are not aware
of any investigation using existing com-
putational methods for this particular
task. We investigate the classification
of newspaper articles according to their
degree of dependence upon, or deriva-
tion from, a newswire source using a
simple 3-level scheme designed by jour-
nalists. Three approaches to measur-
ing text similarity are considered: n-
gram overlap, Greedy String Tiling,
and sentence alignment. Measured
against a manually annotated corpus of
source and derived news text, we show
that a combined classifier with fea-
tures automatically selected performs
best overall for the ternary classifica-
tion achieving an average Fl-measure
score of 0.664 across all three cate-
gories.
</bodyText>
<sectionHeader confidence="0.999032" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997764">
A topic of considerable theoretical and practical
interest is that of text reuse: the reuse of existing
written sources in the creation of a new text. Of
course, reusing language is as old as the retelling
of stories, but current technologies for creating,
copying and disseminating electronic text, make
it easier than ever before to take some or all of
any number of existing text sources and reuse
them verbatim or with varying degrees of mod-
ification.
One form of unacceptable text reuse, plagia-
rism, has received considerable attention and
software for automatic plagiarism detection is
now available (see, e.g. (Clough, 2000) for a re-
cent review). But in this paper we present a
benign and acceptable form of text reuse that
is encountered virtually every day: the reuse of
news agency text (called copy) in the produc-
tion of daily newspapers. The question is not
just whether agency copy has been reused, but
to what extent and subject to what transforma-
tions. Using existing approaches from computa-
tional text analysis, we investigate their ability
to classify newspapers articles into categories in-
dicating their dependency on agency copy.
</bodyText>
<sectionHeader confidence="0.741312" genericHeader="introduction">
2 Journalistic reuse of a newswire
</sectionHeader>
<bodyText confidence="0.999962666666667">
The process of gathering, editing and publish-
ing newspaper stories is a complex and spe-
cialised task often operating within specific pub-
lishing constraints such as: 1) short deadlines;
2) prescriptive writing practice (see, e.g. Evans
(1972)); 3) limits of physical size; 4) readability
and audience comprehension, e.g. a tabloid&apos;s
vocabulary limitations; 5) journalistic bias, e.g.
political and 6) a newspaper&apos;s house style. Of-
ten newsworkers, such as the reporter and edi-
tor, will rely upon news agency copy as the basis
of a news story or to verify facts and assess the
</bodyText>
<note confidence="0.720375">
Proceedings of the 40th Annual Meeting of the Association for
</note>
<page confidence="0.291478">
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 152-159.
</page>
<bodyText confidence="0.966656487804878">
importance of a story in the context of all those
appearing on the newswire. Because of the na-
ture of journalistic text reuse, differences will
arise between reused news agency copy and the
original text. For example consider the follow-
ing:
Original (news agency) A drink-driver who
ran into the Queen Mother&apos;s official Daim-
ler was fined $700 and banned from driving
for two years.
Rewrite (tabloid) A DRUNK driver who
ploughed into the Queen Mother&apos;s limo was
fined $700 and banned for two years yes-
terday.
This simple example illustrates the types of
rewrite that can occur even in a very short
sentence. The rewrite makes use of slang and
exaggeration to capture its readers&apos; attention
(e.g. DRUNK, limo, ploughed). Deletion (e.g.
from driving) has also been used and the addi-
tion of yesterday indicates when the event oc-
curred. Many of the transformations we ob-
served between moving from news agency copy
to the newspaper version have also been re-
ported by the summarisation community (see,
e.g., McKeown and Jing (1999)).
Given the value of the information news agen-
cies supply, the ease with which text can be
reused and commercial pressures, it would be
beneficial to be able to identify those news sto-
ries appearing in the newspapers that have relied
upon agency copy in their production. Potential
uses include: 1) monitoring take-up of agency
copy; 2) identifying the most reused stories ; 3)
determining customer dependency upon agency
copy and 4) new methods for charging customers
based upon the amount of copy reused. Given
the large volume of news agency copy output
each day, it would be infeasible to identify and
quantify reuse manually; therefore an automatic
method is required.
</bodyText>
<sectionHeader confidence="0.966966" genericHeader="method">
3 A conceptual framework
</sectionHeader>
<bodyText confidence="0.999985148936171">
To begin to get a handle on measuring text
reuse, we have developed a document-level clas-
sification scheme, indicating the level at which
a newspaper story as a whole is derived from
agency copy, and a lexical-level classification
scheme, indicating the level at which individ-
ual word sequences within a newspaper story
are derived from agency copy. This framework
rests upon the intuitions of trained journalists
to judge text reuse, and not on an explicit lex-
ical/syntactic definition of reuse (which would
presuppose what we are setting out to discover).
At the document level, newspaper stories
are assigned to one of three possible categories
coarsely reflecting the amount of text reused
from the news agency and the dependency of
the newspaper story upon news agency copy
for the provision of &amp;quot;facts&amp;quot;. The categories in-
dicate whether a trained journalist can iden-
tify text rewritten from the news agency in
a candidate derived newspaper article. They
are: 1) wholly-derived (WD): all text in
the newspaper article is rewritten only from
news agency copy; 2) partially-derived (PD):
some text is derived from the news agency, but
other sources have also been used; and 3) non-
derived (ND): news agency has not been used
as the source of the article; although words may
still co-occur between the newspaper article and
news agency copy on the same topic, the jour-
nalist is confident the news agency has not been
used.
At the lexical or word sequence level, individ-
ual words and phrases within a newspaper story
are classified as to whether they are used to ex-
press the same information as words in news
agency copy (i.e. paraphrases) and or used to
express information not found in agency copy.
Once again, three categories are used, based on
the judgement of a trained journalist: 1) verba-
tim: text appearing word-for-word to express
the same information; 2) rewrite: text para-
phrased to create a different surface appearance,
but express the same information and 3) new:
text used to express information not appearing
in agency copy (can include verbatim/rewritten
text, but being used in a different context).
</bodyText>
<subsectionHeader confidence="0.989802">
3.1 The METER corpus
</subsectionHeader>
<bodyText confidence="0.999897">
Based on this conceptual framework, we have
constructed a small annotated corpus of news
texts using the UK Press Association (PA) as
the news agency source and nine British daily
newspapers) who subscribe to the PA as candi-
date reusers. The METER corpus (Gaizauskas
et al., 2001) is a collection of 1716 texts (over
500,000 words) carefully selected from a 12
month period from the areas of law and court
reporting (769 stories) and showbusiness (175
stories). 772 of these texts are PA copy and 944
from the nine newspapers. These texts cover 265
different stories from July 1999 to June 2000 and
all newspaper stories have been manually classi-
fied at the document-level. They include 300
wholly-derived, 438 partially-derived and 206
non-derived (i.e. 77% are thought to have used
PA in some way). In addition, 355 have been
classified according to the lexical-level scheme.
</bodyText>
<sectionHeader confidence="0.786756" genericHeader="method">
4 Approaches to measuring text
similarity
</sectionHeader>
<bodyText confidence="0.999437117647059">
Many problems in computational text analy-
sis involve the measurement of similarity. For
example, the retrieval of documents to fulfil a
user information need, clustering documents ac-
cording to some criterion, multi-document sum-
marisation, aligning sentences from one lan-
guage with those in another, detecting exact and
near duplicates of documents, plagiarism detec-
tion, routing documents according to their style
and identifying authorship attribution. Meth-
ods typically vary depending upon the match-
ing method, e.g. exact or partial, the degree
to which natural language processing techniques
are used and the type of problem, e.g. search-
ing, clustering, aligning etc. We have not had
time to investigate all of these techniques, nor
is there space here to review them. We have
concentrated on just three: ngram overlap mea-
sures, Greedy String Tiling, and sentence align-
ment. The first was investigated because it of-
fers perhaps the simplest approach to the prob-
lem. The second was investigated because it has
been successfully used in plagiarism detection, a
problem which at least superficially is quite close
&apos;The newspapers include five popular papers (e.g. The
Sun, The Daily Mail, Daily Star, Daily Mirror) and four
quality papers (e.g. Daily Telegraph, The Guardian, The
Independent and The Times).
to the text reuse issues we are investigating. Fi-
nally, alignment (treating the derived text as a
&amp;quot;translation&amp;quot; of the first) seemed an intriguing
idea, and contrasts, certainly with the ngram ap-
proach, by focusing more on local, as opposed to
global measures of similarity.
</bodyText>
<subsectionHeader confidence="0.992161">
4.1 Ngram Overlap
</subsectionHeader>
<bodyText confidence="0.999850135135135">
An initial, straightforward approach to assessing
the reuse between two texts is to measure the
number of shared word ngrams. This method
underlies many of the approaches used in copy
detection including the approach taken by Lyon
et al. (2001).
They measure similarity using the set-
theoretic measures of containment and resem-
blance of shared trigrams to separate texts writ-
ten independently and those with sufficient sim-
ilarity to indicate some form of copying.
We treat each document as a set of overlap-
ping n-word sequences (initially considering only
n-word types) and compute a similarity score
from this. Given two sets of ngrams, we use
the set-theoretic containment score to measure
similarity between the documents for ngrams of
length 1 to 10 words. For a source text A and
a possibly derived text B represented by sets of
ngrams Sn(A) and Sn(B) respectively, the pro-
portion of ngrams in B also in A, the ngram con-
tainment Cn(A; B), is given by:
Informally containment measures the number
of matches between the elements of ngram sets
Sn(A) and Sn(B), scaled by the size of Sn(B).
In other words we measure the proportion of
unique n-grams in B that are found in A. The
score ranges from 0 to 1, indicating none to all
newspaper copy shared with PA respectively.
We also compare texts by counting only those
ngrams with low frequency, in particular those
occurring once. For 1-grams, this is the same as
comparing the hapax legomena which has been
shown to discriminate plagiarised texts from
those written independently even when lexical
overlap between the texts is already high (e.g.
70%) (Finlay, 1999). Unlike Finlay&apos;s work, we
</bodyText>
<equation confidence="0.998568333333333">
Cn(A; B) = I Sn(A) n Sn(B)
(1)
I Sn(B) I
</equation>
<bodyText confidence="0.99995">
find that repetition in PA copy drastically re-
duces the number of shared hapax legomena
thereby inhibiting classification of derived and
non-derived texts. Therefore we compute the
containment of hapax legomena (hapax contain-
ment) by comparing words occurring once in the
newspaper, i.e. those 1-grams in St(B) that oc-
cur once with all 1-grams in PA copy, St(A).
This containment score represents the number
of newspaper hapax legomena also appearing at
least once in PA copy.
</bodyText>
<subsectionHeader confidence="0.9859">
4.2 Greedy String-Tiling
</subsectionHeader>
<bodyText confidence="0.999702625">
Greedy String-Tiling (GST) is a substring
matching algorithm which computes the degree
of similarity between two strings, for exam-
ple software code, free text or biological subse-
quences (Wise, 1996). Compared with previous
algorithms for computing string similarity, such
as the Longest Common Subsequence or
Levenshtein distance, GST is able to deal with
transposition of tokens (in earlier approaches
transposition is seen as a number of single inser-
tions/deletions rather than a single block move).
The GST algorithm performs a 1:1 matching
of tokens between two strings so that as much of
one token stream is covered with maximal length
substrings from the other (called tiles). In our
problem, we consider how much newspaper text
can be maximally covered by words from PA
copy. A minimum match length (MML) can be
used to avoid spurious matches (e.g. of 1 or
2 tokens) and the resulting similarity between
the strings can be expressed as a quantitative
similarity match or a qualitative list of common
substrings. Figure 1 shows the result of GST for
the example in Section 2.
</bodyText>
<figureCaption confidence="0.99845">
Figure 1: Example GST results (MML=3)
</figureCaption>
<bodyText confidence="0.955042333333333">
2As stories unfold, PA release copy with new, as well
as previous versions of the story
Given PA copy A, a newspaper text B and a
set of maximal matches, tiles, of a given length
between A and B, the similarity, gstsim(A,B),
is expressed as:
</bodyText>
<subsectionHeader confidence="0.996751">
4.3 Sentence alignment
</subsectionHeader>
<bodyText confidence="0.99986241025641">
In the past decade, various alignment algorithms
have been suggested for aligning multilingual
parallel corpora (Wu, 2000). These algorithms
have been used to map translation equivalents
across different languages. In this specific case,
we investigate whether alignment can map de-
rived texts (or parts of them) to their source
texts. PA copy may be subject to various
changes during text reuse, e.g. a single sen-
tence may derive from parts of several source
sentences. Therefore, strong correlations of sen-
tence length between the derived and source
sentences cannot be guaranteed. As a result,
sentence-length based statistical alignment al-
gorithms (Brown et al., 1991; Gale and Church,
1993) are not appropriate for this case. On the
other hand, cognate-based algorithms (Simard
et al., 1992; Melamed, 1999) are more efficient
for coping with change of text format. There-
fore, a cognate-based approach is adopted for
the METER task. Here cognates are defined as
pairs of terms that are identical, share the same
stems, or are substitutable in the given context.
The algorithm consists of two principal com-
ponents: a comparison strategy and a scoring
function. In brief, the comparison works as fol-
lows (more details may be found in Piao (2001)).
For each sentence in the candidate derived text
DT the sentences in the candidate source text
ST are compared in order to find the best match.
A DT sentence is allowed to match up to three
possibly non-consecutive ST sentences. The
candidate pair with the highest score (see be-
low) above a threshold is accepted as a true
alignment. If no such candidate is found, the
DT sentence is assumed to be independent of
the ST. Based on individual DT sentence align-
ments, the overall possibility of derivation for
the DT is estimated with a score ranging be-
</bodyText>
<equation confidence="0.92145775">
�i�tiles lengthi
gstsim(A� B) =
(2)
� B �
</equation>
<bodyText confidence="0.999788318181818">
tween 0 and 1. This score reflects the propor-
tion of aligned sentences in the newspaper text.
Note that not only may multiple sentences in
the ST be aligned with a single sentence in the
DT, but also multiple sentences in the DT may
be aligned with one sentence in the ST.
Given a candidate derived sentence DS and
a proposed (set of) source sentence(s) SS, the
scoring function works as follows. Three basic
measures are computed for each pair of candi-
date DS and SS: SNG is the sum of lengths
of the maximum length non-overlapping shared
n-grams with n &gt; 2; SWD is the number of
matched words sharing stems not in an n-gram
figuring in SNG; and SUB is the number of
substitutable terms (mainly synonyms) not fig-
uring in SNG or SWD. Let L1 be the length of
the candidate DS and L2 the length of candidate
SS. Then, three scores PD, PS (Dice score) and
PVS are calculated as follows:
These three scores reflect different aspects of
relations between the candidate DS and SS:
</bodyText>
<listItem confidence="0.999908428571429">
1. PSD: The proportion of the DS which is
shared material.
2. PS: The proportion of shared terms in DS
and SS. This measure prefers SS&apos;s which not
only contain many terms in the DS, but also
do not contain many additional terms.
3. PSNG: The proportion of matching n-
</listItem>
<bodyText confidence="0.880290285714286">
grams amongst the shared terms. This
measure captures the intuition that sen-
tences sharing not only words, but word se-
quences are more likely to be related.
These three scores are weighted and combined
together to provide an alignment metric WS
(weighted score), which is calculated as follows:
</bodyText>
<equation confidence="0.98657">
WS = 61PSD + 62PS + 63PSNG
</equation>
<bodyText confidence="0.99992775">
where 61 +62 +63 = 1. The three weighting vari-
ables 6e(i = 1, 2, 3) have been determined empir-
ically and are currently set to: 61 = 0.85, 62 =
0.05, 63 = 0.1.
</bodyText>
<sectionHeader confidence="0.988356" genericHeader="method">
5 Reuse Classifiers
</sectionHeader>
<bodyText confidence="0.999972666666667">
To evaluate the previous approaches for measur-
ing text reuse at the document-level, we cast the
problem into one of a supervised learning task.
</bodyText>
<subsectionHeader confidence="0.961332">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999969324324324">
We used similarity scores as attributes for a ma-
chine learning algorithm and used the Weka 3.2
software (Witten and Frank, 2000). Because of
the small number of examples, we used tenfold
cross-validation repeated 10 times (i.e. 10 runs)
and combined this with stratification to ensure
approximately the same proportion of samples
from each class were used in each fold of the
cross-validation. All 769 newspaper texts from
the courts domain were used for evaluation and
randomly permuted to generate 10 sets. For
each newspaper text, we compared PA source
texts from the same story to create results in
the form: newspaper, class, score. These results
were ordered according to each set to create the
same 10 datasets for each approach thereby en-
abling comparison.
Using this data we first trained five single-
feature Naive Bayes classifiers to do the ternary
classification task. The feature in each case was
a variant of one of the three similarity measures
described in Section 4, computed between the
two texts in the training set. The target classifi-
cation value was the reuse classification category
from the corpus. A Naive Bayes classifier was
used because of its success in previous classifi-
cation tasks, however we are aware of its naive
assumptions that attributes are assumed inde-
pendent and data to be normally distributed.
We evaluated results using the F1-measure
(harmonic mean of precision and recall given
equal weighting). For each run, we calculated
the average F1 score across the classes. The
overall average F1-measure scores were com-
puted from the 10 runs for each class (a single
accuracy measure would suffice but the Weka
package outputs F1-measures). For the 10 runs,
</bodyText>
<equation confidence="0.999589111111111">
L1 + L2
SNG
PSNG =
SWD + SNG + SUB
SWD + SNG + SUB
PSD =
L1
2(SWD + SNG + SUB)
PS =
</equation>
<bodyText confidence="0.999993966666667">
the standard deviation of F1 scores was com-
puted for each class and F1 scores between all
approaches were tested for statistical signifi-
cance using 1-way analysis of variance at a 99%
confidence-level. Statistical differences between
results were identified using Bonferroni analy-
sis3.
After examining the results of these single fea-
ture classifiers, we also trained a &amp;quot;combined&amp;quot;
classifier using a correlation-based filter ap-
proach (Hall and Smith, 1999) to select the com-
bination of features giving the highest classifica-
tion score ( correlation-based filtering evaluates
all possible combinations of features). Feature
selection was carried for each fold during cross-
validation and features used in all 10 folds were
chosen as candidates. Those which occurred in
at least 5 of the 10 runs formed the final selec-
tion.
We also tried splitting the training data into
various binary partitions (e.g. WD/PD vs. ND)
and training binary classifiers, using feature se-
lection, to see how well binary classification
could be performed. Eskin and Bogosian (1998)
have observed that using cascaded binary clas-
sifiers, each of which splits the data well, may
work better on n-ary classification problems
than a single n-way classifier. We then com-
puted how well such a cascaded classifier should
perform using the best binary classifier results.
</bodyText>
<subsectionHeader confidence="0.88422">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999472428571429">
Table 1 shows the results of the single ternary
classifiers. The baseline F1 measure is based
upon the prior probability of a document falling
into one of the classes. The figures in parenthe-
sis are the standard deviations for the F1 scores
across the ten evaluation runs. The final row
shows the results for combining features selected
using the correlation-based filter.
Table 2 shows the result of training binary
classifiers using feature selection to select the
most discriminating features for various binary
splits of the training data.
For both ternary and binary classifiers feature
selection produced better results than using all
</bodyText>
<table confidence="0.991585411764706">
3Using SPSS v10.0 for Windows.
Approach Category Avg F-measure
Baseline WD 0.340 (0.000)
PD 0.444 (0.000)
ND 0.216 (0.000)
total 0.333 (0.000)
3-gram WD 0.631 (0.004)
containment PD 0.624 (0.004)
ND 0.549 (0.005)
total 0.601 (0.003)
GST Sim WD 0.669 (0.004)
MML = 3 PD 0.633 (0.003)
ND 0.556 (0.004)
total 0.620 (0.002)
GST Sim WD 0.681 (0.003)
MML = 1 PD 0.634 (0.003)
ND 0.559 (0.008)
total 0.625 (0.004)
1-gram WD 0.718 (0.003)
containment PD 0.643 (0.003)
ND 0.551 (0.006)
total 0.638 (0.003)
Alignment WD 0.774 (0.003)
PD 0.624 (0.005)
ND 0.537 (0.007)
total 0.645 (0.004)
hapax WD 0.736 (0.003)
containment PD 0.654 (0.003)
ND 0.549 (0.010)
total 0.646 (0.004)
hapax cont. WD 0.756 (0.002)
1-gram cont. PD 0.599 (0.006)
alignment ND 0.629 (0.008)
(&amp;quot;combined&amp;quot;) total 0.664 (0.004)
</table>
<tableCaption confidence="0.999788">
Table 1: A summary of classification results
</tableCaption>
<bodyText confidence="0.986712">
possible features, with the one exception of the
binary classification between PD and ND.
</bodyText>
<subsectionHeader confidence="0.973874">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.99986695">
From Table 1, we find that all classifier results
are significantly higher than the baseline (at
p &lt; 0.01) and all differences are significant ex-
cept between hapax containment and alignment.
The highest F-measure for the 3-class problem
is 0.664 for the &amp;quot;combined&amp;quot; classifier, which is
significantly greater than 0.651 obtained with-
out. We notice that highest WD classification
is with alignment at 0.774, highest PD classi-
fication is 0.654 with hapax containment and
highest ND classification is 0.629 with combined
features. Using hapax containment gives higher
results than 1-gram containment alone and in
fact provides results as good as or better than
the more complex sentence alignment and GST
approaches.
Previous research by (Lyon et al., 2001) and
(Wise, 1996) had shown derived texts could be
distinguished using trigram overlap and tiling
with a match length of 3 or more, respectively.
</bodyText>
<table confidence="0.99950065">
Attributes Category Avg Fl
Correlation- alignment WD 0.942 (0.008)
based ND 0.909 (0.011)
filter total 0.926 (0.010)
alignment PD/ND 0.870 (0.003)
WD 0.770 (0.003)
total 0.820 (0.002)
alignment WD 0.778 (0.003)
PD 0.812 (0.002)
total 0.789 (0.002)
hapax cont. WD/PD 0.882 (0.002)
alignment ND 0.649 (0.007)
1-gram cont. total 0.763 (0.002)
1-gram PD 0.802 (0.002)
GST mml 3 ND 0.638 (0.007)
GST mml 1 total 0.720 (0.004)
alignment
GST mml 1 WD/ND 0.672 (0.002)
alignment PD 0.662 (0.003)
total 0.668 (0.003)
</table>
<tableCaption confidence="0.998098">
Table 2: Binary Classifiers with feature selection
</tableCaption>
<bodyText confidence="0.999917818181818">
However, our results run counter to this be-
cause the highest classification scores are ob-
tained with 1-grams and an MML of 1, i.e. as
n or MML length increases, the F1 scores de-
crease. We believe this results from two factors
which are characteristic of reuse in journalism.
First, since even ND texts are thematically sim-
ilar (same events being described) there is high
likelihood of coincidental overlap of ngrams of
length 3 or more (e.g. quoted speech). Secondly,
when journalists rewrite it is rare for them not
to vary the source.
For the intended application — helping the PA
to monitor text reuse — the cost of different mis-
classifications is not equal. If the classifier makes
a mistake, it is better that WD and ND texts are
mis-classified as PD, and PD as WD. Given the
difference in distribution of documents across
classes where PD contains the most documents,
the classifier will be biased towards this class
anyway as required. Table 3 shows the confu-
sion matrix for the combined ternary classifier.
</bodyText>
<table confidence="0.989124">
WD PD ND
WD 203 55 4
PD 79 192 70
ND 3 53 109
</table>
<tableCaption confidence="0.823533">
Table 3: Confusion matrix for combined ternary
classifier
</tableCaption>
<bodyText confidence="0.999827173913044">
Although the overall F1-measure score is low
(0.664), mis-classification of both WD as ND
and ND as WD is also very low, as most mis-
classifications are as PD. Note the high mis-
classification of PD as both WD and ND, re-
flecting the difficulty of separating this class.
From Table 2, we find alignment is a selected
feature for each binary partition of the data.
The highest binary classification is achieved be-
tween the WD and ND classes using alignment
only, and the highest three scores show WD is
the easiest class to separate from the others.
The PD class is the hardest to isolate, reflect-
ing the mis-classifications seen in Table 3.
To predict how well a cascaded binary classi-
fier will perform we can reason as follows. From
the preceding discussion we see that WD can
be separated most accurately; hence we choose
WD versus PD/ND as the first binary classifier.
This forces the second classifier to be PD versus
ND. From the results in Table 2 and the follow-
ing equation to compute the F1 measure for a
two-stage binary classifier
</bodyText>
<equation confidence="0.928201666666667">
WD + (PDIND)(PD+wD )
2
2
</equation>
<bodyText confidence="0.999709666666667">
we obtain an overall F1 measure for ternary clas-
sication of 0.703, which is significantly higher
than the best single stage ternary classifier.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="method">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999859325581396">
In this paper we have investigated text reuse in
the context of the reuse of news agency copy, an
area of theoretical and practical interest. We
present a conceptual framework in which we
measure reuse and based on which the METER
corpus has been constructed. We have presented
the results of using similarity scores, computed
using n-gram containment, Greedy String Tiling
and an alignment algorithm, as attributes for
a supervised learning algorithm faced with the
task of learning how to classify newspaper sto-
ries as to whether they are wholly, partially or
non-derived from a news agency source. We
show that the best single feature ternary clas-
sifier uses either alignment or simple hapax con-
tainment measures and that a cascaded binary
classifier using a combination of features can
outperform this.
The results are lower than one might like,
and reflect the problems of measuring journalis-
tic reuse, stemming from complex editing trans-
formations and the high amount of verbatim
text overlapping as a result of thematic simi-
larity and &amp;quot;expected&amp;quot; similarity due to, e.g., di-
rect/indirect quotes. Given the relative close-
ness of results obtained by all approaches we
have considered, we speculate that any compar-
ison method based upon lexical similarity will
probably not improve classification results by
much. Perhaps improved performance at this
task may possible by using more advanced nat-
ural language processing techniques, e.g. better
modeling of the lexical variation and syntactic
transformation that goes on in journalistic reuse.
Nevertheless the results we have obtained are
strong enough in some cases (e.g. wholly derived
texts can be identified with &gt; 80% accuracy) to
begin to be exploited.
In summary measuring text reuse is an excit-
ing new area that will have a number of appli-
cations, in particular, but not limited to, mon-
itoring and controlling the copy produced by a
newswire.
</bodyText>
<sectionHeader confidence="0.997992" genericHeader="discussions">
7 Future work
</sectionHeader>
<bodyText confidence="0.999907538461539">
We are adapting the GST algorithm to deal with
simple rewrites (e.g. synonym substitution) and
to observe the effects of rewriting upon finding
longest common substrings. We are also experi-
menting using the more detailed METER corpus
lexical-level annotations to investigate how well
the GST and ngrams approaches can identify
reuse at this level.
A prototype browser-based demo of both the
GST algorithm and alignment program, allow-
ing users to test arbitrary text pairs for simi-
larity, is now available and will continue to be
enhanced.
</bodyText>
<sectionHeader confidence="0.992983" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9993942">
The authors would like to acknowledge the
UK Engineering and Physical Sciences Re-
search Council for funding the METER project
(GR/M34041). Thanks also to Mark Hepple for
helpful comments on earlier drafts.
</bodyText>
<footnote confidence="0.987831">
4See http://www.dcs.shef.ac.uk/nlp/meter.
</footnote>
<sectionHeader confidence="0.927048" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999945245614035">
P.F. Brown, J.C. Lai, and R.L. Mercer. 1991. Aligning
sentences in parallel corpora. In Proceedings of the
29th Annual Meeting of the Assoc. for Computational
Linguistics, pages 169{176, Berkeley, CA, USA.
P Clough. 2000. Plagiarism in natural and programming
languages: An overview of current tools and technolo-
gies. Technical Report CS-00-05, Dept. of Computer
Science, University of Sheffield, UK.
E. Eskin and M. Bogosian. 1998. Classifying text docu-
ments using modular categories and linguistically mo-
tivated indicators. In AAAI-98 Workshop on Learning
for Text Classification.
H. Evans. 1972. Essential English for Journalists, Edi-
tors and Writers. Pimlico, London.
S. Finlay. 1999. Copycatch. Master&apos;s thesis, Dept. of
English. University of Birmingham.
R. Gaizauskas, J. Foster, Y. Wilks, J. Arundel,
P. Clough, and S. Piao. 2001. The meter corpus:
A corpus for analysing journalistic text reuse. In Pro-
ceedings of the Corpus Linguistics 2001 Conference,
pages 214|223.
W.A. Gale and K.W. Church. 1993. A program for align-
ing sentences in bilingual corpus. Computational Lin-
guistics, 19:75{102.
M.A. Hall and L.A. Smith. 1999. Feature selection for
machine learning: Comparing a correlation-based fil-
ter approach to the wrapper. In Proceedings of the
Florida Artificial Intelligence Symposium (FLAIRS-
99), pages 235{239.
C. Lyon, J. Malcolm, and B. Dickerson. 2001. Detecting
short passages of similar text in large document collec-
tions. In Conference on Empirical Methods in Natural
Language Processing (EMNLP2001), pages 118{125.
K. McKeown and H. Jing. 1999. The decomposition of
human-written summary sentences. In SIGIR 1999,
pages 129{136.
I. Dan Melamed. 1999. Bitext maps and alignment via
pattern recognition. Computational Linguistics, pages
107{130.
Scott S.L. Piao. 2001. Detecting and measuring text
reuse via aligning texts. Research Memorandum
CS-01-15, Dept. of Computer Science, University of
Sheffield.
M. Simard, G. Foster, and P. Isabelle. 1992. Using
cognates to align sentences in bilingual corpora. In
Proceedings of the 4th Int. Conf. on Theoretical and
Methodological Issues in Machine Translation, pages
67{81, Montreal, Canada.
M. Wise. 1996. Yap3: Improved detection of similarities
in computer programs and other texts. In Proceedings
of SIGCSE&apos;96, pages 130{134, Philadelphia, USA.
I.H. Witten and E. Frank. 2000. Datamining - practi-
cal machine learning tools and techniques with Java
implementations. Morgan Kaufmann.
D. Wu. 2000. Alignment. In R. Dale and H. Moisl and
H. Somers (eds.), A Handbook of Natural Language
Processing, pages 415{458. New York: Marcel Dekker.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.459143">
<title confidence="0.999082">METER: MEasuring TExt Reuse</title>
<author confidence="0.999748">Paul Clough</author>
<author confidence="0.999748">Robert Gaizauskas</author>
<author confidence="0.999748">Scott S L Piao</author>
<author confidence="0.999748">Yorick Wilks</author>
<affiliation confidence="0.9999045">Department of Computer Science University of Sheffield</affiliation>
<address confidence="0.7983365">Regent Court, 211 Portobello Street, Sheffield, England, S1 4DP</address>
<abstract confidence="0.991636464285714">In this paper we present results from the METER (MEasuring TExt Reuse) project whose aim is to explore issues pertaining to text reuse and derivation, especially in the context of newspapers using newswire sources. Although the reuse of text by journalists has been studied in linguistics, we are not aware of any investigation using existing computational methods for this particular task. We investigate the classification of newspaper articles according to their degree of dependence upon, or derivation from, a newswire source using a simple 3-level scheme designed by journalists. Three approaches to measuring text similarity are considered: ngram overlap, Greedy String Tiling, and sentence alignment. Measured against a manually annotated corpus of source and derived news text, we show that a combined classifier with features automatically selected performs best overall for the ternary classificaachieving an average score of 0.664 across all three categories.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Aligning sentences in parallel corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>169--176</pages>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="13778" citStr="Brown et al., 1991" startWordPosition="2234" endWordPosition="2237">e been suggested for aligning multilingual parallel corpora (Wu, 2000). These algorithms have been used to map translation equivalents across different languages. In this specific case, we investigate whether alignment can map derived texts (or parts of them) to their source texts. PA copy may be subject to various changes during text reuse, e.g. a single sentence may derive from parts of several source sentences. Therefore, strong correlations of sentence length between the derived and source sentences cannot be guaranteed. As a result, sentence-length based statistical alignment algorithms (Brown et al., 1991; Gale and Church, 1993) are not appropriate for this case. On the other hand, cognate-based algorithms (Simard et al., 1992; Melamed, 1999) are more efficient for coping with change of text format. Therefore, a cognate-based approach is adopted for the METER task. Here cognates are defined as pairs of terms that are identical, share the same stems, or are substitutable in the given context. The algorithm consists of two principal components: a comparison strategy and a scoring function. In brief, the comparison works as follows (more details may be found in Piao (2001)). For each sentence in </context>
</contexts>
<marker>Brown, Lai, Mercer, 1991</marker>
<rawString>P.F. Brown, J.C. Lai, and R.L. Mercer. 1991. Aligning sentences in parallel corpora. In Proceedings of the 29th Annual Meeting of the Assoc. for Computational Linguistics, pages 169{176, Berkeley, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clough</author>
</authors>
<title>Plagiarism in natural and programming languages: An overview of current tools and technologies.</title>
<date>2000</date>
<tech>Technical Report CS-00-05,</tech>
<institution>Dept. of Computer Science, University of Sheffield, UK.</institution>
<contexts>
<context position="1894" citStr="Clough, 2000" startWordPosition="289" endWordPosition="290">rable theoretical and practical interest is that of text reuse: the reuse of existing written sources in the creation of a new text. Of course, reusing language is as old as the retelling of stories, but current technologies for creating, copying and disseminating electronic text, make it easier than ever before to take some or all of any number of existing text sources and reuse them verbatim or with varying degrees of modification. One form of unacceptable text reuse, plagiarism, has received considerable attention and software for automatic plagiarism detection is now available (see, e.g. (Clough, 2000) for a recent review). But in this paper we present a benign and acceptable form of text reuse that is encountered virtually every day: the reuse of news agency text (called copy) in the production of daily newspapers. The question is not just whether agency copy has been reused, but to what extent and subject to what transformations. Using existing approaches from computational text analysis, we investigate their ability to classify newspapers articles into categories indicating their dependency on agency copy. 2 Journalistic reuse of a newswire The process of gathering, editing and publishin</context>
</contexts>
<marker>Clough, 2000</marker>
<rawString>P Clough. 2000. Plagiarism in natural and programming languages: An overview of current tools and technologies. Technical Report CS-00-05, Dept. of Computer Science, University of Sheffield, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Eskin</author>
<author>M Bogosian</author>
</authors>
<title>Classifying text documents using modular categories and linguistically motivated indicators.</title>
<date>1998</date>
<booktitle>In AAAI-98 Workshop on Learning for Text Classification.</booktitle>
<contexts>
<context position="19690" citStr="Eskin and Bogosian (1998)" startWordPosition="3253" endWordPosition="3256">ach (Hall and Smith, 1999) to select the combination of features giving the highest classification score ( correlation-based filtering evaluates all possible combinations of features). Feature selection was carried for each fold during crossvalidation and features used in all 10 folds were chosen as candidates. Those which occurred in at least 5 of the 10 runs formed the final selection. We also tried splitting the training data into various binary partitions (e.g. WD/PD vs. ND) and training binary classifiers, using feature selection, to see how well binary classification could be performed. Eskin and Bogosian (1998) have observed that using cascaded binary classifiers, each of which splits the data well, may work better on n-ary classification problems than a single n-way classifier. We then computed how well such a cascaded classifier should perform using the best binary classifier results. 5.2 Results Table 1 shows the results of the single ternary classifiers. The baseline F1 measure is based upon the prior probability of a document falling into one of the classes. The figures in parenthesis are the standard deviations for the F1 scores across the ten evaluation runs. The final row shows the results f</context>
</contexts>
<marker>Eskin, Bogosian, 1998</marker>
<rawString>E. Eskin and M. Bogosian. 1998. Classifying text documents using modular categories and linguistically motivated indicators. In AAAI-98 Workshop on Learning for Text Classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Evans</author>
</authors>
<title>Essential English for Journalists, Editors and Writers.</title>
<date>1972</date>
<location>Pimlico, London.</location>
<contexts>
<context position="2688" citStr="Evans (1972)" startWordPosition="418" endWordPosition="419"> the production of daily newspapers. The question is not just whether agency copy has been reused, but to what extent and subject to what transformations. Using existing approaches from computational text analysis, we investigate their ability to classify newspapers articles into categories indicating their dependency on agency copy. 2 Journalistic reuse of a newswire The process of gathering, editing and publishing newspaper stories is a complex and specialised task often operating within specific publishing constraints such as: 1) short deadlines; 2) prescriptive writing practice (see, e.g. Evans (1972)); 3) limits of physical size; 4) readability and audience comprehension, e.g. a tabloid&apos;s vocabulary limitations; 5) journalistic bias, e.g. political and 6) a newspaper&apos;s house style. Often newsworkers, such as the reporter and editor, will rely upon news agency copy as the basis of a news story or to verify facts and assess the Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 152-159. importance of a story in the context of all those appearing on the newswire. Because of the nature of journalistic text reuse, differe</context>
</contexts>
<marker>Evans, 1972</marker>
<rawString>H. Evans. 1972. Essential English for Journalists, Editors and Writers. Pimlico, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Finlay</author>
</authors>
<date>1999</date>
<tech>Copycatch. Master&apos;s thesis,</tech>
<institution>Dept. of English. University of Birmingham.</institution>
<contexts>
<context position="11153" citStr="Finlay, 1999" startWordPosition="1808" endWordPosition="1809">atches between the elements of ngram sets Sn(A) and Sn(B), scaled by the size of Sn(B). In other words we measure the proportion of unique n-grams in B that are found in A. The score ranges from 0 to 1, indicating none to all newspaper copy shared with PA respectively. We also compare texts by counting only those ngrams with low frequency, in particular those occurring once. For 1-grams, this is the same as comparing the hapax legomena which has been shown to discriminate plagiarised texts from those written independently even when lexical overlap between the texts is already high (e.g. 70%) (Finlay, 1999). Unlike Finlay&apos;s work, we Cn(A; B) = I Sn(A) n Sn(B) (1) I Sn(B) I find that repetition in PA copy drastically reduces the number of shared hapax legomena thereby inhibiting classification of derived and non-derived texts. Therefore we compute the containment of hapax legomena (hapax containment) by comparing words occurring once in the newspaper, i.e. those 1-grams in St(B) that occur once with all 1-grams in PA copy, St(A). This containment score represents the number of newspaper hapax legomena also appearing at least once in PA copy. 4.2 Greedy String-Tiling Greedy String-Tiling (GST) is </context>
</contexts>
<marker>Finlay, 1999</marker>
<rawString>S. Finlay. 1999. Copycatch. Master&apos;s thesis, Dept. of English. University of Birmingham.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gaizauskas</author>
<author>J Foster</author>
<author>Y Wilks</author>
<author>J Arundel</author>
<author>P Clough</author>
<author>S Piao</author>
</authors>
<title>The meter corpus: A corpus for analysing journalistic text reuse.</title>
<date>2001</date>
<booktitle>In Proceedings of the Corpus Linguistics 2001 Conference,</booktitle>
<pages>214--223</pages>
<contexts>
<context position="7296" citStr="Gaizauskas et al., 2001" startWordPosition="1179" endWordPosition="1182">tim: text appearing word-for-word to express the same information; 2) rewrite: text paraphrased to create a different surface appearance, but express the same information and 3) new: text used to express information not appearing in agency copy (can include verbatim/rewritten text, but being used in a different context). 3.1 The METER corpus Based on this conceptual framework, we have constructed a small annotated corpus of news texts using the UK Press Association (PA) as the news agency source and nine British daily newspapers) who subscribe to the PA as candidate reusers. The METER corpus (Gaizauskas et al., 2001) is a collection of 1716 texts (over 500,000 words) carefully selected from a 12 month period from the areas of law and court reporting (769 stories) and showbusiness (175 stories). 772 of these texts are PA copy and 944 from the nine newspapers. These texts cover 265 different stories from July 1999 to June 2000 and all newspaper stories have been manually classified at the document-level. They include 300 wholly-derived, 438 partially-derived and 206 non-derived (i.e. 77% are thought to have used PA in some way). In addition, 355 have been classified according to the lexical-level scheme. 4 </context>
</contexts>
<marker>Gaizauskas, Foster, Wilks, Arundel, Clough, Piao, 2001</marker>
<rawString>R. Gaizauskas, J. Foster, Y. Wilks, J. Arundel, P. Clough, and S. Piao. 2001. The meter corpus: A corpus for analysing journalistic text reuse. In Proceedings of the Corpus Linguistics 2001 Conference, pages 214|223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpus.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--75</pages>
<contexts>
<context position="13802" citStr="Gale and Church, 1993" startWordPosition="2238" endWordPosition="2241"> aligning multilingual parallel corpora (Wu, 2000). These algorithms have been used to map translation equivalents across different languages. In this specific case, we investigate whether alignment can map derived texts (or parts of them) to their source texts. PA copy may be subject to various changes during text reuse, e.g. a single sentence may derive from parts of several source sentences. Therefore, strong correlations of sentence length between the derived and source sentences cannot be guaranteed. As a result, sentence-length based statistical alignment algorithms (Brown et al., 1991; Gale and Church, 1993) are not appropriate for this case. On the other hand, cognate-based algorithms (Simard et al., 1992; Melamed, 1999) are more efficient for coping with change of text format. Therefore, a cognate-based approach is adopted for the METER task. Here cognates are defined as pairs of terms that are identical, share the same stems, or are substitutable in the given context. The algorithm consists of two principal components: a comparison strategy and a scoring function. In brief, the comparison works as follows (more details may be found in Piao (2001)). For each sentence in the candidate derived te</context>
</contexts>
<marker>Gale, Church, 1993</marker>
<rawString>W.A. Gale and K.W. Church. 1993. A program for aligning sentences in bilingual corpus. Computational Linguistics, 19:75{102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hall</author>
<author>L A Smith</author>
</authors>
<title>Feature selection for machine learning: Comparing a correlation-based filter approach to the wrapper.</title>
<date>1999</date>
<booktitle>In Proceedings of the Florida Artificial Intelligence Symposium (FLAIRS99),</booktitle>
<pages>235--239</pages>
<contexts>
<context position="19091" citStr="Hall and Smith, 1999" startWordPosition="3158" endWordPosition="3161">measure would suffice but the Weka package outputs F1-measures). For the 10 runs, L1 + L2 SNG PSNG = SWD + SNG + SUB SWD + SNG + SUB PSD = L1 2(SWD + SNG + SUB) PS = the standard deviation of F1 scores was computed for each class and F1 scores between all approaches were tested for statistical significance using 1-way analysis of variance at a 99% confidence-level. Statistical differences between results were identified using Bonferroni analysis3. After examining the results of these single feature classifiers, we also trained a &amp;quot;combined&amp;quot; classifier using a correlation-based filter approach (Hall and Smith, 1999) to select the combination of features giving the highest classification score ( correlation-based filtering evaluates all possible combinations of features). Feature selection was carried for each fold during crossvalidation and features used in all 10 folds were chosen as candidates. Those which occurred in at least 5 of the 10 runs formed the final selection. We also tried splitting the training data into various binary partitions (e.g. WD/PD vs. ND) and training binary classifiers, using feature selection, to see how well binary classification could be performed. Eskin and Bogosian (1998) </context>
</contexts>
<marker>Hall, Smith, 1999</marker>
<rawString>M.A. Hall and L.A. Smith. 1999. Feature selection for machine learning: Comparing a correlation-based filter approach to the wrapper. In Proceedings of the Florida Artificial Intelligence Symposium (FLAIRS99), pages 235{239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lyon</author>
<author>J Malcolm</author>
<author>B Dickerson</author>
</authors>
<title>Detecting short passages of similar text in large document collections.</title>
<date>2001</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP2001),</booktitle>
<pages>118--125</pages>
<contexts>
<context position="9781" citStr="Lyon et al. (2001)" startWordPosition="1574" endWordPosition="1577">lity papers (e.g. Daily Telegraph, The Guardian, The Independent and The Times). to the text reuse issues we are investigating. Finally, alignment (treating the derived text as a &amp;quot;translation&amp;quot; of the first) seemed an intriguing idea, and contrasts, certainly with the ngram approach, by focusing more on local, as opposed to global measures of similarity. 4.1 Ngram Overlap An initial, straightforward approach to assessing the reuse between two texts is to measure the number of shared word ngrams. This method underlies many of the approaches used in copy detection including the approach taken by Lyon et al. (2001). They measure similarity using the settheoretic measures of containment and resemblance of shared trigrams to separate texts written independently and those with sufficient similarity to indicate some form of copying. We treat each document as a set of overlapping n-word sequences (initially considering only n-word types) and compute a similarity score from this. Given two sets of ngrams, we use the set-theoretic containment score to measure similarity between the documents for ngrams of length 1 to 10 words. For a source text A and a possibly derived text B represented by sets of ngrams Sn(A</context>
<context position="22314" citStr="Lyon et al., 2001" startWordPosition="3671" endWordPosition="3674">ificant except between hapax containment and alignment. The highest F-measure for the 3-class problem is 0.664 for the &amp;quot;combined&amp;quot; classifier, which is significantly greater than 0.651 obtained without. We notice that highest WD classification is with alignment at 0.774, highest PD classification is 0.654 with hapax containment and highest ND classification is 0.629 with combined features. Using hapax containment gives higher results than 1-gram containment alone and in fact provides results as good as or better than the more complex sentence alignment and GST approaches. Previous research by (Lyon et al., 2001) and (Wise, 1996) had shown derived texts could be distinguished using trigram overlap and tiling with a match length of 3 or more, respectively. Attributes Category Avg Fl Correlation- alignment WD 0.942 (0.008) based ND 0.909 (0.011) filter total 0.926 (0.010) alignment PD/ND 0.870 (0.003) WD 0.770 (0.003) total 0.820 (0.002) alignment WD 0.778 (0.003) PD 0.812 (0.002) total 0.789 (0.002) hapax cont. WD/PD 0.882 (0.002) alignment ND 0.649 (0.007) 1-gram cont. total 0.763 (0.002) 1-gram PD 0.802 (0.002) GST mml 3 ND 0.638 (0.007) GST mml 1 total 0.720 (0.004) alignment GST mml 1 WD/ND 0.672 (</context>
</contexts>
<marker>Lyon, Malcolm, Dickerson, 2001</marker>
<rawString>C. Lyon, J. Malcolm, and B. Dickerson. 2001. Detecting short passages of similar text in large document collections. In Conference on Empirical Methods in Natural Language Processing (EMNLP2001), pages 118{125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>H Jing</author>
</authors>
<title>The decomposition of human-written summary sentences.</title>
<date>1999</date>
<booktitle>In SIGIR</booktitle>
<pages>129--136</pages>
<contexts>
<context position="4178" citStr="McKeown and Jing (1999)" startWordPosition="663" endWordPosition="666"> DRUNK driver who ploughed into the Queen Mother&apos;s limo was fined $700 and banned for two years yesterday. This simple example illustrates the types of rewrite that can occur even in a very short sentence. The rewrite makes use of slang and exaggeration to capture its readers&apos; attention (e.g. DRUNK, limo, ploughed). Deletion (e.g. from driving) has also been used and the addition of yesterday indicates when the event occurred. Many of the transformations we observed between moving from news agency copy to the newspaper version have also been reported by the summarisation community (see, e.g., McKeown and Jing (1999)). Given the value of the information news agencies supply, the ease with which text can be reused and commercial pressures, it would be beneficial to be able to identify those news stories appearing in the newspapers that have relied upon agency copy in their production. Potential uses include: 1) monitoring take-up of agency copy; 2) identifying the most reused stories ; 3) determining customer dependency upon agency copy and 4) new methods for charging customers based upon the amount of copy reused. Given the large volume of news agency copy output each day, it would be infeasible to identi</context>
</contexts>
<marker>McKeown, Jing, 1999</marker>
<rawString>K. McKeown and H. Jing. 1999. The decomposition of human-written summary sentences. In SIGIR 1999, pages 129{136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Bitext maps and alignment via pattern recognition. Computational Linguistics,</title>
<date>1999</date>
<pages>107--130</pages>
<contexts>
<context position="13918" citStr="Melamed, 1999" startWordPosition="2258" endWordPosition="2259">fferent languages. In this specific case, we investigate whether alignment can map derived texts (or parts of them) to their source texts. PA copy may be subject to various changes during text reuse, e.g. a single sentence may derive from parts of several source sentences. Therefore, strong correlations of sentence length between the derived and source sentences cannot be guaranteed. As a result, sentence-length based statistical alignment algorithms (Brown et al., 1991; Gale and Church, 1993) are not appropriate for this case. On the other hand, cognate-based algorithms (Simard et al., 1992; Melamed, 1999) are more efficient for coping with change of text format. Therefore, a cognate-based approach is adopted for the METER task. Here cognates are defined as pairs of terms that are identical, share the same stems, or are substitutable in the given context. The algorithm consists of two principal components: a comparison strategy and a scoring function. In brief, the comparison works as follows (more details may be found in Piao (2001)). For each sentence in the candidate derived text DT the sentences in the candidate source text ST are compared in order to find the best match. A DT sentence is a</context>
</contexts>
<marker>Melamed, 1999</marker>
<rawString>I. Dan Melamed. 1999. Bitext maps and alignment via pattern recognition. Computational Linguistics, pages 107{130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott S L Piao</author>
</authors>
<title>Detecting and measuring text reuse via aligning texts.</title>
<date>2001</date>
<journal>Research Memorandum</journal>
<pages>01--15</pages>
<institution>Dept. of Computer Science, University of Sheffield.</institution>
<contexts>
<context position="14354" citStr="Piao (2001)" startWordPosition="2332" endWordPosition="2333">ment algorithms (Brown et al., 1991; Gale and Church, 1993) are not appropriate for this case. On the other hand, cognate-based algorithms (Simard et al., 1992; Melamed, 1999) are more efficient for coping with change of text format. Therefore, a cognate-based approach is adopted for the METER task. Here cognates are defined as pairs of terms that are identical, share the same stems, or are substitutable in the given context. The algorithm consists of two principal components: a comparison strategy and a scoring function. In brief, the comparison works as follows (more details may be found in Piao (2001)). For each sentence in the candidate derived text DT the sentences in the candidate source text ST are compared in order to find the best match. A DT sentence is allowed to match up to three possibly non-consecutive ST sentences. The candidate pair with the highest score (see below) above a threshold is accepted as a true alignment. If no such candidate is found, the DT sentence is assumed to be independent of the ST. Based on individual DT sentence alignments, the overall possibility of derivation for the DT is estimated with a score ranging be�i�tiles lengthi gstsim(A� B) = (2) � B � tween </context>
</contexts>
<marker>Piao, 2001</marker>
<rawString>Scott S.L. Piao. 2001. Detecting and measuring text reuse via aligning texts. Research Memorandum CS-01-15, Dept. of Computer Science, University of Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Simard</author>
<author>G Foster</author>
<author>P Isabelle</author>
</authors>
<title>Using cognates to align sentences in bilingual corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 4th Int. Conf. on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>67--81</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="13902" citStr="Simard et al., 1992" startWordPosition="2254" endWordPosition="2257">equivalents across different languages. In this specific case, we investigate whether alignment can map derived texts (or parts of them) to their source texts. PA copy may be subject to various changes during text reuse, e.g. a single sentence may derive from parts of several source sentences. Therefore, strong correlations of sentence length between the derived and source sentences cannot be guaranteed. As a result, sentence-length based statistical alignment algorithms (Brown et al., 1991; Gale and Church, 1993) are not appropriate for this case. On the other hand, cognate-based algorithms (Simard et al., 1992; Melamed, 1999) are more efficient for coping with change of text format. Therefore, a cognate-based approach is adopted for the METER task. Here cognates are defined as pairs of terms that are identical, share the same stems, or are substitutable in the given context. The algorithm consists of two principal components: a comparison strategy and a scoring function. In brief, the comparison works as follows (more details may be found in Piao (2001)). For each sentence in the candidate derived text DT the sentences in the candidate source text ST are compared in order to find the best match. A </context>
</contexts>
<marker>Simard, Foster, Isabelle, 1992</marker>
<rawString>M. Simard, G. Foster, and P. Isabelle. 1992. Using cognates to align sentences in bilingual corpora. In Proceedings of the 4th Int. Conf. on Theoretical and Methodological Issues in Machine Translation, pages 67{81, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wise</author>
</authors>
<title>Yap3: Improved detection of similarities in computer programs and other texts.</title>
<date>1996</date>
<booktitle>In Proceedings of SIGCSE&apos;96,</booktitle>
<pages>130--134</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="11921" citStr="Wise, 1996" startWordPosition="1932" endWordPosition="1933">hereby inhibiting classification of derived and non-derived texts. Therefore we compute the containment of hapax legomena (hapax containment) by comparing words occurring once in the newspaper, i.e. those 1-grams in St(B) that occur once with all 1-grams in PA copy, St(A). This containment score represents the number of newspaper hapax legomena also appearing at least once in PA copy. 4.2 Greedy String-Tiling Greedy String-Tiling (GST) is a substring matching algorithm which computes the degree of similarity between two strings, for example software code, free text or biological subsequences (Wise, 1996). Compared with previous algorithms for computing string similarity, such as the Longest Common Subsequence or Levenshtein distance, GST is able to deal with transposition of tokens (in earlier approaches transposition is seen as a number of single insertions/deletions rather than a single block move). The GST algorithm performs a 1:1 matching of tokens between two strings so that as much of one token stream is covered with maximal length substrings from the other (called tiles). In our problem, we consider how much newspaper text can be maximally covered by words from PA copy. A minimum match</context>
<context position="22331" citStr="Wise, 1996" startWordPosition="3676" endWordPosition="3677">apax containment and alignment. The highest F-measure for the 3-class problem is 0.664 for the &amp;quot;combined&amp;quot; classifier, which is significantly greater than 0.651 obtained without. We notice that highest WD classification is with alignment at 0.774, highest PD classification is 0.654 with hapax containment and highest ND classification is 0.629 with combined features. Using hapax containment gives higher results than 1-gram containment alone and in fact provides results as good as or better than the more complex sentence alignment and GST approaches. Previous research by (Lyon et al., 2001) and (Wise, 1996) had shown derived texts could be distinguished using trigram overlap and tiling with a match length of 3 or more, respectively. Attributes Category Avg Fl Correlation- alignment WD 0.942 (0.008) based ND 0.909 (0.011) filter total 0.926 (0.010) alignment PD/ND 0.870 (0.003) WD 0.770 (0.003) total 0.820 (0.002) alignment WD 0.778 (0.003) PD 0.812 (0.002) total 0.789 (0.002) hapax cont. WD/PD 0.882 (0.002) alignment ND 0.649 (0.007) 1-gram cont. total 0.763 (0.002) 1-gram PD 0.802 (0.002) GST mml 3 ND 0.638 (0.007) GST mml 1 total 0.720 (0.004) alignment GST mml 1 WD/ND 0.672 (0.002) alignment </context>
</contexts>
<marker>Wise, 1996</marker>
<rawString>M. Wise. 1996. Yap3: Improved detection of similarities in computer programs and other texts. In Proceedings of SIGCSE&apos;96, pages 130{134, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Datamining - practical machine learning tools and techniques with Java implementations.</title>
<date>2000</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="16985" citStr="Witten and Frank, 2000" startWordPosition="2808" endWordPosition="2811">es are weighted and combined together to provide an alignment metric WS (weighted score), which is calculated as follows: WS = 61PSD + 62PS + 63PSNG where 61 +62 +63 = 1. The three weighting variables 6e(i = 1, 2, 3) have been determined empirically and are currently set to: 61 = 0.85, 62 = 0.05, 63 = 0.1. 5 Reuse Classifiers To evaluate the previous approaches for measuring text reuse at the document-level, we cast the problem into one of a supervised learning task. 5.1 Experimental Setup We used similarity scores as attributes for a machine learning algorithm and used the Weka 3.2 software (Witten and Frank, 2000). Because of the small number of examples, we used tenfold cross-validation repeated 10 times (i.e. 10 runs) and combined this with stratification to ensure approximately the same proportion of samples from each class were used in each fold of the cross-validation. All 769 newspaper texts from the courts domain were used for evaluation and randomly permuted to generate 10 sets. For each newspaper text, we compared PA source texts from the same story to create results in the form: newspaper, class, score. These results were ordered according to each set to create the same 10 datasets for each a</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>I.H. Witten and E. Frank. 2000. Datamining - practical machine learning tools and techniques with Java implementations. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<date>2000</date>
<booktitle>A Handbook of Natural Language Processing,</booktitle>
<pages>415--458</pages>
<editor>Alignment. In R. Dale and H. Moisl and H. Somers (eds.),</editor>
<publisher>Marcel Dekker.</publisher>
<location>New York:</location>
<contexts>
<context position="13230" citStr="Wu, 2000" startWordPosition="2150" endWordPosition="2151">rity between the strings can be expressed as a quantitative similarity match or a qualitative list of common substrings. Figure 1 shows the result of GST for the example in Section 2. Figure 1: Example GST results (MML=3) 2As stories unfold, PA release copy with new, as well as previous versions of the story Given PA copy A, a newspaper text B and a set of maximal matches, tiles, of a given length between A and B, the similarity, gstsim(A,B), is expressed as: 4.3 Sentence alignment In the past decade, various alignment algorithms have been suggested for aligning multilingual parallel corpora (Wu, 2000). These algorithms have been used to map translation equivalents across different languages. In this specific case, we investigate whether alignment can map derived texts (or parts of them) to their source texts. PA copy may be subject to various changes during text reuse, e.g. a single sentence may derive from parts of several source sentences. Therefore, strong correlations of sentence length between the derived and source sentences cannot be guaranteed. As a result, sentence-length based statistical alignment algorithms (Brown et al., 1991; Gale and Church, 1993) are not appropriate for thi</context>
</contexts>
<marker>Wu, 2000</marker>
<rawString>D. Wu. 2000. Alignment. In R. Dale and H. Moisl and H. Somers (eds.), A Handbook of Natural Language Processing, pages 415{458. New York: Marcel Dekker.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>