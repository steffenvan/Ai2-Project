<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.97932">
Multi-document Biography Summarization
</title>
<author confidence="0.990955">
Liang Zhou, Miruna Ticrea, Eduard Hovy
</author>
<affiliation confidence="0.993514">
University of Southern California
Information Sciences Institute
</affiliation>
<address confidence="0.7951585">
4676 Admiralty Way
Marina del Rey, CA 90292-6695
</address>
<email confidence="0.988288">
{liangz, miruna, hovy} @isi.edu
</email>
<sectionHeader confidence="0.992384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998642555555556">
In this paper we describe a biography
summarization system using sentence
classification and ideas from information
retrieval. Although the individual techniques
are not new, assembling and applying them to
generate multi-document biographies is new.
Our system was evaluated in DUC2004. It is
among the top performers in task 5–short
summaries focused by person questions.
</bodyText>
<sectionHeader confidence="0.998488" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9882251">
Automatic text summarization is one form of
information management. It is described as
selecting a subset of sentences from a document
that is in size a small percentage of the original and
yet is just as informative. Summaries can serve as
surrogates of the full texts in the context of
Information Retrieval (IR). Summaries are created
from two types of text sources, a single document
or a set of documents. Multi-document
summarization (MDS) is a natural and more
elaborative extension of single-document
summarization, and poses additional difficulties on
algorithm design. Various kinds of summaries fall
into two broad categories: generic summaries are
the direct derivatives of the source texts; special-
interest summaries are generated in response to
queries or topic-oriented questions.
One important application of special-interest
MDS systems is creating biographies to answer
questions like “who is Kofi Annan?”. This task
would be tedious for humans to perform in
situations where the information related to the
person is deeply and sparsely buried in large
quantity of news texts that are not obviously
related. This paper describes a MDS biography
system that responds to the “who is” questions by
identifying information about the person-in-
question using IR and classification techniques,
and creates multi-document biographical
summaries. The overall system design is shown in
</bodyText>
<figureCaption confidence="0.985404">
Figure 1.
Figure 1. Overall design of the biography
summarization system.
</figureCaption>
<bodyText confidence="0.999949888888889">
To determine what and how sentences are
selected and ranked, a simple IR method and
experimental classification methods both
contributed. The set of top-scoring sentences, after
redundancy removal, is the resulting biography.
As yet, the system contains no inter-sentence
‘smoothing’ stage.
In this paper, work in related areas is discussed
in Section 2; a description of our biography corpus
used for training and testing the classification
component is in Section 3; Section 4 explains the
need and the process of classifying sentences
according to their biographical state; the
application of the classification method in
biography extraction/summarization is described in
Section 5; an accompanying evaluation on the
quality of the biography summaries is shown in
Section 6; and future work is outlined in Section 7.
</bodyText>
<sectionHeader confidence="0.995367" genericHeader="introduction">
2 Recent Developments
</sectionHeader>
<bodyText confidence="0.999919489361702">
Two trends have dominated automatic
summarization research (Mani, 2001). One is the
work focusing on generating summaries by
extraction, which is finding a subset of the
document that is indicative of its contents (Kupiec
et al., 1995) using “shallow” linguistic analysis and
statistics. The other influence is the exploration of
“deeper” knowledge-based methods for
condensing information. Knight and Marcu (2000)
equate summarization with compression at
sentence level to achieve grammaticality and
information capture, and push a step beyond
sentence extraction. Many systems use machine-
learning methods to learn from readily aligned
corpora of scientific articles and their
corresponding abstracts. Zhou and Hovy (2003)
show a summarization system trained from
automatically obtained text-summary alignments
obeying the chronological occurrences of news
events.
MDS poses more challenges in assessing
similarities and differences among the set of
documents. The simple idea of extract-and-
concatenate does not respond to problems arisen
from coherence and cohesion. Barzilay et al.
(1999) introduce a combination of extracted
similar phrases and a reformulation through
sentence generation. Lin and Hovy (2002) apply a
collection of known single-document
summarization techniques, cooperating positional
and topical information, clustering, etc., and extend
them to perform MDS.
While many have suggested that conventional
MDS systems can be applied to biography
generation directly, Mani (2001) illustrates that the
added functionality of biographical MDS comes at
the expense of a substantial increase in system
complexity and is somewhat beyond the
capabilities of present day MDS systems. The
discussion was based in part on the only known
MDS biography system (Schiffman et al., 2001)
that uses corpus statistics along with linguistic
knowledge to select and merge description of
people in news. The focus of this work was on
synthesizing succinct descriptions of people by
merging appositives from semantic processing
using WordNet (Miller, 1995).
</bodyText>
<sectionHeader confidence="0.987269" genericHeader="method">
3 Corpus Description
</sectionHeader>
<bodyText confidence="0.93997975">
In order to extract information that is related to a
person from a large set of news texts written not
exclusively about this person, we need to identify
attributes shared among biographies.
Biographies share certain standard components.
We annotated a corpus of 130 biographies of 12
people (activists, artists, leaders, politicians,
scientists, terrorists, etc.). We found 9 common
elements: bio (info on birth and death), fame
factor, personality, personal, social, education,
nationality, scandal, and work. Collected
biographies are appropriately marked at clause-
level with one of the nine tags in XML format, for
example:
Martin Luther King &lt;nationality&gt;
was born in Atlanta, Georgia
&lt;/nationality&gt;. ... He &lt;bio&gt;was
assassinated on April 4, 1968
&lt;/bio&gt;. ... King &lt;education&gt; entered
the Boston University as a
doctoral student &lt;/education&gt;. ...
In all, 3579 biography-related phrases were
identified and recorded for the collection, among
them 321 bio, 423 fame, 114 personality, 465
personal, 293 social, 246 education, 95 nationality,
292 scandal, and 1330 work. We then used 100
biographies for training and 30 for testing the
classification module.
</bodyText>
<sectionHeader confidence="0.994453" genericHeader="method">
4 Sentence Classification
</sectionHeader>
<bodyText confidence="0.999941642857143">
Relating to human practice on summarizing,
three main points are relevant to aid the automation
process (SpŠrck Jones, 1993). The first is a strong
emphasis on particular purposes, e.g., abstracting
or extracting articles of particular genres. The
second is the drafting, writing, and revision cycle
in constructing a summary. Essentially as a
consequence of these first two points, the
summarizing process can be guided by the use of
checklists. The idea of a checklist is especially
useful for the purpose of generating biographical
summaries because a complete biography should
contain various aspects of a person’s life. From a
careful analysis conducted while constructing the
biography corpus, we believe that the checklist is
shared and common among all persons in question,
and consists the 9 biographical elements
introduced in Section 3.
The task of fulfilling the biography checklist
becomes a classification problem. Classification is
defined as a task of classifying examples into one
of a discrete set of possible categories (Mitchell,
1997). Text categorization techniques have been
used extensively to improve the efficiency on
information retrieval and organization. Here the
problem is that sentences, from a set of documents,
need to be categorized into different biography-
related classes.
</bodyText>
<subsectionHeader confidence="0.990805">
4.1 Task Definitions
</subsectionHeader>
<bodyText confidence="0.9964842">
We designed two classification tasks:
1) 10 -Class: Given one or more texts about a
person, the module must categorize each
sentence into one of ten classes. The classes
are the 9 biographical elements plus a class
called none that collects all sentences without
biographical information. This fine-grained
classification task will be beneficial in
generating comprehensive biographies on
people of interest. The classes are:
</bodyText>
<figure confidence="0.871436">
bio
fame
personality
social
education
nationality
scandal
personal
work
none
</figure>
<listItem confidence="0.73167375">
2) 2-Class: The module must make a binary
decision of whether the sentence should be
included in a biography summary. The classes
are:
</listItem>
<bodyText confidence="0.970584">
bio
none
The label bio appears in both task definitions but
bears different meanings. Under 10-Class, class bio
contains information on a person’s birth or death,
and under 2-Class it sums up all 9 biographical
elements from the 10-Class.
</bodyText>
<subsectionHeader confidence="0.988073">
4.2 Machine Learning Methods
</subsectionHeader>
<bodyText confidence="0.9993665">
We experimented with three machine learning
methods for classifying sentences.
</bodyText>
<subsectionHeader confidence="0.693966">
Naïve Bayes
</subsectionHeader>
<bodyText confidence="0.9988175">
The Naïve Bayes classifier is among the most
effective algorithms known for learning to classify
text documents (Mitchell, 1997), calculating
explicit probabilities for hypotheses. Using k
features Fj: j = 1, É, k, we assign to a given
sentence S the class C:
</bodyText>
<equation confidence="0.987677">
C = argmaxC P(C  |F1,F2,...,Fk)
</equation>
<bodyText confidence="0.5963515">
It can be expressed using Bayes’ rule, as (Kupiec
et al., 1995):
</bodyText>
<equation confidence="0.9849095">
P(S ∈ C  |F1,F2,...Fk) = P(F1,F2,...Fj  |S ∈ C)• P(S ∈ C)
P(F1,F2,...Fk)
</equation>
<bodyText confidence="0.993791">
Assuming statistical independence of the
features:
</bodyText>
<equation confidence="0.913866">
P(S C C  |F1,F2,...Fk) = ∏k P(Fj  |S C C) • P(S C C) j=1
</equation>
<bodyText confidence="0.9998225">
We trained on the relative frequency of
P(Fj|S∈C) and P(S∈C), with add-one smoothing.
This method was used in classifying both the 10-
Class and the 2-Class tasks.
</bodyText>
<subsectionHeader confidence="0.789185">
Support Vector Machine
</subsectionHeader>
<bodyText confidence="0.999926">
Support Vector Machines (SVMs) have been
shown to be an effective classifier in text
categorization. We extend the idea of classifying
documents into predefined categories to classifying
sentences into one of the two biography categories
defined by the 2-Class task. Sentences are
categorized based on their biographical saliency (a
percentage of clearly identified biography words)
and their non-biographical saliency (a percentage
of clearly identified non-biography words). We
used LIBSVM (Chang and Lin, 2003) for training
and testing.
</bodyText>
<subsectionHeader confidence="0.439542">
Decision Tree (4.5)
</subsectionHeader>
<bodyText confidence="0.999722">
In addition to SVM, we also used a decision-tree
algorithm, C4.5 (Quinlan, 1993), with the same
training and testing data as SVM.
</bodyText>
<subsectionHeader confidence="0.99949">
4.3 Classification Results
</subsectionHeader>
<bodyText confidence="0.9986584">
The lower performance bound is set by a
baseline system that randomly assigns a
biographical class given a sentence, for both 10-
Class and 2-Class. 2599 testing sentences are from
30 unseen documents.
</bodyText>
<subsectionHeader confidence="0.503242">
10-Class Classification
</subsectionHeader>
<bodyText confidence="0.995610714285714">
The Naïve Bayes classifier was used to perform
the 10-Class task. Table 1 shows its performance
with various features.
P(S C C  |F1,F2,...Fk) = ∏kj=1 P(Fj  |S C C) •P(S C C) Table 1. Performance of 10-Class sentence
classification, using Naïve Bayes Classifier.
k
LP(F.)
i J
Since P(Fj) has no role in selecting C:
Part-of-speech (POS) information (Brill, 1995)
and word stems (Lovins, 1968) were used in some
feature sets.
We bootstrapped 10395 more biography-
indicating words by recording the immediate
hypernyms, using WordNet (Fellbaum, 1998), of
the words collected from the controlled biography
corpus described in Section 3. These words are
called Expanded Unigrams and their frequency
scores are reduced to a fraction of the original
word’s frequency score.
Some sentences in the testing set were labeled
with multiple biography classes due to the fact that
the original corpus was annotated at clause level.
Since the classification was done at sentence level,
we relaxed the matching/evaluating program
allowing a hit when any of the several classes was
matched. This is shown in Table 1 as the Relaxed
cases.
A closer look at the instances where the false
negatives occur indicates that the classifier
mislabeled instances of class work as instances of
class none. To correct this error, we created a list
of 5516 work specific words hoping that this would
set a clearer boundary between the two classes.
However performance did not improve.
</bodyText>
<sectionHeader confidence="0.597238" genericHeader="method">
2-Class Classification
</sectionHeader>
<bodyText confidence="0.9996238">
All three machine learning methods were
evaluated in classifying among 2 classes. The
results are shown in Table 2. The testing data is
slightly skewed with 68% of the sentences being
none.
</bodyText>
<tableCaption confidence="0.8085955">
Table 2. Classification results on 2-Class using
Naïve Bayes, SVM, and C4.5.
</tableCaption>
<bodyText confidence="0.99986925">
In addition to using marked biographical phrases
as training data, we also expanded the
marking/tagging perimeter to sentence boundaries.
As shown in the table, this creates noise.
</bodyText>
<sectionHeader confidence="0.989914" genericHeader="method">
5 Biography Extraction
</sectionHeader>
<bodyText confidence="0.99938">
Biographical sentence classification module is
only one of two components that supply the overall
system with usable biographical contents, and is
followed by other stages of processing (see system
design in Figure 1). We discuss the other modules
next.
</bodyText>
<subsectionHeader confidence="0.972264">
5.1 Name-filter
</subsectionHeader>
<bodyText confidence="0.998509666666667">
A filter scans through all documents in the set,
eliminating sentences that are direct quotes,
dialogues, and too short (under 5 words). Person-
oriented sentences containing any variation (first
name only, last name only, and the full name) of
the person’s name are kept for subsequent steps.
Sentences classified as biography-worthy are
merged with the name-filtered sentences with
duplicates eliminated.
</bodyText>
<subsectionHeader confidence="0.998423">
5.2 Sentence Ranking
</subsectionHeader>
<bodyText confidence="0.979599425">
An essential capability of a multi-document
summarizer is to combine text passages in a useful
manner for the reader (Goldstein et al., 2000).
This includes a sentence ordering parameter (Mani,
2001). Each of the sentences selected by the
name-filter and the biography classifier is either
related to the person-in-question via some news
event or referred to as part of this person’s
biographical profile, or both. We need a
mechanism that will select sentences that are of
informative significance within the source
document set. Using inverse-term-frequency
(ITF), i.e. an estimation of information value,
words with high information value (low ITF) are
distinguished from those with low value (high
ITF). A sorted list of words along with their ITF
scores from a document set—topic ITFs—displays
the important events, persons, etc., from this
particular set of texts. This allows us to identify
passages that are unusual with respect to the texts
about the person.
However, we also need to identify passages that
are unusual in general. We have to quantify how
these important words compare to the rest of the
world. The world is represented by 413307562
words from TREC-9 corpus
(http://trec.nist.gov/data.html), with corresponding
ITFs.
The overall informativeness of each word w is:
d=am
W=v
where ditf is the document set ITF of word w and
Witf is the world ITF of w. A word that occurs
frequently bears a lower Cw score compared to a
rarely used word (bearing high information value)
€
with a higher Cw score.
Top scoring sentences are then extracted
according to:
CK,
</bodyText>
<subsubsectionHeader confidence="0.243932">
Cwi
</subsubsectionHeader>
<bodyText confidence="0.9986365">
The following is a set of sentences extracted
according to the method described so far. The
€person-in-question is the famed cyclist Lance
Armstrong.
</bodyText>
<listItem confidence="0.893434">
1. Cycling helped him win his
battle with cancer, and
</listItem>
<bodyText confidence="0.8397875">
cancer helped him win the
Tour de France.
</bodyText>
<listItem confidence="0.957713384615385">
2. Armstrong underwent four
rounds of intense
chemotherapy.
3. The surgeries and
chemotherapy eliminated the
cancer, and Armstrong began
his cycling comeback.
4. The foundation supports
cancer patients and survivors
through education, awareness
and research.
5. He underwent months of
chemotherapy.
</listItem>
<subsectionHeader confidence="0.988939">
5.3 Redundancy Elimination
</subsectionHeader>
<bodyText confidence="0.999983961538461">
Summaries that emphasize the differences across
documents while synthesizing common
information would be the desirable final results.
Removing similar information is part of all MDS
systems. Redundancy is apparent in the Armstrong
example from Section 5.2. To eliminate repetition
while retaining interesting singletons, we modified
(Marcu, 1999) so that an extract can be
automatically generated by starting with a full text
and systematically removing a sentence at a time
as long as a stable semantic similarity with the
original text is maintained. The original extraction
algorithm was used to automatically create large
volume of (extract, abstract, text) tuples for
training extraction-based summarization systems
with (abstract, text) input pairs.
Top-scoring sentences selected by the ranking
mechanism described in Section 5.2 were the input
to this component. The removal process was
repeated until the desired summary length was
achieved.
Applying this method to the Armstrong example,
the result leaves only one sentence that contains
the topics “chemotherapy” and “cancer”. It
chooses sentence 3, which is not bad, though
sentence 1 might be preferable.
</bodyText>
<sectionHeader confidence="0.998009" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.511129">
6.1 Overview
</subsectionHeader>
<bodyText confidence="0.999982538461539">
Extrinsic and intrinsic evaluations are the two
classes of text summarization evaluation methods
(Sparck Jones and Galliers, 1996). Measuring
content coverage or summary informativeness is an
approach commonly used for intrinsic evaluation.
It measures how much source content was
preserved in the summary.
A complete evaluation should include
evaluations of the accuracy of components
involved in the summarization process (Schiffman
et al., 2001). Performance of the sentence
classifier was shown in Section 4. Here we will
show the performance of the resulting summaries.
</bodyText>
<subsectionHeader confidence="0.999286">
6.2 Coverage Evaluation
</subsectionHeader>
<bodyText confidence="0.985458590909091">
An intrinsic evaluation of biography summary
was recently conducted under the guidance of
Document Understanding Conference (DUC2004)
using the automatic summarization evaluation tool
ROUGE (Recall-Oriented Understudy for Gisting
Evaluation) by Lin and Hovy (2003). 50 TREC
English document clusters, each containing on
average 10 news articles, were the input to the
system. Summary length was restricted to 665
bytes. Brute force truncation was applied on longer
summaries.
The ROUGE-L metric is based on Longest
Common Subsequence (LCS) overlap (Saggion et
al., 2002). Figure 2 shows that our system (86)
performs at an equivalent level with the best
systems 9 and 10, that is, they both lie within our
system’s 95% upper confidence interval. The 2-
class classification module was used in generating
the answers. The figure also shows the
performance data evaluated with lower and higher
confidences set at 95%. The performance data are
from official DUC results.
</bodyText>
<figure confidence="0.960262625">
n
∑
i=1
Cs
len(s)
0.55
0.45
0.35
0.25
0.5
0.4
0.3
B E F H G A D C 9 10 11 12 13 86 15 16 17 18 19 20 5 22 23 24 25 26 27 28 29 30 31
ROUGE-L
95% CI Lower
95% CI Higher
</figure>
<figureCaption confidence="0.98339775">
Figure 2. Official ROUGE performance results from DUC2004. Peer systems are labeled with numeric IDs.
Humans are numbered A–H. 86 is our system with 2-class biography classification. Baseline is 5.
Figure 3. Unofficial ROUGE results. Humans are labeled A–H. Peer systems are labeled with numeric IDs.
86 is our system with 10-class biography classification. Baseline is 5.
</figureCaption>
<figure confidence="0.999073692307692">
0.57
0.52
0.47
0.42
0.37
0.32
0.27
0.22
0.17
B F E G H A D C 9 10 11 12 13 86 15 16 17 18 19 20 21 22 23 24 25 5 27 28 29 30 31
ROUGE-L
95% CL Lower
95% CL Higher
</figure>
<bodyText confidence="0.985964529411765">
Figure 3 shows the performance results of our
system 86, using 10-class sentence classification,
comparing to other systems from DUC by
replicating the official evaluating process. Only
system 9 performs slightly better with its score
being higher than our system’s 95% upper
confidence interval.
A baseline system (5) that takes the first 665
bytes of the most recent text from the set as the
resulting biography was also evaluated amongst
the peer systems. Clearly, humans still perform at a
level much superior to any system.
Measuring fluency and coherence is also
important in reflecting the true quality of machine-
generated summaries. There is no automated tool
for this purpose currently. We plan to incorporate
one for the future development of this work.
</bodyText>
<subsectionHeader confidence="0.982511">
6.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999954074074074">
N-gram recall scores are computed by ROUGE,
in addition to ROUGE-L shown here. While cosine
similarity and unigram and bigram overlap
demonstrate a sufficient measure on content
coverage, they are not sensitive on how
information is sequenced in the text (Saggion et al.,
2002). In evaluating and analyzing MDS results,
metrics, such as ROUGE-L, that consider linguistic
sequence are essential.
Radev and McKeown (1998) point out when
summarizing interesting news events from multiple
sources, one can expect reports with contradictory
and redundant information. An intelligent
summarizer should attain as much information as
possible, combine it, and present it in the most
concise form to the user. When we look at the
different attributes in a person’s life reported in
news articles, a person is described by the job
positions that he/she has held, by education
institutions that he/she has attended, and etc. Those
data are confirmed biographical information and
do not bear the necessary contradiction associated
with evolving news stories. However, we do feel
the need to address and resolve discrepancies if we
were to create comprehensive and detailed
biographies on people-in-news since miscellaneous
personal facts are often overlooked and told in
conflicting reports. Misrepresented biographical
information may well be controversies and may
never be clarified. The scandal element from our
corpus study (Section 3) is sufficient to identify
information of the disputed kind.
Extraction-based MDS summarizers, such as this
one, present the inherent problem of lacking the
discourse-level fluency. While sentence ordering
for single document summarization can be
determined from the ordering of sentences in the
input article, sentences extracted by a MDS system
may be from different articles and thus need a
strategy on ordering to produce a fluent surface
summary (Barzilay et al., 2002). Previous
summarization systems have used temporal
sequence as the guideline on ordering. This is
especially true in generating biographies where a
person is represented by a sequence of events that
occurred in his/her life. Barzilay et al. also
introduced a combinational method with an
alternative strategy that approximates the
information relatedness across the input texts. We
plan to use a fixed-form structure for the majority
of answer construction, fitted for biographies only.
This will be a top-down ordering strategy, contrary
to the bottom-up algorithm shown by Barzilay et
al.
</bodyText>
<sectionHeader confidence="0.981149" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999914928571428">
In this paper, we described a system that uses IR
and text categorization techniques to provide
summary-length answers to biographical questions.
The core problem lies in extracting biography-
related information from large volumes of news
texts and composing them into fluent, concise,
multi-document summaries. The summaries
generated by the system address the question about
the person, though not listing the chronological
events occurring in this person’s life due to the
lack of background information in the news
articles themselves. In order to obtain a “normal”
biography, one should consult other means of
information repositories.
</bodyText>
<subsectionHeader confidence="0.379485">
Question: Who is Sir John Gielgud?
</subsectionHeader>
<bodyText confidence="0.981681888888889">
Answer: Sir John Gielgud, one of
the great actors of the English
stage who enthralled audiences for
more than 70 years with his
eloquent voice and consummate
artistry, died Sunday at his home
Gielgud’s last major film role was
as a surreal Prospero in Peter
G r e e n a w a y ’ s controversial
Shakespearean rhapsody.
Above summary does not directly explain who
the person-in-question is, but indirectly does so in
explanatory sentences. We plan to investigate
combining fixed-form and free-form structures in
answer construction. The summary would include
an introductory sentence of the form “x is
&lt;type/fame-category&gt; É”, possibly through
querying outside online resources. A main body
would follow the introduction with an assembly of
checklist items generated from the 10-Class
classifier. A conclusion would contain open-ended
items of special interest.
Furthermore, we would like to investigate
compression strategies in creating summaries,
specifically for biographies. Our biography corpus
was tailored for this purpose and will be the
starting point for further investigation.
</bodyText>
<sectionHeader confidence="0.969778" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999328666666667">
We would like to thank Chin-Yew Lin from ISI
for many insightful discussions on MDS,
biography generation, and ROUGE.
</bodyText>
<sectionHeader confidence="0.998622" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998486523809524">
Regina Barzilay, Kathleen McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In
Proceedings of the 37th Annual Meeting of the
Association of Computational Linguistics (ACL-
99), University of Maryland, 1999, pp. 550–557.
Regina Barzilay, Noemie Elhadad, Kathleen
McKeown. 2002. Inferring strategies for
sentence ordering in multidocument
summarization. JAIR, 17:35–55, 2002.
Eric Brill. 1995. Transformation-based error-
driven learning and natural language processing:
A case study in part of speech tagging.
Computational Linguistics, December 1995.
Chih-Chung Chang and Chih-Jen Lin. 2003.
LIBSVM—A Library for support vector
machines.
http://www.csie.ntu.edu.tw/~cjlin/libsvm/
Christiane Fellbaum, editor. WordNet: An
electronic lexical database. Cambridge, MA:
MIT Press.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document
summarization by sentence extraction. In
Proceedings of the ANLP’2000 Workshop on
Automatic Summarization, 40–48. New
Brunswick, New Jersey: Association for
Computational Linguistics.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many
relevant features. In Proceedings of the
European Conference on Machine Learning
(ECML), pages 137–142.
Kevin Knight and Daniel Marcu. 2000. Statistics-
Based summarization step one: sentence
compression. In Proceedings of the 17th National
Conference of the American Association for
Artificial Intelligence (AAAI 2000).
Julian Kupiec, Jan Pedersen, and Francine Chen.
1995. A trainable document summarizer. In
SIGIR’95, Proceedings of the 18th Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pp. 68–73.
Chin-Yew Lin and Eduard Hovy. 2002. Automated
multi-document summarization in NeATS. In
Proceedings of the Human Language
Technology Conference (HLT2002), San Diego,
CA, U.S.A., March 23-27, 2002.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-
occurrence statistics. In HLT-NAACL 2003:
Main Proceedings, pp.150–157.
Julie Beth Lovins. 1968. Development of a
stemming algorithm. Mechanical translation and
computational linguistics, 11:22–31, 1968.
Inderjeet Mani. 2001. Automatic summarization
(natural language processing, 3).
Inderjeet Mani. 2001. Recent developments in text
summarization. In CIKM’2001, Proceedings of
the Tenth International Conference on
Information and Knowledge Management,
November 5-10, 2001, 529–531.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research.
The 22nd International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR’99), pages 137-144, Berkeley,
CA, August 1999.
George Miller. 1995. WordNet: a lexical database
for English. Communications of the ACM, pages
39–41.
Tom Mitchell. 1997. Machine Learning. McGraw
Hill, 1997.
Ross J. Quinlan. 1993. C4.5: Programs for
machine learning. San Mateo, CA: Morgan
Kaufmann.
Dragomir R. Radev, Kathleen McKeown. 1998.
Generating natural language summaries from
multiple on-line sources. Computational
Linguistics 24(3): 469–500 (1998).
Horacio Saggion, Dragomir Radev, Simone Teufel,
and Wai Lam. Meta-evaluation of summaries in
a cross-lingual environment using content-based
metrics. In Proceedings of COLING’2002,
Taipei, Taiwan, August 2002.
Barry Schiffman, Inderjeet Mani, and Kristian
Concepcion. 2001. Producing biographical
summaries: combining linguistic knowledge with
corpus statistics. In Proceedings of the 39th
Annual Meeting of the Association for
Computational Linguistics (ACL’2001),
450–457. New Brunswick, New Jersey:
Association for Computational Linguistics.
Karen SpŠrck Jones and Julia R. Galliers. 1996.
Evaluating Natural Language Processing
Systems: An Analysis and Review. Lecture Notes
in Artificial Intelligence 1083. Berlin: Springer.
Karen SpŠrck Jones. 1993. What might be in a
summary? Information Retrieval 1993: 9Ð26.
Liang Zhou and Eduard Hovy. A web-trained
extraction summarization system. In
Proceedings of the Human Language
Technology Conference (HLT-NAACL 2003),
pages 284–290.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.666678">
<title confidence="0.999496">Multi-document Biography Summarization</title>
<author confidence="0.99275">Liang Zhou</author>
<author confidence="0.99275">Miruna Ticrea</author>
<author confidence="0.99275">Eduard</author>
<affiliation confidence="0.998813">University of Southern Information Sciences</affiliation>
<address confidence="0.989931">4676 Admiralty</address>
<author confidence="0.691364">Marina del Rey</author>
<author confidence="0.691364">CA</author>
<email confidence="0.997704">liangz@isi.edu</email>
<email confidence="0.997704">miruna@isi.edu</email>
<email confidence="0.997704">hovy@isi.edu</email>
<abstract confidence="0.9983246">In this paper we describe a biography summarization system using sentence classification and ideas from information retrieval. Although the individual techniques are not new, assembling and applying them to generate multi-document biographies is new. Our system was evaluated in DUC2004. It is among the top performers in task 5–short summaries focused by person questions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
<author>Michael Elhadad</author>
</authors>
<title>Information fusion in the context of multi-document summarization.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association of Computational Linguistics (ACL99),</booktitle>
<pages>550--557</pages>
<institution>University of Maryland,</institution>
<contexts>
<context position="4024" citStr="Barzilay et al. (1999)" startWordPosition="584" endWordPosition="587">l to achieve grammaticality and information capture, and push a step beyond sentence extraction. Many systems use machinelearning methods to learn from readily aligned corpora of scientific articles and their corresponding abstracts. Zhou and Hovy (2003) show a summarization system trained from automatically obtained text-summary alignments obeying the chronological occurrences of news events. MDS poses more challenges in assessing similarities and differences among the set of documents. The simple idea of extract-andconcatenate does not respond to problems arisen from coherence and cohesion. Barzilay et al. (1999) introduce a combination of extracted similar phrases and a reformulation through sentence generation. Lin and Hovy (2002) apply a collection of known single-document summarization techniques, cooperating positional and topical information, clustering, etc., and extend them to perform MDS. While many have suggested that conventional MDS systems can be applied to biography generation directly, Mani (2001) illustrates that the added functionality of biographical MDS comes at the expense of a substantial increase in system complexity and is somewhat beyond the capabilities of present day MDS syst</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>Regina Barzilay, Kathleen McKeown, and Michael Elhadad. 1999. Information fusion in the context of multi-document summarization. In Proceedings of the 37th Annual Meeting of the Association of Computational Linguistics (ACL99), University of Maryland, 1999, pp. 550–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
<author>Kathleen McKeown</author>
</authors>
<title>Inferring strategies for sentence ordering in multidocument summarization.</title>
<date>2002</date>
<journal>JAIR,</journal>
<volume>17</volume>
<contexts>
<context position="21124" citStr="Barzilay et al., 2002" startWordPosition="3250" endWordPosition="3253"> Misrepresented biographical information may well be controversies and may never be clarified. The scandal element from our corpus study (Section 3) is sufficient to identify information of the disputed kind. Extraction-based MDS summarizers, such as this one, present the inherent problem of lacking the discourse-level fluency. While sentence ordering for single document summarization can be determined from the ordering of sentences in the input article, sentences extracted by a MDS system may be from different articles and thus need a strategy on ordering to produce a fluent surface summary (Barzilay et al., 2002). Previous summarization systems have used temporal sequence as the guideline on ordering. This is especially true in generating biographies where a person is represented by a sequence of events that occurred in his/her life. Barzilay et al. also introduced a combinational method with an alternative strategy that approximates the information relatedness across the input texts. We plan to use a fixed-form structure for the majority of answer construction, fitted for biographies only. This will be a top-down ordering strategy, contrary to the bottom-up algorithm shown by Barzilay et al. 7 Conclu</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>Regina Barzilay, Noemie Elhadad, Kathleen McKeown. 2002. Inferring strategies for sentence ordering in multidocument summarization. JAIR, 17:35–55, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based errordriven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics,</title>
<date>1995</date>
<contexts>
<context position="10545" citStr="Brill, 1995" startWordPosition="1582" endWordPosition="1583">4.3 Classification Results The lower performance bound is set by a baseline system that randomly assigns a biographical class given a sentence, for both 10- Class and 2-Class. 2599 testing sentences are from 30 unseen documents. 10-Class Classification The Naïve Bayes classifier was used to perform the 10-Class task. Table 1 shows its performance with various features. P(S C C |F1,F2,...Fk) = ∏kj=1 P(Fj |S C C) •P(S C C) Table 1. Performance of 10-Class sentence classification, using Naïve Bayes Classifier. k LP(F.) i J Since P(Fj) has no role in selecting C: Part-of-speech (POS) information (Brill, 1995) and word stems (Lovins, 1968) were used in some feature sets. We bootstrapped 10395 more biographyindicating words by recording the immediate hypernyms, using WordNet (Fellbaum, 1998), of the words collected from the controlled biography corpus described in Section 3. These words are called Expanded Unigrams and their frequency scores are reduced to a fraction of the original word’s frequency score. Some sentences in the testing set were labeled with multiple biography classes due to the fact that the original corpus was annotated at clause level. Since the classification was done at sentence</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based errordriven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, December 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM—A Library for support vector machines. http://www.csie.ntu.edu.tw/~cjlin/libsvm/</title>
<date>2003</date>
<contexts>
<context position="9756" citStr="Chang and Lin, 2003" startWordPosition="1453" endWordPosition="1456">hing. This method was used in classifying both the 10- Class and the 2-Class tasks. Support Vector Machine Support Vector Machines (SVMs) have been shown to be an effective classifier in text categorization. We extend the idea of classifying documents into predefined categories to classifying sentences into one of the two biography categories defined by the 2-Class task. Sentences are categorized based on their biographical saliency (a percentage of clearly identified biography words) and their non-biographical saliency (a percentage of clearly identified non-biography words). We used LIBSVM (Chang and Lin, 2003) for training and testing. Decision Tree (4.5) In addition to SVM, we also used a decision-tree algorithm, C4.5 (Quinlan, 1993), with the same training and testing data as SVM. 4.3 Classification Results The lower performance bound is set by a baseline system that randomly assigns a biographical class given a sentence, for both 10- Class and 2-Class. 2599 testing sentences are from 30 unseen documents. 10-Class Classification The Naïve Bayes classifier was used to perform the 10-Class task. Table 1 shows its performance with various features. P(S C C |F1,F2,...Fk) = ∏kj=1 P(Fj |S C C) •P(S C C</context>
</contexts>
<marker>Chang, Lin, 2003</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2003. LIBSVM—A Library for support vector machines. http://www.csie.ntu.edu.tw/~cjlin/libsvm/</rawString>
</citation>
<citation valid="false">
<title>WordNet: An electronic lexical database.</title>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker></marker>
<rawString>Christiane Fellbaum, editor. WordNet: An electronic lexical database. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
<author>Mark Kantrowitz</author>
</authors>
<title>Multi-document summarization by sentence extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP’2000 Workshop on Automatic Summarization,</booktitle>
<pages>40--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New Brunswick, New Jersey:</location>
<contexts>
<context position="12967" citStr="Goldstein et al., 2000" startWordPosition="1958" endWordPosition="1961"> 1). We discuss the other modules next. 5.1 Name-filter A filter scans through all documents in the set, eliminating sentences that are direct quotes, dialogues, and too short (under 5 words). Personoriented sentences containing any variation (first name only, last name only, and the full name) of the person’s name are kept for subsequent steps. Sentences classified as biography-worthy are merged with the name-filtered sentences with duplicates eliminated. 5.2 Sentence Ranking An essential capability of a multi-document summarizer is to combine text passages in a useful manner for the reader (Goldstein et al., 2000). This includes a sentence ordering parameter (Mani, 2001). Each of the sentences selected by the name-filter and the biography classifier is either related to the person-in-question via some news event or referred to as part of this person’s biographical profile, or both. We need a mechanism that will select sentences that are of informative significance within the source document set. Using inverse-term-frequency (ITF), i.e. an estimation of information value, words with high information value (low ITF) are distinguished from those with low value (high ITF). A sorted list of words along with</context>
</contexts>
<marker>Goldstein, Mittal, Carbonell, Kantrowitz, 2000</marker>
<rawString>Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by sentence extraction. In Proceedings of the ANLP’2000 Workshop on Automatic Summarization, 40–48. New Brunswick, New Jersey: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the European Conference on Machine Learning (ECML),</booktitle>
<pages>137--142</pages>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of the European Conference on Machine Learning (ECML), pages 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>StatisticsBased summarization step one: sentence compression.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th National Conference of the American Association for Artificial Intelligence (AAAI</booktitle>
<contexts>
<context position="3347" citStr="Knight and Marcu (2000)" startWordPosition="490" endWordPosition="493"> extraction/summarization is described in Section 5; an accompanying evaluation on the quality of the biography summaries is shown in Section 6; and future work is outlined in Section 7. 2 Recent Developments Two trends have dominated automatic summarization research (Mani, 2001). One is the work focusing on generating summaries by extraction, which is finding a subset of the document that is indicative of its contents (Kupiec et al., 1995) using “shallow” linguistic analysis and statistics. The other influence is the exploration of “deeper” knowledge-based methods for condensing information. Knight and Marcu (2000) equate summarization with compression at sentence level to achieve grammaticality and information capture, and push a step beyond sentence extraction. Many systems use machinelearning methods to learn from readily aligned corpora of scientific articles and their corresponding abstracts. Zhou and Hovy (2003) show a summarization system trained from automatically obtained text-summary alignments obeying the chronological occurrences of news events. MDS poses more challenges in assessing similarities and differences among the set of documents. The simple idea of extract-andconcatenate does not r</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. StatisticsBased summarization step one: sentence compression. In Proceedings of the 17th National Conference of the American Association for Artificial Intelligence (AAAI 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In SIGIR’95, Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>68--73</pages>
<contexts>
<context position="3168" citStr="Kupiec et al., 1995" startWordPosition="467" endWordPosition="470">Section 3; Section 4 explains the need and the process of classifying sentences according to their biographical state; the application of the classification method in biography extraction/summarization is described in Section 5; an accompanying evaluation on the quality of the biography summaries is shown in Section 6; and future work is outlined in Section 7. 2 Recent Developments Two trends have dominated automatic summarization research (Mani, 2001). One is the work focusing on generating summaries by extraction, which is finding a subset of the document that is indicative of its contents (Kupiec et al., 1995) using “shallow” linguistic analysis and statistics. The other influence is the exploration of “deeper” knowledge-based methods for condensing information. Knight and Marcu (2000) equate summarization with compression at sentence level to achieve grammaticality and information capture, and push a step beyond sentence extraction. Many systems use machinelearning methods to learn from readily aligned corpora of scientific articles and their corresponding abstracts. Zhou and Hovy (2003) show a summarization system trained from automatically obtained text-summary alignments obeying the chronologic</context>
<context position="8877" citStr="Kupiec et al., 1995" startWordPosition="1316" endWordPosition="1319">0-Class, class bio contains information on a person’s birth or death, and under 2-Class it sums up all 9 biographical elements from the 10-Class. 4.2 Machine Learning Methods We experimented with three machine learning methods for classifying sentences. Naïve Bayes The Naïve Bayes classifier is among the most effective algorithms known for learning to classify text documents (Mitchell, 1997), calculating explicit probabilities for hypotheses. Using k features Fj: j = 1, É, k, we assign to a given sentence S the class C: C = argmaxC P(C |F1,F2,...,Fk) It can be expressed using Bayes’ rule, as (Kupiec et al., 1995): P(S ∈ C |F1,F2,...Fk) = P(F1,F2,...Fj |S ∈ C)• P(S ∈ C) P(F1,F2,...Fk) Assuming statistical independence of the features: P(S C C |F1,F2,...Fk) = ∏k P(Fj |S C C) • P(S C C) j=1 We trained on the relative frequency of P(Fj|S∈C) and P(S∈C), with add-one smoothing. This method was used in classifying both the 10- Class and the 2-Class tasks. Support Vector Machine Support Vector Machines (SVMs) have been shown to be an effective classifier in text categorization. We extend the idea of classifying documents into predefined categories to classifying sentences into one of the two biography categor</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In SIGIR’95, Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automated multi-document summarization in NeATS.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT2002),</booktitle>
<location>San Diego, CA, U.S.A.,</location>
<contexts>
<context position="4146" citStr="Lin and Hovy (2002)" startWordPosition="601" endWordPosition="604">ning methods to learn from readily aligned corpora of scientific articles and their corresponding abstracts. Zhou and Hovy (2003) show a summarization system trained from automatically obtained text-summary alignments obeying the chronological occurrences of news events. MDS poses more challenges in assessing similarities and differences among the set of documents. The simple idea of extract-andconcatenate does not respond to problems arisen from coherence and cohesion. Barzilay et al. (1999) introduce a combination of extracted similar phrases and a reformulation through sentence generation. Lin and Hovy (2002) apply a collection of known single-document summarization techniques, cooperating positional and topical information, clustering, etc., and extend them to perform MDS. While many have suggested that conventional MDS systems can be applied to biography generation directly, Mani (2001) illustrates that the added functionality of biographical MDS comes at the expense of a substantial increase in system complexity and is somewhat beyond the capabilities of present day MDS systems. The discussion was based in part on the only known MDS biography system (Schiffman et al., 2001) that uses corpus sta</context>
</contexts>
<marker>Lin, Hovy, 2002</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2002. Automated multi-document summarization in NeATS. In Proceedings of the Human Language Technology Conference (HLT2002), San Diego, CA, U.S.A., March 23-27, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram cooccurrence statistics.</title>
<date>2003</date>
<booktitle>In HLT-NAACL 2003: Main Proceedings,</booktitle>
<pages>150--157</pages>
<contexts>
<context position="17036" citStr="Lin and Hovy (2003)" startWordPosition="2571" endWordPosition="2574">measures how much source content was preserved in the summary. A complete evaluation should include evaluations of the accuracy of components involved in the summarization process (Schiffman et al., 2001). Performance of the sentence classifier was shown in Section 4. Here we will show the performance of the resulting summaries. 6.2 Coverage Evaluation An intrinsic evaluation of biography summary was recently conducted under the guidance of Document Understanding Conference (DUC2004) using the automatic summarization evaluation tool ROUGE (Recall-Oriented Understudy for Gisting Evaluation) by Lin and Hovy (2003). 50 TREC English document clusters, each containing on average 10 news articles, were the input to the system. Summary length was restricted to 665 bytes. Brute force truncation was applied on longer summaries. The ROUGE-L metric is based on Longest Common Subsequence (LCS) overlap (Saggion et al., 2002). Figure 2 shows that our system (86) performs at an equivalent level with the best systems 9 and 10, that is, they both lie within our system’s 95% upper confidence interval. The 2- class classification module was used in generating the answers. The figure also shows the performance data eval</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In HLT-NAACL 2003: Main Proceedings, pp.150–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Beth Lovins</author>
</authors>
<title>Development of a stemming algorithm. Mechanical translation and computational linguistics,</title>
<date>1968</date>
<pages>11--22</pages>
<contexts>
<context position="10575" citStr="Lovins, 1968" startWordPosition="1587" endWordPosition="1588">e lower performance bound is set by a baseline system that randomly assigns a biographical class given a sentence, for both 10- Class and 2-Class. 2599 testing sentences are from 30 unseen documents. 10-Class Classification The Naïve Bayes classifier was used to perform the 10-Class task. Table 1 shows its performance with various features. P(S C C |F1,F2,...Fk) = ∏kj=1 P(Fj |S C C) •P(S C C) Table 1. Performance of 10-Class sentence classification, using Naïve Bayes Classifier. k LP(F.) i J Since P(Fj) has no role in selecting C: Part-of-speech (POS) information (Brill, 1995) and word stems (Lovins, 1968) were used in some feature sets. We bootstrapped 10395 more biographyindicating words by recording the immediate hypernyms, using WordNet (Fellbaum, 1998), of the words collected from the controlled biography corpus described in Section 3. These words are called Expanded Unigrams and their frequency scores are reduced to a fraction of the original word’s frequency score. Some sentences in the testing set were labeled with multiple biography classes due to the fact that the original corpus was annotated at clause level. Since the classification was done at sentence level, we relaxed the matchin</context>
</contexts>
<marker>Lovins, 1968</marker>
<rawString>Julie Beth Lovins. 1968. Development of a stemming algorithm. Mechanical translation and computational linguistics, 11:22–31, 1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Automatic summarization (natural language processing,</title>
<date>2001</date>
<pages>3</pages>
<contexts>
<context position="3004" citStr="Mani, 2001" startWordPosition="441" endWordPosition="442">r, work in related areas is discussed in Section 2; a description of our biography corpus used for training and testing the classification component is in Section 3; Section 4 explains the need and the process of classifying sentences according to their biographical state; the application of the classification method in biography extraction/summarization is described in Section 5; an accompanying evaluation on the quality of the biography summaries is shown in Section 6; and future work is outlined in Section 7. 2 Recent Developments Two trends have dominated automatic summarization research (Mani, 2001). One is the work focusing on generating summaries by extraction, which is finding a subset of the document that is indicative of its contents (Kupiec et al., 1995) using “shallow” linguistic analysis and statistics. The other influence is the exploration of “deeper” knowledge-based methods for condensing information. Knight and Marcu (2000) equate summarization with compression at sentence level to achieve grammaticality and information capture, and push a step beyond sentence extraction. Many systems use machinelearning methods to learn from readily aligned corpora of scientific articles and</context>
<context position="4431" citStr="Mani (2001)" startWordPosition="641" endWordPosition="642">nges in assessing similarities and differences among the set of documents. The simple idea of extract-andconcatenate does not respond to problems arisen from coherence and cohesion. Barzilay et al. (1999) introduce a combination of extracted similar phrases and a reformulation through sentence generation. Lin and Hovy (2002) apply a collection of known single-document summarization techniques, cooperating positional and topical information, clustering, etc., and extend them to perform MDS. While many have suggested that conventional MDS systems can be applied to biography generation directly, Mani (2001) illustrates that the added functionality of biographical MDS comes at the expense of a substantial increase in system complexity and is somewhat beyond the capabilities of present day MDS systems. The discussion was based in part on the only known MDS biography system (Schiffman et al., 2001) that uses corpus statistics along with linguistic knowledge to select and merge description of people in news. The focus of this work was on synthesizing succinct descriptions of people by merging appositives from semantic processing using WordNet (Miller, 1995). 3 Corpus Description In order to extract </context>
<context position="13025" citStr="Mani, 2001" startWordPosition="1968" endWordPosition="1969"> through all documents in the set, eliminating sentences that are direct quotes, dialogues, and too short (under 5 words). Personoriented sentences containing any variation (first name only, last name only, and the full name) of the person’s name are kept for subsequent steps. Sentences classified as biography-worthy are merged with the name-filtered sentences with duplicates eliminated. 5.2 Sentence Ranking An essential capability of a multi-document summarizer is to combine text passages in a useful manner for the reader (Goldstein et al., 2000). This includes a sentence ordering parameter (Mani, 2001). Each of the sentences selected by the name-filter and the biography classifier is either related to the person-in-question via some news event or referred to as part of this person’s biographical profile, or both. We need a mechanism that will select sentences that are of informative significance within the source document set. Using inverse-term-frequency (ITF), i.e. an estimation of information value, words with high information value (low ITF) are distinguished from those with low value (high ITF). A sorted list of words along with their ITF scores from a document set—topic ITFs—displays </context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Inderjeet Mani. 2001. Automatic summarization (natural language processing, 3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Recent developments in text summarization.</title>
<date>2001</date>
<booktitle>In CIKM’2001, Proceedings of the Tenth International Conference on Information and Knowledge Management,</booktitle>
<pages>529--531</pages>
<contexts>
<context position="3004" citStr="Mani, 2001" startWordPosition="441" endWordPosition="442">r, work in related areas is discussed in Section 2; a description of our biography corpus used for training and testing the classification component is in Section 3; Section 4 explains the need and the process of classifying sentences according to their biographical state; the application of the classification method in biography extraction/summarization is described in Section 5; an accompanying evaluation on the quality of the biography summaries is shown in Section 6; and future work is outlined in Section 7. 2 Recent Developments Two trends have dominated automatic summarization research (Mani, 2001). One is the work focusing on generating summaries by extraction, which is finding a subset of the document that is indicative of its contents (Kupiec et al., 1995) using “shallow” linguistic analysis and statistics. The other influence is the exploration of “deeper” knowledge-based methods for condensing information. Knight and Marcu (2000) equate summarization with compression at sentence level to achieve grammaticality and information capture, and push a step beyond sentence extraction. Many systems use machinelearning methods to learn from readily aligned corpora of scientific articles and</context>
<context position="4431" citStr="Mani (2001)" startWordPosition="641" endWordPosition="642">nges in assessing similarities and differences among the set of documents. The simple idea of extract-andconcatenate does not respond to problems arisen from coherence and cohesion. Barzilay et al. (1999) introduce a combination of extracted similar phrases and a reformulation through sentence generation. Lin and Hovy (2002) apply a collection of known single-document summarization techniques, cooperating positional and topical information, clustering, etc., and extend them to perform MDS. While many have suggested that conventional MDS systems can be applied to biography generation directly, Mani (2001) illustrates that the added functionality of biographical MDS comes at the expense of a substantial increase in system complexity and is somewhat beyond the capabilities of present day MDS systems. The discussion was based in part on the only known MDS biography system (Schiffman et al., 2001) that uses corpus statistics along with linguistic knowledge to select and merge description of people in news. The focus of this work was on synthesizing succinct descriptions of people by merging appositives from semantic processing using WordNet (Miller, 1995). 3 Corpus Description In order to extract </context>
<context position="13025" citStr="Mani, 2001" startWordPosition="1968" endWordPosition="1969"> through all documents in the set, eliminating sentences that are direct quotes, dialogues, and too short (under 5 words). Personoriented sentences containing any variation (first name only, last name only, and the full name) of the person’s name are kept for subsequent steps. Sentences classified as biography-worthy are merged with the name-filtered sentences with duplicates eliminated. 5.2 Sentence Ranking An essential capability of a multi-document summarizer is to combine text passages in a useful manner for the reader (Goldstein et al., 2000). This includes a sentence ordering parameter (Mani, 2001). Each of the sentences selected by the name-filter and the biography classifier is either related to the person-in-question via some news event or referred to as part of this person’s biographical profile, or both. We need a mechanism that will select sentences that are of informative significance within the source document set. Using inverse-term-frequency (ITF), i.e. an estimation of information value, words with high information value (low ITF) are distinguished from those with low value (high ITF). A sorted list of words along with their ITF scores from a document set—topic ITFs—displays </context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Inderjeet Mani. 2001. Recent developments in text summarization. In CIKM’2001, Proceedings of the Tenth International Conference on Information and Knowledge Management, November 5-10, 2001, 529–531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The automatic construction of large-scale corpora for summarization research.</title>
<date>1999</date>
<booktitle>The 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’99),</booktitle>
<pages>137--144</pages>
<location>Berkeley, CA,</location>
<contexts>
<context position="15325" citStr="Marcu, 1999" startWordPosition="2325" endWordPosition="2326">3. The surgeries and chemotherapy eliminated the cancer, and Armstrong began his cycling comeback. 4. The foundation supports cancer patients and survivors through education, awareness and research. 5. He underwent months of chemotherapy. 5.3 Redundancy Elimination Summaries that emphasize the differences across documents while synthesizing common information would be the desirable final results. Removing similar information is part of all MDS systems. Redundancy is apparent in the Armstrong example from Section 5.2. To eliminate repetition while retaining interesting singletons, we modified (Marcu, 1999) so that an extract can be automatically generated by starting with a full text and systematically removing a sentence at a time as long as a stable semantic similarity with the original text is maintained. The original extraction algorithm was used to automatically create large volume of (extract, abstract, text) tuples for training extraction-based summarization systems with (abstract, text) input pairs. Top-scoring sentences selected by the ranking mechanism described in Section 5.2 were the input to this component. The removal process was repeated until the desired summary length was achie</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Daniel Marcu. 1999. The automatic construction of large-scale corpora for summarization research. The 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’99), pages 137-144, Berkeley, CA, August 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<pages>39--41</pages>
<contexts>
<context position="4988" citStr="Miller, 1995" startWordPosition="727" endWordPosition="728">be applied to biography generation directly, Mani (2001) illustrates that the added functionality of biographical MDS comes at the expense of a substantial increase in system complexity and is somewhat beyond the capabilities of present day MDS systems. The discussion was based in part on the only known MDS biography system (Schiffman et al., 2001) that uses corpus statistics along with linguistic knowledge to select and merge description of people in news. The focus of this work was on synthesizing succinct descriptions of people by merging appositives from semantic processing using WordNet (Miller, 1995). 3 Corpus Description In order to extract information that is related to a person from a large set of news texts written not exclusively about this person, we need to identify attributes shared among biographies. Biographies share certain standard components. We annotated a corpus of 130 biographies of 12 people (activists, artists, leaders, politicians, scientists, terrorists, etc.). We found 9 common elements: bio (info on birth and death), fame factor, personality, personal, social, education, nationality, scandal, and work. Collected biographies are appropriately marked at clauselevel wit</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, pages 39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning.</booktitle>
<publisher>McGraw Hill,</publisher>
<contexts>
<context position="7251" citStr="Mitchell, 1997" startWordPosition="1067" endWordPosition="1068">lists. The idea of a checklist is especially useful for the purpose of generating biographical summaries because a complete biography should contain various aspects of a person’s life. From a careful analysis conducted while constructing the biography corpus, we believe that the checklist is shared and common among all persons in question, and consists the 9 biographical elements introduced in Section 3. The task of fulfilling the biography checklist becomes a classification problem. Classification is defined as a task of classifying examples into one of a discrete set of possible categories (Mitchell, 1997). Text categorization techniques have been used extensively to improve the efficiency on information retrieval and organization. Here the problem is that sentences, from a set of documents, need to be categorized into different biographyrelated classes. 4.1 Task Definitions We designed two classification tasks: 1) 10 -Class: Given one or more texts about a person, the module must categorize each sentence into one of ten classes. The classes are the 9 biographical elements plus a class called none that collects all sentences without biographical information. This fine-grained classification tas</context>
<context position="8651" citStr="Mitchell, 1997" startWordPosition="1277" endWordPosition="1278">ass: The module must make a binary decision of whether the sentence should be included in a biography summary. The classes are: bio none The label bio appears in both task definitions but bears different meanings. Under 10-Class, class bio contains information on a person’s birth or death, and under 2-Class it sums up all 9 biographical elements from the 10-Class. 4.2 Machine Learning Methods We experimented with three machine learning methods for classifying sentences. Naïve Bayes The Naïve Bayes classifier is among the most effective algorithms known for learning to classify text documents (Mitchell, 1997), calculating explicit probabilities for hypotheses. Using k features Fj: j = 1, É, k, we assign to a given sentence S the class C: C = argmaxC P(C |F1,F2,...,Fk) It can be expressed using Bayes’ rule, as (Kupiec et al., 1995): P(S ∈ C |F1,F2,...Fk) = P(F1,F2,...Fj |S ∈ C)• P(S ∈ C) P(F1,F2,...Fk) Assuming statistical independence of the features: P(S C C |F1,F2,...Fk) = ∏k P(Fj |S C C) • P(S C C) j=1 We trained on the relative frequency of P(Fj|S∈C) and P(S∈C), with add-one smoothing. This method was used in classifying both the 10- Class and the 2-Class tasks. Support Vector Machine Support </context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Tom Mitchell. 1997. Machine Learning. McGraw Hill, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross J Quinlan</author>
</authors>
<title>C4.5: Programs for machine learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA:</location>
<contexts>
<context position="9883" citStr="Quinlan, 1993" startWordPosition="1475" endWordPosition="1476">VMs) have been shown to be an effective classifier in text categorization. We extend the idea of classifying documents into predefined categories to classifying sentences into one of the two biography categories defined by the 2-Class task. Sentences are categorized based on their biographical saliency (a percentage of clearly identified biography words) and their non-biographical saliency (a percentage of clearly identified non-biography words). We used LIBSVM (Chang and Lin, 2003) for training and testing. Decision Tree (4.5) In addition to SVM, we also used a decision-tree algorithm, C4.5 (Quinlan, 1993), with the same training and testing data as SVM. 4.3 Classification Results The lower performance bound is set by a baseline system that randomly assigns a biographical class given a sentence, for both 10- Class and 2-Class. 2599 testing sentences are from 30 unseen documents. 10-Class Classification The Naïve Bayes classifier was used to perform the 10-Class task. Table 1 shows its performance with various features. P(S C C |F1,F2,...Fk) = ∏kj=1 P(Fj |S C C) •P(S C C) Table 1. Performance of 10-Class sentence classification, using Naïve Bayes Classifier. k LP(F.) i J Since P(Fj) has no role </context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Ross J. Quinlan. 1993. C4.5: Programs for machine learning. San Mateo, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Kathleen McKeown</author>
</authors>
<title>Generating natural language summaries from multiple on-line sources.</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<volume>24</volume>
<issue>3</issue>
<pages>469--500</pages>
<contexts>
<context position="19642" citStr="Radev and McKeown (1998)" startWordPosition="3027" endWordPosition="3030">tant in reflecting the true quality of machinegenerated summaries. There is no automated tool for this purpose currently. We plan to incorporate one for the future development of this work. 6.3 Discussion N-gram recall scores are computed by ROUGE, in addition to ROUGE-L shown here. While cosine similarity and unigram and bigram overlap demonstrate a sufficient measure on content coverage, they are not sensitive on how information is sequenced in the text (Saggion et al., 2002). In evaluating and analyzing MDS results, metrics, such as ROUGE-L, that consider linguistic sequence are essential. Radev and McKeown (1998) point out when summarizing interesting news events from multiple sources, one can expect reports with contradictory and redundant information. An intelligent summarizer should attain as much information as possible, combine it, and present it in the most concise form to the user. When we look at the different attributes in a person’s life reported in news articles, a person is described by the job positions that he/she has held, by education institutions that he/she has attended, and etc. Those data are confirmed biographical information and do not bear the necessary contradiction associated </context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>Dragomir R. Radev, Kathleen McKeown. 1998. Generating natural language summaries from multiple on-line sources. Computational Linguistics 24(3): 469–500 (1998).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Saggion</author>
<author>Dragomir Radev</author>
<author>Simone Teufel</author>
<author>Wai Lam</author>
</authors>
<title>Meta-evaluation of summaries in a cross-lingual environment using content-based metrics.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING’2002,</booktitle>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="17342" citStr="Saggion et al., 2002" startWordPosition="2619" endWordPosition="2622">f the resulting summaries. 6.2 Coverage Evaluation An intrinsic evaluation of biography summary was recently conducted under the guidance of Document Understanding Conference (DUC2004) using the automatic summarization evaluation tool ROUGE (Recall-Oriented Understudy for Gisting Evaluation) by Lin and Hovy (2003). 50 TREC English document clusters, each containing on average 10 news articles, were the input to the system. Summary length was restricted to 665 bytes. Brute force truncation was applied on longer summaries. The ROUGE-L metric is based on Longest Common Subsequence (LCS) overlap (Saggion et al., 2002). Figure 2 shows that our system (86) performs at an equivalent level with the best systems 9 and 10, that is, they both lie within our system’s 95% upper confidence interval. The 2- class classification module was used in generating the answers. The figure also shows the performance data evaluated with lower and higher confidences set at 95%. The performance data are from official DUC results. n ∑ i=1 Cs len(s) 0.55 0.45 0.35 0.25 0.5 0.4 0.3 B E F H G A D C 9 10 11 12 13 86 15 16 17 18 19 20 5 22 23 24 25 26 27 28 29 30 31 ROUGE-L 95% CI Lower 95% CI Higher Figure 2. Official ROUGE performan</context>
<context position="19500" citStr="Saggion et al., 2002" startWordPosition="3007" endWordPosition="3010">ongst the peer systems. Clearly, humans still perform at a level much superior to any system. Measuring fluency and coherence is also important in reflecting the true quality of machinegenerated summaries. There is no automated tool for this purpose currently. We plan to incorporate one for the future development of this work. 6.3 Discussion N-gram recall scores are computed by ROUGE, in addition to ROUGE-L shown here. While cosine similarity and unigram and bigram overlap demonstrate a sufficient measure on content coverage, they are not sensitive on how information is sequenced in the text (Saggion et al., 2002). In evaluating and analyzing MDS results, metrics, such as ROUGE-L, that consider linguistic sequence are essential. Radev and McKeown (1998) point out when summarizing interesting news events from multiple sources, one can expect reports with contradictory and redundant information. An intelligent summarizer should attain as much information as possible, combine it, and present it in the most concise form to the user. When we look at the different attributes in a person’s life reported in news articles, a person is described by the job positions that he/she has held, by education institution</context>
</contexts>
<marker>Saggion, Radev, Teufel, Lam, 2002</marker>
<rawString>Horacio Saggion, Dragomir Radev, Simone Teufel, and Wai Lam. Meta-evaluation of summaries in a cross-lingual environment using content-based metrics. In Proceedings of COLING’2002, Taipei, Taiwan, August 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry Schiffman</author>
<author>Inderjeet Mani</author>
<author>Kristian Concepcion</author>
</authors>
<title>Producing biographical summaries: combining linguistic knowledge with corpus statistics.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL’2001),</booktitle>
<pages>450--457</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New Brunswick, New Jersey:</location>
<contexts>
<context position="4725" citStr="Schiffman et al., 2001" startWordPosition="686" endWordPosition="689">ugh sentence generation. Lin and Hovy (2002) apply a collection of known single-document summarization techniques, cooperating positional and topical information, clustering, etc., and extend them to perform MDS. While many have suggested that conventional MDS systems can be applied to biography generation directly, Mani (2001) illustrates that the added functionality of biographical MDS comes at the expense of a substantial increase in system complexity and is somewhat beyond the capabilities of present day MDS systems. The discussion was based in part on the only known MDS biography system (Schiffman et al., 2001) that uses corpus statistics along with linguistic knowledge to select and merge description of people in news. The focus of this work was on synthesizing succinct descriptions of people by merging appositives from semantic processing using WordNet (Miller, 1995). 3 Corpus Description In order to extract information that is related to a person from a large set of news texts written not exclusively about this person, we need to identify attributes shared among biographies. Biographies share certain standard components. We annotated a corpus of 130 biographies of 12 people (activists, artists, l</context>
<context position="16621" citStr="Schiffman et al., 2001" startWordPosition="2514" endWordPosition="2517">y one sentence that contains the topics “chemotherapy” and “cancer”. It chooses sentence 3, which is not bad, though sentence 1 might be preferable. 6 Evaluation 6.1 Overview Extrinsic and intrinsic evaluations are the two classes of text summarization evaluation methods (Sparck Jones and Galliers, 1996). Measuring content coverage or summary informativeness is an approach commonly used for intrinsic evaluation. It measures how much source content was preserved in the summary. A complete evaluation should include evaluations of the accuracy of components involved in the summarization process (Schiffman et al., 2001). Performance of the sentence classifier was shown in Section 4. Here we will show the performance of the resulting summaries. 6.2 Coverage Evaluation An intrinsic evaluation of biography summary was recently conducted under the guidance of Document Understanding Conference (DUC2004) using the automatic summarization evaluation tool ROUGE (Recall-Oriented Understudy for Gisting Evaluation) by Lin and Hovy (2003). 50 TREC English document clusters, each containing on average 10 news articles, were the input to the system. Summary length was restricted to 665 bytes. Brute force truncation was ap</context>
</contexts>
<marker>Schiffman, Mani, Concepcion, 2001</marker>
<rawString>Barry Schiffman, Inderjeet Mani, and Kristian Concepcion. 2001. Producing biographical summaries: combining linguistic knowledge with corpus statistics. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL’2001), 450–457. New Brunswick, New Jersey: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen SpŠrck Jones</author>
<author>Julia R Galliers</author>
</authors>
<title>Evaluating Natural Language Processing Systems: An Analysis and Review.</title>
<date>1996</date>
<booktitle>Lecture Notes in Artificial Intelligence 1083.</booktitle>
<publisher>Springer.</publisher>
<location>Berlin:</location>
<contexts>
<context position="16303" citStr="Jones and Galliers, 1996" startWordPosition="2469" endWordPosition="2472">summarization systems with (abstract, text) input pairs. Top-scoring sentences selected by the ranking mechanism described in Section 5.2 were the input to this component. The removal process was repeated until the desired summary length was achieved. Applying this method to the Armstrong example, the result leaves only one sentence that contains the topics “chemotherapy” and “cancer”. It chooses sentence 3, which is not bad, though sentence 1 might be preferable. 6 Evaluation 6.1 Overview Extrinsic and intrinsic evaluations are the two classes of text summarization evaluation methods (Sparck Jones and Galliers, 1996). Measuring content coverage or summary informativeness is an approach commonly used for intrinsic evaluation. It measures how much source content was preserved in the summary. A complete evaluation should include evaluations of the accuracy of components involved in the summarization process (Schiffman et al., 2001). Performance of the sentence classifier was shown in Section 4. Here we will show the performance of the resulting summaries. 6.2 Coverage Evaluation An intrinsic evaluation of biography summary was recently conducted under the guidance of Document Understanding Conference (DUC200</context>
</contexts>
<marker>Jones, Galliers, 1996</marker>
<rawString>Karen SpŠrck Jones and Julia R. Galliers. 1996. Evaluating Natural Language Processing Systems: An Analysis and Review. Lecture Notes in Artificial Intelligence 1083. Berlin: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen SpŠrck Jones</author>
</authors>
<title>What might be in a summary? Information Retrieval</title>
<date>1993</date>
<pages>9--26</pages>
<contexts>
<context position="6320" citStr="Jones, 1993" startWordPosition="925" endWordPosition="926">nality&gt;. ... He &lt;bio&gt;was assassinated on April 4, 1968 &lt;/bio&gt;. ... King &lt;education&gt; entered the Boston University as a doctoral student &lt;/education&gt;. ... In all, 3579 biography-related phrases were identified and recorded for the collection, among them 321 bio, 423 fame, 114 personality, 465 personal, 293 social, 246 education, 95 nationality, 292 scandal, and 1330 work. We then used 100 biographies for training and 30 for testing the classification module. 4 Sentence Classification Relating to human practice on summarizing, three main points are relevant to aid the automation process (SpŠrck Jones, 1993). The first is a strong emphasis on particular purposes, e.g., abstracting or extracting articles of particular genres. The second is the drafting, writing, and revision cycle in constructing a summary. Essentially as a consequence of these first two points, the summarizing process can be guided by the use of checklists. The idea of a checklist is especially useful for the purpose of generating biographical summaries because a complete biography should contain various aspects of a person’s life. From a careful analysis conducted while constructing the biography corpus, we believe that the chec</context>
</contexts>
<marker>Jones, 1993</marker>
<rawString>Karen SpŠrck Jones. 1993. What might be in a summary? Information Retrieval 1993: 9Ð26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Eduard Hovy</author>
</authors>
<title>A web-trained extraction summarization system.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT-NAACL</booktitle>
<pages>284--290</pages>
<contexts>
<context position="3656" citStr="Zhou and Hovy (2003)" startWordPosition="533" endWordPosition="536">nerating summaries by extraction, which is finding a subset of the document that is indicative of its contents (Kupiec et al., 1995) using “shallow” linguistic analysis and statistics. The other influence is the exploration of “deeper” knowledge-based methods for condensing information. Knight and Marcu (2000) equate summarization with compression at sentence level to achieve grammaticality and information capture, and push a step beyond sentence extraction. Many systems use machinelearning methods to learn from readily aligned corpora of scientific articles and their corresponding abstracts. Zhou and Hovy (2003) show a summarization system trained from automatically obtained text-summary alignments obeying the chronological occurrences of news events. MDS poses more challenges in assessing similarities and differences among the set of documents. The simple idea of extract-andconcatenate does not respond to problems arisen from coherence and cohesion. Barzilay et al. (1999) introduce a combination of extracted similar phrases and a reformulation through sentence generation. Lin and Hovy (2002) apply a collection of known single-document summarization techniques, cooperating positional and topical info</context>
</contexts>
<marker>Zhou, Hovy, 2003</marker>
<rawString>Liang Zhou and Eduard Hovy. A web-trained extraction summarization system. In Proceedings of the Human Language Technology Conference (HLT-NAACL 2003), pages 284–290.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>