<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000100">
<title confidence="0.998998">
A Cascaded Linear Model for Joint Chinese Word Segmentation and
Part-of-Speech Tagging
</title>
<author confidence="0.99822">
Wenbin Jiang † Liang Huang ‡ Qun Liu † Yajuan L¨u †
</author>
<affiliation confidence="0.888434666666667">
†Key Lab. of Intelligent Information Processing ‡Department of Computer &amp; Information Science
Institute of Computing Technology University of Pennsylvania
Chinese Academy of Sciences Levine Hall, 3330 Walnut Street
</affiliation>
<address confidence="0.755179">
P.O. Box 2704, Beijing 100190, China Philadelphia, PA 19104, USA
</address>
<email confidence="0.999148">
jiangwenbin@ict.ac.cn lhuang3@cis.upenn.edu
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999236875">
We propose a cascaded linear model for
joint Chinese word segmentation and part-
of-speech tagging. With a character-based
perceptron as the core, combined with real-
valued features such as language models, the
cascaded model is able to efficiently uti-
lize knowledge sources that are inconvenient
to incorporate into the perceptron directly.
Experiments show that the cascaded model
achieves improved accuracies on both seg-
mentation only and joint segmentation and
part-of-speech tagging. On the Penn Chinese
Treebank 5.0, we obtain an error reduction of
18.5% on segmentation and 12% on joint seg-
mentation and part-of-speech tagging over the
perceptron-only baseline.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994693877551">
Word segmentation and part-of-speech (POS) tag-
ging are important tasks in computer processing of
Chinese and other Asian languages. Several mod-
els were introduced for these problems, for example,
the Hidden Markov Model (HMM) (Rabiner, 1989),
Maximum Entropy Model (ME) (Ratnaparkhi and
Adwait, 1996), and Conditional Random Fields
(CRFs) (Lafferty et al., 2001). CRFs have the ad-
vantage of flexibility in representing features com-
pared to generative ones such as HMM, and usually
behaves the best in the two tasks. Another widely
used discriminative method is the perceptron algo-
rithm (Collins, 2002), which achieves comparable
performance to CRFs with much faster training, so
we base this work on the perceptron.
To segment and tag a character sequence, there
are two strategies to choose: performing POS tag-
ging following segmentation; or joint segmentation
and POS tagging (Joint S&amp;T). Since the typical ap-
proach of discriminative models treats segmentation
as a labelling problem by assigning each character
a boundary tag (Xue and Shen, 2003), Joint S&amp;T
can be conducted in a labelling fashion by expand-
ing boundary tags to include POS information (Ng
and Low, 2004). Compared to performing segmen-
tation and POS tagging one at a time, Joint S&amp;T can
achieve higher accuracy not only on segmentation
but also on POS tagging (Ng and Low, 2004). Be-
sides the usual character-based features, additional
features dependent on POS’s or words can also be
employed to improve the performance. However, as
such features are generated dynamically during the
decoding procedure, two limitation arise: on the one
hand, the amount of parameters increases rapidly,
which is apt to overfit on training corpus; on the
other hand, exact inference by dynamic program-
ming is intractable because the current predication
relies on the results of prior predications. As a result,
many theoretically useful features such as higher-
order word or POS n-grams are difficult to be in-
corporated in the model efficiently.
To cope with this problem, we propose a cascaded
linear model inspired by the log-linear model (Och
and Ney, 2004) widely used in statistical machine
translation to incorporate different kinds of knowl-
edge sources. Shown in Figure 1, the cascaded
model has a two-layer architecture, with a character-
based perceptron as the core combined with other
real-valued features such as language models. We
</bodyText>
<page confidence="0.974022">
897
</page>
<note confidence="0.718043">
Proceedings of ACL-08: HLT, pages 897–904,
</note>
<page confidence="0.411121">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<figureCaption confidence="0.999857">
Figure 1: Structure of Cascaded Linear Model. |R |denotes the scale of the feature space of the core perceptron.
</figureCaption>
<figure confidence="0.9186014">
f1
f2
Core
Linear Model
(Perceptron)
</figure>
<equation confidence="0.967225571428571">
g1 = Ei αi X fi
Word LM: g2 = Pwlm(W) g2
POS LM: g3 = Ptlm(T) g3
Labelling: g4 = P(T|W) g4
Generating: g5 = P(W|T) g5
Length: g6 = |W  |g6
g1
α
f|R|
w
Outside-layer
Linear Model
S = Ej wj X gj
S
</equation>
<bodyText confidence="0.981417214285714">
will describe it in detail in Section 4. In this ar-
chitecture, knowledge sources that are intractable to
incorporate into the perceptron, can be easily incor-
porated into the outside linear model. In addition,
as these knowledge sources are regarded as separate
features, we can train their corresponding models in-
dependently with each other. This is an interesting
approach when the training corpus is large as it re-
duces the time and space consumption. Experiments
show that our cascaded model can utilize different
knowledge sources effectively and obtain accuracy
improvements on both segmentation and Joint S&amp;T.
2 Segmentation and POS Tagging
Given a Chinese character sequence:
</bodyText>
<equation confidence="0.757260666666667">
C1:n = C1 C2 .. Cn
the segmentation result can be depicted as:
C1:e1 Ce1+1:e2 .. Cem−1+1:em
</equation>
<bodyText confidence="0.993961">
while the segmentation and POS tagging result can
be depicted as:
</bodyText>
<equation confidence="0.582963">
C1:e1/t1 Ce1+1:e2/t2 .. Cem−1+1:em/tm
</equation>
<bodyText confidence="0.9974525">
Here, Ci (i = L.n) denotes Chinese character,
ti (i = L.m) denotes POS tag, and Cl:r (l &lt; r)
denotes character sequence ranges from Cl to Cr.
We can see that segmentation and POS tagging task
is to divide a character sequence into several subse-
quences and label each of them a POS tag.
It is a better idea to perform segmentation and
POS tagging jointly in a uniform framework. Ac-
cording to Ng and Low (2004), the segmentation
task can be transformed to a tagging problem by as-
signing each character a boundary tag of the follow-
ing four types:
</bodyText>
<listItem confidence="0.999992">
• b: the begin of the word
• m: the middle of the word
• e: the end of the word
• s: a single-character word
</listItem>
<bodyText confidence="0.999774705882353">
We can extract segmentation result by splitting
the labelled result into subsequences of pattern s or
bm*e which denote single-character word and multi-
character word respectively. In order to perform
POS tagging at the same time, we expand boundary
tags to include POS information by attaching a POS
to the tail of a boundary tag as a postfix following
Ng and Low (2004). As each tag is now composed
of a boundary part and a POS part, the joint S&amp;T
problem is transformed to a uniform boundary-POS
labelling problem. A subsequence of boundary-POS
labelling result indicates a word with POS t only if
the boundary tag sequence composed of its bound-
ary part conforms to s or bm*e style, and all POS
tags in its POS part equal to t. For example, a tag
sequence b NN m NN e NN represents a three-
character word with POS tag NN.
</bodyText>
<sectionHeader confidence="0.984888" genericHeader="method">
3 The Perceptron
</sectionHeader>
<bodyText confidence="0.997681333333333">
The perceptron algorithm introduced into NLP by
Collins (2002), is a simple but effective discrimina-
tive training method. It has comparable performance
</bodyText>
<page confidence="0.997507">
898
</page>
<table confidence="0.999325125">
Non-lexical-target Instances
Cn (n = −2..2) C−2=-F, C−1=..., C0=U, C1=/, C2=iF7
CnCn+1 (n = −2..1) C−2C−1=-F..., C−1C0=...U, C0C1=U/, C1C2=/�
C−1C1 C−1C1=.../
Lexical-target Instances
C0Cn (n = −2..2) C0C−2=U-F, C0C−1=U..., C0C0=UU, C0C1=U/, C0C2=UiFr
C0CnCn+1 (n = −2..1) C0C−2C−1=U-F..., C0C−1C0=U...U, C0C0C1=UU/, C0C1C2=U/iFr
C0C−1C1 C0C−1C1 = U.../
</table>
<tableCaption confidence="0.999859">
Table 1: Feature templates and instances. Suppose we are considering the third character ”U” in ”-F... U /ir”.
</tableCaption>
<bodyText confidence="0.9999064">
to CRFs, while with much faster training. The per-
ceptron has been used in many NLP tasks, such as
POS tagging (Collins, 2002), Chinese word seg-
mentation (Ng and Low, 2004; Zhang and Clark,
2007) and so on. We trained a character-based per-
ceptron for Chinese Joint S&amp;T, and found that the
perceptron itself could achieve considerably high ac-
curacy on segmentation and Joint S&amp;T. In following
subsections, we describe the feature templates and
the perceptron training algorithm.
</bodyText>
<subsectionHeader confidence="0.998796">
3.1 Feature Templates
</subsectionHeader>
<bodyText confidence="0.99965664">
The feature templates we adopted are selected from
those of Ng and Low (2004). To compare with oth-
ers conveniently, we excluded the ones forbidden by
the close test regulation of SIGHAN, for example,
Pu(C0), indicating whether character C0 is a punc-
tuation.
All feature templates and their instances are
shown in Table 1. C represents a Chinese char-
acter while the subscript of C indicates its posi-
tion in the sentence relative to the current charac-
ter (it has the subscript 0). Templates immediately
borrowed from Ng and Low (2004) are listed in
the upper column named non-lexical-target. We
called them non-lexical-target because predications
derived from them can predicate without consider-
ing the current character C0. Templates in the col-
umn below are expanded from the upper ones. We
add a field C0 to each template in the upper col-
umn, so that it can carry out predication according
to not only the context but also the current char-
acter itself. As predications generated from such
templates depend on the current character, we name
these templates lexical-target. Note that the tem-
plates of Ng and Low (2004) have already con-
tained some lexical-target ones. With the two kinds
</bodyText>
<listItem confidence="0.935747777777778">
Algorithm 1 Perceptron training algorithm.
1: Input: Training examples (xi, yi)
2: α�← 0
3: for t ← 1 .. T do
4: for i ← 1 .. N do
5: zi ← argmax,∈GEN(x;) 4)(xi, z) · α�
6: if zi =6 yi then
7: α� ← α� + 4)(xi, yi) − 4)(xi, zi)
8: Output: Parameters α�
</listItem>
<bodyText confidence="0.99876725">
of predications, the perceptron model will do exact
predicating to the best of its ability, and can back
off to approximately predicating if exact predicating
fails.
</bodyText>
<subsectionHeader confidence="0.999549">
3.2 Training Algorithm
</subsectionHeader>
<bodyText confidence="0.999843142857143">
We adopt the perceptron training algorithm of
Collins (2002) to learn a discriminative model map-
ping from inputs x ∈ X to outputs y ∈ Y , where X
is the set of sentences in the training corpus and Y
is the set of corresponding labelled results. Follow-
ing Collins, we use a function GEN(x) generating
all candidate results of an input x , a representation
4) mapping each training example (x, y) ∈ X × Y
to a feature vector 4)(x, y) ∈ Rd, and a parameter
vector α� ∈ Rd corresponding to the feature vector.
d means the dimension of the vector space, it equals
to the amount of features in the model. For an input
character sequence x, we aim to find an output F(x)
satisfying:
</bodyText>
<equation confidence="0.866375333333333">
F(x) = argmax 4)(x, y) · α� (1)
yEGEN(x)
4)(x, y) · α� represents the inner product of feature
</equation>
<bodyText confidence="0.962291333333333">
vector 4)(x, y) and the parameter vector a. We used
the algorithm depicted in Algorithm 1 to tune the
parameter vector a.
</bodyText>
<page confidence="0.993941">
899
</page>
<bodyText confidence="0.9989645">
To alleviate overfitting on the training examples,
we use the refinement strategy called “averaged pa-
rameters” (Collins, 2002) to the algorithm in Algo-
rithm 1.
</bodyText>
<sectionHeader confidence="0.993221" genericHeader="method">
4 Cascaded Linear Model
</sectionHeader>
<bodyText confidence="0.999938048780488">
In theory, any useful knowledge can be incorporated
into the perceptron directly, besides the character-
based features already adopted. Additional features
most widely used are related to word or POS n-
grams. However, such features are generated dy-
namically during the decoding procedure so that
the feature space enlarges much more rapidly. Fig-
ure 2 shows the growing tendency of feature space
with the introduction of these features as well as the
character-based ones. We noticed that the templates
related to word unigrams and bigrams bring to the
feature space an enlargement much rapider than the
character-base ones, not to mention the higher-order
grams such as trigrams or 4-grams. In addition, even
though these higher grams were managed to be used,
there still remains another problem: as the current
predication relies on the results of prior ones, the
decoding procedure has to resort to approximate in-
ference by maintaining a list of N-best candidates at
each predication position, which evokes a potential
risk to depress the training.
To alleviate the drawbacks, we propose a cas-
caded linear model. It has a two-layer architec-
ture, with a perceptron as the core and another linear
model as the outside-layer. Instead of incorporat-
ing all features into the perceptron directly, we first
trained the perceptron using character-based fea-
tures, and several other sub-models using additional
ones such as word or POS n-grams, then trained the
outside-layer linear model using the outputs of these
sub-models, including the perceptron. Since the per-
ceptron is fixed during the second training step, the
whole training procedure need relative small time
and memory cost.
The outside-layer linear model, similar to those
in SMT, can synthetically utilize different knowl-
edge sources to conduct more accurate comparison
between candidates. In this layer, each knowledge
source is treated as a feature with a corresponding
weight denoting its relative importance. Suppose we
have n features gj (j = 1..n) coupled with n corre-
</bodyText>
<figure confidence="0.997613">
3.3e+006
3e+006
2.7e+006
2.4e+006
2.1e+006
1.8e+006
1.5e+006
1.2e+006
900000
600000
300000
0
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22
Introduction of features
</figure>
<figureCaption confidence="0.993818">
Figure 2: Feature space growing curve. The horizontal
scope X[i:j] denotes the introduction of different tem-
plates. X[0:5]: Cn (n = −2..2); X[5:9]: CnCn+1 (n =
−2..1); X[9:10]: C_1C1; X[10:15]: C0Cn (n =
−2..2); X[15:19]: C0CnCn+1 (n = −2..1); X[19:20]:
C0C_1C1; X[20:21]: W0; X[21:22]: W_1W0. W0 de-
notes the current considering word, while W_1 denotes
the word in front of W0. All the data are collected from
the training procedure on MSR corpus of SIGHAN bake-
off 2.
</figureCaption>
<bodyText confidence="0.999089">
sponding weights wj (j = 1..n), each feature gj
gives a score gj(r) to a candidate r, then the total
score of r is given by:
</bodyText>
<equation confidence="0.940838">
5(r) = � wj x gj(r) (2)
j��..n
</equation>
<bodyText confidence="0.9970205">
The decoding procedure aims to find the candidate
r* with the highest score:
</bodyText>
<equation confidence="0.971641">
r* = argmax 5(r) (3)
r
</equation>
<bodyText confidence="0.999964444444445">
While the mission of the training procedure is to
tune the weights wj(j = 1..n) to guarantee that the
candidate r with the highest score happens to be the
best result with a high probability.
As all the sub-models, including the perceptron,
are regarded as separate features of the outside-layer
linear model, we can train them respectively with
special algorithms. In our experiments we trained
a 3-gram word language model measuring the flu-
ency of the segmentation result, a 4-gram POS lan-
guage model functioning as the product of state-
transition probabilities in HMM, and a word-POS
co-occurrence model describing how much probably
a word sequence coexists with a POS sequence. As
shown in Figure 1, the character-based perceptron is
used as the inside-layer linear model and sends its
output to the outside-layer. Besides the output of the
perceptron, the outside-layer also receive the outputs
</bodyText>
<figure confidence="0.682137">
Feature space
growing curve
</figure>
<page confidence="0.948722">
900
</page>
<bodyText confidence="0.999917333333333">
of the word LM, the POS LM, the co-occurrence
model and a word count penalty which is similar to
the translation length penalty in SMT.
</bodyText>
<subsectionHeader confidence="0.929272">
4.1 Language Model
</subsectionHeader>
<bodyText confidence="0.998874666666667">
Language model (LM) provides linguistic probabil-
ities of a word sequence. It is an important measure
of fluency of the translation in SMT. Formally, an
n-gram word LM approximates the probability of a
word sequence W = w1:m with the following prod-
uct:
</bodyText>
<equation confidence="0.9981325">
Pwlm(W) = Hm Pr(wi|wmax(0,i−n+1):i−1) (4)
i=1
Similarly, the n-gram POS LM of a POS sequence
T = t1:m is:
Ptlm(T) = Hm Pr(ti|tmax(0,i−n+1):i−1) (5)
i=1
</equation>
<bodyText confidence="0.9982175">
Notice that a bi-gram POS LM functions as the prod-
uct of transition probabilities in HMM.
</bodyText>
<subsectionHeader confidence="0.594861">
4.2 Word-POS Co-occurrence Model
</subsectionHeader>
<bodyText confidence="0.9999385">
Given a training corpus with POS tags, we can train
a word-POS co-occurrence model to approximate
the probability that the word sequence of the la-
belled result co-exists with its corresponding POS
sequence. Using W = w1:m to denote the word se-
quence, T = t1:m to denote the corresponding POS
sequence, P (T |W) to denote the probability that W
is labelled as T, and P(W|T) to denote the prob-
ability that T generates W, we can define the co-
occurrence model as follows:
</bodyText>
<equation confidence="0.998368">
Co(W,T) = P(T|W)λ t × P(W|T)λt- (6)
</equation>
<bodyText confidence="0.999813714285714">
λwt and λtw denote the corresponding weights of the
two components.
Suppose the conditional probability Pr(t|w) de-
scribes the probability that the word w is labelled as
the POS t, while Pr(w|t) describes the probability
that the POS t generates the word w, then P(T|W)
can be approximated by:
</bodyText>
<equation confidence="0.949212428571428">
m
P(T|W) ≃ H Pr(tk|wk) (7)
k=1
And P(W|T) can be approximated by:
m
P(W|T) ≃ H Pr(wk|tk) (8)
k=1
</equation>
<bodyText confidence="0.986144666666667">
Pr(w|t) and Pr(t|w) can be easily acquired by
Maximum Likelihood Estimates (MLE) over the
corpus. For instance, if the word w appears N times
in training corpus and is labelled as POS t for n
times, the probability Pr(t|w) can be estimated by
the formula below:
</bodyText>
<equation confidence="0.875677333333333">
n
Pr(t|w) ≃ (9)
N
</equation>
<bodyText confidence="0.999937375">
The probability Pr(w|t) could be estimated through
the same approach.
To facilitate tuning the weights, we use two com-
ponents of the co-occurrence model Co(W,T) to
represent the co-occurrence probability of W and T,
rather than use Co(W, T) itself. In the rest of the
paper, we will call them labelling model and gener-
ating model respectively.
</bodyText>
<sectionHeader confidence="0.998667" genericHeader="method">
5 Decoder
</sectionHeader>
<bodyText confidence="0.999986916666667">
Sequence segmentation and labelling problem can
be solved through a viterbi style decoding proce-
dure. In Chinese Joint S&amp;T, the mission of the de-
coder is to find the boundary-POS labelled sequence
with the highest score. Given a Chinese character
sequence C1:n, the decoding procedure can proceed
in a left-right fashion with a dynamic programming
approach. By maintaining a stack of size N at each
position i of the sequence, we can preserve the top N
best candidate labelled results of subsequence C1:i
during decoding. At each position i, we enumer-
ate all possible word-POS pairs by assigning each
POS to each possible word formed from the charac-
ter subsequence spanning length l = L. min(i, K)
(K is assigned 20 in all our experiments) and ending
at position i, then we derive all candidate results by
attaching each word-POS pair p (of length l) to the
tail of each candidate result at the prior position of p
(position i−l), and select for position i a N-best list
of candidate results from all these candidates. When
we derive a candidate result from a word-POS pair
p and a candidate q at prior position of p, we cal-
culate the scores of the word LM, the POS LM, the
labelling probability and the generating probability,
</bodyText>
<page confidence="0.99538">
901
</page>
<bodyText confidence="0.481684">
Algorithm 2 Decoding algorithm.
</bodyText>
<listItem confidence="0.99562325">
1: Input: character sequence C1:,,,
2: for i +— 1 .. n do
3: G +— 0
4: for l +— 1 .. min(i, K) do
5: w +— Ci−l+1:i
6: for t E POS do
7: p +— label w as t
8: for q E V[i — l] do
9: append D(q, p) to G
10: sort G
11: V[i] +— G[1 : N]
12: Output: n-best results V[n]
</listItem>
<bodyText confidence="0.999913352941177">
as well as the score of the perceptron model. In ad-
dition, we add the score of the word count penalty as
another feature to alleviate the tendency of LMs to
favor shorter candidates. By equation 2, we can syn-
thetically evaluate all these scores to perform more
accurately comparing between candidates.
Algorithm 2 shows the decoding algorithm.
Lines 3 — 11 generate a N-best list for each char-
acter position i. Line 4 scans words of all possible
lengths l (l = 1.. min(i, K), where i points to the
current considering character). Line 6 enumerates
all POS’s for the word w spanning length l and end-
ing at position i. Line 8 considers each candidate
result in N-best list at prior position of the current
word. Function D derives the candidate result from
the word-POS pair p and the candidate q at prior po-
sition of p.
</bodyText>
<sectionHeader confidence="0.998807" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999905">
We reported results from two set of experiments.
The first was conducted to test the performance of
the perceptron on segmentation on the corpus from
SIGHAN Bakeoff 2, including the Academia Sinica
Corpus (AS), the Hong Kong City University Cor-
pus (CityU), the Peking University Corpus (PKU)
and the Microsoft Research Corpus (MSR). The sec-
ond was conducted on the Penn Chinese Treebank
5.0 (CTB5.0) to test the performance of the cascaded
model on segmentation and Joint S&amp;T. In all ex-
periments, we use the averaged parameters for the
perceptrons, and F-measure as the accuracy mea-
sure. With precision P and recall R, the balance
F-measure is defined as: F = 2PR/(P + R).
</bodyText>
<figure confidence="0.725251666666667">
Perceptron Learning Curve
0 1 2 3 4 5 6 7 8 9 10
number of iterations
</figure>
<figureCaption confidence="0.877172">
Figure 3: Averaged perceptron learning curves with Non-
lexical-target and Lexical-target feature templates.
</figureCaption>
<table confidence="0.99970675">
AS CityU PKU MSR
SIGHAN best 0.952 0.943 0.950 0.964
Zhang &amp; Clark 0.946 0.951 0.945 0.972
our model 0.954 0.958 0.940 0.975
</table>
<tableCaption confidence="0.993863">
Table 2: F-measure on SIGHAN bakeoff 2. SIGHAN
best: best scores SIGHAN reported on the four corpus,
cited from Zhang and Clark (2007).
</tableCaption>
<subsectionHeader confidence="0.995918">
6.1 Experiments on SIGHAN Bakeoff
</subsectionHeader>
<bodyText confidence="0.99994984">
For convenience of comparing with others, we focus
only on the close test, which means that any extra
resource is forbidden except the designated train-
ing corpus. In order to test the performance of the
lexical-target templates and meanwhile determine
the best iterations over the training corpus, we ran-
domly chosen 2, 000 shorter sentences (less than 50
words) as the development set and the rest as the
training set (84, 294 sentences), then trained a per-
ceptron model named NON-LEX using only non-
lexical-target features and another named LEX us-
ing both the two kinds of features. Figure 3 shows
their learning curves depicting the F-measure on the
development set after 1 to 10 training iterations. We
found that LEX outperforms NON-LEX with a mar-
gin of about 0.002 at each iteration, and its learn-
ing curve reaches a tableland at iteration 7. Then
we trained LEX on each of the four corpora for 7
iterations. Test results listed in Table 2 shows that
this model obtains higher accuracy than the best of
SIGHAN Bakeoff 2 in three corpora (AS, CityU
and MSR). On the three corpora, it also outper-
formed the word-based perceptron model of Zhang
and Clark (2007). However, the accuracy on PKU
corpus is obvious lower than the best score SIGHAN
</bodyText>
<figure confidence="0.993536230769231">
F-meassure
0.984
0.982
0.978
0.976
0.974
0.972
0.968
0.966
0.98
0.97
Non-lex + avg
Lex + avg
</figure>
<page confidence="0.987821">
902
</page>
<table confidence="0.99981325">
Training setting Test task F-measure
POS- Segmentation 0.971
POS+ Segmentation 0.973
POS+ Joint S&amp;T 0.925
</table>
<tableCaption confidence="0.998744333333333">
Table 3: F-measure on segmentation and Joint S&amp;T of
perceptrons. POS-: perceptron trained without POS,
POS+: perceptron trained with POS.
</tableCaption>
<bodyText confidence="0.9971365">
reported, we need to conduct further research on this
problem.
</bodyText>
<subsectionHeader confidence="0.99671">
6.2 Experiments on CTB5.0
</subsectionHeader>
<bodyText confidence="0.999879628571429">
We turned to experiments on CTB 5.0 to test the per-
formance of the cascaded model. According to the
usual practice in syntactic analysis, we choose chap-
ters 1 − 260 (18074 sentences) as training set, chap-
ter 271 − 300 (348 sentences) as test set and chapter
301 − 325 (350 sentences) as development set.
At the first step, we conducted a group of contrast-
ing experiments on the core perceptron, the first con-
centrated on the segmentation regardless of the POS
information and reported the F-measure on segmen-
tation only, while the second performed Joint S&amp;T
using POS information and reported the F-measure
both on segmentation and on Joint S&amp;T. Note that
the accuracy of Joint S&amp;T means that a word-POS
pair is recognized only if both the boundary tags and
the POS’s are correctly labelled.
The evaluation results are shown in Table 3. We
find that Joint S&amp;T can also improve the segmen-
tation accuracy. However, the F-measure on Joint
S&amp;T is obvious lower, about a rate of 95% to the
F-measure on segmentation. Similar trend appeared
in experiments of Ng and Low (2004), where they
conducted experiments on CTB 3.0 and achieved F-
measure 0.919 on Joint S&amp;T, a ratio of 96% to the
F-measure 0.952 on segmentation.
As the next step, a group of experiments were
conducted to investigate how well the cascaded lin-
ear model performs. Here the core perceptron was
just the POS+ model in experiments above. Be-
sides this perceptron, other sub-models are trained
and used as additional features of the outside-layer
linear model. We used SRI Language Modelling
Toolkit (Stolcke and Andreas, 2002) to train a 3-
gram word LM with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1998), and a 4-gram POS
</bodyText>
<table confidence="0.999834375">
Features Segmentation F1 Joint S&amp;T F1
All 0.9785 0.9341
All - PER 0.9049 0.8432
All - WLM 0.9785 0.9340
All - PLM 0.9752 0.9270
All - GPR 0.9774 0.9329
All - LPR 0.9765 0.9321
All - LEN 0.9772 0.9325
</table>
<tableCaption confidence="0.98906">
Table 4: Contribution of each feture. ALL: all features,
PER: perceptron model, WLM: word language model,
PLM: POS language model, GPR: generating model,
LPR: labelling model, LEN: word count penalty.
</tableCaption>
<bodyText confidence="0.999719457142857">
LM with Witten-Bell smoothing, and we trained
a word-POS co-occurrence model simply by MLE
without smoothing. To obtain their corresponding
weights, we adapted the minimum-error-rate train-
ing algorithm (Och, 2003) to train the outside-layer
model. In order to inspect how much improvement
each feature brings into the cascaded model, every
time we removed a feature while retaining others,
then retrained the model and tested its performance
on the test set.
Table 4 shows experiments results. We find that
the cascaded model achieves a F-measure increment
of about 0.5 points on segmentation and about 0.9
points on Joint S&amp;T, over the perceptron-only model
POS+. We also find that the perceptron model func-
tions as the kernel of the outside-layer linear model.
Without the perceptron, the cascaded model (if we
can still call it “cascaded”) performs poorly on both
segmentation and Joint S&amp;T. Among other features,
the 4-gram POS LM plays the most important role,
removing this feature causes F-measure decrement
of 0.33 points on segmentation and 0.71 points on
Joint S&amp;T. Another important feature is the labelling
model. Without it, the F-measure on segmentation
and Joint S&amp;T both suffer a decrement of 0.2 points.
The generating model, which functions as that in
HMM, brings an improvement of about 0.1 points
to each test item. However unlike the three fea-
tures, the word LM brings very tiny improvement.
We suppose that the character-based features used
in the perceptron play a similar role as the lower-
order word LM, and it would be helpful if we train
a higher-order word LM on a larger scale corpus.
Finally, the word count penalty gives improvement
to the cascaded model, 0.13 points on segmentation
</bodyText>
<page confidence="0.995759">
903
</page>
<bodyText confidence="0.998361555555556">
and 0.16 points on Joint S&amp;T.
In summary, the cascaded model can utilize these
knowledge sources effectively, without causing the
feature space of the percptron becoming even larger.
Experimental results show that, it achieves obvious
improvement over the perceptron-only model, about
from 0.973 to 0.978 on segmentation, and from
0.925 to 0.934 on Joint S&amp;T, with error reductions
of 18.5% and 12% respectively.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999996347826087">
We proposed a cascaded linear model for Chinese
Joint S&amp;T. Under this model, many knowledge
sources that may be intractable to be incorporated
into the perceptron directly, can be utilized effec-
tively in the outside-layer linear model. This is a
substitute method to use both local and non-local
features, and it would be especially useful when the
training corpus is very large.
However, can the perceptron incorporate all the
knowledge used in the outside-layer linear model?
If this cascaded linear model were chosen, could
more accurate generative models (LMs, word-POS
co-occurrence model) be obtained by training on
large scale corpus even if the corpus is not correctly
labelled entirely, or by self-training on raw corpus in
a similar approach to that of McClosky (2006)? In
addition, all knowledge sources we used in the core
perceptron and the outside-layer linear model come
from the training corpus, whereas many open knowl-
edge sources (lexicon etc.) can be used to improve
performance (Ng and Low, 2004). How can we uti-
lize these knowledge sources effectively? We will
investigate these problems in the following work.
</bodyText>
<sectionHeader confidence="0.9736" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99930075">
This work was done while L. H. was visiting
CAS/ICT. The authors were supported by National
Natural Science Foundation of China, Contracts
60736014 and 60573188, and 863 State Key Project
No. 2006AA010108 (W. J., Q. L., and Y. L.), and by
NSF ITR EIA-0205456 (L. H.). We would also like
to Hwee-Tou Ng for sharing his code, and Yang Liu
and Yun Huang for suggestions.
</bodyText>
<sectionHeader confidence="0.996352" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999620853658536">
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1–8, Philadelphia, USA.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of the 18th ICML, pages 282–289, Mas-
sachusetts, USA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings ofACL 2006.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417–449.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings ofACL
2003, pages 160–167.
Lawrence. R. Rabiner. 1989. A tutorial on hidden
markov models and selected applications in speech
recognition. In Proceedings ofIEEE, pages 257–286.
Ratnaparkhi and Adwait. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of the Empirical
Methods in Natural Language Processing Conference.
Stolcke and Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 311–318.
Nianwen Xue and Libin Shen. 2003. Chinese word seg-
mentation as lmr tagging. In Proceedings of SIGHAN
Workshop.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings ofACL 2007.
</reference>
<page confidence="0.998561">
904
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953390">
<title confidence="0.99394">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</title>
<author confidence="0.996484">Jiang Huang Liu L¨u</author>
<affiliation confidence="0.999417">Lab. of Intelligent Information Processing of Computer &amp; Information Science Institute of Computing Technology University of Pennsylvania</affiliation>
<address confidence="0.999566">Chinese Academy of Sciences Levine Hall, 3330 Walnut Street P.O. Box 2704, Beijing 100190, China Philadelphia, PA 19104, USA</address>
<email confidence="0.994002">jiangwenbin@ict.ac.cnlhuang3@cis.upenn.edu</email>
<abstract confidence="0.998529176470588">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging. With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University Center for Research in Computing Technology.</institution>
<contexts>
<context position="23144" citStr="Chen and Goodman, 1998" startWordPosition="3934" endWordPosition="3937">f Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&amp;T, a ratio of 96% to the F-measure 0.952 on segmentation. As the next step, a group of experiments were conducted to investigate how well the cascaded linear model performs. Here the core perceptron was just the POS+ model in experiments above. Besides this perceptron, other sub-models are trained and used as additional features of the outside-layer linear model. We used SRI Language Modelling Toolkit (Stolcke and Andreas, 2002) to train a 3- gram word LM with modified Kneser-Ney smoothing (Chen and Goodman, 1998), and a 4-gram POS Features Segmentation F1 Joint S&amp;T F1 All 0.9785 0.9341 All - PER 0.9049 0.8432 All - WLM 0.9785 0.9340 All - PLM 0.9752 0.9270 All - GPR 0.9774 0.9329 All - LPR 0.9765 0.9321 All - LEN 0.9772 0.9325 Table 4: Contribution of each feture. ALL: all features, PER: perceptron model, WLM: word language model, PLM: POS language model, GPR: generating model, LPR: labelling model, LEN: word count penalty. LM with Witten-Bell smoothing, and we trained a word-POS co-occurrence model simply by MLE without smoothing. To obtain their corresponding weights, we adapted the minimum-error-ra</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University Center for Research in Computing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="1754" citStr="Collins, 2002" startWordPosition="256" endWordPosition="257">Introduction Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages. Several models were introduced for these problems, for example, the Hidden Markov Model (HMM) (Rabiner, 1989), Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), and Conditional Random Fields (CRFs) (Lafferty et al., 2001). CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM, and usually behaves the best in the two tasks. Another widely used discriminative method is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron. To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&amp;T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004). Compared to performing seg</context>
<context position="6453" citStr="Collins (2002)" startWordPosition="1060" endWordPosition="1061">ching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004). As each tag is now composed of a boundary part and a POS part, the joint S&amp;T problem is transformed to a uniform boundary-POS labelling problem. A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN. 3 The Perceptron The perceptron algorithm introduced into NLP by Collins (2002), is a simple but effective discriminative training method. It has comparable performance 898 Non-lexical-target Instances Cn (n = −2..2) C−2=-F, C−1=..., C0=U, C1=/, C2=iF7 CnCn+1 (n = −2..1) C−2C−1=-F..., C−1C0=...U, C0C1=U/, C1C2=/� C−1C1 C−1C1=.../ Lexical-target Instances C0Cn (n = −2..2) C0C−2=U-F, C0C−1=U..., C0C0=UU, C0C1=U/, C0C2=UiFr C0CnCn+1 (n = −2..1) C0C−2C−1=U-F..., C0C−1C0=U...U, C0C0C1=UU/, C0C1C2=U/iFr C0C−1C1 C0C−1C1 = U.../ Table 1: Feature templates and instances. Suppose we are considering the third character ”U” in ”-F... U /ir”. to CRFs, while with much faster training.</context>
<context position="9197" citStr="Collins (2002)" startWordPosition="1512" endWordPosition="1513">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones. With the two kinds Algorithm 1 Perceptron training algorithm. 1: Input: Training examples (xi, yi) 2: α�← 0 3: for t ← 1 .. T do 4: for i ← 1 .. N do 5: zi ← argmax,∈GEN(x;) 4)(xi, z) · α� 6: if zi =6 yi then 7: α� ← α� + 4)(xi, yi) − 4)(xi, zi) 8: Output: Parameters α� of predications, the perceptron model will do exact predicating to the best of its ability, and can back off to approximately predicating if exact predicating fails. 3.2 Training Algorithm We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results. Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) ∈ X × Y to a feature vector 4)(x, y) ∈ Rd, and a parameter vector α� ∈ Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model. For an input character sequence x, we aim to find an output F</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP, pages 1–8, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th ICML,</booktitle>
<pages>282--289</pages>
<location>Massachusetts, USA.</location>
<contexts>
<context position="1515" citStr="Lafferty et al., 2001" startWordPosition="215" endWordPosition="218">entation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, we obtain an error reduction of 18.5% on segmentation and 12% on joint segmentation and part-of-speech tagging over the perceptron-only baseline. 1 Introduction Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages. Several models were introduced for these problems, for example, the Hidden Markov Model (HMM) (Rabiner, 1989), Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), and Conditional Random Fields (CRFs) (Lafferty et al., 2001). CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM, and usually behaves the best in the two tasks. Another widely used discriminative method is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron. To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T). Since the typical approach of discriminative models treats segmentation</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th ICML, pages 282–289, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL</booktitle>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Reranking and self-training for parser adaptation. In Proceedings ofACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based?</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2326" citStr="Ng and Low, 2004" startWordPosition="346" endWordPosition="349">d is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron. To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&amp;T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004). Compared to performing segmentation and POS tagging one at a time, Joint S&amp;T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004). Besides the usual character-based features, additional features dependent on POS’s or words can also be employed to improve the performance. However, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable </context>
<context position="5306" citStr="Ng and Low (2004)" startWordPosition="846" endWordPosition="849">er sequence: C1:n = C1 C2 .. Cn the segmentation result can be depicted as: C1:e1 Ce1+1:e2 .. Cem−1+1:em while the segmentation and POS tagging result can be depicted as: C1:e1/t1 Ce1+1:e2/t2 .. Cem−1+1:em/tm Here, Ci (i = L.n) denotes Chinese character, ti (i = L.m) denotes POS tag, and Cl:r (l &lt; r) denotes character sequence ranges from Cl to Cr. We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag. It is a better idea to perform segmentation and POS tagging jointly in a uniform framework. According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: • b: the begin of the word • m: the middle of the word • e: the end of the word • s: a single-character word We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively. In order to perform POS tagging at the same time, we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng</context>
<context position="7182" citStr="Ng and Low, 2004" startWordPosition="1165" endWordPosition="1168">Instances Cn (n = −2..2) C−2=-F, C−1=..., C0=U, C1=/, C2=iF7 CnCn+1 (n = −2..1) C−2C−1=-F..., C−1C0=...U, C0C1=U/, C1C2=/� C−1C1 C−1C1=.../ Lexical-target Instances C0Cn (n = −2..2) C0C−2=U-F, C0C−1=U..., C0C0=UU, C0C1=U/, C0C2=UiFr C0CnCn+1 (n = −2..1) C0C−2C−1=U-F..., C0C−1C0=U...U, C0C0C1=UU/, C0C1C2=U/iFr C0C−1C1 C0C−1C1 = U.../ Table 1: Feature templates and instances. Suppose we are considering the third character ”U” in ”-F... U /ir”. to CRFs, while with much faster training. The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on. We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T. In following subsections, we describe the feature templates and the perceptron training algorithm. 3.1 Feature Templates The feature templates we adopted are selected from those of Ng and Low (2004). To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation. All feature t</context>
<context position="8627" citStr="Ng and Low (2004)" startWordPosition="1403" endWordPosition="1406">y borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target. We called them non-lexical-target because predications derived from them can predicate without considering the current character C0. Templates in the column below are expanded from the upper ones. We add a field C0 to each template in the upper column, so that it can carry out predication according to not only the context but also the current character itself. As predications generated from such templates depend on the current character, we name these templates lexical-target. Note that the templates of Ng and Low (2004) have already contained some lexical-target ones. With the two kinds Algorithm 1 Perceptron training algorithm. 1: Input: Training examples (xi, yi) 2: α�← 0 3: for t ← 1 .. T do 4: for i ← 1 .. N do 5: zi ← argmax,∈GEN(x;) 4)(xi, z) · α� 6: if zi =6 yi then 7: α� ← α� + 4)(xi, yi) − 4)(xi, zi) 8: Output: Parameters α� of predications, the perceptron model will do exact predicating to the best of its ability, and can back off to approximately predicating if exact predicating fails. 3.2 Training Algorithm We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative mod</context>
<context position="22540" citStr="Ng and Low (2004)" startWordPosition="3833" endWordPosition="3836">he POS information and reported the F-measure on segmentation only, while the second performed Joint S&amp;T using POS information and reported the F-measure both on segmentation and on Joint S&amp;T. Note that the accuracy of Joint S&amp;T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled. The evaluation results are shown in Table 3. We find that Joint S&amp;T can also improve the segmentation accuracy. However, the F-measure on Joint S&amp;T is obvious lower, about a rate of 95% to the F-measure on segmentation. Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&amp;T, a ratio of 96% to the F-measure 0.952 on segmentation. As the next step, a group of experiments were conducted to investigate how well the cascaded linear model performs. Here the core perceptron was just the POS+ model in experiments above. Besides this perceptron, other sub-models are trained and used as additional features of the outside-layer linear model. We used SRI Language Modelling Toolkit (Stolcke and Andreas, 2002) to train a 3- gram word LM with modified Kneser-Ney smoothing (Chen and Goodman, 1</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based? In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="3264" citStr="Och and Ney, 2004" startWordPosition="498" endWordPosition="501">ever, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications. As a result, many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently. To cope with this problem, we propose a cascaded linear model inspired by the log-linear model (Och and Ney, 2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources. Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models. We 897 Proceedings of ACL-08: HLT, pages 897–904, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Figure 1: Structure of Cascaded Linear Model. |R |denotes the scale of the feature space of the core perceptron. f1 f2 Core Linear Model (Perceptron) g1 = Ei αi X fi Word LM: g2 = Pwlm(W) g</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Joseph Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="23777" citStr="Och, 2003" startWordPosition="4039" endWordPosition="4040">ures Segmentation F1 Joint S&amp;T F1 All 0.9785 0.9341 All - PER 0.9049 0.8432 All - WLM 0.9785 0.9340 All - PLM 0.9752 0.9270 All - GPR 0.9774 0.9329 All - LPR 0.9765 0.9321 All - LEN 0.9772 0.9325 Table 4: Contribution of each feture. ALL: all features, PER: perceptron model, WLM: word language model, PLM: POS language model, GPR: generating model, LPR: labelling model, LEN: word count penalty. LM with Witten-Bell smoothing, and we trained a word-POS co-occurrence model simply by MLE without smoothing. To obtain their corresponding weights, we adapted the minimum-error-rate training algorithm (Och, 2003) to train the outside-layer model. In order to inspect how much improvement each feature brings into the cascaded model, every time we removed a feature while retaining others, then retrained the model and tested its performance on the test set. Table 4 shows experiments results. We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&amp;T, over the perceptron-only model POS+. We also find that the perceptron model functions as the kernel of the outside-layer linear model. Without the perceptron, the cascaded model (if we </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Joseph Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings ofACL 2003, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>In Proceedings ofIEEE,</booktitle>
<pages>257--286</pages>
<contexts>
<context position="1394" citStr="Rabiner, 1989" startWordPosition="200" endWordPosition="201"> into the perceptron directly. Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, we obtain an error reduction of 18.5% on segmentation and 12% on joint segmentation and part-of-speech tagging over the perceptron-only baseline. 1 Introduction Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages. Several models were introduced for these problems, for example, the Hidden Markov Model (HMM) (Rabiner, 1989), Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), and Conditional Random Fields (CRFs) (Lafferty et al., 2001). CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM, and usually behaves the best in the two tasks. Another widely used discriminative method is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron. To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; o</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence. R. Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recognition. In Proceedings ofIEEE, pages 257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ratnaparkhi</author>
<author>Adwait</author>
</authors>
<title>A maximum entropy part-of-speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference.</booktitle>
<contexts>
<context position="1453" citStr="Ratnaparkhi and Adwait, 1996" startWordPosition="206" endWordPosition="209">how that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, we obtain an error reduction of 18.5% on segmentation and 12% on joint segmentation and part-of-speech tagging over the perceptron-only baseline. 1 Introduction Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages. Several models were introduced for these problems, for example, the Hidden Markov Model (HMM) (Rabiner, 1989), Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), and Conditional Random Fields (CRFs) (Lafferty et al., 2001). CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM, and usually behaves the best in the two tasks. Another widely used discriminative method is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron. To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T). Since the</context>
</contexts>
<marker>Ratnaparkhi, Adwait, 1996</marker>
<rawString>Ratnaparkhi and Adwait. 1996. A maximum entropy part-of-speech tagger. In Proceedings of the Empirical Methods in Natural Language Processing Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stolcke</author>
<author>Andreas</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="23057" citStr="Stolcke and Andreas, 2002" startWordPosition="3918" endWordPosition="3921">ut a rate of 95% to the F-measure on segmentation. Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&amp;T, a ratio of 96% to the F-measure 0.952 on segmentation. As the next step, a group of experiments were conducted to investigate how well the cascaded linear model performs. Here the core perceptron was just the POS+ model in experiments above. Besides this perceptron, other sub-models are trained and used as additional features of the outside-layer linear model. We used SRI Language Modelling Toolkit (Stolcke and Andreas, 2002) to train a 3- gram word LM with modified Kneser-Ney smoothing (Chen and Goodman, 1998), and a 4-gram POS Features Segmentation F1 Joint S&amp;T F1 All 0.9785 0.9341 All - PER 0.9049 0.8432 All - WLM 0.9785 0.9340 All - PLM 0.9752 0.9270 All - GPR 0.9774 0.9329 All - LPR 0.9765 0.9321 All - LEN 0.9772 0.9325 Table 4: Contribution of each feture. ALL: all features, PER: perceptron model, WLM: word language model, PLM: POS language model, GPR: generating model, LPR: labelling model, LEN: word count penalty. LM with Witten-Bell smoothing, and we trained a word-POS co-occurrence model simply by MLE wi</context>
</contexts>
<marker>Stolcke, Andreas, 2002</marker>
<rawString>Stolcke and Andreas. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Libin Shen</author>
</authors>
<title>Chinese word segmentation as lmr tagging.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGHAN Workshop.</booktitle>
<contexts>
<context position="2202" citStr="Xue and Shen, 2003" startWordPosition="324" endWordPosition="327">mpared to generative ones such as HMM, and usually behaves the best in the two tasks. Another widely used discriminative method is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron. To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&amp;T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004). Compared to performing segmentation and POS tagging one at a time, Joint S&amp;T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004). Besides the usual character-based features, additional features dependent on POS’s or words can also be employed to improve the performance. However, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases ra</context>
</contexts>
<marker>Xue, Shen, 2003</marker>
<rawString>Nianwen Xue and Libin Shen. 2003. Chinese word segmentation as lmr tagging. In Proceedings of SIGHAN Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="7206" citStr="Zhang and Clark, 2007" startWordPosition="1169" endWordPosition="1172">−2..2) C−2=-F, C−1=..., C0=U, C1=/, C2=iF7 CnCn+1 (n = −2..1) C−2C−1=-F..., C−1C0=...U, C0C1=U/, C1C2=/� C−1C1 C−1C1=.../ Lexical-target Instances C0Cn (n = −2..2) C0C−2=U-F, C0C−1=U..., C0C0=UU, C0C1=U/, C0C2=UiFr C0CnCn+1 (n = −2..1) C0C−2C−1=U-F..., C0C−1C0=U...U, C0C0C1=UU/, C0C1C2=U/iFr C0C−1C1 C0C−1C1 = U.../ Table 1: Feature templates and instances. Suppose we are considering the third character ”U” in ”-F... U /ir”. to CRFs, while with much faster training. The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on. We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T. In following subsections, we describe the feature templates and the perceptron training algorithm. 3.1 Feature Templates The feature templates we adopted are selected from those of Ng and Low (2004). To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation. All feature templates and their insta</context>
<context position="19759" citStr="Zhang and Clark (2007)" startWordPosition="3359" endWordPosition="3362">eriments, we use the averaged parameters for the perceptrons, and F-measure as the accuracy measure. With precision P and recall R, the balance F-measure is defined as: F = 2PR/(P + R). Perceptron Learning Curve 0 1 2 3 4 5 6 7 8 9 10 number of iterations Figure 3: Averaged perceptron learning curves with Nonlexical-target and Lexical-target feature templates. AS CityU PKU MSR SIGHAN best 0.952 0.943 0.950 0.964 Zhang &amp; Clark 0.946 0.951 0.945 0.972 our model 0.954 0.958 0.940 0.975 Table 2: F-measure on SIGHAN bakeoff 2. SIGHAN best: best scores SIGHAN reported on the four corpus, cited from Zhang and Clark (2007). 6.1 Experiments on SIGHAN Bakeoff For convenience of comparing with others, we focus only on the close test, which means that any extra resource is forbidden except the designated training corpus. In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In Proceedings ofACL 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>