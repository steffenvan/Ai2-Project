<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008829">
<title confidence="0.995721">
Supervised Models for Coreference Resolution
</title>
<author confidence="0.973261">
Altaf Rahman and Vincent Ng
</author>
<affiliation confidence="0.986874">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.902676">
Richardson, TX 75083-0688
</address>
<email confidence="0.999709">
{altaf,vince}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.997401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951227272727">
Traditional learning-based coreference re-
solvers operate by training a mention-
pair classifier for determining whether two
mentions are coreferent or not. Two in-
dependent lines of recent research have
attempted to improve these mention-pair
classifiers, one by learning a mention-
ranking model to rank preceding men-
tions for a given anaphor, and the other
by training an entity-mention classifier
to determine whether a preceding clus-
ter is coreferent with a given mention.
We propose a cluster-ranking approach to
coreference resolution that combines the
strengths of mention rankers and entity-
mention models. We additionally show
how our cluster-ranking framework natu-
rally allows discourse-new entity detection
to be learned jointly with coreference res-
olution. Experimental results on the ACE
data sets demonstrate its superior perfor-
mance to competing approaches.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939157894737">
Noun phrase (NP) coreference resolution is the
task of identifying which NPs (or mentions) re-
fer to the same real-world entity or concept. Tra-
ditional learning-based coreference resolvers op-
erate by training a model for classifying whether
two mentions are co-referring or not (e.g., Soon
et al. (2001), Ng and Cardie (2002b), Kehler et al.
(2004), Ponzetto and Strube (2006)). Despite their
initial successes, these mention-pair models have
at least two major weaknesses. First, since each
candidate antecedent for a mention to be resolved
(henceforth an active mention) is considered inde-
pendently of the others, these models only deter-
mine how good a candidate antecedent is relative
to the active mention, but not how good a candi-
date antecedent is relative to other candidates. In
other words, they fail to answer the critical ques-
tion of which candidate antecedent is most prob-
able. Second, they have limitations in their ex-
pressiveness: the information extracted from the
two mentions alone may not be sufficient for mak-
ing an informed coreference decision, especially if
the candidate antecedent is a pronoun (which is se-
mantically empty) or a mention that lacks descrip-
tive information such as gender (e.g., Clinton).
To address the first weakness, researchers have
attempted to train a mention-ranking model for
determining which candidate antecedent is most
probable given an active mention (e.g., Denis and
Baldridge (2008)). Ranking is arguably a more
natural reformulation of coreference resolution
than classification, as a ranker allows all candidate
antecedents to be considered simultaneously and
therefore directly captures the competition among
them. Another desirable consequence is that there
exists a natural resolution strategy for a ranking
approach: a mention is resolved to the candidate
antecedent that has the highest rank. This con-
trasts with classification-based approaches, where
many clustering algorithms have been employed
to co-ordinate the pairwise coreference decisions
(because it is unclear which one is the best).
To address the second weakness, researchers
have investigated the acquisition of entity-mention
coreference models (e.g., Luo et al. (2004), Yang
et al. (2004)). Unlike mention-pair models, these
entity-mention models are trained to determine
whether an active mention belongs to a preced-
ing, possibly partially-formed, coreference cluster.
Hence, they can employ cluster-level features (i.e.,
features that are defined over any subset of men-
tions in a preceding cluster), which makes them
more expressive than mention-pair models.
Motivated in part by these recently developed
models, we propose in this paper a cluster-
ranking approach to coreference resolution that
combines the strengths of mention-ranking mod-
</bodyText>
<page confidence="0.962173">
968
</page>
<note confidence="0.996608">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 968–977,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.998715186666667">
els and entity-mention models. Specifically, we
recast coreference as the problem of determining
which of a set of preceding coreference clusters
is the best to link to an active mention using a
learned cluster ranker. In addition, we show how
discourse-new detection (i.e., the task of determin-
ing whether a mention introduces a new entity in
a discourse) can be learned jointly with corefer-
ence resolution in our cluster-ranking framework.
It is worth noting that researchers typically adopt
a pipeline coreference architecture, performing
discourse-new detection prior to coreference res-
olution and using the resulting information to pre-
vent a coreference system from resolving men-
tions that are determined to be discourse-new (see
Poesio et al. (2004) for an overview). As a re-
sult, errors in discourse-new detection could be
propagated to the resolver, possibly leading to a
deterioration of coreference performance (see Ng
and Cardie (2002a)). Jointly learning discourse-
new detection and coreference resolution can po-
tentially address this error-propagation problem.
In sum, we believe our work makes three main
contributions to coreference resolution:
Proposing a simple, yet effective coreference
model. Our work advances the state-of-the-art
in coreference resolution by bringing learning-
based coreference systems to the next level of
performance. When evaluated on the ACE 2005
coreference data sets, cluster rankers outperform
three competing models — mention-pair, entity-
mention, and mention-ranking models — by a
large margin. Also, our joint-learning approach
to discourse-new detection and coreference reso-
lution consistently yields cluster rankers that out-
perform those adopting the pipeline architecture.
Equally importantly, cluster rankers are conceptu-
ally simple and easy to implement and do not rely
on sophisticated training and inference procedures
to make coreference decisions in dependent rela-
tion to each other, unlike relational coreference
models (see McCallum and Wellner (2004)).
Bridging the gap between machine-learning
approaches and linguistically-motivated ap-
proaches to coreference resolution. While ma-
chine learning approaches to coreference resolu-
tion have received a lot of attention since the mid-
90s, popular learning-based coreference frame-
works such as the mention-pair model are ar-
guably rather unsatisfactory from a linguistic point
of view. In particular, they have not leveraged
advances in discourse-based anaphora resolution
research in the 70s and 80s. Our work bridges
this gap by realizing in a new machine learn-
ing framework ideas rooted in Lappin and Leass’s
(1994) heuristic-based pronoun resolver, which in
turn was motivated by classic salience-based ap-
proaches to anaphora resolution.
Revealing the importance of adopting the right
model. While entity-mention models have pre-
viously been shown to be worse or at best
marginally better than their mention-pair counter-
parts (Luo et al., 2004; Yang et al., 2008), our
cluster-ranking models, which are a natural exten-
sion of entity-mention models, significantly out-
performed all competing approaches. This sug-
gests that the use of an appropriate learning frame-
work can bring us a long way towards high-
performance coreference resolution.
The rest of the paper is structured as follows.
Section 2 discusses related work. Section 3 de-
scribes our baseline coreference models: mention-
pair, entity-mention, and mention-ranking. We
discuss our cluster-ranking approach in Section 4,
evaluate it in Section 5, and conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.999878" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999765">
Heuristic-based cluster ranking. As men-
tioned previously, the work most related to ours is
Lappin and Leass (1994), whose goal is to perform
pronoun resolution by assigning an anaphoric pro-
noun to the highest-scored preceding cluster. Nev-
ertheless, Lappin and Leass’s work differs from
ours in several respects. First, they only tackle
pronoun resolution rather than the full coreference
task. Second, their algorithm is heuristic-based; in
particular, the score assigned to a preceding clus-
ter is computed by summing over the weights as-
sociated with the factors applicable to the cluster,
where the weights are determined heuristically,
rather than learned, unlike ours.
Like many heuristic-based pronoun resolvers
(e.g., Mitkov (1998)), they first apply a set of con-
straints to filter grammatically incompatible can-
didate antecedents and then rank the remaining
ones using salience factors. As a result, their
cluster-ranking model employs only factors that
capture the salience of a cluster, and can therefore
be viewed as a simple model of attentional state
(see Grosz and Sidner (1986)) realized by coref-
erence clusters. By contrast, our resolution strat-
egy is learned without applying hand-coded con-
</bodyText>
<page confidence="0.997698">
969
</page>
<bodyText confidence="0.999982759036145">
straints in a separate filtering step. In particular,
we attempt to determine the compatibility between
a cluster and an active mention, using factors that
determine not only salience (e.g., the distance be-
tween the cluster and the mention) but also lexical
and grammatical compatibility, for instance.
Entity-mention coreference models. Luo et al.
(2004) represent one of the earliest attempts to
investigate learning-based entity-mention models.
They use the ANY predicate to generate cluster-
level features as follows: given a binary-valued
feature X defined over a pair of mentions, they
introduce an ANY-X cluster-level feature, which
has the value TRUE if X is true between the active
mention and any mention in the preceding clus-
ter under consideration. Contrary to common wis-
dom, this entity-mention model underperforms its
mention-pair counterpart in spite of the general-
ization from mention-pair to cluster-level features.
In Yang et al.’s (2004) entity-mention model, a
training instance is composed of an active men-
tion mk, a preceding cluster C, and a mention
mj in C that is closest in distance to mk in the
associated text. The feature set used to repre-
sent the instance is primarily composed of fea-
tures that describe the relationship between mj
and mk, as well as a few cluster-level features.
In other words, the model still relies heavily on
features used in a mention-pair model. In par-
ticular, the inclusion of mj in the feature vector
representation to some extent reflects the authors’
lack of confidence that a strong entity-mention
model can be trained without mention-pair-based
features. Our ranking model, on the other hand, is
trained without such features. More recently, Yang
et al. (2008) have proposed another entity-mention
model trained by inductive logic programming.
Like their previous work, the scarcity of cluster-
level predicates (only two are used) under-exploits
the expressiveness of entity-mention models.
Mention ranking. The notion of ranking can-
didate antecedents can be traced back to center-
ing algorithms, many of which use grammatical
roles to rank forward-looking centers (see Grosz
et al. (1995), Walker et al. (1998), and Mitkov
(2002)). However, mention ranking has been
employed in learning-based coreference resolvers
only recently. As mentioned before, Denis and
Baldridge (2008) train a mention-ranking model.
Their work can be viewed as an extension of Yang
et al.’s (2003) twin-candidate coreference model,
which ranks only two candidate antecedents at a
time. Unlike ours, however, their model ranks
mentions rather than clusters, and relies on an
independently-trained discourse-new detector.
Discourse-new detection. Discourse-new de-
tection is often tackled independently of coref-
erence resolution. Pleonastic its have been de-
tected using heuristics (e.g., Kennedy and Bogu-
raev (1996)) and learning-based techniques such
as rule learning (e.g., M¨uller (2006)), kernels (e.g.,
Versley et al. (2008)), and distributional methods
(e.g., Bergsma et al. (2008)). Non-anaphoric defi-
nite descriptions have been detected using heuris-
tics (e.g., Vieira and Poesio (2000)) and unsu-
pervised methods (e.g., Bean and Riloff (1999)).
General discourse-new detectors that are applica-
ble to different types of NPs have been built using
heuristics (e.g., Byron and Gegg-Harrison (2004))
and modeled generatively (e.g., Elsner and Char-
niak (2007)) and discriminatively (e.g., Uryupina
(2003)). There have also been attempts to perform
joint inference for discourse-new detection and
coreference resolution using integer linear pro-
gramming (ILP), where a discourse-new classifier
and a coreference classifier are trained indepen-
dently of each other, and then ILP is applied as a
post-processing step to jointly infer discourse-new
and coreference decisions so that they are consis-
tent with each other (e.g., Denis and Baldridge
(2007)). Joint inference is different from our joint-
learning approach, which allows the two tasks to
be learned jointly and not independently.
</bodyText>
<sectionHeader confidence="0.996647" genericHeader="method">
3 Baseline Coreference Models
</sectionHeader>
<bodyText confidence="0.999987818181818">
In this section, we describe three coreference mod-
els that will serve as our baselines: the mention-
pair model, the entity-mention model, and the
mention-ranking model. For illustrative purposes,
we will use the text segment shown in Figure 1.
Each mention m in the segment is annotated as
[m]cidmid, where mid is the mention id and cid is
the id of the cluster to which m belongs. As we
can see, the mentions are partitioned into four sets,
with Barack Obama, his, and he in one cluster, and
each of the remaining mentions in its own cluster.
</bodyText>
<subsectionHeader confidence="0.871404">
3.1 Mention-Pair Model
</subsectionHeader>
<bodyText confidence="0.99978475">
As noted before, a mention-pair model is a clas-
sifier that decides whether or not an active men-
tion mk is coreferent with a candidate antecedent
mj. Each instance i(mj, mk) represents mj and
</bodyText>
<page confidence="0.98125">
970
</page>
<note confidence="0.616699">
[Barack Obama]11 nominated [Hillary Rodham Clinton]2 2 as
[[his]13 secretary ofstate]34 on [Monday]45. [He]16 ...
</note>
<figureCaption confidence="0.99935">
Figure 1: An illustrative example
</figureCaption>
<bodyText confidence="0.999968071428571">
mk and consists of the 39 features shown in Ta-
ble 1. These features have largely been employed
by state-of-the-art learning-based coreference sys-
tems (e.g., Soon et al. (2001), Ng and Cardie
(2002b), Bengtson and Roth (2008)), and are com-
puted automatically. As can be seen, the features
are divided into four blocks. The first two blocks
consist of features that describe the properties of
mj and mk, respectively, and the last two blocks
of features describe the relationship between mj
and mk. The classification associated with a train-
ing instance is either positive or negative, depend-
ing on whether mj and mk are coreferent.
If one training instance were created from each
pair of mentions, the negative instances would
significantly outnumber the positives, yielding
a skewed class distribution that will typically
have an adverse effect on model training. As
a result, only a subset of mention pairs will
be generated for training. Following Soon et
al. (2001), we create (1) a positive instance for
each discourse-old mention mk and its closest
antecedent mj; and (2) a negative instance for
mk paired with each of the intervening mentions,
mj+1, mj+2, ... , mk−1. In our running example
shown in Figure 1, three training instances will
be generated for He: i(Monday, He), i(secretary
of state, He), and i(his, He). The first two of
these instances will be labeled as negative, and
the last one will be labeled as positive. To train a
mention-pair classifier, we use the SVM learning
algorithm from the SVMlight package (Joachims,
2002), converting all multi-valued features into an
equivalent set of binary-valued features.
After training, the resulting SVM classifier is
used to identify an antecedent for a mention in a
test text. Specifically, an active mention mk se-
lects as its antecedent the closest preceding men-
tion that is classified as coreferent with mk. If mk
is not classified as coreferent with any preceding
mention, it will be considered discourse-new (i.e.,
no antecedent will be selected for mk).
</bodyText>
<subsectionHeader confidence="0.85271">
3.2 Entity-Mention Model
</subsectionHeader>
<bodyText confidence="0.999972113207547">
Unlike a mention-pair model, an entity-mention
model is a classifier that decides whether or not
an active mention mk is coreferent with a par-
tial cluster cj that precedes mk. Each training
instance, i(cj, mk), represents cj and mk. The
features for an instance can be divided into two
types: (1) features that describe mk (i.e, those
shown in the second block of Table 1), and (2)
cluster-level features, which describe the relation-
ship between cj and mk. Motivated by previ-
ous work (Luo et al., 2004; Culotta et al., 2007;
Yang et al., 2008), we create cluster-level fea-
tures from mention-pair features using four pred-
icates: NONE, MOST-FALSE, MOST-TRUE, and
ALL. Specifically, for each feature X shown in
the last two blocks in Table 1, we first convert X
into an equivalent set of binary-valued features if
it is multi-valued. Then, for each resulting binary-
valued feature Xb, we create four binary-valued
cluster-level features: (1) NONE-Xb is true when
Xb is false between mk and each mention in cj; (2)
MOST-FALSE-Xb is true when Xb is true between
mk and less than half (but at least one) of the men-
tions in cj; (3) MOST-TRUE-Xb is true when Xb is
true between mk and at least half (but not all) of
the mentions in cj; and (4) ALL-Xb is true when Xb
is true between mk and each mention in cj. Hence,
for each Xb, exactly one of these four cluster-level
features evaluates to true.
Following Yang et al. (2008), we create (1) a
positive instance for each discourse-old mention
mk and the preceding cluster cj to which it be-
longs; and (2) a negative instance for mk paired
with each partial cluster whose last mention ap-
pears between mk and its closest antecedent (i.e.,
the last mention of cj). Consider again our run-
ning example. Three training instances will be
generated for He: i({Monday}, He), i({secretary
of state}, He), and i({Barack Obama, his}, He).
The first two of these instances will be labeled as
negative, and the last one will be labeled as pos-
itive. As in the mention-pair model, we train an
entity-mention classifier using the SVM learner.
After training, the resulting classifier is used to
identify a preceding cluster for a mention in a test
text. Specifically, the mentions are processed in
a left-to-right manner. For each active mention
mk, a test instance is created between mk and
each of the preceding clusters formed so far. All
the test instances are then presented to the classi-
fier. Finally, mk will be linked to the closest pre-
ceding cluster that is classified as coreferent with
mk. If mk is not classified as coreferent with any
</bodyText>
<page confidence="0.991298">
971
</page>
<table confidence="0.976239903846154">
Features describing mj, a candidate antecedent
1 PRONOUN 1 Y if mj is a pronoun; else N
2 SUBJECT 1 Y if mj is a subject; else N
3 NESTED 1 Y if mj is a nested NP; else N
Features describing mk, the mention to be resolved
4 NUMBER 2 SINGULAR or PLURAL, determined using a lexicon
5 GENDER 2 MALE, FEMALE, NEUTER, or UNKNOWN, determined using a list of common first names
6 PRONOUN 2 Y if mk is a pronoun; else N
7 NESTED 2 Y if mk is a nested NP; else N
8 SEMCLASS 2 the semantic class of mk; can be one of PERSON, LOCATION, ORGANIZATION, DATE, TIME,
MONEY, PERCENT, OBJECT, OTHERS, determined using WordNet and an NE recognizer
9 ANIMACY 2 Y if mk is determined as HUMAN or ANIMAL by WordNet and an NE recognizer; else N
10 PRO TYPE 2 the nominative case of mk if it is a pronoun; else NA. E.g., the feature value for him is HE
Features describing the relationship between mj, a candidate antecedent and mk, the mention to be resolved
11 HEAD MATCH C if the mentions have the same head noun; else I
12 STR MATCH C if the mentions are the same string; else I
13 SUBSTR MATCH C if one mention is a substring of the other; else I
14 PRO STR MATCH C if both mentions are pronominal and are the same string; else I
15 PN STR MATCH C if both mentions are proper names and are the same string; else I
16 NONPRO STR MATCH C if the two mentions are both non-pronominal and are the same string; else I
17 MODIFIER MATCH C if the mentions have the same modifiers; NA if one of both of them don’t have a modifier;
else I
18 PRO TYPE MATCH C if both mentions are pronominal and are either the same pronoun or different only w.r.t.
case; NA if at least one of them is not pronominal; else I
19 NUMBER C if the mentions agree in number; I if they disagree; NA if the number for one or both
mentions cannot be determined
20 GENDER C if the mentions agree in gender; I if they disagree; NA if the gender for one or both mentions
cannot be determined
21 AGREEMENT C if the mentions agree in both gender and number; I if they disagree in both number and
gender; else NA
22 ANIMACY C if the mentions match in animacy; I if they don’t; NA if the animacy for one or both mentions
cannot be determined
23 BOTH PRONOUNS C if both mentions are pronouns; I if neither are pronouns; else NA
24 BOTH PROPER NOUNS C if both mentions are proper nouns; I if neither are proper nouns; else NA
25 MAXIMALNP C if the two mentions does not have the same maximial NP projection; else I
26 SPAN C if neither mention spans the other; else I
27 INDEFINITE C if mk is an indefinite NP and is not in an appositive relationship; else I
28 APPOSITIVE C if the mentions are in an appositive relationship; else I
29 COPULAR C if the mentions are in a copular construction; else I
30 SEMCLASS C if the mentions have the same semantic class; I if they don’t; NA if the semantic class
information for one or both mentions cannot be determined
31 ALIAS C if one mention is an abbreviation or an acronym of the other; else I
32 DISTANCE binned values for sentence distance between the mentions
Additional features describing the relationship between mj, a candidate antecedent and mk, the mention to be resolved
33 NUMBER’ the concatenation of the NUMBER 2 feature values of mj and mk. E.g., if mj is Clinton and
mk is they, the feature value is SINGULAR-PLURAL, since mj is singular and mk is plural
34 GENDER’ the concatenation of the GENDER 2 feature values of mj and mk
35 PRONOUN’ the concatenation of the PRONOUN 2 feature values of mj and mk
36 NESTED’ the concatenation of the NESTED 2 feature values of mj and mk
37 SEMCLASS’ the concatenation of the SEMCLASS 2 feature values of mj and mk
38 ANIMACY’ the concatenation of the ANIMACY 2 feature values of mj and mk
39 PRO TYPE’ the concatenation of the PRO TYPE 2 feature values of mj and mk
</table>
<tableCaption confidence="0.967791666666666">
Table 1: The feature set for coreference resolution. Non-relational features describe a mention and in
most cases take on a value of YES or NO. Relational features describe the relationship between the two
mentions and indicate whether they are COMPATIBLE, INCOMPATIBLE or NOT APPLICABLE.
</tableCaption>
<bodyText confidence="0.98935525">
preceding cluster, it will be considered discourse-
new. Note that all partial clusters preceding mk
are formed incrementally based on the predictions
of the classifier for the first k − 1 mentions.
</bodyText>
<subsectionHeader confidence="0.895529">
3.3 Mention-Ranking Model
</subsectionHeader>
<bodyText confidence="0.9998006">
As noted before, a ranking model imposes a
ranking on all the candidate antecedents of an
active mention mk. To train a ranker, we
use the SVM ranker-learning algorithm from the
SVMlZght package. Like the mention-pair model,
each training instance i(mj, mk) represents mk
and a preceding mention mj. In fact, the fea-
tures that represent the instance as well as the
method for creating training instances are identi-
cal to those employed by the mention-pair model.
</bodyText>
<page confidence="0.990627">
972
</page>
<bodyText confidence="0.999994764705882">
The only difference lies in the assignment of
class values to training instances. Assuming that
Sk is the set of training instances created for
anaphoric mention mk, the class value for an in-
stance i(mj, mk) in Sk is the rank of mj among
competing candidate antecedents, which is 2 if
mj is the closest antecedent of mk, and 1 other-
wise.1 To exemplify, consider our running exam-
ple. As in the mention-pair model, three training
instances will be generated for He: i(Monday, He),
i(secretary of state, He), i(his, He). The third in-
stance will have a class value of 2, and the remain-
ing two will have a class value of 1.
After training, the mention-ranking model is ap-
plied to rank the candidate antecedents for an ac-
tive mention in a test text as follows. Given an ac-
tive mention mk, we follow Denis and Baldridge
(2008) and use an independently-trained classifier
to determine whether mk is discourse-new. If so,
mk will not be resolved. Otherwise, we create test
instances for mk by pairing it with each of its pre-
ceding mentions. The test instances are then pre-
sented to the ranker, and the preceding mention
that is assigned the largest value by the ranker is
selected as the antecedent of mk.
The discourse-new classifier used in the resolu-
tion step is trained with 26 of the 37 features2 de-
scribed in Ng and Cardie (2002a) that are deemed
useful for distinguishing between anaphoric and
non-anaphoric mentions. These features can be
broadly divided into two types: (1) features that
encode the form of the mention (e.g., NP type,
number, definiteness), and (2) features that com-
pare the mention to one of its preceding mentions.
</bodyText>
<sectionHeader confidence="0.967463" genericHeader="method">
4 Coreference as Cluster Ranking
</sectionHeader>
<bodyText confidence="0.99992275">
In this section, we describe our cluster-ranking ap-
proach to NP coreference. As noted before, our
approach aims to combine the strengths of entity-
mention models and mention-ranking models.
</bodyText>
<subsectionHeader confidence="0.996744">
4.1 Training and Applying a Cluster Ranker
</subsectionHeader>
<bodyText confidence="0.999947666666667">
For ease of exposition, we will describe in this
subsection how to train and apply a cluster ranker
when it is used in a pipeline architecture, where
discourse-new detection is performed prior to
coreference resolution. In the next subsection, we
will show how the two tasks can be learned jointly.
</bodyText>
<footnote confidence="0.8970485">
1A larger class value implies a better rank in SVMlight.
2The 11 features that we did not employ are CONJ,
POSSESSIVE, MODIFIER, POSTMODIFIED, SPECIAL NOUNS,
POST, SUBCLASS, TITLE, and the positional features.
</footnote>
<bodyText confidence="0.999960861111111">
Recall that a cluster ranker ranks a set of pre-
ceding clusters for an active mention mk. Since
a cluster ranker is a hybrid of a mention-ranking
model and an entity-mention model, the way it is
trained and applied is also a hybrid of the two.
In particular, the instance representation employed
by a cluster ranker is identical to that used by
an entity-mention model, where each training in-
stance i(cj, mk) represents a preceding cluster cj
and a discourse-old mention mk and consists of
cluster-level features formed from predicates. Un-
like in an entity-mention model, however, in a
cluster ranker, (1) a training instance is created be-
tween each discourse-old mention mk and each of
its preceding clusters; and (2) since we are train-
ing a model for ranking clusters, the assignment of
class values to training instances is similar to that
of a mention ranker. Specifically, the class value of
a training instance i(cj, mk) created for mk is the
rank of cj among the competing clusters, which is
2 if mk belongs to cj, and 1 otherwise.
Applying the learned cluster ranker to a test text
is similar to applying a mention ranker. Specifi-
cally, the mentions are processed in a left-to-right
manner. For each active mention mk, we first
apply an independently-trained classifier to deter-
mine if mk is discourse-new. If so, mk will not be
resolved. Otherwise, we create test instances for
mk by pairing it with each of its preceding clus-
ters. The test instances are then presented to the
ranker, and mk is linked to the cluster that is as-
signed the highest value by the ranker. Note that
these partial clusters preceding mk are formed in-
crementally based on the predictions of the ranker
for the first k−1 mentions; no gold-standard coref-
erence information is used in their formation.
</bodyText>
<subsectionHeader confidence="0.9688945">
4.2 Joint Discourse-New Detection and
Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.99985">
The cluster ranker described above can be used
to determine which preceding cluster a discourse-
old mention should be linked to, but it cannot be
used to determine whether a mention is discourse-
new or not. The reason is simple: all the training
instances are generated from discourse-old men-
tions. Hence, to jointly learn discourse-new de-
tection and coreference resolution, we must train
the ranker using instances generated from both
discourse-old and discourse-new mentions.
Specifically, when training the ranker, we pro-
vide each active mention with the option to start
</bodyText>
<page confidence="0.998082">
973
</page>
<bodyText confidence="0.999971535714286">
a new cluster by creating an additional instance
that (1) contains features that solely describe the
active mention (i.e., the features shown in the sec-
ond block of Table 1), and (2) has the highest rank
value among competing clusters (i.e., 2) if it is
discourse-new and the lowest rank value (i.e., 1)
otherwise. The main advantage of jointly learning
the two tasks is that it allows the ranking model
to evaluate all possible options for an active men-
tion (i.e., whether to resolve it, and if so, which
preceding cluster is the best) simultaneously.
After training, the resulting cluster ranker pro-
cesses the mentions in a test text in a left-to-right
manner. For each active mention mk, we create
test instances for it by pairing it with each of its
preceding clusters. To allow for the possibility that
mk is discourse-new, we create an additional test
instance that contains features that solely describe
the active mention (similar to what we did in the
training step above). All these test instances are
then presented to the ranker. If the additional test
instance is assigned the highest rank value by the
ranker, then mk is classified as discourse-new and
will not be resolved. Otherwise, mk is linked to
the cluster that has the highest rank. As before,
all partial clusters preceding mk are formed incre-
mentally based on the predictions of the ranker for
the first k − 1 mentions.
</bodyText>
<sectionHeader confidence="0.999811" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.993043">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99979294117647">
Corpus. We use the ACE 2005 coreference cor-
pus as released by the LDC, which consists of the
599 training documents used in the official ACE
evaluation.3 To ensure diversity, the corpus was
created by selecting documents from six different
sources: Broadcast News (bn), Broadcast Con-
versations (bc), Newswire (nw), Webblog (wb),
Usenet (un), and conversational telephone speech
(cts). The number of documents belonging to each
source is shown in Table 2. For evaluation, we par-
tition the 599 documents into a training set and a
test set following a 80/20 ratio, ensuring that the
two sets have the same proportion of documents
from the six sources.
Mention extractor. We evaluate each corefer-
ence model using both true mentions (i.e., gold
standard mentions4) and system mentions (i.e., au-
</bodyText>
<footnote confidence="0.987808">
3Since we did not participate in ACE 2005, we do not
have access to the official test set.
4Note that only mention boundaries are used.
</footnote>
<table confidence="0.8895195">
Dataset bn bc nw wl un cts
# of documents 60 226 106 119 49 39
</table>
<tableCaption confidence="0.984501">
Table 2: Statistics for the ACE 2005 corpus
</tableCaption>
<bodyText confidence="0.993987733333334">
tomatically identified mentions). To extract sys-
tem mentions from a test text, we trained a men-
tion extractor on the training texts. Following Flo-
rian et al. (2004), we recast mention extraction as
a sequence labeling task, where we assign to each
token in a test text a label that indicates whether it
begins a mention, is inside a mention, or is outside
a mention. Hence, to learn the extractor, we create
one training instance for each token in a training
text and derive its class value (one of b, i, and o)
from the annotated data. Each instance represents
wi, the token under consideration, and consists of
29 linguistic features, many of which are modeled
after the systems of Bikel et al. (1999) and Florian
et al. (2004), as described below.
Lexical (7): Tokens in a window of 7:
{wi−3, ... , wi+3}.
Capitalization (4): Determine whether wi
IsAllCap, IsInitCap, IsCapPeriod, and
IsAllLower (see Bikel et al. (1999)).
Morphological (8): wi’s prefixes and suffixes of
length one, two, three, and four.
Grammatical (1): The part-of-speech (POS)
tag of wi obtained using the Stanford log-linear
POS tagger (Toutanova et al., 2003).
Semantic (1): The named entity (NE) tag of wi
obtained using the Stanford CRF-based NE recog-
nizer (Finkel et al., 2005).
Gazetteers (8): Eight dictionaries containing
pronouns (77 entries), common words and words
that are not names (399.6k), person names (83.6k),
person titles and honorifics (761), vehicle words
(226), location names (1.8k), company names
(77.6k), and nouns extracted from WordNet that
are hyponyms of PERSON (6.3k).
We employ CRF++5, a C++ implementation of
conditional random fields, for training the mention
detector, which achieves an F-score of 86.7 (86.1
recall, 87.2 precision) on the test set. These ex-
tracted mentions are to be used as system mentions
in our coreference experiments.
Scoring programs. To score the output of a
coreference model, we employ three scoring pro-
grams: MUC (Vilain et al., 1995), B3 (Bagga and
Baldwin, 1998), and φ3-CEAF (Luo, 2005).
</bodyText>
<footnote confidence="0.99521">
5Available from http://crfpp.sourceforge.net
</footnote>
<page confidence="0.997105">
974
</page>
<bodyText confidence="0.99998582051282">
There is a complication, however. When scor-
ing a response (i.e., system-generated) partition
against a key (i.e., gold-standard) partition, a scor-
ing program needs to construct a mapping between
the mentions in the response and those in the key.
If the response is generated using true mentions,
then every mention in the response is mapped to
some mention in the key and vice versa; in other
words, there are no twinless (i.e., unmapped) men-
tions (Stoyanov et al., 2009). However, this is
not the case when system mentions are used. The
aforementioned complication does not arise from
the construction of the mapping, but from the fact
that Bagga and Baldwin (1998) and Luo (2005) do
not specify how to apply B3 and CEAF to score
partitions generated from system mentions.
We propose a simple solution to this problem:
we remove all and only those twinless system
mentions that are singletons before applying B3
and CEAF. The reason is simple: since the coref-
erence resolver has successfully identified these
mentions as singletons, it should not be penal-
ized, and removing them allows us to avoid such
penalty. Note that we only remove twinless (as op-
posed to all) system mentions that are singletons:
this allows us to reward a resolver for success-
ful identification of singleton mentions that have
twins, thus overcoming a major weakness of and
common criticism against the MUC scorer. Also,
we retain twinless system mentions that are non-
singletons, as the resolver should be penalized for
identifying spurious coreference relations. On the
other hand, we do not remove twinless mentions
in the key partition, as we want to ensure that the
resolver makes the correct (non-)coreference de-
cisions for them. We believe that our proposal ad-
dresses Stoyanov et al.’s (2009) problem of hav-
ing very low precision when applying the CEAF
scorer to score partitions of system mentions.
</bodyText>
<subsectionHeader confidence="0.972947">
5.2 Results and Discussions
</subsectionHeader>
<bodyText confidence="0.99985275">
The mention-pair baseline. We train our first
baseline, the mention-pair coreference classifier,
using the SVM learning algorithm as implemented
in the SVMlight package (Joachims, 2002).6 Re-
sults of this baseline using true mentions and sys-
tem mentions, shown in row 1 of Tables 3 and 4,
are reported in terms of recall (R), precision (P),
and F-score (F) provided by the three scoring pro-
</bodyText>
<footnote confidence="0.750114">
6For this and subsequent uses of the SVM learner in our
experiments, we set all parameters to their default values.
</footnote>
<bodyText confidence="0.99994012244898">
grams. As we can see, this baseline achieves F-
scores of 54.3–70.0 and 53.4–62.5 for true men-
tions and system mentions, respectively.
The entity-mention baseline. Next, we train
our second baseline, the entity-mention corefer-
ence classifier, using the SVM learner. Results of
this baseline are shown in row 2 of Tables 3 and
4. For true mentions, this baseline achieves an F-
score of 54.8–70.7. In comparison to the mention-
pair baseline, F-score rises insignificantly accord-
ing to all three scorers.7 Similar trends can be ob-
served for system mentions, where the F-scores
between the two models are statistically indistin-
guishable across the board. While the insignifi-
cant performance difference is somewhat surpris-
ing given the improved expressiveness of entity-
mention models over mention-pair models, similar
trends have been reported by Luo et al. (2004).
The mention-ranking baseline. Our third base-
line is the mention-ranking coreference model,
trained using the ranker-learning algorithm in
SVMlight. To identify discourse-new mentions,
we employ two methods. In the first method, we
adopt a pipeline architecture, where we train an
SVM classifier for discourse-new detection inde-
pendently of the mention ranker on the training set
using the 26 features described in Section 3.3. We
then apply the resulting classifier to each test text
to filter discourse-new mentions prior to corefer-
ence resolution. Results of the mention ranker are
shown in row 3 of Tables 3 and 4. As we can
see, the ranker achieves F-scores of 57.8–71.2 and
54.1–65.4 for true mentions and system mentions,
respectively, yielding a significant improvement
over the entity-mention baseline in all but one case
(MUC/true mentions).
In the second method, we perform discourse-
new detection jointly with coreference resolution
using the method described in Section 4.2. While
we discussed this joint learning method in the con-
text of cluster ranking, it should be easy to see
that the method is equally applicable to a men-
tion ranker. Results of the mention ranker using
this joint architecture are shown in row 4 of Ta-
bles 3 and 4. As we can see, the ranker achieves
F-scores of 61.6–73.4 and 55.6–67.1 for true men-
tions and system mentions, respectively. For both
types of mentions, the improvements over the cor-
responding results for the entity-mention baseline
</bodyText>
<footnote confidence="0.6843515">
7We use Approximate Randomization (Noreen, 1989) for
testing statistical significance, with P set to 0.05.
</footnote>
<page confidence="0.98975">
975
</page>
<table confidence="0.993225125">
Coreference Model R MUC F R CEAF F R B3 F
P P P
Mention-pair model 71.7 69.2 70.4 54.3 54.3 54.3 53.3 63.6 58.0
Entity-mention model 71.7 69.7 70.7 54.8 54.8 54.8 53.2 65.1 58.5
Mention-ranking model (Pipeline) 68.7 73.9 71.2 57.8 57.8 57.8 55.8 63.9 59.6
Mention-ranking model (Joint) 69.4 77.8 73.4 61.6 61.6 61.6 57.0 70.1 62.9
Cluster-ranking model (Pipeline) 71.7 78.2 74.8 61.8 61.8 61.8 58.2 69.1 63.2
Cluster-ranking model (Joint) 69.9 83.3 76.0 63.3 63.3 63.3 56.0 74.6 64.0
</table>
<figure confidence="0.874225833333333">
1
2
3
4
5
6
</figure>
<tableCaption confidence="0.9991">
Table 3: MUC, CEAF, and B3 coreference results using true mentions.
</tableCaption>
<table confidence="0.992182625">
Coreference Model R MUC F R CEAF F R B3 F
P P P
Mention-pair model 70.0 56.4 62.5 56.1 51.0 53.4 50.8 57.9 54.1
Entity-mention model 68.5 57.2 62.3 56.3 50.2 53.1 51.2 57.8 54.3
Mention-ranking model (Pipeline) 62.2 68.9 65.4 51.6 56.7 54.1 52.3 61.8 56.6
Mention-ranking model (Joint) 62.1 73.0 67.1 53.0 58.5 55.6 50.4 65.5 56.9
Cluster-ranking model (Pipeline) 65.3 72.3 68.7 54.1 59.3 56.6 55.3 63.7 59.2
Cluster-ranking model (Joint) 64.1 75.4 69.3 56.7 62.6 59.5 54.4 70.5 61.4
</table>
<figure confidence="0.868247666666667">
1
2
3
4
5
6
</figure>
<tableCaption confidence="0.988722">
Table 4: MUC, CEAF, and B3 coreference results using system mentions.
</tableCaption>
<bodyText confidence="0.999953172413793">
are significant, and suggest that mention ranking is
a precision-enhancing device. Moreover, in com-
parison to the pipeline architecture in row 3, we
see that F-score rises significantly by 2.2–3.8% for
true mentions, and improves by a smaller margin
of 0.3–1.7% for system mentions. These results
demonstrate the benefits of joint modeling.
Our cluster-ranking model. Finally, we evalu-
ate our cluster-ranking model. As in the mention-
ranking baseline, we employ both the pipeline ar-
chitecture and the joint architecture for discourse-
new detection. Results are shown in rows 5 and
6 of Tables 3 and 4, respectively, for the two ar-
chitectures. When true mentions are used, the
pipeline architecture yields an F-score of 61.8–
74.8, which represents a significant improvement
over the mention ranker adopting the pipeline ar-
chitecture. With the joint architecture, the clus-
ter ranker achieves an F-score of 63.3–76.0. This
also represents a significant improvement over the
mention ranker adopting the joint architecture, the
best of the baselines, and suggests that cluster
ranking is abetter precision-enhancing model than
mention ranking. Moreover, comparing the re-
sults in these two rows reveals the superiority of
the joint architecture over the pipeline architec-
ture, particularly in terms of its ability to enhance
system precision. Similar performance trends can
be observed when system mentions are used.
</bodyText>
<sectionHeader confidence="0.999709" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999953764705882">
We have presented a cluster-ranking approach that
recasts the mention resolution process as the prob-
lem of finding the best preceding cluster to link an
active mention to. Crucially, our approach com-
bines the strengths of entity-mention models and
mention-ranking models. Experimental results on
the ACE 2005 corpus show that (1) jointly learn-
ing coreference resolution and discourse-new de-
tection allows the cluster ranker to achieve bet-
ter performance than adopting a pipeline corefer-
ence architecture; and (2) our cluster ranker signif-
icantly outperforms the mention ranker, the best of
the three baseline coreference models, under both
the pipeline architecture and the joint architecture.
Overall, we believe that our cluster-ranking ap-
proach advances the state-of-the-art in coreference
resolution both theoretically and empirically.
</bodyText>
<sectionHeader confidence="0.998631" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999872">
We thank the three anonymous reviewers for their
invaluable comments on the paper. This work was
supported in part by NSF Grant IIS-0812261.
</bodyText>
<sectionHeader confidence="0.998839" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998362166666667">
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space
model. In Proc. of COLING-ACL, pages 79–85.
D. Bean and E. Riloff. 1999. Corpus-based identifica-
tion of non-anaphoric noun phrases. In Proc. of the
ACL, pages 373–380.
E. Bengtson and D. Roth. 2008. Understanding the
values of features for coreference resolution. In
Proc. of EMNLP, pages 294–303.
S. Bergsma, D. Lin, and R. Goebel. 2008. Distribu-
tional identification of non-referential pronouns. In
Proc. ofACL-08:HLT, pages 10–18.
</reference>
<page confidence="0.987209">
976
</page>
<reference confidence="0.999909324786325">
D. Bikel, R. Schwartz, and R. Weischedel. 1999. An
algorithm that learns what’s in a name. Machine
Learning, 34(1–3):211–231.
D. Byron and W. Gegg-Harrison. 2004. Eliminating
non-referring noun phrases from coreference resolu-
tion. In Proc. ofDAARC, pages 21–26.
A. Culotta, M. Wick, and A. McCallum. 2007. First-
order probabilistic models for coreference resolu-
tion. In Proc. ofNAACL-HLT, pages 81–88.
P. Denis and J. Baldridge. 2007. Global, joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proc. of NAACL-HLT,
pages 236–243.
P. Denis and J. Baldridge. 2008. Specialized models
and ranking for coreference resolution. In Proc. of
EMNLP, pages 660–669.
M. Elsner and E. Charniak. 2007. A generative
discourse-new model for text coherence. Technical
Report CS-07-04, Brown University.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Proc. of
the ACL, pages 363–370.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N. Nicolov, and I. Zitouni.
2004. A statistical model for multilingual entity de-
tection and tracking. In Proc. ofHLT/NAACL.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995.
Centering: A framework for modeling the local co-
herence of discourse. Computational Linguistics,
21(2):203–226.
B. J. Grosz and C. L. Sidner. 1986. Attention, inten-
tions, and the structure of discourse. Computational
Linguistics, 12(3):175–204.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proc. ofKDD, pages 133–142.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004.
The (non)utility of predicate-argument frequencies
for pronoun interpretation. In Proc. ofHLT/NAACL.
C. Kennedy and B. Boguraev. 1996. Anaphor for ev-
eryone: Pronominal anaphora resolution without a
parser. In Proc. of COLING, pages 113–118.
S. Lappin and H. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational
Linguistics, 20(4):535–562.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous coref-
erence resolution algorithm based on the Bell tree.
In Proc. of the ACL, pages 135–142.
X. Luo. 2005. On coreference resolution performance
metrics. In Proc. ofHLT/EMNLP, pages 25–32.
A. McCallum and B. Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun
coreference. In Advances in NIPS.
R. Mitkov. 2002. Anaphora Resolution. Longman.
R. Mitkov. 1998. Robust pronoun resolution with lim-
ited knowledge. In Proc. of COLING/ACL, pages
869–875.
C. M¨uller. 2006. Automatic detection of nonrefer-
ential it in spoken multi-party dialog. In Proc. of
EACL, pages 49–56.
V. Ng and C. Cardie. 2002a. Identifying anaphoric and
non-anaphoric noun phrases to improve coreference
resolution. In Proc. of COLING, pages 730–736.
V. Ng and C. Cardie. 2002b. Improving machine learn-
ing approaches to coreference resolution. In Proc. of
the ACL, pages 104–111.
E. W. Noreen. 1989. Computer Intensive Methods for
Testing Hypothesis: An Introduction. John Wiley &amp;
Sons.
M. Poesio, O. Uryupina, R. Vieira, M. Alexandrov-
Kabadjov, and R. Goulart. 2004. Discourse-new
detectors for definite description resolution: A sur-
vey and a preliminary proposal. In Proc. of the ACL
Workshop on Reference Resolution.
S. P. Ponzetto and M. Strube. 2006. Exploiting seman-
tic role labeling, WordNet and Wikipedia for coref-
erence resolution. In Proc. of HLT/NAACL, pages
192–199.
W. M. Soon, H. T. Ng, and D. Lim. 2001. A
machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics,
27(4):521–544.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009.
Conundrums in noun phrase coreference resolution:
Making sense of the state-of-the-art. In Proc. of the
ACL.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with
a cyclic dependency network. In Proc. of HLT-
NAACL, pages 252–259.
O. Uryupina. 2003. High-precision identification of
discourse new and unique noun phrases. In Proc. of
the ACL Student Research Workshop.
Y. Versley, A. Moschitti, M. Poesio, and X. Yang.
2008. Coreference systems based on kernel meth-
ods. In Proc. of COLING, pages 961–968.
R. Vieira and M. Poesio. 2000. Processing definite de-
scriptions in corpora. In Corpus-based and Compu-
tational Approaches to Discourse Anaphora, pages
189–212. UCL Press.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proc. ofMUC-6, pages 45–52.
M. Walker, A. Joshi, and E. Prince, editors. 1998.
Centering Theory in Discourse. Oxford University
Press.
X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003.
Coreference resolution using competitive learning
approach. In Proc. of the ACL, pages 176–183.
X. Yang, J. Su, G. Zhou, and C. L. Tan. 2004. An NP-
cluster based approach to coreference resolution. In
Proc. of COLING, pages 226–232.
X. Yang, J. Su, J. Lang, C. L. Tan, and S. Li. 2008.
An entity-mention model for coreference resolution
with inductive logic programming. In Proc. of the
ACL, pages 843–851.
</reference>
<page confidence="0.997901">
977
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.525140">
<title confidence="0.998917">Supervised Models for Coreference Resolution</title>
<author confidence="0.977108">Rahman</author>
<affiliation confidence="0.9879615">Human Language Technology Research University of Texas at</affiliation>
<address confidence="0.548897">Richardson, TX</address>
<abstract confidence="0.999706739130435">Traditional learning-based coreference reoperate by training a mentionfor determining whether two mentions are coreferent or not. Two independent lines of recent research have attempted to improve these mention-pair one by learning a mentionto rank preceding mentions for a given anaphor, and the other training an to determine whether a preceding cluster is coreferent with a given mention. We propose a cluster-ranking approach to coreference resolution that combines the strengths of mention rankers and entitymention models. We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-based crossdocument coreferencing using the vector space model.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL,</booktitle>
<pages>79--85</pages>
<contexts>
<context position="32427" citStr="Bagga and Baldwin, 1998" startWordPosition="5303" endWordPosition="5306">), person names (83.6k), person titles and honorifics (761), vehicle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms of PERSON (6.3k). We employ CRF++5, a C++ implementation of conditional random fields, for training the mention detector, which achieves an F-score of 86.7 (86.1 recall, 87.2 precision) on the test set. These extracted mentions are to be used as system mentions in our coreference experiments. Scoring programs. To score the output of a coreference model, we employ three scoring programs: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and φ3-CEAF (Luo, 2005). 5Available from http://crfpp.sourceforge.net 974 There is a complication, however. When scoring a response (i.e., system-generated) partition against a key (i.e., gold-standard) partition, a scoring program needs to construct a mapping between the mentions in the response and those in the key. If the response is generated using true mentions, then every mention in the response is mapped to some mention in the key and vice versa; in other words, there are no twinless (i.e., unmapped) mentions (Stoyanov et al., 2009). However, this is not the case when system mentions </context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Entity-based crossdocument coreferencing using the vector space model. In Proc. of COLING-ACL, pages 79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bean</author>
<author>E Riloff</author>
</authors>
<title>Corpus-based identification of non-anaphoric noun phrases.</title>
<date>1999</date>
<booktitle>In Proc. of the ACL,</booktitle>
<pages>373--380</pages>
<contexts>
<context position="11958" citStr="Bean and Riloff (1999)" startWordPosition="1781" endWordPosition="1784">s mentions rather than clusters, and relies on an independently-trained discourse-new detector. Discourse-new detection. Discourse-new detection is often tackled independently of coreference resolution. Pleonastic its have been detected using heuristics (e.g., Kennedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., M¨uller (2006)), kernels (e.g., Versley et al. (2008)), and distributional methods (e.g., Bergsma et al. (2008)). Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999)). General discourse-new detectors that are applicable to different types of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003)). There have also been attempts to perform joint inference for discourse-new detection and coreference resolution using integer linear programming (ILP), where a discourse-new classifier and a coreference classifier are trained independently of each other, and then ILP is applied as a post-processing step to jointly infer discourse-new and c</context>
</contexts>
<marker>Bean, Riloff, 1999</marker>
<rawString>D. Bean and E. Riloff. 1999. Corpus-based identification of non-anaphoric noun phrases. In Proc. of the ACL, pages 373–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bengtson</author>
<author>D Roth</author>
</authors>
<title>Understanding the values of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>294--303</pages>
<contexts>
<context position="13956" citStr="Bengtson and Roth (2008)" startWordPosition="2100" endWordPosition="2103">ning mentions in its own cluster. 3.1 Mention-Pair Model As noted before, a mention-pair model is a classifier that decides whether or not an active mention mk is coreferent with a candidate antecedent mj. Each instance i(mj, mk) represents mj and 970 [Barack Obama]11 nominated [Hillary Rodham Clinton]2 2 as [[his]13 secretary ofstate]34 on [Monday]45. [He]16 ... Figure 1: An illustrative example mk and consists of the 39 features shown in Table 1. These features have largely been employed by state-of-the-art learning-based coreference systems (e.g., Soon et al. (2001), Ng and Cardie (2002b), Bengtson and Roth (2008)), and are computed automatically. As can be seen, the features are divided into four blocks. The first two blocks consist of features that describe the properties of mj and mk, respectively, and the last two blocks of features describe the relationship between mj and mk. The classification associated with a training instance is either positive or negative, depending on whether mj and mk are coreferent. If one training instance were created from each pair of mentions, the negative instances would significantly outnumber the positives, yielding a skewed class distribution that will typically ha</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>E. Bengtson and D. Roth. 2008. Understanding the values of features for coreference resolution. In Proc. of EMNLP, pages 294–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bergsma</author>
<author>D Lin</author>
<author>R Goebel</author>
</authors>
<title>Distributional identification of non-referential pronouns.</title>
<date>2008</date>
<booktitle>In Proc. ofACL-08:HLT,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="11796" citStr="Bergsma et al. (2008)" startWordPosition="1757" endWordPosition="1760"> extension of Yang et al.’s (2003) twin-candidate coreference model, which ranks only two candidate antecedents at a time. Unlike ours, however, their model ranks mentions rather than clusters, and relies on an independently-trained discourse-new detector. Discourse-new detection. Discourse-new detection is often tackled independently of coreference resolution. Pleonastic its have been detected using heuristics (e.g., Kennedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., M¨uller (2006)), kernels (e.g., Versley et al. (2008)), and distributional methods (e.g., Bergsma et al. (2008)). Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999)). General discourse-new detectors that are applicable to different types of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003)). There have also been attempts to perform joint inference for discourse-new detection and coreference resolution using integer linear programming (ILP), where a discourse-new class</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>S. Bergsma, D. Lin, and R. Goebel. 2008. Distributional identification of non-referential pronouns. In Proc. ofACL-08:HLT, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="31130" citStr="Bikel et al. (1999)" startWordPosition="5100" endWordPosition="5103">ained a mention extractor on the training texts. Following Florian et al. (2004), we recast mention extraction as a sequence labeling task, where we assign to each token in a test text a label that indicates whether it begins a mention, is inside a mention, or is outside a mention. Hence, to learn the extractor, we create one training instance for each token in a training text and derive its class value (one of b, i, and o) from the annotated data. Each instance represents wi, the token under consideration, and consists of 29 linguistic features, many of which are modeled after the systems of Bikel et al. (1999) and Florian et al. (2004), as described below. Lexical (7): Tokens in a window of 7: {wi−3, ... , wi+3}. Capitalization (4): Determine whether wi IsAllCap, IsInitCap, IsCapPeriod, and IsAllLower (see Bikel et al. (1999)). Morphological (8): wi’s prefixes and suffixes of length one, two, three, and four. Grammatical (1): The part-of-speech (POS) tag of wi obtained using the Stanford log-linear POS tagger (Toutanova et al., 2003). Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer (Finkel et al., 2005). Gazetteers (8): Eight dictionaries containing</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>D. Bikel, R. Schwartz, and R. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning, 34(1–3):211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Byron</author>
<author>W Gegg-Harrison</author>
</authors>
<title>Eliminating non-referring noun phrases from coreference resolution.</title>
<date>2004</date>
<booktitle>In Proc. ofDAARC,</booktitle>
<pages>21--26</pages>
<contexts>
<context position="12109" citStr="Byron and Gegg-Harrison (2004)" startWordPosition="1803" endWordPosition="1806">n is often tackled independently of coreference resolution. Pleonastic its have been detected using heuristics (e.g., Kennedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., M¨uller (2006)), kernels (e.g., Versley et al. (2008)), and distributional methods (e.g., Bergsma et al. (2008)). Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999)). General discourse-new detectors that are applicable to different types of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003)). There have also been attempts to perform joint inference for discourse-new detection and coreference resolution using integer linear programming (ILP), where a discourse-new classifier and a coreference classifier are trained independently of each other, and then ILP is applied as a post-processing step to jointly infer discourse-new and coreference decisions so that they are consistent with each other (e.g., Denis and Baldridge (2007)). Joint inference is different from our jointlearnin</context>
</contexts>
<marker>Byron, Gegg-Harrison, 2004</marker>
<rawString>D. Byron and W. Gegg-Harrison. 2004. Eliminating non-referring noun phrases from coreference resolution. In Proc. ofDAARC, pages 21–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>M Wick</author>
<author>A McCallum</author>
</authors>
<title>Firstorder probabilistic models for coreference resolution.</title>
<date>2007</date>
<booktitle>In Proc. ofNAACL-HLT,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="16305" citStr="Culotta et al., 2007" startWordPosition="2488" endWordPosition="2491">considered discourse-new (i.e., no antecedent will be selected for mk). 3.2 Entity-Mention Model Unlike a mention-pair model, an entity-mention model is a classifier that decides whether or not an active mention mk is coreferent with a partial cluster cj that precedes mk. Each training instance, i(cj, mk), represents cj and mk. The features for an instance can be divided into two types: (1) features that describe mk (i.e, those shown in the second block of Table 1), and (2) cluster-level features, which describe the relationship between cj and mk. Motivated by previous work (Luo et al., 2004; Culotta et al., 2007; Yang et al., 2008), we create cluster-level features from mention-pair features using four predicates: NONE, MOST-FALSE, MOST-TRUE, and ALL. Specifically, for each feature X shown in the last two blocks in Table 1, we first convert X into an equivalent set of binary-valued features if it is multi-valued. Then, for each resulting binaryvalued feature Xb, we create four binary-valued cluster-level features: (1) NONE-Xb is true when Xb is false between mk and each mention in cj; (2) MOST-FALSE-Xb is true when Xb is true between mk and less than half (but at least one) of the mentions in cj; (3)</context>
</contexts>
<marker>Culotta, Wick, McCallum, 2007</marker>
<rawString>A. Culotta, M. Wick, and A. McCallum. 2007. Firstorder probabilistic models for coreference resolution. In Proc. ofNAACL-HLT, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>Global, joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>236--243</pages>
<contexts>
<context position="12656" citStr="Denis and Baldridge (2007)" startWordPosition="1883" endWordPosition="1886">s of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003)). There have also been attempts to perform joint inference for discourse-new detection and coreference resolution using integer linear programming (ILP), where a discourse-new classifier and a coreference classifier are trained independently of each other, and then ILP is applied as a post-processing step to jointly infer discourse-new and coreference decisions so that they are consistent with each other (e.g., Denis and Baldridge (2007)). Joint inference is different from our jointlearning approach, which allows the two tasks to be learned jointly and not independently. 3 Baseline Coreference Models In this section, we describe three coreference models that will serve as our baselines: the mentionpair model, the entity-mention model, and the mention-ranking model. For illustrative purposes, we will use the text segment shown in Figure 1. Each mention m in the segment is annotated as [m]cidmid, where mid is the mention id and cid is the id of the cluster to which m belongs. As we can see, the mentions are partitioned into fou</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>P. Denis and J. Baldridge. 2007. Global, joint determination of anaphoricity and coreference resolution using integer programming. In Proc. of NAACL-HLT, pages 236–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>Specialized models and ranking for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>660--669</pages>
<contexts>
<context position="2531" citStr="Denis and Baldridge (2008)" startWordPosition="376" endWordPosition="379">to answer the critical question of which candidate antecedent is most probable. Second, they have limitations in their expressiveness: the information extracted from the two mentions alone may not be sufficient for making an informed coreference decision, especially if the candidate antecedent is a pronoun (which is semantically empty) or a mention that lacks descriptive information such as gender (e.g., Clinton). To address the first weakness, researchers have attempted to train a mention-ranking model for determining which candidate antecedent is most probable given an active mention (e.g., Denis and Baldridge (2008)). Ranking is arguably a more natural reformulation of coreference resolution than classification, as a ranker allows all candidate antecedents to be considered simultaneously and therefore directly captures the competition among them. Another desirable consequence is that there exists a natural resolution strategy for a ranking approach: a mention is resolved to the candidate antecedent that has the highest rank. This contrasts with classification-based approaches, where many clustering algorithms have been employed to co-ordinate the pairwise coreference decisions (because it is unclear whic</context>
<context position="11113" citStr="Denis and Baldridge (2008)" startWordPosition="1660" endWordPosition="1663">al. (2008) have proposed another entity-mention model trained by inductive logic programming. Like their previous work, the scarcity of clusterlevel predicates (only two are used) under-exploits the expressiveness of entity-mention models. Mention ranking. The notion of ranking candidate antecedents can be traced back to centering algorithms, many of which use grammatical roles to rank forward-looking centers (see Grosz et al. (1995), Walker et al. (1998), and Mitkov (2002)). However, mention ranking has been employed in learning-based coreference resolvers only recently. As mentioned before, Denis and Baldridge (2008) train a mention-ranking model. Their work can be viewed as an extension of Yang et al.’s (2003) twin-candidate coreference model, which ranks only two candidate antecedents at a time. Unlike ours, however, their model ranks mentions rather than clusters, and relies on an independently-trained discourse-new detector. Discourse-new detection. Discourse-new detection is often tackled independently of coreference resolution. Pleonastic its have been detected using heuristics (e.g., Kennedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., M¨uller (2006)), kernels (e.</context>
<context position="23957" citStr="Denis and Baldridge (2008)" startWordPosition="3890" endWordPosition="3893">ance i(mj, mk) in Sk is the rank of mj among competing candidate antecedents, which is 2 if mj is the closest antecedent of mk, and 1 otherwise.1 To exemplify, consider our running example. As in the mention-pair model, three training instances will be generated for He: i(Monday, He), i(secretary of state, He), i(his, He). The third instance will have a class value of 2, and the remaining two will have a class value of 1. After training, the mention-ranking model is applied to rank the candidate antecedents for an active mention in a test text as follows. Given an active mention mk, we follow Denis and Baldridge (2008) and use an independently-trained classifier to determine whether mk is discourse-new. If so, mk will not be resolved. Otherwise, we create test instances for mk by pairing it with each of its preceding mentions. The test instances are then presented to the ranker, and the preceding mention that is assigned the largest value by the ranker is selected as the antecedent of mk. The discourse-new classifier used in the resolution step is trained with 26 of the 37 features2 described in Ng and Cardie (2002a) that are deemed useful for distinguishing between anaphoric and non-anaphoric mentions. The</context>
</contexts>
<marker>Denis, Baldridge, 2008</marker>
<rawString>P. Denis and J. Baldridge. 2008. Specialized models and ranking for coreference resolution. In Proc. of EMNLP, pages 660–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elsner</author>
<author>E Charniak</author>
</authors>
<title>A generative discourse-new model for text coherence.</title>
<date>2007</date>
<tech>Technical Report CS-07-04,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="12169" citStr="Elsner and Charniak (2007)" startWordPosition="1811" endWordPosition="1815">nastic its have been detected using heuristics (e.g., Kennedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., M¨uller (2006)), kernels (e.g., Versley et al. (2008)), and distributional methods (e.g., Bergsma et al. (2008)). Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999)). General discourse-new detectors that are applicable to different types of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003)). There have also been attempts to perform joint inference for discourse-new detection and coreference resolution using integer linear programming (ILP), where a discourse-new classifier and a coreference classifier are trained independently of each other, and then ILP is applied as a post-processing step to jointly infer discourse-new and coreference decisions so that they are consistent with each other (e.g., Denis and Baldridge (2007)). Joint inference is different from our jointlearning approach, which allows the two tasks to be learned jointly</context>
</contexts>
<marker>Elsner, Charniak, 2007</marker>
<rawString>M. Elsner and E. Charniak. 2007. A generative discourse-new model for text coherence. Technical Report CS-07-04, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="31683" citStr="Finkel et al., 2005" startWordPosition="5188" endWordPosition="5191"> many of which are modeled after the systems of Bikel et al. (1999) and Florian et al. (2004), as described below. Lexical (7): Tokens in a window of 7: {wi−3, ... , wi+3}. Capitalization (4): Determine whether wi IsAllCap, IsInitCap, IsCapPeriod, and IsAllLower (see Bikel et al. (1999)). Morphological (8): wi’s prefixes and suffixes of length one, two, three, and four. Grammatical (1): The part-of-speech (POS) tag of wi obtained using the Stanford log-linear POS tagger (Toutanova et al., 2003). Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer (Finkel et al., 2005). Gazetteers (8): Eight dictionaries containing pronouns (77 entries), common words and words that are not names (399.6k), person names (83.6k), person titles and honorifics (761), vehicle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms of PERSON (6.3k). We employ CRF++5, a C++ implementation of conditional random fields, for training the mention detector, which achieves an F-score of 86.7 (86.1 recall, 87.2 precision) on the test set. These extracted mentions are to be used as system mentions in our coreference experiments. Scoring</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. R. Finkel, T. Grenager, and C. Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proc. of the ACL, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Hassan</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>X Luo</author>
<author>N Nicolov</author>
<author>I Zitouni</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In Proc. ofHLT/NAACL.</booktitle>
<contexts>
<context position="30591" citStr="Florian et al. (2004)" startWordPosition="5003" endWordPosition="5007"> the two sets have the same proportion of documents from the six sources. Mention extractor. We evaluate each coreference model using both true mentions (i.e., gold standard mentions4) and system mentions (i.e., au3Since we did not participate in ACE 2005, we do not have access to the official test set. 4Note that only mention boundaries are used. Dataset bn bc nw wl un cts # of documents 60 226 106 119 49 39 Table 2: Statistics for the ACE 2005 corpus tomatically identified mentions). To extract system mentions from a test text, we trained a mention extractor on the training texts. Following Florian et al. (2004), we recast mention extraction as a sequence labeling task, where we assign to each token in a test text a label that indicates whether it begins a mention, is inside a mention, or is outside a mention. Hence, to learn the extractor, we create one training instance for each token in a training text and derive its class value (one of b, i, and o) from the annotated data. Each instance represents wi, the token under consideration, and consists of 29 linguistic features, many of which are modeled after the systems of Bikel et al. (1999) and Florian et al. (2004), as described below. Lexical (7): </context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Zitouni, 2004</marker>
<rawString>R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and I. Zitouni. 2004. A statistical model for multilingual entity detection and tracking. In Proc. ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>A K Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="10924" citStr="Grosz et al. (1995)" startWordPosition="1634" endWordPosition="1637">that a strong entity-mention model can be trained without mention-pair-based features. Our ranking model, on the other hand, is trained without such features. More recently, Yang et al. (2008) have proposed another entity-mention model trained by inductive logic programming. Like their previous work, the scarcity of clusterlevel predicates (only two are used) under-exploits the expressiveness of entity-mention models. Mention ranking. The notion of ranking candidate antecedents can be traced back to centering algorithms, many of which use grammatical roles to rank forward-looking centers (see Grosz et al. (1995), Walker et al. (1998), and Mitkov (2002)). However, mention ranking has been employed in learning-based coreference resolvers only recently. As mentioned before, Denis and Baldridge (2008) train a mention-ranking model. Their work can be viewed as an extension of Yang et al.’s (2003) twin-candidate coreference model, which ranks only two candidate antecedents at a time. Unlike ours, however, their model ranks mentions rather than clusters, and relies on an independently-trained discourse-new detector. Discourse-new detection. Discourse-new detection is often tackled independently of coreferen</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="8659" citStr="Grosz and Sidner (1986)" startWordPosition="1281" endWordPosition="1284">gned to a preceding cluster is computed by summing over the weights associated with the factors applicable to the cluster, where the weights are determined heuristically, rather than learned, unlike ours. Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors. As a result, their cluster-ranking model employs only factors that capture the salience of a cluster, and can therefore be viewed as a simple model of attentional state (see Grosz and Sidner (1986)) realized by coreference clusters. By contrast, our resolution strategy is learned without applying hand-coded con969 straints in a separate filtering step. In particular, we attempt to determine the compatibility between a cluster and an active mention, using factors that determine not only salience (e.g., the distance between the cluster and the mention) but also lexical and grammatical compatibility, for instance. Entity-mention coreference models. Luo et al. (2004) represent one of the earliest attempts to investigate learning-based entity-mention models. They use the ANY predicate to gen</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>B. J. Grosz and C. L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data. In</title>
<date>2002</date>
<booktitle>Proc. ofKDD,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="15277" citStr="Joachims, 2002" startWordPosition="2320" endWordPosition="2321">training. Following Soon et al. (2001), we create (1) a positive instance for each discourse-old mention mk and its closest antecedent mj; and (2) a negative instance for mk paired with each of the intervening mentions, mj+1, mj+2, ... , mk−1. In our running example shown in Figure 1, three training instances will be generated for He: i(Monday, He), i(secretary of state, He), and i(his, He). The first two of these instances will be labeled as negative, and the last one will be labeled as positive. To train a mention-pair classifier, we use the SVM learning algorithm from the SVMlight package (Joachims, 2002), converting all multi-valued features into an equivalent set of binary-valued features. After training, the resulting SVM classifier is used to identify an antecedent for a mention in a test text. Specifically, an active mention mk selects as its antecedent the closest preceding mention that is classified as coreferent with mk. If mk is not classified as coreferent with any preceding mention, it will be considered discourse-new (i.e., no antecedent will be selected for mk). 3.2 Entity-Mention Model Unlike a mention-pair model, an entity-mention model is a classifier that decides whether or no</context>
<context position="34597" citStr="Joachims, 2002" startWordPosition="5654" endWordPosition="5655">e penalized for identifying spurious coreference relations. On the other hand, we do not remove twinless mentions in the key partition, as we want to ensure that the resolver makes the correct (non-)coreference decisions for them. We believe that our proposal addresses Stoyanov et al.’s (2009) problem of having very low precision when applying the CEAF scorer to score partitions of system mentions. 5.2 Results and Discussions The mention-pair baseline. We train our first baseline, the mention-pair coreference classifier, using the SVM learning algorithm as implemented in the SVMlight package (Joachims, 2002).6 Results of this baseline using true mentions and system mentions, shown in row 1 of Tables 3 and 4, are reported in terms of recall (R), precision (P), and F-score (F) provided by the three scoring pro6For this and subsequent uses of the SVM learner in our experiments, we set all parameters to their default values. grams. As we can see, this baseline achieves Fscores of 54.3–70.0 and 53.4–62.5 for true mentions and system mentions, respectively. The entity-mention baseline. Next, we train our second baseline, the entity-mention coreference classifier, using the SVM learner. Results of this </context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Optimizing search engines using clickthrough data. In Proc. ofKDD, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
<author>D Appelt</author>
<author>L Taylor</author>
<author>A Simma</author>
</authors>
<title>The (non)utility of predicate-argument frequencies for pronoun interpretation. In</title>
<date>2004</date>
<booktitle>Proc. ofHLT/NAACL.</booktitle>
<contexts>
<context position="1443" citStr="Kehler et al. (2004)" startWordPosition="206" endWordPosition="209">We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches. 1 Introduction Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions) refer to the same real-world entity or concept. Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002b), Kehler et al. (2004), Ponzetto and Strube (2006)). Despite their initial successes, these mention-pair models have at least two major weaknesses. First, since each candidate antecedent for a mention to be resolved (henceforth an active mention) is considered independently of the others, these models only determine how good a candidate antecedent is relative to the active mention, but not how good a candidate antecedent is relative to other candidates. In other words, they fail to answer the critical question of which candidate antecedent is most probable. Second, they have limitations in their expressiveness: the</context>
</contexts>
<marker>Kehler, Appelt, Taylor, Simma, 2004</marker>
<rawString>A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The (non)utility of predicate-argument frequencies for pronoun interpretation. In Proc. ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kennedy</author>
<author>B Boguraev</author>
</authors>
<title>Anaphor for everyone: Pronominal anaphora resolution without a parser.</title>
<date>1996</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>113--118</pages>
<contexts>
<context position="11624" citStr="Kennedy and Boguraev (1996)" startWordPosition="1732" endWordPosition="1736">been employed in learning-based coreference resolvers only recently. As mentioned before, Denis and Baldridge (2008) train a mention-ranking model. Their work can be viewed as an extension of Yang et al.’s (2003) twin-candidate coreference model, which ranks only two candidate antecedents at a time. Unlike ours, however, their model ranks mentions rather than clusters, and relies on an independently-trained discourse-new detector. Discourse-new detection. Discourse-new detection is often tackled independently of coreference resolution. Pleonastic its have been detected using heuristics (e.g., Kennedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., M¨uller (2006)), kernels (e.g., Versley et al. (2008)), and distributional methods (e.g., Bergsma et al. (2008)). Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999)). General discourse-new detectors that are applicable to different types of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003)). There h</context>
</contexts>
<marker>Kennedy, Boguraev, 1996</marker>
<rawString>C. Kennedy and B. Boguraev. 1996. Anaphor for everyone: Pronominal anaphora resolution without a parser. In Proc. of COLING, pages 113–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lappin</author>
<author>H Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="7683" citStr="Lappin and Leass (1994)" startWordPosition="1132" endWordPosition="1135">tion models, significantly outperformed all competing approaches. This suggests that the use of an appropriate learning framework can bring us a long way towards highperformance coreference resolution. The rest of the paper is structured as follows. Section 2 discusses related work. Section 3 describes our baseline coreference models: mentionpair, entity-mention, and mention-ranking. We discuss our cluster-ranking approach in Section 4, evaluate it in Section 5, and conclude in Section 6. 2 Related Work Heuristic-based cluster ranking. As mentioned previously, the work most related to ours is Lappin and Leass (1994), whose goal is to perform pronoun resolution by assigning an anaphoric pronoun to the highest-scored preceding cluster. Nevertheless, Lappin and Leass’s work differs from ours in several respects. First, they only tackle pronoun resolution rather than the full coreference task. Second, their algorithm is heuristic-based; in particular, the score assigned to a preceding cluster is computed by summing over the weights associated with the factors applicable to the cluster, where the weights are determined heuristically, rather than learned, unlike ours. Like many heuristic-based pronoun resolver</context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>S. Lappin and H. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):535–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>S Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the Bell tree.</title>
<date>2004</date>
<booktitle>In Proc. of the ACL,</booktitle>
<pages>135--142</pages>
<contexts>
<context position="3290" citStr="Luo et al. (2004)" startWordPosition="482" endWordPosition="485">s to be considered simultaneously and therefore directly captures the competition among them. Another desirable consequence is that there exists a natural resolution strategy for a ranking approach: a mention is resolved to the candidate antecedent that has the highest rank. This contrasts with classification-based approaches, where many clustering algorithms have been employed to co-ordinate the pairwise coreference decisions (because it is unclear which one is the best). To address the second weakness, researchers have investigated the acquisition of entity-mention coreference models (e.g., Luo et al. (2004), Yang et al. (2004)). Unlike mention-pair models, these entity-mention models are trained to determine whether an active mention belongs to a preceding, possibly partially-formed, coreference cluster. Hence, they can employ cluster-level features (i.e., features that are defined over any subset of mentions in a preceding cluster), which makes them more expressive than mention-pair models. Motivated in part by these recently developed models, we propose in this paper a clusterranking approach to coreference resolution that combines the strengths of mention-ranking mod968 Proceedings of the 200</context>
<context position="6967" citStr="Luo et al., 2004" startWordPosition="1021" endWordPosition="1024"> rather unsatisfactory from a linguistic point of view. In particular, they have not leveraged advances in discourse-based anaphora resolution research in the 70s and 80s. Our work bridges this gap by realizing in a new machine learning framework ideas rooted in Lappin and Leass’s (1994) heuristic-based pronoun resolver, which in turn was motivated by classic salience-based approaches to anaphora resolution. Revealing the importance of adopting the right model. While entity-mention models have previously been shown to be worse or at best marginally better than their mention-pair counterparts (Luo et al., 2004; Yang et al., 2008), our cluster-ranking models, which are a natural extension of entity-mention models, significantly outperformed all competing approaches. This suggests that the use of an appropriate learning framework can bring us a long way towards highperformance coreference resolution. The rest of the paper is structured as follows. Section 2 discusses related work. Section 3 describes our baseline coreference models: mentionpair, entity-mention, and mention-ranking. We discuss our cluster-ranking approach in Section 4, evaluate it in Section 5, and conclude in Section 6. 2 Related Wor</context>
<context position="9133" citStr="Luo et al. (2004)" startWordPosition="1352" endWordPosition="1355">factors that capture the salience of a cluster, and can therefore be viewed as a simple model of attentional state (see Grosz and Sidner (1986)) realized by coreference clusters. By contrast, our resolution strategy is learned without applying hand-coded con969 straints in a separate filtering step. In particular, we attempt to determine the compatibility between a cluster and an active mention, using factors that determine not only salience (e.g., the distance between the cluster and the mention) but also lexical and grammatical compatibility, for instance. Entity-mention coreference models. Luo et al. (2004) represent one of the earliest attempts to investigate learning-based entity-mention models. They use the ANY predicate to generate clusterlevel features as follows: given a binary-valued feature X defined over a pair of mentions, they introduce an ANY-X cluster-level feature, which has the value TRUE if X is true between the active mention and any mention in the preceding cluster under consideration. Contrary to common wisdom, this entity-mention model underperforms its mention-pair counterpart in spite of the generalization from mention-pair to cluster-level features. In Yang et al.’s (2004)</context>
<context position="16283" citStr="Luo et al., 2004" startWordPosition="2484" endWordPosition="2487">ntion, it will be considered discourse-new (i.e., no antecedent will be selected for mk). 3.2 Entity-Mention Model Unlike a mention-pair model, an entity-mention model is a classifier that decides whether or not an active mention mk is coreferent with a partial cluster cj that precedes mk. Each training instance, i(cj, mk), represents cj and mk. The features for an instance can be divided into two types: (1) features that describe mk (i.e, those shown in the second block of Table 1), and (2) cluster-level features, which describe the relationship between cj and mk. Motivated by previous work (Luo et al., 2004; Culotta et al., 2007; Yang et al., 2008), we create cluster-level features from mention-pair features using four predicates: NONE, MOST-FALSE, MOST-TRUE, and ALL. Specifically, for each feature X shown in the last two blocks in Table 1, we first convert X into an equivalent set of binary-valued features if it is multi-valued. Then, for each resulting binaryvalued feature Xb, we create four binary-valued cluster-level features: (1) NONE-Xb is true when Xb is false between mk and each mention in cj; (2) MOST-FALSE-Xb is true when Xb is true between mk and less than half (but at least one) of t</context>
<context position="35772" citStr="Luo et al. (2004)" startWordPosition="5846" endWordPosition="5849">r, using the SVM learner. Results of this baseline are shown in row 2 of Tables 3 and 4. For true mentions, this baseline achieves an Fscore of 54.8–70.7. In comparison to the mentionpair baseline, F-score rises insignificantly according to all three scorers.7 Similar trends can be observed for system mentions, where the F-scores between the two models are statistically indistinguishable across the board. While the insignificant performance difference is somewhat surprising given the improved expressiveness of entitymention models over mention-pair models, similar trends have been reported by Luo et al. (2004). The mention-ranking baseline. Our third baseline is the mention-ranking coreference model, trained using the ranker-learning algorithm in SVMlight. To identify discourse-new mentions, we employ two methods. In the first method, we adopt a pipeline architecture, where we train an SVM classifier for discourse-new detection independently of the mention ranker on the training set using the 26 features described in Section 3.3. We then apply the resulting classifier to each test text to filter discourse-new mentions prior to coreference resolution. Results of the mention ranker are shown in row 3</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the Bell tree. In Proc. of the ACL, pages 135–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proc. ofHLT/EMNLP,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="32452" citStr="Luo, 2005" startWordPosition="5309" endWordPosition="5310">and honorifics (761), vehicle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms of PERSON (6.3k). We employ CRF++5, a C++ implementation of conditional random fields, for training the mention detector, which achieves an F-score of 86.7 (86.1 recall, 87.2 precision) on the test set. These extracted mentions are to be used as system mentions in our coreference experiments. Scoring programs. To score the output of a coreference model, we employ three scoring programs: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and φ3-CEAF (Luo, 2005). 5Available from http://crfpp.sourceforge.net 974 There is a complication, however. When scoring a response (i.e., system-generated) partition against a key (i.e., gold-standard) partition, a scoring program needs to construct a mapping between the mentions in the response and those in the key. If the response is generated using true mentions, then every mention in the response is mapped to some mention in the key and vice versa; in other words, there are no twinless (i.e., unmapped) mentions (Stoyanov et al., 2009). However, this is not the case when system mentions are used. The aforementio</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>X. Luo. 2005. On coreference resolution performance metrics. In Proc. ofHLT/EMNLP, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Conditional models of identity uncertainty with application to noun coreference.</title>
<date>2004</date>
<booktitle>In Advances in</booktitle>
<publisher>Longman.</publisher>
<contexts>
<context position="6028" citStr="McCallum and Wellner (2004)" startWordPosition="881" endWordPosition="884">E 2005 coreference data sets, cluster rankers outperform three competing models — mention-pair, entitymention, and mention-ranking models — by a large margin. Also, our joint-learning approach to discourse-new detection and coreference resolution consistently yields cluster rankers that outperform those adopting the pipeline architecture. Equally importantly, cluster rankers are conceptually simple and easy to implement and do not rely on sophisticated training and inference procedures to make coreference decisions in dependent relation to each other, unlike relational coreference models (see McCallum and Wellner (2004)). Bridging the gap between machine-learning approaches and linguistically-motivated approaches to coreference resolution. While machine learning approaches to coreference resolution have received a lot of attention since the mid90s, popular learning-based coreference frameworks such as the mention-pair model are arguably rather unsatisfactory from a linguistic point of view. In particular, they have not leveraged advances in discourse-based anaphora resolution research in the 70s and 80s. Our work bridges this gap by realizing in a new machine learning framework ideas rooted in Lappin and Lea</context>
</contexts>
<marker>McCallum, Wellner, 2004</marker>
<rawString>A. McCallum and B. Wellner. 2004. Conditional models of identity uncertainty with application to noun coreference. In Advances in NIPS. R. Mitkov. 2002. Anaphora Resolution. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitkov</author>
</authors>
<title>Robust pronoun resolution with limited knowledge.</title>
<date>1998</date>
<booktitle>In Proc. of COLING/ACL,</booktitle>
<pages>869--875</pages>
<contexts>
<context position="8305" citStr="Mitkov (1998)" startWordPosition="1226" endWordPosition="1227">oal is to perform pronoun resolution by assigning an anaphoric pronoun to the highest-scored preceding cluster. Nevertheless, Lappin and Leass’s work differs from ours in several respects. First, they only tackle pronoun resolution rather than the full coreference task. Second, their algorithm is heuristic-based; in particular, the score assigned to a preceding cluster is computed by summing over the weights associated with the factors applicable to the cluster, where the weights are determined heuristically, rather than learned, unlike ours. Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors. As a result, their cluster-ranking model employs only factors that capture the salience of a cluster, and can therefore be viewed as a simple model of attentional state (see Grosz and Sidner (1986)) realized by coreference clusters. By contrast, our resolution strategy is learned without applying hand-coded con969 straints in a separate filtering step. In particular, we attempt to determine the compatibility between a cluster and an active</context>
</contexts>
<marker>Mitkov, 1998</marker>
<rawString>R. Mitkov. 1998. Robust pronoun resolution with limited knowledge. In Proc. of COLING/ACL, pages 869–875.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M¨uller</author>
</authors>
<title>Automatic detection of nonreferential it in spoken multi-party dialog.</title>
<date>2006</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>49--56</pages>
<marker>M¨uller, 2006</marker>
<rawString>C. M¨uller. 2006. Automatic detection of nonreferential it in spoken multi-party dialog. In Proc. of EACL, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution.</title>
<date>2002</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>730--736</pages>
<contexts>
<context position="1419" citStr="Ng and Cardie (2002" startWordPosition="202" endWordPosition="205"> entitymention models. We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches. 1 Introduction Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions) refer to the same real-world entity or concept. Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002b), Kehler et al. (2004), Ponzetto and Strube (2006)). Despite their initial successes, these mention-pair models have at least two major weaknesses. First, since each candidate antecedent for a mention to be resolved (henceforth an active mention) is considered independently of the others, these models only determine how good a candidate antecedent is relative to the active mention, but not how good a candidate antecedent is relative to other candidates. In other words, they fail to answer the critical question of which candidate antecedent is most probable. Second, they have limitations in t</context>
<context position="4965" citStr="Ng and Cardie (2002" startWordPosition="734" endWordPosition="737">s a new entity in a discourse) can be learned jointly with coreference resolution in our cluster-ranking framework. It is worth noting that researchers typically adopt a pipeline coreference architecture, performing discourse-new detection prior to coreference resolution and using the resulting information to prevent a coreference system from resolving mentions that are determined to be discourse-new (see Poesio et al. (2004) for an overview). As a result, errors in discourse-new detection could be propagated to the resolver, possibly leading to a deterioration of coreference performance (see Ng and Cardie (2002a)). Jointly learning discoursenew detection and coreference resolution can potentially address this error-propagation problem. In sum, we believe our work makes three main contributions to coreference resolution: Proposing a simple, yet effective coreference model. Our work advances the state-of-the-art in coreference resolution by bringing learningbased coreference systems to the next level of performance. When evaluated on the ACE 2005 coreference data sets, cluster rankers outperform three competing models — mention-pair, entitymention, and mention-ranking models — by a large margin. Also,</context>
<context position="13928" citStr="Ng and Cardie (2002" startWordPosition="2096" endWordPosition="2099">, and each of the remaining mentions in its own cluster. 3.1 Mention-Pair Model As noted before, a mention-pair model is a classifier that decides whether or not an active mention mk is coreferent with a candidate antecedent mj. Each instance i(mj, mk) represents mj and 970 [Barack Obama]11 nominated [Hillary Rodham Clinton]2 2 as [[his]13 secretary ofstate]34 on [Monday]45. [He]16 ... Figure 1: An illustrative example mk and consists of the 39 features shown in Table 1. These features have largely been employed by state-of-the-art learning-based coreference systems (e.g., Soon et al. (2001), Ng and Cardie (2002b), Bengtson and Roth (2008)), and are computed automatically. As can be seen, the features are divided into four blocks. The first two blocks consist of features that describe the properties of mj and mk, respectively, and the last two blocks of features describe the relationship between mj and mk. The classification associated with a training instance is either positive or negative, depending on whether mj and mk are coreferent. If one training instance were created from each pair of mentions, the negative instances would significantly outnumber the positives, yielding a skewed class distrib</context>
<context position="24463" citStr="Ng and Cardie (2002" startWordPosition="3979" endWordPosition="3982">for an active mention in a test text as follows. Given an active mention mk, we follow Denis and Baldridge (2008) and use an independently-trained classifier to determine whether mk is discourse-new. If so, mk will not be resolved. Otherwise, we create test instances for mk by pairing it with each of its preceding mentions. The test instances are then presented to the ranker, and the preceding mention that is assigned the largest value by the ranker is selected as the antecedent of mk. The discourse-new classifier used in the resolution step is trained with 26 of the 37 features2 described in Ng and Cardie (2002a) that are deemed useful for distinguishing between anaphoric and non-anaphoric mentions. These features can be broadly divided into two types: (1) features that encode the form of the mention (e.g., NP type, number, definiteness), and (2) features that compare the mention to one of its preceding mentions. 4 Coreference as Cluster Ranking In this section, we describe our cluster-ranking approach to NP coreference. As noted before, our approach aims to combine the strengths of entitymention models and mention-ranking models. 4.1 Training and Applying a Cluster Ranker For ease of exposition, we</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002a. Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution. In Proc. of COLING, pages 730–736.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proc. of the ACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="1419" citStr="Ng and Cardie (2002" startWordPosition="202" endWordPosition="205"> entitymention models. We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches. 1 Introduction Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions) refer to the same real-world entity or concept. Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002b), Kehler et al. (2004), Ponzetto and Strube (2006)). Despite their initial successes, these mention-pair models have at least two major weaknesses. First, since each candidate antecedent for a mention to be resolved (henceforth an active mention) is considered independently of the others, these models only determine how good a candidate antecedent is relative to the active mention, but not how good a candidate antecedent is relative to other candidates. In other words, they fail to answer the critical question of which candidate antecedent is most probable. Second, they have limitations in t</context>
<context position="4965" citStr="Ng and Cardie (2002" startWordPosition="734" endWordPosition="737">s a new entity in a discourse) can be learned jointly with coreference resolution in our cluster-ranking framework. It is worth noting that researchers typically adopt a pipeline coreference architecture, performing discourse-new detection prior to coreference resolution and using the resulting information to prevent a coreference system from resolving mentions that are determined to be discourse-new (see Poesio et al. (2004) for an overview). As a result, errors in discourse-new detection could be propagated to the resolver, possibly leading to a deterioration of coreference performance (see Ng and Cardie (2002a)). Jointly learning discoursenew detection and coreference resolution can potentially address this error-propagation problem. In sum, we believe our work makes three main contributions to coreference resolution: Proposing a simple, yet effective coreference model. Our work advances the state-of-the-art in coreference resolution by bringing learningbased coreference systems to the next level of performance. When evaluated on the ACE 2005 coreference data sets, cluster rankers outperform three competing models — mention-pair, entitymention, and mention-ranking models — by a large margin. Also,</context>
<context position="13928" citStr="Ng and Cardie (2002" startWordPosition="2096" endWordPosition="2099">, and each of the remaining mentions in its own cluster. 3.1 Mention-Pair Model As noted before, a mention-pair model is a classifier that decides whether or not an active mention mk is coreferent with a candidate antecedent mj. Each instance i(mj, mk) represents mj and 970 [Barack Obama]11 nominated [Hillary Rodham Clinton]2 2 as [[his]13 secretary ofstate]34 on [Monday]45. [He]16 ... Figure 1: An illustrative example mk and consists of the 39 features shown in Table 1. These features have largely been employed by state-of-the-art learning-based coreference systems (e.g., Soon et al. (2001), Ng and Cardie (2002b), Bengtson and Roth (2008)), and are computed automatically. As can be seen, the features are divided into four blocks. The first two blocks consist of features that describe the properties of mj and mk, respectively, and the last two blocks of features describe the relationship between mj and mk. The classification associated with a training instance is either positive or negative, depending on whether mj and mk are coreferent. If one training instance were created from each pair of mentions, the negative instances would significantly outnumber the positives, yielding a skewed class distrib</context>
<context position="24463" citStr="Ng and Cardie (2002" startWordPosition="3979" endWordPosition="3982">for an active mention in a test text as follows. Given an active mention mk, we follow Denis and Baldridge (2008) and use an independently-trained classifier to determine whether mk is discourse-new. If so, mk will not be resolved. Otherwise, we create test instances for mk by pairing it with each of its preceding mentions. The test instances are then presented to the ranker, and the preceding mention that is assigned the largest value by the ranker is selected as the antecedent of mk. The discourse-new classifier used in the resolution step is trained with 26 of the 37 features2 described in Ng and Cardie (2002a) that are deemed useful for distinguishing between anaphoric and non-anaphoric mentions. These features can be broadly divided into two types: (1) features that encode the form of the mention (e.g., NP type, number, definiteness), and (2) features that compare the mention to one of its preceding mentions. 4 Coreference as Cluster Ranking In this section, we describe our cluster-ranking approach to NP coreference. As noted before, our approach aims to combine the strengths of entitymention models and mention-ranking models. 4.1 Training and Applying a Cluster Ranker For ease of exposition, we</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002b. Improving machine learning approaches to coreference resolution. In Proc. of the ACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypothesis: An Introduction.</title>
<date>1989</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="37303" citStr="Noreen, 1989" startWordPosition="6092" endWordPosition="6093">ith coreference resolution using the method described in Section 4.2. While we discussed this joint learning method in the context of cluster ranking, it should be easy to see that the method is equally applicable to a mention ranker. Results of the mention ranker using this joint architecture are shown in row 4 of Tables 3 and 4. As we can see, the ranker achieves F-scores of 61.6–73.4 and 55.6–67.1 for true mentions and system mentions, respectively. For both types of mentions, the improvements over the corresponding results for the entity-mention baseline 7We use Approximate Randomization (Noreen, 1989) for testing statistical significance, with P set to 0.05. 975 Coreference Model R MUC F R CEAF F R B3 F P P P Mention-pair model 71.7 69.2 70.4 54.3 54.3 54.3 53.3 63.6 58.0 Entity-mention model 71.7 69.7 70.7 54.8 54.8 54.8 53.2 65.1 58.5 Mention-ranking model (Pipeline) 68.7 73.9 71.2 57.8 57.8 57.8 55.8 63.9 59.6 Mention-ranking model (Joint) 69.4 77.8 73.4 61.6 61.6 61.6 57.0 70.1 62.9 Cluster-ranking model (Pipeline) 71.7 78.2 74.8 61.8 61.8 61.8 58.2 69.1 63.2 Cluster-ranking model (Joint) 69.9 83.3 76.0 63.3 63.3 63.3 56.0 74.6 64.0 1 2 3 4 5 6 Table 3: MUC, CEAF, and B3 coreference re</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>E. W. Noreen. 1989. Computer Intensive Methods for Testing Hypothesis: An Introduction. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>O Uryupina</author>
<author>R Vieira</author>
<author>M AlexandrovKabadjov</author>
<author>R Goulart</author>
</authors>
<title>Discourse-new detectors for definite description resolution: A survey and a preliminary proposal.</title>
<date>2004</date>
<booktitle>In Proc. of the ACL Workshop on Reference Resolution.</booktitle>
<contexts>
<context position="4775" citStr="Poesio et al. (2004)" startWordPosition="704" endWordPosition="707">e clusters is the best to link to an active mention using a learned cluster ranker. In addition, we show how discourse-new detection (i.e., the task of determining whether a mention introduces a new entity in a discourse) can be learned jointly with coreference resolution in our cluster-ranking framework. It is worth noting that researchers typically adopt a pipeline coreference architecture, performing discourse-new detection prior to coreference resolution and using the resulting information to prevent a coreference system from resolving mentions that are determined to be discourse-new (see Poesio et al. (2004) for an overview). As a result, errors in discourse-new detection could be propagated to the resolver, possibly leading to a deterioration of coreference performance (see Ng and Cardie (2002a)). Jointly learning discoursenew detection and coreference resolution can potentially address this error-propagation problem. In sum, we believe our work makes three main contributions to coreference resolution: Proposing a simple, yet effective coreference model. Our work advances the state-of-the-art in coreference resolution by bringing learningbased coreference systems to the next level of performance</context>
</contexts>
<marker>Poesio, Uryupina, Vieira, AlexandrovKabadjov, Goulart, 2004</marker>
<rawString>M. Poesio, O. Uryupina, R. Vieira, M. AlexandrovKabadjov, and R. Goulart. 2004. Discourse-new detectors for definite description resolution: A survey and a preliminary proposal. In Proc. of the ACL Workshop on Reference Resolution.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proc. of HLT/NAACL,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="1471" citStr="Ponzetto and Strube (2006)" startWordPosition="210" endWordPosition="213">ow our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches. 1 Introduction Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions) refer to the same real-world entity or concept. Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002b), Kehler et al. (2004), Ponzetto and Strube (2006)). Despite their initial successes, these mention-pair models have at least two major weaknesses. First, since each candidate antecedent for a mention to be resolved (henceforth an active mention) is considered independently of the others, these models only determine how good a candidate antecedent is relative to the active mention, but not how good a candidate antecedent is relative to other candidates. In other words, they fail to answer the critical question of which candidate antecedent is most probable. Second, they have limitations in their expressiveness: the information extracted from </context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>S. P. Ponzetto and M. Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proc. of HLT/NAACL, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1398" citStr="Soon et al. (2001)" startWordPosition="198" endWordPosition="201"> mention rankers and entitymention models. We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches. 1 Introduction Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions) refer to the same real-world entity or concept. Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al. (2001), Ng and Cardie (2002b), Kehler et al. (2004), Ponzetto and Strube (2006)). Despite their initial successes, these mention-pair models have at least two major weaknesses. First, since each candidate antecedent for a mention to be resolved (henceforth an active mention) is considered independently of the others, these models only determine how good a candidate antecedent is relative to the active mention, but not how good a candidate antecedent is relative to other candidates. In other words, they fail to answer the critical question of which candidate antecedent is most probable. Second, they </context>
<context position="13907" citStr="Soon et al. (2001)" startWordPosition="2092" endWordPosition="2095">nd he in one cluster, and each of the remaining mentions in its own cluster. 3.1 Mention-Pair Model As noted before, a mention-pair model is a classifier that decides whether or not an active mention mk is coreferent with a candidate antecedent mj. Each instance i(mj, mk) represents mj and 970 [Barack Obama]11 nominated [Hillary Rodham Clinton]2 2 as [[his]13 secretary ofstate]34 on [Monday]45. [He]16 ... Figure 1: An illustrative example mk and consists of the 39 features shown in Table 1. These features have largely been employed by state-of-the-art learning-based coreference systems (e.g., Soon et al. (2001), Ng and Cardie (2002b), Bengtson and Roth (2008)), and are computed automatically. As can be seen, the features are divided into four blocks. The first two blocks consist of features that describe the properties of mj and mk, respectively, and the last two blocks of features describe the relationship between mj and mk. The classification associated with a training instance is either positive or negative, depending on whether mj and mk are coreferent. If one training instance were created from each pair of mentions, the negative instances would significantly outnumber the positives, yielding a</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. M. Soon, H. T. Ng, and D. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>N Gilbert</author>
<author>C Cardie</author>
<author>E Riloff</author>
</authors>
<title>Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art.</title>
<date>2009</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="32974" citStr="Stoyanov et al., 2009" startWordPosition="5390" endWordPosition="5393">e scoring programs: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and φ3-CEAF (Luo, 2005). 5Available from http://crfpp.sourceforge.net 974 There is a complication, however. When scoring a response (i.e., system-generated) partition against a key (i.e., gold-standard) partition, a scoring program needs to construct a mapping between the mentions in the response and those in the key. If the response is generated using true mentions, then every mention in the response is mapped to some mention in the key and vice versa; in other words, there are no twinless (i.e., unmapped) mentions (Stoyanov et al., 2009). However, this is not the case when system mentions are used. The aforementioned complication does not arise from the construction of the mapping, but from the fact that Bagga and Baldwin (1998) and Luo (2005) do not specify how to apply B3 and CEAF to score partitions generated from system mentions. We propose a simple solution to this problem: we remove all and only those twinless system mentions that are singletons before applying B3 and CEAF. The reason is simple: since the coreference resolver has successfully identified these mentions as singletons, it should not be penalized, and remov</context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, 2009</marker>
<rawString>V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009. Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proc. of HLTNAACL,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="31562" citStr="Toutanova et al., 2003" startWordPosition="5167" endWordPosition="5170">from the annotated data. Each instance represents wi, the token under consideration, and consists of 29 linguistic features, many of which are modeled after the systems of Bikel et al. (1999) and Florian et al. (2004), as described below. Lexical (7): Tokens in a window of 7: {wi−3, ... , wi+3}. Capitalization (4): Determine whether wi IsAllCap, IsInitCap, IsCapPeriod, and IsAllLower (see Bikel et al. (1999)). Morphological (8): wi’s prefixes and suffixes of length one, two, three, and four. Grammatical (1): The part-of-speech (POS) tag of wi obtained using the Stanford log-linear POS tagger (Toutanova et al., 2003). Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer (Finkel et al., 2005). Gazetteers (8): Eight dictionaries containing pronouns (77 entries), common words and words that are not names (399.6k), person names (83.6k), person titles and honorifics (761), vehicle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms of PERSON (6.3k). We employ CRF++5, a C++ implementation of conditional random fields, for training the mention detector, which achieves an F-score of 86.7 (86.1 recall, 87.2 preci</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proc. of HLTNAACL, pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Uryupina</author>
</authors>
<title>High-precision identification of discourse new and unique noun phrases.</title>
<date>2003</date>
<booktitle>In Proc. of the ACL Student Research Workshop.</booktitle>
<contexts>
<context position="12214" citStr="Uryupina (2003)" startWordPosition="1819" endWordPosition="1820">nnedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., M¨uller (2006)), kernels (e.g., Versley et al. (2008)), and distributional methods (e.g., Bergsma et al. (2008)). Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999)). General discourse-new detectors that are applicable to different types of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003)). There have also been attempts to perform joint inference for discourse-new detection and coreference resolution using integer linear programming (ILP), where a discourse-new classifier and a coreference classifier are trained independently of each other, and then ILP is applied as a post-processing step to jointly infer discourse-new and coreference decisions so that they are consistent with each other (e.g., Denis and Baldridge (2007)). Joint inference is different from our jointlearning approach, which allows the two tasks to be learned jointly and not independently. 3 Baseline Coreferenc</context>
</contexts>
<marker>Uryupina, 2003</marker>
<rawString>O. Uryupina. 2003. High-precision identification of discourse new and unique noun phrases. In Proc. of the ACL Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Versley</author>
<author>A Moschitti</author>
<author>M Poesio</author>
<author>X Yang</author>
</authors>
<title>Coreference systems based on kernel methods.</title>
<date>2008</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="11738" citStr="Versley et al. (2008)" startWordPosition="1749" endWordPosition="1752">in a mention-ranking model. Their work can be viewed as an extension of Yang et al.’s (2003) twin-candidate coreference model, which ranks only two candidate antecedents at a time. Unlike ours, however, their model ranks mentions rather than clusters, and relies on an independently-trained discourse-new detector. Discourse-new detection. Discourse-new detection is often tackled independently of coreference resolution. Pleonastic its have been detected using heuristics (e.g., Kennedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., M¨uller (2006)), kernels (e.g., Versley et al. (2008)), and distributional methods (e.g., Bergsma et al. (2008)). Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999)). General discourse-new detectors that are applicable to different types of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003)). There have also been attempts to perform joint inference for discourse-new detection and coreference resolution using int</context>
</contexts>
<marker>Versley, Moschitti, Poesio, Yang, 2008</marker>
<rawString>Y. Versley, A. Moschitti, M. Poesio, and X. Yang. 2008. Coreference systems based on kernel methods. In Proc. of COLING, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Vieira</author>
<author>M Poesio</author>
</authors>
<title>Processing definite descriptions in corpora.</title>
<date>2000</date>
<booktitle>In Corpus-based and Computational Approaches to Discourse Anaphora,</booktitle>
<pages>189--212</pages>
<publisher>UCL Press.</publisher>
<contexts>
<context position="11902" citStr="Vieira and Poesio (2000)" startWordPosition="1772" endWordPosition="1775">ecedents at a time. Unlike ours, however, their model ranks mentions rather than clusters, and relies on an independently-trained discourse-new detector. Discourse-new detection. Discourse-new detection is often tackled independently of coreference resolution. Pleonastic its have been detected using heuristics (e.g., Kennedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., M¨uller (2006)), kernels (e.g., Versley et al. (2008)), and distributional methods (e.g., Bergsma et al. (2008)). Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999)). General discourse-new detectors that are applicable to different types of NPs have been built using heuristics (e.g., Byron and Gegg-Harrison (2004)) and modeled generatively (e.g., Elsner and Charniak (2007)) and discriminatively (e.g., Uryupina (2003)). There have also been attempts to perform joint inference for discourse-new detection and coreference resolution using integer linear programming (ILP), where a discourse-new classifier and a coreference classifier are trained independently of each other, and then ILP is applied as a p</context>
</contexts>
<marker>Vieira, Poesio, 2000</marker>
<rawString>R. Vieira and M. Poesio. 2000. Processing definite descriptions in corpora. In Corpus-based and Computational Approaches to Discourse Anaphora, pages 189–212. UCL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proc. ofMUC-6,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="32397" citStr="Vilain et al., 1995" startWordPosition="5298" endWordPosition="5301">that are not names (399.6k), person names (83.6k), person titles and honorifics (761), vehicle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms of PERSON (6.3k). We employ CRF++5, a C++ implementation of conditional random fields, for training the mention detector, which achieves an F-score of 86.7 (86.1 recall, 87.2 precision) on the test set. These extracted mentions are to be used as system mentions in our coreference experiments. Scoring programs. To score the output of a coreference model, we employ three scoring programs: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and φ3-CEAF (Luo, 2005). 5Available from http://crfpp.sourceforge.net 974 There is a complication, however. When scoring a response (i.e., system-generated) partition against a key (i.e., gold-standard) partition, a scoring program needs to construct a mapping between the mentions in the response and those in the key. If the response is generated using true mentions, then every mention in the response is mapped to some mention in the key and vice versa; in other words, there are no twinless (i.e., unmapped) mentions (Stoyanov et al., 2009). However, this is not </context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proc. ofMUC-6, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>A Joshi</author>
<author>E Prince</author>
<author>editors</author>
</authors>
<title>Centering Theory in Discourse.</title>
<date>1998</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="10946" citStr="Walker et al. (1998)" startWordPosition="1638" endWordPosition="1641">mention model can be trained without mention-pair-based features. Our ranking model, on the other hand, is trained without such features. More recently, Yang et al. (2008) have proposed another entity-mention model trained by inductive logic programming. Like their previous work, the scarcity of clusterlevel predicates (only two are used) under-exploits the expressiveness of entity-mention models. Mention ranking. The notion of ranking candidate antecedents can be traced back to centering algorithms, many of which use grammatical roles to rank forward-looking centers (see Grosz et al. (1995), Walker et al. (1998), and Mitkov (2002)). However, mention ranking has been employed in learning-based coreference resolvers only recently. As mentioned before, Denis and Baldridge (2008) train a mention-ranking model. Their work can be viewed as an extension of Yang et al.’s (2003) twin-candidate coreference model, which ranks only two candidate antecedents at a time. Unlike ours, however, their model ranks mentions rather than clusters, and relies on an independently-trained discourse-new detector. Discourse-new detection. Discourse-new detection is often tackled independently of coreference resolution. Pleonas</context>
</contexts>
<marker>Walker, Joshi, Prince, editors, 1998</marker>
<rawString>M. Walker, A. Joshi, and E. Prince, editors. 1998. Centering Theory in Discourse. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>G Zhou</author>
<author>J Su</author>
<author>C L Tan</author>
</authors>
<title>Coreference resolution using competitive learning approach.</title>
<date>2003</date>
<booktitle>In Proc. of the ACL,</booktitle>
<pages>176--183</pages>
<marker>Yang, Zhou, Su, Tan, 2003</marker>
<rawString>X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003. Coreference resolution using competitive learning approach. In Proc. of the ACL, pages 176–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>J Su</author>
<author>G Zhou</author>
<author>C L Tan</author>
</authors>
<title>An NPcluster based approach to coreference resolution.</title>
<date>2004</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>226--232</pages>
<contexts>
<context position="3310" citStr="Yang et al. (2004)" startWordPosition="486" endWordPosition="489">simultaneously and therefore directly captures the competition among them. Another desirable consequence is that there exists a natural resolution strategy for a ranking approach: a mention is resolved to the candidate antecedent that has the highest rank. This contrasts with classification-based approaches, where many clustering algorithms have been employed to co-ordinate the pairwise coreference decisions (because it is unclear which one is the best). To address the second weakness, researchers have investigated the acquisition of entity-mention coreference models (e.g., Luo et al. (2004), Yang et al. (2004)). Unlike mention-pair models, these entity-mention models are trained to determine whether an active mention belongs to a preceding, possibly partially-formed, coreference cluster. Hence, they can employ cluster-level features (i.e., features that are defined over any subset of mentions in a preceding cluster), which makes them more expressive than mention-pair models. Motivated in part by these recently developed models, we propose in this paper a clusterranking approach to coreference resolution that combines the strengths of mention-ranking mod968 Proceedings of the 2009 Conference on Empi</context>
</contexts>
<marker>Yang, Su, Zhou, Tan, 2004</marker>
<rawString>X. Yang, J. Su, G. Zhou, and C. L. Tan. 2004. An NPcluster based approach to coreference resolution. In Proc. of COLING, pages 226–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>J Su</author>
<author>J Lang</author>
<author>C L Tan</author>
<author>S Li</author>
</authors>
<title>An entity-mention model for coreference resolution with inductive logic programming.</title>
<date>2008</date>
<booktitle>In Proc. of the ACL,</booktitle>
<pages>843--851</pages>
<contexts>
<context position="6987" citStr="Yang et al., 2008" startWordPosition="1025" endWordPosition="1028">tory from a linguistic point of view. In particular, they have not leveraged advances in discourse-based anaphora resolution research in the 70s and 80s. Our work bridges this gap by realizing in a new machine learning framework ideas rooted in Lappin and Leass’s (1994) heuristic-based pronoun resolver, which in turn was motivated by classic salience-based approaches to anaphora resolution. Revealing the importance of adopting the right model. While entity-mention models have previously been shown to be worse or at best marginally better than their mention-pair counterparts (Luo et al., 2004; Yang et al., 2008), our cluster-ranking models, which are a natural extension of entity-mention models, significantly outperformed all competing approaches. This suggests that the use of an appropriate learning framework can bring us a long way towards highperformance coreference resolution. The rest of the paper is structured as follows. Section 2 discusses related work. Section 3 describes our baseline coreference models: mentionpair, entity-mention, and mention-ranking. We discuss our cluster-ranking approach in Section 4, evaluate it in Section 5, and conclude in Section 6. 2 Related Work Heuristic-based cl</context>
<context position="10497" citStr="Yang et al. (2008)" startWordPosition="1572" endWordPosition="1575">stance to mk in the associated text. The feature set used to represent the instance is primarily composed of features that describe the relationship between mj and mk, as well as a few cluster-level features. In other words, the model still relies heavily on features used in a mention-pair model. In particular, the inclusion of mj in the feature vector representation to some extent reflects the authors’ lack of confidence that a strong entity-mention model can be trained without mention-pair-based features. Our ranking model, on the other hand, is trained without such features. More recently, Yang et al. (2008) have proposed another entity-mention model trained by inductive logic programming. Like their previous work, the scarcity of clusterlevel predicates (only two are used) under-exploits the expressiveness of entity-mention models. Mention ranking. The notion of ranking candidate antecedents can be traced back to centering algorithms, many of which use grammatical roles to rank forward-looking centers (see Grosz et al. (1995), Walker et al. (1998), and Mitkov (2002)). However, mention ranking has been employed in learning-based coreference resolvers only recently. As mentioned before, Denis and </context>
<context position="16325" citStr="Yang et al., 2008" startWordPosition="2492" endWordPosition="2495">ew (i.e., no antecedent will be selected for mk). 3.2 Entity-Mention Model Unlike a mention-pair model, an entity-mention model is a classifier that decides whether or not an active mention mk is coreferent with a partial cluster cj that precedes mk. Each training instance, i(cj, mk), represents cj and mk. The features for an instance can be divided into two types: (1) features that describe mk (i.e, those shown in the second block of Table 1), and (2) cluster-level features, which describe the relationship between cj and mk. Motivated by previous work (Luo et al., 2004; Culotta et al., 2007; Yang et al., 2008), we create cluster-level features from mention-pair features using four predicates: NONE, MOST-FALSE, MOST-TRUE, and ALL. Specifically, for each feature X shown in the last two blocks in Table 1, we first convert X into an equivalent set of binary-valued features if it is multi-valued. Then, for each resulting binaryvalued feature Xb, we create four binary-valued cluster-level features: (1) NONE-Xb is true when Xb is false between mk and each mention in cj; (2) MOST-FALSE-Xb is true when Xb is true between mk and less than half (but at least one) of the mentions in cj; (3) MOST-TRUE-Xb is tru</context>
</contexts>
<marker>Yang, Su, Lang, Tan, Li, 2008</marker>
<rawString>X. Yang, J. Su, J. Lang, C. L. Tan, and S. Li. 2008. An entity-mention model for coreference resolution with inductive logic programming. In Proc. of the ACL, pages 843–851.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>