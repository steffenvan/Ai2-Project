<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000558">
<title confidence="0.993566">
Align, Disambiguate and Walk: A Unified Approach for
Measuring Semantic Similarity
</title>
<author confidence="0.993281">
Mohammad Taher Pilehvar, David Jurgens and Roberto Navigli
</author>
<affiliation confidence="0.998855">
Department of Computer Science
Sapienza University of Rome
</affiliation>
<email confidence="0.99464">
{pilehvar,jurgens,navigli}@di.uniroma1.it
</email>
<sectionHeader confidence="0.997315" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999585722222222">
Semantic similarity is an essential com-
ponent of many Natural Language Pro-
cessing applications. However, prior meth-
ods for computing semantic similarity of-
ten operate at different levels, e.g., sin-
gle words or entire documents, which re-
quires adapting the method for each data
type. We present a unified approach to se-
mantic similarity that operates at multiple
levels, all the way from comparing word
senses to comparing text documents. Our
method leverages a common probabilistic
representation over word senses in order to
compare different types of linguistic data.
This unified representation shows state-of-
the-art performance on three tasks: seman-
tic textual similarity, word similarity, and
word sense coarsening.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999600357142857">
Semantic similarity is a core technique for many
topics in Natural Language Processing such as
Textual Entailment (Berant et al., 2012), Seman-
tic Role Labeling (F¨urstenau and Lapata, 2012),
and Question Answering (Surdeanu et al., 2011).
For example, textual similarity enables relevant
documents to be identified for information re-
trieval (Hliaoutakis et al., 2006), while identifying
similar words enables tasks such as paraphrasing
(Glickman and Dagan, 2003), lexical substitution
(McCarthy and Navigli, 2009), lexical simplifica-
tion (Biran et al., 2011), and Web search result
clustering (Di Marco and Navigli, 2013).
Approaches to semantic similarity have often
operated at separate levels: methods for word sim-
ilarity are rarely applied to documents or even sin-
gle sentences (Budanitsky and Hirst, 2006; Radin-
sky et al., 2011; Halawi et al., 2012), while
document-based similarity methods require more
linguistic features, which often makes them in-
applicable at the word or microtext level (Salton
et al., 1975; Maguitman et al., 2005; Elsayed et
al., 2008; Turney and Pantel, 2010). Despite the
potential advantages, few approaches to semantic
similarity operate at the sense level due to the chal-
lenge in sense-tagging text (Navigli, 2009); for ex-
ample, none of the top four systems in the recent
SemEval-2012 task on textual similarity compared
semantic representations that incorporated sense
information (Agirre et al., 2012).
We propose a unified approach to semantic sim-
ilarity across multiple representation levels from
senses to documents, which offers two signifi-
cant advantages. First, the method is applicable
independently of the input type, which enables
meaningful similarity comparisons across differ-
ent scales of text or lexical levels. Second, by op-
erating at the sense level, a unified approach is able
to identify the semantic similarities that exist in-
dependently of the text’s lexical forms and any se-
mantic ambiguity therein. For example, consider
the sentences:
</bodyText>
<listItem confidence="0.922383333333333">
t1. A manager fired the worker.
t2. An employee was terminated from work by
his boss.
</listItem>
<bodyText confidence="0.999947285714286">
A surface-based approach would label the sen-
tences as dissimilar due to the minimal lexical
overlap. However, a sense-based representation
enables detection of the similarity between the
meanings of the words, e.g., fire and terminate.
Indeed, an accurate, sense-based representation is
essential for cases where different words are used
to convey the same meaning.
The contributions of this paper are threefold.
First, we propose a new unified representation of
the meaning of an arbitrarily-sized piece of text,
referred to as a lexical item, using a sense-based
probability distribution. Second, we propose a
novel alignment-based method for word sense dis-
</bodyText>
<page confidence="0.934754">
1341
</page>
<note confidence="0.9131215">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1341–1351,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998711875">
ambiguation during semantic comparison. Third,
we demonstrate that this single representation can
achieve state-of-the-art performance on three sim-
ilarity tasks, each operating at a different lexical
level: (1) surpassing the highest scores on the
SemEval-2012 task on textual similarity (Agirre
et al., 2012) that compares sentences, (2) achiev-
ing a near-perfect performance on the TOEFL syn-
onym selection task proposed by Landauer and
Dumais (1997), which measures word pair sim-
ilarity, and also obtaining state-of-the-art perfor-
mance in terms of the correlation with human
judgments on the RG-65 dataset (Rubenstein and
Goodenough, 1965), and finally (3) surpassing the
performance of Snow et al. (2007) in a sense-
coarsening task that measures sense similarity.
</bodyText>
<sectionHeader confidence="0.992704" genericHeader="method">
2 A Unified Semantic Representation
</sectionHeader>
<bodyText confidence="0.999901692307692">
We propose a representation of any lexical item as
a distribution over a set of word senses, referred
to as the item’s semantic signature. We begin
with a formal description of the representation at
the sense level (Section 2.1). Following this, we
describe our alignment-based disambiguation al-
gorithm which enables us to produce sense-based
semantic signatures for those lexical items (e.g.,
words or sentences) which are not sense annotated
(Section 2.2). Finally, we propose three methods
for comparing these signatures (Section 2.3). As
our sense inventory, we use WordNet 3.0 (Fell-
baum, 1998).
</bodyText>
<subsectionHeader confidence="0.985446">
2.1 Semantic Signatures
</subsectionHeader>
<bodyText confidence="0.999968088888889">
The WordNet ontology provides a rich net-
work structure of semantic relatedness, connect-
ing senses directly with their hypernyms, and pro-
viding information on semantically similar senses
by virtue of their nearby locality in the network.
Given a particular node (sense) in the network, re-
peated random walks beginning at that node will
produce a frequency distribution over the nodes
in the graph visited during the walk. To ex-
tend beyond a single sense, the random walk may
be initialized and restarted from a set of senses
(seed nodes), rather than just one; this multi-seed
walk produces a multinomial distribution over all
the senses in WordNet with higher probability as-
signed to senses that are frequently visited from
the seeds. Prior work has demonstrated that multi-
nomials generated from random walks over Word-
Net can be successfully applied to linguistic tasks
such as word similarity (Hughes and Ramage,
2007; Agirre et al., 2009), paraphrase recogni-
tion, textual entailment (Ramage et al., 2009),
and pseudoword generation (Pilehvar and Navigli,
2013).
Formally, we define the semantic signature of
a lexical item as the multinomial distribution gen-
erated from the random walks over WordNet 3.0
where the set of seed nodes is the set of senses
present in the item. This representation encom-
passes both when the item is itself a single sense
and when the item is a sense-tagged sentence.
To construct each semantic signature, we use
the iterative method for calculating topic-sensitive
PageRank (Haveliwala, 2002). Let M be the ad-
jacency matrix for the WordNet network, where
edges connect senses according to the rela-
tions defined in WordNet (e.g., hypernymy and
meronymy). We further enrich M by connecting
a sense with all the other senses that appear in its
disambiguated gloss.1 Let ~v(0) denote the prob-
ability distribution for the starting location of the
random walker in the network. Given the set of
senses S in a lexical item, the probability mass
of ~v(0) is uniformly distributed across the senses
sz E S, with the mass for all sj E/ S set to zero.
The PageRank may then be computed using:
</bodyText>
<equation confidence="0.998874">
v~ (t) = (1 − α) Mv~(t−1) + α v~ (0) (1)
</equation>
<bodyText confidence="0.999815272727273">
where at each iteration the random walker may
jump to any node sz E S with probability α/|S|.
We follow standard convention and set α to 0.15.
We repeat the operation in Eq. 1 for 30 itera-
tions, which is sufficient for the distribution to
converge. The resulting probability vector ~v(t) is
the semantic signature of the lexical item, as it
has aggregated its senses’ similarities over the en-
tire graph. For our semantic signatures we used
the UKB2 off-the-shelf implementation of topic-
sensitive PageRank.
</bodyText>
<subsectionHeader confidence="0.996997">
2.2 Alignment-Based Disambiguation
</subsectionHeader>
<bodyText confidence="0.998553111111111">
Commonly, semantic comparisons are between
word pairs or sentence pairs that do not have their
lexical content sense-annotated, despite the poten-
tial utility of sense annotation in making seman-
tic comparisons. However, traditional forms of
word sense disambiguation are difficult for short
texts and single words because little or no con-
textual information is present to perform the dis-
ambiguation task. Therefore, we propose a novel
</bodyText>
<footnote confidence="0.999975">
1http://wordnet.princeton.edu
2http://ixa2.si.ehu.es/ukb/
</footnote>
<page confidence="0.992594">
1342
</page>
<figureCaption confidence="0.8814515">
Figure 1: (a) Example alignments of the first sense of term manager (in sentence t1) to the two first
senses of the word types in sentence t2, along with the similarity of the two senses’ semantic signatures;
(b) Alignments which maximize the similarities across words in t1 and t2 (the source side of an alignment
is taken as the disambiguated sense of its corresponding word).
</figureCaption>
<bodyText confidence="0.996523891891892">
alignment-based sense disambiguation that lever-
ages the content of the paired item in order to dis-
ambiguate each element. Leveraging the paired
item enables our approach to disambiguate where
traditional sense disambiguation methods can not
due to insufficient context.
We view sense disambiguation as an alignment
problem. Given two arbitrarily ordered texts, we
seek the semantic alignment that maximizes the
similarity of the senses of the context words in
both texts. To find this maximum we use an align-
ment procedure which, for each word type wi in
item T1, assigns wi to the sense that has the max-
imal similarity to any sense of the word types in
the compared text T2. Algorithm 1 formalizes the
alignment process, which produces a sense dis-
ambiguated representation as a result. Senses are
compared in terms of their semantic signatures,
denoted as function R. We consider multiple def-
initions of R, defined later in Section 2.3.
As a part of the disambiguation procedure, we
leverage the one sense per discourse heuristic of
Yarowsky (1995); given all the word types in two
compared lexical items, each type is assigned a
single sense, even if it is used multiple times. Ad-
ditionally, if the same word type appears in both
sentences, both will always be mapped to the same
sense. Although such a sense assignment is poten-
tially incorrect, assigning both types to the same
sense results in a representation that does no worse
than a surface-level comparison.
We illustrate the alignment-based disambigua-
tion procedure using the two example sentences t1
and t2 given in Section 1. Figure 1(a) illustrates
example alignments of the first sense of manager
to the first two senses of the word types in sentence
t2 along with the similarity of the two senses’
</bodyText>
<subsectionHeader confidence="0.342471">
Algorithm 1 Alignment-based Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.66568">
Input: T1 and T2, the sets of word types being compared
Output: P, the set of disambiguated senses for T1
</bodyText>
<listItem confidence="0.951353333333333">
1: P + -0
2: for each token ti E T1
3: max sim +- 0
4: best si +- null
5: for each token tj E T2
6: for each si E Senses(ti), sj E Senses(tj)
7: sim +- R(si, sj)
8: if sim &gt; max sim then
9: max sim = sim
10: best si = si
11: P +- P U {best si}
12: return P
</listItem>
<bodyText confidence="0.999545111111111">
semantic signatures. For the senses of manager,
sense manager1n obtains the maximal similarity
value to boss1n among all the possible pairings of
the senses for the word types in sentence t2, and as
a result is selected as the sense labeling for man-
ager in sentence t1.3 Figure 1(b) shows the final,
maximally-similar sense alignment of the word
types in t1 and t2. The resulting alignment pro-
duces the following sets of senses:
</bodyText>
<equation confidence="0.999944">
Pt1 = {manager1n, fire4v, worker1n}
Pt2 = {employee1n, terminate4v, work3n, boss2n}
</equation>
<bodyText confidence="0.9999615">
where Px denotes the corresponding set of senses
of sentence x.
</bodyText>
<subsectionHeader confidence="0.999356">
2.3 Semantic Signature Similarity
</subsectionHeader>
<bodyText confidence="0.997273333333333">
Cosine Similarity. In order to compare seman-
tic signatures, we adopt the Cosine similarity mea-
sure as a baseline method. The measure is com-
puted by treating each multinomial as a vector and
then calculating the normalized dot product of the
two signatures’ vectors.
</bodyText>
<footnote confidence="0.973748">
3We follow Navigli (2009) and denote with wip the i-th
sense of w in WordNet with part of speech p.
</footnote>
<page confidence="0.986461">
1343
</page>
<bodyText confidence="0.996223916666667">
However, a semantic signature is, in essence,
a weighted ranking of the importance of Word-
Net senses for each lexical item. Given that the
WordNet graph has a non-uniform structure, and
also given that different lexical items may be of
different sizes, the magnitudes of the probabilities
obtained may differ significantly between the two
multinomial distributions. Therefore, for com-
puting the similarity of two signatures, we also
consider two nonparametric methods that use the
ranking of the senses, rather than their probability
values, in the multinomial.
Weighted Overlap. Our first measure provides
a nonparametric similarity by comparing the simi-
larity of the rankings for intersection of the senses
in both semantic signatures. However, we addi-
tionally weight the similarity such that differences
in the highest ranks are penalized more than differ-
ences in lower ranks. We refer to this measure as
the Weighted Overlap. Let 5 denote the intersec-
tion of all senses with non-zero probability in both
signatures and rz denote the rank of sense s. E 5
in signature j, where rank 1 denotes the highest
rank. The sum of the two ranks r1� and r2� for a
sense is then inverted, which (1) weights higher
ranks more and (2) when summed, provides the
maximal value when a sense has the same rank in
both signatures. The unnormalized weighted over-
lap is then calculated as �|S|
�=1(r1 � + r2� )−1. Then,
to bound the similarity value in [0, 1], we normal-
ize the sum by its maximum value, E|S|
�=1(2i)−1,
which occurs when each sense has the same rank
in both signatures.
Top-k Jaccard. Our second measure uses the
ranking to identify the top-k senses in a signa-
ture, which are treated as the best representatives
of the conceptual associates. We hypothesize that
a specific rank ordering may be attributed to small
differences in the multinomial probabilities, which
can lower rank-based similarities when one of the
compared orderings is perturbed due to slightly
different probability values. Therefore, we con-
sider the top-k senses as an unordered set, with
equal importance in the signature. To compare two
signatures, we compute the Jaccard Index of the
two signatures’ sets:
</bodyText>
<equation confidence="0.777273">
RJ�,-(Uk, Vk) = |Uk (1 Vk |(2)
|Uk U Vk|
</equation>
<bodyText confidence="0.9995115">
where Uk denotes the set of k senses with the high-
est probability in the semantic signature U.
</bodyText>
<table confidence="0.967641666666667">
Dataset MSRvid MSRpar SMTeuroparl OnWN SMTnews
Training 750 750 734 - -
Test 750 750 459 750 399
</table>
<tableCaption confidence="0.758299">
Table 1: Statistics of the provided datasets for the
SemEval-2012 Semantic Textual Similarity task.
</tableCaption>
<sectionHeader confidence="0.969804" genericHeader="method">
3 Experiment 1: Textual Similarity
</sectionHeader>
<bodyText confidence="0.999792125">
Measuring semantic similarity of textual items has
applications in a wide variety of NLP tasks. As
our benchmark, we selected the recent SemEval-
2012 task on Semantic Textual Similarity (STS),
which was concerned with measuring the seman-
tic similarity of sentence pairs. The task received
considerable interest by facilitating a meaningful
comparison between approaches.
</bodyText>
<subsectionHeader confidence="0.99645">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999929">
Data. We follow the experimental setup used in
the STS task (Agirre et al., 2012), which provided
five test sets, two of which had accompanying
training data sets for tuning system performance.
Each sentence pair in the datasets was given a
score from 0 to 5 (low to high similarity) by hu-
man judges, with a high inter-annotator agreement
of around 0.90 when measured using the Pearson
correlation coefficient. Table 1 lists the number of
sentence pairs in training and test portions of each
dataset.
Comparison Systems. The top-ranking partic-
ipating systems in the SemEval-2012 task were
generally supervised systems utilizing a variety of
lexical resources and similarity measurement tech-
niques. We compare our results against the top
three systems of the 88 submissions: TLsim and
TLsyn, the two systems of ˇSari´c et al. (2012), and
the UKP2 system (B¨ar et al., 2012). UKP2 utilizes
extensive resources among which are a Distribu-
tional Thesaurus computed on 10M dependency-
parsed English sentences. In addition, the sys-
tem utilizes techniques such as Explicit Semantic
Analysis (Gabrilovich and Markovitch, 2007) and
makes use of resources such as Wiktionary and
Wikipedia, a lexical substitution system based on
supervised word sense disambiguation (Biemann,
2013), and a statistical machine translation sys-
tem. The TLsim system uses the New York Times
Annotated Corpus, Wikipedia, and Google Book
Ngrams. The TLsyn system also uses Google
Book Ngrams, as well as dependency parsing and
named entity recognition.
</bodyText>
<page confidence="0.971133">
1344
</page>
<table confidence="0.9981115">
ALL Ranking Mean System ALL Overall Mean Mpar Dataset-specific SMTn
ALLnrm ALLnrm Mvid SMTe OnWN
1 1 1 ADW 0.866 0.871 0.711 0.694 0.887 0.555 0.706 0.604
2 3 2 UKP2 0.824 0.858 0.677 0.683 0.873 0.528 0.664 0.493
3 4 6 TLsyn 0.814 0.857 0.660 0.698 0.862 0.361 0.704 0.468
4 2 3 TLsim 0.813 0.864 0.675 0.734 0.880 0.477 0.679 0.398
</table>
<tableCaption confidence="0.978892">
Table 2: Performance of our system (ADW) and the 3 top-ranking participating systems (out of 88) in
</tableCaption>
<bodyText confidence="0.997072045454546">
the SemEval-2012 Semantic Textual Similarity task. Rightmost columns report the corresponding Pear-
son correlation r for individual datasets, i.e., MSRpar (Mpar), MSRvid (Mvid), SMTeuroparl (SMTe),
OnWN (OnWN) and SMTnews (SMTn). We also provide scores according to the three official evalua-
tion metrics (i.e., ALL, ALLnrm, and Mean). Rankings are also presented based on the three metrics.
System Configuration. Here we describe the
configuration of our approach, which we have
called Align, Disambiguate and Walk (ADW). The
STS task uses human similarity judgments on an
ordinal scale from 0 to 5. Therefore, in ADW we
adopted a similar approach to generating similar-
ity values to that adopted by other participating
systems, whereby a supervised system is trained
to combine multiple similarity judgments to pro-
duce a final rating consistent with the human an-
notators. We utilized the WEKA toolkit (Hall et
al., 2009) to train a Gaussian Processes regression
model for each of the three training sets (cf. Table
1). The features discussed hereafter were consid-
ered in our regression model.
Main features. We used the scores calculated
using all three of our semantic signature compar-
ison methods as individual features. Although the
Jaccard comparison is parameterized, we avoided
tuning and instead used four features for distinct
values of k: 250, 500, 1000, and 2500.
String-based features. Additionally, because
the texts often contain named entities which are
not present in WordNet, we incorporated the sim-
ilarity values produced by four string-based mea-
sures, which were used by other teams in the STS
task: (1) longest common substring which takes
into account the length of the longest overlap-
ping contiguous sequence of characters (substring)
across two strings (Gusfield, 1997), (2) longest
common subsequence which, instead, finds the
longest overlapping subsequence of two strings
(Allison and Dix, 1986), (3) Greedy String Tiling
which allows reordering in strings (Wise, 1993),
and (4) the character/word n-gram similarity pro-
posed by Barr´on-Cede˜no et al. (2010).
We followed ˇSari´c et al. (2012) and used the
models trained on the SMTeuroparl and MSRpar
datasets for testing on the SMTnews and OnWN
test sets, respectively.
</bodyText>
<subsectionHeader confidence="0.997927">
3.2 STS Results
</subsectionHeader>
<bodyText confidence="0.999982">
Three evaluation metrics are provided by the or-
ganizers of the SemEval-2012 STS task, all of
which are based on Pearson correlation r of human
judgments with system outputs: (1) the correla-
tion value for the concatenation of all five datasets
(ALL), (2) a correlation value obtained on a con-
catenation of the outputs, separately normalized
by least square (ALLnrm), and (3) the weighted
average of Pearson correlations across datasets
(Mean). Table 2 shows the scores obtained by
ADW for the three evaluation metrics, as well as
the Pearson correlation values obtained on each
of the five test sets (rightmost columns). We also
show the results obtained by the three top-ranking
participating systems (i.e., UKP2, TLsim, and TL-
syn). The leftmost three columns show the system
rankings according to the three metrics.
As can be seen from Table 2, our system (ADW)
outperforms all the 88 participating systems ac-
cording to all the evaluation metrics. Our sys-
tem shows a statistically significant improvement
on the SMTnews dataset, with an increase in the
Pearson correlation of over 0.10. MSRpar (MPar)
is the only dataset in which TLsim (ˇSari´c et al.,
2012) achieves a higher correlation with human
judgments. Named entity features used by the TL-
sim system could be the reason for its better per-
formance on the MSRpar dataset, which contains
a large number of named entities.
</bodyText>
<subsectionHeader confidence="0.997675">
3.3 Similarity Measure Analysis
</subsectionHeader>
<bodyText confidence="0.997462875">
To gain more insight into the impact of our
alignment-based disambiguation approach, we
carried out a 10-fold cross-validation on the three
training datasets (cf. Table 1) using the systems
described hereafter.
ADW-MF. To build this system, we utilized our
main features only; i.e., we did not make use of
additional string-based features.
</bodyText>
<page confidence="0.961231">
1345
</page>
<bodyText confidence="0.997569826086956">
DW. Similarly to ADW-MF, this system utilized
the main features only. In DW, however, we re-
placed our alignment-based disambiguation phase
with a random walk-based WSD system that dis-
ambiguated the sentences separately, without per-
forming any alignment. As our WSD system,
we used UKB, a state-of-the-art knowledge-based
WSD system that is based on the same topic-
sensitive PageRank algorithm used by our ap-
proach. UKB initializes the algorithm from all
senses of the words in the context of a word to
be disambiguated. It then picks the most relevant
sense of the word according to the resulting prob-
ability vector. As the lexical knowledge base of
UKB, we used the same semantic network as that
utilized by our approach for calculating semantic
signatures.
Table 3 lists the performance values of the two
above-mentioned systems on the three training
sets in terms of Pearson correlation. In addition,
we present in the table correlation scores for four
other similarity measures reported by B¨ar et al.
(2012):
</bodyText>
<listItem confidence="0.985287368421053">
• Pairwise Word Similarity that comprises of
a set of WordNet-based similarity measures
proposed by Resnik (1995), Jiang and Con-
rath (1997), and Lin (1998b). The aggre-
gation strategy proposed by Corley and Mi-
halcea (2005) has been utilized for extend-
ing these word-to-word similarity measures
for calculating text-to-text similarities.
• Explicit Semantic Analysis (Gabrilovich
and Markovitch, 2007) where the high-
dimensional vectors are obtained on Word-
Net, Wikipedia and Wiktionary.
• Distributional Thesaurus where a similarity
score is computed similarly to that of Lin
(1998a) using a distributional thesaurus ob-
tained from a 10M dependency-parsed sen-
tences of English newswire.
• Character n-grams which were also used as
one of our additional features.
</listItem>
<bodyText confidence="0.890219">
As can be seen from Table 3, our alignment-
based disambiguation approach (ADW-MF) is
better suited to the task than a conventional WSD
approach (DW). Another interesting point is the
high scores achieved by the Character n-grams
</bodyText>
<table confidence="0.9996725">
Similarity measure Dataset
Mpar Mvid SMTe
DW 0.448 0.820 0.660
ADW-MF 0.485 0.842 0.721
Explicit Semantic Analysis 0.427 0.781 0.619
Pairwise Word Similarity 0.564 0.835 0.527
Distributional Thesaurus 0.494 0.481 0.365
Character n-grams 0.658 0.771 0.554
</table>
<tableCaption confidence="0.998277">
Table 3: Performance of our main-feature sys-
</tableCaption>
<bodyText confidence="0.980168882352941">
tem with conventional WSD (DW) and with the
alignment-based disambiguation approach (ADW-
MF) vs. four other similarity measures, using 10-
fold cross validation on the training datasets MSR-
par (Mpar), MSRvid (Mvid), and SMTeuroparl
(SMTe).
measure. This confirms that string-based meth-
ods are strong baselines for semantic textual sim-
ilarity. Except for the MSRpar (Mpar) dataset,
our system (ADW-MF) outperforms all other sim-
ilarity measures. The scores obtained by Explicit
Semantic Analysis and Distributional Thesaurus
are not competitive on any dataset. On the other
hand, Pairwise Word Similarity achieves a high
performance on MSRpar and MSRvid datasets,
but performs surprisingly low on the SMTeuroparl
dataset.
</bodyText>
<sectionHeader confidence="0.997974" genericHeader="method">
4 Experiment 2: Word Similarity
</sectionHeader>
<bodyText confidence="0.999988363636364">
We now proceed from the sentence level to the
word level. Word similarity has been a key prob-
lem for lexical semantics, with significant efforts
being made by approaches in distributional se-
mantics to accurately identify synonymous words
(Turney and Pantel, 2010). Different evaluation
methods exist in the literature for evaluating the
performance of a word-level semantic similarity
measure; we adopted two well-established bench-
marks: synonym recognition and correlating word
similarity judgments with those from human an-
notators.
For synonym recognition, we used the TOEFL
dataset created by Landauer and Dumais (1997).
The dataset consists of 80 multiple-choice syn-
onym questions from the TOEFL test; a word is
paired with four options, one of which is a valid
synonym. Test takers with English as a second
language averaged 64.5% correct. Despite multi-
ple approaches, only recently has the test been an-
swered perfectly (Bullinaria and Levy, 2012), un-
derscoring the challenge of synonym recognition.
</bodyText>
<page confidence="0.978328">
1346
</page>
<table confidence="0.989278555555555">
Approach Accuracy
PPMIC (Bullinaria and Levy, 2007) 85.00%
GLSA (Matveeva et al., 2005) 86.25%
LSA (Rapp, 2003) 92.50%
ADWJac 93.75±2.5%
ADWWO 95.00%
ADWCos 96.25%
PR (Turney et al., 2003) 97.50%
PCCP (Bullinaria and Levy, 2012) 100.00%
</table>
<tableCaption confidence="0.998248">
Table 4: Accuracy on the 80-question TOEFL
</tableCaption>
<bodyText confidence="0.931637866666667">
Synonym test. ADWJac, ADWWO, and ADWCos
correspond to results with the Jaccard, Weighted
Overlap and Cosine signature comparison mea-
sures, respectively.
For the similarity judgment evaluation, we
used as benchmark the RG-65 dataset created by
Rubenstein and Goodenough (1965). The dataset
contains 65 word pairs judged by 51 human sub-
jects on a scale of 0 to 4 according to their seman-
tic similarity. Ideally, a measure’s similarity judg-
ments are expected to be highly correlated with
those of humans. To be consistent with the previ-
ous literature (Hughes and Ramage, 2007; Agirre
et al., 2009), we used Spearman’s rank correlation
in our experiment.
</bodyText>
<subsectionHeader confidence="0.974536">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.998980416666667">
Our alignment-based sense disambiguation trans-
forms the task of comparing individual words
into that of calculating the similarity of the best-
matching sense pair across the two words. As
there is no training data we do not optimize the k
value for computing signature similarity with the
Jaccard index; instead, we report, for the synonym
recognition and the similarity judgment evalua-
tions, the respective range of accuracies and the
average correlation obtained upon using five val-
ues of k randomly selected in the range [50, 2500]:
678,1412,1692, 2358, 2387.
</bodyText>
<subsectionHeader confidence="0.995827">
4.2 Word Similarity Results: TOEFL dataset
</subsectionHeader>
<bodyText confidence="0.94889375">
Table 4 lists the accuracy performance of the sys-
tem in comparison to the existing state of the
art on the TOEFL test. ADWWO, ADWCos,
and ADWJac correspond to our approach when
Weighted Overlap, Cosine, and Jaccard signa-
ture comparison measures are used, respectively.
Despite not being tuned for the task, our model
achieves near-perfect performance, answering all
but three questions correctly with the Cosine mea-
sure. Among the top-performing approaches, only
Word Synonym choices (correct in bold)
fanciful familiar apparent* imaginative† logical
verbal oral† overt fitting verbose*
resolved settled* forgotten† publicized examined
percentage volume sample proportion profit†*
figure list solve* divide† express
highlight alter† imitate accentuate* restore
Table 5: Questions answered incorrectly by our
approach. Symbols † and ? correspond to the
choices of our approach with the Weighted Over-
lap and Cosine signature comparisons respec-
tively. We do not include the mistakes made when
the Jaccard measure was used as they vary with
the k value.
that of Rapp (2003) uses word senses, an approach
that is outperformed by our method.
The errors produced by our system were largely
the result of sense locality in the WordNet net-
work. Table 5 highlights the incorrect responses.
The synonym mistakes reveal cases where senses
of the two words are close in WordNet, indicating
some relatedness. For example, percentage may
be interpreted colloquially as monetary value (e.g.,
“give me my percentage”) and elicits the synonym
of profit in the economic domain, which ADW in-
correctly selects as a synonym.
</bodyText>
<subsectionHeader confidence="0.998216">
4.3 Word Similarity Results: RG-65 dataset
</subsectionHeader>
<bodyText confidence="0.999976555555556">
Table 6 shows the Spearman’s p rank correlation
coefficients with human judgments on the RG-65
dataset. As can be seen from the Table, our ap-
proach with the Weighted Overlap signature com-
parison improves over the similar approach of
Hughes and Ramage (2007) which, however, does
not involve the disambiguation step and considers
a word as a whole unit as represented by the set of
its senses.
</bodyText>
<sectionHeader confidence="0.989776" genericHeader="method">
5 Experiment 3: Sense Similarity
</sectionHeader>
<bodyText confidence="0.999974857142857">
WordNet is known to be a fine-grained sense in-
ventory with many related word senses (Palmer et
al., 2007). Accordingly, multiple approaches have
attempted to identify highly similar senses in or-
der to produce a coarse-grained sense inventory.
We adopt this task as a way of evaluating our sim-
ilarity measure at the sense level.
</bodyText>
<subsectionHeader confidence="0.992112">
5.1 Coarse-graining Background
</subsectionHeader>
<bodyText confidence="0.99001">
Earlier work on reducing the polysemy of sense
inventories has considered WordNet-based sense
relatedness measures (Mihalcea and Moldovan,
2001) and corpus-based vector representations of
</bodyText>
<page confidence="0.983186">
1347
</page>
<table confidence="0.997128571428571">
Approach Correlation
ADWCos 0.825
Agirre et al. (2009) 0.830
Hughes and Ramage (2007) 0.838
Zesch et al. (2008) 0.840
ADWJac 0.841
ADWWO 0.868
</table>
<tableCaption confidence="0.946526">
Table 6: Spearman’s p correlation coefficients
</tableCaption>
<bodyText confidence="0.907749">
with human judgments on the RG-65 dataset.
ADWJac, ADWWO, and ADWCos correspond to
results with the Jaccard, Weighted Overlap and
Cosine signature comparison measures respec-
tively.
</bodyText>
<table confidence="0.998542">
Onto SE-2 Onto + SE-2
Method Noun Verb Noun Verb Adj Noun Verb
RCos 0.406 0.522 0.450 0.465 0.484 0.441 0.485
RWO 0.421 0.544 0.483 0.482 0.531 0.470 0.503
RJac 0.418 0.531 0.478 0.473 0.501 0.465 0.493
SVM 0.370 0.455 NA NA 0.473 0.423 0.432
ODE 0.218 0.396 NA NA 0.371 0.331 0.288
</table>
<tableCaption confidence="0.989652">
Table 7: F-score sense merging evaluation on
</tableCaption>
<bodyText confidence="0.924327086956522">
three hand-labeled datasets: OntoNotes (Onto),
Senseval-2 (SE-2), and combined (Onto+SE-2).
Results are reported for all three of our signature
comparison measures and also for two previous
works (last two rows).
word senses (Agirre and Lopez, 2003; McCarthy,
2006). Navigli (2006) proposed an automatic ap-
proach for mapping WordNet senses to the coarse-
grained sense distinctions of the Oxford Dictio-
nary of English (ODE). The approach leverages
semantic similarities in gloss definitions and the
hierarchical relations between senses in the ODE
to cluster WordNet senses. As current state of
the art, Snow et al. (2007) developed a super-
vised SVM classifier that utilized, as its features,
several earlier sense relatedness techniques such
as those implemented in the WordNet::Similarity
package (Pedersen et al., 2004). The classifier
also made use of resources such as topic signatures
data (Agirre and de Lacalle, 2004), the WordNet
domain dataset (Magnini and Cavagli`a, 2000), and
the mappings of WordNet senses to ODE senses
produced by Navigli (2006).
</bodyText>
<subsectionHeader confidence="0.997266">
5.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9984934">
We benchmark the accuracy of our similarity mea-
sure in grouping word senses against those of Nav-
igli (2006) and Snow et al. (2007) on two datasets
of manually-labeled sense groupings of WordNet
senses: (1) sense groupings provided as a part of
the Senseval-2 English Lexical Sample WSD task
(Kilgarriff, 2001) which includes nouns, verbs and
adjectives; (2) sense groupings included in the
OntoNotes project4 (Hovy et al., 2006) for nouns
and verbs. Following the evaluation methodology
of Snow et al. (2007), we combine the Senseval-2
and OntoNotes datasets into a third dataset.
Snow et al. (2007) considered sense grouping as
a binary classification task whereby for each word
every possible pairing of senses has to be classified
</bodyText>
<footnote confidence="0.788956">
4Sense groupings belong to a pre-version 1.0: http://
cemantix.org/download/sense/ontonotes-sense-groups.tar.gz
</footnote>
<bodyText confidence="0.999357916666667">
as either merged or not-merged. We constructed
a simple threshold-based classifier to perform the
same binary classification. To this end, we cal-
culated the semantic similarity of each sense pair
and then used a threshold value t to classify the
pair as merged if similarity ≥ t and not-merged
otherwise. We sampled out 10% of the dataset for
tuning the value of t, thus adapting our classifier
to the fine granularity of the dataset. We used the
same held-out instances to perform a tuning of the
k value used for Jaccard index, over the same val-
ues of k as in Experiment 1 (cf. Section 3).
</bodyText>
<subsectionHeader confidence="0.998747">
5.3 Sense Merging Results
</subsectionHeader>
<bodyText confidence="0.999568454545454">
For a binary classification task, we can directly
calculate precision, recall and F-score by con-
structing a contingency table. We show in Ta-
ble 7 the F-score performance of our classifier as
obtained by an averaged 10-fold cross-validation.
Results are presented for all three of the mea-
sures of semantic signature comparison and for
the three datasets: OntoNotes, Senseval-2, and
the two combined. In addition, we show in Ta-
ble 7 the F-score results provided by Snow et al.
(2007) for their SVM-based system and for the
mapping-based approach of Navigli (2006), de-
noted by ODE.
Table 7 shows that our methodology yields im-
provements over previous work on both datasets
and for all parts of speech, irrespective of
the semantic signature comparison method used.
Among the three methods, Weighted Overlap
achieves the best performance, which demon-
strates that our transformation of semantic signa-
tures into ordered lists of concepts and calculating
similarity by rank comparison has been helpful.
</bodyText>
<page confidence="0.993198">
1348
</page>
<sectionHeader confidence="0.999871" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999986115942029">
Due to the wide applicability of semantic similar-
ity, significant efforts have been made at different
lexical levels. Early work on document-level sim-
ilarity was driven by information retrieval. Vector
space methods provided initial successes (Salton
et al., 1975), but often suffer from data spar-
sity when using small documents, or when doc-
uments use different word types, as in the case
of paraphrases. Later efforts such as LSI (Deer-
wester et al., 1990), PLSA (Hofmann, 2001) and
Topic Models (Blei et al., 2003; Steyvers and Grif-
fiths, 2007) overcame these sparsity issues using
dimensionality reduction techniques or modeling
the document using latent variables. However,
such methods were still most suitable for compar-
ing longer texts. Complementary approaches have
been developed specifically for comparing shorter
texts, such as those used in the SemEval-2012
STS task (Agirre et al., 2012). Most similar to
our approach are the methods of Islam and Inkpen
(2008) and Corley and Mihalcea (2005), who per-
formed a word-to-word similarity alignment; how-
ever, they did not operate at the sense level. Ram-
age et al. (2009) used a similar semantic represen-
tation of short texts from random walks on Word-
Net, which was applied to paraphrase recognition
and textual entailment. However, unlike our ap-
proach, their method does not perform sense dis-
ambiguation prior to building the representation
and therefore potentially suffers from ambiguity.
A significant amount of effort has also been put
into measuring similarity at the word level, fre-
quently by approaches that use distributional se-
mantics (Turney and Pantel, 2010). These meth-
ods use contextual features to represent semantics
at the word level, whereas our approach represents
word semantics at the sense level. Most similar to
our approach are those of Agirre et al. (2009) and
Hughes and Ramage (2007), which represent word
meaning as the multinomials produced from ran-
dom walks on the WordNet graph. However, un-
like our approach, neither of these disambiguates
the two words being compared, which potentially
conflates the meanings and lowers the similarity
judgment.
Measures of sense relatedness have frequently
leveraged the structural properties of WordNet
(e.g., path lengths) to compare senses. Budanit-
sky and Hirst (2006) provided a survey of such
WordNet-based measures. The main drawback
with these approaches lies in the WordNet struc-
ture itself, where frequently two semantically sim-
ilar senses are distant in the WordNet hierar-
chy. Possible solutions include relying on wider-
coverage networks such as WikiNet (Nastase and
Strube, 2013) or multilingual ones such as Babel-
Net (Navigli and Ponzetto, 2012b). Fewer works
have focused on measuring the similarity – as op-
posed to relatedness – between senses. The topic
signatures method of Agirre and Lopez (2003)
represents each sense as a vector over corpus-
derived features in order to build comparable sense
representations. However, topic signatures often
produce lower quality representations due to spar-
sity in the local structure of WordNet, especially
for rare senses. In contrast, the random walk
used in our approach provides a denser, and thus
more comparable, representation for all WordNet
senses.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999963045454545">
This paper presents a unified approach for comput-
ing semantic similarity at multiple lexical levels,
from word senses to texts. Our method leverages
a common probabilistic representation at the sense
level for all types of linguistic data. We demon-
strate that our semantic representation achieves
state-of-the-art performance in three experiments
using semantic similarity at different lexical levels
(i.e., sense, word, and text), surpassing the per-
formance of previous similarity measures that are
often specifically targeted for each level.
In future work, we plan to explore the impact of
the sense inventory-based network used in our se-
mantic signatures. Specifically, we plan to investi-
gate higher coverage inventories such as BabelNet
(Navigli and Ponzetto, 2012a), which will handle
texts with named entities and rare senses that are
not in WordNet, and will also enable cross-lingual
semantic similarity. Second, we plan to evaluate
our method on larger units of text and formalize
comparison methods between different lexical lev-
els.
</bodyText>
<sectionHeader confidence="0.99881" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999274166666667">
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We would like to thank Sameer S. Pradhan
for providing us with an earlier version of the
OntoNotes dataset.
</bodyText>
<page confidence="0.995264">
1349
</page>
<sectionHeader confidence="0.996054" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998976556521739">
Eneko Agirre and Oier Lopez de Lacalle. 2004. Publicly
available topic signatures for all WordNet nominal senses.
In Proceedings of LREC, pages 1123–1126, Lisbon, Por-
tugal.
Eneko Agirre and Oier Lopez. 2003. Clustering WordNet
word senses. In Proceedings of RANLP, pages 121–130,
Borovets, Bulgaria.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kraval-
ova, Marius Pas¸ca, and Aitor Soroa. 2009. A study
on similarity and relatedness using distributional and
WordNet-based approaches. In Proceedings of NAACL,
pages 19–27, Boulder, Colorado.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-
Agirre. 2012. SemEval-2012 task 6: A pilot on semantic
textual similarity. In Proceedings of SemEval-2012, pages
385–393, Montreal, Canada.
Lloyd Allison and Trevor I. Dix. 1986. A bit-string longest-
common-subsequence algorithm. Information Processing
Letters, 23(6):305–310.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual similar-
ity by combining multiple content similarity measures. In
Proceedings of SemEval-2012, pages 435–440, Montreal,
Canada.
Alberto Barr´on-Cede˜no, Paolo Rosso, Eneko Agirre, and
Gorka Labaka. 2010. Plagiarism detection across distant
language pairs. In Proceedings of COLING, pages 37–45,
Beijing, China.
Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2012.
Learning entailment relations by global graph structure
optimization. Computational Linguistics, 38(1):73–111.
Chris Biemann. 2013. Creating a system for lexical sub-
stitutions from scratch using crowdsourcing. Language
Resources and Evaluation, 47(1):97–122.
Or Biran, Samuel Brody, and No´emie Elhadad. 2011.
Putting it simply: a context-aware approach to lexical sim-
plification. In Proceedings of ACL, pages 496–501, Port-
land, Oregon.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003.
Latent Dirichlet Allocation. The Journal of Machine
Learning Research, 3:993–1022.
Alexander Budanitsky and Graeme Hirst. 2006. Evaluat-
ing WordNet-based measures of Lexical Semantic Relat-
edness. Computational Linguistics, 32(1):13–47.
John A. Bullinaria and Joseph. P. Levy. 2007. Extracting
semantic representations from word co-occurrence statis-
tics: A computational study. Behavior Research Methods,
(3):510.
John A. Bullinaria and Joseph P. Levy. 2012. Extracting
semantic representations from word co-occurrence statis-
tics: stop-lists, stemming, and SVD. Behavior Research
Methods, 44:890–907.
Courtney Corley and Rada Mihalcea. 2005. Measuring the
semantic similarity of texts. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equiva-
lence and Entailment, pages 13–18, Ann Arbor, Michigan.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer,
George W. Furnas, and Richard A. Harshman. 1990. In-
dexing by Latent Semantic Analysis. Journal of American
Society for Information Science, 41(6):391–407.
Antonio Di Marco and Roberto Navigli. 2013. Cluster-
ing and diversifying Web search results with graph-based
Word Sense Induction. Computational Linguistics, 39(3).
Tamer Elsayed, Jimmy Lin, and Douglas W. Oard. 2008.
Pairwise document similarity in large collections with
MapReduce. In Proceedings of ACL-HLT, pages 265–
268, Columbus, Ohio.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
Hagen F¨urstenau and Mirella Lapata. 2012. Semi-supervised
Semantic Role Labeling via structural alignment. Compu-
tational Linguistics, 38(1):135–171.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Comput-
ing semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of IJCAI, pages 1606–
1611, Hyderabad, India.
Oren Glickman and Ido Dagan. 2003. Acquiring lexi-
cal paraphrases from a single corpus. In Proceedings of
RANLP, pages 81–90, Borovets, Bulgaria.
Dan Gusfield. 1997. Algorithms on strings, trees, and se-
quences: computer science and computational biology.
Cambridge University Press.
Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and Yehuda
Koren. 2012. Large-scale learning of word relatedness
with constraints. In Proceedings of KDD, pages 1406–
1414, Beijing, China.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten. 2009.
The WEKA data mining software: an update. ACM
SIGKDD Explorations Newsletter, 11(1):10–18.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank. In
Proceedings of WWW, pages 517–526, Hawaii, USA.
Angelos Hliaoutakis, Giannis Varelas, Epimenidis Voutsakis,
Euripides GM Petrakis, and Evangelos Milios. 2006.
Information retrieval by semantic similarity. Interna-
tional Journal on Semantic Web and Information Systems,
2(3):55–73.
Thomas Hofmann. 2001. Unsupervised Learning by Prob-
abilistic Latent Semantic Analysis. Machine Learning,
42(1):177–196.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The
90% solution. In Proceedings of NAACL, pages 57–60,
NY, USA.
Thad Hughes and Daniel Ramage. 2007. Lexical semantic
relatedness with random graph walks. In Proceedings of
EMNLP-CoNLL, pages 581–589, Prague, Czech Repub-
lic.
Aminul Islam and Diana Inkpen. 2008. Semantic text sim-
ilarity using corpus-based word similarity and string sim-
ilarity. ACM Transactions on Knowledge Discovery from
Data, 2(2):10:1–10:25.
Jay J. Jiang and David W. Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy. In
Proceedings of ROCLING X, pages 19–30, Taiwan.
</reference>
<page confidence="0.757585">
1350
</page>
<reference confidence="0.999928538461539">
Adam Kilgarriff. 2001. English lexical sample task descrip-
tion. In Proceedings of Senseval, pages 17–20, Toulouse,
France.
Thomas K. Landauer and Susan T. Dumais. 1997. A solution
to Plato’s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge.
Psychological Review; Psychological Review, 104(2):211.
Dekang Lin. 1998a. Automatic retrieval and clustering of
similar words. In Proceedings of COLING, pages 768–
774, Montreal, Quebec, Canada.
Dekang Lin. 1998b. An information-theoretic definition of
similarity. In Proceedings of ICML, pages 296–304, San
Francisco, CA.
Bernardo Magnini and Gabriela Cavagli`a. 2000. Integrat-
ing subject field codes into WordNet. In Proceedings of
LREC, pages 1413–1418, Athens, Greece.
Ana G. Maguitman, Filippo Menczer, Heather Roinestad, and
Alessandro Vespignani. 2005. Algorithmic detection of
semantic similarity. In Proceedings of WWW, pages 107–
116, Chiba, Japan.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and
Christiaan Royer. 2005. Terms representation with gener-
alized latent semantic analysis. In Proceedings of RANLP,
Borovets, Bulgaria.
Diana McCarthy and Roberto Navigli. 2009. The English
lexical substitution task. Language Resources and Evalu-
ation, 43(2):139–159.
Diana McCarthy. 2006. Relating WordNet senses for word
sense disambiguation. In Proceedings of the Workshop on
Making Sense of Sense at EACL-06, pages 17–24, Trento,
Italy.
Rada Mihalcea and Dan Moldovan. 2001. Automatic gen-
eration of a coarse grained WordNet. In Proceedings
of NAACL Workshop on WordNet and Other Lexical Re-
sources, Pittsburgh, USA.
Vivi Nastase and Michael Strube. 2013. Transforming
Wikipedia into a large scale multilingual concept network.
Artificial Intelligence, 194:62–85.
Roberto Navigli and Simone Paolo Ponzetto. 2012a. Ba-
belNet: The automatic construction, evaluation and appli-
cation of a wide-coverage multilingual semantic network.
Artificial Intelligence, 193:217–250.
Roberto Navigli and Simone Paolo Ponzetto. 2012b. Babel-
Relate! a joint multilingual approach to computing seman-
tic relatedness. In Proceedings of AAAI, pages 108–114,
Toronto, Canada.
Roberto Navigli. 2006. Meaningful clustering of senses
helps boost Word Sense Disambiguation performance. In
Proceedings of COLING-ACL, pages 105–112, Sydney,
Australia.
Roberto Navigli. 2009. Word Sense Disambiguation: A sur-
vey. ACM Computing Surveys, 41(2):1–69.
Martha Palmer, Hoa Dang, and Christiane Fellbaum. 2007.
Making fine-grained and coarse-grained sense distinc-
tions, both manually and automatically. Natural Lan-
guage Engineering, 13(2):137–163.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. WordNet::Similarity - measuring the relatedness of
concepts. In Proceedings of AAAI, pages 144–152, San
Jose, CA.
Mohammad Taher Pilehvar and Roberto Navigli. 2013.
Paving the way to a large-scale pseudosense-annotated
dataset. In Proceedings of NAACL-HLT, pages 1100–
1109, Atlanta, USA.
Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and
Shaul Markovitch. 2011. A word at a time: comput-
ing word relatedness using temporal semantic analysis. In
Proceedings of WWW, pages 337–346, Hyderabad, India.
Daniel Ramage, Anna N. Rafferty, and Christopher D. Man-
ning. 2009. Random walks for text semantic similarity. In
Proceedings of the 2009 Workshop on Graph-based Meth-
ods for Natural Language Processing, pages 23–31, Sun-
tec, Singapore.
Reinhard Rapp. 2003. Word sense discovery based on sense
descriptor dissimilarity. In Proceedings of the Ninth Ma-
chine Translation Summit, pages 315–322, New Orleans,
LA.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings of
IJCAI, pages 448–453, Montreal, Canada.
Herbert Rubenstein and John B. Goodenough. 1965. Con-
textual correlates of synonymy. Communications of the
ACM, 8(10):627–633.
Gerard Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Communications of
the ACM, 18(11):613–620.
Rion Snow, Sushant Prakash, Daniel Jurafsky, and Andrew Y.
Ng. 2007. Learning to merge word senses. In EMNLP-
CoNLL, pages 1005–1014, Prague, Czech Republic.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of Latent Semantic Analysis,
427(7):424–440.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-factoid
questions from Web collections. Computational Linguis-
tics, 37(2):351–383.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Journal of
Artificial Intelligence Research, 37:141–188.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and
Victor Shnayder. 2003. Combining independent modules
to solve multiple-choice synonym and analogy problems.
In Proceedings of RANLP, pages 482–489, Borovets, Bul-
garia.
Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and
Bojana Dalbelo Baˇsi´c. 2012. Takelab: Systems for
measuring semantic text similarity. In Proceedings of
SemEval-2012, pages 441–448, Montreal, Canada.
Michael J. Wise. 1993. String similarity via greedy string
tiling and running Karp-Rabin matching. In Department
of Computer Science Technical Report, Sydney.
David Yarowsky. 1995. Unsupervised Word Sense Disam-
biguation rivaling supervised methods. In Proceedings of
ACL, pages 189–196, Cambridge, Massachusetts.
Torsten Zesch, Christof M¨uller, and Iryna Gurevych. 2008.
Using Wiktionary for computing semantic relatedness. In
Proceedings of AAAI, pages 861–866, Chicago, Illinois.
</reference>
<page confidence="0.992968">
1351
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948064">
<title confidence="0.9936195">Align, Disambiguate and Walk: A Unified Approach Measuring Semantic Similarity</title>
<author confidence="0.999282">Taher Pilehvar</author>
<author confidence="0.999282">David Jurgens</author>
<affiliation confidence="0.9884515">Department of Computer Sapienza University of</affiliation>
<abstract confidence="0.999122210526316">Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
</authors>
<title>Publicly available topic signatures for all WordNet nominal senses.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>1123--1126</pages>
<location>Lisbon, Portugal.</location>
<marker>Agirre, de Lacalle, 2004</marker>
<rawString>Eneko Agirre and Oier Lopez de Lacalle. 2004. Publicly available topic signatures for all WordNet nominal senses. In Proceedings of LREC, pages 1123–1126, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez</author>
</authors>
<title>Clustering WordNet word senses.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP,</booktitle>
<pages>121--130</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="30177" citStr="Agirre and Lopez, 2003" startWordPosition="4790" endWordPosition="4793">e signature comparison measures respectively. Onto SE-2 Onto + SE-2 Method Noun Verb Noun Verb Adj Noun Verb RCos 0.406 0.522 0.450 0.465 0.484 0.441 0.485 RWO 0.421 0.544 0.483 0.482 0.531 0.470 0.503 RJac 0.418 0.531 0.478 0.473 0.501 0.465 0.493 SVM 0.370 0.455 NA NA 0.473 0.423 0.432 ODE 0.218 0.396 NA NA 0.371 0.331 0.288 Table 7: F-score sense merging evaluation on three hand-labeled datasets: OntoNotes (Onto), Senseval-2 (SE-2), and combined (Onto+SE-2). Results are reported for all three of our signature comparison measures and also for two previous works (last two rows). word senses (Agirre and Lopez, 2003; McCarthy, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English (ODE). The approach leverages semantic similarities in gloss definitions and the hierarchical relations between senses in the ODE to cluster WordNet senses. As current state of the art, Snow et al. (2007) developed a supervised SVM classifier that utilized, as its features, several earlier sense relatedness techniques such as those implemented in the WordNet::Similarity package (Pedersen et al., 2004). The classifier also made u</context>
<context position="36327" citStr="Agirre and Lopez (2003)" startWordPosition="5762" endWordPosition="5765">es of WordNet (e.g., path lengths) to compare senses. Budanitsky and Hirst (2006) provided a survey of such WordNet-based measures. The main drawback with these approaches lies in the WordNet structure itself, where frequently two semantically similar senses are distant in the WordNet hierarchy. Possible solutions include relying on widercoverage networks such as WikiNet (Nastase and Strube, 2013) or multilingual ones such as BabelNet (Navigli and Ponzetto, 2012b). Fewer works have focused on measuring the similarity – as opposed to relatedness – between senses. The topic signatures method of Agirre and Lopez (2003) represents each sense as a vector over corpusderived features in order to build comparable sense representations. However, topic signatures often produce lower quality representations due to sparsity in the local structure of WordNet, especially for rare senses. In contrast, the random walk used in our approach provides a denser, and thus more comparable, representation for all WordNet senses. 7 Conclusions This paper presents a unified approach for computing semantic similarity at multiple lexical levels, from word senses to texts. Our method leverages a common probabilistic representation a</context>
</contexts>
<marker>Agirre, Lopez, 2003</marker>
<rawString>Eneko Agirre and Oier Lopez. 2003. Clustering WordNet word senses. In Proceedings of RANLP, pages 121–130, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and WordNet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>19--27</pages>
<location>Boulder, Colorado.</location>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In Proceedings of NAACL, pages 19–27, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
</authors>
<title>SemEval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of SemEval-2012,</booktitle>
<pages>385--393</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="2425" citStr="Agirre et al., 2012" startWordPosition="352" endWordPosition="355">ky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense information (Agirre et al., 2012). We propose a unified approach to semantic similarity across multiple representation levels from senses to documents, which offers two significant advantages. First, the method is applicable independently of the input type, which enables meaningful similarity comparisons across different scales of text or lexical levels. Second, by operating at the sense level, a unified approach is able to identify the semantic similarities that exist independently of the text’s lexical forms and any semantic ambiguity therein. For example, consider the sentences: t1. A manager fired the worker. t2. An emplo</context>
<context position="4229" citStr="Agirre et al., 2012" startWordPosition="620" endWordPosition="623">m, using a sense-based probability distribution. Second, we propose a novel alignment-based method for word sense dis1341 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1341–1351, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ambiguation during semantic comparison. Third, we demonstrate that this single representation can achieve state-of-the-art performance on three similarity tasks, each operating at a different lexical level: (1) surpassing the highest scores on the SemEval-2012 task on textual similarity (Agirre et al., 2012) that compares sentences, (2) achieving a near-perfect performance on the TOEFL synonym selection task proposed by Landauer and Dumais (1997), which measures word pair similarity, and also obtaining state-of-the-art performance in terms of the correlation with human judgments on the RG-65 dataset (Rubenstein and Goodenough, 1965), and finally (3) surpassing the performance of Snow et al. (2007) in a sensecoarsening task that measures sense similarity. 2 A Unified Semantic Representation We propose a representation of any lexical item as a distribution over a set of word senses, referred to as </context>
<context position="15094" citStr="Agirre et al., 2012" startWordPosition="2424" endWordPosition="2427">59 750 399 Table 1: Statistics of the provided datasets for the SemEval-2012 Semantic Textual Similarity task. 3 Experiment 1: Textual Similarity Measuring semantic similarity of textual items has applications in a wide variety of NLP tasks. As our benchmark, we selected the recent SemEval2012 task on Semantic Textual Similarity (STS), which was concerned with measuring the semantic similarity of sentence pairs. The task received considerable interest by facilitating a meaningful comparison between approaches. 3.1 Experimental Setup Data. We follow the experimental setup used in the STS task (Agirre et al., 2012), which provided five test sets, two of which had accompanying training data sets for tuning system performance. Each sentence pair in the datasets was given a score from 0 to 5 (low to high similarity) by human judges, with a high inter-annotator agreement of around 0.90 when measured using the Pearson correlation coefficient. Table 1 lists the number of sentence pairs in training and test portions of each dataset. Comparison Systems. The top-ranking participating systems in the SemEval-2012 task were generally supervised systems utilizing a variety of lexical resources and similarity measure</context>
<context position="34391" citStr="Agirre et al., 2012" startWordPosition="5456" endWordPosition="5459">fer from data sparsity when using small documents, or when documents use different word types, as in the case of paraphrases. Later efforts such as LSI (Deerwester et al., 1990), PLSA (Hofmann, 2001) and Topic Models (Blei et al., 2003; Steyvers and Griffiths, 2007) overcame these sparsity issues using dimensionality reduction techniques or modeling the document using latent variables. However, such methods were still most suitable for comparing longer texts. Complementary approaches have been developed specifically for comparing shorter texts, such as those used in the SemEval-2012 STS task (Agirre et al., 2012). Most similar to our approach are the methods of Islam and Inkpen (2008) and Corley and Mihalcea (2005), who performed a word-to-word similarity alignment; however, they did not operate at the sense level. Ramage et al. (2009) used a similar semantic representation of short texts from random walks on WordNet, which was applied to paraphrase recognition and textual entailment. However, unlike our approach, their method does not perform sense disambiguation prior to building the representation and therefore potentially suffers from ambiguity. A significant amount of effort has also been put int</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor GonzalezAgirre. 2012. SemEval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of SemEval-2012, pages 385–393, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lloyd Allison</author>
<author>Trevor I Dix</author>
</authors>
<title>A bit-string longestcommon-subsequence algorithm.</title>
<date>1986</date>
<journal>Information Processing Letters,</journal>
<volume>23</volume>
<issue>6</issue>
<contexts>
<context position="18889" citStr="Allison and Dix, 1986" startWordPosition="3023" endWordPosition="3026">tead used four features for distinct values of k: 250, 500, 1000, and 2500. String-based features. Additionally, because the texts often contain named entities which are not present in WordNet, we incorporated the similarity values produced by four string-based measures, which were used by other teams in the STS task: (1) longest common substring which takes into account the length of the longest overlapping contiguous sequence of characters (substring) across two strings (Gusfield, 1997), (2) longest common subsequence which, instead, finds the longest overlapping subsequence of two strings (Allison and Dix, 1986), (3) Greedy String Tiling which allows reordering in strings (Wise, 1993), and (4) the character/word n-gram similarity proposed by Barr´on-Cede˜no et al. (2010). We followed ˇSari´c et al. (2012) and used the models trained on the SMTeuroparl and MSRpar datasets for testing on the SMTnews and OnWN test sets, respectively. 3.2 STS Results Three evaluation metrics are provided by the organizers of the SemEval-2012 STS task, all of which are based on Pearson correlation r of human judgments with system outputs: (1) the correlation value for the concatenation of all five datasets (ALL), (2) a co</context>
</contexts>
<marker>Allison, Dix, 1986</marker>
<rawString>Lloyd Allison and Trevor I. Dix. 1986. A bit-string longestcommon-subsequence algorithm. Information Processing Letters, 23(6):305–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>UKP: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of SemEval-2012,</booktitle>
<pages>435--440</pages>
<location>Montreal, Canada.</location>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. UKP: Computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of SemEval-2012, pages 435–440, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Barr´on-Cede˜no</author>
<author>Paolo Rosso</author>
<author>Eneko Agirre</author>
<author>Gorka Labaka</author>
</authors>
<title>Plagiarism detection across distant language pairs.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>37--45</pages>
<location>Beijing, China.</location>
<marker>Barr´on-Cede˜no, Rosso, Agirre, Labaka, 2010</marker>
<rawString>Alberto Barr´on-Cede˜no, Paolo Rosso, Eneko Agirre, and Gorka Labaka. 2010. Plagiarism detection across distant language pairs. In Proceedings of COLING, pages 37–45, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Learning entailment relations by global graph structure optimization.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="1123" citStr="Berant et al., 2012" startWordPosition="157" endWordPosition="160"> the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2012</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2012. Learning entailment relations by global graph structure optimization. Computational Linguistics, 38(1):73–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Creating a system for lexical substitutions from scratch using crowdsourcing.</title>
<date>2013</date>
<journal>Language Resources and Evaluation,</journal>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="16282" citStr="Biemann, 2013" startWordPosition="2610" endWordPosition="2611">ces and similarity measurement techniques. We compare our results against the top three systems of the 88 submissions: TLsim and TLsyn, the two systems of ˇSari´c et al. (2012), and the UKP2 system (B¨ar et al., 2012). UKP2 utilizes extensive resources among which are a Distributional Thesaurus computed on 10M dependencyparsed English sentences. In addition, the system utilizes techniques such as Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) and makes use of resources such as Wiktionary and Wikipedia, a lexical substitution system based on supervised word sense disambiguation (Biemann, 2013), and a statistical machine translation system. The TLsim system uses the New York Times Annotated Corpus, Wikipedia, and Google Book Ngrams. The TLsyn system also uses Google Book Ngrams, as well as dependency parsing and named entity recognition. 1344 ALL Ranking Mean System ALL Overall Mean Mpar Dataset-specific SMTn ALLnrm ALLnrm Mvid SMTe OnWN 1 1 1 ADW 0.866 0.871 0.711 0.694 0.887 0.555 0.706 0.604 2 3 2 UKP2 0.824 0.858 0.677 0.683 0.873 0.528 0.664 0.493 3 4 6 TLsyn 0.814 0.857 0.660 0.698 0.862 0.361 0.704 0.468 4 2 3 TLsim 0.813 0.864 0.675 0.734 0.880 0.477 0.679 0.398 Table 2: Per</context>
</contexts>
<marker>Biemann, 2013</marker>
<rawString>Chris Biemann. 2013. Creating a system for lexical substitutions from scratch using crowdsourcing. Language Resources and Evaluation, 47(1):97–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Samuel Brody</author>
<author>No´emie Elhadad</author>
</authors>
<title>Putting it simply: a context-aware approach to lexical simplification.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>496--501</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="1546" citStr="Biran et al., 2011" startWordPosition="216" endWordPosition="219">, word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic simila</context>
</contexts>
<marker>Biran, Brody, Elhadad, 2011</marker>
<rawString>Or Biran, Samuel Brody, and No´emie Elhadad. 2011. Putting it simply: a context-aware approach to lexical simplification. In Proceedings of ACL, pages 496–501, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="34006" citStr="Blei et al., 2003" startWordPosition="5401" endWordPosition="5404"> lists of concepts and calculating similarity by rank comparison has been helpful. 1348 6 Related Work Due to the wide applicability of semantic similarity, significant efforts have been made at different lexical levels. Early work on document-level similarity was driven by information retrieval. Vector space methods provided initial successes (Salton et al., 1975), but often suffer from data sparsity when using small documents, or when documents use different word types, as in the case of paraphrases. Later efforts such as LSI (Deerwester et al., 1990), PLSA (Hofmann, 2001) and Topic Models (Blei et al., 2003; Steyvers and Griffiths, 2007) overcame these sparsity issues using dimensionality reduction techniques or modeling the document using latent variables. However, such methods were still most suitable for comparing longer texts. Complementary approaches have been developed specifically for comparing shorter texts, such as those used in the SemEval-2012 STS task (Agirre et al., 2012). Most similar to our approach are the methods of Islam and Inkpen (2008) and Corley and Mihalcea (2005), who performed a word-to-word similarity alignment; however, they did not operate at the sense level. Ramage e</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. The Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of Lexical Semantic Relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="1797" citStr="Budanitsky and Hirst, 2006" startWordPosition="255" endWordPosition="258"> 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense infor</context>
<context position="35785" citStr="Budanitsky and Hirst (2006)" startWordPosition="5675" endWordPosition="5679">o represent semantics at the word level, whereas our approach represents word semantics at the sense level. Most similar to our approach are those of Agirre et al. (2009) and Hughes and Ramage (2007), which represent word meaning as the multinomials produced from random walks on the WordNet graph. However, unlike our approach, neither of these disambiguates the two words being compared, which potentially conflates the meanings and lowers the similarity judgment. Measures of sense relatedness have frequently leveraged the structural properties of WordNet (e.g., path lengths) to compare senses. Budanitsky and Hirst (2006) provided a survey of such WordNet-based measures. The main drawback with these approaches lies in the WordNet structure itself, where frequently two semantically similar senses are distant in the WordNet hierarchy. Possible solutions include relying on widercoverage networks such as WikiNet (Nastase and Strube, 2013) or multilingual ones such as BabelNet (Navigli and Ponzetto, 2012b). Fewer works have focused on measuring the similarity – as opposed to relatedness – between senses. The topic signatures method of Agirre and Lopez (2003) represents each sense as a vector over corpusderived feat</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of Lexical Semantic Relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<pages>3--510</pages>
<contexts>
<context position="25098" citStr="Levy, 2007" startWordPosition="3994" endWordPosition="3995">gnition and correlating word similarity judgments with those from human annotators. For synonym recognition, we used the TOEFL dataset created by Landauer and Dumais (1997). The dataset consists of 80 multiple-choice synonym questions from the TOEFL test; a word is paired with four options, one of which is a valid synonym. Test takers with English as a second language averaged 64.5% correct. Despite multiple approaches, only recently has the test been answered perfectly (Bullinaria and Levy, 2012), underscoring the challenge of synonym recognition. 1346 Approach Accuracy PPMIC (Bullinaria and Levy, 2007) 85.00% GLSA (Matveeva et al., 2005) 86.25% LSA (Rapp, 2003) 92.50% ADWJac 93.75±2.5% ADWWO 95.00% ADWCos 96.25% PR (Turney et al., 2003) 97.50% PCCP (Bullinaria and Levy, 2012) 100.00% Table 4: Accuracy on the 80-question TOEFL Synonym test. ADWJac, ADWWO, and ADWCos correspond to results with the Jaccard, Weighted Overlap and Cosine signature comparison measures, respectively. For the similarity judgment evaluation, we used as benchmark the RG-65 dataset created by Rubenstein and Goodenough (1965). The dataset contains 65 word pairs judged by 51 human subjects on a scale of 0 to 4 according </context>
</contexts>
<marker>Levy, 2007</marker>
<rawString>John A. Bullinaria and Joseph. P. Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, (3):510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and SVD. Behavior Research Methods,</title>
<date>2012</date>
<pages>44--890</pages>
<contexts>
<context position="24989" citStr="Bullinaria and Levy, 2012" startWordPosition="3977" endWordPosition="3980">uating the performance of a word-level semantic similarity measure; we adopted two well-established benchmarks: synonym recognition and correlating word similarity judgments with those from human annotators. For synonym recognition, we used the TOEFL dataset created by Landauer and Dumais (1997). The dataset consists of 80 multiple-choice synonym questions from the TOEFL test; a word is paired with four options, one of which is a valid synonym. Test takers with English as a second language averaged 64.5% correct. Despite multiple approaches, only recently has the test been answered perfectly (Bullinaria and Levy, 2012), underscoring the challenge of synonym recognition. 1346 Approach Accuracy PPMIC (Bullinaria and Levy, 2007) 85.00% GLSA (Matveeva et al., 2005) 86.25% LSA (Rapp, 2003) 92.50% ADWJac 93.75±2.5% ADWWO 95.00% ADWCos 96.25% PR (Turney et al., 2003) 97.50% PCCP (Bullinaria and Levy, 2012) 100.00% Table 4: Accuracy on the 80-question TOEFL Synonym test. ADWJac, ADWWO, and ADWCos correspond to results with the Jaccard, Weighted Overlap and Cosine signature comparison measures, respectively. For the similarity judgment evaluation, we used as benchmark the RG-65 dataset created by Rubenstein and Good</context>
</contexts>
<marker>Bullinaria, Levy, 2012</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2012. Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and SVD. Behavior Research Methods, 44:890–907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Corley</author>
<author>Rada Mihalcea</author>
</authors>
<title>Measuring the semantic similarity of texts.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>13--18</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="22220" citStr="Corley and Mihalcea (2005)" startWordPosition="3561" endWordPosition="3565">vector. As the lexical knowledge base of UKB, we used the same semantic network as that utilized by our approach for calculating semantic signatures. Table 3 lists the performance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which were also used as one of our additional features. As can be seen from Table 3, our alignmentbased disambiguatio</context>
<context position="34495" citStr="Corley and Mihalcea (2005)" startWordPosition="5474" endWordPosition="5477">in the case of paraphrases. Later efforts such as LSI (Deerwester et al., 1990), PLSA (Hofmann, 2001) and Topic Models (Blei et al., 2003; Steyvers and Griffiths, 2007) overcame these sparsity issues using dimensionality reduction techniques or modeling the document using latent variables. However, such methods were still most suitable for comparing longer texts. Complementary approaches have been developed specifically for comparing shorter texts, such as those used in the SemEval-2012 STS task (Agirre et al., 2012). Most similar to our approach are the methods of Islam and Inkpen (2008) and Corley and Mihalcea (2005), who performed a word-to-word similarity alignment; however, they did not operate at the sense level. Ramage et al. (2009) used a similar semantic representation of short texts from random walks on WordNet, which was applied to paraphrase recognition and textual entailment. However, unlike our approach, their method does not perform sense disambiguation prior to building the representation and therefore potentially suffers from ambiguity. A significant amount of effort has also been put into measuring similarity at the word level, frequently by approaches that use distributional semantics (Tu</context>
</contexts>
<marker>Corley, Mihalcea, 2005</marker>
<rawString>Courtney Corley and Rada Mihalcea. 2005. Measuring the semantic similarity of texts. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 13–18, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="33948" citStr="Deerwester et al., 1990" startWordPosition="5390" endWordPosition="5394">rates that our transformation of semantic signatures into ordered lists of concepts and calculating similarity by rank comparison has been helpful. 1348 6 Related Work Due to the wide applicability of semantic similarity, significant efforts have been made at different lexical levels. Early work on document-level similarity was driven by information retrieval. Vector space methods provided initial successes (Salton et al., 1975), but often suffer from data sparsity when using small documents, or when documents use different word types, as in the case of paraphrases. Later efforts such as LSI (Deerwester et al., 1990), PLSA (Hofmann, 2001) and Topic Models (Blei et al., 2003; Steyvers and Griffiths, 2007) overcame these sparsity issues using dimensionality reduction techniques or modeling the document using latent variables. However, such methods were still most suitable for comparing longer texts. Complementary approaches have been developed specifically for comparing shorter texts, such as those used in the SemEval-2012 STS task (Agirre et al., 2012). Most similar to our approach are the methods of Islam and Inkpen (2008) and Corley and Mihalcea (2005), who performed a word-to-word similarity alignment; </context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Di Marco</author>
<author>Roberto Navigli</author>
</authors>
<title>Clustering and diversifying Web search results with graph-based Word Sense Induction.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<marker>Di Marco, Navigli, 2013</marker>
<rawString>Antonio Di Marco and Roberto Navigli. 2013. Clustering and diversifying Web search results with graph-based Word Sense Induction. Computational Linguistics, 39(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamer Elsayed</author>
<author>Jimmy Lin</author>
<author>Douglas W Oard</author>
</authors>
<title>Pairwise document similarity in large collections with MapReduce.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>265--268</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="2051" citStr="Elsayed et al., 2008" startWordPosition="296" endWordPosition="299">an and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense information (Agirre et al., 2012). We propose a unified approach to semantic similarity across multiple representation levels from senses to documents, which offers two significant advantages. First, the method is applicable independently of the input type, w</context>
</contexts>
<marker>Elsayed, Lin, Oard, 2008</marker>
<rawString>Tamer Elsayed, Jimmy Lin, and Douglas W. Oard. 2008. Pairwise document similarity in large collections with MapReduce. In Proceedings of ACL-HLT, pages 265– 268, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen F¨urstenau</author>
<author>Mirella Lapata</author>
</authors>
<title>Semi-supervised Semantic Role Labeling via structural alignment.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<marker>F¨urstenau, Lapata, 2012</marker>
<rawString>Hagen F¨urstenau and Mirella Lapata. 2012. Semi-supervised Semantic Role Labeling via structural alignment. Computational Linguistics, 38(1):135–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1606--1611</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="16129" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="2586" endWordPosition="2589"> of each dataset. Comparison Systems. The top-ranking participating systems in the SemEval-2012 task were generally supervised systems utilizing a variety of lexical resources and similarity measurement techniques. We compare our results against the top three systems of the 88 submissions: TLsim and TLsyn, the two systems of ˇSari´c et al. (2012), and the UKP2 system (B¨ar et al., 2012). UKP2 utilizes extensive resources among which are a Distributional Thesaurus computed on 10M dependencyparsed English sentences. In addition, the system utilizes techniques such as Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) and makes use of resources such as Wiktionary and Wikipedia, a lexical substitution system based on supervised word sense disambiguation (Biemann, 2013), and a statistical machine translation system. The TLsim system uses the New York Times Annotated Corpus, Wikipedia, and Google Book Ngrams. The TLsyn system also uses Google Book Ngrams, as well as dependency parsing and named entity recognition. 1344 ALL Ranking Mean System ALL Overall Mean Mpar Dataset-specific SMTn ALLnrm ALLnrm Mvid SMTe OnWN 1 1 1 ADW 0.866 0.871 0.711 0.694 0.887 0.555 0.706 0.604 2 3 2 UKP2 0.824 0.858 0.677 0.683 0.8</context>
<context position="22398" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="3584" endWordPosition="3587">formance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which were also used as one of our additional features. As can be seen from Table 3, our alignmentbased disambiguation approach (ADW-MF) is better suited to the task than a conventional WSD approach (DW). Another interesting point is the high scores achieved by the Character n-grams Similarity </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using Wikipedia-based explicit semantic analysis. In Proceedings of IJCAI, pages 1606– 1611, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
</authors>
<title>Acquiring lexical paraphrases from a single corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP,</booktitle>
<pages>81--90</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="1450" citStr="Glickman and Dagan, 2003" startWordPosition="203" endWordPosition="206">s unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 200</context>
</contexts>
<marker>Glickman, Dagan, 2003</marker>
<rawString>Oren Glickman and Ido Dagan. 2003. Acquiring lexical paraphrases from a single corpus. In Proceedings of RANLP, pages 81–90, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on strings, trees, and sequences: computer science and computational biology.</title>
<date>1997</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="18760" citStr="Gusfield, 1997" startWordPosition="3007" endWordPosition="3008">ure comparison methods as individual features. Although the Jaccard comparison is parameterized, we avoided tuning and instead used four features for distinct values of k: 250, 500, 1000, and 2500. String-based features. Additionally, because the texts often contain named entities which are not present in WordNet, we incorporated the similarity values produced by four string-based measures, which were used by other teams in the STS task: (1) longest common substring which takes into account the length of the longest overlapping contiguous sequence of characters (substring) across two strings (Gusfield, 1997), (2) longest common subsequence which, instead, finds the longest overlapping subsequence of two strings (Allison and Dix, 1986), (3) Greedy String Tiling which allows reordering in strings (Wise, 1993), and (4) the character/word n-gram similarity proposed by Barr´on-Cede˜no et al. (2010). We followed ˇSari´c et al. (2012) and used the models trained on the SMTeuroparl and MSRpar datasets for testing on the SMTnews and OnWN test sets, respectively. 3.2 STS Results Three evaluation metrics are provided by the organizers of the SemEval-2012 STS task, all of which are based on Pearson correlati</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield. 1997. Algorithms on strings, trees, and sequences: computer science and computational biology. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Halawi</author>
<author>Gideon Dror</author>
<author>Evgeniy Gabrilovich</author>
<author>Yehuda Koren</author>
</authors>
<title>Large-scale learning of word relatedness with constraints.</title>
<date>2012</date>
<booktitle>In Proceedings of KDD,</booktitle>
<pages>1406--1414</pages>
<location>Beijing, China.</location>
<contexts>
<context position="1842" citStr="Halawi et al., 2012" startWordPosition="264" endWordPosition="267">11). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense information (Agirre et al., 2012). We propose a un</context>
</contexts>
<marker>Halawi, Dror, Gabrilovich, Koren, 2012</marker>
<rawString>Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and Yehuda Koren. 2012. Large-scale learning of word relatedness with constraints. In Proceedings of KDD, pages 1406– 1414, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="17889" citStr="Hall et al., 2009" startWordPosition="2870" endWordPosition="2873">s (i.e., ALL, ALLnrm, and Mean). Rankings are also presented based on the three metrics. System Configuration. Here we describe the configuration of our approach, which we have called Align, Disambiguate and Walk (ADW). The STS task uses human similarity judgments on an ordinal scale from 0 to 5. Therefore, in ADW we adopted a similar approach to generating similarity values to that adopted by other participating systems, whereby a supervised system is trained to combine multiple similarity judgments to produce a final rating consistent with the human annotators. We utilized the WEKA toolkit (Hall et al., 2009) to train a Gaussian Processes regression model for each of the three training sets (cf. Table 1). The features discussed hereafter were considered in our regression model. Main features. We used the scores calculated using all three of our semantic signature comparison methods as individual features. Although the Jaccard comparison is parameterized, we avoided tuning and instead used four features for distinct values of k: 250, 500, 1000, and 2500. String-based features. Additionally, because the texts often contain named entities which are not present in WordNet, we incorporated the similari</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. ACM SIGKDD Explorations Newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher H Haveliwala</author>
</authors>
<title>Topic-sensitive PageRank.</title>
<date>2002</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>517--526</pages>
<location>Hawaii, USA.</location>
<contexts>
<context position="6868" citStr="Haveliwala, 2002" startWordPosition="1037" endWordPosition="1038">Ramage, 2007; Agirre et al., 2009), paraphrase recognition, textual entailment (Ramage et al., 2009), and pseudoword generation (Pilehvar and Navigli, 2013). Formally, we define the semantic signature of a lexical item as the multinomial distribution generated from the random walks over WordNet 3.0 where the set of seed nodes is the set of senses present in the item. This representation encompasses both when the item is itself a single sense and when the item is a sense-tagged sentence. To construct each semantic signature, we use the iterative method for calculating topic-sensitive PageRank (Haveliwala, 2002). Let M be the adjacency matrix for the WordNet network, where edges connect senses according to the relations defined in WordNet (e.g., hypernymy and meronymy). We further enrich M by connecting a sense with all the other senses that appear in its disambiguated gloss.1 Let ~v(0) denote the probability distribution for the starting location of the random walker in the network. Given the set of senses S in a lexical item, the probability mass of ~v(0) is uniformly distributed across the senses sz E S, with the mass for all sj E/ S set to zero. The PageRank may then be computed using: v~ (t) = (</context>
</contexts>
<marker>Haveliwala, 2002</marker>
<rawString>Taher H. Haveliwala. 2002. Topic-sensitive PageRank. In Proceedings of WWW, pages 517–526, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelos Hliaoutakis</author>
</authors>
<title>Giannis Varelas, Epimenidis Voutsakis, Euripides GM Petrakis, and Evangelos Milios.</title>
<date>2006</date>
<journal>International Journal on Semantic Web and Information Systems,</journal>
<volume>2</volume>
<issue>3</issue>
<marker>Hliaoutakis, 2006</marker>
<rawString>Angelos Hliaoutakis, Giannis Varelas, Epimenidis Voutsakis, Euripides GM Petrakis, and Evangelos Milios. 2006. Information retrieval by semantic similarity. International Journal on Semantic Web and Information Systems, 2(3):55–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Unsupervised Learning by Probabilistic Latent Semantic Analysis.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="33970" citStr="Hofmann, 2001" startWordPosition="5396" endWordPosition="5397"> semantic signatures into ordered lists of concepts and calculating similarity by rank comparison has been helpful. 1348 6 Related Work Due to the wide applicability of semantic similarity, significant efforts have been made at different lexical levels. Early work on document-level similarity was driven by information retrieval. Vector space methods provided initial successes (Salton et al., 1975), but often suffer from data sparsity when using small documents, or when documents use different word types, as in the case of paraphrases. Later efforts such as LSI (Deerwester et al., 1990), PLSA (Hofmann, 2001) and Topic Models (Blei et al., 2003; Steyvers and Griffiths, 2007) overcame these sparsity issues using dimensionality reduction techniques or modeling the document using latent variables. However, such methods were still most suitable for comparing longer texts. Complementary approaches have been developed specifically for comparing shorter texts, such as those used in the SemEval-2012 STS task (Agirre et al., 2012). Most similar to our approach are the methods of Islam and Inkpen (2008) and Corley and Mihalcea (2005), who performed a word-to-word similarity alignment; however, they did not </context>
</contexts>
<marker>Hofmann, 2001</marker>
<rawString>Thomas Hofmann. 2001. Unsupervised Learning by Probabilistic Latent Semantic Analysis. Machine Learning, 42(1):177–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>57--60</pages>
<location>NY, USA.</location>
<contexts>
<context position="31440" citStr="Hovy et al., 2006" startWordPosition="4986" endWordPosition="4989"> (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavagli`a, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). 5.2 Experimental Setup We benchmark the accuracy of our similarity measure in grouping word senses against those of Navigli (2006) and Snow et al. (2007) on two datasets of manually-labeled sense groupings of WordNet senses: (1) sense groupings provided as a part of the Senseval-2 English Lexical Sample WSD task (Kilgarriff, 2001) which includes nouns, verbs and adjectives; (2) sense groupings included in the OntoNotes project4 (Hovy et al., 2006) for nouns and verbs. Following the evaluation methodology of Snow et al. (2007), we combine the Senseval-2 and OntoNotes datasets into a third dataset. Snow et al. (2007) considered sense grouping as a binary classification task whereby for each word every possible pairing of senses has to be classified 4Sense groupings belong to a pre-version 1.0: http:// cemantix.org/download/sense/ontonotes-sense-groups.tar.gz as either merged or not-merged. We constructed a simple threshold-based classifier to perform the same binary classification. To this end, we calculated the semantic similarity of ea</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The 90% solution. In Proceedings of NAACL, pages 57–60, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thad Hughes</author>
<author>Daniel Ramage</author>
</authors>
<title>Lexical semantic relatedness with random graph walks.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>581--589</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6263" citStr="Hughes and Ramage, 2007" startWordPosition="940" endWordPosition="943">andom walks beginning at that node will produce a frequency distribution over the nodes in the graph visited during the walk. To extend beyond a single sense, the random walk may be initialized and restarted from a set of senses (seed nodes), rather than just one; this multi-seed walk produces a multinomial distribution over all the senses in WordNet with higher probability assigned to senses that are frequently visited from the seeds. Prior work has demonstrated that multinomials generated from random walks over WordNet can be successfully applied to linguistic tasks such as word similarity (Hughes and Ramage, 2007; Agirre et al., 2009), paraphrase recognition, textual entailment (Ramage et al., 2009), and pseudoword generation (Pilehvar and Navigli, 2013). Formally, we define the semantic signature of a lexical item as the multinomial distribution generated from the random walks over WordNet 3.0 where the set of seed nodes is the set of senses present in the item. This representation encompasses both when the item is itself a single sense and when the item is a sense-tagged sentence. To construct each semantic signature, we use the iterative method for calculating topic-sensitive PageRank (Haveliwala, </context>
<context position="25899" citStr="Hughes and Ramage, 2007" startWordPosition="4120" endWordPosition="4123">100.00% Table 4: Accuracy on the 80-question TOEFL Synonym test. ADWJac, ADWWO, and ADWCos correspond to results with the Jaccard, Weighted Overlap and Cosine signature comparison measures, respectively. For the similarity judgment evaluation, we used as benchmark the RG-65 dataset created by Rubenstein and Goodenough (1965). The dataset contains 65 word pairs judged by 51 human subjects on a scale of 0 to 4 according to their semantic similarity. Ideally, a measure’s similarity judgments are expected to be highly correlated with those of humans. To be consistent with the previous literature (Hughes and Ramage, 2007; Agirre et al., 2009), we used Spearman’s rank correlation in our experiment. 4.1 Experimental Setup Our alignment-based sense disambiguation transforms the task of comparing individual words into that of calculating the similarity of the bestmatching sense pair across the two words. As there is no training data we do not optimize the k value for computing signature similarity with the Jaccard index; instead, we report, for the synonym recognition and the similarity judgment evaluations, the respective range of accuracies and the average correlation obtained upon using five values of k random</context>
<context position="28509" citStr="Hughes and Ramage (2007)" startWordPosition="4527" endWordPosition="4530"> The synonym mistakes reveal cases where senses of the two words are close in WordNet, indicating some relatedness. For example, percentage may be interpreted colloquially as monetary value (e.g., “give me my percentage”) and elicits the synonym of profit in the economic domain, which ADW incorrectly selects as a synonym. 4.3 Word Similarity Results: RG-65 dataset Table 6 shows the Spearman’s p rank correlation coefficients with human judgments on the RG-65 dataset. As can be seen from the Table, our approach with the Weighted Overlap signature comparison improves over the similar approach of Hughes and Ramage (2007) which, however, does not involve the disambiguation step and considers a word as a whole unit as represented by the set of its senses. 5 Experiment 3: Sense Similarity WordNet is known to be a fine-grained sense inventory with many related word senses (Palmer et al., 2007). Accordingly, multiple approaches have attempted to identify highly similar senses in order to produce a coarse-grained sense inventory. We adopt this task as a way of evaluating our similarity measure at the sense level. 5.1 Coarse-graining Background Earlier work on reducing the polysemy of sense inventories has considere</context>
<context position="35357" citStr="Hughes and Ramage (2007)" startWordPosition="5613" endWordPosition="5616">e recognition and textual entailment. However, unlike our approach, their method does not perform sense disambiguation prior to building the representation and therefore potentially suffers from ambiguity. A significant amount of effort has also been put into measuring similarity at the word level, frequently by approaches that use distributional semantics (Turney and Pantel, 2010). These methods use contextual features to represent semantics at the word level, whereas our approach represents word semantics at the sense level. Most similar to our approach are those of Agirre et al. (2009) and Hughes and Ramage (2007), which represent word meaning as the multinomials produced from random walks on the WordNet graph. However, unlike our approach, neither of these disambiguates the two words being compared, which potentially conflates the meanings and lowers the similarity judgment. Measures of sense relatedness have frequently leveraged the structural properties of WordNet (e.g., path lengths) to compare senses. Budanitsky and Hirst (2006) provided a survey of such WordNet-based measures. The main drawback with these approaches lies in the WordNet structure itself, where frequently two semantically similar s</context>
</contexts>
<marker>Hughes, Ramage, 2007</marker>
<rawString>Thad Hughes and Daniel Ramage. 2007. Lexical semantic relatedness with random graph walks. In Proceedings of EMNLP-CoNLL, pages 581–589, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic text similarity using corpus-based word similarity and string similarity.</title>
<date>2008</date>
<journal>ACM Transactions on Knowledge Discovery from Data,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="34464" citStr="Islam and Inkpen (2008)" startWordPosition="5469" endWordPosition="5472">se different word types, as in the case of paraphrases. Later efforts such as LSI (Deerwester et al., 1990), PLSA (Hofmann, 2001) and Topic Models (Blei et al., 2003; Steyvers and Griffiths, 2007) overcame these sparsity issues using dimensionality reduction techniques or modeling the document using latent variables. However, such methods were still most suitable for comparing longer texts. Complementary approaches have been developed specifically for comparing shorter texts, such as those used in the SemEval-2012 STS task (Agirre et al., 2012). Most similar to our approach are the methods of Islam and Inkpen (2008) and Corley and Mihalcea (2005), who performed a word-to-word similarity alignment; however, they did not operate at the sense level. Ramage et al. (2009) used a similar semantic representation of short texts from random walks on WordNet, which was applied to paraphrase recognition and textual entailment. However, unlike our approach, their method does not perform sense disambiguation prior to building the representation and therefore potentially suffers from ambiguity. A significant amount of effort has also been put into measuring similarity at the word level, frequently by approaches that u</context>
</contexts>
<marker>Islam, Inkpen, 2008</marker>
<rawString>Aminul Islam and Diana Inkpen. 2008. Semantic text similarity using corpus-based word similarity and string similarity. ACM Transactions on Knowledge Discovery from Data, 2(2):10:1–10:25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of ROCLING X,</booktitle>
<pages>pages</pages>
<contexts>
<context position="22138" citStr="Jiang and Conrath (1997)" startWordPosition="3547" endWordPosition="3551">icks the most relevant sense of the word according to the resulting probability vector. As the lexical knowledge base of UKB, we used the same semantic network as that utilized by our approach for calculating semantic signatures. Table 3 lists the performance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which were also used as one of our </context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of ROCLING X, pages 19–30, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>English lexical sample task description.</title>
<date>2001</date>
<booktitle>In Proceedings of Senseval,</booktitle>
<pages>17--20</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="31321" citStr="Kilgarriff, 2001" startWordPosition="4970" endWordPosition="4971">t::Similarity package (Pedersen et al., 2004). The classifier also made use of resources such as topic signatures data (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavagli`a, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). 5.2 Experimental Setup We benchmark the accuracy of our similarity measure in grouping word senses against those of Navigli (2006) and Snow et al. (2007) on two datasets of manually-labeled sense groupings of WordNet senses: (1) sense groupings provided as a part of the Senseval-2 English Lexical Sample WSD task (Kilgarriff, 2001) which includes nouns, verbs and adjectives; (2) sense groupings included in the OntoNotes project4 (Hovy et al., 2006) for nouns and verbs. Following the evaluation methodology of Snow et al. (2007), we combine the Senseval-2 and OntoNotes datasets into a third dataset. Snow et al. (2007) considered sense grouping as a binary classification task whereby for each word every possible pairing of senses has to be classified 4Sense groupings belong to a pre-version 1.0: http:// cemantix.org/download/sense/ontonotes-sense-groups.tar.gz as either merged or not-merged. We constructed a simple thresho</context>
</contexts>
<marker>Kilgarriff, 2001</marker>
<rawString>Adam Kilgarriff. 2001. English lexical sample task description. In Proceedings of Senseval, pages 17–20, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review; Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="4370" citStr="Landauer and Dumais (1997)" startWordPosition="642" endWordPosition="645">f the 51st Annual Meeting of the Association for Computational Linguistics, pages 1341–1351, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ambiguation during semantic comparison. Third, we demonstrate that this single representation can achieve state-of-the-art performance on three similarity tasks, each operating at a different lexical level: (1) surpassing the highest scores on the SemEval-2012 task on textual similarity (Agirre et al., 2012) that compares sentences, (2) achieving a near-perfect performance on the TOEFL synonym selection task proposed by Landauer and Dumais (1997), which measures word pair similarity, and also obtaining state-of-the-art performance in terms of the correlation with human judgments on the RG-65 dataset (Rubenstein and Goodenough, 1965), and finally (3) surpassing the performance of Snow et al. (2007) in a sensecoarsening task that measures sense similarity. 2 A Unified Semantic Representation We propose a representation of any lexical item as a distribution over a set of word senses, referred to as the item’s semantic signature. We begin with a formal description of the representation at the sense level (Section 2.1). Following this, we </context>
<context position="24659" citStr="Landauer and Dumais (1997)" startWordPosition="3922" endWordPosition="3925">ity We now proceed from the sentence level to the word level. Word similarity has been a key problem for lexical semantics, with significant efforts being made by approaches in distributional semantics to accurately identify synonymous words (Turney and Pantel, 2010). Different evaluation methods exist in the literature for evaluating the performance of a word-level semantic similarity measure; we adopted two well-established benchmarks: synonym recognition and correlating word similarity judgments with those from human annotators. For synonym recognition, we used the TOEFL dataset created by Landauer and Dumais (1997). The dataset consists of 80 multiple-choice synonym questions from the TOEFL test; a word is paired with four options, one of which is a valid synonym. Test takers with English as a second language averaged 64.5% correct. Despite multiple approaches, only recently has the test been answered perfectly (Bullinaria and Levy, 2012), underscoring the challenge of synonym recognition. 1346 Approach Accuracy PPMIC (Bullinaria and Levy, 2007) 85.00% GLSA (Matveeva et al., 2005) 86.25% LSA (Rapp, 2003) 92.50% ADWJac 93.75±2.5% ADWWO 95.00% ADWCos 96.25% PR (Turney et al., 2003) 97.50% PCCP (Bullinaria</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review; Psychological Review, 104(2):211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>768--774</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="22153" citStr="Lin (1998" startWordPosition="3553" endWordPosition="3554">f the word according to the resulting probability vector. As the lexical knowledge base of UKB, we used the same semantic network as that utilized by our approach for calculating semantic signatures. Table 3 lists the performance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which were also used as one of our additional feat</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998a. Automatic retrieval and clustering of similar words. In Proceedings of COLING, pages 768– 774, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>296--304</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="22153" citStr="Lin (1998" startWordPosition="3553" endWordPosition="3554">f the word according to the resulting probability vector. As the lexical knowledge base of UKB, we used the same semantic network as that utilized by our approach for calculating semantic signatures. Table 3 lists the performance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which were also used as one of our additional feat</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998b. An information-theoretic definition of similarity. In Proceedings of ICML, pages 296–304, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Gabriela Cavagli`a</author>
</authors>
<title>Integrating subject field codes into WordNet.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>1413--1418</pages>
<location>Athens, Greece.</location>
<marker>Magnini, Cavagli`a, 2000</marker>
<rawString>Bernardo Magnini and Gabriela Cavagli`a. 2000. Integrating subject field codes into WordNet. In Proceedings of LREC, pages 1413–1418, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana G Maguitman</author>
<author>Filippo Menczer</author>
<author>Heather Roinestad</author>
<author>Alessandro Vespignani</author>
</authors>
<title>Algorithmic detection of semantic similarity.</title>
<date>2005</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>107--116</pages>
<location>Chiba, Japan.</location>
<contexts>
<context position="2029" citStr="Maguitman et al., 2005" startWordPosition="292" endWordPosition="295"> as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense information (Agirre et al., 2012). We propose a unified approach to semantic similarity across multiple representation levels from senses to documents, which offers two significant advantages. First, the method is applicable independentl</context>
</contexts>
<marker>Maguitman, Menczer, Roinestad, Vespignani, 2005</marker>
<rawString>Ana G. Maguitman, Filippo Menczer, Heather Roinestad, and Alessandro Vespignani. 2005. Algorithmic detection of semantic similarity. In Proceedings of WWW, pages 107– 116, Chiba, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irina Matveeva</author>
<author>Gina-Anne Levow</author>
<author>Ayman Farahat</author>
<author>Christiaan Royer</author>
</authors>
<title>Terms representation with generalized latent semantic analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of RANLP, Borovets,</booktitle>
<contexts>
<context position="25134" citStr="Matveeva et al., 2005" startWordPosition="3998" endWordPosition="4001">ord similarity judgments with those from human annotators. For synonym recognition, we used the TOEFL dataset created by Landauer and Dumais (1997). The dataset consists of 80 multiple-choice synonym questions from the TOEFL test; a word is paired with four options, one of which is a valid synonym. Test takers with English as a second language averaged 64.5% correct. Despite multiple approaches, only recently has the test been answered perfectly (Bullinaria and Levy, 2012), underscoring the challenge of synonym recognition. 1346 Approach Accuracy PPMIC (Bullinaria and Levy, 2007) 85.00% GLSA (Matveeva et al., 2005) 86.25% LSA (Rapp, 2003) 92.50% ADWJac 93.75±2.5% ADWWO 95.00% ADWCos 96.25% PR (Turney et al., 2003) 97.50% PCCP (Bullinaria and Levy, 2012) 100.00% Table 4: Accuracy on the 80-question TOEFL Synonym test. ADWJac, ADWWO, and ADWCos correspond to results with the Jaccard, Weighted Overlap and Cosine signature comparison measures, respectively. For the similarity judgment evaluation, we used as benchmark the RG-65 dataset created by Rubenstein and Goodenough (1965). The dataset contains 65 word pairs judged by 51 human subjects on a scale of 0 to 4 according to their semantic similarity. Ideall</context>
</contexts>
<marker>Matveeva, Levow, Farahat, Royer, 2005</marker>
<rawString>Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and Christiaan Royer. 2005. Terms representation with generalized latent semantic analysis. In Proceedings of RANLP, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>The English lexical substitution task.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="1501" citStr="McCarthy and Navigli, 2009" startWordPosition="209" endWordPosition="212">rformance on three tasks: semantic textual similarity, word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential </context>
</contexts>
<marker>McCarthy, Navigli, 2009</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2009. The English lexical substitution task. Language Resources and Evaluation, 43(2):139–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Relating WordNet senses for word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Making Sense of Sense at EACL-06,</booktitle>
<pages>17--24</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="30194" citStr="McCarthy, 2006" startWordPosition="4794" endWordPosition="4795">easures respectively. Onto SE-2 Onto + SE-2 Method Noun Verb Noun Verb Adj Noun Verb RCos 0.406 0.522 0.450 0.465 0.484 0.441 0.485 RWO 0.421 0.544 0.483 0.482 0.531 0.470 0.503 RJac 0.418 0.531 0.478 0.473 0.501 0.465 0.493 SVM 0.370 0.455 NA NA 0.473 0.423 0.432 ODE 0.218 0.396 NA NA 0.371 0.331 0.288 Table 7: F-score sense merging evaluation on three hand-labeled datasets: OntoNotes (Onto), Senseval-2 (SE-2), and combined (Onto+SE-2). Results are reported for all three of our signature comparison measures and also for two previous works (last two rows). word senses (Agirre and Lopez, 2003; McCarthy, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English (ODE). The approach leverages semantic similarities in gloss definitions and the hierarchical relations between senses in the ODE to cluster WordNet senses. As current state of the art, Snow et al. (2007) developed a supervised SVM classifier that utilized, as its features, several earlier sense relatedness techniques such as those implemented in the WordNet::Similarity package (Pedersen et al., 2004). The classifier also made use of resources s</context>
</contexts>
<marker>McCarthy, 2006</marker>
<rawString>Diana McCarthy. 2006. Relating WordNet senses for word sense disambiguation. In Proceedings of the Workshop on Making Sense of Sense at EACL-06, pages 17–24, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>Automatic generation of a coarse grained WordNet.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL Workshop on WordNet and Other Lexical Resources,</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="29181" citStr="Mihalcea and Moldovan, 2001" startWordPosition="4633" endWordPosition="4636">iguation step and considers a word as a whole unit as represented by the set of its senses. 5 Experiment 3: Sense Similarity WordNet is known to be a fine-grained sense inventory with many related word senses (Palmer et al., 2007). Accordingly, multiple approaches have attempted to identify highly similar senses in order to produce a coarse-grained sense inventory. We adopt this task as a way of evaluating our similarity measure at the sense level. 5.1 Coarse-graining Background Earlier work on reducing the polysemy of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of 1347 Approach Correlation ADWCos 0.825 Agirre et al. (2009) 0.830 Hughes and Ramage (2007) 0.838 Zesch et al. (2008) 0.840 ADWJac 0.841 ADWWO 0.868 Table 6: Spearman’s p correlation coefficients with human judgments on the RG-65 dataset. ADWJac, ADWWO, and ADWCos correspond to results with the Jaccard, Weighted Overlap and Cosine signature comparison measures respectively. Onto SE-2 Onto + SE-2 Method Noun Verb Noun Verb Adj Noun Verb RCos 0.406 0.522 0.450 0.465 0.484 0.441 0.485 RWO 0.421 0.544 0.483 0.482 0.531 0.470 0.503 RJac 0.418 0.531 0.478 0</context>
</contexts>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>Rada Mihalcea and Dan Moldovan. 2001. Automatic generation of a coarse grained WordNet. In Proceedings of NAACL Workshop on WordNet and Other Lexical Resources, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Michael Strube</author>
</authors>
<title>Transforming Wikipedia into a large scale multilingual concept network.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--62</pages>
<contexts>
<context position="36104" citStr="Nastase and Strube, 2013" startWordPosition="5725" endWordPosition="5728">proach, neither of these disambiguates the two words being compared, which potentially conflates the meanings and lowers the similarity judgment. Measures of sense relatedness have frequently leveraged the structural properties of WordNet (e.g., path lengths) to compare senses. Budanitsky and Hirst (2006) provided a survey of such WordNet-based measures. The main drawback with these approaches lies in the WordNet structure itself, where frequently two semantically similar senses are distant in the WordNet hierarchy. Possible solutions include relying on widercoverage networks such as WikiNet (Nastase and Strube, 2013) or multilingual ones such as BabelNet (Navigli and Ponzetto, 2012b). Fewer works have focused on measuring the similarity – as opposed to relatedness – between senses. The topic signatures method of Agirre and Lopez (2003) represents each sense as a vector over corpusderived features in order to build comparable sense representations. However, topic signatures often produce lower quality representations due to sparsity in the local structure of WordNet, especially for rare senses. In contrast, the random walk used in our approach provides a denser, and thus more comparable, representation for</context>
</contexts>
<marker>Nastase, Strube, 2013</marker>
<rawString>Vivi Nastase and Michael Strube. 2013. Transforming Wikipedia into a large scale multilingual concept network. Artificial Intelligence, 194:62–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<pages>193--217</pages>
<contexts>
<context position="36170" citStr="Navigli and Ponzetto, 2012" startWordPosition="5736" endWordPosition="5739">ed, which potentially conflates the meanings and lowers the similarity judgment. Measures of sense relatedness have frequently leveraged the structural properties of WordNet (e.g., path lengths) to compare senses. Budanitsky and Hirst (2006) provided a survey of such WordNet-based measures. The main drawback with these approaches lies in the WordNet structure itself, where frequently two semantically similar senses are distant in the WordNet hierarchy. Possible solutions include relying on widercoverage networks such as WikiNet (Nastase and Strube, 2013) or multilingual ones such as BabelNet (Navigli and Ponzetto, 2012b). Fewer works have focused on measuring the similarity – as opposed to relatedness – between senses. The topic signatures method of Agirre and Lopez (2003) represents each sense as a vector over corpusderived features in order to build comparable sense representations. However, topic signatures often produce lower quality representations due to sparsity in the local structure of WordNet, especially for rare senses. In contrast, the random walk used in our approach provides a denser, and thus more comparable, representation for all WordNet senses. 7 Conclusions This paper presents a unified a</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012a. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelRelate! a joint multilingual approach to computing semantic relatedness.</title>
<date>2012</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>108--114</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="36170" citStr="Navigli and Ponzetto, 2012" startWordPosition="5736" endWordPosition="5739">ed, which potentially conflates the meanings and lowers the similarity judgment. Measures of sense relatedness have frequently leveraged the structural properties of WordNet (e.g., path lengths) to compare senses. Budanitsky and Hirst (2006) provided a survey of such WordNet-based measures. The main drawback with these approaches lies in the WordNet structure itself, where frequently two semantically similar senses are distant in the WordNet hierarchy. Possible solutions include relying on widercoverage networks such as WikiNet (Nastase and Strube, 2013) or multilingual ones such as BabelNet (Navigli and Ponzetto, 2012b). Fewer works have focused on measuring the similarity – as opposed to relatedness – between senses. The topic signatures method of Agirre and Lopez (2003) represents each sense as a vector over corpusderived features in order to build comparable sense representations. However, topic signatures often produce lower quality representations due to sparsity in the local structure of WordNet, especially for rare senses. In contrast, the random walk used in our approach provides a denser, and thus more comparable, representation for all WordNet senses. 7 Conclusions This paper presents a unified a</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012b. BabelRelate! a joint multilingual approach to computing semantic relatedness. In Proceedings of AAAI, pages 108–114, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Meaningful clustering of senses helps boost Word Sense Disambiguation performance.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>105--112</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="30210" citStr="Navigli (2006)" startWordPosition="4796" endWordPosition="4797">ely. Onto SE-2 Onto + SE-2 Method Noun Verb Noun Verb Adj Noun Verb RCos 0.406 0.522 0.450 0.465 0.484 0.441 0.485 RWO 0.421 0.544 0.483 0.482 0.531 0.470 0.503 RJac 0.418 0.531 0.478 0.473 0.501 0.465 0.493 SVM 0.370 0.455 NA NA 0.473 0.423 0.432 ODE 0.218 0.396 NA NA 0.371 0.331 0.288 Table 7: F-score sense merging evaluation on three hand-labeled datasets: OntoNotes (Onto), Senseval-2 (SE-2), and combined (Onto+SE-2). Results are reported for all three of our signature comparison measures and also for two previous works (last two rows). word senses (Agirre and Lopez, 2003; McCarthy, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English (ODE). The approach leverages semantic similarities in gloss definitions and the hierarchical relations between senses in the ODE to cluster WordNet senses. As current state of the art, Snow et al. (2007) developed a supervised SVM classifier that utilized, as its features, several earlier sense relatedness techniques such as those implemented in the WordNet::Similarity package (Pedersen et al., 2004). The classifier also made use of resources such as topic sig</context>
<context position="33037" citStr="Navigli (2006)" startWordPosition="5248" endWordPosition="5249">eriment 1 (cf. Section 3). 5.3 Sense Merging Results For a binary classification task, we can directly calculate precision, recall and F-score by constructing a contingency table. We show in Table 7 the F-score performance of our classifier as obtained by an averaged 10-fold cross-validation. Results are presented for all three of the measures of semantic signature comparison and for the three datasets: OntoNotes, Senseval-2, and the two combined. In addition, we show in Table 7 the F-score results provided by Snow et al. (2007) for their SVM-based system and for the mapping-based approach of Navigli (2006), denoted by ODE. Table 7 shows that our methodology yields improvements over previous work on both datasets and for all parts of speech, irrespective of the semantic signature comparison method used. Among the three methods, Weighted Overlap achieves the best performance, which demonstrates that our transformation of semantic signatures into ordered lists of concepts and calculating similarity by rank comparison has been helpful. 1348 6 Related Work Due to the wide applicability of semantic similarity, significant efforts have been made at different lexical levels. Early work on document-leve</context>
</contexts>
<marker>Navigli, 2006</marker>
<rawString>Roberto Navigli. 2006. Meaningful clustering of senses helps boost Word Sense Disambiguation performance. In Proceedings of COLING-ACL, pages 105–112, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word Sense Disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="1501" citStr="Navigli, 2009" startWordPosition="211" endWordPosition="212">three tasks: semantic textual similarity, word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential </context>
<context position="11993" citStr="Navigli (2009)" startWordPosition="1912" endWordPosition="1913"> the final, maximally-similar sense alignment of the word types in t1 and t2. The resulting alignment produces the following sets of senses: Pt1 = {manager1n, fire4v, worker1n} Pt2 = {employee1n, terminate4v, work3n, boss2n} where Px denotes the corresponding set of senses of sentence x. 2.3 Semantic Signature Similarity Cosine Similarity. In order to compare semantic signatures, we adopt the Cosine similarity measure as a baseline method. The measure is computed by treating each multinomial as a vector and then calculating the normalized dot product of the two signatures’ vectors. 3We follow Navigli (2009) and denote with wip the i-th sense of w in WordNet with part of speech p. 1343 However, a semantic signature is, in essence, a weighted ranking of the importance of WordNet senses for each lexical item. Given that the WordNet graph has a non-uniform structure, and also given that different lexical items may be of different sizes, the magnitudes of the probabilities obtained may differ significantly between the two multinomial distributions. Therefore, for computing the similarity of two signatures, we also consider two nonparametric methods that use the ranking of the senses, rather than thei</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word Sense Disambiguation: A survey. ACM Computing Surveys, 41(2):1–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Hoa Dang</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Making fine-grained and coarse-grained sense distinctions, both manually and automatically.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="28783" citStr="Palmer et al., 2007" startWordPosition="4575" endWordPosition="4578">, which ADW incorrectly selects as a synonym. 4.3 Word Similarity Results: RG-65 dataset Table 6 shows the Spearman’s p rank correlation coefficients with human judgments on the RG-65 dataset. As can be seen from the Table, our approach with the Weighted Overlap signature comparison improves over the similar approach of Hughes and Ramage (2007) which, however, does not involve the disambiguation step and considers a word as a whole unit as represented by the set of its senses. 5 Experiment 3: Sense Similarity WordNet is known to be a fine-grained sense inventory with many related word senses (Palmer et al., 2007). Accordingly, multiple approaches have attempted to identify highly similar senses in order to produce a coarse-grained sense inventory. We adopt this task as a way of evaluating our similarity measure at the sense level. 5.1 Coarse-graining Background Earlier work on reducing the polysemy of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of 1347 Approach Correlation ADWCos 0.825 Agirre et al. (2009) 0.830 Hughes and Ramage (2007) 0.838 Zesch et al. (2008) 0.840 ADWJac 0.841 ADWWO 0.868 Table 6: S</context>
</contexts>
<marker>Palmer, Dang, Fellbaum, 2007</marker>
<rawString>Martha Palmer, Hoa Dang, and Christiane Fellbaum. 2007. Making fine-grained and coarse-grained sense distinctions, both manually and automatically. Natural Language Engineering, 13(2):137–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity - measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>144--152</pages>
<location>San Jose, CA.</location>
<contexts>
<context position="30749" citStr="Pedersen et al., 2004" startWordPosition="4876" endWordPosition="4879">ast two rows). word senses (Agirre and Lopez, 2003; McCarthy, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English (ODE). The approach leverages semantic similarities in gloss definitions and the hierarchical relations between senses in the ODE to cluster WordNet senses. As current state of the art, Snow et al. (2007) developed a supervised SVM classifier that utilized, as its features, several earlier sense relatedness techniques such as those implemented in the WordNet::Similarity package (Pedersen et al., 2004). The classifier also made use of resources such as topic signatures data (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavagli`a, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). 5.2 Experimental Setup We benchmark the accuracy of our similarity measure in grouping word senses against those of Navigli (2006) and Snow et al. (2007) on two datasets of manually-labeled sense groupings of WordNet senses: (1) sense groupings provided as a part of the Senseval-2 English Lexical Sample WSD task (Kilgarriff, 2001) which includes nouns, verbs</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity - measuring the relatedness of concepts. In Proceedings of AAAI, pages 144–152, San Jose, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>Paving the way to a large-scale pseudosense-annotated dataset.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>1100--1109</pages>
<location>Atlanta, USA.</location>
<contexts>
<context position="6407" citStr="Pilehvar and Navigli, 2013" startWordPosition="960" endWordPosition="963">nd a single sense, the random walk may be initialized and restarted from a set of senses (seed nodes), rather than just one; this multi-seed walk produces a multinomial distribution over all the senses in WordNet with higher probability assigned to senses that are frequently visited from the seeds. Prior work has demonstrated that multinomials generated from random walks over WordNet can be successfully applied to linguistic tasks such as word similarity (Hughes and Ramage, 2007; Agirre et al., 2009), paraphrase recognition, textual entailment (Ramage et al., 2009), and pseudoword generation (Pilehvar and Navigli, 2013). Formally, we define the semantic signature of a lexical item as the multinomial distribution generated from the random walks over WordNet 3.0 where the set of seed nodes is the set of senses present in the item. This representation encompasses both when the item is itself a single sense and when the item is a sense-tagged sentence. To construct each semantic signature, we use the iterative method for calculating topic-sensitive PageRank (Haveliwala, 2002). Let M be the adjacency matrix for the WordNet network, where edges connect senses according to the relations defined in WordNet (e.g., hy</context>
</contexts>
<marker>Pilehvar, Navigli, 2013</marker>
<rawString>Mohammad Taher Pilehvar and Roberto Navigli. 2013. Paving the way to a large-scale pseudosense-annotated dataset. In Proceedings of NAACL-HLT, pages 1100– 1109, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Eugene Agichtein</author>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>A word at a time: computing word relatedness using temporal semantic analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>337--346</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="1820" citStr="Radinsky et al., 2011" startWordPosition="259" endWordPosition="263">ng (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense information (Agirre et al., </context>
</contexts>
<marker>Radinsky, Agichtein, Gabrilovich, Markovitch, 2011</marker>
<rawString>Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. 2011. A word at a time: computing word relatedness using temporal semantic analysis. In Proceedings of WWW, pages 337–346, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>Anna N Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Random walks for text semantic similarity.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing,</booktitle>
<pages>23--31</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="6351" citStr="Ramage et al., 2009" startWordPosition="953" endWordPosition="956">the graph visited during the walk. To extend beyond a single sense, the random walk may be initialized and restarted from a set of senses (seed nodes), rather than just one; this multi-seed walk produces a multinomial distribution over all the senses in WordNet with higher probability assigned to senses that are frequently visited from the seeds. Prior work has demonstrated that multinomials generated from random walks over WordNet can be successfully applied to linguistic tasks such as word similarity (Hughes and Ramage, 2007; Agirre et al., 2009), paraphrase recognition, textual entailment (Ramage et al., 2009), and pseudoword generation (Pilehvar and Navigli, 2013). Formally, we define the semantic signature of a lexical item as the multinomial distribution generated from the random walks over WordNet 3.0 where the set of seed nodes is the set of senses present in the item. This representation encompasses both when the item is itself a single sense and when the item is a sense-tagged sentence. To construct each semantic signature, we use the iterative method for calculating topic-sensitive PageRank (Haveliwala, 2002). Let M be the adjacency matrix for the WordNet network, where edges connect senses</context>
<context position="34618" citStr="Ramage et al. (2009)" startWordPosition="5495" endWordPosition="5499">l., 2003; Steyvers and Griffiths, 2007) overcame these sparsity issues using dimensionality reduction techniques or modeling the document using latent variables. However, such methods were still most suitable for comparing longer texts. Complementary approaches have been developed specifically for comparing shorter texts, such as those used in the SemEval-2012 STS task (Agirre et al., 2012). Most similar to our approach are the methods of Islam and Inkpen (2008) and Corley and Mihalcea (2005), who performed a word-to-word similarity alignment; however, they did not operate at the sense level. Ramage et al. (2009) used a similar semantic representation of short texts from random walks on WordNet, which was applied to paraphrase recognition and textual entailment. However, unlike our approach, their method does not perform sense disambiguation prior to building the representation and therefore potentially suffers from ambiguity. A significant amount of effort has also been put into measuring similarity at the word level, frequently by approaches that use distributional semantics (Turney and Pantel, 2010). These methods use contextual features to represent semantics at the word level, whereas our approac</context>
</contexts>
<marker>Ramage, Rafferty, Manning, 2009</marker>
<rawString>Daniel Ramage, Anna N. Rafferty, and Christopher D. Manning. 2009. Random walks for text semantic similarity. In Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, pages 23–31, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Word sense discovery based on sense descriptor dissimilarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the Ninth Machine Translation Summit,</booktitle>
<pages>315--322</pages>
<location>New Orleans, LA.</location>
<contexts>
<context position="25158" citStr="Rapp, 2003" startWordPosition="4004" endWordPosition="4005"> from human annotators. For synonym recognition, we used the TOEFL dataset created by Landauer and Dumais (1997). The dataset consists of 80 multiple-choice synonym questions from the TOEFL test; a word is paired with four options, one of which is a valid synonym. Test takers with English as a second language averaged 64.5% correct. Despite multiple approaches, only recently has the test been answered perfectly (Bullinaria and Levy, 2012), underscoring the challenge of synonym recognition. 1346 Approach Accuracy PPMIC (Bullinaria and Levy, 2007) 85.00% GLSA (Matveeva et al., 2005) 86.25% LSA (Rapp, 2003) 92.50% ADWJac 93.75±2.5% ADWWO 95.00% ADWCos 96.25% PR (Turney et al., 2003) 97.50% PCCP (Bullinaria and Levy, 2012) 100.00% Table 4: Accuracy on the 80-question TOEFL Synonym test. ADWJac, ADWWO, and ADWCos correspond to results with the Jaccard, Weighted Overlap and Cosine signature comparison measures, respectively. For the similarity judgment evaluation, we used as benchmark the RG-65 dataset created by Rubenstein and Goodenough (1965). The dataset contains 65 word pairs judged by 51 human subjects on a scale of 0 to 4 according to their semantic similarity. Ideally, a measure’s similarit</context>
<context position="27675" citStr="Rapp (2003)" startWordPosition="4394" endWordPosition="4395">only Word Synonym choices (correct in bold) fanciful familiar apparent* imaginative† logical verbal oral† overt fitting verbose* resolved settled* forgotten† publicized examined percentage volume sample proportion profit†* figure list solve* divide† express highlight alter† imitate accentuate* restore Table 5: Questions answered incorrectly by our approach. Symbols † and ? correspond to the choices of our approach with the Weighted Overlap and Cosine signature comparisons respectively. We do not include the mistakes made when the Jaccard measure was used as they vary with the k value. that of Rapp (2003) uses word senses, an approach that is outperformed by our method. The errors produced by our system were largely the result of sense locality in the WordNet network. Table 5 highlights the incorrect responses. The synonym mistakes reveal cases where senses of the two words are close in WordNet, indicating some relatedness. For example, percentage may be interpreted colloquially as monetary value (e.g., “give me my percentage”) and elicits the synonym of profit in the economic domain, which ADW incorrectly selects as a synonym. 4.3 Word Similarity Results: RG-65 dataset Table 6 shows the Spear</context>
</contexts>
<marker>Rapp, 2003</marker>
<rawString>Reinhard Rapp. 2003. Word sense discovery based on sense descriptor dissimilarity. In Proceedings of the Ninth Machine Translation Summit, pages 315–322, New Orleans, LA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>448--453</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="22112" citStr="Resnik (1995)" startWordPosition="3545" endWordPosition="3546">ated. It then picks the most relevant sense of the word according to the resulting probability vector. As the lexical knowledge base of UKB, we used the same semantic network as that utilized by our approach for calculating semantic signatures. Table 3 lists the performance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which wer</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of IJCAI, pages 448–453, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="4560" citStr="Rubenstein and Goodenough, 1965" startWordPosition="670" endWordPosition="673">ion during semantic comparison. Third, we demonstrate that this single representation can achieve state-of-the-art performance on three similarity tasks, each operating at a different lexical level: (1) surpassing the highest scores on the SemEval-2012 task on textual similarity (Agirre et al., 2012) that compares sentences, (2) achieving a near-perfect performance on the TOEFL synonym selection task proposed by Landauer and Dumais (1997), which measures word pair similarity, and also obtaining state-of-the-art performance in terms of the correlation with human judgments on the RG-65 dataset (Rubenstein and Goodenough, 1965), and finally (3) surpassing the performance of Snow et al. (2007) in a sensecoarsening task that measures sense similarity. 2 A Unified Semantic Representation We propose a representation of any lexical item as a distribution over a set of word senses, referred to as the item’s semantic signature. We begin with a formal description of the representation at the sense level (Section 2.1). Following this, we describe our alignment-based disambiguation algorithm which enables us to produce sense-based semantic signatures for those lexical items (e.g., words or sentences) which are not sense annot</context>
<context position="25602" citStr="Rubenstein and Goodenough (1965)" startWordPosition="4067" endWordPosition="4070">ria and Levy, 2012), underscoring the challenge of synonym recognition. 1346 Approach Accuracy PPMIC (Bullinaria and Levy, 2007) 85.00% GLSA (Matveeva et al., 2005) 86.25% LSA (Rapp, 2003) 92.50% ADWJac 93.75±2.5% ADWWO 95.00% ADWCos 96.25% PR (Turney et al., 2003) 97.50% PCCP (Bullinaria and Levy, 2012) 100.00% Table 4: Accuracy on the 80-question TOEFL Synonym test. ADWJac, ADWWO, and ADWCos correspond to results with the Jaccard, Weighted Overlap and Cosine signature comparison measures, respectively. For the similarity judgment evaluation, we used as benchmark the RG-65 dataset created by Rubenstein and Goodenough (1965). The dataset contains 65 word pairs judged by 51 human subjects on a scale of 0 to 4 according to their semantic similarity. Ideally, a measure’s similarity judgments are expected to be highly correlated with those of humans. To be consistent with the previous literature (Hughes and Ramage, 2007; Agirre et al., 2009), we used Spearman’s rank correlation in our experiment. 4.1 Experimental Setup Our alignment-based sense disambiguation transforms the task of comparing individual words into that of calculating the similarity of the bestmatching sense pair across the two words. As there is no tr</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="2005" citStr="Salton et al., 1975" startWordPosition="288" endWordPosition="291">ds enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense information (Agirre et al., 2012). We propose a unified approach to semantic similarity across multiple representation levels from senses to documents, which offers two significant advantages. First, the method is</context>
<context position="33756" citStr="Salton et al., 1975" startWordPosition="5356" endWordPosition="5359"> datasets and for all parts of speech, irrespective of the semantic signature comparison method used. Among the three methods, Weighted Overlap achieves the best performance, which demonstrates that our transformation of semantic signatures into ordered lists of concepts and calculating similarity by rank comparison has been helpful. 1348 6 Related Work Due to the wide applicability of semantic similarity, significant efforts have been made at different lexical levels. Early work on document-level similarity was driven by information retrieval. Vector space methods provided initial successes (Salton et al., 1975), but often suffer from data sparsity when using small documents, or when documents use different word types, as in the case of paraphrases. Later efforts such as LSI (Deerwester et al., 1990), PLSA (Hofmann, 2001) and Topic Models (Blei et al., 2003; Steyvers and Griffiths, 2007) overcame these sparsity issues using dimensionality reduction techniques or modeling the document using latent variables. However, such methods were still most suitable for comparing longer texts. Complementary approaches have been developed specifically for comparing shorter texts, such as those used in the SemEval-</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>Gerard Salton, A. Wong, and C. S. Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Sushant Prakash</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning to merge word senses.</title>
<date>2007</date>
<booktitle>In EMNLPCoNLL,</booktitle>
<pages>1005--1014</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4626" citStr="Snow et al. (2007)" startWordPosition="681" endWordPosition="684">ation can achieve state-of-the-art performance on three similarity tasks, each operating at a different lexical level: (1) surpassing the highest scores on the SemEval-2012 task on textual similarity (Agirre et al., 2012) that compares sentences, (2) achieving a near-perfect performance on the TOEFL synonym selection task proposed by Landauer and Dumais (1997), which measures word pair similarity, and also obtaining state-of-the-art performance in terms of the correlation with human judgments on the RG-65 dataset (Rubenstein and Goodenough, 1965), and finally (3) surpassing the performance of Snow et al. (2007) in a sensecoarsening task that measures sense similarity. 2 A Unified Semantic Representation We propose a representation of any lexical item as a distribution over a set of word senses, referred to as the item’s semantic signature. We begin with a formal description of the representation at the sense level (Section 2.1). Following this, we describe our alignment-based disambiguation algorithm which enables us to produce sense-based semantic signatures for those lexical items (e.g., words or sentences) which are not sense annotated (Section 2.2). Finally, we propose three methods for comparin</context>
<context position="30549" citStr="Snow et al. (2007)" startWordPosition="4848" endWordPosition="4851">ree hand-labeled datasets: OntoNotes (Onto), Senseval-2 (SE-2), and combined (Onto+SE-2). Results are reported for all three of our signature comparison measures and also for two previous works (last two rows). word senses (Agirre and Lopez, 2003; McCarthy, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English (ODE). The approach leverages semantic similarities in gloss definitions and the hierarchical relations between senses in the ODE to cluster WordNet senses. As current state of the art, Snow et al. (2007) developed a supervised SVM classifier that utilized, as its features, several earlier sense relatedness techniques such as those implemented in the WordNet::Similarity package (Pedersen et al., 2004). The classifier also made use of resources such as topic signatures data (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavagli`a, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). 5.2 Experimental Setup We benchmark the accuracy of our similarity measure in grouping word senses against those of Navigli (2006) and Snow et al. (2007) on two</context>
<context position="32957" citStr="Snow et al. (2007)" startWordPosition="5234" endWordPosition="5237"> a tuning of the k value used for Jaccard index, over the same values of k as in Experiment 1 (cf. Section 3). 5.3 Sense Merging Results For a binary classification task, we can directly calculate precision, recall and F-score by constructing a contingency table. We show in Table 7 the F-score performance of our classifier as obtained by an averaged 10-fold cross-validation. Results are presented for all three of the measures of semantic signature comparison and for the three datasets: OntoNotes, Senseval-2, and the two combined. In addition, we show in Table 7 the F-score results provided by Snow et al. (2007) for their SVM-based system and for the mapping-based approach of Navigli (2006), denoted by ODE. Table 7 shows that our methodology yields improvements over previous work on both datasets and for all parts of speech, irrespective of the semantic signature comparison method used. Among the three methods, Weighted Overlap achieves the best performance, which demonstrates that our transformation of semantic signatures into ordered lists of concepts and calculating similarity by rank comparison has been helpful. 1348 6 Related Work Due to the wide applicability of semantic similarity, significant</context>
</contexts>
<marker>Snow, Prakash, Jurafsky, Ng, 2007</marker>
<rawString>Rion Snow, Sushant Prakash, Daniel Jurafsky, and Andrew Y. Ng. 2007. Learning to merge word senses. In EMNLPCoNLL, pages 1005–1014, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Tom Griffiths</author>
</authors>
<date>2007</date>
<booktitle>Probabilistic topic models. Handbook of Latent Semantic Analysis,</booktitle>
<pages>427--7</pages>
<contexts>
<context position="34037" citStr="Steyvers and Griffiths, 2007" startWordPosition="5405" endWordPosition="5409">and calculating similarity by rank comparison has been helpful. 1348 6 Related Work Due to the wide applicability of semantic similarity, significant efforts have been made at different lexical levels. Early work on document-level similarity was driven by information retrieval. Vector space methods provided initial successes (Salton et al., 1975), but often suffer from data sparsity when using small documents, or when documents use different word types, as in the case of paraphrases. Later efforts such as LSI (Deerwester et al., 1990), PLSA (Hofmann, 2001) and Topic Models (Blei et al., 2003; Steyvers and Griffiths, 2007) overcame these sparsity issues using dimensionality reduction techniques or modeling the document using latent variables. However, such methods were still most suitable for comparing longer texts. Complementary approaches have been developed specifically for comparing shorter texts, such as those used in the SemEval-2012 STS task (Agirre et al., 2012). Most similar to our approach are the methods of Islam and Inkpen (2008) and Corley and Mihalcea (2005), who performed a word-to-word similarity alignment; however, they did not operate at the sense level. Ramage et al. (2009) used a similar sem</context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>Mark Steyvers and Tom Griffiths. 2007. Probabilistic topic models. Handbook of Latent Semantic Analysis, 427(7):424–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Learning to rank answers to non-factoid questions from Web collections.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="1225" citStr="Surdeanu et al., 2011" startWordPosition="172" endWordPosition="175">t multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Hal</context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2011</marker>
<rawString>Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to rank answers to non-factoid questions from Web collections. Computational Linguistics, 37(2):351–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="2077" citStr="Turney and Pantel, 2010" startWordPosition="300" endWordPosition="303">exical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense information (Agirre et al., 2012). We propose a unified approach to semantic similarity across multiple representation levels from senses to documents, which offers two significant advantages. First, the method is applicable independently of the input type, which enables meaningful si</context>
<context position="24300" citStr="Turney and Pantel, 2010" startWordPosition="3872" endWordPosition="3875"> (ADW-MF) outperforms all other similarity measures. The scores obtained by Explicit Semantic Analysis and Distributional Thesaurus are not competitive on any dataset. On the other hand, Pairwise Word Similarity achieves a high performance on MSRpar and MSRvid datasets, but performs surprisingly low on the SMTeuroparl dataset. 4 Experiment 2: Word Similarity We now proceed from the sentence level to the word level. Word similarity has been a key problem for lexical semantics, with significant efforts being made by approaches in distributional semantics to accurately identify synonymous words (Turney and Pantel, 2010). Different evaluation methods exist in the literature for evaluating the performance of a word-level semantic similarity measure; we adopted two well-established benchmarks: synonym recognition and correlating word similarity judgments with those from human annotators. For synonym recognition, we used the TOEFL dataset created by Landauer and Dumais (1997). The dataset consists of 80 multiple-choice synonym questions from the TOEFL test; a word is paired with four options, one of which is a valid synonym. Test takers with English as a second language averaged 64.5% correct. Despite multiple a</context>
<context position="35117" citStr="Turney and Pantel, 2010" startWordPosition="5573" endWordPosition="5576">5), who performed a word-to-word similarity alignment; however, they did not operate at the sense level. Ramage et al. (2009) used a similar semantic representation of short texts from random walks on WordNet, which was applied to paraphrase recognition and textual entailment. However, unlike our approach, their method does not perform sense disambiguation prior to building the representation and therefore potentially suffers from ambiguity. A significant amount of effort has also been put into measuring similarity at the word level, frequently by approaches that use distributional semantics (Turney and Pantel, 2010). These methods use contextual features to represent semantics at the word level, whereas our approach represents word semantics at the sense level. Most similar to our approach are those of Agirre et al. (2009) and Hughes and Ramage (2007), which represent word meaning as the multinomials produced from random walks on the WordNet graph. However, unlike our approach, neither of these disambiguates the two words being compared, which potentially conflates the meanings and lowers the similarity judgment. Measures of sense relatedness have frequently leveraged the structural properties of WordNet</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
<author>Jeffrey Bigham</author>
<author>Victor Shnayder</author>
</authors>
<title>Combining independent modules to solve multiple-choice synonym and analogy problems.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP,</booktitle>
<pages>482--489</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="25235" citStr="Turney et al., 2003" startWordPosition="4014" endWordPosition="4017">ataset created by Landauer and Dumais (1997). The dataset consists of 80 multiple-choice synonym questions from the TOEFL test; a word is paired with four options, one of which is a valid synonym. Test takers with English as a second language averaged 64.5% correct. Despite multiple approaches, only recently has the test been answered perfectly (Bullinaria and Levy, 2012), underscoring the challenge of synonym recognition. 1346 Approach Accuracy PPMIC (Bullinaria and Levy, 2007) 85.00% GLSA (Matveeva et al., 2005) 86.25% LSA (Rapp, 2003) 92.50% ADWJac 93.75±2.5% ADWWO 95.00% ADWCos 96.25% PR (Turney et al., 2003) 97.50% PCCP (Bullinaria and Levy, 2012) 100.00% Table 4: Accuracy on the 80-question TOEFL Synonym test. ADWJac, ADWWO, and ADWCos correspond to results with the Jaccard, Weighted Overlap and Cosine signature comparison measures, respectively. For the similarity judgment evaluation, we used as benchmark the RG-65 dataset created by Rubenstein and Goodenough (1965). The dataset contains 65 word pairs judged by 51 human subjects on a scale of 0 to 4 according to their semantic similarity. Ideally, a measure’s similarity judgments are expected to be highly correlated with those of humans. To be </context>
</contexts>
<marker>Turney, Littman, Bigham, Shnayder, 2003</marker>
<rawString>Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. 2003. Combining independent modules to solve multiple-choice synonym and analogy problems. In Proceedings of RANLP, pages 482–489, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane ˇSari´c</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Baˇsi´c.</title>
<date></date>
<booktitle>In Proceedings of SemEval-2012,</booktitle>
<pages>441--448</pages>
<location>Montreal, Canada.</location>
<marker>ˇSari´c, Glavaˇs, Karan, </marker>
<rawString>Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsi´c. 2012. Takelab: Systems for measuring semantic text similarity. In Proceedings of SemEval-2012, pages 441–448, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Wise</author>
</authors>
<title>String similarity via greedy string tiling and running Karp-Rabin matching.</title>
<date>1993</date>
<tech>Technical Report, Sydney.</tech>
<institution>In Department of Computer Science</institution>
<contexts>
<context position="18963" citStr="Wise, 1993" startWordPosition="3036" endWordPosition="3037">sed features. Additionally, because the texts often contain named entities which are not present in WordNet, we incorporated the similarity values produced by four string-based measures, which were used by other teams in the STS task: (1) longest common substring which takes into account the length of the longest overlapping contiguous sequence of characters (substring) across two strings (Gusfield, 1997), (2) longest common subsequence which, instead, finds the longest overlapping subsequence of two strings (Allison and Dix, 1986), (3) Greedy String Tiling which allows reordering in strings (Wise, 1993), and (4) the character/word n-gram similarity proposed by Barr´on-Cede˜no et al. (2010). We followed ˇSari´c et al. (2012) and used the models trained on the SMTeuroparl and MSRpar datasets for testing on the SMTnews and OnWN test sets, respectively. 3.2 STS Results Three evaluation metrics are provided by the organizers of the SemEval-2012 STS task, all of which are based on Pearson correlation r of human judgments with system outputs: (1) the correlation value for the concatenation of all five datasets (ALL), (2) a correlation value obtained on a concatenation of the outputs, separately nor</context>
</contexts>
<marker>Wise, 1993</marker>
<rawString>Michael J. Wise. 1993. String similarity via greedy string tiling and running Karp-Rabin matching. In Department of Computer Science Technical Report, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>189--196</pages>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="9965" citStr="Yarowsky (1995)" startWordPosition="1549" endWordPosition="1550">s of the context words in both texts. To find this maximum we use an alignment procedure which, for each word type wi in item T1, assigns wi to the sense that has the maximal similarity to any sense of the word types in the compared text T2. Algorithm 1 formalizes the alignment process, which produces a sense disambiguated representation as a result. Senses are compared in terms of their semantic signatures, denoted as function R. We consider multiple definitions of R, defined later in Section 2.3. As a part of the disambiguation procedure, we leverage the one sense per discourse heuristic of Yarowsky (1995); given all the word types in two compared lexical items, each type is assigned a single sense, even if it is used multiple times. Additionally, if the same word type appears in both sentences, both will always be mapped to the same sense. Although such a sense assignment is potentially incorrect, assigning both types to the same sense results in a representation that does no worse than a surface-level comparison. We illustrate the alignment-based disambiguation procedure using the two example sentences t1 and t2 given in Section 1. Figure 1(a) illustrates example alignments of the first sense</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised Word Sense Disambiguation rivaling supervised methods. In Proceedings of ACL, pages 189–196, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Christof M¨uller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using Wiktionary for computing semantic relatedness.</title>
<date>2008</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>861--866</pages>
<location>Chicago, Illinois.</location>
<marker>Zesch, M¨uller, Gurevych, 2008</marker>
<rawString>Torsten Zesch, Christof M¨uller, and Iryna Gurevych. 2008. Using Wiktionary for computing semantic relatedness. In Proceedings of AAAI, pages 861–866, Chicago, Illinois.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>