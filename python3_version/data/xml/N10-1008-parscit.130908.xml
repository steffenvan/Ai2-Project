<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000143">
<title confidence="0.99681">
Dialogue-Oriented Review Summary Generation for Spoken Dialogue
Recommendation Systems
</title>
<author confidence="0.886574">
Jingjing Liu, Stephanie Seneff, Victor Zue
</author>
<affiliation confidence="0.634176">
MIT Computer Science &amp; Artificial Intelligence Laboratory
</affiliation>
<address confidence="0.833083">
32 Vassar Street, Cambridge, MA 02139
</address>
<email confidence="0.987782">
{jingl, seneff, zue}@csail.mit.edu
</email>
<sectionHeader confidence="0.995823" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9977630625">
In this paper we present an opinion summari-
zation technique in spoken dialogue systems.
Opinion mining has been well studied for
years, but very few have considered its appli-
cation in spoken dialogue systems. Review
summarization, when applied to real dialogue
systems, is much more complicated than pure
text-based summarization. We conduct a sys-
tematic study on dialogue-system-oriented
review analysis and propose a three-level
framework for a recommendation dialogue
system. In previous work we have explored a
linguistic parsing approach to phrase extrac-
tion from reviews. In this paper we will de-
scribe an approach using statistical models
such as decision trees and SVMs to select the
most representative phrases from the ex-
tracted phrase set. We will also explain how
to generate informative yet concise review
summaries for dialogue purposes. Experimen-
tal results in the restaurant domain show that
the proposed approach using decision tree al-
gorithms achieves an outperformance of 13%
compared to SVM models and an improve-
ment of 36% over a heuristic rule baseline.
Experiments also show that the decision-tree-
based phrase selection model can achieve ra-
ther reliable predictions on the phrase label,
comparable to human judgment. The pro-
posed statistical approach is based on do-
main-independent learning features and can
be extended to other domains effectively.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962682926829">
Spoken dialogue systems are presently available
for many purposes, such as weather inquiry (Zue
et al., 2000), bus schedules and route guidance
(Raux et al., 2003), customer service (Gorin et al.,
1997), and train timetable inquiry (Eckert et al.,
1993). These systems have been well developed
for laboratory research, and some have become
commercially viable.
The next generation of intelligent dialogue sys-
tems is expected to go beyond factoid question
answering and straightforward task fulfillment, by
providing active assistance and subjective recom-
mendations, thus behaving more like human
agents. For example, an intelligent dialogue sys-
tem may suggest which airline is a better choice,
considering cost, flight duration, take-off time,
available seats, etc.; or suggest which digital cam-
era is the most popular among teenagers or highest
rated by professional photographers; or which res-
taurant is a perfect spot for a semi-formal business
meeting or a romantic date.
Luckily, there are enormous amounts of reviews
published by general users on the web every day.
These are perfect resources for providing subjec-
tive recommendations and collective opinions. If
there exists a systematic framework that harvests
these reviews from general users, extracts the es-
sence from the reviews and presents it appropriate-
ly in human-computer conversations, then we can
enable dialogue systems to behave like a human
shopping assistant, a travel agent, or a local friend
who tells you where to find the best restaurant.
Summarization from online reviews, therefore,
plays an important role for such dialogue systems.
There have been previous studies on review analy-
sis for text-based summarization systems (Mei et
al., 2007; Titov and McDonald, 2008a; Branavan
et al., 2008). Mixture models and topic models are
used to predict the underlying topics of each doc-
ument and generate a phrase-level summary. An
aspect rating on each facet is also automatically
</bodyText>
<page confidence="0.992954">
64
</page>
<note confidence="0.7560085">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 64–72,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9928861875">
learned with statistical models (Snyder and Barzi-
lay, 2007; Titov and McDonald, 2008b; Baccia-
nella et al., 2009). These approaches are all very
effective, and the review databases generated are
well presented.
So the first thought for developing a recom-
mendation dialogue system is to use such a cate-
gorized summary in a table-lookup fashion. For
example, a dialogue system for restaurant recom-
mendations can look up a summary table as exem-
plified in Table 1, and generate a response
utterance from each row: “Restaurant A has good
service and bad food; restaurant B has good ser-
vice and good food; restaurant C has great service
and nice atmosphere; restaurant D has poor service
and reasonable price.”
</bodyText>
<table confidence="0.9534334">
Restaurant Summary
A Good service, bad food,
B Good service, good food
C Great service, nice atmosphere
D Poor service, reasonable price
</table>
<tableCaption confidence="0.978995">
Table 1. A partial table of categorization-based review
summaries.
</tableCaption>
<bodyText confidence="0.997728790697674">
Such a dialogue system is, however, not very
informative. First of all, there is too much redun-
dancy. Long utterances repeated in the same pat-
tern on the same topic are quite boring, and the
information density is very low. Second, such a
summary is too coarse-grained to be helpful. A
user querying a restaurant recommendation system
expects more fine-grained information such as
house specials, wine selections and choices on
desserts rather than just general ‘good food.’
In contrast to a ‘text’ summarization system, the
textual space in a dialogue turn is often very li-
mited. Speech is inherently serial, and it cannot be
skipped and scanned easily. A dialogue system
which speaks long diatribes in each single conver-
sation turn would likely not be well received. We
believe that the best unit of review summary for
dialogue purposes is short succinct phrases (e.g.,
‘amazing sauce’ and ‘romantic ambiance’) which
are catalogued into categories to answer high-level
questions such as “How is the food at this restau-
rant?” or “What about the atmosphere?” Also,
domain-specific phrases are greatly needed to
generate correct answers to specific queries, such
as “Which restaurants have fresh sushi?” and
“Show me restaurants that serve fantastic cock-
tails.”
Thus, the generally used correlated topic mod-
els, although very effective in text-based systems,
are not quite suitable for interactive dialogue sys-
tems. The missing piece is an HCI-oriented (hu-
man computer interaction), fine-grained,
informative yet concise review summarization. A
good recommendation system should be able to
provide sufficient yet specific information to help
users choose a restaurant, a movie or a consumer
product. Figure 1 shows an example of a conversa-
tion with a good recommendation system, which 1)
provides a generalized yet high-density review
summary in human-friendly dialogue; 2) provides
both a coarse-grained summary (e.g., ‘authentic
food’) and fine-grained information such as house
specialties.
</bodyText>
<listItem confidence="0.2365014">
U: Are there any French restaurants in Cambridge?
S: There are five French restaurants in Cambridge.
Most of them are on Backbay Street. And most of
them have attentive service.
U: Show me the ones that have nice ambiance.
</listItem>
<subsectionHeader confidence="0.5663025">
S: Two of them have nice ambiance, Pierrot Bistrot
Francais and Coriander.
</subsectionHeader>
<bodyText confidence="0.49922">
U: How about the food?
</bodyText>
<figureCaption confidence="0.9351163">
S: Pierrot Bistrot Francais has authentic food,
fresh salad, and absolutely delicious spaghetti
carbonara. The food at Coriander is below aver-
age. It has very greasy chicken tikka masala and
dry bread.
Figure 1. Example of a conversation with a good rec-
ommendation dialogue system (‘U’ denotes the user
and ‘S’ denotes the dialogue system. The responses to
the user queries are produced by our system and the
actual dialogue was an illustration of system capacities).
</figureCaption>
<bodyText confidence="0.999586357142857">
Therefore, the task of developing recommenda-
tion dialogue systems is decomposed into three
problems: 1) how to extract context-related phras-
es, both coarse-grained and fine-grained, from
online reviews; 2) how to select a representative
set from the extracted phrases to create an infor-
mative yet concise dialogue-oriented summary
database; 3) how to generate human-friendly di-
alogue responses from the review summary data-
base.
To tackle these problems, we propose a three-
level framework. In previous work (Liu and Seneff,
2009), we explored the first level by proposing a
linguistic parse-and-paraphrase paradigm for re-
</bodyText>
<page confidence="0.999273">
65
</page>
<bodyText confidence="0.999918">
view phrase extraction. In this paper, we address
the second problem: dialogue-oriented review
summary generation. We propose an automatic
approach to classifying high/low informative
phrases using statistical models. Experiments con-
ducted on a restaurant-domain dataset indicate that
the proposed approach can predict phrase labels
consistently with human judgment and can gener-
ate high-quality review summaries for dialogue
purposes.
The rest of the paper is organized as follows:
Section 2 gives an overview of the three-level
framework for recommendation dialogue systems.
In Section 3, we explain the proposed approach to
dialogue-oriented review summary generation.
Section 4 provides a systematic evaluation of the
proposed approach, and Section 5 gives a further
discussion on the experimental results. Section 6
summarizes the paper as well as pointing to future
work.
</bodyText>
<sectionHeader confidence="0.948206" genericHeader="method">
2 System Overview
</sectionHeader>
<bodyText confidence="0.999902525">
The three-level framework of a review-summary-
based recommendation dialogue system is shown
in Figure 2. The bottom level is linguistic phrase
extraction. In previous work (Liu and Seneff,
2009), we employed a probabilistic lexicalized
grammar to parse review sentences into a hierar-
chical representation, which we call a linguistic
frame. From the linguistic frames, phrases are ex-
tracted by capturing a set of adjective-noun rela-
tionships. Adverbs and negations conjoined with
the adjectives are also captured. We also calcu-
lated a numerical score for sentiment strength for
each adjective and adverb, and further applied a
cumulative offset model to assign a sentiment
score to each phrase.
The approach relies on linguistic features that
are independent of frequency statistics; therefore it
can retrieve very rare phrases such as ‘very greasy
chicken tikka masala’ and ‘absolutely delicious
spaghetti carbonara’, which are very hard to derive
from correlated topic models. Experimental results
showed that the linguistic paradigm outperforms
existing methods of phrase extraction which em-
ploy shallow parsing features (e.g., part-of-speech).
The main contribution came from the linguistic
frame, which preserves linguistic structure of a
sentence by encoding different layers of semantic
dependencies. This allows us to employ more so-
phisticated high-level linguistic features (e.g., long
distance semantic dependencies) for phrase extrac-
tion.
However, the linguistic approach fails to distin-
guish highly informative and relevant phrases
from uninformative ones (e.g., ‘drunken husband’,
‘whole staff’). To apply these extracted phrases
within a recommendation dialogue system, we
have to filter out low quality or irrelevant phrases
and maintain a concise summary database. This is
the second level: dialogue-oriented review sum-
mary generation.
</bodyText>
<figureCaption confidence="0.995721">
Figure 2. Three-level framework of review-based rec-
ommendation dialogue systems.
</figureCaption>
<bodyText confidence="0.999983538461539">
The standard of highly informative and relevant
phrases is a very subjective problem. To gain in-
sights on human judgment on this, the first two
authors separately labeled a set of review-related
phrases in a restaurant domain as ‘good’ and ‘bad’
summary phrases. We surveyed several subjects,
all of whom indicated that, when querying a dialo-
gue system for information about a restaurant,
they care much more about special dishes served
in this restaurant than generic descriptions such as
‘good food.’ This knowledge informed the annota-
tion task: to judge whether a phrase delivered by a
dialogue recommendation system would be help-
</bodyText>
<page confidence="0.94649">
66
</page>
<bodyText confidence="0.999985931818182">
ful for users to make a decision. Surprisingly, al-
though this is a difficult and subjective problem,
the judgment from the two annotators is substan-
tially consistent. By examining the annotations we
observed that phrases such as ‘great value’ and
‘good quality’ are often treated as ‘uninformative’
as they are too common to be representative for a
particular product, a restaurant or a movie. Phrases
with neutral sentiment (e.g., ‘green beans’ and
‘whole staff’) are often considered as uninforma-
tive too. Phrases on specific topics such as house
specialties (e.g., ‘absolutely delicious spaghetti
carbonara’) are what the annotators care about
most and are often considered as highly relevant,
even though they may have only been seen once in
a large database.
Driven by these criteria, from each phrase we
extract a set of statistical features such as uni-
gram/bigram probabilities and sentiment features
such as sentiment orientation degree of the phrase,
as well as underlying semantic features (e.g.,
whether the topic of the phrase fits in a domain-
specific ontology). Classification models such as
SVMs and decision tree algorithms are then
trained on these features to automatically classify
high/low informative phrases. Phrases identified
as ‘good’ candidates are further pruned and cata-
logued to create concise summaries for dialogue
purposes.
After generating the review summary database,
the third level is to modify the response generation
component in dialogue systems to create genera-
lized and interactive conversations, as exemplified
in Figure 1. The utterance from users is piped
through speech recognition and language under-
standing. The meaning representation is then sent
to the dialogue management component for re-
view-summary database lookup. A response is
then generated by the language generation compo-
nent, and a speech utterance is generated by the
synthesizer and sent back to the user. The dialogue
system implementation is beyond the scope of this
paper and will be discussed later in a separate pa-
per.
</bodyText>
<sectionHeader confidence="0.987501" genericHeader="method">
3 Dialogue-oriented Review Summary
Generation
</sectionHeader>
<bodyText confidence="0.999628333333333">
Given an inquiry from users, the answer from a
recommendation system should be helpful and
relevant. So the first task is to identify a phrase as
‘helpful’ or not. The task of identifying a phrase as
informative and relevant, therefore, is defined as a
classification problem:
</bodyText>
<equation confidence="0.9992915">
Y = 9� -x = Z 91�1
71=1 (1)
</equation>
<bodyText confidence="0.999087260869565">
where y is the label of a phrase, assigned as ‘1’ if
the phrase is highly informative and relevant, and
‘-1’ if the phrase is uninformative. x is the feature
vector extracted from the phrase, and 9 is the
coefficient vector.
We employ statistical models such as SVMs
(Joachims, 1998) and decision trees (Quinlan,
1986) to train the classification model. For model
learning, we employ a feature set including statis-
tical features, sentiment features and semantic
features.
Generally speaking, phrases with neutral senti-
ment are less informative than those with strong
sentiment, either positive or negative. For example,
‘fried seafood appetizer’, ‘baked halibut’, ‘elec-
tronic bill’ and ‘red drink’ do not indicate whether
a restaurant is worth trying, as they did not express
whether the fried seafood appetizer or the baked
halibut are good or bad. Therefore, we take the
sentiment score of each phrase generated from a
cumulative offset model (Liu and Seneff, 2009) as
a sentiment feature. Sentiment scores of phrases
are exemplified in Table 2 (on a scale of 1 to 5).
</bodyText>
<table confidence="0.965100625">
Phrase Sc. Phrase Sc.
really welcoming 4.8 truly amazing flavor 4.6
atmosphere
perfect portions 4.4 very tasty meat 4.3
busy place 3.1 typical Italian restaurant 3.1
a little bit high 2.2 pretty bad soup 1.8
price
sloppy service 1.8 absolute worst service 1.4
</table>
<tableCaption confidence="0.999531">
Table 2. Examples of sentiment scores of phrases.
</tableCaption>
<bodyText confidence="0.999977181818182">
We also employ a set of statistical features for
model training, such as the unigram probability of
the adjective in a phrase, the unigram probability
of the noun in a phrase, the unigram probability of
the phrase and the bigram probability of the adjec-
tive-noun pair in a phrase.
Statistical features, however, fail to reveal the
underlying semantic meaning of phrases. For ex-
ample, phrases ‘greasy chicken tikka masala’ and
‘drunken husband’ have the same n-gram proba-
bilities in our corpus (a single observation), but
</bodyText>
<page confidence="0.997508">
67
</page>
<bodyText confidence="0.9996212">
they should certainly not be treated as the same.
To capture the semantic meanings of phrases, we
first cluster the topics of phrases into generic se-
mantic categories. The language-model based al-
gorithm is given by:
</bodyText>
<equation confidence="0.999405428571429">
&apos;(() |(%) = ∑+∈. &apos;(()|+) ∙ &apos;(+|(%)
= ∑ &apos;(+,())
&apos;(+) ∙ &apos;(+,(%)
+∈. &apos;((%)
= 1
&apos;((%) ∑ 1
+∈.&apos;(+) ∙ &apos;(+,()) ∙ &apos;(+,(%) (2)
</equation>
<bodyText confidence="0.999372923076923">
where A represents the set of all the adjectives in
the corpus. We select a small set of initial topics
with the highest frequency counts (e.g., ‘food’,
‘service’ and ‘atmosphere’). For each of the other
topics tc (e.g., ‘chicken’, ‘waitress’ and ‘décor’),
we calculate its similarity with each initial topic (%
based on the bigram probability statistics. For
those topics with conditional probability higher
than a threshold for an initial topic (%, we assign
them to the cluster of (%. We use this as a semantic
feature, e.g., whether the topic of a phrase belongs
to a generic semantic category. Table 3 gives some
clustering examples.
</bodyText>
<table confidence="0.5057713">
Category Relevant Topics
food appetizer, beer, bread, fish, fries, ice
cream, margaritas, menu, pizza, pasta,
rib, roll, sauce, seafood, sandwich,
steak, sushi, dessert, cocktail, brunch
service waiter, staff, management, server,
hostess, chef, bartender, waitstaff
atmosphere décor, ambiance, music, vibe, setting,
environment, crowd
price bill, pricing, prices
</table>
<tableCaption confidence="0.981374">
Table 3. Topic to semantic category clustering.
</tableCaption>
<bodyText confidence="0.999937694444444">
This language-model-based method relies on
bigram probability statistics and can well cluster
highly frequent topics. Categories such as ‘service’
and ‘atmosphere’ contain very limited related top-
ics, most of which have high frequencies (e.g.,
‘waiter’, ‘staff’, ‘ambiance’ and ‘vibe’). The cate-
gory ‘food’, however, is very domain-specific and
contains a very large vocabulary, from generic
sub-categories such as ‘sushi’, ‘dessert’ and
‘sandwich’ as shown in the examples, to specific
courses such as ‘bosc pear bread pudding’ and
‘herb roasted vermont pheasant wine cap mu-
shrooms’. These domain-specific topics have very
low frequencies, yet they are very relevant and
valuable. But many of them are discarded by the
clustering. It would be a similar case in other do-
mains. For example, consumer products, movies
and books all have domain-independent semantic
categories (e.g., ‘price’ and ‘released date’) and
domain-specific categories (e.g., technical features
of consumer products, casts of movies and authors
of books).
To recover these context-relevant topics, we
employ domain context relations such as a con-
text-related ontology. A context-related ontology
can be constructed from structured web resources
such as online menus of restaurants, names of ac-
tors and actresses from movie databases, and spe-
cifications of products from online shops. An
example of a partial online menu of a restaurant is
shown in Figure 3. From these structured web re-
sources, we can build up a hierarchical ontology,
based on which a set of semantic features can be
extracted (e.g., whether a phrase contains a course
name, or an actress’s name, or a dimension of
technical features of a consumer product).
</bodyText>
<table confidence="0.688884">
Entree
Roasted Pork Loin Wrapped In Bacon with watermelon and
red onion salad spicy honey-mustard bbq sauce
Spicy Halibut And Clam Roast with bacon braised greens,
white beans and black trumpet mushrooms
Parmesan and Caramelized Shallot Wrapper Style Ravi-
oli turnip greens and white truffle oil
Herb Roasted Vermont Pheasant Wine Cap Mushrooms,
Pearl Onions and Fava Beans
Dessert
Chocolate Tasting Plate of white chocolate bombe milk choc-
olate creme brulée and dark chocolate flourless cake
White Fruit Tasting Plate of warm apple strudel butterscotch,
Bosc Pear bread pudding and toasted coconut panna cotta
Entrée Pork loin, bacon, watermelon, red onion
salad, honey, mustard, bbq sauce
Dessert Chocolate, milk, crème brulee, cake
</table>
<figureCaption confidence="0.986803">
Figure 3. Example of a partial online menu and an ex-
emplary ontology derived.
</figureCaption>
<bodyText confidence="0.999404333333333">
After the classification, phrases identified as
‘highly informative and relevant’ are clustered
into different aspects according to the semantic
category clustering and the hierarchical ontology.
An average sentiment score for each aspect is then
calculated:
</bodyText>
<equation confidence="0.965845">
+/0(1() = ∑3∈41 23 (3)
</equation>
<page confidence="0.80073">
|41|
68
</page>
<bodyText confidence="0.9999845">
where st represents the aspect s of entry t (e.g., a
restaurant, a movie, or a consumer product), Ns
represents the set of phrases in the cluster of as-
pect s, and r represents the sentiment score of
phrase j in the cluster.
The set of phrases selected for one entry may
come from several reviews on this single entry,
and many of them may include the same noun
(e.g., ‘good fish’, ‘not bad fish’ and ‘above-
average fish’ for one restaurant). Thus, the next
step is multi-phrase redundancy resolution. We
select the phrase with a sentiment score closest to
the average score of its cluster as the most repre-
sentative phrase on each topic:
</bodyText>
<equation confidence="0.98971">
m = ar9minj ENi(|r− ave(st)|) (4)
</equation>
<bodyText confidence="0.994867666666667">
where ave(st) represents the average sentiment
score of aspect s, Ni represents the set of phrases
on the same topic i in the cluster s , and r
represents the sentiment score of phrase j.
This sequence of topic categorization, ontology
construction, phrase pruning and redundancy eli-
mination leads to a summary database, which can
be utilized for dialogue generation in spoken rec-
ommendation systems. A review summary data-
base entry generated by the proposed approaches
is exemplified in Figure 4.
{ restaurant &amp;quot;dali restaurant and tapas bar&amp;quot;
:atmosphere ( &amp;quot;wonderful evening&amp;quot;, &amp;quot;cozy atmos-
phere&amp;quot;, &amp;quot;fun decor&amp;quot;, &amp;quot;romantic date&amp;quot; )
:atmosphere_rating &amp;quot;4.1&amp;quot;
</bodyText>
<figure confidence="0.563803428571429">
:food ( &amp;quot;very fresh ingredients&amp;quot;, &amp;quot;tasty fish&amp;quot;,
&amp;quot;creative dishes&amp;quot;, &amp;quot;good sangria&amp;quot; )
:food_rating &amp;quot;3.9&amp;quot;
:service ( &amp;quot;fast service&amp;quot; )
:service_rating &amp;quot;3.9&amp;quot;
:general (&amp;quot;romantic restaurant&amp;quot;,&amp;quot;small space&amp;quot; )
:general_rating &amp;quot;3.6&amp;quot; }
</figure>
<figureCaption confidence="0.986109">
Figure 4. Example of a review summary database entry
generated by the proposed approaches.
</figureCaption>
<sectionHeader confidence="0.999239" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999779836363636">
In this project, we substantiate the proposed ap-
proach in a restaurant domain for our spoken di-
alogue system (Gruenstein and Seneff, 2007),
which is a web-based multimodal dialogue system
allowing users to inquire about information about
restaurants, museums, subways, etc. We harvested
a data collection of 137,569 reviews on 24,043
restaurants in 9 cities in the U.S. from an online
restaurant evaluation website1. From the dataset,
857,466 sentences were subjected to parse analysis;
and a total of 434,372 phrases (114,369 unique
ones) were extracted from the parsable subset
(78.6%) of the sentences.
Most pros/cons consist of well-formatted phras-
es; thus, we select 3,000 phrases extracted from
pros/cons as training data. To generate a human
judgment-consistent training set, we manually la-
bel the training samples with ‘good’ and ‘bad’ la-
bels. We then randomly select a subset of 3,000
phrases extracted from review texts as the test set
and label the phrases. The kappa agreement be-
tween two sets of annotations is 0.73, indicating
substantial consistency. We use the two annotation
sets as the ground truth.
To extract context-related semantic features, we
collect a large pool of well-formatted menus from
an online resource2, which contains 16,141 restau-
rant menus. Based on the hierarchical structure of
these collected menus, we build up a context-
related ontology and extract a set of semantic fea-
tures from the ontology, such as whether the topic
of a phrase is on category-level (e.g., ‘entrée’,
‘dessert’, ‘appetizers’, ‘salad’), whether the topic
is on course-level (e.g., ‘Roasted Pork Loin’, ‘Spi-
cy Halibut and Clam Roast’), and whether the top-
ic is on ingredient-level (e.g., ‘beans’, ‘chicken’,
‘mushrooms’, ‘scallop’).
We employ the three types of features as afore-
mentioned to train the SVMs and the decision tree
models. To select the most valuable features for
model training, we conducted a set of leave-one-
feature-out experiments for both the SVMs and the
decision tree models. We found that all the fea-
tures except the adjective unigram probability
contribute positively to model learning. From fur-
ther data analysis we observed that many phrases
with popular adjectives have context-unrelated
nouns, which makes the adjective unigram proba-
bility fail to become a dominant factor for phrase
relevance. Using the adjective unigram probability
as a learning feature will mislead the system into
trusting an adjective that is common but has a poor
bigram affinity to the noun in the phrase. Thus, we
eliminate this feature for both the SVMs and the
decision tree learning.
</bodyText>
<footnote confidence="0.999547">
1 http://www.citysearch.com
2 http://www.menupages.com
</footnote>
<page confidence="0.999228">
69
</page>
<bodyText confidence="0.998616">
To evaluate the performance of the classifica-
tion models, we take a set of intuitively motivated
heuristic rules as the baseline. Figure 5 gives the
pseudo-code of the heuristic rule algorithm, which
uses variations of all the features except the uni-
gram probability of adjectives.
</bodyText>
<figure confidence="0.91589">
If(sentiment score of the phrase exists)
if(sentiment score is within neutral range) label=-1;
else
if(phrase appeared in the training data)
if((3&lt;frequency of phrase &lt; 100)) label = 1;
else
if(frequency of phrase &gt;= 100) label = -1;
else if(topic belongs to ontology) label = 1;
else label = -1;
else
if(topic belongs to ontology) label = 1;
else label = -1;
else
if(phrase appeared in the training data)
if((3&lt;frequency of phrase &lt; 100))
if(topic belongs to ontology) label = 1;
else label = -1;
else
if(frequency of phrase &gt;= 100) label = -1;
else
if(topic belongs to ontology) label = 1;
else if(frequency of noun &gt; 100) label = 1;
else label = -1;
else
if(topic belongs to ontology) label = 1;
else if(frequency of noun &gt; 100) label = 1;
else label = -1;
</figure>
<figureCaption confidence="0.999958">
Figure 5. Pseudo-code of the heuristic rule algorithm.
</figureCaption>
<bodyText confidence="0.99950605">
The performance of classification by different
models is shown in Table 4. Although the heuris-
tic rule algorithm is complicated and involves hu-
man knowledge, the statistical models trained by
SVMs and the decision tree algorithms both out-
perform the baseline significantly. The SVM mod-
el outperforms the baseline by 10.5% and 11.9%
on the two annotation sets respectively. The deci-
sion tree model outperforms the baseline by 16.4%
and 23.2% (average relative improvement of 36%),
and it also outperforms the SVM model by 5.9%
and 11.3% (average relative improvement of 13%).
The classification model using the decision tree
algorithm can achieve a precision of 77.9% and
74.5% compared with the ground truth, which is
quite comparable to human judgment (the preci-
sion of one annotation set based on the other is
74%). This shows that the decision tree model can
predict phrase labels as reliably as human judg-
ment.
</bodyText>
<table confidence="0.99917425">
Baseline SVM Decision
tree
Annotation 1 61.5% 72.0% 77.9%
Annotation 2 51.3% 63.2% 74.5%
</table>
<tableCaption confidence="0.957958333333333">
Table 4. Precision of phrase classification using the
heuristic rule baseline, the SVM model, and the deci-
sion tree algorithm.
</tableCaption>
<bodyText confidence="0.9962072">
To gain further insight on the contributions of
each feature set to the decision tree learning, Table
5 gives the experimental results on leaving each
feature out of model training. As shown, without
semantic features, the precision is 70.6% and 65.4%
on the two annotation sets, lower by 7.3% and 9.1%
than the case of training the model with all the
features (77.9% and 74.5%). This shows that the
semantic features significantly contribute to the
decision tree learning.
</bodyText>
<table confidence="0.981702083333333">
Feature set A1 A2
all features 77.9% 74.5%
without bigram probability 56.6% 63.9%
of adjective-noun pair (-21.3%) (-10.6%)
without unigram probability 57.6% 64.3%
of the phrase (-20.3%) (-10.2%)
without unigram probability 59.8% 67.8%
of the noun (-18.1%) (-6.7%)
without sentiment score of 63.4% 66.6%
the phrase (-14.5%) (-7.9%)
without underlying semantic 70.6% 65.4%
features (-7.3%) (-9.1%)
</table>
<tableCaption confidence="0.976114">
Table 5. Performance of the decision tree model by
leaving each feature out of model training (‘A1’ and
‘A2’ represent the annotation set 1 and 2 respectively).
</tableCaption>
<bodyText confidence="0.999779333333333">
The experimental results also show that the fea-
ture of bigram probability of the adjective-noun
pair contributes the most to the model learning.
Without this feature, the precision drops by 21.3%
and 10.6%, reaching the lowest precision among
all the leave-one-out experiments. This confirms
our observation that although a single adjective is
not dominant, the pair of the adjective and the
noun that co-occurs with it plays an important role
in the classification.
The sentiment of phrases also plays an impor-
tant role. Without sentiment features, the precision
</bodyText>
<page confidence="0.992805">
70
</page>
<bodyText confidence="0.99876675">
drops to 63.4% and 66.6% respectively on the two
annotations, decreasing by 14.5% and 7.9%. This
shows that the sentiment features contribute sig-
nificantly to the classification.
</bodyText>
<sectionHeader confidence="0.977878" genericHeader="method">
5 Discussions
</sectionHeader>
<bodyText confidence="0.99980716">
tering rules was manually created to eliminate
low-quality mappings. In our approach, we use an
automatic approach to classifying high/low infor-
mative phrases. The learning features are domain-
independent with no hand-crafted rules, and can
be extended to other domains effortlessly.
Experimental results show that the decision tree
algorithm outperforms the SVMs on this particular
classification problem, and it outperforms the heu-
ristic rule baseline significantly. Thus, although
the identification of informativeness and relevance
of phrases is a rather subjective problem, which is
difficult to predict using only human knowledge, it
can be well defined by decision trees. Part of the
reason is that the decision tree algorithm can make
better use of a combination of Boolean value fea-
tures (e.g., whether a topic belongs to a context-
related ontology) and continuous value features.
Also, as the phrase classification task is very sub-
jective, it is very similar to a ‘hierarchical if-else
decision problem’ in human cognition, where de-
cision tree algorithms can fit well. Figure 6 shows
a partial simplified decision tree learned from our
model, which can give an intuitive idea of the de-
cision tree models.
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999943173913044">
Sentiment classification and opinion mining have
been well studied for years. Most studies have fo-
cused on text-based systems, such as document-
level sentiment classification and sentence-level
opinion aggregation (Turney, 2002; Pang et al.,
2002; Dave et al., 2003; Hu and Liu, 2004; Popes-
cu and Etzioni, 2005; Wilson et al., 2005; Zhuang
et al., 2006; Kim and Hovy, 2006).
There was a study conducted by Carenini et al.
in 2006, which proposed a combination of a sen-
tence extraction-based approach and a language
generation-based approach for summarizing eva-
luative arguments. In our work, we utilize a lower-
level phrase-based extraction approach, which uti-
lizes high level linguistic features and syntactic
structure to capture phrase patterns.
There was also a study on using reviews to gen-
erate a dictionary of mappings between semantic
representations and realizations of concepts for
dialogue systems (Higashinaka et al., 2006; Higa-
shinaka, 2007). They also used the association
between user ratings and reviews to capture se-
mantic-syntactic structure mappings. A set of fil-
</bodyText>
<sectionHeader confidence="0.999095" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999342583333333">
In this paper we proposed a three-level framework
for review-based recommendation dialogue sys-
tems, including linguistic phrase extraction, dialo-
gue-oriented review summary generation, and
human-friendly dialogue generation. The contribu-
tions of this paper are three-fold: 1) it identified
and defined the research goal of utilizing opinion
summarization for real human-computer conversa-
tion; 2) it formulated an evaluation methodology
for high-density review summary for dialogue
purposes; 3) it proposed an approach to automatic
classification of high/low informative phrases us-
ing a decision tree model. Experimental results
showed that the decision tree model significantly
outperforms a heuristic rule baseline and the SVM
model, and can resolve the phrase classification
problem comparably to humans consistently.
Future work will focus on: 1) applying the sen-
timent scoring model to noun/verb sentiment as-
sessment; 2) application of the review summary
generation approach in other domains and other
languages; 3) data collection on user engagement
with our dialogue systems involving review-
summary evaluation.
</bodyText>
<figureCaption confidence="0.9818905">
Figure 6. A partial simplified decision tree learned from
our model.
</figureCaption>
<page confidence="0.99719">
71
</page>
<sectionHeader confidence="0.995831" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9994825">
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Multi-facet Rating of Product Reviews.
In Proceedings of European Conference on Informa-
tion Retrieval.
S.R.K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning document-level
semantic properties from free-text annotations. In
Proc. of ACL.
Giuseppe Carenini, Raymond Ng, and Adam Pauls.
2006. Multi-Document Summarization of Evaluative
Text. In Proceedings of the Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
Proceedings of the International Conference on
World Wide Web.
W. Eckert, T. Kuhn, H. Niemann, S. Rieck, A. Scheuer,
and E. G. Schukat-talamazzini. 1993. A Spoken Di-
alogue System for German Intercity Train Timetable
Inquiries. In Proc. European Conf. on Speech Tech-
nology.
Alexander Gruenstein and Stephanie Seneff. 2007. Re-
leasing a Multimodal Dialogue System into the
Wild: User Support Mechanisms. In Proceedings of
the 8th SIGdial Workshop on Discourse and Dialo-
gue, Antwerp, pages 111-119.
A. L. Gorin, G. Riccardi and J. H. Wright. 1997. “How
may I help you?” Speech Communication, vol. 23,
pp. 113–127.
Ryuichiro Higashinaka, Rashmi Prasad and Marilyn
Walker. 2006. Learning to Generate
Naturalistic Utterances Using Reviews in Spoken
Dialogue Systems. In Proceedings of COLING-ACL.
Ryuichiro Higashinaka, Marilyn Walker and Rashmi
Prasad. 2007. An Unsupervised Method
for Learning Generation Dictionaries for Spoken Di-
alogue Systems by Mining User Reviews.
Journal of ACM Transactions on Speech and Lan-
guage Processing.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 2004
ACM SIGKDD international conference on Know-
ledge Discovery and Data mining.
S.M. Kim and E.H. Hovy. 2006. Identifying and Ana-
lyzing Judgment Opinions. In Proc. of HLT/NAACL.
Jingjing Liu and Stephanie Seneff. 2009. Review Sen-
timent Scoring via a Parse-and-Paraphrase Para-
digm. In proceedings of EMNLP.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic Sentiment Mix-
ture: Modeling Facets and Opinions in Weblogs. In
Proc. of WWW.
Bo Pang, Lillian Lee, and S. Vaithyanathan. 2002.
Thumbs up? Sentiment classification using machine
learning techniques. In Proceedings of EMNLP.
A.M. Popescu and O. Etzioni. 2005. Extracting product
features and opinions from reviews. In Proceedings
of EMNLP.
JR Quinlan, 1986. Induction of decision trees. Machine
learning, Springer-Netherlands.
A. Raux, B. Langner, A. Black, and M. Eskenazi. 2003.
LET&apos;S GO: Improving Spoken Dialog Systems for
the Elderly and Non-natives. In Proc. Eurospeech.
Benjamin Snyder and Regina Barzilay. 2007. Multiple
Aspect Ranking using the Good Grief Algorithm. In
Proceedings of NAACL-HLT.
Ivan Titov and Ryan McDonald. 2008a. Modeling On-
line Reviews with Multi-Grain Topic Models. In
Proc. of WWW.
Ivan Titov and Ryan McDonald. 2008b. A Joint Model
of Text and Aspect Ratings for Sentiment Summari-
zation. In Proceedings of the Annual Conference of
the Association for Computational Linguistics.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Sentiment orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the Annual
Conference of the Association for Computational
Linguistics.
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. In Proc. of ECML, p. 137–142.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-Level Senti-
ment Analysis. In Proc. of HLT/EMNLP.
Victor Zue, Stephanie Seneff, James Glass, Joseph Po-
lifroni, Christine Pao, Timothy J. Hazen, and Lee
Hetherington. 2000. JUPITER: A Telephone-Based
Conversational Interface for Weather Information. In
IEEE Transactions on Speech and Audio Processing,
Vol. 8 , No. 1.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie
review mining and summarization. In Proceedings of
the 15th ACM international conference on Informa-
tion and knowledge management.
</reference>
<page confidence="0.998695">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958810">
<title confidence="0.9993625">Dialogue-Oriented Review Summary Generation for Spoken Recommendation Systems</title>
<author confidence="0.995403">Jingjing Liu</author>
<author confidence="0.995403">Stephanie Seneff</author>
<author confidence="0.995403">Victor</author>
<affiliation confidence="0.999243">MIT Computer Science &amp; Artificial Intelligence</affiliation>
<address confidence="0.994411">32 Vassar Street, Cambridge, MA</address>
<email confidence="0.998632">jingl@csail.mit.edu</email>
<email confidence="0.998632">seneff@csail.mit.edu</email>
<email confidence="0.998632">zue@csail.mit.edu</email>
<abstract confidence="0.999106878787879">In this paper we present an opinion summarization technique in spoken dialogue systems. Opinion mining has been well studied for years, but very few have considered its application in spoken dialogue systems. Review summarization, when applied to real dialogue systems, is much more complicated than pure text-based summarization. We conduct a systematic study on dialogue-system-oriented review analysis and propose a three-level framework for a recommendation dialogue system. In previous work we have explored a linguistic parsing approach to phrase extraction from reviews. In this paper we will describe an approach using statistical models such as decision trees and SVMs to select the most representative phrases from the extracted phrase set. We will also explain how to generate informative yet concise review summaries for dialogue purposes. Experimental results in the restaurant domain show that the proposed approach using decision tree algorithms achieves an outperformance of 13% compared to SVM models and an improvement of 36% over a heuristic rule baseline. Experiments also show that the decision-treebased phrase selection model can achieve rather reliable predictions on the phrase label, comparable to human judgment. The proposed statistical approach is based on domain-independent learning features and can be extended to other domains effectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Multi-facet Rating of Product Reviews.</title>
<date>2009</date>
<booktitle>In Proceedings of European Conference on Information Retrieval.</booktitle>
<contexts>
<context position="3922" citStr="Baccianella et al., 2009" startWordPosition="588" endWordPosition="592">tudies on review analysis for text-based summarization systems (Mei et al., 2007; Titov and McDonald, 2008a; Branavan et al., 2008). Mixture models and topic models are used to predict the underlying topics of each document and generate a phrase-level summary. An aspect rating on each facet is also automatically 64 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 64–72, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics learned with statistical models (Snyder and Barzilay, 2007; Titov and McDonald, 2008b; Baccianella et al., 2009). These approaches are all very effective, and the review databases generated are well presented. So the first thought for developing a recommendation dialogue system is to use such a categorized summary in a table-lookup fashion. For example, a dialogue system for restaurant recommendations can look up a summary table as exemplified in Table 1, and generate a response utterance from each row: “Restaurant A has good service and bad food; restaurant B has good service and good food; restaurant C has great service and nice atmosphere; restaurant D has poor service and reasonable price.” Restaura</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2009</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2009. Multi-facet Rating of Product Reviews. In Proceedings of European Conference on Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Learning document-level semantic properties from free-text annotations.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3428" citStr="Branavan et al., 2008" startWordPosition="513" endWordPosition="516">lective opinions. If there exists a systematic framework that harvests these reviews from general users, extracts the essence from the reviews and presents it appropriately in human-computer conversations, then we can enable dialogue systems to behave like a human shopping assistant, a travel agent, or a local friend who tells you where to find the best restaurant. Summarization from online reviews, therefore, plays an important role for such dialogue systems. There have been previous studies on review analysis for text-based summarization systems (Mei et al., 2007; Titov and McDonald, 2008a; Branavan et al., 2008). Mixture models and topic models are used to predict the underlying topics of each document and generate a phrase-level summary. An aspect rating on each facet is also automatically 64 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 64–72, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics learned with statistical models (Snyder and Barzilay, 2007; Titov and McDonald, 2008b; Baccianella et al., 2009). These approaches are all very effective, and the review databases generated are well presented. So the f</context>
</contexts>
<marker>Branavan, Chen, Eisenstein, Barzilay, 2008</marker>
<rawString>S.R.K. Branavan, Harr Chen, Jacob Eisenstein, and Regina Barzilay. 2008. Learning document-level semantic properties from free-text annotations. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
<author>Adam Pauls</author>
</authors>
<title>Multi-Document Summarization of Evaluative Text.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<marker>Carenini, Ng, Pauls, 2006</marker>
<rawString>Giuseppe Carenini, Raymond Ng, and Adam Pauls. 2006. Multi-Document Summarization of Evaluative Text. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on World Wide Web.</booktitle>
<contexts>
<context position="29882" citStr="Dave et al., 2003" startWordPosition="4666" endWordPosition="4669">ue features. Also, as the phrase classification task is very subjective, it is very similar to a ‘hierarchical if-else decision problem’ in human cognition, where decision tree algorithms can fit well. Figure 6 shows a partial simplified decision tree learned from our model, which can give an intuitive idea of the decision tree models. 6 Related Work Sentiment classification and opinion mining have been well studied for years. Most studies have focused on text-based systems, such as documentlevel sentiment classification and sentence-level opinion aggregation (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005; Wilson et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). There was a study conducted by Carenini et al. in 2006, which proposed a combination of a sentence extraction-based approach and a language generation-based approach for summarizing evaluative arguments. In our work, we utilize a lowerlevel phrase-based extraction approach, which utilizes high level linguistic features and syntactic structure to capture phrase patterns. There was also a study on using reviews to generate a dictionary of mappings between semantic representations and rea</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: opinion extraction and semantic classification of product reviews. In Proceedings of the International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Eckert</author>
<author>T Kuhn</author>
<author>H Niemann</author>
<author>S Rieck</author>
<author>A Scheuer</author>
<author>E G Schukat-talamazzini</author>
</authors>
<title>A Spoken Dialogue System for German Intercity Train Timetable Inquiries. In</title>
<date>1993</date>
<booktitle>Proc. European Conf. on Speech Technology.</booktitle>
<contexts>
<context position="1911" citStr="Eckert et al., 1993" startWordPosition="282" endWordPosition="285"> an improvement of 36% over a heuristic rule baseline. Experiments also show that the decision-treebased phrase selection model can achieve rather reliable predictions on the phrase label, comparable to human judgment. The proposed statistical approach is based on domain-independent learning features and can be extended to other domains effectively. 1 Introduction Spoken dialogue systems are presently available for many purposes, such as weather inquiry (Zue et al., 2000), bus schedules and route guidance (Raux et al., 2003), customer service (Gorin et al., 1997), and train timetable inquiry (Eckert et al., 1993). These systems have been well developed for laboratory research, and some have become commercially viable. The next generation of intelligent dialogue systems is expected to go beyond factoid question answering and straightforward task fulfillment, by providing active assistance and subjective recommendations, thus behaving more like human agents. For example, an intelligent dialogue system may suggest which airline is a better choice, considering cost, flight duration, take-off time, available seats, etc.; or suggest which digital camera is the most popular among teenagers or highest rated b</context>
</contexts>
<marker>Eckert, Kuhn, Niemann, Rieck, Scheuer, Schukat-talamazzini, 1993</marker>
<rawString>W. Eckert, T. Kuhn, H. Niemann, S. Rieck, A. Scheuer, and E. G. Schukat-talamazzini. 1993. A Spoken Dialogue System for German Intercity Train Timetable Inquiries. In Proc. European Conf. on Speech Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Gruenstein</author>
<author>Stephanie Seneff</author>
</authors>
<title>Releasing a Multimodal Dialogue System into the Wild: User Support Mechanisms.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>111--119</pages>
<location>Antwerp,</location>
<contexts>
<context position="21852" citStr="Gruenstein and Seneff, 2007" startWordPosition="3391" endWordPosition="3394">e 4. { restaurant &amp;quot;dali restaurant and tapas bar&amp;quot; :atmosphere ( &amp;quot;wonderful evening&amp;quot;, &amp;quot;cozy atmosphere&amp;quot;, &amp;quot;fun decor&amp;quot;, &amp;quot;romantic date&amp;quot; ) :atmosphere_rating &amp;quot;4.1&amp;quot; :food ( &amp;quot;very fresh ingredients&amp;quot;, &amp;quot;tasty fish&amp;quot;, &amp;quot;creative dishes&amp;quot;, &amp;quot;good sangria&amp;quot; ) :food_rating &amp;quot;3.9&amp;quot; :service ( &amp;quot;fast service&amp;quot; ) :service_rating &amp;quot;3.9&amp;quot; :general (&amp;quot;romantic restaurant&amp;quot;,&amp;quot;small space&amp;quot; ) :general_rating &amp;quot;3.6&amp;quot; } Figure 4. Example of a review summary database entry generated by the proposed approaches. 4 Experiments In this project, we substantiate the proposed approach in a restaurant domain for our spoken dialogue system (Gruenstein and Seneff, 2007), which is a web-based multimodal dialogue system allowing users to inquire about information about restaurants, museums, subways, etc. We harvested a data collection of 137,569 reviews on 24,043 restaurants in 9 cities in the U.S. from an online restaurant evaluation website1. From the dataset, 857,466 sentences were subjected to parse analysis; and a total of 434,372 phrases (114,369 unique ones) were extracted from the parsable subset (78.6%) of the sentences. Most pros/cons consist of well-formatted phrases; thus, we select 3,000 phrases extracted from pros/cons as training data. To genera</context>
</contexts>
<marker>Gruenstein, Seneff, 2007</marker>
<rawString>Alexander Gruenstein and Stephanie Seneff. 2007. Releasing a Multimodal Dialogue System into the Wild: User Support Mechanisms. In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue, Antwerp, pages 111-119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Gorin</author>
<author>G Riccardi</author>
<author>J H Wright</author>
</authors>
<date>1997</date>
<journal>How may I help you?” Speech Communication,</journal>
<volume>23</volume>
<pages>113--127</pages>
<contexts>
<context position="1860" citStr="Gorin et al., 1997" startWordPosition="274" endWordPosition="277">n outperformance of 13% compared to SVM models and an improvement of 36% over a heuristic rule baseline. Experiments also show that the decision-treebased phrase selection model can achieve rather reliable predictions on the phrase label, comparable to human judgment. The proposed statistical approach is based on domain-independent learning features and can be extended to other domains effectively. 1 Introduction Spoken dialogue systems are presently available for many purposes, such as weather inquiry (Zue et al., 2000), bus schedules and route guidance (Raux et al., 2003), customer service (Gorin et al., 1997), and train timetable inquiry (Eckert et al., 1993). These systems have been well developed for laboratory research, and some have become commercially viable. The next generation of intelligent dialogue systems is expected to go beyond factoid question answering and straightforward task fulfillment, by providing active assistance and subjective recommendations, thus behaving more like human agents. For example, an intelligent dialogue system may suggest which airline is a better choice, considering cost, flight duration, take-off time, available seats, etc.; or suggest which digital camera is </context>
</contexts>
<marker>Gorin, Riccardi, Wright, 1997</marker>
<rawString>A. L. Gorin, G. Riccardi and J. H. Wright. 1997. “How may I help you?” Speech Communication, vol. 23, pp. 113–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichiro Higashinaka</author>
<author>Rashmi Prasad</author>
<author>Marilyn Walker</author>
</authors>
<title>Learning to Generate Naturalistic Utterances Using Reviews in Spoken Dialogue Systems.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="30550" citStr="Higashinaka et al., 2006" startWordPosition="4772" endWordPosition="4775">5; Wilson et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). There was a study conducted by Carenini et al. in 2006, which proposed a combination of a sentence extraction-based approach and a language generation-based approach for summarizing evaluative arguments. In our work, we utilize a lowerlevel phrase-based extraction approach, which utilizes high level linguistic features and syntactic structure to capture phrase patterns. There was also a study on using reviews to generate a dictionary of mappings between semantic representations and realizations of concepts for dialogue systems (Higashinaka et al., 2006; Higashinaka, 2007). They also used the association between user ratings and reviews to capture semantic-syntactic structure mappings. A set of fil7 Conclusions In this paper we proposed a three-level framework for review-based recommendation dialogue systems, including linguistic phrase extraction, dialogue-oriented review summary generation, and human-friendly dialogue generation. The contributions of this paper are three-fold: 1) it identified and defined the research goal of utilizing opinion summarization for real human-computer conversation; 2) it formulated an evaluation methodology fo</context>
</contexts>
<marker>Higashinaka, Prasad, Walker, 2006</marker>
<rawString>Ryuichiro Higashinaka, Rashmi Prasad and Marilyn Walker. 2006. Learning to Generate Naturalistic Utterances Using Reviews in Spoken Dialogue Systems. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichiro Higashinaka</author>
<author>Marilyn Walker</author>
<author>Rashmi Prasad</author>
</authors>
<title>An Unsupervised Method for Learning Generation Dictionaries for Spoken Dialogue Systems by Mining User Reviews.</title>
<date>2007</date>
<journal>Journal of ACM Transactions on Speech and Language Processing.</journal>
<marker>Higashinaka, Walker, Prasad, 2007</marker>
<rawString>Ryuichiro Higashinaka, Marilyn Walker and Rashmi Prasad. 2007. An Unsupervised Method for Learning Generation Dictionaries for Spoken Dialogue Systems by Mining User Reviews. Journal of ACM Transactions on Speech and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 ACM SIGKDD international conference on Knowledge Discovery and Data mining.</booktitle>
<contexts>
<context position="29900" citStr="Hu and Liu, 2004" startWordPosition="4670" endWordPosition="4673">as the phrase classification task is very subjective, it is very similar to a ‘hierarchical if-else decision problem’ in human cognition, where decision tree algorithms can fit well. Figure 6 shows a partial simplified decision tree learned from our model, which can give an intuitive idea of the decision tree models. 6 Related Work Sentiment classification and opinion mining have been well studied for years. Most studies have focused on text-based systems, such as documentlevel sentiment classification and sentence-level opinion aggregation (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005; Wilson et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). There was a study conducted by Carenini et al. in 2006, which proposed a combination of a sentence extraction-based approach and a language generation-based approach for summarizing evaluative arguments. In our work, we utilize a lowerlevel phrase-based extraction approach, which utilizes high level linguistic features and syntactic structure to capture phrase patterns. There was also a study on using reviews to generate a dictionary of mappings between semantic representations and realizations of conce</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the 2004 ACM SIGKDD international conference on Knowledge Discovery and Data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Kim</author>
<author>E H Hovy</author>
</authors>
<title>Identifying and Analyzing Judgment Opinions.</title>
<date>2006</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<contexts>
<context position="29990" citStr="Kim and Hovy, 2006" startWordPosition="4687" endWordPosition="4690">ical if-else decision problem’ in human cognition, where decision tree algorithms can fit well. Figure 6 shows a partial simplified decision tree learned from our model, which can give an intuitive idea of the decision tree models. 6 Related Work Sentiment classification and opinion mining have been well studied for years. Most studies have focused on text-based systems, such as documentlevel sentiment classification and sentence-level opinion aggregation (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005; Wilson et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). There was a study conducted by Carenini et al. in 2006, which proposed a combination of a sentence extraction-based approach and a language generation-based approach for summarizing evaluative arguments. In our work, we utilize a lowerlevel phrase-based extraction approach, which utilizes high level linguistic features and syntactic structure to capture phrase patterns. There was also a study on using reviews to generate a dictionary of mappings between semantic representations and realizations of concepts for dialogue systems (Higashinaka et al., 2006; Higashinaka, 2007). They also used the</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>S.M. Kim and E.H. Hovy. 2006. Identifying and Analyzing Judgment Opinions. In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingjing Liu</author>
<author>Stephanie Seneff</author>
</authors>
<title>Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm.</title>
<date>2009</date>
<booktitle>In proceedings of EMNLP.</booktitle>
<contexts>
<context position="8028" citStr="Liu and Seneff, 2009" startWordPosition="1239" endWordPosition="1242">re produced by our system and the actual dialogue was an illustration of system capacities). Therefore, the task of developing recommendation dialogue systems is decomposed into three problems: 1) how to extract context-related phrases, both coarse-grained and fine-grained, from online reviews; 2) how to select a representative set from the extracted phrases to create an informative yet concise dialogue-oriented summary database; 3) how to generate human-friendly dialogue responses from the review summary database. To tackle these problems, we propose a threelevel framework. In previous work (Liu and Seneff, 2009), we explored the first level by proposing a linguistic parse-and-paraphrase paradigm for re65 view phrase extraction. In this paper, we address the second problem: dialogue-oriented review summary generation. We propose an automatic approach to classifying high/low informative phrases using statistical models. Experiments conducted on a restaurant-domain dataset indicate that the proposed approach can predict phrase labels consistently with human judgment and can generate high-quality review summaries for dialogue purposes. The rest of the paper is organized as follows: Section 2 gives an ove</context>
<context position="14938" citStr="Liu and Seneff, 2009" startWordPosition="2293" endWordPosition="2296"> model. For model learning, we employ a feature set including statistical features, sentiment features and semantic features. Generally speaking, phrases with neutral sentiment are less informative than those with strong sentiment, either positive or negative. For example, ‘fried seafood appetizer’, ‘baked halibut’, ‘electronic bill’ and ‘red drink’ do not indicate whether a restaurant is worth trying, as they did not express whether the fried seafood appetizer or the baked halibut are good or bad. Therefore, we take the sentiment score of each phrase generated from a cumulative offset model (Liu and Seneff, 2009) as a sentiment feature. Sentiment scores of phrases are exemplified in Table 2 (on a scale of 1 to 5). Phrase Sc. Phrase Sc. really welcoming 4.8 truly amazing flavor 4.6 atmosphere perfect portions 4.4 very tasty meat 4.3 busy place 3.1 typical Italian restaurant 3.1 a little bit high 2.2 pretty bad soup 1.8 price sloppy service 1.8 absolute worst service 1.4 Table 2. Examples of sentiment scores of phrases. We also employ a set of statistical features for model training, such as the unigram probability of the adjective in a phrase, the unigram probability of the noun in a phrase, the unigra</context>
</contexts>
<marker>Liu, Seneff, 2009</marker>
<rawString>Jingjing Liu and Stephanie Seneff. 2009. Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm. In proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xu Ling</author>
<author>Matthew Wondra</author>
<author>Hang Su</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic Sentiment Mixture: Modeling Facets and Opinions in Weblogs.</title>
<date>2007</date>
<booktitle>In Proc. of WWW.</booktitle>
<contexts>
<context position="3377" citStr="Mei et al., 2007" startWordPosition="505" endWordPosition="508"> providing subjective recommendations and collective opinions. If there exists a systematic framework that harvests these reviews from general users, extracts the essence from the reviews and presents it appropriately in human-computer conversations, then we can enable dialogue systems to behave like a human shopping assistant, a travel agent, or a local friend who tells you where to find the best restaurant. Summarization from online reviews, therefore, plays an important role for such dialogue systems. There have been previous studies on review analysis for text-based summarization systems (Mei et al., 2007; Titov and McDonald, 2008a; Branavan et al., 2008). Mixture models and topic models are used to predict the underlying topics of each document and generate a phrase-level summary. An aspect rating on each facet is also automatically 64 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 64–72, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics learned with statistical models (Snyder and Barzilay, 2007; Titov and McDonald, 2008b; Baccianella et al., 2009). These approaches are all very effective, and the revi</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic Sentiment Mixture: Modeling Facets and Opinions in Weblogs. In Proc. of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="29863" citStr="Pang et al., 2002" startWordPosition="4662" endWordPosition="4665"> and continuous value features. Also, as the phrase classification task is very subjective, it is very similar to a ‘hierarchical if-else decision problem’ in human cognition, where decision tree algorithms can fit well. Figure 6 shows a partial simplified decision tree learned from our model, which can give an intuitive idea of the decision tree models. 6 Related Work Sentiment classification and opinion mining have been well studied for years. Most studies have focused on text-based systems, such as documentlevel sentiment classification and sentence-level opinion aggregation (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005; Wilson et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). There was a study conducted by Carenini et al. in 2006, which proposed a combination of a sentence extraction-based approach and a language generation-based approach for summarizing evaluative arguments. In our work, we utilize a lowerlevel phrase-based extraction approach, which utilizes high level linguistic features and syntactic structure to capture phrase patterns. There was also a study on using reviews to generate a dictionary of mappings between semantic repr</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Popescu</author>
<author>O Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="29927" citStr="Popescu and Etzioni, 2005" startWordPosition="4674" endWordPosition="4678">sification task is very subjective, it is very similar to a ‘hierarchical if-else decision problem’ in human cognition, where decision tree algorithms can fit well. Figure 6 shows a partial simplified decision tree learned from our model, which can give an intuitive idea of the decision tree models. 6 Related Work Sentiment classification and opinion mining have been well studied for years. Most studies have focused on text-based systems, such as documentlevel sentiment classification and sentence-level opinion aggregation (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005; Wilson et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). There was a study conducted by Carenini et al. in 2006, which proposed a combination of a sentence extraction-based approach and a language generation-based approach for summarizing evaluative arguments. In our work, we utilize a lowerlevel phrase-based extraction approach, which utilizes high level linguistic features and syntactic structure to capture phrase patterns. There was also a study on using reviews to generate a dictionary of mappings between semantic representations and realizations of concepts for dialogue systems (H</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>A.M. Popescu and O. Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JR Quinlan</author>
</authors>
<title>Induction of decision trees.</title>
<date>1986</date>
<booktitle>Machine learning,</booktitle>
<publisher>Springer-Netherlands.</publisher>
<contexts>
<context position="14289" citStr="Quinlan, 1986" startWordPosition="2195" endWordPosition="2196">inquiry from users, the answer from a recommendation system should be helpful and relevant. So the first task is to identify a phrase as ‘helpful’ or not. The task of identifying a phrase as informative and relevant, therefore, is defined as a classification problem: Y = 9� -x = Z 91�1 71=1 (1) where y is the label of a phrase, assigned as ‘1’ if the phrase is highly informative and relevant, and ‘-1’ if the phrase is uninformative. x is the feature vector extracted from the phrase, and 9 is the coefficient vector. We employ statistical models such as SVMs (Joachims, 1998) and decision trees (Quinlan, 1986) to train the classification model. For model learning, we employ a feature set including statistical features, sentiment features and semantic features. Generally speaking, phrases with neutral sentiment are less informative than those with strong sentiment, either positive or negative. For example, ‘fried seafood appetizer’, ‘baked halibut’, ‘electronic bill’ and ‘red drink’ do not indicate whether a restaurant is worth trying, as they did not express whether the fried seafood appetizer or the baked halibut are good or bad. Therefore, we take the sentiment score of each phrase generated from</context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>JR Quinlan, 1986. Induction of decision trees. Machine learning, Springer-Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>B Langner</author>
<author>A Black</author>
<author>M Eskenazi</author>
</authors>
<title>LET&apos;S GO: Improving Spoken Dialog Systems for the Elderly and Non-natives. In</title>
<date>2003</date>
<booktitle>Proc. Eurospeech.</booktitle>
<contexts>
<context position="1821" citStr="Raux et al., 2003" startWordPosition="268" endWordPosition="271">ng decision tree algorithms achieves an outperformance of 13% compared to SVM models and an improvement of 36% over a heuristic rule baseline. Experiments also show that the decision-treebased phrase selection model can achieve rather reliable predictions on the phrase label, comparable to human judgment. The proposed statistical approach is based on domain-independent learning features and can be extended to other domains effectively. 1 Introduction Spoken dialogue systems are presently available for many purposes, such as weather inquiry (Zue et al., 2000), bus schedules and route guidance (Raux et al., 2003), customer service (Gorin et al., 1997), and train timetable inquiry (Eckert et al., 1993). These systems have been well developed for laboratory research, and some have become commercially viable. The next generation of intelligent dialogue systems is expected to go beyond factoid question answering and straightforward task fulfillment, by providing active assistance and subjective recommendations, thus behaving more like human agents. For example, an intelligent dialogue system may suggest which airline is a better choice, considering cost, flight duration, take-off time, available seats, et</context>
</contexts>
<marker>Raux, Langner, Black, Eskenazi, 2003</marker>
<rawString>A. Raux, B. Langner, A. Black, and M. Eskenazi. 2003. LET&apos;S GO: Improving Spoken Dialog Systems for the Elderly and Non-natives. In Proc. Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Multiple Aspect Ranking using the Good Grief Algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="3868" citStr="Snyder and Barzilay, 2007" startWordPosition="579" endWordPosition="583"> for such dialogue systems. There have been previous studies on review analysis for text-based summarization systems (Mei et al., 2007; Titov and McDonald, 2008a; Branavan et al., 2008). Mixture models and topic models are used to predict the underlying topics of each document and generate a phrase-level summary. An aspect rating on each facet is also automatically 64 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 64–72, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics learned with statistical models (Snyder and Barzilay, 2007; Titov and McDonald, 2008b; Baccianella et al., 2009). These approaches are all very effective, and the review databases generated are well presented. So the first thought for developing a recommendation dialogue system is to use such a categorized summary in a table-lookup fashion. For example, a dialogue system for restaurant recommendations can look up a summary table as exemplified in Table 1, and generate a response utterance from each row: “Restaurant A has good service and bad food; restaurant B has good service and good food; restaurant C has great service and nice atmosphere; restaur</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2007. Multiple Aspect Ranking using the Good Grief Algorithm. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>Modeling Online Reviews with Multi-Grain Topic Models.</title>
<date>2008</date>
<booktitle>In Proc. of WWW.</booktitle>
<contexts>
<context position="3403" citStr="Titov and McDonald, 2008" startWordPosition="509" endWordPosition="512">ive recommendations and collective opinions. If there exists a systematic framework that harvests these reviews from general users, extracts the essence from the reviews and presents it appropriately in human-computer conversations, then we can enable dialogue systems to behave like a human shopping assistant, a travel agent, or a local friend who tells you where to find the best restaurant. Summarization from online reviews, therefore, plays an important role for such dialogue systems. There have been previous studies on review analysis for text-based summarization systems (Mei et al., 2007; Titov and McDonald, 2008a; Branavan et al., 2008). Mixture models and topic models are used to predict the underlying topics of each document and generate a phrase-level summary. An aspect rating on each facet is also automatically 64 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 64–72, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics learned with statistical models (Snyder and Barzilay, 2007; Titov and McDonald, 2008b; Baccianella et al., 2009). These approaches are all very effective, and the review databases generated are</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008a. Modeling Online Reviews with Multi-Grain Topic Models. In Proc. of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>A Joint Model of Text and Aspect Ratings for Sentiment Summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3403" citStr="Titov and McDonald, 2008" startWordPosition="509" endWordPosition="512">ive recommendations and collective opinions. If there exists a systematic framework that harvests these reviews from general users, extracts the essence from the reviews and presents it appropriately in human-computer conversations, then we can enable dialogue systems to behave like a human shopping assistant, a travel agent, or a local friend who tells you where to find the best restaurant. Summarization from online reviews, therefore, plays an important role for such dialogue systems. There have been previous studies on review analysis for text-based summarization systems (Mei et al., 2007; Titov and McDonald, 2008a; Branavan et al., 2008). Mixture models and topic models are used to predict the underlying topics of each document and generate a phrase-level summary. An aspect rating on each facet is also automatically 64 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 64–72, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics learned with statistical models (Snyder and Barzilay, 2007; Titov and McDonald, 2008b; Baccianella et al., 2009). These approaches are all very effective, and the review databases generated are</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008b. A Joint Model of Text and Aspect Ratings for Sentiment Summarization. In Proceedings of the Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? Sentiment orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="29844" citStr="Turney, 2002" startWordPosition="4660" endWordPosition="4661">ated ontology) and continuous value features. Also, as the phrase classification task is very subjective, it is very similar to a ‘hierarchical if-else decision problem’ in human cognition, where decision tree algorithms can fit well. Figure 6 shows a partial simplified decision tree learned from our model, which can give an intuitive idea of the decision tree models. 6 Related Work Sentiment classification and opinion mining have been well studied for years. Most studies have focused on text-based systems, such as documentlevel sentiment classification and sentence-level opinion aggregation (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005; Wilson et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). There was a study conducted by Carenini et al. in 2006, which proposed a combination of a sentence extraction-based approach and a language generation-based approach for summarizing evaluative arguments. In our work, we utilize a lowerlevel phrase-based extraction approach, which utilizes high level linguistic features and syntactic structure to capture phrase patterns. There was also a study on using reviews to generate a dictionary of mappings be</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? Sentiment orientation applied to unsupervised classification of reviews. In Proceedings of the Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proc. of ECML,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="14254" citStr="Joachims, 1998" startWordPosition="2190" endWordPosition="2191"> Review Summary Generation Given an inquiry from users, the answer from a recommendation system should be helpful and relevant. So the first task is to identify a phrase as ‘helpful’ or not. The task of identifying a phrase as informative and relevant, therefore, is defined as a classification problem: Y = 9� -x = Z 91�1 71=1 (1) where y is the label of a phrase, assigned as ‘1’ if the phrase is highly informative and relevant, and ‘-1’ if the phrase is uninformative. x is the feature vector extracted from the phrase, and 9 is the coefficient vector. We employ statistical models such as SVMs (Joachims, 1998) and decision trees (Quinlan, 1986) to train the classification model. For model learning, we employ a feature set including statistical features, sentiment features and semantic features. Generally speaking, phrases with neutral sentiment are less informative than those with strong sentiment, either positive or negative. For example, ‘fried seafood appetizer’, ‘baked halibut’, ‘electronic bill’ and ‘red drink’ do not indicate whether a restaurant is worth trying, as they did not express whether the fried seafood appetizer or the baked halibut are good or bad. Therefore, we take the sentiment </context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proc. of ECML, p. 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP.</booktitle>
<contexts>
<context position="29948" citStr="Wilson et al., 2005" startWordPosition="4679" endWordPosition="4682">jective, it is very similar to a ‘hierarchical if-else decision problem’ in human cognition, where decision tree algorithms can fit well. Figure 6 shows a partial simplified decision tree learned from our model, which can give an intuitive idea of the decision tree models. 6 Related Work Sentiment classification and opinion mining have been well studied for years. Most studies have focused on text-based systems, such as documentlevel sentiment classification and sentence-level opinion aggregation (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005; Wilson et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). There was a study conducted by Carenini et al. in 2006, which proposed a combination of a sentence extraction-based approach and a language generation-based approach for summarizing evaluative arguments. In our work, we utilize a lowerlevel phrase-based extraction approach, which utilizes high level linguistic features and syntactic structure to capture phrase patterns. There was also a study on using reviews to generate a dictionary of mappings between semantic representations and realizations of concepts for dialogue systems (Higashinaka et al., 20</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. In Proc. of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Zue</author>
<author>Stephanie Seneff</author>
<author>James Glass</author>
<author>Joseph Polifroni</author>
<author>Christine Pao</author>
<author>Timothy J Hazen</author>
<author>Lee Hetherington</author>
</authors>
<title>JUPITER: A Telephone-Based Conversational Interface for Weather Information.</title>
<date>2000</date>
<booktitle>In IEEE Transactions on Speech and Audio Processing,</booktitle>
<volume>8</volume>
<contexts>
<context position="1767" citStr="Zue et al., 2000" startWordPosition="259" endWordPosition="262">restaurant domain show that the proposed approach using decision tree algorithms achieves an outperformance of 13% compared to SVM models and an improvement of 36% over a heuristic rule baseline. Experiments also show that the decision-treebased phrase selection model can achieve rather reliable predictions on the phrase label, comparable to human judgment. The proposed statistical approach is based on domain-independent learning features and can be extended to other domains effectively. 1 Introduction Spoken dialogue systems are presently available for many purposes, such as weather inquiry (Zue et al., 2000), bus schedules and route guidance (Raux et al., 2003), customer service (Gorin et al., 1997), and train timetable inquiry (Eckert et al., 1993). These systems have been well developed for laboratory research, and some have become commercially viable. The next generation of intelligent dialogue systems is expected to go beyond factoid question answering and straightforward task fulfillment, by providing active assistance and subjective recommendations, thus behaving more like human agents. For example, an intelligent dialogue system may suggest which airline is a better choice, considering cos</context>
</contexts>
<marker>Zue, Seneff, Glass, Polifroni, Pao, Hazen, Hetherington, 2000</marker>
<rawString>Victor Zue, Stephanie Seneff, James Glass, Joseph Polifroni, Christine Pao, Timothy J. Hazen, and Lee Hetherington. 2000. JUPITER: A Telephone-Based Conversational Interface for Weather Information. In IEEE Transactions on Speech and Audio Processing, Vol. 8 , No. 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Zhuang</author>
<author>Feng Jing</author>
<author>Xiao-Yan Zhu</author>
</authors>
<title>Movie review mining and summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th ACM international conference on Information and knowledge management.</booktitle>
<contexts>
<context position="29969" citStr="Zhuang et al., 2006" startWordPosition="4683" endWordPosition="4686">imilar to a ‘hierarchical if-else decision problem’ in human cognition, where decision tree algorithms can fit well. Figure 6 shows a partial simplified decision tree learned from our model, which can give an intuitive idea of the decision tree models. 6 Related Work Sentiment classification and opinion mining have been well studied for years. Most studies have focused on text-based systems, such as documentlevel sentiment classification and sentence-level opinion aggregation (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005; Wilson et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). There was a study conducted by Carenini et al. in 2006, which proposed a combination of a sentence extraction-based approach and a language generation-based approach for summarizing evaluative arguments. In our work, we utilize a lowerlevel phrase-based extraction approach, which utilizes high level linguistic features and syntactic structure to capture phrase patterns. There was also a study on using reviews to generate a dictionary of mappings between semantic representations and realizations of concepts for dialogue systems (Higashinaka et al., 2006; Higashinaka, 2007</context>
</contexts>
<marker>Zhuang, Jing, Zhu, 2006</marker>
<rawString>Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie review mining and summarization. In Proceedings of the 15th ACM international conference on Information and knowledge management.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>