<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007032">
<title confidence="0.998692">
Correcting Automatic Translations through Collaborations between MT
and Monolingual Target-Language Users
</title>
<author confidence="0.981034">
Joshua S. Albrecht and Rebecca Hwa and G. Elisabeta Marai
</author>
<affiliation confidence="0.9983405">
Department of Computer Science
University of Pittsburgh
</affiliation>
<email confidence="0.999214">
{jsa8,hwa,marai}@cs.pitt.edu
</email>
<sectionHeader confidence="0.997401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999341857142857">
Machine translation (MT) systems have
improved significantly; however, their out-
puts often contain too many errors to com-
municate the intended meaning to their
users. This paper describes a collabora-
tive approach for mediating between an
MT system and users who do not under-
stand the source language and thus cannot
easily detect translation mistakes on their
own. Through a visualization of multi-
ple linguistic resources, this approach en-
ables the users to correct difficult transla-
tion errors and understand translated pas-
sages that were otherwise baffling.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999918676470588">
Recent advances in machine translation (MT) have
given us some very good translation systems.
They can automatically translate between many
languages for a variety of texts; and they are
widely accessible to the public via the web. The
quality of the MT outputs, however, is not reliably
high. People who do not understand the source
language may be especially baffled by the MT out-
puts because they have little means to recover from
translation mistakes.
The goal of this work is to help monolingual
target-language users to obtain better translations
by enabling them to identify and overcome er-
rors produced by the MT system. We argue for a
human-computer collaborative approach because
both the users and the MT system have gaps in
their abilities that the other could compensate. To
facilitate this collaboration, we propose an inter-
face that mediates between the user and the MT
system. It manages additional NLP tools for the
source language and translation resources so that
the user can explore this extra information to gain
enough understanding of the source text to correct
MT errors. The interactions between the users and
the MT system may, in turn, offer researchers in-
sights into the translation process and inspirations
for better translation models.
We have conducted an experiment in which we
asked non-Chinese speakers to correct the outputs
of a Chinese-English MT system for several short
passages of different genres. They performed the
correction task both with the help of the visual-
ization interface and without. Our experiment ad-
dresses the following questions:
</bodyText>
<listItem confidence="0.9934986">
• To what extent can the visual interface help
the user to understand the source text?
• In what way do factors such as the user’s
backgrounds, the properties of source text,
and the quality of the MT system and other
NLP resources impact that understanding?
• What resources or strategies are more help-
ful to the users? What research directions
do these observations suggest in terms of im-
proving the translation models?
</listItem>
<bodyText confidence="0.999547">
Through qualitative and quantitative analysis of
the user actions and timing statistics, we have
found that users of the interface achieved a more
accurate understanding of the source texts and
corrected more difficult translation mistakes than
those who were given the MT outputs alone. Fur-
thermore, we observed that some users made bet-
ter use of the interface for certain genres, such
as sports news, suggesting that the translation
model may be improved by a better integration of
document-level contexts.
</bodyText>
<note confidence="0.9229915">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 60–68,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.99762">
60
</page>
<sectionHeader confidence="0.993674" genericHeader="introduction">
2 Collaborative Translation
</sectionHeader>
<bodyText confidence="0.99995085">
The idea of leveraging human-computer collab-
orations to improve MT is not new; computer-
aided translation, for instance, was proposed by
Kay (1980). The focus of these efforts has been on
improving the performance of professional trans-
lators. In contrast, our intended users cannot read
the source text.
These users do, however, have the world knowl-
edge and the language model to put together co-
herent sentences in the target-language. From the
MT research perspective, this raises an interesting
question: given that they are missing a transla-
tion model, what would it take to make these users
into effective “decoders?” While some transla-
tion mistakes are recoverable from a strong lan-
guage model alone, and some might become read-
ily apparent if one can choose from some possi-
ble phrasal translations; the most difficult mistakes
may require greater contextual knowledge about
the source. Consider the range of translation re-
sources available to an MT decoder–which ones
might the users find informative, handicapped as
they are for not knowing the source language?
Studying the users’ interactions with these re-
sources may provide insights into how we might
build a better translation model and a better de-
coder.
In exploring the collaborative approach, the de-
sign considerations for facilitating human com-
puter interaction are crucial. We chose to make
available relatively few resources to prevent the
users from becoming overwhelmed by the options.
We also need to determine how to present the in-
formation from the resources so that the users can
easily interpret them. This is a challenge because
the Chinese processing tools and the translation
resources are imperfect themselves. The informa-
tion should be displayed in such a way that con-
flicting analyses between different resources are
highlighted.
</bodyText>
<sectionHeader confidence="0.993225" genericHeader="method">
3 Prototype Design
</sectionHeader>
<bodyText confidence="0.931123333333333">
We present an overview of our prototype for a col-
laborative translation interface, named The Chi-
nese Room1. A screen-shot is shown in Figure 1. It
</bodyText>
<footnote confidence="0.999081">
1The inspiration for the name of our system came from
Searle’s thought experiment(Searle, 1980). We realize that
there are major differences between our system and Searle’s
description. Importantly, our users get to insert their knowl-
edge rather than purely operate based on instructions. We felt
</footnote>
<figureCaption confidence="0.999393">
Figure 1: A screen-shot of the visual interface. It
</figureCaption>
<bodyText confidence="0.984853666666667">
consists of two main regions. The left pane is a
workspace for users to explore the sentence; the
right pane provides multiple tabs that offer addi-
tional functionalities.
is a graphical environment that supports five main
sources of information and functionalities. The
space separates into two regions. On the left pane
is a large workspace for the user to explore the
source text one sentence at a time. On the right
pane are tabbed panels that provide the users with
access to a document view of the MT outputs as
well as additional functionalities for interpreting
the source. In our prototype, the MT output is ob-
tained by querying Google’s Translation API2. In
the interest of exploiting user interactions as a di-
agnostic tool for improving MT, we chose infor-
mation sources that are commonly used by mod-
ern MT systems.
First, we display the word alignments between
MT output and segmented Chinese3. Even with-
out knowing the Chinese characters, the users
can visually detect potential misalignments and
poor word reordering. For instance, the automatic
translation shown in Figure 1 begins: Two years
ago this month... It is fluent but incorrect. The
crossed alignments offer users a clue that “two”
and “months” should not have been split up. Users
can also explore alternative orderings by dragging
the English tokens around.
Second, we make available the glosses for
words and characters from a bilingual dictionary4.
the name was nonetheless evocative in that the user requires
additional resources to process the input “squiggles.”
</bodyText>
<footnote confidence="0.9995428">
2http://code.google.com/apis/translate/
research
3The Chinese segmentation is obtained as a by-product of
Google’s translation process.
4We used the Chinese-English Translation Lexi-
</footnote>
<page confidence="0.999444">
61
</page>
<bodyText confidence="0.999978435897436">
The placement of the word gloss presents a chal-
lenge because there are often alternative Chi-
nese segmentations. We place glosses for multi-
character words in the column closer to the source.
When the user mouses over each definition, the
corresponding characters are highlighted, helping
the user to notice potential mis-segmentation in
the Chinese.
Third, the Chinese sentence is annotated with
its parse structure5. Constituents are displayed
as brackets around the source sentence. They
have been color-coded into four major types (noun
phrase, verb phrases, prepositional phrases, and
other). Users can collapse and expand the brack-
ets to keep the workspace uncluttered as they work
through the Chinese sentence. This also indicates
to us which fragments held the user’s focus.
Fourth, based on previous studies reporting
that automatic translations may improve when
given decomposed source inputs (Mellebeek et al.,
2005), we allow the users to select a substring
from the source text for the MT system to trans-
late. We display the N-best alternatives in the
Translation Tab. The list is kept short; its purpose
is less for reranking but more to give the users a
sense of the kinds of hypotheses that the MT sys-
tem is considering.
Fifth, users can select a substring from the
source text and search for source sentences from
a bilingual corpus and a monolingual corpus that
contain phrases similar to the query6. The re-
trieved sentences are displayed in the Example
Tab. For sentences from the bilingual corpus, hu-
man translations for the queried phrase are high-
lighted. For sentences retrieved from the monolin-
gual corpus, their automatic translations are pro-
vided. If the users wished to examine any of the
retrieved translation pairs in detail, they can push
it onto the sentence workspace.
</bodyText>
<sectionHeader confidence="0.997721" genericHeader="method">
4 Experimental Methodology
</sectionHeader>
<bodyText confidence="0.998514666666667">
We asked eight non-Chinese speakers to correct
the machine translations of four short Chinese pas-
con released by the LDC; for a handful of char-
acters that serve as function words, we added the
functional definitions using an online dictionary
http://www.mandarintools.com/worddict.html.
</bodyText>
<footnote confidence="0.990921166666667">
5It is automatically generated by the Stanford Parser for
Chinese (Klein and Manning, 2003).
6We used Lemur (2006) for the information retrieval
back-end; the parallel corpus is from the Federal Broadcast
Information Service corpus; the monolingual corpus is from
the Chinese Gigaword corpus.
</footnote>
<figureCaption confidence="0.78233475">
Figure 2: The interface for users who are correct-
ing translations without help; they have access to
the document view, but they do not have access to
any of the other resources.
</figureCaption>
<bodyText confidence="0.99998203030303">
sages, with an average length of 11.5 sentences.
Two passages are news articles and two are ex-
cerpts of a fictional work. Each participant was
instructed to correct the translations for one news
article and one fictional passage using all the re-
sources made available by The Chinese Room and
the other two passages without. To keep the ex-
perimental conditions as similar as possible, we
provided them with a restricted version of the in-
terface (see Figure 2 for a screen-shot) in which all
additional functionalities except for the Document
View Tab are disabled. We assigned each person
to alternate between working with the full and the
restricted versions of the system; half began with-
out, and the others began with. Thus, every pas-
sage received four sets of corrections made collab-
oratively with the system and four sets of correc-
tions made based solely on the participants’ inter-
nal language models. All together, there are 184
participant corrected sentences (11.5 sentences x
4 passages x 4 participants) for each condition.
The participants were asked to complete each
passage in one sitting. Within a passage, they
could work on the sentences in any arbitrary order.
They could also elect to “pass” any part of a sen-
tence if they found it too difficult to correct. Tim-
ing statistics were automatically collected while
they made their corrections. We interviewed each
participant for qualitative feedbacks after all four
passages were corrected.
Next, we asked two bilingual speakers to eval-
uate all the corrected translations. The outcomes
between different groups of users are compared,
</bodyText>
<page confidence="0.995698">
62
</page>
<bodyText confidence="0.999976297872341">
and the significance of the difference is deter-
mined using the two-sample t-test assuming un-
equal variances. We require 90% confidence (al-
pha=0.1) as the cut-off for a difference to be con-
sidered statistically significant; when the differ-
ence can be established with higher confidence,
we report that value. In the following subsections,
we describe the conditions of this study in more
details.
Participants’ Background For this study, we
strove to maintain a relatively heterogeneous pop-
ulation; participants were selected to be varied in
their exposures to NLP, experiences with foreign
languages, as well as their age and gender. A sum-
mary of their backgrounds is shown in Table 1.
Prior to the start of the study, the participants
received a 20 minute long presentational tutorial
about the basic functionalities supported by our
system, but they did not have an opportunity to ex-
plore the system on their own. This helps us to de-
termine whether our interface is intuitive enough
for new users to pick up quickly.
Data The four passages used for this study were
chosen to span a range of difficulties and genre
types. The easiest of the four is a news arti-
cle about a new Tamagotchi-like product from
Bandai. It was taken from a webpage that offers
bilingual news to help Chinese students to learn
English. A harder news article is taken from a
past NIST Chinese-English MT Evaluation; it is
about Michael Jordan’s knee injury. For a dif-
ferent genre, we considered two fictional excerpts
from the first chapter of Martin Eden, a novel by
Jack London that has been professionally trans-
lated into Chinese7. One excerpt featured a short
dialog, while the other one was purely descriptive.
Evaluation of Translations Bilingual human
judges are presented with the source text as well as
the parallel English text for reference. Each judge
is then shown a set of candidate translations (the
original MT output, an alternative translation by
a bilingual speaker, and corrected translations by
the participants) in a randomized order. Since the
human corrected translations are likely to be flu-
ent, we have instructed the judges to concentrate
more on the adequacy of the meaning conveyed.
They are asked to rate each sentence on an abso-
</bodyText>
<footnote confidence="0.832912">
7We chose an American story so as to not rely on a
user’s knowledge about Chinese culture. The participants
confirmed that they were not familiar with the chosen story.
</footnote>
<tableCaption confidence="0.769042">
Table 2: The guideline used by bilingual judges
</tableCaption>
<bodyText confidence="0.7781545">
for evaluating the translation quality of the MT
outputs and the participants’ corrections.
</bodyText>
<note confidence="0.696337714285714">
9-10 The meaning of the Chinese sentence
is fully conveyed in the translation.
7-8 Most of the meaning is conveyed.
5-6 Misunderstands the sentence in a
major way; or has many small mistakes.
3-4 Very little meaning is conveyed.
1-2 The translation makes no sense at all.
</note>
<bodyText confidence="0.99985525">
lute scale of 1-10 using the guideline in Table 2.
To reduce the biases in the rating scales of differ-
ent judges, we normalized the judges’ scores, fol-
lowing standard practices in MT evaluation (Blatz
et al., 2003). Post normalization, the correlation
coefficient between the judges is 0.64. The final
assessment score for each translated sentence is
the average of judges’ scores, on a scale of 0-1.
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999989931034483">
The results of human evaluations for the user ex-
periment are summarized in Table 3, and the corre-
sponding timing statistics (average minutes spent
editing a sentence) is shown in Table 4. We ob-
served that typical MT outputs contain a range of
errors. Some are primarily problems in fluency
such that the participants who used the restricted
interface, which provided no additional resources
other than the Document View Tab, were still able
to improve the MT quality from 0.35 to 0.42. On
the other hand, there are also a number of more
serious errors that require the participants to gain
some level of understanding of the source in order
to correct them. The participants who had access
to the full collaborative interface were able to im-
prove the quality from 0.35 to 0.53, closing the
gap between the MT and the bilingual translations
by 36.9%. These differences are all statistically
significant (with &gt;98% confidence).
The higher quality of corrections does require
the participants to put in more time. Overall, the
participants took 2.5 times as long when they have
the interface than when they do not. This may be
partly because the participants have more sources
of information to explore and partly because the
participants tended to “pass” on fewer sentences.
The average Levenshtein edit distance (with words
as the atomic unit, and with the score normalized
to the interval [0,1]) between the original MT out-
</bodyText>
<page confidence="0.999468">
63
</page>
<tableCaption confidence="0.928859">
Table 1: A summary of participants’ background. $User5 recognizes some simple Kanji characters, but
does not have enough knowledge to gain any additional information beyond what the MT system and the
dictionary already provided.
</tableCaption>
<table confidence="0.999517">
User1 User2 User3 User4 User5$ User6 User7 User8
NLP background intro grad none none intro grad intro none
Native English yes no yes yes yes yes yes yes
Other Languages French multiple none none Japanese none none Greek
(beginner) (fluent) (beginner) (beginner)
Gender M F F M M M F M
Education Ugrad PhD PhD Ugrad Ugrad PhD Ugrad Ugrad
</table>
<bodyText confidence="0.999831272727273">
puts and the corrected sentences made by partic-
ipants using The Chinese Room is 0.59; in con-
trast, the edit distance is shorter, at 0.40, when par-
ticipants correct MT outputs directly. The timing
statistics are informative, but they reflect the inter-
actions of many factors (e.g., the difficulty of the
source text, the quality of the machine translation,
the background and motivation of the user). Thus,
in the next few subsections, we examine how these
factors correlate with the quality of the participant
corrections.
</bodyText>
<subsectionHeader confidence="0.991403">
5.1 Impact of Document Variation
</subsectionHeader>
<bodyText confidence="0.999935487804878">
Since the quality of MT varies depending on the
difficulty and genre of the source text, we inves-
tigate how these factors impact our participants’
performances. Columns 3-6 of Table 3 (and Ta-
ble 4) compare the corrected translations on a per-
document basis.
Of the four documents, the baseline MT sys-
tem performed the best on the product announce-
ment. Because the article is straight-forward, par-
ticipants found it relatively easy to guess the in-
tended translation. The major obstacle is in de-
tecting and translating Chinese transliteration of
Japanese names, which stumped everyone. The
quality difference between the two groups of par-
ticipants on this document was not statistically sig-
nificant. Relatedly, the difference in the amount of
time spent is the smallest for this document; par-
ticipants using The Chinese Room took about 1.5
times longer.
The other news article was much more difficult.
The baseline MT made many mistakes, and both
groups of participants spent longer on sentences
from this article than the others. Although sports
news is fairly formulaic, participants who only
read MT outputs were baffled, whereas those who
had access to additional resources were able to re-
cover from MT errors and produced good quality
translations.
Finally, as expected, the two fictional excerpts
were the most challenging. Since the participants
were not given any information about the story,
they also have little context to go on. In both cases,
participants who collaborated with The Chinese
Room made higher quality corrections than those
who did not. The difference is statistically signif-
icant at 97% confidence for the first excerpt, and
93% confidence for the second. The differences in
time spent between the two groups are greater for
these passages because the participants who had
to make corrections without help tended to give
up more often.
</bodyText>
<subsectionHeader confidence="0.999613">
5.2 Impact of Participants’ Background
</subsectionHeader>
<bodyText confidence="0.99994175">
We further analyze the results by separating the
participants into two groups according to four
factors: whether they were familiar with NLP,
whether they studied another language, their gen-
der, and their education level.
Exposure to NLP One of our design objectives
for The Chinese Room is accessibility by a diverse
population of end-users, many of whom may not
be familiar with human language technologies. To
determine how prior knowledge of NLP may im-
pact a user’s experience, we analyze the exper-
imental results with respect to the participants’
background. In columns 2 and 3 of Table 5, we
compare the quality of the corrections made by
the two groups. When making corrections on their
own, participants who had been exposed to NLP
held a significant edge (0.35 vs. 0.47). When both
groups of participants used The Chinese Room, the
difference is reduced (0.51 vs. 0.54) and is not sta-
tistically significant. Because all the participants
were given the same short tutorial prior to the start
of the study, we are optimistic that the interface is
intuitive for many users.
None of the other factors distinguished one
</bodyText>
<page confidence="0.999756">
64
</page>
<tableCaption confidence="0.988198">
Table 3: Averaged human judgments of the translation quality of the four different approaches: automatic
</tableCaption>
<bodyText confidence="0.856246">
MT, corrections by participants without help, corrections by participants using The Chinese Room, and
translation produced by a bilingual speaker. The second column reports score for all documents; columns
3-6 show the per-document scores.
</bodyText>
<table confidence="0.9846254">
Overall News (product) News (sports) Story1 Story2
Machine translation 0.35 0.45 0.30 0.25 0.26
Corrections without The Chinese Room 0.42 0.56 0.35 0.33 0.41
Corrections with The Chinese Room 0.53 0.55 0.62 0.42 0.49
Bilingual translation 0.83 0.83 0.73 0.92 0.88
</table>
<tableCaption confidence="0.825831285714286">
Table 4: The average amount of time (minutes) participants spent on correcting a sentence.
Overall News (product) News (sports) Story1 Story2
Corrections without The Chinese Room 2.5 1.9 3.2 2.9 2.3
Corrections with The Chinese Room 6.3 2.9 8.7 6.5 8.5
Table 6: The quality of the corrections produced
by four participants using The Chinese Room for
the sports news article.
</tableCaption>
<table confidence="0.9840486">
User1 0.57
User2 0.46
User5 0.70
User6 0.73
bilingual translator 0.73
</table>
<bodyText confidence="0.999194">
group of participants from the others. The results
are summarized in columns 4-9 of Table 5. In each
case, the two groups had similar levels of perfor-
mance, and the differences between their correc-
tions were not statistically significant. This trend
holds for both when they were collaborating with
the system and when editing on their own.
Prior Knowledge Another factor that may im-
pact the success of the outcome is the user’s
knowledge about the domain of the source text.
An example from our study is the sports news ar-
ticle. Table 6 lists the scores that the four partic-
ipants who used The Chinese Room received for
their corrected translations for that passage (aver-
aged over sentences). User5 and User6 were more
familiar with the basketball domain; with the help
of the system, they produced translations that were
comparable to those from the bilingual translator
(the differences are not statistically significant).
</bodyText>
<subsectionHeader confidence="0.999268">
5.3 Impact of Available Resources
</subsectionHeader>
<bodyText confidence="0.9035125">
Post-experiment, we asked the participants to de-
scribe the strategies they developed for collaborat-
ing with the system. Their responses fall into three
main categories:
</bodyText>
<figureCaption confidence="0.838047">
Figure 3: This graph shows the average counts of
access per sentence for different resources.
</figureCaption>
<bodyText confidence="0.979933333333333">
Divide and Conquer Some users found the syn-
tactic trees helpful in identifying phrasal units for
N-best re-translations or example searches. For
longer sentences, they used the constituent col-
lapse feature to help them reduce clutter and focus
on a portion of the sentence.
Example Retrieval Using the search interface,
users examined the highlighted query terms to de-
termine whether the MT system made any seg-
mentation errors. Sometimes, they used the exam-
ples to arbitrate whether they should trust any of
the dictionary glosses or the MT’s lexical choices.
Typically, though, they did not attempt to inspect
the example translations in detail.
Document Coherence and Word Glosses
Users often referred to the document view to
determine the context for the sentence they are
editing. Together with the word glosses and other
</bodyText>
<page confidence="0.999788">
65
</page>
<tableCaption confidence="0.995576">
Table 5: A comparison of translation quality, grouped by four characteristics of participant backgrounds:
their level of exposure to NLP, exposure to another language, their gender, and education level.
</tableCaption>
<table confidence="0.815849333333333">
No NLP NLP No 2nd Lang. 2nd Lang. Female Male Ugrad PhD
without The Chinese Room 0.35 0.47 0.41 0.43 0.41 0.43 0.41 0.45
with The Chinese Room 0.51 0.54 0.56 0.51 0.50 0.55 0.52 0.54
</table>
<bodyText confidence="0.998510928571429">
resources, the discourse level clues helped to
guide users to make better lexical choices than
when they made corrections without the full
system, relying on sentence coherence alone.
Figure 3 compares the average access counts
(per sentence) of different resources (aggregated
over all participants and documents). The option
of inspect retrieved examples in detail (i.e., bring
them up on the sentence workspace) was rarely
used. The inspiration for this feature was from
work on translation memory (Macklovitch et al.,
2000); however, it was not as informative for our
participants because they experienced a greater de-
gree of uncertainty than professional translators.
</bodyText>
<sectionHeader confidence="0.998827" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999988094339623">
The results suggest that collaborative translation
is a promising approach. Participant experiences
were generally positive. Because they felt like
they understood the translations better, they did
not mind putting in the time to collaborate with
the system. Table 7 shows some of the partici-
pants’ outputs. Although there are some transla-
tion errors that cannot be overcome with our cur-
rent system (e.g., transliterated names), the partic-
ipants taken as a collective performed surprisingly
well. For many mistakes, even when the users can-
not correct them, they recognized a problem; and
often, one or two managed to intuit the intended
meaning with the help of the available resources.
As an upper-bound for the effectiveness of the sys-
tem, we construct a combined “oracle” user out of
all 4 users that used the interface for each sentence.
The oracle user’s average score is 0.70; in contrast,
an oracle of users who did not use the system is
0.54 (cf. the MT’s overall of 0.35 and the bilin-
gual translator’s overall of 0.83). This suggests
The Chinese Room affords a potential for human-
human collaboration as well.
The experiment also made clear some limita-
tions of the current resources. One is domain de-
pendency. Because NLP technologies are typi-
cally trained on news corpora, their bias toward
the news domain may mislead our users. For ex-
ample, there is a Chinese character (pronounced
mei3) that could mean either “beautiful” or “the
United States.” In one of the passages, the in-
tended translation should have been: He was re-
sponsive to beauty... but the corresponding MT
output was He was sensitive to the United States...
Although many participants suspected that it was
wrong, they were unable to recover from this mis-
take because the resources (the searchable exam-
ples, the part-of-speech tags, and the MT system)
did not offer a viable alternative. This suggests
that collaborative translation may serve as a useful
diagnostic tool to help MT researchers verify ideas
about what types of models and data are useful in
translation. It may also provide a means of data
collection for MT training. To be sure, there are
important challenges to be addressed, such as par-
ticipation incentive and quality assurance, but sim-
ilar types of collaborative efforts have been shown
fruitful in other domains (Cosley et al., 2007). Fi-
nally, the statistics of user actions may be useful
for translation evaluation. They may be informa-
tive features for developing automatic metrics for
sentence-level evaluations (Kulesza and Shieber,
2004).
</bodyText>
<sectionHeader confidence="0.999977" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999948411764706">
While there have been many successful computer-
aided translation systems both for research and as
commercial products (Bowker, 2002; Langlais et
al., 2000), collaborative translation has not been
as widely explored. Previous efforts such as
DerivTool (DeNeefe et al., 2005) and Linear B
(Callison-Burch, 2005) placed stronger emphasis
on improving MT. They elicited more in-depth in-
teractions between the users and the MT system’s
phrase tables. These approaches may be more ap-
propriate for users who are MT researchers them-
selves. In contrast, our approach focuses on pro-
viding intuitive visualization of a variety of in-
formation sources for users who may not be MT-
savvy. By tracking the types of information they
consulted, the portions of translations they se-
lected to modify, and the portions of the source
</bodyText>
<page confidence="0.993354">
66
</page>
<tableCaption confidence="0.996501">
Table 7: Some examples of translations corrected by the participants and their scores.
</tableCaption>
<bodyText confidence="0.9160013">
Score Translation
MT 0.34 He is being discovered almost hit an arm in the pile of books on the desktop, just
like frightened horse as a Lieju Wangbangbian almost Pengfan the piano stool.
without The Chinese Room 0.26 Startled, he almost knocked over a pile of book on his desk, just like a frightened
horse as a Lieju Wangbangbian almost Pengfan the piano stool.
with The Chinese Room 0.78 He was nervous, and when one of his arms nearly hit a stack of books on the
desktop, he startled like a horse, falling back and almost knocking over the piano
stool.
Bilingual Translator 0.93 Feeling nervous, he discovered that one of his arms almost hit the pile of books
on the table. Like a frightened horse, he stumbled aside, almost turning over a
piano stool.
MT 0.50 Bandai Group, a spokeswoman for the U.S. to be SIN-West said: “We want to
bring women of all ages that ’the flavor of life’.”
without The Chinese Room 0.67 SIN-West, a spokeswoman for the U.S. Bandai Group declared: “We want to
bring to women of all ages that ’flavor of life’.”
with The Chinese Room 0.68 West, a spokeswoman for the U.S. Toy Manufacturing Group, and soon to be
Vice President-said: “We want to bring women of all ages that ’flavor of life’.”
Bilingual Translator 0.75 “We wanted to let women of all ages taste the ’flavor of life’,” said Bandai’s
spokeswoman Kasumi Nakanishi.
text they attempted to understand, we may alter
the design of our translation model. Our objective
is also related to that of cross-language informa-
tion retrieval (Resnik et al., 2001). This work can
be seen as providing the next step in helping users
to gain some understanding of the information in
the documents once they are retrieved.
By facilitating better collaborations between
MT and target-language readers, we can naturally
increase human annotated data for exploring al-
ternative MT models. This form of symbiosis is
akin to the paradigm proposed by von Ahn and
Dabbish (2004). They designed interactive games
in which the player generated data could be used
to improve image tagging and other classification
tasks (von Ahn, 2006). While our interface does
not have the entertainment value of a game, its
application serves a purpose. Because users are
motivated to understand the documents, they may
willingly spend time to collaborate and make de-
tailed corrections to MT outputs.
</bodyText>
<sectionHeader confidence="0.998583" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99999155">
We have presented a collaborative approach for
mediating between an MT system and monolin-
gual target-language users. The approach encour-
ages users to combine evidences from comple-
mentary information sources to infer alternative
hypotheses based on their world knowledge. Ex-
perimental evidences suggest that the collabora-
tive effort results in better translations than ei-
ther the original MT or uninformed human ed-
its. Moreover, users who are knowledgeable in the
document domain were enabled to correct transla-
tions with a quality approaching that of a bilin-
gual speaker. From the participants’ feedbacks,
we learned that the factors that contributed to their
understanding include: document coherence, syn-
tactic constraints, and re-translation at the phrasal
level. We believe that the collaborative translation
approach can provide insights about the transla-
tion process and help to gather training examples
for future MT development.
</bodyText>
<sectionHeader confidence="0.999205" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993791272727273">
This work has been supported by NSF Grants IIS-
0710695 and IIS-0745914. We would like to thank
Jarrett Billingsley, Ric Crabbe, Joanna Drum-
mund, Nick Farnan, Matt Kaniaris Brian Mad-
den, Karen Thickman, Julia Hockenmaier, Pauline
Hwa, and Dorothea Wei for their help with the ex-
periment. We are also grateful to Chris Callison-
Burch for discussions about collaborative trans-
lations and to Adam Lopez and the anonymous
reviewers for their comments and suggestions on
this paper.
</bodyText>
<page confidence="0.999287">
67
</page>
<sectionHeader confidence="0.998342" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999907884057971">
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence es-
timation for machine translation. Technical Report
Natural Language Engineering Workshop Final Re-
port, Johns Hopkins University.
Lynne Bowker. 2002. Computer-Aided Translation
Technology. University of Ottawa Press, Ottawa,
Canada.
Chris Callison-Burch. 2005. Linear B System descrip-
tion for the 2005 NIST MT Evaluation. In The Pro-
ceedings of Machine Translation Evaluation Work-
shop.
Dan Cosley, Dan Frankowski, Loren Terveen, and John
Riedl. 2007. Suggestbot: using intelligent task rout-
ing to help people find work in wikipedia. In IUI
’07: Proceedings of the 12th international confer-
ence on Intelligent user interfaces, pages 32–41.
Steve DeNeefe, Kevin Knight, and Hayward H. Chan.
2005. Interactively exploring a machine transla-
tion model. In Proceedings of the ACL Interactive
Poster and Demonstration Sessions, pages 97–100,
Ann Arbor, Michigan, June.
Martin Kay. 1980. The proper place of men and
machines in language translation. Technical Re-
port CSL-80-11, Xerox. Later reprinted in Machine
Translation, vol. 12 no.(1-2), 1997.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. Advances in Neural Information
Processing Systems, 15.
Alex Kulesza and Stuart M. Shieber. 2004. A learn-
ing approach to improving sentence-level MT evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI), Baltimore, MD, Octo-
ber.
Philippe Langlais, George Foster, and Guy Lapalme.
2000. Transtype: a computer-aided translation typ-
ing system. In Workshop on Embedded Machine
Translation Systems, pages 46–51, May.
Lemur. 2006. Lemur toolkit for language modeling
and information retrieval. The Lemur Project is a
collaborative project between CMU and UMASS.
Elliott Macklovitch, Michel Simard, and Philippe
Langlais. 2000. Transsearch: A free translation
memory on the world wide web. In Proceedings of
the Second International Conference on Language
Resources &amp; Evaluation (LREC).
Bart Mellebeek, Anna Khasin, Josef van Genabith, and
Andy Way. 2005. Transbooster: Boosting the per-
formance of wide-coverage machine translation sys-
tems. In Proceedings of the 10th Annual Conference
of the European Association for Machine Transla-
tion (EAMT), pages 189–197.
Philip S. Resnik, Douglas W. Oard, and Gina-Anne
Levow. 2001. Improved cross-language retrieval us-
ing backoff translation. In Human Language Tech-
nology Conference (HLT-2001), San Diego, CA,
March.
John R. Searle. 1980. Minds, brains, and programs.
Behavioral and Brain Sciences, 3:417–457.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In CHI ’04: Proceed-
ings of the SIGCHI conference on Human factors in
computing systems, pages 319–326, New York, NY,
USA. ACM.
Luis von Ahn. 2006. Games with a purpose. Com-
puter, 39(6):92–94.
</reference>
<page confidence="0.999446">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.980706">
<title confidence="0.996981">Correcting Automatic Translations through Collaborations between MT and Monolingual Target-Language Users</title>
<author confidence="0.999729">S Albrecht Hwa Elisabeta Marai</author>
<affiliation confidence="0.9998815">Department of Computer Science University of Pittsburgh</affiliation>
<abstract confidence="0.999134133333333">Machine translation (MT) systems have improved significantly; however, their outputs often contain too many errors to communicate the intended meaning to their users. This paper describes a collaborative approach for mediating between an MT system and users who do not understand the source language and thus cannot easily detect translation mistakes on their own. Through a visualization of multiple linguistic resources, this approach enables the users to correct difficult translation errors and understand translated passages that were otherwise baffling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2003</date>
<tech>Technical Report Natural Language Engineering Workshop Final Report,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="14821" citStr="Blatz et al., 2003" startWordPosition="2378" endWordPosition="2381">: The guideline used by bilingual judges for evaluating the translation quality of the MT outputs and the participants’ corrections. 9-10 The meaning of the Chinese sentence is fully conveyed in the translation. 7-8 Most of the meaning is conveyed. 5-6 Misunderstands the sentence in a major way; or has many small mistakes. 3-4 Very little meaning is conveyed. 1-2 The translation makes no sense at all. lute scale of 1-10 using the guideline in Table 2. To reduce the biases in the rating scales of different judges, we normalized the judges’ scores, following standard practices in MT evaluation (Blatz et al., 2003). Post normalization, the correlation coefficient between the judges is 0.64. The final assessment score for each translated sentence is the average of judges’ scores, on a scale of 0-1. 5 Results The results of human evaluations for the user experiment are summarized in Table 3, and the corresponding timing statistics (average minutes spent editing a sentence) is shown in Table 4. We observed that typical MT outputs contain a range of errors. Some are primarily problems in fluency such that the participants who used the restricted interface, which provided no additional resources other than t</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2003. Confidence estimation for machine translation. Technical Report Natural Language Engineering Workshop Final Report, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynne Bowker</author>
</authors>
<title>Computer-Aided Translation Technology.</title>
<date>2002</date>
<publisher>University of Ottawa Press,</publisher>
<location>Ottawa, Canada.</location>
<contexts>
<context position="27456" citStr="Bowker, 2002" startWordPosition="4426" endWordPosition="4427">of data collection for MT training. To be sure, there are important challenges to be addressed, such as participation incentive and quality assurance, but similar types of collaborative efforts have been shown fruitful in other domains (Cosley et al., 2007). Finally, the statistics of user actions may be useful for translation evaluation. They may be informative features for developing automatic metrics for sentence-level evaluations (Kulesza and Shieber, 2004). 7 Related Work While there have been many successful computeraided translation systems both for research and as commercial products (Bowker, 2002; Langlais et al., 2000), collaborative translation has not been as widely explored. Previous efforts such as DerivTool (DeNeefe et al., 2005) and Linear B (Callison-Burch, 2005) placed stronger emphasis on improving MT. They elicited more in-depth interactions between the users and the MT system’s phrase tables. These approaches may be more appropriate for users who are MT researchers themselves. In contrast, our approach focuses on providing intuitive visualization of a variety of information sources for users who may not be MTsavvy. By tracking the types of information they consulted, the p</context>
</contexts>
<marker>Bowker, 2002</marker>
<rawString>Lynne Bowker. 2002. Computer-Aided Translation Technology. University of Ottawa Press, Ottawa, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Linear B System description for the 2005 NIST MT Evaluation.</title>
<date>2005</date>
<booktitle>In The Proceedings of Machine Translation Evaluation Workshop.</booktitle>
<contexts>
<context position="27634" citStr="Callison-Burch, 2005" startWordPosition="4452" endWordPosition="4453">of collaborative efforts have been shown fruitful in other domains (Cosley et al., 2007). Finally, the statistics of user actions may be useful for translation evaluation. They may be informative features for developing automatic metrics for sentence-level evaluations (Kulesza and Shieber, 2004). 7 Related Work While there have been many successful computeraided translation systems both for research and as commercial products (Bowker, 2002; Langlais et al., 2000), collaborative translation has not been as widely explored. Previous efforts such as DerivTool (DeNeefe et al., 2005) and Linear B (Callison-Burch, 2005) placed stronger emphasis on improving MT. They elicited more in-depth interactions between the users and the MT system’s phrase tables. These approaches may be more appropriate for users who are MT researchers themselves. In contrast, our approach focuses on providing intuitive visualization of a variety of information sources for users who may not be MTsavvy. By tracking the types of information they consulted, the portions of translations they selected to modify, and the portions of the source 66 Table 7: Some examples of translations corrected by the participants and their scores. Score Tr</context>
</contexts>
<marker>Callison-Burch, 2005</marker>
<rawString>Chris Callison-Burch. 2005. Linear B System description for the 2005 NIST MT Evaluation. In The Proceedings of Machine Translation Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Cosley</author>
<author>Dan Frankowski</author>
<author>Loren Terveen</author>
<author>John Riedl</author>
</authors>
<title>Suggestbot: using intelligent task routing to help people find work in wikipedia.</title>
<date>2007</date>
<booktitle>In IUI ’07: Proceedings of the 12th international conference on Intelligent user interfaces,</booktitle>
<pages>32--41</pages>
<contexts>
<context position="27101" citStr="Cosley et al., 2007" startWordPosition="4372" endWordPosition="4375">ecover from this mistake because the resources (the searchable examples, the part-of-speech tags, and the MT system) did not offer a viable alternative. This suggests that collaborative translation may serve as a useful diagnostic tool to help MT researchers verify ideas about what types of models and data are useful in translation. It may also provide a means of data collection for MT training. To be sure, there are important challenges to be addressed, such as participation incentive and quality assurance, but similar types of collaborative efforts have been shown fruitful in other domains (Cosley et al., 2007). Finally, the statistics of user actions may be useful for translation evaluation. They may be informative features for developing automatic metrics for sentence-level evaluations (Kulesza and Shieber, 2004). 7 Related Work While there have been many successful computeraided translation systems both for research and as commercial products (Bowker, 2002; Langlais et al., 2000), collaborative translation has not been as widely explored. Previous efforts such as DerivTool (DeNeefe et al., 2005) and Linear B (Callison-Burch, 2005) placed stronger emphasis on improving MT. They elicited more in-de</context>
</contexts>
<marker>Cosley, Frankowski, Terveen, Riedl, 2007</marker>
<rawString>Dan Cosley, Dan Frankowski, Loren Terveen, and John Riedl. 2007. Suggestbot: using intelligent task routing to help people find work in wikipedia. In IUI ’07: Proceedings of the 12th international conference on Intelligent user interfaces, pages 32–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
<author>Hayward H Chan</author>
</authors>
<title>Interactively exploring a machine translation model.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Interactive Poster and Demonstration Sessions,</booktitle>
<pages>97--100</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="27598" citStr="DeNeefe et al., 2005" startWordPosition="4445" endWordPosition="4448">uality assurance, but similar types of collaborative efforts have been shown fruitful in other domains (Cosley et al., 2007). Finally, the statistics of user actions may be useful for translation evaluation. They may be informative features for developing automatic metrics for sentence-level evaluations (Kulesza and Shieber, 2004). 7 Related Work While there have been many successful computeraided translation systems both for research and as commercial products (Bowker, 2002; Langlais et al., 2000), collaborative translation has not been as widely explored. Previous efforts such as DerivTool (DeNeefe et al., 2005) and Linear B (Callison-Burch, 2005) placed stronger emphasis on improving MT. They elicited more in-depth interactions between the users and the MT system’s phrase tables. These approaches may be more appropriate for users who are MT researchers themselves. In contrast, our approach focuses on providing intuitive visualization of a variety of information sources for users who may not be MTsavvy. By tracking the types of information they consulted, the portions of translations they selected to modify, and the portions of the source 66 Table 7: Some examples of translations corrected by the par</context>
</contexts>
<marker>DeNeefe, Knight, Chan, 2005</marker>
<rawString>Steve DeNeefe, Kevin Knight, and Hayward H. Chan. 2005. Interactively exploring a machine translation model. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 97–100, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>The proper place of men and machines in language translation.</title>
<date>1980</date>
<booktitle>Xerox. Later reprinted in Machine Translation,</booktitle>
<tech>Technical Report CSL-80-11,</tech>
<volume>12</volume>
<pages>1--2</pages>
<contexts>
<context position="3704" citStr="Kay (1980)" startWordPosition="580" endWordPosition="581"> who were given the MT outputs alone. Furthermore, we observed that some users made better use of the interface for certain genres, such as sports news, suggesting that the translation model may be improved by a better integration of document-level contexts. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 60–68, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 60 2 Collaborative Translation The idea of leveraging human-computer collaborations to improve MT is not new; computeraided translation, for instance, was proposed by Kay (1980). The focus of these efforts has been on improving the performance of professional translators. In contrast, our intended users cannot read the source text. These users do, however, have the world knowledge and the language model to put together coherent sentences in the target-language. From the MT research perspective, this raises an interesting question: given that they are missing a translation model, what would it take to make these users into effective “decoders?” While some translation mistakes are recoverable from a strong language model alone, and some might become readily apparent if</context>
</contexts>
<marker>Kay, 1980</marker>
<rawString>Martin Kay. 1980. The proper place of men and machines in language translation. Technical Report CSL-80-11, Xerox. Later reprinted in Machine Translation, vol. 12 no.(1-2), 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>15</pages>
<contexts>
<context position="9811" citStr="Klein and Manning, 2003" startWordPosition="1547" endWordPosition="1550">sentences retrieved from the monolingual corpus, their automatic translations are provided. If the users wished to examine any of the retrieved translation pairs in detail, they can push it onto the sentence workspace. 4 Experimental Methodology We asked eight non-Chinese speakers to correct the machine translations of four short Chinese pascon released by the LDC; for a handful of characters that serve as function words, we added the functional definitions using an online dictionary http://www.mandarintools.com/worddict.html. 5It is automatically generated by the Stanford Parser for Chinese (Klein and Manning, 2003). 6We used Lemur (2006) for the information retrieval back-end; the parallel corpus is from the Federal Broadcast Information Service corpus; the monolingual corpus is from the Chinese Gigaword corpus. Figure 2: The interface for users who are correcting translations without help; they have access to the document view, but they do not have access to any of the other resources. sages, with an average length of 11.5 sentences. Two passages are news articles and two are excerpts of a fictional work. Each participant was instructed to correct the translations for one news article and one fictional</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. Advances in Neural Information Processing Systems, 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Kulesza</author>
<author>Stuart M Shieber</author>
</authors>
<title>A learning approach to improving sentence-level MT evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI),</booktitle>
<location>Baltimore, MD,</location>
<contexts>
<context position="27309" citStr="Kulesza and Shieber, 2004" startWordPosition="4402" endWordPosition="4405">serve as a useful diagnostic tool to help MT researchers verify ideas about what types of models and data are useful in translation. It may also provide a means of data collection for MT training. To be sure, there are important challenges to be addressed, such as participation incentive and quality assurance, but similar types of collaborative efforts have been shown fruitful in other domains (Cosley et al., 2007). Finally, the statistics of user actions may be useful for translation evaluation. They may be informative features for developing automatic metrics for sentence-level evaluations (Kulesza and Shieber, 2004). 7 Related Work While there have been many successful computeraided translation systems both for research and as commercial products (Bowker, 2002; Langlais et al., 2000), collaborative translation has not been as widely explored. Previous efforts such as DerivTool (DeNeefe et al., 2005) and Linear B (Callison-Burch, 2005) placed stronger emphasis on improving MT. They elicited more in-depth interactions between the users and the MT system’s phrase tables. These approaches may be more appropriate for users who are MT researchers themselves. In contrast, our approach focuses on providing intui</context>
</contexts>
<marker>Kulesza, Shieber, 2004</marker>
<rawString>Alex Kulesza and Stuart M. Shieber. 2004. A learning approach to improving sentence-level MT evaluation. In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Baltimore, MD, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>George Foster</author>
<author>Guy Lapalme</author>
</authors>
<title>Transtype: a computer-aided translation typing system.</title>
<date>2000</date>
<booktitle>In Workshop on Embedded Machine Translation Systems,</booktitle>
<pages>46--51</pages>
<contexts>
<context position="27480" citStr="Langlais et al., 2000" startWordPosition="4428" endWordPosition="4431">tion for MT training. To be sure, there are important challenges to be addressed, such as participation incentive and quality assurance, but similar types of collaborative efforts have been shown fruitful in other domains (Cosley et al., 2007). Finally, the statistics of user actions may be useful for translation evaluation. They may be informative features for developing automatic metrics for sentence-level evaluations (Kulesza and Shieber, 2004). 7 Related Work While there have been many successful computeraided translation systems both for research and as commercial products (Bowker, 2002; Langlais et al., 2000), collaborative translation has not been as widely explored. Previous efforts such as DerivTool (DeNeefe et al., 2005) and Linear B (Callison-Burch, 2005) placed stronger emphasis on improving MT. They elicited more in-depth interactions between the users and the MT system’s phrase tables. These approaches may be more appropriate for users who are MT researchers themselves. In contrast, our approach focuses on providing intuitive visualization of a variety of information sources for users who may not be MTsavvy. By tracking the types of information they consulted, the portions of translations </context>
</contexts>
<marker>Langlais, Foster, Lapalme, 2000</marker>
<rawString>Philippe Langlais, George Foster, and Guy Lapalme. 2000. Transtype: a computer-aided translation typing system. In Workshop on Embedded Machine Translation Systems, pages 46–51, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lemur</author>
</authors>
<title>Lemur toolkit for language modeling and information retrieval. The Lemur Project is a collaborative project between CMU and UMASS.</title>
<date>2006</date>
<contexts>
<context position="9834" citStr="Lemur (2006)" startWordPosition="1553" endWordPosition="1554">ngual corpus, their automatic translations are provided. If the users wished to examine any of the retrieved translation pairs in detail, they can push it onto the sentence workspace. 4 Experimental Methodology We asked eight non-Chinese speakers to correct the machine translations of four short Chinese pascon released by the LDC; for a handful of characters that serve as function words, we added the functional definitions using an online dictionary http://www.mandarintools.com/worddict.html. 5It is automatically generated by the Stanford Parser for Chinese (Klein and Manning, 2003). 6We used Lemur (2006) for the information retrieval back-end; the parallel corpus is from the Federal Broadcast Information Service corpus; the monolingual corpus is from the Chinese Gigaword corpus. Figure 2: The interface for users who are correcting translations without help; they have access to the document view, but they do not have access to any of the other resources. sages, with an average length of 11.5 sentences. Two passages are news articles and two are excerpts of a fictional work. Each participant was instructed to correct the translations for one news article and one fictional passage using all the </context>
</contexts>
<marker>Lemur, 2006</marker>
<rawString>Lemur. 2006. Lemur toolkit for language modeling and information retrieval. The Lemur Project is a collaborative project between CMU and UMASS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elliott Macklovitch</author>
<author>Michel Simard</author>
<author>Philippe Langlais</author>
</authors>
<title>Transsearch: A free translation memory on the world wide web.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources &amp; Evaluation (LREC).</booktitle>
<contexts>
<context position="24612" citStr="Macklovitch et al., 2000" startWordPosition="3963" endWordPosition="3966"> 0.47 0.41 0.43 0.41 0.43 0.41 0.45 with The Chinese Room 0.51 0.54 0.56 0.51 0.50 0.55 0.52 0.54 resources, the discourse level clues helped to guide users to make better lexical choices than when they made corrections without the full system, relying on sentence coherence alone. Figure 3 compares the average access counts (per sentence) of different resources (aggregated over all participants and documents). The option of inspect retrieved examples in detail (i.e., bring them up on the sentence workspace) was rarely used. The inspiration for this feature was from work on translation memory (Macklovitch et al., 2000); however, it was not as informative for our participants because they experienced a greater degree of uncertainty than professional translators. 6 Discussion The results suggest that collaborative translation is a promising approach. Participant experiences were generally positive. Because they felt like they understood the translations better, they did not mind putting in the time to collaborate with the system. Table 7 shows some of the participants’ outputs. Although there are some translation errors that cannot be overcome with our current system (e.g., transliterated names), the particip</context>
</contexts>
<marker>Macklovitch, Simard, Langlais, 2000</marker>
<rawString>Elliott Macklovitch, Michel Simard, and Philippe Langlais. 2000. Transsearch: A free translation memory on the world wide web. In Proceedings of the Second International Conference on Language Resources &amp; Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Mellebeek</author>
<author>Anna Khasin</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Transbooster: Boosting the performance of wide-coverage machine translation systems.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Annual Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>189--197</pages>
<marker>Mellebeek, Khasin, van Genabith, Way, 2005</marker>
<rawString>Bart Mellebeek, Anna Khasin, Josef van Genabith, and Andy Way. 2005. Transbooster: Boosting the performance of wide-coverage machine translation systems. In Proceedings of the 10th Annual Conference of the European Association for Machine Translation (EAMT), pages 189–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip S Resnik</author>
<author>Douglas W Oard</author>
<author>Gina-Anne Levow</author>
</authors>
<title>Improved cross-language retrieval using backoff translation.</title>
<date>2001</date>
<booktitle>In Human Language Technology Conference (HLT-2001),</booktitle>
<location>San Diego, CA,</location>
<contexts>
<context position="29769" citStr="Resnik et al., 2001" startWordPosition="4819" endWordPosition="4822">-West, a spokeswoman for the U.S. Bandai Group declared: “We want to bring to women of all ages that ’flavor of life’.” with The Chinese Room 0.68 West, a spokeswoman for the U.S. Toy Manufacturing Group, and soon to be Vice President-said: “We want to bring women of all ages that ’flavor of life’.” Bilingual Translator 0.75 “We wanted to let women of all ages taste the ’flavor of life’,” said Bandai’s spokeswoman Kasumi Nakanishi. text they attempted to understand, we may alter the design of our translation model. Our objective is also related to that of cross-language information retrieval (Resnik et al., 2001). This work can be seen as providing the next step in helping users to gain some understanding of the information in the documents once they are retrieved. By facilitating better collaborations between MT and target-language readers, we can naturally increase human annotated data for exploring alternative MT models. This form of symbiosis is akin to the paradigm proposed by von Ahn and Dabbish (2004). They designed interactive games in which the player generated data could be used to improve image tagging and other classification tasks (von Ahn, 2006). While our interface does not have the ent</context>
</contexts>
<marker>Resnik, Oard, Levow, 2001</marker>
<rawString>Philip S. Resnik, Douglas W. Oard, and Gina-Anne Levow. 2001. Improved cross-language retrieval using backoff translation. In Human Language Technology Conference (HLT-2001), San Diego, CA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Searle</author>
</authors>
<title>Minds, brains, and programs.</title>
<date>1980</date>
<booktitle>Behavioral and Brain Sciences,</booktitle>
<pages>3--417</pages>
<contexts>
<context position="5628" citStr="Searle, 1980" startWordPosition="888" endWordPosition="889">also need to determine how to present the information from the resources so that the users can easily interpret them. This is a challenge because the Chinese processing tools and the translation resources are imperfect themselves. The information should be displayed in such a way that conflicting analyses between different resources are highlighted. 3 Prototype Design We present an overview of our prototype for a collaborative translation interface, named The Chinese Room1. A screen-shot is shown in Figure 1. It 1The inspiration for the name of our system came from Searle’s thought experiment(Searle, 1980). We realize that there are major differences between our system and Searle’s description. Importantly, our users get to insert their knowledge rather than purely operate based on instructions. We felt Figure 1: A screen-shot of the visual interface. It consists of two main regions. The left pane is a workspace for users to explore the sentence; the right pane provides multiple tabs that offer additional functionalities. is a graphical environment that supports five main sources of information and functionalities. The space separates into two regions. On the left pane is a large workspace for </context>
</contexts>
<marker>Searle, 1980</marker>
<rawString>John R. Searle. 1980. Minds, brains, and programs. Behavioral and Brain Sciences, 3:417–457.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In CHI ’04: Proceedings of the SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>319--326</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>Luis von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In CHI ’04: Proceedings of the SIGCHI conference on Human factors in computing systems, pages 319–326, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
</authors>
<title>Games with a purpose.</title>
<date>2006</date>
<journal>Computer,</journal>
<volume>39</volume>
<issue>6</issue>
<marker>von Ahn, 2006</marker>
<rawString>Luis von Ahn. 2006. Games with a purpose. Computer, 39(6):92–94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>