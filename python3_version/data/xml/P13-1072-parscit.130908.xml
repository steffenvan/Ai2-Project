<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9443085">
Mining Informal Language from Chinese Microtext:
Joint Word Recognition and Segmentation
</title>
<author confidence="0.97778">
Aobo Wang1 Min-Yen Kan1,2∗
</author>
<affiliation confidence="0.964398333333333">
1 Web IR / NLP Group (WING)
2 Interactive and Digital Media Institute (IDMI)
National University of Singapore
</affiliation>
<address confidence="0.828542">
13 Computing Link, Singapore 117590
</address>
<email confidence="0.999238">
{wangaobo,kanmy}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.994808" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948909090909">
We address the problem of informal word
recognition in Chinese microblogs. A key
problem is the lack of word delimiters in
Chinese. We exploit this reliance as an
opportunity: recognizing the relation be-
tween informal word recognition and Chi-
nese word segmentation, we propose to
model the two tasks jointly. Our joint in-
ference method significantly outperforms
baseline systems that conduct the tasks in-
dividually or sequentially.
</bodyText>
<sectionHeader confidence="0.998423" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999709391304348">
User generated content (UGC) – including mi-
croblogs, comments, SMS, chat and instant mes-
saging – collectively referred to as microtext by
Gouwset et al. (2011) or network informal lan-
guage by Xia et al. (2005), is the hallmark of the
participatory Web.
While a rich source that many applications are
interested in mining for knowledge, microtext pro-
cessing is difficult to process. One key reason
for this difficulty is the ubiquitous presence of
informal words – anomalous terms that manifest
as ad hoc abbreviations, neologisms, unconven-
tional spellings and phonetic substitutions. Such
informality is often present in oral conversation,
and user-generated microblogs reflect this infor-
mality. Natural language processing (NLP) tools
largely fail to work properly on microtext, as they
have largely been trained on formally written text
(i.e., newswire). Recent work has started to ad-
dress these shortcomings (Xia and Wong, 2006;
Kobus et al., 2008; Han and Baldwin, 2011). In-
formal words and their usage in microtext evolves
quickly, following social trends and news events.
</bodyText>
<footnote confidence="0.95234">
∗This research is supported by the Singapore National
Research Foundation under its International Research Centre
@ Singapore Funding Initiative and administered by the IDM
Programme Office.
</footnote>
<bodyText confidence="0.999804358974359">
These characteristics make it difficult for lexicog-
raphers to compile lexica to keep with the pace of
language change.
We focus on this problem in the Chinese lan-
guage. Through our analysis of a gathered Chinese
microblog corpus, we observe that Chinese infor-
mal words originate from three primary sources,
as given in Table 1.
But unlike noisy words in English, Chinese in-
formal words are more difficult to mechanically
recognize for two critical reasons: first, Chinese
does not employ word delimiters; second, Chinese
informal words combine numbers, alphabetic let-
ters and Chinese characters. Techniques for En-
glish informal word detection that rely on word
boundaries and informal word orthography need
to be adapted for Chinese. Consider the micro-
text “T,ONUJA” (meaning “Don’t tell me the
spoilers (to a movie or joke)”, also in Table 1).
If “TNO” (“don’t”) and “ ” (past tense marker)
are correctly recognized as two words, we may
predict the previously unseen characters “H�JA”
(“tell spoilers”) as an informal word, based on
the learned Chinese language patterns. However,
state-of-the-art Chinese segmenters1 incorrectly
yield “TN0 H�J A”, preferring to chunk “A
” (“thoroughly”) as a word, as they do not con-
sider the possibility that “WJA” (“spoiler”) could
be an informal word. This example illustrates the
mutual dependency between Chinese word seg-
mentation (henceforth, CWS) and informal word
recognition (IWR) that should be solved jointly.
Hence, rather than pipeline the two processes
serially as previous work, we formulate it as a two-
layer sequential labeling problem. We employ fac-
torial conditional random field (FCRF) to solve
both CWS and IWR jointly. To our best knowl-
edge, this is the first work that shows how Chi-
nese microtext can be analyzed from raw text to
</bodyText>
<footnote confidence="0.979348">
1http://www.ictclas.org/index.html
</footnote>
<page confidence="0.908297">
731
</page>
<note confidence="0.95083">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 731–741,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<tableCaption confidence="0.9570045">
Table 1: Our classification of Chinese informal words as originating from three primary sources. For
Phonetic Substitutions, pronunciation is indicated by the phonetic Pinyin transcription system.
</tableCaption>
<table confidence="0.971150875">
Informal Word Formal Word Example Sentence English Translation
1) Phonetic (�Wmu4 you3) {-;h(mei2 you3) JFRX(;hffiV-,V- No taxi in the development area
Substitutions iAì(hai2 zhi3 men) i&apos;fì(hai2 zi men) it�*iAì Get up kids
bs 94Z(bi shi) -Rbst, I despise you
2) Abbreviation A� A��� eA�� Let’s play board games
N�1i1 Nil�31A T-9JAA Don’t tell (me) the spoilers
3) Neologisms �� �� -A��p1 So awesome!
�@ zLAAp AA#@� Quickly purchase it
</table>
<bodyText confidence="0.999503928571429">
derive joint solutions for both problems of CWS
and IWR. We also propose novel features for in-
put to the joint inference. Our techniques signif-
icantly outperform both research and commercial
state-of-the-art for these problems, including two-
step linear CRF baselines which perform the two
tasks sequentially.
We detail our methods in Section 2. In Sec-
tion 3, we first describe the details of our dataset
and baseline systems, followed by demonstrating
two sets of experiments for CWS and IWR, re-
spectively. Section 4 offers the discussion on error
analysis and limitations. We discuss related work
in Section 5, before concluding our paper.
</bodyText>
<sectionHeader confidence="0.997401" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999456">
Given an input Chinese microblog post, our
method simultaneously segments the sentences
into words (the Chinese Word Segmentation,
CWS, task), and marks the component words as
informal or formal ones (the Informal Word Re-
congition, IWR, task).
</bodyText>
<subsectionHeader confidence="0.987214">
2.1 Problem Formalization
</subsectionHeader>
<bodyText confidence="0.999966466666667">
The two tasks are simple to formalize. The IWR
task labels each Chinese character with either an F
(part of a formal word) or IF (informal word). For
the CWS task, we follow the widely-used BIES
coding scheme (Low et al., 2005; Hai et al., 2006),
where B, I, E and S stand for beginning of a
word, inside a word, end of a word and single-
character word, respectively. As a result, we have
two (hidden) labels to associate with each (ob-
servable) character. Figure 1 illustrates an exam-
ple microblog post graphically, where the labels
are in circles and the observations are in squares.
The two informal words in the example post are
“(V” (normalized form: “&amp;V”; English gloss:
“no”) and “rp” (“ºPPI”; “luck”).
</bodyText>
<subsectionHeader confidence="0.999656">
2.2 Conditional Random Field Models
</subsectionHeader>
<bodyText confidence="0.9999360625">
Given the general performance and discrimi-
native framework, Conditional Random Fields
(CRFs) (Lafferty et al., 2001) is a suitable frame-
work for tackling sequence labeling problems.
Other alternative frameworks such as Markov
Logic Networks (MLNs) and Integer Linear Pro-
gramming (ILP) could also be considered. How-
ever, we feel that for this task, formulating effi-
cient global formulas (constraints) for MLN (ILP)
is comparatively less straightforward than in other
tasks (e.g, compared to Semantic Role Labeling,
where the rules may come directly from grammat-
ical constraints). CRFs represent a basic, simple
and well-understood framework for sequence la-
beling, making it a suitable framework for adapt-
ing to perform joint inference.
</bodyText>
<subsectionHeader confidence="0.587417">
2.2.1 Linear-Chain CRF
</subsectionHeader>
<bodyText confidence="0.998875142857143">
A linear-chain CRF (LCRF; Figure 2a) predicts
the output label based on feature functions pro-
vided by the scientist on the input. In fact, the
LCRF has been used for the exact problem of
CWS (Sun and Xu, 2011), garnering state-of-the-
art performance, and as such, validate it as a strong
baseline for comparison.
</bodyText>
<subsectionHeader confidence="0.768563">
2.2.2 Factorial CRF
</subsectionHeader>
<bodyText confidence="0.999685384615384">
To properly model the interplay between the
two sub-problems, we employ the factorial CRF
(FCRF) model, which is based on the dynamic
CRF (DCRF) (Sutton et al., 2007). By introduc-
ing a pairwise factor between different variables
at each position, the FCRF model results as a spe-
cial case of the DCRF. A FCRF captures the joint
distribution among various layers and jointly pre-
dicts across layers. Figure 2 illustrates both the
LCRF and FCRF models, where cliques include
within-chain edges (e.g., yt, yt+1) in both LCRF
and FCRF models, and the between-chain edges
(e.g., yt, zt) only in the FCRF.
</bodyText>
<page confidence="0.950262">
732
</page>
<figure confidence="0.421743166666667">
开
F F F IF IF F F F F IF IF F F F
B I E B E B I E S B I E S S
发
区
木
有
出
租
车
,
r
p
值
低
啊
开 发 区 没 有 出 租 车 , 人 品 值 低 啊
There is no taxi in the development zone , how poor my character is
</figure>
<figureCaption confidence="0.987768">
Figure 1: A Chinese microtext (bottom layer) with annotations for IWR (top layer) and CWS (middle
layer). The bottom three lines give the normalized Chinese form, its pronuniciation in Pinyin and aligned
English translation.
Figure 2: Graphical representations of the two
</figureCaption>
<bodyText confidence="0.956545571428572">
types of CRFs used in this work. yt denotes the
1st layer label, zt denotes the 2nd layer label, and
xt denotes the observation sequence.
Although the FCRF can be collapsed into a
LCRF whose state space is the cross-product of
the outcomes of the state variables (i.e., 8 labels
in this case), Sutton et al. (2007) noted that such
a LCRF requires not only more parameters in the
number of variables, but also more training data
to achieve equivalent performance with an FCRF.
Given the limited scale of the state space and train-
ing data, we follow the FCRF model, using exact
Junction Tree (Jensen, 1996) inference and decod-
ing algorithm to perform prediction.
</bodyText>
<subsectionHeader confidence="0.998109">
2.3 CRF Features
</subsectionHeader>
<bodyText confidence="0.998215705882353">
We use three broad feature classes – lexical,
dictionary-based and statistical features – aiming
to distinguish the output classes for the CWS and
IRW problems. Character-based sequence label-
ing is employed for word segmentation due to its
simplicity and robustness to the unknown word
problem (Xue, 2003).
A key contribution of our work is also to
propose novel features for joint inference. We
propose new features for the dictionary-based and
statistical feature classes, which we have marked
in the discussion below with “(*)”. We later
examine their efficacy in Section 3.
Lexical Features. As a foundation, we employ
lexical (n-gram) features informed by the previous
state-of-the-art for CWS (Sun and Xu, 2011; Low
et al., 2005). These features are listed below2:
</bodyText>
<listItem confidence="0.999828833333333">
• Character 1-gram: Ck(i − 4 &lt; k &lt; i + 4)
• Character 2-gram: CkCk+1(i − 4 &lt; k &lt; i +
3)
• Character 3-gram: CkCk+1Ck+2(i − 3 &lt;
k &lt; i + 2)
• Character lexicon: C−1C1
</listItem>
<bodyText confidence="0.99825375">
This feature is used to capture the common
indicators in Chinese interrogative sentences.
(e.g., “ATAq” (“whether or not”), “0TN0”
(“OK or not”))
</bodyText>
<listItem confidence="0.9968515">
• Whether Ck and Ck+1 are identical, for i −
4 &lt; k &lt; i + 3.
</listItem>
<bodyText confidence="0.997729333333333">
This feature is used to capture the words
of employing character doubling in Chinese.
(e.g., “� �” (“see you”), “R R” (“ every
</bodyText>
<equation confidence="0.354839">
day”)) J
</equation>
<footnote confidence="0.8506709">
Dictionary-based Features. We use features
that indicate whether the input character sequence
2For notational convenience, we denote a candidate char-
acter token Ci as having a context ...Ci−1CiCi+1.... We use
Cm:n to express a subsequence starting at the position m and
ending at n. len stands for the length of the subsequence, and
offset denotes the position offset of Cm:n from the current
character Ci. We use b (beginning), m (middle) and e (end-
ing) to indicate the position of Ck (m &lt; k &lt; n) within the
string Cm:n. — —
</footnote>
<figure confidence="0.998061666666667">
(a) Linear-chain CRF (b) Two-layer Factorial CRF
xt-1
yt-1
xt
yt
xt+1
yt+1
xt-1
yt-1
zt-1
xt
yt
zt zt+1
xt+1
yt+1
</figure>
<page confidence="0.99577">
733
</page>
<bodyText confidence="0.9997504">
matches entries in certain lexica. We use the on-
line dictionary from Peking University as the for-
mal lexicon and the compiled informal word list
from our training instances as the informal lex-
icon. In addition, we employ additional online
word lists3 to distinguish named entities and func-
tion words from potential informal words.
As shown in Table 1, alphabetic sequences in
microblogs may refer to Chinese Pinyin or Pinyin
abbreviations, rather than English (e.g., “bs” for
bi shi; “to despise”). Hence, we added dictionary-
based features to indicate the presence of Pinyin
initials, finals and standard Pinyin expansions, us-
ing a UK English word list4. The final list of
dictionary-based features employed are:
</bodyText>
<listItem confidence="0.924652153846154">
• If Ck (i − 4 &lt; k &lt; i + 4) is a surname:
Surname@k
• (*) If Ck (i − 4 &lt; k &lt; i + 4) is a stop word:
StopW@k
• (*) If Ck (i−4 &lt; k &lt; i+4) is a noun-suffix:
NSuffix@k
• (*) If Ck (i − 4 &lt; k &lt; i + 4) is a Pinyin
Initial: Initial@k
• (*) If Ck (i−4 &lt; k &lt; i+4) is a Pinyin Final:
Final@k
• If Ck (i − 4 &lt; k &lt; i + 4) is a English letter:
En@k
•
If Cm:n (i−4 &lt; m &lt; n &lt; i+4, 0 &lt; n−m &lt;
5) matches one entry in the Peking University
dictionary:
FW@m:n; len@offset; FW-Ck@b-offset,
FW-Ck@n-offset or FW-Ck@e-offset
• (*) If Cm:n (i − 4 &lt; m &lt; n &lt; i + 4,0 &lt;
n − m &lt; 5) matches one entry in the infor-
mal word list:
IFW@m:n; len@offset; IFW-Ck@b-offset,
IFW-Ck@n-offset or IFW-Ck@e-offset
• (*) If Cm:n (i − 4 &lt; m &lt; n &lt; i + 4, 0 &lt;
n − m &lt; 5) matches one entry in the valid
Pinyin list:
</listItem>
<construct confidence="0.8781435">
PY@m:n; len@offset; PY-Ck@b-offset, PY-
Ck@n-offset or PY-Ck@e-offset
</construct>
<bodyText confidence="0.8810055">
Statistical Features. We use pointwise mutual
information (PMI) variant (Church and Hanks,
</bodyText>
<footnote confidence="0.98261925">
3Resources are available at http://www.sogou.
com/labs/resources.html
4http://www.bckelk.uklinux.net/menu.
html
</footnote>
<bodyText confidence="0.999870342857143">
1990) to account for global, corpus-wide informa-
tion. This measures the difference between the ob-
served probability of an event (i.e., several charac-
ters combined as an informal word) and its expec-
tation, based on the probabilities of the individual
events (i.e., the probability of the individual char-
acters occurring in the corpus). Compared with
other standard association measures such as MI,
PMI tends to assign rare events higher scores. This
makes it a useful signal for IWR, as it is sensi-
tive to informal words which often have low fre-
quency. However, the word frequency alone is
not reliable enough to distinguish informal words
from uncommon but formal words.
In response to these difficulties in differentiat-
ing linguistic registers, we compute two different
PMI scores for character-based bigrams from two
large corpora representing news and microblogs as
features. We also use the difference between the
two PMI scores as a differential feature. In ad-
dition, we also convert all the character-based bi-
grams into Pinyin-based bigrams (ignoring tones5)
and compute the Pinyin-level PMI in the same
way. These features capture inconsistent use of
the bigram across the two domains, which assists
to distinguish informal words. Note that we es-
chew smoothing in our computation of PMI, as it
is important to capture the inconsistent character
bigrams usage between the two domains. For ex-
ample, the word “rp” appears in the microblog do-
main, but not in news. If smoothing is conducted,
the character bigram “rp” will be given a non-zero
probability in both domains, not reflective of ac-
tual use. For each character CZ, we incorporate
the PMI of the character bigrams as follows:
</bodyText>
<listItem confidence="0.9465118">
• (*) If CkCk+1 (i − 4 &lt; k &lt; i + 4) is not a
Chinese word recorded in dictionaries:
CPMI-N@k+i; CPMI-M@k+i; CDiff@k+i;
PYPMI-N@k+i; PYPMI-M@k+i; PYD-
iff@k+i
</listItem>
<sectionHeader confidence="0.991798" genericHeader="method">
3 Experiment
</sectionHeader>
<bodyText confidence="0.9997645">
We discuss the dataset, baseline systems and ex-
periments results in detail in the following.
</bodyText>
<subsectionHeader confidence="0.998808">
3.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.996162">
We utilize the Chinese social media archive,
PrEV (Cui et al., 2012), to obtain Chinese mi-
</bodyText>
<footnote confidence="0.901051333333333">
5The informal word may have the same Pinyin transcrip-
tion as its formal counterpart without considering the differ-
ences in tones.
</footnote>
<page confidence="0.996184">
734
</page>
<bodyText confidence="0.999976903225806">
croblog posts from the public timeline of Sina
Weibo6. Sina Weibo is the largest microblogging
in China, where over 100 million Chinese mi-
croblog posts are posted daily (Cao, 2012), likely
the largest public source of informal and daily
Chinese language use. Our dataset has a total of
6,678,021 messages, covering two months from
June to July of 2011. To annotate the corpus,
we employ Zhubajie7, one of China mainland’s
largest crowdsourcing (Wang et al., 2010) plat-
forms to obtain informal word annotations. In
total, we spent US$110 on assembling a sub-
set of 5,500 posts (12,446 sentences) in which
1, 658 unique informal words are annotated within
five weeks via Zhubajie. Each post was anno-
tated by three annotators with moderate (0.57)
inter-annotator agreement measured by Fleiss’
n (Joseph, 1971), and conflicts were resolved by
majority voting.
We divided the annotated corpus, taking 4, 000
posts for training, and the remainder (1, 500) for
testing. Through inspection, we note that 79.8%
of the informal words annotated in the testing set
are not covered by the training set. We also follow
Wang et al. (2012)’s conventions and apply rule-
sets to preprocess the corpus’ URLs, emoticons,
“@usernames” and Hashtags as pre-segmented
words, before input to CWS and IWR. For the
CWS task, the first author manually labelled the
same corpus following the segmentation guide-
lines published with the SIGHAN-58 MSR dataset.
</bodyText>
<subsectionHeader confidence="0.998307">
3.2 Baseline Systems
</subsectionHeader>
<bodyText confidence="0.99995825">
We implemented several baseline systems to com-
pare with proposed FCRF joint inference method.
Existing Systems. We re-implemented Xia and
Wong (2008)’s extended Support Vector Machine
(SVM) based microtext IWR system to compare
with our method. Their system only does IWR,
using the CWS and POS tagging otuput of the
ICTCLAS segmenter (Zhang et al., 2003) as in-
put. To compare our joint inference versus other
learning models, we also employed a decision tree
(DT) learner, equipped with the same feature set
as our FCRF. Both the SVM and DT models are
provided by the Weka3 (Hall et al., 2009) toolkit,
using its default configuration.
To evaluate CWS performance, we compare
with two recent segmenters. Sun and Xu (2011)’s
</bodyText>
<footnote confidence="0.999984">
6http://open.weibo.com
7http://www.zhubajie.com
8http://www.sighan.org
</footnote>
<bodyText confidence="0.996834052631579">
work achieves state-of-the-art performance and
is publicly available. They employ a LCRF
taking as input both lexical and statistical fea-
tures derived from unlabeled data. As a sec-
ond baseline, we also evaluate against a widely-
used, commercially-available alternative, the re-
cently released 2011 ICTCLAS segmenter9.
Two-stage Sequential Systems. To benchmark
the improvement that the factorial CRF model
has by doing the two tasks jointly, we com-
pare with a LCRF solution that chains these two
tasks together. For completeness, we test pipelin-
ing in both directions – CWS feeding features
for IWR (LCRFcws&gt;-LCRFiwr), and the reverse
(LCRFiwr&gt;-LCRFcws). We modify the open-
source Mallet GRMM package (Sutton, 2006) to
implement both this sequential LCRF model and
our proposed FCRF model. Both models take the
whole feature set described in Section 2.3.
Upper Bound Systems. To measure the upper-
bound achievable with perfect support from the
complementary task, we also provided gold stan-
dard labels of one task (e.g., IWR) as an input
feature to the other task (e.g., CWS). These sys-
tems (hereafter denoted as LCRF&gt;-LCRF-UB and
FCRF-UB) are meant for reference only, as they
have access to answers for the opposing tasks.
Adapted SVM for Joint Classification. For
completeness, we also compared our work against
the standard SVM classification model that per-
forms both tasks by predicting the cross-product
of the CWS and IWR individual classes (in to-
tal, 8 classes). We train the SVM classifier on the
same set of features as the FCRF, by providing the
cross-product of two layer labels as gold labels.
This system (hereafter denoted as SVM-JC) was
implemented using the LibSVM package (Chang
and Lin, 2011).
</bodyText>
<subsectionHeader confidence="0.993349">
3.3 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9999305">
We use the standard metrics of precision, recall
and F1 for the IWR task. Only words that exactly
match the manually-annotated labels are consid-
ered correct. For example given the sentence “�
HËH0nLpfv!” (“�H7kH0nLpfv!”; “How deli-
cious it is”), if the IWR component identifies “Ë
H” as an informal word, it will be considered cor-
rect, whereas both “ËH0” and “Ë” are deemed
incorrect. For CWS evaluation, we employ the
conventional scoring script provided in SIGHAN-
</bodyText>
<footnote confidence="0.959629">
9http://www.ictclas.org/index.html
</footnote>
<page confidence="0.995903">
735
</page>
<bodyText confidence="0.999946818181818">
5, which also provides out-of-vocabulary recall
(OOVR).
To determine statistical significance of the im-
provements, we also compute paired, one-tailed
t tests. As pointed out by Yeh and Alexan-
der (2000), the randomization method is more re-
liable in measuring the significance of F1 through
handling non-linear functions of random variables.
Thus we employ Pad´o (2006)’s implementation of
randomization algorithm to measure the signifi-
cance of F1.
</bodyText>
<subsectionHeader confidence="0.975501">
3.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.997179533333333">
The goal of our experiments is to answer the fol-
lowing research questions:
RQ1 Do the two tasks of CWS and IWR benefit
from each other?
RQ2 Is jointly modeling both tasks more efficient
than conducting each task separately or se-
quentially?
RQ3 What is the upper bound improvement that
can be achieved with perfect support from the
opposing task?
RQ4 Are the features we designed for the joint
inference method effective?
RQ5 Is there a significant difference between the
performance of the joint inference of a cross-
product SVM and our proposed FCRF?
</bodyText>
<subsectionHeader confidence="0.359082">
3.4.1 CWS Performance
</subsectionHeader>
<bodyText confidence="0.93175625">
Table 2: Performance comparison on the CWS
task. The two bottom-most rows show upper
bound performance. ‘‡’(‘∗’) in the top four lines
indicates statistical significance at p &lt; 0.001
(0.05) when compared with the previous row.
Symbols in the bottom two lines indicate signifi-
cant difference between upper bound systems and
their corresponding counterparts.
</bodyText>
<table confidence="0.993314714285714">
Pre Rec Fl OOVR
ICTCLAS (2003) 0.640 0.767 0.698 0.551
Sun and Xu (2011) 0.661‡ 0.691‡ 0.675 0.572‡
LCRFiwr&gt;-LCRFcws 0.741‡ 0.775‡ 0.758∗ 0.607∗
FCRF 0.757‡ 0.801‡ 0.778∗ 0.633∗
LCRFiwr&gt;-LCRFcws-UB 0.807‡ 0.815‡ 0.811∗ 0.731‡
FCRF-UB 0.820‡ 0.833‡ 0.826∗ 0.758‡
</table>
<bodyText confidence="0.999476026315789">
In general, our FCRF yields the best perfor-
mance among all systems (top portion of Table 2),
answering RQ1. Given microblog posts as test
data, the F1 of ICTCLAS drops from 0.98510 to
0.698, clearly showing the difficulty of process-
ing microtext. The sequential LCRF model and
FCRF model both outperform the baselines, which
means with the novel features shared by the two
tasks, CWS benefits significantly from the results
of IWR. Hence our segmenter outperforms the ex-
isting segmenters by tackling one of the bottle-
necks of recognizing informal words in Chinese
microtext.
To illustrate, the sequence “...� ( � �...”
(“...����...”; “...is there anyone...”), is cor-
rectly labeled as BIES by our FCRF model but
mislabeled by baseline systems as SSBE. This is
likely due to the ignorance of the informal word
“T(V”, leading baseline systems to keep the
formal word “TA” (“someone”) as a segment.
More importantly, by jointly optimizing the
probabilities of labels on both layers, the FCRF
model slightly but significantly improves over the
sequential LCRF method, answering RQ2. Thus
we conclude that jointly modeling both tasks is
more effective than performing the tasks sequen-
tially.
For RQ3, the last two rows presents the upper-
bound systems that have access to gold standard
labels for IWR. Both upper-bound systems sta-
tistically outperform their counterparts, indicating
that there is still room to improve CWS perfor-
mance with better IWR as input. This also vali-
dates our assumption that CWS can benefit from
joint consideration of IWR. Taking the best pre-
vious work as our lower bound (0.69 F1), we see
that our FCRF methodology (0.77) makes signifi-
cant progress towards the upper bound (0.82).
</bodyText>
<subsectionHeader confidence="0.702479">
3.4.2 IWR Performance
</subsectionHeader>
<bodyText confidence="0.999692166666667">
For RQ1 and RQ2, Table 3 compares the per-
formance of our method with the baseline sys-
tems on the IWR task. Overall, the FCRF
method again outperforms all the baseline sys-
tems. We note that the CRF based models achieve
much higher precision score than baseline sys-
tems, which means that the CRF based models
can make accurate predictions without enlarging
the scope of prospective informal words. Com-
pared with the CRF based models, the SVM and
DT both over-predict informal words, incurring
a larger precision penalty. Studying this phe-
</bodyText>
<footnote confidence="0.9693525">
10Self-declared segmentation accuracy on formal
text.http://www.ictclas.org/
</footnote>
<page confidence="0.998266">
736
</page>
<tableCaption confidence="0.977153">
Table 3: Performance comparison on the IWR
</tableCaption>
<bodyText confidence="0.9674288">
task. ‘$’ or ‘*’ in the top four rows indicates sta-
tistical significance at p &lt; 0.001 or &lt; 0.05 com-
pared with the previous row. Symbols in the bot-
tom two rows indicate differences between upper
bound systems and their counterparts.
</bodyText>
<table confidence="0.999038428571429">
Pre Rec Fl
SVM 0.382 0.621 0.473
DT 0.402* 0.714* 0.514*
LCRFcws&gt;-LCRFiwr 0.8581 0.5911 0.699*
FCRF 0.877* 0.655* 0.750*
LCRFcws&gt;-LCRFiwr-UB 0.840 0.726* 0.779*
FCRF-UB 0.878 0.752* 0.810*
</table>
<bodyText confidence="0.958212230769231">
nomenon more closely, we find it is difficult for
the baseline systems to classify segments mixed
with formal and informal characters. Taking the
microblog “Z,HËH0WZp)v&apos;” (“Z,H H0WZ
p)v&apos;”; “how delicious it is”) as an example, with-
out considering the possible word boundaries sug-
gested by the contextual formal words – i.e., “Z,
H” (“how”) and “0WZ” (“delicious”) – the base-
lines chunk the informal words (i.e., “ËH”) to-
gether with adjacent characters mistakenly as “ Ë
H0” or, “HËH”.
As indicated by the bold figures in Table 3, the
FCRF performs slightly better than the sequential
LCRF (p &lt; 0.05) – a weaker trend when com-
pared with the CWS case. As an example, the se-
quential LCRF method fails to recognize “1AA”
(“iPhone”) as an informal word in the sentence
“A6M1A0�” (“my iPhone is fun”), where the
FCRF succeeds. Inspecting the output, the LCRF
segmenter mislabels “1MA” as SS. By jointly con-
sidering the probabilities of the two layers, the
FCRF model infers better quality segmentation la-
bels, which in turn enhances the FCRF’s capabil-
ity to recognize the sequence of two characters as
an informal word. This is further validated by
the significant performance gulf between the up-
per bound and the basic system shown in the lower
half of the table.
For RQ3, interestingly, the difference in perfor-
mance between the LCRF and FCRF upper-bound
systems is not significant. However, these are up-
per bounds, and we expect on real-world data that
CWS performance will not be perfect. As such,
we still recommend using the FCRF model, as the
joint process is more robust to noisy input from
one channel.
Table 4: F1 comparison between FCRF and
FCRF−new. (‘*’) indicates statistical significance
at p &lt; 0.05 when compared with the previous row.
</bodyText>
<sectionHeader confidence="0.57195" genericHeader="method">
CWS IWR
</sectionHeader>
<bodyText confidence="0.8596245">
FCRF−new 0.690 0.552
FCRF 0.778* 0.750*
</bodyText>
<subsectionHeader confidence="0.847268">
3.4.3 Feature set evaluation
</subsectionHeader>
<bodyText confidence="0.999736545454545">
For RQ4, to evaluate the effectiveness of our
newly-introduced feature sets (those marked with
“*” in Section 2.3), we also test a FCRF
(FCRF−new) without our new features. Accord-
ing to Table 4, performance drops by a signifi-
cant amount: 0.088 F1 on CWS and 0.198 F1 on
IWR. FCRF−new makes many mistakes identical
to the baselines: segmenting informal words into
several single-character words and chunking ad-
jacent characters from informal and formal words
together.
</bodyText>
<subsectionHeader confidence="0.442812">
3.4.4 Adapted SVM-JC vs. FCRF
</subsectionHeader>
<bodyText confidence="0.988035869565217">
Table 5: F1 comparison between SVM, SVM-JC
and FCRF. ‘$’(‘*’) indicates statistical significance
at p &lt; 0.001 (0.05) when compared with the pre-
vious row.
For RQ5, according to Table 5, our SVM trained
to predict the cross-product CWS/IWR classifica-
tion (SVM-JC) performs quite well on its own.
Unsurprisingly, it does not outperform our pro-
posed FCRF, which has access to more struc-
tural correlation among the CWS and IWR labels.
SVM-JC significantly (p &lt; 0.001) outperforms
the baseline SVM system by 0.151 in the IWR
task, which we think is partially explained by its
good performance (0.761) on the CWS task. The
over-prediction tendency of the individual SVM
is largely solved by simultaneously modeling the
CWS task, whereas FCRF turns out to be more
effective in solving joint inference problem, al-
though in a weaker trend in terms of the statistical
significance (p &lt; 0.05).
We conclude that the use of the FCRF model
and the addition of our new features are both es-
sential for the high performance of our system.
</bodyText>
<figure confidence="0.8700559">
CWS IWR
SVM
SVM-JC
FCRF
— 0.473
0.741 0.624$
0.778* 0.750*
737
4 Discussion Table 6: Sample Chinese freestyle named entities
that are usernames.
</figure>
<bodyText confidence="0.99981388">
We wish to understand the causes of errors in our
models so that we may better understand its weak-
nesses. Manually inspecting the errors of our sys-
tem, we found three major categories of errors
which we dissect here.
For IWR, the major source of error, accounting
for more than 60% of all errors, is caused by what
we term the partially observed informal word phe-
nomenon. This refers to informal words contain-
ing multiple characters, where some of its compo-
nents have appeared in the training data as infor-
mal words individually. For instance, the single-
character informal word, “à” (“IV; “very”) ap-
pears in training multiple times, thus the unseen
informal word “àE” (“TRE”; “long time”) is a
partially observed informal word. In this case, the
model incorrectly labels the known, single charac-
ter “à” with IF S as an informal word, instead of
labeling the unseen sequence “àE” with correct
labels IF B IF E. Errors then result in both tasks.
This observation motivates the use of the rela-
tion between the known informal word and its for-
mal counterpart in order to inform the model to
better predict in cases of partial observations. Fol-
lowing the same example, given that “à” is an in-
formal word, if the model also considers the prob-
ability of normalizing “à” to “W”, while con-
sidering the higher probability that the character
sequence “TRE” could be a formal word, there
would be a higher likelihood of correctly predict-
ing the sequence “àE” as an informal word. So
informal word normalization is also an intrinsic
component of IWR and CWS, and we believe it
is an interesting direction for future work.
Another source of error is a side effect of mi-
crotext being extremely short. For example, in the
sentence “ME¶!�kVo00” (“Q¶!�k
Vo00”; “Go home! Exhausted.”), the un-
seen informal word “E¶” itself forms a short
sentence. Although it has a subsequent sentence
“�Vo00” (“Exhausted”) as context, and
the two are pragmatically related, (i.e., “I am ex-
hausted! [And as a result,] I want to go home.”),
the lexical relationship between the sentences is
weak; i.e., “�CVo00” appears frequently as
the context of various sentences, making the con-
text difficult to utilize. These phenomena makes it
difficult to recognize “0¶” as an informal word.
A possible solution could factor in proximity,
similar to density-based matching, as in Tellex et
</bodyText>
<table confidence="0.89919325">
Freestyle Named Explanation
Entity
“ �INNOR” “��” (“durian”), “&apos;N” (“snow”),
“f�ft” (“charming lady”)
“ f,” It is short for the cartoon name “�
W”.
“dj_M”, “*pp” Usernames mixed of Chinese and al-
phabetic characters
</table>
<bodyText confidence="0.99883652">
al. (2003). We can assign a higher weight to fea-
tures related to characters closer to the current
target character. In particular, for this example,
given the current target character “ AE”, we can as-
sign higher weight to features generated from fea-
tures from the proximal context “AE¶”, and lower
weight to features extracted from distal contexts.
Another major group of errors come from what
we term freestyle named entities as exemplified in
Table 6; i.e., person names in the form of user IDs
and nicknames, that have less constraint on form
in terms of length, canonical structure (not sur-
names with given names; as is standard in Chinese
names) and may mix alphabetic characters. Most
of these belong to the category of Person Name
(PER), as defined in CoNLL-200311 Named En-
tity Recognition shared task. Such freestyle en-
tities are often misrecognized as informal words,
as they share some of the same stylistic markings,
and are not marked by features used to recognize
previous Chinese named entity recognition meth-
ods (Gao et al., 2005; Zhao and Kit, 2008) that
work on news or general domain text. We recog-
nize this as a challenge in Chinese microtext, but
beyond the scope of our current work.
</bodyText>
<sectionHeader confidence="0.999838" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.998865833333333">
In English, IWR has typically been investigated
alongside normalization. Several recent works
(Han and Baldwin, 2011; Gouws et al., 2011; Han
et al., 2012) aim to produce informal/formal word
lexicons and mappings. These works are based on
distributional similarity and string similarity that
address concerns of lexical variation and spelling.
These methods propose two-step unsupervised ap-
proaches to first detect and then normalize de-
tected informal words using dictionaries.
In processing Chinese informal language, work
conducted by Xia and Wong address the problem
</bodyText>
<footnote confidence="0.990023">
11http://www.cnts.ua.ac.be/conll2003/
ner/
</footnote>
<page confidence="0.994766">
738
</page>
<bodyText confidence="0.999806772727273">
of in bulletin board system (BBS) chats. They em-
ploy pattern matching and SVM-based classifica-
tion to recognize Chinese informal sentences (not
individual words) chat (Xia et al., 2005). Both
methods had their advantages: the learning-based
method did better on recall, while the pattern
matching performed better on precision. To obtain
consistent performance on new unseen data, they
further employed an error-driven method which
performed more consistently over time-varying
data (Xia and Wong, 2006). In contrast, our
work identifies individual informal words, a finer-
grained (and more difficult) task.
While seminal, we feel that the difference in
scope (informal sentence detection rather than
word detection) shows the limitation of their work
for microblog IWR. Their chats cover only 651
unique informal words, as opposed to our study
covering almost triple the word types (1, 658).
Our corpus demonstrates a higher ratio of infor-
mal word use (a new informal word appears in
12,446 = 13% of sentences, as opposed to 651
</bodyText>
<equation confidence="0.547296">
1,658 22,400 =
</equation>
<bodyText confidence="0.999881717948718">
2% in their BBS corpus). Further analysis of
their corpus reveals that phonetic substitution is
the primary origin of informal words in their cor-
pus – 99.2% as reported in (Wong and Xia, 2008).
In contrast, the origin for informal words in mi-
croblogs is more varied, where phonetic substitu-
tions abbreviations and neologisms, account for
53.1%, 21.4% and 18.7% of the informal word
types, respectively. Their method is best suited
for phonetic substitution, thus performing well on
their corpus but poorly on ours.
More closely related, Li and Yarowsky (2008)
tackle Chinese IWR. They bootstrap 500 infor-
mal/formal word pairs by using manually-tuned
queries to find definition sentences on the Web.
The resulting noisy list is further re-ranked based
on n-gram co-occurrence. However, their method
makes a basic assumption that informal/formal
word pairs co-occur within a definition sentence
(i.e., “&lt;informal word&gt; means &lt;formal word&gt;”)
may not hold in microblog data, as microbloggers
largely do not define the words they use.
Closely related to our work is the task of
Chinese new word detection, normally treated
as a separate process from word segmentation in
most previous works (Chen and Bai, 1998; Wu
and Jiang, 2000; Chen and Ma, 2002; Gao et al.,
2005). Aiming to improve both tasks, work by
Peng et al. (2004) and Sun et al. (2012) conduct
segmentation and detection sequentially, but in
an iterative manner rather than joint. This is
a weakness as their linear CRF model requires
re-training. Their method also requires thresholds
to be set through heuristic tuning, as to whether
the segmented words are indeed new words. We
note that the task of new word detection refers
to out-of-vocabulary (OOV) detection, and is
distinctly different from IWR (new words could
be both formal or informal words).
</bodyText>
<sectionHeader confidence="0.998916" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999990862068966">
There is a close dependency between Chinese
word segmentation (CWS) and informal word
recognition (IWR). To leverage this, we employ a
factorial conditional random field to perform both
tasks of CWS and IWR jointly.
We propose novel features including statistical
and lexical features that improve the performance
of the inference process. We evaluate our method
on a manually-constructed data set and compare it
with multiple research and industrial baselines that
perform CWS and IWR individually or sequen-
tially. Our experimental results show our joint
inference model yields significantly better F1 for
both tasks. For analysis, we also construct upper
bound systems to assess the potential maximal im-
provement, by feeding one task with the gold stan-
dard labels from the complementary task. These
experiments further verify the necessity and ef-
fectiveness of modeling the two tasks jointly, and
point to the possibility of even better performance
with improved per-task performance.
Analyzing the classes of errors made by our sys-
tem, we identify a promising future work topic to
handle errors arising from partially observed in-
formal words – where parts of a multi-character
informal word have been observed before. We be-
lieve incorporating informal word normalization
into the inference process may help address this
important source of error.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998170142857143">
We would like to thank the anonymous reviewers
for their valuable comments. We also appreciate
the proofreading effort made by Tao Chen, Xi-
angnan He, Ning Fang, Yushi Wang and Haochen
Zhan from WING. This work also benefits from
the discussion with Yang Liu, associate professor
from Tsinghua University.
</bodyText>
<page confidence="0.997519">
739
</page>
<sectionHeader confidence="0.979559" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999438403846154">
Belinda Cao. 2012. Sina’s weibo outlook buoys inter-
net stock gains: China overnight.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
pages 27:1–27:27.
Keh-Jiann Chen and Ming-Hong Bai. 1998. Un-
known Word Detection for Chinese by a Corpus-
Based Learning Method. International Journal of
Computational Linguistics and Chinese Language
Processing, pages 27–44.
Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown
Word Extraction for Chinese Documents. In Pro-
ceedings of the 19th international conference on
Computational linguistics, pages 1–7.
Kenneth Ward Church and Patrick Hanks. 1990. Word
Association Norms, Mutual Information, and Lexi-
cography. Computional Linguistic, pages 22–29.
Anqi Cui, Liner Yang, Dejun Hou, Min-Yen Kan,
Yiqun Liu, Min Zhang, and Shaoping Ma. 2012.
PrEV: Preservation Explorer and Vault for Web 2.0
User-Generated Content. Theory and Practice of
Digital Libraries, pages 101–112.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning
Huang. 2005. Chinese Word Segmentation and
Named Entity Recognition: A Pragmatic Approach.
Computitional Linguistic, pages 531–574.
Stephan Gouws, Donald Metzler, Congxing Cai, and
Eduard Hovy. 2011. Contextual Bearing on Lin-
guistic Variation in Social Media. In Proceedings of
the Workshop on Language in Social Media, pages
20–29.
Zhao Hai, Huang Chang-Ning, Li Mu, and Lu Bao-
Liang. 2006. Effective Tag Set Selection in Chi-
nese Word Segmentation via Conditional Random
Field Modeling. The 20th Pacific Asia Conference
on Language, Information and Computation, pages
pp.87–94.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer Pfahringer, Peter Reutemann, and
Ian H. Witten. 2009. The WEKA Data Mining
Software: An Update. ACM Special Interest Groups
on Knowledge Discovery and Data Mining Explo-
rations Newsletter, pages 10–18.
Bo Han and Timothy Baldwin. 2011. Lexical Normal-
isation of Short Text Messages: Makn Sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 368–378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically Constructing a Normalisation Dictio-
nary for Microblogs. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 421–432.
Finn V Jensen. 1996. An Introduction to Bayesian
Networks, volume 74.
Fleiss L Joseph. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, pages 378–382.
Catherine Kobus, Franc¸ois Yvon, and G´eraldine
Damnati. 2008. Normalizing SMS: Are Two
Metaphors Better Than One? In International Con-
ference on Computational Linguistics, pages 441–
448.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282–289.
Zhifei Li and David Yarowsky. 2008. Mining and
Modeling Relations between Formal and Informal
Chinese Phrases from Web Corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 1031–1040.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word
Segmentation. Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing.
Sebastian Pad´o, 2006. User’s Guide to SIGF: Signifi-
cance Testing by Approximate Randomisation.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese Segmentation and New Word Detec-
tion Using Conditional Random Fields. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, page 562.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
Word Segmentation Using Unlabeled Data. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 970–979.
Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast
Online Training with Frequency-Adaptive Learn-
ing Rates for Chinese Word Segmentation and New
Word Detection. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, pages 253–262.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic Conditional Ran-
dom Fields: Factorized Probabilistic Models for La-
beling and Segmenting Sequence Data. Journal of
Machine Learning Research, pages 693–723.
Charles Sutton. 2006. GRMM: GRaphical Models in
Mallet. In URL http://mallet. cs. umass. edu/grmm.
</reference>
<page confidence="0.960689">
740
</page>
<reference confidence="0.99983762">
Stephanie Tellex, Boris Katz, Jimmy Lin, Aaron Fer-
nandes, and Gregory Marton. 2003. Quantitative
evaluation of passage retrieval algorithms for ques-
tion answering. In Proceedings of the 26th annual
international ACM SIGIR conference on Research
and Development in Information Retrieval, pages
41–47.
Aobo. Wang, Cong Duy Vu Hoang, and Min-Yen Kan.
2010. Perspectives on Crowdsourcing Annotations
for Natural Language Processing, journal = Lan-
guage Resources and Evaluation. pages 1–23.
Aobo Wang, Tao Chen, and Min-Yen Kan. 2012. Re-
tweeting From A Linguistic Perspective. In Pro-
ceedings of the Second Workshop on Language in
Social Media, pages 46–55.
Kam-Fai Wong and Yunqing Xia. 2008. Normal-
ization of Chinese Chat Language. Language Re-
sources and Evaluation, pages 219–242.
Andi Wu and Zixin Jiang. 2000. Statistically-
Enhanced New Word Identification in A Rule-based
Chinese Aystem. In Proceedings of the second
workshop on Chinese Language Processing, pages
46–51.
Yunqing Xia and Kam-Fai Wong. 2006. Anomaly De-
tecting within Dynamic Chinese Chat Text. NEW
TEXT Wikis and blogs and other dynamic text
sources, page 48.
Yunqing Xia, Kam-Fai Wong, and Wei Gao. 2005.
NIL Is Not Nothing: Recognition of Chinese Net-
work Informal Language Expressions. In 4th
SIGHAN Workshop on Chinese Language Process-
ing, volume 5.
Nianwen Xue. 2003. Chinese Word Segmentation as
Character Tagging. Computational Linguistics and
Chinese Language Processing, pages 29–48.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th conference on Computational
linguistics - Volume 2, pages 947–953.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. HHMM-based Chinese Lexical An-
alyzer ICTCLAS. In Proceedings of the second
SIGHAN workshop on Chinese language process-
ing, pages 184–187.
Hai Zhao and Chunyu Kit. 2008. Unsupervised Seg-
mentation Helps Supervised Learning of Character
Tagging for Word Segmentation and Named Entity
Recognition. In Proceedings of the Sixth SIGHAN
Workshop on Chinese Language Processing, pages
106–111.
</reference>
<page confidence="0.997858">
741
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.446154">
<title confidence="0.8097445">Mining Informal Language from Chinese Joint Word Recognition and Segmentation</title>
<affiliation confidence="0.958498">IR / NLP Group and Digital Media Institute National University of</affiliation>
<address confidence="0.978285">13 Computing Link, Singapore</address>
<abstract confidence="0.992938583333333">We address the problem of informal word recognition in Chinese microblogs. A key problem is the lack of word delimiters in Chinese. We exploit this reliance as an opportunity: recognizing the relation between informal word recognition and Chinese word segmentation, we propose to model the two tasks jointly. Our joint inference method significantly outperforms baseline systems that conduct the tasks individually or sequentially.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Belinda Cao</author>
</authors>
<title>Sina’s weibo outlook buoys internet stock gains: China overnight.</title>
<date>2012</date>
<contexts>
<context position="15218" citStr="Cao, 2012" startWordPosition="2550" endWordPosition="2551">aries: CPMI-N@k+i; CPMI-M@k+i; CDiff@k+i; PYPMI-N@k+i; PYPMI-M@k+i; PYDiff@k+i 3 Experiment We discuss the dataset, baseline systems and experiments results in detail in the following. 3.1 Data Preparation We utilize the Chinese social media archive, PrEV (Cui et al., 2012), to obtain Chinese mi5The informal word may have the same Pinyin transcription as its formal counterpart without considering the differences in tones. 734 croblog posts from the public timeline of Sina Weibo6. Sina Weibo is the largest microblogging in China, where over 100 million Chinese microblog posts are posted daily (Cao, 2012), likely the largest public source of informal and daily Chinese language use. Our dataset has a total of 6,678,021 messages, covering two months from June to July of 2011. To annotate the corpus, we employ Zhubajie7, one of China mainland’s largest crowdsourcing (Wang et al., 2010) platforms to obtain informal word annotations. In total, we spent US$110 on assembling a subset of 5,500 posts (12,446 sentences) in which 1, 658 unique informal words are annotated within five weeks via Zhubajie. Each post was annotated by three annotators with moderate (0.57) inter-annotator agreement measured by</context>
</contexts>
<marker>Cao, 2012</marker>
<rawString>Belinda Cao. 2012. Sina’s weibo outlook buoys internet stock gains: China overnight.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>27--1</pages>
<contexts>
<context position="18989" citStr="Chang and Lin, 2011" startWordPosition="3147" endWordPosition="3150">(hereafter denoted as LCRF&gt;-LCRF-UB and FCRF-UB) are meant for reference only, as they have access to answers for the opposing tasks. Adapted SVM for Joint Classification. For completeness, we also compared our work against the standard SVM classification model that performs both tasks by predicting the cross-product of the CWS and IWR individual classes (in total, 8 classes). We train the SVM classifier on the same set of features as the FCRF, by providing the cross-product of two layer labels as gold labels. This system (hereafter denoted as SVM-JC) was implemented using the LibSVM package (Chang and Lin, 2011). 3.3 Evaluation Metrics We use the standard metrics of precision, recall and F1 for the IWR task. Only words that exactly match the manually-annotated labels are considered correct. For example given the sentence “� HËH0nLpfv!” (“�H7kH0nLpfv!”; “How delicious it is”), if the IWR component identifies “Ë H” as an informal word, it will be considered correct, whereas both “ËH0” and “Ë” are deemed incorrect. For CWS evaluation, we employ the conventional scoring script provided in SIGHAN9http://www.ictclas.org/index.html 735 5, which also provides out-of-vocabulary recall (OOVR). To determine sta</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, pages 27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Ming-Hong Bai</author>
</authors>
<title>Unknown Word Detection for Chinese by a CorpusBased Learning Method.</title>
<date>1998</date>
<booktitle>International Journal of Computational Linguistics and Chinese Language Processing,</booktitle>
<pages>27--44</pages>
<contexts>
<context position="34125" citStr="Chen and Bai, 1998" startWordPosition="5608" endWordPosition="5611">otstrap 500 informal/formal word pairs by using manually-tuned queries to find definition sentences on the Web. The resulting noisy list is further re-ranked based on n-gram co-occurrence. However, their method makes a basic assumption that informal/formal word pairs co-occur within a definition sentence (i.e., “&lt;informal word&gt; means &lt;formal word&gt;”) may not hold in microblog data, as microbloggers largely do not define the words they use. Closely related to our work is the task of Chinese new word detection, normally treated as a separate process from word segmentation in most previous works (Chen and Bai, 1998; Wu and Jiang, 2000; Chen and Ma, 2002; Gao et al., 2005). Aiming to improve both tasks, work by Peng et al. (2004) and Sun et al. (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. This is a weakness as their linear CRF model requires re-training. Their method also requires thresholds to be set through heuristic tuning, as to whether the segmented words are indeed new words. We note that the task of new word detection refers to out-of-vocabulary (OOV) detection, and is distinctly different from IWR (new words could be both formal or informal</context>
</contexts>
<marker>Chen, Bai, 1998</marker>
<rawString>Keh-Jiann Chen and Ming-Hong Bai. 1998. Unknown Word Detection for Chinese by a CorpusBased Learning Method. International Journal of Computational Linguistics and Chinese Language Processing, pages 27–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Wei-Yun Ma</author>
</authors>
<title>Unknown Word Extraction for Chinese Documents.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="34164" citStr="Chen and Ma, 2002" startWordPosition="5616" endWordPosition="5619">y using manually-tuned queries to find definition sentences on the Web. The resulting noisy list is further re-ranked based on n-gram co-occurrence. However, their method makes a basic assumption that informal/formal word pairs co-occur within a definition sentence (i.e., “&lt;informal word&gt; means &lt;formal word&gt;”) may not hold in microblog data, as microbloggers largely do not define the words they use. Closely related to our work is the task of Chinese new word detection, normally treated as a separate process from word segmentation in most previous works (Chen and Bai, 1998; Wu and Jiang, 2000; Chen and Ma, 2002; Gao et al., 2005). Aiming to improve both tasks, work by Peng et al. (2004) and Sun et al. (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. This is a weakness as their linear CRF model requires re-training. Their method also requires thresholds to be set through heuristic tuning, as to whether the segmented words are indeed new words. We note that the task of new word detection refers to out-of-vocabulary (OOV) detection, and is distinctly different from IWR (new words could be both formal or informal words). 6 Conclusion There is a close </context>
</contexts>
<marker>Chen, Ma, 2002</marker>
<rawString>Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown Word Extraction for Chinese Documents. In Proceedings of the 19th international conference on Computational linguistics, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word Association Norms, Mutual Information, and Lexicography. Computional Linguistic,</title>
<date>1990</date>
<pages>22--29</pages>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word Association Norms, Mutual Information, and Lexicography. Computional Linguistic, pages 22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anqi Cui</author>
<author>Liner Yang</author>
<author>Dejun Hou</author>
<author>Min-Yen Kan</author>
<author>Yiqun Liu</author>
<author>Min Zhang</author>
<author>Shaoping Ma</author>
</authors>
<title>PrEV: Preservation Explorer and Vault for Web 2.0 User-Generated Content. Theory and Practice of Digital Libraries,</title>
<date>2012</date>
<pages>101--112</pages>
<contexts>
<context position="14882" citStr="Cui et al., 2012" startWordPosition="2492" endWordPosition="2495">ars in the microblog domain, but not in news. If smoothing is conducted, the character bigram “rp” will be given a non-zero probability in both domains, not reflective of actual use. For each character CZ, we incorporate the PMI of the character bigrams as follows: • (*) If CkCk+1 (i − 4 &lt; k &lt; i + 4) is not a Chinese word recorded in dictionaries: CPMI-N@k+i; CPMI-M@k+i; CDiff@k+i; PYPMI-N@k+i; PYPMI-M@k+i; PYDiff@k+i 3 Experiment We discuss the dataset, baseline systems and experiments results in detail in the following. 3.1 Data Preparation We utilize the Chinese social media archive, PrEV (Cui et al., 2012), to obtain Chinese mi5The informal word may have the same Pinyin transcription as its formal counterpart without considering the differences in tones. 734 croblog posts from the public timeline of Sina Weibo6. Sina Weibo is the largest microblogging in China, where over 100 million Chinese microblog posts are posted daily (Cao, 2012), likely the largest public source of informal and daily Chinese language use. Our dataset has a total of 6,678,021 messages, covering two months from June to July of 2011. To annotate the corpus, we employ Zhubajie7, one of China mainland’s largest crowdsourcing </context>
</contexts>
<marker>Cui, Yang, Hou, Kan, Liu, Zhang, Ma, 2012</marker>
<rawString>Anqi Cui, Liner Yang, Dejun Hou, Min-Yen Kan, Yiqun Liu, Min Zhang, and Shaoping Ma. 2012. PrEV: Preservation Explorer and Vault for Web 2.0 User-Generated Content. Theory and Practice of Digital Libraries, pages 101–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Andi Wu</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach. Computitional Linguistic,</title>
<date>2005</date>
<pages>531--574</pages>
<contexts>
<context position="31082" citStr="Gao et al., 2005" startWordPosition="5134" endWordPosition="5137">s exemplified in Table 6; i.e., person names in the form of user IDs and nicknames, that have less constraint on form in terms of length, canonical structure (not surnames with given names; as is standard in Chinese names) and may mix alphabetic characters. Most of these belong to the category of Person Name (PER), as defined in CoNLL-200311 Named Entity Recognition shared task. Such freestyle entities are often misrecognized as informal words, as they share some of the same stylistic markings, and are not marked by features used to recognize previous Chinese named entity recognition methods (Gao et al., 2005; Zhao and Kit, 2008) that work on news or general domain text. We recognize this as a challenge in Chinese microtext, but beyond the scope of our current work. 5 Related Work In English, IWR has typically been investigated alongside normalization. Several recent works (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) aim to produce informal/formal word lexicons and mappings. These works are based on distributional similarity and string similarity that address concerns of lexical variation and spelling. These methods propose two-step unsupervised approaches to first detect and then</context>
<context position="34183" citStr="Gao et al., 2005" startWordPosition="5620" endWordPosition="5623">ned queries to find definition sentences on the Web. The resulting noisy list is further re-ranked based on n-gram co-occurrence. However, their method makes a basic assumption that informal/formal word pairs co-occur within a definition sentence (i.e., “&lt;informal word&gt; means &lt;formal word&gt;”) may not hold in microblog data, as microbloggers largely do not define the words they use. Closely related to our work is the task of Chinese new word detection, normally treated as a separate process from word segmentation in most previous works (Chen and Bai, 1998; Wu and Jiang, 2000; Chen and Ma, 2002; Gao et al., 2005). Aiming to improve both tasks, work by Peng et al. (2004) and Sun et al. (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. This is a weakness as their linear CRF model requires re-training. Their method also requires thresholds to be set through heuristic tuning, as to whether the segmented words are indeed new words. We note that the task of new word detection refers to out-of-vocabulary (OOV) detection, and is distinctly different from IWR (new words could be both formal or informal words). 6 Conclusion There is a close dependency between </context>
</contexts>
<marker>Gao, Li, Wu, Huang, 2005</marker>
<rawString>Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang. 2005. Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach. Computitional Linguistic, pages 531–574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Donald Metzler</author>
<author>Congxing Cai</author>
<author>Eduard Hovy</author>
</authors>
<title>Contextual Bearing on Linguistic Variation in Social Media.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Language in Social Media,</booktitle>
<pages>20--29</pages>
<contexts>
<context position="31394" citStr="Gouws et al., 2011" startWordPosition="5187" endWordPosition="5190">(PER), as defined in CoNLL-200311 Named Entity Recognition shared task. Such freestyle entities are often misrecognized as informal words, as they share some of the same stylistic markings, and are not marked by features used to recognize previous Chinese named entity recognition methods (Gao et al., 2005; Zhao and Kit, 2008) that work on news or general domain text. We recognize this as a challenge in Chinese microtext, but beyond the scope of our current work. 5 Related Work In English, IWR has typically been investigated alongside normalization. Several recent works (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) aim to produce informal/formal word lexicons and mappings. These works are based on distributional similarity and string similarity that address concerns of lexical variation and spelling. These methods propose two-step unsupervised approaches to first detect and then normalize detected informal words using dictionaries. In processing Chinese informal language, work conducted by Xia and Wong address the problem 11http://www.cnts.ua.ac.be/conll2003/ ner/ 738 of in bulletin board system (BBS) chats. They employ pattern matching and SVM-based classification to recognize Chines</context>
</contexts>
<marker>Gouws, Metzler, Cai, Hovy, 2011</marker>
<rawString>Stephan Gouws, Donald Metzler, Congxing Cai, and Eduard Hovy. 2011. Contextual Bearing on Linguistic Variation in Social Media. In Proceedings of the Workshop on Language in Social Media, pages 20–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhao Hai</author>
<author>Huang Chang-Ning</author>
<author>Li Mu</author>
<author>Lu BaoLiang</author>
</authors>
<title>Effective Tag Set Selection in Chinese Word Segmentation via Conditional Random Field Modeling.</title>
<date>2006</date>
<booktitle>The 20th Pacific Asia Conference on Language, Information and Computation,</booktitle>
<pages>87--94</pages>
<contexts>
<context position="5842" citStr="Hai et al., 2006" startWordPosition="899" endWordPosition="902">sis and limitations. We discuss related work in Section 5, before concluding our paper. 2 Methodology Given an input Chinese microblog post, our method simultaneously segments the sentences into words (the Chinese Word Segmentation, CWS, task), and marks the component words as informal or formal ones (the Informal Word Recongition, IWR, task). 2.1 Problem Formalization The two tasks are simple to formalize. The IWR task labels each Chinese character with either an F (part of a formal word) or IF (informal word). For the CWS task, we follow the widely-used BIES coding scheme (Low et al., 2005; Hai et al., 2006), where B, I, E and S stand for beginning of a word, inside a word, end of a word and singlecharacter word, respectively. As a result, we have two (hidden) labels to associate with each (observable) character. Figure 1 illustrates an example microblog post graphically, where the labels are in circles and the observations are in squares. The two informal words in the example post are “(V” (normalized form: “&amp;V”; English gloss: “no”) and “rp” (“ºPPI”; “luck”). 2.2 Conditional Random Field Models Given the general performance and discriminative framework, Conditional Random Fields (CRFs) (Laffert</context>
</contexts>
<marker>Hai, Chang-Ning, Mu, BaoLiang, 2006</marker>
<rawString>Zhao Hai, Huang Chang-Ning, Li Mu, and Lu BaoLiang. 2006. Effective Tag Set Selection in Chinese Word Segmentation via Conditional Random Field Modeling. The 20th Pacific Asia Conference on Language, Information and Computation, pages pp.87–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>ACM Special Interest Groups on Knowledge Discovery and Data Mining Explorations Newsletter,</journal>
<pages>10--18</pages>
<contexts>
<context position="17080" citStr="Hall et al., 2009" startWordPosition="2852" endWordPosition="2855">t. 3.2 Baseline Systems We implemented several baseline systems to compare with proposed FCRF joint inference method. Existing Systems. We re-implemented Xia and Wong (2008)’s extended Support Vector Machine (SVM) based microtext IWR system to compare with our method. Their system only does IWR, using the CWS and POS tagging otuput of the ICTCLAS segmenter (Zhang et al., 2003) as input. To compare our joint inference versus other learning models, we also employed a decision tree (DT) learner, equipped with the same feature set as our FCRF. Both the SVM and DT models are provided by the Weka3 (Hall et al., 2009) toolkit, using its default configuration. To evaluate CWS performance, we compare with two recent segmenters. Sun and Xu (2011)’s 6http://open.weibo.com 7http://www.zhubajie.com 8http://www.sighan.org work achieves state-of-the-art performance and is publicly available. They employ a LCRF taking as input both lexical and statistical features derived from unlabeled data. As a second baseline, we also evaluate against a widelyused, commercially-available alternative, the recently released 2011 ICTCLAS segmenter9. Two-stage Sequential Systems. To benchmark the improvement that the factorial CRF </context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. ACM Special Interest Groups on Knowledge Discovery and Data Mining Explorations Newsletter, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical Normalisation of Short Text Messages: Makn Sens a #twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>368--378</pages>
<contexts>
<context position="1725" citStr="Han and Baldwin, 2011" startWordPosition="257" endWordPosition="260">sing is difficult to process. One key reason for this difficulty is the ubiquitous presence of informal words – anomalous terms that manifest as ad hoc abbreviations, neologisms, unconventional spellings and phonetic substitutions. Such informality is often present in oral conversation, and user-generated microblogs reflect this informality. Natural language processing (NLP) tools largely fail to work properly on microtext, as they have largely been trained on formally written text (i.e., newswire). Recent work has started to address these shortcomings (Xia and Wong, 2006; Kobus et al., 2008; Han and Baldwin, 2011). Informal words and their usage in microtext evolves quickly, following social trends and news events. ∗This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office. These characteristics make it difficult for lexicographers to compile lexica to keep with the pace of language change. We focus on this problem in the Chinese language. Through our analysis of a gathered Chinese microblog corpus, we observe that Chinese informal words originate from three primary sources</context>
<context position="31374" citStr="Han and Baldwin, 2011" startWordPosition="5183" endWordPosition="5186">ategory of Person Name (PER), as defined in CoNLL-200311 Named Entity Recognition shared task. Such freestyle entities are often misrecognized as informal words, as they share some of the same stylistic markings, and are not marked by features used to recognize previous Chinese named entity recognition methods (Gao et al., 2005; Zhao and Kit, 2008) that work on news or general domain text. We recognize this as a challenge in Chinese microtext, but beyond the scope of our current work. 5 Related Work In English, IWR has typically been investigated alongside normalization. Several recent works (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) aim to produce informal/formal word lexicons and mappings. These works are based on distributional similarity and string similarity that address concerns of lexical variation and spelling. These methods propose two-step unsupervised approaches to first detect and then normalize detected informal words using dictionaries. In processing Chinese informal language, work conducted by Xia and Wong address the problem 11http://www.cnts.ua.ac.be/conll2003/ ner/ 738 of in bulletin board system (BBS) chats. They employ pattern matching and SVM-based classification</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical Normalisation of Short Text Messages: Makn Sens a #twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 368–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatically Constructing a Normalisation Dictionary for Microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>421--432</pages>
<contexts>
<context position="31413" citStr="Han et al., 2012" startWordPosition="5191" endWordPosition="5194"> CoNLL-200311 Named Entity Recognition shared task. Such freestyle entities are often misrecognized as informal words, as they share some of the same stylistic markings, and are not marked by features used to recognize previous Chinese named entity recognition methods (Gao et al., 2005; Zhao and Kit, 2008) that work on news or general domain text. We recognize this as a challenge in Chinese microtext, but beyond the scope of our current work. 5 Related Work In English, IWR has typically been investigated alongside normalization. Several recent works (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) aim to produce informal/formal word lexicons and mappings. These works are based on distributional similarity and string similarity that address concerns of lexical variation and spelling. These methods propose two-step unsupervised approaches to first detect and then normalize detected informal words using dictionaries. In processing Chinese informal language, work conducted by Xia and Wong address the problem 11http://www.cnts.ua.ac.be/conll2003/ ner/ 738 of in bulletin board system (BBS) chats. They employ pattern matching and SVM-based classification to recognize Chinese informal sentence</context>
</contexts>
<marker>Han, Cook, Baldwin, 2012</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically Constructing a Normalisation Dictionary for Microblogs. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 421–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn V Jensen</author>
</authors>
<title>An Introduction to Bayesian Networks,</title>
<date>1996</date>
<volume>74</volume>
<contexts>
<context position="9093" citStr="Jensen, 1996" startWordPosition="1476" endWordPosition="1477"> the two types of CRFs used in this work. yt denotes the 1st layer label, zt denotes the 2nd layer label, and xt denotes the observation sequence. Although the FCRF can be collapsed into a LCRF whose state space is the cross-product of the outcomes of the state variables (i.e., 8 labels in this case), Sutton et al. (2007) noted that such a LCRF requires not only more parameters in the number of variables, but also more training data to achieve equivalent performance with an FCRF. Given the limited scale of the state space and training data, we follow the FCRF model, using exact Junction Tree (Jensen, 1996) inference and decoding algorithm to perform prediction. 2.3 CRF Features We use three broad feature classes – lexical, dictionary-based and statistical features – aiming to distinguish the output classes for the CWS and IRW problems. Character-based sequence labeling is employed for word segmentation due to its simplicity and robustness to the unknown word problem (Xue, 2003). A key contribution of our work is also to propose novel features for joint inference. We propose new features for the dictionary-based and statistical feature classes, which we have marked in the discussion below with “</context>
</contexts>
<marker>Jensen, 1996</marker>
<rawString>Finn V Jensen. 1996. An Introduction to Bayesian Networks, volume 74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fleiss L Joseph</author>
</authors>
<title>Measuring nominal scale agreement among many raters. Psychological bulletin,</title>
<date>1971</date>
<pages>378--382</pages>
<contexts>
<context position="15843" citStr="Joseph, 1971" startWordPosition="2651" endWordPosition="2652"> largest public source of informal and daily Chinese language use. Our dataset has a total of 6,678,021 messages, covering two months from June to July of 2011. To annotate the corpus, we employ Zhubajie7, one of China mainland’s largest crowdsourcing (Wang et al., 2010) platforms to obtain informal word annotations. In total, we spent US$110 on assembling a subset of 5,500 posts (12,446 sentences) in which 1, 658 unique informal words are annotated within five weeks via Zhubajie. Each post was annotated by three annotators with moderate (0.57) inter-annotator agreement measured by Fleiss’ n (Joseph, 1971), and conflicts were resolved by majority voting. We divided the annotated corpus, taking 4, 000 posts for training, and the remainder (1, 500) for testing. Through inspection, we note that 79.8% of the informal words annotated in the testing set are not covered by the training set. We also follow Wang et al. (2012)’s conventions and apply rulesets to preprocess the corpus’ URLs, emoticons, “@usernames” and Hashtags as pre-segmented words, before input to CWS and IWR. For the CWS task, the first author manually labelled the same corpus following the segmentation guidelines published with the S</context>
</contexts>
<marker>Joseph, 1971</marker>
<rawString>Fleiss L Joseph. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, pages 378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Kobus</author>
<author>Franc¸ois Yvon</author>
<author>G´eraldine Damnati</author>
</authors>
<title>Normalizing SMS: Are Two Metaphors Better Than One?</title>
<date>2008</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<pages>441--448</pages>
<contexts>
<context position="1701" citStr="Kobus et al., 2008" startWordPosition="253" endWordPosition="256">ge, microtext processing is difficult to process. One key reason for this difficulty is the ubiquitous presence of informal words – anomalous terms that manifest as ad hoc abbreviations, neologisms, unconventional spellings and phonetic substitutions. Such informality is often present in oral conversation, and user-generated microblogs reflect this informality. Natural language processing (NLP) tools largely fail to work properly on microtext, as they have largely been trained on formally written text (i.e., newswire). Recent work has started to address these shortcomings (Xia and Wong, 2006; Kobus et al., 2008; Han and Baldwin, 2011). Informal words and their usage in microtext evolves quickly, following social trends and news events. ∗This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office. These characteristics make it difficult for lexicographers to compile lexica to keep with the pace of language change. We focus on this problem in the Chinese language. Through our analysis of a gathered Chinese microblog corpus, we observe that Chinese informal words originate fr</context>
</contexts>
<marker>Kobus, Yvon, Damnati, 2008</marker>
<rawString>Catherine Kobus, Franc¸ois Yvon, and G´eraldine Damnati. 2008. Normalizing SMS: Are Two Metaphors Better Than One? In International Conference on Computational Linguistics, pages 441– 448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="6457" citStr="Lafferty et al., 2001" startWordPosition="1000" endWordPosition="1003">, 2006), where B, I, E and S stand for beginning of a word, inside a word, end of a word and singlecharacter word, respectively. As a result, we have two (hidden) labels to associate with each (observable) character. Figure 1 illustrates an example microblog post graphically, where the labels are in circles and the observations are in squares. The two informal words in the example post are “(V” (normalized form: “&amp;V”; English gloss: “no”) and “rp” (“ºPPI”; “luck”). 2.2 Conditional Random Field Models Given the general performance and discriminative framework, Conditional Random Fields (CRFs) (Lafferty et al., 2001) is a suitable framework for tackling sequence labeling problems. Other alternative frameworks such as Markov Logic Networks (MLNs) and Integer Linear Programming (ILP) could also be considered. However, we feel that for this task, formulating efficient global formulas (constraints) for MLN (ILP) is comparatively less straightforward than in other tasks (e.g, compared to Semantic Role Labeling, where the rules may come directly from grammatical constraints). CRFs represent a basic, simple and well-understood framework for sequence labeling, making it a suitable framework for adapting to perfor</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>David Yarowsky</author>
</authors>
<title>Mining and Modeling Relations between Formal and Informal Chinese Phrases from Web Corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1031--1040</pages>
<contexts>
<context position="33479" citStr="Li and Yarowsky (2008)" startWordPosition="5508" endWordPosition="5511">in 12,446 = 13% of sentences, as opposed to 651 1,658 22,400 = 2% in their BBS corpus). Further analysis of their corpus reveals that phonetic substitution is the primary origin of informal words in their corpus – 99.2% as reported in (Wong and Xia, 2008). In contrast, the origin for informal words in microblogs is more varied, where phonetic substitutions abbreviations and neologisms, account for 53.1%, 21.4% and 18.7% of the informal word types, respectively. Their method is best suited for phonetic substitution, thus performing well on their corpus but poorly on ours. More closely related, Li and Yarowsky (2008) tackle Chinese IWR. They bootstrap 500 informal/formal word pairs by using manually-tuned queries to find definition sentences on the Web. The resulting noisy list is further re-ranked based on n-gram co-occurrence. However, their method makes a basic assumption that informal/formal word pairs co-occur within a definition sentence (i.e., “&lt;informal word&gt; means &lt;formal word&gt;”) may not hold in microblog data, as microbloggers largely do not define the words they use. Closely related to our work is the task of Chinese new word detection, normally treated as a separate process from word segmentat</context>
</contexts>
<marker>Li, Yarowsky, 2008</marker>
<rawString>Zhifei Li and David Yarowsky. 2008. Mining and Modeling Relations between Formal and Informal Chinese Phrases from Web Corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1031–1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A Maximum Entropy Approach to Chinese Word Segmentation.</title>
<date>2005</date>
<booktitle>Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="5823" citStr="Low et al., 2005" startWordPosition="895" endWordPosition="898">ion on error analysis and limitations. We discuss related work in Section 5, before concluding our paper. 2 Methodology Given an input Chinese microblog post, our method simultaneously segments the sentences into words (the Chinese Word Segmentation, CWS, task), and marks the component words as informal or formal ones (the Informal Word Recongition, IWR, task). 2.1 Problem Formalization The two tasks are simple to formalize. The IWR task labels each Chinese character with either an F (part of a formal word) or IF (informal word). For the CWS task, we follow the widely-used BIES coding scheme (Low et al., 2005; Hai et al., 2006), where B, I, E and S stand for beginning of a word, inside a word, end of a word and singlecharacter word, respectively. As a result, we have two (hidden) labels to associate with each (observable) character. Figure 1 illustrates an example microblog post graphically, where the labels are in circles and the observations are in squares. The two informal words in the example post are “(V” (normalized form: “&amp;V”; English gloss: “no”) and “rp” (“ºPPI”; “luck”). 2.2 Conditional Random Field Models Given the general performance and discriminative framework, Conditional Random Fie</context>
<context position="9902" citStr="Low et al., 2005" startWordPosition="1601" endWordPosition="1604">tput classes for the CWS and IRW problems. Character-based sequence labeling is employed for word segmentation due to its simplicity and robustness to the unknown word problem (Xue, 2003). A key contribution of our work is also to propose novel features for joint inference. We propose new features for the dictionary-based and statistical feature classes, which we have marked in the discussion below with “(*)”. We later examine their efficacy in Section 3. Lexical Features. As a foundation, we employ lexical (n-gram) features informed by the previous state-of-the-art for CWS (Sun and Xu, 2011; Low et al., 2005). These features are listed below2: • Character 1-gram: Ck(i − 4 &lt; k &lt; i + 4) • Character 2-gram: CkCk+1(i − 4 &lt; k &lt; i + 3) • Character 3-gram: CkCk+1Ck+2(i − 3 &lt; k &lt; i + 2) • Character lexicon: C−1C1 This feature is used to capture the common indicators in Chinese interrogative sentences. (e.g., “ATAq” (“whether or not”), “0TN0” (“OK or not”)) • Whether Ck and Ck+1 are identical, for i − 4 &lt; k &lt; i + 3. This feature is used to capture the words of employing character doubling in Chinese. (e.g., “� �” (“see you”), “R R” (“ every day”)) J Dictionary-based Features. We use features that indicate </context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A Maximum Entropy Approach to Chinese Word Segmentation. Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
</authors>
<title>User’s Guide to SIGF: Significance Testing by Approximate Randomisation.</title>
<date>2006</date>
<marker>Pad´o, 2006</marker>
<rawString>Sebastian Pad´o, 2006. User’s Guide to SIGF: Significance Testing by Approximate Randomisation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese Segmentation and New Word Detection Using Conditional Random Fields.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>562</pages>
<contexts>
<context position="34241" citStr="Peng et al. (2004)" startWordPosition="5631" endWordPosition="5634">resulting noisy list is further re-ranked based on n-gram co-occurrence. However, their method makes a basic assumption that informal/formal word pairs co-occur within a definition sentence (i.e., “&lt;informal word&gt; means &lt;formal word&gt;”) may not hold in microblog data, as microbloggers largely do not define the words they use. Closely related to our work is the task of Chinese new word detection, normally treated as a separate process from word segmentation in most previous works (Chen and Bai, 1998; Wu and Jiang, 2000; Chen and Ma, 2002; Gao et al., 2005). Aiming to improve both tasks, work by Peng et al. (2004) and Sun et al. (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. This is a weakness as their linear CRF model requires re-training. Their method also requires thresholds to be set through heuristic tuning, as to whether the segmented words are indeed new words. We note that the task of new word detection refers to out-of-vocabulary (OOV) detection, and is distinctly different from IWR (new words could be both formal or informal words). 6 Conclusion There is a close dependency between Chinese word segmentation (CWS) and informal word recognit</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese Segmentation and New Word Detection Using Conditional Random Fields. In Proceedings of the 20th international conference on Computational Linguistics, page 562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Jia Xu</author>
</authors>
<title>Enhancing Chinese Word Segmentation Using Unlabeled Data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>970--979</pages>
<contexts>
<context position="7308" citStr="Sun and Xu, 2011" startWordPosition="1136" endWordPosition="1139">rmulating efficient global formulas (constraints) for MLN (ILP) is comparatively less straightforward than in other tasks (e.g, compared to Semantic Role Labeling, where the rules may come directly from grammatical constraints). CRFs represent a basic, simple and well-understood framework for sequence labeling, making it a suitable framework for adapting to perform joint inference. 2.2.1 Linear-Chain CRF A linear-chain CRF (LCRF; Figure 2a) predicts the output label based on feature functions provided by the scientist on the input. In fact, the LCRF has been used for the exact problem of CWS (Sun and Xu, 2011), garnering state-of-theart performance, and as such, validate it as a strong baseline for comparison. 2.2.2 Factorial CRF To properly model the interplay between the two sub-problems, we employ the factorial CRF (FCRF) model, which is based on the dynamic CRF (DCRF) (Sutton et al., 2007). By introducing a pairwise factor between different variables at each position, the FCRF model results as a special case of the DCRF. A FCRF captures the joint distribution among various layers and jointly predicts across layers. Figure 2 illustrates both the LCRF and FCRF models, where cliques include within</context>
<context position="9883" citStr="Sun and Xu, 2011" startWordPosition="1597" endWordPosition="1600">distinguish the output classes for the CWS and IRW problems. Character-based sequence labeling is employed for word segmentation due to its simplicity and robustness to the unknown word problem (Xue, 2003). A key contribution of our work is also to propose novel features for joint inference. We propose new features for the dictionary-based and statistical feature classes, which we have marked in the discussion below with “(*)”. We later examine their efficacy in Section 3. Lexical Features. As a foundation, we employ lexical (n-gram) features informed by the previous state-of-the-art for CWS (Sun and Xu, 2011; Low et al., 2005). These features are listed below2: • Character 1-gram: Ck(i − 4 &lt; k &lt; i + 4) • Character 2-gram: CkCk+1(i − 4 &lt; k &lt; i + 3) • Character 3-gram: CkCk+1Ck+2(i − 3 &lt; k &lt; i + 2) • Character lexicon: C−1C1 This feature is used to capture the common indicators in Chinese interrogative sentences. (e.g., “ATAq” (“whether or not”), “0TN0” (“OK or not”)) • Whether Ck and Ck+1 are identical, for i − 4 &lt; k &lt; i + 3. This feature is used to capture the words of employing character doubling in Chinese. (e.g., “� �” (“see you”), “R R” (“ every day”)) J Dictionary-based Features. We use feat</context>
<context position="17208" citStr="Sun and Xu (2011)" startWordPosition="2871" endWordPosition="2874">stems. We re-implemented Xia and Wong (2008)’s extended Support Vector Machine (SVM) based microtext IWR system to compare with our method. Their system only does IWR, using the CWS and POS tagging otuput of the ICTCLAS segmenter (Zhang et al., 2003) as input. To compare our joint inference versus other learning models, we also employed a decision tree (DT) learner, equipped with the same feature set as our FCRF. Both the SVM and DT models are provided by the Weka3 (Hall et al., 2009) toolkit, using its default configuration. To evaluate CWS performance, we compare with two recent segmenters. Sun and Xu (2011)’s 6http://open.weibo.com 7http://www.zhubajie.com 8http://www.sighan.org work achieves state-of-the-art performance and is publicly available. They employ a LCRF taking as input both lexical and statistical features derived from unlabeled data. As a second baseline, we also evaluate against a widelyused, commercially-available alternative, the recently released 2011 ICTCLAS segmenter9. Two-stage Sequential Systems. To benchmark the improvement that the factorial CRF model has by doing the two tasks jointly, we compare with a LCRF solution that chains these two tasks together. For completeness</context>
<context position="20991" citStr="Sun and Xu (2011)" startWordPosition="3463" endWordPosition="3466">e joint inference method effective? RQ5 Is there a significant difference between the performance of the joint inference of a crossproduct SVM and our proposed FCRF? 3.4.1 CWS Performance Table 2: Performance comparison on the CWS task. The two bottom-most rows show upper bound performance. ‘‡’(‘∗’) in the top four lines indicates statistical significance at p &lt; 0.001 (0.05) when compared with the previous row. Symbols in the bottom two lines indicate significant difference between upper bound systems and their corresponding counterparts. Pre Rec Fl OOVR ICTCLAS (2003) 0.640 0.767 0.698 0.551 Sun and Xu (2011) 0.661‡ 0.691‡ 0.675 0.572‡ LCRFiwr&gt;-LCRFcws 0.741‡ 0.775‡ 0.758∗ 0.607∗ FCRF 0.757‡ 0.801‡ 0.778∗ 0.633∗ LCRFiwr&gt;-LCRFcws-UB 0.807‡ 0.815‡ 0.811∗ 0.731‡ FCRF-UB 0.820‡ 0.833‡ 0.826∗ 0.758‡ In general, our FCRF yields the best performance among all systems (top portion of Table 2), answering RQ1. Given microblog posts as test data, the F1 of ICTCLAS drops from 0.98510 to 0.698, clearly showing the difficulty of processing microtext. The sequential LCRF model and FCRF model both outperform the baselines, which means with the novel features shared by the two tasks, CWS benefits significantly fro</context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Weiwei Sun and Jia Xu. 2011. Enhancing Chinese Word Segmentation Using Unlabeled Data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 970–979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Houfeng Wang</author>
<author>Wenjie Li</author>
</authors>
<title>Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>253--262</pages>
<contexts>
<context position="34263" citStr="Sun et al. (2012)" startWordPosition="5636" endWordPosition="5639"> further re-ranked based on n-gram co-occurrence. However, their method makes a basic assumption that informal/formal word pairs co-occur within a definition sentence (i.e., “&lt;informal word&gt; means &lt;formal word&gt;”) may not hold in microblog data, as microbloggers largely do not define the words they use. Closely related to our work is the task of Chinese new word detection, normally treated as a separate process from word segmentation in most previous works (Chen and Bai, 1998; Wu and Jiang, 2000; Chen and Ma, 2002; Gao et al., 2005). Aiming to improve both tasks, work by Peng et al. (2004) and Sun et al. (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. This is a weakness as their linear CRF model requires re-training. Their method also requires thresholds to be set through heuristic tuning, as to whether the segmented words are indeed new words. We note that the task of new word detection refers to out-of-vocabulary (OOV) detection, and is distinctly different from IWR (new words could be both formal or informal words). 6 Conclusion There is a close dependency between Chinese word segmentation (CWS) and informal word recognition (IWR). To leverage</context>
</contexts>
<marker>Sun, Wang, Li, 2012</marker>
<rawString>Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 253–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
<author>Khashayar Rohanimanesh</author>
</authors>
<title>Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>693--723</pages>
<contexts>
<context position="7597" citStr="Sutton et al., 2007" startWordPosition="1182" endWordPosition="1185">ork for sequence labeling, making it a suitable framework for adapting to perform joint inference. 2.2.1 Linear-Chain CRF A linear-chain CRF (LCRF; Figure 2a) predicts the output label based on feature functions provided by the scientist on the input. In fact, the LCRF has been used for the exact problem of CWS (Sun and Xu, 2011), garnering state-of-theart performance, and as such, validate it as a strong baseline for comparison. 2.2.2 Factorial CRF To properly model the interplay between the two sub-problems, we employ the factorial CRF (FCRF) model, which is based on the dynamic CRF (DCRF) (Sutton et al., 2007). By introducing a pairwise factor between different variables at each position, the FCRF model results as a special case of the DCRF. A FCRF captures the joint distribution among various layers and jointly predicts across layers. Figure 2 illustrates both the LCRF and FCRF models, where cliques include within-chain edges (e.g., yt, yt+1) in both LCRF and FCRF models, and the between-chain edges (e.g., yt, zt) only in the FCRF. 732 开 F F F IF IF F F F F IF IF F F F B I E B E B I E S B I E S S 发 区 木 有 出 租 车 , r p 值 低 啊 开 发 区 没 有 出 租 车 , 人 品 值 低 啊 There is no taxi in the development zone , how p</context>
</contexts>
<marker>Sutton, McCallum, Rohanimanesh, 2007</marker>
<rawString>Charles Sutton, Andrew McCallum, and Khashayar Rohanimanesh. 2007. Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data. Journal of Machine Learning Research, pages 693–723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
</authors>
<title>GRMM: GRaphical Models in Mallet.</title>
<date>2006</date>
<booktitle>In URL</booktitle>
<note>http://mallet. cs. umass. edu/grmm.</note>
<contexts>
<context position="17994" citStr="Sutton, 2006" startWordPosition="2984" endWordPosition="2985">input both lexical and statistical features derived from unlabeled data. As a second baseline, we also evaluate against a widelyused, commercially-available alternative, the recently released 2011 ICTCLAS segmenter9. Two-stage Sequential Systems. To benchmark the improvement that the factorial CRF model has by doing the two tasks jointly, we compare with a LCRF solution that chains these two tasks together. For completeness, we test pipelining in both directions – CWS feeding features for IWR (LCRFcws&gt;-LCRFiwr), and the reverse (LCRFiwr&gt;-LCRFcws). We modify the opensource Mallet GRMM package (Sutton, 2006) to implement both this sequential LCRF model and our proposed FCRF model. Both models take the whole feature set described in Section 2.3. Upper Bound Systems. To measure the upperbound achievable with perfect support from the complementary task, we also provided gold standard labels of one task (e.g., IWR) as an input feature to the other task (e.g., CWS). These systems (hereafter denoted as LCRF&gt;-LCRF-UB and FCRF-UB) are meant for reference only, as they have access to answers for the opposing tasks. Adapted SVM for Joint Classification. For completeness, we also compared our work against t</context>
</contexts>
<marker>Sutton, 2006</marker>
<rawString>Charles Sutton. 2006. GRMM: GRaphical Models in Mallet. In URL http://mallet. cs. umass. edu/grmm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Tellex</author>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
<author>Aaron Fernandes</author>
<author>Gregory Marton</author>
</authors>
<title>Quantitative evaluation of passage retrieval algorithms for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and Development in Information Retrieval,</booktitle>
<pages>41--47</pages>
<marker>Tellex, Katz, Lin, Fernandes, Marton, 2003</marker>
<rawString>Stephanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernandes, and Gregory Marton. 2003. Quantitative evaluation of passage retrieval algorithms for question answering. In Proceedings of the 26th annual international ACM SIGIR conference on Research and Development in Information Retrieval, pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cong Duy Vu Hoang Wang</author>
<author>Min-Yen Kan</author>
</authors>
<title>Perspectives on Crowdsourcing Annotations for Natural Language Processing, journal = Language Resources and Evaluation.</title>
<date>2010</date>
<pages>1--23</pages>
<marker>Wang, Kan, 2010</marker>
<rawString>Aobo. Wang, Cong Duy Vu Hoang, and Min-Yen Kan. 2010. Perspectives on Crowdsourcing Annotations for Natural Language Processing, journal = Language Resources and Evaluation. pages 1–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aobo Wang</author>
<author>Tao Chen</author>
<author>Min-Yen Kan</author>
</authors>
<title>Retweeting From A Linguistic Perspective.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media,</booktitle>
<pages>46--55</pages>
<contexts>
<context position="16160" citStr="Wang et al. (2012)" startWordPosition="2703" endWordPosition="2706">tions. In total, we spent US$110 on assembling a subset of 5,500 posts (12,446 sentences) in which 1, 658 unique informal words are annotated within five weeks via Zhubajie. Each post was annotated by three annotators with moderate (0.57) inter-annotator agreement measured by Fleiss’ n (Joseph, 1971), and conflicts were resolved by majority voting. We divided the annotated corpus, taking 4, 000 posts for training, and the remainder (1, 500) for testing. Through inspection, we note that 79.8% of the informal words annotated in the testing set are not covered by the training set. We also follow Wang et al. (2012)’s conventions and apply rulesets to preprocess the corpus’ URLs, emoticons, “@usernames” and Hashtags as pre-segmented words, before input to CWS and IWR. For the CWS task, the first author manually labelled the same corpus following the segmentation guidelines published with the SIGHAN-58 MSR dataset. 3.2 Baseline Systems We implemented several baseline systems to compare with proposed FCRF joint inference method. Existing Systems. We re-implemented Xia and Wong (2008)’s extended Support Vector Machine (SVM) based microtext IWR system to compare with our method. Their system only does IWR, u</context>
</contexts>
<marker>Wang, Chen, Kan, 2012</marker>
<rawString>Aobo Wang, Tao Chen, and Min-Yen Kan. 2012. Retweeting From A Linguistic Perspective. In Proceedings of the Second Workshop on Language in Social Media, pages 46–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kam-Fai Wong</author>
<author>Yunqing Xia</author>
</authors>
<title>Normalization of Chinese Chat Language. Language Resources and Evaluation,</title>
<date>2008</date>
<pages>219--242</pages>
<contexts>
<context position="33112" citStr="Wong and Xia, 2008" startWordPosition="5451" endWordPosition="5454">nal, we feel that the difference in scope (informal sentence detection rather than word detection) shows the limitation of their work for microblog IWR. Their chats cover only 651 unique informal words, as opposed to our study covering almost triple the word types (1, 658). Our corpus demonstrates a higher ratio of informal word use (a new informal word appears in 12,446 = 13% of sentences, as opposed to 651 1,658 22,400 = 2% in their BBS corpus). Further analysis of their corpus reveals that phonetic substitution is the primary origin of informal words in their corpus – 99.2% as reported in (Wong and Xia, 2008). In contrast, the origin for informal words in microblogs is more varied, where phonetic substitutions abbreviations and neologisms, account for 53.1%, 21.4% and 18.7% of the informal word types, respectively. Their method is best suited for phonetic substitution, thus performing well on their corpus but poorly on ours. More closely related, Li and Yarowsky (2008) tackle Chinese IWR. They bootstrap 500 informal/formal word pairs by using manually-tuned queries to find definition sentences on the Web. The resulting noisy list is further re-ranked based on n-gram co-occurrence. However, their m</context>
</contexts>
<marker>Wong, Xia, 2008</marker>
<rawString>Kam-Fai Wong and Yunqing Xia. 2008. Normalization of Chinese Chat Language. Language Resources and Evaluation, pages 219–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andi Wu</author>
<author>Zixin Jiang</author>
</authors>
<title>StatisticallyEnhanced New Word Identification in A Rule-based Chinese Aystem.</title>
<date>2000</date>
<booktitle>In Proceedings of the second workshop on Chinese Language Processing,</booktitle>
<pages>46--51</pages>
<contexts>
<context position="34145" citStr="Wu and Jiang, 2000" startWordPosition="5612" endWordPosition="5615">/formal word pairs by using manually-tuned queries to find definition sentences on the Web. The resulting noisy list is further re-ranked based on n-gram co-occurrence. However, their method makes a basic assumption that informal/formal word pairs co-occur within a definition sentence (i.e., “&lt;informal word&gt; means &lt;formal word&gt;”) may not hold in microblog data, as microbloggers largely do not define the words they use. Closely related to our work is the task of Chinese new word detection, normally treated as a separate process from word segmentation in most previous works (Chen and Bai, 1998; Wu and Jiang, 2000; Chen and Ma, 2002; Gao et al., 2005). Aiming to improve both tasks, work by Peng et al. (2004) and Sun et al. (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. This is a weakness as their linear CRF model requires re-training. Their method also requires thresholds to be set through heuristic tuning, as to whether the segmented words are indeed new words. We note that the task of new word detection refers to out-of-vocabulary (OOV) detection, and is distinctly different from IWR (new words could be both formal or informal words). 6 Conclusio</context>
</contexts>
<marker>Wu, Jiang, 2000</marker>
<rawString>Andi Wu and Zixin Jiang. 2000. StatisticallyEnhanced New Word Identification in A Rule-based Chinese Aystem. In Proceedings of the second workshop on Chinese Language Processing, pages 46–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunqing Xia</author>
<author>Kam-Fai Wong</author>
</authors>
<title>Anomaly Detecting within Dynamic Chinese Chat Text. NEW TEXT Wikis and blogs and other dynamic text sources,</title>
<date>2006</date>
<pages>48</pages>
<contexts>
<context position="1681" citStr="Xia and Wong, 2006" startWordPosition="249" endWordPosition="252">n mining for knowledge, microtext processing is difficult to process. One key reason for this difficulty is the ubiquitous presence of informal words – anomalous terms that manifest as ad hoc abbreviations, neologisms, unconventional spellings and phonetic substitutions. Such informality is often present in oral conversation, and user-generated microblogs reflect this informality. Natural language processing (NLP) tools largely fail to work properly on microtext, as they have largely been trained on formally written text (i.e., newswire). Recent work has started to address these shortcomings (Xia and Wong, 2006; Kobus et al., 2008; Han and Baldwin, 2011). Informal words and their usage in microtext evolves quickly, following social trends and news events. ∗This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office. These characteristics make it difficult for lexicographers to compile lexica to keep with the pace of language change. We focus on this problem in the Chinese language. Through our analysis of a gathered Chinese microblog corpus, we observe that Chinese informa</context>
<context position="32379" citStr="Xia and Wong, 2006" startWordPosition="5327" endWordPosition="5330">nformal language, work conducted by Xia and Wong address the problem 11http://www.cnts.ua.ac.be/conll2003/ ner/ 738 of in bulletin board system (BBS) chats. They employ pattern matching and SVM-based classification to recognize Chinese informal sentences (not individual words) chat (Xia et al., 2005). Both methods had their advantages: the learning-based method did better on recall, while the pattern matching performed better on precision. To obtain consistent performance on new unseen data, they further employed an error-driven method which performed more consistently over time-varying data (Xia and Wong, 2006). In contrast, our work identifies individual informal words, a finergrained (and more difficult) task. While seminal, we feel that the difference in scope (informal sentence detection rather than word detection) shows the limitation of their work for microblog IWR. Their chats cover only 651 unique informal words, as opposed to our study covering almost triple the word types (1, 658). Our corpus demonstrates a higher ratio of informal word use (a new informal word appears in 12,446 = 13% of sentences, as opposed to 651 1,658 22,400 = 2% in their BBS corpus). Further analysis of their corpus r</context>
</contexts>
<marker>Xia, Wong, 2006</marker>
<rawString>Yunqing Xia and Kam-Fai Wong. 2006. Anomaly Detecting within Dynamic Chinese Chat Text. NEW TEXT Wikis and blogs and other dynamic text sources, page 48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunqing Xia</author>
<author>Kam-Fai Wong</author>
<author>Wei Gao</author>
</authors>
<title>NIL Is Not Nothing: Recognition of Chinese Network Informal Language Expressions.</title>
<date>2005</date>
<booktitle>In 4th SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>5</volume>
<contexts>
<context position="960" citStr="Xia et al. (2005)" startWordPosition="141" endWordPosition="144"> recognition in Chinese microblogs. A key problem is the lack of word delimiters in Chinese. We exploit this reliance as an opportunity: recognizing the relation between informal word recognition and Chinese word segmentation, we propose to model the two tasks jointly. Our joint inference method significantly outperforms baseline systems that conduct the tasks individually or sequentially. 1 Introduction User generated content (UGC) – including microblogs, comments, SMS, chat and instant messaging – collectively referred to as microtext by Gouwset et al. (2011) or network informal language by Xia et al. (2005), is the hallmark of the participatory Web. While a rich source that many applications are interested in mining for knowledge, microtext processing is difficult to process. One key reason for this difficulty is the ubiquitous presence of informal words – anomalous terms that manifest as ad hoc abbreviations, neologisms, unconventional spellings and phonetic substitutions. Such informality is often present in oral conversation, and user-generated microblogs reflect this informality. Natural language processing (NLP) tools largely fail to work properly on microtext, as they have largely been tra</context>
<context position="32061" citStr="Xia et al., 2005" startWordPosition="5282" endWordPosition="5285"> word lexicons and mappings. These works are based on distributional similarity and string similarity that address concerns of lexical variation and spelling. These methods propose two-step unsupervised approaches to first detect and then normalize detected informal words using dictionaries. In processing Chinese informal language, work conducted by Xia and Wong address the problem 11http://www.cnts.ua.ac.be/conll2003/ ner/ 738 of in bulletin board system (BBS) chats. They employ pattern matching and SVM-based classification to recognize Chinese informal sentences (not individual words) chat (Xia et al., 2005). Both methods had their advantages: the learning-based method did better on recall, while the pattern matching performed better on precision. To obtain consistent performance on new unseen data, they further employed an error-driven method which performed more consistently over time-varying data (Xia and Wong, 2006). In contrast, our work identifies individual informal words, a finergrained (and more difficult) task. While seminal, we feel that the difference in scope (informal sentence detection rather than word detection) shows the limitation of their work for microblog IWR. Their chats cov</context>
</contexts>
<marker>Xia, Wong, Gao, 2005</marker>
<rawString>Yunqing Xia, Kam-Fai Wong, and Wei Gao. 2005. NIL Is Not Nothing: Recognition of Chinese Network Informal Language Expressions. In 4th SIGHAN Workshop on Chinese Language Processing, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese Word Segmentation as Character Tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<pages>29--48</pages>
<contexts>
<context position="9472" citStr="Xue, 2003" startWordPosition="1534" endWordPosition="1535">n the number of variables, but also more training data to achieve equivalent performance with an FCRF. Given the limited scale of the state space and training data, we follow the FCRF model, using exact Junction Tree (Jensen, 1996) inference and decoding algorithm to perform prediction. 2.3 CRF Features We use three broad feature classes – lexical, dictionary-based and statistical features – aiming to distinguish the output classes for the CWS and IRW problems. Character-based sequence labeling is employed for word segmentation due to its simplicity and robustness to the unknown word problem (Xue, 2003). A key contribution of our work is also to propose novel features for joint inference. We propose new features for the dictionary-based and statistical feature classes, which we have marked in the discussion below with “(*)”. We later examine their efficacy in Section 3. Lexical Features. As a foundation, we employ lexical (n-gram) features informed by the previous state-of-the-art for CWS (Sun and Xu, 2011; Low et al., 2005). These features are listed below2: • Character 1-gram: Ck(i − 4 &lt; k &lt; i + 4) • Character 2-gram: CkCk+1(i − 4 &lt; k &lt; i + 3) • Character 3-gram: CkCk+1Ck+2(i − 3 &lt; k &lt; i +</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese Word Segmentation as Character Tagging. Computational Linguistics and Chinese Language Processing, pages 29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics -</booktitle>
<volume>2</volume>
<pages>947--953</pages>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th conference on Computational linguistics - Volume 2, pages 947–953.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua-Ping Zhang</author>
<author>Hong-Kui Yu</author>
<author>De-Yi Xiong</author>
<author>Qun Liu</author>
</authors>
<title>HHMM-based Chinese Lexical Analyzer ICTCLAS.</title>
<date>2003</date>
<booktitle>In Proceedings of the second SIGHAN workshop on Chinese language processing,</booktitle>
<pages>184--187</pages>
<contexts>
<context position="16841" citStr="Zhang et al., 2003" startWordPosition="2808" endWordPosition="2811">Ls, emoticons, “@usernames” and Hashtags as pre-segmented words, before input to CWS and IWR. For the CWS task, the first author manually labelled the same corpus following the segmentation guidelines published with the SIGHAN-58 MSR dataset. 3.2 Baseline Systems We implemented several baseline systems to compare with proposed FCRF joint inference method. Existing Systems. We re-implemented Xia and Wong (2008)’s extended Support Vector Machine (SVM) based microtext IWR system to compare with our method. Their system only does IWR, using the CWS and POS tagging otuput of the ICTCLAS segmenter (Zhang et al., 2003) as input. To compare our joint inference versus other learning models, we also employed a decision tree (DT) learner, equipped with the same feature set as our FCRF. Both the SVM and DT models are provided by the Weka3 (Hall et al., 2009) toolkit, using its default configuration. To evaluate CWS performance, we compare with two recent segmenters. Sun and Xu (2011)’s 6http://open.weibo.com 7http://www.zhubajie.com 8http://www.sighan.org work achieves state-of-the-art performance and is publicly available. They employ a LCRF taking as input both lexical and statistical features derived from unl</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun Liu. 2003. HHMM-based Chinese Lexical Analyzer ICTCLAS. In Proceedings of the second SIGHAN workshop on Chinese language processing, pages 184–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Unsupervised Segmentation Helps Supervised Learning of Character Tagging for Word Segmentation and Named Entity Recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>106--111</pages>
<contexts>
<context position="31103" citStr="Zhao and Kit, 2008" startWordPosition="5138" endWordPosition="5141">able 6; i.e., person names in the form of user IDs and nicknames, that have less constraint on form in terms of length, canonical structure (not surnames with given names; as is standard in Chinese names) and may mix alphabetic characters. Most of these belong to the category of Person Name (PER), as defined in CoNLL-200311 Named Entity Recognition shared task. Such freestyle entities are often misrecognized as informal words, as they share some of the same stylistic markings, and are not marked by features used to recognize previous Chinese named entity recognition methods (Gao et al., 2005; Zhao and Kit, 2008) that work on news or general domain text. We recognize this as a challenge in Chinese microtext, but beyond the scope of our current work. 5 Related Work In English, IWR has typically been investigated alongside normalization. Several recent works (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) aim to produce informal/formal word lexicons and mappings. These works are based on distributional similarity and string similarity that address concerns of lexical variation and spelling. These methods propose two-step unsupervised approaches to first detect and then normalize detected i</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. Unsupervised Segmentation Helps Supervised Learning of Character Tagging for Word Segmentation and Named Entity Recognition. In Proceedings of the Sixth SIGHAN Workshop on Chinese Language Processing, pages 106–111.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>