<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000853">
<title confidence="0.991392">
Perplexity Minimization for Translation Model Domain Adaptation in
Statistical Machine Translation
</title>
<author confidence="0.996194">
Rico Sennrich
</author>
<affiliation confidence="0.9985595">
Institute of Computational Linguistics
University of Zurich
</affiliation>
<address confidence="0.931883">
Binzmühlestr. 14
CH-8050 Zürich
</address>
<email confidence="0.998781">
sennrich@cl.uzh.ch
</email>
<sectionHeader confidence="0.997383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9988728125">
We investigate the problem of domain
adaptation for parallel data in Statistical
Machine Translation (SMT). While tech-
niques for domain adaptation of monolin-
gual data can be borrowed for parallel data,
we explore conceptual differences between
translation model and language model do-
main adaptation and their effect on per-
formance, such as the fact that translation
models typically consist of several features
that have different characteristics and can
be optimized separately. We also explore
adapting multiple (4–10) data sets with no
a priori distinction between in-domain and
out-of-domain data except for an in-domain
development set.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999930307692308">
The increasing availability of parallel corpora
from various sources, welcome as it may be,
leads to new challenges when building a statis-
tical machine translation system for a specific
domain. The task of determining which par-
allel texts should be included for training, and
which ones hurt translation performance, is te-
dious when performed through trial-and-error.
Alternatively, methods for a weighted combina-
tion exist, but there is conflicting evidence as to
which approach works best, and the issue of de-
termining weights is not adequately resolved.
The picture looks better in language mod-
elling, where model interpolation through per-
plexity minimization has become a widespread
method of domain adaptation. We investigate the
applicability of this method for translation mod-
els, and discuss possible applications.
We move the focus away from a binary com-
bination of in-domain and out-of-domain data. If
we can scale up the number of models whose con-
tributions we weight, this reduces the need for a
priori knowledge about the fitness1 of each poten-
tial training text, and opens new research oppor-
tunities, for instance experiments with clustered
training data.
</bodyText>
<sectionHeader confidence="0.989814" genericHeader="method">
2 Domain Adaptation for Translation
Models
</sectionHeader>
<bodyText confidence="0.999941045454545">
To motivate efforts in domain adaptation, let us
review why additional training data can improve,
but also decrease translation quality.
Adding more training data to a translation sys-
tem is easy to motivate through the data sparse-
ness problem. Koehn and Knight (2001) show
that translation quality correlates strongly with
how often a word occurs in the training corpus.
Rare words or phrases pose a problem in sev-
eral stages of MT modelling, from word align-
ment to the computation of translation probabil-
ities through Maximum Likelihood Estimation.
Unknown words are typically copied verbatim to
the target text, which may be a good strategy for
named entities, but is often wrong otherwise. In
general, more data allows for a better word align-
ment, a better estimation of translation probabili-
ties, and for the consideration of more context (in
phrase-based or syntactic SMT).
A second effect of additional data is not nec-
essarily positive. Translations are inherently am-
biguous, and a strong source of ambiguity is the
</bodyText>
<footnote confidence="0.991214">
1We borrow this term from early evolutionary biology to
emphasize that the question in domain adaptation is not how
“good” or “bad” the data is, but how well-adapted it is to the
task at hand.
</footnote>
<page confidence="0.93019">
539
</page>
<note confidence="0.977859">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 539–549,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999016458333334">
domain of a text. The German word “Wort” (engl.
word) is typically translated as floor in Europarl,
a corpus of Parliamentary Proceedings (Koehn,
2005), owing to the high frequency of phrases
such as you have the floor, which is translated into
German as Sie haben das Wort. This translation
is highly idiomatic and unlikely to occur in other
contexts. Still, adding Europarl as out-of-domain
training data shifts the probability distribution of
p(t|“Wort”) in favour of p(“floor”|“Wort”), and
may thus lead to improper translations.
We will refer to the two problems as the data
sparseness problem and the ambiguity problem.
Adding out-of-domain data typically mitigates the
data sparseness problem, but exacerbates the am-
biguity problem. The net gain (or loss) of adding
more data changes from case to case. Because
there are (to our knowledge) no tools that predict
this net effect, it is a matter of empirical investi-
gation (or, in less suave terms, trial-and-error), to
determine which corpora to use.2
From this understanding of the reasons for and
against out-of-domain data, we formulate the fol-
lowing hypotheses:
</bodyText>
<listItem confidence="0.996729">
1. A weighted combination can control the con-
tribution of the out-of-domain corpus on the
probability distribution, and thus limit the
ambiguity problem.
2. A weighted combination eliminates the need
</listItem>
<bodyText confidence="0.932438125">
for data selection, offering a robust baseline
for domain-specific machine translation.
We will discuss three mixture modelling tech-
niques for translation models. Our aim is to adapt
all four features of the standard Moses SMT trans-
lation model: the phrase translation probabilities
p(t|s) and p(s|t), and the lexical weights lex(t|s)
and lex(s|t).3
</bodyText>
<subsectionHeader confidence="0.955284">
2.1 Linear Interpolation
</subsectionHeader>
<bodyText confidence="0.999031333333333">
A well-established approach in language mod-
elling is the linear interpolation of several mod-
els, i.e. computing the weighted average of the in-
</bodyText>
<footnote confidence="0.797972714285714">
2A frustrating side-effect is that these findings rarely gen-
eralize. For instance, we were unable to reproduce the find-
ing by Ceau¸su et al. (2011) that patent translation systems
are highly domain-sensitive and suffer from the inclusion of
parallel training data from other patent subdomains.
3We can ignore the fifth feature, the phrase penalty,
which is a constant.
</footnote>
<bodyText confidence="0.630342">
dividual model probabilities. It is defined as fol-
lows:
</bodyText>
<equation confidence="0.993736666666667">
n
p(x|y; A) = Aipi(x|y) (1)
i=1
</equation>
<bodyText confidence="0.9999186">
with Ai being the interpolation weight of each
model i, and with (Ei Ai) = 1.
For SMT, linear interpolation of translation
models has been used in numerous systems. The
approaches diverge in how they set the inter-
polation weights. Some authors use uniform
weights (Cohn and Lapata, 2007), others em-
pirically test different interpolation coefficients
(Finch and Sumita, 2008; Yasuda et al., 2008;
Nakov and Ng, 2009; Axelrod et al., 2011), others
apply monolingual metrics to set the weights for
TM interpolation (Foster and Kuhn, 2007; Koehn
et al., 2010).
There are reasons against all these approaches.
Uniform weights are easy to implement, but give
little control. Empirically, it has been shown that
they often do not perform optimally (Finch and
Sumita, 2008; Yasuda et al., 2008). An opti-
mization of BLEU scores on a development set is
promising, but slow and impractical. There is no
easy way to integrate linear interpolation into log-
linear SMT frameworks and perform optimization
through MERT. Monolingual optimization objec-
tives such as language model perplexity have the
advantage of being well-known and readily avail-
able, but their relation to the ambiguity problem
is indirect at best.
Linear interpolation is seemingly well-defined
in equation 1. Still, there are a few implemen-
tation details worth pointing out. If we directly
interpolate each feature in the translation model,
and define the feature values of non-occurring
phrase pairs as 0, this disregards the meaning of
each feature. If we estimate p(x|y) via MLE as in
equation 2, and c(y) = 0, then p(x|y) is strictly
speaking undefined. Alternatively to a naive al-
gorithm, which treats unknown phrase pairs as
having a probability of 0, which results in a defi-
cient probability distribution, we propose and im-
plement the following algorithm. For each value
pair (x, y) for which we compute p(x|y), we re-
place Ai with 0 for all models i with p(y) =
0, then renormalize the weight vector A to 1.
We do this for p(t|s) and lex(t|s), but not for
p(s|t) and lex(s|t), the reasoning being the con-
</bodyText>
<page confidence="0.990643">
540
</page>
<bodyText confidence="0.999884882352941">
sequences for perplexity minimization (see sec-
tion 2.4). Namely, we do not want to penalize
a small in-domain model for having a high out-
of-vocabulary rate on the source side, but we do
want to penalize models that know the source
phrase, but not its correct translation. A sec-
ond modification pertains to the lexical weights
lex(sIt) and lex(tIs), which form no true proba-
bility distribution, but are derived from the indi-
vidual word translation probabilities of a phrase
pair (see (Koehn et al., 2003)). We propose to
not interpolate the features directly, but the word
translation probabilities which are the basis of the
lexical weight computation. The reason for this is
that word pairs are less sparse than phrase pairs,
so that we can even compute lexical weights for
phrase pairs which are unknown in a model.4
</bodyText>
<subsectionHeader confidence="0.995181">
2.2 Weighted Counts
</subsectionHeader>
<bodyText confidence="0.999970333333333">
Weighting of different corpora can also be imple-
mented through a modified Maximum Likelihood
Estimation. The traditional equation for MLE is:
</bodyText>
<equation confidence="0.989247">
c(x,y) (2)
Ex, c(x&apos;, y)
</equation>
<bodyText confidence="0.999995">
where c denotes the count of an observation, and
p the model probability. If we generalize the for-
mula to compute a probability from n corpora,
and assign a weight λi to each, we get5:
</bodyText>
<equation confidence="0.998812">
Eni=1 λici(x, y) (3)
p(x|y; λ) = Ei=1 Ex, λici(xl, y)
</equation>
<bodyText confidence="0.9998796875">
The main difference to linear interpolation is
that this equation takes into account how well-
evidenced a phrase pair is. This includes the dis-
tinction between lack of evidence and negative ev-
idence, which is missing in a naive implementa-
tion of linear interpolation.
Translation models trained with weighted
counts have been discussed before, and have
been shown to outperform uniform ones in some
settings. However, researchers who demon-
strated this fact did so with arbitrary weights (e.g.
(Koehn, 2002)), or by empirically testing differ-
ent weights (e.g. (Nakov and Ng, 2009)). We do
not know of any research on automatically deter-
mining weights for this method, or which is not
limited to two corpora.
</bodyText>
<footnote confidence="0.93773525">
4For instance if the word pairs (the,der) and (man,Mann)
are known, but the phrase pair (the man, der Mann) is not.
5Unlike equation 1, equation 3 does not require that
(Ei Ai) = 1.
</footnote>
<subsectionHeader confidence="0.999095">
2.3 Alternative Paths
</subsectionHeader>
<bodyText confidence="0.999994482758621">
A third method is using multiple translation mod-
els as alternative decoding paths (Birch et al.,
2007), an idea which Koehn and Schroeder (2007)
first used for domain adaptation. This approach
has the attractive theoretical property that adding
new models is guaranteed to lead to equal or bet-
ter performance, given the right weights. At best,
a model is beneficial with appropriate weights. At
worst, we can set the feature weights so that the
decoding paths of one model are never picked for
the final translation. In practice, each translation
model adds 5 features and thus 5 more dimensions
to the weight space, which leads to longer search,
search errors, and/or overfitting. The expectation
is that, at least with MERT, using alternative de-
coding paths does not scale well to a high number
of models.
A suboptimal choice of weights is not the only
weakness of alternative paths, however. Let us
assume that all models have the same weights.
Note that, if a phrase pair occurs in several mod-
els, combining models through alternative paths
means that the decoder selects the path with the
highest probability, whereas with linear interpo-
lation, the probability of the phrase pair would
be the (weighted) average of all models. Select-
ing the highest-scoring phrase pair favours statis-
tical outliers and hence is the less robust decision,
prone to data noise and data sparseness.
</bodyText>
<subsectionHeader confidence="0.995357">
2.4 Perplexity Minimization
</subsectionHeader>
<bodyText confidence="0.939055055555556">
In language modelling, perplexity is frequently
used as a quality measure for language models
(Chen and Goodman, 1998). Among other appli-
cations, language model perplexity has been used
for domain adaptation (Foster and Kuhn, 2007).
For translation models, perplexity is most closely
associated with EM word alignment (Brown et
al., 1993) and has been used to evaluate different
alignment algorithms (Al-Onaizan et al., 1999).
We investigate translation model perplexity
minimization as a method to set model weights
in mixture modelling. For the purpose of opti-
mization, the cross-entropy H(p), the perplexity
2H(p), and other derived measures are equivalent.
The cross-entropy H(p) is defined as:6
6See (Chen and Goodman, 1998) for a short discussion
of the equation. In short, a lower cross-entropy indicates that
the model is better able to predict the development set.
</bodyText>
<equation confidence="0.961997166666667">
c(x, y)
p(x|y) =
c(y)
541
H(p) = − � ˜p(x, y) log2 p(x|y) (4)
x,y
</equation>
<bodyText confidence="0.999709181818182">
The phrase pairs (x, y) whose probability we
measure, and their empirical probability p˜ need
to be extracted from a development set, whereas
p is the model probability. To obtain the phrase
pairs, we process the development set with the
same word alignment and phrase extraction tools
that we use for training, i.e. GIZA++ and heuris-
tics for phrase extraction (Och and Ney, 2003).
The objective function is the minimization of the
cross-entropy, with the weight vector λ as argu-
ment:
</bodyText>
<equation confidence="0.997076">
�λˆ = arg min −
A x,y ˜p(x, y) log2 p(x|y; λ) (5)
</equation>
<bodyText confidence="0.964813217391304">
We can fill in equations 1 or 3 for p(x|y; λ). The
optimization itself is convex and can be done with
off-the-shelf software.7 We use L-BFGS with
numerically approximated gradients (Byrd et al.,
1995).
Perplexity minimization has the advantage that
it is well-defined for both weighted counts and lin-
ear interpolation, and can be quickly computed.
Other than in language modelling, where p(x|y)
is the probability of a word given a n-gram his-
tory, conditional probabilities in translation mod-
els express the probability of a target phrase given
a source phrase (or vice versa), which connects
the perplexity to the ambiguity problem. The
higher the probability of “correct” phrase pairs,
the lower the perplexity, and the more likely
the model is to successfully resolve the ambigu-
ity. The question is in how far perplexity min-
imization coincides with empirically good mix-
ture weights.8 This depends, among others, on
the other model components in the SMT frame-
work, for instance the language model. We will
not evaluate perplexity minimization against em-
pirically optimized mixture weights, but apply it
in situations where the latter is infeasible, e.g. be-
cause of the number of models.
7A quick demonstration of convexity: equation 1 is
affine; equation 3 linear-fractional. Both are convex in the
domain R&gt;o. Consequently, equation 4 is also convex be-
cause it is the weighted sum of convex functions.
8There are tasks for which perplexity is known to be un-
reliable, e.g. for comparing models with different vocabular-
ies. However, such confounding factors do not affect the op-
timization algorithm, which works with a fixed set of phrase
pairs, and merely varies A.
Our main technical contributions are as fol-
lows: Additionally to perplexity optimization for
linear interpolation, which was first applied by
Foster et al. (2010), we propose perplexity opti-
mization for weighted counts (equation 3), and a
modified implementation of linear interpolation.
Also, we independently perform perplexity mini-
mization for all four features of the standard SMT
translation model: the phrase translation proba-
bilities p(t|s) and p(s|t), and the lexical weights
lex(t|s) and lex(s|t).
</bodyText>
<sectionHeader confidence="0.991281" genericHeader="method">
3 Other Domain Adaptation Techniques
</sectionHeader>
<bodyText confidence="0.999959789473684">
So far, we discussed mixture modelling for trans-
lation models, which is only a subset of domain
adaptation techniques in SMT.
Mixture-modelling for language models is well
established (Foster and Kuhn, 2007). Language
model adaptation serves the same purpose as
translation model adaptation, i.e. skewing the
probability distribution in favour of in-domain
translations. This means that LM adaptation may
have similar effects as TM adaptation, and that
the two are to some extent redundant. Foster and
Kuhn (2007) find that “both TM and LM adap-
tation are effective”, but that “combined LM and
TM adaptation is not better than LM adaptation
on its own”.
A second strand of research in domain adap-
tation is data selection, i.e. choosing a subset of
the training data that is considered more relevant
for the task at hand. This has been done for lan-
guage models using techniques from information
retrieval (Zhao et al., 2004), or perplexity (Lin et
al., 1997; Moore and Lewis, 2010). Data selec-
tion has also been proposed for translation mod-
els (Axelrod et al., 2011). Note that for transla-
tion models, data selection offers an unattractive
trade-off between the data sparseness and the am-
biguity problem, and that the optimal amount of
data to select is hard to determine.
Our discussion of mixture-modelling is rela-
tively coarse-grained, with 2-10 models being
combined. Matsoukas et al. (2009) propose an ap-
proach where each sentence is weighted accord-
ing to a classifier, and Foster et al. (2010) ex-
tend this approach by weighting individual phrase
pairs. These more fine-grained methods need not
be seen as alternatives to coarse-grained ones.
Foster et al. (2010) combine the two, apply-
ing linear interpolation to combine the instance-
</bodyText>
<page confidence="0.986397">
542
</page>
<bodyText confidence="0.9894475">
weighted out-of-domain model with an in-domain
model.
</bodyText>
<sectionHeader confidence="0.99602" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.820142333333333">
Apart from measuring the performance of the ap-
proaches introduced in section 2, we want to in-
vestigate the following open research questions.
</bodyText>
<listItem confidence="0.962228076923077">
1. Does an implementation of linear interpola-
tion that is more closely tailored to trans-
lation modelling outperform a naive imple-
mentation?
2. How do the approaches perform outside a
binary setting, i.e. when we do not work
with one in-domain and one out-of-domain
model, but with a higher number of models?
3. Can we apply perplexity minimization to
other translation model features such as the
lexical weights, and if yes, does a separate
optimization of each translation model fea-
ture improve performance?
</listItem>
<subsectionHeader confidence="0.990565">
4.1 Data and Methods
</subsectionHeader>
<bodyText confidence="0.999964652173913">
In terms of tools and techniques used, we mostly
adhere to the work flow described for the WMT
2011 baseline system9. The main tools are Moses
(Koehn et al., 2007), SRILM (Stolcke, 2002), and
GIZA++ (Och and Ney, 2003), with settings as
described in the WMT 2011 guide. We report
two translation measures: BLEU (Papineni et al.,
2002) and METEOR 1.3 (Denkowski and Lavie,
2011). All results are lowercased and tokenized,
measured with five independent runs of MERT
(Och and Ney, 2003) and MultEval (Clark et al.,
2011) for resampling and significance testing.
We compare three baselines and four transla-
tion model mixture techniques. The three base-
lines are a purely in-domain model, a purely out-
of-domain model, and a model trained on the con-
catenation of the two, which corresponds to equa-
tion 3 with uniform weights. Additionally, we
evaluate perplexity optimization with weighted
counts and the two implementations of linear in-
terpolation contrasted in section 2.1. The two lin-
ear interpolations that are contrasted are a naive
one, i.e. a direct, unnormalized interpolation of
</bodyText>
<footnote confidence="0.9776255">
9http://www.statmt.org/wmt11/baseline.
html
</footnote>
<table confidence="0.99996325">
Data set sentences words (fr)
Alpine (in-domain) 220k 4 700k
Europarl 1 500k 44 000k
JRC Acquis 1 100k 24 000k
OpenSubtitles v2 2 300k 18 000k
Total train 5 200k 91 000k
Dev 1424 33 000
Test 991 21 000
</table>
<tableCaption confidence="0.9718145">
Table 1: Parallel data sets for German – French trans-
lation task.
</tableCaption>
<table confidence="0.999962333333333">
Data set sentences words
Alpine (in-domain) 650k 13 000k
News-commentary 150k 4 000k
Europarl 2 000k 60 000k
News 25 000k 610 000k
Total 28 000k 690 000k
</table>
<tableCaption confidence="0.9832175">
Table 2: Monolingual French data sets for German –
French translation task.
</tableCaption>
<bodyText confidence="0.999948366666667">
all translation model features, and a modified one
that normalizes A for each phrase pair (s, t) for
p(t1s) and recomputes the lexical weights based
on interpolated word translation probabilites. The
fourth weighted combination is using alternative
decoding paths with weights set through MERT.
The four weighted combinations are evaluated
twice: once applied to the original four or ten par-
allel data sets, once in a binary setting in which
all out-of-domain data sets are first concatenated.
Since we want to concentrate on translation
model domain adaptation, we keep other model
components, namely word alignment and the lex-
ical reordering model, constant throughout the ex-
periments. We contrast two language models. An
unadapted, out-of-domain language model trained
on data sets provided for the WMT 2011 transla-
tion task, and an adapted language model which is
the linear interpolation of all data sets, optimized
for minimal perplexity on the in-domain develop-
ment set.
While unadapted language models are becom-
ing more rare in domain adaptation research, they
allow us to contrast different TM mixtures with-
out the effect on performance being (partially)
hidden by language model adaptation with the
same effect.
The first data set is a DE–FR translation sce-
nario in the domain of mountaineering. The in-
domain corpus is a collection of Alpine Club pub-
</bodyText>
<page confidence="0.99336">
543
</page>
<bodyText confidence="0.999848390243903">
lications (Volk et al., 2010). As parallel out-of-
domain dataset, we use Europarl, a collection of
parliamentary proceedings (Koehn, 2005), JRC-
Acquis, a collection of legislative texts (Stein-
berger et al., 2006), and OpenSubtitles v2, a par-
allel corpus extracted from film subtitles10 (Tiede-
mann, 2009). For language modelling, we use in-
domain data and data from the 2011 Workshop
on Statistical Machine Translation. The respec-
tive sizes of the data sets are listed in tables 1 and
2.
As the second data set, we use the Haitian Cre-
ole – English data from the WMT 2011 featured
translation task. It consists of emergency SMS
sent in the wake of the 2010 Haiti earthquake.
Originally, Microsoft Research and CMU oper-
ated under severe time constraints to build a trans-
lation system for this language pair. This limits
the ability to empirically verify how much each
data set contributes to translation quality, and in-
creases the importance of automated and quick
domain adaptation methods.
Note that both data sets have a relatively high
ratio of in-domain to out-of-domain parallel train-
ing data (1:20 for DE–EN and 1:5 for HT–EN)
Previous research has been performed with ratios
of 1:100 (Foster et al., 2010) or 1:400 (Axelrod
et al., 2011). Since domain adaptation becomes
more important when the ratio of IN to OUT is
low, and since such low ratios are also realistic11,
we also include results for which the amount of
in-domain parallel data has been restricted to 10%
of the available data set.
We used the same development set for lan-
guage/translation model adaptation and setting
the global model weights with MERT. While it
is theoretically possible that MERT will give too
high weights to models that are optimized on the
same development set, we found no empirical evi-
dence for this in experiments with separate devel-
opment sets.
</bodyText>
<sectionHeader confidence="0.542894" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.9998775">
The results are shown in tables 5 and 6. In the
DE–FR translation task, results vary between 13.5
and 18.9 BLEU points; in the HT–EN task, be-
tween 24.3 and 33.8. Unsurprisingly, an adapted
</bodyText>
<footnote confidence="0.98943125">
10http://www.opensubtitles.org
11We predict that the availability of parallel data will
steadily increase, most data being out-of-domain for any
given task.
</footnote>
<table confidence="0.999963928571429">
Data set units words (en)
SMS (in-domain) 16 500 380 000
Medical 1 600 10 000
Newswire 13 500 330 000
Glossary 35 700 90 000
Wikipedia 8 500 110 000
Wikipedia NE 10 500 34 000
Bible 30 000 920 000
Haitisurf dict 3 700 4000
Krengle dict 1 600 2 600
Krengle 650 4 200
Total train 120 000 1 900 000
Dev 900 22 000
Test 1274 25 000
</table>
<tableCaption confidence="0.9781785">
Table 3: Parallel data sets for Haiti Creole – English
translation task.
</tableCaption>
<table confidence="0.975884333333333">
Data set sentences words
SMS (in-domain) 16k 380k
News 113 000k 2 650 000k
</table>
<tableCaption confidence="0.9541245">
Table 4: Monolingual English data sets for Haiti Cre-
ole – English translation task.
</tableCaption>
<bodyText confidence="0.99994132">
LM performs better than an out-of-domain one,
and using all available in-domain parallel data is
better than using only part of it. The same is not
true for out-of-domain data, which highlights the
problem discussed in the introduction. For the
DE–FR task, adding 86 million words of out-of-
domain parallel data to the 5 million in-domain
data set does not lead to consistent performance
gains. We observe a decrease of 0.3 BLEU points
with an out-of-domain LM, and an increase of 0.4
BLEU points with an adapted LM. The out-of-
domain training data has a larger positive effect
if less in-domain data is available, with a gain of
1.4 BLEU points. The results in the HT–EN trans-
lation task (table 6) paint a similar picture. An
interesting side note is that even tiny amounts of
in-domain parallel data can have strong effects on
performance. A training set of 1600 emergency
SMS (38 000 tokens) yields a comparable perfor-
mance to an out-of-domain data set of 1.5 million
tokens.
As to the domain adaptation experiments,
weights optimized through perplexity minimiza-
tion are significantly better in the majority of
cases, and never significantly worse, than uniform
</bodyText>
<page confidence="0.996167">
544
</page>
<table confidence="0.982393714285714">
out-of-domain LM adapted LM
full IN TM full IN TM small IN TM
BLEU METEOR BLEU METEOR BLEU METEOR
16.8 35.9 17.9 37.0 15.7 33.5
13.5 31.3 14.8 32.3 14.8 32.3
16.5 35.7 18.3 37.3 17.1 35.4
17.4 36.6 18.7 37.9 17.6 36.2
17.4 36.7 18.8 37.9 17.6 36.1
17.2 36.5 18.9 38.0 17.6 36.2
17.2 36.5 18.6 37.8 17.4 36.0
17.3 36.6 18.8 37.8 17.4 36.0
17.1 36.5 18.5 37.7 17.3 35.9
17.2 36.5 18.7 37.9 17.3 36.0
17.0 36.2 18.3 37.4 16.3 35.1
</table>
<figure confidence="0.990100928571429">
System
in-domain
out-of-domain
counts (concatenation)
binary in/out
weighted counts
linear interpolation (naive)
linear interpolation (modified)
alternative paths
4 models
weighted counts
linear interpolation (naive)
linear interpolation (modified)
alternative paths
</figure>
<tableCaption confidence="0.895193">
Table 5: Domain adaptation results DE–FR. Domain: Alpine texts. Full IN TM: Using the full in-domain parallel
corpus; small IN TM: using 10% of available in-domain parallel data.
</tableCaption>
<bodyText confidence="0.999493">
weights.12 However, the difference is smaller for
the experiments with an adapted language model
than for those with an out-of-domain one, which
confirms that the benefit of language model adap-
tation and translation model adaptation are not
fully cumulative. Performance-wise, there seems
to be no clear winner between weighted counts
and the two alternative implementations of lin-
ear interpolation. We can still argue for weighted
counts on theoretical grounds. A weighted MLE
(equation 3) returns a true probability distribution,
whereas a naive implementation of linear interpo-
lation results in a deficient model. Consequently,
probabilities are typically lower in the naively in-
terpolated model, which results in higher (worse)
perplexities. While the deficiency did not affect
MERT or decoding negatively, it might become
problematic in other applications, for instance if
we want to use an interpolated model as a compo-
nent in a second perplexity-based combination of
models.13
When moving from a binary setting with
one in-domain and one out-of-domain transla-
tion model (trained on all available out-of-domain
data) to 4–10 translation models, we observe a
serious performance degradation for alternative
paths, while performance of the perplexity opti-
</bodyText>
<footnote confidence="0.97063775">
12This also applies to linear interpolation with uniform
weights, which is not shown in the tables.
13Specifically, a deficient model would be dispreferred by
the perplexity minimization algorithm.
</footnote>
<bodyText confidence="0.999510857142857">
mization methods does not change significantly.
This is positive for perplexity optimization be-
cause it demonstrates that it requires less a priori
information, and opens up new research possibil-
ities, i.e. experiments with different clusterings of
parallel data. The performance degradation for
alternative paths is partially due to optimization
problems in MERT, but also due to a higher sus-
ceptibility to statistical outliers, as discussed in
section 2.3.14
A pessimistic interpretation of the results
would point out that performance gains compared
to the best baseline system are modest or even
inexistent in some settings. However, we want
to stress two important points. First, we often
do not know a priori whether adding an out-of-
domain data set boosts or weakens translation per-
formance. An automatic weighting of data sets re-
duces the need for trial-and-error experimentation
and is worthwhile even if a performance increase
is not guaranteed. Second, the potential impact
of a weighted combination depends on the trans-
lation scenario and the available data sets. Gen-
erally, we expect non-uniform weighting to have
a bigger impact when the models that are com-
bined are more dissimilar (in terms of fitness for
the task), and if the ratio of in-domain to out-of-
domain data is low. Conversely, there are situa-
</bodyText>
<footnote confidence="0.959694666666667">
14We empirically verified this weakness in a synthetic ex-
periment with a randomly split training corpus and identical
weights for each path.
</footnote>
<page confidence="0.989186">
545
</page>
<table confidence="0.997616785714286">
out-of-domain LM adapted LM
full IN TM full IN TM small IN TM
BLEU METEOR BLEU METEOR BLEU METEOR
30.4 30.7 33.4 31.7 29.7 28.6
24.3 28.0 28.9 30.2 28.9 30.2
30.3 31.2 33.6 32.4 31.3 31.3
31.0 31.6 33.8 32.4 31.5 31.3
30.8 31.4 33.7 32.4 31.9 31.3
30.8 31.5 33.7 32.4 31.7 31.2
30.8 31.3 33.2 32.4 29.8 30.7
31.0 31.5 33.5 32.3 31.8 31.5
30.9 31.4 33.8 32.4 31.9 31.3
31.0 31.6 33.8 32.5 32.1 31.5
25.9 29.2 24.3 29.1 29.8 30.9
</table>
<figure confidence="0.9751945">
System
in-domain
out-of-domain
counts (concatenation)
binary in/out
weighted counts
linear interpolation (naive)
linear interpolation (modified)
alternative paths
10 models
weighted counts
linear interpolation (naive)
linear interpolation (modified)
alternative paths
</figure>
<tableCaption confidence="0.957048">
Table 6: Domain adaptation results HT–EN. Domain: emergency SMS. Full IN TM: Using the full in-domain
parallel corpus; small IN TM: using 10% of available in-domain parallel data.
</tableCaption>
<bodyText confidence="0.998434">
tions where we actually expect a simple concate-
nation to be optimal, e.g. when the data sets have
very similar probability distributions.
</bodyText>
<subsectionHeader confidence="0.7648015">
4.2.1 Individually Optimizing Each TM
Feature
</subsectionHeader>
<bodyText confidence="0.99864244">
It is hard to empirically show how translation
model perplexity optimization compares to using
monolingual perplexity measures for the purpose
of weighting translation models, as e.g. done by
(Foster and Kuhn, 2007; Koehn et al., 2010). One
problem is that there are many different possible
configurations for the latter. We can use source
side or target side language models, operate with
different vocabularies, smoothing techniques, and
n-gram orders.
One of the theoretical considerations that
favour measuring perplexity on the translation
model rather than using monolingual measures
is that we can optimize each translation model
feature separately. In the default Moses transla-
tion model, the four features are p(s1t), lex(slt),
p(t1s) and lex(tjs).
We empirically test different optimization
schemes as follows. We optimize perplexity on
each feature independently, obtaining 4 weight
vectors. We then compute one model with one
weight vector per feature (namely the feature that
the vector was optimized on), and four models
that use one of the weight vectors for all features.
A further model uses a weight vector that is the
</bodyText>
<table confidence="0.999660157894737">
weights perplexity BLEU
1 2 3 4
weighted counts
uniform 5.12 7.68 4.84 13.67 30.3
separate 4.68 6.62 4.24 8.57 31.0
1 4.68 6.84 4.50 10.86 30.3
2 4.78 6.62 4.48 10.54 30.3
3 4.86 7.31 4.24 9.15 30.8
4 5.33 7.87 4.52 8.57 30.9
average 4.72 6.71 4.38 9.95 30.4
linear interpolation (modified)
uniform 19.89 82.78 4.80 10.78 30.6
separate 5.45 8.56 4.28 8.85 31.0
1 5.45 8.79 4.40 8.89 30.8
2 5.71 8.56 4.54 8.91 30.9
3 6.46 11.88 4.28 9.07 31.0
4 6.12 10.86 4.47 8.85 30.9
average 5.73 9.72 4.34 8.89 30.9
LM 6.01 9.83 4.56 8.96 30.8
</table>
<tableCaption confidence="0.99005625">
Table 7: Contrast between a separate optimization of
each feature and applying the weight vector optimized
on one feature to the whole model. HT–EN with out-
of-domain LM.
</tableCaption>
<page confidence="0.997052">
546
</page>
<bodyText confidence="0.999950037037037">
average of the other four. For linear interpolation,
we also include a model whose weights have been
optimized through language model perplexity op-
timization, with a 3-gram language model (modi-
fied Knesey-Ney smoothing) trained on the target
side of each parallel data set.
Table 7 shows the results. In terms of BLEU
score, a separate optimization of each feature is a
winner in our experiment in that no other scheme
is better, with 8 of the 11 alternative weighting
schemes (excluding uniform weights) being sig-
nificantly worse than a separate optimization. The
differences in BLEU score are small, however,
since the alternative weighting schemes are gen-
erally felicitious in that they yield both a lower
perplexity and better BLEU scores than uniform
weighting. While our general expectation is that
lower perplexities correlate with higher transla-
tion performance, this relation is complicated by
several facts. Since the interpolated models are
deficient (i.e. their probabilities do not sum to 1),
perplexities for weighted counts and our imple-
mentation of linear interpolation cannot be com-
pard. Also, note that not all features are equally
important for decoding. Their weights in the log-
linear model are set through MERT and vary be-
tween optimization runs.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.960484666666667">
This paper contributes to SMT domain adaptation
research in several ways. We expand on work
by (Foster et al., 2010) in establishing transla-
tion model perplexity minimization as a robust
baseline for a weighted combination of translation
models.15 We demonstrate perplexity optimiza-
tion for weighted counts, which are a natural ex-
tension of unadapted MLE training, but are of lit-
tle prominence in domain adaptation research. We
also show that we can separately optimize the four
variable features in the Moses translation model
through perplexity optimization.
We break with prior domain adaptation re-
search in that we do not rely on a binary clustering
of in-domain and out-of-domain training data. We
demonstrate that perplexity minimization scales
well to a higher number of translation models.
This is not only useful for domain adaptation, but
for various tasks that profit from mixture mod-
15The source code is available in the Moses repository
http://github.com/moses-smt/mosesdecoder
elling. We envision that a weighted combination
could be useful to deal with noisy datasets, or ap-
plied after a clustering of training data.
</bodyText>
<sectionHeader confidence="0.996498" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.6035925">
This research was funded by the Swiss National
Science Foundation under grant 105215_126999.
</bodyText>
<sectionHeader confidence="0.999098" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999766934782609">
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation.
Technical report, Final Report, JHU Summer Work-
shop.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the EMNLP 2011
Workshop on Statistical Machine Translation.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical ma-
chine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
9–16, Prague, Czech Republic, June. Association
for Computational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19(2):263–311.
Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and
Ciyou Zhu. 1995. A limited memory algorithm
for bound constrained optimization. SIAM J. Sci.
Comput., 16:1190–1208, September.
Alexandru Ceau¸su, John Tinsley, Jian Zhang, and
Andy Way. 2011. Experiments on domain adap-
tation for patent machine translation in the PLuTO
project. In Proceedings of the 15th conference of
the European Association for Machine Translation,
Leuven, Belgium.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Computer Speech &amp; Language, 13:359–
393.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
176–181, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2007. Machine
Translation by Triangulation: Making Effective Use
of Multi-Parallel Corpora. In Proceedings of the
</reference>
<page confidence="0.989204">
547
</page>
<reference confidence="0.998830522123893">
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 728–735, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
Andrew Finch and Eiichiro Sumita. 2008. Dynamic
model interpolation for statistical machine transla-
tion. In Proceedings of the Third Workshop on
Statistical Machine Translation, StatMT ’08, pages
208–215, Stroudsburg, PA, USA. Association for
Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, StatMT ’07, pages 128–135, Stroudsburg, PA,
USA. Association for Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 451–459,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Philipp Koehn and Kevin Knight. 2001. Knowledge
sources for word-level translation models. In Lil-
lian Lee and Donna Harman, editors, Proceedings
of the 2001 Conference on Empirical Methods in
Natural Language Processing, pages 27–35.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
’07, pages 224–227, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48–54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In ACL 2007, Proceedings of the
45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn, Barry Haddow, Philip Williams, and
Hieu Hoang. 2010. More linguistic annotation
for statistical machine translation. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 115–120, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Philipp Koehn. 2002. Europarl: A Multilingual Cor-
pus for Evaluation of Machine Translation.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Machine Transla-
tion Summit X, pages 79–86, Phuket, Thailand.
Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien,
Keh-Jiann Chen, and Lin-Shan Lee. 1997. Chinese
language model adaptation based on document clas-
sification and multiple domain-specific language
models. In George Kokkinakis, Nikos Fakotakis,
and Evangelos Dermatas, editors, EUROSPEECH.
ISCA.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight esti-
mation for machine translation. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 2 - Volume 2,
pages 708–717, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ’10, pages 220–224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Preslav Nakov and Hwee Tou Ng. 2009. Improved
statistical machine translation for resource-poor
languages using related resource-rich languages. In
Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing: Vol-
ume 3 - Volume 3, EMNLP ’09, pages 1358–1367,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In ACL ’02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311–318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Daniel Varga. 2006. The JRC-Acquis: A multilin-
gual aligned parallel corpus with 20+ languages. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC’2006).
</reference>
<page confidence="0.973854">
548
</page>
<reference confidence="0.999777548387097">
A. Stolcke. 2002. SRILM – An Extensible Language
Modeling Toolkit. In Seventh International Confer-
ence on Spoken Language Processing, pages 901–
904, Denver, CO, USA.
Jörg Tiedemann. 2009. News from opus - a col-
lection of multilingual parallel corpora with tools
and interfaces. In N. Nicolov, K. Bontcheva,
G. Angelova, and R. Mitkov, editors, Recent
Advances in Natural Language Processing, vol-
ume V, pages 237–248. John Benjamins, Amster-
dam/Philadelphia, Borovets, Bulgaria.
Martin Volk, Noah Bubenhofer, Adrian Althaus, Maya
Bangerter, Lenz Furrer, and Beni Ruef. 2010. Chal-
lenges in building a multilingual alpine heritage
corpus. In Proceedings of the Seventh conference
on International Language Resources and Evalu-
ation (LREC’10), Valletta, Malta. European Lan-
guage Resources Association (ELRA).
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,
and Eiichiro Sumita. 2008. Method of selecting
training data to build a compact and efficient trans-
lation model. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP).
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the 20th international conference on
Computational Linguistics, COLING ’04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.99873">
549
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.346358">
<title confidence="0.998262">Perplexity Minimization for Translation Model Domain Adaptation Statistical Machine Translation</title>
<author confidence="0.994008">Rico</author>
<affiliation confidence="0.803836">Institute of Computational University of Binzmühlestr.</affiliation>
<address confidence="0.617741">CH-8050</address>
<email confidence="0.86438">sennrich@cl.uzh.ch</email>
<abstract confidence="0.998039823529412">We investigate the problem of domain adaptation for parallel data in Statistical Machine Translation (SMT). While techniques for domain adaptation of monolingual data can be borrowed for parallel data, we explore conceptual differences between translation model and language model domain adaptation and their effect on performance, such as the fact that translation models typically consist of several features that have different characteristics and can be optimized separately. We also explore adapting multiple (4–10) data sets with no priori between in-domain and out-of-domain data except for an in-domain development set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation.</title>
<date>1999</date>
<tech>Technical report, Final Report, JHU Summer Workshop.</tech>
<contexts>
<context position="11887" citStr="Al-Onaizan et al., 1999" startWordPosition="1905" endWordPosition="1908">rage of all models. Selecting the highest-scoring phrase pair favours statistical outliers and hence is the less robust decision, prone to data noise and data sparseness. 2.4 Perplexity Minimization In language modelling, perplexity is frequently used as a quality measure for language models (Chen and Goodman, 1998). Among other applications, language model perplexity has been used for domain adaptation (Foster and Kuhn, 2007). For translation models, perplexity is most closely associated with EM word alignment (Brown et al., 1993) and has been used to evaluate different alignment algorithms (Al-Onaizan et al., 1999). We investigate translation model perplexity minimization as a method to set model weights in mixture modelling. For the purpose of optimization, the cross-entropy H(p), the perplexity 2H(p), and other derived measures are equivalent. The cross-entropy H(p) is defined as:6 6See (Chen and Goodman, 1998) for a short discussion of the equation. In short, a lower cross-entropy indicates that the model is better able to predict the development set. c(x, y) p(x|y) = c(y) 541 H(p) = − � ˜p(x, y) log2 p(x|y) (4) x,y The phrase pairs (x, y) whose probability we measure, and their empirical probability</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation. Technical report, Final Report, JHU Summer Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="6265" citStr="Axelrod et al., 2011" startWordPosition="971" endWordPosition="974">r patent subdomains. 3We can ignore the fifth feature, the phrase penalty, which is a constant. dividual model probabilities. It is defined as follows: n p(x|y; A) = Aipi(x|y) (1) i=1 with Ai being the interpolation weight of each model i, and with (Ei Ai) = 1. For SMT, linear interpolation of translation models has been used in numerous systems. The approaches diverge in how they set the interpolation weights. Some authors use uniform weights (Cohn and Lapata, 2007), others empirically test different interpolation coefficients (Finch and Sumita, 2008; Yasuda et al., 2008; Nakov and Ng, 2009; Axelrod et al., 2011), others apply monolingual metrics to set the weights for TM interpolation (Foster and Kuhn, 2007; Koehn et al., 2010). There are reasons against all these approaches. Uniform weights are easy to implement, but give little control. Empirically, it has been shown that they often do not perform optimally (Finch and Sumita, 2008; Yasuda et al., 2008). An optimization of BLEU scores on a development set is promising, but slow and impractical. There is no easy way to integrate linear interpolation into loglinear SMT frameworks and perform optimization through MERT. Monolingual optimization objectiv</context>
<context position="16213" citStr="Axelrod et al., 2011" startWordPosition="2609" endWordPosition="2612"> and that the two are to some extent redundant. Foster and Kuhn (2007) find that “both TM and LM adaptation are effective”, but that “combined LM and TM adaptation is not better than LM adaptation on its own”. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand. This has been done for language models using techniques from information retrieval (Zhao et al., 2004), or perplexity (Lin et al., 1997; Moore and Lewis, 2010). Data selection has also been proposed for translation models (Axelrod et al., 2011). Note that for translation models, data selection offers an unattractive trade-off between the data sparseness and the ambiguity problem, and that the optimal amount of data to select is hard to determine. Our discussion of mixture-modelling is relatively coarse-grained, with 2-10 models being combined. Matsoukas et al. (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al. (2010) extend this approach by weighting individual phrase pairs. These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al. (201</context>
<context position="21856" citStr="Axelrod et al., 2011" startWordPosition="3531" endWordPosition="3534"> SMS sent in the wake of the 2010 Haiti earthquake. Originally, Microsoft Research and CMU operated under severe time constraints to build a translation system for this language pair. This limits the ability to empirically verify how much each data set contributes to translation quality, and increases the importance of automated and quick domain adaptation methods. Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE–EN and 1:5 for HT–EN) Previous research has been performed with ratios of 1:100 (Foster et al., 2010) or 1:400 (Axelrod et al., 2011). Since domain adaptation becomes more important when the ratio of IN to OUT is low, and since such low ratios are also realistic11, we also include results for which the amount of in-domain parallel data has been restricted to 10% of the available data set. We used the same development set for language/translation model adaptation and setting the global model weights with MERT. While it is theoretically possible that MERT will give too high weights to models that are optimized on the same development set, we found no empirical evidence for this in experiments with separate development sets. 4</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>CCG supertags in factored statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="10153" citStr="Birch et al., 2007" startWordPosition="1626" endWordPosition="1629">some settings. However, researchers who demonstrated this fact did so with arbitrary weights (e.g. (Koehn, 2002)), or by empirically testing different weights (e.g. (Nakov and Ng, 2009)). We do not know of any research on automatically determining weights for this method, or which is not limited to two corpora. 4For instance if the word pairs (the,der) and (man,Mann) are known, but the phrase pair (the man, der Mann) is not. 5Unlike equation 1, equation 3 does not require that (Ei Ai) = 1. 2.3 Alternative Paths A third method is using multiple translation models as alternative decoding paths (Birch et al., 2007), an idea which Koehn and Schroeder (2007) first used for domain adaptation. This approach has the attractive theoretical property that adding new models is guaranteed to lead to equal or better performance, given the right weights. At best, a model is beneficial with appropriate weights. At worst, we can set the feature weights so that the decoding paths of one model are never picked for the final translation. In practice, each translation model adds 5 features and thus 5 more dimensions to the weight space, which leads to longer search, search errors, and/or overfitting. The expectation is t</context>
</contexts>
<marker>Birch, Osborne, Koehn, 2007</marker>
<rawString>Alexandra Birch, Miles Osborne, and Philipp Koehn. 2007. CCG supertags in factored statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 9–16, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="11800" citStr="Brown et al., 1993" startWordPosition="1892" endWordPosition="1895">near interpolation, the probability of the phrase pair would be the (weighted) average of all models. Selecting the highest-scoring phrase pair favours statistical outliers and hence is the less robust decision, prone to data noise and data sparseness. 2.4 Perplexity Minimization In language modelling, perplexity is frequently used as a quality measure for language models (Chen and Goodman, 1998). Among other applications, language model perplexity has been used for domain adaptation (Foster and Kuhn, 2007). For translation models, perplexity is most closely associated with EM word alignment (Brown et al., 1993) and has been used to evaluate different alignment algorithms (Al-Onaizan et al., 1999). We investigate translation model perplexity minimization as a method to set model weights in mixture modelling. For the purpose of optimization, the cross-entropy H(p), the perplexity 2H(p), and other derived measures are equivalent. The cross-entropy H(p) is defined as:6 6See (Chen and Goodman, 1998) for a short discussion of the equation. In short, a lower cross-entropy indicates that the model is better able to predict the development set. c(x, y) p(x|y) = c(y) 541 H(p) = − � ˜p(x, y) log2 p(x|y) (4) x,</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard H Byrd</author>
<author>Peihuang Lu</author>
<author>Jorge Nocedal</author>
<author>Ciyou Zhu</author>
</authors>
<title>A limited memory algorithm for bound constrained optimization.</title>
<date>1995</date>
<journal>SIAM J. Sci. Comput.,</journal>
<pages>16--1190</pages>
<contexts>
<context position="13137" citStr="Byrd et al., 1995" startWordPosition="2118" endWordPosition="2121">development set, whereas p is the model probability. To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). The objective function is the minimization of the cross-entropy, with the weight vector λ as argument: �λˆ = arg min − A x,y ˜p(x, y) log2 p(x|y; λ) (5) We can fill in equations 1 or 3 for p(x|y; λ). The optimization itself is convex and can be done with off-the-shelf software.7 We use L-BFGS with numerically approximated gradients (Byrd et al., 1995). Perplexity minimization has the advantage that it is well-defined for both weighted counts and linear interpolation, and can be quickly computed. Other than in language modelling, where p(x|y) is the probability of a word given a n-gram history, conditional probabilities in translation models express the probability of a target phrase given a source phrase (or vice versa), which connects the perplexity to the ambiguity problem. The higher the probability of “correct” phrase pairs, the lower the perplexity, and the more likely the model is to successfully resolve the ambiguity. The question i</context>
</contexts>
<marker>Byrd, Lu, Nocedal, Zhu, 1995</marker>
<rawString>Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. 1995. A limited memory algorithm for bound constrained optimization. SIAM J. Sci. Comput., 16:1190–1208, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandru Ceau¸su</author>
<author>John Tinsley</author>
<author>Jian Zhang</author>
<author>Andy Way</author>
</authors>
<title>Experiments on domain adaptation for patent machine translation in the PLuTO project.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th conference of the European Association for Machine Translation,</booktitle>
<location>Leuven, Belgium.</location>
<marker>Ceau¸su, Tinsley, Zhang, Way, 2011</marker>
<rawString>Alexandru Ceau¸su, John Tinsley, Jian Zhang, and Andy Way. 2011. Experiments on domain adaptation for patent machine translation in the PLuTO project. In Proceedings of the 15th conference of the European Association for Machine Translation, Leuven, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>13</volume>
<pages>393</pages>
<contexts>
<context position="11580" citStr="Chen and Goodman, 1998" startWordPosition="1859" endWordPosition="1862">me that all models have the same weights. Note that, if a phrase pair occurs in several models, combining models through alternative paths means that the decoder selects the path with the highest probability, whereas with linear interpolation, the probability of the phrase pair would be the (weighted) average of all models. Selecting the highest-scoring phrase pair favours statistical outliers and hence is the less robust decision, prone to data noise and data sparseness. 2.4 Perplexity Minimization In language modelling, perplexity is frequently used as a quality measure for language models (Chen and Goodman, 1998). Among other applications, language model perplexity has been used for domain adaptation (Foster and Kuhn, 2007). For translation models, perplexity is most closely associated with EM word alignment (Brown et al., 1993) and has been used to evaluate different alignment algorithms (Al-Onaizan et al., 1999). We investigate translation model perplexity minimization as a method to set model weights in mixture modelling. For the purpose of optimization, the cross-entropy H(p), the perplexity 2H(p), and other derived measures are equivalent. The cross-entropy H(p) is defined as:6 6See (Chen and Goo</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Computer Speech &amp; Language, 13:359– 393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>176--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="18147" citStr="Clark et al., 2011" startWordPosition="2924" endWordPosition="2927"> a separate optimization of each translation model feature improve performance? 4.1 Data and Methods In terms of tools and techniques used, we mostly adhere to the work flow described for the WMT 2011 baseline system9. The main tools are Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. We report two translation measures: BLEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al., 2011) for resampling and significance testing. We compare three baselines and four translation model mixture techniques. The three baselines are a purely in-domain model, a purely outof-domain model, and a model trained on the concatenation of the two, which corresponds to equation 3 with uniform weights. Additionally, we evaluate perplexity optimization with weighted counts and the two implementations of linear interpolation contrasted in section 2.1. The two linear interpolations that are contrasted are a naive one, i.e. a direct, unnormalized interpolation of 9http://www.statmt.org/wmt11/baselin</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 176–181, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>728--735</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6115" citStr="Cohn and Lapata, 2007" startWordPosition="948" endWordPosition="951"> by Ceau¸su et al. (2011) that patent translation systems are highly domain-sensitive and suffer from the inclusion of parallel training data from other patent subdomains. 3We can ignore the fifth feature, the phrase penalty, which is a constant. dividual model probabilities. It is defined as follows: n p(x|y; A) = Aipi(x|y) (1) i=1 with Ai being the interpolation weight of each model i, and with (Ei Ai) = 1. For SMT, linear interpolation of translation models has been used in numerous systems. The approaches diverge in how they set the interpolation weights. Some authors use uniform weights (Cohn and Lapata, 2007), others empirically test different interpolation coefficients (Finch and Sumita, 2008; Yasuda et al., 2008; Nakov and Ng, 2009; Axelrod et al., 2011), others apply monolingual metrics to set the weights for TM interpolation (Foster and Kuhn, 2007; Koehn et al., 2010). There are reasons against all these approaches. Uniform weights are easy to implement, but give little control. Empirically, it has been shown that they often do not perform optimally (Finch and Sumita, 2008; Yasuda et al., 2008). An optimization of BLEU scores on a development set is promising, but slow and impractical. There i</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 728–735, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="18006" citStr="Denkowski and Lavie, 2011" startWordPosition="2901" endWordPosition="2904">a higher number of models? 3. Can we apply perplexity minimization to other translation model features such as the lexical weights, and if yes, does a separate optimization of each translation model feature improve performance? 4.1 Data and Methods In terms of tools and techniques used, we mostly adhere to the work flow described for the WMT 2011 baseline system9. The main tools are Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. We report two translation measures: BLEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al., 2011) for resampling and significance testing. We compare three baselines and four translation model mixture techniques. The three baselines are a purely in-domain model, a purely outof-domain model, and a model trained on the concatenation of the two, which corresponds to equation 3 with uniform weights. Additionally, we evaluate perplexity optimization with weighted counts and the two implementations of linear interpolation contrasted in section 2.1. The two</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Dynamic model interpolation for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation, StatMT ’08,</booktitle>
<pages>208--215</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6201" citStr="Finch and Sumita, 2008" startWordPosition="959" endWordPosition="962">and suffer from the inclusion of parallel training data from other patent subdomains. 3We can ignore the fifth feature, the phrase penalty, which is a constant. dividual model probabilities. It is defined as follows: n p(x|y; A) = Aipi(x|y) (1) i=1 with Ai being the interpolation weight of each model i, and with (Ei Ai) = 1. For SMT, linear interpolation of translation models has been used in numerous systems. The approaches diverge in how they set the interpolation weights. Some authors use uniform weights (Cohn and Lapata, 2007), others empirically test different interpolation coefficients (Finch and Sumita, 2008; Yasuda et al., 2008; Nakov and Ng, 2009; Axelrod et al., 2011), others apply monolingual metrics to set the weights for TM interpolation (Foster and Kuhn, 2007; Koehn et al., 2010). There are reasons against all these approaches. Uniform weights are easy to implement, but give little control. Empirically, it has been shown that they often do not perform optimally (Finch and Sumita, 2008; Yasuda et al., 2008). An optimization of BLEU scores on a development set is promising, but slow and impractical. There is no easy way to integrate linear interpolation into loglinear SMT frameworks and perf</context>
</contexts>
<marker>Finch, Sumita, 2008</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2008. Dynamic model interpolation for statistical machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, StatMT ’08, pages 208–215, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for smt.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>128--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6362" citStr="Foster and Kuhn, 2007" startWordPosition="986" endWordPosition="989">ividual model probabilities. It is defined as follows: n p(x|y; A) = Aipi(x|y) (1) i=1 with Ai being the interpolation weight of each model i, and with (Ei Ai) = 1. For SMT, linear interpolation of translation models has been used in numerous systems. The approaches diverge in how they set the interpolation weights. Some authors use uniform weights (Cohn and Lapata, 2007), others empirically test different interpolation coefficients (Finch and Sumita, 2008; Yasuda et al., 2008; Nakov and Ng, 2009; Axelrod et al., 2011), others apply monolingual metrics to set the weights for TM interpolation (Foster and Kuhn, 2007; Koehn et al., 2010). There are reasons against all these approaches. Uniform weights are easy to implement, but give little control. Empirically, it has been shown that they often do not perform optimally (Finch and Sumita, 2008; Yasuda et al., 2008). An optimization of BLEU scores on a development set is promising, but slow and impractical. There is no easy way to integrate linear interpolation into loglinear SMT frameworks and perform optimization through MERT. Monolingual optimization objectives such as language model perplexity have the advantage of being well-known and readily available</context>
<context position="11693" citStr="Foster and Kuhn, 2007" startWordPosition="1876" endWordPosition="1879">hrough alternative paths means that the decoder selects the path with the highest probability, whereas with linear interpolation, the probability of the phrase pair would be the (weighted) average of all models. Selecting the highest-scoring phrase pair favours statistical outliers and hence is the less robust decision, prone to data noise and data sparseness. 2.4 Perplexity Minimization In language modelling, perplexity is frequently used as a quality measure for language models (Chen and Goodman, 1998). Among other applications, language model perplexity has been used for domain adaptation (Foster and Kuhn, 2007). For translation models, perplexity is most closely associated with EM word alignment (Brown et al., 1993) and has been used to evaluate different alignment algorithms (Al-Onaizan et al., 1999). We investigate translation model perplexity minimization as a method to set model weights in mixture modelling. For the purpose of optimization, the cross-entropy H(p), the perplexity 2H(p), and other derived measures are equivalent. The cross-entropy H(p) is defined as:6 6See (Chen and Goodman, 1998) for a short discussion of the equation. In short, a lower cross-entropy indicates that the model is b</context>
<context position="15356" citStr="Foster and Kuhn, 2007" startWordPosition="2465" endWordPosition="2468">t applied by Foster et al. (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMT translation model: the phrase translation probabilities p(t|s) and p(s|t), and the lexical weights lex(t|s) and lex(s|t). 3 Other Domain Adaptation Techniques So far, we discussed mixture modelling for translation models, which is only a subset of domain adaptation techniques in SMT. Mixture-modelling for language models is well established (Foster and Kuhn, 2007). Language model adaptation serves the same purpose as translation model adaptation, i.e. skewing the probability distribution in favour of in-domain translations. This means that LM adaptation may have similar effects as TM adaptation, and that the two are to some extent redundant. Foster and Kuhn (2007) find that “both TM and LM adaptation are effective”, but that “combined LM and TM adaptation is not better than LM adaptation on its own”. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the t</context>
<context position="29608" citStr="Foster and Kuhn, 2007" startWordPosition="4787" endWordPosition="4790">interpolation (modified) alternative paths Table 6: Domain adaptation results HT–EN. Domain: emergency SMS. Full IN TM: Using the full in-domain parallel corpus; small IN TM: using 10% of available in-domain parallel data. tions where we actually expect a simple concatenation to be optimal, e.g. when the data sets have very similar probability distributions. 4.2.1 Individually Optimizing Each TM Feature It is hard to empirically show how translation model perplexity optimization compares to using monolingual perplexity measures for the purpose of weighting translation models, as e.g. done by (Foster and Kuhn, 2007; Koehn et al., 2010). One problem is that there are many different possible configurations for the latter. We can use source side or target side language models, operate with different vocabularies, smoothing techniques, and n-gram orders. One of the theoretical considerations that favour measuring perplexity on the translation model rather than using monolingual measures is that we can optimize each translation model feature separately. In the default Moses translation model, the four features are p(s1t), lex(slt), p(t1s) and lex(tjs). We empirically test different optimization schemes as fo</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for smt. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 128–135, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>451--459</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14767" citStr="Foster et al. (2010)" startWordPosition="2379" endWordPosition="2382">n of convexity: equation 1 is affine; equation 3 linear-fractional. Both are convex in the domain R&gt;o. Consequently, equation 4 is also convex because it is the weighted sum of convex functions. 8There are tasks for which perplexity is known to be unreliable, e.g. for comparing models with different vocabularies. However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs, and merely varies A. Our main technical contributions are as follows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al. (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMT translation model: the phrase translation probabilities p(t|s) and p(s|t), and the lexical weights lex(t|s) and lex(s|t). 3 Other Domain Adaptation Techniques So far, we discussed mixture modelling for translation models, which is only a subset of domain adaptation techniques in SMT. Mixture-modelling for language models is well established (Foster and Kuhn, 2007). Language </context>
<context position="16646" citStr="Foster et al. (2010)" startWordPosition="2679" endWordPosition="2682">rom information retrieval (Zhao et al., 2004), or perplexity (Lin et al., 1997; Moore and Lewis, 2010). Data selection has also been proposed for translation models (Axelrod et al., 2011). Note that for translation models, data selection offers an unattractive trade-off between the data sparseness and the ambiguity problem, and that the optimal amount of data to select is hard to determine. Our discussion of mixture-modelling is relatively coarse-grained, with 2-10 models being combined. Matsoukas et al. (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al. (2010) extend this approach by weighting individual phrase pairs. These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al. (2010) combine the two, applying linear interpolation to combine the instance542 weighted out-of-domain model with an in-domain model. 4 Evaluation Apart from measuring the performance of the approaches introduced in section 2, we want to investigate the following open research questions. 1. Does an implementation of linear interpolation that is more closely tailored to translation modelling outperform a naive implementation? 2. How </context>
<context position="21824" citStr="Foster et al., 2010" startWordPosition="3525" endWordPosition="3528"> task. It consists of emergency SMS sent in the wake of the 2010 Haiti earthquake. Originally, Microsoft Research and CMU operated under severe time constraints to build a translation system for this language pair. This limits the ability to empirically verify how much each data set contributes to translation quality, and increases the importance of automated and quick domain adaptation methods. Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE–EN and 1:5 for HT–EN) Previous research has been performed with ratios of 1:100 (Foster et al., 2010) or 1:400 (Axelrod et al., 2011). Since domain adaptation becomes more important when the ratio of IN to OUT is low, and since such low ratios are also realistic11, we also include results for which the amount of in-domain parallel data has been restricted to 10% of the available data set. We used the same development set for language/translation model adaptation and setting the global model weights with MERT. While it is theoretically possible that MERT will give too high weights to models that are optimized on the same development set, we found no empirical evidence for this in experiments w</context>
<context position="32634" citStr="Foster et al., 2010" startWordPosition="5283" endWordPosition="5286">general expectation is that lower perplexities correlate with higher translation performance, this relation is complicated by several facts. Since the interpolated models are deficient (i.e. their probabilities do not sum to 1), perplexities for weighted counts and our implementation of linear interpolation cannot be compard. Also, note that not all features are equally important for decoding. Their weights in the loglinear model are set through MERT and vary between optimization runs. 5 Conclusion This paper contributes to SMT domain adaptation research in several ways. We expand on work by (Foster et al., 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models.15 We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research. We also show that we can separately optimize the four variable features in the Moses translation model through perplexity optimization. We break with prior domain adaptation research in that we do not rely on a binary clustering of in-domain and out-of-domain training data. We demonstrate t</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 451–459, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Knowledge sources for word-level translation models.</title>
<date>2001</date>
<booktitle>Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>27--35</pages>
<editor>In Lillian Lee and Donna Harman, editors,</editor>
<contexts>
<context position="2370" citStr="Koehn and Knight (2001)" startWordPosition="351" endWordPosition="354">ry combination of in-domain and out-of-domain data. If we can scale up the number of models whose contributions we weight, this reduces the need for a priori knowledge about the fitness1 of each potential training text, and opens new research opportunities, for instance experiments with clustered training data. 2 Domain Adaptation for Translation Models To motivate efforts in domain adaptation, let us review why additional training data can improve, but also decrease translation quality. Adding more training data to a translation system is easy to motivate through the data sparseness problem. Koehn and Knight (2001) show that translation quality correlates strongly with how often a word occurs in the training corpus. Rare words or phrases pose a problem in several stages of MT modelling, from word alignment to the computation of translation probabilities through Maximum Likelihood Estimation. Unknown words are typically copied verbatim to the target text, which may be a good strategy for named entities, but is often wrong otherwise. In general, more data allows for a better word alignment, a better estimation of translation probabilities, and for the consideration of more context (in phrase-based or synt</context>
</contexts>
<marker>Koehn, Knight, 2001</marker>
<rawString>Philipp Koehn and Kevin Knight. 2001. Knowledge sources for word-level translation models. In Lillian Lee and Donna Harman, editors, Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 27–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>224--227</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10195" citStr="Koehn and Schroeder (2007)" startWordPosition="1633" endWordPosition="1636"> who demonstrated this fact did so with arbitrary weights (e.g. (Koehn, 2002)), or by empirically testing different weights (e.g. (Nakov and Ng, 2009)). We do not know of any research on automatically determining weights for this method, or which is not limited to two corpora. 4For instance if the word pairs (the,der) and (man,Mann) are known, but the phrase pair (the man, der Mann) is not. 5Unlike equation 1, equation 3 does not require that (Ei Ai) = 1. 2.3 Alternative Paths A third method is using multiple translation models as alternative decoding paths (Birch et al., 2007), an idea which Koehn and Schroeder (2007) first used for domain adaptation. This approach has the attractive theoretical property that adding new models is guaranteed to lead to equal or better performance, given the right weights. At best, a model is beneficial with appropriate weights. At worst, we can set the feature weights so that the decoding paths of one model are never picked for the final translation. In practice, each translation model adds 5 features and thus 5 more dimensions to the weight space, which leads to longer search, search errors, and/or overfitting. The expectation is that, at least with MERT, using alternative</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 224–227, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8400" citStr="Koehn et al., 2003" startWordPosition="1329" endWordPosition="1332"> vector A to 1. We do this for p(t|s) and lex(t|s), but not for p(s|t) and lex(s|t), the reasoning being the con540 sequences for perplexity minimization (see section 2.4). Namely, we do not want to penalize a small in-domain model for having a high outof-vocabulary rate on the source side, but we do want to penalize models that know the source phrase, but not its correct translation. A second modification pertains to the lexical weights lex(sIt) and lex(tIs), which form no true probability distribution, but are derived from the individual word translation probabilities of a phrase pair (see (Koehn et al., 2003)). We propose to not interpolate the features directly, but the word translation probabilities which are the basis of the lexical weight computation. The reason for this is that word pairs are less sparse than phrase pairs, so that we can even compute lexical weights for phrase pairs which are unknown in a model.4 2.2 Weighted Counts Weighting of different corpora can also be implemented through a modified Maximum Likelihood Estimation. The traditional equation for MLE is: c(x,y) (2) Ex, c(x&apos;, y) where c denotes the count of an observation, and p the model probability. If we generalize the for</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="17792" citStr="Koehn et al., 2007" startWordPosition="2866" endWordPosition="2869">lored to translation modelling outperform a naive implementation? 2. How do the approaches perform outside a binary setting, i.e. when we do not work with one in-domain and one out-of-domain model, but with a higher number of models? 3. Can we apply perplexity minimization to other translation model features such as the lexical weights, and if yes, does a separate optimization of each translation model feature improve performance? 4.1 Data and Methods In terms of tools and techniques used, we mostly adhere to the work flow described for the WMT 2011 baseline system9. The main tools are Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. We report two translation measures: BLEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al., 2011) for resampling and significance testing. We compare three baselines and four translation model mixture techniques. The three baselines are a purely in-domain model, a purely outof-domain model, and a model trained on the concatenation of the tw</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177– 180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Barry Haddow</author>
<author>Philip Williams</author>
<author>Hieu Hoang</author>
</authors>
<title>More linguistic annotation for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>115--120</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="6383" citStr="Koehn et al., 2010" startWordPosition="990" endWordPosition="993">ties. It is defined as follows: n p(x|y; A) = Aipi(x|y) (1) i=1 with Ai being the interpolation weight of each model i, and with (Ei Ai) = 1. For SMT, linear interpolation of translation models has been used in numerous systems. The approaches diverge in how they set the interpolation weights. Some authors use uniform weights (Cohn and Lapata, 2007), others empirically test different interpolation coefficients (Finch and Sumita, 2008; Yasuda et al., 2008; Nakov and Ng, 2009; Axelrod et al., 2011), others apply monolingual metrics to set the weights for TM interpolation (Foster and Kuhn, 2007; Koehn et al., 2010). There are reasons against all these approaches. Uniform weights are easy to implement, but give little control. Empirically, it has been shown that they often do not perform optimally (Finch and Sumita, 2008; Yasuda et al., 2008). An optimization of BLEU scores on a development set is promising, but slow and impractical. There is no easy way to integrate linear interpolation into loglinear SMT frameworks and perform optimization through MERT. Monolingual optimization objectives such as language model perplexity have the advantage of being well-known and readily available, but their relation </context>
<context position="29629" citStr="Koehn et al., 2010" startWordPosition="4791" endWordPosition="4794">) alternative paths Table 6: Domain adaptation results HT–EN. Domain: emergency SMS. Full IN TM: Using the full in-domain parallel corpus; small IN TM: using 10% of available in-domain parallel data. tions where we actually expect a simple concatenation to be optimal, e.g. when the data sets have very similar probability distributions. 4.2.1 Individually Optimizing Each TM Feature It is hard to empirically show how translation model perplexity optimization compares to using monolingual perplexity measures for the purpose of weighting translation models, as e.g. done by (Foster and Kuhn, 2007; Koehn et al., 2010). One problem is that there are many different possible configurations for the latter. We can use source side or target side language models, operate with different vocabularies, smoothing techniques, and n-gram orders. One of the theoretical considerations that favour measuring perplexity on the translation model rather than using monolingual measures is that we can optimize each translation model feature separately. In the default Moses translation model, the four features are p(s1t), lex(slt), p(t1s) and lex(tjs). We empirically test different optimization schemes as follows. We optimize pe</context>
</contexts>
<marker>Koehn, Haddow, Williams, Hoang, 2010</marker>
<rawString>Philipp Koehn, Barry Haddow, Philip Williams, and Hieu Hoang. 2010. More linguistic annotation for statistical machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 115–120, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Multilingual Corpus for Evaluation of Machine Translation.</title>
<date>2002</date>
<contexts>
<context position="9646" citStr="Koehn, 2002" startWordPosition="1539" endWordPosition="1540"> n corpora, and assign a weight λi to each, we get5: Eni=1 λici(x, y) (3) p(x|y; λ) = Ei=1 Ex, λici(xl, y) The main difference to linear interpolation is that this equation takes into account how wellevidenced a phrase pair is. This includes the distinction between lack of evidence and negative evidence, which is missing in a naive implementation of linear interpolation. Translation models trained with weighted counts have been discussed before, and have been shown to outperform uniform ones in some settings. However, researchers who demonstrated this fact did so with arbitrary weights (e.g. (Koehn, 2002)), or by empirically testing different weights (e.g. (Nakov and Ng, 2009)). We do not know of any research on automatically determining weights for this method, or which is not limited to two corpora. 4For instance if the word pairs (the,der) and (man,Mann) are known, but the phrase pair (the man, der Mann) is not. 5Unlike equation 1, equation 3 does not require that (Ei Ai) = 1. 2.3 Alternative Paths A third method is using multiple translation models as alternative decoding paths (Birch et al., 2007), an idea which Koehn and Schroeder (2007) first used for domain adaptation. This approach ha</context>
</contexts>
<marker>Koehn, 2002</marker>
<rawString>Philipp Koehn. 2002. Europarl: A Multilingual Corpus for Evaluation of Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Machine Translation Summit X,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="3683" citStr="Koehn, 2005" startWordPosition="565" endWordPosition="566">ly ambiguous, and a strong source of ambiguity is the 1We borrow this term from early evolutionary biology to emphasize that the question in domain adaptation is not how “good” or “bad” the data is, but how well-adapted it is to the task at hand. 539 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 539–549, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics domain of a text. The German word “Wort” (engl. word) is typically translated as floor in Europarl, a corpus of Parliamentary Proceedings (Koehn, 2005), owing to the high frequency of phrases such as you have the floor, which is translated into German as Sie haben das Wort. This translation is highly idiomatic and unlikely to occur in other contexts. Still, adding Europarl as out-of-domain training data shifts the probability distribution of p(t|“Wort”) in favour of p(“floor”|“Wort”), and may thus lead to improper translations. We will refer to the two problems as the data sparseness problem and the ambiguity problem. Adding out-of-domain data typically mitigates the data sparseness problem, but exacerbates the ambiguity problem. The net gai</context>
<context position="20754" citStr="Koehn, 2005" startWordPosition="3346" endWordPosition="3347">ion of all data sets, optimized for minimal perplexity on the in-domain development set. While unadapted language models are becoming more rare in domain adaptation research, they allow us to contrast different TM mixtures without the effect on performance being (partially) hidden by language model adaptation with the same effect. The first data set is a DE–FR translation scenario in the domain of mountaineering. The indomain corpus is a collection of Alpine Club pub543 lications (Volk et al., 2010). As parallel out-ofdomain dataset, we use Europarl, a collection of parliamentary proceedings (Koehn, 2005), JRCAcquis, a collection of legislative texts (Steinberger et al., 2006), and OpenSubtitles v2, a parallel corpus extracted from film subtitles10 (Tiedemann, 2009). For language modelling, we use indomain data and data from the 2011 Workshop on Statistical Machine Translation. The respective sizes of the data sets are listed in tables 1 and 2. As the second data set, we use the Haitian Creole – English data from the WMT 2011 featured translation task. It consists of emergency SMS sent in the wake of the 2010 Haiti earthquake. Originally, Microsoft Research and CMU operated under severe time c</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Machine Translation Summit X, pages 79–86, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung-Chien Lin</author>
<author>Chi-Lung Tsai</author>
<author>Lee-Feng Chien</author>
<author>Keh-Jiann Chen</author>
<author>Lin-Shan Lee</author>
</authors>
<title>Chinese language model adaptation based on document classification and multiple domain-specific language models.</title>
<date>1997</date>
<editor>In George Kokkinakis, Nikos Fakotakis, and Evangelos Dermatas, editors, EUROSPEECH.</editor>
<publisher>ISCA.</publisher>
<contexts>
<context position="16104" citStr="Lin et al., 1997" startWordPosition="2590" endWordPosition="2593">vour of in-domain translations. This means that LM adaptation may have similar effects as TM adaptation, and that the two are to some extent redundant. Foster and Kuhn (2007) find that “both TM and LM adaptation are effective”, but that “combined LM and TM adaptation is not better than LM adaptation on its own”. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand. This has been done for language models using techniques from information retrieval (Zhao et al., 2004), or perplexity (Lin et al., 1997; Moore and Lewis, 2010). Data selection has also been proposed for translation models (Axelrod et al., 2011). Note that for translation models, data selection offers an unattractive trade-off between the data sparseness and the ambiguity problem, and that the optimal amount of data to select is hard to determine. Our discussion of mixture-modelling is relatively coarse-grained, with 2-10 models being combined. Matsoukas et al. (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al. (2010) extend this approach by weighting individual phrase pairs</context>
</contexts>
<marker>Lin, Tsai, Chien, Chen, Lee, 1997</marker>
<rawString>Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Keh-Jiann Chen, and Lin-Shan Lee. 1997. Chinese language model adaptation based on document classification and multiple domain-specific language models. In George Kokkinakis, Nikos Fakotakis, and Evangelos Dermatas, editors, EUROSPEECH. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>708--717</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16542" citStr="Matsoukas et al. (2009)" startWordPosition="2660" endWordPosition="2663">is considered more relevant for the task at hand. This has been done for language models using techniques from information retrieval (Zhao et al., 2004), or perplexity (Lin et al., 1997; Moore and Lewis, 2010). Data selection has also been proposed for translation models (Axelrod et al., 2011). Note that for translation models, data selection offers an unattractive trade-off between the data sparseness and the ambiguity problem, and that the optimal amount of data to select is hard to determine. Our discussion of mixture-modelling is relatively coarse-grained, with 2-10 models being combined. Matsoukas et al. (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al. (2010) extend this approach by weighting individual phrase pairs. These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al. (2010) combine the two, applying linear interpolation to combine the instance542 weighted out-of-domain model with an in-domain model. 4 Evaluation Apart from measuring the performance of the approaches introduced in section 2, we want to investigate the following open research questions. 1. Does an implementation of linear interpo</context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, pages 708–717, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10,</booktitle>
<pages>220--224</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16128" citStr="Moore and Lewis, 2010" startWordPosition="2594" endWordPosition="2597">translations. This means that LM adaptation may have similar effects as TM adaptation, and that the two are to some extent redundant. Foster and Kuhn (2007) find that “both TM and LM adaptation are effective”, but that “combined LM and TM adaptation is not better than LM adaptation on its own”. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand. This has been done for language models using techniques from information retrieval (Zhao et al., 2004), or perplexity (Lin et al., 1997; Moore and Lewis, 2010). Data selection has also been proposed for translation models (Axelrod et al., 2011). Note that for translation models, data selection offers an unattractive trade-off between the data sparseness and the ambiguity problem, and that the optimal amount of data to select is hard to determine. Our discussion of mixture-modelling is relatively coarse-grained, with 2-10 models being combined. Matsoukas et al. (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al. (2010) extend this approach by weighting individual phrase pairs. These more fine-graine</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, pages 220–224, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Improved statistical machine translation for resource-poor languages using related resource-rich languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, EMNLP ’09,</booktitle>
<pages>1358--1367</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6242" citStr="Nakov and Ng, 2009" startWordPosition="967" endWordPosition="970">ining data from other patent subdomains. 3We can ignore the fifth feature, the phrase penalty, which is a constant. dividual model probabilities. It is defined as follows: n p(x|y; A) = Aipi(x|y) (1) i=1 with Ai being the interpolation weight of each model i, and with (Ei Ai) = 1. For SMT, linear interpolation of translation models has been used in numerous systems. The approaches diverge in how they set the interpolation weights. Some authors use uniform weights (Cohn and Lapata, 2007), others empirically test different interpolation coefficients (Finch and Sumita, 2008; Yasuda et al., 2008; Nakov and Ng, 2009; Axelrod et al., 2011), others apply monolingual metrics to set the weights for TM interpolation (Foster and Kuhn, 2007; Koehn et al., 2010). There are reasons against all these approaches. Uniform weights are easy to implement, but give little control. Empirically, it has been shown that they often do not perform optimally (Finch and Sumita, 2008; Yasuda et al., 2008). An optimization of BLEU scores on a development set is promising, but slow and impractical. There is no easy way to integrate linear interpolation into loglinear SMT frameworks and perform optimization through MERT. Monolingua</context>
<context position="9719" citStr="Nakov and Ng, 2009" startWordPosition="1549" endWordPosition="1552"> y) (3) p(x|y; λ) = Ei=1 Ex, λici(xl, y) The main difference to linear interpolation is that this equation takes into account how wellevidenced a phrase pair is. This includes the distinction between lack of evidence and negative evidence, which is missing in a naive implementation of linear interpolation. Translation models trained with weighted counts have been discussed before, and have been shown to outperform uniform ones in some settings. However, researchers who demonstrated this fact did so with arbitrary weights (e.g. (Koehn, 2002)), or by empirically testing different weights (e.g. (Nakov and Ng, 2009)). We do not know of any research on automatically determining weights for this method, or which is not limited to two corpora. 4For instance if the word pairs (the,der) and (man,Mann) are known, but the phrase pair (the man, der Mann) is not. 5Unlike equation 1, equation 3 does not require that (Ei Ai) = 1. 2.3 Alternative Paths A third method is using multiple translation models as alternative decoding paths (Birch et al., 2007), an idea which Koehn and Schroeder (2007) first used for domain adaptation. This approach has the attractive theoretical property that adding new models is guarantee</context>
</contexts>
<marker>Nakov, Ng, 2009</marker>
<rawString>Preslav Nakov and Hwee Tou Ng. 2009. Improved statistical machine translation for resource-poor languages using related resource-rich languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, EMNLP ’09, pages 1358–1367, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="12782" citStr="Och and Ney, 2003" startWordPosition="2054" endWordPosition="2057">ee (Chen and Goodman, 1998) for a short discussion of the equation. In short, a lower cross-entropy indicates that the model is better able to predict the development set. c(x, y) p(x|y) = c(y) 541 H(p) = − � ˜p(x, y) log2 p(x|y) (4) x,y The phrase pairs (x, y) whose probability we measure, and their empirical probability p˜ need to be extracted from a development set, whereas p is the model probability. To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). The objective function is the minimization of the cross-entropy, with the weight vector λ as argument: �λˆ = arg min − A x,y ˜p(x, y) log2 p(x|y; λ) (5) We can fill in equations 1 or 3 for p(x|y; λ). The optimization itself is convex and can be done with off-the-shelf software.7 We use L-BFGS with numerically approximated gradients (Byrd et al., 1995). Perplexity minimization has the advantage that it is well-defined for both weighted counts and linear interpolation, and can be quickly computed. Other than in language modelling, where p(x|y) is the probability of a word given a n-gram histor</context>
<context position="17847" citStr="Och and Ney, 2003" startWordPosition="2875" endWordPosition="2878">entation? 2. How do the approaches perform outside a binary setting, i.e. when we do not work with one in-domain and one out-of-domain model, but with a higher number of models? 3. Can we apply perplexity minimization to other translation model features such as the lexical weights, and if yes, does a separate optimization of each translation model feature improve performance? 4.1 Data and Methods In terms of tools and techniques used, we mostly adhere to the work flow described for the WMT 2011 baseline system9. The main tools are Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. We report two translation measures: BLEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al., 2011) for resampling and significance testing. We compare three baselines and four translation model mixture techniques. The three baselines are a purely in-domain model, a purely outof-domain model, and a model trained on the concatenation of the two, which corresponds to equation 3 with uniform weights</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="17963" citStr="Papineni et al., 2002" startWordPosition="2894" endWordPosition="2897"> and one out-of-domain model, but with a higher number of models? 3. Can we apply perplexity minimization to other translation model features such as the lexical weights, and if yes, does a separate optimization of each translation model feature improve performance? 4.1 Data and Methods In terms of tools and techniques used, we mostly adhere to the work flow described for the WMT 2011 baseline system9. The main tools are Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. We report two translation measures: BLEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al., 2011) for resampling and significance testing. We compare three baselines and four translation model mixture techniques. The three baselines are a purely in-domain model, a purely outof-domain model, and a model trained on the concatenation of the two, which corresponds to equation 3 with uniform weights. Additionally, we evaluate perplexity optimization with weighted counts and the two implementations of linear inter</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
<author>Bruno Pouliquen</author>
<author>Anna Widiger</author>
<author>Camelia Ignat</author>
<author>Tomaz Erjavec</author>
<author>Dan Tufis</author>
<author>Daniel Varga</author>
</authors>
<title>The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC’2006).</booktitle>
<contexts>
<context position="20827" citStr="Steinberger et al., 2006" startWordPosition="3355" endWordPosition="3359">e in-domain development set. While unadapted language models are becoming more rare in domain adaptation research, they allow us to contrast different TM mixtures without the effect on performance being (partially) hidden by language model adaptation with the same effect. The first data set is a DE–FR translation scenario in the domain of mountaineering. The indomain corpus is a collection of Alpine Club pub543 lications (Volk et al., 2010). As parallel out-ofdomain dataset, we use Europarl, a collection of parliamentary proceedings (Koehn, 2005), JRCAcquis, a collection of legislative texts (Steinberger et al., 2006), and OpenSubtitles v2, a parallel corpus extracted from film subtitles10 (Tiedemann, 2009). For language modelling, we use indomain data and data from the 2011 Workshop on Statistical Machine Translation. The respective sizes of the data sets are listed in tables 1 and 2. As the second data set, we use the Haitian Creole – English data from the WMT 2011 featured translation task. It consists of emergency SMS sent in the wake of the 2010 Haiti earthquake. Originally, Microsoft Research and CMU operated under severe time constraints to build a translation system for this language pair. This lim</context>
</contexts>
<marker>Steinberger, Pouliquen, Widiger, Ignat, Erjavec, Tufis, Varga, 2006</marker>
<rawString>Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Daniel Varga. 2006. The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC’2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Seventh International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO, USA.</location>
<contexts>
<context position="17815" citStr="Stolcke, 2002" startWordPosition="2871" endWordPosition="2872">ng outperform a naive implementation? 2. How do the approaches perform outside a binary setting, i.e. when we do not work with one in-domain and one out-of-domain model, but with a higher number of models? 3. Can we apply perplexity minimization to other translation model features such as the lexical weights, and if yes, does a separate optimization of each translation model feature improve performance? 4.1 Data and Methods In terms of tools and techniques used, we mostly adhere to the work flow described for the WMT 2011 baseline system9. The main tools are Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. We report two translation measures: BLEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al., 2011) for resampling and significance testing. We compare three baselines and four translation model mixture techniques. The three baselines are a purely in-domain model, a purely outof-domain model, and a model trained on the concatenation of the two, which corresponds to</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Seventh International Conference on Spoken Language Processing, pages 901– 904, Denver, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jörg Tiedemann</author>
</authors>
<title>News from opus - a collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing,</booktitle>
<volume>volume V,</volume>
<pages>237--248</pages>
<editor>In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors,</editor>
<location>Amsterdam/Philadelphia, Borovets, Bulgaria.</location>
<contexts>
<context position="20918" citStr="Tiedemann, 2009" startWordPosition="3371" endWordPosition="3373">tion research, they allow us to contrast different TM mixtures without the effect on performance being (partially) hidden by language model adaptation with the same effect. The first data set is a DE–FR translation scenario in the domain of mountaineering. The indomain corpus is a collection of Alpine Club pub543 lications (Volk et al., 2010). As parallel out-ofdomain dataset, we use Europarl, a collection of parliamentary proceedings (Koehn, 2005), JRCAcquis, a collection of legislative texts (Steinberger et al., 2006), and OpenSubtitles v2, a parallel corpus extracted from film subtitles10 (Tiedemann, 2009). For language modelling, we use indomain data and data from the 2011 Workshop on Statistical Machine Translation. The respective sizes of the data sets are listed in tables 1 and 2. As the second data set, we use the Haitian Creole – English data from the WMT 2011 featured translation task. It consists of emergency SMS sent in the wake of the 2010 Haiti earthquake. Originally, Microsoft Research and CMU operated under severe time constraints to build a translation system for this language pair. This limits the ability to empirically verify how much each data set contributes to translation qua</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>Jörg Tiedemann. 2009. News from opus - a collection of multilingual parallel corpora with tools and interfaces. In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing, volume V, pages 237–248. John Benjamins, Amsterdam/Philadelphia, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Volk</author>
<author>Noah Bubenhofer</author>
<author>Adrian Althaus</author>
<author>Maya Bangerter</author>
<author>Lenz Furrer</author>
<author>Beni Ruef</author>
</authors>
<title>Challenges in building a multilingual alpine heritage corpus.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="20646" citStr="Volk et al., 2010" startWordPosition="3329" endWordPosition="3332">data sets provided for the WMT 2011 translation task, and an adapted language model which is the linear interpolation of all data sets, optimized for minimal perplexity on the in-domain development set. While unadapted language models are becoming more rare in domain adaptation research, they allow us to contrast different TM mixtures without the effect on performance being (partially) hidden by language model adaptation with the same effect. The first data set is a DE–FR translation scenario in the domain of mountaineering. The indomain corpus is a collection of Alpine Club pub543 lications (Volk et al., 2010). As parallel out-ofdomain dataset, we use Europarl, a collection of parliamentary proceedings (Koehn, 2005), JRCAcquis, a collection of legislative texts (Steinberger et al., 2006), and OpenSubtitles v2, a parallel corpus extracted from film subtitles10 (Tiedemann, 2009). For language modelling, we use indomain data and data from the 2011 Workshop on Statistical Machine Translation. The respective sizes of the data sets are listed in tables 1 and 2. As the second data set, we use the Haitian Creole – English data from the WMT 2011 featured translation task. It consists of emergency SMS sent i</context>
</contexts>
<marker>Volk, Bubenhofer, Althaus, Bangerter, Furrer, Ruef, 2010</marker>
<rawString>Martin Volk, Noah Bubenhofer, Adrian Althaus, Maya Bangerter, Lenz Furrer, and Beni Ruef. 2010. Challenges in building a multilingual alpine heritage corpus. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Yasuda</author>
<author>Ruiqiang Zhang</author>
<author>Hirofumi Yamamoto</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Method of selecting training data to build a compact and efficient translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing (IJCNLP).</booktitle>
<contexts>
<context position="6222" citStr="Yasuda et al., 2008" startWordPosition="963" endWordPosition="966">usion of parallel training data from other patent subdomains. 3We can ignore the fifth feature, the phrase penalty, which is a constant. dividual model probabilities. It is defined as follows: n p(x|y; A) = Aipi(x|y) (1) i=1 with Ai being the interpolation weight of each model i, and with (Ei Ai) = 1. For SMT, linear interpolation of translation models has been used in numerous systems. The approaches diverge in how they set the interpolation weights. Some authors use uniform weights (Cohn and Lapata, 2007), others empirically test different interpolation coefficients (Finch and Sumita, 2008; Yasuda et al., 2008; Nakov and Ng, 2009; Axelrod et al., 2011), others apply monolingual metrics to set the weights for TM interpolation (Foster and Kuhn, 2007; Koehn et al., 2010). There are reasons against all these approaches. Uniform weights are easy to implement, but give little control. Empirically, it has been shown that they often do not perform optimally (Finch and Sumita, 2008; Yasuda et al., 2008). An optimization of BLEU scores on a development set is promising, but slow and impractical. There is no easy way to integrate linear interpolation into loglinear SMT frameworks and perform optimization thro</context>
</contexts>
<marker>Yasuda, Zhang, Yamamoto, Sumita, 2008</marker>
<rawString>Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto, and Eiichiro Sumita. 2008. Method of selecting training data to build a compact and efficient translation model. In Proceedings of the 3rd International Joint Conference on Natural Language Processing (IJCNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
</authors>
<title>Language model adaptation for statistical machine translation with structured query models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16071" citStr="Zhao et al., 2004" startWordPosition="2584" endWordPosition="2587"> the probability distribution in favour of in-domain translations. This means that LM adaptation may have similar effects as TM adaptation, and that the two are to some extent redundant. Foster and Kuhn (2007) find that “both TM and LM adaptation are effective”, but that “combined LM and TM adaptation is not better than LM adaptation on its own”. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand. This has been done for language models using techniques from information retrieval (Zhao et al., 2004), or perplexity (Lin et al., 1997; Moore and Lewis, 2010). Data selection has also been proposed for translation models (Axelrod et al., 2011). Note that for translation models, data selection offers an unattractive trade-off between the data sparseness and the ambiguity problem, and that the optimal amount of data to select is hard to determine. Our discussion of mixture-modelling is relatively coarse-grained, with 2-10 models being combined. Matsoukas et al. (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al. (2010) extend this approach by </context>
</contexts>
<marker>Zhao, Eck, Vogel, 2004</marker>
<rawString>Bing Zhao, Matthias Eck, and Stephan Vogel. 2004. Language model adaptation for statistical machine translation with structured query models. In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>