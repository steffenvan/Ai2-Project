<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002576">
<title confidence="0.993906">
Modelling lexical redundancy for machine translation
</title>
<author confidence="0.99843">
David Talbot and Miles Osborne
</author>
<affiliation confidence="0.939607">
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, UK
</affiliation>
<email confidence="0.992789">
d.r.talbot@sms.ed.ac.uk, miles@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993773" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993536">
Certain distinctions made in the lexicon
of one language may be redundant when
translating into another language. We
quantify redundancy among source types
by the similarity of their distributions over
target types. We propose a language-
independent framework for minimising
lexical redundancy that can be optimised
directly from parallel text. Optimisation
of the source lexicon for a given target lan-
guage is viewed as model selection over a
set of cluster-based translation models.
Redundant distinctions between types may
exhibit monolingual regularities, for ex-
ample, inflexion patterns. We define a
prior over model structure using a Markov
random field and learn features over sets
of monolingual types that are predictive
of bilingual redundancy. The prior makes
model selection more robust without the
need for language-specific assumptions re-
garding redundancy. Using these mod-
els in a phrase-based SMT system, we
show significant improvements in transla-
tion quality for certain language pairs.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939641509434">
Data-driven machine translation (MT) relies on
models that can be efficiently estimated from par-
allel text. Token-level independence assumptions
based on word-alignments can be used to decom-
pose parallel corpora into manageable units for pa-
rameter estimation. However, if training data is
scarce or language pairs encode significantly dif-
ferent information in the lexicon, such as Czech
and English, additional independence assumptions
may assist the model estimation process.
Standard statistical translation models use sep-
arate parameters for each pair of source and target
types. In these models, distinctions in either lex-
icon that are redundant to the translation process
will result in unwarranted model complexity and
make parameter estimation from limited parallel
data more difficult. A natural way to eliminate
such lexical redundancy is to group types into ho-
mogeneous clusters that do not differ significantly
in their distributions over types in the other lan-
guage. Cluster-based translation models capture
the corresponding independence assumptions.
Previous work on bilingual clustering has fo-
cused on coarse partitions of the lexicon that
resemble automatically induced part-of-speech
classes. These were used to model generic
word-alignment patterns such as noun-adjective
re-ordering between English and French (Och,
1998). In contrast, we induce fine-grained parti-
tions of the lexicon, conceptually closer to auto-
matic lemmatisation, optimised specifically to as-
sign translation probabilities. Unlike lemmatisa-
tion or stemming, our method specifically quanti-
fies lexical redundancy in a bilingual setting and
does not make language-specific assumptions.
We tackle the problem of redundancy in the
translation lexicon via Bayesian model selection
over a set of cluster-based translation models. We
search for the model, defined by a clustering of
the source lexicon, that maximises the marginal
likelihood of target tokens in parallel data. In this
optimisation, source types are combined into clus-
ters if their distributions over target types are too
similar to warrant distinct parameters.
Redundant distinctions between types may ex-
hibit regularities within a language, for instance,
inflexion patterns. These can be used to guide
model selection. Here we show that the inclusion
of a model ‘prior’ over the lexicon structure leads
to more robust translation models. Although a pri-
ori we do not know which monolingual features
characterise redundancy for a given language pair,
by defining a model over the prior monolingual
</bodyText>
<page confidence="0.977973">
969
</page>
<note confidence="0.533062">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 969–976,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999966142857143">
space of source types and cluster assignments, we
can introduce an inductive bias that allows cluster-
ing decisions in different parts of the lexicon to in-
fluence one another via monolingual features. We
use an EM-type algorithm to learn weights for a
Markov random field parameterisation of this prior
over lexicon structure.
We obtain significant improvements in transla-
tion quality as measured by BLEU, incorporating
these optimised model within a phrase-based SMT
system for three different language pairs. The
MRF prior improves the results and picks up fea-
tures that appear to agree with linguistic intuitions
of redundancy for the language pairs considered.
</bodyText>
<sectionHeader confidence="0.849726" genericHeader="method">
2 Lexical redundancy between languages
</sectionHeader>
<bodyText confidence="0.999919169811321">
In statistical MT, the source and target lexicons
are usually defined as the sets of distinct types ob-
served in the parallel training corpus for each lan-
guage. Such models may not be optimal for cer-
tain language pairs and training regimes.
A word-level statistical translation model ap-
proximates the probability Pr(E|F) that a source
type indexed by F will be translated as a target
type indexed by E. Standard models, e.g. Brown
et al. (1993), consist of discrete probability distri-
butions with separate parameters for each unique
pairing of a source and target types; no attempt is
made to leverage structure within the event spaces
£ and F during parameter estimation. This results
in a large number of parameters that must be esti-
mated from limited amounts of parallel corpora.
We refer to distinctions made between lexical
types in one language that do not result in different
distributions over types in the other language as
lexically redundant for the language pair. Since
the role of the translation model is to determine a
distribution over target types given a source type,
when the corresponding target distributions do not
vary significantly over a set of source types, the
model gains nothing by maintaining a distinct set
of parameters for each member of this set.
Lexical redundancy may arise when languages
differ in the specificity with which they refer to the
same concepts. For instance, colours of the spec-
trum may be partitioned differently (e.g. blue in
English v.s. sinii and goluboi in Russian). It will
also arise when languages explicitly encode differ-
ent information in the lexicon. For example, trans-
lating from French to English, a standard model
would treat the following pairs of source and tar-
get types as distinct events with entirely unre-
lated parameters: (vert, green), (verte, green),
(verts, green) and (vertes, green). Here the
French types differ only in their final suffixes due
to adjectival agreement. Since there is no equiva-
lent mechanism in English, these distinctions are
redundant with respect to this target language.
Distinctions that are redundant in the source
lexicon when translating into one language may,
however, be significant when translating into an-
other. For instance, the French adjectival number
agreement (the addition of an s) may be significant
when translating to Russian which also marks ad-
jectives for number (the inflexion to -ye).
We can remove redundancy from the translation
model by conflating redundant types, e.g. vert = .
{vert, verte, verts, vertes}, and averaging bilin-
gual statistics associated with these events.
</bodyText>
<sectionHeader confidence="0.743071" genericHeader="method">
3 Eliminating redundancy in the model
</sectionHeader>
<bodyText confidence="0.99996725">
Redundancy in the translation model can be
viewed as unwarranted model complexity. A
cluster-based translation model defined via a hard-
clustering of the lexicon can reduce this com-
plexity by introducing additional independence as-
sumptions: given the source cluster label, cj, the
target type, ez, is assumed to be independent of the
exact source type, fj, observed, i.e., p(ez|fj) �
p(ez|cj). Optimising the model for lexical redun-
dancy can be viewed as model selection over a set
of such cluster-based translation models.
We formulate model search as a maximum a
posteriori optimisation: the data-dependent term,
p(D|C), quantifies evidence provided for a model,
C, by bilingual training data, D, while the prior,
p(C), can assert a preference for a particular
model structure (clustering of the source lexicon)
on the basis of monolingual features. Both terms
have parameters that are estimated from data. For-
mally, we search for C*,
</bodyText>
<equation confidence="0.9990795">
C* = arg maxC p(C|D)
=argmaxC p(C)p(D|C). (1)
</equation>
<bodyText confidence="0.9997845">
Evaluating the data-dependent term, p(D|C), for
different partitions of the source lexicon, we can
compare how well different models predict the tar-
get tokens aligned in a parallel corpus. This term
will prefer models that group together source types
with similar distributions over target types. By
using the marginal likelihood (integrating out the
parameters of the translation model) to calculate
</bodyText>
<page confidence="0.99422">
970
</page>
<bodyText confidence="0.9999775">
p(D|C), we can account explicitly for the com-
plexity of the translation model and compare mod-
els with different numbers of clusters as well as
different assignments of types to clusters.
In addition to an implicit uniform prior over
cluster labels as in k-means clustering (e.g. Chou
(1991)), we also consider a Markov random field
(MRF) parameterisation of the p(C) term to cap-
ture monolingual regularities in the lexicon. The
MRF induces dependencies between clustering
decisions in different parts of the lexicon via a
monolingual feature space biasing the search to-
wards models that exhibit monolingual regulari-
ties. Rather than assuming a priori knowledge of
redundant distinctions in the source language, we
use an EM algorithm to update parameters for fea-
tures defined over sets of source types on the basis
of existing cluster assignments. While initially the
model search will be guided only by information
from the bilingual statistics in p(D|C), monolin-
gual regularities in the lexicon, such as inflexion
patterns, may gradually be propagated through the
model as p(C) becomes informative. Our exper-
iments suggest that the MRF prior enables more
robust model selection.
As stated, the model selection procedure ac-
counts for redundancy in the source lexicon us-
ing the target distributions. The target lexicon
can be optimised analogously. Clustering target
types allows the implementation of independence
assumptions asserting that the exact specification
of a target type is independent of the source type
given knowledge of the target cluster label. For ex-
ample, when translating an English adjective into
French it may be more efficient to use the trans-
lation model to specify only that the translation
lies within a certain set of French adjectives, corre-
sponding to a single lemma, and have the language
model select the exact form. Our experiments sug-
gest that it can be useful to account for redundancy
in both languages in this way; this can be incorpo-
rated simply within our optimisation procedure.
In Section 3.1 we describe the bilingual
marginal likelihood, p(D|C), clustering proce-
dure; in Section 3.2 we introduce the MRF param-
eterisation of the prior, p(C), over model struc-
ture; and in Section 3.3, we describe algorithmic
approximations.
</bodyText>
<subsectionHeader confidence="0.996801">
3.1 Bilingual model selection
</subsectionHeader>
<bodyText confidence="0.999967090909091">
Assume we are optimising the source lexicon (the
target lexicon is optimised analogously). A clus-
tering of the lexicon is a unique mapping CF :
F —* CF defined for all f E F where, in addition
to all source types observed in the parallel training
corpus, F may include items seen in other mono-
lingual corpora (and, in the case of the source lex-
icon only, the development and test data). The
standard SMT lexicon can be viewed as a cluster-
ing with each type observed in the parallel training
corpus assigned to a distinct cluster and all other
types assigned to a single ‘unknown word’ cluster.
We optimise a conditional model of target to-
kens from word-aligned parallel corpora, D =
{Dc0, ..., Dcn,}, where Dci represents the set of
target words that were aligned to the set of source
types in cluster ci. We assume that each target to-
ken in the corpus is generated conditionally i.i.d.
given the cluster label of the source type to which
it is aligned. Sufficient statistics for this model
consist of co-occurrence counts of source and tar-
get types summed across each source cluster,
</bodyText>
<equation confidence="0.9674425">
�#cf (e) = . #(e, fl). (2)
f&apos;Ecf
</equation>
<bodyText confidence="0.999939888888889">
Maximising the likelihood of the data under this
model would require us to specify the number of
clusters (the size of the lexicon) in advance. In-
stead we place a Dirichlet prior parameterised by
α1 over the translation model parameters of each
cluster, µcf,e, defining the conditional distribu-
tions over target types. Given a clustering, the
Dirichlet prior, and independent parameters, the
distribution over data and parameters factorises,
</bodyText>
<equation confidence="0.97051175">
p(D, µ|CF, α) = 11 p(Dcf, µcf |cf, α)
cf ECF
α−1+#�f (e)
µcf,e
</equation>
<bodyText confidence="0.99991225">
We optimise cluster assignments with respect to
the marginal likelihood which averages the like-
lihood of the set of counts assigned to a cluster,
Dcf, under the current model over the prior,
</bodyText>
<equation confidence="0.791609">
fp(Dcf |α, cf) = p(µcf |α)p(Dcf |µcf, cf)dµcf.
</equation>
<bodyText confidence="0.998891">
This can be evaluated analytically for a Dirichlet
prior with multinomial parameters.
Assuming a (fixed) uniform prior over model
structure, p(C), model selection involves itera-
tively re-assigning source types to clusters such
as to maximise the marginal likelihood. Re-
assignments may alter the total number of clusters
</bodyText>
<footnote confidence="0.604736">
1Distinct from the prior over model structure, p(C).
</footnote>
<figure confidence="0.3651695">
11 a 11
cf ECF eE£
</figure>
<page confidence="0.978628">
971
</page>
<bodyText confidence="0.9999296">
at any point. Updates can be calculated locally, for
instance, given the sets of target tokens Dci and
Dcj aligned to source types currently in clusters
ci and cj, the change in log marginal likelihood if
clusters ci and cj are merged into cluster c� is,
</bodyText>
<equation confidence="0.995914666666667">
p(D�c|α, �c)
�ci,cj��c = log (3)
p(Dci |α, ci)p(Dcj |α, cj)
</equation>
<bodyText confidence="0.999941125">
which is a Bayes factor in favour of the hypothe-
sis that Dci and Dcj were sampled from the same
distribution (Wolpert, 1995). Unlike its equivalent
in maximum likelihood clustering, Eq.(3) may as-
sume positive values favouring a smaller number
of clusters when the data does not support a more
complex hypothesis. The more complex model,
with ci and cj modelled separately, is penalised
for being able to model a wider range of data sets.
The hyperparameter, α, is tied across clusters
and taken to be proportional to the marginal (the
‘background’) distribution over target types in the
corpus. Under this prior, source types aligned to
the same target types, will be clustered together
more readily if these target types are less frequent
in the corpus as a whole.
</bodyText>
<subsectionHeader confidence="0.998997">
3.2 Markov random field model prior
</subsectionHeader>
<bodyText confidence="0.966364">
As described above we consider a Markov random
field (MRF) parameterisation of the prior over
model structure, p(C). This defines a distribution
over cluster assignments of the source lexicon as a
whole based solely on monolingual characteristics
of the lexical types and the relations between their
respective cluster assignments.
Viewed as graph, each variable in the MRF is
modelled as conditionally independent of all other
variables given the values of its neighbours (the
Markov property; (Geman and Geman, 1984)).
Each variable in the MRF prior corresponds to a
lexical source type and its cluster assignment. Fig.
1 shows a section of the complete model including
the MRF prior for a Welsh source lexicon; shad-
ing denotes cluster assignments and English tar-
get tokens are shown as directed nodes.2 From the
Markov property it follows that this prior decom-
poses over neighbourhoods,
pMRF(C)a eβ Ef EJ— Ef,E,vf Kλiψi(f,f,,cf,c,f)
Here Nf is the set of neighbours of source type f;
i indexes a set of functions ψi(�) that pick out fea-
tures of a clique; each function has a parameter λi
</bodyText>
<footnote confidence="0.9378865">
2The plates represent repeated sampling; each Welsh
source type may be aligned to multiple English tokens.
</footnote>
<figureCaption confidence="0.999868">
Figure 1: Model with Markov random field prior
</figureCaption>
<bodyText confidence="0.999799533333333">
that we learn from the data; these are tied across
the graph. β is a free parameter used to control the
overall contribution of the prior in Eq. (1). Here
features are defined over pairs of types but higher-
order interactions can also be modelled. We only
consider ‘positive’ prior knowledge that is indica-
tive of redundancy among source types. Hence all
features are non-zero only when their arguments
are assigned to the same cluster.
Features can be defined over any aspects of the
lexicon; in our experiments we use binary features
over constrained string edits between types. The
following feature would be 1, for instance, if the
Welsh types cymru and gymru (see Fig. 1), were
assigned to the same cluster.3
</bodyText>
<equation confidence="0.961303">
ψ1(fi = (c —) ∧ fj = (g —) ∧ ci = cj)
</equation>
<bodyText confidence="0.999943954545455">
Setting the parameters of the MRF prior over
this feature space by hand would require a priori
knowledge of redundancies for the language pair.
In the absence of such knowledge, we use an it-
erative EM algorithm to update the parameters on
the basis of the previous solution to the bilingual
clustering procedure. EM parameter estimation
forces the cluster assignments of the MRF prior to
agree with those obtained on the basis of bilingual
data using monolingual features alone. Since fea-
tures are tied across the MRF, patterns that char-
acterise redundant relations between types will be
re-enforced across the model. For instance (see
Fig. 1), if cymru and gymru are clustered to-
gether, the parameter for feature ψ1, shown above,
may increase. This induces a prior preference for
car and gar to form a cluster on subsequent it-
erations. A similar feature defined for mar and
gar in the a priori string edit feature space, on
the other hand, may remain uninformative if not
observed frequently on pairs of types assigned to
the same clusters. In this way, the model learns to
</bodyText>
<footnote confidence="0.532418">
3Here — matches a common substring of both arguments.
</footnote>
<figure confidence="0.997672642857143">
car
car
#(f)
gar
car
#(f)
mar
bar
#(f) #(f)
cymru
#(f)
wales
gymru
wales
</figure>
<page confidence="0.993367">
972
</page>
<bodyText confidence="0.99955375">
generalise language-specific redundancy patterns
from a large a priori feature space. Changes in the
prior due to re-assignments can be calculated lo-
cally and combined with the marginal likelihood.
</bodyText>
<subsectionHeader confidence="0.999309">
3.3 Algorithmic approximations
</subsectionHeader>
<bodyText confidence="0.999984777777778">
The model selection procedure is an EM algo-
rithm. Each source type is initially assigned to
its own cluster and the MRF parameters, AZ, are
initialised to zero. A greedy E-step iteratively re-
assigns each source type to the cluster that max-
imises Eq. (1); cluster statistics are updated af-
ter any re-assignment. To reduce computation, we
only consider re-assignments that would cause at
least one (non-zero) feature in the MRF to fire, or
to clusters containing types sharing target word-
alignments with the current type; types may also
be re-assigned to a cluster of their own at any iter-
ation. When clustering both languages simultane-
ously, we average ‘target’ statistics over the num-
ber of events in each ‘target’ cluster in Eq. (2).
We re-estimate the MRF parameters after each
pass through the vocabulary. These are updated
according to MLE using a pseudolikelihood ap-
proximation (Besag, 1986). Since MRF parame-
ters can only be non-zero for features observed on
types clustered together during an E-step, we use
lazy instantiation to work with a large implicit fea-
ture set defined by a constrained string edit.
The algorithm has two free parameters: α deter-
mining the strength of the Dirichlet prior used in
the marginal likelihood, p(DIC), and Q which de-
termines the contribution of pMRF(C) to Eq. (1).
</bodyText>
<sectionHeader confidence="0.999649" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99978">
Phrase-based SMT systems have been shown to
outperform word-based approaches (Koehn et al.,
2003). We evaluate the effects of lexicon model
selection on translation quality by considering two
applications within a phrase-based SMT system.
</bodyText>
<subsectionHeader confidence="0.999212">
4.1 Applications to phrase-based SMT
</subsectionHeader>
<bodyText confidence="0.9996465">
A phrase-based translation model can be estimated
in two stages: first a parallel corpus is aligned at
the word-level and then phrase pairs are extracted
(Koehn et al., 2003). Aligning tokens in paral-
lel sentences using the IBM Models (Brown et
al., 1993), (Och and Ney, 2003) may require less
information than full-blown translation since the
task is constrained by the source and target tokens
present in each sentence pair. In the phrase-level
translation table, however, the model must assign
</bodyText>
<table confidence="0.9982375">
Source Tokens Types Singletons Test OOV
Czech 468K 54K 29K 6K 469
French 5682K 53K 19K 16K 112
Welsh 4578K 46K 18K 15K 64
</table>
<tableCaption confidence="0.999925">
Table 1: Parallel corpora used in the experiments.
</tableCaption>
<bodyText confidence="0.99978315">
probabilities to a potentially unconstrained set of
target phrases. We anticipate the optimal model
sizes to be different for these two tasks.
We can incorporate an optimised lexicon at the
word-alignment stage by mapping tokens in the
training corpus to their cluster labels. The map-
ping will not change the number of tokens in a
sentence, hence the word-alignments can be asso-
ciated with the original corpus (see Exp. 1).
To extrapolate a mapping over phrases from our
type-level models we can map each type within
a phrase to its corresponding cluster label. This,
however, results in a large number of distinct
phrases being collapsed down to a single ‘clus-
tered phrase’. Using these directly may spread
probability mass too widely. Instead we use
them to smooth the phrase translation model (see
Exp. 2). Here we consider a simple interpolation
scheme; they could also be used within a backoff
model (Yang and Kirchhoff, 2006).
</bodyText>
<subsectionHeader confidence="0.926076">
4.2 Experimental set-up
</subsectionHeader>
<bodyText confidence="0.999971941176471">
The system we use is described in (Koehn,
2004). The phrase-based translation model in-
cludes phrase-level and lexical weightings in both
directions. We use the decoder’s default behaviour
for unknown words copying them verbatim to the
output. Smoothed trigram language models are es-
timated on training sections of the parallel corpus.
We used the parallel sections of the Prague
Treebank (Cmejrek et al., 2004), French and En-
glish sections of the Europarl corpus (Koehn,
2005) and parallel text from the Welsh Assem-
bly4 (see Table1). The source languages, Czech,
French and Welsh, were chosen on the basis that
they may exhibit different degrees of redundancy
with respect to English and that they differ mor-
phologically. Only the Czech corpus has explicit
morphological annotation.
</bodyText>
<subsectionHeader confidence="0.998007">
4.3 Models
</subsectionHeader>
<bodyText confidence="0.999814333333333">
All models used in the experiments are defined as
mappings of the source and target vocabularies.
The target vocabulary includes all distinct types
</bodyText>
<footnote confidence="0.994701">
4This Welsh-English parallel text is in the public domain.
Contact the first author for details.
</footnote>
<page confidence="0.998384">
973
</page>
<bodyText confidence="0.999957666666667">
seen in the training corpus; the source vocabu-
lary also includes types seen only in development
and test data. Free parameters were set to max-
imize our evaluation metric, BLEU, on develop-
ment data. The results are reported on the test sets
(see Table 1). The baseline mappings used were:
</bodyText>
<listItem confidence="0.9999076">
• standard: the identity mapping;
• max-pref: a prefix of no more than n letters;
• min-freq: a prefix with a frequency of at least
n in the parallel training corpus.
• lemmatize: morphological lemmas (Czech)
</listItem>
<bodyText confidence="0.998288230769231">
standard corresponds to the standard SMT lexi-
con. max-pref and min-freq are both simple stem-
ming algorithms that can be applied to raw text.
These mappings result in models defined over
fewer distinct events that will have higher frequen-
cies; min-freq optimises the latter directly. We
optimise over (possibly different) values of n for
source and target languages. The lemmatize map-
ping which maps types to their lemmas was only
applicable to the Czech corpus.
The optimised lexicon models define mappings
directly via their clusterings of the vocabulary. We
consider the following four models:
</bodyText>
<listItem confidence="0.9995475">
• src: clustered source lexicon;
• src+mrf: as src with MRF prior;
• src+trg: clustered source and target lexicons;
• src+trg+mrf: as src+trg with MRF priors.
</listItem>
<bodyText confidence="0.826467">
In each case we optimise over α (a single value for
both languages) and, when using the MRF prior,
over Q (a single value for both languages).
</bodyText>
<subsectionHeader confidence="0.987946">
4.4 Experiments
</subsectionHeader>
<bodyText confidence="0.999795">
The two sets of experiments evaluate the base-
line models and optimised lexicon models dur-
ing word-alignment and phrase-level translation
model estimation respectively.
</bodyText>
<listItem confidence="0.9884804">
• Exp. 1: map the parallel corpus, perform
word-alignment; estimate the phrase transla-
tion model using the original corpus.
• Exp. 2: smooth the phrase translation model,
#(e, f) + &apos;Y#(ce, cf)
</listItem>
<bodyText confidence="0.999922">
experiments on development data. Word-
alignments were generated using the optimal
max-pref mapping for each training set.
</bodyText>
<sectionHeader confidence="0.999658" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999983148148148">
Table 2 shows the changes in BLEU when we in-
corporate the lexicon mappings during the word-
alignment process. The standard SMT lexicon
model is not optimal, as measured by BLEU, for
any of the languages or training set sizes consid-
ered. Increases over this baseline, however, di-
minish with more training data. For both Czech
and Welsh, the explicit model selection procedure
that we have proposed results in better translations
than all of the baseline models when the MRF
prior is used; again these increases diminish with
larger training sets. We note that the stemming
baseline models appear to be more effective for
Czech than for Welsh. The impact of the MRF
prior is also greater for smaller training sets.
Table 3 shows the results of using these models
to smooth the phrase translation table.5 With the
exception of Czech, the improvements are smaller
than for Exp 1. For all source languages and mod-
els we found that it was optimal to leave the tar-
get lexicon unmapped when smoothing the phrase
translation model.
Using lemmatize for word-alignment on the
Czech corpus gave BLEU scores of 32.71 and
37.21 for the 10K and 21K training sets respec-
tively; used to smooth the phrase translation model
it gave scores of 33.96 and 37.18.
</bodyText>
<subsectionHeader confidence="0.960199">
5.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999728533333333">
Model selection had the largest impact for smaller
data sets suggesting that the complexity of the
standard model is most excessive in sparse data
conditions. The larger improvements seen for
Czech and Welsh suggest that these languages en-
code more redundant information in the lexicon
with respect to English. Potential sources could be
grammatical case markings (Czech) and mutation
patterns (Welsh). The impact of the MRF prior for
smaller data sets suggests it overcomes sparsity in
the bilingual statistics during model selection.
The location of redundancies, in the form of
case markings, at the ends of words in Czech as
assumed by the stemming algorithms may explain
why these performed better on this language than
</bodyText>
<footnote confidence="0.981715">
5The standard model in Exp. 2 is equivalent to the opti-
mised max-pref in Exp. 1.
</footnote>
<equation confidence="0.969677">
n(e|f) = #(f) + &apos;Y#(cf)
</equation>
<bodyText confidence="0.993801333333333">
Here e, f and ce, cf are phrases mapped un-
der the standard model and the model be-
ing tested respectively; &apos;Y is set once for all
</bodyText>
<page confidence="0.998709">
974
</page>
<tableCaption confidence="0.99394">
Table 2: BLEU scores with optimised lexicon applied during word-alignment (Exp. 1)
</tableCaption>
<table confidence="0.999858">
Model Czech-English French-English Welsh-English
10K sent. 21K 10K 25K 100K 250K 10K 25K 100K 250K
standard 32.31 36.17 20.76 23.17 26.61 27.63 35.45 39.92 45.02 46.47
max-pref 34.18 37.34 21.63 23.94 26.45 28.25 35.88 41.03 44.82 46.11
min-freq 33.95 36.98 21.22 23.77 26.74 27.98 36.23 40.65 45.38 46.35
src 33.95 37.27 21.43 24.42 26.99 27.82 36.98 40.98 45.81 46.45
src+mrf 33.97 37.89 21.63 24.38 26.74 28.39 37.36 41.13 46.50 46.56
src+trg 34.24 38.28 22.05 24.02 26.53 27.80 36.83 41.31 45.22 46.51
src+trg+mrf 34.70 38.44 22.33 23.95 26.69 27.75 37.56 42.19 45.18 46.48
</table>
<tableCaption confidence="0.980234">
Table 3: BLEU scores with optimised lexicon used to smooth phrase-based translation model (Exp. 2)
</tableCaption>
<table confidence="0.999850285714286">
Model Czech-English French-English Welsh-English
10K sent. 21K 10K 25K 100K 250K 10K 25K 100K 250K
(standard)&apos; 34.18 37.34 21.63 23.94 26.45 28.25 35.88 41.03 44.82 46.11
max-pref 35.63 38.81 22.49 24.10 26.99 28.26 37.31 40.09 45.57 46.41
min-freq 34.65 37.75 21.14 23.41 26.29 27.47 36.40 40.84 45.75 46.45
src 34.38 37.98 21.28 24.17 26.88 28.35 36.94 39.99 45.75 46.65
src+mrf 36.24 39.70 22.02 24.10 26.82 28.09 37.81 41.04 46.16 46.51
</table>
<tableCaption confidence="0.999718">
Table 4: System output (Welsh 25K; Exp. 2)
</tableCaption>
<bodyText confidence="0.999506371428571">
Src ehangu o ffilm i deledu.
Ref an expansion from film into television.
standard expansion of footage to deledu.
max-pref expansion of ffilm to television.
src+mrf expansion of film to television.
Src yw gwarchod cymru fel gwlad brydferth
Ref safeguarding wales as a picturesque country
standard protection of wales as a country brydferth
max-pref protection of wales as a country brydferth
src+mrf protecting wales as a beautiful country
Src cynhyrchu canlyniadau llai na pherffaith
Ref produces results that are less than perfect
standard produce results less than pherffaith
max-pref produce results less than pherffaith
src+mrf generates less than perfect results
Src y dynodiad o graidd y broblem
Ref the identification of the nub of the problem
standard the dynodiad of the heart of the problem
max-pref the dynodiad of the heart of the problem
src+mrf the identified crux of the problem
on Welsh. The highest scoring features in the
MRF (see Table 5) show that Welsh redundancies,
on the other hand, are primarily between initial
characters. Inspection of system output confirms
that OOV types could be mapped to known Welsh
words with the MRF prior but not via stemming
(see Table 4). For each language pair the MRF
learned features that capture intuitively redundant
patterns: adjectival endings for French, case mark-
ings for Czech, and mutation patterns for Welsh.
The greater improvements in Exp. 1 were mir-
rored by higher compression rates for these lex-
icons (see Table. 6) supporting the conjecture
that word-alignment requires less information than
full-blown translation. The results of the lemma-
</bodyText>
<tableCaption confidence="0.993699">
Table 5: Features learned by MRF prior
</tableCaption>
<table confidence="0.996695555555556">
Czech French Welsh
(∼, ∼ rni) (∼,∼ s) (c ∼,g ∼)
(∼, ∼u) (∼, ∼ e) (d ∼, dd ∼)
(∼, ∼a) (∼, ∼ es) (d ∼, t ∼)
(∼, ∼ch) (∼ e, ∼ es) (b ∼,p ∼)
(∼, ∼ho) (∼ e, ∼ er) (c ∼, ch ∼)
(∼ a, ∼ u) (∼ e, ∼ ent) (b ∼, f ∼)
Note: Features defined over pairs of source types assigned to
the same cluster; here - matches a common substring.
</table>
<tableCaption confidence="0.916812">
Table 6: Optimal lexicon size (ratio of raw vocab.)
</tableCaption>
<table confidence="0.944665666666667">
Czech French Welsh
Word-alignment 0.26 0.22 0.24
TM smoothing 0.28 0.38 0.51
</table>
<bodyText confidence="0.9848875">
tize model on Czech show the model selection pro-
cedure improving on a simple supervised baseline.
</bodyText>
<sectionHeader confidence="0.999987" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999945076923077">
Previous work on automatic bilingual word clus-
tering has been motivated somewhat differently
and not made use of cluster-based models to as-
sign translation probabilities directly (Wang et
al., 1996), (Och, 1998). There is, however, a
large body of work using morphological analy-
sis to define cluster-based translation models sim-
ilar to ours but in a supervised manner (Zens and
Ney, 2004), (Niessen and Ney, 2004). These
approaches have used morphological annotation
(e.g. lemmas and part of speech tags) to pro-
vide explicit supervision. They have also involved
manually specifying which morphological distinc-
</bodyText>
<page confidence="0.996155">
975
</page>
<bodyText confidence="0.999675095238095">
tions are redundant (Goldwater and McClosky,
2005). In contrast, we attempt to learn both equiv-
alence classes and redundant relations automat-
ically. Our experiments with orthographic fea-
tures suggest that some morphological redundan-
cies can be acquired in an unsupervised fashion.
The marginal likelihood hard-clustering algo-
rithm that we propose here for translation model
selection can be viewed as a Bayesian k-means al-
gorithm and is an application of Bayesian model
selection techniques, e.g., (Wolpert, 1995). The
Markov random field prior over model structure
extends the fixed uniform prior over clusters im-
plicit in k-means clustering and is common in
computer vision (Geman and Geman, 1984). Re-
cently Basu et al. (2004) used an MRF to embody
hard constraints within semi-supervised cluster-
ing. In contrast, we use an iterative EM algo-
rithm to learn soft constraints within the ‘prior’
monolingual space based on the results of cluster-
ing with bilingual statistics.
</bodyText>
<sectionHeader confidence="0.997872" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999996347826087">
We proposed a framework for modelling lexical
redundancy in machine translation and tackled op-
timisation of the lexicon via Bayesian model se-
lection over a set of cluster-based translation mod-
els. We showed improvements in translation qual-
ity incorporating these models within a phrase-
based SMT sytem. Additional gains resulted from
the inclusion of an MRF prior over model struc-
ture. We demonstrated that this prior could be
used to learn weights for monolingual features that
characterise bilingual redundancy. Preliminary
experiments defining MRF features over morpho-
logical annotation suggest this model can also
identify redundant distinctions categorised lin-
guistically (for instance, that morphological case
is redundant on Czech nouns and adjectives with
respect to English, while number is redundant only
on adjectives). In future work we will investigate
the use of linguistic resources to define feature sets
for the MRF prior. Lexical redundancy would ide-
ally be addressed in the context of phrases, how-
ever, computation and statistical estimation may
then be significantly more challenging.
</bodyText>
<sectionHeader confidence="0.997517" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997810666666667">
The authors would like to thank Philipp Koehn for providing
training scripts used in this work; and Steve Renals, Mirella
Lapata and members of the Edinburgh SMT Group for valu-
able comments. This work was supported by an MRC Prior-
ity Area Studentship to the School of Informatics, University
of Edinburgh.
</bodyText>
<sectionHeader confidence="0.99605" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999270689655172">
Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney.
2004. A probabilistic framework for semi-supervised
clustering. In Proc. of the 10th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data
Mining (KDD-2004).
Julian Besag. 1986. The statistical analysis of dirty pictures.
Journal of the Royal Society Series B, 48(2):259–302.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and
Robert Mercer. 1993. The mathematics of machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263–311.
Philip A. Chou. 1991. Optimal partitioning for classification
and regression trees. IEEE Trans. on Pattern Analysis and
Machine Intelligence, 13(4).
M. Cmejrek, J. Curin, J. Havelka, J. Hajic, and V. Kubon.
2004. Prague Czech-English dependency treebank: Syn-
tactically annotated resources for machine translation. In
4th International Conference on Language Resources and
Evaluation, Lisbon, Portugal
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of im-
ages. IEEE Trans. on Pattern Analysis and Machine Intel-
ligence, 6:721–741.
Sharon Goldwater and David McClosky. 2005. Improving
statistical MT through morphological analysis. In Proc.
of the 2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002).
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of the
HLT/NAACL 2003.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
Proceedings of the AMTA 2004.
Philipp Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit 2005.
S. Niessen and H. Ney. 2004. Statistical machine transla-
tion with scarce resources using morpho-syntactic infor-
mation. Computational Linguistics, 30(2):181–204.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51.
F.-J. Och. 1998. An efficient method for determining bilin-
gual word classes. In Proc. of the European Chapter of
the Association for Computational Linguistics 1998.
Ye-Yi Wang, John Lafferty, and Alex Waibel. 1996. Word
clustering with parallel spoken language corpora. In Proc.
of 4th International Conference on Spoken Language Pro-
cessing, ICSLP 96, Philadelphia, PA.
D.H. Wolpert. 1995. Determining whether two data sets are
from the same distribution. In 15th international work-
shop on Maximum Entropy and Bayesian Methods.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based back-
off models for machine translation of highly inflected lan-
guages. In Proc. of the the European Chapter of the Asso-
ciation for Computational Linguistics 2006.
R. Zens and H. Ney. 2004. Improvements in phrase-based
statistical machine translation. In Proc. of the Human
Language Technology Conference (HLT-NAACL 2004).
</reference>
<page confidence="0.998776">
976
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.966676">
<title confidence="0.99997">Modelling lexical redundancy for machine translation</title>
<author confidence="0.999991">David Talbot</author>
<author confidence="0.999991">Miles Osborne</author>
<affiliation confidence="0.999989">School of Informatics, University of Edinburgh</affiliation>
<address confidence="0.989862">2 Buccleuch Place, Edinburgh, EH8 9LW, UK</address>
<email confidence="0.998016">d.r.talbot@sms.ed.ac.uk,miles@inf.ed.ac.uk</email>
<abstract confidence="0.999171038461538">Certain distinctions made in the lexicon one language may be translating into another language. We quantify redundancy among source types by the similarity of their distributions over target types. We propose a languageindependent framework for minimising redundancy can be optimised directly from parallel text. Optimisation of the source lexicon for a given target language is viewed as model selection over a set of cluster-based translation models. Redundant distinctions between types may exhibit monolingual regularities, for example, inflexion patterns. We define a prior over model structure using a Markov random field and learn features over sets of monolingual types that are predictive of bilingual redundancy. The prior makes model selection more robust without the need for language-specific assumptions regarding redundancy. Using these models in a phrase-based SMT system, we show significant improvements in translation quality for certain language pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sugato Basu</author>
<author>Mikhail Bilenko</author>
<author>Raymond J Mooney</author>
</authors>
<title>A probabilistic framework for semi-supervised clustering.</title>
<date>2004</date>
<booktitle>In Proc. of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004).</booktitle>
<contexts>
<context position="31000" citStr="Basu et al. (2004)" startWordPosition="5007" endWordPosition="5010">and redundant relations automatically. Our experiments with orthographic features suggest that some morphological redundancies can be acquired in an unsupervised fashion. The marginal likelihood hard-clustering algorithm that we propose here for translation model selection can be viewed as a Bayesian k-means algorithm and is an application of Bayesian model selection techniques, e.g., (Wolpert, 1995). The Markov random field prior over model structure extends the fixed uniform prior over clusters implicit in k-means clustering and is common in computer vision (Geman and Geman, 1984). Recently Basu et al. (2004) used an MRF to embody hard constraints within semi-supervised clustering. In contrast, we use an iterative EM algorithm to learn soft constraints within the ‘prior’ monolingual space based on the results of clustering with bilingual statistics. 7 Conclusions and Future Work We proposed a framework for modelling lexical redundancy in machine translation and tackled optimisation of the lexicon via Bayesian model selection over a set of cluster-based translation models. We showed improvements in translation quality incorporating these models within a phrasebased SMT sytem. Additional gains resul</context>
</contexts>
<marker>Basu, Bilenko, Mooney, 2004</marker>
<rawString>Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney. 2004. A probabilistic framework for semi-supervised clustering. In Proc. of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Besag</author>
</authors>
<title>The statistical analysis of dirty pictures.</title>
<date>1986</date>
<journal>Journal of the Royal Society Series B,</journal>
<volume>48</volume>
<issue>2</issue>
<contexts>
<context position="18710" citStr="Besag, 1986" startWordPosition="2993" endWordPosition="2994">d after any re-assignment. To reduce computation, we only consider re-assignments that would cause at least one (non-zero) feature in the MRF to fire, or to clusters containing types sharing target wordalignments with the current type; types may also be re-assigned to a cluster of their own at any iteration. When clustering both languages simultaneously, we average ‘target’ statistics over the number of events in each ‘target’ cluster in Eq. (2). We re-estimate the MRF parameters after each pass through the vocabulary. These are updated according to MLE using a pseudolikelihood approximation (Besag, 1986). Since MRF parameters can only be non-zero for features observed on types clustered together during an E-step, we use lazy instantiation to work with a large implicit feature set defined by a constrained string edit. The algorithm has two free parameters: α determining the strength of the Dirichlet prior used in the marginal likelihood, p(DIC), and Q which determines the contribution of pMRF(C) to Eq. (1). 4 Experiments Phrase-based SMT systems have been shown to outperform word-based approaches (Koehn et al., 2003). We evaluate the effects of lexicon model selection on translation quality by</context>
</contexts>
<marker>Besag, 1986</marker>
<rawString>Julian Besag. 1986. The statistical analysis of dirty pictures. Journal of the Royal Society Series B, 48(2):259–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5138" citStr="Brown et al. (1993)" startWordPosition="759" endWordPosition="762">roves the results and picks up features that appear to agree with linguistic intuitions of redundancy for the language pairs considered. 2 Lexical redundancy between languages In statistical MT, the source and target lexicons are usually defined as the sets of distinct types observed in the parallel training corpus for each language. Such models may not be optimal for certain language pairs and training regimes. A word-level statistical translation model approximates the probability Pr(E|F) that a source type indexed by F will be translated as a target type indexed by E. Standard models, e.g. Brown et al. (1993), consist of discrete probability distributions with separate parameters for each unique pairing of a source and target types; no attempt is made to leverage structure within the event spaces £ and F during parameter estimation. This results in a large number of parameters that must be estimated from limited amounts of parallel corpora. We refer to distinctions made between lexical types in one language that do not result in different distributions over types in the other language as lexically redundant for the language pair. Since the role of the translation model is to determine a distributi</context>
<context position="19666" citStr="Brown et al., 1993" startWordPosition="3145" endWordPosition="3148">d, p(DIC), and Q which determines the contribution of pMRF(C) to Eq. (1). 4 Experiments Phrase-based SMT systems have been shown to outperform word-based approaches (Koehn et al., 2003). We evaluate the effects of lexicon model selection on translation quality by considering two applications within a phrase-based SMT system. 4.1 Applications to phrase-based SMT A phrase-based translation model can be estimated in two stages: first a parallel corpus is aligned at the word-level and then phrase pairs are extracted (Koehn et al., 2003). Aligning tokens in parallel sentences using the IBM Models (Brown et al., 1993), (Och and Ney, 2003) may require less information than full-blown translation since the task is constrained by the source and target tokens present in each sentence pair. In the phrase-level translation table, however, the model must assign Source Tokens Types Singletons Test OOV Czech 468K 54K 29K 6K 469 French 5682K 53K 19K 16K 112 Welsh 4578K 46K 18K 15K 64 Table 1: Parallel corpora used in the experiments. probabilities to a potentially unconstrained set of target phrases. We anticipate the optimal model sizes to be different for these two tasks. We can incorporate an optimised lexicon at</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip A Chou</author>
</authors>
<title>Optimal partitioning for classification and regression trees.</title>
<date>1991</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Intelligence,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="9004" citStr="Chou (1991)" startWordPosition="1374" endWordPosition="1375">exicon, we can compare how well different models predict the target tokens aligned in a parallel corpus. This term will prefer models that group together source types with similar distributions over target types. By using the marginal likelihood (integrating out the parameters of the translation model) to calculate 970 p(D|C), we can account explicitly for the complexity of the translation model and compare models with different numbers of clusters as well as different assignments of types to clusters. In addition to an implicit uniform prior over cluster labels as in k-means clustering (e.g. Chou (1991)), we also consider a Markov random field (MRF) parameterisation of the p(C) term to capture monolingual regularities in the lexicon. The MRF induces dependencies between clustering decisions in different parts of the lexicon via a monolingual feature space biasing the search towards models that exhibit monolingual regularities. Rather than assuming a priori knowledge of redundant distinctions in the source language, we use an EM algorithm to update parameters for features defined over sets of source types on the basis of existing cluster assignments. While initially the model search will be g</context>
</contexts>
<marker>Chou, 1991</marker>
<rawString>Philip A. Chou. 1991. Optimal partitioning for classification and regression trees. IEEE Trans. on Pattern Analysis and Machine Intelligence, 13(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cmejrek</author>
<author>J Curin</author>
<author>J Havelka</author>
<author>J Hajic</author>
<author>V Kubon</author>
</authors>
<title>Prague Czech-English dependency treebank: Syntactically annotated resources for machine translation.</title>
<date>2004</date>
<booktitle>In 4th International Conference on Language Resources and Evaluation,</booktitle>
<location>Lisbon, Portugal</location>
<contexts>
<context position="21448" citStr="Cmejrek et al., 2004" startWordPosition="3435" endWordPosition="3438">e use them to smooth the phrase translation model (see Exp. 2). Here we consider a simple interpolation scheme; they could also be used within a backoff model (Yang and Kirchhoff, 2006). 4.2 Experimental set-up The system we use is described in (Koehn, 2004). The phrase-based translation model includes phrase-level and lexical weightings in both directions. We use the decoder’s default behaviour for unknown words copying them verbatim to the output. Smoothed trigram language models are estimated on training sections of the parallel corpus. We used the parallel sections of the Prague Treebank (Cmejrek et al., 2004), French and English sections of the Europarl corpus (Koehn, 2005) and parallel text from the Welsh Assembly4 (see Table1). The source languages, Czech, French and Welsh, were chosen on the basis that they may exhibit different degrees of redundancy with respect to English and that they differ morphologically. Only the Czech corpus has explicit morphological annotation. 4.3 Models All models used in the experiments are defined as mappings of the source and target vocabularies. The target vocabulary includes all distinct types 4This Welsh-English parallel text is in the public domain. Contact t</context>
</contexts>
<marker>Cmejrek, Curin, Havelka, Hajic, Kubon, 2004</marker>
<rawString>M. Cmejrek, J. Curin, J. Havelka, J. Hajic, and V. Kubon. 2004. Prague Czech-English dependency treebank: Syntactically annotated resources for machine translation. In 4th International Conference on Language Resources and Evaluation, Lisbon, Portugal</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Intelligence,</journal>
<pages>6--721</pages>
<contexts>
<context position="14901" citStr="Geman and Geman, 1984" startWordPosition="2344" endWordPosition="2347">y if these target types are less frequent in the corpus as a whole. 3.2 Markov random field model prior As described above we consider a Markov random field (MRF) parameterisation of the prior over model structure, p(C). This defines a distribution over cluster assignments of the source lexicon as a whole based solely on monolingual characteristics of the lexical types and the relations between their respective cluster assignments. Viewed as graph, each variable in the MRF is modelled as conditionally independent of all other variables given the values of its neighbours (the Markov property; (Geman and Geman, 1984)). Each variable in the MRF prior corresponds to a lexical source type and its cluster assignment. Fig. 1 shows a section of the complete model including the MRF prior for a Welsh source lexicon; shading denotes cluster assignments and English target tokens are shown as directed nodes.2 From the Markov property it follows that this prior decomposes over neighbourhoods, pMRF(C)a eβ Ef EJ— Ef,E,vf Kλiψi(f,f,,cf,c,f) Here Nf is the set of neighbours of source type f; i indexes a set of functions ψi(�) that pick out features of a clique; each function has a parameter λi 2The plates represent repea</context>
<context position="30971" citStr="Geman and Geman, 1984" startWordPosition="5001" endWordPosition="5004">o learn both equivalence classes and redundant relations automatically. Our experiments with orthographic features suggest that some morphological redundancies can be acquired in an unsupervised fashion. The marginal likelihood hard-clustering algorithm that we propose here for translation model selection can be viewed as a Bayesian k-means algorithm and is an application of Bayesian model selection techniques, e.g., (Wolpert, 1995). The Markov random field prior over model structure extends the fixed uniform prior over clusters implicit in k-means clustering and is common in computer vision (Geman and Geman, 1984). Recently Basu et al. (2004) used an MRF to embody hard constraints within semi-supervised clustering. In contrast, we use an iterative EM algorithm to learn soft constraints within the ‘prior’ monolingual space based on the results of clustering with bilingual statistics. 7 Conclusions and Future Work We proposed a framework for modelling lexical redundancy in machine translation and tackled optimisation of the lexicon via Bayesian model selection over a set of cluster-based translation models. We showed improvements in translation quality incorporating these models within a phrasebased SMT </context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>S. Geman and D. Geman. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Trans. on Pattern Analysis and Machine Intelligence, 6:721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>David McClosky</author>
</authors>
<title>Improving statistical MT through morphological analysis.</title>
<date>2005</date>
<booktitle>In Proc. of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="30322" citStr="Goldwater and McClosky, 2005" startWordPosition="4902" endWordPosition="4905">al word clustering has been motivated somewhat differently and not made use of cluster-based models to assign translation probabilities directly (Wang et al., 1996), (Och, 1998). There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). These approaches have used morphological annotation (e.g. lemmas and part of speech tags) to provide explicit supervision. They have also involved manually specifying which morphological distinc975 tions are redundant (Goldwater and McClosky, 2005). In contrast, we attempt to learn both equivalence classes and redundant relations automatically. Our experiments with orthographic features suggest that some morphological redundancies can be acquired in an unsupervised fashion. The marginal likelihood hard-clustering algorithm that we propose here for translation model selection can be viewed as a Bayesian k-means algorithm and is an application of Bayesian model selection techniques, e.g., (Wolpert, 1995). The Markov random field prior over model structure extends the fixed uniform prior over clusters implicit in k-means clustering and is </context>
</contexts>
<marker>Goldwater, McClosky, 2005</marker>
<rawString>Sharon Goldwater and David McClosky. 2005. Improving statistical MT through morphological analysis. In Proc. of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT/NAACL</booktitle>
<contexts>
<context position="19232" citStr="Koehn et al., 2003" startWordPosition="3077" endWordPosition="3080">cabulary. These are updated according to MLE using a pseudolikelihood approximation (Besag, 1986). Since MRF parameters can only be non-zero for features observed on types clustered together during an E-step, we use lazy instantiation to work with a large implicit feature set defined by a constrained string edit. The algorithm has two free parameters: α determining the strength of the Dirichlet prior used in the marginal likelihood, p(DIC), and Q which determines the contribution of pMRF(C) to Eq. (1). 4 Experiments Phrase-based SMT systems have been shown to outperform word-based approaches (Koehn et al., 2003). We evaluate the effects of lexicon model selection on translation quality by considering two applications within a phrase-based SMT system. 4.1 Applications to phrase-based SMT A phrase-based translation model can be estimated in two stages: first a parallel corpus is aligned at the word-level and then phrase pairs are extracted (Koehn et al., 2003). Aligning tokens in parallel sentences using the IBM Models (Brown et al., 1993), (Och and Ney, 2003) may require less information than full-blown translation since the task is constrained by the source and target tokens present in each sentence </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the HLT/NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of the AMTA</booktitle>
<contexts>
<context position="21085" citStr="Koehn, 2004" startWordPosition="3382" endWordPosition="3383">e original corpus (see Exp. 1). To extrapolate a mapping over phrases from our type-level models we can map each type within a phrase to its corresponding cluster label. This, however, results in a large number of distinct phrases being collapsed down to a single ‘clustered phrase’. Using these directly may spread probability mass too widely. Instead we use them to smooth the phrase translation model (see Exp. 2). Here we consider a simple interpolation scheme; they could also be used within a backoff model (Yang and Kirchhoff, 2006). 4.2 Experimental set-up The system we use is described in (Koehn, 2004). The phrase-based translation model includes phrase-level and lexical weightings in both directions. We use the decoder’s default behaviour for unknown words copying them verbatim to the output. Smoothed trigram language models are estimated on training sections of the parallel corpus. We used the parallel sections of the Prague Treebank (Cmejrek et al., 2004), French and English sections of the Europarl corpus (Koehn, 2005) and parallel text from the Welsh Assembly4 (see Table1). The source languages, Czech, French and Welsh, were chosen on the basis that they may exhibit different degrees o</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings of the AMTA 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit</booktitle>
<contexts>
<context position="21514" citStr="Koehn, 2005" startWordPosition="3448" endWordPosition="3449">sider a simple interpolation scheme; they could also be used within a backoff model (Yang and Kirchhoff, 2006). 4.2 Experimental set-up The system we use is described in (Koehn, 2004). The phrase-based translation model includes phrase-level and lexical weightings in both directions. We use the decoder’s default behaviour for unknown words copying them verbatim to the output. Smoothed trigram language models are estimated on training sections of the parallel corpus. We used the parallel sections of the Prague Treebank (Cmejrek et al., 2004), French and English sections of the Europarl corpus (Koehn, 2005) and parallel text from the Welsh Assembly4 (see Table1). The source languages, Czech, French and Welsh, were chosen on the basis that they may exhibit different degrees of redundancy with respect to English and that they differ morphologically. Only the Czech corpus has explicit morphological annotation. 4.3 Models All models used in the experiments are defined as mappings of the source and target vocabularies. The target vocabulary includes all distinct types 4This Welsh-English parallel text is in the public domain. Contact the first author for details. 973 seen in the training corpus; the </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Niessen</author>
<author>H Ney</author>
</authors>
<title>Statistical machine translation with scarce resources using morpho-syntactic information.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="30072" citStr="Niessen and Ney, 2004" startWordPosition="4867" endWordPosition="4870">tio of raw vocab.) Czech French Welsh Word-alignment 0.26 0.22 0.24 TM smoothing 0.28 0.38 0.51 tize model on Czech show the model selection procedure improving on a simple supervised baseline. 6 Related Work Previous work on automatic bilingual word clustering has been motivated somewhat differently and not made use of cluster-based models to assign translation probabilities directly (Wang et al., 1996), (Och, 1998). There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). These approaches have used morphological annotation (e.g. lemmas and part of speech tags) to provide explicit supervision. They have also involved manually specifying which morphological distinc975 tions are redundant (Goldwater and McClosky, 2005). In contrast, we attempt to learn both equivalence classes and redundant relations automatically. Our experiments with orthographic features suggest that some morphological redundancies can be acquired in an unsupervised fashion. The marginal likelihood hard-clustering algorithm that we propose here for translation model selection can be viewed as</context>
</contexts>
<marker>Niessen, Ney, 2004</marker>
<rawString>S. Niessen and H. Ney. 2004. Statistical machine translation with scarce resources using morpho-syntactic information. Computational Linguistics, 30(2):181–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="19687" citStr="Och and Ney, 2003" startWordPosition="3149" endWordPosition="3152"> determines the contribution of pMRF(C) to Eq. (1). 4 Experiments Phrase-based SMT systems have been shown to outperform word-based approaches (Koehn et al., 2003). We evaluate the effects of lexicon model selection on translation quality by considering two applications within a phrase-based SMT system. 4.1 Applications to phrase-based SMT A phrase-based translation model can be estimated in two stages: first a parallel corpus is aligned at the word-level and then phrase pairs are extracted (Koehn et al., 2003). Aligning tokens in parallel sentences using the IBM Models (Brown et al., 1993), (Och and Ney, 2003) may require less information than full-blown translation since the task is constrained by the source and target tokens present in each sentence pair. In the phrase-level translation table, however, the model must assign Source Tokens Types Singletons Test OOV Czech 468K 54K 29K 6K 469 French 5682K 53K 19K 16K 112 Welsh 4578K 46K 18K 15K 64 Table 1: Parallel corpora used in the experiments. probabilities to a potentially unconstrained set of target phrases. We anticipate the optimal model sizes to be different for these two tasks. We can incorporate an optimised lexicon at the word-alignment s</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F-J Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1998</date>
<booktitle>In Proc. of the European Chapter of the Association for Computational Linguistics</booktitle>
<contexts>
<context position="2585" citStr="Och, 1998" startWordPosition="368" endWordPosition="369">parameter estimation from limited parallel data more difficult. A natural way to eliminate such lexical redundancy is to group types into homogeneous clusters that do not differ significantly in their distributions over types in the other language. Cluster-based translation models capture the corresponding independence assumptions. Previous work on bilingual clustering has focused on coarse partitions of the lexicon that resemble automatically induced part-of-speech classes. These were used to model generic word-alignment patterns such as noun-adjective re-ordering between English and French (Och, 1998). In contrast, we induce fine-grained partitions of the lexicon, conceptually closer to automatic lemmatisation, optimised specifically to assign translation probabilities. Unlike lemmatisation or stemming, our method specifically quantifies lexical redundancy in a bilingual setting and does not make language-specific assumptions. We tackle the problem of redundancy in the translation lexicon via Bayesian model selection over a set of cluster-based translation models. We search for the model, defined by a clustering of the source lexicon, that maximises the marginal likelihood of target tokens</context>
<context position="29870" citStr="Och, 1998" startWordPosition="4835" endWordPosition="4836"> ch ∼) (∼ a, ∼ u) (∼ e, ∼ ent) (b ∼, f ∼) Note: Features defined over pairs of source types assigned to the same cluster; here - matches a common substring. Table 6: Optimal lexicon size (ratio of raw vocab.) Czech French Welsh Word-alignment 0.26 0.22 0.24 TM smoothing 0.28 0.38 0.51 tize model on Czech show the model selection procedure improving on a simple supervised baseline. 6 Related Work Previous work on automatic bilingual word clustering has been motivated somewhat differently and not made use of cluster-based models to assign translation probabilities directly (Wang et al., 1996), (Och, 1998). There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). These approaches have used morphological annotation (e.g. lemmas and part of speech tags) to provide explicit supervision. They have also involved manually specifying which morphological distinc975 tions are redundant (Goldwater and McClosky, 2005). In contrast, we attempt to learn both equivalence classes and redundant relations automatically. Our experiments with orthographic features sugges</context>
</contexts>
<marker>Och, 1998</marker>
<rawString>F.-J. Och. 1998. An efficient method for determining bilingual word classes. In Proc. of the European Chapter of the Association for Computational Linguistics 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>John Lafferty</author>
<author>Alex Waibel</author>
</authors>
<title>Word clustering with parallel spoken language corpora.</title>
<date>1996</date>
<booktitle>In Proc. of 4th International Conference on Spoken Language Processing, ICSLP 96,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="29857" citStr="Wang et al., 1996" startWordPosition="4831" endWordPosition="4834">ho) (∼ e, ∼ er) (c ∼, ch ∼) (∼ a, ∼ u) (∼ e, ∼ ent) (b ∼, f ∼) Note: Features defined over pairs of source types assigned to the same cluster; here - matches a common substring. Table 6: Optimal lexicon size (ratio of raw vocab.) Czech French Welsh Word-alignment 0.26 0.22 0.24 TM smoothing 0.28 0.38 0.51 tize model on Czech show the model selection procedure improving on a simple supervised baseline. 6 Related Work Previous work on automatic bilingual word clustering has been motivated somewhat differently and not made use of cluster-based models to assign translation probabilities directly (Wang et al., 1996), (Och, 1998). There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). These approaches have used morphological annotation (e.g. lemmas and part of speech tags) to provide explicit supervision. They have also involved manually specifying which morphological distinc975 tions are redundant (Goldwater and McClosky, 2005). In contrast, we attempt to learn both equivalence classes and redundant relations automatically. Our experiments with orthographic fe</context>
</contexts>
<marker>Wang, Lafferty, Waibel, 1996</marker>
<rawString>Ye-Yi Wang, John Lafferty, and Alex Waibel. 1996. Word clustering with parallel spoken language corpora. In Proc. of 4th International Conference on Spoken Language Processing, ICSLP 96, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Wolpert</author>
</authors>
<title>Determining whether two data sets are from the same distribution.</title>
<date>1995</date>
<booktitle>In 15th international workshop on Maximum Entropy and Bayesian Methods.</booktitle>
<contexts>
<context position="13705" citStr="Wolpert, 1995" startWordPosition="2155" endWordPosition="2156"> such as to maximise the marginal likelihood. Reassignments may alter the total number of clusters 1Distinct from the prior over model structure, p(C). 11 a 11 cf ECF eE£ 971 at any point. Updates can be calculated locally, for instance, given the sets of target tokens Dci and Dcj aligned to source types currently in clusters ci and cj, the change in log marginal likelihood if clusters ci and cj are merged into cluster c� is, p(D�c|α, �c) �ci,cj��c = log (3) p(Dci |α, ci)p(Dcj |α, cj) which is a Bayes factor in favour of the hypothesis that Dci and Dcj were sampled from the same distribution (Wolpert, 1995). Unlike its equivalent in maximum likelihood clustering, Eq.(3) may assume positive values favouring a smaller number of clusters when the data does not support a more complex hypothesis. The more complex model, with ci and cj modelled separately, is penalised for being able to model a wider range of data sets. The hyperparameter, α, is tied across clusters and taken to be proportional to the marginal (the ‘background’) distribution over target types in the corpus. Under this prior, source types aligned to the same target types, will be clustered together more readily if these target types ar</context>
<context position="30785" citStr="Wolpert, 1995" startWordPosition="4973" endWordPosition="4974">ide explicit supervision. They have also involved manually specifying which morphological distinc975 tions are redundant (Goldwater and McClosky, 2005). In contrast, we attempt to learn both equivalence classes and redundant relations automatically. Our experiments with orthographic features suggest that some morphological redundancies can be acquired in an unsupervised fashion. The marginal likelihood hard-clustering algorithm that we propose here for translation model selection can be viewed as a Bayesian k-means algorithm and is an application of Bayesian model selection techniques, e.g., (Wolpert, 1995). The Markov random field prior over model structure extends the fixed uniform prior over clusters implicit in k-means clustering and is common in computer vision (Geman and Geman, 1984). Recently Basu et al. (2004) used an MRF to embody hard constraints within semi-supervised clustering. In contrast, we use an iterative EM algorithm to learn soft constraints within the ‘prior’ monolingual space based on the results of clustering with bilingual statistics. 7 Conclusions and Future Work We proposed a framework for modelling lexical redundancy in machine translation and tackled optimisation of t</context>
</contexts>
<marker>Wolpert, 1995</marker>
<rawString>D.H. Wolpert. 1995. Determining whether two data sets are from the same distribution. In 15th international workshop on Maximum Entropy and Bayesian Methods.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mei Yang</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Phrase-based backoff models for machine translation of highly inflected languages.</title>
<date>2006</date>
<booktitle>In Proc. of the the European Chapter of the Association for Computational Linguistics</booktitle>
<contexts>
<context position="21012" citStr="Yang and Kirchhoff, 2006" startWordPosition="3368" endWordPosition="3371">he number of tokens in a sentence, hence the word-alignments can be associated with the original corpus (see Exp. 1). To extrapolate a mapping over phrases from our type-level models we can map each type within a phrase to its corresponding cluster label. This, however, results in a large number of distinct phrases being collapsed down to a single ‘clustered phrase’. Using these directly may spread probability mass too widely. Instead we use them to smooth the phrase translation model (see Exp. 2). Here we consider a simple interpolation scheme; they could also be used within a backoff model (Yang and Kirchhoff, 2006). 4.2 Experimental set-up The system we use is described in (Koehn, 2004). The phrase-based translation model includes phrase-level and lexical weightings in both directions. We use the decoder’s default behaviour for unknown words copying them verbatim to the output. Smoothed trigram language models are estimated on training sections of the parallel corpus. We used the parallel sections of the Prague Treebank (Cmejrek et al., 2004), French and English sections of the Europarl corpus (Koehn, 2005) and parallel text from the Welsh Assembly4 (see Table1). The source languages, Czech, French and </context>
</contexts>
<marker>Yang, Kirchhoff, 2006</marker>
<rawString>Mei Yang and Katrin Kirchhoff. 2006. Phrase-based backoff models for machine translation of highly inflected languages. In Proc. of the the European Chapter of the Association for Computational Linguistics 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Improvements in phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of the Human Language Technology Conference (HLT-NAACL</booktitle>
<contexts>
<context position="30047" citStr="Zens and Ney, 2004" startWordPosition="4863" endWordPosition="4866">timal lexicon size (ratio of raw vocab.) Czech French Welsh Word-alignment 0.26 0.22 0.24 TM smoothing 0.28 0.38 0.51 tize model on Czech show the model selection procedure improving on a simple supervised baseline. 6 Related Work Previous work on automatic bilingual word clustering has been motivated somewhat differently and not made use of cluster-based models to assign translation probabilities directly (Wang et al., 1996), (Och, 1998). There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). These approaches have used morphological annotation (e.g. lemmas and part of speech tags) to provide explicit supervision. They have also involved manually specifying which morphological distinc975 tions are redundant (Goldwater and McClosky, 2005). In contrast, we attempt to learn both equivalence classes and redundant relations automatically. Our experiments with orthographic features suggest that some morphological redundancies can be acquired in an unsupervised fashion. The marginal likelihood hard-clustering algorithm that we propose here for translation model s</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>R. Zens and H. Ney. 2004. Improvements in phrase-based statistical machine translation. In Proc. of the Human Language Technology Conference (HLT-NAACL 2004).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>