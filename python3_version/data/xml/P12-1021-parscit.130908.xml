<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99757">
Discriminative Pronunciation Modeling:
A Large-Margin, Feature-Rich Approach
</title>
<author confidence="0.957656">
Hao Tang, Joseph Keshet, and Karen Livescu
</author>
<affiliation confidence="0.960633">
Toyota Technological Institute at Chicago
</affiliation>
<address confidence="0.517905">
Chicago, IL USA
</address>
<email confidence="0.99869">
{haotang,jkeshet,klivescu}@ttic.edu
</email>
<sectionHeader confidence="0.996659" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999632958333333">
We address the problem of learning the map-
ping between words and their possible pro-
nunciations in terms of sub-word units. Most
previous approaches have involved genera-
tive modeling of the distribution of pronuncia-
tions, usually trained to maximize likelihood.
We propose a discriminative, feature-rich ap-
proach using large-margin learning. This ap-
proach allows us to optimize an objective
closely related to a discriminative task, to
incorporate a large number of complex fea-
tures, and still do inference efficiently. We
test the approach on the task of lexical access;
that is, the prediction of a word given a pho-
netic transcription. In experiments on a sub-
set of the Switchboard conversational speech
corpus, our models thus far improve classi-
fication error rates from a previously pub-
lished result of 29.1% to about 15%. We
find that large-margin approaches outperform
conditional random field learning, and that
the Passive-Aggressive algorithm for large-
margin learning is faster to converge than the
Pegasos algorithm.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.940607627906977">
One of the problems faced by automatic speech
recognition, especially of conversational speech, is
that of modeling the mapping between words and
their possible pronunciations in terms of sub-word
units such as phones. While pronouncing dictionar-
ies provide each word’s canonical pronunciation(s)
in terms of phoneme strings, running speech of-
ten includes pronunciations that differ greatly from
the dictionary. For example, some pronunciations
of “probably” in the Switchboard conversational
speech database are [p r aa b iy], [p r aa l iy], [p r
ay], and [p ow ih] (Greenberg et al., 1996). While
some words (e.g., common words) are more prone
to such variation than others, the effect is extremely
general: In the phonetically transcribed portion of
Switchboard, fewer than half of the word tokens
are pronounced canonically (Fosler-Lussier, 1999).
In addition, pronunciation variants sometimes in-
clude sounds not present in the dictionary at all,
such as nasalized vowels (“can’t” —* [k ae n n t])
or fricatives introduced due to incomplete consonant
closures (“legal” —* [l iy g fr ix l]).1 This varia-
tion makes pronunciation modeling one of the major
challenges facing speech recognition (McAllaster et
al., 1998; Jurafsky et al., 2001; Sarac¸lar and Khu-
danpur, 2004; Bourlard et al., 1999). 2
Most efforts to address the problem have involved
either learning alternative pronunciations and/or
their probabilities (Holter and Svendsen, 1999) or
using phonetic transformation (substitution, inser-
tion, and deletion) rules, which can come from lin-
guistic knowledge or be learned from data (Riley
et al., 1999; Hazen et al., 2005; Hutchinson and
Droppo, 2011). These have produced some im-
provements in recognition performance. However,
they also tend to cause additional confusability due
to the introduction of additional homonyms (Fosler-
1We use the ARPAbet phonetic alphabet with additional di-
acritics, such as [ n] for nasalization and [ fr] for frication.
2This problem is separate from the grapheme-to-phoneme
problem, in which pronunciations are predicted from a word’s
spelling; here, we assume the availability of a dictionary of
canonical pronunciations as is usual in speech recognition.
</bodyText>
<page confidence="0.967269">
194
</page>
<note confidence="0.9873665">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 194–203,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99535293442623">
Lussier et al., 2002). Some other alternatives are
articulatory pronunciation models, in which words
are represented as multiple parallel sequences of ar-
ticulatory features rather than single sequences of
phones, and which outperform phone-based models
on some tasks (Livescu and Glass, 2004; Jyothi et
al., 2011); and models for learning edit distances be-
tween dictionary and actual pronunciations (Ristad
and Yianilos, 1998; Filali and Bilmes, 2005).
All of these approaches are generative—i.e., they
provide distributions over possible pronunciations
given the canonical one(s)—and they are typically
trained by maximizing the likelihood over train-
ing data. In some recent work, discriminative ap-
proaches have been proposed, in which an objective
more closely related to the task at hand is optimized.
For example, (Vinyals et al., 2009; Korkmazskiy
and Juang, 1997) optimize a minimum classification
error (MCE) criterion to learn the weights (equiv-
alently, probabilities) of alternative pronunciations
for each word; (Schramm and Beyerlein, 2001) use
a similar approach with discriminative model com-
bination. In this work, the weighted alternatives are
then used in a standard (generative) speech recog-
nizer. In other words, these approaches optimize
generative models using discriminative criteria.
We propose a general, flexible discriminative ap-
proach to pronunciation modeling, rather than dis-
criminatively optimizing a generative model. We
formulate a linear model with a large number
of word-level and subword-level feature functions,
whose weights are learned by optimizing a discrim-
inative criterion. The approach is related to the re-
cently proposed segmental conditional random field
(SCRF) approach to speech recognition (Zweig et
al., 2011). The main differences are that we opti-
mize large-margin objective functions, which lead
to sparser, faster, and better-performing models than
conditional random field optimization in our exper-
iments; and we use a large set of different feature
functions tailored to pronunciation modeling.
In order to focus attention on the pronunciation
model alone, our experiments focus on a task that
measures only the mapping between words and sub-
word units. Pronunciation models have in the past
been tested using a variety of measures. For gener-
ative models, phonetic error rate of generated pro-
nunciations (Venkataramani and Byrne, 2001) and
phone- or frame-level perplexity (Riley et al., 1999;
Jyothi et al., 2011) are appropriate measures. For
our discriminative models, we consider the task
of lexical access; that is, prediction of a single
word given its pronunciation in terms of sub-word
units (Fissore et al., 1989; Jyothi et al., 2011). This
task is also sometimes referred to as “pronunciation
recognition” (Ristad and Yianilos, 1998) or “pro-
nunciation classification” (Filali and Bilmes, 2005).)
As we show below, our approach outperforms both
traditional phonetic rule-based models and the best
previously published results on our data set obtained
with generative articulatory approaches.
</bodyText>
<sectionHeader confidence="0.920602" genericHeader="method">
2 Problem setting
</sectionHeader>
<bodyText confidence="0.999808757575757">
We define a pronunciation of a word as a representa-
tion of the way it is produced by a speaker in terms
of some set of linguistically meaningful sub-word
units. A pronunciation can be, for example, a se-
quence of phones or multiple sequences of articu-
latory features such as nasality, voicing, and tongue
and lip positions. For purposes of this paper, we will
assume that a pronunciation is a single sequence of
units, but the approach applies to other representa-
tions. We distinguish between two types of pronun-
ciations of a word: (i) canonical pronunciations, the
ones typically found in the dictionary, and (ii) sur-
face pronunciations, the ways a speaker may actu-
ally produce the word. In the task of lexical access
we are given a surface pronunciation of a word, and
our goal is to predict the word.
Formally, we define a pronunciation as a sequence
of sub-word units p = (p1, p2, ... , pK), where pk E
P for all 1 G k G K and P is the set of all sub-word
units. The index k can represent either a fixed-length
frame or a variable-length segment. P* denotes the
set of all finite-length sequences over P. We denote
a word by w E V where V is the vocabulary. Our
goal is to find a function f : P* --+ V that takes as
input a surface pronunciation and returns the word
from the vocabulary that was spoken.
In this paper we propose a discriminative super-
vised learning approach for learning the function f
from a training set of pairs (p, w). We aim to find a
function f that performs well on the training set as
well as on unseen examples. Let w� = f(p) be the
predicted word given the pronunciation p. We assess
the quality of the function f by the zero-one loss: if
</bodyText>
<page confidence="0.998385">
195
</page>
<bodyText confidence="0.999863857142857">
w =� w� then the error is one, otherwise the error is
zero. The goal of the learning process is to mini-
mize the expected zero-one loss, where the expec-
tation is taken with respect to a fixed but unknown
distribution over words and surface pronunciations.
In the next section we present a learning algorithm
that aims to minimize the expected zero-one loss.
</bodyText>
<sectionHeader confidence="0.993866" genericHeader="method">
3 Algorithm
</sectionHeader>
<bodyText confidence="0.99993352173913">
Similarly to previous work in structured prediction
(Taskar et al., 2003; Tsochantaridis et al., 2005),
we construct the function f from a predefined set
of N feature functions, 1φjJN j=1, each of the form
φj : P* xV —* R. Each feature function takes a sur-
face pronunciation p and a proposed word w and re-
turns a scalar which, intuitively, should be correlated
with whether the pronunciation p corresponds to the
word w. The feature functions map pronunciations
of different lengths along with a proposed word to a
vector of fixed dimension in RN. For example, one
feature function might measure the Levenshtein dis-
tance between the pronunciation p and the canonical
pronunciation of the word w. This feature function
counts the minimum number of edit operations (in-
sertions, deletions, and substitutions) that are needed
to convert the surface pronunciation to the canonical
pronunciation; it is low if the surface pronunciation
is close to the canonical one and high otherwise.
The function f maximizes a score relating the
word w to the pronunciation p. We restrict our-
selves to scores that are linear in the feature func-
tions, where each φj is scaled by a weight θj:
</bodyText>
<equation confidence="0.9930635">
EN θjφj(p, w) = θ · φ(p, w),
j=1
</equation>
<bodyText confidence="0.999691642857143">
where we have used vector notation for the feature
functions φ = (φ1, ... , φN) and for the weights
θ = (θ1, ... , θN). Linearity is not a very strong
restriction, since the feature functions can be arbi-
trarily non-linear. The function f is defined as the
word w that maximizes the score,
where nπ is 1 if predicate π holds and 0 other-
wise, and where ρ is an (unknown) distribution from
which the examples in our training set are sampled
i.i.d. Let S = I(p1, w1), ... , (pm, wm)I be the
training set. Instead of working directly with the
zero-one loss, which is non-smooth and non-convex,
we use the surrogate hinge loss, which upper-bounds
the zero-one loss:
</bodyText>
<equation confidence="0.9887215">
L(θ, pi, wi) = wEaVx Inwi#w
�− θ · φ(pi, wi) + θ · φ(pi, w) . (1)
</equation>
<bodyText confidence="0.9999016">
Finding the weight vector θ that minimizes the
`2-regularized average of this loss function is the
structured support vector machine (SVM) problem
(Taskar et al., 2003; Tsochantaridis et al., 2005):
where λ is a user-defined tuning parameter that bal-
ances between regularization and loss minimization.
In practice, we have found that solving the
quadratic optimization problem given in Eq. (2) con-
verges very slowly using standard methods such as
stochastic gradient descent (Shalev-Shwartz et al.,
2007). We use a slightly different algorithm, the
Passive-Aggressive (PA) algorithm (Crammer et al.,
2006), whose average loss is comparable to that of
the structured SVM solution (Keshet et al., 2007).
The Passive-Aggressive algorithm is an efficient
online algorithm that, under some conditions, can
be viewed as a dual-coordinate ascent minimizer of
Eq. (2) (The connection to dual-coordinate ascent
can be found in (Hsieh et al., 2008)). The algorithm
begins by setting θ = 0 and proceeds in rounds.
In the t-th round the algorithm picks an example
(pi, wi) from S at random uniformly without re-
placement. Denote by θt−1 the value of the weight
vector before the t-th round. Let wti denote the pre-
dicted word for the i-th example according to θt−1:
</bodyText>
<equation confidence="0.917109555555556">
θ* = argmin m L(θ, pi, wi), (2)
θ 211θ112 + m E
i=1
f(p) = argmax θ · φ(p, w). wti = argmax θt−1 · φ(pi, w) + nwi#w.
wEV wEV
Our goal in learning θ is to minimize the expected
zero-one loss:
θ* = argmin [nw�=f(p) � ,
E(p,w)_ρ
</equation>
<bodyText confidence="0.832124333333333">
θ
Let Aφt i = φ(pi, wi) − φ(pi, wti). Then the algo-
rithm updates the weight vector θt as follows:
</bodyText>
<equation confidence="0.9673995">
θt = θt−1 + αt iAφt (3)
i
</equation>
<page confidence="0.971505">
196
</page>
<bodyText confidence="0.991611">
where
</bodyText>
<equation confidence="0.996652">
� �
1 ✶wi�= ˆwt i − � · Δ�t i
αt i =min
λm, kΔ(0tik
L(8, pi, wi)
(4)
</equation>
<bodyText confidence="0.923309">
where the probability is defined as
</bodyText>
<equation confidence="0.610865">
�wEV
</equation>
<bodyText confidence="0.965404875">
Minimization of Eq. (2) under the log-loss results in
a probabilistic model commonly known as a condi-
tional random field (CRF) (Lafferty et al., 2001). By
taking the sub-gradient of Eq. (4), we can obtain an
update rule similar to the one shown in Eq. (3).
4 Feature functions
Before defining the feature functions, we define
some notation. Suppose p
</bodyText>
<equation confidence="0.3352515">
P* is a sequence of
sub-word units. We use
</equation>
<bodyText confidence="0.909389277777778">
to denote the n-gram
substring
... pn. The two substrings a and b are
said to be equal if they have the same length and
ai = bi for 1
i
n. For a given sub-word unit n-
Pn, we use the shorthand u
p to mean
that we can find u in p; i.e., there exists an index i
such that
= u. We use
to denote the length
of the sequence p.
We assume we have a pronunciation dictionary,
which is a set of words and their baseforms. We ac-
cess the dictionary through the function pron, which
takes a word w
</bodyText>
<equation confidence="0.947321066666667">
V and return
=−logPθ(wi|pi)
eθ&apos;φ(pi,wi)Pθ(wi|pi)
eθ�φ(p,w) .
∈
p1:n
p1
≤
≤
∈
∈
pi:i+n
|p|
∈
s a set of baseforms.
</equation>
<bodyText confidence="0.873016888888889">
functions
ing asequence of sub-word units as a
and n-gram sub-sequences as
In this anal-
ogy, we use sub-sequences in surface pronunciations
for baseforms in the dictionary
“document”
“words.”
“search”
. These
observed pronunciations of a given word in the train-
ing set, along with the discriminative power of the n-
The term frequency of a sub-word unit n-gram
u
Pn in a sequence p is the length-normali
∈
zed
frequency of the n-gram in the sequence:
</bodyText>
<equation confidence="0.959809">
u
+
|p|�|u|+1
1|p|−|
|
1 � ✶u=pi:i+�u�−1.
</equation>
<bodyText confidence="0.99923475">
Next, define the set of words in the training set that
contain the n-gram u as Vu = {w ∈ V  |(p, w) ∈
S, u ∈ p}. The inverse document frequency (IDF)
of an n-gram u is defined as
</bodyText>
<figure confidence="0.313759">
Fu =log |V|
|Vu|.
197 features measure the frequency of each n-gram in
and
Suppose the vocabulary is indexed: V =
... , wn}. Define ew as a binary
IDF.
{w1,
vector with
elements
</figure>
<figureCaption confidence="0.164309">
gram. These features are therefore only meaningful
</figureCaption>
<bodyText confidence="0.916039">
for words actually observed in training.
</bodyText>
<equation confidence="0.9649365">
(ew)i = ✶wi=w.
w)
(TFu(p)
IDFu)
</equation>
<bodyText confidence="0.943382571428571">
where
is the tensor
product. We therefore have as many TF-IDF feature
functions as we have n-grams. In practice, we only
consider n-grams of a certain order (e.g., bigrams).
The following toy example demonstrates how the
TF-IDF features are computed. Suppose we have
</bodyText>
<equation confidence="0.834449631578948">
V = {problem, probably}. The dictionary maps
to /pcl p r
bcl b
ax m/ and
to /pcl p r
bcl b
iy/, and our input is
(p, w)
r
b
iy], problem). Then for the bi-
gram
iy/, we have
= 1/5 (one out of
five bigrams in p), and
= log(2/1) (one
word out of two in the dictionary). The indicator
vector is
=
φu(p,
=
×
⊗ ew,
⊗:Raxb×Rcxd→Racxbd
“problem”
aa
l
“prob-
ably”
aa
l
=([p
aa
l
/l
TF/l iy/(p)
IDF/l iy/
eproblem
</equation>
<bodyText confidence="0.967552125">
[10]T, so the final feature is
In practice we iterate over the m examples in the
training set several times; each such iteration is an
epoch. The final weight vector is set to the average
over all weight vectors during training.
An alternative loss function that is often used to
solve structured prediction problems is the log-loss:
gram u
</bodyText>
<subsectionHeader confidence="0.90391">
4.1 TF-IDF feature
</subsectionHeader>
<bodyText confidence="0.9997778">
Term frequency (TF) and inverse document fre-
quency (IDF) are measures that have been heavily
used in information retrieval to search for documents
using word queries (Salton et al., 1975). Similarly to
(Zweig et al., 2010), we adapt TF and IDF by treat-
</bodyText>
<equation confidence="0.888774333333333">
to
TFu(p) =
ID
</equation>
<bodyText confidence="0.990875">
IDF represents the discriminative power of an n-
gram: An n-gram that occurs in few words is better
at word discrimination than a very common n-gram.
Finally, we define word-specific features using TF
We define the TF-IDF feature function of u as
</bodyText>
<equation confidence="0.519763">
L0J
.
</equation>
<subsectionHeader confidence="0.984164">
4.2 Length feature function
</subsectionHeader>
<bodyText confidence="0.97545745">
The length feature functions measure how the length
of a word’s surface form tends to deviate from the
baseform. These functions are parameterized by a
and b and are defined as
0a&lt;Δ`&lt;b(p, w) = na&lt;Δ`&lt;b ⊗ ew,
where At = |p |− |v|, for some baseform v ∈
pron(w). The parameters a and b can be either posi-
tive or negative, so the model can learn whether the
surface pronunciations of a word tend to be longer
or shorter than the baseform. Like the TF-IDF fea-
tures, this feature is only meaningful for words ac-
tually observed in training.
As an example, suppose we have V =
{problem, probably}, and the word “probably” has
two baseforms, /pcl p r aa bcl b l iy/ (of length
eight) and /pcl p r aa bcl b ax bcl b l iy/ (of length
eleven). If we are given an input (p, w) =
([pcl p r aa bcl l ax m], probably), whose length of
the surface form is eight, then the length features for
the ranges 0 ≤ At &lt; 1 and −3 ≤ At &lt; −2 are
</bodyText>
<equation confidence="0.99959975">
T
00&lt;Δ`&lt;1(p, w) = [0 1] ,
T
0−3&lt;Δ`&lt;−2(p,w) = [0 1] ,
</equation>
<bodyText confidence="0.991571">
respectively. Other length features are all zero.
</bodyText>
<subsectionHeader confidence="0.959392">
4.3 Phonetic alignment feature functions
</subsectionHeader>
<bodyText confidence="0.996444772727273">
Beyond the length, we also measure specific pho-
netic deviations from the dictionary. We define pho-
netic alignment features that count the (normalized)
frequencies of phonetic insertions, phonetic dele-
tions, and substitutions of one surface phone for an-
other baseform phone. Given (p, w), we use dy-
namic programming to align the surface form p with
all of the baseforms of w. Following (Riley et al.,
1999), we encode a phoneme/phone with a 4-tuple:
consonant manner, consonant place, vowel manner,
and vowel place. Let the dash symbol “−” be a
gap in the alignment (corresponding to an inser-
tion/deletion). Given p, q ∈ P ∪ {−}, we say that
a pair (p, q) is a deletion if p ∈ P and q = −, is
an insertion if p = − and q ∈ P, and is a substi-
tution if both p, q ∈ P. Given p, q ∈ P ∪ {−}, let
(s1, s2, s3, s4) and (t1, t2, t3, t4) be the correspond-
ing 4-tuple encoding of p and q, respectively. The
pcl p r aa pcl p er l iy
pcl p r aa bcl b − l iy
pcl p r aa pcl p er − − l iy
pcl p r aa bcl b ax bcl b l iy
</bodyText>
<tableCaption confidence="0.993672">
Table 1: Possible alignments of [p r aa pcl p er l iy] with
two baseforms of “probably” in the dictionary.
</tableCaption>
<bodyText confidence="0.577588">
similarity between p and q is defined as
</bodyText>
<equation confidence="0.91905675">
�
1, if p = − or q = −;
�4 1 n si=ti otherwise.
,
</equation>
<bodyText confidence="0.993108857142857">
Consider aligning p with the Kw = |pron(w)|
baseforms of w. Define the length of the align-
ment with the k-th baseform as Lk, for 1 ≤ k ≤
Kw. The resulting alignment is a sequence of pairs
(ak,1, bk,1), ... , (ak,Lk, bk,Lk), where ak,i, bk,i ∈
P ∪ {−} for 1 ≤ i ≤ Lk. Now we define the align-
ment features, given p, q ∈ P ∪ {−}, as
</bodyText>
<equation confidence="0.819139714285714">
//,„„ 1 Kw L k
0p—q (P, w) Z E E nak,i=p, bk,i=q,
p k=1 i=1
E=Z where the normalization term is
Kw Lk
=1Ei=1 nak,i=p, p ∈ P;
p |p |· Kw if p = −.
</equation>
<bodyText confidence="0.999081333333333">
The normalization for insertions differs from the
normalization for substitutions and deletions, so that
the resulting values always lie between zero and one.
As an example, consider the input pair (p, w) =
([p r aa pcl p er l iy], probably) and suppose there
are two baseforms of the word “probably” in the
dictionary. Let one possible alignments be the one
shown in Table 1. Since /p/ occurs four times in the
alignments and two of them are aligned to [b], the
feature for p → b is then 0p—b(p, w) = 2/4.
Unlike the TF-IDF feature functions and the
length feature functions, the alignment feature func-
tions can assign a non-zero score to words that are
not seen at training time (but are in the dictionary),
as long as there is a good alignment with their base-
forms. The weights given to the alignment fea-
tures are the analogue of substitution, insertion, and
deletion rule probabilities in traditional phone-based
pronunciation models such as (Riley et al., 1999);
they can also be seen as a generalized version of the
Levenshtein features of (Zweig et al., 2011).
</bodyText>
<equation confidence="0.929219">
s(p, q) =
</equation>
<page confidence="0.994975">
198
</page>
<subsectionHeader confidence="0.984405">
4.4 Dictionary feature function
</subsectionHeader>
<bodyText confidence="0.9999975">
The dictionary feature is an indicator of whether
a pronunciation is an exact match to a baseform,
which also generalizes to words unseen in training.
We define the dictionary feature as
</bodyText>
<equation confidence="0.956049">
φdict(p,w) = ✶p∈pron(w).
</equation>
<bodyText confidence="0.994062">
For example, assume there is a baseform
/pcl p r aa bcl b l iy/ for the word “probably” in
the dictionary, and p = /pcl p r aa bcl b l iy/. Then
φdict (p, probably) = 1, while φdict(p, problem) = 0.
</bodyText>
<subsectionHeader confidence="0.989302">
4.5 Articulatory feature functions
</subsectionHeader>
<bodyText confidence="0.989185051282051">
Articulatory models represented as dynamic
Bayesian networks (DBNs) have been successful
in the past on the lexical access task (Livescu
and Glass, 2004; Jyothi et al., 2011). In such
models, pronunciation variation is seen as the
result of asynchrony between the articulators (lips,
tongue, etc.) and deviations from the intended
articulatory positions. Given a sequence p and a
word w, we use the DBN to produce an alignment
at the articulatory level, which is a sequence of
7-tuples, representing the articulatory variables3 lip
opening, tongue tip location and opening, tongue
body location and opening, velum opening, and
glottis opening. We extract three kinds of features
from the output—substitutions, asynchrony, and
log-likelihood.
The substitution features are similar to the pho-
netic alignment features in Section 4.3, except that
the alignment is not a sequence of pairs but a se-
quence of 14-tuples (7 for the baseform and 7 for the
surface form). The DBN model is based on articu-
latory phonology (Browman and Goldstein, 1992),
in which there are no insertions and deletions, only
substitutions (apparent insertions and deletions are
accounted for by articulatory asynchrony). For-
mally, consider the seven sets of articulatory vari-
able values F1, ... , F7. For example, F1 could be
all of the values of lip opening, F1 ={closed, crit-
ical, narrow, wide}. Let T = {F1, ... , F7}. Con-
sider an articulatory variable F E T. Suppose the
alignment for F is (a1, b1), ... , (aL, bL), where L
3We use the term “articulatory variable” for the “articulatory
features” of (Livescu and Glass, 2004; Jyothi et al., 2011), in
order to avoid confusion with our feature functions.
is the length of the alignment and ai, bi E F, for
1 &lt; i &lt; L. Here the ai are the intended articulatory
variable values according to the baseform, and the
bi are the corresponding realized values. For each
a, b E F we define a substitution feature function:
</bodyText>
<equation confidence="0.99407275">
XL
1
φa→b(p, w) = L
i=1
</equation>
<bodyText confidence="0.999728">
The asynchrony features are also extracted from
the DBN alignments. Articulators are not always
synchronized, which is one cause of pronunciation
variation. We measure this by looking at the phones
that two articulators are aiming to produce, and find
the time difference between them. Formally, we
consider two articulatory variables Fh, Fk E T.
Let the alignment between the two variables be
(a1, b1), ... , (aL, bL), where now ai E Fh and bi E
Fk. Each ai and bi can be mapped back to the cor-
responding phone index th,i and tk,i, for 1 &lt; i &lt; L.
The average degree of asynchrony is then defined as
</bodyText>
<equation confidence="0.997786">
async(Fh, Fk) = L
1 XL (th,i − tk,i) .
i=1
</equation>
<bodyText confidence="0.981337">
More generally, we compute the average asynchrony
between any two sets of variables T1, T2 C T as
</bodyText>
<equation confidence="0.848862333333333">
async(T1, T2) =
⎤
1|T1|FX1th,i − |T2|FkX2tk,i
</equation>
<bodyText confidence="0.791512">
We then define the asynchrony features as
</bodyText>
<equation confidence="0.760226">
φa≤async(F1,F2)≤b = ✶a≤async(F1,F2)≤b.
</equation>
<bodyText confidence="0.9043083">
Finally, the log-likelihood feature is the DBN
alignment score, shifted and scaled so that the value
lies between zero and one,
φdbn-LL(p, w) = L(p, w) − h
c ,
where L is the log-likelihood function of the DBN,
h is the shift, and c is the scale.
Note that none of the DBN features are word-
specific, so that they generalize to words in the dic-
tionary that are unseen in the training set.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.972967">
All experiments are conducted on a subset of the
Switchboard conversational speech corpus that has
</bodyText>
<figure confidence="0.839667857142857">
✶ai=a, bi=b.
⎡
⎣
1
L
XL
i=1
</figure>
<page confidence="0.995767">
199
</page>
<bodyText confidence="0.999982955555555">
been labeled at a fine phonetic level (Greenberg et
al., 1996); these phonetic transcriptions are the input
to our lexical access models. The data subset, phone
set P, and dictionary are the same as ones previ-
ously used in (Livescu and Glass, 2004; Jyothi et al.,
2011). The dictionary contains 3328 words, consist-
ing of the 5000 most frequent words in Switchboard,
excluding ones with fewer than four phones in their
baseforms. The baseforms use a similar, slightly
smaller phone set (lacking, e.g., nasalization). We
measure performance by error rate (ER), the propor-
tion of test examples predicted incorrectly.
The TF-IDF features used in the experiments
are based on phone bigrams. For all of the ar-
ticulatory DBN features, we use the DBN from
(Livescu, 2005) (the one in (Jyothi et al., 2011)
is more sophisticated and may be used in fu-
ture work). For the asynchrony features, the ar-
ticulatory pairs are (F1, F2) ∈ {({tongue tip},
{tongue body}), ({lip opening}, {tongue tip,
tongue body}), and ({lip opening, tongue tip,
tongue body}, {glottis, velum})}, as in (Livescu,
2005). The parameters (a, b) of the length and
asynchrony features are drawn from (a, b) ∈
{(−3,−2), (−2,−1),... (2,3)}.
We compare the CRF4, Passive-Aggressive (PA),
and Pegasos learning algorithms. The regularization
parameter A is tuned on the development set. We run
all three algorithms for multiple epochs and pick the
best epoch based on development set performance.
For the first set of experiments, we use the same
division of the corpus as in (Livescu and Glass,
2004; Jyothi et al., 2011) into a 2492-word train-
ing set, a 165-word development set, and a 236-
word test set. To give a sense of the difficulty of
the task, we test two simple baselines. One is a lex-
icon lookup: If the surface form is found in the dic-
tionary, predict the corresponding word; otherwise,
guess randomly. For a second baseline, we calcu-
late the Levenshtein (0-1 edit) distance between the
input pronunciation and each dictionary baseform,
and predict the word corresponding to the baseform
closest to the input. The results are shown in the first
two rows of Table 2. We can see that, by adding just
the Levenshtein distance, the error rate drops signif-
</bodyText>
<footnote confidence="0.972564">
4We use the term “CRF” since the learning algorithm corre-
sponds to CRF learning, although the task is multiclass classifi-
cation rather than a sequence or structure prediction task.
</footnote>
<table confidence="0.99927675">
Model ER
lexicon lookup (from (Livescu, 2005)) 59.3%
lexicon + Levenshtein distance 41.8%
(Jyothi et al., 2011) 29.1%
CRF/DP+ 21.5%
PA/DP+ 15.2%
Pegasos/DP+ 14.8%
PA/ALL 15.2%
</table>
<tableCaption confidence="0.59594775">
Table 2: Lexical access error rates (ER) on the same data
split as in (Livescu and Glass, 2004; Jyothi et al., 2011).
Models labeled X/Y use learning algorithm X and feature
set Y. The feature set DP+ contains TF-IDF, DP align-
ment, dictionary, and length features. The set ALL con-
tains DP+ and the articulatory DBN features. The best
results are in bold; the differences among them are in-
significant (according to McNemar’s test with p = .05).
</tableCaption>
<bodyText confidence="0.998071451612903">
icantly. However, both baselines do quite poorly.
Table 2 shows the best previous result on this data
set from the articulatory model of Jyothi et al., which
greatly improves over our baselines as well as over
a much more complex phone-based model (Jyothi
et al., 2011). The remaining rows of Table 2 give
results with our feature functions and various learn-
ing algorithms. The best result for PA/DP+ (the PA
algorithm using all features besides the DBN fea-
tures) on the development set is with A = 100 and 5
epochs. Tested on the test set, this model improves
over (Jyothi et al., 2011) by 13.9% absolute (47.8%
relative). The best result for Pegasos with the same
features on the development set is with A = 0.01 and
10 epochs. On the test set, this model gives a 14.3%
absolute improvement (49.1% relative). CRF learn-
ing with the same features performs about 6% worse
than the corresponding PA and Pegasos models.
The single-threaded running time for PA/DP+ and
Pegasos/DP+ is about 40 minutes per epoch, mea-
sured on a dual-core AMD 2.4GHz CPU with 8GB
of memory; for CRF, it takes about 100 minutes for
each epoch, which is almost entirely because the
weight vector 0 is less sparse with CRF learning.
In the PA and Pegasos algorithms, we only update 0
for the most confusable word, while in CRF learn-
ing, we sum over all words. In our case, the number
of non-zero entries in 0 for PA and Pegasos is around
800,000; for CRF, it is over 4,000,000. Though PA
and Pegasos take roughly the same amount of time
per epoch, Pegasos tends to require more epochs to
</bodyText>
<page confidence="0.992944">
200
</page>
<figureCaption confidence="0.999599142857143">
Figure 1: 5-fold cross validation (CV) results. The lex-
icon lookup baseline is labeled lex; lex + lev = lexi-
con lookup with Levenshtein distance. Each point cor-
responds to the test set error rate for one of the 5 data
splits. The horizontal red line marks the mean of the re-
sults with means labeled, and the vertical red line indi-
cates the mean plus and minus one standard deviation.
</figureCaption>
<bodyText confidence="0.997215">
achieve the same performance as PA.
For the second experiment, we perform 5-fold
cross-validation. We combine the training, devel-
opment, and test sets from the previous experiment,
and divide the data into five folds. We take three
folds for training, one fold for tuning A and the best
epoch, and the remaining fold for testing. The re-
sults on the test fold are shown in Figure 1, which
compares the learning algorithms, and Figure 2,
which compares feature sets. Overall, the results
are consistent with our first experiment. The fea-
ture selection experiments in Figure 2 shows that
the TF-IDF features alone are quite weak, while the
dynamic programming alignment features alone are
quite good. Combining the two gives close to our
best result. Although the marginal improvement gets
smaller as we add more features, in general perfor-
mance keeps improving the more features we add.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999450909090909">
The results in Section 5 are the best obtained thus
far on the lexical access task on this conversational
data set. Large-margin learning, using the Passive-
Aggressive and Pegasos algorithms, has benefits
over CRF learning for our task: It produces sparser
models, is faster, and produces better lexical access
results. In addition, the PA algorithm is faster than
Pegasos on our task, as it requires fewer epochs.
Our ultimate goal is to incorporate such models
into complete speech recognizers, that is to predict
word sequences from acoustics. This requires (1)
</bodyText>
<figureCaption confidence="0.99942675">
Figure 2: Feature selection results for five-fold cross val-
idation. In the figure, phone bigram TF-IDF is labeled
p2; phonetic alignment with dynamic programming is la-
beled DP. The dots and lines are as defined in Figure 1.
</figureCaption>
<bodyText confidence="0.991609541666667">
extension of the model and learning algorithm to
word sequences and (2) feature functions that re-
late acoustic measurements to sub-word units. The
extension to sequences can be done analogously to
segmental conditional random fields (SCRFs). The
main difference between SCRFs and our approach
would be the large-margin learning, which can be
straightforwardly applied to sequences. To incorpo-
rate acoustics, we can use feature functions based on
classifiers of sub-word units, similarly to previous
work on CRF-based speech recognition (Gunawar-
dana et al., 2005; Morris and Fosler-Lussier, 2008;
Prabhavalkar et al., 2011). Richer, longer-span (e.g.,
word-level) feature functions are also possible.
Thus far we have restricted the pronunciation-to-
word score to linear combinations of feature func-
tions. This can be extended to non-linear combi-
nations using a kernel. This may be challenging in
a high-dimensional feature space. One possibility
is to approximate the kernels as in (Keshet et al.,
2011). Additional extensions include new feature
functions, such as context-sensitive alignment fea-
tures, and joint inference and learning of the align-
ment models embedded in the feature functions.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999075">
We thank Raman Arora, Arild Næss, and the anony-
mous reviewers for helpful suggestions. This re-
search was supported in part by NSF grant IIS-
0905633. The opinions expressed in this work are
those of the authors and do not necessarily reflect
the views of the funding agency.
</bodyText>
<page confidence="0.997393">
201
</page>
<sectionHeader confidence="0.983437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999115754716981">
H. Bourlard, S. Furui, N. Morgan, and H. Strik. 1999.
Special issue on modeling pronunciation variation for
automatic speech recognition. Speech Communica-
tion, 29(2-4).
C. P. Browman and L. Goldstein. 1992. Articulatory
phonology: an overview. Phonetica, 49(3-4).
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive aggressive al-
gorithms. Journal of Machine Learning Research, 7.
K. Filali and J. Bilmes. 2005. A dynamic Bayesian
framework to model context and memory in edit dis-
tance learning: An application to pronunciation classi-
fication. In Proc. Association for Computational Lin-
guistics (ACL).
L. Fissore, P. Laface, G. Micca, and R. Pieraccini. 1989.
Lexical access to large vocabularies for speech recog-
nition. IEEE Transactions on Acoustics, Speech, and
Signal Processing, 37(8).
E. Fosler-Lussier, I. Amdal, and H.-K. J. Kuo. 2002. On
the road to improved lexical confusability metrics. In
ISCA Tutorial and Research Workshop (ITRW) on Pro-
nunciation Modeling and Lexicon Adaptation for Spo-
ken Language Technology.
J. E. Fosler-Lussier. 1999. Dynamic Pronunciation Mod-
els forAutomatic Speech Recognition. Ph.D. thesis, U.
C. Berkeley.
S. Greenberg, J. Hollenback, and D. Ellis. 1996. Insights
into spoken language gleaned from phonetic transcrip-
tion of the Switchboard corpus. In Proc. International
Conference on Spoken Language Processing (ICSLP).
A. Gunawardana, M. Mahajan, A. Acero, and J. Platt.
2005. Hidden conditional random fields for phone
classification. In Proc. Interspeech.
T. J. Hazen, I. L. Hetherington, H. Shu, and K. Livescu.
2005. Pronunciation modeling using a finite-state
transducer representation. Speech Communication,
46(2).
T. Holter and T. Svendsen. 1999. Maximum likelihood
modelling of pronunciation variation. Speech Commu-
nication.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear SVM. In Proc. Interna-
tional Conference on Machine Learning (ICML).
B. Hutchinson and J. Droppo. 2011. Learning non-
parametric models of pronunciation. In Proc. Inter-
national Conference on Acoustics, Speech, and Signal
Processing (ICASSP).
D. Jurafsky, W. Ward, Z. Jianping, K. Herold, Y. Xi-
uyang, and Z. Sen. 2001. What kind of pronunciation
variation is hard for triphones to model? In Proc. In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing (ICASSP).
P. Jyothi, K. Livescu, and E. Fosler-Lussier. 2011. Lex-
ical access experiments with context-dependent artic-
ulatory feature-based models. In Proc. International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP).
J. Keshet, S. Shalev-Shwartz, Y. Singer, and D. Chazan.
2007. A large margin algorithm for speech and au-
dio segmentation. IEEE Transactions on Acoustics,
Speech, and Language Processing, 15(8).
J. Keshet, D. McAllester, and T. Hazan. 2011. PAC-
Bayesian approach for minimization of phoneme error
rate. In Proc. International Conference on Acoustics,
Speech, and Signal Processing (ICASSP).
F. Korkmazskiy and B.-H. Juang. 1997. Discriminative
training of the pronunciation networks. In Proc. IEEE
Workshop on Automatic Speech Recognition and Un-
derstanding (ASRU).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional Random Fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. Interna-
tional Conference on Machine Learning (ICML).
K. Livescu and J. Glass. 2004. Feature-based pronun-
ciation modeling with trainable asynchrony probabil-
ities. In Proc. International Conference on Spoken
Language Processing (ICSLP).
K. Livescu. 2005. Feature-based Pronunciation Model-
ing for Automatic Speech Recognition. Ph.D. thesis,
Massachusetts Institute of Technology.
D. McAllaster, L. Gillick, F. Scattone, and M. Newman.
1998. Fabricating conversational speech data with
acoustic models : A program to examine model-data
mismatch. In Proc. International Conference on Spo-
ken Language Processing (ICSLP).
J. Morris and E. Fosler-Lussier. 2008. Conditional ran-
dom fields for integrating local discriminative classi-
fiers. IEEE Transactions on Acoustics, Speech, and
Language Processing, 16(3).
R. Prabhavalkar, E. Fosler-Lussier, and K. Livescu. 2011.
A factored conditional random field model for artic-
ulatory feature forced transcription. In Proc. IEEE
Workshop on Automatic Speech Recognition and Un-
derstanding (ASRU).
M. Riley, W. Byrne, M. Finke, S. Khudanpur, A. Ljolje,
J. McDonough, H. Nock, M. Saraclar, C. Wooters, and
G. Zavaliagkos. 1999. Stochastic pronunciation mod-
elling from hand-labelled phonetic corpora. Speech
Communication, 29(2-4).
E. S. Ristad and P. N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 20(2).
G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Commun. ACM,
18.
</reference>
<page confidence="0.971938">
202
</page>
<reference confidence="0.999475108108108">
M. Sarac¸lar and S. Khudanpur. 2004. Pronunciation
change in conversational speech and its implications
for automatic speech recognition. Computer Speech
and Language, 18(4).
H. Schramm and P. Beyerlein. 2001. Towards discrimi-
native lexicon optimization. In Proc. Eurospeech.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pega-
sos: Primal Estimated sub-GrAdient SOlver for SVM.
In Proc. International Conference on Machine Learn-
ing (ICML).
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Advances in Neural Information
Processing Systems (NIPS) 17.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6.
V. Venkataramani and W. Byrne. 2001. MLLR adap-
tation techniques for pronunciation modeling. In
Proc. IEEE Workshop on Automatic Speech Recogni-
tion and Understanding (ASRU).
O. Vinyals, L. Deng, D. Yu, and A. Acero. 2009. Dis-
criminative pronunciation learning using phonetic de-
coder and minimum-classification-error criterion. In
Proc. International Conference on Acoustics, Speech,
and Signal Processing (ICASSP).
G. Zweig, P. Nguyen, and A. Acero. 2010. Continuous
speech recognition with a TF-IDF acoustic model. In
Proc. Interspeech.
G. Zweig, P. Nguyen, D. Van Compernolle, K. De-
muynck, L. Atlas, P. Clark, G. Sell, M. Wang, F. Sha,
H. Hermansky, D. Karakos, A. Jansen, S. Thomas,
G.S.V.S. Sivaram, S. Bowman, and J. Kao. 2011.
Speech recognition with segmental conditional ran-
dom fields: A summary of the JHU CLSP 2010 sum-
mer workshop. In Proc. International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
</reference>
<page confidence="0.999204">
203
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.290931">
<title confidence="0.992885">Discriminative Pronunciation A Large-Margin, Feature-Rich Approach</title>
<author confidence="0.887423">Hao Tang</author>
<author confidence="0.887423">Joseph Keshet</author>
<author confidence="0.887423">Karen</author>
<note confidence="0.5295335">Toyota Technological Institute at Chicago, IL</note>
<abstract confidence="0.98874268">We address the problem of learning the mapping between words and their possible pronunciations in terms of sub-word units. Most previous approaches have involved generative modeling of the distribution of pronunciations, usually trained to maximize likelihood. We propose a discriminative, feature-rich approach using large-margin learning. This approach allows us to optimize an objective closely related to a discriminative task, to incorporate a large number of complex features, and still do inference efficiently. We test the approach on the task of lexical access; that is, the prediction of a word given a phonetic transcription. In experiments on a subset of the Switchboard conversational speech corpus, our models thus far improve classification error rates from a previously published result of 29.1% to about 15%. We find that large-margin approaches outperform conditional random field learning, and that the Passive-Aggressive algorithm for largemargin learning is faster to converge than the Pegasos algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Bourlard</author>
<author>S Furui</author>
<author>N Morgan</author>
<author>H Strik</author>
</authors>
<title>Special issue on modeling pronunciation variation for automatic speech recognition.</title>
<date>1999</date>
<journal>Speech Communication,</journal>
<pages>29--2</pages>
<contexts>
<context position="2560" citStr="Bourlard et al., 1999" startWordPosition="386" endWordPosition="389">s, the effect is extremely general: In the phonetically transcribed portion of Switchboard, fewer than half of the word tokens are pronounced canonically (Fosler-Lussier, 1999). In addition, pronunciation variants sometimes include sounds not present in the dictionary at all, such as nasalized vowels (“can’t” —* [k ae n n t]) or fricatives introduced due to incomplete consonant closures (“legal” —* [l iy g fr ix l]).1 This variation makes pronunciation modeling one of the major challenges facing speech recognition (McAllaster et al., 1998; Jurafsky et al., 2001; Sarac¸lar and Khudanpur, 2004; Bourlard et al., 1999). 2 Most efforts to address the problem have involved either learning alternative pronunciations and/or their probabilities (Holter and Svendsen, 1999) or using phonetic transformation (substitution, insertion, and deletion) rules, which can come from linguistic knowledge or be learned from data (Riley et al., 1999; Hazen et al., 2005; Hutchinson and Droppo, 2011). These have produced some improvements in recognition performance. However, they also tend to cause additional confusability due to the introduction of additional homonyms (Fosler1We use the ARPAbet phonetic alphabet with additional </context>
</contexts>
<marker>Bourlard, Furui, Morgan, Strik, 1999</marker>
<rawString>H. Bourlard, S. Furui, N. Morgan, and H. Strik. 1999. Special issue on modeling pronunciation variation for automatic speech recognition. Speech Communication, 29(2-4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Browman</author>
<author>L Goldstein</author>
</authors>
<title>Articulatory phonology: an overview.</title>
<date>1992</date>
<journal>Phonetica,</journal>
<pages>49--3</pages>
<contexts>
<context position="21305" citStr="Browman and Goldstein, 1992" startWordPosition="3718" endWordPosition="3721">ce an alignment at the articulatory level, which is a sequence of 7-tuples, representing the articulatory variables3 lip opening, tongue tip location and opening, tongue body location and opening, velum opening, and glottis opening. We extract three kinds of features from the output—substitutions, asynchrony, and log-likelihood. The substitution features are similar to the phonetic alignment features in Section 4.3, except that the alignment is not a sequence of pairs but a sequence of 14-tuples (7 for the baseform and 7 for the surface form). The DBN model is based on articulatory phonology (Browman and Goldstein, 1992), in which there are no insertions and deletions, only substitutions (apparent insertions and deletions are accounted for by articulatory asynchrony). Formally, consider the seven sets of articulatory variable values F1, ... , F7. For example, F1 could be all of the values of lip opening, F1 ={closed, critical, narrow, wide}. Let T = {F1, ... , F7}. Consider an articulatory variable F E T. Suppose the alignment for F is (a1, b1), ... , (aL, bL), where L 3We use the term “articulatory variable” for the “articulatory features” of (Livescu and Glass, 2004; Jyothi et al., 2011), in order to avoid </context>
</contexts>
<marker>Browman, Goldstein, 1992</marker>
<rawString>C. P. Browman and L. Goldstein. 1992. Articulatory phonology: an overview. Phonetica, 49(3-4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>7</volume>
<contexts>
<context position="11310" citStr="Crammer et al., 2006" startWordPosition="1816" endWordPosition="1819"> φ(pi, w) . (1) Finding the weight vector θ that minimizes the `2-regularized average of this loss function is the structured support vector machine (SVM) problem (Taskar et al., 2003; Tsochantaridis et al., 2005): where λ is a user-defined tuning parameter that balances between regularization and loss minimization. In practice, we have found that solving the quadratic optimization problem given in Eq. (2) converges very slowly using standard methods such as stochastic gradient descent (Shalev-Shwartz et al., 2007). We use a slightly different algorithm, the Passive-Aggressive (PA) algorithm (Crammer et al., 2006), whose average loss is comparable to that of the structured SVM solution (Keshet et al., 2007). The Passive-Aggressive algorithm is an efficient online algorithm that, under some conditions, can be viewed as a dual-coordinate ascent minimizer of Eq. (2) (The connection to dual-coordinate ascent can be found in (Hsieh et al., 2008)). The algorithm begins by setting θ = 0 and proceeds in rounds. In the t-th round the algorithm picks an example (pi, wi) from S at random uniformly without replacement. Denote by θt−1 the value of the weight vector before the t-th round. Let wti denote the predicte</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive aggressive algorithms. Journal of Machine Learning Research, 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Filali</author>
<author>J Bilmes</author>
</authors>
<title>A dynamic Bayesian framework to model context and memory in edit distance learning: An application to pronunciation classification.</title>
<date>2005</date>
<booktitle>In Proc. Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4114" citStr="Filali and Bilmes, 2005" startWordPosition="613" endWordPosition="616">th Annual Meeting of the Association for Computational Linguistics, pages 194–203, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Lussier et al., 2002). Some other alternatives are articulatory pronunciation models, in which words are represented as multiple parallel sequences of articulatory features rather than single sequences of phones, and which outperform phone-based models on some tasks (Livescu and Glass, 2004; Jyothi et al., 2011); and models for learning edit distances between dictionary and actual pronunciations (Ristad and Yianilos, 1998; Filali and Bilmes, 2005). All of these approaches are generative—i.e., they provide distributions over possible pronunciations given the canonical one(s)—and they are typically trained by maximizing the likelihood over training data. In some recent work, discriminative approaches have been proposed, in which an objective more closely related to the task at hand is optimized. For example, (Vinyals et al., 2009; Korkmazskiy and Juang, 1997) optimize a minimum classification error (MCE) criterion to learn the weights (equivalently, probabilities) of alternative pronunciations for each word; (Schramm and Beyerlein, 2001)</context>
<context position="6517" citStr="Filali and Bilmes, 2005" startWordPosition="970" endWordPosition="973"> in the past been tested using a variety of measures. For generative models, phonetic error rate of generated pronunciations (Venkataramani and Byrne, 2001) and phone- or frame-level perplexity (Riley et al., 1999; Jyothi et al., 2011) are appropriate measures. For our discriminative models, we consider the task of lexical access; that is, prediction of a single word given its pronunciation in terms of sub-word units (Fissore et al., 1989; Jyothi et al., 2011). This task is also sometimes referred to as “pronunciation recognition” (Ristad and Yianilos, 1998) or “pronunciation classification” (Filali and Bilmes, 2005).) As we show below, our approach outperforms both traditional phonetic rule-based models and the best previously published results on our data set obtained with generative articulatory approaches. 2 Problem setting We define a pronunciation of a word as a representation of the way it is produced by a speaker in terms of some set of linguistically meaningful sub-word units. A pronunciation can be, for example, a sequence of phones or multiple sequences of articulatory features such as nasality, voicing, and tongue and lip positions. For purposes of this paper, we will assume that a pronunciati</context>
</contexts>
<marker>Filali, Bilmes, 2005</marker>
<rawString>K. Filali and J. Bilmes. 2005. A dynamic Bayesian framework to model context and memory in edit distance learning: An application to pronunciation classification. In Proc. Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Fissore</author>
<author>P Laface</author>
<author>G Micca</author>
<author>R Pieraccini</author>
</authors>
<title>Lexical access to large vocabularies for speech recognition.</title>
<date>1989</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<volume>37</volume>
<issue>8</issue>
<contexts>
<context position="6335" citStr="Fissore et al., 1989" startWordPosition="944" endWordPosition="947">der to focus attention on the pronunciation model alone, our experiments focus on a task that measures only the mapping between words and subword units. Pronunciation models have in the past been tested using a variety of measures. For generative models, phonetic error rate of generated pronunciations (Venkataramani and Byrne, 2001) and phone- or frame-level perplexity (Riley et al., 1999; Jyothi et al., 2011) are appropriate measures. For our discriminative models, we consider the task of lexical access; that is, prediction of a single word given its pronunciation in terms of sub-word units (Fissore et al., 1989; Jyothi et al., 2011). This task is also sometimes referred to as “pronunciation recognition” (Ristad and Yianilos, 1998) or “pronunciation classification” (Filali and Bilmes, 2005).) As we show below, our approach outperforms both traditional phonetic rule-based models and the best previously published results on our data set obtained with generative articulatory approaches. 2 Problem setting We define a pronunciation of a word as a representation of the way it is produced by a speaker in terms of some set of linguistically meaningful sub-word units. A pronunciation can be, for example, a se</context>
</contexts>
<marker>Fissore, Laface, Micca, Pieraccini, 1989</marker>
<rawString>L. Fissore, P. Laface, G. Micca, and R. Pieraccini. 1989. Lexical access to large vocabularies for speech recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing, 37(8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Fosler-Lussier</author>
<author>I Amdal</author>
<author>H-K J Kuo</author>
</authors>
<title>On the road to improved lexical confusability metrics.</title>
<date>2002</date>
<booktitle>In ISCA Tutorial and Research Workshop (ITRW) on Pronunciation Modeling and Lexicon Adaptation for Spoken Language Technology.</booktitle>
<marker>Fosler-Lussier, Amdal, Kuo, 2002</marker>
<rawString>E. Fosler-Lussier, I. Amdal, and H.-K. J. Kuo. 2002. On the road to improved lexical confusability metrics. In ISCA Tutorial and Research Workshop (ITRW) on Pronunciation Modeling and Lexicon Adaptation for Spoken Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Fosler-Lussier</author>
</authors>
<title>Dynamic Pronunciation Models forAutomatic Speech Recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<contexts>
<context position="2114" citStr="Fosler-Lussier, 1999" startWordPosition="315" endWordPosition="316">ouncing dictionaries provide each word’s canonical pronunciation(s) in terms of phoneme strings, running speech often includes pronunciations that differ greatly from the dictionary. For example, some pronunciations of “probably” in the Switchboard conversational speech database are [p r aa b iy], [p r aa l iy], [p r ay], and [p ow ih] (Greenberg et al., 1996). While some words (e.g., common words) are more prone to such variation than others, the effect is extremely general: In the phonetically transcribed portion of Switchboard, fewer than half of the word tokens are pronounced canonically (Fosler-Lussier, 1999). In addition, pronunciation variants sometimes include sounds not present in the dictionary at all, such as nasalized vowels (“can’t” —* [k ae n n t]) or fricatives introduced due to incomplete consonant closures (“legal” —* [l iy g fr ix l]).1 This variation makes pronunciation modeling one of the major challenges facing speech recognition (McAllaster et al., 1998; Jurafsky et al., 2001; Sarac¸lar and Khudanpur, 2004; Bourlard et al., 1999). 2 Most efforts to address the problem have involved either learning alternative pronunciations and/or their probabilities (Holter and Svendsen, 1999) or</context>
</contexts>
<marker>Fosler-Lussier, 1999</marker>
<rawString>J. E. Fosler-Lussier. 1999. Dynamic Pronunciation Models forAutomatic Speech Recognition. Ph.D. thesis, U. C. Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Greenberg</author>
<author>J Hollenback</author>
<author>D Ellis</author>
</authors>
<title>Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus.</title>
<date>1996</date>
<booktitle>In Proc. International Conference on Spoken Language Processing (ICSLP).</booktitle>
<contexts>
<context position="1855" citStr="Greenberg et al., 1996" startWordPosition="275" endWordPosition="278">gasos algorithm. 1 Introduction One of the problems faced by automatic speech recognition, especially of conversational speech, is that of modeling the mapping between words and their possible pronunciations in terms of sub-word units such as phones. While pronouncing dictionaries provide each word’s canonical pronunciation(s) in terms of phoneme strings, running speech often includes pronunciations that differ greatly from the dictionary. For example, some pronunciations of “probably” in the Switchboard conversational speech database are [p r aa b iy], [p r aa l iy], [p r ay], and [p ow ih] (Greenberg et al., 1996). While some words (e.g., common words) are more prone to such variation than others, the effect is extremely general: In the phonetically transcribed portion of Switchboard, fewer than half of the word tokens are pronounced canonically (Fosler-Lussier, 1999). In addition, pronunciation variants sometimes include sounds not present in the dictionary at all, such as nasalized vowels (“can’t” —* [k ae n n t]) or fricatives introduced due to incomplete consonant closures (“legal” —* [l iy g fr ix l]).1 This variation makes pronunciation modeling one of the major challenges facing speech recogniti</context>
<context position="23688" citStr="Greenberg et al., 1996" startWordPosition="4152" endWordPosition="4155">,F2)≤b = ✶a≤async(F1,F2)≤b. Finally, the log-likelihood feature is the DBN alignment score, shifted and scaled so that the value lies between zero and one, φdbn-LL(p, w) = L(p, w) − h c , where L is the log-likelihood function of the DBN, h is the shift, and c is the scale. Note that none of the DBN features are wordspecific, so that they generalize to words in the dictionary that are unseen in the training set. 5 Experiments All experiments are conducted on a subset of the Switchboard conversational speech corpus that has ✶ai=a, bi=b. ⎡ ⎣ 1 L XL i=1 199 been labeled at a fine phonetic level (Greenberg et al., 1996); these phonetic transcriptions are the input to our lexical access models. The data subset, phone set P, and dictionary are the same as ones previously used in (Livescu and Glass, 2004; Jyothi et al., 2011). The dictionary contains 3328 words, consisting of the 5000 most frequent words in Switchboard, excluding ones with fewer than four phones in their baseforms. The baseforms use a similar, slightly smaller phone set (lacking, e.g., nasalization). We measure performance by error rate (ER), the proportion of test examples predicted incorrectly. The TF-IDF features used in the experiments are </context>
</contexts>
<marker>Greenberg, Hollenback, Ellis, 1996</marker>
<rawString>S. Greenberg, J. Hollenback, and D. Ellis. 1996. Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus. In Proc. International Conference on Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gunawardana</author>
<author>M Mahajan</author>
<author>A Acero</author>
<author>J Platt</author>
</authors>
<title>Hidden conditional random fields for phone classification. In</title>
<date>2005</date>
<booktitle>Proc. Interspeech.</booktitle>
<contexts>
<context position="30842" citStr="Gunawardana et al., 2005" startWordPosition="5358" endWordPosition="5362"> is labeled DP. The dots and lines are as defined in Figure 1. extension of the model and learning algorithm to word sequences and (2) feature functions that relate acoustic measurements to sub-word units. The extension to sequences can be done analogously to segmental conditional random fields (SCRFs). The main difference between SCRFs and our approach would be the large-margin learning, which can be straightforwardly applied to sequences. To incorporate acoustics, we can use feature functions based on classifiers of sub-word units, similarly to previous work on CRF-based speech recognition (Gunawardana et al., 2005; Morris and Fosler-Lussier, 2008; Prabhavalkar et al., 2011). Richer, longer-span (e.g., word-level) feature functions are also possible. Thus far we have restricted the pronunciation-toword score to linear combinations of feature functions. This can be extended to non-linear combinations using a kernel. This may be challenging in a high-dimensional feature space. One possibility is to approximate the kernels as in (Keshet et al., 2011). Additional extensions include new feature functions, such as context-sensitive alignment features, and joint inference and learning of the alignment models e</context>
</contexts>
<marker>Gunawardana, Mahajan, Acero, Platt, 2005</marker>
<rawString>A. Gunawardana, M. Mahajan, A. Acero, and J. Platt. 2005. Hidden conditional random fields for phone classification. In Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T J Hazen</author>
<author>I L Hetherington</author>
<author>H Shu</author>
<author>K Livescu</author>
</authors>
<title>Pronunciation modeling using a finite-state transducer representation.</title>
<date>2005</date>
<journal>Speech Communication,</journal>
<volume>46</volume>
<issue>2</issue>
<contexts>
<context position="2896" citStr="Hazen et al., 2005" startWordPosition="436" endWordPosition="439">s introduced due to incomplete consonant closures (“legal” —* [l iy g fr ix l]).1 This variation makes pronunciation modeling one of the major challenges facing speech recognition (McAllaster et al., 1998; Jurafsky et al., 2001; Sarac¸lar and Khudanpur, 2004; Bourlard et al., 1999). 2 Most efforts to address the problem have involved either learning alternative pronunciations and/or their probabilities (Holter and Svendsen, 1999) or using phonetic transformation (substitution, insertion, and deletion) rules, which can come from linguistic knowledge or be learned from data (Riley et al., 1999; Hazen et al., 2005; Hutchinson and Droppo, 2011). These have produced some improvements in recognition performance. However, they also tend to cause additional confusability due to the introduction of additional homonyms (Fosler1We use the ARPAbet phonetic alphabet with additional diacritics, such as [ n] for nasalization and [ fr] for frication. 2This problem is separate from the grapheme-to-phoneme problem, in which pronunciations are predicted from a word’s spelling; here, we assume the availability of a dictionary of canonical pronunciations as is usual in speech recognition. 194 Proceedings of the 50th Ann</context>
</contexts>
<marker>Hazen, Hetherington, Shu, Livescu, 2005</marker>
<rawString>T. J. Hazen, I. L. Hetherington, H. Shu, and K. Livescu. 2005. Pronunciation modeling using a finite-state transducer representation. Speech Communication, 46(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Holter</author>
<author>T Svendsen</author>
</authors>
<title>Maximum likelihood modelling of pronunciation variation. Speech Communication.</title>
<date>1999</date>
<contexts>
<context position="2711" citStr="Holter and Svendsen, 1999" startWordPosition="406" endWordPosition="409">ically (Fosler-Lussier, 1999). In addition, pronunciation variants sometimes include sounds not present in the dictionary at all, such as nasalized vowels (“can’t” —* [k ae n n t]) or fricatives introduced due to incomplete consonant closures (“legal” —* [l iy g fr ix l]).1 This variation makes pronunciation modeling one of the major challenges facing speech recognition (McAllaster et al., 1998; Jurafsky et al., 2001; Sarac¸lar and Khudanpur, 2004; Bourlard et al., 1999). 2 Most efforts to address the problem have involved either learning alternative pronunciations and/or their probabilities (Holter and Svendsen, 1999) or using phonetic transformation (substitution, insertion, and deletion) rules, which can come from linguistic knowledge or be learned from data (Riley et al., 1999; Hazen et al., 2005; Hutchinson and Droppo, 2011). These have produced some improvements in recognition performance. However, they also tend to cause additional confusability due to the introduction of additional homonyms (Fosler1We use the ARPAbet phonetic alphabet with additional diacritics, such as [ n] for nasalization and [ fr] for frication. 2This problem is separate from the grapheme-to-phoneme problem, in which pronunciati</context>
</contexts>
<marker>Holter, Svendsen, 1999</marker>
<rawString>T. Holter and T. Svendsen. 1999. Maximum likelihood modelling of pronunciation variation. Speech Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-J Hsieh</author>
<author>K-W Chang</author>
<author>C-J Lin</author>
<author>S S Keerthi</author>
<author>S Sundararajan</author>
</authors>
<title>A dual coordinate descent method for large-scale linear SVM.</title>
<date>2008</date>
<booktitle>In Proc. International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="11643" citStr="Hsieh et al., 2008" startWordPosition="1868" endWordPosition="1871">ave found that solving the quadratic optimization problem given in Eq. (2) converges very slowly using standard methods such as stochastic gradient descent (Shalev-Shwartz et al., 2007). We use a slightly different algorithm, the Passive-Aggressive (PA) algorithm (Crammer et al., 2006), whose average loss is comparable to that of the structured SVM solution (Keshet et al., 2007). The Passive-Aggressive algorithm is an efficient online algorithm that, under some conditions, can be viewed as a dual-coordinate ascent minimizer of Eq. (2) (The connection to dual-coordinate ascent can be found in (Hsieh et al., 2008)). The algorithm begins by setting θ = 0 and proceeds in rounds. In the t-th round the algorithm picks an example (pi, wi) from S at random uniformly without replacement. Denote by θt−1 the value of the weight vector before the t-th round. Let wti denote the predicted word for the i-th example according to θt−1: θ* = argmin m L(θ, pi, wi), (2) θ 211θ112 + m E i=1 f(p) = argmax θ · φ(p, w). wti = argmax θt−1 · φ(pi, w) + nwi#w. wEV wEV Our goal in learning θ is to minimize the expected zero-one loss: θ* = argmin [nw�=f(p) � , E(p,w)_ρ θ Let Aφt i = φ(pi, wi) − φ(pi, wti). Then the algorithm upd</context>
</contexts>
<marker>Hsieh, Chang, Lin, Keerthi, Sundararajan, 2008</marker>
<rawString>C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and S. Sundararajan. 2008. A dual coordinate descent method for large-scale linear SVM. In Proc. International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hutchinson</author>
<author>J Droppo</author>
</authors>
<title>Learning nonparametric models of pronunciation.</title>
<date>2011</date>
<booktitle>In Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="2926" citStr="Hutchinson and Droppo, 2011" startWordPosition="440" endWordPosition="443">incomplete consonant closures (“legal” —* [l iy g fr ix l]).1 This variation makes pronunciation modeling one of the major challenges facing speech recognition (McAllaster et al., 1998; Jurafsky et al., 2001; Sarac¸lar and Khudanpur, 2004; Bourlard et al., 1999). 2 Most efforts to address the problem have involved either learning alternative pronunciations and/or their probabilities (Holter and Svendsen, 1999) or using phonetic transformation (substitution, insertion, and deletion) rules, which can come from linguistic knowledge or be learned from data (Riley et al., 1999; Hazen et al., 2005; Hutchinson and Droppo, 2011). These have produced some improvements in recognition performance. However, they also tend to cause additional confusability due to the introduction of additional homonyms (Fosler1We use the ARPAbet phonetic alphabet with additional diacritics, such as [ n] for nasalization and [ fr] for frication. 2This problem is separate from the grapheme-to-phoneme problem, in which pronunciations are predicted from a word’s spelling; here, we assume the availability of a dictionary of canonical pronunciations as is usual in speech recognition. 194 Proceedings of the 50th Annual Meeting of the Association</context>
</contexts>
<marker>Hutchinson, Droppo, 2011</marker>
<rawString>B. Hutchinson and J. Droppo. 2011. Learning nonparametric models of pronunciation. In Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>W Ward</author>
<author>Z Jianping</author>
<author>K Herold</author>
<author>Y Xiuyang</author>
<author>Z Sen</author>
</authors>
<title>What kind of pronunciation variation is hard for triphones to model?</title>
<date>2001</date>
<booktitle>In Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="2505" citStr="Jurafsky et al., 2001" startWordPosition="377" endWordPosition="380">mon words) are more prone to such variation than others, the effect is extremely general: In the phonetically transcribed portion of Switchboard, fewer than half of the word tokens are pronounced canonically (Fosler-Lussier, 1999). In addition, pronunciation variants sometimes include sounds not present in the dictionary at all, such as nasalized vowels (“can’t” —* [k ae n n t]) or fricatives introduced due to incomplete consonant closures (“legal” —* [l iy g fr ix l]).1 This variation makes pronunciation modeling one of the major challenges facing speech recognition (McAllaster et al., 1998; Jurafsky et al., 2001; Sarac¸lar and Khudanpur, 2004; Bourlard et al., 1999). 2 Most efforts to address the problem have involved either learning alternative pronunciations and/or their probabilities (Holter and Svendsen, 1999) or using phonetic transformation (substitution, insertion, and deletion) rules, which can come from linguistic knowledge or be learned from data (Riley et al., 1999; Hazen et al., 2005; Hutchinson and Droppo, 2011). These have produced some improvements in recognition performance. However, they also tend to cause additional confusability due to the introduction of additional homonyms (Fosle</context>
</contexts>
<marker>Jurafsky, Ward, Jianping, Herold, Xiuyang, Sen, 2001</marker>
<rawString>D. Jurafsky, W. Ward, Z. Jianping, K. Herold, Y. Xiuyang, and Z. Sen. 2001. What kind of pronunciation variation is hard for triphones to model? In Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jyothi</author>
<author>K Livescu</author>
<author>E Fosler-Lussier</author>
</authors>
<title>Lexical access experiments with context-dependent articulatory feature-based models.</title>
<date>2011</date>
<booktitle>In Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="3976" citStr="Jyothi et al., 2011" startWordPosition="593" endWordPosition="596">e, we assume the availability of a dictionary of canonical pronunciations as is usual in speech recognition. 194 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 194–203, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Lussier et al., 2002). Some other alternatives are articulatory pronunciation models, in which words are represented as multiple parallel sequences of articulatory features rather than single sequences of phones, and which outperform phone-based models on some tasks (Livescu and Glass, 2004; Jyothi et al., 2011); and models for learning edit distances between dictionary and actual pronunciations (Ristad and Yianilos, 1998; Filali and Bilmes, 2005). All of these approaches are generative—i.e., they provide distributions over possible pronunciations given the canonical one(s)—and they are typically trained by maximizing the likelihood over training data. In some recent work, discriminative approaches have been proposed, in which an objective more closely related to the task at hand is optimized. For example, (Vinyals et al., 2009; Korkmazskiy and Juang, 1997) optimize a minimum classification error (MC</context>
<context position="6128" citStr="Jyothi et al., 2011" startWordPosition="911" endWordPosition="914">to sparser, faster, and better-performing models than conditional random field optimization in our experiments; and we use a large set of different feature functions tailored to pronunciation modeling. In order to focus attention on the pronunciation model alone, our experiments focus on a task that measures only the mapping between words and subword units. Pronunciation models have in the past been tested using a variety of measures. For generative models, phonetic error rate of generated pronunciations (Venkataramani and Byrne, 2001) and phone- or frame-level perplexity (Riley et al., 1999; Jyothi et al., 2011) are appropriate measures. For our discriminative models, we consider the task of lexical access; that is, prediction of a single word given its pronunciation in terms of sub-word units (Fissore et al., 1989; Jyothi et al., 2011). This task is also sometimes referred to as “pronunciation recognition” (Ristad and Yianilos, 1998) or “pronunciation classification” (Filali and Bilmes, 2005).) As we show below, our approach outperforms both traditional phonetic rule-based models and the best previously published results on our data set obtained with generative articulatory approaches. 2 Problem set</context>
<context position="20440" citStr="Jyothi et al., 2011" startWordPosition="3581" endWordPosition="3584">The dictionary feature is an indicator of whether a pronunciation is an exact match to a baseform, which also generalizes to words unseen in training. We define the dictionary feature as φdict(p,w) = ✶p∈pron(w). For example, assume there is a baseform /pcl p r aa bcl b l iy/ for the word “probably” in the dictionary, and p = /pcl p r aa bcl b l iy/. Then φdict (p, probably) = 1, while φdict(p, problem) = 0. 4.5 Articulatory feature functions Articulatory models represented as dynamic Bayesian networks (DBNs) have been successful in the past on the lexical access task (Livescu and Glass, 2004; Jyothi et al., 2011). In such models, pronunciation variation is seen as the result of asynchrony between the articulators (lips, tongue, etc.) and deviations from the intended articulatory positions. Given a sequence p and a word w, we use the DBN to produce an alignment at the articulatory level, which is a sequence of 7-tuples, representing the articulatory variables3 lip opening, tongue tip location and opening, tongue body location and opening, velum opening, and glottis opening. We extract three kinds of features from the output—substitutions, asynchrony, and log-likelihood. The substitution features are si</context>
<context position="21885" citStr="Jyothi et al., 2011" startWordPosition="3819" endWordPosition="3822"> phonology (Browman and Goldstein, 1992), in which there are no insertions and deletions, only substitutions (apparent insertions and deletions are accounted for by articulatory asynchrony). Formally, consider the seven sets of articulatory variable values F1, ... , F7. For example, F1 could be all of the values of lip opening, F1 ={closed, critical, narrow, wide}. Let T = {F1, ... , F7}. Consider an articulatory variable F E T. Suppose the alignment for F is (a1, b1), ... , (aL, bL), where L 3We use the term “articulatory variable” for the “articulatory features” of (Livescu and Glass, 2004; Jyothi et al., 2011), in order to avoid confusion with our feature functions. is the length of the alignment and ai, bi E F, for 1 &lt; i &lt; L. Here the ai are the intended articulatory variable values according to the baseform, and the bi are the corresponding realized values. For each a, b E F we define a substitution feature function: XL 1 φa→b(p, w) = L i=1 The asynchrony features are also extracted from the DBN alignments. Articulators are not always synchronized, which is one cause of pronunciation variation. We measure this by looking at the phones that two articulators are aiming to produce, and find the time</context>
<context position="23895" citStr="Jyothi et al., 2011" startWordPosition="4188" endWordPosition="4191">elihood function of the DBN, h is the shift, and c is the scale. Note that none of the DBN features are wordspecific, so that they generalize to words in the dictionary that are unseen in the training set. 5 Experiments All experiments are conducted on a subset of the Switchboard conversational speech corpus that has ✶ai=a, bi=b. ⎡ ⎣ 1 L XL i=1 199 been labeled at a fine phonetic level (Greenberg et al., 1996); these phonetic transcriptions are the input to our lexical access models. The data subset, phone set P, and dictionary are the same as ones previously used in (Livescu and Glass, 2004; Jyothi et al., 2011). The dictionary contains 3328 words, consisting of the 5000 most frequent words in Switchboard, excluding ones with fewer than four phones in their baseforms. The baseforms use a similar, slightly smaller phone set (lacking, e.g., nasalization). We measure performance by error rate (ER), the proportion of test examples predicted incorrectly. The TF-IDF features used in the experiments are based on phone bigrams. For all of the articulatory DBN features, we use the DBN from (Livescu, 2005) (the one in (Jyothi et al., 2011) is more sophisticated and may be used in future work). For the asynchro</context>
<context position="25202" citStr="Jyothi et al., 2011" startWordPosition="4401" endWordPosition="4404">p opening}, {tongue tip, tongue body}), and ({lip opening, tongue tip, tongue body}, {glottis, velum})}, as in (Livescu, 2005). The parameters (a, b) of the length and asynchrony features are drawn from (a, b) ∈ {(−3,−2), (−2,−1),... (2,3)}. We compare the CRF4, Passive-Aggressive (PA), and Pegasos learning algorithms. The regularization parameter A is tuned on the development set. We run all three algorithms for multiple epochs and pick the best epoch based on development set performance. For the first set of experiments, we use the same division of the corpus as in (Livescu and Glass, 2004; Jyothi et al., 2011) into a 2492-word training set, a 165-word development set, and a 236- word test set. To give a sense of the difficulty of the task, we test two simple baselines. One is a lexicon lookup: If the surface form is found in the dictionary, predict the corresponding word; otherwise, guess randomly. For a second baseline, we calculate the Levenshtein (0-1 edit) distance between the input pronunciation and each dictionary baseform, and predict the word corresponding to the baseform closest to the input. The results are shown in the first two rows of Table 2. We can see that, by adding just the Levens</context>
<context position="26915" citStr="Jyothi et al., 2011" startWordPosition="4693" endWordPosition="4696">4; Jyothi et al., 2011). Models labeled X/Y use learning algorithm X and feature set Y. The feature set DP+ contains TF-IDF, DP alignment, dictionary, and length features. The set ALL contains DP+ and the articulatory DBN features. The best results are in bold; the differences among them are insignificant (according to McNemar’s test with p = .05). icantly. However, both baselines do quite poorly. Table 2 shows the best previous result on this data set from the articulatory model of Jyothi et al., which greatly improves over our baselines as well as over a much more complex phone-based model (Jyothi et al., 2011). The remaining rows of Table 2 give results with our feature functions and various learning algorithms. The best result for PA/DP+ (the PA algorithm using all features besides the DBN features) on the development set is with A = 100 and 5 epochs. Tested on the test set, this model improves over (Jyothi et al., 2011) by 13.9% absolute (47.8% relative). The best result for Pegasos with the same features on the development set is with A = 0.01 and 10 epochs. On the test set, this model gives a 14.3% absolute improvement (49.1% relative). CRF learning with the same features performs about 6% wors</context>
</contexts>
<marker>Jyothi, Livescu, Fosler-Lussier, 2011</marker>
<rawString>P. Jyothi, K. Livescu, and E. Fosler-Lussier. 2011. Lexical access experiments with context-dependent articulatory feature-based models. In Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
<author>D Chazan</author>
</authors>
<title>A large margin algorithm for speech and audio segmentation.</title>
<date>2007</date>
<journal>IEEE Transactions on Acoustics, Speech, and Language Processing,</journal>
<volume>15</volume>
<issue>8</issue>
<contexts>
<context position="11405" citStr="Keshet et al., 2007" startWordPosition="1832" endWordPosition="1835">ss function is the structured support vector machine (SVM) problem (Taskar et al., 2003; Tsochantaridis et al., 2005): where λ is a user-defined tuning parameter that balances between regularization and loss minimization. In practice, we have found that solving the quadratic optimization problem given in Eq. (2) converges very slowly using standard methods such as stochastic gradient descent (Shalev-Shwartz et al., 2007). We use a slightly different algorithm, the Passive-Aggressive (PA) algorithm (Crammer et al., 2006), whose average loss is comparable to that of the structured SVM solution (Keshet et al., 2007). The Passive-Aggressive algorithm is an efficient online algorithm that, under some conditions, can be viewed as a dual-coordinate ascent minimizer of Eq. (2) (The connection to dual-coordinate ascent can be found in (Hsieh et al., 2008)). The algorithm begins by setting θ = 0 and proceeds in rounds. In the t-th round the algorithm picks an example (pi, wi) from S at random uniformly without replacement. Denote by θt−1 the value of the weight vector before the t-th round. Let wti denote the predicted word for the i-th example according to θt−1: θ* = argmin m L(θ, pi, wi), (2) θ 211θ112 + m E </context>
</contexts>
<marker>Keshet, Shalev-Shwartz, Singer, Chazan, 2007</marker>
<rawString>J. Keshet, S. Shalev-Shwartz, Y. Singer, and D. Chazan. 2007. A large margin algorithm for speech and audio segmentation. IEEE Transactions on Acoustics, Speech, and Language Processing, 15(8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Keshet</author>
<author>D McAllester</author>
<author>T Hazan</author>
</authors>
<title>PACBayesian approach for minimization of phoneme error rate.</title>
<date>2011</date>
<booktitle>In Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<marker>Keshet, McAllester, Hazan, 2011</marker>
<rawString>J. Keshet, D. McAllester, and T. Hazan. 2011. PACBayesian approach for minimization of phoneme error rate. In Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Korkmazskiy</author>
<author>B-H Juang</author>
</authors>
<title>Discriminative training of the pronunciation networks.</title>
<date>1997</date>
<booktitle>In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</booktitle>
<contexts>
<context position="4532" citStr="Korkmazskiy and Juang, 1997" startWordPosition="675" endWordPosition="678">ased models on some tasks (Livescu and Glass, 2004; Jyothi et al., 2011); and models for learning edit distances between dictionary and actual pronunciations (Ristad and Yianilos, 1998; Filali and Bilmes, 2005). All of these approaches are generative—i.e., they provide distributions over possible pronunciations given the canonical one(s)—and they are typically trained by maximizing the likelihood over training data. In some recent work, discriminative approaches have been proposed, in which an objective more closely related to the task at hand is optimized. For example, (Vinyals et al., 2009; Korkmazskiy and Juang, 1997) optimize a minimum classification error (MCE) criterion to learn the weights (equivalently, probabilities) of alternative pronunciations for each word; (Schramm and Beyerlein, 2001) use a similar approach with discriminative model combination. In this work, the weighted alternatives are then used in a standard (generative) speech recognizer. In other words, these approaches optimize generative models using discriminative criteria. We propose a general, flexible discriminative approach to pronunciation modeling, rather than discriminatively optimizing a generative model. We formulate a linear </context>
</contexts>
<marker>Korkmazskiy, Juang, 1997</marker>
<rawString>F. Korkmazskiy and B.-H. Juang. 1997. Discriminative training of the pronunciation networks. In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="12577" citStr="Lafferty et al., 2001" startWordPosition="2060" endWordPosition="2063">: θ* = argmin m L(θ, pi, wi), (2) θ 211θ112 + m E i=1 f(p) = argmax θ · φ(p, w). wti = argmax θt−1 · φ(pi, w) + nwi#w. wEV wEV Our goal in learning θ is to minimize the expected zero-one loss: θ* = argmin [nw�=f(p) � , E(p,w)_ρ θ Let Aφt i = φ(pi, wi) − φ(pi, wti). Then the algorithm updates the weight vector θt as follows: θt = θt−1 + αt iAφt (3) i 196 where � � 1 ✶wi�= ˆwt i − � · Δ�t i αt i =min λm, kΔ(0tik L(8, pi, wi) (4) where the probability is defined as �wEV Minimization of Eq. (2) under the log-loss results in a probabilistic model commonly known as a conditional random field (CRF) (Lafferty et al., 2001). By taking the sub-gradient of Eq. (4), we can obtain an update rule similar to the one shown in Eq. (3). 4 Feature functions Before defining the feature functions, we define some notation. Suppose p P* is a sequence of sub-word units. We use to denote the n-gram substring ... pn. The two substrings a and b are said to be equal if they have the same length and ai = bi for 1 i n. For a given sub-word unit nPn, we use the shorthand u p to mean that we can find u in p; i.e., there exists an index i such that = u. We use to denote the length of the sequence p. We assume we have a pronunciation di</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In Proc. International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Livescu</author>
<author>J Glass</author>
</authors>
<title>Feature-based pronunciation modeling with trainable asynchrony probabilities.</title>
<date>2004</date>
<booktitle>In Proc. International Conference on Spoken Language Processing (ICSLP).</booktitle>
<contexts>
<context position="3954" citStr="Livescu and Glass, 2004" startWordPosition="589" endWordPosition="592">om a word’s spelling; here, we assume the availability of a dictionary of canonical pronunciations as is usual in speech recognition. 194 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 194–203, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Lussier et al., 2002). Some other alternatives are articulatory pronunciation models, in which words are represented as multiple parallel sequences of articulatory features rather than single sequences of phones, and which outperform phone-based models on some tasks (Livescu and Glass, 2004; Jyothi et al., 2011); and models for learning edit distances between dictionary and actual pronunciations (Ristad and Yianilos, 1998; Filali and Bilmes, 2005). All of these approaches are generative—i.e., they provide distributions over possible pronunciations given the canonical one(s)—and they are typically trained by maximizing the likelihood over training data. In some recent work, discriminative approaches have been proposed, in which an objective more closely related to the task at hand is optimized. For example, (Vinyals et al., 2009; Korkmazskiy and Juang, 1997) optimize a minimum cl</context>
<context position="20418" citStr="Livescu and Glass, 2004" startWordPosition="3577" endWordPosition="3580">tionary feature function The dictionary feature is an indicator of whether a pronunciation is an exact match to a baseform, which also generalizes to words unseen in training. We define the dictionary feature as φdict(p,w) = ✶p∈pron(w). For example, assume there is a baseform /pcl p r aa bcl b l iy/ for the word “probably” in the dictionary, and p = /pcl p r aa bcl b l iy/. Then φdict (p, probably) = 1, while φdict(p, problem) = 0. 4.5 Articulatory feature functions Articulatory models represented as dynamic Bayesian networks (DBNs) have been successful in the past on the lexical access task (Livescu and Glass, 2004; Jyothi et al., 2011). In such models, pronunciation variation is seen as the result of asynchrony between the articulators (lips, tongue, etc.) and deviations from the intended articulatory positions. Given a sequence p and a word w, we use the DBN to produce an alignment at the articulatory level, which is a sequence of 7-tuples, representing the articulatory variables3 lip opening, tongue tip location and opening, tongue body location and opening, velum opening, and glottis opening. We extract three kinds of features from the output—substitutions, asynchrony, and log-likelihood. The substi</context>
<context position="21863" citStr="Livescu and Glass, 2004" startWordPosition="3815" endWordPosition="3818"> is based on articulatory phonology (Browman and Goldstein, 1992), in which there are no insertions and deletions, only substitutions (apparent insertions and deletions are accounted for by articulatory asynchrony). Formally, consider the seven sets of articulatory variable values F1, ... , F7. For example, F1 could be all of the values of lip opening, F1 ={closed, critical, narrow, wide}. Let T = {F1, ... , F7}. Consider an articulatory variable F E T. Suppose the alignment for F is (a1, b1), ... , (aL, bL), where L 3We use the term “articulatory variable” for the “articulatory features” of (Livescu and Glass, 2004; Jyothi et al., 2011), in order to avoid confusion with our feature functions. is the length of the alignment and ai, bi E F, for 1 &lt; i &lt; L. Here the ai are the intended articulatory variable values according to the baseform, and the bi are the corresponding realized values. For each a, b E F we define a substitution feature function: XL 1 φa→b(p, w) = L i=1 The asynchrony features are also extracted from the DBN alignments. Articulators are not always synchronized, which is one cause of pronunciation variation. We measure this by looking at the phones that two articulators are aiming to prod</context>
<context position="23873" citStr="Livescu and Glass, 2004" startWordPosition="4184" endWordPosition="4187"> , where L is the log-likelihood function of the DBN, h is the shift, and c is the scale. Note that none of the DBN features are wordspecific, so that they generalize to words in the dictionary that are unseen in the training set. 5 Experiments All experiments are conducted on a subset of the Switchboard conversational speech corpus that has ✶ai=a, bi=b. ⎡ ⎣ 1 L XL i=1 199 been labeled at a fine phonetic level (Greenberg et al., 1996); these phonetic transcriptions are the input to our lexical access models. The data subset, phone set P, and dictionary are the same as ones previously used in (Livescu and Glass, 2004; Jyothi et al., 2011). The dictionary contains 3328 words, consisting of the 5000 most frequent words in Switchboard, excluding ones with fewer than four phones in their baseforms. The baseforms use a similar, slightly smaller phone set (lacking, e.g., nasalization). We measure performance by error rate (ER), the proportion of test examples predicted incorrectly. The TF-IDF features used in the experiments are based on phone bigrams. For all of the articulatory DBN features, we use the DBN from (Livescu, 2005) (the one in (Jyothi et al., 2011) is more sophisticated and may be used in future w</context>
<context position="25180" citStr="Livescu and Glass, 2004" startWordPosition="4397" endWordPosition="4400">ip}, {tongue body}), ({lip opening}, {tongue tip, tongue body}), and ({lip opening, tongue tip, tongue body}, {glottis, velum})}, as in (Livescu, 2005). The parameters (a, b) of the length and asynchrony features are drawn from (a, b) ∈ {(−3,−2), (−2,−1),... (2,3)}. We compare the CRF4, Passive-Aggressive (PA), and Pegasos learning algorithms. The regularization parameter A is tuned on the development set. We run all three algorithms for multiple epochs and pick the best epoch based on development set performance. For the first set of experiments, we use the same division of the corpus as in (Livescu and Glass, 2004; Jyothi et al., 2011) into a 2492-word training set, a 165-word development set, and a 236- word test set. To give a sense of the difficulty of the task, we test two simple baselines. One is a lexicon lookup: If the surface form is found in the dictionary, predict the corresponding word; otherwise, guess randomly. For a second baseline, we calculate the Levenshtein (0-1 edit) distance between the input pronunciation and each dictionary baseform, and predict the word corresponding to the baseform closest to the input. The results are shown in the first two rows of Table 2. We can see that, by </context>
</contexts>
<marker>Livescu, Glass, 2004</marker>
<rawString>K. Livescu and J. Glass. 2004. Feature-based pronunciation modeling with trainable asynchrony probabilities. In Proc. International Conference on Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Livescu</author>
</authors>
<title>Feature-based Pronunciation Modeling for Automatic Speech Recognition.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="24389" citStr="Livescu, 2005" startWordPosition="4269" endWordPosition="4270">bset, phone set P, and dictionary are the same as ones previously used in (Livescu and Glass, 2004; Jyothi et al., 2011). The dictionary contains 3328 words, consisting of the 5000 most frequent words in Switchboard, excluding ones with fewer than four phones in their baseforms. The baseforms use a similar, slightly smaller phone set (lacking, e.g., nasalization). We measure performance by error rate (ER), the proportion of test examples predicted incorrectly. The TF-IDF features used in the experiments are based on phone bigrams. For all of the articulatory DBN features, we use the DBN from (Livescu, 2005) (the one in (Jyothi et al., 2011) is more sophisticated and may be used in future work). For the asynchrony features, the articulatory pairs are (F1, F2) ∈ {({tongue tip}, {tongue body}), ({lip opening}, {tongue tip, tongue body}), and ({lip opening, tongue tip, tongue body}, {glottis, velum})}, as in (Livescu, 2005). The parameters (a, b) of the length and asynchrony features are drawn from (a, b) ∈ {(−3,−2), (−2,−1),... (2,3)}. We compare the CRF4, Passive-Aggressive (PA), and Pegasos learning algorithms. The regularization parameter A is tuned on the development set. We run all three algor</context>
<context position="26071" citStr="Livescu, 2005" startWordPosition="4552" endWordPosition="4553">onding word; otherwise, guess randomly. For a second baseline, we calculate the Levenshtein (0-1 edit) distance between the input pronunciation and each dictionary baseform, and predict the word corresponding to the baseform closest to the input. The results are shown in the first two rows of Table 2. We can see that, by adding just the Levenshtein distance, the error rate drops signif4We use the term “CRF” since the learning algorithm corresponds to CRF learning, although the task is multiclass classification rather than a sequence or structure prediction task. Model ER lexicon lookup (from (Livescu, 2005)) 59.3% lexicon + Levenshtein distance 41.8% (Jyothi et al., 2011) 29.1% CRF/DP+ 21.5% PA/DP+ 15.2% Pegasos/DP+ 14.8% PA/ALL 15.2% Table 2: Lexical access error rates (ER) on the same data split as in (Livescu and Glass, 2004; Jyothi et al., 2011). Models labeled X/Y use learning algorithm X and feature set Y. The feature set DP+ contains TF-IDF, DP alignment, dictionary, and length features. The set ALL contains DP+ and the articulatory DBN features. The best results are in bold; the differences among them are insignificant (according to McNemar’s test with p = .05). icantly. However, both ba</context>
</contexts>
<marker>Livescu, 2005</marker>
<rawString>K. Livescu. 2005. Feature-based Pronunciation Modeling for Automatic Speech Recognition. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McAllaster</author>
<author>L Gillick</author>
<author>F Scattone</author>
<author>M Newman</author>
</authors>
<title>Fabricating conversational speech data with acoustic models : A program to examine model-data mismatch.</title>
<date>1998</date>
<booktitle>In Proc. International Conference on Spoken Language Processing (ICSLP).</booktitle>
<contexts>
<context position="2482" citStr="McAllaster et al., 1998" startWordPosition="373" endWordPosition="376">ile some words (e.g., common words) are more prone to such variation than others, the effect is extremely general: In the phonetically transcribed portion of Switchboard, fewer than half of the word tokens are pronounced canonically (Fosler-Lussier, 1999). In addition, pronunciation variants sometimes include sounds not present in the dictionary at all, such as nasalized vowels (“can’t” —* [k ae n n t]) or fricatives introduced due to incomplete consonant closures (“legal” —* [l iy g fr ix l]).1 This variation makes pronunciation modeling one of the major challenges facing speech recognition (McAllaster et al., 1998; Jurafsky et al., 2001; Sarac¸lar and Khudanpur, 2004; Bourlard et al., 1999). 2 Most efforts to address the problem have involved either learning alternative pronunciations and/or their probabilities (Holter and Svendsen, 1999) or using phonetic transformation (substitution, insertion, and deletion) rules, which can come from linguistic knowledge or be learned from data (Riley et al., 1999; Hazen et al., 2005; Hutchinson and Droppo, 2011). These have produced some improvements in recognition performance. However, they also tend to cause additional confusability due to the introduction of add</context>
</contexts>
<marker>McAllaster, Gillick, Scattone, Newman, 1998</marker>
<rawString>D. McAllaster, L. Gillick, F. Scattone, and M. Newman. 1998. Fabricating conversational speech data with acoustic models : A program to examine model-data mismatch. In Proc. International Conference on Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>E Fosler-Lussier</author>
</authors>
<title>Conditional random fields for integrating local discriminative classifiers.</title>
<date>2008</date>
<journal>IEEE Transactions on Acoustics, Speech, and Language Processing,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="30875" citStr="Morris and Fosler-Lussier, 2008" startWordPosition="5363" endWordPosition="5366">nd lines are as defined in Figure 1. extension of the model and learning algorithm to word sequences and (2) feature functions that relate acoustic measurements to sub-word units. The extension to sequences can be done analogously to segmental conditional random fields (SCRFs). The main difference between SCRFs and our approach would be the large-margin learning, which can be straightforwardly applied to sequences. To incorporate acoustics, we can use feature functions based on classifiers of sub-word units, similarly to previous work on CRF-based speech recognition (Gunawardana et al., 2005; Morris and Fosler-Lussier, 2008; Prabhavalkar et al., 2011). Richer, longer-span (e.g., word-level) feature functions are also possible. Thus far we have restricted the pronunciation-toword score to linear combinations of feature functions. This can be extended to non-linear combinations using a kernel. This may be challenging in a high-dimensional feature space. One possibility is to approximate the kernels as in (Keshet et al., 2011). Additional extensions include new feature functions, such as context-sensitive alignment features, and joint inference and learning of the alignment models embedded in the feature functions.</context>
</contexts>
<marker>Morris, Fosler-Lussier, 2008</marker>
<rawString>J. Morris and E. Fosler-Lussier. 2008. Conditional random fields for integrating local discriminative classifiers. IEEE Transactions on Acoustics, Speech, and Language Processing, 16(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prabhavalkar</author>
<author>E Fosler-Lussier</author>
<author>K Livescu</author>
</authors>
<title>A factored conditional random field model for articulatory feature forced transcription.</title>
<date>2011</date>
<booktitle>In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</booktitle>
<contexts>
<context position="30903" citStr="Prabhavalkar et al., 2011" startWordPosition="5367" endWordPosition="5370"> 1. extension of the model and learning algorithm to word sequences and (2) feature functions that relate acoustic measurements to sub-word units. The extension to sequences can be done analogously to segmental conditional random fields (SCRFs). The main difference between SCRFs and our approach would be the large-margin learning, which can be straightforwardly applied to sequences. To incorporate acoustics, we can use feature functions based on classifiers of sub-word units, similarly to previous work on CRF-based speech recognition (Gunawardana et al., 2005; Morris and Fosler-Lussier, 2008; Prabhavalkar et al., 2011). Richer, longer-span (e.g., word-level) feature functions are also possible. Thus far we have restricted the pronunciation-toword score to linear combinations of feature functions. This can be extended to non-linear combinations using a kernel. This may be challenging in a high-dimensional feature space. One possibility is to approximate the kernels as in (Keshet et al., 2011). Additional extensions include new feature functions, such as context-sensitive alignment features, and joint inference and learning of the alignment models embedded in the feature functions. Acknowledgments We thank Ra</context>
</contexts>
<marker>Prabhavalkar, Fosler-Lussier, Livescu, 2011</marker>
<rawString>R. Prabhavalkar, E. Fosler-Lussier, and K. Livescu. 2011. A factored conditional random field model for articulatory feature forced transcription. In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Riley</author>
<author>W Byrne</author>
<author>M Finke</author>
<author>S Khudanpur</author>
<author>A Ljolje</author>
<author>J McDonough</author>
<author>H Nock</author>
<author>M Saraclar</author>
<author>C Wooters</author>
<author>G Zavaliagkos</author>
</authors>
<title>Stochastic pronunciation modelling from hand-labelled phonetic corpora.</title>
<date>1999</date>
<journal>Speech Communication,</journal>
<pages>29--2</pages>
<contexts>
<context position="2876" citStr="Riley et al., 1999" startWordPosition="432" endWordPosition="435">n n t]) or fricatives introduced due to incomplete consonant closures (“legal” —* [l iy g fr ix l]).1 This variation makes pronunciation modeling one of the major challenges facing speech recognition (McAllaster et al., 1998; Jurafsky et al., 2001; Sarac¸lar and Khudanpur, 2004; Bourlard et al., 1999). 2 Most efforts to address the problem have involved either learning alternative pronunciations and/or their probabilities (Holter and Svendsen, 1999) or using phonetic transformation (substitution, insertion, and deletion) rules, which can come from linguistic knowledge or be learned from data (Riley et al., 1999; Hazen et al., 2005; Hutchinson and Droppo, 2011). These have produced some improvements in recognition performance. However, they also tend to cause additional confusability due to the introduction of additional homonyms (Fosler1We use the ARPAbet phonetic alphabet with additional diacritics, such as [ n] for nasalization and [ fr] for frication. 2This problem is separate from the grapheme-to-phoneme problem, in which pronunciations are predicted from a word’s spelling; here, we assume the availability of a dictionary of canonical pronunciations as is usual in speech recognition. 194 Proceed</context>
<context position="6106" citStr="Riley et al., 1999" startWordPosition="907" endWordPosition="910">nctions, which lead to sparser, faster, and better-performing models than conditional random field optimization in our experiments; and we use a large set of different feature functions tailored to pronunciation modeling. In order to focus attention on the pronunciation model alone, our experiments focus on a task that measures only the mapping between words and subword units. Pronunciation models have in the past been tested using a variety of measures. For generative models, phonetic error rate of generated pronunciations (Venkataramani and Byrne, 2001) and phone- or frame-level perplexity (Riley et al., 1999; Jyothi et al., 2011) are appropriate measures. For our discriminative models, we consider the task of lexical access; that is, prediction of a single word given its pronunciation in terms of sub-word units (Fissore et al., 1989; Jyothi et al., 2011). This task is also sometimes referred to as “pronunciation recognition” (Ristad and Yianilos, 1998) or “pronunciation classification” (Filali and Bilmes, 2005).) As we show below, our approach outperforms both traditional phonetic rule-based models and the best previously published results on our data set obtained with generative articulatory app</context>
<context position="17431" citStr="Riley et al., 1999" startWordPosition="2975" endWordPosition="2978">en the length features for the ranges 0 ≤ At &lt; 1 and −3 ≤ At &lt; −2 are T 00&lt;Δ`&lt;1(p, w) = [0 1] , T 0−3&lt;Δ`&lt;−2(p,w) = [0 1] , respectively. Other length features are all zero. 4.3 Phonetic alignment feature functions Beyond the length, we also measure specific phonetic deviations from the dictionary. We define phonetic alignment features that count the (normalized) frequencies of phonetic insertions, phonetic deletions, and substitutions of one surface phone for another baseform phone. Given (p, w), we use dynamic programming to align the surface form p with all of the baseforms of w. Following (Riley et al., 1999), we encode a phoneme/phone with a 4-tuple: consonant manner, consonant place, vowel manner, and vowel place. Let the dash symbol “−” be a gap in the alignment (corresponding to an insertion/deletion). Given p, q ∈ P ∪ {−}, we say that a pair (p, q) is a deletion if p ∈ P and q = −, is an insertion if p = − and q ∈ P, and is a substitution if both p, q ∈ P. Given p, q ∈ P ∪ {−}, let (s1, s2, s3, s4) and (t1, t2, t3, t4) be the corresponding 4-tuple encoding of p and q, respectively. The pcl p r aa pcl p er l iy pcl p r aa bcl b − l iy pcl p r aa pcl p er − − l iy pcl p r aa bcl b ax bcl b l iy</context>
<context position="19672" citStr="Riley et al., 1999" startWordPosition="3446" endWordPosition="3449"> be the one shown in Table 1. Since /p/ occurs four times in the alignments and two of them are aligned to [b], the feature for p → b is then 0p—b(p, w) = 2/4. Unlike the TF-IDF feature functions and the length feature functions, the alignment feature functions can assign a non-zero score to words that are not seen at training time (but are in the dictionary), as long as there is a good alignment with their baseforms. The weights given to the alignment features are the analogue of substitution, insertion, and deletion rule probabilities in traditional phone-based pronunciation models such as (Riley et al., 1999); they can also be seen as a generalized version of the Levenshtein features of (Zweig et al., 2011). s(p, q) = 198 4.4 Dictionary feature function The dictionary feature is an indicator of whether a pronunciation is an exact match to a baseform, which also generalizes to words unseen in training. We define the dictionary feature as φdict(p,w) = ✶p∈pron(w). For example, assume there is a baseform /pcl p r aa bcl b l iy/ for the word “probably” in the dictionary, and p = /pcl p r aa bcl b l iy/. Then φdict (p, probably) = 1, while φdict(p, problem) = 0. 4.5 Articulatory feature functions Articu</context>
</contexts>
<marker>Riley, Byrne, Finke, Khudanpur, Ljolje, McDonough, Nock, Saraclar, Wooters, Zavaliagkos, 1999</marker>
<rawString>M. Riley, W. Byrne, M. Finke, S. Khudanpur, A. Ljolje, J. McDonough, H. Nock, M. Saraclar, C. Wooters, and G. Zavaliagkos. 1999. Stochastic pronunciation modelling from hand-labelled phonetic corpora. Speech Communication, 29(2-4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S Ristad</author>
<author>P N Yianilos</author>
</authors>
<title>Learning string edit distance.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="4088" citStr="Ristad and Yianilos, 1998" startWordPosition="609" endWordPosition="612">. 194 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 194–203, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Lussier et al., 2002). Some other alternatives are articulatory pronunciation models, in which words are represented as multiple parallel sequences of articulatory features rather than single sequences of phones, and which outperform phone-based models on some tasks (Livescu and Glass, 2004; Jyothi et al., 2011); and models for learning edit distances between dictionary and actual pronunciations (Ristad and Yianilos, 1998; Filali and Bilmes, 2005). All of these approaches are generative—i.e., they provide distributions over possible pronunciations given the canonical one(s)—and they are typically trained by maximizing the likelihood over training data. In some recent work, discriminative approaches have been proposed, in which an objective more closely related to the task at hand is optimized. For example, (Vinyals et al., 2009; Korkmazskiy and Juang, 1997) optimize a minimum classification error (MCE) criterion to learn the weights (equivalently, probabilities) of alternative pronunciations for each word; (Sc</context>
<context position="6457" citStr="Ristad and Yianilos, 1998" startWordPosition="962" endWordPosition="965">ing between words and subword units. Pronunciation models have in the past been tested using a variety of measures. For generative models, phonetic error rate of generated pronunciations (Venkataramani and Byrne, 2001) and phone- or frame-level perplexity (Riley et al., 1999; Jyothi et al., 2011) are appropriate measures. For our discriminative models, we consider the task of lexical access; that is, prediction of a single word given its pronunciation in terms of sub-word units (Fissore et al., 1989; Jyothi et al., 2011). This task is also sometimes referred to as “pronunciation recognition” (Ristad and Yianilos, 1998) or “pronunciation classification” (Filali and Bilmes, 2005).) As we show below, our approach outperforms both traditional phonetic rule-based models and the best previously published results on our data set obtained with generative articulatory approaches. 2 Problem setting We define a pronunciation of a word as a representation of the way it is produced by a speaker in terms of some set of linguistically meaningful sub-word units. A pronunciation can be, for example, a sequence of phones or multiple sequences of articulatory features such as nasality, voicing, and tongue and lip positions. F</context>
</contexts>
<marker>Ristad, Yianilos, 1998</marker>
<rawString>E. S. Ristad and P. N. Yianilos. 1998. Learning string edit distance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Commun. ACM,</journal>
<volume>18</volume>
<contexts>
<context position="15605" citStr="Salton et al., 1975" startWordPosition="2631" endWordPosition="2634">roblem” aa l “probably” aa l =([p aa l /l TF/l iy/(p) IDF/l iy/ eproblem [10]T, so the final feature is In practice we iterate over the m examples in the training set several times; each such iteration is an epoch. The final weight vector is set to the average over all weight vectors during training. An alternative loss function that is often used to solve structured prediction problems is the log-loss: gram u 4.1 TF-IDF feature Term frequency (TF) and inverse document frequency (IDF) are measures that have been heavily used in information retrieval to search for documents using word queries (Salton et al., 1975). Similarly to (Zweig et al., 2010), we adapt TF and IDF by treatto TFu(p) = ID IDF represents the discriminative power of an ngram: An n-gram that occurs in few words is better at word discrimination than a very common n-gram. Finally, we define word-specific features using TF We define the TF-IDF feature function of u as L0J . 4.2 Length feature function The length feature functions measure how the length of a word’s surface form tends to deviate from the baseform. These functions are parameterized by a and b and are defined as 0a&lt;Δ`&lt;b(p, w) = na&lt;Δ`&lt;b ⊗ ew, where At = |p |− |v|, for some bas</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. S. Yang. 1975. A vector space model for automatic indexing. Commun. ACM, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sarac¸lar</author>
<author>S Khudanpur</author>
</authors>
<title>Pronunciation change in conversational speech and its implications for automatic speech recognition.</title>
<date>2004</date>
<journal>Computer Speech and Language,</journal>
<volume>18</volume>
<issue>4</issue>
<marker>Sarac¸lar, Khudanpur, 2004</marker>
<rawString>M. Sarac¸lar and S. Khudanpur. 2004. Pronunciation change in conversational speech and its implications for automatic speech recognition. Computer Speech and Language, 18(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schramm</author>
<author>P Beyerlein</author>
</authors>
<title>Towards discriminative lexicon optimization.</title>
<date>2001</date>
<booktitle>In Proc. Eurospeech.</booktitle>
<contexts>
<context position="4714" citStr="Schramm and Beyerlein, 2001" startWordPosition="699" endWordPosition="702">98; Filali and Bilmes, 2005). All of these approaches are generative—i.e., they provide distributions over possible pronunciations given the canonical one(s)—and they are typically trained by maximizing the likelihood over training data. In some recent work, discriminative approaches have been proposed, in which an objective more closely related to the task at hand is optimized. For example, (Vinyals et al., 2009; Korkmazskiy and Juang, 1997) optimize a minimum classification error (MCE) criterion to learn the weights (equivalently, probabilities) of alternative pronunciations for each word; (Schramm and Beyerlein, 2001) use a similar approach with discriminative model combination. In this work, the weighted alternatives are then used in a standard (generative) speech recognizer. In other words, these approaches optimize generative models using discriminative criteria. We propose a general, flexible discriminative approach to pronunciation modeling, rather than discriminatively optimizing a generative model. We formulate a linear model with a large number of word-level and subword-level feature functions, whose weights are learned by optimizing a discriminative criterion. The approach is related to the recent</context>
</contexts>
<marker>Schramm, Beyerlein, 2001</marker>
<rawString>H. Schramm and P. Beyerlein. 2001. Towards discriminative lexicon optimization. In Proc. Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
<author>N Srebro</author>
</authors>
<title>Pegasos: Primal Estimated sub-GrAdient SOlver for SVM.</title>
<date>2007</date>
<booktitle>In Proc. International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="11209" citStr="Shalev-Shwartz et al., 2007" startWordPosition="1802" endWordPosition="1805">rrogate hinge loss, which upper-bounds the zero-one loss: L(θ, pi, wi) = wEaVx Inwi#w �− θ · φ(pi, wi) + θ · φ(pi, w) . (1) Finding the weight vector θ that minimizes the `2-regularized average of this loss function is the structured support vector machine (SVM) problem (Taskar et al., 2003; Tsochantaridis et al., 2005): where λ is a user-defined tuning parameter that balances between regularization and loss minimization. In practice, we have found that solving the quadratic optimization problem given in Eq. (2) converges very slowly using standard methods such as stochastic gradient descent (Shalev-Shwartz et al., 2007). We use a slightly different algorithm, the Passive-Aggressive (PA) algorithm (Crammer et al., 2006), whose average loss is comparable to that of the structured SVM solution (Keshet et al., 2007). The Passive-Aggressive algorithm is an efficient online algorithm that, under some conditions, can be viewed as a dual-coordinate ascent minimizer of Eq. (2) (The connection to dual-coordinate ascent can be found in (Hsieh et al., 2008)). The algorithm begins by setting θ = 0 and proceeds in rounds. In the t-th round the algorithm picks an example (pi, wi) from S at random uniformly without replacem</context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pegasos: Primal Estimated sub-GrAdient SOlver for SVM. In Proc. International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS) 17.</booktitle>
<contexts>
<context position="8847" citStr="Taskar et al., 2003" startWordPosition="1390" endWordPosition="1393">e training set as well as on unseen examples. Let w� = f(p) be the predicted word given the pronunciation p. We assess the quality of the function f by the zero-one loss: if 195 w =� w� then the error is one, otherwise the error is zero. The goal of the learning process is to minimize the expected zero-one loss, where the expectation is taken with respect to a fixed but unknown distribution over words and surface pronunciations. In the next section we present a learning algorithm that aims to minimize the expected zero-one loss. 3 Algorithm Similarly to previous work in structured prediction (Taskar et al., 2003; Tsochantaridis et al., 2005), we construct the function f from a predefined set of N feature functions, 1φjJN j=1, each of the form φj : P* xV —* R. Each feature function takes a surface pronunciation p and a proposed word w and returns a scalar which, intuitively, should be correlated with whether the pronunciation p corresponds to the word w. The feature functions map pronunciations of different lengths along with a proposed word to a vector of fixed dimension in RN. For example, one feature function might measure the Levenshtein distance between the pronunciation p and the canonical pronu</context>
<context position="10872" citStr="Taskar et al., 2003" startWordPosition="1752" endWordPosition="1755">zes the score, where nπ is 1 if predicate π holds and 0 otherwise, and where ρ is an (unknown) distribution from which the examples in our training set are sampled i.i.d. Let S = I(p1, w1), ... , (pm, wm)I be the training set. Instead of working directly with the zero-one loss, which is non-smooth and non-convex, we use the surrogate hinge loss, which upper-bounds the zero-one loss: L(θ, pi, wi) = wEaVx Inwi#w �− θ · φ(pi, wi) + θ · φ(pi, w) . (1) Finding the weight vector θ that minimizes the `2-regularized average of this loss function is the structured support vector machine (SVM) problem (Taskar et al., 2003; Tsochantaridis et al., 2005): where λ is a user-defined tuning parameter that balances between regularization and loss minimization. In practice, we have found that solving the quadratic optimization problem given in Eq. (2) converges very slowly using standard methods such as stochastic gradient descent (Shalev-Shwartz et al., 2007). We use a slightly different algorithm, the Passive-Aggressive (PA) algorithm (Crammer et al., 2006), whose average loss is comparable to that of the structured SVM solution (Keshet et al., 2007). The Passive-Aggressive algorithm is an efficient online algorithm</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin Markov networks. In Advances in Neural Information Processing Systems (NIPS) 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Joachims</author>
<author>T Hofmann</author>
<author>Y Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>6</volume>
<contexts>
<context position="8877" citStr="Tsochantaridis et al., 2005" startWordPosition="1394" endWordPosition="1397">l as on unseen examples. Let w� = f(p) be the predicted word given the pronunciation p. We assess the quality of the function f by the zero-one loss: if 195 w =� w� then the error is one, otherwise the error is zero. The goal of the learning process is to minimize the expected zero-one loss, where the expectation is taken with respect to a fixed but unknown distribution over words and surface pronunciations. In the next section we present a learning algorithm that aims to minimize the expected zero-one loss. 3 Algorithm Similarly to previous work in structured prediction (Taskar et al., 2003; Tsochantaridis et al., 2005), we construct the function f from a predefined set of N feature functions, 1φjJN j=1, each of the form φj : P* xV —* R. Each feature function takes a surface pronunciation p and a proposed word w and returns a scalar which, intuitively, should be correlated with whether the pronunciation p corresponds to the word w. The feature functions map pronunciations of different lengths along with a proposed word to a vector of fixed dimension in RN. For example, one feature function might measure the Levenshtein distance between the pronunciation p and the canonical pronunciation of the word w. This f</context>
<context position="10902" citStr="Tsochantaridis et al., 2005" startWordPosition="1756" endWordPosition="1759">nπ is 1 if predicate π holds and 0 otherwise, and where ρ is an (unknown) distribution from which the examples in our training set are sampled i.i.d. Let S = I(p1, w1), ... , (pm, wm)I be the training set. Instead of working directly with the zero-one loss, which is non-smooth and non-convex, we use the surrogate hinge loss, which upper-bounds the zero-one loss: L(θ, pi, wi) = wEaVx Inwi#w �− θ · φ(pi, wi) + θ · φ(pi, w) . (1) Finding the weight vector θ that minimizes the `2-regularized average of this loss function is the structured support vector machine (SVM) problem (Taskar et al., 2003; Tsochantaridis et al., 2005): where λ is a user-defined tuning parameter that balances between regularization and loss minimization. In practice, we have found that solving the quadratic optimization problem given in Eq. (2) converges very slowly using standard methods such as stochastic gradient descent (Shalev-Shwartz et al., 2007). We use a slightly different algorithm, the Passive-Aggressive (PA) algorithm (Crammer et al., 2006), whose average loss is comparable to that of the structured SVM solution (Keshet et al., 2007). The Passive-Aggressive algorithm is an efficient online algorithm that, under some conditions, </context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. 2005. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Venkataramani</author>
<author>W Byrne</author>
</authors>
<title>MLLR adaptation techniques for pronunciation modeling.</title>
<date>2001</date>
<booktitle>In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</booktitle>
<contexts>
<context position="6049" citStr="Venkataramani and Byrne, 2001" startWordPosition="898" endWordPosition="901">. The main differences are that we optimize large-margin objective functions, which lead to sparser, faster, and better-performing models than conditional random field optimization in our experiments; and we use a large set of different feature functions tailored to pronunciation modeling. In order to focus attention on the pronunciation model alone, our experiments focus on a task that measures only the mapping between words and subword units. Pronunciation models have in the past been tested using a variety of measures. For generative models, phonetic error rate of generated pronunciations (Venkataramani and Byrne, 2001) and phone- or frame-level perplexity (Riley et al., 1999; Jyothi et al., 2011) are appropriate measures. For our discriminative models, we consider the task of lexical access; that is, prediction of a single word given its pronunciation in terms of sub-word units (Fissore et al., 1989; Jyothi et al., 2011). This task is also sometimes referred to as “pronunciation recognition” (Ristad and Yianilos, 1998) or “pronunciation classification” (Filali and Bilmes, 2005).) As we show below, our approach outperforms both traditional phonetic rule-based models and the best previously published results </context>
</contexts>
<marker>Venkataramani, Byrne, 2001</marker>
<rawString>V. Venkataramani and W. Byrne. 2001. MLLR adaptation techniques for pronunciation modeling. In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Vinyals</author>
<author>L Deng</author>
<author>D Yu</author>
<author>A Acero</author>
</authors>
<title>Discriminative pronunciation learning using phonetic decoder and minimum-classification-error criterion.</title>
<date>2009</date>
<booktitle>In Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="4502" citStr="Vinyals et al., 2009" startWordPosition="671" endWordPosition="674">ich outperform phone-based models on some tasks (Livescu and Glass, 2004; Jyothi et al., 2011); and models for learning edit distances between dictionary and actual pronunciations (Ristad and Yianilos, 1998; Filali and Bilmes, 2005). All of these approaches are generative—i.e., they provide distributions over possible pronunciations given the canonical one(s)—and they are typically trained by maximizing the likelihood over training data. In some recent work, discriminative approaches have been proposed, in which an objective more closely related to the task at hand is optimized. For example, (Vinyals et al., 2009; Korkmazskiy and Juang, 1997) optimize a minimum classification error (MCE) criterion to learn the weights (equivalently, probabilities) of alternative pronunciations for each word; (Schramm and Beyerlein, 2001) use a similar approach with discriminative model combination. In this work, the weighted alternatives are then used in a standard (generative) speech recognizer. In other words, these approaches optimize generative models using discriminative criteria. We propose a general, flexible discriminative approach to pronunciation modeling, rather than discriminatively optimizing a generative</context>
</contexts>
<marker>Vinyals, Deng, Yu, Acero, 2009</marker>
<rawString>O. Vinyals, L. Deng, D. Yu, and A. Acero. 2009. Discriminative pronunciation learning using phonetic decoder and minimum-classification-error criterion. In Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zweig</author>
<author>P Nguyen</author>
<author>A Acero</author>
</authors>
<title>Continuous speech recognition with a TF-IDF acoustic model.</title>
<date>2010</date>
<booktitle>In Proc. Interspeech.</booktitle>
<contexts>
<context position="15640" citStr="Zweig et al., 2010" startWordPosition="2637" endWordPosition="2640"> l /l TF/l iy/(p) IDF/l iy/ eproblem [10]T, so the final feature is In practice we iterate over the m examples in the training set several times; each such iteration is an epoch. The final weight vector is set to the average over all weight vectors during training. An alternative loss function that is often used to solve structured prediction problems is the log-loss: gram u 4.1 TF-IDF feature Term frequency (TF) and inverse document frequency (IDF) are measures that have been heavily used in information retrieval to search for documents using word queries (Salton et al., 1975). Similarly to (Zweig et al., 2010), we adapt TF and IDF by treatto TFu(p) = ID IDF represents the discriminative power of an ngram: An n-gram that occurs in few words is better at word discrimination than a very common n-gram. Finally, we define word-specific features using TF We define the TF-IDF feature function of u as L0J . 4.2 Length feature function The length feature functions measure how the length of a word’s surface form tends to deviate from the baseform. These functions are parameterized by a and b and are defined as 0a&lt;Δ`&lt;b(p, w) = na&lt;Δ`&lt;b ⊗ ew, where At = |p |− |v|, for some baseform v ∈ pron(w). The parameters a</context>
</contexts>
<marker>Zweig, Nguyen, Acero, 2010</marker>
<rawString>G. Zweig, P. Nguyen, and A. Acero. 2010. Continuous speech recognition with a TF-IDF acoustic model. In Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zweig</author>
<author>P Nguyen</author>
<author>D Van Compernolle</author>
<author>K Demuynck</author>
<author>L Atlas</author>
<author>P Clark</author>
<author>G Sell</author>
<author>M Wang</author>
<author>F Sha</author>
<author>H Hermansky</author>
<author>D Karakos</author>
<author>A Jansen</author>
<author>S Thomas</author>
<author>G S V S Sivaram</author>
<author>S Bowman</author>
<author>J Kao</author>
</authors>
<title>Speech recognition with segmental conditional random fields:</title>
<date>2011</date>
<journal>A summary of the JHU CLSP</journal>
<booktitle>In Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<marker>Zweig, Nguyen, Van Compernolle, Demuynck, Atlas, Clark, Sell, Wang, Sha, Hermansky, Karakos, Jansen, Thomas, Sivaram, Bowman, Kao, 2011</marker>
<rawString>G. Zweig, P. Nguyen, D. Van Compernolle, K. Demuynck, L. Atlas, P. Clark, G. Sell, M. Wang, F. Sha, H. Hermansky, D. Karakos, A. Jansen, S. Thomas, G.S.V.S. Sivaram, S. Bowman, and J. Kao. 2011. Speech recognition with segmental conditional random fields: A summary of the JHU CLSP 2010 summer workshop. In Proc. International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>