<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012761">
<title confidence="0.973007">
Shared Task: Statistical Machine Translation between European Languages
</title>
<author confidence="0.997405">
Philipp Koehn
</author>
<affiliation confidence="0.9980325">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.975869">
Edinburgh, EH8 9LW, UK
</address>
<email confidence="0.99843">
pkoehn@inf.ed.ac.uk
</email>
<author confidence="0.722103">
Christof Monz
</author>
<affiliation confidence="0.766228">
UMIACS
University of Maryland
</affiliation>
<address confidence="0.869902">
College Park, MD 20742, USA
</address>
<email confidence="0.999252">
christof@umiacs.umd.edu
</email>
<sectionHeader confidence="0.995605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999908431034483">
The ACL-2005 Workshop on Paral-
lel Texts hosted a shared task on
building statistical machine translation
systems for four European language
pairs: French–English, German–English,
Spanish–English, and Finnish–English.
Eleven groups participated in the event.
This paper describes the goals, the task
definition and resources, as well as results
and some analysis.
Statistical machine translation is currently the
dominant paradigm in machine translation research.
Annual competitions are held for Chinese–English
and Arabic–English by NIST (sponsored by the US
military funding agency DARPA), which creates a
forum to present and compare novel ideas and leads
to steady progress in the field.
One of the advantages of statistical machine trans-
lation is that the currently applied methods are fairly
language-independent. Building a new machine
translation system for a new language pair is not
much more than a matter of running a training pro-
cess on a training corpus of parallel text (a text in
one language paired with a translation in another).
It is therefore possible to hold a competition
where research groups have only a few weeks to
build machine translation systems for language pairs
that they have not previously worked on. We effec-
tively demonstrated this with our shared task. For in-
stance, seven teams built Finnish–English machine
translation systems, a language pair that was cer-
tainly not of their immediate concern before.
In contrast to the bigger NIST competition, we
wanted to keep the barrier of entry as low as possi-
ble. We provided not only training data from the Eu-
roparl corpus (Koehn, 2005), but also additional re-
sources: sentence and word alignments, the decoder
Pharaoh1 (Koehn, 2004b), and a language model,
so that participation was feasible even as a graduate
level class project.
Using about 15 million words of translated text,
participants were asked to build a phrase-based sta-
tistical machine translation system. The focus of
the task was to build a probabilistic phrase transla-
tion table, since most of the other resources were
provided — for more on phrase-based statistical
machine translation, refer to Koehn et al. (2003).
The participants’ systems were compared by how
well they translated 2000 previously unseen test sen-
tences from the same domain.
The shared task operated within an extremely
short timeframe. The workshop and hence the
shared task was accepted on February 22, 2005 and
announced on March 3. The official test data was
made available on April 3, results were due one
week later. Despite this tight schedule, eleven re-
search groups participated and built a total of 32 ma-
chine translation systems for the four language pairs.
</bodyText>
<sectionHeader confidence="0.983062" genericHeader="keywords">
1 Goals
</sectionHeader>
<bodyText confidence="0.947958">
When setting up this competition, we were moti-
vated by a number of goals. We set out to:
Create a platform to demonstrate the effective-
ness of novel ideas: The research community is
easily balkanized, where different groups work on
</bodyText>
<footnote confidence="0.993258">
1http://www.isi.edu/licensed-sw/pharaoh/
</footnote>
<page confidence="0.933461">
119
</page>
<note confidence="0.8297415">
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 119–124,
Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<bodyText confidence="0.999497386363636">
different data sets and under different conditions,
so that it becomes often hard to assess, how effec-
tive a novel method is. By creating an environment
with common test and training sets, language model,
preprocessing, and even decoder, the effect of other
model choices can be more easily demonstrated.
Work on new language pairs, new problems:
Different language pairs pose different challenges.
We picked Finnish–English and German–English
for the special problems of rich morphology, word
order, which are a challenge to current phrase-based
SMT methods.
Enable more researchers to get engaged in
SMT research: One of our main goals with provid-
ing as many resources as possible was to keep the
barrier of entry low. Participants could use the word
alignment and other resources and focus on phrase
extraction. We hoped to attract researchers that are
relatively new to the field. We were satisfied to learn
that many entries are by graduate students working
on their own.
Promote and create free resources: Academic
research thrives on freely available resources. The
field of statistical machine translation has been
blessed with a long tradition of freely available soft-
ware tools — such as GIZA++ (Och and Ney, 2003)
— and parallel corpora — such as the Canadian
Hansards2. Following this lead, we made word
alignments and a language model available for this
competition in addition to our previously published
resources (Europarl and Pharaoh). The competition
created resources as well. Most teams agreed to
share system output and their model files. You can
download them from the competition web site3.
Promote work on European language pairs:
Finally, we wanted to promote work on European
languages. The increasing economic and political
ties within the European Union create a huge need
for translation services. We would like to see re-
searchers rise to the challenge of creating high qual-
ity machine translation systems to fill these needs.
We are very grateful for the strong participation,
especially by researchers who are relatively new to
the field.
</bodyText>
<footnote confidence="0.999581">
2http://www.isi.edu/natural-language/download/hansard/
3http://www.statmt.org/wpt05/mt-shared-task/
</footnote>
<sectionHeader confidence="0.753904" genericHeader="introduction">
2 Rules of Engagement
</sectionHeader>
<bodyText confidence="0.999714340909091">
We set up a machine translation competition for
four language pairs. We chose Spanish–English and
French–English, because many researchers would
be familiar with these languages. We chose
German–English for its special problems with word
order (such as nested constructions and split verb
groups) and morphology. Finally, we picked
Finnish–English for the rich agglutinative morphol-
ogy of Finnish.
Statistical machine translation systems are typi-
cally trained on sentence-aligned parallel corpora.
We selected Europarl4, a freely available parallel
corpus in eleven languages. In addition, we also
made a word alignment available, which was de-
rived using a variant of the current default method
for word alignment – Och and Ney (2003)’s refined
method.
Figure 1 details some properties of the parallel
corpora. The training corpus is most of the Europarl
corpus, only the text of sessions from last quarter of
the year 2000 was reserved for testing. The corpus
has the size of roughly 15 million English words in
700,000 sentences – these numbers differ for each of
the four parallel corpora due to the different number
of discarded sentences during sentence alignment
and after enforcing a 40 word length limit for sen-
tences.
The number of foreign words differs even more
dramatically. The effect of Finnish morphology
manifests itself in a low number of words (just over
11 million), but a high number of distinct words
(more than 5 times as many as in the English half).
The test corpus consists of 2000 sentences aligned
across all five languages. Note that the output of
each system is compared against the same English
references for all source languages. The number of
total words, distinct words, and words not seen in the
training data reflects again the morphology effect.
For researchers willing to create their own word
alignment, we suggested the use of GIZA++5, an
implementation of the IBM word-based machine
translation models, which also assisted the creation
of the provided word alignments.
We trained a language model on the English part
</bodyText>
<footnote confidence="0.9999075">
4http://www.statmt.org/europarl/
5http://www.fjoch.com/GIZA++.html
</footnote>
<page confidence="0.913763">
120
</page>
<table confidence="0.999912785714286">
Spanish–English French–English Finnish–English German–English
Training corpus
Sentences 730,740 688,031 716,960 751,088
Source words 15,676,710 15,323,737 11,318,287 15,256,793
English words 15,222,105 13,808,104 15,492,903 16,052,269
Distinct source words 102,886 80,349 358,345 195,291
Distinct English words 64,123 61,627 64,662 65,889
Test corpus
Sentences 2,000
Source words 60,276 65,029 41,431 54,247
English words 57,945
Distinct source words 7,782 7,285 11,996 8,666
Distinct English words 6,054
Unseen source words 209 143 737 377
</table>
<figureCaption confidence="0.999344">
Figure 1: Properties of the Europarl training and test corpora used in the shared task
</figureCaption>
<bodyText confidence="0.9990516">
of the Europarl corpus using the SRI language mod-
eling toolkit (Stolke, 2002). Finally, we suggested
the use of Pharaoh (Koehn, 2004b), a phrase-based
machine translation decoder.
How well does this setup match the state of the
art? The MIT system using the Pharaoh decoder
(Koehn, 2004a) proved to be very competitive in
last year’s NIST evaluation. However, the field is
moving fast, and a number of steps help to improve
upon the provided baseline setup, e.g., larger lan-
guage models trained on general text (up to a bil-
lion words have been used), better reodering mod-
els (e.g., suggested by Tillman (2004) and Och
et al. (2004)), better language-specific preprocessing
(Koehn and Knight, 2003) and restructuring (Collins
et al., 2005), additional feature functions such as
word class language models, and minimum error
rate training (Och, 2003) to optimize parameters.
Some of these steps (e.g., improved reorder-
ing models) go beyond the current capabilities of
Pharaoh. However, we are hopeful that freely avail-
able software continues to match or at least follow
closely the state of the art.
We announced the shared task on March 3, and
provided all the resources mentioned above (also a
development test corpus to track the quality of sys-
tems being developed). The test schedule called for
the translation of 2000 sentence for each of the four
language pairs in the week between April 3–10. We
allowed late submissions up to April 17.
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.999979642857143">
Eleven teams from eight institutions in Europe and
North America participated, see Figure 2 for a com-
plete list. The figure also indicates, if a team used
the Pharaoh decoder (eight teams), the provided lan-
guage model (seven teams) and the provided word
alignment (four did, three of those with additional
preprocessing or additional data).
Translation performance was measured using the
BLEU score (Papineni et al., 2002), which measures
n-gram overlap with a reference translation. In our
case, we only used a single reference translation,
since the test set was taken from a held-out portion
of the Europarl corpus. On the other hand we used a
relatively large number of test sentences to guaran-
tee that the BLEU results are stable despite the fact
that we used only one reference translation for each
sentence.
Shared tasks like this one, of course, bring out the
competitive spirit of participants and can draw criti-
cisms about being a horse race. From an outside per-
spective, however, it is far more interesting to learn
which methods and ideas proved to be successful,
than who won the competition.
Taking stock of the results — see Figure 3 — one
observes a very packed field at the top. While the
participants from the University of Washington pro-
duced the best translations for every single language
pair, the distance to many other participant scores
</bodyText>
<page confidence="0.996874">
121
</page>
<note confidence="0.710563">
ID Team Pharaoh LM Word Al.
</note>
<affiliation confidence="0.927170636363636">
cmu-b Carnegie Mellon University, USA - Bing Zhao yes yes no
cmu-j Carnegie Mellon University, USA - Ying (Joy) Zhang yes yes no
glasgow University of Glasgow, UK yes yes yes+
nrc National Research Council, Canada no no no
rali University of Montreal / RALI, Canada yes yes no
saar Saarland University, Germany yes yes yes
uji University Jaume I, Spain yes yes yes+
upc-j Polytechnic University of Catalonia, Spain - Jesus Gimenez yes yes no
upc-m Polytechnic University of Catalonia, Spain - Marta Ruiz no no no
upc-r Polytechnic University of Catalonia, Spain - Rafael Banchs no no no
uw University of Washington, USA yes no yes+
</affiliation>
<figureCaption confidence="0.900417">
Figure 2: The eleven participating teams: the table also lists, if the Pharaoh decoder, the provided language
model, and the provided word alignment was used (yes+ indicates additional preprocessing)
</figureCaption>
<bodyText confidence="0.997989866666667">
is within a BLEU percentage point or two. As one
might have expected, the scores are best for Spanish
and French, and worst for Finnish. Figure 4 shows
some typical output of the submitted systems.
The proceedings to the workshop include detailed
system descriptions of all participants. Novel phrase
extraction approaches were proposed, along with
better preprocessing, language modeling, rescoring,
and other ideas. We are certain that better perfor-
mance can be achieved by combining some of the
methods used by different participants.
And hence, we would like to pose the challenge to
the research community to build and test better sys-
tems using the provided resources. We will gladly
list additional results on the competition web site.
</bodyText>
<sectionHeader confidence="0.997652" genericHeader="method">
4 Survey
</sectionHeader>
<bodyText confidence="0.999968230769231">
Following the end of the competition, we sent out a
questionnaire to the participants. One of the ques-
tions what they would like to see different in a po-
tential future competition.
We listed four potential changes: 70% of the re-
spondends checked translation from English, 50%
checked out of domain test data, 40% checked more
language pairs, 0% checked fewer language pairs.
Additional suggestions were: alternatives to the
BLEU scoring method (maybe human judgment by
participants themselves), transitive translation using
pivot languages, translation of resource-poor lan-
guages, and more time to prepare for the task.
</bodyText>
<sectionHeader confidence="0.998576" genericHeader="conclusions">
5 Outlook
</sectionHeader>
<bodyText confidence="0.9999814">
Given the short timeframe, one should view the sys-
tem performances (albeit very competitive with the
state of the art) as a baseline effort on the task of
open domain text translation between European lan-
guages.
We hope that future researchers will use the pro-
vided environment as a test bed for their machine
translation systems. We will continue to publish any
scores reported to us.
Since we placed much of the systems’ output on-
line, the interested reader may be inspired to more
closely explore the quality and shortcomings. Even
some of the model files have been made available,
so it is even possible to download and install some
of the systems.
</bodyText>
<sectionHeader confidence="0.814572" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.966285166666667">
Collins, M., Koehn, P., and Kucerova, I. (2005).
Clause restructuring for statistical machine trans-
lation. In Proceedings of the 43rd Meeting
of the Association for Computational Linguistics
(ACL’05), Main Volume, pages 531–540, Ann Ar-
bor, Michigan.
Koehn, P. (2004a). The foundation for statistical ma-
chine translation at MIT. In Proceedings of Ma-
chine Translation Evaluation Workshop 2004.
Koehn, P. (2004b). Pharaoh: a beam search decoder
for statistical machine translation. In 6th Confer-
ence of the Association for Machine Translation
</bodyText>
<page confidence="0.993166">
122
</page>
<table confidence="0.99485">
Spanish-English
System BLEU 1/2/3/4-gram precision (bp)
uw 30.95 64.1/36.6/24.0/16.3 (1.000)
upc-r 30.07 63.1/35.8/23.2/15.6 (1.000)
upc-m 29.84 63.9/35.5/23.0/15.5 (0.995)
nrc 29.08 62.7/34.9/22.2/14.7 (1.000)
rali 28.49 62.4/34.5/21.9/14.4 (0.992)
upc-j 28.13 61.5/33.8/21.4/14.1 (1.000)
saar 26.69 61.0/33.1/20.7/13.5 (0.973)
cmu-j 26.14 61.2/32.4/19.8/12.6 (0.986)
uji 21.65 59.7/27.8/15.2/8.7 (1.000)
French-English
System BLEU 1/2/3/4-gram precision (bp)
uw 30.27 64.8/36.8/23.8/16.0 (0.981)
upc-r 30.20 63.9/36.2/23.3/15.6 (0.998)
nrc 29.53 63.7/35.8/22.7/14.9 (0.997)
rali 28.89 62.6/34.7/22.0/14.6 (1.000)
cmu-b 27.65 63.1/34.0/20.9/13.3 (0.995)
cmu-j 26.71 61.9/33.0/20.3/13.1 (0.984)
saar 26.29 60.8/32.5/20.1/12.9 (0.982)
glasgow 23.01 57.3/28.0/16.7/10.5 (1.000)
uji 21.25 59.8/27.7/14.8/8.3 (1.000)
Finnish-English
System BLEU 1/2/3/4-gram precision (bp)
uw 22.01 59.0/28.6/16.1/9.4 (0.979)
nrc 20.95 57.8/27.2/14.8/8.4 (0.996)
upc-r 20.31 56.6/26.0/14.3/8.3 (0.993)
rali 18.87 55.2/24.7/13.1/7.1 (0.998)
saar 16.76 58.4/26.3/14.2/8.0 (0.819)
uji 13.79 60.0/23.2/10.8/5.3 (0.821)
cmu-j 12.66 53.9/21.7/10.7/5.7 (0.775)
German-English
System BLEU 1/2/3/4-gram precision (bp)
uw 24.77 62.2/31.8/18.8/11.7 (0.965)
upc-r 24.26 59.7/30.1/17.6/11.0 (1.000)
nrc 23.21 60.3/29.8/17.1/10.3 (0.979)
rali 22.91 58.9/29.0/16.8/10.3 (0.982)
saar 20.48 58.0/27.5/15.5/9.2 (0.938)
cmu-j 18.93 59.2/26.8/14.3/8.1 (0.914)
uji 18.89 59.3/25.5/13.0/7.2 (0.976)
</table>
<figureCaption confidence="0.813553333333333">
Figure 3: The scores for the participating systems
(BLEU and its components n-gram-precision and
brevity penalty)
</figureCaption>
<reference confidence="0.973796652173913">
in the Americas, AMTA, Lecture Notes in Com-
puter Science. Springer.
Koehn, P. (2005). Europarl: A parallel corpus for
statistical machine translation. In MT Summit X
(submitted).
Koehn, P. and Knight, K. (2003). Empirical methods
for compound splitting. In Proceedings of Meet-
ing of the European Chapter of the Association of
Computational Linguistics (EACL).
Koehn, P., Och, F. J., and Marcu, D. (2003). Statisti-
cal phrase based translation. In Proceedings of the
Joint Conference on Human Language Technolo-
gies and the Annual Meeting of the North Ameri-
can Chapter of the Association of Computational
Linguistics (HLT-NAACL).
Och, F. J. (2003). Minimum error rate training for
statistical machine translation. In Proceedings
of the 41st Annual Meeting of the Association of
Computational Linguistics (ACL).
Och, F. J., Gildea, D., Khudanpur, S., Sarkar, A.,
Yamada, K., Fraser, A., Kumar, S., Shen, L.,
Smith, D., Eng, K., Jain, V., Jin, Z., and Radev,
D. (2004). A smorgasbord of features for statis-
tical machine translation. In Proceedings of the
Joint Conference on Human Language Technolo-
gies and the Annual Meeting of the North Ameri-
can Chapter of the Association of Computational
Linguistics (HLT-NAACL).
Och, F. J. and Ney, H. (2003). A systematic compar-
ison of various statistical alignment models. Com-
putational Linguistics, 29(1):19–52.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of the
40th Annual Meeting of the Association of Com-
putational Linguistics (ACL).
Stolke, A. (2002). SRILM - an extensible language
modeling toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Process-
ing.
Tillman, C. (2004). A unigram orientation model
for statistical machine translation. In Proceed-
ings of the Joint Conference on Human Language
Technologies and the Annual Meeting ofthe North
American Chapter of the Association of Computa-
tional Linguistics (HLT-NAACL).
</reference>
<page confidence="0.996606">
123
</page>
<bodyText confidence="0.9969432">
Reference
We know all too well that the present Treaties are inadequate and that the Union will need a better
and different structure in future , a more constitutional structure which clearly distinguishes the
powers of the Member States and those of the Union.
Input Spanish
Sabemos muy bien que los Tratados actuales no bastan y que , en el futuro , ser´a necesario desarrol-
lar una estructura mejor y diferente para la Uni´on Europea , una estructura m´as constitucional que
tambi´en deje bien claras cu´ales son las competencias de los Estados miembros y cu´ales pertenecen
a la Uni´on .
Best system (Spanish–English)
we all know very well that the current treaties are not enough and that , in the future , it will
be necessary to develop a structure better and different for the european union , a structure more
constitutional also make it clear what the competences of the member states and what belongs to
the union.
Worst System (Spanish–English)
we know very well that the current treaties not enough and that, in the future , will be necessary
develop a better structure and different to the european union , a structure more constitutional that
also be well clear the powers of the member states and what belong to the union.
Input French
Nous savons tr`es bien que les Trait´es actuels ne suffisent pas et qu ’ il sera n´ecessaire a` l ’ avenir
de d´evelopper une structure plus efficace et diff´erente pour l ’ Union , une structure plus consti-
tutionnelle qui indique clairement quelles sont les comp´etences des ´etats membres et quelles sont
les comp´etences de l ’ Union .
Best system (French–English)
we know very well that the current treaties are not enough and that it will be needed in the future
to develop a structure more effective and different for the union , a structure more constitutional
which clearly indicates what are the competence of member states and what are the powers of the
union.
Input Finnish
Tied¨amme oikein hyvin , ett¨a nykyiset perustamissopimukset eiv¨at ole riitt¨avi¨a ja ett¨a tulevaisu-
udessa on tarpeen kehitt¨a¨a unionille parempi ja toisenlainen rakenne , siis perustuslaillisempi
rakenne , jossa mys ilmaistaan selke¨ammin , mit¨a j¨asenvaltioiden ja unionin toimivaltaan kuuluu
Best system (Finnish–English)
we know very well that the existing founding treaties do not need to be developed for the union
and a different structure , therefore perustuslaillisempi structure , which also expresses clearly
what the member states and the union ’s competence is not sufficient and that better in the future .
Input German
Uns ist sehr wohl bewusst , dass die geltenden Vertr¨age unzul¨anglich sind und k¨unftig eine andere
, effizientere Struktur f¨ur die Union entwickelt werden muss , n¨amlich eine st¨arker konstitutionell
ausgepr¨agte Struktur mit einer klaren Abgrenzung zwischen den Befugnissen der Mitgliedstaaten
und den Kompetenzen der Union.
Best system (German–English)
the union must be developed , with a major institutional structure with a clear demarcation
between the powers of the member states and the competences of the union is well aware that the
existing treaties are inadequate and in the future , a different , more efficient structure for us .
</bodyText>
<figureCaption confidence="0.993561">
Figure 4: The first sentence of the test corpus and system translations
</figureCaption>
<page confidence="0.994966">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.999179">Shared Task: Statistical Machine Translation between European Languages</title>
<author confidence="0.989371">Philipp</author>
<affiliation confidence="0.999065">School of University of</affiliation>
<address confidence="0.716744">Edinburgh, EH8 9LW,</address>
<email confidence="0.995158">pkoehn@inf.ed.ac.uk</email>
<author confidence="0.57823">Christof</author>
<affiliation confidence="0.999247">University of</affiliation>
<address confidence="0.996716">College Park, MD 20742,</address>
<email confidence="0.999803">christof@umiacs.umd.edu</email>
<abstract confidence="0.99136645625">The ACL-2005 Workshop on Parallel Texts hosted a shared task on building statistical machine translation systems for four European language pairs: French–English, German–English, Spanish–English, and Finnish–English. Eleven groups participated in the event. This paper describes the goals, the task definition and resources, as well as results and some analysis. Statistical machine translation is currently the dominant paradigm in machine translation research. Annual competitions are held for Chinese–English and Arabic–English by NIST (sponsored by the US military funding agency DARPA), which creates a forum to present and compare novel ideas and leads to steady progress in the field. One of the advantages of statistical machine translation is that the currently applied methods are fairly language-independent. Building a new machine translation system for a new language pair is not much more than a matter of running a training process on a training corpus of parallel text (a text in one language paired with a translation in another). It is therefore possible to hold a competition where research groups have only a few weeks to build machine translation systems for language pairs that they have not previously worked on. We effectively demonstrated this with our shared task. For instance, seven teams built Finnish–English machine translation systems, a language pair that was certainly not of their immediate concern before. In contrast to the bigger NIST competition, we wanted to keep the barrier of entry as low as possible. We provided not only training data from the Europarl corpus (Koehn, 2005), but also additional resources: sentence and word alignments, the decoder (Koehn, 2004b), and a language model, so that participation was feasible even as a graduate level class project. Using about 15 million words of translated text, participants were asked to build a phrase-based statistical machine translation system. The focus of the task was to build a probabilistic phrase translation table, since most of the other resources were provided — for more on phrase-based statistical machine translation, refer to Koehn et al. (2003). The participants’ systems were compared by how well they translated 2000 previously unseen test sentences from the same domain. The shared task operated within an extremely short timeframe. The workshop and hence the shared task was accepted on February 22, 2005 and announced on March 3. The official test data was made available on April 3, results were due one week later. Despite this tight schedule, eleven research groups participated and built a total of 32 machine translation systems for the four language pairs. 1 Goals When setting up this competition, we were motivated by a number of goals. We set out to: Create a platform to demonstrate the effectiveof novel ideas: research community is easily balkanized, where different groups work on 119 of the ACL Workshop on Building and Using Parallel pages 119–124, Arbor, June 2005. for Computational Linguistics, 2005 different data sets and under different conditions, so that it becomes often hard to assess, how effective a novel method is. By creating an environment with common test and training sets, language model, preprocessing, and even decoder, the effect of other model choices can be more easily demonstrated. Work on new language pairs, new problems: Different language pairs pose different challenges. We picked Finnish–English and German–English for the special problems of rich morphology, word order, which are a challenge to current phrase-based SMT methods. Enable more researchers to get engaged in research: of our main goals with providing as many resources as possible was to keep the barrier of entry low. Participants could use the word alignment and other resources and focus on phrase extraction. We hoped to attract researchers that are relatively new to the field. We were satisfied to learn that many entries are by graduate students working on their own. and create free resources: research thrives on freely available resources. The field of statistical machine translation has been blessed with a long tradition of freely available software tools — such as GIZA++ (Och and Ney, 2003) — and parallel corpora — such as the Canadian Following this lead, we made word alignments and a language model available for this competition in addition to our previously published resources (Europarl and Pharaoh). The competition created resources as well. Most teams agreed to share system output and their model files. You can them from the competition web Promote work on European language pairs: Finally, we wanted to promote work on European languages. The increasing economic and political ties within the European Union create a huge need for translation services. We would like to see researchers rise to the challenge of creating high quality machine translation systems to fill these needs. We are very grateful for the strong participation, especially by researchers who are relatively new to the field. 2 Rules of Engagement We set up a machine translation competition for four language pairs. We chose Spanish–English and French–English, because many researchers would be familiar with these languages. We chose German–English for its special problems with word order (such as nested constructions and split verb groups) and morphology. Finally, we picked Finnish–English for the rich agglutinative morphology of Finnish. Statistical machine translation systems are typically trained on sentence-aligned parallel corpora. selected a freely available parallel corpus in eleven languages. In addition, we also made a word alignment available, which was derived using a variant of the current default method for word alignment – Och and Ney (2003)’s refined method. Figure 1 details some properties of the parallel corpora. The training corpus is most of the Europarl corpus, only the text of sessions from last quarter of the year 2000 was reserved for testing. The corpus has the size of roughly 15 million English words in 700,000 sentences – these numbers differ for each of the four parallel corpora due to the different number of discarded sentences during sentence alignment and after enforcing a 40 word length limit for sentences. The number of foreign words differs even more dramatically. The effect of Finnish morphology manifests itself in a low number of words (just over 11 million), but a high number of distinct words (more than 5 times as many as in the English half). The test corpus consists of 2000 sentences aligned across all five languages. Note that the output of each system is compared against the same English references for all source languages. The number of total words, distinct words, and words not seen in the training data reflects again the morphology effect. For researchers willing to create their own word we suggested the use of an implementation of the IBM word-based machine translation models, which also assisted the creation of the provided word alignments. We trained a language model on the English part 120 Spanish–English French–English Finnish–English German–English Training corpus</abstract>
<note confidence="0.907910615384615">Sentences 730,740 688,031 716,960 751,088 Source words 15,676,710 15,323,737 11,318,287 15,256,793 English words 15,222,105 13,808,104 15,492,903 16,052,269 Distinct source words 102,886 80,349 358,345 195,291 Distinct English words 64,123 61,627 64,662 65,889 Test corpus Sentences 2,000 Source words 60,276 65,029 41,431 54,247 English words 57,945 Distinct source words 7,782 7,285 11,996 8,666 Distinct English words 6,054 Unseen source words 209 143 737 377 Figure 1: Properties of the Europarl training and test corpora used in the shared task</note>
<abstract confidence="0.997617798319328">of the Europarl corpus using the SRI language modeling toolkit (Stolke, 2002). Finally, we suggested the use of Pharaoh (Koehn, 2004b), a phrase-based machine translation decoder. How well does this setup match the state of the art? The MIT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year’s NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004)), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005), additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development test corpus to track the quality of systems being developed). The test schedule called for the translation of 2000 sentence for each of the four language pairs in the week between April 3–10. We allowed late submissions up to April 17. 3 Results Eleven teams from eight institutions in Europe and North America participated, see Figure 2 for a complete list. The figure also indicates, if a team used the Pharaoh decoder (eight teams), the provided language model (seven teams) and the provided word alignment (four did, three of those with additional preprocessing or additional data). Translation performance was measured using the BLEU score (Papineni et al., 2002), which measures n-gram overlap with a reference translation. In our case, we only used a single reference translation, since the test set was taken from a held-out portion of the Europarl corpus. On the other hand we used a relatively large number of test sentences to guarantee that the BLEU results are stable despite the fact that we used only one reference translation for each sentence. Shared tasks like this one, of course, bring out the competitive spirit of participants and can draw criticisms about being a horse race. From an outside perspective, however, it is far more interesting to learn which methods and ideas proved to be successful, than who won the competition. Taking stock of the results — see Figure 3 — one observes a very packed field at the top. While the participants from the University of Washington produced the best translations for every single language pair, the distance to many other participant scores 121 ID Team Pharaoh LM Word Al. cmu-b Carnegie Mellon University, USA - Bing Zhao yes yes no cmu-j Carnegie Mellon University, USA - Ying (Joy) Zhang yes yes no glasgow University of Glasgow, UK yes yes yes+ nrc National Research Council, Canada no no no rali University of Montreal / RALI, Canada yes yes no saar Saarland University, Germany yes yes yes uji University Jaume I, Spain yes yes yes+ upc-j Polytechnic University of Catalonia, Spain - Jesus Gimenez yes yes no upc-m Polytechnic University of Catalonia, Spain - Marta Ruiz no no no upc-r Polytechnic University of Catalonia, Spain - Rafael Banchs no no no uw University of Washington, USA yes no yes+ Figure 2: The eleven participating teams: the table also lists, if the Pharaoh decoder, the provided language model, and the provided word alignment was used (yes+ indicates additional preprocessing) is within a BLEU percentage point or two. As one might have expected, the scores are best for Spanish and French, and worst for Finnish. Figure 4 shows some typical output of the submitted systems. The proceedings to the workshop include detailed system descriptions of all participants. Novel phrase extraction approaches were proposed, along with better preprocessing, language modeling, rescoring, and other ideas. We are certain that better performance can be achieved by combining some of the methods used by different participants. And hence, we would like to pose the challenge to the research community to build and test better systems using the provided resources. We will gladly list additional results on the competition web site. 4 Survey Following the end of the competition, we sent out a questionnaire to the participants. One of the questions what they would like to see different in a potential future competition. We listed four potential changes: 70% of the rechecked from 50% of domain test 40% checked 0% checked language Additional suggestions were: alternatives to the BLEU scoring method (maybe human judgment by participants themselves), transitive translation using pivot languages, translation of resource-poor languages, and more time to prepare for the task. 5 Outlook Given the short timeframe, one should view the system performances (albeit very competitive with the state of the art) as a baseline effort on the task of open domain text translation between European languages. We hope that future researchers will use the provided environment as a test bed for their machine translation systems. We will continue to publish any scores reported to us. Since we placed much of the systems’ output online, the interested reader may be inspired to more closely explore the quality and shortcomings. Even some of the model files have been made available, so it is even possible to download and install some of the systems.</abstract>
<note confidence="0.91550925">References Collins, M., Koehn, P., and Kucerova, I. (2005). Clause restructuring for statistical machine trans- In of the 43rd Meeting of the Association for Computational Linguistics Main pages 531–540, Ann Arbor, Michigan. Koehn, P. (2004a). The foundation for statistical ma-</note>
<title confidence="0.7414525">translation at MIT. In of Ma- Translation Evaluation Workshop</title>
<author confidence="0.916838">Pharaoh a beam search decoder</author>
<affiliation confidence="0.7594905">statistical machine translation. In Conference of the Association for Machine Translation</affiliation>
<address confidence="0.530388">122</address>
<abstract confidence="0.9727095">Spanish-English System BLEU 1/2/3/4-gram precision (bp) uw 30.95 64.1/36.6/24.0/16.3 (1.000) upc-r 30.07 63.1/35.8/23.2/15.6 (1.000) upc-m 29.84 63.9/35.5/23.0/15.5 (0.995) nrc 29.08 62.7/34.9/22.2/14.7 (1.000) rali 28.49 62.4/34.5/21.9/14.4 (0.992) upc-j 28.13 61.5/33.8/21.4/14.1 (1.000) saar 26.69 61.0/33.1/20.7/13.5 (0.973) cmu-j 26.14 61.2/32.4/19.8/12.6 (0.986) uji 21.65 59.7/27.8/15.2/8.7 (1.000) French-English System BLEU 1/2/3/4-gram precision (bp) uw 30.27 64.8/36.8/23.8/16.0 (0.981) upc-r 30.20 63.9/36.2/23.3/15.6 (0.998) nrc 29.53 63.7/35.8/22.7/14.9 (0.997) rali 28.89 62.6/34.7/22.0/14.6 (1.000) cmu-b 27.65 63.1/34.0/20.9/13.3 (0.995) cmu-j 26.71 61.9/33.0/20.3/13.1 (0.984) saar 26.29 60.8/32.5/20.1/12.9 (0.982) glasgow 23.01 57.3/28.0/16.7/10.5 (1.000) uji 21.25 59.8/27.7/14.8/8.3 (1.000) Finnish-English System BLEU 1/2/3/4-gram precision (bp) uw 22.01 59.0/28.6/16.1/9.4 (0.979) nrc 20.95 57.8/27.2/14.8/8.4 (0.996) upc-r 20.31 56.6/26.0/14.3/8.3 (0.993) rali 18.87 55.2/24.7/13.1/7.1 (0.998) saar 16.76 58.4/26.3/14.2/8.0 (0.819) uji 13.79 60.0/23.2/10.8/5.3 (0.821) cmu-j 12.66 53.9/21.7/10.7/5.7 (0.775) German-English System BLEU 1/2/3/4-gram precision (bp) uw 24.77 62.2/31.8/18.8/11.7 (0.965) upc-r 24.26 59.7/30.1/17.6/11.0 (1.000) nrc 23.21 60.3/29.8/17.1/10.3 (0.979) rali 22.91 58.9/29.0/16.8/10.3 (0.982) saar 20.48 58.0/27.5/15.5/9.2 (0.938) cmu-j 18.93 59.2/26.8/14.3/8.1 (0.914) uji 18.89 59.3/25.5/13.0/7.2 (0.976) Figure 3: The scores for the participating systems (BLEU and its components n-gram-precision and brevity penalty) the Americas, Lecture Notes in Computer Science. Springer. Koehn, P. (2005). Europarl: A parallel corpus for machine translation. In Summit X Koehn, P. and Knight, K. (2003). Empirical methods compound splitting. In of Meeting of the European Chapter of the Association of Linguistics Koehn, P., Och, F. J., and Marcu, D. (2003). Statistiphrase based translation. In of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Och, F. J. (2003). Minimum error rate training for machine translation. In of the 41st Annual Meeting of the Association of Linguistics</abstract>
<affiliation confidence="0.491457">Och, F. J., Gildea, D., Khudanpur, S., Sarkar, A.,</affiliation>
<address confidence="0.5411235">Yamada, K., Fraser, A., Kumar, S., Shen, L., Smith, D., Eng, K., Jain, V., Jin, Z., and Radev,</address>
<abstract confidence="0.775763684210526">D. (2004). A smorgasbord of features for statismachine translation. In of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Och, F. J. and Ney, H. (2003). A systematic comparof various statistical alignment models. Com- 29(1):19–52. Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). BLEU: a method for automatic evaluaof machine translation. In of the 40th Annual Meeting of the Association of Com- Linguistics Stolke, A. (2002). SRILM an extensible language toolkit. In of the International Conference on Spoken Language Process- Tillman, C. (2004). A unigram orientation model statistical machine translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting ofthe North American Chapter of the Association of Computa- Linguistics 123 Reference We know all too well that the present Treaties are inadequate and that the Union will need a better and different structure in future , a more constitutional structure which clearly distinguishes the powers of the Member States and those of the Union. Input Spanish Sabemos muy bien que los Tratados actuales no bastan y que , en el futuro , ser´a necesario desarrol-lar una estructura mejor y diferente para la Uni´on Europea , una estructura m´as constitucional que tambi´en deje bien claras cu´ales son las competencias de los Estados miembros y cu´ales pertenecen a la Uni´on . Best system (Spanish–English) we all know very well that the current treaties are not enough and that , in the future , it will be necessary to develop a structure better and different for the european union , a structure more constitutional also make it clear what the competences of the member states and what belongs to the union. Worst System (Spanish–English) we know very well that the current treaties not enough and that, in the future , will be necessary develop a better structure and different to the european union , a structure more constitutional that also be well clear the powers of the member states and what belong to the union. Input French Nous savons tr`es bien que les Trait´es actuels ne suffisent pas et qu ’ il sera n´ecessaire a` l ’ avenir de d´evelopper une structure plus efficace et diff´erente pour l ’ Union , une structure plus consti-tutionnelle qui indique clairement quelles sont les comp´etences des ´etats membres et quelles sont les comp´etences de l ’ Union . Best system (French–English) we know very well that the current treaties are not enough and that it will be needed in the future to develop a structure more effective and different for the union , a structure more constitutional which clearly indicates what are the competence of member states and what are the powers of the union. Input Finnish Tied¨amme oikein hyvin , ett¨a nykyiset perustamissopimukset eiv¨at ole riitt¨avi¨a ja ett¨a tulevaisu-udessa on tarpeen kehitt¨a¨a unionille parempi ja toisenlainen rakenne , siis perustuslaillisempi rakenne , jossa mys ilmaistaan selke¨ammin , mit¨a j¨asenvaltioiden ja unionin toimivaltaan kuuluu Best system (Finnish–English) we know very well that the existing founding treaties do not need to be developed for the union and a different structure , therefore perustuslaillisempi structure , which also expresses clearly what the member states and the union ’s competence is not sufficient and that better in the future .</abstract>
<note confidence="0.6572515">Input German Uns ist sehr wohl bewusst , dass die geltenden Vertr¨age unzul¨anglich sind und k¨unftig eine andere , effizientere Struktur f¨ur die Union entwickelt werden muss , n¨amlich eine st¨arker konstitutionell ausgepr¨agte Struktur mit einer klaren Abgrenzung zwischen den Befugnissen der Mitgliedstaaten und den Kompetenzen der Union. Best system (German–English) the union must be developed , with a major institutional structure with a clear demarcation between the powers of the member states and the competences of the union is well aware that the existing treaties are inadequate and in the future , a different , more efficient structure for us . Figure 4: The first sentence of the test corpus and system translations 124</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>in the Americas, AMTA, Lecture Notes in Computer Science.</booktitle>
<publisher>Springer.</publisher>
<marker></marker>
<rawString>in the Americas, AMTA, Lecture Notes in Computer Science. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit X (submitted).</booktitle>
<contexts>
<context position="1898" citStr="Koehn, 2005" startWordPosition="286" endWordPosition="287">ge paired with a translation in another). It is therefore possible to hold a competition where research groups have only a few weeks to build machine translation systems for language pairs that they have not previously worked on. We effectively demonstrated this with our shared task. For instance, seven teams built Finnish–English machine translation systems, a language pair that was certainly not of their immediate concern before. In contrast to the bigger NIST competition, we wanted to keep the barrier of entry as low as possible. We provided not only training data from the Europarl corpus (Koehn, 2005), but also additional resources: sentence and word alignments, the decoder Pharaoh1 (Koehn, 2004b), and a language model, so that participation was feasible even as a graduate level class project. Using about 15 million words of translated text, participants were asked to build a phrase-based statistical machine translation system. The focus of the task was to build a probabilistic phrase translation table, since most of the other resources were provided — for more on phrase-based statistical machine translation, refer to Koehn et al. (2003). The participants’ systems were compared by how well</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Koehn, P. (2005). Europarl: A parallel corpus for statistical machine translation. In MT Summit X (submitted).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of Meeting of the European Chapter of the Association of Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="9057" citStr="Koehn and Knight, 2003" startWordPosition="1389" endWordPosition="1392">02). Finally, we suggested the use of Pharaoh (Koehn, 2004b), a phrase-based machine translation decoder. How well does this setup match the state of the art? The MIT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year’s NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004)), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005), additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development test corpus to track the quality of systems being developed). The test schedule called for </context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Koehn, P. and Knight, K. (2003). Empirical methods for compound splitting. In Proceedings of Meeting of the European Chapter of the Association of Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="2445" citStr="Koehn et al. (2003)" startWordPosition="370" endWordPosition="373">e provided not only training data from the Europarl corpus (Koehn, 2005), but also additional resources: sentence and word alignments, the decoder Pharaoh1 (Koehn, 2004b), and a language model, so that participation was feasible even as a graduate level class project. Using about 15 million words of translated text, participants were asked to build a phrase-based statistical machine translation system. The focus of the task was to build a probabilistic phrase translation table, since most of the other resources were provided — for more on phrase-based statistical machine translation, refer to Koehn et al. (2003). The participants’ systems were compared by how well they translated 2000 previously unseen test sentences from the same domain. The shared task operated within an extremely short timeframe. The workshop and hence the shared task was accepted on February 22, 2005 and announced on March 3. The official test data was made available on April 3, results were due one week later. Despite this tight schedule, eleven research groups participated and built a total of 32 machine translation systems for the four language pairs. 1 Goals When setting up this competition, we were motivated by a number of g</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, P., Och, F. J., and Marcu, D. (2003). Statistical phrase based translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="9208" citStr="Och, 2003" startWordPosition="1413" endWordPosition="1414">IT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year’s NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004)), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005), additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development test corpus to track the quality of systems being developed). The test schedule called for the translation of 2000 sentence for each of the four language pairs in the week between April 3–10. We allowed late submissions up to April 17. 3 Resu</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, F. J. (2003). Minimum error rate training for statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="false">
<authors>
<author>F J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A smorgasbord of features for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="8991" citStr="Och et al. (2004)" startWordPosition="1382" endWordPosition="1385">l corpus using the SRI language modeling toolkit (Stolke, 2002). Finally, we suggested the use of Pharaoh (Koehn, 2004b), a phrase-based machine translation decoder. How well does this setup match the state of the art? The MIT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year’s NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004)), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005), additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development test corpus to track the </context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>Och, F. J., Gildea, D., Khudanpur, S., Sarkar, A., Yamada, K., Fraser, A., Kumar, S., Shen, L., Smith, D., Eng, K., Jain, V., Jin, Z., and Radev, D. (2004). A smorgasbord of features for statistical machine translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4635" citStr="Och and Ney, 2003" startWordPosition="719" endWordPosition="722">research: One of our main goals with providing as many resources as possible was to keep the barrier of entry low. Participants could use the word alignment and other resources and focus on phrase extraction. We hoped to attract researchers that are relatively new to the field. We were satisfied to learn that many entries are by graduate students working on their own. Promote and create free resources: Academic research thrives on freely available resources. The field of statistical machine translation has been blessed with a long tradition of freely available software tools — such as GIZA++ (Och and Ney, 2003) — and parallel corpora — such as the Canadian Hansards2. Following this lead, we made word alignments and a language model available for this competition in addition to our previously published resources (Europarl and Pharaoh). The competition created resources as well. Most teams agreed to share system output and their model files. You can download them from the competition web site3. Promote work on European language pairs: Finally, we wanted to promote work on European languages. The increasing economic and political ties within the European Union create a huge need for translation service</context>
<context position="6337" citStr="Och and Ney (2003)" startWordPosition="968" endWordPosition="971">cause many researchers would be familiar with these languages. We chose German–English for its special problems with word order (such as nested constructions and split verb groups) and morphology. Finally, we picked Finnish–English for the rich agglutinative morphology of Finnish. Statistical machine translation systems are typically trained on sentence-aligned parallel corpora. We selected Europarl4, a freely available parallel corpus in eleven languages. In addition, we also made a word alignment available, which was derived using a variant of the current default method for word alignment – Och and Ney (2003)’s refined method. Figure 1 details some properties of the parallel corpora. The training corpus is most of the Europarl corpus, only the text of sessions from last quarter of the year 2000 was reserved for testing. The corpus has the size of roughly 15 million English words in 700,000 sentences – these numbers differ for each of the four parallel corpora due to the different number of discarded sentences during sentence alignment and after enforcing a 40 word length limit for sentences. The number of foreign words differs even more dramatically. The effect of Finnish morphology manifests itse</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, F. J. and Ney, H. (2003). A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="10234" citStr="Papineni et al., 2002" startWordPosition="1581" endWordPosition="1584">ing developed). The test schedule called for the translation of 2000 sentence for each of the four language pairs in the week between April 3–10. We allowed late submissions up to April 17. 3 Results Eleven teams from eight institutions in Europe and North America participated, see Figure 2 for a complete list. The figure also indicates, if a team used the Pharaoh decoder (eight teams), the provided language model (seven teams) and the provided word alignment (four did, three of those with additional preprocessing or additional data). Translation performance was measured using the BLEU score (Papineni et al., 2002), which measures n-gram overlap with a reference translation. In our case, we only used a single reference translation, since the test set was taken from a held-out portion of the Europarl corpus. On the other hand we used a relatively large number of test sentences to guarantee that the BLEU results are stable despite the fact that we used only one reference translation for each sentence. Shared tasks like this one, of course, bring out the competitive spirit of participants and can draw criticisms about being a horse race. From an outside perspective, however, it is far more interesting to l</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="8437" citStr="Stolke, 2002" startWordPosition="1289" endWordPosition="1290">740 688,031 716,960 751,088 Source words 15,676,710 15,323,737 11,318,287 15,256,793 English words 15,222,105 13,808,104 15,492,903 16,052,269 Distinct source words 102,886 80,349 358,345 195,291 Distinct English words 64,123 61,627 64,662 65,889 Test corpus Sentences 2,000 Source words 60,276 65,029 41,431 54,247 English words 57,945 Distinct source words 7,782 7,285 11,996 8,666 Distinct English words 6,054 Unseen source words 209 143 737 377 Figure 1: Properties of the Europarl training and test corpora used in the shared task of the Europarl corpus using the SRI language modeling toolkit (Stolke, 2002). Finally, we suggested the use of Pharaoh (Koehn, 2004b), a phrase-based machine translation decoder. How well does this setup match the state of the art? The MIT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year’s NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004)), better language-specific preprocessing (Koe</context>
</contexts>
<marker>Stolke, 2002</marker>
<rawString>Stolke, A. (2002). SRILM - an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting ofthe North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="8969" citStr="Tillman (2004)" startWordPosition="1379" endWordPosition="1380">task of the Europarl corpus using the SRI language modeling toolkit (Stolke, 2002). Finally, we suggested the use of Pharaoh (Koehn, 2004b), a phrase-based machine translation decoder. How well does this setup match the state of the art? The MIT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year’s NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004)), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005), additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development tes</context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Tillman, C. (2004). A unigram orientation model for statistical machine translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting ofthe North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>