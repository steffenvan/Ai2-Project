<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000055">
<title confidence="0.969544">
Chained Machine Translation Using Morphemes as Pivot Language
</title>
<author confidence="0.776337">
Lei Chen
</author>
<affiliation confidence="0.753276">
Institute of Intelligent
</affiliation>
<note confidence="0.641284">
Machines, Chinese
Academy of Sciences
</note>
<email confidence="0.845622">
alan.cl@163.com
</email>
<author confidence="0.995406">
Wen Li
</author>
<affiliation confidence="0.9849266">
Institute of Intelligent
Machines, Chinese
Academy of Sciences,
University of Science
and Technology of China
</affiliation>
<email confidence="0.996751">
xtliwen@mail.ustc.edu.cn
</email>
<author confidence="0.559348">
Wudabala
</author>
<affiliation confidence="0.695349666666667">
Institute of Intelligent
Machines, Chinese
Academy of Sciences
</affiliation>
<email confidence="0.988123">
hwdbl@126.com
</email>
<author confidence="0.981078">
Miao Li
</author>
<affiliation confidence="0.948435333333333">
Institute of Intelligent
Machines, Chinese
Academy of Sciences
</affiliation>
<email confidence="0.997979">
mli@iim.ac.cn
</email>
<sectionHeader confidence="0.993874" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99981825">
As the smallest meaning-bearing ele-
ments of the languages which have rich
morphology information, morphemes
are often integrated into state-of-the-art
statistical machine translation to improve
translation quality. The paper proposes
an approach which novelly uses mor-
phemes as pivot language in a chained
machine translation system. A machine
translation based method is used therein
to find the mapping relations between
morphemes and words. Experiments
show the effectiveness of our approach,
achieving 18.6 percent increase in BLEU
score over the baseline phrase-based ma-
chine translation system.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999084769230769">
Recently, most evaluations of machine transla-
tion systems (Callison-Burch et al., 2009) indi-
cate that the performance of corpus-based statis-
tical machine translation (SMT) has come up to
the traditional rule-based method. In the corpus-
based SMT, it is difficult to exactly select the
correct inflections (word-endings) if the target
language is highly inflected. This problem will
be more severe if the source language is an iso-
lated language with non-morphology (eg. Chi-
nese) and the target language is an agglutinative
language with productive derivational and inflec-
tional morphology (eg. Mongolian: a minor-
ity language of China). In addition, the lack of
large-scale parallel corpus may cause the sparse
data problem, which will be more severe if one
of the source language and the target language
is highly inflected. As the smallest meaning-
bearing elements of the languages which have
rich morphology information, morphemes are
the compact representation of words. Using mor-
phemes as the semantic units in the parallel cor-
pus can not only help choose the correct inflec-
tions, but also alleviate the data sparseness prob-
lem partially.
Many strategies of integrating morphol-
ogy information into state-of-the-art SMT
systems in different stages have been pro-
posed. (Ramanathan et al., 2009) proposed
a preprocessing approach for incorporating
syntactic and Morphological information within
a phrase-based English-Hindi SMT system.
(Watanabe et al., 2006) proposed a method
which uses Porter stems and even 4-letter pre-
fixes for word alignment. (Koehn et al., 2007)
proposed the factored translation models which
combine feature functions to handle syntactic,
morphological, and other linguistic informa-
tion in a log-linear model during training.
(Minkov et al., 2007) made use of the infor-
mation of morphological structure and source
language in postprocessing to improve SMT
quality. (de Gispert et al., 2009) adopted the
Minimum Bayes Risk decoding strategy to
combine output from identical SMT system,
which is trained on alternative morphological
decompositions of the source language.
Meanwhile, the SMT-based methods are
widely used in the area of natural lan-
guage processing. (Quirk et al., 2004) ap-
plied SMT to generate novel paraphrases.
(Riezler et al., 2007) adopted an SMT-based
</bodyText>
<page confidence="0.983713">
169
</page>
<note confidence="0.8755035">
Proceedings of the 8th Workshop on Asian Language Resources, pages 169–177,
Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing
</note>
<bodyText confidence="0.99992827027027">
method to query expansion in answer retrieval.
(Jiang and Zhou, 2008) used SMT to generate
the second sentence of the Chinese couplets.
As opposed to the above strategies, the pa-
per proposes an approach that uses morphemes
as pivot language in a chained SMT system, for
translating Chinese into Mongolian, which con-
sists of two SMT systems. First, Chinese sen-
tences are translated into Mongolian morphemes
instead of Mongolian words in the Chinese-
Morphemes SMT (SMT1). Then Mongolian
words are generated from morphemes in the
Morphemes-Mongolian SMT (SMT2). The es-
sential part of the chained SMT system is how
to find the mapping relations between the mor-
phemes and words, which is considered as a pro-
cedure of machine translation in our approach.
More concretely, the first challenge of this ap-
proach is to investigate some effective strategies
to segment the Mongolian corpus in the Chinese-
Mongolian parallel corpus. And the second chal-
lenge is how to efficiently generate Mongolian
words from morphemes. Additionally, on the
one hand Mongolian words may have multiple
kinds of morphological segmentations. On the
other hand there is also the ambiguity of word
boundaries in the processing of generating Mon-
golian words from morphemes. In order to solve
these ambiguities, a SMT-based method is ap-
plied in that word context and morphemes con-
text can be taken into account in this method.
The remainder of the paper is organized as
follows. Section 2 introduces two methods of
morphological segmentation. Section 3 presents
the details of chained SMT system. Section 4
describes the experiment results and evaluation.
Section 5 gives concluding remarks.
</bodyText>
<sectionHeader confidence="0.94043" genericHeader="method">
2 Morphological segmentation
</sectionHeader>
<bodyText confidence="0.999951363636364">
Mongolian is a highly agglutinative language
with a rich set of affixes. Mongolian contains
about 30,000 stems, 297 distinct affixes. A big
growth in the number of possible word forms
may occur due to the inflectional and deriva-
tional productions. An inflectional suffix is a
terminal affix that does not change the parts of
speech of the root during concatenation, which
is added to maintain the syntactic environment
of the root. For instance, the Mongolian word
“YABVGSAN” (walking) in the present con-
tinuous tense syntactic environment consists of
the root “YABV” (walk) and the suffix “GSAN”
(ing). Whereas, when a verb root “UILED” (do)
concatenates a noun derivational suffix “BURI”,
it changes to a noun “UILEDBURI” (factory).
According to that whether linguistic lemmatiza-
tion (the reduction to base form) is considered
or not, the paper proposes two methods of mor-
phological segmentation. The two methods are
tested on the same training databases.
The root lemmatization is concerned in the
first method, which is called the SMT-based
morphological segmentation (SMT-MS) in this
paper. Given the Mongolian-morphemes par-
allel corpus, this method trains a Mongolian-
morphemes SMT to segment Mongolian words.
The root lemmatization is considered in the orig-
inal morphological pre-segmented training cor-
pus. So the SMT-based method can also deal
with root lemmatization when it segments a
Mongolian word. For instance, the Mongo-
lian word “BAYIG A” exhibits the change of
spelling during the concatenation of the mor-
phemes “BAI” and “G A”. We also investi-
gate whether it is effective if those roots are
identical to the original word forms. In other
words, the root lemmatization is ignored in the
second method, which takes the gold standard
morphological segmentation corpus as a trained
model of Morfessor (Creutz and Lagus, 2007)
and uses the Viterbi decoding algorithm to seg-
ment new words. Therefore, this method is
called the Morfessor-based morphological seg-
mentation (Mor-MS). For instance, the word
“BAYIG A” will be segmented to “BAYI” and
“G A” instead of “BAI” and “G A”.
The mathematical description of SMT-MS is
the same as the traditional machine transla-
tion system. In the Mor-MS method, the mor-
phological segmentation of a word can be re-
garded as a flat tree (morphological segmenta-
tion tree), where the root node corresponds to
the whole word and the leaves correspond to
morphemes of this word. Figure 1 gives an ex-
</bodyText>
<page confidence="0.977731">
170
</page>
<bodyText confidence="0.997668666666667">
ample. First, the joint probabilistic distribution
(Creutz and Lagus, 2007) of all morphemes in
the morphological segmentation tree are calcu-
lated. And then by using the Viterbi decoding
algorithm, the maximum probability segmenta-
tion combination is selected.
</bodyText>
<figure confidence="0.7753965">
$0G0DB0RILAGDAHV-ACA
$0G0DB0RILA GDA HV -ACA
</figure>
<figureCaption confidence="0.999275">
Figure 1: Morphological segmentation tree
</figureCaption>
<sectionHeader confidence="0.840616" genericHeader="method">
3 Chained SMT system
</sectionHeader>
<bodyText confidence="0.870396">
Figure 2 illustrates the overview of chained
SMT system.
</bodyText>
<subsectionHeader confidence="0.995222">
3.2 Phrase-based SMT
</subsectionHeader>
<bodyText confidence="0.999971909090909">
The authors assume the reader to be famil-
iar with current approaches to machine trans-
lation, so that we briefly introduce the phrase-
based statistical machine translation model
(Koehn et al., 2003) here, which is the founda-
tion of chained SMT system.
In statistical machine translation, given a
source language f, the aim is to seek a target
language e, such that P(e|f) is maximized. The
phrase-based translation model can be expressed
by the following formula:
</bodyText>
<equation confidence="0.7834145">
3.1 Overview e* = arg max P(e|f) = arg max {P(f|e)P(e)}
e e
</equation>
<bodyText confidence="0.999965875">
In order to improve the performance of Chinese-
Mongolian machine translation, the paper pro-
poses an approach which incorporates the mor-
phology information within a chained SMT sys-
tem. More concretely, this system first translates
Chinese into Mongolian morphemes instead of
Mongolian words by the Chinese-Morphemes
SMT. And then it uses the Morphemes-
Mongolian SMT to translate Mongolian mor-
phemes into Mongolian words. Namely, mor-
phemes are regarded as pivot language in this
system.
The chained SMT system consists of a mor-
phological segmentation system and two phrase-
base machine translation systems, which are
given as follows:
</bodyText>
<listItem confidence="0.994786461538462">
• Morphological segmentation: segment-
ing Mongolian words (from the Chinese-
Mongolian parallel corpus) into Mongo-
lian morphemes and obtaining two paral-
lel corpus: Chinese-Morphemes parallel
corpus and Morphemes-Mongolian parallel
corpus.
• SMT1: training the Chinese-Morphemes
SMT on the Chinese-Morphemes parallel
corpus.
• SMT2: training the Morphemes-Mongolian
SMT on the Morphemes-Mongolian paral-
lel corpus.
</listItem>
<bodyText confidence="0.9999075">
where e* indicates the best result, P(e) is the
language model and P(f|e) is the translation
model. According to the standard log-linear
model proposed by (Och and Ney, 2002), the
best result e* that maximizes P(e|f) can be ex-
pressed as follows:
</bodyText>
<equation confidence="0.9835855">
e* = arg max
e
</equation>
<bodyText confidence="0.999907857142857">
where M is the number of feature functions,
Am is the corresponding feature weight, each
hm(e, f) is a feature function.
In our chained SMT system, SMT1, SMT2
and the SMT for morphological segmentation
(namely SMT-MS in Section 2) are all phrase-
based SMTs.
</bodyText>
<subsectionHeader confidence="0.999432">
3.3 Features of Chained SMT system
</subsectionHeader>
<bodyText confidence="0.999785727272727">
As shown in Figure 2, Chinese is translated into
Mongolian morphemes in SMT1, which is the
core part of the chained SMT system. Here mor-
phemes are regarded as words. Therefore, mor-
phemes can play important roles in SMT1 as fol-
lows: the roots present the meaning of the word
and the suffixes help select the correct grammat-
ical environment. The word alignments between
Chinese words and Mongolian morphemes are
learned automatically by GIZA++. Figure 3
gives an instance of word alignment in SMT1.
</bodyText>
<figure confidence="0.396477222222222">
M
{1: Amhm(e, f)}
m=1
171
Chinese (corpus) Chinese (input)
SMT1: Chinese-Morphemes
Mongolian (corpus) Morphological segmentation Morphemes (corpus)
SMT2: Morphemes-Mongolian
Mongolian (output)
</figure>
<figureCaption confidence="0.993803">
Figure 2: Morphemes as pivot language in Chained SMT system
</figureCaption>
<bodyText confidence="0.59034325">
We can see that the morphemes “BI”,“TAN” etc.
are all regarded as words.
wo he ni yiqi qu meiyou yidian bangzhu
BI TAN -TAI 0CI GAD -CV NEMERI ALAG_A
</bodyText>
<figureCaption confidence="0.9954935">
Figure 3: Word alignments between Chinese
words and Mongolian morphemes in SMT1
</figureCaption>
<bodyText confidence="0.99995435483871">
All the most commonly used features of stan-
dard phrase-based SMT, including phrase trans-
lation model, language model, distortion model
and word penalty, are selected in SMT1. These
commonly used features determine the quality
of translation together. The phrases of f and
e are ensured to be good translations of each
other in the phrase translation model P(f|e).
The fluent output is guaranteed in the language
model LM(e). The reordering of the input sen-
tence is allowed in the distortion model D(e, f).
The translation is however more expensive with
the more reordering. The translation results are
guaranteed neither too long nor too short in the
word penalty W(e).
In SMT-MS and SMT2, the task is to find
the mapping relations between Mongolian mor-
phemes and Mongolian words, which is consid-
ered as the word-for-word translation. There-
fore, only phrase translation model and language
model are considered. All the features weights
are uniform distribution by default. Mongolian
words may have multiple kinds of morphologi-
cal segmentations. And there is the ambiguity of
word boundaries in the processing of generating
Mongolian words from morphemes. These am-
biguities can be solved in SMT-MS and SMT2
respectively, since the SMT-based method can
endure mapping errors and solve mapping am-
biguities by the multiple features which can con-
sider the context of Mongolian words.
</bodyText>
<sectionHeader confidence="0.999927" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995866">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.9999826">
In the experiments, first we preprocess the cor-
pus, such as converting Mongolian into Latin
Mongolian and filtering the apparent noisy seg-
mentation of the gold standard morphological
segmentation corpus. And then we evaluate the
effectiveness of the SMTs which find the map-
ping relations between the morphemes and their
corresponding word forms. Namely, SMT-MS
and SMT2. As mentioned above, SMT1 is the
core part of the chained SMT system, which de-
cides the final quality of translation results. So
the evaluation of SMT1 can be reflected by the
evaluation of translation results of whole chained
SMT system. Finally, we evaluate and analyze
the performance of the chained SMT system by
using the automatic evaluation tools.
The translation model consists of a stan-
dard phrase-table with lexicalized reordering.
Bidirectional word alignments obtained with
GIZA++ are intersected using the grow-diag-
final heuristic (Koehn et al., 2003). Translations
of phrases of up to 7 words long are collected
and scored with translation probabilities and lex-
ical weighting. The language model of mor-
phemes is a 5-gram model with Kneser-Ney
</bodyText>
<page confidence="0.990876">
172
</page>
<bodyText confidence="0.9877785">
smoothing. The language model of Mongo-
lian word is 3-gram model with Kneser-Ney
smoothing too. All the language models are
built with the SRI language modeling toolkit
(Stolcke, 2002). The log-linear model feature
weights are learned by using minimum error rate
training (MERT) (Och, 2003) with BLEU score
(Papineni et al., 2002) as the objective function.
</bodyText>
<subsectionHeader confidence="0.998583">
4.2 Corpus preprocessing
</subsectionHeader>
<bodyText confidence="0.999966483870968">
The Chinese-Mongolian parallel corpus and
monolingual sentences are obtain from the 5th
China Workshop on Machine Translation. In
the experiments, we first convert Mongolian
corpus into Latin Mongolian. In morphologi-
cal segmentation, the gold standard morpholog-
ical segmentation corpus contains 38000 Mon-
golian sentences, which are produced semi-
automatically by using the morphological ana-
lyzer Darhan (Nashunwukoutu, 1997) of Inner
Mongolia University. Moreover, in order to ob-
tain the higher quality corpus, most of the wrong
segmentation in the results of morphological an-
alyzer are modified manually by the linguistic
experts. However, there are still some wrong
segmentation in the gold standard corpus. There-
fore, we adopt a strategy to filter the apparent
noisy segmentation. In this strategy, the sum of
the lengths of all the morphemes is required to
be equivalent to the length of the original word.
After filtering, there are still 37967 sentences re-
mained. In addition, the word alignment is vul-
nerable to punctuation in SMT-MS. So all punc-
tuation of the gold standard morphological seg-
mentation corpus are removed to eliminate some
mistakes of the word alignment.
Meanwhile, since the Chinese language
does not have explicit word boundaries, we
also need to do the segmentation of Chinese
words. The word segmentation tool ICTCLAS
(Zhang, 2008) is used in the experiments.
</bodyText>
<subsectionHeader confidence="0.999513">
4.3 Evaluation of SMT-MS and SMT2
</subsectionHeader>
<bodyText confidence="0.999991172413793">
The tasks of SMT-MS and SMT2 are to find
the mapping relations between the morphemes
and their corresponding word forms. Morpho-
logical segmentation is done by SMT-MS. Con-
trarily, SMT2 is used to generate the words
from morphemes. To evaluate the effectiveness
of SMT-MS and SMT2, we divide the filtered
gold standard corpus into two sets for training
(90%) and testing (10%) respectively. The cor-
rect morpheme boundaries are counted for SMT-
MS evaluation, while the correct word bound-
aries are counted for SMT2 evaluation. We use
the two measures precision and recall on dis-
covered word boundaries to evaluate the effec-
tiveness of SMT-MS and SMT2, where precision
is the proportion of correctly discovered bound-
aries among all discovered boundaries by the al-
gorithm, and recall is the proportion of correctly
discovered boundaries among all correct bound-
aries. A high precision indicates that a mor-
pheme boundary is probably correct when it is
suggested. However the proportion of missed
boundaries can not be obtained from it. A high
recall indicates that most of the desired bound-
aries were indeed discovered. However it can not
point out how many incorrect boundaries were
suggested either. In order to get a comprehensive
idea, we also make use of the evaluation method:
F-measure as a compromise.
</bodyText>
<equation confidence="0.995029">
F-measure = 1 1 1 1
2 precision + recall)
</equation>
<bodyText confidence="0.99992355">
These measures assume values between zero and
100%, where high values reflect good perfor-
mance. Therefore, we evaluate the SMT-based
methods by incrementally evaluating the features
used in our phrase-based SMT model.
Table 1 gives the evaluation results, where
PTM denotes Phrase Translation Model, LW de-
notes Lexical Weight, LM denotes Language
Model, IPTM denotes Inverted PTM, ILW de-
notes Inverted LW. Table 1(a) and Table 1(b)
are corresponding to the evaluations of SMT-MS
and SMT2 respectively, where P, R and F de-
note the three measures, namely precision, recall
and F-measure.
The results show that when we add more fea-
tures incrementally, the precision, recall and F-
measure are improved consistently. These in-
dicate that the features are helpful for finding
the mapping relations between morphemes and
Mongolian words.
</bodyText>
<page confidence="0.999546">
173
</page>
<tableCaption confidence="0.999753">
Table 1: Evaluation of SMT-MS and SMT2
</tableCaption>
<table confidence="0.9594298">
(a) Evaluation of SMT-MS
Feature P(%) R(%) F(%)
(1): PTM+LW 73.35 72.45 72.90
(2): (1)+LM 94.91 94.91 94.91
(3): (2)+IPTM+ILW 94.95 94.95 94.95
(b) Evaluation of SMT2
Feature P(%) R(%) F(%)
(1): PTM+LW 75.86 60.04 67.03
(2): (1)+LM 95.13 89.92 92.45
(3): (2)+IPTM+ILW 95.13 90.02 92.51
</table>
<tableCaption confidence="0.94287">
Table 2: Evaluation of systems
</tableCaption>
<figure confidence="0.9962237">
(a) without MERT
NIST BLEU (%)
Baseline 5.3586 20.71
Chain1 5.6471 23.91
Chain2 5.6565 24.57
(b) with MERT
NIST BLEU (%)
Baseline 5.6911 24.13
Chain1 5.7439 24.70
Chain2 5.8401 25.80
</figure>
<subsectionHeader confidence="0.998062">
4.4 Evaluation of chained SMT system
</subsectionHeader>
<bodyText confidence="0.999942396226415">
We use NIST score (Doddington, 2002) and
BLEU score (Papineni et al., 2002) to evaluate
chained SMT system. The training set contains
67288 Chinese-Mongolian parallel sentences.
The test set contains 400 sentences, where each
sentence has four reference sentences which are
translated by native experts.
In the training phase, we convert Mongolian
into Latin Mongolian. And while in the test
phase, we convert the Latin Mongolian back
into the traditional Mongolian words. We com-
pare the chained SMT system with the standard
phrase-based SMT. Table 2 gives the evaluation
of experiment result of each system, where Base-
line is the standard phrase-based SMT, Chain1 is
a chained SMT consisting of SMT-MS, SMT1
and SMT2, Chain2 is also a chained SMT con-
sisting of Mor-MS, SMT1 and SMT2. In Table
2(b), we use MERT to train the feature weights
of the baseline system and the feature weights of
SMT1 in Chain1 and Chain2.
The experiment results show that both Chain1
and Chain2 are much better than the baseline sys-
tem. The BLEU score is improved by 18.6 per-
cent, from 20.71 (Baseline) to 24.57 (Chain2).
In addition, Chain2 is better than Chain1. We
believe that it is essentially related to the dif-
ferent morphemes corpus of Chain1 and Chain2.
The morphemes corpus of Chain1 takes lemma-
tization into account, while the morphemes cor-
pus of Chain2 changes all morphemes to in-
flected forms which are identical to the original
word forms. As the example in Section 2, the
word “BAYIG A” is segmented into “BAI+G A”
in Chain1 and “BAYI+G A” in Chain2. Mean-
while, “BAI” is an independent Mongolian word
in the corpus. So Chain1 can not discriminate the
word “BAI” from the morpheme “BAI”.
As well known, the translation quality of SMT
relies on the performance of morphological seg-
mentation. We give the following example to
intuitively show the quality of translation of the
chained SMT system.
Example 1 Table 3 gives four examples of
translating Chinese into Mongolian. In each
example, four reference sentences translated by
native experts are also given. These examples
indicate that the chained SMT system can help
choose the correct inflections, and partly allevi-
ate the data sparseness problem.
In Table 3(a), the Mongolian word “HAGAS”
(corresponding to the Chinese word “yiban”)
has multiple inflectional forms as follows:
</bodyText>
<figure confidence="0.3605906">
Mongolian Chinese
HAGAS-VN yi bande
HAGAS-IYAR yiban de
HAGAS-TV zaiban
HAGAS-I ba ban
</figure>
<bodyText confidence="0.8401215">
From the above example, we can see that
the baseline system translates the Chinese word
“ban” to the incorrect inflection “HAGAS-TV,
while Chain2 translates it to the correct inflec-
tion “HAGAS” which is the morpheme of all the
other inflections.
</bodyText>
<page confidence="0.998997">
174
</page>
<tableCaption confidence="0.999281">
Table 3: Examples of translating Chinese into Mongolian
</tableCaption>
<table confidence="0.947719527777778">
(a) Lexicalization of morphemes
Chinese xianzai shi jiu dian ban .
Baseline 0D0 B0L YISUN CAG HAGAS-TV.
Chain1 0D0 B0L YISUN CAG HAGAS-TV.
Chain2 0D0 B0L YISUN CAG HAGAS B0LJV BAYIN A.
References 0D0 YISUN CAG HAGAS B0LJV BAYIN A.
0D0 YISUN CAG HAGAS.
0D0 YISUN CAG HAGAS B0LBA.
0D0 YISUN CAG HAGAS B0LJV BAYIN A.
(b) Tense
Chinese qunian zheshihou ni zai ganshenme ?
Baseline NIDVNVN ENE HIRI CI YAGV HIJU BAYIHV BVI?
Chain1 NIDVNVN ENE UYES CI YAGV HIJU BAYIHV BVI?
Chain2 NIDVNVN ENE UY E-DU CI YAGV HIJU BAYIG A BVI?
References NIDVNVN-V ENE UYE-DU TA YAGV HIJU BAYIG A BVI?
NIDVNVN ENE UY E-DU TA YAGV HIJU BAYIBA?
NIDVNVN JIL-VN ENE UYES TA YAGV HIJU BAYIBA?
0D0 NIDVNVN-V ENE UYE-DU TA YAGV HIJU BAYIG A BVI?
(c) Syntax
Chinese wo xiwang jinnian dong tian hui xiaxue .
Baseline BI ENE JIL EBUL-UN EDUR-UN CASV 0R0JV B0L0N A.
Chain1 BI ENE EBUL-UN EDUR CASV 0R0HV-YI HUSEJU BAYIN A.
Chain2 BI ENE EBUL-UN EDUR CASV 0R0HV-YI HUSEJU BAYIN A.
References BI ENE EBUL CAS 0R0HV-YI HUSEJU BAYIN A.
ENE EBUL CASV 0R0HV-YI HUSEJU BAYIN A.
BI ENE EBUL CASV 0R0HV-YI HUSEN E.
BI ENE EBUL CAS 0R0HV-YI HUSEJU BAYIN A.
(d) Out-Of-Vocabulary words
Chinese wo guoqu chang yidazao chuqu sanbu .
Baseline ... URGULJI yidazao GADAN A GARCV SELEGUCEN ALHVLABA.
Chain1 ... URGULJI BODORIHU-BER GADAGVR ALHVLAN A.
Chain2 ... URGULJI ORLOGE ERTE GARCV ALHVLAN A.
References ... URGULJI OROLGE ERTE GARCV AVHVDAG.
... URGULJI ORLOGE ERTE GADAGVR ALHVLADAG.
... YERU NI ORLOGE ERTE B0S0GAD GADAGVR ALHVLADAG.
... URGULJI OROLGE ERTE GARCV AVHVDAG.
</table>
<bodyText confidence="0.9733422">
In Table 3(b), the word “BAYIN” in the result
ofthe baseline system indicates the past tense en-
vironment. However, the correct environment is
the past continuous tense which is indicated by
the word “BAYIN A” appearing in the results of
chain1 and chain2.
In Table 3(c), the baseline system translates
“dongtian” into “EDUR-UN” as an attribute,
while the correct translation should be “EDUR”
as the subject of the object clause.
</bodyText>
<page confidence="0.995937">
175
</page>
<bodyText confidence="0.9956796">
The statistical data-sets from word alignment
corpus show that the vocabularies of the base-
line system includes 376203 Chinese-Mongolian
word pairs, while Chain1 and Chain2 contain
326847 and 291957 Chinese-Morphemes pairs
respectively. This indicates that the chained SMT
system can partly alleviates the data sparseness
problem. As shown in Table 3(d), the baseline
system can not translate the word “yidazao”,
while Chain1 and Chain2 can.
</bodyText>
<sectionHeader confidence="0.97962" genericHeader="conclusions">
5 Concluding remarks
</sectionHeader>
<bodyText confidence="0.999985380952381">
The paper proposes the chained SMT system us-
ing morphemes as pivot language for translat-
ing an isolated language with non-morphology
into an agglutinative language with productive
derivational and inflectional morphology. The
experiment results show that the performance of
the chained SMT system is encouraging. And
the SMT-based method is quite effective for find-
ing mapping relations between morphemes and
words. When adding more features, the preci-
sion, recall and F-measure are all improved more
obviously.
In the future, we will consider the confusion
network or lattice of N-best translation results in-
stead of one best translation result in the chained
SMT system. Meanwhile, the distortion of mor-
pheme order in Mongolian is still obscure and
needs more investigation. And comparing our
work with other language pairs, such as English-
to-French translation, English-to-Spanish trans-
lation, and so on, is also concerned.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999881285714286">
The authors would like to thank the anonymous
reviewers for their helpful reviews. The work is
supported by the National Key Technology R&amp;D
Program of China under No. 2009BAH41B06
and the Dean Foundation (2009) of Hefei Insti-
tutes of Physical Science, Chinese Academy of
Sciences.
</bodyText>
<sectionHeader confidence="0.999465" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999860862745098">
[Callison-Burch et al.2009] Callison-Burch, Chris,
Philipp Koehn, Christof Monz, and Josh
Schroeder. 2009. Findings of the 2009 workshop
on statistical machine translation. In StatMT,
pages 1–28.
[Creutz and Lagus2007] Creutz, Mathias and Krista
Lagus. 2007. Unsupervised models for morpheme
segmentation and morphology learning. TSLP,
4(1):1–34.
[de Gispert et al.2009] de Gispert, Adri`a, Sami Virpi-
oja, Mikko Kurimo, and William Byrne. 2009.
Minimum bayes risk combination of translation
hypotheses from alternative morphological de-
compositions. In HLT, pages 73–76.
[Doddington2002] Doddington, George. 2002. Auto-
matic evaluation of machine translation quality us-
ing n-gram co-occurrence statistics. In HLT, pages
128–132.
[Jiang and Zhou2008] Jiang, Long and Ming Zhou.
2008. Monolingual machine translation for para-
phrase generation. In COLING, pages 377–384.
[Koehn et al.2003] Koehn, Philipp, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-based
translation. In HLT-NAACL, pages 48–54.
[Koehn et al.2007] Koehn, Philipp, Hieu Hoang,
Alexandra Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan, Wade
Shen, Christine Moran, Richard Zens, Chris Dyer,
Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for sta-
tistical machine translation. In ACL, pages 177–
180.
[Minkov et al.2007] Minkov, Einat, Kristina
Toutanova, and Hisami Suzuki. 2007. Generating
complex morphology for machine translation. In
ACL, pages 128–135.
[Nashunwukoutu1997] Nashunwukoutu. 1997. An
automatic segmentation system for the root, stem,
suffix of the mongolian. Journal of Inner Mongo-
lia University, 29(2):53–57.
[Och and Ney2002] Och, Franz Josef and Hermann
Ney. 2002. Discriminative training and maximum
entropy models for statistical machine translation.
In ACL, pages 295–302.
[Och2003] Och, Franz Josef. 2003. Minimum error
rate training in statistical machine translation. In
ACL, pages 160–167.
[Papineni et al.2002] Papineni, Kishore, Salim
Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
BLEU: a method for automatic evaluation of
machine translation. In ACL, pages 311–318.
</reference>
<page confidence="0.985916">
176
</page>
<reference confidence="0.999595">
[Quirk et al.2004] Quirk, Chris, Chris Brockett, and
William B. Dolan. 2004. Generating chinese cou-
plets using a statistical MT approach. In EMNLP,
pages 142–149.
[Ramanathan et al.2009] Ramanathan, Ananthakrish-
nan, Hansraj Choudhary, Avishek Ghosh, and
Pushpak Bhattacharyya. 2009. Case markers and
morphology: addressing the crux of the fluency
problem in English-Hindi SMT. In ACL-IJCNLP,
pages 800–808.
[Riezler et al.2007] Riezler, Stefan, Alexander
Vasserman, Ioannis Tsochantaridis, Vibhu O.
Mittal, and Yi Liu. 2007. Statistical machine
translation for query expansion in answer retrieval.
In ACL, pages 464–471.
[Stolcke2002] Stolcke, Andreas. 2002. SRILM - an
extensible language modeling toolkit. In Proc.
Intl. Conf. on Spoken Language Processing, pages
901–904.
[Watanabe et al.2006] Watanabe, Taro, Hajime
Tsukada, and Hideki Isozaki. 2006. Ntt system
description for the wmt2006 shared task. In WMT,
pages 122–125.
[Zhang2008] Zhang, Huaping. 2008. ICTCLAS.
http://ictclas.org/.
</reference>
<page confidence="0.997698">
177
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.219611">
<title confidence="0.998932">Chained Machine Translation Using Morphemes as Pivot Language</title>
<author confidence="0.999748">Lei Chen</author>
<affiliation confidence="0.965617666666667">Institute of Intelligent Machines, Academy of Sciences</affiliation>
<email confidence="0.979659">alan.cl@163.com</email>
<author confidence="0.998847">Wen Li</author>
<affiliation confidence="0.929168">Institute of Intelligent Machines, Chinese Academy of Sciences, University of Science and Technology of China</affiliation>
<email confidence="0.627618">xtliwen@mail.ustc.edu.cn</email>
<affiliation confidence="0.957463">Institute of Machines, Academy of Sciences</affiliation>
<email confidence="0.981842">hwdbl@126.com</email>
<author confidence="0.927253">Miao</author>
<affiliation confidence="0.937346">Institute of Machines, Academy of Sciences</affiliation>
<email confidence="0.816834">mli@iim.ac.cn</email>
<abstract confidence="0.999383470588235">As the smallest meaning-bearing elements of the languages which have rich morphology information, morphemes are often integrated into state-of-the-art statistical machine translation to improve translation quality. The paper proposes an approach which novelly uses morphemes as pivot language in a chained machine translation system. A machine translation based method is used therein to find the mapping relations between morphemes and words. Experiments show the effectiveness of our approach, achieving 18.6 percent increase in BLEU score over the baseline phrase-based machine translation system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Findings of the 2009 workshop on statistical machine translation. In StatMT,</title>
<date>2009</date>
<pages>1--28</pages>
<marker>[Callison-Burch et al.2009]</marker>
<rawString>Callison-Burch, Chris, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 workshop on statistical machine translation. In StatMT, pages 1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>TSLP,</journal>
<volume>4</volume>
<issue>1</issue>
<marker>[Creutz and Lagus2007]</marker>
<rawString>Creutz, Mathias and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. TSLP, 4(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Sami Virpioja</author>
<author>Mikko Kurimo</author>
<author>William Byrne</author>
</authors>
<title>Minimum bayes risk combination of translation hypotheses from alternative morphological decompositions.</title>
<date>2009</date>
<booktitle>In HLT,</booktitle>
<pages>73--76</pages>
<marker>[de Gispert et al.2009]</marker>
<rawString>de Gispert, Adri`a, Sami Virpioja, Mikko Kurimo, and William Byrne. 2009. Minimum bayes risk combination of translation hypotheses from alternative morphological decompositions. In HLT, pages 73–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In HLT,</booktitle>
<pages>128--132</pages>
<marker>[Doddington2002]</marker>
<rawString>Doddington, George. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In HLT, pages 128–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Ming Zhou</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2008</date>
<booktitle>In COLING,</booktitle>
<pages>377--384</pages>
<marker>[Jiang and Zhou2008]</marker>
<rawString>Jiang, Long and Ming Zhou. 2008. Monolingual machine translation for paraphrase generation. In COLING, pages 377–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>48--54</pages>
<marker>[Koehn et al.2003]</marker>
<rawString>Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLT-NAACL, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation. In</title>
<date>2007</date>
<booktitle>ACL,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<marker>[Koehn et al.2007]</marker>
<rawString>Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, pages 177– 180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
</authors>
<title>Generating complex morphology for machine translation. In</title>
<date>2007</date>
<booktitle>ACL,</booktitle>
<pages>128--135</pages>
<marker>[Minkov et al.2007]</marker>
<rawString>Minkov, Einat, Kristina Toutanova, and Hisami Suzuki. 2007. Generating complex morphology for machine translation. In ACL, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nashunwukoutu</author>
</authors>
<title>An automatic segmentation system for the root, stem, suffix of the mongolian.</title>
<date>1997</date>
<journal>Journal of Inner Mongolia University,</journal>
<volume>29</volume>
<issue>2</issue>
<marker>[Nashunwukoutu1997]</marker>
<rawString>Nashunwukoutu. 1997. An automatic segmentation system for the root, stem, suffix of the mongolian. Journal of Inner Mongolia University, 29(2):53–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>295--302</pages>
<marker>[Och and Ney2002]</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>160--167</pages>
<marker>[Och2003]</marker>
<rawString>Och, Franz Josef. 2003. Minimum error rate training in statistical machine translation. In ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>311--318</pages>
<marker>[Papineni et al.2002]</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William B Dolan</author>
</authors>
<title>Generating chinese couplets using a statistical MT approach.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<pages>142--149</pages>
<marker>[Quirk et al.2004]</marker>
<rawString>Quirk, Chris, Chris Brockett, and William B. Dolan. 2004. Generating chinese couplets using a statistical MT approach. In EMNLP, pages 142–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananthakrishnan Ramanathan</author>
<author>Hansraj Choudhary</author>
<author>Avishek Ghosh</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Case markers and morphology: addressing the crux of the fluency problem in English-Hindi SMT.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP,</booktitle>
<pages>800--808</pages>
<marker>[Ramanathan et al.2009]</marker>
<rawString>Ramanathan, Ananthakrishnan, Hansraj Choudhary, Avishek Ghosh, and Pushpak Bhattacharyya. 2009. Case markers and morphology: addressing the crux of the fluency problem in English-Hindi SMT. In ACL-IJCNLP, pages 800–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
<author>Ioannis Tsochantaridis</author>
<author>Vibhu O Mittal</author>
<author>Yi Liu</author>
</authors>
<title>Statistical machine translation for query expansion in answer retrieval.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>464--471</pages>
<marker>[Riezler et al.2007]</marker>
<rawString>Riezler, Stefan, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu O. Mittal, and Yi Liu. 2007. Statistical machine translation for query expansion in answer retrieval. In ACL, pages 464–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Intl. Conf. on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<marker>[Stolcke2002]</marker>
<rawString>Stolcke, Andreas. 2002. SRILM - an extensible language modeling toolkit. In Proc. Intl. Conf. on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Ntt system description for the wmt2006 shared task.</title>
<date>2006</date>
<booktitle>In WMT,</booktitle>
<pages>122--125</pages>
<marker>[Watanabe et al.2006]</marker>
<rawString>Watanabe, Taro, Hajime Tsukada, and Hideki Isozaki. 2006. Ntt system description for the wmt2006 shared task. In WMT, pages 122–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huaping Zhang</author>
</authors>
<date>2008</date>
<note>ICTCLAS. http://ictclas.org/.</note>
<marker>[Zhang2008]</marker>
<rawString>Zhang, Huaping. 2008. ICTCLAS. http://ictclas.org/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>