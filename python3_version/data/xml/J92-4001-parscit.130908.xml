<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9974345">
The Acquisition and Use of
Context-Dependent Grammars for English
</title>
<author confidence="0.999489">
Robert F. Simmons* Yeong-Ho Yut
</author>
<affiliation confidence="0.999205">
University of Texas University of Texas
</affiliation>
<bodyText confidence="0.997091222222222">
This paper introduces a paradigm of context-dependent grammar (CDG) and an acquisition
system that, through interactive teaching sessions, accumulates the CDG rules. The resulting
context-sensitive rules are used by a stack-based, shift/reduce parser to compute unambiguous
syntactic structures of sentences. The acquisition system and parser have been applied to the phrase
structure and case analyses of 345 sentences, mainly from newswire stories, with 99% accuracy.
Extrapolation from our current grammar predicts that about 25 thousand CDG rule examples
will be sufficient to train the system in phrase structure analysis of most news stories. Overall,
this research concludes that CDG is a computationally and conceptually tractable approach for
the construction of sentence grammar for large subsets of natural language text.
</bodyText>
<sectionHeader confidence="0.993589" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999838173913044">
An enduring goal for natural language processing (NLP) researchers has been to con-
struct computer programs that can read narrative, descriptive texts such as newspaper
stories and translate them into knowledge structures that can answer questions, clas-
sify the content, and provide summaries or other useful abstractions of the text. An
essential aspect of any such NLP system is parsing—to translate the indefinitely long,
recursively embedded strings of words into definite ordered structures of constituent
elements. Despite decades of research, parsing remains a difficult computation that
often results in incomplete, ambiguous structures; and computational grammars for
natural languages remain notably incomplete. In this paper we suggest that a solution
to these problems may be found in the use of context-sensitive rules applied by a
deterministic shift! reduce parser.
A system is described for rapid acquisition of a context-sensitive grammar based
on ordinary news text. The resulting grammar is accessed by deterministic, bottom-
up parsers to compute phrase structure or case analyses of texts that the grammars
cover. The acquisition system allows a linguist to teach a CDG grammar by showing
examples of parsing successive constituents of sentences. At this writing, 16,275 ex-
ample constituents have been shown to the system and used to parse 345 sentences
ranging from 10 to 60 words in length achieving 99% accuracy. These examples com-
press to a grammar of 3,843 rules that are equally effective in parsing. Extrapolation
from our data suggests that acquiring an almost complete phrase structure grammar
for AP Wire text will require about 25,000 example rules. The procedure is further
demonstrated to apply directly to computing superficial case analyses from English
sentences.
</bodyText>
<affiliation confidence="0.924505">
* Department of Computer Sciences, Al Lab, University of Texas, Austin TX 78712. E-mail @cs.texas.edu
t Boeing Helicopter Computer Svces, Philadelphia, PA
</affiliation>
<note confidence="0.8410215">
© 1992 Association for Computational Linguistics
Computational Linguistics Volume 18, Number 4
</note>
<bodyText confidence="0.999965357142857">
One of the first lessons in natural or formal language analysis is the Chomsky
(1957) hierarchy of formal grammars, which classifies grammar forms from unre-
stricted rewrite rules, through context-sensitive, context-free, and the most restricted,
regular grammars. It is usually conceded that pure, context-free grammars are not
powerful enough to account for the syntactic analysis of natural languages (NL) such
as English, Japanese, or Dutch, and most NL research in computational linguistics has
used either augmented context-free or ad hoc grammars. The conventional wisdom
is that context-sensitive grammars probably would be too large and conceptually and
computationally untractable. There is also an unspoken supposition that the use of
a context-sensitive grammar implies using the kind of complex parser required for
parsing a fully context-sensitive language.
However, NL research based on simulated neural networks took a context-based
approach. One of the first hints came from the striking finding from Sejnowski and
Rosenberg&apos;s NETtalk (1988), that seven-character contexts were largely sufficient to
map each character of a printed word into its corresponding phoneme—where each
character actually maps in various contexts into several different phonemes. For ac-
complishing linguistic case analyses McClelland and Kawamoto (1986) and Miikku-
lainen and Dyer (1989) used the entire context of phrases and sentences to map string
contexts into case structures. Robert Allen (1987) mapped nine-word sentences of En-
glish into Spanish translations, and Yu and Simmons (1990) accomplished comparable
context-sensitive translations between English and German simple sentences. It was
apparent that the contexts in which a word occurred provided information to a neural
network that was sufficient to select correct word sense and syntactic structure for
otherwise ambiguous usages of language.
In order to solve a problem of accepting indefinitely long, complex sentences in
a fixed-size neural network, Simmons and Yu (1990) showed a method for training
a network to act as a context-sensitive grammar. A sequential program accessed that
grammar with a deterministic, single-path parser and accurately parsed descriptive
texts. Continuing that research, 2,000 rules were accumulated and a network was
trained using a back-propagation method. The training of this network required ten
days of continuous computation on a Symbolics Lisp Machine. We observed that the
training cost increased by more than the square of the number of training examples and
calculated that 10,000-20,000 rules might well tax a supercomputer. So we decided that
storing the grammar in a hash table would form a far less expensive option, provided
we could define a selection algorithm comparable to that provided by the trained
neural network.
In this paper we describe such a selection formula to select rules for context-
sensitive parsing, a system for acquiring context-sensitive rules, and experiments in
analysis and application of the grammar to ordinary newspaper text. We show that
the application of context-sensitive rules by a deterministic shift/reduce parser is a
conceptually and computationally tractable approach to NLP that may allow us to
accumulate practical grammars for large subsets of English texts.
</bodyText>
<sectionHeader confidence="0.977325" genericHeader="keywords">
2. Context-Dependent Parsing
</sectionHeader>
<bodyText confidence="0.99993175">
In NL research most interest has centered on context-free grammars (CFG), augmented
with feature tests and transformations, used to describe the phrase structure of sen-
tences. There is a broad literature on Generalized Phrase Structure Grammar (Gazdar
et al. 1985), Unification Grammars of various types (Shieber 1986), and Augmented
</bodyText>
<page confidence="0.987124">
392
</page>
<note confidence="0.938945">
Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English
</note>
<bodyText confidence="0.9945996">
Transition Networks (J. Allen 1987). Gazdar (1988) calls attention to a subcategory of
context-sensitive grammars called indexed languages and illustrates some applicability
to natural languages, and Joshi illustrates an application of &amp;quot;mild context-sensitivity&amp;quot;
(Joshi 1987), but in general, NL computation with context-sensitive grammars is a
largely unexplored area.
While a few advanced NLP laboratories have developed grammars and parsing
capabilities for significantly large subsets of natural language,1 it cannot be denied that
massive effort was required and that the results are plagued by ambiguous interpreta-
tions. These grammars are typically a context-free form, augmented by complex feature
tests, transformations, and occasionally, arbitrary programs. The combination of even
an efficient parser with such intricate grammars may greatly increase computational
complexity of the parsing system (Tomita 1985). It is extremely difficult to write and
maintain such grammars, and they must frequently be revised and retested to ensure
internal consistency as new rules are added. We argue here that an acquisition sys-
tem for accumulating context-sensitive rules and their application by a deterministic
shift/reduce parser will greatly simplify the process of constructing and maintaining
natural language parsing systems.
Although we use context-sensitive rules of the form
uXv uYv
they are interpreted by a shift/reduce parser with the result that they can be applied
successfully to the LR(k) subset of context-free languages. Unless the parser is aug-
mented to include shifts in both directions, the system cannot parse context-sensitive
languages. It is an open question as to whether English is or is not context-sensitive,
but it definitely includes discontinuous constituents that may be separated by indefi-
nitely many symbols. For this reason, future developments of the system may require
operations beyond shift and reduce in the parser. To avoid the easy misinterpretation
that our present system applies to context-sensitive languages, we call it Context-
Dependent Grammar (CDG).
We begin with the simple notion of a shift/reduce parser. Given a stack and an
input string of symbols, the shift/reduce parser may only shift a symbol to the stack
(Figure la) or reduce n symbols on the stack by rewriting them as a single symbol
(Figure lb). We further constrain the parser to reduce no more than two symbols on
the stack to a single symbol. The parsing terminates when the stack contains only a
single root element and the input string is empty. Usually this class of parser applies
a CFG to a sentence, but it is equally applicable to CDG.
</bodyText>
<subsectionHeader confidence="0.986215">
2.1 CDG Rule Forms
</subsectionHeader>
<bodyText confidence="0.999944">
The theoretical viewpoint is that the parse of a sentence is a sequence of states, each
composed of a condition of the stack and the input string. The sequence ends success-
fully when the stack contains only the root element (e.g. SNT), and the input string is
</bodyText>
<footnote confidence="0.928961">
1 Notable examples include the large augmented CFGs at IBM Yorktown Hts, the Univ. of Pennsylvania,
and the Linguistic Research Ctr. at the Univ. of Texas.
</footnote>
<page confidence="0.998465">
393
</page>
<figure confidence="0.996895125">
Computational Linguistics Volume 18, Number 4
INPUT SENTENCE
t_i t_i+I t_i+2 . . .
t_m
NT_k
ii
STACK
bottom
INPUT SENTENCE
t_i+I t_1+2 t_11+3
t_i
t_ns
NT _k
STACK
bottom
t i,t m,...,t 1 are terminals.
NT It is a non-terminaL
(a) Shift Operation
INPUT SENTENCE INPUT SENTENCE
t_i t t_i+2 . . . .
On
Arr_k
STACK
bottom
t_i t_i+2 . . . .
NT j
ii
STACK
bottom
t 1,t m,...,t 1 are terminals.
NT j are non-terminals.
(b) Reduce Operation
</figure>
<figureCaption confidence="0.995179">
Figure 1
</figureCaption>
<bodyText confidence="0.936154333333334">
Shift/reduce parser.
empty Each state can be seen as the left half of a context-sensitive rule whose right
half is the succeeding state.
stacksinputs stacks±iinputs+1
However, sentences may be of any length and are often more than forty words,
so the resulting strings and stacks would form very cumbersome rules of variable
lengths. To avoid this difficulty, the stack and input parts of a rule are limited to five
symbols each. In the following example the stack and input parts are separated by
the symbol &amp;quot;k,&amp;quot; as the idea is applied to the sentence &amp;quot;The old man from Spain ate
fish.&amp;quot; The symbol _ stands for blank, art for article, adj for adjective, p for preposition,
n for noun, and v for verb. The syntactic classes are assigned by dictionary lookup in
a context-sensitive dictionary.&apos;
</bodyText>
<page confidence="0.997082">
394
</page>
<note confidence="0.564375">
Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English
</note>
<bodyText confidence="0.864642666666667">
The old man from Spain ate fish
art adj n p n v n
* art adj n p n
</bodyText>
<listItem confidence="0.9575225">
- _ _ _ art * adj npnv
_art adj*npnvn
- _ art adj n*pnvn
__ art np*pnvn_
__np*pnvn
- __npp*nvn
</listItem>
<equation confidence="0.990422625">
_ np pn*vn_
_ _
_ _ np pp * v n _
_ _ np * v n
- _ np v * n
_ _ np v n *
- - _ np vp *
_ _ _ snt *
</equation>
<bodyText confidence="0.99859825">
The analysis terminates with an empty input string and the single symbol &amp;quot;snt&amp;quot;
on the stack, successfully completing the parse. Note that the first four operations can
be described as shifts followed by the two reductions, adj n np, and art np np.
Subsequently the p and n were shifted onto the stack and then reduced to a pp; then
the np and pp on the stack were reduced to an np, followed by the shifting of v and
n, their reduction to vp, and a final reduction of np vp snt. Illustrations similar to
this are often used to introduce the concept of parsing in Al texts on natural language
(e.g. J. Allen 1987).
We could perfectly well record the grammar in pairs of successive states as follows:
nppp*vn___
but some economy can be achieved by recording the operation and possible label as
the right half of a rule. So for the example immediately above, we record:
</bodyText>
<equation confidence="0.9194755">
___npp*nyn__—+ (5)
nppn*vn___--(R pp)
</equation>
<bodyText confidence="0.999588">
where S shifts and (R pp) replaces the top two elements of the stack with pp to form
the next state of the parse.
Thus a windowed context of ten symbols is created as the left half of a rule and an
operation as the right half. Note that if the stack were limited to the top two elements,
and the input to a single element, the rule system would reduce to a binary rule CFG.
The example in Figure 2 shows how a sentence &amp;quot;Treatment is a complete rest and
a special diet&amp;quot; is parsed by a context sensitive shift/reduce parser. Terminal symbols
are lowercase, while nonterminals are uppercase. The shaded areas represent the parts
</bodyText>
<footnote confidence="0.623386">
2 Described in Section 7.3.
</footnote>
<page confidence="0.995725">
395
</page>
<figure confidence="0.97406023255814">
Computational Linguistics
Volume 18, Number 4
Treatment is a complete rest and a special diet.
( n v det adj n cnj det adj n)
Stack Input
bouom■—■ top next last
n v
n v det
det adj
✓ det adj n
n v det NP
n v NP
U v NP cnj
✓ NP cnj det
NP cnj det adj
NP cnj det adj
NP cnj det NP
✓ NP cnj NP
NP CNP
n v NP
VP
U v det adj n
✓ det adj n cnj
det adj n cnj det
adj n cnj det adj
n cnj det adj n
cnj det adj n
cnj det adj n
cnj det adj n
det adj n
adj n
Operation
shift
shift
shift
shift
shift
reduce to NP
reduce to NP
shift
shift
ft
shift
</figure>
<construct confidence="0.973174285714286">
reduce to NP
reduce to NP
reduce to CNP
reduce to NP
reduce to VP
reduce to S
done
</construct>
<figure confidence="0.520611">
Windowed Context
</figure>
<figureCaption confidence="0.822811">
Figure 2
</figureCaption>
<bodyText confidence="0.938322833333333">
An example of windowed context.
of the context invisible to the system. The next operation is solely decided by the
windowed context. It can be observed that the last state in the analysis is the single
symbol SNT—the designated root symbol, on the stack along with an empty input
string, successfully completing the parse.
And this is the CDG form of rule used in the phrase structure analysis.
</bodyText>
<subsectionHeader confidence="0.998565">
2.2 Algorithm for the Shift/Reduce Parser
</subsectionHeader>
<bodyText confidence="0.997608">
The parser accepts a string of syntactic word classes as its input and forms a ten-
symbol vector, five symbols each from the stack and the input string. It looks up this
vector as the left half of a production in the grammar and interprets the right half of
the production as an instruction to modify the stack and input sequences to construct
the next state of the parse. To accomplish these tasks, it maintains two stacks, one for
the input string and one for the syntactic constituents. These stacks may be arbitrarily
large.
An algorithm for the parser is described in Figure 3. The most important part
of this algorithm is to find an applicable CDG rule from the grammar. Finding such
a rule is based on the current windowed context. If there is a rule whose left side
exactly matches the current windowed context, that rule will be applied. However,
realistically, it is often the case that there is no exact match with any rule. Therefore,
it is necessary to find a rule that best matches the current context.
</bodyText>
<page confidence="0.997224">
396
</page>
<figure confidence="0.814881307692308">
Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English
CD-SR-Parser(Input,Cdg)
Input is a string of syntactic classes for the given sentence.
Cdg is the given CDG grammar rules.
Stack := empty
do until(Input = empty and Stack = (SNT))
Windowed-context := Append(Top_five(stack),First_five(input))
Operation := Consult_CDG(Window-context ,Cdg)
if First(Operation) = SHIFT
then Stack := Push(First(Input),Stack)
Input := Rest(Input)
else Stack := Push(Second(Operation),Pop(Pop(Stack)))
end do
</figure>
<figureCaption confidence="0.983872">
The functions, Top_five and First_five, return the lists of top (or first) five elements of the
Stack and the Input respectively. If there are not enough elements, these procedures pad
with blanks. The function Append concatenates two lists into one. Consult_CDG consults
the given CDG rules to find the next operation to take. The details of this function are the
subject of the next section. Push and Pop add or delete one element to/from a stack while
First and Second return the first or second elements of a list, respectively. Rest returns the
given list minus the first element.
Figure 3
</figureCaption>
<bodyText confidence="0.763464">
Context-sensitive shift reduce parser.
</bodyText>
<subsectionHeader confidence="0.999961">
2.3 Consulting the CDG Rules
</subsectionHeader>
<bodyText confidence="0.999954523809524">
There are two related issues in consulting the CDG rules. One is the computational
representation of CDG rules, and the other is the method for selecting an applicable
rule.
In the traditional CFG paradigms, a CFG rule is applicable if the left-hand side of
the rule exactly matches the top elements of the stack. However, in our CDG paradigm,
a perfect match between the left side of a CDG rule and the current state cannot be
assured, and in most cases, a partial match must suffice for the rule to be applied. Since
many rules may partially match the current context, the best matching rule should be
selected.
One way to do this is to use a neural network. Through the back-propagation
algorithm (Rumelhart, Hinton, and Williams 1986), a feed-forward network can be
trained to memorize the CDG rules. After successful training, the network can be used
to retrieve the best matching rule. However, this approach based on neural network
usually takes considerable training time. For instance, in our previous experiment
(Simmons and Yu 1990), training a network for about 2,000 CDG rules took several
days of computation. Therefore, this approach has an intrinsic problem for scaling up,
at least on the present generation of neural net simulation software.
Another method is based on a hash table in which every CDG rule is stored
according to its top two elements of the stack—the fourth and fifth elements of the
left half of the rule. Given the current windowed context, the top two elements of the
stack are used to retrieve all the relevant rules from the hash table.
</bodyText>
<page confidence="0.971319">
397
</page>
<note confidence="0.313904">
Computational Linguistics Volume 18, Number 4
</note>
<bodyText confidence="0.979139166666667">
We use no more than 64 word and phrase class symbols, so there can be no more
than 4,096 possible pairs. The effect is to divide the large number of rules into no
more than 4,096 subgroups, each of which will have a manageable subset. In fact,
with 16,275 rules we discovered that we have only 823 pairs and the average number
of rules per subgroup is 19.8; however, for frequently occurring pairs the number of
rules in the subgroups can be much larger. The problem is to determine what scoring
formula should be used to find the rule that best matches a parsing context.
Sejnowski and Rosenberg (1988) analyzed the weight matrix that resulted from
training NETtalk and discovered a triangular function with the apex centered at the
character in the window and the weights falling off in proportion to distance from
that character. We decided that the best matching rule in our system would follow
a similar pattern with maximum weights for the top two elements on the stack with
weights decreasing in both directions with distance from those positions. The scoring
function we use is developed as follows:
Let R be the set of vectors {Ri, R2, • • • ,R}
where R, is the vector [r1, r27 • 7 r10]
Let C be the vector [ci , c2) • 7 c10]
Let p(c„ ri) be a matching function whose value is 1 if ci Ti, and 0 otherwise.
7Z is the entire set of rules, R, is (the left half of) a particular rule, and C is the
parse context.
Then 7Z&apos; is the subset of R. where if Ri E RI then it(r14, c4) • p(ri5,c5) = 1.
Access of the hash table with the top two elements of the stack, c4, c5 produces
the set R.&apos;.
We can now define the scoring function for each R, E RI.
</bodyText>
<equation confidence="0.997898666666667">
3 10
Score = E ti(ci,ri) • i + E p,(ci,ri)(11 — i)
i=1 i=6
</equation>
<bodyText confidence="0.978345846153846">
The first summation scores the matches between the stack elements of the rule
and the current context, and the second summation scores the matches between the
elements in the input string. If two items of the rule and context match, the total score
is increased by the weight assigned to that position. The maximum score for a perfect
match is 21 according to the above formula.
From several experiments, varying the length of vector and the weights, particu-
larly those assigned to blanks, it has been determined that this formula gave the best
performance among those tested. More importantly, it has worked well in the current
phrase structure and case analysis experiments.
It was an unexpected surprise to us3 that using context-sensitive productions, an
elementary, deterministic, parsing algorithm proved adequate to provide 99% correct,
unambiguous analyses for the entire text studied.
A
</bodyText>
<sectionHeader confidence="0.697785" genericHeader="introduction">
3. Grammar Acquisition for CDG
</sectionHeader>
<bodyText confidence="0.999745">
Constructing an augmented phrase structure grammar of whatever type—unification,
GPSG, or ATN—is a painful process usually involving a well-trained linguistic team
of several people. These types of grammar require that a CFG recognition rule such
</bodyText>
<footnote confidence="0.431925">
3 But perhaps not to Marcus (1980) and Berwick (1985), who promote the study of deterministic parsing.
</footnote>
<page confidence="0.99305">
398
</page>
<subsectionHeader confidence="0.418">
Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English
</subsectionHeader>
<bodyText confidence="0.99981775">
as np vp snt be supported by such additional information as the fact that the np
and vp agree in number, that the np is characterized by particular features such as
count, animate, etc., and that the vp can or cannot accept certain types of complements.
The additional features make the rules exceedingly complex and difficult to prepare
and debug. College students can be taught easily to make a phrase structure tree to
represent a sentence, but it requires considerable linguistic training to deal successfully
with a feature grammar.
We have seen in the preceding section that a CFG is derived from recording the
successive states of the parses of sentences. Thus it was natural for us to develop an
interactive acquisition system that would assist a linguist (or a student) in constructing
such parses to produce easily large sets of example CFG rules.&apos; The system continued
to evolve as a consequence of our use until we had included capabilities to:
</bodyText>
<listItem confidence="0.99989975">
• read in text and data files
• compile dictionary and grammar tables from completed text files
• select a sentence to continue processing or revise
• look up words in a dictionary to suggest the syntactic class for the word
in context when assigning syntactic classes to the words in a sentence
• compare each state of the parse with rules in the current grammar to
predict the shift/reduce operation. A carriage return signals that the user
accepts the prompt, or the typing in of the desired operation overrides it.
• compute and display the parse tree from the local grammar after
completion of each sentence, or from the global total grammar at any
time
• provide backing up and editing capability to correct errors
• print help messages and guide the user
• compile dictionary and grammar entries at the completion of each
sentence, insuring no duplicate entries
• save completed or partially completed grammar files.
</listItem>
<bodyText confidence="0.998704411764706">
The resulting tool, GRAMAQ, enables a linguist to construct a context-sensitive
grammar for a text corpus at the rate of several sentences per hour. Thousands of
rules are accumulated with only weeks of effort in contrast to the years required for a
comparable system of augmented CFG rules. About ten weeks of effort were required
to produce the 16,275 rules on which this study is based. Since GRAMAQ&apos;s prompts
become more accurate as the dictionary and grammar grow in size, there is a positive
acceleration in the speed of grammar accumulation and the linguist&apos;s task gradually
converges to one of alert supervision of the system&apos;s prompts.
A slightly different version of GRAMAQ is Caseaq, which uses operations that
create case constituents to accumulate a context-sensitive grammar that transforms
4 Starting with an Emacs editor, it was fairly easy to read in a file of sentences and to assign each word
its syntactic class according to its context. Then the asterisk was inserted at the beginning of the
syntactic string, the string was copied to the next line, the asterisk moved if a shift operation was
indicated, or the top two symbols on the stack were rewritten if a reduce was required—just as we
constructed the example in the preceding section. Naturally enough, we soon made Emacs macros to
help us, and then escalated to a Lisp program that would print the stack-*-string and interpret our
shift/reduce commands to produce a new state of the parse.
</bodyText>
<page confidence="0.996563">
399
</page>
<table confidence="0.999687111111111">
Computational Linguistics Volume 18, Number 4
Text States Sentences WdsISnt Mn-WdsISnt
Hepatitis 236 12 4-19 10.3
Measles 316 10 4-25 16.3
News Story 470 10 9-51 23.5
APWire-Robots 1005 21 11-53 26.0
APWire-Rocket 1437 25 8-47 29.2
APWire-Shuttle 598 14 12-32 21.9
Total 4062 92 4-53 22.8
</table>
<tableCaption confidence="0.999601">
Table 1
</tableCaption>
<subsectionHeader confidence="0.900162">
Characteristics of a sample of the text corpus.
</subsectionHeader>
<bodyText confidence="0.9995252">
sentences directly to case structures with no intermediate stage of phrase structure
trees. It has the same functionality as GRAMAQ but allows the linguist user to specify
a case argument and value as the transformation of syntactic elements on the stack,
and to rename the head of such a constituent by a syntactic label. Figure 9 in Section 7.3
illustrates the acquisition of case grammar.
</bodyText>
<sectionHeader confidence="0.704147" genericHeader="method">
4. Experiments with CDG
</sectionHeader>
<bodyText confidence="0.9994515">
There are a number of critical questions that need be answered if the claim that CDG
grammars are useful is to be supported.
</bodyText>
<listItem confidence="0.998897">
• Can they be used to obtain accurate parses for real texts?
• Do they reduce ambiguity in the parsing process?
• How well do the rules generalize to new texts?
• How large must a CFG be to encompass the syntactic structures for most
newspaper text?
</listItem>
<subsectionHeader confidence="0.998554">
4.1 Parsing and Ambiguity with CDG
</subsectionHeader>
<bodyText confidence="0.999917722222222">
Over the course of this study we accumulated 345 sentences mainly from newswire
texts. The first two articles were brief disease descriptions from a youth encyclopedia;
the remaining fifteen were newspaper articles from February 1989 using the terms &amp;quot;star
wars,&amp;quot; &amp;quot;SDI,&amp;quot; or &amp;quot;Strategic Defense Initiative.&amp;quot; Table 1 characterizes typical articles by
the number of CDG rules or states, number of sentences, the range of sentence lengths,
and the average number of words per sentence.
We developed our approach to acquiring and parsing context-sensitive grammars
on the first two simple texts, and then used GRAMAQ to redo those texts and to
construct productions for the news stories. The total text numbered 345 sentences,
which accumulated 16,275 context-sensitive rules—an average of 47 per sentence.
The parser embodying the algorithm illustrated earlier in Figure 1 was augmented
to compare the constituents it constructed with those prescribed during grammar ac-
quisition by the linguist. In parsing the 345 sentences, 335 parses exactly matched the
linguist&apos;s original judgement. In nine cases in which differences occurred, the parses
were judged correct, but slightly different sequences of parse states occurred. The
tenth case clearly made an attachment error—of an introductory adverbial phrase in
the sentence &amp;quot;Hours later, Baghdad announced....&amp;quot; This was mistakenly attached to
&amp;quot;Baghdad.&amp;quot; This evaluation shows that the grammar was in precise agreement with
</bodyText>
<page confidence="0.971001">
400
</page>
<figure confidence="0.950769485714286">
Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English
Another mission soon scheduled that also would have priority over the shuttle is the first
firing of a trident two intercontinental range missile from a submerged submarine.
art another
mission
NP
soon
paprt — scheduled
relpron — that
adv
RELSNT also
VP modal — would
( VP
n — priority
have
over
NPPP „the
gut
n shuttle
vbe — is
art the
NP &lt; adj first
prprt — firing
of
PP rZjj Pan a
NP
&lt; trident
two
intercontinental
range
missile
from
a rt a
NP &lt;PaP
submarinesubmerged
</figure>
<figureCaption confidence="0.996619">
Figure 4
</figureCaption>
<bodyText confidence="0.988472954545454">
Sentence parse.
the linguist 97% of the time and completed correct parses in 99.7% of the 345 sentences
from which it was derived. Since our primary interest was in evaluating the effective-
ness of the CDG, all these evaluations were based on using correct syntactic classes
for the words in the sentences. The context-sensitive dictionary lookup procedure de-
scribed in Section 7.3 is 99.5% accurate, but it assigns 40 word classes incorrectly. As a
consequence, using this procedure would result in a reduction of about 10% accuracy
in parsing.
An output of a sentence from the parser is displayed as a tree in Figure 4. Since
the whole mechanism is coded in Lisp, the actual output of the system is a nested list
that is then printed as a tree.
Notice in this figure that the PP at the bottom modifies the NP composed of &amp;quot;the
first firing of a trident two intercontinental range missile&amp;quot; not just the word &amp;quot;firing.&amp;quot;
Since the parsing is bottom-up, left-to-right, the constituents are formed in the natural
order of words encountered in the sentence and the terminals of the tree can be read
top-to-bottom to give their ordering in the sentence.
Although 345 sentences totaling 8594 words is a small selection from the infinite
set of possible English sentences, it is large enough to assure us that the CDG is
a reasonable form of grammar. Since the deterministic parsing algorithm selects a
single interpretation, which we have seen almost perfectly agrees with the linguist&apos;s
parsings, it is apparent that, at least for this size text sample, there is little difficulty
with ambiguous interpretations.
</bodyText>
<figure confidence="0.9933365">
NP
VP
adv
SNT
VP
NP adj--
NP &lt;
PP —•••
</figure>
<page confidence="0.537808">
401
</page>
<note confidence="0.301246">
Computational Linguistics Volume 18, Number 4
</note>
<sectionHeader confidence="0.522097" genericHeader="method">
5. Generalization of CDG
</sectionHeader>
<bodyText confidence="0.99871">
The purpose of accumulating sample rules from texts is to achieve a grammar gen-
eral enough to analyze new texts it has never seen. To be useful, the grammar must
generalize. There are at least three aspects of generalization to be considered.
</bodyText>
<listItem confidence="0.9445472">
• How well does the grammar generalize at the sentence level? That is,
how well does the grammar parse new sentences that it has not
previously experienced?
• How well does the grammar generalize at the operation level? That is,
how well does the grammar predict the correct Shift/Reduce operation
during acquisition of new sentences?
• How much does the rule retention strategy affect generalization? For
instance, when the grammar predicts the same output as a new rule
does, and the new rule is not saved, how well does the resulting
grammar parse?
</listItem>
<subsectionHeader confidence="0.72719">
5.1 Generalization at the Sentence Level
</subsectionHeader>
<bodyText confidence="0.999972230769231">
The complete parse of a sentence is a sequence of states recognized by the grammar
(whether it be CDG or any other). If all the constituents of the new sentence can be
recognized, the new sentence can be parsed correctly. It will be seen in a later para-
graph that with 16,275 rules, the grammar predicts the output of new rules correctly
about 85% of the time. For the average sentence with 47 states, only 85% or about 40
states can be expected to be predicted correctly; consequently the deterministic parse
will frequently fail. In fact, 5 of 14 new sentences parsed correctly in a brief experiment
that used a grammar based on 320 sentences to attempt to parse the new, 20-sentence
text. Considering that only a single path was followed by the deterministic parser, we
predicted that a multiple-path parser would perform somewhat better for this aspect
of generalization. In fact, our initial experiments with a beam search parser resulted
in successful parses of 15 of the 20 new sentences using the same grammar based on
the 320 sentences.
</bodyText>
<subsectionHeader confidence="0.988705">
5.2 Generalization at the Operation Level
</subsectionHeader>
<bodyText confidence="0.999920923076923">
This level of generalization is of central significance to the grammar acquisition system.
When GRAMAQ looks up a state in the grammar it finds the best matching state with
the same top two elements on the stack, and offers the right half of this rule as its
suggestion to the linguist. How often is this prediction correct?
To answer this question we compiled the grammar of 16,275 rules in cumulative
increments of 1,017 rules using a procedure, union-grammar, that would only add a rule
to the grammar if the grammar did not already predict its operation. We call the result
a &amp;quot;minimal-grammar,&amp;quot; and it contains 3,843 rules. The black line of Figure 5 shows
that with the first 1,000 rules 40% were new; with an accumulation of 5,000, 18% were
new rules. By the time 16,000 rules have been accumulated, the curve has flattened to
an average of 16% new rules added. This means that the acquisition system will make
correct prompts about 84% of the time and the linguist will only need to correct the
system&apos;s suggestions about 3 or 4 times in 20 context presentations.
</bodyText>
<page confidence="0.989358">
402
</page>
<figure confidence="0.868827666666667">
Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English
0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
Accumulated Rules by Thousands
</figure>
<figureCaption confidence="0.89961">
Figure 5
</figureCaption>
<bodyText confidence="0.736877">
Generalization of CDG rules.
</bodyText>
<subsectionHeader confidence="0.999329">
5.3 Rule Retention and Generalization
</subsectionHeader>
<bodyText confidence="0.9996984">
If two parsing grammars account equally well for the same sentences, the one with
fewer rules is less redundant, more abstract, and the one to be preferred. We used the
union-grammar procedure to produce and study the minimal grammar for the 16,275
rules (rule-examples) derived from the sample text. Union-grammar records a new
rule for a rule-example:5
</bodyText>
<listItem confidence="0.9964988">
1. if best matching rule has an operation that doesn&apos;t match
2. if best matching rule ties with another rule whose operation does not
match
3. if 2 is true, and score = 21 we have a full contradiction and list the rule
as an error.
</listItem>
<bodyText confidence="0.9990262">
Six contradictions occurred in the grammar; five were inconsistent treatments of &amp;quot;SNT&amp;quot;
followed by one or more punctuation marks, while the sixth offered both a shift and a
&amp;quot;pp&amp;quot; for a preposition-noun followed by a preposition. The latter case is an attachment
ambiguity not resolvable by syntax.
In the first pass as shown in Table 2, the text resulted in 3,194 rules compared with
16,275 possible rules. That is, 13,081 possible CDG rules were not retained because
already existing rules would match and predict the operation. However, using those
rules to parse the same text gave very poor results: zero correct parses at the sentence
level. Therefore, the process of compiling a minimal grammar was repeated starting
with those 3,194 rules. This time only 619 new rules were added. The purpose of this
</bodyText>
<figure confidence="0.6465231">
5 These definite conditions are due to an analysis by Mark Ring.
So ,
i +
1 t130-
+ 4.
CFO
, r f
&apos;..&amp;quot;&apos;44\NN.u.......,
50
40
</figure>
<page confidence="0.903442">
403
</page>
<table confidence="0.9987285">
Computational Linguistics Volume 18, Number 4
Pass linretained Retained Total Rules
1 13081 3194 16275
2 15656 619 16275
3 16245 18 16275
4 16275 0 16275
</table>
<tableCaption confidence="0.99869">
Table 2
</tableCaption>
<subsectionHeader confidence="0.470353">
Four passes with minimal grammar.
</subsectionHeader>
<bodyText confidence="0.998991909090909">
repetition is to get rid of the effect that the rules added later change the predictions
made earlier. Finally, in a fourth repetition of the process no rules were new.
The resulting grammar of 3,843 rules succeeds in parsing the text with only occa-
sional minor errors in attaching constituents. It is to be emphasized that the unretained
rules are similar but not identical to those in the minimal grammar.
We can observe that this technique of minimal retention by &amp;quot;unioning&amp;quot; new rules
to the grammar results in a compression of the order 16,275/3,843 or 4.2 to 1, without
increase in error. If this ratio holds for larger grammars, then if the linguist accu-
mulates 40,000 training-example rules to account for the syntax of a given subset of
language, that grammar can be compressed automatically to about 10,000 rules that
will accomplish the same task.
</bodyText>
<subsectionHeader confidence="0.490056">
6. Predicting the Size of CDGs
</subsectionHeader>
<bodyText confidence="0.99981175">
When any kind of acquisition system is used to accumulate knowledge, one very
interesting question is, when will the knowledge be complete enough for the intended
application? In our case, how many CDG rules will be sufficient to cover almost all
newswire stories? To answer this question, an extrapolation can be used to find a point
when the solid line of Figure 5 intersects with the y-axis. However, the CDG curve is
descending too slowly to make a reliable extrapolation.
Therefore, another question was investigated instead: when will the CDG rules
include a complete set of CFG rules? Note that a CDG rule is equivalent to a CFG rule
if the context is limited to the top two elements of the stack. What the other elements
in the context accomplish is to make one rule preferable to another that has the same
top two elements of the stack, but a different context.
We allow 64 symbols in our phrase structure analysis. That means, there are 642
possible combinations for the top two elements of the stack. For each combination,
there are 65 possible operations:6 a shift or a reduction to another symbol. Among
16,275 CDG rules, we studied how many different CFG rules can be derived by elim-
inating the context. We found 844 different CFG rules that used 600 different left-side
pairs of symbols. This shows that a given context free pair of symbols averages 1.4
different operations.&apos;
Then, as we did with CDG rules, we measured how many new CFG rules were
added in an accumulative fashion. The shaded line of Figure 5 shows the result.
</bodyText>
<footnote confidence="0.99172425">
6 Actually, there are fewer than 65 possible operations since the stack elements can be reduced only to
nonterminal symbols.
7 We actually use only 48 different symbols, so only 482 or 2,304 combinations could have occurred. The
fraction 600/2,304 yields .26, the proportion of the combinatoric space that is actually used, so far.
</footnote>
<page confidence="0.998312">
404
</page>
<tableCaption confidence="0.3421855">
Extrapolation, the gray line, predicts that 99% of the context free pairs will be achieved with the
accumulation of 25,000 context sensitive rules.
</tableCaption>
<figureCaption confidence="0.724213">
Figure 6
Log-log plot of new CFG rules.
</figureCaption>
<bodyText confidence="0.9998978">
Notice that the line has descended to about 1.5% errors at 16,000 rules. To make an
extrapolation easier, a log-log graph shows the same data in Figure 6. From this graph,
it can be predicted that, after about 25,000 CDG rules are accumulated, the grammar
will encompass a CFG component that is 99% complete. Beyond this point, additional
CDG rules will add almost no new CFG rules, but only fine-tune the grammar so that
it can resolve ambiguities more effectively.
Also, it is our belief that, after the CDG reaches that point, a multi-path, beam-
search parser will be able to parse most newswire stories very reliably. This belief is
based on our initial experiment that used a beam search parser to test generalization
of the grammar to find parses for fifteen out of twenty new sentences.
</bodyText>
<sectionHeader confidence="0.612024" genericHeader="method">
7. Acquiring Case Grammar
</sectionHeader>
<bodyText confidence="0.999842153846154">
Explicating the phrase structure constituents of sentences is an essential aspect in
computer recognition of meaning. Case analysis organizes the constituents into a hi-
erarchical structure of labeled propositions. The propositions can be used directly to
answer questions and are the basis of schemas, scripts, and frames that are used to
add meaning to otherwise inexplicit texts. As a result of the experiments with acquir-
ing CDG and exploring its properties for parsing phrase structures, we became fairly
confident that we could generalize the system to acquisition and parsing based on a
grammar that would compute syntactic case structures directly from syntactic strings.
Direct translation from string to structure is supported by neural network experiments
such as those by McClelland and Kawamoto (1986), Miikkulainen and Dyer (1989), Yu
and Simmons (1990), and Leow and Simmons (1990). We reasoned that if we could
acquire case grammar with something approaching the simplicity of acquiring phrase
structure rules, the result could be of great value for NL applications.
</bodyText>
<figure confidence="0.93877">
Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English
100
1,000 /0,000 25,000 100,000
Nbr of Accumulated Rules
</figure>
<page confidence="0.7755">
405
</page>
<note confidence="0.393445">
Computational Linguistics Volume 18, Number 4
</note>
<subsectionHeader confidence="0.996221">
7.1 Case Structure
</subsectionHeader>
<bodyText confidence="0.999965619047619">
Cook (1989) reviewed twenty years of linguistic research on case analysis of natural
language sentences. He synthesized the various theories into a system that depends
on the subclassification of verbs into twelve categories, and it is apparent from his
review that with a fine subcategorization of verbs and nominals, case analysis can be
accomplished as a purely syntactic operation—subject to the limitations of attachment
ambiguities that are not resolvable by syntax. This conclusion is somewhat at variance
with those Al approaches that require a syntactic analysis to be followed by a semantic
operation that filters and transforms syntactic constituents to compute case-labeled
propositions (e.g. Rim 1990), but it is consistent with the neural network experience
of directly mapping from sentence to case structure, and with the AT research that
seeks to integrate syntactic and semantic processing while translating sentences to
propositional structures.
Linguistic theories of case structure have been concerned only with single propo-
sitions headed by verb predications; they have been largely silent with regard to the
structure of noun phrases and the relations among embedded and sequential proposi-
tions. Additional conventions for managing these complications have been developed
in Simmons (1984) and Alterman (1985) and are used here.
The central notion of a case analysis is to translate sentence strings into a nested
structure of case relations (or predicates) where each relation has a head term and an
indefinite number of labeled arguments. An argument may itself be a case relation.
Thus a sentence, as in the examples below, forms a tree of case relations.
</bodyText>
<figure confidence="0.30876325">
The old man from Spain ate fish.
(eat Agt (man Mod old From spain) Obj fish)
Another mission scheduled soon is the first firing of a trident missile
from a submerged submarine.
</figure>
<figureCaption confidence="0.655549333333333">
(is Obj1 (mission Mod another Obj* (scheduled Vmod soon))
Obj2 (firing Mod first Det the Of (missile Nmod trident Det a)
From (submarine Mod submerged Det a)))
</figureCaption>
<bodyText confidence="0.9997935">
Note that mission is in Obj* relation to scheduled. This means the object of scheduled
is mission, and the expression can be read as &amp;quot;another mission such that mission is
scheduled soon.&amp;quot; An asterisk as a suffix to a label always signals the reverse direction
for the label.
There is a small set of case relations for verb arguments, such as verbmodifier,
agent, object, beneficiary, experiencer, location, state, time, direction, etc. For nouns there
are determiner, modifier, quantifier, amount, nounmodifier, preposition, and reverse verb
relations, agt*, obj*, ben*, etc. Prepositions and conjunctions are usually used directly
as argument labels while sentence conjunctions such as because, while, before, after, etc.
are represented as heads of propositions that relate two other propositions with the
labels preceding, post, antecedent, and consequent. For example, &amp;quot;Because she ate fish and
chips earlier, Mary was not hungry.&amp;quot;
</bodyText>
<footnote confidence="0.59185225">
(because Ante (ate Agt she Obj (fish And chips) Vmod earlier)
Conse (was Vmod not Objl mary State hungry))
Verbs are subcategorized as vao, vabo, vo, va, vhav, vbe where a is agent, o is object,
b is beneficiary and vhav is a form of have and vbe a form of be. So far, only the
</footnote>
<page confidence="0.997667">
406
</page>
<note confidence="0.870321">
Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English
</note>
<bodyText confidence="0.9999314">
subcategory of time has been necessary in subcategorizing nouns to accomplish this
form of case analysis, but in general, a lexical semantics is required to resolve syntactic
attachment ambiguities. The complete set of case relations is presumed to be small,
but no one has yet claimed a complete enumeration of them.
Other case systems such as those taught by Schank (1980) and Jackendoff (1983)
classify predicate names into such primitives as Do, Event, Thing, Mtrans, Ptrans, Go,
Action, etc., to approximate some form of &amp;quot;language of thought&amp;quot; but the present ap-
proach is less ambitious, proposing merely to represent in a fairly formal fashion the
organization of the words in a sentence. Subsequent operations on this admittedly
superficial class of case structures, when augmented with a system of shallow lexi-
cal semantics, have been shown to accomplish question answering, focus tracking of
topics throughout a text, automatic outlining, and summarization of texts (Seo 1990;
Rim 1990). One strong constraint on this type of analysis is that the resulting case
structure must maintain all information present in the text so that the text may be
exactly reconstituted from the analysis.
</bodyText>
<subsectionHeader confidence="0.999613">
7.2 Syntactic Analysis of Case Structure
</subsectionHeader>
<bodyText confidence="0.999981727272727">
We&apos;ve seen earlier that a shift/reduce-rename operation is sufficient to parse most
sentences into phrase structures. Case structure, however, requires transformations
in addition to these operations. To form a case structure it is frequently necessary
to change the order of constituents and to insert case labels. Following jackendoff&apos;s
principle of grammatical constraint, which argues essentially that semantic interpretation
is frequently reflected in the syntactic form, case transformations are accomplished as
each syntactic constituent is discovered. Thus when a verb, say throw and an NP, say
coconuts are on top of the stack, one must not only create a VP, but also decide the
case, Obj, and form the constituent, (throw Obj coconuts). This can be accomplished in
customary approaches to parsing by using augmented context free recognition rules
of the form:
</bodyText>
<equation confidence="0.571873">
VP VP NP / 1 obj 2
</equation>
<bodyText confidence="0.999613428571429">
where the numbers following the slash refer to the text dominated by the syntactic
class in the referenced position, (ordered left-to-right) in the right half of the rule.
The resulting constituents can be accumulated to form the case analysis of a sentence
(Simmons 1984).
We develop augmented context-sensitive rules following the same principle. Let
us look again at the example &amp;quot;The old man from Spain ate fish,&amp;quot; this time to develop
case relations.
</bodyText>
<listItem confidence="0.575666125">
n art adj n from n vao n ; shift
art * adj n from n vao n ; shift
art adj * n from n vao n ; shift
art adj n * from n vao n ; 1 mod 2 (man Mod old)
art n * from n vao n ; 1 det 2 (man Mod old Det the)
n * from n vao n ; shift
n from * n vao n ; shift
n from n * vao n ; 3 2 1 (man Mod old Det the From spain)
</listItem>
<equation confidence="0.560737">
n * vao n ; shift
n vao * n ; 2 agt 1 (ate Agt (man Mod old ... )
vao * n ; shift
vao n * ; 1 obj 2 (ate Agt (man ...) Obj fish)
</equation>
<page confidence="0.99358">
407
</page>
<table confidence="0.998428">
Computational Linguistics Volume 18, Number 4
Stack Case-Transform
adj n n mod adj
n1 n2 n2 nmod n1
n vao 1 agt 2
n vo 1 obj 2
vbe v 1 vbe 2 vpasv
vabo n 2 ben 1 vao
n vpasv 1 obj 2
v.. prep n 3 2 1
n prep n 3 2 1
vpasv by n 1 prep 2
snt because 1 conse 2
because snt 2 ante 1
n and n 1 2 3
snt after 1 pre 2
after snt 2 post 1
</table>
<tableCaption confidence="0.926278">
Table 3
Some typical case transformations for syntactic constituents
</tableCaption>
<bodyText confidence="0.950274909090909">
In this example the case transformation immediately follows the semicolon, and the
result of the transformation is shown in parentheses further to the right. The result in
the final constituent is:
(ate Agt (man Mod old Det the From spain) Obj fish).
Note that we did not rename the syntactic constituents as NP or VP in this example,
because we were not interested in showing the phrase structure tree. Renaming in case
analysis need only be done when it is necessary to pass on information accumulated
from an earlier constituent.
For example, in &amp;quot;fish were eaten by birds,&amp;quot; the CS parse is as follows:
n n vbe ppart by n ; shift
n * vbe ppart by n ; shift
</bodyText>
<equation confidence="0.925692285714286">
n vbe * ppart by n ; shift
n vbe ppart * by n ; 1 vbe 2, vpasv (eaten Vbe were)
n vpasv * by n ; 1 obj 2 (eaten Vbe were Obj fish)
vpasv * by n ; shift
vpasv by * n ; shift
vpasv by n * ; 1 prep 2 (birds Prep by)
vpasv n * ; 2 agt 1 (eaten Vbe were Obj fish Agt (birds Prep by))
</equation>
<bodyText confidence="0.999887818181818">
Here, it was necessary to rename the combination of a past participle and its auxiliary
as a passive verb, vpasv, so that the syntactic subject and object could be recognized
as Obj and Agent, respectively. We also chose to use the argument name Prep to form
(birds Prep by) so that we could then call that constituent Agent.
We can see that the reduce operation has become a reduce-transform-rename opera-
tion where numbers refer to elements of the stack, the second term provides a case
argument label, the ordering provides a transformation, and an optional fourth ele-
ment may rename the constituent. A sample of typical case transformations is shown
associated with the top elements of the stack in Table 3. In this table, the first element
of the stack is in the third position in the left side of the table, and the number 1 refers
to that position, 2 to the second, and 3 to the first. As an aid to the reader the first two
</bodyText>
<page confidence="0.9973">
408
</page>
<note confidence="0.507688">
Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English
CS-CASE-Parser(input,cdg)
</note>
<footnote confidence="0.716474">
Input is a string of syntactic classes for the given sentence.
Cdg is the given CDG grammar rules.
</footnote>
<equation confidence="0.920445454545455">
stack := empty
outputstack := empty
do until(input = empty and 2nd(stack) = blank)
window-context := append(top_five(stack),first_five(input))
operation := consult_CDG(window-context,cdg)
if first(operation) = SHIFT
then stack := push(first(input),stack)
input := rest (input )
else stack := push(select(operation),pop(pop(stack)))
outputstack := make_constituent(operation,outputstack)
end do
</equation>
<figureCaption confidence="0.985416">
Figure 7
</figureCaption>
<bodyText confidence="0.942542333333333">
Algorithm for case parse.
entries in the table refer literally by symbol rather than by reference to the stack. The
symbols vao and vabo are subclasses of verbs that take, respectively, agent and object;
and agent, beneficiary, and object. The symbol v.. refers to any verb. Forms of the verb
be are referred to as vbe, and passivization is marked by relabeling a verb by adding
the suffix -pasv.
</bodyText>
<subsectionHeader confidence="0.73116">
Parsing case structures
</subsectionHeader>
<bodyText confidence="0.999717">
From the discussion above we may observe that the flow of control in accomplishing
a case parse is identical to that of a phrase structure parse. The difference lies in the
fact that when a constituent is recognized (see Figure 7):
</bodyText>
<listItem confidence="0.7742225">
• in phrase structure, a new name is substituted for its stack elements, and
a constituent is formed by listing the name and its elements
• in case analysis, a case transformation is applied to designated elements
on the stack to construct a constituent, and the head (i.e. the first element
of the transformation) is substituted for its elements—unless a new name
is provided for that substitution.
</listItem>
<bodyText confidence="0.9991834">
Consequently the algorithm used in phrase structure analysis is easily adapted to case
analysis. The difference lies in interpreting and applying the operation to make a new
constituent and a new stack.
In the algorithm shown above, we revise the stack by attaching either the head
of the new constituent, or its new name, to the stack resulting from the removal of
all elements in the new constituent. The function select chooses either a new name
if present, or the first element, the head of the operation. Makeconstituent applies the
transformation rule to form a new constituent from the output stack and pushes the
constituent onto the output stack, which is first reduced by removing the elements
used in the constituent. Again, the algorithm is a deterministic, first (best) path parser
</bodyText>
<page confidence="0.99679">
409
</page>
<note confidence="0.618599">
Computational Linguistics Volume 18, Number 4
</note>
<bodyText confidence="0.93972">
with behavior essentially the same as the phrase structure parser. But this version
accomplishes transformations to construct a case structure analysis.
</bodyText>
<subsectionHeader confidence="0.998322">
7.3 Acquisition System for Case Grammar
</subsectionHeader>
<bodyText confidence="0.999990083333334">
The acquisition system, like the parser, required only minor revisions to accept case
grammar. It must apply a shift or any transformation to construct the new stack-string
for the linguist user, and it must record the shift or transformation as the right half
of a context-sensitive rule—still composed of a ten-symbol left half and an operation
as the right half. Consequently, the system will be illustrated in Figure 9 rather than
described in detail.
Earlier we mentioned the context-sensitive dictionary. This is compiled by associ-
ating with each word the linguist&apos;s in-context assignments of each syntactic word class
in which it is experienced. When the dictionary is built, the occurrence frequencies of
each word class are accumulated for each word. A primitive grammar of four-tuples
terminating with each word class is also formed and hashed in a table of syntactic
paths. The procedure to determine a word class in context,
</bodyText>
<listItem confidence="0.983289875">
• first obtains the candidates from the dictionary.
• For each candidate wc, it forms a four-tuple, vec, by adding it to the cdr
of each immediately preceding vec, stored in IPC.
• Each such vec is tested against the table of syntactic paths;
- if it has been seen previously, it is added to the list of IPCs,
- otherwise it is eliminated.
• If the union of first elements of the IPC list is a single word class, that is
the choice. If not, the word&apos;s most frequent word class among the union
</listItem>
<bodyText confidence="0.979947363636364">
of surviving classes for the word is chosen.
The effect of this procedure is to examine a context of plus and minus three words
to determine the word class in question. Although a larger context based on five-
tuple paths is slightly more effective, there is a tradeoff between accuracy and storage
requirements.
The word class selection procedure was tested on the 8,310 words of the 345-
sentence sample of text. A score of 99.52% correct was achieved, with 8,270 words
correctly assigned. As a comparison, the most frequent category for a word resulted
in 8,137 correct assignments for a score of 97.52%. Although there are only 3,298
word types with an average of 3.7 tokens per type, the occurrence of single word class
usages for words in this sample is very high, thus accounting for the effectiveness of the
simpler heuristic of assignment of the most frequent category. However, since the effect
of misassignment of word class can often ruin the parse, the use of the more complex
procedure is amply justified. Analysis of the 40 errors in word class assignment showed
7 confusions of nouns and verbs that will certainly cause errors in parsing; other
confusions of adjective/noun, and adverb/preposition are less devastating, but still
serious enough to require further improvements in the procedure.
The word class selection procedure is adequate to form the prompts in the lexical
acquisition phase, but the statistics on parsing effectiveness given earlier depend on
perfect word class assignments.
Shown in Figure 8 is the system&apos;s presentation of a sentence and its requests for
each word&apos;s syntactic class. The protocol in Figure 9 shows the acquisition of shift
</bodyText>
<page confidence="0.996092">
410
</page>
<note confidence="0.600556">
Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English
</note>
<table confidence="0.881332333333333">
Lexical Acquisition: The system prompts for syntactic classes are in capitals. The user accepts the
system&apos;s prompt with a carriage return, cr or types in a syntactic class in lower case. We show user&apos;s
responses in bold-face, using cr for carriage return. Other abbreviations are wc for word class, y or n for
yes or no, and b for backup.
(THE LAUNCH OF DISCOVERY AND ITS FIVE ASTRONAUTS HAS BEEN DELAYED AT-LEAST
TWO DAYS UNTIL MARCH ELEVENTH BECAUSE-OF A CRUSHED ELECTRICAL PART ON A
</table>
<tableCaption confidence="0.968814368421053">
MAIN ENGINE COMMA OFFICIALS SAID)
process this one? - y or it y
THE cr for default else wc or b default is: ART Cr
LAUNCH cr for default else wc or b cr ;user made an error since there was no default
LAUNCH cr for default else wc or b n ;system repeated the question
OF cr for default else wc or b default is: OF cr
DISCOVERY cr for default else wc or b n
AND cr for default else wc or b default is: CONJ cr
ITS cr for default else wc or b b ;user decided to redo &amp;quot;and&amp;quot;
AND cr for default else wc or b default is: CONJ and
ITS cr for default else wc or b ppron
. .
skipping most of the sentence...
A cr for default else wc or b default is: ART cr
MAIN cr for default else wc or b n
ENGINE cr for default else wc or b n
COMMA cr for default else wc or b default is: COMMA Cr
OFFICIALS cr for default else wc or b n
SAID cr for default else wc or b vao
</tableCaption>
<figureCaption confidence="0.86967">
Figure 8
</figureCaption>
<bodyText confidence="0.980887791666667">
Illustration of dictionary acquisition.
and transformation rules for the sentence. What we notice in this second protocol is
that the stack shows syntactic labels but the input string presented to the linguist
is in English. As the system constructs a CS rule, however, the vector containing five
elements of stack and five of input string is composed entirely of syntactic classes. The
English input string better enables the linguist to maintain the meaningful context he
or she uses to analyze the sentence. About five to ten minutes were required to make
the judgments for this sentence. Appendix A shows the rules acquired in the session.
When rules for the sentence were completed, the system added the new syntactic
classes and rules to the grammar, then offered to parse the sentence. The resulting
parse is shown in Figure 10.
The case acquisition system was used on the texts described earlier in Table 1 to
accumulate 3,700 example CDG case rules. Because the case transformations refer to
three stack elements and the number of case labels is large, we expected and found that
a much larger sample of text would be required to obtain the levels of generalization
seen in the phrase structure experiments.
Accumulated in increments of 400 rules, the case curve flattens at about 2,400 rules
with an average of 33% error in prediction compared to the 20% found in analysis
of the same number of phrase structure rules. The compressed or minimal grammar
for this set of case rules reduces the 3,700 rules to 1,633, a compression ratio in this
case of 2.3 examples accounted for by each rule. The resulting compressed grammar
parses the texts with 99% accuracy. These statistics are from our initial study of a case
grammar, and they should be taken only as preliminary estimates of what a more
thorough study may show.
</bodyText>
<page confidence="0.996224">
411
</page>
<note confidence="0.457152">
Computational Linguistics Volume 18, Number 4
</note>
<tableCaption confidence="0.546405">
Case-Grammar Acquisition: The options are h for a help message, b for backup one state, s for shift,
case-trans for a case transformation, and cr for carriage return to accept a system prompt. System prompts
are capitalized in parentheses, user responses are in lower case. Where no apparent response is shown, the
user did a carriage return to accept the prompt. The first line shows the syntactic classes for the words in
the sentence.
</tableCaption>
<figure confidence="0.97517670212766">
(ART N OF N AND PPRON ADJ N VHAV VBE VAO AT-LEAST ADJ N UNTIL N N BECAUSE-OF
ART PPART ADJ N ON ART N N COMMA N VAO)
(* THE LAUNCH OF DISCOVERY AND ITS FIVE ASTRONAUTS HAS BEEN DELAYED AT-LEAST
TWO DAYS UNTIL MARCH ELEVENTH BECAUSE-OF A CRUSHED ELECTRICAL PART ON A
MAIN ENGINE COMMA OFFICIALS SAID)
options-are h b s case-trans or cr for default: (5)
(ART * LAUNCH OF DISCOVERY AND ITS ... SAID)
options-are h b s case-trans or cr for default: (S)
(ART N * OF DISCOVERY AND ITS FIVE ... SAID)
options-are h b s case-trans or cr for default: (S) 1 det 2
(N * OF DISCOVERY AND ITS FIVE ... SAID)
options-are h b s case-trans or Cr for default: (S)
: skipping several shifts
(N OF N AND PPRON ADJ N * HAS BEEN DELAYED AT-LEAST ... SAID)
options-are h b s case-trans or cr for default: (S) 1 mod 2
(N OF N AND PPRON N * HAS BEEN DELAYED AT-LEAST ... SAID)
options-are h b s case-trans or cr for default: NIL 1 possby 2
(N OF N AND N * HAS BEEN DELAYED AT-LEAST ... SAID)
options-are h b s case-trans or cr for default: NIL 3 2 1
(N OF N * HAS BEEN DELAYED AT-LEAST TWO ... SAID)
options-are h b s case-trans or cr for default: (3 2 1)
(N * HAS BEEN DELAYED AT-LEAST TWO SAID)
options-are h b s case-trans or cr for default: (S)
(N VHAV * BEEN DELAYED AT-LEAST TWO DAYS ... SAID)
options-are h b a case-trans or cr for default: (1 OBJ 2)
(N VHAV VBE * DELAYED AT-LEAST TWO DAYS ... SAID)
options-are h b s case-trans or cr for default: (S) 1 aux 2
(N VBE * DELAYED AT-LEAST TWO DAYS UNTIL ... SAID)
options-are h b s case-trans or cr for default: (S)
(N VBE VAO * AT-LEAST TWO DAYS UNTIL MARCH .. SAID)
options-are h b s case-trans or cr for default: (1 VBE 2 VAOPASV)
(N VAOPASV * AT-LEAST TWO DAYS UNTIL MARCH ... SAID)
options-are h b s case-trans or Cr for default: (1 OBJ 2)
: skipping now to BECAUSE
(VAOPASV UNTIL N * BECAUSE-OF A CRUSHED ELECTRICAL ... SAID)
options-are h b s case-trans or Cr for default: (S) 3 2 1
(VAOPASV * BECAUSE-OF A CRUSHED ELECTRICAL PART ... SAID)
options-are h b s case-trans or Cr for default: (S)
(VAOPASV BECAUSE-OF * A CRUSHED ELECTRICAL PART ON.. SAID)
options-are h b s case-trans or cr for default: NIL 1 conse 2
(BECAUSE-OF * A CRUSHED ELECTRICAL PART ON ... SAID)
options-are h b s case-trans or cr for default: NIL s
: skipping now to the end
(BECAUSE-OF COMMA N VAO *)
options-are h b s case-trans or cr for default: (1 OBJ 2) 1 agt 2
(BECAUSE-OF COMMA VAO *)
options-are h b s case-trans or cr for default: NIL 1 obj 3
</figure>
<figureCaption confidence="0.976415">
Figure 9
</figureCaption>
<bodyText confidence="0.760633">
Illustration of case grammar acquisition.
</bodyText>
<sectionHeader confidence="0.572202" genericHeader="method">
8. Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.9918775">
It seems remarkable that although the theory of context-sensitive grammars appeared
in Chomsky (1957), formal context-sensitive rules seem not to have been used pre-
</bodyText>
<page confidence="0.995737">
412
</page>
<note confidence="0.590415">
Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English
</note>
<table confidence="0.973471">
said — AGT -0- Officials
OW because-of
— CONSE -0- delayed — VBE -0- been AUX-w- has
— OBJ -0- launch— DET-&apos; the
OF-10 discovery — AND ▪ astronauts
MOD --0- five
POSSBY -0- its
— AT-LEAST -10&apos; days — MOD -0- two
— UNTIL -0- eleventh — NMOD -0- march
— ANTE -10 part — MOD -0- electrical
— MOD -0- crushed
— DET -0- a
— ON --Iv- engine
</table>
<construct confidence="0.708063">
1VMOD -0- main
DET --40- a
</construct>
<bodyText confidence="0.698121">
The launch of discovery and its five astronauts has been delayed at-least two days until
march eleventh because-of a crushed electrical part on a main engine comma officials said.
</bodyText>
<figureCaption confidence="0.484091">
Figure 10
</figureCaption>
<bodyText confidence="0.926271">
Case analysis of a sentence.
viously in computational parsing. As researchers we seem simply to have assumed,
without experimentation, that context-sensitive grammars would be too large and
cumbersome to be a practical approach to automatic parsing. In fact, context-sensitive,
binary phrase structure rules with a context composed of the preceding three stack
symbols and the next five input symbols,
stack1_3 binary-rule input1.5 operation
provide several encouraging properties.
• The linguist uses the full context of the sentence to make a simple
decision: either shift a new element onto the stack or combine the top
two elements into a phrase category.
• The system compiles a CS rule composed of ten symbols, the top five
elements of the stack and the next five elements of the input string. The
context of the embedded binary rule specializes that rule for use in
similar environments, thus providing selection criteria to the parser for
the choice of shift or reduce, and for assigning the phrase name that has
most frequently been used in similar environments. The context provides
a simple but powerful approach to preference parsing.
</bodyText>
<page confidence="0.996364">
413
</page>
<note confidence="0.432738">
Computational Linguistics Volume 18, Number 4
</note>
<listItem confidence="0.978889111111111">
• As a result, a deterministic bottom-up parser is notably successful in
finding precisely the parse tree that the linguist who constructed the
analysis of a sentence had in mind—and this is true whether the
grammar is stored as a trained neural network or in the form of
hash-table entries.
• Despite the large combinatoric space for selecting 1 of 64 symbols in
each of 10 slots in the rules-641° possible patterns—experiments in
accumulating phrase structure grammar suggest that a fairly complete
grammar will require only about 25,000 CS rules.
• It is also the case that when redundant rules are removed the CS
grammar is reduced by a factor of four and still maintains its accuracy in
parsing.
• Because of the simplicity and regular form of the rule structure, it has
proved possible to construct an acquisition system that greatly facilitates
the accumulation of grammar. The acquisition system presents contexts
and suggests operations that have previously been used with similar
contexts; thus it helps the linguist to maintain consistency of judgments.
• Parsing with context-sensitive rules generalizes from phrase structure
</listItem>
<bodyText confidence="0.991150095238095">
rewriting rules to the transformational rules required by case analysis.
Since the case analysis rules retain a regular, simple form, the acquisition
system also generalizes to case grammar.
Despite such advantageous properties, a few cautions should be noted. First, the
deterministic parsing algorithm is sufficient to apply the CDG to the sentences from
which the grammar was derived, but to accomplish effective generalization to new
sentences, a bandwidth parsing algorithm that follows multiple parsing paths is supe-
rior. Second, the 99% accuracy of the parsing will deteriorate markedly if the dictionary
lookup makes errors in word assignment. Thirdly, the shift/reduce parsing is unable
to give correct analyses for such embedded discontinuous constituents as &amp;quot;I saw the
man yesterday who ....&amp;quot; Finally, the actual parsing structures that we have presented
here are skeletal. We did not mark mood, aspect or tense of verbs, number for nouns,
or deal with long distance dependencies. We do not resolve pronoun references; and
we do not complete ellipses in conjunctive and other constructions.
Each of these shortcomings is the subject of continuing research. For the present,
the output of the case parser provides the nested, labeled, propositional structures
which, supported by a semantic knowledge base, we have customarily used to ac-
complish focus-tracking of topics through a continuous text to compute labeled out-
lines and other forms of discourse structure (Seo 1990; Rim 1990; Alterman 1985).
During this process of discourse analysis, some degapping, completion of ellipsis, and
pronoun resolution is accomplished.
</bodyText>
<subsectionHeader confidence="0.836359">
8.1 Conclusions
</subsectionHeader>
<bodyText confidence="0.9162744">
From the studies presented in this paper we conclude:
1. Context-Dependent Grammars (CDGs) are computationally and
conceptually tractable formalisms that can be composed easily by a
linguist and effectively used by a deterministic parser to compute phrase
structures and case analyses for subsets of newspaper English.
</bodyText>
<page confidence="0.996994">
414
</page>
<note confidence="0.907134">
Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English
</note>
<bodyText confidence="0.8682612">
2. The contextual portions of the CDG rules and the scoring formula that
selects the rule that best matches the parsing context allow a
deterministic parser to provide preferred parses, reflecting the linguist&apos;s
meaning-based judgments.
3. The CDG acquisition system described earlier simplifies linguistic
judgments and greatly improves a linguist&apos;s ability to construct relatively
large grammars rapidly.
4. Although a deterministic, bottom-up parser has been sufficient to
provide highly accurate parses for the 345-sentence sample of news text
studied here, we believe that a multi-path parser proves superior in its
ability to analyze sentences beyond the sample on which the grammar
was developed.
5. With 3,843 compressed CDG rules, the acquisition system is about 85%
accurate in suggesting the correct parsing for constituents from texts it
has not experienced.
</bodyText>
<listItem confidence="0.9932875">
6. For phrase structure analysis, the context-free core of the CS rules will be
99% complete when we have accumulated about 25,000 CS rules. At that
point it should be possible for a multi-path parser to find a satisfactory
analysis for almost all news story sentences.
</listItem>
<bodyText confidence="0.9998996">
We have shown that the acquisition and parsing techniques apply also to CDG
grammars for computing structures of case propositions to represent sentences. In
this application, however, much more research is needed to better define linguistic
systems for case analysis, and for their application to higher levels of natural language
understanding.
</bodyText>
<sectionHeader confidence="0.992563" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.986656666666667">
This work was partially supported by the
Army Research Office under contract
DAAG29-84-K-0060.
</bodyText>
<sectionHeader confidence="0.993099" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.999707531914893">
Alterman, Richard (1985). &amp;quot;A dictionary
based on concept coherence,&amp;quot; Artificial
Intelligence, 25,153-186.
Allen, James (1987). Natural Language
Understanding. Benjamin Cummings.
Allen, Robert (1987). &amp;quot;Several studies on
natural language and back propagation.&amp;quot;
In Proceedings, International Conference on
Neural Networks. San Diego.
Berwick, Robert C. (1985). The Acquisition of
Syntactic Knowledge, Vol. 2,335-341. MIT
Press.
Chomsky, Noam (1957). Syntactic Structures.
Mouton.
Cook, Walter (1989). Case Grammar Theory.
Georgetown University Press.
Gazdar, Gerald (1988). &amp;quot;Applicability of
indexed grammars to natural languages.&amp;quot;
In Linguistic Theory and Computer
Applications, edited by P. Whitelock, et al.,
Academic Press, 37-67.
Gazdar, Gerald, and Mellish, Chris (1989).
Natural Language Processing in LISP.
Addison-Wesley.
Gazdar, Gerald; Klein, E.; and Pullum, G.;
and Sag, I. (1985). Generalized Phrase
Structure Grammar. Harvard University
Press.
Jackendoff, Ray (1983). Semantics and
Cognition, MIT Press, Cambridge, Mass.,
1983.
Joshi, Aravind (1987). &amp;quot;An introduction to
tree-adjoining grammars&amp;quot; In Mathematics
of Language, edited by A. Manaster-Ramer,
87-114. John Benjamins.
Leow, Wee-Keng, and Simmons, R. F. (1990).
&amp;quot;A constraint satisfaction network for
case analysis,&amp;quot; Al Technical Report
AI90-129, Department of Computer
Science, University of Texas, Austin.
Marcus, M. P. (1980). A Theory of Syntactic
Recognition for Natural Language. MIT
Press.
McClelland, J. L., and Kawamoto, A. H.
(1986). &amp;quot;Mechanisms of sentence
processing: Assigning roles to
constituents.&amp;quot; In Parallel Distributed
</reference>
<page confidence="0.970999">
415
</page>
<reference confidence="0.985087652173913">
Computational Linguistics Volume 18, Number 4
Processing, Vol. 2, edited by
J. L. McClelland and D. E. Rumelhart,
MIT Press, 272-326.
Miikkulainen, Risto, and Dyer, M. (1989). &amp;quot;A
modular neural network architecture for
sequential paraphrasing of script-based
stories,&amp;quot; Artificial Intelligence Lab.,
Department of Computer Science, UCLA.
Rim, Hae-Chang (1990). Computing outlines
from descriptive texts. Doctoral dissertation,
University of Texas, Austin.
Rumelhart, David E.; Hinton, G. E.; and
Williams, R. J. (1986). &amp;quot;Learning internal
representations by error propagation.&amp;quot; In
Parallel Distributed Processing, edited by
D. E. Rumelhart and J. L. McClelland,
MIT Press, 318-362.
Schank, Roger C. (1980). &amp;quot;Language and
memory,&amp;quot; Cognitive Science, 4(3).
Seo, Jungyun (1990). Text driven construction
of discourse structures for understanding
descriptive texts. Doctoral dissertation,
University of Texas, Austin.
Sejnowski, Terrence J., and Rosenberg, C.
(1988). &amp;quot;NETtalk: A parallel network that
learns to read aloud.&amp;quot; In Neurocomputing,
edited by Anderson and Rosenfeld, MIT
Press.
Shieber, Stuart M. (1986). An Introduction to
Unification Based Approaches to Grammar.
University of Chicago Press.
Simmons, Robert F. (1984). Computations
from the English. Prentice Hall.
Simmons, Robert F., and Yu, Yeong-Ho
(1990). &amp;quot;Training a neural network to be a
context sensitive grammar.&amp;quot; In
Proceedings, 5th Rocky Mountain Al
Conference. Las Cruces, NM.
Tomita, M. (1985). Efficient Parsing for Natural
Language. Kluwer Academic Publishers.
Yu, Yeong-Ho, and Simmons, R. F. (1990).
&amp;quot;Descending epsilon in back-propagation:
A technique for better generalization,&amp;quot; in
press, Proc. Int. Jt. Conf. Neural Networks,
San Diego.
</reference>
<page confidence="0.998819">
416
</page>
<note confidence="0.851098">
Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English
</note>
<sectionHeader confidence="0.581095" genericHeader="method">
Appendix A. Rules from the Case Acquisition Session
</sectionHeader>
<bodyText confidence="0.871304">
Blanks in the 10-symbol vectors are signified by the letter B.
</bodyText>
<sectionHeader confidence="0.921984666666667" genericHeader="method">
((THE LAUNCH OF DISCOVERY AND ITS FIVE ASTRONAUTS HAS BEEN
DELAYED AT-LEAST TWO DAYS UNTIL MARCH ELEVENTH BECAUSE-OF A
CRUSHED ELECTRICAL PART ON A MAIN ENGINE COMMA OFFICIALS SAID)
</sectionHeader>
<bodyText confidence="0.7063235">
(ART N OF N AND PPRON ADJ N VHAV VBE VAO AT-LEAST ADJ N UNTIL
N N BECAUSE-OF ART PPART ADJ N ON ART N N COMMA N VAO)
</bodyText>
<figure confidence="0.934335075">
(((BBBBBARTNOFNAND) (S))
((B B B B ART N OF N AND PPRON) (S))
((B B B ART N OF N AND PPRON ADJ) (1 DET 2))
((BBBBNOFNAND PPRON ADJ) (S))
UB B B N OF N AND PPRON ADJ N) (S))
((B B N OF N AND PPRON ADJ N VHAV) (S))
((B N OF N AND PPRON ADJ N VHAV VBE) (S))
((N OF N AND PPRON ADJ N VHAV VBE VAO) (S))
((OF N AND PPRON ADJ N VHAV VBE VAO AT-LEAST) (S))
UN AND PPRON ADJ N VHAV VBE VAO AT-LEAST ADJ) (1 MOD 2))
((OF N AND PPRON N VHAV VBE VAO AT-LEAST ADJ) (1 POSSBY 2))
UN OF N AND N VHAV VBE VAO AT-LEAST ADJ) (3 2 1))
((B B N OF N VHAV VBE VAO AT-LEAST ADJ) (3 2 1))
((BBBBNVHAV VBE VAO AT-LEAST ADJ) (S))
((B B B N VHAV VBE VAO AT-LEAST ADJ N) (S))
((B B N VHAV VBE VAO AT-LEAST ADJ N UNTIL) (1 AUX 2))
((B B B N VBE VAO AT-LEAST ADJ N UNTIL) (S))
((B B N VBE VAO AT-LEAST ADJ N UNTIL N) (1 VBE 2 VAOPASV))
((B B B N VAOPASV AT-LEAST ADJ N UNTIL N) (1 OBJ 2))
((B B B B VAOPASV AT-LEAST ADJ N UNTIL N) (S))
UB B B VAOPASV AT-LEAST ADJ N UNTIL N N) (S))
((B B VAOPASV AT-LEAST ADJ N UNTIL N N BECAUSE-OF) (S))
((B VAOPASV AT-LEAST ADJ N UNTIL N N BECAUSE-OF ART) (1 MOD 2))
((B B VAOPASV AT-LEAST N UNTIL N N BECAUSE-OF ART) (3 2 1))
((B B B B VAOPASV UNTIL N N BECAUSE-OF ART) (S))
((B B B VAOPASV UNTIL N N BECAUSE-OF ART PPART) (S))
UB B VAOPASV UNTIL N N BECAUSE-OF ART PPART ADJ) (5))
UB VAOPASV UNTIL N N BECAUSE-OF ART PPART ADJ N) (1 NMOD 2))
((B B VAOPASV UNTIL N BECAUSE-OF ART PPART ADJ N) (3 2 1))
((B B B B VAOPASV BECAUSE-OF ART PPART ADJ N) (S))
((B B B VAOPASV BECAUSE-OF ART PPART ADJ N ON)
(1 CONSE 2))
((B B B B BECAUSE-OF ART PPART ADJ N ON) (S))
((B B B BECAUSE-OF ART PPART ADJ N ON ART) (S))
((B B BECAUSE-OF ART PPART ADJ N ON ART N) (S))
UB BECAUSE-OF ART PPART ADJ N ON ART N N) (S))
((BECAUSE-OF ART PPART ADJ N ON ART N N COMMA) (1 MOD 2))
((B BECAUSE-OF ART PPART N ON ART N N COMMA) (1 MOD 2))
417
Computational Linguistics Volume 18, Number 4
</figure>
<reference confidence="0.922609071428571">
((B B BECAUSE-OF ART N ON ART N N COMMA) (1 DET 2))
UB B B BECAUSE-OF N ON ART N N COMMA) (S))
((B B BECAUSE-OF N ON ART N N COMMA N) (S))
UB BECAUSE-OF N ON ART N N COMMA N VAO) (S))
((BECAUSE-OF N ON ART N N COMMA N VAO B) (S))
UN ON ART N N COMMA N VAO B B) (1 NMOD 2))
((BECAUSE-OF N ON ART N COMMA N VAO B B) (1 DET 2))
((B BECAUSE-OF N ON N COMMA N VAO B B) (3 2 1))
UB B B BECAUSE-OF N COMMA N VAO B B) (2 ANTE 1))
UB B B B BECAUSE-OF COMMA N VAO B B) (S))
((B B B BECAUSE-OF COMMA N VAO B B B) (S))
UB B BECAUSE-OF COMMA N VAO B B B B) (S))
((B BECAUSE-OF COMMANVAOBBBBB) (1 AGT 2))
UBBBECAUSE-OF COMMA VAOBBBBB) (1 OBJ 3))))
</reference>
<page confidence="0.992262">
418
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998151">The Acquisition and Use of Context-Dependent Grammars for English</title>
<author confidence="0.999071">Robert F Simmons Yeong-Ho Yut</author>
<affiliation confidence="0.971378">University of Texas University of Texas</affiliation>
<abstract confidence="0.9904555625">This paper introduces a paradigm of context-dependent grammar (CDG) and an acquisition system that, through interactive teaching sessions, accumulates the CDG rules. The resulting context-sensitive rules are used by a stack-based, shift/reduce parser to compute unambiguous syntactic structures of sentences. The acquisition system and parser have been applied to the phrase structure and case analyses of 345 sentences, mainly from newswire stories, with 99% accuracy. Extrapolation from our current grammar predicts that about 25 thousand CDG rule examples will be sufficient to train the system in phrase structure analysis of most news stories. Overall, this research concludes that CDG is a computationally and conceptually tractable approach for the construction of sentence grammar for large subsets of natural language text. An enduring goal for natural language processing (NLP) researchers has been to construct computer programs that can read narrative, descriptive texts such as newspaper stories and translate them into knowledge structures that can answer questions, classify the content, and provide summaries or other useful abstractions of the text. An essential aspect of any such NLP system is parsing—to translate the indefinitely long, recursively embedded strings of words into definite ordered structures of constituent elements. Despite decades of research, parsing remains a difficult computation that often results in incomplete, ambiguous structures; and computational grammars for natural languages remain notably incomplete. In this paper we suggest that a solution to these problems may be found in the use of context-sensitive rules applied by a deterministic shift! reduce parser. A system is described for rapid acquisition of a context-sensitive grammar based on ordinary news text. The resulting grammar is accessed by deterministic, bottomup parsers to compute phrase structure or case analyses of texts that the grammars cover. The acquisition system allows a linguist to teach a CDG grammar by showing examples of parsing successive constituents of sentences. At this writing, 16,275 example constituents have been shown to the system and used to parse 345 sentences ranging from 10 to 60 words in length achieving 99% accuracy. These examples compress to a grammar of 3,843 rules that are equally effective in parsing. Extrapolation from our data suggests that acquiring an almost complete phrase structure grammar for AP Wire text will require about 25,000 example rules. The procedure is further demonstrated to apply directly to computing superficial case analyses from English sentences.</abstract>
<note confidence="0.70041675">Department of Computer Sciences, Al Lab, University of Texas, Austin TX 78712. E-mail @cs.texas.edu t Boeing Helicopter Computer Svces, Philadelphia, PA © 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 4</note>
<abstract confidence="0.988995732954546">One of the first lessons in natural or formal language analysis is the Chomsky (1957) hierarchy of formal grammars, which classifies grammar forms from unrestricted rewrite rules, through context-sensitive, context-free, and the most restricted, regular grammars. It is usually conceded that pure, context-free grammars are not powerful enough to account for the syntactic analysis of natural languages (NL) such as English, Japanese, or Dutch, and most NL research in computational linguistics has used either augmented context-free or ad hoc grammars. The conventional wisdom is that context-sensitive grammars probably would be too large and conceptually and computationally untractable. There is also an unspoken supposition that the use of a context-sensitive grammar implies using the kind of complex parser required for parsing a fully context-sensitive language. However, NL research based on simulated neural networks took a context-based approach. One of the first hints came from the striking finding from Sejnowski and Rosenberg&apos;s NETtalk (1988), that seven-character contexts were largely sufficient to map each character of a printed word into its corresponding phoneme—where each character actually maps in various contexts into several different phonemes. For accomplishing linguistic case analyses McClelland and Kawamoto (1986) and Miikkulainen and Dyer (1989) used the entire context of phrases and sentences to map string contexts into case structures. Robert Allen (1987) mapped nine-word sentences of English into Spanish translations, and Yu and Simmons (1990) accomplished comparable context-sensitive translations between English and German simple sentences. It was apparent that the contexts in which a word occurred provided information to a neural network that was sufficient to select correct word sense and syntactic structure for otherwise ambiguous usages of language. In order to solve a problem of accepting indefinitely long, complex sentences in a fixed-size neural network, Simmons and Yu (1990) showed a method for training a network to act as a context-sensitive grammar. A sequential program accessed that grammar with a deterministic, single-path parser and accurately parsed descriptive texts. Continuing that research, 2,000 rules were accumulated and a network was trained using a back-propagation method. The training of this network required ten days of continuous computation on a Symbolics Lisp Machine. We observed that the training cost increased by more than the square of the number of training examples and calculated that 10,000-20,000 rules might well tax a supercomputer. So we decided that storing the grammar in a hash table would form a far less expensive option, provided we could define a selection algorithm comparable to that provided by the trained neural network. In this paper we describe such a selection formula to select rules for contextsensitive parsing, a system for acquiring context-sensitive rules, and experiments in analysis and application of the grammar to ordinary newspaper text. We show that the application of context-sensitive rules by a deterministic shift/reduce parser is a conceptually and computationally tractable approach to NLP that may allow us to accumulate practical grammars for large subsets of English texts. Parsing In NL research most interest has centered on context-free grammars (CFG), augmented with feature tests and transformations, used to describe the phrase structure of sentences. There is a broad literature on Generalized Phrase Structure Grammar (Gazdar et al. 1985), Unification Grammars of various types (Shieber 1986), and Augmented 392 Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English Transition Networks (J. Allen 1987). Gazdar (1988) calls attention to a subcategory of context-sensitive grammars called indexed languages and illustrates some applicability to natural languages, and Joshi illustrates an application of &amp;quot;mild context-sensitivity&amp;quot; (Joshi 1987), but in general, NL computation with context-sensitive grammars is a largely unexplored area. While a few advanced NLP laboratories have developed grammars and parsing for significantly large subsets of natural it cannot be denied that massive effort was required and that the results are plagued by ambiguous interpretations. These grammars are typically a context-free form, augmented by complex feature tests, transformations, and occasionally, arbitrary programs. The combination of even an efficient parser with such intricate grammars may greatly increase computational complexity of the parsing system (Tomita 1985). It is extremely difficult to write and maintain such grammars, and they must frequently be revised and retested to ensure internal consistency as new rules are added. We argue here that an acquisition system for accumulating context-sensitive rules and their application by a deterministic shift/reduce parser will greatly simplify the process of constructing and maintaining natural language parsing systems. Although we use context-sensitive rules of the form uXv uYv they are interpreted by a shift/reduce parser with the result that they can be applied successfully to the LR(k) subset of context-free languages. Unless the parser is augmented to include shifts in both directions, the system cannot parse context-sensitive languages. It is an open question as to whether English is or is not context-sensitive, but it definitely includes discontinuous constituents that may be separated by indefinitely many symbols. For this reason, future developments of the system may require operations beyond shift and reduce in the parser. To avoid the easy misinterpretation that our present system applies to context-sensitive languages, we call it Context- Dependent Grammar (CDG). We begin with the simple notion of a shift/reduce parser. Given a stack and an input string of symbols, the shift/reduce parser may only shift a symbol to the stack la) or reduce on the stack by rewriting them as a single symbol (Figure lb). We further constrain the parser to reduce no more than two symbols on the stack to a single symbol. The parsing terminates when the stack contains only a single root element and the input string is empty. Usually this class of parser applies a CFG to a sentence, but it is equally applicable to CDG. 2.1 CDG Rule Forms The theoretical viewpoint is that the parse of a sentence is a sequence of states, each composed of a condition of the stack and the input string. The sequence ends successfully when the stack contains only the root element (e.g. SNT), and the input string is 1 Notable examples include the large augmented CFGs at IBM Yorktown Hts, the Univ. of Pennsylvania, and the Linguistic Research Ctr. at the Univ. of Texas. 393 Computational Linguistics Volume 18, Number 4 INPUT SENTENCE t_i t_i+I t_i+2 . . . t_m NT_k ii STACK bottom INPUT SENTENCE t_i+I t_1+2 t_11+3 t_i t_ns NT _k STACK bottom t i,t m,...,t 1 are terminals. NT It is a non-terminaL (a) Shift Operation INPUT SENTENCE INPUT SENTENCE t_i t t_i+2 . . . . On Arr_k STACK bottom t_i t_i+2 . . . . NT j ii STACK bottom m,...,t 1 terminals. NT j are non-terminals. (b) Reduce Operation Figure 1 Shift/reduce parser. empty Each state can be seen as the left half of a context-sensitive rule whose right half is the succeeding state. sentences may be of any length and are often more than words, so the resulting strings and stacks would form very cumbersome rules of variable lengths. To avoid this difficulty, the stack and input parts of a rule are limited to five symbols each. In the following example the stack and input parts are separated by the symbol &amp;quot;k,&amp;quot; as the idea is applied to the sentence &amp;quot;The old man from Spain ate fish.&amp;quot; The symbol _ stands for blank, art for article, adj for adjective, p for preposition, n for noun, and v for verb. The syntactic classes are assigned by dictionary lookup in a context-sensitive dictionary.&apos; 394 Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English The old man from Spain ate fish art adj n p n v n * art adj n p n - _ _ _ art * adj npnv _art adj*npnvn - _ art adj n*pnvn __ art np*pnvn_ __np*pnvn - __npp*nvn _ np pn*vn_ _ _ _ _ np pp * v n _ _ _ np * v n - _ np v * n _ _ np v n * np vp * _ _ _ snt * The analysis terminates with an empty input string and the single symbol &amp;quot;snt&amp;quot; on the stack, successfully completing the parse. Note that the first four operations can be described as shifts followed by the two reductions, adj n np, and art np np. Subsequently the p and n were shifted onto the stack and then reduced to a pp; then the np and pp on the stack were reduced to an np, followed by the shifting of v and n, their reduction to vp, and a final reduction of np vp snt. Illustrations similar this are often used to introduce the concept of parsing in Al texts on natural language (e.g. J. Allen 1987). We could perfectly well record the grammar in pairs of successive states as follows: nppp*vn___ but some economy can be achieved by recording the operation and possible label as the right half of a rule. So for the example immediately above, we record: ___npp*nyn__—+ (5) where S shifts and (R pp) replaces the top two elements of the stack with pp to form the next state of the parse. a context ten symbols is created as the left half of a rule and an operation as the right half. Note that if the stack were limited to the top two elements, and the input to a single element, the rule system would reduce to a binary rule CFG. The example in Figure 2 shows how a sentence &amp;quot;Treatment is a complete rest and a special diet&amp;quot; is parsed by a context sensitive shift/reduce parser. Terminal symbols are lowercase, while nonterminals are uppercase. The shaded areas represent the parts 2 Described in Section 7.3.</abstract>
<note confidence="0.6676935">395 Computational Linguistics Volume 18, Number 4 Treatment is a complete rest and a special diet. ( n v det adj n cnj det adj n) Stack Input</note>
<abstract confidence="0.962065860824743">next last n v n v det det adj ✓ det adj n n v det NP n v NP U v NP cnj ✓ NP cnj det NP cnj det adj NP cnj det adj NP cnj det NP ✓ NP cnj NP NP CNP n v NP VP U v det adj n det adj n det adj n cnj det adj n cnj det adj n cnj det adj n cnj det adj n cnj det adj n cnj det adj n det adj n adj n Operation shift shift shift shift shift reduce to NP reduce to NP shift shift ft shift reduce to NP reduce to NP reduce to CNP reduce to NP reduce to VP reduce to S done Windowed Context Figure 2 An example of windowed context. of the context invisible to the system. The next operation is solely decided by the windowed context. It can be observed that the last state in the analysis is the single symbol SNT—the designated root symbol, on the stack along with an empty input string, successfully completing the parse. And this is the CDG form of rule used in the phrase structure analysis. 2.2 Algorithm for the Shift/Reduce Parser The parser accepts a string of syntactic word classes as its input and forms a tensymbol vector, five symbols each from the stack and the input string. It looks up this vector as the left half of a production in the grammar and interprets the right half of the production as an instruction to modify the stack and input sequences to construct the next state of the parse. To accomplish these tasks, it maintains two stacks, one for the input string and one for the syntactic constituents. These stacks may be arbitrarily large. An algorithm for the parser is described in Figure 3. The most important part of this algorithm is to find an applicable CDG rule from the grammar. Finding such a rule is based on the current windowed context. If there is a rule whose left side exactly matches the current windowed context, that rule will be applied. However, realistically, it is often the case that there is no exact match with any rule. Therefore, is necessary to find a rule that the current context. 396 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English Input is a string of syntactic classes for the given sentence. Cdg is the given CDG grammar rules. := until(Input = = (SNT)) Windowed-context := Append(Top_five(stack),First_five(input)) Operation := Consult_CDG(Window-context ,Cdg) = SHIFT := Push(First(Input),Stack) Input := Rest(Input) := Push(Second(Operation),Pop(Pop(Stack))) end do functions, and First_five, return lists of top (or first) five elements of the Input respectively. If there are not enough elements, these procedures pad blanks. The function two lists into one. the given CDG rules to find the next operation to take. The details of this function are the of the next section. and Pop or delete one element to/from a stack while return first or second elements of a list, respectively. the given list minus the first element. Context-sensitive shift reduce parser. 2.3 Consulting the CDG Rules There are two related issues in consulting the CDG rules. One is the computational representation of CDG rules, and the other is the method for selecting an applicable rule. In the traditional CFG paradigms, a CFG rule is applicable if the left-hand side of the rule exactly matches the top elements of the stack. However, in our CDG paradigm, a perfect match between the left side of a CDG rule and the current state cannot be assured, and in most cases, a partial match must suffice for the rule to be applied. Since rules may partially match the current context, the rule should be selected. One way to do this is to use a neural network. Through the back-propagation algorithm (Rumelhart, Hinton, and Williams 1986), a feed-forward network can be trained to memorize the CDG rules. After successful training, the network can be used to retrieve the best matching rule. However, this approach based on neural network usually takes considerable training time. For instance, in our previous experiment (Simmons and Yu 1990), training a network for about 2,000 CDG rules took several days of computation. Therefore, this approach has an intrinsic problem for scaling up, at least on the present generation of neural net simulation software. Another method is based on a hash table in which every CDG rule is stored according to its top two elements of the stack—the fourth and fifth elements of the left half of the rule. Given the current windowed context, the top two elements of the stack are used to retrieve all the relevant rules from the hash table. 397 Computational Linguistics Volume 18, Number 4 use no than 64 word and phrase class symbols, so there can be no more than 4,096 possible pairs. The effect is to divide the large number of rules into no more than 4,096 subgroups, each of which will have a manageable subset. In fact, with 16,275 rules we discovered that we have only 823 pairs and the average number of rules per subgroup is 19.8; however, for frequently occurring pairs the number of rules in the subgroups can be much larger. The problem is to determine what scoring formula should be used to find the rule that best matches a parsing context. Sejnowski and Rosenberg (1988) analyzed the weight matrix that resulted from training NETtalk and discovered a triangular function with the apex centered at the character in the window and the weights falling off in proportion to distance from that character. We decided that the best matching rule in our system would follow a similar pattern with maximum weights for the top two elements on the stack with weights decreasing in both directions with distance from those positions. The scoring function we use is developed as follows: R be the set of vectors {Ri, R2, • • • the vector • 7 C the vector [ci , • a matching function whose value is 1 Ti, 0 otherwise. is the entire set of rules, (the left half of) a particular rule, and C is the parse context. 7Z&apos; is the subset of R. where if then c4) • = of the hash table with the top two elements of the stack, produces the set R.&apos;. can now define the scoring function for each 3 10 i + i=1 i=6 The first summation scores the matches between the stack elements of the rule and the current context, and the second summation scores the matches between the elements in the input string. If two items of the rule and context match, the total score is increased by the weight assigned to that position. The maximum score for a perfect match is 21 according to the above formula. From several experiments, varying the length of vector and the weights, particularly those assigned to blanks, it has been determined that this formula gave the best performance among those tested. More importantly, it has worked well in the current phrase structure and case analysis experiments. was an unexpected surprise to that using context-sensitive productions, an elementary, deterministic, parsing algorithm proved adequate to provide 99% correct, unambiguous analyses for the entire text studied. A 3. Grammar Acquisition for CDG Constructing an augmented phrase structure grammar of whatever type—unification, GPSG, or ATN—is a painful process usually involving a well-trained linguistic team of several people. These types of grammar require that a CFG recognition rule such 3 But perhaps not to Marcus (1980) and Berwick (1985), who promote the study of deterministic parsing. 398 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English as np vp snt be supported by such additional information as the fact that the np and vp agree in number, that the np is characterized by particular features such as animate, and that the vp can or cannot accept certain types of complements. The additional features make the rules exceedingly complex and difficult to prepare and debug. College students can be taught easily to make a phrase structure tree to represent a sentence, but it requires considerable linguistic training to deal successfully with a feature grammar. We have seen in the preceding section that a CFG is derived from recording the successive states of the parses of sentences. Thus it was natural for us to develop an interactive acquisition system that would assist a linguist (or a student) in constructing such parses to produce easily large sets of example CFG rules.&apos; The system continued to evolve as a consequence of our use until we had included capabilities to: • read in text and data files • compile dictionary and grammar tables from completed text files • select a sentence to continue processing or revise • look up words in a dictionary to suggest the syntactic class for the word in context when assigning syntactic classes to the words in a sentence • compare each state of the parse with rules in the current grammar to predict the shift/reduce operation. A carriage return signals that the user accepts the prompt, or the typing in of the desired operation overrides it. • compute and display the parse tree from the local grammar after completion of each sentence, or from the global total grammar at any time • provide backing up and editing capability to correct errors • print help messages and guide the user • compile dictionary and grammar entries at the completion of each sentence, insuring no duplicate entries • save completed or partially completed grammar files. The resulting tool, GRAMAQ, enables a linguist to construct a context-sensitive grammar for a text corpus at the rate of several sentences per hour. Thousands of rules are accumulated with only weeks of effort in contrast to the years required for a comparable system of augmented CFG rules. About ten weeks of effort were required to produce the 16,275 rules on which this study is based. Since GRAMAQ&apos;s prompts become more accurate as the dictionary and grammar grow in size, there is a positive acceleration in the speed of grammar accumulation and the linguist&apos;s task gradually converges to one of alert supervision of the system&apos;s prompts. A slightly different version of GRAMAQ is Caseaq, which uses operations that create case constituents to accumulate a context-sensitive grammar that transforms 4 Starting with an Emacs editor, it was fairly easy to read in a file of sentences and to assign each word its syntactic class according to its context. Then the asterisk was inserted at the beginning of the syntactic string, the string was copied to the next line, the asterisk moved if a shift operation was indicated, or the top two symbols on the stack were rewritten if a reduce was required—just as we constructed the example in the preceding section. Naturally enough, we soon made Emacs macros to us, and then escalated to a Lisp program that would print the interpret our shift/reduce commands to produce a new state of the parse. 399 Computational Linguistics Volume 18, Number 4 Text States Sentences WdsISnt Mn-WdsISnt Hepatitis 236 12 4-19 10.3 Measles 316 10 4-25 16.3 News Story 470 10 9-51 23.5 APWire-Robots 1005 21 11-53 26.0 APWire-Rocket 1437 25 8-47 29.2 APWire-Shuttle 598 14 12-32 21.9 Total 4062 92 4-53 22.8 Table 1 Characteristics of a sample of the text corpus. sentences directly to case structures with no intermediate stage of phrase structure trees. It has the same functionality as GRAMAQ but allows the linguist user to specify a case argument and value as the transformation of syntactic elements on the stack, and to rename the head of such a constituent by a syntactic label. Figure 9 in Section 7.3 illustrates the acquisition of case grammar. 4. Experiments with CDG There are a number of critical questions that need be answered if the claim that CDG grammars are useful is to be supported. • Can they be used to obtain accurate parses for real texts? • Do they reduce ambiguity in the parsing process? • How well do the rules generalize to new texts? • How large must a CFG be to encompass the syntactic structures for most newspaper text? 4.1 Parsing and Ambiguity with CDG Over the course of this study we accumulated 345 sentences mainly from newswire texts. The first two articles were brief disease descriptions from a youth encyclopedia; the remaining fifteen were newspaper articles from February 1989 using the terms &amp;quot;star wars,&amp;quot; &amp;quot;SDI,&amp;quot; or &amp;quot;Strategic Defense Initiative.&amp;quot; Table 1 characterizes typical articles by the number of CDG rules or states, number of sentences, the range of sentence lengths, and the average number of words per sentence. We developed our approach to acquiring and parsing context-sensitive grammars the first two simple texts, then used GRAMAQ to redo those texts and to construct productions for the news stories. The total text numbered 345 sentences, which accumulated 16,275 context-sensitive rules—an average of 47 per sentence. The parser embodying the algorithm illustrated earlier in Figure 1 was augmented to compare the constituents it constructed with those prescribed during grammar acquisition by the linguist. In parsing the 345 sentences, 335 parses exactly matched the linguist&apos;s original judgement. In nine cases in which differences occurred, the parses were judged correct, but slightly different sequences of parse states occurred. The tenth case clearly made an attachment error—of an introductory adverbial phrase in the sentence &amp;quot;Hours later, Baghdad announced....&amp;quot; This was mistakenly attached to &amp;quot;Baghdad.&amp;quot; This evaluation shows that the grammar was in precise agreement with 400 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English Another mission soon scheduled that also would have priority over the shuttle is the first firing of a trident two intercontinental range missile from a submerged submarine. art another mission NP soon paprt — scheduled relpron — that adv RELSNT also VP modal — ( VP n — priority have over „the gut n shuttle vbe — is art the NP &lt; prprt — firing of NP &lt; trident two intercontinental range missile from rt submarinesubmerged Figure 4 Sentence parse. the linguist 97% of the time and completed correct parses in 99.7% of the 345 sentences from which it was derived. Since our primary interest was in evaluating the effectiveness of the CDG, all these evaluations were based on using correct syntactic classes the words in the sentences. The context-sensitive dictionary lookup procedure described in Section 7.3 is 99.5% accurate, but it assigns 40 word classes incorrectly. As a consequence, using this procedure would result in a reduction of about 10% accuracy in parsing. An output of a sentence from the parser is displayed as a tree in Figure 4. Since the whole mechanism is coded in Lisp, the actual output of the system is a nested list that is then printed as a tree. Notice in this figure that the PP at the bottom modifies the NP composed of &amp;quot;the first firing of a trident two intercontinental range missile&amp;quot; not just the word &amp;quot;firing.&amp;quot; Since the parsing is bottom-up, left-to-right, the constituents are formed in the natural order of words encountered in the sentence and the terminals of the tree can be read top-to-bottom to give their ordering in the sentence. Although 345 sentences totaling 8594 words is a small selection from the infinite set of possible English sentences, it is large enough to assure us that the CDG is a reasonable form of grammar. Since the deterministic parsing algorithm selects a single interpretation, which we have seen almost perfectly agrees with the linguist&apos;s parsings, it is apparent that, at least for this size text sample, there is little difficulty with ambiguous interpretations. NP VP adv SNT VP adj-- NP &lt; PP —••• 401 Computational Linguistics Volume 18, Number 4 5. Generalization of CDG The purpose of accumulating sample rules from texts is to achieve a grammar general enough to analyze new texts it has never seen. To be useful, the grammar must generalize. There are at least three aspects of generalization to be considered. • How well does the grammar generalize at the sentence level? That is, how well does the grammar parse new sentences that it has not previously experienced? • How well does the grammar generalize at the operation level? That is, how well does the grammar predict the correct Shift/Reduce operation during acquisition of new sentences? • How much does the rule retention strategy affect generalization? For instance, when the grammar predicts the same output as a new rule does, and the new rule is not saved, how well does the resulting grammar parse? 5.1 Generalization at the Sentence Level The complete parse of a sentence is a sequence of states recognized by the grammar (whether it be CDG or any other). If all the constituents of the new sentence can be recognized, the new sentence can be parsed correctly. It will be seen in a later paragraph that with 16,275 rules, the grammar predicts the output of new rules correctly about 85% of the time. For the average sentence with 47 states, only 85% or about 40 states can be expected to be predicted correctly; consequently the deterministic parse will frequently fail. In fact, 5 of 14 new sentences parsed correctly in a brief experiment that used a grammar based on 320 sentences to attempt to parse the new, 20-sentence text. Considering that only a single path was followed by the deterministic parser, we predicted that a multiple-path parser would perform somewhat better for this aspect of generalization. In fact, our initial experiments with a beam search parser resulted in successful parses of 15 of the 20 new sentences using the same grammar based on the 320 sentences. 5.2 Generalization at the Operation Level This level of generalization is of central significance to the grammar acquisition system. When GRAMAQ looks up a state in the grammar it finds the best matching state with the same top two elements on the stack, and offers the right half of this rule as its suggestion to the linguist. How often is this prediction correct? To answer this question we compiled the grammar of 16,275 rules in cumulative of 1,017 rules using a procedure, would only add a rule to the grammar if the grammar did not already predict its operation. We call the result a &amp;quot;minimal-grammar,&amp;quot; and it contains 3,843 rules. The black line of Figure 5 shows that with the first 1,000 rules 40% were new; with an accumulation of 5,000, 18% were new rules. By the time 16,000 rules have been accumulated, the curve has flattened to an average of 16% new rules added. This means that the acquisition system will make correct prompts about 84% of the time and the linguist will only need to correct the suggestions about 3 or 4 times in context presentations. 402 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Accumulated Rules by Thousands Figure 5 Generalization of CDG rules. 5.3 Rule Retention and Generalization If two parsing grammars account equally well for the same sentences, the one with fewer rules is less redundant, more abstract, and the one to be preferred. We used the to produce and study the minimal grammar for the 16,275 rules (rule-examples) derived from the sample text. Union-grammar records a new for a 1. if best matching rule has an operation that doesn&apos;t match 2. if best matching rule ties with another rule whose operation does not match 3. if 2 is true, and score = 21 we have a full contradiction and list the rule as an error. Six contradictions occurred in the grammar; five were inconsistent treatments of &amp;quot;SNT&amp;quot; followed by one or more punctuation marks, while the sixth offered both a shift and a &amp;quot;pp&amp;quot; for a preposition-noun followed by a preposition. The latter case is an attachment ambiguity not resolvable by syntax. In the first pass as shown in Table 2, the text resulted in 3,194 rules compared with 16,275 possible rules. That is, 13,081 possible CDG rules were not retained because already existing rules would match and predict the operation. However, using those rules to parse the same text gave very poor results: zero correct parses at the sentence level. Therefore, the process of compiling a minimal grammar was repeated starting with those 3,194 rules. This time only 619 new rules were added. The purpose of this 5 These definite conditions are due to an analysis by Mark Ring. i + CFO r</abstract>
<note confidence="0.765612166666667">50 40 403 Computational Linguistics Volume 18, Number 4 Pass linretained Retained Total Rules 1 13081 3194 16275</note>
<phone confidence="0.664317">2 15656 619 16275 3 16245 18 16275 4 16275 0 16275</phone>
<abstract confidence="0.996370030303031">Table 2 Four passes with minimal grammar. repetition is to get rid of the effect that the rules added later change the predictions made earlier. Finally, in a fourth repetition of the process no rules were new. The resulting grammar of 3,843 rules succeeds in parsing the text with only occasional minor errors in attaching constituents. It is to be emphasized that the unretained are not identical to those in the minimal grammar. We can observe that this technique of minimal retention by &amp;quot;unioning&amp;quot; new rules to the grammar results in a compression of the order 16,275/3,843 or 4.2 to 1, without increase in error. If this ratio holds for larger grammars, then if the linguist accumulates 40,000 training-example rules to account for the syntax of a given subset of language, that grammar can be compressed automatically to about 10,000 rules that will accomplish the same task. 6. Predicting the Size of CDGs When any kind of acquisition system is used to accumulate knowledge, one very interesting question is, when will the knowledge be complete enough for the intended application? In our case, how many CDG rules will be sufficient to cover almost all newswire stories? To answer this question, an extrapolation can be used to find a point when the solid line of Figure 5 intersects with the y-axis. However, the CDG curve is descending too slowly to make a reliable extrapolation. Therefore, another question was investigated instead: when will the CDG rules include a complete set of CFG rules? Note that a CDG rule is equivalent to a CFG rule if the context is limited to the top two elements of the stack. What the other elements in the context accomplish is to make one rule preferable to another that has the same top two elements of the stack, but a different context. allow 64 symbols in our phrase structure analysis. That means, there are possible combinations for the top two elements of the stack. For each combination, are 65 possible a shift or a reduction to another symbol. Among 16,275 CDG rules, we studied how many different CFG rules can be derived by eliminating the context. We found 844 different CFG rules that used 600 different left-side pairs of symbols. This shows that a given context free pair of symbols averages 1.4 different operations.&apos; Then, as we did with CDG rules, we measured how many new CFG rules were added in an accumulative fashion. The shaded line of Figure 5 shows the result. 6 Actually, there are fewer than 65 possible operations since the stack elements can be reduced only to We actually use only 48 different symbols, so only or 2,304 combinations could have occurred. The fraction 600/2,304 yields .26, the proportion of the combinatoric space that is actually used, so far. 404 Extrapolation, the gray line, predicts that 99% of the context free pairs will be achieved with the accumulation of 25,000 context sensitive rules. Figure 6 Log-log plot of new CFG rules. that the line descended to about 1.5% errors at 16,000 rules. To make an extrapolation easier, a log-log graph shows the same data in Figure 6. From this graph, it can be predicted that, after about 25,000 CDG rules are accumulated, the grammar will encompass a CFG component that is 99% complete. Beyond this point, additional CDG rules will add almost no new CFG rules, but only fine-tune the grammar so that it can resolve ambiguities more effectively. Also, it is our belief that, after the CDG reaches that point, a multi-path, beamsearch parser will be able to parse most newswire stories very reliably. This belief is based on our initial experiment that used a beam search parser to test generalization of the grammar to find parses for fifteen out of twenty new sentences. 7. Acquiring Case Grammar Explicating the phrase structure constituents of sentences is an essential aspect in computer recognition of meaning. Case analysis organizes the constituents into a hierarchical structure of labeled propositions. The propositions can be used directly to answer questions and are the basis of schemas, scripts, and frames that are used to add meaning to otherwise inexplicit texts. As a result of the experiments with acquiring CDG and exploring its properties for parsing phrase structures, we became fairly confident that we could generalize the system to acquisition and parsing based on a that would compute syntactic case structures syntactic strings. Direct translation from string to structure is supported by neural network experiments such as those by McClelland and Kawamoto (1986), Miikkulainen and Dyer (1989), Yu and Simmons (1990), and Leow and Simmons (1990). We reasoned that if we could acquire case grammar with something approaching the simplicity of acquiring phrase structure rules, the result could be of great value for NL applications.</abstract>
<note confidence="0.649656125">E Simmons and Yeong-Ho Yu Grammars for English 100 /0,000 Nbr of Accumulated Rules 405 Computational Linguistics Volume 18, Number 4 7.1 Case Structure Cook (1989) reviewed twenty years of linguistic research on case analysis of natural</note>
<abstract confidence="0.985492709172259">language sentences. He synthesized the various theories into a system that depends on the subclassification of verbs into twelve categories, and it is apparent from his review that with a fine subcategorization of verbs and nominals, case analysis can be accomplished as a purely syntactic operation—subject to the limitations of attachment ambiguities that are not resolvable by syntax. This conclusion is somewhat at variance with those Al approaches that require a syntactic analysis to be followed by a semantic operation that filters and transforms syntactic constituents to compute case-labeled propositions (e.g. Rim 1990), but it is consistent with the neural network experience of directly mapping from sentence to case structure, and with the AT research that seeks to integrate syntactic and semantic processing while translating sentences to propositional structures. Linguistic theories of case structure have been concerned only with single propositions headed by verb predications; they have been largely silent with regard to the structure of noun phrases and the relations among embedded and sequential propositions. Additional conventions for managing these complications have been developed in Simmons (1984) and Alterman (1985) and are used here. The central notion of a case analysis is to translate sentence strings into a nested structure of case relations (or predicates) where each relation has a head term and an indefinite number of labeled arguments. An argument may itself be a case relation. Thus a sentence, as in the examples below, forms a tree of case relations. The old man from Spain ate fish. (man Mod old From spain) Obj fish) Another mission scheduled soon is the first firing of a trident from a submerged submarine. (is Obj1 (mission Mod another Obj* (scheduled Vmod soon)) Obj2 (firing Mod first Det the Of (missile Nmod trident Det a) From (submarine Mod submerged Det a))) that in to means the object of the expression can be read as &amp;quot;another mission that is scheduled soon.&amp;quot; An asterisk as a suffix to a label always signals the reverse direction for the label. is a small set of case relations for verb arguments, such as object, beneficiary, experiencer, location, state, time, direction, For nouns there modifier, quantifier, amount, nounmodifier, preposition, reverse verb obj*, ben*, Prepositions and conjunctions are usually used directly argument labels while sentence conjunctions such as while, before, after, are represented as heads of propositions that relate two other propositions with the post, antecedent, example, &amp;quot;Because she ate fish and chips earlier, Mary was not hungry.&amp;quot; (because Ante (ate Agt she Obj (fish And chips) Vmod Conse (was Vmod not Objl mary State hungry)) are subcategorized as vabo, vo, va, vhav, vbe a is agent, o is object, is beneficiary and vhav is a form of vbe a form of far, only the 406 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English of been necessary in subcategorizing nouns to accomplish this form of case analysis, but in general, a lexical semantics is required to resolve syntactic attachment ambiguities. The complete set of case relations is presumed to be small, but no one has yet claimed a complete enumeration of them. Other case systems such as those taught by Schank (1980) and Jackendoff (1983) predicate names into such primitives as Event, Thing, Mtrans, Ptrans, Go, to approximate some form of &amp;quot;language of thought&amp;quot; but the present approach is less ambitious, proposing merely to represent in a fairly formal fashion the organization of the words in a sentence. Subsequent operations on this admittedly superficial class of case structures, when augmented with a system of shallow lexical semantics, have been shown to accomplish question answering, focus tracking of topics throughout a text, automatic outlining, and summarization of texts (Seo 1990; Rim 1990). One strong constraint on this type of analysis is that the resulting case must maintain present in the text so that the text may be exactly reconstituted from the analysis. 7.2 Syntactic Analysis of Case Structure We&apos;ve seen earlier that a shift/reduce-rename operation is sufficient to parse most sentences into phrase structures. Case structure, however, requires transformations in addition to these operations. To form a case structure it is frequently necessary to change the order of constituents and to insert case labels. Following jackendoff&apos;s of constraint, argues essentially that semantic interpretation is frequently reflected in the syntactic form, case transformations are accomplished as syntactic constituent is discovered. Thus when a verb, say an NP, say on top of the stack, one must not only create a VP, but also decide the form the constituent, Obj coconuts). can be accomplished in customary approaches to parsing by using augmented context free recognition rules of the form: VP VP NP / 1 obj 2 where the numbers following the slash refer to the text dominated by the syntactic class in the referenced position, (ordered left-to-right) in the right half of the rule. The resulting constituents can be accumulated to form the case analysis of a sentence (Simmons 1984). We develop augmented context-sensitive rules following the same principle. Let us look again at the example &amp;quot;The old man from Spain ate fish,&amp;quot; this time to develop case relations. n art adj n from n vao n ; shift art * adj n from n vao n ; shift art adj * n from n vao n ; shift art adj n * from n vao n ; 1 mod 2 (man Mod old) n * from n vao n ; 1 det 2 old Det the) n * from n vao n ; shift n from * n vao n ; shift n from n * vao n ; 3 2 1 (man Mod old Det the From spain) n * vao n ; shift n vao * n ; 2 agt 1 (ate Agt (man Mod old ... ) vao * n ; shift vao n * ; 1 obj 2 (ate Agt (man ...) Obj fish) 407 Computational Linguistics Volume 18, Number 4 Stack Case-Transform adj n n mod adj n1 n2 n2 nmod n1 n vao 1 agt 2 n vo 1 obj 2 vbe v 1 vbe 2 vpasv vabo n 2 ben 1 vao n vpasv 1 obj 2 v.. prep n 3 2 1 n prep n 3 2 1 vpasv by n 1 prep 2 snt because 1 conse 2 because snt 2 ante 1 n and n 1 2 3 snt after 1 pre 2 after snt 2 post 1 Some typical case transformations for syntactic constituents In this example the case transformation immediately follows the semicolon, and the result of the transformation is shown in parentheses further to the right. The result in the final constituent is: (ate Agt (man Mod old Det the From spain) Obj fish). Note that we did not rename the syntactic constituents as NP or VP in this example, we not interested in showing the phrase structure tree. Renaming in case analysis need only be done when it is necessary to pass on information accumulated from an earlier constituent. For example, in &amp;quot;fish were eaten by birds,&amp;quot; the CS parse is as follows: n n vbe ppart by n ; shift n * vbe ppart by n ; shift n vbe * ppart by n ; shift n vbe ppart * by n ; 1 vbe 2, vpasv (eaten Vbe were) vpasv * n ; obj 2 (eaten Vbe were Obj fish) * by n shift vpasv by * n ; shift vpasv by n * ; 1 prep 2 (birds Prep by) n * 2 agt 1 (eaten Vbe were Obj fish Agt (birds Prep by)) Here, it was necessary to rename the combination of a past participle and its auxiliary a passive verb, that the syntactic subject and object could be recognized We also chose to use the argument name form Prep by) that we could then call that constituent can see that the has become a operation where numbers refer to elements of the stack, the second term provides a case argument label, the ordering provides a transformation, and an optional fourth element may rename the constituent. A sample of typical case transformations is shown associated with the top elements of the stack in Table 3. In this table, the first element of the stack is in the third position in the left side of the table, and the number 1 refers that position, to the second, and 3 to the first. As an aid to the the first two 408 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English Input is a string of syntactic classes for the given sentence. is the given CDG := := until(input = = := append(top_five(stack),first_five(input)) := if first(operation) = SHIFT then stack := push(first(input),stack) input := rest (input ) else stack := push(select(operation),pop(pop(stack))) outputstack := make_constituent(operation,outputstack) end do Algorithm for case parse. entries in the table refer literally by symbol rather than by reference to the stack. The subclasses of verbs that take, respectively, agent and object; and agent, beneficiary, and object. The symbol v.. refers to any verb. Forms of the verb referred to as passivization is marked by relabeling a verb by adding suffix Parsing case structures From the discussion above we may observe that the flow of control in accomplishing a case parse is identical to that of a phrase structure parse. The difference lies in the that when a constituent is recognized (see Figure • in phrase structure, a new name is substituted for its stack elements, and a constituent is formed by listing the name and its elements • in case analysis, a case transformation is applied to designated elements on the stack to construct a constituent, and the head (i.e. the first element of the transformation) is substituted for its elements—unless a new name is provided for that substitution. Consequently the algorithm used in phrase structure analysis is easily adapted to case analysis. The difference lies in interpreting and applying the operation to make a new constituent and a new stack. In the algorithm shown above, we revise the stack by attaching either the head of the new constituent, or its new name, to the stack resulting from the removal of elements in the new constituent. The function either a new name present, or the first element, the head of the operation. the transformation rule to form a new constituent from the output stack and pushes the constituent onto the output stack, which is first reduced by removing the elements used in the constituent. Again, the algorithm is a deterministic, first (best) path parser 409 Computational Linguistics Volume 18, Number 4 with behavior essentially the same as the phrase structure parser. But this version accomplishes transformations to construct a case structure analysis. 7.3 Acquisition System for Case Grammar The acquisition system, like the parser, required only minor revisions to accept case grammar. It must apply a shift or any transformation to construct the new stack-string for the linguist user, and it must record the shift or transformation as the right half of a context-sensitive rule—still composed of a ten-symbol left half and an operation as the right half. Consequently, the system will be illustrated in Figure 9 rather than described in detail. Earlier we mentioned the context-sensitive dictionary. This is compiled by associating with each word the linguist&apos;s in-context assignments of each syntactic word class in which it is experienced. When the dictionary is built, the occurrence frequencies of each word class are accumulated for each word. A primitive grammar of four-tuples terminating with each word class is also formed and hashed in a table of syntactic paths. The procedure to determine a word class in context, • first obtains the candidates from the dictionary. • For each candidate wc, it forms a four-tuple, vec, by adding it to the cdr of each immediately preceding vec, stored in IPC. • Each such vec is tested against the table of syntactic paths; it has been seen previously, it is added to the list of IPCs, it is eliminated. • If the union of first elements of the IPC list is a single word class, that is the choice. If not, the word&apos;s most frequent word class among the union of surviving classes for the word is chosen. The effect of this procedure is to examine a context of plus and minus three words to determine the word class in question. Although a larger context based on fivetuple paths is slightly more effective, there is a tradeoff between accuracy and storage requirements. The word class selection procedure was tested on the 8,310 words of the 345sentence sample of text. A score of 99.52% correct was achieved, with 8,270 words correctly assigned. As a comparison, the most frequent category for a word resulted in 8,137 correct assignments for a score of 97.52%. Although there are only 3,298 word types with an average of 3.7 tokens per type, the occurrence of single word class usages for words in this sample is very high, thus accounting for the effectiveness of the simpler heuristic of assignment of the most frequent category. However, since the effect of misassignment of word class can often ruin the parse, the use of the more complex procedure is amply justified. Analysis of the 40 errors in word class assignment showed 7 confusions of nouns and verbs that will certainly cause errors in parsing; other confusions of adjective/noun, and adverb/preposition are less devastating, but still serious enough to require further improvements in the procedure. The word class selection procedure is adequate to form the prompts in the lexical acquisition phase, but the statistics on parsing effectiveness given earlier depend on perfect word class assignments. Shown in Figure 8 is the system&apos;s presentation of a sentence and its requests for word&apos;s syntactic class. The in Figure 9 shows the acquisition of shift 410 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English Lexical Acquisition: The system prompts for syntactic classes are in capitals. The user accepts the system&apos;s prompt with a carriage return, cr or types in a syntactic class in lower case. We show user&apos;s responses in bold-face, using cr for carriage return. Other abbreviations are wc for word class, y or n for yes or no, and b for backup. (THE LAUNCH OF DISCOVERY AND ITS FIVE ASTRONAUTS HAS BEEN DELAYED AT-LEAST TWO DAYS UNTIL MARCH ELEVENTH BECAUSE-OF A CRUSHED ELECTRICAL PART ON A MAIN ENGINE COMMA OFFICIALS SAID) this one? y or it THE cr for default else wc or b default is: ART Cr LAUNCH cr for default else wc or b cr ;user made an error since there was no default LAUNCH cr for default else wc or b n ;system repeated the question OF cr for default else wc or b default is: OF cr cr for default else wc or n cr for default else wc or b default is: CONJ cr for default else wc or b decided to redo &amp;quot;and&amp;quot; cr for default else wc or b default is: CONJ cr for default else wc or b . . skipping most of the sentence... cr for default else wc or b default is: cr cr for default else wc or b cr for default else wc or b cr for default else wc or b default is: COMMA cr for default else wc or b cr for default else wc or b Figure 8 Illustration of dictionary acquisition. and transformation rules for the sentence. What we notice in this second protocol is that the stack shows syntactic labels but the input string presented to the linguist is in English. As the system constructs a CS rule, however, the vector containing five elements of stack and five of input string is composed entirely of syntactic classes. The English input string better enables the linguist to maintain the meaningful context he or she uses to analyze the sentence. About five to ten minutes were required to make the judgments for this sentence. Appendix A shows the rules acquired in the session. When rules for the sentence were completed, the system added the new syntactic classes and rules to the grammar, then offered to parse the sentence. The resulting parse is shown in Figure 10. The case acquisition system was used on the texts described earlier in Table 1 to accumulate 3,700 example CDG case rules. Because the case transformations refer to three stack elements and the number of case labels is large, we expected and found that a much larger sample of text would be required to obtain the levels of generalization seen in the phrase structure experiments. Accumulated in increments of 400 rules, the case curve flattens at about 2,400 rules with an average of 33% error in prediction compared to the 20% found in analysis of the same number of phrase structure rules. The compressed or minimal grammar for this set of case rules reduces the 3,700 rules to 1,633, a compression ratio in this of examples for by each rule. The compressed grammar parses the texts with 99% accuracy. These statistics are from our initial study of a case grammar, and they should be taken only as preliminary estimates of what a more thorough study may show. 411 Computational Linguistics Volume 18, Number 4 Case-Grammar Acquisition: The options are h for a help message, b for backup one state, s for shift, case-trans for a case transformation, and cr for carriage return to accept a system prompt. System prompts are capitalized in parentheses, user responses are in lower case. Where no apparent response is shown, the user did a carriage return to accept the prompt. The first line shows the syntactic classes for the words in the sentence. (ART N OF N AND PPRON ADJ N VHAV VBE VAO AT-LEAST ADJ N UNTIL N N BECAUSE-OF ART PPART ADJ N ON ART N N COMMA N VAO) (* THE LAUNCH OF DISCOVERY AND ITS FIVE ASTRONAUTS HAS BEEN DELAYED AT-LEAST TWO DAYS UNTIL MARCH ELEVENTH BECAUSE-OF A CRUSHED ELECTRICAL PART ON A MAIN ENGINE COMMA OFFICIALS SAID) options-are h b s case-trans or cr for default: (5) (ART * LAUNCH OF DISCOVERY AND ITS ... SAID) options-are h b s case-trans or cr for default: (S) (ART N * OF DISCOVERY AND ITS FIVE ... SAID) options-are h b s case-trans or cr for default: (S) 1 det 2 (N * OF DISCOVERY AND ITS FIVE ... SAID) options-are h b s case-trans or Cr for default: (S) : skipping several shifts (N OF N AND PPRON ADJ N * HAS BEEN DELAYED AT-LEAST ... SAID) options-are h b s case-trans or cr for default: (S) 1 mod 2 (N OF N AND PPRON N * HAS BEEN DELAYED AT-LEAST ... SAID) options-are h b s case-trans or cr for default: NIL 1 possby 2 (N OF N AND N * HAS BEEN DELAYED AT-LEAST ... SAID) options-are h b s case-trans or cr for default: NIL 3 2 1 (N OF N * HAS BEEN DELAYED AT-LEAST TWO ... SAID) options-are h b s case-trans or cr for default: (3 2 1) (N * HAS BEEN DELAYED AT-LEAST TWO SAID) options-are h b s case-trans or cr for default: (S) (N VHAV * BEEN DELAYED AT-LEAST TWO DAYS ... SAID) options-are h b a case-trans or cr for default: (1 OBJ 2) (N VHAV VBE * DELAYED AT-LEAST TWO DAYS ... SAID) h b s case-trans or cr for default: (S) 1 aux (N VBE * DELAYED AT-LEAST TWO DAYS UNTIL ... SAID) options-are h b s case-trans or cr for default: (S) VBE VAO * AT-LEAST TWO DAYS UNTIL .. options-are h b s case-trans or cr for default: (1 VBE 2 VAOPASV) (N VAOPASV * AT-LEAST TWO DAYS UNTIL MARCH ... SAID) options-are h b s case-trans or Cr for default: (1 OBJ 2) skipping now BECAUSE (VAOPASV UNTIL N * BECAUSE-OF A CRUSHED ELECTRICAL ... SAID) h b s case-trans or Cr for default: (S) 2 (VAOPASV * BECAUSE-OF A CRUSHED ELECTRICAL PART ... SAID) options-are h b s case-trans or Cr for default: (S) (VAOPASV BECAUSE-OF * A CRUSHED ELECTRICAL PART ON.. SAID) options-are h b s case-trans or cr for default: NIL 1 conse 2 (BECAUSE-OF * A CRUSHED ELECTRICAL PART ON ... SAID) options-are h b s case-trans or cr for default: NIL s : skipping now to the end (BECAUSE-OF COMMA N VAO *) options-are h b s case-trans or cr for default: (1 OBJ 2) 1 agt 2 (BECAUSE-OF COMMA VAO *) options-are h b s case-trans or cr for default: NIL 1 obj 3 Figure 9 Illustration of case grammar acquisition. 8. Discussion and Conclusions It seems remarkable that although the theory of context-sensitive grammars appeared Chomsky (1957), formal context-sensitive rules seem not to have been used pre- 412 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English — Officials CONSE — -0-been has OBJ discovery — AND five its AT-LEAST — UNTIL -0- — march ANTE -10part — MOD -0-electrical MOD -0-crushed DET -0-a ON engine -0-main The launch of discovery and its five astronauts has been delayed at-least two days until march eleventh because-of a crushed electrical part on a main engine comma officials said. Figure 10 Case analysis of a sentence. viously in computational parsing. As researchers we seem simply to have assumed, without experimentation, that context-sensitive grammars would be too large and cumbersome to be a practical approach to automatic parsing. In fact, context-sensitive, binary phrase structure rules with a context composed of the preceding three stack symbols and the next five input symbols, provide several encouraging properties. • The linguist uses the full context of the sentence to make a simple decision: either shift a new element onto the stack or combine the top two elements into a phrase category. • The system compiles a CS rule composed of ten symbols, the top five elements of the stack and the next five elements of the input string. The context of the embedded binary rule specializes that rule for use in similar environments, thus providing selection criteria to the parser for choice of for assigning the phrase name that has most frequently been used in similar environments. The context provides simple but powerful approach to parsing. 413 Computational Linguistics Volume 18, Number 4 • As a result, a deterministic bottom-up parser is notably successful in finding precisely the parse tree that the linguist who constructed the analysis of a sentence had in mind—and this is true whether the grammar is stored as a trained neural network or in the form of hash-table entries. • Despite the large combinatoric space for selecting 1 of 64 symbols in of 10 slots in the possible patterns—experiments in accumulating phrase structure grammar suggest that a fairly complete grammar will require only about 25,000 CS rules. • It is also the case that when redundant rules are removed the CS grammar is reduced by a factor of four and still maintains its accuracy in parsing. • Because of the simplicity and regular form of the rule structure, it has proved possible to construct an acquisition system that greatly facilitates the accumulation of grammar. The acquisition system presents contexts and suggests operations that have previously been used with similar contexts; thus it helps the linguist to maintain consistency of judgments. • Parsing with context-sensitive rules generalizes from phrase structure rewriting rules to the transformational rules required by case analysis. Since the case analysis rules retain a regular, simple form, the acquisition system also generalizes to case grammar. Despite such advantageous properties, a few cautions should be noted. First, the deterministic parsing algorithm is sufficient to apply the CDG to the sentences from which the grammar was derived, but to accomplish effective generalization to new sentences, a bandwidth parsing algorithm that follows multiple parsing paths is superior. Second, the 99% accuracy of the parsing will deteriorate markedly if the dictionary lookup makes errors in word assignment. Thirdly, the shift/reduce parsing is unable to give correct analyses for such embedded discontinuous constituents as &amp;quot;I saw the man yesterday who ....&amp;quot; Finally, the actual parsing structures that we have presented here are skeletal. We did not mark mood, aspect or tense of verbs, number for nouns, or deal with long distance dependencies. We do not resolve pronoun references; and we do not complete ellipses in conjunctive and other constructions. Each of these shortcomings is the subject of continuing research. For the present, the output of the case parser provides the nested, labeled, propositional structures which, supported by a semantic knowledge base, we have customarily used to accomplish focus-tracking of topics through a continuous text to compute labeled outlines and other forms of discourse structure (Seo 1990; Rim 1990; Alterman 1985). During this process of discourse analysis, some degapping, completion of ellipsis, and pronoun resolution is accomplished. 8.1 Conclusions From the studies presented in this paper we conclude: 1. Context-Dependent Grammars (CDGs) are computationally and conceptually tractable formalisms that can be composed easily by a linguist and effectively used by a deterministic parser to compute phrase structures and case analyses for subsets of newspaper English. 414 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English 2. The contextual portions of the CDG rules and the scoring formula that selects the rule that best matches the parsing context allow a parser to provide reflecting the linguist&apos;s meaning-based judgments. 3. The CDG acquisition system described earlier simplifies linguistic judgments and greatly improves a linguist&apos;s ability to construct relatively large grammars rapidly. 4. Although a deterministic, bottom-up parser has been sufficient to provide highly accurate parses for the 345-sentence sample of news text studied here, we believe that a multi-path parser proves superior in its ability to analyze sentences beyond the sample on which the grammar was developed. 5. With 3,843 compressed CDG rules, the acquisition system is about 85% accurate in suggesting the correct parsing for constituents from texts it has not experienced. 6. For phrase structure analysis, the context-free core of the CS rules will be 99% complete when we have accumulated about 25,000 CS rules. At that point it should be possible for a multi-path parser to find a satisfactory analysis for almost all news story sentences. We have shown that the acquisition and parsing techniques apply also to CDG grammars for computing structures of case propositions to represent sentences. In this application, however, much more research is needed to better define linguistic systems for case analysis, and for their application to higher levels of natural language understanding.</abstract>
<note confidence="0.966778684210527">Acknowledgments This work was partially supported by the Army Research Office under contract DAAG29-84-K-0060. References Alterman, Richard (1985). &amp;quot;A dictionary on concept coherence,&amp;quot; James (1987). Language Cummings. Allen, Robert (1987). &amp;quot;Several studies on natural language and back propagation.&amp;quot; International Conference on Networks. Diego. Robert C. (1985). Acquisition of Knowledge, 2,335-341. MIT Press. Noam (1957). Structures. Mouton. Walter (1989). Grammar Theory.</note>
<affiliation confidence="0.835099">Georgetown University Press.</affiliation>
<address confidence="0.766091">Gazdar, Gerald (1988). &amp;quot;Applicability of</address>
<title confidence="0.829802">indexed grammars to natural languages.&amp;quot; Theory and Computer</title>
<author confidence="0.848008">by P Whitelock</author>
<note confidence="0.874613727272727">Academic Press, 37-67. Gazdar, Gerald, and Mellish, Chris (1989). Natural Language Processing in LISP. Addison-Wesley. Gazdar, Gerald; Klein, E.; and Pullum, G.; Sag, I. (1985). Phrase Grammar. University Press. Ray (1983). and Press, Cambridge, Mass., 1983.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Richard Alterman</author>
</authors>
<title>A dictionary based on concept coherence,&amp;quot;</title>
<date>1985</date>
<journal>Artificial Intelligence,</journal>
<pages>25--153</pages>
<contexts>
<context position="40573" citStr="Alterman (1985)" startWordPosition="6752" endWordPosition="6753">ut it is consistent with the neural network experience of directly mapping from sentence to case structure, and with the AT research that seeks to integrate syntactic and semantic processing while translating sentences to propositional structures. Linguistic theories of case structure have been concerned only with single propositions headed by verb predications; they have been largely silent with regard to the structure of noun phrases and the relations among embedded and sequential propositions. Additional conventions for managing these complications have been developed in Simmons (1984) and Alterman (1985) and are used here. The central notion of a case analysis is to translate sentence strings into a nested structure of case relations (or predicates) where each relation has a head term and an indefinite number of labeled arguments. An argument may itself be a case relation. Thus a sentence, as in the examples below, forms a tree of case relations. The old man from Spain ate fish. (eat Agt (man Mod old From spain) Obj fish) Another mission scheduled soon is the first firing of a trident missile from a submerged submarine. (is Obj1 (mission Mod another Obj* (scheduled Vmod soon)) Obj2 (firing Mo</context>
<context position="64561" citStr="Alterman 1985" startWordPosition="10949" endWordPosition="10950"> mark mood, aspect or tense of verbs, number for nouns, or deal with long distance dependencies. We do not resolve pronoun references; and we do not complete ellipses in conjunctive and other constructions. Each of these shortcomings is the subject of continuing research. For the present, the output of the case parser provides the nested, labeled, propositional structures which, supported by a semantic knowledge base, we have customarily used to accomplish focus-tracking of topics through a continuous text to compute labeled outlines and other forms of discourse structure (Seo 1990; Rim 1990; Alterman 1985). During this process of discourse analysis, some degapping, completion of ellipsis, and pronoun resolution is accomplished. 8.1 Conclusions From the studies presented in this paper we conclude: 1. Context-Dependent Grammars (CDGs) are computationally and conceptually tractable formalisms that can be composed easily by a linguist and effectively used by a deterministic parser to compute phrase structures and case analyses for subsets of newspaper English. 414 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English 2. The contextual portions of the CDG rules and the scoring for</context>
</contexts>
<marker>Alterman, 1985</marker>
<rawString>Alterman, Richard (1985). &amp;quot;A dictionary based on concept coherence,&amp;quot; Artificial Intelligence, 25,153-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Natural Language Understanding. Benjamin Cummings.</title>
<date>1987</date>
<contexts>
<context position="4527" citStr="Allen (1987)" startWordPosition="661" endWordPosition="662">NL research based on simulated neural networks took a context-based approach. One of the first hints came from the striking finding from Sejnowski and Rosenberg&apos;s NETtalk (1988), that seven-character contexts were largely sufficient to map each character of a printed word into its corresponding phoneme—where each character actually maps in various contexts into several different phonemes. For accomplishing linguistic case analyses McClelland and Kawamoto (1986) and Miikkulainen and Dyer (1989) used the entire context of phrases and sentences to map string contexts into case structures. Robert Allen (1987) mapped nine-word sentences of English into Spanish translations, and Yu and Simmons (1990) accomplished comparable context-sensitive translations between English and German simple sentences. It was apparent that the contexts in which a word occurred provided information to a neural network that was sufficient to select correct word sense and syntactic structure for otherwise ambiguous usages of language. In order to solve a problem of accepting indefinitely long, complex sentences in a fixed-size neural network, Simmons and Yu (1990) showed a method for training a network to act as a context-</context>
<context position="6816" citStr="Allen 1987" startWordPosition="1002" endWordPosition="1003">lly and computationally tractable approach to NLP that may allow us to accumulate practical grammars for large subsets of English texts. 2. Context-Dependent Parsing In NL research most interest has centered on context-free grammars (CFG), augmented with feature tests and transformations, used to describe the phrase structure of sentences. There is a broad literature on Generalized Phrase Structure Grammar (Gazdar et al. 1985), Unification Grammars of various types (Shieber 1986), and Augmented 392 Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English Transition Networks (J. Allen 1987). Gazdar (1988) calls attention to a subcategory of context-sensitive grammars called indexed languages and illustrates some applicability to natural languages, and Joshi illustrates an application of &amp;quot;mild context-sensitivity&amp;quot; (Joshi 1987), but in general, NL computation with context-sensitive grammars is a largely unexplored area. While a few advanced NLP laboratories have developed grammars and parsing capabilities for significantly large subsets of natural language,1 it cannot be denied that massive effort was required and that the results are plagued by ambiguous interpretations. These gr</context>
<context position="12064" citStr="Allen 1987" startWordPosition="1910" endWordPosition="1911">The analysis terminates with an empty input string and the single symbol &amp;quot;snt&amp;quot; on the stack, successfully completing the parse. Note that the first four operations can be described as shifts followed by the two reductions, adj n np, and art np np. Subsequently the p and n were shifted onto the stack and then reduced to a pp; then the np and pp on the stack were reduced to an np, followed by the shifting of v and n, their reduction to vp, and a final reduction of np vp snt. Illustrations similar to this are often used to introduce the concept of parsing in Al texts on natural language (e.g. J. Allen 1987). We could perfectly well record the grammar in pairs of successive states as follows: nppp*vn___ but some economy can be achieved by recording the operation and possible label as the right half of a rule. So for the example immediately above, we record: ___npp*nyn__—+ (5) nppn*vn___--(R pp) where S shifts and (R pp) replaces the top two elements of the stack with pp to form the next state of the parse. Thus a windowed context of ten symbols is created as the left half of a rule and an operation as the right half. Note that if the stack were limited to the top two elements, and the input to a </context>
</contexts>
<marker>Allen, 1987</marker>
<rawString>Allen, James (1987). Natural Language Understanding. Benjamin Cummings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Allen</author>
</authors>
<title>Several studies on natural language and back propagation.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, International Conference on Neural Networks.</booktitle>
<location>San Diego.</location>
<contexts>
<context position="4527" citStr="Allen (1987)" startWordPosition="661" endWordPosition="662">NL research based on simulated neural networks took a context-based approach. One of the first hints came from the striking finding from Sejnowski and Rosenberg&apos;s NETtalk (1988), that seven-character contexts were largely sufficient to map each character of a printed word into its corresponding phoneme—where each character actually maps in various contexts into several different phonemes. For accomplishing linguistic case analyses McClelland and Kawamoto (1986) and Miikkulainen and Dyer (1989) used the entire context of phrases and sentences to map string contexts into case structures. Robert Allen (1987) mapped nine-word sentences of English into Spanish translations, and Yu and Simmons (1990) accomplished comparable context-sensitive translations between English and German simple sentences. It was apparent that the contexts in which a word occurred provided information to a neural network that was sufficient to select correct word sense and syntactic structure for otherwise ambiguous usages of language. In order to solve a problem of accepting indefinitely long, complex sentences in a fixed-size neural network, Simmons and Yu (1990) showed a method for training a network to act as a context-</context>
<context position="6816" citStr="Allen 1987" startWordPosition="1002" endWordPosition="1003">lly and computationally tractable approach to NLP that may allow us to accumulate practical grammars for large subsets of English texts. 2. Context-Dependent Parsing In NL research most interest has centered on context-free grammars (CFG), augmented with feature tests and transformations, used to describe the phrase structure of sentences. There is a broad literature on Generalized Phrase Structure Grammar (Gazdar et al. 1985), Unification Grammars of various types (Shieber 1986), and Augmented 392 Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English Transition Networks (J. Allen 1987). Gazdar (1988) calls attention to a subcategory of context-sensitive grammars called indexed languages and illustrates some applicability to natural languages, and Joshi illustrates an application of &amp;quot;mild context-sensitivity&amp;quot; (Joshi 1987), but in general, NL computation with context-sensitive grammars is a largely unexplored area. While a few advanced NLP laboratories have developed grammars and parsing capabilities for significantly large subsets of natural language,1 it cannot be denied that massive effort was required and that the results are plagued by ambiguous interpretations. These gr</context>
<context position="12064" citStr="Allen 1987" startWordPosition="1910" endWordPosition="1911">The analysis terminates with an empty input string and the single symbol &amp;quot;snt&amp;quot; on the stack, successfully completing the parse. Note that the first four operations can be described as shifts followed by the two reductions, adj n np, and art np np. Subsequently the p and n were shifted onto the stack and then reduced to a pp; then the np and pp on the stack were reduced to an np, followed by the shifting of v and n, their reduction to vp, and a final reduction of np vp snt. Illustrations similar to this are often used to introduce the concept of parsing in Al texts on natural language (e.g. J. Allen 1987). We could perfectly well record the grammar in pairs of successive states as follows: nppp*vn___ but some economy can be achieved by recording the operation and possible label as the right half of a rule. So for the example immediately above, we record: ___npp*nyn__—+ (5) nppn*vn___--(R pp) where S shifts and (R pp) replaces the top two elements of the stack with pp to form the next state of the parse. Thus a windowed context of ten symbols is created as the left half of a rule and an operation as the right half. Note that if the stack were limited to the top two elements, and the input to a </context>
</contexts>
<marker>Allen, 1987</marker>
<rawString>Allen, Robert (1987). &amp;quot;Several studies on natural language and back propagation.&amp;quot; In Proceedings, International Conference on Neural Networks. San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Berwick</author>
</authors>
<date>1985</date>
<journal>The Acquisition of Syntactic Knowledge,</journal>
<volume>Vol.</volume>
<pages>2--335</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="20851" citStr="Berwick (1985)" startWordPosition="3465" endWordPosition="3466"> in the current phrase structure and case analysis experiments. It was an unexpected surprise to us3 that using context-sensitive productions, an elementary, deterministic, parsing algorithm proved adequate to provide 99% correct, unambiguous analyses for the entire text studied. A 3. Grammar Acquisition for CDG Constructing an augmented phrase structure grammar of whatever type—unification, GPSG, or ATN—is a painful process usually involving a well-trained linguistic team of several people. These types of grammar require that a CFG recognition rule such 3 But perhaps not to Marcus (1980) and Berwick (1985), who promote the study of deterministic parsing. 398 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English as np vp snt be supported by such additional information as the fact that the np and vp agree in number, that the np is characterized by particular features such as count, animate, etc., and that the vp can or cannot accept certain types of complements. The additional features make the rules exceedingly complex and difficult to prepare and debug. College students can be taught easily to make a phrase structure tree to represent a sentence, but it requires considerable </context>
</contexts>
<marker>Berwick, 1985</marker>
<rawString>Berwick, Robert C. (1985). The Acquisition of Syntactic Knowledge, Vol. 2,335-341. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Syntactic Structures.</title>
<date>1957</date>
<publisher>Mouton.</publisher>
<contexts>
<context position="3120" citStr="Chomsky (1957)" startWordPosition="460" endWordPosition="461">sing. Extrapolation from our data suggests that acquiring an almost complete phrase structure grammar for AP Wire text will require about 25,000 example rules. The procedure is further demonstrated to apply directly to computing superficial case analyses from English sentences. * Department of Computer Sciences, Al Lab, University of Texas, Austin TX 78712. E-mail @cs.texas.edu t Boeing Helicopter Computer Svces, Philadelphia, PA © 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 4 One of the first lessons in natural or formal language analysis is the Chomsky (1957) hierarchy of formal grammars, which classifies grammar forms from unrestricted rewrite rules, through context-sensitive, context-free, and the most restricted, regular grammars. It is usually conceded that pure, context-free grammars are not powerful enough to account for the syntactic analysis of natural languages (NL) such as English, Japanese, or Dutch, and most NL research in computational linguistics has used either augmented context-free or ad hoc grammars. The conventional wisdom is that context-sensitive grammars probably would be too large and conceptually and computationally untract</context>
<context position="60025" citStr="Chomsky (1957)" startWordPosition="10218" endWordPosition="10219">(VAOPASV BECAUSE-OF * A CRUSHED ELECTRICAL PART ON.. SAID) options-are h b s case-trans or cr for default: NIL 1 conse 2 (BECAUSE-OF * A CRUSHED ELECTRICAL PART ON ... SAID) options-are h b s case-trans or cr for default: NIL s : skipping now to the end (BECAUSE-OF COMMA N VAO *) options-are h b s case-trans or cr for default: (1 OBJ 2) 1 agt 2 (BECAUSE-OF COMMA VAO *) options-are h b s case-trans or cr for default: NIL 1 obj 3 Figure 9 Illustration of case grammar acquisition. 8. Discussion and Conclusions It seems remarkable that although the theory of context-sensitive grammars appeared in Chomsky (1957), formal context-sensitive rules seem not to have been used pre412 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English said — AGT -0- Officials OW because-of — CONSE -0- delayed — VBE -0- been AUX-w- has — OBJ -0- launch— DET-&apos; the OF-10 discovery — AND ▪ astronauts MOD --0- five POSSBY -0- its — AT-LEAST -10&apos; days — MOD -0- two — UNTIL -0- eleventh — NMOD -0- march — ANTE -10 part — MOD -0- electrical — MOD -0- crushed — DET -0- a — ON --Iv- engine 1VMOD -0- main DET --40- a The launch of discovery and its five astronauts has been delayed at-least two days until march ele</context>
</contexts>
<marker>Chomsky, 1957</marker>
<rawString>Chomsky, Noam (1957). Syntactic Structures. Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Cook</author>
</authors>
<title>Case Grammar Theory.</title>
<date>1989</date>
<publisher>Georgetown University Press.</publisher>
<contexts>
<context position="39252" citStr="Cook (1989)" startWordPosition="6558" endWordPosition="6559">irect translation from string to structure is supported by neural network experiments such as those by McClelland and Kawamoto (1986), Miikkulainen and Dyer (1989), Yu and Simmons (1990), and Leow and Simmons (1990). We reasoned that if we could acquire case grammar with something approaching the simplicity of acquiring phrase structure rules, the result could be of great value for NL applications. Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English 100 1,000 /0,000 25,000 100,000 Nbr of Accumulated Rules 405 Computational Linguistics Volume 18, Number 4 7.1 Case Structure Cook (1989) reviewed twenty years of linguistic research on case analysis of natural language sentences. He synthesized the various theories into a system that depends on the subclassification of verbs into twelve categories, and it is apparent from his review that with a fine subcategorization of verbs and nominals, case analysis can be accomplished as a purely syntactic operation—subject to the limitations of attachment ambiguities that are not resolvable by syntax. This conclusion is somewhat at variance with those Al approaches that require a syntactic analysis to be followed by a semantic operation </context>
</contexts>
<marker>Cook, 1989</marker>
<rawString>Cook, Walter (1989). Case Grammar Theory. Georgetown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
</authors>
<title>Applicability of indexed grammars to natural languages.&amp;quot;</title>
<date>1988</date>
<booktitle>In Linguistic Theory and Computer Applications,</booktitle>
<pages>37--67</pages>
<publisher>Academic Press,</publisher>
<note>edited by</note>
<contexts>
<context position="6831" citStr="Gazdar (1988)" startWordPosition="1004" endWordPosition="1005">tationally tractable approach to NLP that may allow us to accumulate practical grammars for large subsets of English texts. 2. Context-Dependent Parsing In NL research most interest has centered on context-free grammars (CFG), augmented with feature tests and transformations, used to describe the phrase structure of sentences. There is a broad literature on Generalized Phrase Structure Grammar (Gazdar et al. 1985), Unification Grammars of various types (Shieber 1986), and Augmented 392 Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English Transition Networks (J. Allen 1987). Gazdar (1988) calls attention to a subcategory of context-sensitive grammars called indexed languages and illustrates some applicability to natural languages, and Joshi illustrates an application of &amp;quot;mild context-sensitivity&amp;quot; (Joshi 1987), but in general, NL computation with context-sensitive grammars is a largely unexplored area. While a few advanced NLP laboratories have developed grammars and parsing capabilities for significantly large subsets of natural language,1 it cannot be denied that massive effort was required and that the results are plagued by ambiguous interpretations. These grammars are typi</context>
</contexts>
<marker>Gazdar, 1988</marker>
<rawString>Gazdar, Gerald (1988). &amp;quot;Applicability of indexed grammars to natural languages.&amp;quot; In Linguistic Theory and Computer Applications, edited by P. Whitelock, et al., Academic Press, 37-67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Chris Mellish</author>
</authors>
<title>Natural Language Processing in LISP.</title>
<date>1989</date>
<publisher>Addison-Wesley.</publisher>
<marker>Gazdar, Mellish, 1989</marker>
<rawString>Gazdar, Gerald, and Mellish, Chris (1989). Natural Language Processing in LISP. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="6635" citStr="Gazdar et al. 1985" startWordPosition="975" endWordPosition="978">eriments in analysis and application of the grammar to ordinary newspaper text. We show that the application of context-sensitive rules by a deterministic shift/reduce parser is a conceptually and computationally tractable approach to NLP that may allow us to accumulate practical grammars for large subsets of English texts. 2. Context-Dependent Parsing In NL research most interest has centered on context-free grammars (CFG), augmented with feature tests and transformations, used to describe the phrase structure of sentences. There is a broad literature on Generalized Phrase Structure Grammar (Gazdar et al. 1985), Unification Grammars of various types (Shieber 1986), and Augmented 392 Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English Transition Networks (J. Allen 1987). Gazdar (1988) calls attention to a subcategory of context-sensitive grammars called indexed languages and illustrates some applicability to natural languages, and Joshi illustrates an application of &amp;quot;mild context-sensitivity&amp;quot; (Joshi 1987), but in general, NL computation with context-sensitive grammars is a largely unexplored area. While a few advanced NLP laboratories have developed grammars and parsing capabiliti</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, Gerald; Klein, E.; and Pullum, G.; and Sag, I. (1985). Generalized Phrase Structure Grammar. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantics and Cognition,</title>
<date>1983</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.,</location>
<contexts>
<context position="42947" citStr="Jackendoff (1983)" startWordPosition="7144" endWordPosition="7145"> vao, vabo, vo, va, vhav, vbe where a is agent, o is object, b is beneficiary and vhav is a form of have and vbe a form of be. So far, only the 406 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English subcategory of time has been necessary in subcategorizing nouns to accomplish this form of case analysis, but in general, a lexical semantics is required to resolve syntactic attachment ambiguities. The complete set of case relations is presumed to be small, but no one has yet claimed a complete enumeration of them. Other case systems such as those taught by Schank (1980) and Jackendoff (1983) classify predicate names into such primitives as Do, Event, Thing, Mtrans, Ptrans, Go, Action, etc., to approximate some form of &amp;quot;language of thought&amp;quot; but the present approach is less ambitious, proposing merely to represent in a fairly formal fashion the organization of the words in a sentence. Subsequent operations on this admittedly superficial class of case structures, when augmented with a system of shallow lexical semantics, have been shown to accomplish question answering, focus tracking of topics throughout a text, automatic outlining, and summarization of texts (Seo 1990; Rim 1990). </context>
</contexts>
<marker>Jackendoff, 1983</marker>
<rawString>Jackendoff, Ray (1983). Semantics and Cognition, MIT Press, Cambridge, Mass., 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
</authors>
<title>An introduction to tree-adjoining grammars&amp;quot;</title>
<date>1987</date>
<journal>In Mathematics of Language, edited by A. Manaster-Ramer,</journal>
<pages>87--114</pages>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="7056" citStr="Joshi 1987" startWordPosition="1032" endWordPosition="1033">augmented with feature tests and transformations, used to describe the phrase structure of sentences. There is a broad literature on Generalized Phrase Structure Grammar (Gazdar et al. 1985), Unification Grammars of various types (Shieber 1986), and Augmented 392 Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English Transition Networks (J. Allen 1987). Gazdar (1988) calls attention to a subcategory of context-sensitive grammars called indexed languages and illustrates some applicability to natural languages, and Joshi illustrates an application of &amp;quot;mild context-sensitivity&amp;quot; (Joshi 1987), but in general, NL computation with context-sensitive grammars is a largely unexplored area. While a few advanced NLP laboratories have developed grammars and parsing capabilities for significantly large subsets of natural language,1 it cannot be denied that massive effort was required and that the results are plagued by ambiguous interpretations. These grammars are typically a context-free form, augmented by complex feature tests, transformations, and occasionally, arbitrary programs. The combination of even an efficient parser with such intricate grammars may greatly increase computational</context>
</contexts>
<marker>Joshi, 1987</marker>
<rawString>Joshi, Aravind (1987). &amp;quot;An introduction to tree-adjoining grammars&amp;quot; In Mathematics of Language, edited by A. Manaster-Ramer, 87-114. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee-Keng Leow</author>
<author>R F Simmons</author>
</authors>
<title>A constraint satisfaction network for case analysis,&amp;quot; Al</title>
<date>1990</date>
<tech>Technical Report AI90-129,</tech>
<institution>Department of Computer Science, University of Texas,</institution>
<location>Austin.</location>
<contexts>
<context position="38856" citStr="Leow and Simmons (1990)" startWordPosition="6496" endWordPosition="6499">and are the basis of schemas, scripts, and frames that are used to add meaning to otherwise inexplicit texts. As a result of the experiments with acquiring CDG and exploring its properties for parsing phrase structures, we became fairly confident that we could generalize the system to acquisition and parsing based on a grammar that would compute syntactic case structures directly from syntactic strings. Direct translation from string to structure is supported by neural network experiments such as those by McClelland and Kawamoto (1986), Miikkulainen and Dyer (1989), Yu and Simmons (1990), and Leow and Simmons (1990). We reasoned that if we could acquire case grammar with something approaching the simplicity of acquiring phrase structure rules, the result could be of great value for NL applications. Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English 100 1,000 /0,000 25,000 100,000 Nbr of Accumulated Rules 405 Computational Linguistics Volume 18, Number 4 7.1 Case Structure Cook (1989) reviewed twenty years of linguistic research on case analysis of natural language sentences. He synthesized the various theories into a system that depends on the subclassification of verbs into twelve c</context>
</contexts>
<marker>Leow, Simmons, 1990</marker>
<rawString>Leow, Wee-Keng, and Simmons, R. F. (1990). &amp;quot;A constraint satisfaction network for case analysis,&amp;quot; Al Technical Report AI90-129, Department of Computer Science, University of Texas, Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="20832" citStr="Marcus (1980)" startWordPosition="3462" endWordPosition="3463">it has worked well in the current phrase structure and case analysis experiments. It was an unexpected surprise to us3 that using context-sensitive productions, an elementary, deterministic, parsing algorithm proved adequate to provide 99% correct, unambiguous analyses for the entire text studied. A 3. Grammar Acquisition for CDG Constructing an augmented phrase structure grammar of whatever type—unification, GPSG, or ATN—is a painful process usually involving a well-trained linguistic team of several people. These types of grammar require that a CFG recognition rule such 3 But perhaps not to Marcus (1980) and Berwick (1985), who promote the study of deterministic parsing. 398 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English as np vp snt be supported by such additional information as the fact that the np and vp agree in number, that the np is characterized by particular features such as count, animate, etc., and that the vp can or cannot accept certain types of complements. The additional features make the rules exceedingly complex and difficult to prepare and debug. College students can be taught easily to make a phrase structure tree to represent a sentence, but it req</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Marcus, M. P. (1980). A Theory of Syntactic Recognition for Natural Language. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L McClelland</author>
<author>A H Kawamoto</author>
</authors>
<title>Mechanisms of sentence processing: Assigning roles to constituents.&amp;quot;</title>
<date>1986</date>
<booktitle>In Parallel Distributed Computational Linguistics Volume 18, Number 4 Processing,</booktitle>
<volume>2</volume>
<pages>272--326</pages>
<publisher>MIT Press,</publisher>
<note>edited by</note>
<contexts>
<context position="4380" citStr="McClelland and Kawamoto (1986)" startWordPosition="635" endWordPosition="638">n supposition that the use of a context-sensitive grammar implies using the kind of complex parser required for parsing a fully context-sensitive language. However, NL research based on simulated neural networks took a context-based approach. One of the first hints came from the striking finding from Sejnowski and Rosenberg&apos;s NETtalk (1988), that seven-character contexts were largely sufficient to map each character of a printed word into its corresponding phoneme—where each character actually maps in various contexts into several different phonemes. For accomplishing linguistic case analyses McClelland and Kawamoto (1986) and Miikkulainen and Dyer (1989) used the entire context of phrases and sentences to map string contexts into case structures. Robert Allen (1987) mapped nine-word sentences of English into Spanish translations, and Yu and Simmons (1990) accomplished comparable context-sensitive translations between English and German simple sentences. It was apparent that the contexts in which a word occurred provided information to a neural network that was sufficient to select correct word sense and syntactic structure for otherwise ambiguous usages of language. In order to solve a problem of accepting ind</context>
<context position="38774" citStr="McClelland and Kawamoto (1986)" startWordPosition="6483" endWordPosition="6486">cture of labeled propositions. The propositions can be used directly to answer questions and are the basis of schemas, scripts, and frames that are used to add meaning to otherwise inexplicit texts. As a result of the experiments with acquiring CDG and exploring its properties for parsing phrase structures, we became fairly confident that we could generalize the system to acquisition and parsing based on a grammar that would compute syntactic case structures directly from syntactic strings. Direct translation from string to structure is supported by neural network experiments such as those by McClelland and Kawamoto (1986), Miikkulainen and Dyer (1989), Yu and Simmons (1990), and Leow and Simmons (1990). We reasoned that if we could acquire case grammar with something approaching the simplicity of acquiring phrase structure rules, the result could be of great value for NL applications. Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English 100 1,000 /0,000 25,000 100,000 Nbr of Accumulated Rules 405 Computational Linguistics Volume 18, Number 4 7.1 Case Structure Cook (1989) reviewed twenty years of linguistic research on case analysis of natural language sentences. He synthesized the various t</context>
</contexts>
<marker>McClelland, Kawamoto, 1986</marker>
<rawString>McClelland, J. L., and Kawamoto, A. H. (1986). &amp;quot;Mechanisms of sentence processing: Assigning roles to constituents.&amp;quot; In Parallel Distributed Computational Linguistics Volume 18, Number 4 Processing, Vol. 2, edited by J. L. McClelland and D. E. Rumelhart, MIT Press, 272-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Risto Miikkulainen</author>
<author>M Dyer</author>
</authors>
<title>A modular neural network architecture for sequential paraphrasing of script-based stories,&amp;quot;</title>
<date>1989</date>
<journal>Artificial Intelligence</journal>
<institution>Lab., Department of Computer Science, UCLA.</institution>
<contexts>
<context position="4413" citStr="Miikkulainen and Dyer (1989)" startWordPosition="640" endWordPosition="644">text-sensitive grammar implies using the kind of complex parser required for parsing a fully context-sensitive language. However, NL research based on simulated neural networks took a context-based approach. One of the first hints came from the striking finding from Sejnowski and Rosenberg&apos;s NETtalk (1988), that seven-character contexts were largely sufficient to map each character of a printed word into its corresponding phoneme—where each character actually maps in various contexts into several different phonemes. For accomplishing linguistic case analyses McClelland and Kawamoto (1986) and Miikkulainen and Dyer (1989) used the entire context of phrases and sentences to map string contexts into case structures. Robert Allen (1987) mapped nine-word sentences of English into Spanish translations, and Yu and Simmons (1990) accomplished comparable context-sensitive translations between English and German simple sentences. It was apparent that the contexts in which a word occurred provided information to a neural network that was sufficient to select correct word sense and syntactic structure for otherwise ambiguous usages of language. In order to solve a problem of accepting indefinitely long, complex sentences</context>
<context position="38804" citStr="Miikkulainen and Dyer (1989)" startWordPosition="6487" endWordPosition="6490">he propositions can be used directly to answer questions and are the basis of schemas, scripts, and frames that are used to add meaning to otherwise inexplicit texts. As a result of the experiments with acquiring CDG and exploring its properties for parsing phrase structures, we became fairly confident that we could generalize the system to acquisition and parsing based on a grammar that would compute syntactic case structures directly from syntactic strings. Direct translation from string to structure is supported by neural network experiments such as those by McClelland and Kawamoto (1986), Miikkulainen and Dyer (1989), Yu and Simmons (1990), and Leow and Simmons (1990). We reasoned that if we could acquire case grammar with something approaching the simplicity of acquiring phrase structure rules, the result could be of great value for NL applications. Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English 100 1,000 /0,000 25,000 100,000 Nbr of Accumulated Rules 405 Computational Linguistics Volume 18, Number 4 7.1 Case Structure Cook (1989) reviewed twenty years of linguistic research on case analysis of natural language sentences. He synthesized the various theories into a system that dep</context>
</contexts>
<marker>Miikkulainen, Dyer, 1989</marker>
<rawString>Miikkulainen, Risto, and Dyer, M. (1989). &amp;quot;A modular neural network architecture for sequential paraphrasing of script-based stories,&amp;quot; Artificial Intelligence Lab., Department of Computer Science, UCLA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hae-Chang Rim</author>
</authors>
<title>Computing outlines from descriptive texts. Doctoral dissertation,</title>
<date>1990</date>
<institution>University of Texas,</institution>
<location>Austin.</location>
<contexts>
<context position="39955" citStr="Rim 1990" startWordPosition="6662" endWordPosition="6663">He synthesized the various theories into a system that depends on the subclassification of verbs into twelve categories, and it is apparent from his review that with a fine subcategorization of verbs and nominals, case analysis can be accomplished as a purely syntactic operation—subject to the limitations of attachment ambiguities that are not resolvable by syntax. This conclusion is somewhat at variance with those Al approaches that require a syntactic analysis to be followed by a semantic operation that filters and transforms syntactic constituents to compute case-labeled propositions (e.g. Rim 1990), but it is consistent with the neural network experience of directly mapping from sentence to case structure, and with the AT research that seeks to integrate syntactic and semantic processing while translating sentences to propositional structures. Linguistic theories of case structure have been concerned only with single propositions headed by verb predications; they have been largely silent with regard to the structure of noun phrases and the relations among embedded and sequential propositions. Additional conventions for managing these complications have been developed in Simmons (1984) a</context>
<context position="43545" citStr="Rim 1990" startWordPosition="7236" endWordPosition="7237">doff (1983) classify predicate names into such primitives as Do, Event, Thing, Mtrans, Ptrans, Go, Action, etc., to approximate some form of &amp;quot;language of thought&amp;quot; but the present approach is less ambitious, proposing merely to represent in a fairly formal fashion the organization of the words in a sentence. Subsequent operations on this admittedly superficial class of case structures, when augmented with a system of shallow lexical semantics, have been shown to accomplish question answering, focus tracking of topics throughout a text, automatic outlining, and summarization of texts (Seo 1990; Rim 1990). One strong constraint on this type of analysis is that the resulting case structure must maintain all information present in the text so that the text may be exactly reconstituted from the analysis. 7.2 Syntactic Analysis of Case Structure We&apos;ve seen earlier that a shift/reduce-rename operation is sufficient to parse most sentences into phrase structures. Case structure, however, requires transformations in addition to these operations. To form a case structure it is frequently necessary to change the order of constituents and to insert case labels. Following jackendoff&apos;s principle of gramma</context>
<context position="64545" citStr="Rim 1990" startWordPosition="10947" endWordPosition="10948">We did not mark mood, aspect or tense of verbs, number for nouns, or deal with long distance dependencies. We do not resolve pronoun references; and we do not complete ellipses in conjunctive and other constructions. Each of these shortcomings is the subject of continuing research. For the present, the output of the case parser provides the nested, labeled, propositional structures which, supported by a semantic knowledge base, we have customarily used to accomplish focus-tracking of topics through a continuous text to compute labeled outlines and other forms of discourse structure (Seo 1990; Rim 1990; Alterman 1985). During this process of discourse analysis, some degapping, completion of ellipsis, and pronoun resolution is accomplished. 8.1 Conclusions From the studies presented in this paper we conclude: 1. Context-Dependent Grammars (CDGs) are computationally and conceptually tractable formalisms that can be composed easily by a linguist and effectively used by a deterministic parser to compute phrase structures and case analyses for subsets of newspaper English. 414 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English 2. The contextual portions of the CDG rules and</context>
</contexts>
<marker>Rim, 1990</marker>
<rawString>Rim, Hae-Chang (1990). Computing outlines from descriptive texts. Doctoral dissertation, University of Texas, Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>G E Hinton</author>
<author>R J Williams</author>
</authors>
<title>Learning internal representations by error propagation.&amp;quot;</title>
<date>1986</date>
<booktitle>In Parallel Distributed Processing,</booktitle>
<pages>318--362</pages>
<publisher>MIT Press,</publisher>
<note>edited by</note>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>Rumelhart, David E.; Hinton, G. E.; and Williams, R. J. (1986). &amp;quot;Learning internal representations by error propagation.&amp;quot; In Parallel Distributed Processing, edited by D. E. Rumelhart and J. L. McClelland, MIT Press, 318-362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
</authors>
<title>Language and memory,&amp;quot;</title>
<date>1980</date>
<journal>Cognitive Science,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="42925" citStr="Schank (1980)" startWordPosition="7141" endWordPosition="7142"> subcategorized as vao, vabo, vo, va, vhav, vbe where a is agent, o is object, b is beneficiary and vhav is a form of have and vbe a form of be. So far, only the 406 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English subcategory of time has been necessary in subcategorizing nouns to accomplish this form of case analysis, but in general, a lexical semantics is required to resolve syntactic attachment ambiguities. The complete set of case relations is presumed to be small, but no one has yet claimed a complete enumeration of them. Other case systems such as those taught by Schank (1980) and Jackendoff (1983) classify predicate names into such primitives as Do, Event, Thing, Mtrans, Ptrans, Go, Action, etc., to approximate some form of &amp;quot;language of thought&amp;quot; but the present approach is less ambitious, proposing merely to represent in a fairly formal fashion the organization of the words in a sentence. Subsequent operations on this admittedly superficial class of case structures, when augmented with a system of shallow lexical semantics, have been shown to accomplish question answering, focus tracking of topics throughout a text, automatic outlining, and summarization of texts </context>
</contexts>
<marker>Schank, 1980</marker>
<rawString>Schank, Roger C. (1980). &amp;quot;Language and memory,&amp;quot; Cognitive Science, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jungyun Seo</author>
</authors>
<title>Text driven construction of discourse structures for understanding descriptive texts. Doctoral dissertation,</title>
<date>1990</date>
<institution>University of Texas,</institution>
<location>Austin.</location>
<contexts>
<context position="43534" citStr="Seo 1990" startWordPosition="7234" endWordPosition="7235">and Jackendoff (1983) classify predicate names into such primitives as Do, Event, Thing, Mtrans, Ptrans, Go, Action, etc., to approximate some form of &amp;quot;language of thought&amp;quot; but the present approach is less ambitious, proposing merely to represent in a fairly formal fashion the organization of the words in a sentence. Subsequent operations on this admittedly superficial class of case structures, when augmented with a system of shallow lexical semantics, have been shown to accomplish question answering, focus tracking of topics throughout a text, automatic outlining, and summarization of texts (Seo 1990; Rim 1990). One strong constraint on this type of analysis is that the resulting case structure must maintain all information present in the text so that the text may be exactly reconstituted from the analysis. 7.2 Syntactic Analysis of Case Structure We&apos;ve seen earlier that a shift/reduce-rename operation is sufficient to parse most sentences into phrase structures. Case structure, however, requires transformations in addition to these operations. To form a case structure it is frequently necessary to change the order of constituents and to insert case labels. Following jackendoff&apos;s principl</context>
<context position="64535" citStr="Seo 1990" startWordPosition="10945" endWordPosition="10946">skeletal. We did not mark mood, aspect or tense of verbs, number for nouns, or deal with long distance dependencies. We do not resolve pronoun references; and we do not complete ellipses in conjunctive and other constructions. Each of these shortcomings is the subject of continuing research. For the present, the output of the case parser provides the nested, labeled, propositional structures which, supported by a semantic knowledge base, we have customarily used to accomplish focus-tracking of topics through a continuous text to compute labeled outlines and other forms of discourse structure (Seo 1990; Rim 1990; Alterman 1985). During this process of discourse analysis, some degapping, completion of ellipsis, and pronoun resolution is accomplished. 8.1 Conclusions From the studies presented in this paper we conclude: 1. Context-Dependent Grammars (CDGs) are computationally and conceptually tractable formalisms that can be composed easily by a linguist and effectively used by a deterministic parser to compute phrase structures and case analyses for subsets of newspaper English. 414 Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for English 2. The contextual portions of the CDG</context>
</contexts>
<marker>Seo, 1990</marker>
<rawString>Seo, Jungyun (1990). Text driven construction of discourse structures for understanding descriptive texts. Doctoral dissertation, University of Texas, Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terrence J Sejnowski</author>
<author>C Rosenberg</author>
</authors>
<title>NETtalk: A parallel network that learns to read aloud.&amp;quot; In Neurocomputing, edited by Anderson and Rosenfeld,</title>
<date>1988</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="18519" citStr="Sejnowski and Rosenberg (1988)" startWordPosition="3051" endWordPosition="3054">istics Volume 18, Number 4 We use no more than 64 word and phrase class symbols, so there can be no more than 4,096 possible pairs. The effect is to divide the large number of rules into no more than 4,096 subgroups, each of which will have a manageable subset. In fact, with 16,275 rules we discovered that we have only 823 pairs and the average number of rules per subgroup is 19.8; however, for frequently occurring pairs the number of rules in the subgroups can be much larger. The problem is to determine what scoring formula should be used to find the rule that best matches a parsing context. Sejnowski and Rosenberg (1988) analyzed the weight matrix that resulted from training NETtalk and discovered a triangular function with the apex centered at the character in the window and the weights falling off in proportion to distance from that character. We decided that the best matching rule in our system would follow a similar pattern with maximum weights for the top two elements on the stack with weights decreasing in both directions with distance from those positions. The scoring function we use is developed as follows: Let R be the set of vectors {Ri, R2, • • • ,R} where R, is the vector [r1, r27 • 7 r10] Let C b</context>
</contexts>
<marker>Sejnowski, Rosenberg, 1988</marker>
<rawString>Sejnowski, Terrence J., and Rosenberg, C. (1988). &amp;quot;NETtalk: A parallel network that learns to read aloud.&amp;quot; In Neurocomputing, edited by Anderson and Rosenfeld, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>An Introduction to Unification Based Approaches to Grammar.</title>
<date>1986</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="6689" citStr="Shieber 1986" startWordPosition="984" endWordPosition="985">ary newspaper text. We show that the application of context-sensitive rules by a deterministic shift/reduce parser is a conceptually and computationally tractable approach to NLP that may allow us to accumulate practical grammars for large subsets of English texts. 2. Context-Dependent Parsing In NL research most interest has centered on context-free grammars (CFG), augmented with feature tests and transformations, used to describe the phrase structure of sentences. There is a broad literature on Generalized Phrase Structure Grammar (Gazdar et al. 1985), Unification Grammars of various types (Shieber 1986), and Augmented 392 Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English Transition Networks (J. Allen 1987). Gazdar (1988) calls attention to a subcategory of context-sensitive grammars called indexed languages and illustrates some applicability to natural languages, and Joshi illustrates an application of &amp;quot;mild context-sensitivity&amp;quot; (Joshi 1987), but in general, NL computation with context-sensitive grammars is a largely unexplored area. While a few advanced NLP laboratories have developed grammars and parsing capabilities for significantly large subsets of natural language</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>Shieber, Stuart M. (1986). An Introduction to Unification Based Approaches to Grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert F Simmons</author>
</authors>
<title>Computations from the English.</title>
<date>1984</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="40553" citStr="Simmons (1984)" startWordPosition="6749" endWordPosition="6750"> (e.g. Rim 1990), but it is consistent with the neural network experience of directly mapping from sentence to case structure, and with the AT research that seeks to integrate syntactic and semantic processing while translating sentences to propositional structures. Linguistic theories of case structure have been concerned only with single propositions headed by verb predications; they have been largely silent with regard to the structure of noun phrases and the relations among embedded and sequential propositions. Additional conventions for managing these complications have been developed in Simmons (1984) and Alterman (1985) and are used here. The central notion of a case analysis is to translate sentence strings into a nested structure of case relations (or predicates) where each relation has a head term and an indefinite number of labeled arguments. An argument may itself be a case relation. Thus a sentence, as in the examples below, forms a tree of case relations. The old man from Spain ate fish. (eat Agt (man Mod old From spain) Obj fish) Another mission scheduled soon is the first firing of a trident missile from a submerged submarine. (is Obj1 (mission Mod another Obj* (scheduled Vmod so</context>
<context position="44948" citStr="Simmons 1984" startWordPosition="7457" endWordPosition="7458">discovered. Thus when a verb, say throw and an NP, say coconuts are on top of the stack, one must not only create a VP, but also decide the case, Obj, and form the constituent, (throw Obj coconuts). This can be accomplished in customary approaches to parsing by using augmented context free recognition rules of the form: VP VP NP / 1 obj 2 where the numbers following the slash refer to the text dominated by the syntactic class in the referenced position, (ordered left-to-right) in the right half of the rule. The resulting constituents can be accumulated to form the case analysis of a sentence (Simmons 1984). We develop augmented context-sensitive rules following the same principle. Let us look again at the example &amp;quot;The old man from Spain ate fish,&amp;quot; this time to develop case relations. n art adj n from n vao n ; shift art * adj n from n vao n ; shift art adj * n from n vao n ; shift art adj n * from n vao n ; 1 mod 2 (man Mod old) art n * from n vao n ; 1 det 2 (man Mod old Det the) n * from n vao n ; shift n from * n vao n ; shift n from n * vao n ; 3 2 1 (man Mod old Det the From spain) n * vao n ; shift n vao * n ; 2 agt 1 (ate Agt (man Mod old ... ) vao * n ; shift vao n * ; 1 obj 2 (ate Agt </context>
</contexts>
<marker>Simmons, 1984</marker>
<rawString>Simmons, Robert F. (1984). Computations from the English. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert F Simmons</author>
<author>Yeong-Ho Yu</author>
</authors>
<title>Training a neural network to be a context sensitive grammar.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 5th Rocky Mountain Al Conference. Las</booktitle>
<location>Cruces, NM.</location>
<contexts>
<context position="5067" citStr="Simmons and Yu (1990)" startWordPosition="738" endWordPosition="741">es and sentences to map string contexts into case structures. Robert Allen (1987) mapped nine-word sentences of English into Spanish translations, and Yu and Simmons (1990) accomplished comparable context-sensitive translations between English and German simple sentences. It was apparent that the contexts in which a word occurred provided information to a neural network that was sufficient to select correct word sense and syntactic structure for otherwise ambiguous usages of language. In order to solve a problem of accepting indefinitely long, complex sentences in a fixed-size neural network, Simmons and Yu (1990) showed a method for training a network to act as a context-sensitive grammar. A sequential program accessed that grammar with a deterministic, single-path parser and accurately parsed descriptive texts. Continuing that research, 2,000 rules were accumulated and a network was trained using a back-propagation method. The training of this network required ten days of continuous computation on a Symbolics Lisp Machine. We observed that the training cost increased by more than the square of the number of training examples and calculated that 10,000-20,000 rules might well tax a supercomputer. So w</context>
<context position="17333" citStr="Simmons and Yu 1990" startWordPosition="2842" endWordPosition="2845">t be assured, and in most cases, a partial match must suffice for the rule to be applied. Since many rules may partially match the current context, the best matching rule should be selected. One way to do this is to use a neural network. Through the back-propagation algorithm (Rumelhart, Hinton, and Williams 1986), a feed-forward network can be trained to memorize the CDG rules. After successful training, the network can be used to retrieve the best matching rule. However, this approach based on neural network usually takes considerable training time. For instance, in our previous experiment (Simmons and Yu 1990), training a network for about 2,000 CDG rules took several days of computation. Therefore, this approach has an intrinsic problem for scaling up, at least on the present generation of neural net simulation software. Another method is based on a hash table in which every CDG rule is stored according to its top two elements of the stack—the fourth and fifth elements of the left half of the rule. Given the current windowed context, the top two elements of the stack are used to retrieve all the relevant rules from the hash table. 397 Computational Linguistics Volume 18, Number 4 We use no more th</context>
</contexts>
<marker>Simmons, Yu, 1990</marker>
<rawString>Simmons, Robert F., and Yu, Yeong-Ho (1990). &amp;quot;Training a neural network to be a context sensitive grammar.&amp;quot; In Proceedings, 5th Rocky Mountain Al Conference. Las Cruces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language.</title>
<date>1985</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="7703" citStr="Tomita 1985" startWordPosition="1122" endWordPosition="1123">th context-sensitive grammars is a largely unexplored area. While a few advanced NLP laboratories have developed grammars and parsing capabilities for significantly large subsets of natural language,1 it cannot be denied that massive effort was required and that the results are plagued by ambiguous interpretations. These grammars are typically a context-free form, augmented by complex feature tests, transformations, and occasionally, arbitrary programs. The combination of even an efficient parser with such intricate grammars may greatly increase computational complexity of the parsing system (Tomita 1985). It is extremely difficult to write and maintain such grammars, and they must frequently be revised and retested to ensure internal consistency as new rules are added. We argue here that an acquisition system for accumulating context-sensitive rules and their application by a deterministic shift/reduce parser will greatly simplify the process of constructing and maintaining natural language parsing systems. Although we use context-sensitive rules of the form uXv uYv they are interpreted by a shift/reduce parser with the result that they can be applied successfully to the LR(k) subset of conte</context>
</contexts>
<marker>Tomita, 1985</marker>
<rawString>Tomita, M. (1985). Efficient Parsing for Natural Language. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yeong-Ho Yu</author>
<author>R F Simmons</author>
</authors>
<title>Descending epsilon in back-propagation: A technique for better generalization,&amp;quot; in press,</title>
<date>1990</date>
<booktitle>Proc. Int. Jt. Conf. Neural Networks,</booktitle>
<location>San Diego.</location>
<contexts>
<context position="4618" citStr="Yu and Simmons (1990)" startWordPosition="673" endWordPosition="676">of the first hints came from the striking finding from Sejnowski and Rosenberg&apos;s NETtalk (1988), that seven-character contexts were largely sufficient to map each character of a printed word into its corresponding phoneme—where each character actually maps in various contexts into several different phonemes. For accomplishing linguistic case analyses McClelland and Kawamoto (1986) and Miikkulainen and Dyer (1989) used the entire context of phrases and sentences to map string contexts into case structures. Robert Allen (1987) mapped nine-word sentences of English into Spanish translations, and Yu and Simmons (1990) accomplished comparable context-sensitive translations between English and German simple sentences. It was apparent that the contexts in which a word occurred provided information to a neural network that was sufficient to select correct word sense and syntactic structure for otherwise ambiguous usages of language. In order to solve a problem of accepting indefinitely long, complex sentences in a fixed-size neural network, Simmons and Yu (1990) showed a method for training a network to act as a context-sensitive grammar. A sequential program accessed that grammar with a deterministic, single-</context>
<context position="38827" citStr="Yu and Simmons (1990)" startWordPosition="6491" endWordPosition="6494">rectly to answer questions and are the basis of schemas, scripts, and frames that are used to add meaning to otherwise inexplicit texts. As a result of the experiments with acquiring CDG and exploring its properties for parsing phrase structures, we became fairly confident that we could generalize the system to acquisition and parsing based on a grammar that would compute syntactic case structures directly from syntactic strings. Direct translation from string to structure is supported by neural network experiments such as those by McClelland and Kawamoto (1986), Miikkulainen and Dyer (1989), Yu and Simmons (1990), and Leow and Simmons (1990). We reasoned that if we could acquire case grammar with something approaching the simplicity of acquiring phrase structure rules, the result could be of great value for NL applications. Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English 100 1,000 /0,000 25,000 100,000 Nbr of Accumulated Rules 405 Computational Linguistics Volume 18, Number 4 7.1 Case Structure Cook (1989) reviewed twenty years of linguistic research on case analysis of natural language sentences. He synthesized the various theories into a system that depends on the subclassifi</context>
</contexts>
<marker>Yu, Simmons, 1990</marker>
<rawString>Yu, Yeong-Ho, and Simmons, R. F. (1990). &amp;quot;Descending epsilon in back-propagation: A technique for better generalization,&amp;quot; in press, Proc. Int. Jt. Conf. Neural Networks, San Diego.</rawString>
</citation>
<citation valid="false">
<journal>B B BECAUSE-OF ART N ON ART N N COMMA) (1 DET 2)) UB B B BECAUSE-OF N ON ART N N COMMA) (S)) ((B B BECAUSE-OF N ON ART N N COMMA N) (S)) UB BECAUSE-OF N ON ART N N COMMA N VAO) (S)) ((BECAUSE-OF N ON ART N N COMMA N VAO B) (S)) UN ON ART N N COMMA N VAO B B</journal>
<volume>1</volume>
<marker></marker>
<rawString>((B B BECAUSE-OF ART N ON ART N N COMMA) (1 DET 2)) UB B B BECAUSE-OF N ON ART N N COMMA) (S)) ((B B BECAUSE-OF N ON ART N N COMMA N) (S)) UB BECAUSE-OF N ON ART N N COMMA N VAO) (S)) ((BECAUSE-OF N ON ART N N COMMA N VAO B) (S)) UN ON ART N N COMMA N VAO B B) (1 NMOD 2)) ((BECAUSE-OF N ON ART N COMMA N VAO B B) (1 DET 2)) ((B BECAUSE-OF N ON N COMMA N VAO B B) (3 2 1)) UB B B BECAUSE-OF N COMMA N VAO B B) (2 ANTE 1)) UB B B B BECAUSE-OF COMMA N VAO B B) (S)) ((B B B BECAUSE-OF COMMA N VAO B B B) (S)) UB B BECAUSE-OF COMMA N VAO B B B B) (S)) ((B BECAUSE-OF COMMANVAOBBBBB) (1 AGT 2)) UBBBECAUSE-OF COMMA VAOBBBBB) (1 OBJ 3))))</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>