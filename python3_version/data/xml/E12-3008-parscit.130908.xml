<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.998696">
Hierarchical Bayesian Language Modelling
for the Linguistically Informed
</title>
<author confidence="0.99588">
Jan A. Botha
</author>
<affiliation confidence="0.993134">
Department of Computer Science
University of Oxford, UK
</affiliation>
<email confidence="0.983708">
jan.botha@cs.ox.ac.uk
</email>
<sectionHeader confidence="0.994805" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999836333333333">
In this work I address the challenge of aug-
menting n-gram language models accord-
ing to prior linguistic intuitions. I argue
that the family of hierarchical Pitman-Yor
language models is an attractive vehicle
through which to address the problem, and
demonstrate the approach by proposing a
model for German compounds. In an em-
pirical evaluation, the model outperforms
the Kneser-Ney model in terms of perplex-
ity, and achieves preliminary improvements
in English-German translation.
</bodyText>
<sectionHeader confidence="0.998106" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999934315789474">
The importance of effective language models in
machine translation (MT) and automatic speech
recognition (ASR) is widely recognised. n-gram
models, in particular ones using Kneser-Ney
(KN) smoothing, have become the standard
workhorse for these tasks. These models are not
ideal for languages that have relatively free word
order and/or complex morphology. The ability to
encode additional linguistic intuitions into models
that already have certain attractive properties is an
important piece of the puzzle of improving ma-
chine translation quality for those languages. But
despite their widespread use, KN n-gram mod-
els are not easily extensible with additional model
components that target particular linguistic phe-
nomena.
I argue in this paper that the family of hierarchi-
cal Pitman-Yor language models (HPYLM) (Teh,
2006; Goldwater et al., 2006) are suitable for
investigations into more linguistically-informed
n-gram language models. Firstly, the flexibility
to specify arbitrary back-off distributions makes it
easy to incorporate multiple models into a larger
n-gram model. Secondly, the Pitman-Yor process
prior (Pitman and Yor, 1997) generates distribu-
tions that are well-suited to a variety of power-
law behaviours, as is often observed in language.
Catering for a variety of those is important since
the frequency distributions of, say, suffixes, could
be quite different from that of words. KN smooth-
ing is less flexibility in this regard. And thirdly,
the basic inference algorithms have been paral-
lelised (Huang and Renals, 2009), which should
in principle allow the approach to still scale to
large data sizes.
As a test bed, I consider compounding in Ger-
man, a common phenomenon that creates chal-
lenges for machine translation into German.
</bodyText>
<sectionHeader confidence="0.90253" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.9983155">
n-gram language models assign probabilities to
word sequences. Their key approximation is that
a word is assumed to be fully determined by n−1
words preceding it, which keeps the number of in-
dependent probabilities to estimate in a range that
is computationally attractive. This basic model
structure, largely devoid of syntactic insight, is
surprisingly effective at biasing MT and ASR sys-
tems toward more fluent output, given a suitable
choice of target language.
But the real challenge in constructing n-gram
models, as in many other probabilistic settings, is
how to do smoothing, since the vast majority of
linguistically plausible n-grams will occur rarely
or be absent altogether from a training corpus,
which often renders empirical model estimates
misleading. The general picture is that probability
mass must be shifted away from some events and
redistributed across others.
The method of Kneser and Ney (1995) and
</bodyText>
<page confidence="0.657998">
64
</page>
<note confidence="0.996491">
Proceedings of the EACL 2012 Student Research Workshop, pages 64–73,
Avignon, France, 26 April 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.997578192307692">
its later modified version (Chen and Goodman,
1998) generally perform best at this smoothing,
and are based on the idea that the number of
distinct contexts a word appears in is an impor-
tant factor in determining the probability of that
word. Part of this smoothing involves discount-
ing the counts of n-grams in the training data;
the modified version uses different levels of dis-
counting depending on the frequency of the count.
These methods were designed with surface word
distributions, and are not necessarily suitable for
smoothing distributions of other kinds of surface
units.
Bilmes and Kirchhoff (2003) proposed a more
general framework for n-gram language mod-
elling. Their Factored Language Model (FLM)
views a word as a vector of features, such that a
particular feature value is generated conditional
on some history of preceding feature values. This
allowed the inclusion of n-gram models over se-
quences of elements like PoS tags and semantic
classes. In tandem, they proposed more compli-
cated back-off paths; for example, trigrams can
back-off to two underlying bigram distributions,
one dropping the left-most context word and the
other the right-most. With the right combina-
tion of features and back-off structure they got
good perplexity reductions, and obtained some
improvements in translation quality by applying
these ideas to the smoothing of the bilingual
phrase table (Yang and Kirchhoff, 2006).
My approach has some similarity to the FLM:
both decompose surface word forms into elements
that are generated from unrelated conditional dis-
tributions. They differ predominantly along two
dimensions: the types of decompositions and con-
ditioning possible, and my use of a particular
Bayesian prior for handling smoothing.
In addition to the HPYLM for n-gram lan-
guage modelling (Teh, 2006), models based on
the Pitman-Yor process prior have also been ap-
plied to good effect in word segmentation (Gold-
water et al., 2006; Mochihashi et al., 2009) and
speech recognition (Huang and Renals, 2007;
Neubig et al., 2010). The Graphical Pitman-Yor
process enables branching back-off paths, which
I briefly revisit in §7, and have proved effective
in language model domain-adaptation (Wood and
Teh, 2009). Here, I extend this general line of
inquiry by considering how one might incorpo-
rate linguistically informed sub-models into the
HPYLM framework.
</bodyText>
<sectionHeader confidence="0.989644" genericHeader="method">
3 Compound Nouns
</sectionHeader>
<bodyText confidence="0.979039481481481">
I focus on compound nouns in this work for two
reasons: Firstly, compounding is in general a very
productive process, and in some languages (in-
cluding German, Swedish and Dutch) they are
written as single orthographic units. This in-
creases data sparsity and creates significant chal-
lenges for NLP systems that use whitespace to
identify their elementary modelling units. A
proper account of compounds in terms of their
component words therefore holds the potential of
improving the performance of such systems.
Secondly, there is a clear linguistic intuition to
exploit: the morphosyntactic properties of these
compounds are often fully determined by the head
component within the compound. For example,
in “Geburtstagskind” (birthday kid), it is “Kind”
that establishes this compound noun as singular
neuter, which determine how it would need to
agree with verbs, articles and adjectives. In the
next section, I propose a model in the suggested
framework that encodes this intuition.
The basic structure of German compounds
comprises a head component, preceded by one or
more modifier components, with optional linker
elements between consecutive components (Gold-
smith and Reutter, 1998).
Examples
</bodyText>
<listItem confidence="0.91149375">
• The basic form is just the concatenation of two
nouns
Auto + Unfall = Autounfall (car crash)
• Linker elements are sometimes added be-
tween components
K¨uche + Tisch = K¨uchentisch (kitchen table)
• Components can undergo stemming during
composition
Schule + Hof = Schulhof (schoolyard)
• The process is potentially recursive
(Geburt + Tag) + Kind = Geburtstag + Kind
= Geburtstagskind (birthday kid)
</listItem>
<bodyText confidence="0.9998664">
The process is not limited to using nouns as
components, for example, the numeral in Zwei-
Euro-M¨unze (two Euro coin) or the verb “fahren”
(to drive) in Fahrzeug (vehicle). I will treat all
these cases the same.
</bodyText>
<page confidence="0.953504">
65
</page>
<subsectionHeader confidence="0.996087">
3.1 Fluency amid sparsity
</subsectionHeader>
<bodyText confidence="0.975298777777778">
Consider the following example from the training
corpus used in the subsequent evaluations:
de: Die Neuinfektionen ¨ubersteigen weiterhin die
Behandlungsbem¨uhungen.
en: New infections continue to outpace treatment ef-
forts.
The corpus contains numerous other compounds
ending in “infektionen” (16) or “bem¨uhungen”
(117). A standard word-based n-gram model
discriminates among those alternatives using as
many independent parameters.
However, we could gauge the approximate syn-
tactic fluency of the sentence almost as well if we
ignore the compound modifiers. Collapsing all
the variants in this way reduces sparsity and yields
better n-gram probability estimates.
To account for the compound modifiers, a sim-
ple approach is to use a reverse n-gram language
model over compound components, without con-
ditioning on the sentential context. Such a model
essentially answers the question, “Given that the
word ends in ‘infektionen’, what modifier(s), if
any, are likely to precede it?” The vast majority of
nouns will never occur in that position, meaning
that the conditional distributions will be sharply
peaked.
mit der Draht·seil·bahn
</bodyText>
<figureCaption confidence="0.93266">
Figure 1: Intuition for the proposed generative pro-
</figureCaption>
<bodyText confidence="0.789937">
cess of a compound word: The context generates the
head component, which generates a modifier compo-
nent, which in turn generates another modifier. (Trans-
lation: “with the cable car”)
have also been employed for speech recognition
(Berton et al., 1996) and predictive-text input
(Baroni and Matiasek, 2002), where single-token
compounds also pose challenges.
</bodyText>
<sectionHeader confidence="0.970342" genericHeader="method">
4 Model Description
</sectionHeader>
<subsectionHeader confidence="0.902797">
4.1 HPYLM
</subsectionHeader>
<bodyText confidence="0.99812575">
Formally speaking, an n-gram model is an
(n — 1)-th order Markov model that approxi-
mates the joint probability of a sequence of
words w as
</bodyText>
<equation confidence="0.9998145">
P(w) � � |w |P(wi|wi−n+1, ... , wi−1),
i=1
</equation>
<bodyText confidence="0.999935388888889">
for which I will occasionally abbreviate a con-
text [wi, ... , wj] as u. In the HPYLM, the condi-
tional distributions P(w|u) are smoothed by plac-
ing Pitman-Yor process priors (PYP) over them.
The PYP is defined through its base distribution,
and a strength (θ) and discount (d) hyperparame-
ter that control its deviation away from its mean
(which equals the base distribution).
Let G[u,v] be the PYP-distributed trigram distri-
bution P(w|u, v). The hierarchy arises by using
as base distribution for the prior of G[u,v] another
PYP-distributed G[v], i.e. the distribution P(w|v).
The recursion bottoms out at the unigram distri-
bution Go, which is drawn from a PYP with base
distribution equal to the uniform distribution over
the vocabulary W. The hyperparameters are tied
across all priors with the same context length |u|,
and estimated during training.
</bodyText>
<equation confidence="0.9993575">
G0 = Uniform(|W|)
Go — PY (d0, θ0, G0)
</equation>
<bodyText confidence="0.87063">
...
</bodyText>
<subsectionHeader confidence="0.886004">
3.2 Related Work on Compounds
</subsectionHeader>
<bodyText confidence="0.99987375">
In machine translation and speech recognition,
one approach has been to split compounds as a
preprocessing step and merge them back together
during postprocessing, while using otherwise un-
modified NLP systems. Frequency-based meth-
ods have been used for determining how aggres-
sively to split (Koehn and Knight, 2003), since
the maximal, linguistically correct segmentation
is not necessarily optimal for translation. This
gave rise to slight improvements in machine trans-
lation evaluations (Koehn et al., 2008), with fine-
tuning explored in (Stymne, 2009). Similar ideas
</bodyText>
<equation confidence="0.999975">
Gπ(u) — PY (d|π(u)|, θ|π(u)|, Gπ(π(u)))
Gu — PY (d|u|, θ|u|, Gπ(u))
w — Gu,
</equation>
<bodyText confidence="0.9999895">
where π(u) truncates the context u by dropping
the left-most word in it.
</bodyText>
<subsectionHeader confidence="0.992939">
4.2 HPYLM+c
</subsectionHeader>
<bodyText confidence="0.9757255">
Define a compound word w� as a sequence of
components [c1, ... , ck], plus a sentinel symbol $
marking either the left or the right boundary of the
word, depending on the direction of the model. To
maintain generality over this choice of direction,
66
let A be an index set over the positions, such that
cA1 always designates the head component.
Following the motivation in §3.1, I set up the
model to generate the head component cA1 condi-
tioned on the word context u, while the remaining
components w� \ cA1 are generated by some model
F, independently of u.
To encode this, I modify the HPYLM in two
ways: 1) Replace the support with the reduced vo-
cabulary M, the set of unique components c ob-
tained when segmenting the items in W. 2) Add
an additional level of conditional distributions Hu
(with |u |= n −1) where items from M combine
to form the observed surface words:
</bodyText>
<equation confidence="0.934181666666667">
Gu ... (as before, except G0 =Uniform(|M|))
Hu ∼ PY (d|u|, θ|u|, Gu × F)
w� ∼ Hu
</equation>
<bodyText confidence="0.99992175">
So the base distribution for the prior of the word
n-gram distribution Hu is the product of a distri-
bution Gu over compound heads, given the same
context u, and another (n0-gram) language model
F over compound modifiers, conditioned on the
head component.
Choosing F to be a bigram model (n0=2) yields
the following procedure for generating a word:
</bodyText>
<equation confidence="0.914947666666667">
cA1 ∼ Gu
for i = 2 to k
cAi ∼ F(·|cAi−1)
</equation>
<bodyText confidence="0.99997702631579">
The linguistically motivated choice for condi-
tioning in F is Aling = [k, k − 1, ... ,1] such that
cA1 is the true head component; $ is drawn from
F(·|c1) and marks the left word boundary.
In order to see if the correct linguistic intuition
has any bearing on the model’s extrinsic perfor-
mance, we will also consider the reverse, sup-
posing that the left-most component were actu-
ally more important in this task, and letting the
remaining components be generated left-to-right.
This is expressed by Ainv = [1, ... , k], where $
this time marks the right word boundary and is
drawn from F(·|ck).
To test whether Kneser-Ney smoothing is in-
deed sometimes less appropriate, as conjectured
earlier, I will also compare the case where
F = FKN, a KN-smoothed model, with the case
where F = FHPY LM, another HPYLM.
Linker Elements In the preceding definition of
compound segmentation, the linker elements do
not form part of the vocabulary M. Regarding
linker elements as components in their own right
would sacrifice important contextual information
and disrupt the conditionals F(·|cAi−1). That is,
given K¨uche·n·tisch, we want P(K¨uche|Tisch) in
the model, but not P (K¨uche|n).
But linker elements need to be accounted
for somehow to have a well-defined generative
model. I follow the pragmatic option of merg-
ing any linkers onto the adjacent component – for
Aling merging happens onto the preceding compo-
nent, while for Ainv it is onto the succeeding one.
This keeps the ‘head’ component cA1 in tact.
More involved strategies could be considered,
and it is worth noting that for German the pres-
ence and identity of linker elements between ci
and ci+1 are in fact governed by the preceding
component ci (Goldsmith and Reutter, 1998).
</bodyText>
<sectionHeader confidence="0.979072" genericHeader="method">
5 Training
</sectionHeader>
<bodyText confidence="0.999908413793104">
For ease of exposition I describe inference with
reference to the trigram HPYLM+c model with
a bigram HPYLM for F, but the general case
should be clear.
The model is specified by the latent vari-
ables (G[∅], G[v], G[u,v], H[u,v], F∅, Fc), where
u, v ∈ W, c ∈ M, and hyperparameters Q =
{di, θi} ∪ {d0j, θ0j} ∪ {d00�, θ00�}, where i = 0, 1, 2,
j = 0, 1, single primes designate the hyperpa-
rameters in FHPY LM and double primes those of
H[u,v]. We can construct a collapsed Gibbs sam-
pler by marginalising out these latent variables,
giving rise to a variant of the hierarchical Chinese
Restaurant Process in which it is straightforward
to do inference.
Chinese Restaurant Process A direct repre-
sentation of a random variable G drawn from a
PYP can be obtained from the so-called stick-
breaking construction. But the more indirect rep-
resentation by means of the Chinese Restaurant
Process (CRP) (Pitman, 2002) is more suitable
here since it relates to distributions over items
drawn from such a G. This fits the current set-
ting, where words w are being drawn from a PYP-
distributed G.
Imagine that a corpus is created in two phases:
Firstly, a sequence of blank tokens xi is instanti-
ated, and in a second phase lexical identities wi
are assigned to these tokens, giving rise to the
</bodyText>
<page confidence="0.849986">
67
</page>
<bodyText confidence="0.998736947368421">
observed corpus. In the CRP metaphor , the se-
quence of tokens xi are equated with a sequence
of customers that enter a restaurant one-by-one to
be seated at one of an infinite number of tables.
When a customer sits at an unoccupied table k,
they order a dish φk for the table, but customers
joining an occupied table have to dine on the dish
already served there. The dish φi that each cus-
tomer eats is equated to the lexical identity wi of
the corresponding token, and the way in which ta-
bles and dishes are chosen give rise to the charac-
teristic properties of the CRP:
More formally, let x1, x2,... be draws from G,
while t is the number of occupied tables, c the
number of customers in the restaurant, and ck the
number of customers at the k-th table. Condi-
tioned on preceding customers x1, ... , xi_1 and
their arrangement, the i-th customer sits at table
k = k&apos; according to the following probabilities:
</bodyText>
<equation confidence="0.8940035">
�
ck� − d occupied table k&apos;
Pr(k&apos; |. . . ) a
θ + dt unoccupied table t + 1
</equation>
<bodyText confidence="0.999807380952381">
Ordering a dish for a new table corresponds to
drawing a value φk from the base distribution G0,
and it is perfectly acceptable to serve the same
kind of dish at multiple tables.
Some characteristic behaviour of the CRP can
be observed easily from this description: 1) As
more customers join a table, that table becomes
a more likely choice for future customers too.
2) Regardless of how many customers there are,
there is always a non-zero probability of joining
an unoccupied table, and this probability also de-
pends on the number of total tables.
The dish draws can be seen as backing off to
the underlying base distribution G0, an important
consideration in the context of the hierarchical
variant of the process explained shortly. Note that
the strength and discount parameters control the
extent to which new dishes are drawn, and thus
the extent of reliance on the base distribution.
The predictive probability of a word w given a
seating arrangement is given by
</bodyText>
<equation confidence="0.96153">
Pr(w |... ) a cw − dtw + (θ + dt)G0(w)
</equation>
<bodyText confidence="0.999951634146342">
In smoothing terminology, the first term can be
interpreted as applying a discount of dtw to the
observed count cw of w; the amount of dis-
count therefore depends on the prevalence of the
word (via tw). This is one significant way in
which the PYP/CRP gives more nuanced smooth-
ing than modified Kneser-Ney, which only uses
four different discount levels (Chen and Good-
man, 1998). Similarly, if the seating dynamics
are constrained such that each dish is only served
once (tw = 1 for any w), a single discount level
is affected, establishing direct correspondence to
original interpolated Kneser-Ney smoothing (Teh,
2006).
Hierarchical CRP When the prior of Gu has
a base distribution Gπ(u) that is itself PYP-
distributed, as in the HPYLM, the restaurant
metaphor changes slightly. In general, each node
in the hierarchy has an associated restaurant.
Whenever a new table is opened in some restau-
rant R, another customer is plucked out of thin air
and sent to join the parent restaurant pa(R). This
induces a consistency constraint over the hierar-
chy: the number of tables tw in restaurant R must
equal the number of customers cw in its parent
pa(R).
In the proposed HPYLM+c model using
FHPYLM, there is a further constraint of a simi-
lar nature: When a new table is opened and serves
dish φ = w� in the trigram restaurant for H[u,v],
a customer cA1 is sent to the corresponding bi-
gram restaurant for G[u,v], and customers cA2:k,$
are sent to the restaurants for Fc,, for contexts
c&apos; = cA1:k_1. This latter requirement is novel here
compared to the hierarchical CRP used to realise
the original HPYLM.
Sampling Although the CRP allows us to re-
place the priors with seating arrangements S,
those seating arrangements are simply latent vari-
ables that need to be integrated out to get a true
predictive probability of a word:
</bodyText>
<equation confidence="0.933575">
p(w|D) = Is,n p(w|S,Q)p(S,Q|D),
</equation>
<bodyText confidence="0.999984833333333">
where D is the training data and, as before, Q are
the parameters. This integral can be approximated
by averaging over m posterior samples (S, Q)
generated using Markov chain Monte Carlo meth-
ods. The simple form of the conditionals in the
CRP allows us to do a Gibbs update whereby the
table index k of a customer is resampled condi-
tioned on all the other variables. Sampling a new
seating arrangement S for the trigram HPYLM+c
thus corresponds to visiting each customer in the
restaurants for H[u,v], removing them while cas-
cading as necessary to observe the consistency
</bodyText>
<page confidence="0.830053">
68
</page>
<bodyText confidence="0.999827571428571">
across the hierarchy, and seating them anew at
some table W.
In the absence of any strong intuitions about ap-
propriate values for the hyperparameters, I place
vague priors over them and use slice sampling1
(Neal, 2003) to update their values during gener-
ation of the posterior samples:
</bodyText>
<equation confidence="0.708039">
d — Beta(1,1) 0 — Gamma(1,1)
</equation>
<bodyText confidence="0.971519">
Lastly, I make the further approximation of
m = 1, i.e. predictive probabilities are informed
by a single posterior sample (5, Q).
</bodyText>
<sectionHeader confidence="0.999539" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999984">
The aim of the experiments reported here is to test
whether the richer account of compounds in the
proposed language models has positive effects on
the predictability of unseen text and the genera-
tion of better translations.
</bodyText>
<subsectionHeader confidence="0.976775">
6.1 Methods
</subsectionHeader>
<bodyText confidence="0.999964925925926">
Data and Tools Standard data preprocessing
steps included normalising punctuation, tokenis-
ing and lowercasing all words. All data sets are
from the WMT11 shared-task.2. The full English-
German bitext was filtered to exclude sentences
longer than 50, resulting in 1.7 million parallel
sentences; word alignments were inferred from
this using the Berkeley Aligner (Liang et al.,
2006) and used as basis from which to extract a
Hiero-style synchronous CFG (Chiang, 2007).
The weights of the log-linear translation mod-
els were tuned towards the BLEU metric on
development data using cdec’s (Dyer et al.,
2010) implementation of MERT (Och, 2003).
For this, the set news-test2008 (2051 sen-
tences) was used, while final case-insensitive
BLEU scores are measured on the official test set
newstest2011 (3003 sentences).
All language models were trained on the target
side of the preprocessed bitext containing 38 mil-
lion tokens, and tested on all the German devel-
opment data (i.e. news-test2008,9,10).
Compound segmentation To construct a seg-
mentation dictionary, I used the 1-best segmenta-
tions from a supervised MaxEnt compound split-
ter (Dyer, 2009) run on all token types in bitext. In
addition, word-internal hyphens were also taken
</bodyText>
<footnote confidence="0.976489666666667">
1Mark Johnson’s implementation, http://www.cog.
brown.edu/˜mj/Software.htm
2http://www.statmt.org/wmt11/
</footnote>
<bodyText confidence="0.9999165">
as segmentation points. Finally, linker elements
were merged onto components as discussed in
§4.2. Any token that is split into more than one
part by this procedure is regarded as a compound.
The effect of the individual steps is summarised
in Table 1.
</bodyText>
<figure confidence="0.300376">
# Types Example
None 350998 Geburtstagskind
pre-merge 201328 Geburtstag·kind
merge, Aling 150980 Geburtstags·kind
merge, Ainv 162722 Geburtstag·skind
</figure>
<tableCaption confidence="0.997054">
Table 1: Effect of segmentation on vocabulary size.
</tableCaption>
<bodyText confidence="0.995486333333333">
Metrics For intrinsic evaluation of language
models, perplexity is a common metric. Given a
trained model q, the perplexity over the words T
</bodyText>
<equation confidence="0.976080666666667">
( � �
in unseen test set T is exp �1 � ln(q(T)) .
�� �
</equation>
<bodyText confidence="0.999970322580645">
One convenience of this per-word perplexity is
that it can be compared consistently across dif-
ferent test sets regardless of their lengths; its neat
interpretation is another: a model that achieves a
perplexity of q on a test set is on average q-ways
confused about each word. Less confusion and
therefore lower test set perplexity is indicative of
a better model. This allows different models to be
compared relative to the same test set.
The exponent above can be regarded as an
approximation of the cross-entropy between the
model q and a hypothetical model p from which
both the training and test set were putatively gen-
erated. It is sometimes convenient to use this as
an alternative measure.
But a language model only really becomes use-
ful when it allows some extrinsic task to be exe-
cuted better. When that extrinsic task is machine
translation, the translation quality can be assessed
to see if one language model aids it more than an-
other. The obligatory metric for evaluating ma-
chine translation quality is BLEU (Papineni et al.,
2001), a precision based metric that measures how
close the machine output is to a known correct
translation (the reference sentences in the test set).
Higher precision means the translation system is
getting more phrases right.
Better language model perplexities sometimes
lead to improvements in translation quality, but
it is not guaranteed. Moreover, even when real
translation improvements are obtained, they are
</bodyText>
<page confidence="0.805873">
69
</page>
<table confidence="0.998279714285714">
PPL c-Cross-ent.
mKN 441.32 0.1981
HPYLM 429.17 0.1994
FKN Aling 432.95 0.2028
FKN Ainv 446.84 0.2125
FHPYLM Aling 421.63 0.1987
FHPYLM Ainv 435.79 0.2079
</table>
<tableCaption confidence="0.785069">
Table 2: Monolingual evaluation results. The second
column shows perplexity measured all WMT11 Ger-
</tableCaption>
<bodyText confidence="0.91460225">
man development data (7065 sentences). At the word
level, all are trigram models, while F are bigram mod-
els using the specified segmentation scheme. The third
column has test cross-entropies measured only on the
6099 compounds in the test set (given their contexts ).
not guaranteed to be noticeable in the BLEU
score, especially when targeting an arguably nar-
row phenomenon like compounding.
</bodyText>
<table confidence="0.9925472">
BLEU
mKN 13.11
HPYLM 13.20
FHPYLM, Aling 13.24
FHPYLM, Ainv 13.32
</table>
<tableCaption confidence="0.998478666666667">
Table 3: Translation results, BLEU (1-ref), 3003 test
sentences. Trigram language models, no count prun-
ing, no “unknown word” token.
</tableCaption>
<table confidence="0.9997928">
P / R / F
mKN 22.0 / 17.3 / 19.4
HPYLM 21.0 / 17.8 / 19.3
FHPYLM, Aling 23.6 / 17.3 / 19.9
FHPYLM, Ainv 24.1 / 16.5 / 19.6
</table>
<tableCaption confidence="0.981758666666667">
Table 4: Precision, Recall and F-score of compound
translations, relative to reference set (72661 tokens, of
which 2649 are compounds).
</tableCaption>
<subsectionHeader confidence="0.999539">
6.2 Main Results
</subsectionHeader>
<bodyText confidence="0.999989171428572">
For the monolingual evaluation, I used an interpo-
lated, modified Kneser-Ney model (mKN) and an
HPYLM as baselines. It has been shown for other
languages that HPYLM tends to outperform mKN
(Okita and Way, 2010), but I am not aware of this
result being demonstrated on German before, as I
do in Table 2.
The main model of interest is HPYLM+c us-
ing the Aling segmentation and a model FHPYLM
over modifiers; this model achieves the lowest
perplexity, 4.4% lower than the mKN baseline.
Next, note that using FKN to handle the modi-
fiers does worse than FHPYLM, confirming our
expectation that KN is less appropriate for that
task, although it still does better than the original
mKN baseline.
The models that use the linguistically im-
plausible segmentation scheme Ainv both fare
worse than their counterparts that use the sensible
scheme, but of all tested models only FKN &amp; Ainv
fails to beat the mKN baseline. This suggests that
in some sense having any account whatsoever of
compound formation tends to have a beneficial ef-
fect on this test set – the richer statistics due to a
smaller vocabulary could be sufficient to explain
this – but to get the most out of it one needs the
superior smoothing over modifiers (provided by
FHPYLM) and adherence to linguistic intuition
(via Aling).
As for the translation experiments, the rela-
tive qualitative performance of the two baseline
language models carries over to the BLEU score
(HPYLM does 0.09 points better than KN), and is
further improved upon slightly by using two vari-
ants of HPYLM+c (Table 3).
</bodyText>
<subsectionHeader confidence="0.999708">
6.3 Analysis
</subsectionHeader>
<bodyText confidence="0.999905333333333">
To get a better idea of how the extended mod-
els employ the increased expressiveness, I calcu-
lated the cross-entropy over only the compound
words in the monolingual test set (second column
of Table 2). Among the HPYLM+c variants, we
see that their performance on compounds only is
consistent with their performance (relative to each
other) on the whole corpus. This implies that
the differences in whole-corpus perplexities are at
least in part due to their different levels of adept-
ness at handling compounds, as opposed to some
fluke event.
It is, however, somewhat surprising to observe
that HPYLM+c do not achieve a lower com-
pound cross-entropy than the mKN baseline, as it
suggests that HPYLM+c’s perplexity reductions
compared to mKN arise in part from something
other than compound handling, which is their
whole point.
This discrepancy could be related to the fair-
ness of this direct comparison of models that ul-
</bodyText>
<page confidence="0.742228">
70
</page>
<bodyText confidence="0.999980066666667">
timately model different sets of things: Accord-
ing to the generative process of HPYLM+c (§4),
there is no limit on the number of components in
a compound: in theory, an arbitrary number of
components c E M can combine to form a word.
HPYLM+c is thus defined over a countably infi-
nite set of words, thereby reserving some prob-
ability mass for items that will never be realised
in any corpus, whereas the baseline models are
defined only over the finite set W. These direct
comparisons are thus lightly skewed in favour of
the baselines. This bolsters confidence in the per-
plexity reductions presented in the previous sec-
tion, but the skew may afflict compounds more
starkly, leading to the slight discrepancy observed
in the compound cross-entropies. What matters
more is the performance among the HPYLM+c
variants, since they are directly comparable.
To home in still further on the compound mod-
elling, I selected those compounds for which
HPYLM+c (FHPYLM, Aling) does best/worst in
terms of the probabilities assigned, compared to
the mKN baseline (see Table 5). One pattern that
emerges is that the “top” compounds mostly con-
sist of components that are likely to be quite com-
mon, and that this improves estimates both for n-
grams that are very rare (the singleton “senkun-
gen der treibhausgasemmissionen” = decreases in
green house gas emissions) or relatively common
(158, “der hauptstadt” = of the capital).
</bodyText>
<subsectionHeader confidence="0.76983">
n-gram A C
</subsectionHeader>
<bodyText confidence="0.954129888888889">
gesichts·punkten
700 milliarden us-·dollar
s. der treibhausgas·emissionen
r. der treibhausgas·emissionen
ministerium f¨ur land·wirtschaft
bildungs·niveaus
newt ging·rich*
nouri al-·maliki*
klerikers moqtada al-·sadr*
nuri al-·maliki*
sankt peters·burg*
n¨achtlichem flug·l¨arm
Table 5: Compound n-grams in the test set for which
the absolute difference A = PHPYLM+c−PmKN is great-
est. C is n-gram count in the training data. Asterisks
denote words that are not compounds, linguistically
speaking. Abbrevs: r. = reduktionen, s.= senkungen
On the other hand, the “bottom” compounds
are mostly ones whose components will be un-
common; in fact, many of them are not truly com-
pounds but artefacts of the somewhat greedy seg-
mentation procedure I used. Alternative proce-
dures will be tested in future work.
Since the BLEU scores do not reveal much
about the new language models’ effect on com-
pound translation, I also calculated compound-
specific accuracies, using precision, recall and
F-score (Table 4). Here, the precision for a
single sentence would be 100% if all the com-
pounds in the output sentence occur in the ref-
erence translation. Compared to the baselines,
the compound precision goes up noticeably under
the HPYLM+c models used in translation, with-
out sacrificing on recall. This suggests that these
models are helping to weed out incorrectly hy-
pothesised compounds.
</bodyText>
<subsectionHeader confidence="0.995408">
6.4 Caveats
</subsectionHeader>
<bodyText confidence="0.999990375">
All results are based on single runs and are there-
fore not entirely robust. In particular, MERT
tuning of the translation model is known to in-
troduce significant variance in translation perfor-
mance across different runs, and the small differ-
ences in BLEU scores reported in Table 3 are very
likely to lie in that region.
Markov chain convergence also needs further
attention. In absence of complex latent struc-
ture (for the dishes), the chain should mix fairly
quickly, and as attested by Figure 2 it ‘converges’
with respect to the test metric after about 20 sam-
ples, although the log posterior (not shown) had
not converged after 40. The use of a single poste-
rior sample could also be having a negative effect
on results.
</bodyText>
<sectionHeader confidence="0.997588" genericHeader="method">
7 Future Directions
</sectionHeader>
<bodyText confidence="0.999973916666667">
The first goal will be to get more robust ex-
perimental results, and to scale up to 4-gram
models estimated on all the available monolin-
gual training data. If good performance can be
demonstrated under those conditions, this gen-
eral approach could pass as a viable alternative to
the current Kneser-Ney dominated state-of-the art
setup in MT.
Much of the power of the HPYLM+c model
has not been exploited in this evaluation, in par-
ticular its ability to score unseen compounds con-
sisting of known components. This feature was
</bodyText>
<figure confidence="0.99668">
0.064 335
0.021 2
0.018 1
0.011 3
0.009 11
0.009 14
-0.257 2
-0.257 3
-0.258 1
-0.337 3
-0.413 35
-0.454 2
71
</figure>
<figureCaption confidence="0.999933">
Figure 2: Convergence of test set perplexities.
</figureCaption>
<bodyText confidence="0.999990633333333">
not active in these evaluations, mostly due to the
current phase of implementation. A second area
of focus is thus to modify the decoder to gen-
erate such unseen compounds in translation hy-
potheses. Given the current low compound recall
rates, this could greatly benefit translation quality.
An informal analysis of the reference translations
in the bilingual test set showed that 991 of the
1406 out-of-vocabulary compounds (out of 2692
OOVs in total) fall into this category of unseen-
but-recognisable compounds.
Ultimately the idea is to apply this modelling
approach to other linguistic phenomena as well.
In particular, the objective is to model instances
of concatenative morphology beyond compound-
ing, with the aim of improving translation into
morphologically rich languages. Complex agree-
ment patterns could be captured by condition-
ing functional morphemes in the target word on
morphemes in the n-gram context, or by stem-
ming context words during back-off. Such ad-
ditional back-off paths can be readily encoded in
the Graphical Pitman-Yor process (Wood and Teh,
2009).
These more complex models may require
longer to train. To this end, I intend to use the
single table per dish approximation (§5) to reduce
training to a single deterministic pass through the
data, conjecturing that this will have little effect
on extrinsic performance.
</bodyText>
<sectionHeader confidence="0.99686" genericHeader="conclusions">
8 Summary
</sectionHeader>
<bodyText confidence="0.999968333333333">
I have argued for further explorations into the
use of a family of hierarchical Bayesian models
for targeting linguistic phenomena that may not
be captured well by standard n-gram language
models. To ground this investigation, I focused
on German compounds and showed how these
models are an appropriate vehicle for encoding
prior linguistic intuitions about such compounds.
The proposed generative model beats the popu-
lar modified Kneser-Ney model in monolingual
evaluations, and preliminarily achieves small im-
provements in translation from English into Ger-
man. In this translation task, single-token Ger-
man compounds traditionally pose challenges to
translation systems, and preliminary results show
a small increase in the F-score accuracy of com-
pounds in the translation output. Finally, I have
outlined the intended steps for expanding this line
of inquiry into other related linguistic phenomena
and for adapting a translation system to get opti-
mal value out of such improved language models.
</bodyText>
<sectionHeader confidence="0.995577" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999968166666667">
Thanks goes to my supervisor, Phil Blunsom, for
continued support and advice; to Chris Dyer for
suggesting the focus on German compounds and
supplying a freshly trained compound splitter; to
the Rhodes Trust for financial support; and to the
anonymous reviewers for their helpful feedback.
</bodyText>
<sectionHeader confidence="0.995966" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999356384615384">
Marco Baroni and Johannes Matiasek. 2002. Pre-
dicting the components of German nominal com-
pounds. In ECAI, pages 470–474.
Andre Berton, Pablo Fetter, and Peter Regel-
Brietzmann. 1996. Compound Words in Large-
Vocabulary German Speech Recognition Systems.
In Proceedings of Fourth International Conference
on Spoken Language Processing. ICSLP ’96, vol-
ume 2, pages 1165–1168. IEEE.
Jeff A Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel back-
off. In Proceedings of NAACL-HLT (short papers),
pages 4–6, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stanley F Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical report.
David Chiang. 2007. Hierarchical Phrase-
Based Translation. Computational Linguistics,
33(2):201–228, June.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Se-
tiawan, Vladimir Eidelman, and Philip Resnik.
2010. cdec: A Decoder, Alignment, and Learning
framework for finite-state and context-free trans-
lation models. In Proceedings of the Association
</reference>
<figure confidence="0.996607875">
0 10 20 0 0
Iteration
Perplexity
20
00
20
0
0
0
0
0
KN Ain
HPYLM
HPYLM+c Ain
mKN-baseline
72
</figure>
<reference confidence="0.995209727272727">
for Computational Linguistics (Demonstration ses-
sion), pages 7–12, Uppsala, Sweden. Association
for Computational Linguistics.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for MT. In Proceed-
ings of NAACL, pages 406–414. Association for
Computational Linguistics.
John Goldsmith and Tom Reutter. 1998. Automatic
Collection and Analysis of German Compounds. In
F. Busa F. et al., editor, The Computational Treat-
ment of Nominals, pages 61–69. Universite de Mon-
treal, Canada.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Interpolating Between Types and
Tokens by Estimating Power-Law Generators. In
Advances in Neural Information Processing Sys-
tems, Volume 18.
Songfang Huang and Steve Renals. 2007. Hierarchi-
cal Pitman-Yor Language Models For ASR in Meet-
ings. IEEEASRU, pages 124–129.
Songfang Huang and Steve Renals. 2009. A paral-
lel training algorithm for hierarchical Pitman-Yor
process language models. In Proceedings of Inter-
speech, volume 9, pages 2695–2698.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modelling. In
Proceedings of the IEEE International Conference
on Acoustics, Speech and SIgnal Processing, pages
181–184.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of EACL, pages 187–193. Association for Compu-
tational Linguistics.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better Machine Translation Qual-
ity for the German – English Language Pairs. In
Third Workshop on Statistical Machine Translation,
number June, pages 139–142. Association for Com-
putational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by Agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104–111, New York City,
USA, June. Association for Computational Linguis-
tics.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word seg-
mentation with nested Pitman-Yor language mod-
eling. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 1 - ACL-IJCNLP
’09, pages 100–108, Suntec, Singapore. Associa-
tion for Computational Linguistics.
Radford M Neal. 2003. Slice Sampling. The Annals
of Statistics, 31(3):705–741.
Graham Neubig, Masato Mimura, Shinsuke Mori, and
Tatsuya Kawahara. 2010. Learning a Language
Model from Continuous Speech. In Interspeech,
pages 1053–1056, Chiba, Japan.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL, pages 160–167.
Tsuyoshi Okita and Andy Way. 2010. Hierarchical
Pitman-Yor Language Model for Machine Transla-
tion. Proceedings of the International Conference
on Asian Language Processing, pages 245–248.
Kishore Papineni, Salim Roukos, Todd Ward, Wei-
jing Zhu, Thomas J Watson, and Yorktown Heights.
2001. Bleu: A Method for Automatic Evaluation of
Machine Translation. Technical report, IBM.
J Pitman and M. Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a sta-
ble subordinator. The Annals of Probability,
25:855–900.
J. Pitman. 2002. Combinatorial stochastic processes.
Technical report, Department of Statistics, Univer-
sity of California at Berkeley.
Sara Stymne. 2009. A comparison of merging strate-
gies for translation of German compounds. Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Student Research Workshop, pages 61–69.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the ACL, pages 985–992. Association for
Computational Linguistics.
Frank Wood and Yee Whye Teh. 2009. A Hierarchi-
cal Nonparametric Bayesian Approach to Statistical
Language Model Domain Adaptation. In Proceed-
ings of the 12th International Conference on Arti-
ficial Intelligence and Statistics (AISTATS), pages
607–614, Clearwater Beach, Florida, USA.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
Backoff Models for Machine Translation of Highly
Inflected Languages. In Proceedings of the EACL,
pages 41–48.
</reference>
<page confidence="0.954098">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961792">
<title confidence="0.9994835">Hierarchical Bayesian Language for the Linguistically Informed</title>
<author confidence="0.99968">A Jan</author>
<affiliation confidence="0.999963">Department of Computer University of Oxford,</affiliation>
<email confidence="0.99938">jan.botha@cs.ox.ac.uk</email>
<abstract confidence="0.997111153846154">In this work I address the challenge of augmenting n-gram language models according to prior linguistic intuitions. I argue that the family of hierarchical Pitman-Yor language models is an attractive vehicle through which to address the problem, and demonstrate the approach by proposing a model for German compounds. In an empirical evaluation, the model outperforms the Kneser-Ney model in terms of perplexity, and achieves preliminary improvements in English-German translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Johannes Matiasek</author>
</authors>
<title>Predicting the components of German nominal compounds.</title>
<date>2002</date>
<booktitle>In ECAI,</booktitle>
<pages>470--474</pages>
<contexts>
<context position="9246" citStr="Baroni and Matiasek, 2002" startWordPosition="1413" endWordPosition="1416">swers the question, “Given that the word ends in ‘infektionen’, what modifier(s), if any, are likely to precede it?” The vast majority of nouns will never occur in that position, meaning that the conditional distributions will be sharply peaked. mit der Draht·seil·bahn Figure 1: Intuition for the proposed generative process of a compound word: The context generates the head component, which generates a modifier component, which in turn generates another modifier. (Translation: “with the cable car”) have also been employed for speech recognition (Berton et al., 1996) and predictive-text input (Baroni and Matiasek, 2002), where single-token compounds also pose challenges. 4 Model Description 4.1 HPYLM Formally speaking, an n-gram model is an (n — 1)-th order Markov model that approximates the joint probability of a sequence of words w as P(w) � � |w |P(wi|wi−n+1, ... , wi−1), i=1 for which I will occasionally abbreviate a context [wi, ... , wj] as u. In the HPYLM, the conditional distributions P(w|u) are smoothed by placing Pitman-Yor process priors (PYP) over them. The PYP is defined through its base distribution, and a strength (θ) and discount (d) hyperparameter that control its deviation away from its mea</context>
</contexts>
<marker>Baroni, Matiasek, 2002</marker>
<rawString>Marco Baroni and Johannes Matiasek. 2002. Predicting the components of German nominal compounds. In ECAI, pages 470–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Berton</author>
<author>Pablo Fetter</author>
<author>Peter RegelBrietzmann</author>
</authors>
<title>Compound Words in LargeVocabulary German Speech Recognition Systems.</title>
<date>1996</date>
<booktitle>In Proceedings of Fourth International Conference on Spoken Language Processing. ICSLP ’96,</booktitle>
<volume>2</volume>
<pages>1165--1168</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="9192" citStr="Berton et al., 1996" startWordPosition="1406" endWordPosition="1409"> sentential context. Such a model essentially answers the question, “Given that the word ends in ‘infektionen’, what modifier(s), if any, are likely to precede it?” The vast majority of nouns will never occur in that position, meaning that the conditional distributions will be sharply peaked. mit der Draht·seil·bahn Figure 1: Intuition for the proposed generative process of a compound word: The context generates the head component, which generates a modifier component, which in turn generates another modifier. (Translation: “with the cable car”) have also been employed for speech recognition (Berton et al., 1996) and predictive-text input (Baroni and Matiasek, 2002), where single-token compounds also pose challenges. 4 Model Description 4.1 HPYLM Formally speaking, an n-gram model is an (n — 1)-th order Markov model that approximates the joint probability of a sequence of words w as P(w) � � |w |P(wi|wi−n+1, ... , wi−1), i=1 for which I will occasionally abbreviate a context [wi, ... , wj] as u. In the HPYLM, the conditional distributions P(w|u) are smoothed by placing Pitman-Yor process priors (PYP) over them. The PYP is defined through its base distribution, and a strength (θ) and discount (d) hyper</context>
</contexts>
<marker>Berton, Fetter, RegelBrietzmann, 1996</marker>
<rawString>Andre Berton, Pablo Fetter, and Peter RegelBrietzmann. 1996. Compound Words in LargeVocabulary German Speech Recognition Systems. In Proceedings of Fourth International Conference on Spoken Language Processing. ICSLP ’96, volume 2, pages 1165–1168. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff A Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-HLT (short papers),</booktitle>
<pages>4--6</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4146" citStr="Bilmes and Kirchhoff (2003)" startWordPosition="627" endWordPosition="630">al Linguistics its later modified version (Chen and Goodman, 1998) generally perform best at this smoothing, and are based on the idea that the number of distinct contexts a word appears in is an important factor in determining the probability of that word. Part of this smoothing involves discounting the counts of n-grams in the training data; the modified version uses different levels of discounting depending on the frequency of the count. These methods were designed with surface word distributions, and are not necessarily suitable for smoothing distributions of other kinds of surface units. Bilmes and Kirchhoff (2003) proposed a more general framework for n-gram language modelling. Their Factored Language Model (FLM) views a word as a vector of features, such that a particular feature value is generated conditional on some history of preceding feature values. This allowed the inclusion of n-gram models over sequences of elements like PoS tags and semantic classes. In tandem, they proposed more complicated back-off paths; for example, trigrams can back-off to two underlying bigram distributions, one dropping the left-most context word and the other the right-most. With the right combination of features and </context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Jeff A Bilmes and Katrin Kirchhoff. 2003. Factored language models and generalized parallel backoff. In Proceedings of NAACL-HLT (short papers), pages 4–6, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1998</date>
<tech>Technical report.</tech>
<contexts>
<context position="3585" citStr="Chen and Goodman, 1998" startWordPosition="536" endWordPosition="539">-gram models, as in many other probabilistic settings, is how to do smoothing, since the vast majority of linguistically plausible n-grams will occur rarely or be absent altogether from a training corpus, which often renders empirical model estimates misleading. The general picture is that probability mass must be shifted away from some events and redistributed across others. The method of Kneser and Ney (1995) and 64 Proceedings of the EACL 2012 Student Research Workshop, pages 64–73, Avignon, France, 26 April 2012. c�2012 Association for Computational Linguistics its later modified version (Chen and Goodman, 1998) generally perform best at this smoothing, and are based on the idea that the number of distinct contexts a word appears in is an important factor in determining the probability of that word. Part of this smoothing involves discounting the counts of n-grams in the training data; the modified version uses different levels of discounting depending on the frequency of the count. These methods were designed with surface word distributions, and are not necessarily suitable for smoothing distributions of other kinds of surface units. Bilmes and Kirchhoff (2003) proposed a more general framework for </context>
<context position="17900" citStr="Chen and Goodman, 1998" startWordPosition="2933" endWordPosition="2937">discount parameters control the extent to which new dishes are drawn, and thus the extent of reliance on the base distribution. The predictive probability of a word w given a seating arrangement is given by Pr(w |... ) a cw − dtw + (θ + dt)G0(w) In smoothing terminology, the first term can be interpreted as applying a discount of dtw to the observed count cw of w; the amount of discount therefore depends on the prevalence of the word (via tw). This is one significant way in which the PYP/CRP gives more nuanced smoothing than modified Kneser-Ney, which only uses four different discount levels (Chen and Goodman, 1998). Similarly, if the seating dynamics are constrained such that each dish is only served once (tw = 1 for any w), a single discount level is affected, establishing direct correspondence to original interpolated Kneser-Ney smoothing (Teh, 2006). Hierarchical CRP When the prior of Gu has a base distribution Gπ(u) that is itself PYPdistributed, as in the HPYLM, the restaurant metaphor changes slightly. In general, each node in the hierarchy has an associated restaurant. Whenever a new table is opened in some restaurant R, another customer is plucked out of thin air and sent to join the parent rest</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F Chen and Joshua Goodman. 1998. An Empirical Study of Smoothing Techniques for Language Modeling. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical PhraseBased Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="21112" citStr="Chiang, 2007" startWordPosition="3475" endWordPosition="3476">s in the proposed language models has positive effects on the predictability of unseen text and the generation of better translations. 6.1 Methods Data and Tools Standard data preprocessing steps included normalising punctuation, tokenising and lowercasing all words. All data sets are from the WMT11 shared-task.2. The full EnglishGerman bitext was filtered to exclude sentences longer than 50, resulting in 1.7 million parallel sentences; word alignments were inferred from this using the Berkeley Aligner (Liang et al., 2006) and used as basis from which to extract a Hiero-style synchronous CFG (Chiang, 2007). The weights of the log-linear translation models were tuned towards the BLEU metric on development data using cdec’s (Dyer et al., 2010) implementation of MERT (Och, 2003). For this, the set news-test2008 (2051 sentences) was used, while final case-insensitive BLEU scores are measured on the official test set newstest2011 (3003 sentences). All language models were trained on the target side of the preprocessed bitext containing 38 million tokens, and tested on all the German development data (i.e. news-test2008,9,10). Compound segmentation To construct a segmentation dictionary, I used the 1</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical PhraseBased Translation. Computational Linguistics, 33(2):201–228, June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Johnathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A Decoder, Alignment, and Learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (Demonstration session),</booktitle>
<pages>7--12</pages>
<institution>Uppsala, Sweden. Association for Computational Linguistics.</institution>
<contexts>
<context position="21250" citStr="Dyer et al., 2010" startWordPosition="3496" endWordPosition="3499">. 6.1 Methods Data and Tools Standard data preprocessing steps included normalising punctuation, tokenising and lowercasing all words. All data sets are from the WMT11 shared-task.2. The full EnglishGerman bitext was filtered to exclude sentences longer than 50, resulting in 1.7 million parallel sentences; word alignments were inferred from this using the Berkeley Aligner (Liang et al., 2006) and used as basis from which to extract a Hiero-style synchronous CFG (Chiang, 2007). The weights of the log-linear translation models were tuned towards the BLEU metric on development data using cdec’s (Dyer et al., 2010) implementation of MERT (Och, 2003). For this, the set news-test2008 (2051 sentences) was used, while final case-insensitive BLEU scores are measured on the official test set newstest2011 (3003 sentences). All language models were trained on the target side of the preprocessed bitext containing 38 million tokens, and tested on all the German development data (i.e. news-test2008,9,10). Compound segmentation To construct a segmentation dictionary, I used the 1-best segmentations from a supervised MaxEnt compound splitter (Dyer, 2009) run on all token types in bitext. In addition, word-internal h</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A Decoder, Alignment, and Learning framework for finite-state and context-free translation models. In Proceedings of the Association for Computational Linguistics (Demonstration session), pages 7–12, Uppsala, Sweden. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
</authors>
<title>Using a maximum entropy model to build segmentation lattices for MT.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>406--414</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21787" citStr="Dyer, 2009" startWordPosition="3580" endWordPosition="3581">wards the BLEU metric on development data using cdec’s (Dyer et al., 2010) implementation of MERT (Och, 2003). For this, the set news-test2008 (2051 sentences) was used, while final case-insensitive BLEU scores are measured on the official test set newstest2011 (3003 sentences). All language models were trained on the target side of the preprocessed bitext containing 38 million tokens, and tested on all the German development data (i.e. news-test2008,9,10). Compound segmentation To construct a segmentation dictionary, I used the 1-best segmentations from a supervised MaxEnt compound splitter (Dyer, 2009) run on all token types in bitext. In addition, word-internal hyphens were also taken 1Mark Johnson’s implementation, http://www.cog. brown.edu/˜mj/Software.htm 2http://www.statmt.org/wmt11/ as segmentation points. Finally, linker elements were merged onto components as discussed in §4.2. Any token that is split into more than one part by this procedure is regarded as a compound. The effect of the individual steps is summarised in Table 1. # Types Example None 350998 Geburtstagskind pre-merge 201328 Geburtstag·kind merge, Aling 150980 Geburtstags·kind merge, Ainv 162722 Geburtstag·skind Table </context>
</contexts>
<marker>Dyer, 2009</marker>
<rawString>Chris Dyer. 2009. Using a maximum entropy model to build segmentation lattices for MT. In Proceedings of NAACL, pages 406–414. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
<author>Tom Reutter</author>
</authors>
<title>Automatic Collection and Analysis of German Compounds.</title>
<date>1998</date>
<booktitle>The Computational Treatment of Nominals,</booktitle>
<pages>61--69</pages>
<editor>In F. Busa F. et al., editor,</editor>
<location>Montreal, Canada.</location>
<contexts>
<context position="7102" citStr="Goldsmith and Reutter, 1998" startWordPosition="1082" endWordPosition="1086">o exploit: the morphosyntactic properties of these compounds are often fully determined by the head component within the compound. For example, in “Geburtstagskind” (birthday kid), it is “Kind” that establishes this compound noun as singular neuter, which determine how it would need to agree with verbs, articles and adjectives. In the next section, I propose a model in the suggested framework that encodes this intuition. The basic structure of German compounds comprises a head component, preceded by one or more modifier components, with optional linker elements between consecutive components (Goldsmith and Reutter, 1998). Examples • The basic form is just the concatenation of two nouns Auto + Unfall = Autounfall (car crash) • Linker elements are sometimes added between components K¨uche + Tisch = K¨uchentisch (kitchen table) • Components can undergo stemming during composition Schule + Hof = Schulhof (schoolyard) • The process is potentially recursive (Geburt + Tag) + Kind = Geburtstag + Kind = Geburtstagskind (birthday kid) The process is not limited to using nouns as components, for example, the numeral in ZweiEuro-M¨unze (two Euro coin) or the verb “fahren” (to drive) in Fahrzeug (vehicle). I will treat al</context>
<context position="14238" citStr="Goldsmith and Reutter, 1998" startWordPosition="2269" endWordPosition="2272">uche·n·tisch, we want P(K¨uche|Tisch) in the model, but not P (K¨uche|n). But linker elements need to be accounted for somehow to have a well-defined generative model. I follow the pragmatic option of merging any linkers onto the adjacent component – for Aling merging happens onto the preceding component, while for Ainv it is onto the succeeding one. This keeps the ‘head’ component cA1 in tact. More involved strategies could be considered, and it is worth noting that for German the presence and identity of linker elements between ci and ci+1 are in fact governed by the preceding component ci (Goldsmith and Reutter, 1998). 5 Training For ease of exposition I describe inference with reference to the trigram HPYLM+c model with a bigram HPYLM for F, but the general case should be clear. The model is specified by the latent variables (G[∅], G[v], G[u,v], H[u,v], F∅, Fc), where u, v ∈ W, c ∈ M, and hyperparameters Q = {di, θi} ∪ {d0j, θ0j} ∪ {d00�, θ00�}, where i = 0, 1, 2, j = 0, 1, single primes designate the hyperparameters in FHPY LM and double primes those of H[u,v]. We can construct a collapsed Gibbs sampler by marginalising out these latent variables, giving rise to a variant of the hierarchical Chinese Rest</context>
</contexts>
<marker>Goldsmith, Reutter, 1998</marker>
<rawString>John Goldsmith and Tom Reutter. 1998. Automatic Collection and Analysis of German Compounds. In F. Busa F. et al., editor, The Computational Treatment of Nominals, pages 61–69. Universite de Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating Between Types and Tokens by Estimating Power-Law Generators.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems, Volume</booktitle>
<volume>18</volume>
<contexts>
<context position="1519" citStr="Goldwater et al., 2006" startWordPosition="218" endWordPosition="221">orkhorse for these tasks. These models are not ideal for languages that have relatively free word order and/or complex morphology. The ability to encode additional linguistic intuitions into models that already have certain attractive properties is an important piece of the puzzle of improving machine translation quality for those languages. But despite their widespread use, KN n-gram models are not easily extensible with additional model components that target particular linguistic phenomena. I argue in this paper that the family of hierarchical Pitman-Yor language models (HPYLM) (Teh, 2006; Goldwater et al., 2006) are suitable for investigations into more linguistically-informed n-gram language models. Firstly, the flexibility to specify arbitrary back-off distributions makes it easy to incorporate multiple models into a larger n-gram model. Secondly, the Pitman-Yor process prior (Pitman and Yor, 1997) generates distributions that are well-suited to a variety of powerlaw behaviours, as is often observed in language. Catering for a variety of those is important since the frequency distributions of, say, suffixes, could be quite different from that of words. KN smoothing is less flexibility in this regar</context>
<context position="5472" citStr="Goldwater et al., 2006" startWordPosition="834" endWordPosition="838">ality by applying these ideas to the smoothing of the bilingual phrase table (Yang and Kirchhoff, 2006). My approach has some similarity to the FLM: both decompose surface word forms into elements that are generated from unrelated conditional distributions. They differ predominantly along two dimensions: the types of decompositions and conditioning possible, and my use of a particular Bayesian prior for handling smoothing. In addition to the HPYLM for n-gram language modelling (Teh, 2006), models based on the Pitman-Yor process prior have also been applied to good effect in word segmentation (Goldwater et al., 2006; Mochihashi et al., 2009) and speech recognition (Huang and Renals, 2007; Neubig et al., 2010). The Graphical Pitman-Yor process enables branching back-off paths, which I briefly revisit in §7, and have proved effective in language model domain-adaptation (Wood and Teh, 2009). Here, I extend this general line of inquiry by considering how one might incorporate linguistically informed sub-models into the HPYLM framework. 3 Compound Nouns I focus on compound nouns in this work for two reasons: Firstly, compounding is in general a very productive process, and in some languages (including German,</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Interpolating Between Types and Tokens by Estimating Power-Law Generators. In Advances in Neural Information Processing Systems, Volume 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Songfang Huang</author>
<author>Steve Renals</author>
</authors>
<title>Hierarchical Pitman-Yor Language Models For ASR in Meetings.</title>
<date>2007</date>
<booktitle>IEEEASRU,</booktitle>
<pages>124--129</pages>
<contexts>
<context position="5545" citStr="Huang and Renals, 2007" startWordPosition="846" endWordPosition="849">ble (Yang and Kirchhoff, 2006). My approach has some similarity to the FLM: both decompose surface word forms into elements that are generated from unrelated conditional distributions. They differ predominantly along two dimensions: the types of decompositions and conditioning possible, and my use of a particular Bayesian prior for handling smoothing. In addition to the HPYLM for n-gram language modelling (Teh, 2006), models based on the Pitman-Yor process prior have also been applied to good effect in word segmentation (Goldwater et al., 2006; Mochihashi et al., 2009) and speech recognition (Huang and Renals, 2007; Neubig et al., 2010). The Graphical Pitman-Yor process enables branching back-off paths, which I briefly revisit in §7, and have proved effective in language model domain-adaptation (Wood and Teh, 2009). Here, I extend this general line of inquiry by considering how one might incorporate linguistically informed sub-models into the HPYLM framework. 3 Compound Nouns I focus on compound nouns in this work for two reasons: Firstly, compounding is in general a very productive process, and in some languages (including German, Swedish and Dutch) they are written as single orthographic units. This i</context>
</contexts>
<marker>Huang, Renals, 2007</marker>
<rawString>Songfang Huang and Steve Renals. 2007. Hierarchical Pitman-Yor Language Models For ASR in Meetings. IEEEASRU, pages 124–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Songfang Huang</author>
<author>Steve Renals</author>
</authors>
<title>A parallel training algorithm for hierarchical Pitman-Yor process language models.</title>
<date>2009</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<volume>9</volume>
<pages>2695--2698</pages>
<contexts>
<context position="2213" citStr="Huang and Renals, 2009" startWordPosition="322" endWordPosition="325">am language models. Firstly, the flexibility to specify arbitrary back-off distributions makes it easy to incorporate multiple models into a larger n-gram model. Secondly, the Pitman-Yor process prior (Pitman and Yor, 1997) generates distributions that are well-suited to a variety of powerlaw behaviours, as is often observed in language. Catering for a variety of those is important since the frequency distributions of, say, suffixes, could be quite different from that of words. KN smoothing is less flexibility in this regard. And thirdly, the basic inference algorithms have been parallelised (Huang and Renals, 2009), which should in principle allow the approach to still scale to large data sizes. As a test bed, I consider compounding in German, a common phenomenon that creates challenges for machine translation into German. 2 Background and Related Work n-gram language models assign probabilities to word sequences. Their key approximation is that a word is assumed to be fully determined by n−1 words preceding it, which keeps the number of independent probabilities to estimate in a range that is computationally attractive. This basic model structure, largely devoid of syntactic insight, is surprisingly ef</context>
</contexts>
<marker>Huang, Renals, 2009</marker>
<rawString>Songfang Huang and Steve Renals. 2009. A parallel training algorithm for hierarchical Pitman-Yor process language models. In Proceedings of Interspeech, volume 9, pages 2695–2698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modelling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and SIgnal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="3376" citStr="Kneser and Ney (1995)" startWordPosition="506" endWordPosition="509">e, largely devoid of syntactic insight, is surprisingly effective at biasing MT and ASR systems toward more fluent output, given a suitable choice of target language. But the real challenge in constructing n-gram models, as in many other probabilistic settings, is how to do smoothing, since the vast majority of linguistically plausible n-grams will occur rarely or be absent altogether from a training corpus, which often renders empirical model estimates misleading. The general picture is that probability mass must be shifted away from some events and redistributed across others. The method of Kneser and Ney (1995) and 64 Proceedings of the EACL 2012 Student Research Workshop, pages 64–73, Avignon, France, 26 April 2012. c�2012 Association for Computational Linguistics its later modified version (Chen and Goodman, 1998) generally perform best at this smoothing, and are based on the idea that the number of distinct contexts a word appears in is an important factor in determining the probability of that word. Part of this smoothing involves discounting the counts of n-grams in the training data; the modified version uses different levels of discounting depending on the frequency of the count. These method</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modelling. In Proceedings of the IEEE International Conference on Acoustics, Speech and SIgnal Processing, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical Methods for Compound Splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>187--193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10751" citStr="Koehn and Knight, 2003" startWordPosition="1661" endWordPosition="1664">distribution Go, which is drawn from a PYP with base distribution equal to the uniform distribution over the vocabulary W. The hyperparameters are tied across all priors with the same context length |u|, and estimated during training. G0 = Uniform(|W|) Go — PY (d0, θ0, G0) ... 3.2 Related Work on Compounds In machine translation and speech recognition, one approach has been to split compounds as a preprocessing step and merge them back together during postprocessing, while using otherwise unmodified NLP systems. Frequency-based methods have been used for determining how aggressively to split (Koehn and Knight, 2003), since the maximal, linguistically correct segmentation is not necessarily optimal for translation. This gave rise to slight improvements in machine translation evaluations (Koehn et al., 2008), with finetuning explored in (Stymne, 2009). Similar ideas Gπ(u) — PY (d|π(u)|, θ|π(u)|, Gπ(π(u))) Gu — PY (d|u|, θ|u|, Gπ(u)) w — Gu, where π(u) truncates the context u by dropping the left-most word in it. 4.2 HPYLM+c Define a compound word w� as a sequence of components [c1, ... , ck], plus a sentinel symbol $ marking either the left or the right boundary of the word, depending on the direction of t</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical Methods for Compound Splitting. In Proceedings of EACL, pages 187–193. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Abhishek Arun</author>
<author>Hieu Hoang</author>
</authors>
<title>Towards better Machine Translation Quality for the German – English Language Pairs.</title>
<date>2008</date>
<booktitle>In Third Workshop on Statistical Machine Translation, number June,</booktitle>
<pages>139--142</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10945" citStr="Koehn et al., 2008" startWordPosition="1688" endWordPosition="1691"> |u|, and estimated during training. G0 = Uniform(|W|) Go — PY (d0, θ0, G0) ... 3.2 Related Work on Compounds In machine translation and speech recognition, one approach has been to split compounds as a preprocessing step and merge them back together during postprocessing, while using otherwise unmodified NLP systems. Frequency-based methods have been used for determining how aggressively to split (Koehn and Knight, 2003), since the maximal, linguistically correct segmentation is not necessarily optimal for translation. This gave rise to slight improvements in machine translation evaluations (Koehn et al., 2008), with finetuning explored in (Stymne, 2009). Similar ideas Gπ(u) — PY (d|π(u)|, θ|π(u)|, Gπ(π(u))) Gu — PY (d|u|, θ|u|, Gπ(u)) w — Gu, where π(u) truncates the context u by dropping the left-most word in it. 4.2 HPYLM+c Define a compound word w� as a sequence of components [c1, ... , ck], plus a sentinel symbol $ marking either the left or the right boundary of the word, depending on the direction of the model. To maintain generality over this choice of direction, 66 let A be an index set over the positions, such that cA1 always designates the head component. Following the motivation in §3.1,</context>
</contexts>
<marker>Koehn, Arun, Hoang, 2008</marker>
<rawString>Philipp Koehn, Abhishek Arun, and Hieu Hoang. 2008. Towards better Machine Translation Quality for the German – English Language Pairs. In Third Workshop on Statistical Machine Translation, number June, pages 139–142. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by Agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="21027" citStr="Liang et al., 2006" startWordPosition="3459" endWordPosition="3462"> The aim of the experiments reported here is to test whether the richer account of compounds in the proposed language models has positive effects on the predictability of unseen text and the generation of better translations. 6.1 Methods Data and Tools Standard data preprocessing steps included normalising punctuation, tokenising and lowercasing all words. All data sets are from the WMT11 shared-task.2. The full EnglishGerman bitext was filtered to exclude sentences longer than 50, resulting in 1.7 million parallel sentences; word alignments were inferred from this using the Berkeley Aligner (Liang et al., 2006) and used as basis from which to extract a Hiero-style synchronous CFG (Chiang, 2007). The weights of the log-linear translation models were tuned towards the BLEU metric on development data using cdec’s (Dyer et al., 2010) implementation of MERT (Och, 2003). For this, the set news-test2008 (2051 sentences) was used, while final case-insensitive BLEU scores are measured on the official test set newstest2011 (3003 sentences). All language models were trained on the target side of the preprocessed bitext containing 38 million tokens, and tested on all the German development data (i.e. news-test2</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by Agreement. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 104–111, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - ACL-IJCNLP ’09,</booktitle>
<pages>100--108</pages>
<institution>Suntec, Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="5498" citStr="Mochihashi et al., 2009" startWordPosition="839" endWordPosition="842">ideas to the smoothing of the bilingual phrase table (Yang and Kirchhoff, 2006). My approach has some similarity to the FLM: both decompose surface word forms into elements that are generated from unrelated conditional distributions. They differ predominantly along two dimensions: the types of decompositions and conditioning possible, and my use of a particular Bayesian prior for handling smoothing. In addition to the HPYLM for n-gram language modelling (Teh, 2006), models based on the Pitman-Yor process prior have also been applied to good effect in word segmentation (Goldwater et al., 2006; Mochihashi et al., 2009) and speech recognition (Huang and Renals, 2007; Neubig et al., 2010). The Graphical Pitman-Yor process enables branching back-off paths, which I briefly revisit in §7, and have proved effective in language model domain-adaptation (Wood and Teh, 2009). Here, I extend this general line of inquiry by considering how one might incorporate linguistically informed sub-models into the HPYLM framework. 3 Compound Nouns I focus on compound nouns in this work for two reasons: Firstly, compounding is in general a very productive process, and in some languages (including German, Swedish and Dutch) they a</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - ACL-IJCNLP ’09, pages 100–108, Suntec, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Slice Sampling.</title>
<date>2003</date>
<journal>The Annals of Statistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="20167" citStr="Neal, 2003" startWordPosition="3324" endWordPosition="3325">arlo methods. The simple form of the conditionals in the CRP allows us to do a Gibbs update whereby the table index k of a customer is resampled conditioned on all the other variables. Sampling a new seating arrangement S for the trigram HPYLM+c thus corresponds to visiting each customer in the restaurants for H[u,v], removing them while cascading as necessary to observe the consistency 68 across the hierarchy, and seating them anew at some table W. In the absence of any strong intuitions about appropriate values for the hyperparameters, I place vague priors over them and use slice sampling1 (Neal, 2003) to update their values during generation of the posterior samples: d — Beta(1,1) 0 — Gamma(1,1) Lastly, I make the further approximation of m = 1, i.e. predictive probabilities are informed by a single posterior sample (5, Q). 6 Experiments The aim of the experiments reported here is to test whether the richer account of compounds in the proposed language models has positive effects on the predictability of unseen text and the generation of better translations. 6.1 Methods Data and Tools Standard data preprocessing steps included normalising punctuation, tokenising and lowercasing all words. </context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>Radford M Neal. 2003. Slice Sampling. The Annals of Statistics, 31(3):705–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Masato Mimura</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>Learning a Language Model from Continuous Speech. In</title>
<date>2010</date>
<booktitle>Interspeech,</booktitle>
<pages>1053--1056</pages>
<location>Chiba, Japan.</location>
<contexts>
<context position="5567" citStr="Neubig et al., 2010" startWordPosition="850" endWordPosition="853"> 2006). My approach has some similarity to the FLM: both decompose surface word forms into elements that are generated from unrelated conditional distributions. They differ predominantly along two dimensions: the types of decompositions and conditioning possible, and my use of a particular Bayesian prior for handling smoothing. In addition to the HPYLM for n-gram language modelling (Teh, 2006), models based on the Pitman-Yor process prior have also been applied to good effect in word segmentation (Goldwater et al., 2006; Mochihashi et al., 2009) and speech recognition (Huang and Renals, 2007; Neubig et al., 2010). The Graphical Pitman-Yor process enables branching back-off paths, which I briefly revisit in §7, and have proved effective in language model domain-adaptation (Wood and Teh, 2009). Here, I extend this general line of inquiry by considering how one might incorporate linguistically informed sub-models into the HPYLM framework. 3 Compound Nouns I focus on compound nouns in this work for two reasons: Firstly, compounding is in general a very productive process, and in some languages (including German, Swedish and Dutch) they are written as single orthographic units. This increases data sparsity</context>
</contexts>
<marker>Neubig, Mimura, Mori, Kawahara, 2010</marker>
<rawString>Graham Neubig, Masato Mimura, Shinsuke Mori, and Tatsuya Kawahara. 2010. Learning a Language Model from Continuous Speech. In Interspeech, pages 1053–1056, Chiba, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="21285" citStr="Och, 2003" startWordPosition="3503" endWordPosition="3504">preprocessing steps included normalising punctuation, tokenising and lowercasing all words. All data sets are from the WMT11 shared-task.2. The full EnglishGerman bitext was filtered to exclude sentences longer than 50, resulting in 1.7 million parallel sentences; word alignments were inferred from this using the Berkeley Aligner (Liang et al., 2006) and used as basis from which to extract a Hiero-style synchronous CFG (Chiang, 2007). The weights of the log-linear translation models were tuned towards the BLEU metric on development data using cdec’s (Dyer et al., 2010) implementation of MERT (Och, 2003). For this, the set news-test2008 (2051 sentences) was used, while final case-insensitive BLEU scores are measured on the official test set newstest2011 (3003 sentences). All language models were trained on the target side of the preprocessed bitext containing 38 million tokens, and tested on all the German development data (i.e. news-test2008,9,10). Compound segmentation To construct a segmentation dictionary, I used the 1-best segmentations from a supervised MaxEnt compound splitter (Dyer, 2009) run on all token types in bitext. In addition, word-internal hyphens were also taken 1Mark Johnso</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsuyoshi Okita</author>
<author>Andy Way</author>
</authors>
<title>Hierarchical Pitman-Yor Language Model for Machine Translation.</title>
<date>2010</date>
<booktitle>Proceedings of the International Conference on Asian Language Processing,</booktitle>
<pages>245--248</pages>
<contexts>
<context position="25420" citStr="Okita and Way, 2010" startWordPosition="4166" endWordPosition="4169">2 Table 3: Translation results, BLEU (1-ref), 3003 test sentences. Trigram language models, no count pruning, no “unknown word” token. P / R / F mKN 22.0 / 17.3 / 19.4 HPYLM 21.0 / 17.8 / 19.3 FHPYLM, Aling 23.6 / 17.3 / 19.9 FHPYLM, Ainv 24.1 / 16.5 / 19.6 Table 4: Precision, Recall and F-score of compound translations, relative to reference set (72661 tokens, of which 2649 are compounds). 6.2 Main Results For the monolingual evaluation, I used an interpolated, modified Kneser-Ney model (mKN) and an HPYLM as baselines. It has been shown for other languages that HPYLM tends to outperform mKN (Okita and Way, 2010), but I am not aware of this result being demonstrated on German before, as I do in Table 2. The main model of interest is HPYLM+c using the Aling segmentation and a model FHPYLM over modifiers; this model achieves the lowest perplexity, 4.4% lower than the mKN baseline. Next, note that using FKN to handle the modifiers does worse than FHPYLM, confirming our expectation that KN is less appropriate for that task, although it still does better than the original mKN baseline. The models that use the linguistically implausible segmentation scheme Ainv both fare worse than their counterparts that u</context>
</contexts>
<marker>Okita, Way, 2010</marker>
<rawString>Tsuyoshi Okita and Andy Way. 2010. Hierarchical Pitman-Yor Language Model for Machine Translation. Proceedings of the International Conference on Asian Language Processing, pages 245–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
<author>Thomas J Watson</author>
<author>Yorktown Heights</author>
</authors>
<title>Bleu: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<tech>Technical report, IBM.</tech>
<contexts>
<context position="23674" citStr="Papineni et al., 2001" startWordPosition="3883" endWordPosition="3886">e same test set. The exponent above can be regarded as an approximation of the cross-entropy between the model q and a hypothetical model p from which both the training and test set were putatively generated. It is sometimes convenient to use this as an alternative measure. But a language model only really becomes useful when it allows some extrinsic task to be executed better. When that extrinsic task is machine translation, the translation quality can be assessed to see if one language model aids it more than another. The obligatory metric for evaluating machine translation quality is BLEU (Papineni et al., 2001), a precision based metric that measures how close the machine output is to a known correct translation (the reference sentences in the test set). Higher precision means the translation system is getting more phrases right. Better language model perplexities sometimes lead to improvements in translation quality, but it is not guaranteed. Moreover, even when real translation improvements are obtained, they are 69 PPL c-Cross-ent. mKN 441.32 0.1981 HPYLM 429.17 0.1994 FKN Aling 432.95 0.2028 FKN Ainv 446.84 0.2125 FHPYLM Aling 421.63 0.1987 FHPYLM Ainv 435.79 0.2079 Table 2: Monolingual evaluati</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, Watson, Heights, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, Weijing Zhu, Thomas J Watson, and Yorktown Heights. 2001. Bleu: A Method for Automatic Evaluation of Machine Translation. Technical report, IBM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pitman</author>
<author>M Yor</author>
</authors>
<title>The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. The Annals of Probability,</title>
<date>1997</date>
<pages>25--855</pages>
<contexts>
<context position="1813" citStr="Pitman and Yor, 1997" startWordPosition="257" endWordPosition="260">achine translation quality for those languages. But despite their widespread use, KN n-gram models are not easily extensible with additional model components that target particular linguistic phenomena. I argue in this paper that the family of hierarchical Pitman-Yor language models (HPYLM) (Teh, 2006; Goldwater et al., 2006) are suitable for investigations into more linguistically-informed n-gram language models. Firstly, the flexibility to specify arbitrary back-off distributions makes it easy to incorporate multiple models into a larger n-gram model. Secondly, the Pitman-Yor process prior (Pitman and Yor, 1997) generates distributions that are well-suited to a variety of powerlaw behaviours, as is often observed in language. Catering for a variety of those is important since the frequency distributions of, say, suffixes, could be quite different from that of words. KN smoothing is less flexibility in this regard. And thirdly, the basic inference algorithms have been parallelised (Huang and Renals, 2009), which should in principle allow the approach to still scale to large data sizes. As a test bed, I consider compounding in German, a common phenomenon that creates challenges for machine translation </context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>J Pitman and M. Yor. 1997. The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. The Annals of Probability, 25:855–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pitman</author>
</authors>
<title>Combinatorial stochastic processes.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>Department of Statistics, University of California at Berkeley.</institution>
<contexts>
<context position="15155" citStr="Pitman, 2002" startWordPosition="2434" endWordPosition="2435">i} ∪ {d0j, θ0j} ∪ {d00�, θ00�}, where i = 0, 1, 2, j = 0, 1, single primes designate the hyperparameters in FHPY LM and double primes those of H[u,v]. We can construct a collapsed Gibbs sampler by marginalising out these latent variables, giving rise to a variant of the hierarchical Chinese Restaurant Process in which it is straightforward to do inference. Chinese Restaurant Process A direct representation of a random variable G drawn from a PYP can be obtained from the so-called stickbreaking construction. But the more indirect representation by means of the Chinese Restaurant Process (CRP) (Pitman, 2002) is more suitable here since it relates to distributions over items drawn from such a G. This fits the current setting, where words w are being drawn from a PYPdistributed G. Imagine that a corpus is created in two phases: Firstly, a sequence of blank tokens xi is instantiated, and in a second phase lexical identities wi are assigned to these tokens, giving rise to the 67 observed corpus. In the CRP metaphor , the sequence of tokens xi are equated with a sequence of customers that enter a restaurant one-by-one to be seated at one of an infinite number of tables. When a customer sits at an unoc</context>
</contexts>
<marker>Pitman, 2002</marker>
<rawString>J. Pitman. 2002. Combinatorial stochastic processes. Technical report, Department of Statistics, University of California at Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
</authors>
<title>A comparison of merging strategies for translation of German compounds.</title>
<date>2009</date>
<booktitle>Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop,</booktitle>
<pages>61--69</pages>
<contexts>
<context position="10989" citStr="Stymne, 2009" startWordPosition="1697" endWordPosition="1698">|W|) Go — PY (d0, θ0, G0) ... 3.2 Related Work on Compounds In machine translation and speech recognition, one approach has been to split compounds as a preprocessing step and merge them back together during postprocessing, while using otherwise unmodified NLP systems. Frequency-based methods have been used for determining how aggressively to split (Koehn and Knight, 2003), since the maximal, linguistically correct segmentation is not necessarily optimal for translation. This gave rise to slight improvements in machine translation evaluations (Koehn et al., 2008), with finetuning explored in (Stymne, 2009). Similar ideas Gπ(u) — PY (d|π(u)|, θ|π(u)|, Gπ(π(u))) Gu — PY (d|u|, θ|u|, Gπ(u)) w — Gu, where π(u) truncates the context u by dropping the left-most word in it. 4.2 HPYLM+c Define a compound word w� as a sequence of components [c1, ... , ck], plus a sentinel symbol $ marking either the left or the right boundary of the word, depending on the direction of the model. To maintain generality over this choice of direction, 66 let A be an index set over the positions, such that cA1 always designates the head component. Following the motivation in §3.1, I set up the model to generate the head com</context>
</contexts>
<marker>Stymne, 2009</marker>
<rawString>Sara Stymne. 2009. A comparison of merging strategies for translation of German compounds. Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 61–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL,</booktitle>
<pages>985--992</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1494" citStr="Teh, 2006" startWordPosition="216" endWordPosition="217"> standard workhorse for these tasks. These models are not ideal for languages that have relatively free word order and/or complex morphology. The ability to encode additional linguistic intuitions into models that already have certain attractive properties is an important piece of the puzzle of improving machine translation quality for those languages. But despite their widespread use, KN n-gram models are not easily extensible with additional model components that target particular linguistic phenomena. I argue in this paper that the family of hierarchical Pitman-Yor language models (HPYLM) (Teh, 2006; Goldwater et al., 2006) are suitable for investigations into more linguistically-informed n-gram language models. Firstly, the flexibility to specify arbitrary back-off distributions makes it easy to incorporate multiple models into a larger n-gram model. Secondly, the Pitman-Yor process prior (Pitman and Yor, 1997) generates distributions that are well-suited to a variety of powerlaw behaviours, as is often observed in language. Catering for a variety of those is important since the frequency distributions of, say, suffixes, could be quite different from that of words. KN smoothing is less </context>
<context position="5343" citStr="Teh, 2006" startWordPosition="814" endWordPosition="815">features and back-off structure they got good perplexity reductions, and obtained some improvements in translation quality by applying these ideas to the smoothing of the bilingual phrase table (Yang and Kirchhoff, 2006). My approach has some similarity to the FLM: both decompose surface word forms into elements that are generated from unrelated conditional distributions. They differ predominantly along two dimensions: the types of decompositions and conditioning possible, and my use of a particular Bayesian prior for handling smoothing. In addition to the HPYLM for n-gram language modelling (Teh, 2006), models based on the Pitman-Yor process prior have also been applied to good effect in word segmentation (Goldwater et al., 2006; Mochihashi et al., 2009) and speech recognition (Huang and Renals, 2007; Neubig et al., 2010). The Graphical Pitman-Yor process enables branching back-off paths, which I briefly revisit in §7, and have proved effective in language model domain-adaptation (Wood and Teh, 2009). Here, I extend this general line of inquiry by considering how one might incorporate linguistically informed sub-models into the HPYLM framework. 3 Compound Nouns I focus on compound nouns in </context>
<context position="18142" citStr="Teh, 2006" startWordPosition="2973" endWordPosition="2974">ing terminology, the first term can be interpreted as applying a discount of dtw to the observed count cw of w; the amount of discount therefore depends on the prevalence of the word (via tw). This is one significant way in which the PYP/CRP gives more nuanced smoothing than modified Kneser-Ney, which only uses four different discount levels (Chen and Goodman, 1998). Similarly, if the seating dynamics are constrained such that each dish is only served once (tw = 1 for any w), a single discount level is affected, establishing direct correspondence to original interpolated Kneser-Ney smoothing (Teh, 2006). Hierarchical CRP When the prior of Gu has a base distribution Gπ(u) that is itself PYPdistributed, as in the HPYLM, the restaurant metaphor changes slightly. In general, each node in the hierarchy has an associated restaurant. Whenever a new table is opened in some restaurant R, another customer is plucked out of thin air and sent to join the parent restaurant pa(R). This induces a consistency constraint over the hierarchy: the number of tables tw in restaurant R must equal the number of customers cw in its parent pa(R). In the proposed HPYLM+c model using FHPYLM, there is a further constrai</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL, pages 985–992. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Wood</author>
<author>Yee Whye Teh</author>
</authors>
<title>A Hierarchical Nonparametric Bayesian Approach to Statistical Language Model Domain Adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS),</booktitle>
<pages>607--614</pages>
<location>Clearwater Beach, Florida, USA.</location>
<contexts>
<context position="5749" citStr="Wood and Teh, 2009" startWordPosition="876" endWordPosition="879">tly along two dimensions: the types of decompositions and conditioning possible, and my use of a particular Bayesian prior for handling smoothing. In addition to the HPYLM for n-gram language modelling (Teh, 2006), models based on the Pitman-Yor process prior have also been applied to good effect in word segmentation (Goldwater et al., 2006; Mochihashi et al., 2009) and speech recognition (Huang and Renals, 2007; Neubig et al., 2010). The Graphical Pitman-Yor process enables branching back-off paths, which I briefly revisit in §7, and have proved effective in language model domain-adaptation (Wood and Teh, 2009). Here, I extend this general line of inquiry by considering how one might incorporate linguistically informed sub-models into the HPYLM framework. 3 Compound Nouns I focus on compound nouns in this work for two reasons: Firstly, compounding is in general a very productive process, and in some languages (including German, Swedish and Dutch) they are written as single orthographic units. This increases data sparsity and creates significant challenges for NLP systems that use whitespace to identify their elementary modelling units. A proper account of compounds in terms of their component words </context>
<context position="32996" citStr="Wood and Teh, 2009" startWordPosition="5416" endWordPosition="5419"> into this category of unseenbut-recognisable compounds. Ultimately the idea is to apply this modelling approach to other linguistic phenomena as well. In particular, the objective is to model instances of concatenative morphology beyond compounding, with the aim of improving translation into morphologically rich languages. Complex agreement patterns could be captured by conditioning functional morphemes in the target word on morphemes in the n-gram context, or by stemming context words during back-off. Such additional back-off paths can be readily encoded in the Graphical Pitman-Yor process (Wood and Teh, 2009). These more complex models may require longer to train. To this end, I intend to use the single table per dish approximation (§5) to reduce training to a single deterministic pass through the data, conjecturing that this will have little effect on extrinsic performance. 8 Summary I have argued for further explorations into the use of a family of hierarchical Bayesian models for targeting linguistic phenomena that may not be captured well by standard n-gram language models. To ground this investigation, I focused on German compounds and showed how these models are an appropriate vehicle for en</context>
</contexts>
<marker>Wood, Teh, 2009</marker>
<rawString>Frank Wood and Yee Whye Teh. 2009. A Hierarchical Nonparametric Bayesian Approach to Statistical Language Model Domain Adaptation. In Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 607–614, Clearwater Beach, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mei Yang</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Phrase-based Backoff Models for Machine Translation of Highly Inflected Languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="4953" citStr="Yang and Kirchhoff, 2006" startWordPosition="752" endWordPosition="755">nerated conditional on some history of preceding feature values. This allowed the inclusion of n-gram models over sequences of elements like PoS tags and semantic classes. In tandem, they proposed more complicated back-off paths; for example, trigrams can back-off to two underlying bigram distributions, one dropping the left-most context word and the other the right-most. With the right combination of features and back-off structure they got good perplexity reductions, and obtained some improvements in translation quality by applying these ideas to the smoothing of the bilingual phrase table (Yang and Kirchhoff, 2006). My approach has some similarity to the FLM: both decompose surface word forms into elements that are generated from unrelated conditional distributions. They differ predominantly along two dimensions: the types of decompositions and conditioning possible, and my use of a particular Bayesian prior for handling smoothing. In addition to the HPYLM for n-gram language modelling (Teh, 2006), models based on the Pitman-Yor process prior have also been applied to good effect in word segmentation (Goldwater et al., 2006; Mochihashi et al., 2009) and speech recognition (Huang and Renals, 2007; Neubig</context>
</contexts>
<marker>Yang, Kirchhoff, 2006</marker>
<rawString>Mei Yang and Katrin Kirchhoff. 2006. Phrase-based Backoff Models for Machine Translation of Highly Inflected Languages. In Proceedings of the EACL, pages 41–48.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>