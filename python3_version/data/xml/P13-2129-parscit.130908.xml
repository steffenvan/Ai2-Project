<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000060">
<note confidence="0.5655678">
Diathesis alternation approximation for verb clustering
Lin Sun
Greedy Intelligence Ltd
Hangzhou, China
lin.sun@greedyint.com
Diana McCarthy and Anna Korhonen
DTAL and Computer Laboratory
University of Cambridge
Cambridge, UK
diana@dianamccarthy.co.uk
</note>
<email confidence="0.986552">
alk23@cam.ac.uk
</email>
<sectionHeader confidence="0.994525" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999936277777778">
Although diathesis alternations have been
used as features for manual verb clas-
sification, and there is recent work on
incorporating such features in computa-
tional models of human language acquisi-
tion, work on large scale verb classifica-
tion has yet to examine the potential for
using diathesis alternations as input fea-
tures to the clustering process. This pa-
per proposes a method for approximating
diathesis alternation behaviour in corpus
data and shows, using a state-of-the-art
verb clustering system, that features based
on alternation approximation outperform
those based on independent subcategoriza-
tion frames. Our alternation-based ap-
proach is particularly adept at leveraging
information from less frequent data.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999920737704918">
Diathesis alternations (DAs) are regular alterna-
tions of the syntactic expression of verbal argu-
ments, sometimes accompanied by a change in
meaning. For example, The man broke the win-
dow ↔ The window broke. The syntactic phe-
nomena are triggered by the underlying semantics
of the participating verbs. Levin (1993)’s seminal
book provides a manual inventory both of DAs and
verb classes where membership is determined ac-
cording to participation in these alternations. For
example, most of the COOK verbs (e.g. bake,
cook, fry ... ) can all take various DAs, such as
the causative alternation, middle alternation and
instrument subject alternation.
In computational linguistics, work inspired by
Levin’s classification has exploited the link be-
tween syntax and semantics for producing clas-
sifications of verbs. Such classifications are use-
ful for a wide variety of purposes such as se-
mantic role labelling (Gildea and Jurafsky, 2002),
predicting unseen syntax (Parisien and Steven-
son, 2010), argument zoning (Guo et al., 2011)
and metaphor identification (Shutova et al., 2010).
While Levin’s classification can be extended man-
ually (Kipper-Schuler, 2005), a large body of re-
search has developed methods for automatic verb
classification since such methods can be applied
easily to other domains and languages.
Existing work on automatic classification relies
largely on syntactic features such as subcatego-
rization frames (SCF)s (Schulte im Walde, 2006;
Sun and Korhonen, 2011; Vlachos et al., 2009;
Brew and Schulte im Walde, 2002). There has also
been some success incorporating selectional pref-
erences (Sun and Korhonen, 2009).
Few have attempted to use, or approximate,
diathesis features directly for verb classification
although manual classifications have relied on
them heavily, and there has been related work on
identifying the DAs themselves automatically us-
ing SCF and semantic information (Resnik, 1993;
McCarthy and Korhonen, 1998; Lapata, 1999;
McCarthy, 2000; Tsang and Stevenson, 2004).
Exceptions to this include Merlo and Stevenson
(2001), Joanis et al. (2008) and Parisien and
Stevenson (2010, 2011). Merlo and Stevenson
(2001) used cues such as passive voice, animacy
and syntactic frames coupled with the overlap
of lexical fillers between the alternating slots to
predict a 3-way classification (unergative, unac-
cusative and object-drop). Joanis et al. (2008)
used similar features to classify verbs on a much
larger scale. They classify up to 496 verbs us-
ing 11 different classifications each having be-
tween 2 and 14 classes. Parisien and Steven-
son (2010, 2011) used hierarchical Bayesian mod-
els on slot frequency data obtained from child-
directed speech parsed with a dependency parser
to model acquisition of SCF, alternations and ul-
timately verb classes which provided predictions
for unseen syntactic behaviour of class members.
</bodyText>
<page confidence="0.979701">
736
</page>
<note confidence="0.5299175">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 736–741,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<table confidence="0.9993792">
Frame Example sentence Freq
NP+PPon Jessica sprayed paint on the wall 40
NP+PPwith Jessica sprayed the wall with paint 30
PPwith *The wall sprayed with paint 0
PPon Jessica sprayed paint on the wall 30
</table>
<tableCaption confidence="0.999953">
Table 1: Example frames for verb spray
</tableCaption>
<bodyText confidence="0.999953625">
In this paper, like Sun and Korhonen (2009);
Joanis et al. (2008) we seek to automatically clas-
sify verbs into a broad range of classes. Like Joa-
nis et al., we include evidence of DA, but we do
not manually select features attributed to specific
alternations but rather experiment with syntactic
evidence for alternation approximation. We use
the verb clustering system presented in Sun and
Korhonen (2009) because it achieves state-of-the-
art results on several datasets, including those of
Joanis et al., even without the additional boost in
performance from the selectional preference data.
We are interested in the improvement that can be
achieved to verb clustering using approximations
for DAs, rather than the DA per se. As such we
make the simple assumption that if a pair of SCFs
tends to occur with the same verbs, we have a po-
tential occurrence of DA. Although this approx-
imation can give rise to false positives (pairs of
frames that co-occur frequently but are not DA)
we are nevertheless interested in investigating its
potential usefulness for verb classification. One
attractive aspect of this method is that it does not
require a pre-defined list of possible alternations.
</bodyText>
<sectionHeader confidence="0.964514" genericHeader="method">
2 Diathesis Alternation Approximation
</sectionHeader>
<bodyText confidence="0.99997352631579">
A DA can be approximated by a pair of SCFs.
We parameterize frames involving prepositional
phrases with the preposition. Example SCFs for
the verb “spray” are shown in Table 1. The feature
value of a single frame feature is the frequency
of the SCF. Given two frames fv(i), fv(j) of a
verb v, they can be transformed into a feature pair
(fv(i), fv(j)) as an approximation to a DA. The
feature value of the DA feature (fv(i), fv(j)) is ap-
proximated by the joint probability of the pair of
frames p(fv(i), fv(j)|v), obtained by integrating
all the possible DAs. The key assumption is that
the joint probability of two SCFs has a strong cor-
relation with a DA on the grounds that the DA gives
rise to both SCFs in the pair. We use the DA feature
(fv(i), fv(j)) with its value p(fv(i), fv(j)|v) as a
new feature for verb clustering. As a comparison
point, we can ignore the DA and make a frame in-
dependence assumption. The joint probability is
</bodyText>
<equation confidence="0.778625">
decomposed as:
p(fv(i), fv(j)|v), p(fv(i)|v) - p(fv(j)|v) (1)
</equation>
<bodyText confidence="0.9991756">
We assume that SCFs are dependent as they are
generated by the underlying meaning components
(Levin and Hovav, 2006). The frame dependency
is represented by a simple graphical model in fig-
ure 1.
</bodyText>
<figureCaption confidence="0.769513333333333">
Figure 1: Graphical model for the joint probability of pairs of
frames. v represents a verb, a represents a DA and f repre-
sents a specific frame in total of M possible frames
</figureCaption>
<bodyText confidence="0.99989825">
In the data, the verb (v) and frames (f) are ob-
served, and any underlying alternation (a) is hid-
den. The aim is to approximate but not to detect a
DA, so a is summed out:
</bodyText>
<equation confidence="0.967964333333333">
J:
p(fv(i), fv(j)|v) = p(fv(i), fv(j)|a) - p(a|v)
a (2)
</equation>
<bodyText confidence="0.98098325">
In order to evaluate this sum, we use a relaxation
1: the sum in equation 1 is replaced with the max-
imum (max). This is a reasonable relaxation, as a
pair of frames rarely participates in more than one
type of a DA.
p(fv(i), fv(j)|v) Pz max(p(fv(i), fv(j)|a)-p(a|v))
The second relaxation further relaxes the first one
by replacing the max with the least upper bound
(sup): If fv(i) occurs a times, fv(j) occurs b times
and b &lt; a, the number of times that a DA occurs
between fv(i) and fv(j) must be smaller or equal
to b.
</bodyText>
<equation confidence="0.9551604">
p(fv(i), fv(j)|v) ≈ sup{p(fv(i), fv(j)|a)} · sup{p(a|v)}
sup{p(fv(i), fv(j)|a)} = Z−1 · min(fv(i), fv(j))
sup{p(a|v)} = 1
�Z = min(fv(m), fv(n))
m n
</equation>
<footnote confidence="0.931646333333333">
1A relaxation is used in mathematical optimization for re-
laxing the strict requirement, by either substituting it with an
easier requirement or dropping it completely.
</footnote>
<page confidence="0.98329">
737
</page>
<table confidence="0.997613636363636">
Frame pair Possible DA Frequency
NP+PPon NP+PPwith Locative 30
NP+PPon PPwith Causative(with) 0
NP+PPon PPon Causative(on) 30
NP+PPwith PPwith ? 0
NP+PPwith PPon ? 30
PPwith PPon ? 0
NP+PPon NP+PPon - 40
NP+PPwith NP+PPwith - 30
PPwith PPwith - 0
PPon PPon - 30
</table>
<tableCaption confidence="0.999795">
Table 2: Example frame pair features for spray
</tableCaption>
<bodyText confidence="0.973645">
So we end up with a simple form:
</bodyText>
<equation confidence="0.87715">
p(fv(i), fv(j)|v) ,: Z−1 · min(fv(i), fv(j)) (5)
</equation>
<bodyText confidence="0.9997727">
The equation is intuitive: If fv(i) occurs 40 times
and fv(j) 30 times, the DA between fv(i) and
fv(j) G 30 times. This upper bound value is used
as the feature value of the DA feature. The original
feature vector f of dimension M is transformed
into M2 dimensions feature vector ˜f. Table 2
shows the transformed feature space for spray.
The feature space matches our expectation well:
valid DAs have a value greater than 0 and invalid
DAs have a value of 0.
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.995052324324324">
We evaluated this model by performing verb clus-
tering experiments using three feature sets:
F1: SCF parameterized with preposition. Exam-
ples are shown in Table 1.
F2: The frame pair features built from F1 with
the frame independence assumption (equa-
tion 1). This feature is not a DA feature as
it ignores the inter-dependency of the frames.
F3: The frame pair features (DAs) built from
F1 with the frame dependency assumption
(equation 4). This is the DA feature which
considers the correlation of the two frames
which are generated from the alternation.
F3 implicitly includes F1, as a frame can pair
with itself. 2 In the example in Table 2, the frame
pair “PP(on) PP(on)” will always have the same
value as the “PP(on)” frame in F1.
We extracted the SCFs using the system of
Preiss et al. (2007) which classifies each corpus
2We did this so that F3 included the SCF features as well
as the DA approximation features. It would be possible in
future work to exclude the pairs involving identical frames,
thereby relying solely on the DA approximations, and com-
pare performance with the results obtained here.
occurrence of a verb as a member of one of the 168
SCFs on the basis of grammatical relations iden-
tified by the RASP (Briscoe et al., 2006) parser.
We experimented with two datasets that have been
used in prior work on verb clustering: the test sets
7-11 (3-14 classes) in Joanis et al. (2008), and the
17 classes set in Sun et al. (2008).
We used the spectral clustering (SPEC) method
and settings as in Sun and Korhonen (2009) but
adopted the Bhattacharyya kernel (Jebara and
Kondor, 2003) to improve the computational effi-
ciency of the approach given the high dimension-
ality of the quadratic feature space.
</bodyText>
<equation confidence="0.996818666666667">
D
wb(v, v&apos;) = (vdvd)1/2 (6)
d=1
</equation>
<bodyText confidence="0.99426224">
The mean-filed bound of the Bhattacharyya kernel
is very similar to the KL divergence kernel (Jebara
et al., 2004) which is frequently used in verb clus-
tering experiments (Korhonen et al., 2003; Sun
and Korhonen, 2009).
To further reduce computational complexity, we
restricted our scope to the more frequent features.
In the experiment described in this section we used
the 50 most frequent features for the 3-6 way clas-
sifications (Joanis et al.’s test set 7-9) and 100 fea-
tures for the 7-17 way classifications. In the next
section, we will demonstrate that F3 outperforms
F1 regardless of the feature number setting. The
features are normalized to sum 1.
The clustering results are evaluated using F-
Measure as in Sun and Korhonen (2009) which
provides the harmonic mean of precision (P) and
recall (R)
P is calculated using modified purity – a global
measure which evaluates the mean precision of
clusters. Each cluster (ki E K) is associated
with the gold-standard class to which the major-
ity of its members belong. The number of verbs
in a cluster (ki) that take this class is denoted by
nprevalent(ki).
</bodyText>
<equation confidence="0.6536435">
Eki∈K:nprevalent(ki)&gt;2 nprevalent(ki)
|verbs|
</equation>
<bodyText confidence="0.454845">
R is calculated using weighted class accuracy:
the proportion of members of the dominant cluster
DOM-CLUSTi within each of the gold-standard
classes ci E C.
P=
</bodyText>
<page confidence="0.980925">
738
</page>
<table confidence="0.9614655">
Datasets
Joanis et al. Sun et al.
7 8 9 10 11
F1 54.54 49.97 35.77 46.61 38.81 60.03
F2 50.00 49.50 32.79 54.13 40.61 64.00
F3 56.36 53.79 52.90 66.32 50.97 69.62
</table>
<tableCaption confidence="0.886172">
Table 3: Results when using F3 (DA), F2 (pair of independent
frames) and F1 (single frame) features with Bhattacharyya
kernel on Joanis et al. and Sun et al. datasets
</tableCaption>
<equation confidence="0.8862155">
R _ �IC|1 |verbs in DOM-CLUSTi|
IverbsI
</equation>
<bodyText confidence="0.9972055">
The results are shown in Table 3. The result of
F2 is lower than that of F3, and even lower than
that of F1 for 3-6 way classification. This indi-
cates that the frame independence assumption is
a poor assumption. F3 yields substantially better
result than F2 and F1. The result of F3 is 6.4%
higher than the result (F=63.28) reported in Sun
and Korhonen (2009) using the F1 feature.
This experiment shows, on two datasets, that DA
features are clearly more effective than the frame
features for verb clustering, even when relaxations
are used.
</bodyText>
<sectionHeader confidence="0.801435" genericHeader="method">
4 Analysis of Feature Frequency
</sectionHeader>
<bodyText confidence="0.999666923076923">
A further experiment was carried out using F1 and
F3 on Joanis et al. (2008)’s test sets 10 and 11.
The frequency ranked features were added to the
clustering one at a time, starting from the most
frequent one. The results are shown in figure 2.
F3 outperforms F1 clearly on all the feature num-
ber settings. After adding some highly frequent
frames (22 for test set 10 and 67 for test set 11),
the performance for F1 is not further improved.
The performance of F3, in contrast, is improved
for almost all (including the mid-range frequency)
frames, although to a lesser degree for low fre-
quency frames.
</bodyText>
<sectionHeader confidence="0.999932" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999579222222222">
Parisien and Stevenson (2010) introduced a hier-
archical Bayesian model capable of learning verb
alternations and constructions from syntactic in-
put. The focus was on modelling and explaining
the child alternation acquisition rather than on au-
tomatic verb classification. Therefore, no quanti-
tative evaluation of the clustering is reported, and
the number of verbs under the novel verb gen-
eralization test is relatively small. Parisien and
</bodyText>
<figureCaption confidence="0.6144885">
Figure 2: Comparison between frame features (F1) and DA
features (F3) with different feature number settings. DA fea-
tures clearly outperform frame features. The top figure is the
result on test set 10 (8 ways). The bottom figure is the result
on test set 11 (14 ways). The x axis is the number of features.
The y axis is the F-Measure result.
</figureCaption>
<bodyText confidence="0.999161833333333">
Stevenson (2011) extended this work by adding
semantic features.
Parisien and Stevenson’s (2010) model 2 has a
similar structure to the graphic model in figure 1.
A fundamental difference is that we explicitly use
a probability distribution over alternations (pair of
frames) to represent a verb, whereas they represent
a verb by a distribution over the observed frames
similar to Vlachos et al. (2009) ’s approach. Also
the parameters in their model were inferred by
Gibbs sampling whereas we avoided this inference
step by using relaxation.
</bodyText>
<sectionHeader confidence="0.98028" genericHeader="conclusions">
6 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.9997593">
We have demonstrated the merits of using DAs for
verb clustering compared to the SCF data from
which they are derived on standard verb classi-
fication datasets and when integrated in a state-
of-the-art verb clustering system. We have also
demonstrated that the performance of frame fea-
tures is dominated by the high frequency frames.
In contrast, the DA features enable the mid-range
frequency frames to further improve the perfor-
mance.
</bodyText>
<figure confidence="0.99919248">
1 22 100
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
SinF1
Pai
F3
1 67 149
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
</figure>
<page confidence="0.995844">
739
</page>
<bodyText confidence="0.999978653846154">
In the future, we plan to evaluate the perfor-
mance of DA features in a larger scale experiment.
Due to the high dimensionality of the transformed
feature space (quadratic of the original feature
space), we will need to improve the computational
efficiency further, e.g. via use of an unsupervised
dimensionality reduction technique Zhao and Liu
(2007). Moreover, we plan to use Bayesian in-
ference as in Vlachos et al. (2009); Parisien and
Stevenson (2010, 2011) to infer the actual param-
eter values and avoid the relaxation.
Finally, we plan to supplement the DA feature
with evidence from the slot fillers of the alternat-
ing slots, in the spirit of earlier work (McCarthy,
2000; Merlo and Stevenson, 2001; Joanis et al.,
2008). Unlike these previous works, we will use
selectional preferences to generalize the argument
heads but will do so using preferences from dis-
tributional data (Sun and Korhonen, 2009) rather
than WordNet, and use all argument head data in
all frames. We envisage using maximum average
distributional similarity of the argument heads in
any potentially alternating slots in a pair of co-
occurring frames as a feature, just as we currently
use the frequency of the less frequent co-occurring
frame.
</bodyText>
<sectionHeader confidence="0.947834" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.956619333333333">
Our work was funded by the Royal Society
University Research Fellowship (AK) and the
Dorothy Hodgkin Postgraduate Award (LS).
</bodyText>
<sectionHeader confidence="0.986349" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.901508393939394">
C. Brew and S. Schulte im Walde. Spectral clus-
tering for German verbs. In Proceedings of
EMNLP, 2002.
E. Briscoe, J. Carroll, and R. Watson. The second
release of the RASP system. In Proceedings
of the COLING/ACL on Interactive presentation
sessions, pages 77–80, 2006.
D. Gildea and D. Jurafsky. Automatic labeling
of semantic roles. Computational Linguistics,
28(3):245–288, 2002.
Y. Guo, A. Korhonen, and T. Poibeau. A
weakly-supervised approach to argumentative
zoning of scientific documents. In Proceedings
of EMNLP, pages 273–283, Stroudsburg, PA,
USA, 2011. ACL.
T. Jebara and R. Kondor. Bhattacharyya and ex-
pected likelihood kernels. In Learning Theory
and Kernel Machines: 16th Annual Conference
on Learning Theory and 7th Kernel Workshop,
page 57. Springer, 2003.
T. Jebara, R. Kondor, and A. Howard. Probability
product kernels. The Journal of Machine Learn-
ing Research, 5:819–844, 2004.
E. Joanis, S. Stevenson, and D. James. A general
feature space for automatic verb classification.
Natural Language Engineering, 2008.
K. Kipper-Schuler. VerbNet: A broad-coverage,
comprehensive verb lexicon. PhD thesis, Com-
puter and Information Science Dept., University
of Pennsylvania, Philadelphia, PA, June 2005.
A. Korhonen, Y. Krymolowski, and Z. Marx.
Clustering polysemic subcategorization frame
distributions semantically. In Proceedings of
ACL, pages 64–71, Morristown, NJ, USA,
2003. ACL.
M. Lapata. Acquiring lexical generalizations from
corpora: A case study for diathesis alternations.
In Proceedings of ACL, pages 397–404. ACL
Morristown, NJ, USA, 1999.
B. Levin and M. Hovav. Argument realiza-
tion. Computational Linguistics, 32(3):447–
450, 2006.
B. Levin. English Verb Classes and Alterna-
tions: a preliminary investigation. University
of Chicago Press, Chicago and London, 1993.
D. McCarthy and A. Korhonen. Detecting verbal
participation in diathesis alternations. In Pro-
ceedings ofACL, volume 36, pages 1493–1495.
ACL, 1998.
D. McCarthy. Using semantic preferences to iden-
tify verbal participation in role switching alter-
nations. In Proceedings of NAACL, pages 256–
263. Morgan Kaufmann Publishers Inc. San
Francisco, CA, USA, 2000.
P. Merlo and S. Stevenson. Automatic verb clas-
sification based on statistical distributions of ar-
gument structure. Computational Linguistics,
27(3):373–408, 2001.
C. Parisien and S. Stevenson. Learning verb al-
ternations in a usage-based Bayesian model. In
Proceedings of the 32nd annual meeting of the
Cognitive Science Society, 2010.
C. Parisien and S. Stevenson. Generalizing be-
tween form and meaning using learned verb
classes. In Proceedings of the 33rd Annual
Meeting of the Cognitive Science Society, 2011.
</reference>
<page confidence="0.969394">
740
</page>
<reference confidence="0.97784687804878">
J. Preiss, T. Briscoe, and A. Korhonen. A system
for large-scale acquisition of verbal, nominal
and adjectival subcategorization frames from
corpora. In Proceedings of ACL, volume 45,
page 912, 2007.
P. Resnik. Selection and Information: A Class-
Based Approach to Lexical Relationships. PhD
thesis, University of Pennsylvania, 1993.
S. Schulte im Walde. Experiments on the au-
tomatic induction of German semantic verb
classes. Computational Linguistics, 32(2):159–
194, 2006.
E. Shutova, L. Sun, and A. Korhonen. Metaphor
identification using verb and noun clustering.
In Proceedings of COLING, pages 1002–1010.
ACL, 2010.
L. Sun and A. Korhonen. Improving verb clus-
tering with automatically acquired selectional
preferences. In Proceedings of EMNLP, pages
638–647, 2009.
L. Sun and A. Korhonen. Hierarchical verb clus-
tering using graph factorization. In Proceedings
of EMNLP, pages 1023–1033, Edinburgh, Scot-
land, UK., July 2011. ACL.
L. Sun, A. Korhonen, and Y. Krymolowski. Verb
class discovery from rich syntactic data. Lecture
Notes in Computer Science, 4919:16, 2008.
V. Tsang and S. Stevenson. Using selectional
profile distance to detect verb alternations. In
HLT/NAACL 2004 Workshop on Computational
Lexical Semantics, 2004.
A. Vlachos, A. Korhonen, and Z. Ghahramani.
Unsupervised and constrained dirichlet process
mixture models for verb clustering. In Proceed-
ings of the Workshop on Geometrical Models
of Natural Language Semantics, pages 74–82,
2009.
Z. Zhao and H. Liu. Spectral feature selection
for supervised and unsupervised learning. In
Proceedings of ICML, pages 1151–1157, New
York, NY, USA, 2007. ACM.
</reference>
<page confidence="0.997861">
741
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.289408">
<title confidence="0.993929">Diathesis alternation approximation for verb clustering</title>
<author confidence="0.979592">Lin</author>
<affiliation confidence="0.713836">Greedy Intelligence</affiliation>
<address confidence="0.566264">Hangzhou,</address>
<email confidence="0.999692">lin.sun@greedyint.com</email>
<author confidence="0.98751">Diana McCarthy</author>
<author confidence="0.98751">Anna</author>
<affiliation confidence="0.9230575">DTAL and Computer University of</affiliation>
<address confidence="0.723559">Cambridge,</address>
<email confidence="0.994981">alk23@cam.ac.uk</email>
<abstract confidence="0.999480842105263">Although diathesis alternations have been used as features for manual verb classification, and there is recent work on incorporating such features in computational models of human language acquisition, work on large scale verb classification has yet to examine the potential for using diathesis alternations as input features to the clustering process. This paper proposes a method for approximating diathesis alternation behaviour in corpus data and shows, using a state-of-the-art verb clustering system, that features based on alternation approximation outperform those based on independent subcategorization frames. Our alternation-based approach is particularly adept at leveraging information from less frequent data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Brew</author>
<author>S Schulte im Walde</author>
</authors>
<title>Spectral clustering for German verbs.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<marker>Brew, Walde, 2002</marker>
<rawString>C. Brew and S. Schulte im Walde. Spectral clustering for German verbs. In Proceedings of EMNLP, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
<author>R Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Interactive presentation sessions,</booktitle>
<pages>77--80</pages>
<contexts>
<context position="10086" citStr="Briscoe et al., 2006" startWordPosition="1648" endWordPosition="1651">ple in Table 2, the frame pair “PP(on) PP(on)” will always have the same value as the “PP(on)” frame in F1. We extracted the SCFs using the system of Preiss et al. (2007) which classifies each corpus 2We did this so that F3 included the SCF features as well as the DA approximation features. It would be possible in future work to exclude the pairs involving identical frames, thereby relying solely on the DA approximations, and compare performance with the results obtained here. occurrence of a verb as a member of one of the 168 SCFs on the basis of grammatical relations identified by the RASP (Briscoe et al., 2006) parser. We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (2008), and the 17 classes set in Sun et al. (2008). We used the spectral clustering (SPEC) method and settings as in Sun and Korhonen (2009) but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space. D wb(v, v&apos;) = (vdvd)1/2 (6) d=1 The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel (Jeb</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>E. Briscoe, J. Carroll, and R. Watson. The second release of the RASP system. In Proceedings of the COLING/ACL on Interactive presentation sessions, pages 77–80, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1947" citStr="Gildea and Jurafsky, 2002" startWordPosition="281" endWordPosition="284">n (1993)’s seminal book provides a manual inventory both of DAs and verb classes where membership is determined according to participation in these alternations. For example, most of the COOK verbs (e.g. bake, cook, fry ... ) can all take various DAs, such as the causative alternation, middle alternation and instrument subject alternation. In computational linguistics, work inspired by Levin’s classification has exploited the link between syntax and semantics for producing classifications of verbs. Such classifications are useful for a wide variety of purposes such as semantic role labelling (Gildea and Jurafsky, 2002), predicting unseen syntax (Parisien and Stevenson, 2010), argument zoning (Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002)</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Guo</author>
<author>A Korhonen</author>
<author>T Poibeau</author>
</authors>
<title>A weakly-supervised approach to argumentative zoning of scientific documents.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>273--283</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="2040" citStr="Guo et al., 2011" startWordPosition="295" endWordPosition="298">etermined according to participation in these alternations. For example, most of the COOK verbs (e.g. bake, cook, fry ... ) can all take various DAs, such as the causative alternation, middle alternation and instrument subject alternation. In computational linguistics, work inspired by Levin’s classification has exploited the link between syntax and semantics for producing classifications of verbs. Such classifications are useful for a wide variety of purposes such as semantic role labelling (Gildea and Jurafsky, 2002), predicting unseen syntax (Parisien and Stevenson, 2010), argument zoning (Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2</context>
</contexts>
<marker>Guo, Korhonen, Poibeau, 2011</marker>
<rawString>Y. Guo, A. Korhonen, and T. Poibeau. A weakly-supervised approach to argumentative zoning of scientific documents. In Proceedings of EMNLP, pages 273–283, Stroudsburg, PA, USA, 2011. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Jebara</author>
<author>R Kondor</author>
</authors>
<title>Bhattacharyya and expected likelihood kernels.</title>
<date>2003</date>
<booktitle>In Learning Theory and Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop,</booktitle>
<pages>57</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="10438" citStr="Jebara and Kondor, 2003" startWordPosition="1709" endWordPosition="1712">involving identical frames, thereby relying solely on the DA approximations, and compare performance with the results obtained here. occurrence of a verb as a member of one of the 168 SCFs on the basis of grammatical relations identified by the RASP (Briscoe et al., 2006) parser. We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (2008), and the 17 classes set in Sun et al. (2008). We used the spectral clustering (SPEC) method and settings as in Sun and Korhonen (2009) but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space. D wb(v, v&apos;) = (vdvd)1/2 (6) d=1 The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel (Jebara et al., 2004) which is frequently used in verb clustering experiments (Korhonen et al., 2003; Sun and Korhonen, 2009). To further reduce computational complexity, we restricted our scope to the more frequent features. In the experiment described in this section we used the 50 most frequent features for the 3-6 way classifications (Joanis et al.’s</context>
</contexts>
<marker>Jebara, Kondor, 2003</marker>
<rawString>T. Jebara and R. Kondor. Bhattacharyya and expected likelihood kernels. In Learning Theory and Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, page 57. Springer, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Jebara</author>
<author>R Kondor</author>
<author>A Howard</author>
</authors>
<title>Probability product kernels.</title>
<date>2004</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>5</volume>
<contexts>
<context position="10703" citStr="Jebara et al., 2004" startWordPosition="1754" endWordPosition="1757">06) parser. We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (2008), and the 17 classes set in Sun et al. (2008). We used the spectral clustering (SPEC) method and settings as in Sun and Korhonen (2009) but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space. D wb(v, v&apos;) = (vdvd)1/2 (6) d=1 The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel (Jebara et al., 2004) which is frequently used in verb clustering experiments (Korhonen et al., 2003; Sun and Korhonen, 2009). To further reduce computational complexity, we restricted our scope to the more frequent features. In the experiment described in this section we used the 50 most frequent features for the 3-6 way classifications (Joanis et al.’s test set 7-9) and 100 features for the 7-17 way classifications. In the next section, we will demonstrate that F3 outperforms F1 regardless of the feature number setting. The features are normalized to sum 1. The clustering results are evaluated using FMeasure as </context>
</contexts>
<marker>Jebara, Kondor, Howard, 2004</marker>
<rawString>T. Jebara, R. Kondor, and A. Howard. Probability product kernels. The Journal of Machine Learning Research, 5:819–844, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Joanis</author>
<author>S Stevenson</author>
<author>D James</author>
</authors>
<title>A general feature space for automatic verb classification. Natural Language Engineering,</title>
<date>2008</date>
<contexts>
<context position="3094" citStr="Joanis et al. (2008)" startWordPosition="450" endWordPosition="453">nd Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes. Parisien and Stevenson (2010, 2011) used hierarchical Bayesian models on slot frequency data obtained from childdirected speech p</context>
<context position="4368" citStr="Joanis et al. (2008)" startWordPosition="650" endWordPosition="653"> SCF, alternations and ultimately verb classes which provided predictions for unseen syntactic behaviour of class members. 736 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 736–741, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Frame Example sentence Freq NP+PPon Jessica sprayed paint on the wall 40 NP+PPwith Jessica sprayed the wall with paint 30 PPwith *The wall sprayed with paint 0 PPon Jessica sprayed paint on the wall 30 Table 1: Example frames for verb spray In this paper, like Sun and Korhonen (2009); Joanis et al. (2008) we seek to automatically classify verbs into a broad range of classes. Like Joanis et al., we include evidence of DA, but we do not manually select features attributed to specific alternations but rather experiment with syntactic evidence for alternation approximation. We use the verb clustering system presented in Sun and Korhonen (2009) because it achieves state-of-theart results on several datasets, including those of Joanis et al., even without the additional boost in performance from the selectional preference data. We are interested in the improvement that can be achieved to verb cluste</context>
<context position="10240" citStr="Joanis et al. (2008)" startWordPosition="1675" endWordPosition="1678">s et al. (2007) which classifies each corpus 2We did this so that F3 included the SCF features as well as the DA approximation features. It would be possible in future work to exclude the pairs involving identical frames, thereby relying solely on the DA approximations, and compare performance with the results obtained here. occurrence of a verb as a member of one of the 168 SCFs on the basis of grammatical relations identified by the RASP (Briscoe et al., 2006) parser. We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (2008), and the 17 classes set in Sun et al. (2008). We used the spectral clustering (SPEC) method and settings as in Sun and Korhonen (2009) but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space. D wb(v, v&apos;) = (vdvd)1/2 (6) d=1 The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel (Jebara et al., 2004) which is frequently used in verb clustering experiments (Korhonen et al., 2003; Sun and Korhonen, 2009). To further reduce computational</context>
<context position="12930" citStr="Joanis et al. (2008)" startWordPosition="2136" endWordPosition="2139">3. The result of F2 is lower than that of F3, and even lower than that of F1 for 3-6 way classification. This indicates that the frame independence assumption is a poor assumption. F3 yields substantially better result than F2 and F1. The result of F3 is 6.4% higher than the result (F=63.28) reported in Sun and Korhonen (2009) using the F1 feature. This experiment shows, on two datasets, that DA features are clearly more effective than the frame features for verb clustering, even when relaxations are used. 4 Analysis of Feature Frequency A further experiment was carried out using F1 and F3 on Joanis et al. (2008)’s test sets 10 and 11. The frequency ranked features were added to the clustering one at a time, starting from the most frequent one. The results are shown in figure 2. F3 outperforms F1 clearly on all the feature number settings. After adding some highly frequent frames (22 for test set 10 and 67 for test set 11), the performance for F1 is not further improved. The performance of F3, in contrast, is improved for almost all (including the mid-range frequency) frames, although to a lesser degree for low frequency frames. 5 Related work Parisien and Stevenson (2010) introduced a hierarchical Ba</context>
<context position="16113" citStr="Joanis et al., 2008" startWordPosition="2673" endWordPosition="2676">high dimensionality of the transformed feature space (quadratic of the original feature space), we will need to improve the computational efficiency further, e.g. via use of an unsupervised dimensionality reduction technique Zhao and Liu (2007). Moreover, we plan to use Bayesian inference as in Vlachos et al. (2009); Parisien and Stevenson (2010, 2011) to infer the actual parameter values and avoid the relaxation. Finally, we plan to supplement the DA feature with evidence from the slot fillers of the alternating slots, in the spirit of earlier work (McCarthy, 2000; Merlo and Stevenson, 2001; Joanis et al., 2008). Unlike these previous works, we will use selectional preferences to generalize the argument heads but will do so using preferences from distributional data (Sun and Korhonen, 2009) rather than WordNet, and use all argument head data in all frames. We envisage using maximum average distributional similarity of the argument heads in any potentially alternating slots in a pair of cooccurring frames as a feature, just as we currently use the frequency of the less frequent co-occurring frame. Acknowledgement Our work was funded by the Royal Society University Research Fellowship (AK) and the Doro</context>
</contexts>
<marker>Joanis, Stevenson, James, 2008</marker>
<rawString>E. Joanis, S. Stevenson, and D. James. A general feature space for automatic verb classification. Natural Language Engineering, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper-Schuler</author>
</authors>
<title>VerbNet: A broad-coverage, comprehensive verb lexicon.</title>
<date>2005</date>
<tech>PhD thesis,</tech>
<institution>Computer and Information Science Dept., University of Pennsylvania,</institution>
<location>Philadelphia, PA,</location>
<contexts>
<context position="2169" citStr="Kipper-Schuler, 2005" startWordPosition="314" endWordPosition="315">an all take various DAs, such as the causative alternation, middle alternation and instrument subject alternation. In computational linguistics, work inspired by Levin’s classification has exploited the link between syntax and semantics for producing classifications of verbs. Such classifications are useful for a wide variety of purposes such as semantic role labelling (Gildea and Jurafsky, 2002), predicting unseen syntax (Parisien and Stevenson, 2010), argument zoning (Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classificati</context>
</contexts>
<marker>Kipper-Schuler, 2005</marker>
<rawString>K. Kipper-Schuler. VerbNet: A broad-coverage, comprehensive verb lexicon. PhD thesis, Computer and Information Science Dept., University of Pennsylvania, Philadelphia, PA, June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
<author>Y Krymolowski</author>
<author>Z Marx</author>
</authors>
<title>Clustering polysemic subcategorization frame distributions semantically.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>64--71</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="10782" citStr="Korhonen et al., 2003" startWordPosition="1767" endWordPosition="1770">k on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (2008), and the 17 classes set in Sun et al. (2008). We used the spectral clustering (SPEC) method and settings as in Sun and Korhonen (2009) but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space. D wb(v, v&apos;) = (vdvd)1/2 (6) d=1 The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel (Jebara et al., 2004) which is frequently used in verb clustering experiments (Korhonen et al., 2003; Sun and Korhonen, 2009). To further reduce computational complexity, we restricted our scope to the more frequent features. In the experiment described in this section we used the 50 most frequent features for the 3-6 way classifications (Joanis et al.’s test set 7-9) and 100 features for the 7-17 way classifications. In the next section, we will demonstrate that F3 outperforms F1 regardless of the feature number setting. The features are normalized to sum 1. The clustering results are evaluated using FMeasure as in Sun and Korhonen (2009) which provides the harmonic mean of precision (P) an</context>
</contexts>
<marker>Korhonen, Krymolowski, Marx, 2003</marker>
<rawString>A. Korhonen, Y. Krymolowski, and Z. Marx. Clustering polysemic subcategorization frame distributions semantically. In Proceedings of ACL, pages 64–71, Morristown, NJ, USA, 2003. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
</authors>
<title>Acquiring lexical generalizations from corpora: A case study for diathesis alternations.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>397--404</pages>
<publisher>ACL</publisher>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="2973" citStr="Lapata, 1999" startWordPosition="434" endWordPosition="435">ation relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes. Parisien</context>
</contexts>
<marker>Lapata, 1999</marker>
<rawString>M. Lapata. Acquiring lexical generalizations from corpora: A case study for diathesis alternations. In Proceedings of ACL, pages 397–404. ACL Morristown, NJ, USA, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
<author>M Hovav</author>
</authors>
<title>Argument realization.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<pages>450</pages>
<contexts>
<context position="6647" citStr="Levin and Hovav, 2006" startWordPosition="1035" endWordPosition="1038">p(fv(i), fv(j)|v), obtained by integrating all the possible DAs. The key assumption is that the joint probability of two SCFs has a strong correlation with a DA on the grounds that the DA gives rise to both SCFs in the pair. We use the DA feature (fv(i), fv(j)) with its value p(fv(i), fv(j)|v) as a new feature for verb clustering. As a comparison point, we can ignore the DA and make a frame independence assumption. The joint probability is decomposed as: p(fv(i), fv(j)|v), p(fv(i)|v) - p(fv(j)|v) (1) We assume that SCFs are dependent as they are generated by the underlying meaning components (Levin and Hovav, 2006). The frame dependency is represented by a simple graphical model in figure 1. Figure 1: Graphical model for the joint probability of pairs of frames. v represents a verb, a represents a DA and f represents a specific frame in total of M possible frames In the data, the verb (v) and frames (f) are observed, and any underlying alternation (a) is hidden. The aim is to approximate but not to detect a DA, so a is summed out: J: p(fv(i), fv(j)|v) = p(fv(i), fv(j)|a) - p(a|v) a (2) In order to evaluate this sum, we use a relaxation 1: the sum in equation 1 is replaced with the maximum (max). This is</context>
</contexts>
<marker>Levin, Hovav, 2006</marker>
<rawString>B. Levin and M. Hovav. Argument realization. Computational Linguistics, 32(3):447– 450, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations: a preliminary investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago and London,</location>
<contexts>
<context position="1329" citStr="Levin (1993)" startWordPosition="187" endWordPosition="188">s data and shows, using a state-of-the-art verb clustering system, that features based on alternation approximation outperform those based on independent subcategorization frames. Our alternation-based approach is particularly adept at leveraging information from less frequent data. 1 Introduction Diathesis alternations (DAs) are regular alternations of the syntactic expression of verbal arguments, sometimes accompanied by a change in meaning. For example, The man broke the window ↔ The window broke. The syntactic phenomena are triggered by the underlying semantics of the participating verbs. Levin (1993)’s seminal book provides a manual inventory both of DAs and verb classes where membership is determined according to participation in these alternations. For example, most of the COOK verbs (e.g. bake, cook, fry ... ) can all take various DAs, such as the causative alternation, middle alternation and instrument subject alternation. In computational linguistics, work inspired by Levin’s classification has exploited the link between syntax and semantics for producing classifications of verbs. Such classifications are useful for a wide variety of purposes such as semantic role labelling (Gildea a</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>B. Levin. English Verb Classes and Alternations: a preliminary investigation. University of Chicago Press, Chicago and London, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>A Korhonen</author>
</authors>
<title>Detecting verbal participation in diathesis alternations.</title>
<date>1998</date>
<booktitle>In Proceedings ofACL,</booktitle>
<volume>36</volume>
<pages>1493--1495</pages>
<publisher>ACL,</publisher>
<contexts>
<context position="2959" citStr="McCarthy and Korhonen, 1998" startWordPosition="430" endWordPosition="433">g work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 cla</context>
</contexts>
<marker>McCarthy, Korhonen, 1998</marker>
<rawString>D. McCarthy and A. Korhonen. Detecting verbal participation in diathesis alternations. In Proceedings ofACL, volume 36, pages 1493–1495. ACL, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
</authors>
<title>Using semantic preferences to identify verbal participation in role switching alternations.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>256--263</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA,</location>
<contexts>
<context position="2989" citStr="McCarthy, 2000" startWordPosition="436" endWordPosition="437">argely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes. Parisien and Stevenson (</context>
<context position="16064" citStr="McCarthy, 2000" startWordPosition="2667" endWordPosition="2668">s in a larger scale experiment. Due to the high dimensionality of the transformed feature space (quadratic of the original feature space), we will need to improve the computational efficiency further, e.g. via use of an unsupervised dimensionality reduction technique Zhao and Liu (2007). Moreover, we plan to use Bayesian inference as in Vlachos et al. (2009); Parisien and Stevenson (2010, 2011) to infer the actual parameter values and avoid the relaxation. Finally, we plan to supplement the DA feature with evidence from the slot fillers of the alternating slots, in the spirit of earlier work (McCarthy, 2000; Merlo and Stevenson, 2001; Joanis et al., 2008). Unlike these previous works, we will use selectional preferences to generalize the argument heads but will do so using preferences from distributional data (Sun and Korhonen, 2009) rather than WordNet, and use all argument head data in all frames. We envisage using maximum average distributional similarity of the argument heads in any potentially alternating slots in a pair of cooccurring frames as a feature, just as we currently use the frequency of the less frequent co-occurring frame. Acknowledgement Our work was funded by the Royal Society</context>
</contexts>
<marker>McCarthy, 2000</marker>
<rawString>D. McCarthy. Using semantic preferences to identify verbal participation in role switching alternations. In Proceedings of NAACL, pages 256– 263. Morgan Kaufmann Publishers Inc. San Francisco, CA, USA, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Merlo</author>
<author>S Stevenson</author>
</authors>
<title>Automatic verb classification based on statistical distributions of argument structure.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="3072" citStr="Merlo and Stevenson (2001)" startWordPosition="446" endWordPosition="449">chulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes. Parisien and Stevenson (2010, 2011) used hierarchical Bayesian models on slot frequency data obtained from </context>
<context position="16091" citStr="Merlo and Stevenson, 2001" startWordPosition="2669" endWordPosition="2672">ale experiment. Due to the high dimensionality of the transformed feature space (quadratic of the original feature space), we will need to improve the computational efficiency further, e.g. via use of an unsupervised dimensionality reduction technique Zhao and Liu (2007). Moreover, we plan to use Bayesian inference as in Vlachos et al. (2009); Parisien and Stevenson (2010, 2011) to infer the actual parameter values and avoid the relaxation. Finally, we plan to supplement the DA feature with evidence from the slot fillers of the alternating slots, in the spirit of earlier work (McCarthy, 2000; Merlo and Stevenson, 2001; Joanis et al., 2008). Unlike these previous works, we will use selectional preferences to generalize the argument heads but will do so using preferences from distributional data (Sun and Korhonen, 2009) rather than WordNet, and use all argument head data in all frames. We envisage using maximum average distributional similarity of the argument heads in any potentially alternating slots in a pair of cooccurring frames as a feature, just as we currently use the frequency of the less frequent co-occurring frame. Acknowledgement Our work was funded by the Royal Society University Research Fellow</context>
</contexts>
<marker>Merlo, Stevenson, 2001</marker>
<rawString>P. Merlo and S. Stevenson. Automatic verb classification based on statistical distributions of argument structure. Computational Linguistics, 27(3):373–408, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Parisien</author>
<author>S Stevenson</author>
</authors>
<title>Learning verb alternations in a usage-based Bayesian model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 32nd annual meeting of the Cognitive Science Society,</booktitle>
<contexts>
<context position="2004" citStr="Parisien and Stevenson, 2010" startWordPosition="288" endWordPosition="292">th of DAs and verb classes where membership is determined according to participation in these alternations. For example, most of the COOK verbs (e.g. bake, cook, fry ... ) can all take various DAs, such as the causative alternation, middle alternation and instrument subject alternation. In computational linguistics, work inspired by Levin’s classification has exploited the link between syntax and semantics for producing classifications of verbs. Such classifications are useful for a wide variety of purposes such as semantic role labelling (Gildea and Jurafsky, 2002), predicting unseen syntax (Parisien and Stevenson, 2010), argument zoning (Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectio</context>
<context position="3593" citStr="Parisien and Stevenson (2010" startWordPosition="529" endWordPosition="533">ta, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes. Parisien and Stevenson (2010, 2011) used hierarchical Bayesian models on slot frequency data obtained from childdirected speech parsed with a dependency parser to model acquisition of SCF, alternations and ultimately verb classes which provided predictions for unseen syntactic behaviour of class members. 736 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 736–741, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Frame Example sentence Freq NP+PPon Jessica sprayed paint on the wall 40 NP+PPwith Jessica sprayed the wall with paint 30 PPwith *T</context>
<context position="13501" citStr="Parisien and Stevenson (2010)" startWordPosition="2236" endWordPosition="2239">nt was carried out using F1 and F3 on Joanis et al. (2008)’s test sets 10 and 11. The frequency ranked features were added to the clustering one at a time, starting from the most frequent one. The results are shown in figure 2. F3 outperforms F1 clearly on all the feature number settings. After adding some highly frequent frames (22 for test set 10 and 67 for test set 11), the performance for F1 is not further improved. The performance of F3, in contrast, is improved for almost all (including the mid-range frequency) frames, although to a lesser degree for low frequency frames. 5 Related work Parisien and Stevenson (2010) introduced a hierarchical Bayesian model capable of learning verb alternations and constructions from syntactic input. The focus was on modelling and explaining the child alternation acquisition rather than on automatic verb classification. Therefore, no quantitative evaluation of the clustering is reported, and the number of verbs under the novel verb generalization test is relatively small. Parisien and Figure 2: Comparison between frame features (F1) and DA features (F3) with different feature number settings. DA features clearly outperform frame features. The top figure is the result on t</context>
<context position="15840" citStr="Parisien and Stevenson (2010" startWordPosition="2626" endWordPosition="2629">uency frames to further improve the performance. 1 22 100 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 SinF1 Pai F3 1 67 149 0.7 0.65 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 739 In the future, we plan to evaluate the performance of DA features in a larger scale experiment. Due to the high dimensionality of the transformed feature space (quadratic of the original feature space), we will need to improve the computational efficiency further, e.g. via use of an unsupervised dimensionality reduction technique Zhao and Liu (2007). Moreover, we plan to use Bayesian inference as in Vlachos et al. (2009); Parisien and Stevenson (2010, 2011) to infer the actual parameter values and avoid the relaxation. Finally, we plan to supplement the DA feature with evidence from the slot fillers of the alternating slots, in the spirit of earlier work (McCarthy, 2000; Merlo and Stevenson, 2001; Joanis et al., 2008). Unlike these previous works, we will use selectional preferences to generalize the argument heads but will do so using preferences from distributional data (Sun and Korhonen, 2009) rather than WordNet, and use all argument head data in all frames. We envisage using maximum average distributional similarity of the argument h</context>
</contexts>
<marker>Parisien, Stevenson, 2010</marker>
<rawString>C. Parisien and S. Stevenson. Learning verb alternations in a usage-based Bayesian model. In Proceedings of the 32nd annual meeting of the Cognitive Science Society, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Parisien</author>
<author>S Stevenson</author>
</authors>
<title>Generalizing between form and meaning using learned verb classes.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Cognitive Science Society,</booktitle>
<marker>Parisien, Stevenson, 2011</marker>
<rawString>C. Parisien and S. Stevenson. Generalizing between form and meaning using learned verb classes. In Proceedings of the 33rd Annual Meeting of the Cognitive Science Society, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Preiss</author>
<author>T Briscoe</author>
<author>A Korhonen</author>
</authors>
<title>A system for large-scale acquisition of verbal, nominal and adjectival subcategorization frames from corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<volume>45</volume>
<pages>912</pages>
<contexts>
<context position="9635" citStr="Preiss et al. (2007)" startWordPosition="1569" endWordPosition="1572">air features built from F1 with the frame independence assumption (equation 1). This feature is not a DA feature as it ignores the inter-dependency of the frames. F3: The frame pair features (DAs) built from F1 with the frame dependency assumption (equation 4). This is the DA feature which considers the correlation of the two frames which are generated from the alternation. F3 implicitly includes F1, as a frame can pair with itself. 2 In the example in Table 2, the frame pair “PP(on) PP(on)” will always have the same value as the “PP(on)” frame in F1. We extracted the SCFs using the system of Preiss et al. (2007) which classifies each corpus 2We did this so that F3 included the SCF features as well as the DA approximation features. It would be possible in future work to exclude the pairs involving identical frames, thereby relying solely on the DA approximations, and compare performance with the results obtained here. occurrence of a verb as a member of one of the 168 SCFs on the basis of grammatical relations identified by the RASP (Briscoe et al., 2006) parser. We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (</context>
</contexts>
<marker>Preiss, Briscoe, Korhonen, 2007</marker>
<rawString>J. Preiss, T. Briscoe, and A. Korhonen. A system for large-scale acquisition of verbal, nominal and adjectival subcategorization frames from corpora. In Proceedings of ACL, volume 45, page 912, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selection and Information: A ClassBased Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="2930" citStr="Resnik, 1993" startWordPosition="428" endWordPosition="429">uages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications eac</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>P. Resnik. Selection and Information: A ClassBased Approach to Lexical Relationships. PhD thesis, University of Pennsylvania, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schulte im Walde</author>
</authors>
<title>Experiments on the automatic induction of German semantic verb classes.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>2</issue>
<pages>194</pages>
<contexts>
<context position="2467" citStr="Walde, 2006" startWordPosition="359" endWordPosition="360"> for a wide variety of purposes such as semantic role labelling (Gildea and Jurafsky, 2002), predicting unseen syntax (Parisien and Stevenson, 2010), argument zoning (Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (</context>
</contexts>
<marker>Walde, 2006</marker>
<rawString>S. Schulte im Walde. Experiments on the automatic induction of German semantic verb classes. Computational Linguistics, 32(2):159– 194, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shutova</author>
<author>L Sun</author>
<author>A Korhonen</author>
</authors>
<title>Metaphor identification using verb and noun clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1002--1010</pages>
<publisher>ACL,</publisher>
<contexts>
<context position="2091" citStr="Shutova et al., 2010" startWordPosition="302" endWordPosition="305">lternations. For example, most of the COOK verbs (e.g. bake, cook, fry ... ) can all take various DAs, such as the causative alternation, middle alternation and instrument subject alternation. In computational linguistics, work inspired by Levin’s classification has exploited the link between syntax and semantics for producing classifications of verbs. Such classifications are useful for a wide variety of purposes such as semantic role labelling (Gildea and Jurafsky, 2002), predicting unseen syntax (Parisien and Stevenson, 2010), argument zoning (Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, di</context>
</contexts>
<marker>Shutova, Sun, Korhonen, 2010</marker>
<rawString>E. Shutova, L. Sun, and A. Korhonen. Metaphor identification using verb and noun clustering. In Proceedings of COLING, pages 1002–1010. ACL, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Sun</author>
<author>A Korhonen</author>
</authors>
<title>Improving verb clustering with automatically acquired selectional preferences.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>638--647</pages>
<contexts>
<context position="2644" citStr="Sun and Korhonen, 2009" startWordPosition="385" endWordPosition="388"> (Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the ov</context>
<context position="4346" citStr="Sun and Korhonen (2009)" startWordPosition="646" endWordPosition="649">r to model acquisition of SCF, alternations and ultimately verb classes which provided predictions for unseen syntactic behaviour of class members. 736 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 736–741, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Frame Example sentence Freq NP+PPon Jessica sprayed paint on the wall 40 NP+PPwith Jessica sprayed the wall with paint 30 PPwith *The wall sprayed with paint 0 PPon Jessica sprayed paint on the wall 30 Table 1: Example frames for verb spray In this paper, like Sun and Korhonen (2009); Joanis et al. (2008) we seek to automatically classify verbs into a broad range of classes. Like Joanis et al., we include evidence of DA, but we do not manually select features attributed to specific alternations but rather experiment with syntactic evidence for alternation approximation. We use the verb clustering system presented in Sun and Korhonen (2009) because it achieves state-of-theart results on several datasets, including those of Joanis et al., even without the additional boost in performance from the selectional preference data. We are interested in the improvement that can be a</context>
<context position="10375" citStr="Sun and Korhonen (2009)" startWordPosition="1700" endWordPosition="1703">res. It would be possible in future work to exclude the pairs involving identical frames, thereby relying solely on the DA approximations, and compare performance with the results obtained here. occurrence of a verb as a member of one of the 168 SCFs on the basis of grammatical relations identified by the RASP (Briscoe et al., 2006) parser. We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (2008), and the 17 classes set in Sun et al. (2008). We used the spectral clustering (SPEC) method and settings as in Sun and Korhonen (2009) but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space. D wb(v, v&apos;) = (vdvd)1/2 (6) d=1 The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel (Jebara et al., 2004) which is frequently used in verb clustering experiments (Korhonen et al., 2003; Sun and Korhonen, 2009). To further reduce computational complexity, we restricted our scope to the more frequent features. In the experiment described in this section we used the 50 most fre</context>
<context position="12638" citStr="Sun and Korhonen (2009)" startWordPosition="2087" endWordPosition="2090">54.13 40.61 64.00 F3 56.36 53.79 52.90 66.32 50.97 69.62 Table 3: Results when using F3 (DA), F2 (pair of independent frames) and F1 (single frame) features with Bhattacharyya kernel on Joanis et al. and Sun et al. datasets R _ �IC|1 |verbs in DOM-CLUSTi| IverbsI The results are shown in Table 3. The result of F2 is lower than that of F3, and even lower than that of F1 for 3-6 way classification. This indicates that the frame independence assumption is a poor assumption. F3 yields substantially better result than F2 and F1. The result of F3 is 6.4% higher than the result (F=63.28) reported in Sun and Korhonen (2009) using the F1 feature. This experiment shows, on two datasets, that DA features are clearly more effective than the frame features for verb clustering, even when relaxations are used. 4 Analysis of Feature Frequency A further experiment was carried out using F1 and F3 on Joanis et al. (2008)’s test sets 10 and 11. The frequency ranked features were added to the clustering one at a time, starting from the most frequent one. The results are shown in figure 2. F3 outperforms F1 clearly on all the feature number settings. After adding some highly frequent frames (22 for test set 10 and 67 for test</context>
</contexts>
<marker>Sun, Korhonen, 2009</marker>
<rawString>L. Sun and A. Korhonen. Improving verb clustering with automatically acquired selectional preferences. In Proceedings of EMNLP, pages 638–647, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Sun</author>
<author>A Korhonen</author>
</authors>
<title>Hierarchical verb clustering using graph factorization.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1023--1033</pages>
<publisher>ACL.</publisher>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="2491" citStr="Sun and Korhonen, 2011" startWordPosition="361" endWordPosition="364">ariety of purposes such as semantic role labelling (Gildea and Jurafsky, 2002), predicting unseen syntax (Parisien and Stevenson, 2010), argument zoning (Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (20</context>
</contexts>
<marker>Sun, Korhonen, 2011</marker>
<rawString>L. Sun and A. Korhonen. Hierarchical verb clustering using graph factorization. In Proceedings of EMNLP, pages 1023–1033, Edinburgh, Scotland, UK., July 2011. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Sun</author>
<author>A Korhonen</author>
<author>Y Krymolowski</author>
</authors>
<title>Verb class discovery from rich syntactic data.</title>
<date>2008</date>
<journal>Lecture Notes in Computer Science,</journal>
<volume>4919</volume>
<contexts>
<context position="10285" citStr="Sun et al. (2008)" startWordPosition="1685" endWordPosition="1688"> did this so that F3 included the SCF features as well as the DA approximation features. It would be possible in future work to exclude the pairs involving identical frames, thereby relying solely on the DA approximations, and compare performance with the results obtained here. occurrence of a verb as a member of one of the 168 SCFs on the basis of grammatical relations identified by the RASP (Briscoe et al., 2006) parser. We experimented with two datasets that have been used in prior work on verb clustering: the test sets 7-11 (3-14 classes) in Joanis et al. (2008), and the 17 classes set in Sun et al. (2008). We used the spectral clustering (SPEC) method and settings as in Sun and Korhonen (2009) but adopted the Bhattacharyya kernel (Jebara and Kondor, 2003) to improve the computational efficiency of the approach given the high dimensionality of the quadratic feature space. D wb(v, v&apos;) = (vdvd)1/2 (6) d=1 The mean-filed bound of the Bhattacharyya kernel is very similar to the KL divergence kernel (Jebara et al., 2004) which is frequently used in verb clustering experiments (Korhonen et al., 2003; Sun and Korhonen, 2009). To further reduce computational complexity, we restricted our scope to the m</context>
</contexts>
<marker>Sun, Korhonen, Krymolowski, 2008</marker>
<rawString>L. Sun, A. Korhonen, and Y. Krymolowski. Verb class discovery from rich syntactic data. Lecture Notes in Computer Science, 4919:16, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Tsang</author>
<author>S Stevenson</author>
</authors>
<title>Using selectional profile distance to detect verb alternations.</title>
<date>2004</date>
<booktitle>In HLT/NAACL 2004 Workshop on Computational Lexical Semantics,</booktitle>
<contexts>
<context position="3017" citStr="Tsang and Stevenson, 2004" startWordPosition="438" endWordPosition="441">tic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and Stevenson (2010, 2011). Merlo and Stevenson (2001) used cues such as passive voice, animacy and syntactic frames coupled with the overlap of lexical fillers between the alternating slots to predict a 3-way classification (unergative, unaccusative and object-drop). Joanis et al. (2008) used similar features to classify verbs on a much larger scale. They classify up to 496 verbs using 11 different classifications each having between 2 and 14 classes. Parisien and Stevenson (2010, 2011) used hierarchica</context>
</contexts>
<marker>Tsang, Stevenson, 2004</marker>
<rawString>V. Tsang and S. Stevenson. Using selectional profile distance to detect verb alternations. In HLT/NAACL 2004 Workshop on Computational Lexical Semantics, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vlachos</author>
<author>A Korhonen</author>
<author>Z Ghahramani</author>
</authors>
<title>Unsupervised and constrained dirichlet process mixture models for verb clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>74--82</pages>
<contexts>
<context position="2513" citStr="Vlachos et al., 2009" startWordPosition="365" endWordPosition="368">as semantic role labelling (Gildea and Jurafsky, 2002), predicting unseen syntax (Parisien and Stevenson, 2010), argument zoning (Guo et al., 2011) and metaphor identification (Shutova et al., 2010). While Levin’s classification can be extended manually (Kipper-Schuler, 2005), a large body of research has developed methods for automatic verb classification since such methods can be applied easily to other domains and languages. Existing work on automatic classification relies largely on syntactic features such as subcategorization frames (SCF)s (Schulte im Walde, 2006; Sun and Korhonen, 2011; Vlachos et al., 2009; Brew and Schulte im Walde, 2002). There has also been some success incorporating selectional preferences (Sun and Korhonen, 2009). Few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on them heavily, and there has been related work on identifying the DAs themselves automatically using SCF and semantic information (Resnik, 1993; McCarthy and Korhonen, 1998; Lapata, 1999; McCarthy, 2000; Tsang and Stevenson, 2004). Exceptions to this include Merlo and Stevenson (2001), Joanis et al. (2008) and Parisien and S</context>
<context position="14656" citStr="Vlachos et al. (2009)" startWordPosition="2424" endWordPosition="2427">early outperform frame features. The top figure is the result on test set 10 (8 ways). The bottom figure is the result on test set 11 (14 ways). The x axis is the number of features. The y axis is the F-Measure result. Stevenson (2011) extended this work by adding semantic features. Parisien and Stevenson’s (2010) model 2 has a similar structure to the graphic model in figure 1. A fundamental difference is that we explicitly use a probability distribution over alternations (pair of frames) to represent a verb, whereas they represent a verb by a distribution over the observed frames similar to Vlachos et al. (2009) ’s approach. Also the parameters in their model were inferred by Gibbs sampling whereas we avoided this inference step by using relaxation. 6 Conclusion and Future work We have demonstrated the merits of using DAs for verb clustering compared to the SCF data from which they are derived on standard verb classification datasets and when integrated in a stateof-the-art verb clustering system. We have also demonstrated that the performance of frame features is dominated by the high frequency frames. In contrast, the DA features enable the mid-range frequency frames to further improve the performa</context>
</contexts>
<marker>Vlachos, Korhonen, Ghahramani, 2009</marker>
<rawString>A. Vlachos, A. Korhonen, and Z. Ghahramani. Unsupervised and constrained dirichlet process mixture models for verb clustering. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 74–82, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhao</author>
<author>H Liu</author>
</authors>
<title>Spectral feature selection for supervised and unsupervised learning.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>1151--1157</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="15737" citStr="Zhao and Liu (2007)" startWordPosition="2608" endWordPosition="2611">dominated by the high frequency frames. In contrast, the DA features enable the mid-range frequency frames to further improve the performance. 1 22 100 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 SinF1 Pai F3 1 67 149 0.7 0.65 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 739 In the future, we plan to evaluate the performance of DA features in a larger scale experiment. Due to the high dimensionality of the transformed feature space (quadratic of the original feature space), we will need to improve the computational efficiency further, e.g. via use of an unsupervised dimensionality reduction technique Zhao and Liu (2007). Moreover, we plan to use Bayesian inference as in Vlachos et al. (2009); Parisien and Stevenson (2010, 2011) to infer the actual parameter values and avoid the relaxation. Finally, we plan to supplement the DA feature with evidence from the slot fillers of the alternating slots, in the spirit of earlier work (McCarthy, 2000; Merlo and Stevenson, 2001; Joanis et al., 2008). Unlike these previous works, we will use selectional preferences to generalize the argument heads but will do so using preferences from distributional data (Sun and Korhonen, 2009) rather than WordNet, and use all argument</context>
</contexts>
<marker>Zhao, Liu, 2007</marker>
<rawString>Z. Zhao and H. Liu. Spectral feature selection for supervised and unsupervised learning. In Proceedings of ICML, pages 1151–1157, New York, NY, USA, 2007. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>