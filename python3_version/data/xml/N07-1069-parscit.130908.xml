<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000557">
<title confidence="0.974887">
Can Semantic Roles Generalize Across Genres?
</title>
<author confidence="0.999245">
Szu-ting Yi Edward Loper Martha Palmer
</author>
<affiliation confidence="0.865290666666667">
Dept of Computer Science Dept of Computer Science Dept of Computer Science
University of Pennsylvania University of Pennsylvania University of Colorado at Boulder
Philadelphia, PA 19104 Philadelphia, PA 19104 Boulder, CO 80309
</affiliation>
<sectionHeader confidence="0.981818" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995575">
PropBank has been widely used as train-
ing data for Semantic Role Labeling.
However, because this training data is
taken from the WSJ, the resulting machine
learning models tend to overfit on idiosyn-
crasies of that text’s style, and do not port
well to other genres. In addition, since
PropBank was designed on a verb-by-verb
basis, the argument labels Arg2 - Arg5 get
used for very diverse argument roles with
inconsistent training instances. For exam-
ple, the verb “make” uses Arg2 for the
“Material” argument; but the verb “multi-
ply” uses Arg2 for the “Extent” argument.
As a result, it can be difficult for auto-
matic classifiers to learn to distinguish ar-
guments Arg2-Arg5. We have created a
mapping between PropBank and VerbNet
that provides a VerbNet thematic role la-
bel for each verb-specific PropBank label.
Since VerbNet uses argument labels that
are more consistent across verbs, we are
able to demonstrate that these new labels
are easier to learn.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9988312">
Correctly identifying semantic entities and success-
fully disambiguating the relations between them and
their predicates is an important and necessary step
for successful natural language processing applica-
tions, such as text summarization, question answer-
ing, and machine translation. For example, in or-
der to determine that question (1a) is answered by
sentence (1b), but not by sentence (1c), we must de-
termine the relationships between the relevant verbs
(eat and feed) and their arguments.
</bodyText>
<listItem confidence="0.97486825">
(1) a. What do lobsters like to eat?
b. Recent studies have shown that lobsters pri-
marily feed on live fish, dig for clams, sea
urchins, and feed on algae and eel-grass.
</listItem>
<bodyText confidence="0.9799525">
c. In the early 20th century, Mainers would
only eat lobsters because the fish they
caught was too valuable to eat themselves.
An important part of this task is Semantic Role
Labeling (SRL), where the goal is to locate the con-
stituents which are arguments of a given verb, and to
assign them appropriate semantic roles that describe
how they relate to the verb. Many researchers have
investigated applying machine learning to corpus
specifically annotated with this task in mind, Prop-
Bank, since 2000 (Chen and Rambow, 2003; Gildea
and Hockenmaier, 2003; Hacioglu et al., 2003; Mos-
chitti, 2004; Yi and Palmer, 2004; Pradhan et al.,
2005b; Punyakanok et al., 2005; Toutanova et al.,
2005). For two years, the CoNLL workshop has
made this problem the shared task (Carreras and
M´arquez, 2005). However, there is still little con-
sensus in the linguistic and NLP communities about
what set of role labels are most appropriate. The
Proposition Bank (PropBank) corpus (Palmer et al.,
2005) avoids this issue by using theory-agnostic la-
bels (Arg0, Arg1, ..., Arg5), and by defining those
labels to have verb-specific meanings. Under this
scheme, PropBank can avoid making any claims
</bodyText>
<page confidence="0.957446">
548
</page>
<note confidence="0.801353">
Proceedings of NAACL HLT 2007, pages 548–555,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.997410984848485">
about how any one verb’s arguments relate to other
verbs’ arguments, or about general distinctions be-
tween verb arguments and adjuncts.
However, there are several limitations to this ap-
proach. The first is that it can be difficult to make
inferences and generalizations based on role labels
that are only meaningful with respect to a single
verb. Since each role label is verb-specific, we can
not confidently determine when two different verbs’
arguments have the same role; and since no encoded
meaning is associated with each tag, we can not
make generalizations across verb classes. In con-
trast, the use of a shared set of role labels, such as
thematic roles, would facilitate both inferencing and
generalization.
The second issue with PropBank’s verb-specific
approach is that it can make training automatic se-
mantic role labeling (SRL) systems more difficult.
A vast amount of data would be needed to train the
verb-specific models that are theoretically mandated
by PropBank’s design. Instead, researchers typically
build a single model for the numbered arguments
(Arg0, Arg1, ..., Arg5). This approach works sur-
prisingly well, mainly because an explicit effort was
made to use arguments Arg0 and Arg1 consistently
across different verbs; and because those two argu-
ment labels account for 85% of all arguments. How-
ever, this approach causes the system to conflate
different argument types, especially with the highly
overloaded arguments Arg2-Arg5. As a result, these
argument labels are quite difficult to learn.
A final difficulty with PropBank’s current ap-
proach is that it limits SRL system robustness in
the face of verb senses, verbs or verb constructions
that were not included in the training data, and the
training data is all Wall Street Journal corpora. If
a PropBank-trained SRL system encounters a novel
verb or verb usage, then there is no way for it to
know which role labels are used for which argument
types, since role labels are defined so specifically.
This is especially problematic for Arg2-5. Similarly,
PropBank-trained SRL systems can have difficulty
generalizing when a known verb is encountered in
a novel construction. These problems can happen
quite frequently if the training data comes from a
different genre than the test data. This issue is re-
flected in the relatively poor performance of most
state-of-the-art SRL systems when tested on a novel
genre, the Brown corpus, during CoNLL 2005. For
example, the SRL system described in (Pradhan et
al., 2005b; Pradhan et al., 2005a) achieves an F-
score of 81% when tested on the same genre as it
is trained on (WSJ); but that score drops to 68.5%
when the same system is tested on a different genre
(the Brown corpus). DARPA-GALE is funding an
ongoing effort to PropBank additional genres, but
better techniques for generalizing the semantic role
labeling task are still needed.
In this paper, we demonstrate an increase in the
generality of our semantic role labeling based on a
mapping that has been developed between PropBank
and another lexical resource, VerbNet. By taking ad-
vantage of VerbNet’s more consistent set of labels,
we can generate more useful role label annotations
with a resulting improvement in SRL performance
on novel genres.
</bodyText>
<sectionHeader confidence="0.97881" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.959746">
2.1 PropBank
</subsectionHeader>
<bodyText confidence="0.999975269230769">
PropBank (Palmer et al., 2005) is an annotation of
one million words of the Wall Street Journal por-
tion of the Penn Treebank II (Marcus et al., 1994)
with predicate-argument structures for verbs, using
semantic role labels for each verb argument. In or-
der to remain theory neutral, and to increase anno-
tation speed, role labels were defined on a per-verb-
sense basis. Although the same tags were used for
all verbs, (namely Arg0, Arg1, ..., Arg5), these tags
are meant to have a verb-specific meaning.
Thus, the use of a given argument label should
be consistent across different uses of that verb, in-
cluding syntactic alternations. For example, the
Arg1 (underlined) in “John broke the window” is the
same window that is annotated as the Arg1 in “The
window broke”, even though it is the syntactic sub-
ject in one sentence and the syntactic object in the
other. However, there is no guarantee that an argu-
ment label will be used consistently across different
verbs. For example, the Arg2 label is used to des-
ignate the destination of the verb “bring;” but the
extent of the verb “rise.” Generally, the arguments
are simply listed in the order of their prominence
for each verb. However, an explicit effort was made
when PropBank was created to use Arg0 for argu-
ments that fulfill Dowty’s criteria for “prototypical
</bodyText>
<page confidence="0.997464">
549
</page>
<bodyText confidence="0.999872666666667">
agent,” and Arg1 for arguments that fulfill the cri-
teria for “prototypical patient.” (Dowty, 1991) As
a result, these two argument labels are significantly
more consistent across verbs than the other three.
But nevertheless, there are still some inter-verb in-
consistencies for even Arg0 and Arg1.
</bodyText>
<subsectionHeader confidence="0.994344">
2.2 VerbNet
</subsectionHeader>
<bodyText confidence="0.999978074074074">
VerbNet (Schuler, 2005) consists of hierarchically
arranged verb classes, inspired by and extended
from classes of Levin 1993 (Levin, 1993). Each
class and subclass is characterized extensionally by
its set of verbs, and intensionally by a list of the
arguments of those verbs and syntactic and seman-
tic information about the verbs. The argument list
consists of thematic roles (23 in total) and pos-
sible selectional restrictions on the arguments ex-
pressed using binary predicates. The syntactic infor-
mation maps the list of thematic arguments to deep-
syntactic arguments (i.e., normalized for voice alter-
nations, and transformations). The semantic predi-
cates describe the participants during various stages
of the event described by the syntactic frame.
The same thematic role can occur in different
classes, where it will appear in different predicates,
providing a class-specific interpretation of the role.
VerbNet has been extended from the original Levin
classes, and now covers 4526 senses for 3769 verbs.
A primary emphasis for VerbNet is the grouping of
verbs into classes that have a coherent syntactic and
semantic characterization, that will eventually facil-
itate the acquisition of new class members based on
observable syntactic and semantic behavior. The hi-
erarchical structure and small number of thematic
roles is aimed at supporting generalizations.
</bodyText>
<subsectionHeader confidence="0.998915">
2.3 Mapping PropBank to VerbNet
</subsectionHeader>
<bodyText confidence="0.999986325">
Because PropBank includes a large corpus of man-
ually annotated predicate-argument data, it can be
used to train supervised machine learning algo-
rithms, which can in turn provide PropBank-style
annotations for novel or unseen text. However, as
we discussed in the introduction, PropBank’s verb-
specific role labels are somewhat problematic. Fur-
thermore, PropBank lacks much of the information
that is contained in VerbNet, including information
about selectional restrictions, verb semantics, and
inter-verb relationships.
We have therefore created a mapping between
VerbNet and PropBank (Loper et al., 2007), which
will allow us to use the machine learning tech-
niques that have been developed for PropBank anno-
tations to generate more semantically abstract Verb-
Net representations. Additionally, the mapping can
be used to translate PropBank-style numbered ar-
guments (Arg0... Arg5) to VerbNet thematic roles
(Agent, Patient, Theme, etc.), which should allow us
to overcome the verb-specific nature of PropBank.
The mapping between VerbNet and PropBank
consists of two parts: a lexical mapping and an in-
stance classifier. The lexical mapping is responsible
for specifying the potential mappings between Prop-
Bank and VerbNet for a given word; but it does not
specify which of those mappings should be used for
any given occurrence of the word. That is the job
of the instance classifier, which looks at the word
in context, and decides which of the mappings is
most appropriate. In essence, the instance classi-
fier is performing word sense disambiguation, de-
ciding which lexeme from each database is correct
for a given occurrence of a word. In order to train
the instance classifier, we semi-automatically anno-
tated each verb in the PropBank corpus with Verb-
Net class information.1 This mapped corpus was
then used to build the instance classifier. More de-
tails about the mapping, and how it was created, can
be found in (Loper et al., 2007).
</bodyText>
<sectionHeader confidence="0.581916" genericHeader="method">
3 Analysis of the Mapping
</sectionHeader>
<bodyText confidence="0.999954571428572">
In order to confirm our belief that PropBank roles
Arg0 and Arg1 are relatively coherent, while roles
Arg2-5 are much more overloaded, we performed
a preliminary analysis of how argument roles were
mapped. Figure 1 shows how often each PropBank
role was mapped to each VerbNet thematic role, cal-
culated as a fraction of instances in the mapped cor-
pus. From this figure, we can see that Arg0 maps to
agent-like roles, such as “agent” and “experiencer,”
over 94% of the time; and Arg1 maps to patient-
like roles, including “theme,” “topic,” and “patient,”
over 82% of the time. In contrast, arguments Arg2-5
get mapped to a much broader variety of roles. It is
also worth noting that the sample size for arguments
</bodyText>
<footnote confidence="0.969795">
1Excepting verbs whose senses are not present in VerbNet
(24.5% of instances).
</footnote>
<page confidence="0.994506">
550
</page>
<bodyText confidence="0.99989025">
Arg3-5 is quite small in comparison with arguments
Arg0-2, suggesting that any automatically built clas-
sifier for arguments Arg3-5 will suffer severe sparse
data problems for those arguments.
</bodyText>
<note confidence="0.5051745">
4 Training a SRL system with VerbNet
Roles to Achieve Robustness
</note>
<bodyText confidence="0.999984653846154">
An important issue for state-of-the-art automatic
SRL systems is robustness: although they receive
high performance scores when tested on the Wall
Street Journal (WSJ) corpus, that performance drops
significantly when the same systems are tested on a
corpus from another genre. This performance drop
reflects the fact that the WSJ corpus is highly spe-
cialized, and tends to use genre-specific word senses
for many verbs. The 2005 CoNLL shared task has
addressed this issue of robustness by evaluating par-
ticipating systems on a test set extracted from the
Brown corpus, which is very different from the WSJ
corpus that was used for training. The results sug-
gest that there is much work to be done in order to
improve system robustness.
One of the reasons that current SRL systems have
difficulty deciding which role label to assign to a
given argument is that role labels are defined on a
per-verb basis. This is less problematic for Arg0
and Arg1, where a conscious effort was made to be
consistent across verbs; but is a significant problem
for Args[2-5], which tend to have very verb-specific
meanings. This problem is exacerbated even fur-
ther on novel genres, where SRL systems are more
likely to encounter unseen verbs and uses of argu-
ments that were not encountered in the training data.
</bodyText>
<subsectionHeader confidence="0.997959">
4.1 Addressing Current SRL Problems via
Lexical Mappings
</subsectionHeader>
<bodyText confidence="0.997232727272727">
By exploiting the mapping between PropBank and
VerbNet, we can transform the data to make it more
consistent, and to expand the size and variety of the
training data. In particular, we can use the map-
ping to transform the verb-specific PropBank role
labels into the more general thematic role labels that
are used by VerbNet. Unlike the PropBank labels,
the VerbNet labels are defined consistently across
verbs; and therefore it should be easier for statisti-
cal SRL systems to model them. Furthermore, since
the VerbNet role labels are significantly less verb-
</bodyText>
<table confidence="0.999885104166667">
Arg1 (59,884)
Theme 47.0%
Topic 23.0%
Patient 10.8%
Product 2.9%
Predicate 2.5%
Patient1 2.4%
Stimulus 2.0%
Experiencer 1.9%
Cause 1.8%
Destination 0.9%
Theme2 0.7%
Location 0.7%
Source 0.7%
Theme1 0.6%
Actor2 0.6%
Recipient 0.5%
Agent 0.4%
Attribute 0.2%
Asset 0.2%
Patient2 0.2%
Material 0.2%
Beneficiary 0.0%
Arg2 (11,077)
Recipient 22.3%
Extent 14.7%
Predicate 13.4%
Destination 8.6%
Attribute 7.6%
Location 6.5%
Theme 5.5%
Patient2 5.3%
Source 5.2%
Topic 3.1%
Theme2 2.5%
Product 1.5%
Cause 1.2%
Material 0.8%
Instrument 0.6%
Beneficiary 0.5%
Experiencer 0.3%
Actor2 0.2%
Asset 0.0%
Theme1 0.0%
Arg4 (18)
Beneficiary 61.1%
Product 33.3%
Location 5.6%
</table>
<figureCaption confidence="0.993362">
Figure 1: The frequency with which each PropBank
</figureCaption>
<bodyText confidence="0.7384924">
numbered argument is mapped to each VerbNet the-
matic role in the mapped corpus. The numbers
next to each PropBank argument reflects the num-
ber of occurrences of that numbered argument in the
mapped corpus.
</bodyText>
<table confidence="0.997708925925926">
Arg0 (45,579)
Agent 85.4%
Experiencer 7.2%
Theme 2.1%
Cause 1.9%
Actor1 1.8%
Theme1 0.8%
Patient1 0.2%
Location 0.2%
Theme2 0.2%
Product 0.1%
Patient 0.0%
Attribute 0.0%
Arg3 (609)
Asset 38.6%
Source 25.1%
Beneficiary 10.7%
Cause 9.7%
Predicate 9.0%
Location 2.0%
Material 1.8%
Theme1 1.6%
Theme 0.8%
Destination 0.3%
Instrument 0.3%
Arg5 (17)
Location 100.0%
</table>
<page confidence="0.998019">
551
</page>
<bodyText confidence="0.999716333333333">
dependent than the PropBank roles, the SRL’s mod-
els should generalize better to novel verbs, and to
novel uses of known verbs.
</bodyText>
<sectionHeader confidence="0.914781" genericHeader="method">
5 SRL Experiments on Linked Lexical
Resources
</sectionHeader>
<bodyText confidence="0.9978542">
In order to verify the feasibility of performing se-
mantic role labeling with VerbNet thematic roles, we
re-trained our existing SRL system, which originally
used PropBank role labels, with a new label set that
makes use of VerbNet thematic role information.
</bodyText>
<subsectionHeader confidence="0.995628">
5.1 The SRL System
</subsectionHeader>
<bodyText confidence="0.999954">
Our SRL system is a Maximum Entropy based
pipelined system which consists of four compo-
nents: Pre-processing, Argument Identification, Ar-
gument Classification, and Post Processing. The
Pre-processing component pipes a sentence through
a syntactic parser and filters out constituents which
are unlikely to be semantic arguments based on a
constituents location in the parse tree. The Argu-
ment Identification component is a binary MaxEnt
classifier, which tags candidate constituents as ar-
guments or non-arguments. The Argument Classifi-
cation component is a multi-class MaxEnt classifier
which assigns a semantic role to each constituent.
The Post Processing component further selects the
final arguments based on global constraints. Our ex-
periments mainly focused on changes to the Argu-
ment Classification stage of the SRL pipeline, and
in particular, on changes to the set of output tags.
For more information on our SRL system, see (Yi
and Palmer, 2004; Yi and Palmer, 2005).
The evaluation of SRL systems is typically ex-
pressed by precision, recall and the F1-measure.
Precision is the number of correct arguments pre-
dicted by a system divided by the total number of
arguments proposed. Recall is the number of cor-
rect arguments divided by the number of the total
number of arguments in the Gold Standard Data. F1
computes the harmonic mean of precision and recall.
</bodyText>
<subsectionHeader confidence="0.99597">
5.2 SRL Experiments on Mapped VerbNet
Thematic Roles
</subsectionHeader>
<bodyText confidence="0.988687">
Since PropBank arguments Arg0 and Arg1 are al-
ready quite coherent, we left them as-is in the new
label set. But since arguments Arg2-Arg5 are highly
</bodyText>
<table confidence="0.989502285714286">
Group 1 Group 2 Group 3 Group 4 Group 5
Recipient Extent Predicate Patient2 Instrument
Destination Asset Attribute Product Cause
Location Theme Experiencer
Source Theme1 Actor2
Material Theme2
Beneficiary Topic
</table>
<figureCaption confidence="0.975947">
Figure 2: Thematic Role Groupings for the exper-
iments on linked lexical resources; and for Arg2 in
the experiments on arguments with different verb in-
dependency.
</figureCaption>
<bodyText confidence="0.998907304347826">
overloaded, we replaced them by mapping them
to their corresponding VerbNet thematic role. We
found that mapping directly to individual role labels
created a significant sparse data problem, since the
number of output tags was increased from 6 to 23.
We therefore grouped the VerbNet thematic roles
into five coherent groups of similar thematic roles,
shown in Figure 2.2 Our new tag set therefore in-
cluded the following tags: Arg0 (agent); Arg1 (pa-
tient); Group1 (goal); Group2 (extent); Group3
(predicate/attrib); Group4 (product); and Group5
(instrument/cause).
Training our SRL system using these thematic
role groups, we obtained performance similar to the
original SRL system. However, it is important to
note that these performance figures are not directly
comparable, since the two systems are performing
different tasks: The Original system labels Arg0-
5,ArgA and ArgM and the Mapped system labels
Arg0, Arg1, ArgA, ArgM and Group1-5. In partic-
ular, the role labels generated by the original system
are verb-specific, while the role labels generated by
the new system are less verb-dependent.
</bodyText>
<sectionHeader confidence="0.892056" genericHeader="evaluation">
5.2.1 Results
</sectionHeader>
<bodyText confidence="0.997179">
For our testing and training, we used the portion
of Penn Treebank II that is covered by the mapping,
and where at least one of Arg2-5 is used. Training
was performed using sections 2-21 of the Treebank
(10,783 instances of argument); and testing was per-
formed on section 23 (859 instances). Table 1 dis-
plays the performance score for the SRL system us-
ing the augmented tag set (“Mapped”). The per-
formance score of the original system (“Original”)
is also listed, for reference; however, as was dis-
</bodyText>
<footnote confidence="0.948034">
2Karin Kipper assisted in creating the groupings.
</footnote>
<page confidence="0.98236">
552
</page>
<table confidence="0.993815333333333">
System Precision Recall F1
Original 90.65 85.43 87.97
Mapped 88.85 84.56 86.65
</table>
<tableCaption confidence="0.915685333333333">
Table 1: Overall SRL System performance using the
PropBank tag set (“Original”) and the augmented
tag set (“Mapped”)
</tableCaption>
<table confidence="0.999689333333333">
System Precision Recall F1
Original 97.60 83.67 90.10
Mapped 91.70 82.86 87.06
</table>
<tableCaption confidence="0.878386">
Table 2: SRL System performance evaluated on only
Arg2-5 (Original) or Group1-5 (Mapped).
</tableCaption>
<bodyText confidence="0.99980252">
cussed above, these results are not directly compara-
ble because the two systems are performing different
tasks.
The results indicate that the performance drops
when we train on the new argument labels, espe-
cially on precision when we evaluate the systems
on only Arg2-5/Group1-5 (see Table 2). However,
it is premature to conclude that there is no benefit
from the VerbNet thematic role labels. Firstly, we
have very few mapped Arg3-5 instances (less than
1,000 instances); secondly, we lack test data gen-
erated from a genre other than WSJ to allow us to
evaluate the robustness (generality) of SRL trained
on the new argument labels.
We therefore redesigned our experiments by lim-
iting the scope to mapped instances of Arg1 and
Arg2. By doing this, we should be able to accom-
plish the following: 1) we can map new argument la-
bels back to the original PropBank labels; therefore
we can directly compare results; 2) With the ability
of testing our systems on other test data, we can eval-
uate the influence of the mapping on SRL robust-
ness; 3) We can validate our original hypothesis that
the behavior of Arg1 is primarily verb-independent
while Arg2 is more verb-specific.
</bodyText>
<subsectionHeader confidence="0.9966535">
5.3 SRL Experiments on Arguments with
Different Verb Independency
</subsectionHeader>
<bodyText confidence="0.9940204">
We conducted two further sets of experiments: one
to test the effect of the mapping on learning Arg2;
and one to test the effect on learning Arg1. Since
Arg2 is used in very verb-dependent ways, we ex-
pect that mapping it to VerbNet role labels will in-
</bodyText>
<table confidence="0.982934222222222">
Group 1 Group 2 Group 3 Group 4 Group 5
Theme Source Patient Agent Topic
Theme1 Location Product Actor2
Theme2 Destination Patient1 Experiencer
Predicate Recipient Patient2 Cause
Stimulus Beneficiary
Attribute Material
Group 6
Asset
</table>
<figureCaption confidence="0.869175333333333">
Figure 3: Thematic Role Groupings for Arg1 in the
experiments on arguments with different verb inde-
pendency.
</figureCaption>
<bodyText confidence="0.997887083333333">
crease our performance. However, since a conscious
effort was made to keep the meaning of Arg1 consis-
tent across verbs, we expect that mapping it to Verb-
Net labels will provide less of an improvement.
Each experiment compares two SRL systems: one
trained using the original PropBank role labels; the
other trained with the argument role under consid-
eration (Arg1 or Arg2) subdivided based on which
VerbNet role label it maps to. In order to prevent
the training data from these subdivided labels from
becoming too sparse (which would impair system
performance) we grouped similar thematic roles to-
gether. For Arg2, we used the same groupings as the
previous experiment, shown in Figure 2. The argu-
ment role groupings we used for Arg1 are shown in
Figure 3.
The training data for both experiments is the por-
tion of Penn Treebank II (sections 02-21) that is cov-
ered by the mapping. We evaluated each experi-
mental system using two test sets: section 23 of the
Penn Treebank II, which represents the same genre
as the training data; and the PropBank-ed portion of
the Brown corpus, which represents a very different
genre.
</bodyText>
<subsubsectionHeader confidence="0.38229">
5.3.1 Results and Discussion
</subsubsectionHeader>
<bodyText confidence="0.999982666666667">
Table 3 describes the results of SRL overall per-
formance tested on the WSJ corpus Section 23; Ta-
ble 4 demonstrates the SRL overall system perfor-
mance tested on the Brown corpus. Systems Arg1-
Original and Arg2-Original are trained using the
original PropBank labels, and show the baseline
performance of our SRL system. Systems Arg1-
Mapped and Arg2-Mapped are trained using Prop-
Bank labels augmented with VerbNet thematic role
groups. In order to allow comparison between the
system using the original PropBank labels and the
systems that augmented those labels with VerbNet
</bodyText>
<page confidence="0.995896">
553
</page>
<table confidence="0.9996452">
System Precision Recall F1
Arg1-Original 89.24 77.32 82.85
Arg1-Mapped 90.00 76.35 82.61
Arg2-Original 73.04 57.44 64.31
Arg2-Mapped 84.11 60.55 70.41
</table>
<tableCaption confidence="0.629361">
Table 3: SRL System Performance on Arg1 Map-
ping and Arg2 Mapping, tested using the WSJ cor-
pus (section 23). This represents performance on the
same genre as the training corpus.
</tableCaption>
<table confidence="0.9995538">
System Precision Recall F1
Arg1-Original 86.01 71.46 78.07
Arg1-Mapped 88.24 71.15 78.78
Arg2-Original 66.74 52.22 58.59
Arg2-Mapped 81.45 58.45 68.06
</table>
<tableCaption confidence="0.990198">
Table 4: SRL System Performance on Arg1 Map-
</tableCaption>
<bodyText confidence="0.986655714285715">
ping and Arg2 Mapping, tested using the PropBank-
ed Brown corpus. This represents performance on a
different genre from the training corpus.
thematic role groups, system performance was eval-
uated based solely on the PropBank role label that
was assigned.
We had hypothesized that with the use of thematic
roles, we would be able to create a more consis-
tent training data set which would result in an im-
provement in system performance. In addition, the
thematic roles would behave more consistently than
the overloaded Args[2-5] across verbs, which should
enhance robustness. However, since in practice we
are also increasing the number of argument labels
an SRL system needs to tag, the system might suf-
fer from data sparseness. Our hope is that the en-
hancement gained from the mapping will outweigh
the loss due to data sparseness.
From Table 3 and Table 4 we see the F1 scores of
Arg1-Original and Arg1-Mapped are statistically in-
different both on the WSJ corpus and the Brown cor-
pus. These results confirm the observation that Arg1
in the PropBank behaves fairly verb-independently
so that the VerbNet mapping does not provide much
benefit. The increase of precision due to a more co-
herent training data set is compensated for by the
loss of recall due to data sparseness.
The results of the Arg2 experiments tell a differ-
</bodyText>
<table confidence="0.999070090909091">
Confusion ARG2-Original
Matrix
ARG1 ARG2 ARGM
ARG2- ARG0 53 50 -
Mapped
ARG1 - 716 -
ARG2 1 - 2
ARG3 - 1 -
ARGM 1 482 -
233 ARG2-Mapped arguments are not labeled by ARG2-
Original
</table>
<tableCaption confidence="0.941592666666667">
Table 5: Confusion matrix on the 1,539 instances
which ARG2-Mapped tags correctly and ARG2-
Original fails to predict.
</tableCaption>
<bodyText confidence="0.9998392">
ent story. Both precision and recall are improved
significantly, which demonstrates that the Arg2 label
in the PropBank is quite overloaded. The Arg2 map-
ping improves the overall results (F1) on the WSJ
by 6% and on the Brown corpus by almost 10%. As
a more diverse corpus, the Brown corpus provides
many more opportunities for generalizing to new us-
ages. Our new SRL system handles these cases more
robustly, demonstrating the consistency and useful-
ness of the thematic role categories.
</bodyText>
<subsectionHeader confidence="0.505292">
5.4 Improved Argument Distinction via
Mapping
</subsectionHeader>
<bodyText confidence="0.999938">
The ARG2-Mapped system generalizes well both
on the WSJ corpus and the Brown corpus. In or-
der to explore the improved robustness brought by
the mapping, we extracted and observed the 1,539
instances to which the system ARG2-Mapped as-
signed the correct semantic role label, but which the
system ARG2-Original failed to predict. From the
confusion matrix depicted in Table 5, we discover
the following:
The mapping makes ARG2 more clearly defined,
and as a result there is a better distinction be-
tween ARG2 and other argument labels: Among
the 1,539 instances that ARG2-Original didn’t tag
correctly, 233 instances are not assigned an argu-
ment label, and 1,252 instances ARG2-Original con-
fuse the ARG2 label with another argument label:
the system ARG2-Original assigned the ARG2 la-
bel to 50 ARG0’s, 716 ARG1’s, 1 ARG3 and 482
ARGM’s, and assigned other argument labels to 3
ARG2’s.
</bodyText>
<page confidence="0.997835">
554
</page>
<sectionHeader confidence="0.999646" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999968421052632">
In conclusion, we have described a mapping from
the annotated PropBank corpus to VerbNet verb
classes with associated thematic role labels. We hy-
pothesized that these labels would be more verb-
independent and less overloaded than the PropBank
Args2-5, and would therefore provide more consis-
tent training instances which would generalize better
to new genres. Our preliminary experiments confirm
this hypothesis, with a 6% performance improve-
ment on the WSJ and a 10% performance improve-
ment on the Brown corpus for Arg2.
In future work, we will map the PropBank-ed
Brown corpus to VerbNet as well, which will allow
much more thorough testing of our hypothesis. We
will also examine back-off to verb class membership
as a technique for improving performance on out of
vocabulary verbs. Finally, we plan to explore the ef-
fect of different thematic role groupings on system
performance.
</bodyText>
<sectionHeader confidence="0.999092" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999870469696969">
Xavier Carreras and Llu´ıs M´arquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling.
In Proceedings of CoNLL.
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of EMNLP-2003,
Sapporo, Japan.
D. R. Dowty. 1991. Thematic proto-roles and argument
selection. Language, 67:574–619.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using Combinatory Categorial Gram-
mar. In 2003 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 57–64,
Sapporo, Japan.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2003. Shallow semantic
parsing using support vector machines. Technical re-
port, The Center for Spoken Language Research at the
University of Colorado (CSLR).
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press.
Edward Loper, Szu-ting Yi, and Martha Palmer. 2007.
Empirical evidence for useful semantic role categories.
In Proceedings of the International Workshop on Com-
putational Linguistics.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn treebank: Annotating predicate argu-
ment structure.
Alessandro Moschitti. 2004. A study on convolution
kernel for shallow semantic parsing. In Proceedings
of the 42-th Conference on Association for Computa-
tional Linguistic (ACL-2004), Barcelona, Spain.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: A corpus annotated with
semantic roles. Computational Linguistics, 31(1):71–
106.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, H. Mar-
tin, James, and Daniel Jurafsky. 2005a. Semantic role
chunking combining complementary syntactic views.
In Proceedings of CoNLL-2005.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2005b. Semantic role la-
beling using different syntactic views. In Proceedings
of the Association for Computational Linguistics 43rd
annual meeting (ACL-2005), Ann Arbor, MI.
V. Punyakanok, D. Roth, and W. Yih. 2005. The ne-
cessity of syntactic parsing for semantic role labeling.
In Proceedings of the 19th International Joint Confer-
ence on Artificial Intelligence (IJCAI-05).
Karin Kipper Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Kristina Toutanova, Aria Haghighi, and Christopher D.
2005. Joint learning improves semantic role labeling.
In Proceedings of the Association for Computational
Linguistics 43rd annual meeting (ACL-2005), Ann Ar-
bor, MI.
Szu-ting Yi and Martha Palmer. 2004. Pushing the
boundaries of semantic role labeling with svm. In Pro-
ceedings of the International Conference on Natural
Language Processing.
Szu-ting Yi and Martha Palmer. 2005. The integration of
syntactic parsing and semantic role labeling. In Pro-
ceedings of CoNLL-2005.
</reference>
<page confidence="0.998524">
555
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984345">
<title confidence="0.995933">Can Semantic Roles Generalize Across Genres?</title>
<author confidence="0.998821">Szu-ting Yi Edward Loper Martha Palmer</author>
<affiliation confidence="0.9998755">Dept of Computer Science Dept of Computer Science Dept of Computer Science University of Pennsylvania University of Pennsylvania University of Colorado at Boulder</affiliation>
<address confidence="0.999552">Philadelphia, PA 19104 Philadelphia, PA 19104 Boulder, CO 80309</address>
<abstract confidence="0.99958504">PropBank has been widely used as training data for Semantic Role Labeling. However, because this training data is taken from the WSJ, the resulting machine learning models tend to overfit on idiosyncrasies of that text’s style, and do not port well to other genres. In addition, since PropBank was designed on a verb-by-verb basis, the argument labels Arg2 - Arg5 get used for very diverse argument roles with inconsistent training instances. For example, the verb “make” uses Arg2 for the “Material” argument; but the verb “multiply” uses Arg2 for the “Extent” argument. As a result, it can be difficult for automatic classifiers to learn to distinguish arguments Arg2-Arg5. We have created a mapping between PropBank and VerbNet that provides a VerbNet thematic role label for each verb-specific PropBank label. Since VerbNet uses argument labels that are more consistent across verbs, we are able to demonstrate that these new labels are easier to learn.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M´arquez</author>
</authors>
<title>Introduction to the conll-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<marker>Carreras, M´arquez, 2005</marker>
<rawString>Xavier Carreras and Llu´ıs M´arquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>Owen Rambow</author>
</authors>
<title>Use of deep linguistic features for the recognition and labeling of semantic arguments.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-2003,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2480" citStr="Chen and Rambow, 2003" startWordPosition="397" endWordPosition="400">obsters primarily feed on live fish, dig for clams, sea urchins, and feed on algae and eel-grass. c. In the early 20th century, Mainers would only eat lobsters because the fish they caught was too valuable to eat themselves. An important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, ..., Arg5), and by defining those labels to have verb-specific meanings. Und</context>
</contexts>
<marker>Chen, Rambow, 2003</marker>
<rawString>John Chen and Owen Rambow. 2003. Use of deep linguistic features for the recognition and labeling of semantic arguments. In Proceedings of EMNLP-2003, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Dowty</author>
</authors>
<title>Thematic proto-roles and argument selection.</title>
<date>1991</date>
<journal>Language,</journal>
<pages>67--574</pages>
<contexts>
<context position="7933" citStr="Dowty, 1991" startWordPosition="1291" endWordPosition="1292">t in one sentence and the syntactic object in the other. However, there is no guarantee that an argument label will be used consistently across different verbs. For example, the Arg2 label is used to designate the destination of the verb “bring;” but the extent of the verb “rise.” Generally, the arguments are simply listed in the order of their prominence for each verb. However, an explicit effort was made when PropBank was created to use Arg0 for arguments that fulfill Dowty’s criteria for “prototypical 549 agent,” and Arg1 for arguments that fulfill the criteria for “prototypical patient.” (Dowty, 1991) As a result, these two argument labels are significantly more consistent across verbs than the other three. But nevertheless, there are still some inter-verb inconsistencies for even Arg0 and Arg1. 2.2 VerbNet VerbNet (Schuler, 2005) consists of hierarchically arranged verb classes, inspired by and extended from classes of Levin 1993 (Levin, 1993). Each class and subclass is characterized extensionally by its set of verbs, and intensionally by a list of the arguments of those verbs and syntactic and semantic information about the verbs. The argument list consists of thematic roles (23 in tota</context>
</contexts>
<marker>Dowty, 1991</marker>
<rawString>D. R. Dowty. 1991. Thematic proto-roles and argument selection. Language, 67:574–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Identifying semantic roles using Combinatory Categorial Grammar.</title>
<date>2003</date>
<booktitle>In 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>57--64</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2510" citStr="Gildea and Hockenmaier, 2003" startWordPosition="401" endWordPosition="404">on live fish, dig for clams, sea urchins, and feed on algae and eel-grass. c. In the early 20th century, Mainers would only eat lobsters because the fish they caught was too valuable to eat themselves. An important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, ..., Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can a</context>
</contexts>
<marker>Gildea, Hockenmaier, 2003</marker>
<rawString>Daniel Gildea and Julia Hockenmaier. 2003. Identifying semantic roles using Combinatory Categorial Grammar. In 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 57–64, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>The Center for Spoken Language Research at the University of Colorado (CSLR).</institution>
<contexts>
<context position="2533" citStr="Hacioglu et al., 2003" startWordPosition="405" endWordPosition="408">ea urchins, and feed on algae and eel-grass. c. In the early 20th century, Mainers would only eat lobsters because the fish they caught was too valuable to eat themselves. An important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, ..., Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can avoid making any claims </context>
</contexts>
<marker>Hacioglu, Pradhan, Ward, Martin, Jurafsky, 2003</marker>
<rawString>Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2003. Shallow semantic parsing using support vector machines. Technical report, The Center for Spoken Language Research at the University of Colorado (CSLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="8269" citStr="Levin 1993" startWordPosition="1342" endWordPosition="1343">of their prominence for each verb. However, an explicit effort was made when PropBank was created to use Arg0 for arguments that fulfill Dowty’s criteria for “prototypical 549 agent,” and Arg1 for arguments that fulfill the criteria for “prototypical patient.” (Dowty, 1991) As a result, these two argument labels are significantly more consistent across verbs than the other three. But nevertheless, there are still some inter-verb inconsistencies for even Arg0 and Arg1. 2.2 VerbNet VerbNet (Schuler, 2005) consists of hierarchically arranged verb classes, inspired by and extended from classes of Levin 1993 (Levin, 1993). Each class and subclass is characterized extensionally by its set of verbs, and intensionally by a list of the arguments of those verbs and syntactic and semantic information about the verbs. The argument list consists of thematic roles (23 in total) and possible selectional restrictions on the arguments expressed using binary predicates. The syntactic information maps the list of thematic arguments to deepsyntactic arguments (i.e., normalized for voice alternations, and transformations). The semantic predicates describe the participants during various stages of the event descr</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Szu-ting Yi</author>
<author>Martha Palmer</author>
</authors>
<title>Empirical evidence for useful semantic role categories.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Workshop on Computational Linguistics.</booktitle>
<contexts>
<context position="10149" citStr="Loper et al., 2007" startWordPosition="1622" endWordPosition="1625">k to VerbNet Because PropBank includes a large corpus of manually annotated predicate-argument data, it can be used to train supervised machine learning algorithms, which can in turn provide PropBank-style annotations for novel or unseen text. However, as we discussed in the introduction, PropBank’s verbspecific role labels are somewhat problematic. Furthermore, PropBank lacks much of the information that is contained in VerbNet, including information about selectional restrictions, verb semantics, and inter-verb relationships. We have therefore created a mapping between VerbNet and PropBank (Loper et al., 2007), which will allow us to use the machine learning techniques that have been developed for PropBank annotations to generate more semantically abstract VerbNet representations. Additionally, the mapping can be used to translate PropBank-style numbered arguments (Arg0... Arg5) to VerbNet thematic roles (Agent, Patient, Theme, etc.), which should allow us to overcome the verb-specific nature of PropBank. The mapping between VerbNet and PropBank consists of two parts: a lexical mapping and an instance classifier. The lexical mapping is responsible for specifying the potential mappings between PropB</context>
<context position="11482" citStr="Loper et al., 2007" startWordPosition="1841" endWordPosition="1844">urrence of the word. That is the job of the instance classifier, which looks at the word in context, and decides which of the mappings is most appropriate. In essence, the instance classifier is performing word sense disambiguation, deciding which lexeme from each database is correct for a given occurrence of a word. In order to train the instance classifier, we semi-automatically annotated each verb in the PropBank corpus with VerbNet class information.1 This mapped corpus was then used to build the instance classifier. More details about the mapping, and how it was created, can be found in (Loper et al., 2007). 3 Analysis of the Mapping In order to confirm our belief that PropBank roles Arg0 and Arg1 are relatively coherent, while roles Arg2-5 are much more overloaded, we performed a preliminary analysis of how argument roles were mapped. Figure 1 shows how often each PropBank role was mapped to each VerbNet thematic role, calculated as a fraction of instances in the mapped corpus. From this figure, we can see that Arg0 maps to agent-like roles, such as “agent” and “experiencer,” over 94% of the time; and Arg1 maps to patientlike roles, including “theme,” “topic,” and “patient,” over 82% of the tim</context>
</contexts>
<marker>Loper, Yi, Palmer, 2007</marker>
<rawString>Edward Loper, Szu-ting Yi, and Martha Palmer. 2007. Empirical evidence for useful semantic role categories. In Proceedings of the International Workshop on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>G Kim</author>
<author>M Marcinkiewicz</author>
<author>R MacIntyre</author>
<author>A Bies</author>
<author>M Ferguson</author>
<author>K Katz</author>
<author>B Schasberger</author>
</authors>
<title>The Penn treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<contexts>
<context position="6666" citStr="Marcus et al., 1994" startWordPosition="1075" endWordPosition="1078">chniques for generalizing the semantic role labeling task are still needed. In this paper, we demonstrate an increase in the generality of our semantic role labeling based on a mapping that has been developed between PropBank and another lexical resource, VerbNet. By taking advantage of VerbNet’s more consistent set of labels, we can generate more useful role label annotations with a resulting improvement in SRL performance on novel genres. 2 Background 2.1 PropBank PropBank (Palmer et al., 2005) is an annotation of one million words of the Wall Street Journal portion of the Penn Treebank II (Marcus et al., 1994) with predicate-argument structures for verbs, using semantic role labels for each verb argument. In order to remain theory neutral, and to increase annotation speed, role labels were defined on a per-verbsense basis. Although the same tags were used for all verbs, (namely Arg0, Arg1, ..., Arg5), these tags are meant to have a verb-specific meaning. Thus, the use of a given argument label should be consistent across different uses of that verb, including syntactic alternations. For example, the Arg1 (underlined) in “John broke the window” is the same window that is annotated as the Arg1 in “Th</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies, M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn treebank: Annotating predicate argument structure.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernel for shallow semantic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42-th Conference on Association for Computational Linguistic (ACL-2004),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2550" citStr="Moschitti, 2004" startWordPosition="409" endWordPosition="411"> algae and eel-grass. c. In the early 20th century, Mainers would only eat lobsters because the fish they caught was too valuable to eat themselves. An important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, ..., Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can avoid making any claims 548 Proceedings o</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernel for shallow semantic parsing. In Proceedings of the 42-th Conference on Association for Computational Linguistic (ACL-2004), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>106</pages>
<contexts>
<context position="2940" citStr="Palmer et al., 2005" startWordPosition="472" endWordPosition="475">any researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, ..., Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can avoid making any claims 548 Proceedings of NAACL HLT 2007, pages 548–555, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics about how any one verb’s arguments relate to other verbs’ arguments, or about general distinctions between verb arguments and adjuncts. However, there are several limitations to this approach. The first is that it can be difficult to make inferences and generalizations based on ro</context>
<context position="6547" citStr="Palmer et al., 2005" startWordPosition="1052" endWordPosition="1055">ifferent genre (the Brown corpus). DARPA-GALE is funding an ongoing effort to PropBank additional genres, but better techniques for generalizing the semantic role labeling task are still needed. In this paper, we demonstrate an increase in the generality of our semantic role labeling based on a mapping that has been developed between PropBank and another lexical resource, VerbNet. By taking advantage of VerbNet’s more consistent set of labels, we can generate more useful role label annotations with a resulting improvement in SRL performance on novel genres. 2 Background 2.1 PropBank PropBank (Palmer et al., 2005) is an annotation of one million words of the Wall Street Journal portion of the Penn Treebank II (Marcus et al., 1994) with predicate-argument structures for verbs, using semantic role labels for each verb argument. In order to remain theory neutral, and to increase annotation speed, role labels were defined on a per-verbsense basis. Although the same tags were used for all verbs, (namely Arg0, Arg1, ..., Arg5), these tags are meant to have a verb-specific meaning. Thus, the use of a given argument label should be consistent across different uses of that verb, including syntactic alternations</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: A corpus annotated with semantic roles. Computational Linguistics, 31(1):71– 106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Wayne Ward</author>
<author>H Martin</author>
<author>James</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Semantic role chunking combining complementary syntactic views.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005.</booktitle>
<contexts>
<context position="2593" citStr="Pradhan et al., 2005" startWordPosition="416" endWordPosition="419">20th century, Mainers would only eat lobsters because the fish they caught was too valuable to eat themselves. An important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, ..., Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can avoid making any claims 548 Proceedings of NAACL HLT 2007, pages 548–555, Rochester,</context>
<context position="5751" citStr="Pradhan et al., 2005" startWordPosition="920" endWordPosition="923">to know which role labels are used for which argument types, since role labels are defined so specifically. This is especially problematic for Arg2-5. Similarly, PropBank-trained SRL systems can have difficulty generalizing when a known verb is encountered in a novel construction. These problems can happen quite frequently if the training data comes from a different genre than the test data. This issue is reflected in the relatively poor performance of most state-of-the-art SRL systems when tested on a novel genre, the Brown corpus, during CoNLL 2005. For example, the SRL system described in (Pradhan et al., 2005b; Pradhan et al., 2005a) achieves an Fscore of 81% when tested on the same genre as it is trained on (WSJ); but that score drops to 68.5% when the same system is tested on a different genre (the Brown corpus). DARPA-GALE is funding an ongoing effort to PropBank additional genres, but better techniques for generalizing the semantic role labeling task are still needed. In this paper, we demonstrate an increase in the generality of our semantic role labeling based on a mapping that has been developed between PropBank and another lexical resource, VerbNet. By taking advantage of VerbNet’s more co</context>
</contexts>
<marker>Pradhan, Hacioglu, Ward, Martin, James, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Wayne Ward, H. Martin, James, and Daniel Jurafsky. 2005a. Semantic role chunking combining complementary syntactic views. In Proceedings of CoNLL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Semantic role labeling using different syntactic views.</title>
<date>2005</date>
<booktitle>In Proceedings of the Association for Computational Linguistics 43rd annual meeting (ACL-2005),</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2593" citStr="Pradhan et al., 2005" startWordPosition="416" endWordPosition="419">20th century, Mainers would only eat lobsters because the fish they caught was too valuable to eat themselves. An important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, ..., Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can avoid making any claims 548 Proceedings of NAACL HLT 2007, pages 548–555, Rochester,</context>
<context position="5751" citStr="Pradhan et al., 2005" startWordPosition="920" endWordPosition="923">to know which role labels are used for which argument types, since role labels are defined so specifically. This is especially problematic for Arg2-5. Similarly, PropBank-trained SRL systems can have difficulty generalizing when a known verb is encountered in a novel construction. These problems can happen quite frequently if the training data comes from a different genre than the test data. This issue is reflected in the relatively poor performance of most state-of-the-art SRL systems when tested on a novel genre, the Brown corpus, during CoNLL 2005. For example, the SRL system described in (Pradhan et al., 2005b; Pradhan et al., 2005a) achieves an Fscore of 81% when tested on the same genre as it is trained on (WSJ); but that score drops to 68.5% when the same system is tested on a different genre (the Brown corpus). DARPA-GALE is funding an ongoing effort to PropBank additional genres, but better techniques for generalizing the semantic role labeling task are still needed. In this paper, we demonstrate an increase in the generality of our semantic role labeling based on a mapping that has been developed between PropBank and another lexical resource, VerbNet. By taking advantage of VerbNet’s more co</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky. 2005b. Semantic role labeling using different syntactic views. In Proceedings of the Association for Computational Linguistics 43rd annual meeting (ACL-2005), Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The necessity of syntactic parsing for semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI-05).</booktitle>
<contexts>
<context position="2619" citStr="Punyakanok et al., 2005" startWordPosition="420" endWordPosition="423">ould only eat lobsters because the fish they caught was too valuable to eat themselves. An important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, ..., Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can avoid making any claims 548 Proceedings of NAACL HLT 2007, pages 548–555, Rochester, NY, April 2007. c�2007 As</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2005</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2005. The necessity of syntactic parsing for semantic role labeling. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI-05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper Schuler</author>
</authors>
<title>VerbNet: A broadcoverage, comprehensive verb lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="8167" citStr="Schuler, 2005" startWordPosition="1327" endWordPosition="1328"> verb “bring;” but the extent of the verb “rise.” Generally, the arguments are simply listed in the order of their prominence for each verb. However, an explicit effort was made when PropBank was created to use Arg0 for arguments that fulfill Dowty’s criteria for “prototypical 549 agent,” and Arg1 for arguments that fulfill the criteria for “prototypical patient.” (Dowty, 1991) As a result, these two argument labels are significantly more consistent across verbs than the other three. But nevertheless, there are still some inter-verb inconsistencies for even Arg0 and Arg1. 2.2 VerbNet VerbNet (Schuler, 2005) consists of hierarchically arranged verb classes, inspired by and extended from classes of Levin 1993 (Levin, 1993). Each class and subclass is characterized extensionally by its set of verbs, and intensionally by a list of the arguments of those verbs and syntactic and semantic information about the verbs. The argument list consists of thematic roles (23 in total) and possible selectional restrictions on the arguments expressed using binary predicates. The syntactic information maps the list of thematic arguments to deepsyntactic arguments (i.e., normalized for voice alternations, and transf</context>
</contexts>
<marker>Schuler, 2005</marker>
<rawString>Karin Kipper Schuler. 2005. VerbNet: A broadcoverage, comprehensive verb lexicon. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>D Christopher</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Association for Computational Linguistics 43rd annual meeting (ACL-2005),</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2644" citStr="Toutanova et al., 2005" startWordPosition="424" endWordPosition="427">cause the fish they caught was too valuable to eat themselves. An important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, ..., Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can avoid making any claims 548 Proceedings of NAACL HLT 2007, pages 548–555, Rochester, NY, April 2007. c�2007 Association for Computation</context>
</contexts>
<marker>Toutanova, Haghighi, Christopher, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. 2005. Joint learning improves semantic role labeling. In Proceedings of the Association for Computational Linguistics 43rd annual meeting (ACL-2005), Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Szu-ting Yi</author>
<author>Martha Palmer</author>
</authors>
<title>Pushing the boundaries of semantic role labeling with svm.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="2571" citStr="Yi and Palmer, 2004" startWordPosition="412" endWordPosition="415">ass. c. In the early 20th century, Mainers would only eat lobsters because the fish they caught was too valuable to eat themselves. An important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, ..., Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can avoid making any claims 548 Proceedings of NAACL HLT 2007, pag</context>
<context position="17156" citStr="Yi and Palmer, 2004" startWordPosition="2751" endWordPosition="2754">n a constituents location in the parse tree. The Argument Identification component is a binary MaxEnt classifier, which tags candidate constituents as arguments or non-arguments. The Argument Classification component is a multi-class MaxEnt classifier which assigns a semantic role to each constituent. The Post Processing component further selects the final arguments based on global constraints. Our experiments mainly focused on changes to the Argument Classification stage of the SRL pipeline, and in particular, on changes to the set of output tags. For more information on our SRL system, see (Yi and Palmer, 2004; Yi and Palmer, 2005). The evaluation of SRL systems is typically expressed by precision, recall and the F1-measure. Precision is the number of correct arguments predicted by a system divided by the total number of arguments proposed. Recall is the number of correct arguments divided by the number of the total number of arguments in the Gold Standard Data. F1 computes the harmonic mean of precision and recall. 5.2 SRL Experiments on Mapped VerbNet Thematic Roles Since PropBank arguments Arg0 and Arg1 are already quite coherent, we left them as-is in the new label set. But since arguments Arg2</context>
</contexts>
<marker>Yi, Palmer, 2004</marker>
<rawString>Szu-ting Yi and Martha Palmer. 2004. Pushing the boundaries of semantic role labeling with svm. In Proceedings of the International Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Szu-ting Yi</author>
<author>Martha Palmer</author>
</authors>
<title>The integration of syntactic parsing and semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005.</booktitle>
<contexts>
<context position="17178" citStr="Yi and Palmer, 2005" startWordPosition="2755" endWordPosition="2758">tion in the parse tree. The Argument Identification component is a binary MaxEnt classifier, which tags candidate constituents as arguments or non-arguments. The Argument Classification component is a multi-class MaxEnt classifier which assigns a semantic role to each constituent. The Post Processing component further selects the final arguments based on global constraints. Our experiments mainly focused on changes to the Argument Classification stage of the SRL pipeline, and in particular, on changes to the set of output tags. For more information on our SRL system, see (Yi and Palmer, 2004; Yi and Palmer, 2005). The evaluation of SRL systems is typically expressed by precision, recall and the F1-measure. Precision is the number of correct arguments predicted by a system divided by the total number of arguments proposed. Recall is the number of correct arguments divided by the number of the total number of arguments in the Gold Standard Data. F1 computes the harmonic mean of precision and recall. 5.2 SRL Experiments on Mapped VerbNet Thematic Roles Since PropBank arguments Arg0 and Arg1 are already quite coherent, we left them as-is in the new label set. But since arguments Arg2-Arg5 are highly Group</context>
</contexts>
<marker>Yi, Palmer, 2005</marker>
<rawString>Szu-ting Yi and Martha Palmer. 2005. The integration of syntactic parsing and semantic role labeling. In Proceedings of CoNLL-2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>