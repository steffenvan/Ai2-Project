<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.997882">
A Statistical Model for Domain-Independent Text Segmentation
</title>
<author confidence="0.900268">
Masao Utiyama and Hitoshi Isahara
</author>
<affiliation confidence="0.898531">
Communications Research Laboratory
</affiliation>
<address confidence="0.9578875">
2-2-2 Hikaridai Seika-cho, Soraku-gun,
Kyoto, 619-0289 Japan
</address>
<email confidence="0.997609">
mutiyama@crl.go.jp and isahara@crl.go.jp
</email>
<sectionHeader confidence="0.995588" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999609545454546">
We propose a statistical method that
finds the maximum-probability seg-
mentation of a given text. This method
does not require training data because
it estimates probabilities from the given
text. Therefore, it can be applied to
any text in any domain. An experi-
ment showed that the method is more
accurate than or at least as accurate as
a state-of-the-art text segmentation sys-
tem.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996764702380952">
Documents usually include various topics. Identi-
fying and isolating topics by dividing documents,
which is called text segmentation, is important
for many natural language processing tasks, in-
cluding information retrieval (Hearst and Plaunt,
1993; Salton et al., 1996) and summarization
(Kan et al., 1998; Nakao, 2000). In informa-
tion retrieval, users are often interested in par-
ticular topics (parts) of retrieved documents, in-
stead of the documents themselves. To meet such
needs, documents should be segmented into co-
herent topics. Summarization is often used for a
long document that includes multiple topics. A
summary of such a document can be composed
of summaries of the component topics. Identifi-
cation of topics is the task of text segmentation.
A lot of research has been done on text seg-
mentation (Kozima, 1993; Hearst, 1994; Oku-
mura and Honda, 1994; Salton et al., 1996; Yaari,
1997; Kan et al., 1998; Choi, 2000; Nakao, 2000).
A major characteristic of the methods used in this
research is that they do not require training data
to segment given texts. Hearst (1994), for exam-
ple, used only the similarity of word distributions
in a given text to segment the text. Consequently,
these methods can be applied to any text in any
domain, even if training data do not exist. This
property is important when text segmentation is
applied to information retrieval or summarization,
because both tasks deal with domain-independent
documents.
Another application of text segmentation is
the segmentation of a continuous broadcast news
story into individual stories (Allan et al., 1998).
In this application, systems relying on supervised
learning (Yamron et al., 1998; Beeferman et al.,
1999) achieve good performance because there
are plenty of training data in the domain. These
systems, however, can not be applied to domains
for which no training data exist.
The text segmentation algorithm described in
this paper is intended to be applied to the sum-
marization of documents or speeches. Therefore,
it should be able to handle domain-independent
texts. The algorithm thus does not use any train-
ing data. It requires only the given documents for
segmentation. It can, however, incorporate train-
ing data when they are available, as discussed in
Section 5.
The algorithm selects the optimum segmen-
tation in terms of the probability defined by a
statistical model. This is a new approach for
domain-independent text segmentation. Previous
approaches usually used lexical cohesion to seg-
ment texts into topics. Kozima (1993), for exam-
then and hold.
This means that and correspond to each
other.
Under our assumptions, can be de-
composed as follows:
Next, we define as:
where is the number of words in that
are the same as and is the number of different
words in . For example, if , where
and , then ,
, , ,and .
Equation (4) is known as Laplace’s law (Manning
and Sch¨utze, 1999).
can be defined as:
ple, used cohesion based on the spreading activa-
tion on a semantic network. Hearst (1994) used
the similarity of word distributions as measured
by the cosine to gauge cohesion. Reynar (1994)
used word repetition as a measure of cohesion.
Choi (2000) used the rank of the cosine, rather
than the cosine itself, to measure the similarity of
sentences.
The statistical model for the algorithm is de-
scribed in Section 2, and the algorithm for ob-
taining the maximum-probability segmentation is
described in Section 3. Experimental results are
presented in Section 4. Further discussion and our
conclusions are given in Sections 5 and 6, respec-
tively.
</bodyText>
<sectionHeader confidence="0.9739475" genericHeader="method">
2 Statistical Model for Text
Segmentation
</sectionHeader>
<bodyText confidence="0.99246075">
We first define the probability of a segmentation
of a given text in this section. In the next section,
we then describe the algorithm for selecting the
most likely segmentation.
Let be a text consisting of
words, and let be a segmen-
tation of consisting of segments. Then the
probability of the segmentation is defined by:
</bodyText>
<equation confidence="0.676279">
(1)
The most likely segmentation is given by:
for
(5)
(2)
</equation>
<bodyText confidence="0.886798">
because is a constant for a given text .
The definitions of and
given below, in that order.
</bodyText>
<subsectionHeader confidence="0.997638">
2.1 Definition of
</subsectionHeader>
<bodyText confidence="0.9931789">
We define a topic by the distribution of words in
that topic. We assume that different topics have
different word distributions. We further assume
that different topics are statistically independent
of each other. We also assume that the words
within the scope of a topic are statistically inde-
pendent of each other given the topic.
Let be the number of words in segment ,
and let be the -th word in . If we define
as
</bodyText>
<figure confidence="0.442488">
(6)
are
</figure>
<subsectionHeader confidence="0.96083">
2.2 Definition of
</subsectionHeader>
<bodyText confidence="0.9864715625">
The definition of can vary depending on
our prior information about the possibility of seg-
mentation . For example, we might know the
average length of the segments and want to incor-
porate it into .
.
Equations (5) and (6) are used in Section 3 to
describe the algorithm for finding the maximum-
probability segmentation.
where when and are the
same word and otherwise. For
example,
Our assumption, however, is that we do not
have such prior information. Thus, we have to
use some uninformative prior probability.
We define as
</bodyText>
<equation confidence="0.988119">
(7)
Equation (7) is determined on the basis of its de-
(8)
</equation>
<bodyText confidence="0.989108833333333">
where bits.2 This description
length is derived as follows:
Suppose that there are two people, a sender and
a receiver, both of whom know the text to be seg-
mented. Only the sender knows the exact seg-
mentation, and he/she should send a message so
that the receiver can segment the text correctly.
To this end, it is sufficient for the sender to send
integers, i.e., , because these
integers represent the lengths of segments and
thus uniquely determine the segmentation once
the text is known.
A segment length can be encoded using
bits, because is a number between 1 and .
The total description length for all the segment
lengths is thus bits.3
Generally speaking, takes a large value
when the number of segments is small. On the
other hand, takes a large value when the
number of segments is large. If only is
used to segment the text, then the resulting seg-
mentation will have too many segments. By using
both and , we can get a reason-
able number of segments.
</bodyText>
<sectionHeader confidence="0.9991" genericHeader="method">
3 Algorithm for Finding the
</sectionHeader>
<subsectionHeader confidence="0.779862">
Maximum-Probability Segmentation
</subsectionHeader>
<bodyText confidence="0.9999425">
To find the maximum-probability segmentation
, we first define the cost of segmentation as
</bodyText>
<listItem confidence="0.27034725">
(9)
1Stolcke and Omohundro uses description length priors
to induce the structure of hidden Markov models (Stolcke
and Omohundro, 1994).
</listItem>
<bodyText confidence="0.819728888888889">
2‘Log’ denotes the logarithm to the base 2.
3We have used as before. But we use
in this paper, because it is easily interpreted as a
description length and the experimental results obtained by
using are slightly better than those obtained by us-
ing . An anonymous reviewer suggests using a Pois-
son distribution whose parameter is , the average length
of a segment (in words), as prior probability. We leave it
for future work to compare the suitability of various prior
probabilities for text segmentation.
where
(12)
We further rewrite Equation (12) in the form
of Equation (13) below by using Equation (5)
and replacing with , where
is the length of words, i.e.,the number
of word tokens in words. Equation (13) is used to
describe our algorithm in Section 3.1:
</bodyText>
<subsectionHeader confidence="0.997532">
3.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.9923812">
This section describes an algorithm for finding the
minimum-cost segmentation. First, we define the
terms and symbols used to describe the algorithm.
Given a text consisting of
words, we define as the position between
and , so that is just before and is
just after .
Next, we define a graph , where
is a set of nodes and is a set of edges. is
defined as
</bodyText>
<equation confidence="0.512335">
(14)
</equation>
<bodyText confidence="0.9948485">
can be decomposed as follows:
and we then minimize to obtain , because
</bodyText>
<equation confidence="0.993903333333333">
(13)
scription length,1 ; i.e.,
(15)
</equation>
<bodyText confidence="0.994804121212121">
where the edges are ordered; the initial vertex and
the terminal vertex of are and , respec-
tively. An example of is shown in Figure 1.
We say that covers
(16)
where is the number of different words in .
Given these definitions, we describe the algo-
rithm to find the minimum-cost segmentation or
maximum-probability segmentation as follows:
Step 2. Find the minimum-cost path from to
.
Algorithms for finding the minimum-cost path in
a graph are well known. An algorithm that can
provide a solution for Step 2 will be a simpler ver-
sion of the algorithm used to find the maximum-
probability solution in Japanese morphological
analysis (Nagata, 1994). Therefore, a solution can
be obtained by applying a dynamic programming
(DP) algorithm.4 DP algorithms have also been
used for text segmentation by other researchers
(Ponte and Croft, 1997; Heinonen, 1998).
The path thus obtained represents the
minimum-cost segmentation in when edges
correspond with segments. In Figure 1, for
example, if is the minimum-cost path,
then is the minimum-cost
segmentation.
The algorithm automatically determines the
number of segments. But the number of segments
can also be specified explicitly by specifying the
number of edges in the minimum-cost path.
The algorithm allows the text to be segmented
anywhere between words; i.e., all the positions
</bodyText>
<footnote confidence="0.9869865">
4A program that implements the algorithm described in
this section is available at http:
//www.crl.go.jp/jt/a132/members/mutiyama
/softwares.html.
</footnote>
<bodyText confidence="0.999606285714286">
between words are candidates for segment bound-
aries. It is easy, however, to modify the algorithm
so that the text can only be segmented at partic-
ular positions, such as the ends of sentences or
paragraphs. This is done by using a subset of
in Equation (15). We use only the edges whose
initial and terminal vertices are candidate bound-
aries that meet particular conditions, such as be-
ing the ends of sentences or paragraphs. We then
obtain the minimum-cost path by doing Steps 1
and 2. The minimum-cost segmentation thus ob-
tained meets the boundary conditions. In this pa-
per, we assume that the segment boundaries are at
the ends of sentences.
</bodyText>
<subsectionHeader confidence="0.99815">
3.2 Properties of the segmentation
</subsectionHeader>
<bodyText confidence="0.999908692307692">
Generally speaking, the number of segments ob-
tained by our algorithm is not sensitive to the
length of a given text, which is counted in words.
In other words, the number of segments is rela-
tively stable with respect to variation in the text
length. For example, the algorithm divides a
newspaper editorial consisting of about 27 sen-
tences into 4 to 6 segments, while on the other
hand, it divides a long text consisting of over 1000
sentences into 10 to 20 segments. Thus, the num-
ber of segments is not proportional to text length.
This is due to the term in Equation (11).
The value of this term increases as the number of
words increases. The term thus suppresses the di-
vision of a text when the length of the text is long.
This stability is desirable for summarization,
because summarizing a given text requires select-
ing a relatively small number of topics from it.
If a text segmentation system divides a given text
into a relatively small number of segments, then
a summary of the original text can be composed
by combining summaries of the component seg-
ments (Kan et al., 1998; Nakao, 2000). A finer
segmentation can be obtained by applying our
algorithm recursively to each segment, if neces-
sary.5
</bodyText>
<footnote confidence="0.977462">
5We segmented various texts without rigorous evaluation
and found that our method is good at segmenting a text into a
relatively small number of segments. On the other hand, the
method is not good at segmenting a text into a large num-
ber of segments. For example, the method is good at seg-
menting a 1000-sentence text into 10 segments. In such a
case, the segment boundaries seem to correspond well with
topic boundaries. But, if the method is forced to segment
the same text into 50 segments by specifying the number of
</footnote>
<figureCaption confidence="0.401292666666667">
Step 1. Calculate the cost of edge for
by using Equation (16).
and is defined as
</figureCaption>
<figure confidence="0.7647345">
.
This means that represents a segment
. Thus, we define the cost of
edge by using Equation (13):
</figure>
<figureCaption confidence="0.999634">
Figure 1: Example of a graph.
</figureCaption>
<equation confidence="0.99797375">
e14 e35
e01
e13 e45
g0 w1 g1 w2 g2 w3 g3 w4 g4 w5 g5
</equation>
<sectionHeader confidence="0.999155" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.893342">
4.1 Material
</subsectionHeader>
<bodyText confidence="0.992481111111111">
We used publicly available data to evaluate our
system. This data was used by Choi (2000) to
compare various domain-independent text seg-
mentation systems.6 He evaluated (Choi,
2000), TextTiling (Hearst, 1994), DotPlot (Rey-
nar, 1998), and Segmenter (Kan et al., 1998) by
using the data and reported that achieved the
best performance among these systems.
The data description is as follows: “An artifi-
cial test corpus of 700 samples is used to assess
the accuracy and speed performance of segmen-
tation algorithms. A sample is a concatenation of
ten text segments. A segment is the first sen-
tences of a randomly selected document from the
Brown corpus. A sample is characterised by the
range .” (Choi, 2000) Table 1 gives the corpus
statistics.
racy.
</bodyText>
<subsectionHeader confidence="0.999264">
4.2 Experimental procedure and results
</subsectionHeader>
<bodyText confidence="0.999319866666667">
The sample texts were preprocessed – i.e., punc-
tuation and stop words were removed and the re-
maining words were stemmed – by a program us-
ing the libraries available in Choi’s package. The
texts were then segmented by the systems listed
in Tables 2 and 3. The segmentation boundaries
were placed at the ends of sentences. The seg-
mentations were evaluated by applying an evalu-
ation program in Choi’s package.
The results are listed in Tables 2 and 3. is
the result for our system when the numbers of seg-
ments were determined by the system. is
the result for our system when the numbers of seg-
ments were given beforehand.8 and
are the corresponding results for the systems de-
</bodyText>
<figure confidence="0.582931909090909">
scribed in Choi’s paper (Choi, 2000).9
100
100
400
100
# samples
Total
11% 13% 6% 6% 10%
13% 18% 10% 10% 13%
prob 7.9E-5 4.9E-3 2.5E-5 7.5E-8 9.7E-12
Range of
</figure>
<tableCaption confidence="0.757868">
Table 1: Test corpus statistics. (Choi, 2000)
Segmentation accuracy was measured by the
</tableCaption>
<bodyText confidence="0.9976577">
probabilistic error metric proposed by Beefer-
man, et al. (1999).7 Low indicates high accu-
edges in the minimum-cost path, then the resulting segmen-
tation often contains very small segments consisting of only
one or two sentences. We found empirically that segments
obtained by recursive segmentation were better than those
obtained by minimum-cost segmentation when the specified
number of segments was somewhat larger than that of the
minimum-cost path, whose number of segments was auto-
matically determined by the algorithm.
</bodyText>
<footnote confidence="0.985145777777778">
6The data is available from
http://www.cs.man.ac.uk/˜choif/software/
C99-1.2-release.tgz.
We used
naacl00Exp/data/ 1,2,3➴/
3-11,3-5,6-8,9-11➴ /*,
which is contained in the package, for our experiment.
7Let be a correct segmentation and let be a seg-
mentation proposed by a text segmentation system: Then the
</footnote>
<tableCaption confidence="0.7996965">
Table 2: Comparison of : the numbers of seg-
ments were determined by the systems.
</tableCaption>
<bodyText confidence="0.9550982">
In these tables, the symbol “ ” indicates that
the difference in between the two systems is
statistically significant at the 1% level, based on
“number is the probability that a randomly
chosen pair of words a distance of words apart is inconsis-
tently classified; that is, for one of the segmentations the pair
lies in the same segment, while for the other the pair spans
a segment boundary” (Beeferman et al., 1999), where is
chosen to be half the average reference segment length (in
words).
</bodyText>
<footnote confidence="0.996758">
8If two segmentations have the same cost, then our sys-
tems arbitrarily select one of them; i.e., the systems select
the segmentation processed previously.
9The results for in Table 3 are slightly different
from those listed in Table 6 of Choi’s paper (Choi, 2000).
This is because the original results in that paper were based
on 500 samples, while the results in our Table 3 were based
on 700 samples (Choi, personal communication).
</footnote>
<table confidence="0.999104">
Total
10% 9% 7% 5% 9%
12% 11% 10% 9% 11%
prob 2.7E-4 0.080 2.3E-3 1.0E-4 6.8E-9
</table>
<tableCaption confidence="0.999352">
Table 3: Comparision of : the numbers of seg-
</tableCaption>
<bodyText confidence="0.943119764705883">
ments were given beforehand.
a one-sided -test of the null hypothesis of equal
means. The probability of the null hypothesis
being true is displayed in the row indicated by
“prob”. The column labels, such as “ ”, in-
dicate that the numbers in the column are the av-
erages of over the corresponding sample texts.
“Total” indicates the averages of over all the
text samples.
These tables show statistically that our system
is more accurate than or at least as accurate as
. This means that our system is more accurate
than or at least as accurate as previous domain-
independent text segmentation systems, because
has been shown to be more accurate than pre-
vious domain-independent text segmentation sys-
tems.10
</bodyText>
<sectionHeader confidence="0.998011" genericHeader="method">
5 Discussion
</sectionHeader>
<subsectionHeader confidence="0.949658">
5.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.9998247">
Evaluation of the output of text segmentation sys-
tems is difficult because the required segmenta-
tions depend on the application. In this paper, we
have used an artificial corpus to evaluate our sys-
tem. We regard this as appropriate for comparing
relative performance among systems.
It is important, however, to assess the perfor-
mance of systems by using real texts. These
texts should be domain independent. They should
also be multi-lingual if we want to test the mul-
</bodyText>
<footnote confidence="0.77427525">
10Speed performance is not our main concern in this pa-
per. Our implementations of and are not opti-
mum. However, and , which are implemented in
C, run as fast as and , which are implemented in
Java (Choi, 2000), due to the difference in programming lan-
guages. The average run times for a sample text were
sec.
sec.
sec.
sec.
on a Pentium III 750-MHz PC with 384-MB RAM running
RedHat Linux 6.2.
</footnote>
<bodyText confidence="0.9985745">
tilinguality of systems. For English, Klavans, et
al. describe a segmentation corpus in which the
texts were segmented by humans (Klavans et al.,
1998). But, there are no such corpora for other
languages. We are planning to build a segmen-
tation corpus for Japanese, based on a corpus
of speech transcriptions (Maekawa and Koiso,
2000).
</bodyText>
<subsectionHeader confidence="0.615534">
5.2 Related work
</subsectionHeader>
<bodyText confidence="0.999963379310345">
Our proposed algorithm finds the maximum-
probability segmentation of a given text. This
is a new approach for domain-independent text
segmentation. A probabilistic approach, however,
has already been proposed by Yamron, et al. for
domain-dependent text segmentation (broadcast
news story segmentation) (Yamron et al., 1998).
They trained a hidden Markov model (HMM),
whose states correspond to topics. Given a word
sequence, their system assigns each word a topic
so that the maximum-probability topic sequence
is obtained. Their model is basically the same as
that used for HMM part-of-speech (POS) taggers
(Manning and Sch¨utze, 1999), if we regard topics
as POS tags.11 Finding topic boundaries is equiv-
alent to finding topic transitions; i.e., a continuous
topic or segment is a sequence of words with the
same topic.
Their approach is indirect compared with our
approach, which directly finds the maximum-
probability segmentation. As a result, their model
can not straightforwardly incorporate features
pertaining to a segment itself, such as the average
length of segments. Our model, on the other hand,
can incorporate this information quite naturally.
Suppose that the length of a segment follows
a normal distribution , with a mean of
and standard deviation of (Ponte and Croft,
1997). Then Equation (13) can be augmented to
</bodyText>
<footnote confidence="0.76906">
11The details are different, though.
</footnote>
<page confidence="0.854609">
(17)
</page>
<bodyText confidence="0.999960384615385">
where . Equation (17) favors seg-
ments whose lengths are similar to the average
length (in words).
Another major difference from their algorithm
is that our algorithm does not require training data
to estimate probabilities, while their algorithm
does. Therefore, our algorithm can be applied to
domain-independent texts, while their algorithm
is restricted to domains for which training data
are available. It would be interesting, however,
to compare our algorithm with their algorithm for
the case when training data are available. In such
a case, our model should be extended to incor-
porate various features such as the average seg-
ment length, clue words, named entities, and so
on (Reynar, 1999; Beeferman et al., 1999).
Our proposed algorithm naturally estimates the
probabilities of words in segments. These prob-
abilities, which are called word densities, have
been used to detect important descriptions of
words in texts (Kurohashi et al., 1997). This
method is based on the assumption that the den-
sity of a word is high in a segment in which the
word is discussed (defined and/or explained) in
some depth. It would be interesting to apply our
method to this application.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999908875">
We have proposed a statistical model for domain-
independent text segmentation. This method finds
the maximum-probability segmentation of a given
text. The method has been shown to be more
accurate than or at least as accurate as previous
methods. We are planning to build a segmenta-
tion corpus for Japanese and evaluate our method
against this corpus.
</bodyText>
<sectionHeader confidence="0.996525" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9952745">
We thank Freddy Y. Y. Choi for his text segmen-
tation package.
</bodyText>
<sectionHeader confidence="0.998232" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999194928571429">
James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang. 1998. Topic
detection and tracking pilot study final report. In
Proc. of the DARPA Broadcast News Transcription
and Understanding Workshop.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177–210.
Freddy Y. Y. Choi. 2000. Advances in domain in-
dependent linear text segmentation. In Proc. of
NAACL-2000.
Marti A. Hearst and Christian Plaunt. 1993. Subtopic
structuring for full-length document access. In
Proc. of the Sixteenth Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 59–68.
Marti A. Hearst. 1994. Multi-paragraph segmentation
of expository text. In Proc. ofACL’94.
Oskari Heinonen. 1998. Optimal multi-paragraph text
segmentation by dynamic programming. In Proc.
of COLING-ACL’98.
Min-Yen Kan, Judith L. Klavans, and Kathleen R.
McKeown. 1998. Linear segmentation and seg-
ment significance. In Proc. of WVLC-6, pages 197–
205.
Judith L. Klavans, Kathleen R. McKeown, Min-Yen
Kan, and Susan Lee. 1998. Resources for the eval-
uation of summarization techniques. In Proceed-
ings of the 1st International Conference on Lan-
guage Resources and Evaluation (LREC), pages
899–902.
Hideki Kozima. 1993. Text segmentation based on
similarity between words. In Proc. ofACL’93.
Sadao Kurohashi, Nobuyuki Shiraki, and Makoto Na-
gao. 1997. A method for detecting important de-
scriptions of a word based on its density distribution
in text (in Japanese). IPSJ (Information Processing
Society ofJapan) Journal, 38(4):845–854.
Kikuo Maekawa and Hanae Koiso. 2000. Design of
spontaneous speech corpus for Japanese. In Proc of
International Symposium: Toward the Realization
of Spontaneous Speech Engineering, pages 70–77.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press.
Masaaki Nagata. 1994. A stochastic Japanese mor-
phological analyzer using a forward-DP backward-
A n-best search algorithm. In Proc. of COL-
ING’94, pages 201–207.
Yoshio Nakao. 2000. An algorithm for one-page sum-
marization of a long text based on thematic hierar-
chy detection. In Proc. of ACL’2000, pages 302–
309.
Manabu Okumura and Takeo Honda. 1994. Word
sense disambiguation and text segmentation based
on lexical cohesion. In Proc. of COLING-94.
Jay M. Ponte and W. Bruce Croft. 1997. Text seg-
mentation by topic. In Proc. of the First European
Conference on Research and Advanced Technology
for Digital Libraries, pages 120–129.
Jeffrey C. Reynar. 1994. An automatic method of
finding topic boundaries. In Proc. ofACL-94.
Jeffrey C. Reynar. 1998. Topic segmentation: Algo-
rithms and applications. Ph.D. thesis, Computer
and Information Science, University of Pennsylva-
nia.
Jeffrey C. Reynar. 1999. Statistical models for topic
segmentation. In Proc. ofACL-99, pages 357–364.
Gerard Salton, Amit Singhal, Chris Buckley, and Man-
dar Mitra. 1996. Automatic text decomposition
using text segments and text themes. In Proc. of
Hypertext’96.
Andreas Stolcke and Stephen M. Omohundro. 1994.
Best-first model merging for hidden Markov model
induction. Technical Report TR-94-003, ICSI,
Berkeley, CA.
Yaakov Yaari. 1997. Segmentation of expository texts
by hierarchical agglomerative clustering. In Proc.
of the Recent Advances in Natural Language Pro-
cessing.
J. P. Yamron, I. Carp, S. Lowe, and P. van Mul-
bregt. 1998. A hidden Markov model approach
to text segmentation and event tracking. In Proc. of
ICASSP-98.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.501825">
<title confidence="0.998746">A Statistical Model for Domain-Independent Text Segmentation</title>
<author confidence="0.798678">Utiyama Isahara</author>
<affiliation confidence="0.997661">Communications Research Laboratory</affiliation>
<address confidence="0.885102">2-2-2 Hikaridai Seika-cho, Soraku-gun, Kyoto, 619-0289 Japan</address>
<abstract confidence="0.9823065">We propose a statistical method that finds the maximum-probability segmentation of a given text. This method does not require training data because it estimates probabilities from the given text. Therefore, it can be applied to any text in any domain. An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Jaime Carbonell</author>
<author>George Doddington</author>
<author>Jonathan Yamron</author>
<author>Yiming Yang</author>
</authors>
<title>Topic detection and tracking pilot study final report.</title>
<date>1998</date>
<booktitle>In Proc. of the DARPA Broadcast News Transcription and Understanding Workshop.</booktitle>
<contexts>
<context position="2229" citStr="Allan et al., 1998" startWordPosition="342" endWordPosition="345">methods used in this research is that they do not require training data to segment given texts. Hearst (1994), for example, used only the similarity of word distributions in a given text to segment the text. Consequently, these methods can be applied to any text in any domain, even if training data do not exist. This property is important when text segmentation is applied to information retrieval or summarization, because both tasks deal with domain-independent documents. Another application of text segmentation is the segmentation of a continuous broadcast news story into individual stories (Allan et al., 1998). In this application, systems relying on supervised learning (Yamron et al., 1998; Beeferman et al., 1999) achieve good performance because there are plenty of training data in the domain. These systems, however, can not be applied to domains for which no training data exist. The text segmentation algorithm described in this paper is intended to be applied to the summarization of documents or speeches. Therefore, it should be able to handle domain-independent texts. The algorithm thus does not use any training data. It requires only the given documents for segmentation. It can, however, incor</context>
</contexts>
<marker>Allan, Carbonell, Doddington, Yamron, Yang, 1998</marker>
<rawString>James Allan, Jaime Carbonell, George Doddington, Jonathan Yamron, and Yiming Yang. 1998. Topic detection and tracking pilot study final report. In Proc. of the DARPA Broadcast News Transcription and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="2336" citStr="Beeferman et al., 1999" startWordPosition="358" endWordPosition="361">(1994), for example, used only the similarity of word distributions in a given text to segment the text. Consequently, these methods can be applied to any text in any domain, even if training data do not exist. This property is important when text segmentation is applied to information retrieval or summarization, because both tasks deal with domain-independent documents. Another application of text segmentation is the segmentation of a continuous broadcast news story into individual stories (Allan et al., 1998). In this application, systems relying on supervised learning (Yamron et al., 1998; Beeferman et al., 1999) achieve good performance because there are plenty of training data in the domain. These systems, however, can not be applied to domains for which no training data exist. The text segmentation algorithm described in this paper is intended to be applied to the summarization of documents or speeches. Therefore, it should be able to handle domain-independent texts. The algorithm thus does not use any training data. It requires only the given documents for segmentation. It can, however, incorporate training data when they are available, as discussed in Section 5. The algorithm selects the optimum </context>
<context position="14185" citStr="Beeferman, et al. (1999)" startWordPosition="2403" endWordPosition="2407">pplying an evaluation program in Choi’s package. The results are listed in Tables 2 and 3. is the result for our system when the numbers of segments were determined by the system. is the result for our system when the numbers of segments were given beforehand.8 and are the corresponding results for the systems described in Choi’s paper (Choi, 2000).9 100 100 400 100 # samples Total 11% 13% 6% 6% 10% 13% 18% 10% 10% 13% prob 7.9E-5 4.9E-3 2.5E-5 7.5E-8 9.7E-12 Range of Table 1: Test corpus statistics. (Choi, 2000) Segmentation accuracy was measured by the probabilistic error metric proposed by Beeferman, et al. (1999).7 Low indicates high accuedges in the minimum-cost path, then the resulting segmentation often contains very small segments consisting of only one or two sentences. We found empirically that segments obtained by recursive segmentation were better than those obtained by minimum-cost segmentation when the specified number of segments was somewhat larger than that of the minimum-cost path, whose number of segments was automatically determined by the algorithm. 6The data is available from http://www.cs.man.ac.uk/˜choif/software/ C99-1.2-release.tgz. We used naacl00Exp/data/ 1,2,3➴/ 3-11,3-5,6-8,9</context>
<context position="15452" citStr="Beeferman et al., 1999" startWordPosition="2602" endWordPosition="2605">for our experiment. 7Let be a correct segmentation and let be a segmentation proposed by a text segmentation system: Then the Table 2: Comparison of : the numbers of segments were determined by the systems. In these tables, the symbol “ ” indicates that the difference in between the two systems is statistically significant at the 1% level, based on “number is the probability that a randomly chosen pair of words a distance of words apart is inconsistently classified; that is, for one of the segmentations the pair lies in the same segment, while for the other the pair spans a segment boundary” (Beeferman et al., 1999), where is chosen to be half the average reference segment length (in words). 8If two segmentations have the same cost, then our systems arbitrarily select one of them; i.e., the systems select the segmentation processed previously. 9The results for in Table 3 are slightly different from those listed in Table 6 of Choi’s paper (Choi, 2000). This is because the original results in that paper were based on 500 samples, while the results in our Table 3 were based on 700 samples (Choi, personal communication). Total 10% 9% 7% 5% 9% 12% 11% 10% 9% 11% prob 2.7E-4 0.080 2.3E-3 1.0E-4 6.8E-9 Table 3:</context>
<context position="20133" citStr="Beeferman et al., 1999" startWordPosition="3375" endWordPosition="3378">other major difference from their algorithm is that our algorithm does not require training data to estimate probabilities, while their algorithm does. Therefore, our algorithm can be applied to domain-independent texts, while their algorithm is restricted to domains for which training data are available. It would be interesting, however, to compare our algorithm with their algorithm for the case when training data are available. In such a case, our model should be extended to incorporate various features such as the average segment length, clue words, named entities, and so on (Reynar, 1999; Beeferman et al., 1999). Our proposed algorithm naturally estimates the probabilities of words in segments. These probabilities, which are called word densities, have been used to detect important descriptions of words in texts (Kurohashi et al., 1997). This method is based on the assumption that the density of a word is high in a segment in which the word is discussed (defined and/or explained) in some depth. It would be interesting to apply our method to this application. 6 Conclusion We have proposed a statistical model for domainindependent text segmentation. This method finds the maximum-probability segmentatio</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34(1-3):177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
</authors>
<title>Advances in domain independent linear text segmentation.</title>
<date>2000</date>
<booktitle>In Proc. of NAACL-2000.</booktitle>
<contexts>
<context position="1564" citStr="Choi, 2000" startWordPosition="239" endWordPosition="240">00). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, instead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation. A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Choi, 2000; Nakao, 2000). A major characteristic of the methods used in this research is that they do not require training data to segment given texts. Hearst (1994), for example, used only the similarity of word distributions in a given text to segment the text. Consequently, these methods can be applied to any text in any domain, even if training data do not exist. This property is important when text segmentation is applied to information retrieval or summarization, because both tasks deal with domain-independent documents. Another application of text segmentation is the segmentation of a continuous </context>
<context position="3791" citStr="Choi (2000)" startWordPosition="608" endWordPosition="609">n and hold. This means that and correspond to each other. Under our assumptions, can be decomposed as follows: Next, we define as: where is the number of words in that are the same as and is the number of different words in . For example, if , where and , then , , , ,and . Equation (4) is known as Laplace’s law (Manning and Sch¨utze, 1999). can be defined as: ple, used cohesion based on the spreading activation on a semantic network. Hearst (1994) used the similarity of word distributions as measured by the cosine to gauge cohesion. Reynar (1994) used word repetition as a measure of cohesion. Choi (2000) used the rank of the cosine, rather than the cosine itself, to measure the similarity of sentences. The statistical model for the algorithm is described in Section 2, and the algorithm for obtaining the maximum-probability segmentation is described in Section 3. Experimental results are presented in Section 4. Further discussion and our conclusions are given in Sections 5 and 6, respectively. 2 Statistical Model for Text Segmentation We first define the probability of a segmentation of a given text in this section. In the next section, we then describe the algorithm for selecting the most lik</context>
<context position="12504" citStr="Choi (2000)" startWordPosition="2118" endWordPosition="2119">gmenting a 1000-sentence text into 10 segments. In such a case, the segment boundaries seem to correspond well with topic boundaries. But, if the method is forced to segment the same text into 50 segments by specifying the number of Step 1. Calculate the cost of edge for by using Equation (16). and is defined as . This means that represents a segment . Thus, we define the cost of edge by using Equation (13): Figure 1: Example of a graph. e14 e35 e01 e13 e45 g0 w1 g1 w2 g2 w3 g3 w4 g4 w5 g5 4 Experiments 4.1 Material We used publicly available data to evaluate our system. This data was used by Choi (2000) to compare various domain-independent text segmentation systems.6 He evaluated (Choi, 2000), TextTiling (Hearst, 1994), DotPlot (Reynar, 1998), and Segmenter (Kan et al., 1998) by using the data and reported that achieved the best performance among these systems. The data description is as follows: “An artificial test corpus of 700 samples is used to assess the accuracy and speed performance of segmentation algorithms. A sample is a concatenation of ten text segments. A segment is the first sentences of a randomly selected document from the Brown corpus. A sample is characterised by the range</context>
<context position="13911" citStr="Choi, 2000" startWordPosition="2358" endWordPosition="2359">ing words were stemmed – by a program using the libraries available in Choi’s package. The texts were then segmented by the systems listed in Tables 2 and 3. The segmentation boundaries were placed at the ends of sentences. The segmentations were evaluated by applying an evaluation program in Choi’s package. The results are listed in Tables 2 and 3. is the result for our system when the numbers of segments were determined by the system. is the result for our system when the numbers of segments were given beforehand.8 and are the corresponding results for the systems described in Choi’s paper (Choi, 2000).9 100 100 400 100 # samples Total 11% 13% 6% 6% 10% 13% 18% 10% 10% 13% prob 7.9E-5 4.9E-3 2.5E-5 7.5E-8 9.7E-12 Range of Table 1: Test corpus statistics. (Choi, 2000) Segmentation accuracy was measured by the probabilistic error metric proposed by Beeferman, et al. (1999).7 Low indicates high accuedges in the minimum-cost path, then the resulting segmentation often contains very small segments consisting of only one or two sentences. We found empirically that segments obtained by recursive segmentation were better than those obtained by minimum-cost segmentation when the specified number of </context>
<context position="15793" citStr="Choi, 2000" startWordPosition="2661" endWordPosition="2662">umber is the probability that a randomly chosen pair of words a distance of words apart is inconsistently classified; that is, for one of the segmentations the pair lies in the same segment, while for the other the pair spans a segment boundary” (Beeferman et al., 1999), where is chosen to be half the average reference segment length (in words). 8If two segmentations have the same cost, then our systems arbitrarily select one of them; i.e., the systems select the segmentation processed previously. 9The results for in Table 3 are slightly different from those listed in Table 6 of Choi’s paper (Choi, 2000). This is because the original results in that paper were based on 500 samples, while the results in our Table 3 were based on 700 samples (Choi, personal communication). Total 10% 9% 7% 5% 9% 12% 11% 10% 9% 11% prob 2.7E-4 0.080 2.3E-3 1.0E-4 6.8E-9 Table 3: Comparision of : the numbers of segments were given beforehand. a one-sided -test of the null hypothesis of equal means. The probability of the null hypothesis being true is displayed in the row indicated by “prob”. The column labels, such as “ ”, indicate that the numbers in the column are the averages of over the corresponding sample te</context>
<context position="17498" citStr="Choi, 2000" startWordPosition="2958" endWordPosition="2959"> because the required segmentations depend on the application. In this paper, we have used an artificial corpus to evaluate our system. We regard this as appropriate for comparing relative performance among systems. It is important, however, to assess the performance of systems by using real texts. These texts should be domain independent. They should also be multi-lingual if we want to test the mul10Speed performance is not our main concern in this paper. Our implementations of and are not optimum. However, and , which are implemented in C, run as fast as and , which are implemented in Java (Choi, 2000), due to the difference in programming languages. The average run times for a sample text were sec. sec. sec. sec. on a Pentium III 750-MHz PC with 384-MB RAM running RedHat Linux 6.2. tilinguality of systems. For English, Klavans, et al. describe a segmentation corpus in which the texts were segmented by humans (Klavans et al., 1998). But, there are no such corpora for other languages. We are planning to build a segmentation corpus for Japanese, based on a corpus of speech transcriptions (Maekawa and Koiso, 2000). 5.2 Related work Our proposed algorithm finds the maximumprobability segmentati</context>
</contexts>
<marker>Choi, 2000</marker>
<rawString>Freddy Y. Y. Choi. 2000. Advances in domain independent linear text segmentation. In Proc. of NAACL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
<author>Christian Plaunt</author>
</authors>
<title>Subtopic structuring for full-length document access.</title>
<date>1993</date>
<booktitle>In Proc. of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>59--68</pages>
<contexts>
<context position="885" citStr="Hearst and Plaunt, 1993" startWordPosition="122" endWordPosition="125">cal method that finds the maximum-probability segmentation of a given text. This method does not require training data because it estimates probabilities from the given text. Therefore, it can be applied to any text in any domain. An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system. 1 Introduction Documents usually include various topics. Identifying and isolating topics by dividing documents, which is called text segmentation, is important for many natural language processing tasks, including information retrieval (Hearst and Plaunt, 1993; Salton et al., 1996) and summarization (Kan et al., 1998; Nakao, 2000). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, instead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation. A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura </context>
</contexts>
<marker>Hearst, Plaunt, 1993</marker>
<rawString>Marti A. Hearst and Christian Plaunt. 1993. Subtopic structuring for full-length document access. In Proc. of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 59–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proc. ofACL’94.</booktitle>
<contexts>
<context position="1475" citStr="Hearst, 1994" startWordPosition="222" endWordPosition="223">earst and Plaunt, 1993; Salton et al., 1996) and summarization (Kan et al., 1998; Nakao, 2000). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, instead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation. A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Choi, 2000; Nakao, 2000). A major characteristic of the methods used in this research is that they do not require training data to segment given texts. Hearst (1994), for example, used only the similarity of word distributions in a given text to segment the text. Consequently, these methods can be applied to any text in any domain, even if training data do not exist. This property is important when text segmentation is applied to information retrieval or summarization, because both tasks deal with domain-independent</context>
<context position="3631" citStr="Hearst (1994)" startWordPosition="582" endWordPosition="583"> new approach for domain-independent text segmentation. Previous approaches usually used lexical cohesion to segment texts into topics. Kozima (1993), for examthen and hold. This means that and correspond to each other. Under our assumptions, can be decomposed as follows: Next, we define as: where is the number of words in that are the same as and is the number of different words in . For example, if , where and , then , , , ,and . Equation (4) is known as Laplace’s law (Manning and Sch¨utze, 1999). can be defined as: ple, used cohesion based on the spreading activation on a semantic network. Hearst (1994) used the similarity of word distributions as measured by the cosine to gauge cohesion. Reynar (1994) used word repetition as a measure of cohesion. Choi (2000) used the rank of the cosine, rather than the cosine itself, to measure the similarity of sentences. The statistical model for the algorithm is described in Section 2, and the algorithm for obtaining the maximum-probability segmentation is described in Section 3. Experimental results are presented in Section 4. Further discussion and our conclusions are given in Sections 5 and 6, respectively. 2 Statistical Model for Text Segmentation W</context>
<context position="12623" citStr="Hearst, 1994" startWordPosition="2133" endWordPosition="2134">opic boundaries. But, if the method is forced to segment the same text into 50 segments by specifying the number of Step 1. Calculate the cost of edge for by using Equation (16). and is defined as . This means that represents a segment . Thus, we define the cost of edge by using Equation (13): Figure 1: Example of a graph. e14 e35 e01 e13 e45 g0 w1 g1 w2 g2 w3 g3 w4 g4 w5 g5 4 Experiments 4.1 Material We used publicly available data to evaluate our system. This data was used by Choi (2000) to compare various domain-independent text segmentation systems.6 He evaluated (Choi, 2000), TextTiling (Hearst, 1994), DotPlot (Reynar, 1998), and Segmenter (Kan et al., 1998) by using the data and reported that achieved the best performance among these systems. The data description is as follows: “An artificial test corpus of 700 samples is used to assess the accuracy and speed performance of segmentation algorithms. A sample is a concatenation of ten text segments. A segment is the first sentences of a randomly selected document from the Brown corpus. A sample is characterised by the range .” (Choi, 2000) Table 1 gives the corpus statistics. racy. 4.2 Experimental procedure and results The sample texts wer</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Marti A. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proc. ofACL’94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oskari Heinonen</author>
</authors>
<title>Optimal multi-paragraph text segmentation by dynamic programming.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL’98.</booktitle>
<contexts>
<context position="9094" citStr="Heinonen, 1998" startWordPosition="1528" endWordPosition="1529">e the algorithm to find the minimum-cost segmentation or maximum-probability segmentation as follows: Step 2. Find the minimum-cost path from to . Algorithms for finding the minimum-cost path in a graph are well known. An algorithm that can provide a solution for Step 2 will be a simpler version of the algorithm used to find the maximumprobability solution in Japanese morphological analysis (Nagata, 1994). Therefore, a solution can be obtained by applying a dynamic programming (DP) algorithm.4 DP algorithms have also been used for text segmentation by other researchers (Ponte and Croft, 1997; Heinonen, 1998). The path thus obtained represents the minimum-cost segmentation in when edges correspond with segments. In Figure 1, for example, if is the minimum-cost path, then is the minimum-cost segmentation. The algorithm automatically determines the number of segments. But the number of segments can also be specified explicitly by specifying the number of edges in the minimum-cost path. The algorithm allows the text to be segmented anywhere between words; i.e., all the positions 4A program that implements the algorithm described in this section is available at http: //www.crl.go.jp/jt/a132/members/mu</context>
</contexts>
<marker>Heinonen, 1998</marker>
<rawString>Oskari Heinonen. 1998. Optimal multi-paragraph text segmentation by dynamic programming. In Proc. of COLING-ACL’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Yen Kan</author>
<author>Judith L Klavans</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Linear segmentation and segment significance.</title>
<date>1998</date>
<booktitle>In Proc. of WVLC-6,</booktitle>
<pages>197--205</pages>
<contexts>
<context position="943" citStr="Kan et al., 1998" startWordPosition="132" endWordPosition="135">iven text. This method does not require training data because it estimates probabilities from the given text. Therefore, it can be applied to any text in any domain. An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system. 1 Introduction Documents usually include various topics. Identifying and isolating topics by dividing documents, which is called text segmentation, is important for many natural language processing tasks, including information retrieval (Hearst and Plaunt, 1993; Salton et al., 1996) and summarization (Kan et al., 1998; Nakao, 2000). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, instead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation. A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et </context>
<context position="11482" citStr="Kan et al., 1998" startWordPosition="1927" endWordPosition="1930">Thus, the number of segments is not proportional to text length. This is due to the term in Equation (11). The value of this term increases as the number of words increases. The term thus suppresses the division of a text when the length of the text is long. This stability is desirable for summarization, because summarizing a given text requires selecting a relatively small number of topics from it. If a text segmentation system divides a given text into a relatively small number of segments, then a summary of the original text can be composed by combining summaries of the component segments (Kan et al., 1998; Nakao, 2000). A finer segmentation can be obtained by applying our algorithm recursively to each segment, if necessary.5 5We segmented various texts without rigorous evaluation and found that our method is good at segmenting a text into a relatively small number of segments. On the other hand, the method is not good at segmenting a text into a large number of segments. For example, the method is good at segmenting a 1000-sentence text into 10 segments. In such a case, the segment boundaries seem to correspond well with topic boundaries. But, if the method is forced to segment the same text i</context>
</contexts>
<marker>Kan, Klavans, McKeown, 1998</marker>
<rawString>Min-Yen Kan, Judith L. Klavans, and Kathleen R. McKeown. 1998. Linear segmentation and segment significance. In Proc. of WVLC-6, pages 197– 205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith L Klavans</author>
<author>Kathleen R McKeown</author>
<author>Min-Yen Kan</author>
<author>Susan Lee</author>
</authors>
<title>Resources for the evaluation of summarization techniques.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>899--902</pages>
<contexts>
<context position="17834" citStr="Klavans et al., 1998" startWordPosition="3014" endWordPosition="3017">in independent. They should also be multi-lingual if we want to test the mul10Speed performance is not our main concern in this paper. Our implementations of and are not optimum. However, and , which are implemented in C, run as fast as and , which are implemented in Java (Choi, 2000), due to the difference in programming languages. The average run times for a sample text were sec. sec. sec. sec. on a Pentium III 750-MHz PC with 384-MB RAM running RedHat Linux 6.2. tilinguality of systems. For English, Klavans, et al. describe a segmentation corpus in which the texts were segmented by humans (Klavans et al., 1998). But, there are no such corpora for other languages. We are planning to build a segmentation corpus for Japanese, based on a corpus of speech transcriptions (Maekawa and Koiso, 2000). 5.2 Related work Our proposed algorithm finds the maximumprobability segmentation of a given text. This is a new approach for domain-independent text segmentation. A probabilistic approach, however, has already been proposed by Yamron, et al. for domain-dependent text segmentation (broadcast news story segmentation) (Yamron et al., 1998). They trained a hidden Markov model (HMM), whose states correspond to topic</context>
</contexts>
<marker>Klavans, McKeown, Kan, Lee, 1998</marker>
<rawString>Judith L. Klavans, Kathleen R. McKeown, Min-Yen Kan, and Susan Lee. 1998. Resources for the evaluation of summarization techniques. In Proceedings of the 1st International Conference on Language Resources and Evaluation (LREC), pages 899–902.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Kozima</author>
</authors>
<title>Text segmentation based on similarity between words.</title>
<date>1993</date>
<booktitle>In Proc. ofACL’93.</booktitle>
<contexts>
<context position="1461" citStr="Kozima, 1993" startWordPosition="220" endWordPosition="221">n retrieval (Hearst and Plaunt, 1993; Salton et al., 1996) and summarization (Kan et al., 1998; Nakao, 2000). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, instead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation. A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Choi, 2000; Nakao, 2000). A major characteristic of the methods used in this research is that they do not require training data to segment given texts. Hearst (1994), for example, used only the similarity of word distributions in a given text to segment the text. Consequently, these methods can be applied to any text in any domain, even if training data do not exist. This property is important when text segmentation is applied to information retrieval or summarization, because both tasks deal with doma</context>
<context position="3167" citStr="Kozima (1993)" startWordPosition="492" endWordPosition="493">is paper is intended to be applied to the summarization of documents or speeches. Therefore, it should be able to handle domain-independent texts. The algorithm thus does not use any training data. It requires only the given documents for segmentation. It can, however, incorporate training data when they are available, as discussed in Section 5. The algorithm selects the optimum segmentation in terms of the probability defined by a statistical model. This is a new approach for domain-independent text segmentation. Previous approaches usually used lexical cohesion to segment texts into topics. Kozima (1993), for examthen and hold. This means that and correspond to each other. Under our assumptions, can be decomposed as follows: Next, we define as: where is the number of words in that are the same as and is the number of different words in . For example, if , where and , then , , , ,and . Equation (4) is known as Laplace’s law (Manning and Sch¨utze, 1999). can be defined as: ple, used cohesion based on the spreading activation on a semantic network. Hearst (1994) used the similarity of word distributions as measured by the cosine to gauge cohesion. Reynar (1994) used word repetition as a measure </context>
</contexts>
<marker>Kozima, 1993</marker>
<rawString>Hideki Kozima. 1993. Text segmentation based on similarity between words. In Proc. ofACL’93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Nobuyuki Shiraki</author>
<author>Makoto Nagao</author>
</authors>
<title>A method for detecting important descriptions of a word based on its density distribution in text (in Japanese).</title>
<date>1997</date>
<journal>IPSJ (Information Processing Society ofJapan) Journal,</journal>
<volume>38</volume>
<issue>4</issue>
<contexts>
<context position="20362" citStr="Kurohashi et al., 1997" startWordPosition="3409" endWordPosition="3412">eir algorithm is restricted to domains for which training data are available. It would be interesting, however, to compare our algorithm with their algorithm for the case when training data are available. In such a case, our model should be extended to incorporate various features such as the average segment length, clue words, named entities, and so on (Reynar, 1999; Beeferman et al., 1999). Our proposed algorithm naturally estimates the probabilities of words in segments. These probabilities, which are called word densities, have been used to detect important descriptions of words in texts (Kurohashi et al., 1997). This method is based on the assumption that the density of a word is high in a segment in which the word is discussed (defined and/or explained) in some depth. It would be interesting to apply our method to this application. 6 Conclusion We have proposed a statistical model for domainindependent text segmentation. This method finds the maximum-probability segmentation of a given text. The method has been shown to be more accurate than or at least as accurate as previous methods. We are planning to build a segmentation corpus for Japanese and evaluate our method against this corpus. Acknowled</context>
</contexts>
<marker>Kurohashi, Shiraki, Nagao, 1997</marker>
<rawString>Sadao Kurohashi, Nobuyuki Shiraki, and Makoto Nagao. 1997. A method for detecting important descriptions of a word based on its density distribution in text (in Japanese). IPSJ (Information Processing Society ofJapan) Journal, 38(4):845–854.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kikuo Maekawa</author>
<author>Hanae Koiso</author>
</authors>
<title>Design of spontaneous speech corpus for Japanese. In</title>
<date>2000</date>
<booktitle>Proc of International Symposium: Toward the Realization of Spontaneous Speech Engineering,</booktitle>
<pages>70--77</pages>
<contexts>
<context position="18017" citStr="Maekawa and Koiso, 2000" startWordPosition="3045" endWordPosition="3048">owever, and , which are implemented in C, run as fast as and , which are implemented in Java (Choi, 2000), due to the difference in programming languages. The average run times for a sample text were sec. sec. sec. sec. on a Pentium III 750-MHz PC with 384-MB RAM running RedHat Linux 6.2. tilinguality of systems. For English, Klavans, et al. describe a segmentation corpus in which the texts were segmented by humans (Klavans et al., 1998). But, there are no such corpora for other languages. We are planning to build a segmentation corpus for Japanese, based on a corpus of speech transcriptions (Maekawa and Koiso, 2000). 5.2 Related work Our proposed algorithm finds the maximumprobability segmentation of a given text. This is a new approach for domain-independent text segmentation. A probabilistic approach, however, has already been proposed by Yamron, et al. for domain-dependent text segmentation (broadcast news story segmentation) (Yamron et al., 1998). They trained a hidden Markov model (HMM), whose states correspond to topics. Given a word sequence, their system assigns each word a topic so that the maximum-probability topic sequence is obtained. Their model is basically the same as that used for HMM par</context>
</contexts>
<marker>Maekawa, Koiso, 2000</marker>
<rawString>Kikuo Maekawa and Hanae Koiso. 2000. Design of spontaneous speech corpus for Japanese. In Proc of International Symposium: Toward the Realization of Spontaneous Speech Engineering, pages 70–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A stochastic Japanese morphological analyzer using a forward-DP backwardA n-best search algorithm.</title>
<date>1994</date>
<booktitle>In Proc. of COLING’94,</booktitle>
<pages>201--207</pages>
<contexts>
<context position="8887" citStr="Nagata, 1994" startWordPosition="1497" endWordPosition="1498">initial vertex and the terminal vertex of are and , respectively. An example of is shown in Figure 1. We say that covers (16) where is the number of different words in . Given these definitions, we describe the algorithm to find the minimum-cost segmentation or maximum-probability segmentation as follows: Step 2. Find the minimum-cost path from to . Algorithms for finding the minimum-cost path in a graph are well known. An algorithm that can provide a solution for Step 2 will be a simpler version of the algorithm used to find the maximumprobability solution in Japanese morphological analysis (Nagata, 1994). Therefore, a solution can be obtained by applying a dynamic programming (DP) algorithm.4 DP algorithms have also been used for text segmentation by other researchers (Ponte and Croft, 1997; Heinonen, 1998). The path thus obtained represents the minimum-cost segmentation in when edges correspond with segments. In Figure 1, for example, if is the minimum-cost path, then is the minimum-cost segmentation. The algorithm automatically determines the number of segments. But the number of segments can also be specified explicitly by specifying the number of edges in the minimum-cost path. The algori</context>
</contexts>
<marker>Nagata, 1994</marker>
<rawString>Masaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-DP backwardA n-best search algorithm. In Proc. of COLING’94, pages 201–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshio Nakao</author>
</authors>
<title>An algorithm for one-page summarization of a long text based on thematic hierarchy detection.</title>
<date>2000</date>
<booktitle>In Proc. of ACL’2000,</booktitle>
<pages>302--309</pages>
<contexts>
<context position="957" citStr="Nakao, 2000" startWordPosition="136" endWordPosition="137">thod does not require training data because it estimates probabilities from the given text. Therefore, it can be applied to any text in any domain. An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system. 1 Introduction Documents usually include various topics. Identifying and isolating topics by dividing documents, which is called text segmentation, is important for many natural language processing tasks, including information retrieval (Hearst and Plaunt, 1993; Salton et al., 1996) and summarization (Kan et al., 1998; Nakao, 2000). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, instead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation. A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Cho</context>
<context position="11496" citStr="Nakao, 2000" startWordPosition="1931" endWordPosition="1932">f segments is not proportional to text length. This is due to the term in Equation (11). The value of this term increases as the number of words increases. The term thus suppresses the division of a text when the length of the text is long. This stability is desirable for summarization, because summarizing a given text requires selecting a relatively small number of topics from it. If a text segmentation system divides a given text into a relatively small number of segments, then a summary of the original text can be composed by combining summaries of the component segments (Kan et al., 1998; Nakao, 2000). A finer segmentation can be obtained by applying our algorithm recursively to each segment, if necessary.5 5We segmented various texts without rigorous evaluation and found that our method is good at segmenting a text into a relatively small number of segments. On the other hand, the method is not good at segmenting a text into a large number of segments. For example, the method is good at segmenting a 1000-sentence text into 10 segments. In such a case, the segment boundaries seem to correspond well with topic boundaries. But, if the method is forced to segment the same text into 50 segment</context>
</contexts>
<marker>Nakao, 2000</marker>
<rawString>Yoshio Nakao. 2000. An algorithm for one-page summarization of a long text based on thematic hierarchy detection. In Proc. of ACL’2000, pages 302– 309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Okumura</author>
<author>Takeo Honda</author>
</authors>
<title>Word sense disambiguation and text segmentation based on lexical cohesion.</title>
<date>1994</date>
<booktitle>In Proc. of COLING-94.</booktitle>
<contexts>
<context position="1500" citStr="Okumura and Honda, 1994" startWordPosition="224" endWordPosition="228">nt, 1993; Salton et al., 1996) and summarization (Kan et al., 1998; Nakao, 2000). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, instead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation. A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Choi, 2000; Nakao, 2000). A major characteristic of the methods used in this research is that they do not require training data to segment given texts. Hearst (1994), for example, used only the similarity of word distributions in a given text to segment the text. Consequently, these methods can be applied to any text in any domain, even if training data do not exist. This property is important when text segmentation is applied to information retrieval or summarization, because both tasks deal with domain-independent documents. Another appli</context>
</contexts>
<marker>Okumura, Honda, 1994</marker>
<rawString>Manabu Okumura and Takeo Honda. 1994. Word sense disambiguation and text segmentation based on lexical cohesion. In Proc. of COLING-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Ponte</author>
<author>W Bruce Croft</author>
</authors>
<title>Text segmentation by topic.</title>
<date>1997</date>
<booktitle>In Proc. of the First European Conference on Research and Advanced Technology for Digital Libraries,</booktitle>
<pages>120--129</pages>
<contexts>
<context position="9077" citStr="Ponte and Croft, 1997" startWordPosition="1524" endWordPosition="1527">definitions, we describe the algorithm to find the minimum-cost segmentation or maximum-probability segmentation as follows: Step 2. Find the minimum-cost path from to . Algorithms for finding the minimum-cost path in a graph are well known. An algorithm that can provide a solution for Step 2 will be a simpler version of the algorithm used to find the maximumprobability solution in Japanese morphological analysis (Nagata, 1994). Therefore, a solution can be obtained by applying a dynamic programming (DP) algorithm.4 DP algorithms have also been used for text segmentation by other researchers (Ponte and Croft, 1997; Heinonen, 1998). The path thus obtained represents the minimum-cost segmentation in when edges correspond with segments. In Figure 1, for example, if is the minimum-cost path, then is the minimum-cost segmentation. The algorithm automatically determines the number of segments. But the number of segments can also be specified explicitly by specifying the number of edges in the minimum-cost path. The algorithm allows the text to be segmented anywhere between words; i.e., all the positions 4A program that implements the algorithm described in this section is available at http: //www.crl.go.jp/j</context>
<context position="19327" citStr="Ponte and Croft, 1997" startWordPosition="3247" endWordPosition="3250">11 Finding topic boundaries is equivalent to finding topic transitions; i.e., a continuous topic or segment is a sequence of words with the same topic. Their approach is indirect compared with our approach, which directly finds the maximumprobability segmentation. As a result, their model can not straightforwardly incorporate features pertaining to a segment itself, such as the average length of segments. Our model, on the other hand, can incorporate this information quite naturally. Suppose that the length of a segment follows a normal distribution , with a mean of and standard deviation of (Ponte and Croft, 1997). Then Equation (13) can be augmented to 11The details are different, though. (17) where . Equation (17) favors segments whose lengths are similar to the average length (in words). Another major difference from their algorithm is that our algorithm does not require training data to estimate probabilities, while their algorithm does. Therefore, our algorithm can be applied to domain-independent texts, while their algorithm is restricted to domains for which training data are available. It would be interesting, however, to compare our algorithm with their algorithm for the case when training dat</context>
</contexts>
<marker>Ponte, Croft, 1997</marker>
<rawString>Jay M. Ponte and W. Bruce Croft. 1997. Text segmentation by topic. In Proc. of the First European Conference on Research and Advanced Technology for Digital Libraries, pages 120–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>An automatic method of finding topic boundaries.</title>
<date>1994</date>
<booktitle>In Proc. ofACL-94.</booktitle>
<contexts>
<context position="3732" citStr="Reynar (1994)" startWordPosition="598" endWordPosition="599">sion to segment texts into topics. Kozima (1993), for examthen and hold. This means that and correspond to each other. Under our assumptions, can be decomposed as follows: Next, we define as: where is the number of words in that are the same as and is the number of different words in . For example, if , where and , then , , , ,and . Equation (4) is known as Laplace’s law (Manning and Sch¨utze, 1999). can be defined as: ple, used cohesion based on the spreading activation on a semantic network. Hearst (1994) used the similarity of word distributions as measured by the cosine to gauge cohesion. Reynar (1994) used word repetition as a measure of cohesion. Choi (2000) used the rank of the cosine, rather than the cosine itself, to measure the similarity of sentences. The statistical model for the algorithm is described in Section 2, and the algorithm for obtaining the maximum-probability segmentation is described in Section 3. Experimental results are presented in Section 4. Further discussion and our conclusions are given in Sections 5 and 6, respectively. 2 Statistical Model for Text Segmentation We first define the probability of a segmentation of a given text in this section. In the next section</context>
</contexts>
<marker>Reynar, 1994</marker>
<rawString>Jeffrey C. Reynar. 1994. An automatic method of finding topic boundaries. In Proc. ofACL-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>Topic segmentation: Algorithms and applications.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="12647" citStr="Reynar, 1998" startWordPosition="2136" endWordPosition="2138"> the method is forced to segment the same text into 50 segments by specifying the number of Step 1. Calculate the cost of edge for by using Equation (16). and is defined as . This means that represents a segment . Thus, we define the cost of edge by using Equation (13): Figure 1: Example of a graph. e14 e35 e01 e13 e45 g0 w1 g1 w2 g2 w3 g3 w4 g4 w5 g5 4 Experiments 4.1 Material We used publicly available data to evaluate our system. This data was used by Choi (2000) to compare various domain-independent text segmentation systems.6 He evaluated (Choi, 2000), TextTiling (Hearst, 1994), DotPlot (Reynar, 1998), and Segmenter (Kan et al., 1998) by using the data and reported that achieved the best performance among these systems. The data description is as follows: “An artificial test corpus of 700 samples is used to assess the accuracy and speed performance of segmentation algorithms. A sample is a concatenation of ten text segments. A segment is the first sentences of a randomly selected document from the Brown corpus. A sample is characterised by the range .” (Choi, 2000) Table 1 gives the corpus statistics. racy. 4.2 Experimental procedure and results The sample texts were preprocessed – i.e., p</context>
</contexts>
<marker>Reynar, 1998</marker>
<rawString>Jeffrey C. Reynar. 1998. Topic segmentation: Algorithms and applications. Ph.D. thesis, Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>Statistical models for topic segmentation.</title>
<date>1999</date>
<booktitle>In Proc. ofACL-99,</booktitle>
<pages>357--364</pages>
<contexts>
<context position="20108" citStr="Reynar, 1999" startWordPosition="3373" endWordPosition="3374">(in words). Another major difference from their algorithm is that our algorithm does not require training data to estimate probabilities, while their algorithm does. Therefore, our algorithm can be applied to domain-independent texts, while their algorithm is restricted to domains for which training data are available. It would be interesting, however, to compare our algorithm with their algorithm for the case when training data are available. In such a case, our model should be extended to incorporate various features such as the average segment length, clue words, named entities, and so on (Reynar, 1999; Beeferman et al., 1999). Our proposed algorithm naturally estimates the probabilities of words in segments. These probabilities, which are called word densities, have been used to detect important descriptions of words in texts (Kurohashi et al., 1997). This method is based on the assumption that the density of a word is high in a segment in which the word is discussed (defined and/or explained) in some depth. It would be interesting to apply our method to this application. 6 Conclusion We have proposed a statistical model for domainindependent text segmentation. This method finds the maximu</context>
</contexts>
<marker>Reynar, 1999</marker>
<rawString>Jeffrey C. Reynar. 1999. Statistical models for topic segmentation. In Proc. ofACL-99, pages 357–364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Amit Singhal</author>
<author>Chris Buckley</author>
<author>Mandar Mitra</author>
</authors>
<title>Automatic text decomposition using text segments and text themes.</title>
<date>1996</date>
<booktitle>In Proc. of Hypertext’96.</booktitle>
<contexts>
<context position="907" citStr="Salton et al., 1996" startWordPosition="126" endWordPosition="129"> maximum-probability segmentation of a given text. This method does not require training data because it estimates probabilities from the given text. Therefore, it can be applied to any text in any domain. An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system. 1 Introduction Documents usually include various topics. Identifying and isolating topics by dividing documents, which is called text segmentation, is important for many natural language processing tasks, including information retrieval (Hearst and Plaunt, 1993; Salton et al., 1996) and summarization (Kan et al., 1998; Nakao, 2000). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, instead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation. A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salto</context>
</contexts>
<marker>Salton, Singhal, Buckley, Mitra, 1996</marker>
<rawString>Gerard Salton, Amit Singhal, Chris Buckley, and Mandar Mitra. 1996. Automatic text decomposition using text segments and text themes. In Proc. of Hypertext’96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Stephen M Omohundro</author>
</authors>
<title>Best-first model merging for hidden Markov model induction.</title>
<date>1994</date>
<tech>Technical Report TR-94-003,</tech>
<location>ICSI, Berkeley, CA.</location>
<contexts>
<context position="6995" citStr="Stolcke and Omohundro, 1994" startWordPosition="1163" endWordPosition="1166"> lengths is thus bits.3 Generally speaking, takes a large value when the number of segments is small. On the other hand, takes a large value when the number of segments is large. If only is used to segment the text, then the resulting segmentation will have too many segments. By using both and , we can get a reasonable number of segments. 3 Algorithm for Finding the Maximum-Probability Segmentation To find the maximum-probability segmentation , we first define the cost of segmentation as (9) 1Stolcke and Omohundro uses description length priors to induce the structure of hidden Markov models (Stolcke and Omohundro, 1994). 2‘Log’ denotes the logarithm to the base 2. 3We have used as before. But we use in this paper, because it is easily interpreted as a description length and the experimental results obtained by using are slightly better than those obtained by using . An anonymous reviewer suggests using a Poisson distribution whose parameter is , the average length of a segment (in words), as prior probability. We leave it for future work to compare the suitability of various prior probabilities for text segmentation. where (12) We further rewrite Equation (12) in the form of Equation (13) below by using Equa</context>
</contexts>
<marker>Stolcke, Omohundro, 1994</marker>
<rawString>Andreas Stolcke and Stephen M. Omohundro. 1994. Best-first model merging for hidden Markov model induction. Technical Report TR-94-003, ICSI, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaakov Yaari</author>
</authors>
<title>Segmentation of expository texts by hierarchical agglomerative clustering.</title>
<date>1997</date>
<booktitle>In Proc. of the Recent Advances in Natural Language Processing.</booktitle>
<contexts>
<context position="1534" citStr="Yaari, 1997" startWordPosition="233" endWordPosition="234">on (Kan et al., 1998; Nakao, 2000). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, instead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation. A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Choi, 2000; Nakao, 2000). A major characteristic of the methods used in this research is that they do not require training data to segment given texts. Hearst (1994), for example, used only the similarity of word distributions in a given text to segment the text. Consequently, these methods can be applied to any text in any domain, even if training data do not exist. This property is important when text segmentation is applied to information retrieval or summarization, because both tasks deal with domain-independent documents. Another application of text segmentation is the</context>
</contexts>
<marker>Yaari, 1997</marker>
<rawString>Yaakov Yaari. 1997. Segmentation of expository texts by hierarchical agglomerative clustering. In Proc. of the Recent Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Yamron</author>
<author>I Carp</author>
<author>S Lowe</author>
<author>P van Mulbregt</author>
</authors>
<title>A hidden Markov model approach to text segmentation and event tracking.</title>
<date>1998</date>
<booktitle>In Proc. of ICASSP-98.</booktitle>
<marker>Yamron, Carp, Lowe, van Mulbregt, 1998</marker>
<rawString>J. P. Yamron, I. Carp, S. Lowe, and P. van Mulbregt. 1998. A hidden Markov model approach to text segmentation and event tracking. In Proc. of ICASSP-98.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>