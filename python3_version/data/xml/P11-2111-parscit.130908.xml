<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010133">
<title confidence="0.976458">
Chinese sentence segmentation as comma classification
</title>
<author confidence="0.978533">
Nianwen Xue and Yaqin Yang
</author>
<affiliation confidence="0.979523">
Brandeis University, Computer Science Department
</affiliation>
<address confidence="0.91347">
Waltham, MA, 02453
</address>
<email confidence="0.999774">
{xuen,yaqin}@brandeis.edu
</email>
<sectionHeader confidence="0.996675" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.957481533333333">
We describe a method for disambiguating Chi-
nese commas that is central to Chinese sen-
tence segmentation. Chinese sentence seg-
mentation is viewed as the detection of loosely
coordinated clauses separated by commas.
Trained and tested on data derived from the
Chinese Treebank, our model achieves a clas-
sification accuracy of close to 90% overall,
which translates to an F1 score of 70% for
detecting commas that signal sentence bound-
aries.
tences that can only be plausibly translated into mul-
tiple English sentences. An example is given in (1),
where one Chinese sentence is plausibly translated
into three English sentences.
</bodyText>
<figure confidence="0.993875344827586">
(1) 7$ R RfI&apos;7 —A � V �
this period time AS AS pay attention to this
� nano 3 [1] I �W7 &gt;_ T
CL Nano even in person visit AS
3 ,
A � q rpq r$ik , [2] øÔƒ
a few AS computer market , comparatively
�PEI-
speaking
, [3] #
, Zhuoyue
� ��
’s price
a
relatively
, [4] f1_E
and
Wil
guarantee
14 1:i
be genuine
�
can
,
1 Introduction
0 �,
DE
1_1
low
</figure>
<bodyText confidence="0.99923335">
Sentence segmentation, or the detection of sentence
boundaries, is very much a solved problem for En-
glish. Sentence boundaries can be determined by
looking for periods, exclamation marks and ques-
tion marks. Although the symbol (dot) that is used to
represent period is ambiguous because it is also used
as the decimal point or in abbreviations, its resolu-
tion only requires local context. It can be resolved
fairly easily with rules in the form of regular expres-
sions or in a machine-learning framework (Reynar
and Ratnaparkhi, 1997).
Chinese also uses periods (albeit with a different
symbol), question marks, and exclamation marks to
indicate sentence boundaries. Where these punctua-
tion marks exist, sentence boundaries can be unam-
biguously detected. The difference is that the Chi-
nese comma also functions similarly as the English
period in some context and signals the boundary of a
sentence. As a result, if the commas are not disam-
biguated, Chinese would have these “run-on” sen-
</bodyText>
<sectionHeader confidence="0.970915" genericHeader="keywords">
,[5] )WWA
</sectionHeader>
<bodyText confidence="0.987755388888889">
, therefore
“I have been paying attention to this Nano 3 re-
cently, [1] and I even visited a few computer
stores in person. [2] Comparatively speaking,
[3] Zhuoyue’s prices are relatively low, [4]
and they can also guarantee that their products
are genuine. [5] Therefore I placed the order.”
In this paper, we formulate Chinese sentence seg-
mentation as a comma disambiguation problem. The
problem is basically one of separating commas that
mark sentence boundaries (such as [2] and [5] in (1))
from those that do not (such as [1], [3] and [4]).
Sentences that can be split on commas are gener-
ally loosely coordinated structures that are syntacti-
cally and semantically complete on their own, and
they do not have a close syntactic relation with one
another. We believe that a sentence boundary detec-
tion task that disambiguates commas, if successfully
</bodyText>
<equation confidence="0.6421885">
T T � .
place [AS] order
</equation>
<page confidence="0.948749">
631
</page>
<note confidence="0.5726035">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 631–635,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999210666666667">
solved, simplifies downstream tasks such as parsing
and Machine Translation.
The rest of the paper is organized as follows. In
Section 2, we describe our procedure for deriving
training and test data from the Chinese Treebank
(Xue et al., 2005). In Section 3, we present our
learning procedure. In Section 4 we report our re-
sults. Section 5 discusses related work. Section 6
concludes our paper.
</bodyText>
<sectionHeader confidence="0.799726" genericHeader="introduction">
2 Obtaining data
</sectionHeader>
<bodyText confidence="0.99808268">
To our knowledge, there is no data in the public
domain with commas explicitly annotated based on
whether they mark sentence boundaries. One could
imagine using parallel data where a Chinese sen-
tence is word-aligned with multiple English sen-
tences, but such data is generally noisy and com-
mas are not disambiguated based on a uniform stan-
dard. We instead pursued a different path and de-
rived our training and test data from the Chinese
Treebank (CTB). The CTB does not disambiguate
commas explicitly, and just like the Penn English
Treebank (Marcus et al., 1993), the sentence bound-
aries in the CTB are identified by periods, exclama-
tion and question marks. However, there are clear
syntactic patterns that can be used to disambiguate
the two types of commas. Commas that mark sen-
tence boundaries delimit loosely coordinated top-
level IPs, as illustrated in Figure 1, and commas that
don’t cover all other cases. One such example is
Figure 2, where a PP is separated from the rest of
the sentence with a comma. We devised a heuristic
algorithm to detect loosely coordinated structures in
the Chinese Treebank, and labeled each comma with
either EOS (end of a sentence) or Non-EOS (not the
end of a sentence).
</bodyText>
<sectionHeader confidence="0.979682" genericHeader="method">
3 Learning
</sectionHeader>
<bodyText confidence="0.999629222222222">
After the commas are labeled, we have basically
turned comma disambiguation into a binary classi-
fication problem. The syntactic structures are an
obvious source of information for this classification
task, so we parsed the entire CTB 6.0 in a round-
robin fashion. We divided CTB 6.0 into 10 portions,
and parsed each portion with a model trained on
other portions, using the Berkeley parser (Petrov and
Klein, 2007). The labels for the commas are derived
</bodyText>
<figure confidence="0.998870344827586">
IP
A±
ilLh emit �t1+
-t] R.Ti
VV NP
IP PU IP PU IP PU
NP VP
VP
NP
�
�
�
34%2�.1
71EK
VV
4i1� Wt]
A
VP
ADVP
VV NP
&apos;pro&apos;
NP VP
ADVP VP
IP
ME
VV
4i
NP VP
ArT !a* &amp;quot;
</figure>
<figureCaption confidence="0.995327">
Figure 1: Sentence-boundary denoting comma
</figureCaption>
<figure confidence="0.99771075">
IP
VV
�
PP PU NP NP VP PU
P NP I DNP NP
t�ft
ik +VQ � J27V M
49
NP DEG
MIA
J27V AM *a A�W 3F* AM
*9R
</figure>
<figureCaption confidence="0.995152">
Figure 2: Non-sentence boundary denoting comma
</figureCaption>
<bodyText confidence="0.999689823529412">
from the gold-standard parses using the heuristics
described in Section 2, as they obviously should be.
We first established a baseline by applying the same
heuristic algorithm to the automatic parses. This will
give us a sense of how accurately commas can be
disambiguated given imperfect parses. The research
question we’re trying to address here basically is:
can we improve on the baseline accuracy with a ma-
chine learning model?
We conducted our experiments with a Maximum
Entropy classifier trained with the Mallet package
(McCallum, 2002). The following are the features
we used to train our classifier. All features are de-
scribed relative to the comma being classified and
the context is the sentence that the comma is in. The
actual feature values for the first comma in Figure 1
are given as examples:
</bodyText>
<listItem confidence="0.5333352">
1. Part-of-speech tag of the previous word, and
the string representation of the previous word
if it has a frequency of greater than 20 in the
training corpus, e.g., f1=VV, f2=1X.
2. Part-of-speech of the following word and the
</listItem>
<page confidence="0.993436">
632
</page>
<bodyText confidence="0.998625666666667">
string representation of the following word if it
has a frequency of greater than 20 in the train-
ing corpus, e.g., f3=JJ, f4=�4)�
</bodyText>
<listItem confidence="0.99443">
3. The string representation of the following word
if it occurs more than 12,000 times in sentence-
initial positions in a large corpus external to our
training and test data.1
4. The phrase label of the left sibling and the
phrase label of their right sibling in the syntac-
tic parse tree, as well as their conjunction, e.g,
f6=IP, f7=IP, f8=IP+IP
5. The conjunction of the ancestors, the phrase la-
bel of the left sibling, and the phrase label of
the right sibling. The ancestor is defined as the
path from the parent of the comma to the root
node of the parse tree, e.g., f9=IP+IP+IP.
6. Whether there is a subordinating conjunction
</listItem>
<bodyText confidence="0.93030025">
(e.g., “if”, “because”) to the left of the comma.
The search starts at the comma and stops at the
previous punctuation mark or the beginning of
the sentence, e.g., f10=noCS.
</bodyText>
<listItem confidence="0.902557277777778">
7. Whether the parent of the comma is a coordi-
nating IP construction. A coordinating IP con-
struction is an IP that dominates a list of coor-
dinated IPs, e.g., f11=CoordIP.
8. Whether the comma is a top-level child, defined
as the child of the root node of the syntactic
tree, e.g., f12=top.
9. Whether the parent of the comma is a
top-level coordinating IP construction, e.g.,
f13=top+coordIP.
10. The punctuation mark template for this sen-
tence, e.g., f14=,+,+o
11. whether the length difference between the left
and right segments of the comma is smaller
than 7. The left (right) segment spans from the
previous (next) punctuation mark or the begin-
ning (end) of the sentence to the comma, e.g.,
f15=&gt;7
</listItem>
<sectionHeader confidence="0.994956" genericHeader="method">
4 Results and discussion
</sectionHeader>
<bodyText confidence="0.999916">
Our comma disambiguation models are trained and
evaluated on a subset of the Chinese TreeBank
(CTB) 6.0, released by the LDC. The unused por-
tion of CTB 6.0 consists of broadcast news data that
</bodyText>
<footnote confidence="0.9876855">
1This feature is not instantiated here because the following
word in this example does not occur with sufficient accuracy.
</footnote>
<bodyText confidence="0.998715636363636">
contains disfluencies, different from the rest of the
CTB 6.0. We used the training/test data split rec-
ommended in the Chinese Treebank documentation.
The CTB file IDs used in our experiments are listed
in Table 1. The automatic parses in each test set
are produced by retraining the Berkeley parser on
its corresponding training set, plus the unused por-
tion of the CTB 6.0. Measured by the ParsEval met-
ric (Black et al., 1991), the parsing accuracy on the
CTB test set stands at 83.63% (F-score), with a pre-
cision of 85.66% and a recall of 81.69%.
</bodyText>
<table confidence="0.998845">
Data Train Test
41-325, 400-454, 500-554 1-40
CTB 590-596, 600-885, 900 901-931
1001-1078, 1100-1151
</table>
<tableCaption confidence="0.999833">
Table 1: Data set division.
</tableCaption>
<bodyText confidence="0.9999437">
There are 1,510 commas in the test set, and our
heuristic baseline algorithm is able to correctly label
1,321 or 87.5% of the commas. Among these, 250
or 16.6% of them are EOS commas that mark sen-
tence boundaries and 1,260 of them are Non-EOS
commas. The results of our experiments are pre-
sented in Table 2. The baseline precision and recall
for the EOS commas are 59.1% and 79.6% respec-
tively with an F1 score of 67.8% . For Non-EOS
commas, the baseline precision and recall are 95.7%
and 89.0% respectively, amounting to an F1 score of
70.1%. The learned maximum classifier achieved a
modest improvement over the baseline. The over-
all accuracy of the learned model is 89.2%, just shy
of 90%. The precision and recall for EOS commas
are 64.7% and 76.4% respectively and the combined
F1 score is 70.1%. For Non-EOS commas, the pre-
cision and recall are 95.1% and 91.7% respectively,
with the F1 score being 93.4%. Other than a list
of most frequent words that start a sentence, all the
features are extracted from the sentence the comma
occurs in. Given that the heuristic algorithm and the
learned model use essentially the same source of in-
formation, we attribute the improvement to the use
of lexical features that the heuristic algorithm cannot
easily take advantage of.
Table 3 shows the contribution of individual fea-
ture groups. The numbers reflect the accuracy when
each feature group is taken out of the model. While
all the features have made a contribution to the over-
</bodyText>
<page confidence="0.997889">
633
</page>
<table confidence="0.999505333333333">
Baseline Learning
(%) p r f1 p r f1
Overall 87.5 89.2
EOS 59.1 79.6 67.8 64.7 76.4 70.1
Non- 95.7 89.0 92.2 95.1 91.7 93.4
EOS
</table>
<tableCaption confidence="0.979079">
Table 2: Accuracy for the baseline heuristic algorithm
and the learned model
</tableCaption>
<bodyText confidence="0.999455111111111">
all accuracy on the development set, some of the
features (3 and 8) actually hurt the overall perfor-
mance slightly on the test set. What’s interesting is
while the heuristic algorithm that is based entirely
on syntactic structure produced a strong baseline,
when formulated as features they are not at all effec-
tive. In particular, feature groups 7, 8, 9 are explicit
reformulations of the heuristic algorithm, but they
all contributed very little to or even slightly hurt the
overall performance. The more effective features are
the lexical features (1, 2, 10, 11) probably because
they are more robust. What this suggests is that we
can get reasonable sentence segmentation accuracy
without having to parse the sentence (or rather, the
multi-sentence group) first. The sentence segmenta-
tion can thus come before parsing in the processing
pipeline even in a language like Chinese where sen-
tences are not unambiguously marked.
</bodyText>
<table confidence="0.999513416666666">
overall f1 (EOS) f1 (non-EOS)
all 89.2 70.1 93.4
- (1,2) 87.5 67.7 92.3
-10 87.8 67.5 92.5
-11 88.6 68.6 93.1
-4 89.0 69.6 93.3
-5 89.1 69.5 93.3
-6 89.1 69.9 93.4
-7 89.1 70.1 93.4
-9 89.1 69.7 93.3
-8 89.2 70.5 93.4
- 3 89.4 70.5 93.5
</table>
<tableCaption confidence="0.998046">
Table 3: Feature effectiveness
</tableCaption>
<sectionHeader confidence="0.999897" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999963142857143">
There has been a fair amount of research on punctua-
tion prediction or generation in the context of spoken
language processing (Lu and Ng, 2010; Guo et al.,
2010). The task presented here is different in that the
punctuation marks are already present in the text and
we are only concerned with punctuation marks that
are semantically ambiguous. Our specific focus is
on the Chinese comma, which sometimes signals a
sentence boundary and sometimes doesn’t. The Chi-
nese comma has also been studied in the context of
syntactic parsing for long sentences (Jin et al., 2004;
Li et al., 2005), where the study of comma is seen as
part of a “divide-and-conquer” strategy to syntactic
parsing. Long sentences are split into shorter sen-
tence segments on commas before they are parsed,
and the syntactic parses for the shorter sentence seg-
ments are then assembled into the syntactic parse for
the original sentence. We study comma disambigua-
tion in its own right aimed at helping a wide range of
NLP applications that include parsing and Machine
Translation.
</bodyText>
<sectionHeader confidence="0.999014" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999921875">
The main goal of this short paper is to bring to
the attention of the field a problem that has largely
been taken for granted. We show that while sen-
tence boundary detection in Chinese is a relatively
easy task if formulated based on purely orthographic
grounds, the problem becomes much more challeng-
ing if we delve deeper and consider the semantic and
possibly the discourse basis on which sentences are
segmented. Seen in this light, the central problem
to Chinese sentence segmentation is comma disam-
biguation. We trained a statistical model using data
derived from the Chinese Treebank and reported
promising preliminary results. Much remains to be
done regarding how sentences in Chinese should be
segmented and how this problem should be modeled
in a statistical learning framework.
</bodyText>
<sectionHeader confidence="0.998903" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.5187805">
This work is supported by the National Science
Foundation via Grant No. 0910532 entitled “Richer
Representations for Machine Translation”. All
views expressed in this paper are those of the au-
thors and do not necessarily represent the view of
the National Science Foundation.
</bodyText>
<page confidence="0.999016">
634
</page>
<sectionHeader confidence="0.993886" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999274">
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitively comparing the syntactic coverage
of English grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop, pages 306–
311.
Yuqing Guo, Haifeng Wang, and Josef Van Genabith.
2010. A Linguistically Inspired Statistical Model for
Chinese Punctuation Generation. ACM Transactions
on Asian Language Processing, 9(2).
Meixun Jin, Mi-Young Kim, Dong-Il Kim, and Jong-
Hyeok Lee. 2004. Segmentation of Chinese Long
Sentences Using Commas. In Proceedings of the
SIGHANN Workshop on Chinese Language Process-
ing.
Xing Li, Chengqing Zong, and Rile Hu. 2005. A Hier-
archical Parsing Approach with Punctuation Process-
ing for Long Sentence Sentences. In Proceedings of
the Second International Joint Conference on Natural
Language Processing: Companion Volume including
Posters/Demos and Tutorial Abstracts.
We Lu and Hwee Tou Ng. 2010. Better Punctuation
Prediction with Dynamic Conditional Random Fields.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, MIT, Mas-
sachusetts.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of English:
the Penn Treebank. Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Slav Petrov and Dan Klein. 2007. Improved Inferencing
for Unlexicalized Parsing. In Proc of HLT-NAACL.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
Maximum Entropy Approach to Identifying Sentence
Boundaries. In Proceedings of the Fifth Conference on
Applied Natural Language Processing (ANLP), Wash-
ington, D.C.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207–238.
</reference>
<page confidence="0.998787">
635
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.344487">
<title confidence="0.997738">Chinese sentence segmentation as comma classification</title>
<author confidence="0.856908">Nianwen Xue</author>
<author confidence="0.856908">Yaqin</author>
<affiliation confidence="0.989339">Brandeis University, Computer Science</affiliation>
<address confidence="0.748028">Waltham, MA,</address>
<abstract confidence="0.998568527777778">We describe a method for disambiguating Chinese commas that is central to Chinese sentence segmentation. Chinese sentence segmentation is viewed as the detection of loosely coordinated clauses separated by commas. Trained and tested on data derived from the Chinese Treebank, our model achieves a classification accuracy of close to 90% overall, which translates to an F1 score of 70% for detecting commas that signal sentence boundaries. tences that can only be plausibly translated into multiple English sentences. An example is given in (1), where one Chinese sentence is plausibly translated into three English sentences. R RfI&apos;7 AS � V � this this period time AS pay attention to � nano Nano 3 in person &gt;_ T CL even visit AS 3 , A � AS q rpq r$ik [2] a few computer market , comparatively �PEIspeaking [3] , Zhuoyue price a relatively [4] and guarantee be genuine � can</abstract>
<intro confidence="0.557195"></intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickinger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<pages>306--311</pages>
<contexts>
<context position="9114" citStr="Black et al., 1991" startWordPosition="1567" endWordPosition="1570">. The unused portion of CTB 6.0 consists of broadcast news data that 1This feature is not instantiated here because the following word in this example does not occur with sufficient accuracy. contains disfluencies, different from the rest of the CTB 6.0. We used the training/test data split recommended in the Chinese Treebank documentation. The CTB file IDs used in our experiments are listed in Table 1. The automatic parses in each test set are produced by retraining the Berkeley parser on its corresponding training set, plus the unused portion of the CTB 6.0. Measured by the ParsEval metric (Black et al., 1991), the parsing accuracy on the CTB test set stands at 83.63% (F-score), with a precision of 85.66% and a recall of 81.69%. Data Train Test 41-325, 400-454, 500-554 1-40 CTB 590-596, 600-885, 900 901-931 1001-1078, 1100-1151 Table 1: Data set division. There are 1,510 commas in the test set, and our heuristic baseline algorithm is able to correctly label 1,321 or 87.5% of the commas. Among these, 250 or 16.6% of them are EOS commas that mark sentence boundaries and 1,260 of them are Non-EOS commas. The results of our experiments are presented in Table 2. The baseline precision and recall for the</context>
</contexts>
<marker>Black, Abney, Flickinger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitively comparing the syntactic coverage of English grammars. In Proceedings of the DARPA Speech and Natural Language Workshop, pages 306– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqing Guo</author>
<author>Haifeng Wang</author>
<author>Josef Van Genabith</author>
</authors>
<title>A Linguistically Inspired Statistical Model for Chinese Punctuation Generation.</title>
<date>2010</date>
<journal>ACM Transactions on Asian Language Processing,</journal>
<volume>9</volume>
<issue>2</issue>
<marker>Guo, Wang, Van Genabith, 2010</marker>
<rawString>Yuqing Guo, Haifeng Wang, and Josef Van Genabith. 2010. A Linguistically Inspired Statistical Model for Chinese Punctuation Generation. ACM Transactions on Asian Language Processing, 9(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meixun Jin</author>
<author>Mi-Young Kim</author>
<author>Dong-Il Kim</author>
<author>JongHyeok Lee</author>
</authors>
<title>Segmentation of Chinese Long Sentences Using Commas.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGHANN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="12828" citStr="Jin et al., 2004" startWordPosition="2210" endWordPosition="2213">.5 Table 3: Feature effectiveness 5 Related work There has been a fair amount of research on punctuation prediction or generation in the context of spoken language processing (Lu and Ng, 2010; Guo et al., 2010). The task presented here is different in that the punctuation marks are already present in the text and we are only concerned with punctuation marks that are semantically ambiguous. Our specific focus is on the Chinese comma, which sometimes signals a sentence boundary and sometimes doesn’t. The Chinese comma has also been studied in the context of syntactic parsing for long sentences (Jin et al., 2004; Li et al., 2005), where the study of comma is seen as part of a “divide-and-conquer” strategy to syntactic parsing. Long sentences are split into shorter sentence segments on commas before they are parsed, and the syntactic parses for the shorter sentence segments are then assembled into the syntactic parse for the original sentence. We study comma disambiguation in its own right aimed at helping a wide range of NLP applications that include parsing and Machine Translation. 6 Conclusion The main goal of this short paper is to bring to the attention of the field a problem that has largely bee</context>
</contexts>
<marker>Jin, Kim, Kim, Lee, 2004</marker>
<rawString>Meixun Jin, Mi-Young Kim, Dong-Il Kim, and JongHyeok Lee. 2004. Segmentation of Chinese Long Sentences Using Commas. In Proceedings of the SIGHANN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xing Li</author>
<author>Chengqing Zong</author>
<author>Rile Hu</author>
</authors>
<title>A Hierarchical Parsing Approach with Punctuation Processing for Long Sentence Sentences.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second International Joint Conference on Natural Language Processing: Companion Volume including Posters/Demos and Tutorial Abstracts.</booktitle>
<contexts>
<context position="12846" citStr="Li et al., 2005" startWordPosition="2214" endWordPosition="2217">e effectiveness 5 Related work There has been a fair amount of research on punctuation prediction or generation in the context of spoken language processing (Lu and Ng, 2010; Guo et al., 2010). The task presented here is different in that the punctuation marks are already present in the text and we are only concerned with punctuation marks that are semantically ambiguous. Our specific focus is on the Chinese comma, which sometimes signals a sentence boundary and sometimes doesn’t. The Chinese comma has also been studied in the context of syntactic parsing for long sentences (Jin et al., 2004; Li et al., 2005), where the study of comma is seen as part of a “divide-and-conquer” strategy to syntactic parsing. Long sentences are split into shorter sentence segments on commas before they are parsed, and the syntactic parses for the shorter sentence segments are then assembled into the syntactic parse for the original sentence. We study comma disambiguation in its own right aimed at helping a wide range of NLP applications that include parsing and Machine Translation. 6 Conclusion The main goal of this short paper is to bring to the attention of the field a problem that has largely been taken for grante</context>
</contexts>
<marker>Li, Zong, Hu, 2005</marker>
<rawString>Xing Li, Chengqing Zong, and Rile Hu. 2005. A Hierarchical Parsing Approach with Punctuation Processing for Long Sentence Sentences. In Proceedings of the Second International Joint Conference on Natural Language Processing: Companion Volume including Posters/Demos and Tutorial Abstracts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>We Lu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better Punctuation Prediction with Dynamic Conditional Random Fields.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>MIT, Massachusetts.</location>
<contexts>
<context position="12403" citStr="Lu and Ng, 2010" startWordPosition="2139" endWordPosition="2142"> multi-sentence group) first. The sentence segmentation can thus come before parsing in the processing pipeline even in a language like Chinese where sentences are not unambiguously marked. overall f1 (EOS) f1 (non-EOS) all 89.2 70.1 93.4 - (1,2) 87.5 67.7 92.3 -10 87.8 67.5 92.5 -11 88.6 68.6 93.1 -4 89.0 69.6 93.3 -5 89.1 69.5 93.3 -6 89.1 69.9 93.4 -7 89.1 70.1 93.4 -9 89.1 69.7 93.3 -8 89.2 70.5 93.4 - 3 89.4 70.5 93.5 Table 3: Feature effectiveness 5 Related work There has been a fair amount of research on punctuation prediction or generation in the context of spoken language processing (Lu and Ng, 2010; Guo et al., 2010). The task presented here is different in that the punctuation marks are already present in the text and we are only concerned with punctuation marks that are semantically ambiguous. Our specific focus is on the Chinese comma, which sometimes signals a sentence boundary and sometimes doesn’t. The Chinese comma has also been studied in the context of syntactic parsing for long sentences (Jin et al., 2004; Li et al., 2005), where the study of comma is seen as part of a “divide-and-conquer” strategy to syntactic parsing. Long sentences are split into shorter sentence segments o</context>
</contexts>
<marker>Lu, Ng, 2010</marker>
<rawString>We Lu and Hwee Tou Ng. 2010. Better Punctuation Prediction with Dynamic Conditional Random Fields. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, MIT, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="4200" citStr="Marcus et al., 1993" startWordPosition="699" endWordPosition="702">ted work. Section 6 concludes our paper. 2 Obtaining data To our knowledge, there is no data in the public domain with commas explicitly annotated based on whether they mark sentence boundaries. One could imagine using parallel data where a Chinese sentence is word-aligned with multiple English sentences, but such data is generally noisy and commas are not disambiguated based on a uniform standard. We instead pursued a different path and derived our training and test data from the Chinese Treebank (CTB). The CTB does not disambiguate commas explicitly, and just like the Penn English Treebank (Marcus et al., 1993), the sentence boundaries in the CTB are identified by periods, exclamation and question marks. However, there are clear syntactic patterns that can be used to disambiguate the two types of commas. Commas that mark sentence boundaries delimit loosely coordinated toplevel IPs, as illustrated in Figure 1, and commas that don’t cover all other cases. One such example is Figure 2, where a PP is separated from the rest of the sentence with a comma. We devised a heuristic algorithm to detect loosely coordinated structures in the Chinese Treebank, and labeled each comma with either EOS (end of a sent</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="6208" citStr="McCallum, 2002" startWordPosition="1063" endWordPosition="1064">V AM *a A�W 3F* AM *9R Figure 2: Non-sentence boundary denoting comma from the gold-standard parses using the heuristics described in Section 2, as they obviously should be. We first established a baseline by applying the same heuristic algorithm to the automatic parses. This will give us a sense of how accurately commas can be disambiguated given imperfect parses. The research question we’re trying to address here basically is: can we improve on the baseline accuracy with a machine learning model? We conducted our experiments with a Maximum Entropy classifier trained with the Mallet package (McCallum, 2002). The following are the features we used to train our classifier. All features are described relative to the comma being classified and the context is the sentence that the comma is in. The actual feature values for the first comma in Figure 1 are given as examples: 1. Part-of-speech tag of the previous word, and the string representation of the previous word if it has a frequency of greater than 20 in the training corpus, e.g., f1=VV, f2=1X. 2. Part-of-speech of the following word and the 632 string representation of the following word if it has a frequency of greater than 20 in the training </context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inferencing for Unlexicalized Parsing. In</title>
<date>2007</date>
<booktitle>Proc of HLT-NAACL.</booktitle>
<contexts>
<context position="5271" citStr="Petrov and Klein, 2007" startWordPosition="880" endWordPosition="883">We devised a heuristic algorithm to detect loosely coordinated structures in the Chinese Treebank, and labeled each comma with either EOS (end of a sentence) or Non-EOS (not the end of a sentence). 3 Learning After the commas are labeled, we have basically turned comma disambiguation into a binary classification problem. The syntactic structures are an obvious source of information for this classification task, so we parsed the entire CTB 6.0 in a roundrobin fashion. We divided CTB 6.0 into 10 portions, and parsed each portion with a model trained on other portions, using the Berkeley parser (Petrov and Klein, 2007). The labels for the commas are derived IP A± ilLh emit �t1+ -t] R.Ti VV NP IP PU IP PU IP PU NP VP VP NP � � � 34%2�.1 71EK VV 4i1� Wt] A VP ADVP VV NP &apos;pro&apos; NP VP ADVP VP IP ME VV 4i NP VP ArT !a* &amp;quot; Figure 1: Sentence-boundary denoting comma IP VV � PP PU NP NP VP PU P NP I DNP NP t�ft ik +VQ � J27V M 49 NP DEG MIA J27V AM *a A�W 3F* AM *9R Figure 2: Non-sentence boundary denoting comma from the gold-standard parses using the heuristics described in Section 2, as they obviously should be. We first established a baseline by applying the same heuristic algorithm to the automatic parses. This w</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inferencing for Unlexicalized Parsing. In Proc of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Approach to Identifying Sentence Boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP),</booktitle>
<location>Washington, D.C.</location>
<contexts>
<context position="1682" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="286" endWordPosition="289">elatively , [4] f1_E and Wil guarantee 14 1:i be genuine � can , 1 Introduction 0 �, DE 1_1 low Sentence segmentation, or the detection of sentence boundaries, is very much a solved problem for English. Sentence boundaries can be determined by looking for periods, exclamation marks and question marks. Although the symbol (dot) that is used to represent period is ambiguous because it is also used as the decimal point or in abbreviations, its resolution only requires local context. It can be resolved fairly easily with rules in the form of regular expressions or in a machine-learning framework (Reynar and Ratnaparkhi, 1997). Chinese also uses periods (albeit with a different symbol), question marks, and exclamation marks to indicate sentence boundaries. Where these punctuation marks exist, sentence boundaries can be unambiguously detected. The difference is that the Chinese comma also functions similarly as the English period in some context and signals the boundary of a sentence. As a result, if the commas are not disambiguated, Chinese would have these “run-on” sen,[5] )WWA , therefore “I have been paying attention to this Nano 3 recently, [1] and I even visited a few computer stores in person. [2] Comparative</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A Maximum Entropy Approach to Identifying Sentence Boundaries. In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP), Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="3469" citStr="Xue et al., 2005" startWordPosition="575" endWordPosition="578">ave a close syntactic relation with one another. We believe that a sentence boundary detection task that disambiguates commas, if successfully T T � . place [AS] order 631 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 631–635, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics solved, simplifies downstream tasks such as parsing and Machine Translation. The rest of the paper is organized as follows. In Section 2, we describe our procedure for deriving training and test data from the Chinese Treebank (Xue et al., 2005). In Section 3, we present our learning procedure. In Section 4 we report our results. Section 5 discusses related work. Section 6 concludes our paper. 2 Obtaining data To our knowledge, there is no data in the public domain with commas explicitly annotated based on whether they mark sentence boundaries. One could imagine using parallel data where a Chinese sentence is word-aligned with multiple English sentences, but such data is generally noisy and commas are not disambiguated based on a uniform standard. We instead pursued a different path and derived our training and test data from the Chi</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>