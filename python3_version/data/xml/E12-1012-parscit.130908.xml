<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004398">
<title confidence="0.999286">
Adaptation of Statistical Machine Translation Model for Cross-Lingual
Information Retrieval in a Service Context
</title>
<author confidence="0.888989">
Vassilina Nikoulina
</author>
<affiliation confidence="0.830992">
Xerox Research Center Europe
</affiliation>
<email confidence="0.981217">
vassilina.nikoulina@xrce.xerox.com
</email>
<author confidence="0.94703">
Nikolaos Lagos
</author>
<affiliation confidence="0.908303">
Xerox Research Center Europe
</affiliation>
<email confidence="0.990946">
nikolaos.lagos@xrce.xerox.com
</email>
<author confidence="0.981647">
Bogomil Kovachev
</author>
<affiliation confidence="0.9921995">
Informatics Institute
University of Amsterdam
</affiliation>
<email confidence="0.973572">
B.K.Kovachev@uva.nl
</email>
<author confidence="0.993762">
Christof Monz
</author>
<affiliation confidence="0.995424">
Informatics Institute
University of Amsterdam
</affiliation>
<email confidence="0.994275">
C.Monz@uva.nl
</email>
<sectionHeader confidence="0.993828" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999764521739131">
This work proposes to adapt an existing
general SMT model for the task of translat-
ing queries that are subsequently going to
be used to retrieve information from a tar-
get language collection. In the scenario that
we focus on access to the document collec-
tion itself is not available and changes to
the IR model are not possible. We propose
two ways to achieve the adaptation effect
and both of them are aimed at tuning pa-
rameter weights on a set of parallel queries.
The first approach is via a standard tuning
procedure optimizing for BLEU score and
the second one is via a reranking approach
optimizing for MAP score. We also extend
the second approach by using syntax-based
features. Our experiments show improve-
ments of 1-2.5 in terms of MAP score over
the retrieval with the non-adapted transla-
tion. We show that these improvements are
due both to the integration of the adapta-
tion and syntax-features for the query trans-
lation task.
</bodyText>
<sectionHeader confidence="0.999024" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9984128125">
Cross Lingual Information Retrieval (CLIR) is an
important feature for any digital content provider
in today’s multilingual environment. However,
many of the content providers are not willing to
change existing well-established document index-
ing and search tools, nor to provide access to
their document collection by a third-party exter-
nal service. The work presented in this paper as-
sumes such a context of use, where a query trans-
lation service allows translating queries posed to
the search engine of a content provider into sev-
eral target languages, without requiring changes
to the undelying IR system used and without ac-
cessing, at translation time, the content provider’s
document set. Keeping in mind these constraints,
we present two approaches on query translation
optimisation.
One of the important observations done dur-
ing the CLEF 2009 campaign (Ferro and Peters,
2009) related to CLIR was that the usage of Sta-
tistical Machine Translation (SMT) systems (eg.
Google Translate) for query translation led to
important improvements in the cross-lingual re-
trieval performance (the best CLIR performance
increased from ˜55% of the monolingual baseline
in 2008 to more than 90% in 2009 for French
and German target languages). However, general-
purpose SMT systems are not necessarily adapted
for query translation. That is because SMT sys-
tems trained on a corpus of standard parallel
phrases take into account the phrase structure im-
plicitly. The structure of queries is very differ-
ent from the standard phrase structure: queries are
very short and the word order might be different
than the typical full phrase one. This problem can
be seen as a problem of genre adaptation for SMT,
where the genre is “query”.
To our knowledge, no suitable corpora of par-
allel queries is available to train an adapted SMT
system. Small corpora of parallel queries1 how-
ever can be obtained (eg. CLEF tracks) or man-
ually created. We suggest to use such corpora
in order to adapt the SMT model parameters for
query translation. In our approach the parameters
of the SMT models are optimized on the basis of
the parallel queries set. This is achieved either di-
rectly in the SMT system using the MERT (Mini-
mum Error Rate Training) algorithm and optimiz-
</bodyText>
<footnote confidence="0.847447">
1Insufficient for a full SMT system training (˜500 entries)
</footnote>
<page confidence="0.959142">
109
</page>
<note confidence="0.977259">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 109–119,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999922166666667">
ing according to the BLEU2(Papineni et al., 2001)
score, or via reranking the Nbest translation can-
didates generated by a baseline system based on
new parameters (and possibly new features) that
aim to optimize a retrieval metric.
It is important to note that both of the pro-
posed approaches allow keeping the MT system
independent of the document collection and in-
dexing, and thus suitable for a query translation
service. These two approaches can also be com-
bined by using the model produced with the first
approach as a baseline that produces the Nbest list
of translations that is then given to the reranking
approach.
The remainder of this paper is organized as fol-
lows. We first present related work addressing the
problem of query translation. We then describe
two approaches towards adapting an SMT system
to the query-genre: tuning the SMT system on a
parallel set of queries (Section 3.1) and adapting
machine translation via the reranking framework
(Section 3.2). We then present our experimental
settings and results (Section 4) and conclude in
section 5.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999117913043478">
We may distinguish two main groups of ap-
proaches to CLIR: document translation and
query translation. We concentrate on the second
group which is more relevant to our settings. The
standard query translation methods use different
translation resources such as bilingual dictionar-
ies, parallel corpora and/or machine translation.
The aspect of disambiguation is important for the
first two techniques.
Different methods were proposed to deal with
disambiguation issues, often relying on the docu-
ment collection or embedding the translation step
directly into the retrieval model (Hiemstra and
Jong, 1999; Berger et al., 1999; Kraaij et al.,
2003). Other methods rely on external resources
like query logs (Gao et al., 2010), Wikipedia (Ja-
didinejad and Mahmoudi, 2009) or the web (Nie
and Chen, 2002; Hu et al., 2008). (Gao et al.,
2006) proposes syntax-based translation models
to deal with the disambiguation issues (NP-based,
dependency-based). The candidate translations
proposed by these models are then reranked with
the model learned to minimize the translation er-
</bodyText>
<footnote confidence="0.773433">
2Standard MT evaluation metric
</footnote>
<bodyText confidence="0.999557066666667">
ror on the training data.
To our knowledge, existing work that use MT-
based techniques for query translation use an out-
of-the-box MT system, without adapting it for
query translation in particular (Jones et al., 1999;
Wu et al., 2008) (although some query expan-
sion techniques might be applied to the produced
translation afterwards (Wu and He, 2010)).
There is a number of works done for do-
main adaptation in Statistical Machine Transla-
tion. However, we want to distinguish between
genre and domain adaptation in this work. Gen-
erally, genre can be seen as a sub-problem of do-
main. Thus, we consider genre to be the general
style of the text e.g. conversation, news, blog,
query (responsible mostly for the text structure)
while the domain reflects more what the text is
about – eg. social science, healthcare, history, so
domain adaptation involves lexical disambigua-
tion and extra lexical coverage problems. To our
knowledge, there is not much work addressing ex-
plicitly the problem of genre adaptation for SMT.
Some work done on domain adaptation could be
applied to genre adaptation, such as incorporating
available in-domain corpora in the SMT model:
either monolingual (Bertoldi and Federico, 2009;
Wu et al., 2008; Zhao et al., 2004; Koehn and
Schroeder, 2007), or small parallel data used for
tuning the SMT parameters (Zheng et al., 2010;
Pecina et al., 2011).
</bodyText>
<sectionHeader confidence="0.963333" genericHeader="method">
3 Our approach
</sectionHeader>
<bodyText confidence="0.999968368421053">
This work is based on the hypothesis that the
general-purpose SMT system needs to be adapted
for query translation. Although in (Ferro and
Peters, 2009) it has been mentioned that using
Google translate (general-purpose MT) for query
translation allowed to CLEF participants to obtain
the best CLIR performance, there is still 10% gap
between monolingual and cross-lingual IR. We
believe that, as in (Clinchant and Renders, 2007),
more adapted query translation, possibly further
combined with query expansion techniques, can
lead to improved retrieval.
The problem of the SMT adaptation for query-
genre translation has different quality aspects.
On the one hand, we want our model to pro-
duce a “good” translation (well-formed and trans-
mitting the information contained in the source
query) of an input query. On the other hand, we
want to obtain good retrieval performance using
</bodyText>
<page confidence="0.995998">
110
</page>
<bodyText confidence="0.9998906">
the proposed translation. These two aspects are
not necessarily correlated: a bag-of-word transla-
tion can lead to good retrieval performance, even
though it won’t be syntactically well-formed; at
the same time a well-formed translation can lead
to worse retrieval if the wrong lexical choice is
done. Moreover, often the retrieval demands some
linguistic preprocessing (eg. lemmatisation, PoS
tagging) which in interaction with badly-formed
translations might bring some noise.
A couple of works studied the correlation be-
tween the standard MT evaluation metrics and
the retrieval precision. Thus, (Fujii et al., 2009)
showed a good correlation of the BLEU scores
with the MAP scores for Cross-Lingual Patent
Retrieval. However, the topics in patent search
(long and well structured) are very different from
standard queries. (Kettunen, 2009) also found a
pretty high correlation ( 0.8 − 0.9) between stan-
dard MT evaluation metrics (METEOR(Banerjee
and Lavie, 2005), BLEU, NIST(Doddington,
2002)) and retrieval precision for long queries.
However, the same work shows that the correla-
tion decreases ( 0.6 − 0.7) for short queries.
In this paper we propose two approaches to
SMT adaptation for queries. The first one op-
timizes BLEU, while the second one optimizes
Mean Average Precision (MAP), a standard met-
ric in information retrieval. We’ll address the is-
sue of the correlation between BLEU and MAP in
Section 4.
Both of the proposed approaches rely on the
phrase-based SMT (PBMT) model (Koehn et al.,
2003) implemented in the Open Source SMT
toolkit MOSES (Koehn et al., 2007).
</bodyText>
<subsectionHeader confidence="0.99743">
3.1 Tuning for genre adaptation
</subsectionHeader>
<bodyText confidence="0.999994375">
First, we propose to adapt the PBMT model by
tuning the model’s weights on a parallel set of
queries. This approach addresses the first as-
pect of the problem, which is producing a “good”
translation. The PBMT model combines differ-
ent types of features via a log-linear model. The
standard features include (Koehn, 2010, Chapter
5): language model, word penalty, distortion, dif-
ferent translation models, etc. The weights of
these features are learned during the tuning step
with the MERT (Och, 2003) algorithm. Roughly
the MERT algorithm tunes feature weights one by
one and optimizes them according to the BLEU
score obtained.
Our hypothesis is that the impact of different
features should be different depending on whether
we translate a full sentence, or a query-genre en-
try. Thus, one would expect that in the case
of query-genre the language model or the distor-
tion features should get less importance than in
the case of the full-sentence translation. MERT
tuning on a genre-adapted parallel corpus should
leverage this information from the data, adapting
the SMT model to the query-genre. We would
also like to note that the tuning approach (pro-
posed for domain adaptation by (Zheng et al.,
2010)) seems to be more appropriate for genre
adaptation than for domain adaptation where the
problem of lexical ambiguity is encoded in the
translation model and re-weighting the main fea-
tures might not be sufficient.
We use the MERT implementation provided
with the Moses toolkit with default settings. Our
assumption is that this procedure although not ex-
plicitly aimed at improving retrieval performance
will nevertheless lead to “better” query transla-
tions when compared to the baseline. The results
of this apporach allow us also to observe whether
and to what extent changes in BLEU scores are
correlated to changes in MAP scores.
</bodyText>
<subsectionHeader confidence="0.911834">
3.2 Reranking framework for query
translation
</subsectionHeader>
<bodyText confidence="0.999974136363636">
The second approach addresses the retrieval qual-
ity problem. An SMT system is usually trained to
optimize the quality of the translation (eg. BLEU
score for SMT), which is not necessarily corre-
lated with the retrieval quality (especially for the
short queries). Thus, for example, the word or-
der which is crucial for translation quality (and is
taken into account by most MT evaluation met-
rics) is often ignored by IR models. Our second
approach follows (Nie, 2010, pp.106) argument
that “the translation problem is an integral part
of the whole CLIR problem, and unified CLIR
models integrating translation should be defined”.
We propose integrating the IR metric (MAP) into
the translation model optimisation step via the
reranking framework.
Previous attempts to apply the reranking ap-
proach to SMT did not show significant improve-
ments in terms of MT evaluation metrics (Och
et al., 2003; Nikoulina and Dymetman, 2008).
One of the reasons being the poor diversity of the
Nbest list of the translations. However, we be-
</bodyText>
<page confidence="0.993726">
111
</page>
<bodyText confidence="0.999960051282051">
lieve that this approach has more potential in the
context of query translation.
First of all the average query length is ˜5 words,
which means that the Nbest list of the translations
is more diverse than in the case of general phrase
translation (average length 25-30 words).
Moreover, the retrieval precision is more natu-
rally integrated into the reranking framework than
standard MT evaluation metrics such as BLEU.
The main reason is that the notion of Average Re-
trieval Precision is well defined for a single query
translation, while BLEU is defined on the corpus
level and correlates poorly with human quality
judgements for the individual translations (Specia
et al., 2009; Callison-Burch et al., 2009).
Finally, the reranking framework allows a lot
of flexibility. Thus, it allows enriching the base-
line translation model with new complex features
which might be difficult to introduce into the
translation model directly.
Other works applied the reranking framework
to different NLP tasks such as Named Entities
Extraction (Collins, 2001), parsing (Collins and
Roark, 2004), and language modelling (Roark et
al., 2004). Most of these works used the reranking
framework to combine generative and discrimina-
tive methods when both approaches aim at solv-
ing the same problem: the generative model pro-
duces a set of hypotheses, and the best hypoth-
esis is chosen afterwards via the discriminative
reranking model, which allows to enrich the base-
line model with the new complex and heteroge-
neous features. We suggest using the reranking
framework to combine two different tasks: Ma-
chine Translation and Cross-lingual Information
Retrieval. In this context the reranking framework
doesn’t only allow enriching the baseline transla-
tion model but also performing training using a
more appropriate evaluation metric.
</bodyText>
<subsectionHeader confidence="0.975069">
3.2.1 Reranking training
</subsectionHeader>
<bodyText confidence="0.999799">
Generally, the reranking framework can be re-
sumed in the following steps :
</bodyText>
<listItem confidence="0.996520857142857">
1. The baseline (generic-purpose) MT system
generates a list of candidate translations
GEN(q) for each query q;
2. A vector of features F(t) is assigned to each
translation t E GEN(q);
3. The best translation t� is chosen as the one
maximizing the translation score, which is
</listItem>
<bodyText confidence="0.9960688">
defined as a weighted linear combination of
features: �t(λ) = arg maxtEGEN(q) λ- F(t)
As shown above the best translation is selected ac-
cording to features’ weights λ. In order to learn
the weights λ maximizing the retrieval perfor-
mance, an appropriate annotated training set has
to be created. We use the CLEF tracks to create
the training set. The retrieval scores annotations
are based on the document relevance annotations
performed by human annotators during the CLEF
campaign.
The annotated training set is created out of
queries {q1, ..., qK} with an Nbest list of trans-
lations GEN(qi) of each query qi, i E {1..K} as
follows:
</bodyText>
<listItem confidence="0.9929619">
• A list of N (we take N = 1000) translations
(GEN(qi)) is produced by the baseline MT
model for each query qi, i = 1..K.
• Each translation t E GEN(qi) is used
to perform a retrieval from a target docu-
ment collection, and an Average Precision
score (AP(t)) is computed for each t E
GEN(qi) by comparing its retrieval to the
relevance annotations done during the CLEF
campaign.
</listItem>
<bodyText confidence="0.99998375">
The weights λ are learned with the objective of
maximizing MAP for all the queries of the train-
ing set, and, therefore, are optimized for retrieval
quality.
The weights optimization is done with
the Margin Infused Relaxed Algorithm
(MIRA)(Crammer and Singer, 2003), which
was applied to SMT by (Watanabe et al., 2007;
Chiang et al., 2008). MIRA is an online learning
algorithm where each weights update is done to
keep the new weights as close as possible to the
old weights (first term), and score oracle trans-
lation (the translation giving the best retrieval
score : tz = arg maxt AP(t)) higher than each
non-oracle translation (tij) by a margin at least as
wide as the loss lij (second term):
</bodyText>
<equation confidence="0.756016">
λ= minλ&apos; 12I Iλ&apos; − λI I2 +
\ )
C �K i=1 maxj=1..N lij − λ&apos; - (F(t� i) − F(tij)
</equation>
<bodyText confidence="0.9985388">
The loss lij is defined as the difference in the re-
trieval average precision between the oracle and
non-oracle translations: lij = AP(tz) − AP(tij).
C is the regularization parameter which is chosen
via 5-fold cross-validation.
</bodyText>
<page confidence="0.995426">
112
</page>
<sectionHeader confidence="0.831962" genericHeader="method">
3.2.2 Features
</sectionHeader>
<bodyText confidence="0.9993802">
One of the advantages of the reranking frame-
work is that new complex features can be easily
integrated. We suggest to enrich the reranking
model with different syntax-based features, such
as:
</bodyText>
<listItem confidence="0.9889856">
• features relying on dependency structures:
called therein coupling features (proposed by
(Nikoulina and Dymetman, 2008));
• features relying on Part of Speech Tagging:
called therein PoS mapping features.
</listItem>
<bodyText confidence="0.99954268">
By integrating the syntax-based features we
have a double goal: showing the potential of
the reranking framework with more complex fea-
tures, and examining whether the integration of
syntactic information could be useful for query
translation.
Coupling features. The goal of the coupling
features is to measure the similarity between
source and target dependency structures. The ini-
tial hypothesis is that a better translation should
have a dependency structure closer to the one of
the source query.
In this work we experiment with two dif-
ferent coupling variants proposed in (Nikoulina
and Dymetman, 2008), namely, Lexicalised and
Label-specific coupling features.
The generic coupling features are based on
the notion of “rectangles” that are of the follow-
ing type : ((s1, ds12, s2), (t1, dt12, t2)), where
ds12 is an edge between source words s1 and s2,
dt12 is an edge between target words t1 and t2,
s1 is aligned with t1 and s2 is aligned with t2.
Lexicalised features take into account the qual-
ity of lexical alignment, by weighting each rect-
angle (s1, s2, t1, t2) by a probability of align-
ing s1 to t1 and s2 to t2 (eg. p(s1|t1)p(s2|t2) or
p(t1|s1)p(t2|s2)).
The Label-Specific features take into account
the nature of the aligned dependencies. Thus, the
rectangles of the form ((s1, subj, s2), (t1, subj,
t2)) will get more weight than a rectangle ((s1,
subj, s2), (t1, nmod, t2)). The importance of
each “rectangle” is learned on the parallel anno-
tated corpus by introducing a collection of Label-
Specific coupling features, each for a specific pair
of source label and target label.
PoS mapping features. The goal of the PoS
mapping features is to control the correspondence
of Part Of Speech Tags between an input query
and its translation. As the coupling features, the
PoS mapping features rely on the word align-
ments between the source sentence and its trans-
lation3. A vector of sparse features is introduced
where each component corresponds to a pair of
PoS tags aligned in the training data. We intro-
duce a generic PoS map variant, which counts a
number of occurrences of a specific pair of PoS
tags, and lexical PoS map variant, which weights
down these pairs by a lexical alignment score
(p(s|t) or p(t|s)).
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.9496115">
4.1 Experimental basis
4.1.1 Data
</subsectionHeader>
<bodyText confidence="0.999901086956522">
To simulate parallel query data we used trans-
lation equivalent CLEF topics. The data set used
for the first approach consists of the CLEF topic
data from the following years and tasks: AdHoc-
main track from 2000 to 2008; CLEF AdHoc-
TEL track 2008; Domain Specific tracks from
2000 to 2008; CLEF robust tracks 2007 and 2008;
GeoCLEf tracks 2005-2007. To avoid the issue of
overlapping topics we removed duplicates. The
created parallel queries set contained 500 − 700
parallel entries (depending on the language pair,
Table 1) and was used for Moses parameters tun-
ing.
In order to create the training set for the rerank-
ing approach, we need to have access to the rele-
vance judgements. We didn’t have access to all
relevance judgements of the previously desribed
tracks. Thus we used only a subset of the previ-
ously extracted parallel set, which includes CLEF
2000-2008 topics from the AdHoc-main, AdHoc-
TEL and GeoCLEF tracks.
The number of queries obtained altogether is
shown in (Table 1).
</bodyText>
<sectionHeader confidence="0.520549" genericHeader="method">
4.1.2 Baseline
</sectionHeader>
<bodyText confidence="0.9993205">
We tested our approaches on the CLEF AdHoc-
TEL 2009 task (50 topics). This task dealt
with monolingual and cross-lingual search in a
library catalog. The monolingual retrieval is
</bodyText>
<footnote confidence="0.963700666666667">
3This alignment can be either produced by a toolkit like
GIZA++(Och and Ney, 2003) or obtained directly by a sys-
tem that produced the Nbest list of the translations (Moses).
</footnote>
<page confidence="0.984844">
113
</page>
<table confidence="0.999531571428572">
Language pair Number of queries
Total queries
En - Fr, Fr - En 470
En - De, De - En 714
Annotated queries
En - Fr, Fr - En 400
En - De, De - En 350
</table>
<tableCaption confidence="0.789636">
Table 1: Top: total number of parallel queries gathered
from all the CLEF tasks (size of the tuning set). Bot-
tom: number of queries extracted from the tasks for
which the human relevance judgements were availble
(size of the reranking training set).
</tableCaption>
<bodyText confidence="0.997694875">
performed with the lemur4 toolkit (Ogilvie and
Callan, 2001). The preprocessing includes lem-
matisation (with the Xerox Incremental Parser-
XIP (A¨ıt-Mokhtar et al., 2002)) and filtering out
the function words (based on XIP PoS tagging).
Table 2 shows the performance of the monolin-
gual retrieval model for each collection. The
monolingual retrieval results are comparable to
the CLEF AdHoc-TEL 2009 participants (Ferro
and Peters, 2009). Let us note here that it is not
the case for our CLIR results since we didn’t ex-
ploit the fact that each of the collections could ac-
tually contain the entries in a language other than
the official language of the collection.
The cross-lingual retrieval is performed as fol-
lows :
</bodyText>
<listItem confidence="0.958499333333333">
• the input query (eg. in English) is first trans-
lated into the language of the collection (eg.
German);
• this translation is used to search the target
collection (eg. Austrian National Library for
German ) .
</listItem>
<bodyText confidence="0.999757583333333">
The baseline translation is produced with
Moses trained on Europarl. Table 2 reports the
baseline performance both in terms of MT evalu-
ation metrics (BLEU) and Information Retrieval
evaluation metric MAP (Mean Average Preci-
sion).
The 1best MAP score corresponds to the case
when the single translation is proposed for the
retrieval by the query translation model. 5best
MAP score corresponds to the case when the 5
top translations proposed by the translation ser-
vice are concatenated and used for the retrieval.
</bodyText>
<footnote confidence="0.822449">
4http://www.lemurproject.org/
</footnote>
<bodyText confidence="0.999929375">
The 5best retrieval can be seen as a sort of query
expansion, without accessing the document col-
lection or any external resources.
Given that the query length is shorter than for a
standard sentence, the 4-gramm BLEU (used for
standart MT evaluation) might not be able to cap-
ture the difference between the translations (eg.
English-German 4-gramm BLEU is equal to 0 for
our task). For that reason we report both 3- and
4-gramm BLEU scores.
Note, that the French-English baseline retrieval
quality is much better than the German-English.
This is probably due to the fact that our German-
English translation system doesn’t use any de-
coumpounding, which results into many non-
translated words.
</bodyText>
<sectionHeader confidence="0.729813" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999993076923077">
We performed the query-genre adaptation ex-
periments for English-French, French-English,
German-English and English-German language
pairs.
Ideally, we would have liked to combine the
two approaches we proposed: use the query-
genre-tuned model to produce the Nbest list
which is then reranked to optimize the MAP
score. However, it was not possible in our exper-
imental settings due to the small amount of train-
ing data available. We thus simply compare these
two approaches to a baseline approach and com-
ment on their respective performance.
</bodyText>
<subsectionHeader confidence="0.667169">
4.2.1 Query-genre tuning approach
</subsectionHeader>
<bodyText confidence="0.999749">
For the CLEF-tuning experiments we used the
same translation model and language model as for
the baseline (Europarl-based). The weights were
then tuned on the CLEF topics described in sec-
tion 4.1.1. We then tested the system obtained on
50 parallel queries from the CLEF AdHoc-TEL
2009 task.
Table 3 describes the results of the evalua-
tion. We observe consistent 1-best MAP improve-
ments, but unstable BLEU (3-gramm) (improve-
ments for English-German, and degradation for
other language pairs), although one would have
expected BLEU to be improved in this experi-
mental setting given that BLEU was the objective
function for MERT. These results, on one side,
confirm the remark of (Kettunen, 2009) that there
is a correlation (although low) between BLEU
and MAP scores. The unstable BLEU scores
</bodyText>
<page confidence="0.996815">
114
</page>
<table confidence="0.999325">
MAP MAP MAP BLEU BLEU
1-best 5-best 4-gramm 3-gramm
Monolingual IR Bilingual IR
English 0.3159 French-English 0.1828 0.2186 0.1199 0.1568
German-English 0.0941 0.0942 0.2351 0.2923
French 0.2386 English-French 0.1504 0.1543 0.2863 0.3423
German 0.2162 English-German 0.1009 0.1157 0.0000 0.1218
</table>
<tableCaption confidence="0.970833">
Table 2: Baseline MAP scores for monolingual and bilingual CLEF AdHoc TEL 2009 task.
</tableCaption>
<table confidence="0.9999415">
MAP MAP BLEU BLEU
1-best 5-best 4-gramm 3-gramm
Fr-En 0.1954 0.2229 0.1062 0.1489
De-En 0.1018 0.1078 0.2240 0.2486
En-Fr 0.1611 0.1516 0.2072 0.2908
En-De 0.1062 0.1132 0.0000 0.1924
</table>
<tableCaption confidence="0.999789">
Table 3: BLEU and MAP performance on CLEF AdHoc TEL 2009 task for the genre-tuned model.
</tableCaption>
<bodyText confidence="0.999865315789474">
might also be explained by the small size of the
test set (compared to a standard test set of 1000
full-sentences).
Secondly, we looked at the weights of the fea-
tures both in the baseline model (Europarl-tuned)
and in the adapted model (CLEF-tuned), shown in
Table 4. We are unsure how suitable the sizes of
the CLEF tuning sets are, especially for the pairs
involving English and French. Nevertheless we
do observe and comment on some patterns.
For the pairs involving English and German
the distortion weight is much higher when tuning
with CLEF data compared to tuning with Europarl
data. The picture is reversed when looking at the
two pairs involving English and French. This is
to be expected if we interpret a high distortion
weight as follows: “it is not encouraged to place
source words that are near to each other far away
from each other in the translation”. Indeed, the lo-
cal reorderings are much more frequent between
English and French (e.g. white house = maison
blanche), while the long-distance reorderings are
more typcal between English and German.
The word penalty is consistenly higher over all
pairs when tuning with CLEF data compared to
tuning with Europarl data. We could see an ex-
planation for this pattern in the smaller size of
the CLEF sentences if we interpret higher word
penalty as a preference for shorter translations.
This can be explained both with the smaller aver-
age size of the queries and with the specific query
structure: mostly content words and fewer func-
tion words when compared to the full sentence.
The language model weight is consistently
though not drastically smaller when tuning with
CLEF data. We suppose that this is due to the
fact that a Europarl-base language model is not
the best choice for translating query data.
</bodyText>
<subsectionHeader confidence="0.980271">
4.2.2 Reranking approach
</subsectionHeader>
<bodyText confidence="0.999932956521739">
The reranking experiments include different
features combinations. First, we experiment with
the Moses features only in order to make this ap-
proach comparable with the first one. Secondly,
we compare different syntax-based features com-
binations, as described in section 3.2.2. Thus, we
compare the following reranking models (defined
by the feature set): moses, lex (lexical coupling
+ moses features), lab (label-specific coupling +
moses features), posmaplex (lexical PoS mapping
+ moses features ), lab-lex (label-specific cou-
pling + lexical coupling + moses features), lab-
lex-posmap (label-specific coupling + lexical cou-
pling features + generic PoS mapping). To reduce
the size of feature-functions vectors we take only
the 20 most frequent features in the training data
for Label-specific coupling and PoS mapping fea-
tures. The computation of the syntax features is
based on the rule-based XIP parser, where some
heuristics specific to query processing have been
integrated into English and French (but not Ger-
man) grammars (Brun et al., 2012).
The results of these experiments are illustrated
</bodyText>
<page confidence="0.996293">
115
</page>
<table confidence="0.999363">
Lng pair Tune set DW LM 0(f|e) lex(f|e) 0(e|f) lex(elf) PP WP
Fr-En Europarl 0.0801 0.1397 0.0431 0.0625 0.1463 0.0638 -0.0670 -0.3975
CLEF 0.0015 0.0795 -0.0046 0.0348 0.1977 0.0208 -0.2904 0.3707
De-En Europarl 0.0588 0.1341 0.0380 0.0181 0.1382 0.0398 -0.0904 -0.4822
CLEF 0.3568 0.1151 0.1168 0.0549 0.0932 0.0805 0.0391 -0.1434
En-Fr Europarl 0.0789 0.1373 0.0002 0.0766 0.1798 0.0293 -0.0978 -0.4002
CLEF 0.0322 0.1251 0.0350 0.1023 0.0534 0.0365 -0.3182 -0.2972
En-De Europarl 0.0584 0.1396 0.0092 0.0821 0.1823 0.0437 -0.1613 -0.3233
CLEF 0.3451 0.1001 0.0248 0.0872 0.2629 0.0153 -0.0431 0.1214
</table>
<tableCaption confidence="0.9910135">
Table 4: Feature weights for the query-genre tuned model. Abbreviations: DW - distortion weight, LM - language
model weight, PP - phrase penalty, WP - word penalty, 0-phrase translation probability, lex-lexical weighting.
</tableCaption>
<table confidence="0.999895952380952">
Query Example MAP bleu1
Src1 Weibliche M¨artyrer
Ref Female Martyrs
T1 female martyrs 0.07 1
T2 Women martyr 0.4 0
Src 2 Genmanipulation am
Menschen
Ref Human Gene Manipula-
tion
T1 On the genetic manipula- 0.044 0.167
tion of people
T2 genetic manipulation of 0.069 0.286
the human being
Src 3 Arbeitsrecht in der Eu-
rop¨aischen Union
Ref European Union Labour
Laws
T1 Labour law in the Euro- 0.015 0.5
pean Union
T2 labour legislation in the 0.036 0.5
European Union
</table>
<tableCaption confidence="0.763092333333333">
Table 5: Some examples of queries translations (T1:
baseline, T2: after reranking with lab-lex), MAP and
1-gramm BLEU scores for German-English.
</tableCaption>
<bodyText confidence="0.999987588235294">
in Figure 1. To keep the figure more readable,
we report only on 3-gramm BLEU scores. When
computing the 5best MAP score, the order in the
Nbest list is defined by a corresponding reranking
model. Each reranking model is illustrated by a
single horizontal red bar. We compare the rerank-
ing results to the baseline model (vertical line) and
also to the results of the first approach (yellow bar
labelled MERT:moses) on the same figure.
First, we remark that the adapted models
(query-genre tuning and reranking) outperform
the baseline in terms of MAP (1best and 5 best)
for French-English and German-English transla-
tions for most of the models. The only exception
is posmaplex model (based on PoS tagging) for
German which can be explained by the fact that
the German grammar used for query processing
was not adapted for queries as opposed to English
and French grammars. However, we do not ob-
serve the same tendency for BLEU score, where
only a few of the adapted models outperform the
baseline, which confirms the hypothesis of the
low correlation between BLEU and MAP scores
in these settings. Table 5 gives some examples of
the queries translations before (T1) and after (T2)
reranking. These examples also illustrate differ-
ent types of disagreement between MAP and 1-
gramm BLEU5 score.
The results for English-German and English-
French look more confusing. This can be partly
due to the more rich morphology of the target lan-
guages which may create more noise in the syn-
tax structure. Reranking however improves over
the 1-best MAP baseline for English-German, and
5-best MAP is also improved excluding the mod-
els involving PoS tagging for German (posmap,
posmaplex, lab-lex-posmap). The results for
English-French are more difficult to interpret. To
find out the reason of such a behavior, we looked
at the translations. We observed the following to-
kenization problem for French: the apostrophe is
systematically separated, e.g. “d ’ aujourd ’ hui”.
This leads to both noisy pre-retrieval preprocess-
ing (eg. d is tagged as a NOUN) and noisy syntax-
based feature values, which might explain the un-
stable results.
Finally, we can see that the syntax-based fea-
tures can be beneficial for the final retrieval qual-
ity: the models with syntax features can outper-
form the model basd on the moses features only.
The syntax-based features leading to the most sta-
</bodyText>
<footnote confidence="0.9543665">
5The higher order BLEU scores are equal to 0 for most
of the individual translations.
</footnote>
<page confidence="0.996152">
116
</page>
<figureCaption confidence="0.9956375">
Figure 1: Reranking results. The vertical line corresponds to the baseline scores. The lowest bar (MERT:moses,
in yellow): the results of the tuning approach, other bars(in red): the results of the reranking approach.
</figureCaption>
<bodyText confidence="0.999991333333333">
ble results seem to be lab-lex (combination of lex-
ical and label-specific coupling): it leads to the
best gains over 1-best and 5-best MAP for all lan-
guage pairs excluding English-French. This is a
surprising result given the fact that the underlying
IR model doesn’t take syntax into account in any
way. In our opinion, this is probably due to the
interaction between the pre-retrieval preprocess-
ing (lemmatisation, PoS tagging) done with the
linguistic tools which might produce noisy results
when applied to the SMT outputs. The rerank-
ing with syntax-based features allows to choose
a better-formed query for which the PoS tagging
and lemmatisation tools produce less noise which
leads to a better retrieval.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999765625">
In this work we proposed two methods for query-
genre adaptation of an SMT model: the first
method addressing the translation quality aspect
and the second one the retrieval precision aspect.
We have shown that CLIR performance in terms
of MAP is improved between 1-2.5 points. We
believe that the combination of these two meth-
ods would be the most beneficial setting, although
we were not able to prove this experimentally
(due to the lack of training data). None of these
methods require access to the document collec-
tion at test time, and can be used in the context
of a query translation service. The combination
of our adapted SMT model with other state-of-the
art CLIR techniques (eg. query expansion with
PRF) will be explored in future work.
</bodyText>
<sectionHeader confidence="0.995113" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9568094">
This research was supported by the European
Union’s ICT Policy Support Programme as part of
the Competitiveness and Innovation Framework
Programme, CIP ICT-PSP under grant agreement
nr 250430 (Project GALATEAS).
</bodyText>
<sectionHeader confidence="0.992934" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.833778">
Salah A¨ıt-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2002. Robustness beyond shallowness: in-
</reference>
<page confidence="0.992495">
117
</page>
<reference confidence="0.998387869565217">
cremental deep parsing. Natural Language Engi-
neering, 8:121–144, June.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
an automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65–72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Adam Berger, John Lafferty, and John La Erty. 1999.
The weaver system for document retrieval. In In
Proceedings of the Eighth Text REtrieval Confer-
ence (TREC-8, pages 163–174.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of
the Fourth Workshop on Statistical Machine Trans-
lation, pages 182–189. Association for Computa-
tional Linguistics.
Caroline Brun, Vassilina Nikoulina, and Nikolaos La-
gos. 2012. Linguistically-adapted structural query
annotation for digital libraries in the social sciences.
In Proceedings of the 6th EACL Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities, Avignon, France, April.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1–28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 224–233. Association
for Computational Linguistics.
St´ephane Clinchant and Jean-Michel Renders. 2007.
Query translation through dictionary adaptation. In
CLEF’07, pages 182–187.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In ACL ’04:
Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics.
Michael Collins. 2001. Ranking algorithms for
named-entity extraction: boosting and the voted
perceptron. In ACL’02: Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 489–496, Philadelphia, Pennsyl-
vania. Association for Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951–991.
George Doddington. 2002. Automatic evaluation
of Machine Translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, pages 138–145, San Diego,
California. Morgan Kaufmann Publishers Inc.
Nicola Ferro and Carol Peters. 2009. CLEF 2009
ad hoc track overview: TEL and persian tasks.
In Working Notes for the CLEF 2009 Workshop,
Corfu, Greece.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2009. Evaluating effects of ma-
chine translation accuracy on cross-lingual patent
retrieval. In Proceedings of the 32nd international
ACM SIGIR conference on Research and develop-
ment in information retrieval, SIGIR ’09, pages
674–675.
Jianfeng Gao, Jian-Yun Nie, and Ming Zhou. 2006.
Statistical query translation models for cross-
language information retrieval. 5:323–359, Decem-
ber.
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Kam-
Fai Wong, and Hsiao-Wuen Hon. 2010. Exploit-
ing query logs for cross-lingual query suggestions.
ACM Trans. Inf. Syst., 28(2).
Djoerd Hiemstra and Franciska de Jong. 1999. Dis-
ambiguation strategies for cross-language informa-
tion retrieval. In Proceedings of the Third European
Conference on Research and Advanced Technology
for Digital Libraries, pages 274–293.
Rong Hu, Weizhu Chen, Peng Bai, Yansheng Lu,
Zheng Chen, and Qiang Yang. 2008. Web query
translation via web log mining. In Proceedings of
the 31st annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ’08, pages 749–750. ACM.
Amir Hossein Jadidinejad and Fariborz Mahmoudi.
2009. Cross-language information retrieval us-
ing meta-language index construction and structural
queries. In Proceedings of the 10th cross-language
evaluation forum conference on Multilingual in-
formation access evaluation: text retrieval experi-
ments, CLEF’09, pages 70–77, Berlin, Heidelberg.
Springer-Verlag.
Gareth Jones, Sakai Tetsuya, Nigel Collier, Akira Ku-
mano, and Kazuo Sumita. 1999. Exploring the
use of machine translation resources for english-
japanese cross-language information retrieval. In In
Proceedings of MT Summit VII Workshop on Ma-
chine Translation for Cross Language Information
Retrieval, pages 181–188.
Kimmo Kettunen. 2009. Choosing the best mt pro-
grams for clir purposes — can mt metrics be help-
ful? In Proceedings of the 31th European Confer-
ence on IR Research on Advances in Information
Retrieval, ECIR ’09, pages 706–712, Berlin, Hei-
delberg. Springer-Verlag.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
</reference>
<page confidence="0.984541">
118
</page>
<reference confidence="0.999908383177569">
’07, pages 224–227. Association for Computational
Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48–54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In ACL ’07: Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, pages 177–180. As-
sociation for Computational Linguistics.
Philip Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Wessel Kraaij, Jian-Yun Nie, and Michel Simard.
2003. Embedding web-based statistical trans-
lation models in cross-language information re-
trieval. Computational Linguistiques, 29:381–419,
September.
Jian-yun Nie and Jiang Chen. 2002. Exploiting the
web as parallel corpora for cross-language informa-
tion retrieval. Web Intelligence, pages 218–239.
Jian-Yun Nie. 2010. Cross-Language Information Re-
trieval. Morgan &amp; Claypool Publishers.
Vassilina Nikoulina and Marc Dymetman. 2008. Ex-
periments in discriminating phrase-based transla-
tions on the basis of syntactic coupling features. In
Proceedings of the ACL-08: HLT Second Workshop
on Syntax and Structure in Statistical Translation
(SSST-2), pages 55–60. Association for Computa-
tional Linguistics, June.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003.
Syntax for Statistical Machine Translation: Final
report of John Hopkins 2003 Summer Workshop.
Technical report, John Hopkins University.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ’03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160–
167, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Paul Ogilvie and James P. Callan. 2001. Experiments
using the lemur toolkit. In TREC.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation.
Pavel Pecina, Antonio Toral, Andy Way, Vassilis Pa-
pavassiliou, Prokopis Prokopidis, and Maria Gi-
agkou. 2011. Towards using web-crawled data for
domain adaptation in statistical machine translation.
In Proceedings of the 15th Annual Conference of
the European Associtation forMachine Translation,
pages 297–304, Leuven, Belgium. European Asso-
ciation for Machine Translation.
Brian Roark, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proceedings of the 42nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL’04), July.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of the 13th Annual Confer-
ence of the EAMT, page 28–35, Barcelona, Spain.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 764–773, Prague, Czech Republic.
Association for Computational Linguistics.
Dan Wu and Daqing He. 2010. A study of query
translation using google machine translation sys-
tem. Computational Intelligence and Software En-
gineering (CiSE).
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the 22nd International
Conference on Computational Linguistics (Col-
ing2008), pages 993–100.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the 20th international conference on
Computational Linguistics, COLING ’04. Associ-
ation for Computational Linguistics.
Zhongguang Zheng, Zhongjun He, Yao Meng, and
Hao Yu. 2010. Domain adaptation for statisti-
cal machine translation in development corpus se-
lection. In Universal Communication Symposium
(IUCS), 2010 4th International, pages 2–7. IEEE.
</reference>
<page confidence="0.999086">
119
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.036671">
<title confidence="0.854606666666667">Adaptation of Statistical Machine Translation Model for Information Retrieval in a Service Context Vassilina</title>
<author confidence="0.604212">Xerox Research Center Europe</author>
<email confidence="0.99725">vassilina.nikoulina@xrce.xerox.com</email>
<author confidence="0.962357">Nikolaos Lagos</author>
<affiliation confidence="0.99706">Xerox Research Center</affiliation>
<email confidence="0.685917">nikolaos.lagos@xrce.xerox.comBogomil</email>
<affiliation confidence="0.845552">Informatics University of</affiliation>
<email confidence="0.813002">B.K.Kovachev@uva.nl</email>
<author confidence="0.258598">Christof</author>
<affiliation confidence="0.774461">Informatics University of</affiliation>
<email confidence="0.841047">C.Monz@uva.nl</email>
<abstract confidence="0.998578">This work proposes to adapt an existing general SMT model for the task of translating queries that are subsequently going to be used to retrieve information from a target language collection. In the scenario that we focus on access to the document collection itself is not available and changes to the IR model are not possible. We propose two ways to achieve the adaptation effect and both of them are aimed at tuning parameter weights on a set of parallel queries. The first approach is via a standard tuning procedure optimizing for BLEU score and the second one is via a reranking approach optimizing for MAP score. We also extend the second approach by using syntax-based features. Our experiments show improvements of 1-2.5 in terms of MAP score over the retrieval with the non-adapted translation. We show that these improvements are due both to the integration of the adaptation and syntax-features for the query translation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Salah A¨ıt-Mokhtar</author>
<author>Jean-Pierre Chanod</author>
<author>Claude Roux</author>
</authors>
<title>Robustness beyond shallowness: incremental deep parsing.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<pages>8--121</pages>
<marker>A¨ıt-Mokhtar, Chanod, Roux, 2002</marker>
<rawString>Salah A¨ıt-Mokhtar, Jean-Pierre Chanod, and Claude Roux. 2002. Robustness beyond shallowness: incremental deep parsing. Natural Language Engineering, 8:121–144, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: an automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="9319" citStr="Banerjee and Lavie, 2005" startWordPosition="1470" endWordPosition="1473">ome linguistic preprocessing (eg. lemmatisation, PoS tagging) which in interaction with badly-formed translations might bring some noise. A couple of works studied the correlation between the standard MT evaluation metrics and the retrieval precision. Thus, (Fujii et al., 2009) showed a good correlation of the BLEU scores with the MAP scores for Cross-Lingual Patent Retrieval. However, the topics in patent search (long and well structured) are very different from standard queries. (Kettunen, 2009) also found a pretty high correlation ( 0.8 − 0.9) between standard MT evaluation metrics (METEOR(Banerjee and Lavie, 2005), BLEU, NIST(Doddington, 2002)) and retrieval precision for long queries. However, the same work shows that the correlation decreases ( 0.6 − 0.7) for short queries. In this paper we propose two approaches to SMT adaptation for queries. The first one optimizes BLEU, while the second one optimizes Mean Average Precision (MAP), a standard metric in information retrieval. We’ll address the issue of the correlation between BLEU and MAP in Section 4. Both of the proposed approaches rely on the phrase-based SMT (PBMT) model (Koehn et al., 2003) implemented in the Open Source SMT toolkit MOSES (Koehn</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>John Lafferty</author>
<author>John La Erty</author>
</authors>
<title>The weaver system for document retrieval. In</title>
<date>1999</date>
<booktitle>In Proceedings of the Eighth Text REtrieval Conference (TREC-8,</booktitle>
<pages>163--174</pages>
<contexts>
<context position="5610" citStr="Berger et al., 1999" startWordPosition="884" endWordPosition="887">ay distinguish two main groups of approaches to CLIR: document translation and query translation. We concentrate on the second group which is more relevant to our settings. The standard query translation methods use different translation resources such as bilingual dictionaries, parallel corpora and/or machine translation. The aspect of disambiguation is important for the first two techniques. Different methods were proposed to deal with disambiguation issues, often relying on the document collection or embedding the translation step directly into the retrieval model (Hiemstra and Jong, 1999; Berger et al., 1999; Kraaij et al., 2003). Other methods rely on external resources like query logs (Gao et al., 2010), Wikipedia (Jadidinejad and Mahmoudi, 2009) or the web (Nie and Chen, 2002; Hu et al., 2008). (Gao et al., 2006) proposes syntax-based translation models to deal with the disambiguation issues (NP-based, dependency-based). The candidate translations proposed by these models are then reranked with the model learned to minimize the translation er2Standard MT evaluation metric ror on the training data. To our knowledge, existing work that use MTbased techniques for query translation use an outof-th</context>
</contexts>
<marker>Berger, Lafferty, Erty, 1999</marker>
<rawString>Adam Berger, John Lafferty, and John La Erty. 1999. The weaver system for document retrieval. In In Proceedings of the Eighth Text REtrieval Conference (TREC-8, pages 163–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain adaptation for statistical machine translation with monolingual resources.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>182--189</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7289" citStr="Bertoldi and Federico, 2009" startWordPosition="1153" endWordPosition="1156">main. Thus, we consider genre to be the general style of the text e.g. conversation, news, blog, query (responsible mostly for the text structure) while the domain reflects more what the text is about – eg. social science, healthcare, history, so domain adaptation involves lexical disambiguation and extra lexical coverage problems. To our knowledge, there is not much work addressing explicitly the problem of genre adaptation for SMT. Some work done on domain adaptation could be applied to genre adaptation, such as incorporating available in-domain corpora in the SMT model: either monolingual (Bertoldi and Federico, 2009; Wu et al., 2008; Zhao et al., 2004; Koehn and Schroeder, 2007), or small parallel data used for tuning the SMT parameters (Zheng et al., 2010; Pecina et al., 2011). 3 Our approach This work is based on the hypothesis that the general-purpose SMT system needs to be adapted for query translation. Although in (Ferro and Peters, 2009) it has been mentioned that using Google translate (general-purpose MT) for query translation allowed to CLEF participants to obtain the best CLIR performance, there is still 10% gap between monolingual and cross-lingual IR. We believe that, as in (Clinchant and Ren</context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolingual resources. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 182–189. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Brun</author>
<author>Vassilina Nikoulina</author>
<author>Nikolaos Lagos</author>
</authors>
<title>Linguistically-adapted structural query annotation for digital libraries in the social sciences.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities,</booktitle>
<location>Avignon, France,</location>
<contexts>
<context position="28568" citStr="Brun et al., 2012" startWordPosition="4636" endWordPosition="4639"> + moses features), posmaplex (lexical PoS mapping + moses features ), lab-lex (label-specific coupling + lexical coupling + moses features), lablex-posmap (label-specific coupling + lexical coupling features + generic PoS mapping). To reduce the size of feature-functions vectors we take only the 20 most frequent features in the training data for Label-specific coupling and PoS mapping features. The computation of the syntax features is based on the rule-based XIP parser, where some heuristics specific to query processing have been integrated into English and French (but not German) grammars (Brun et al., 2012). The results of these experiments are illustrated 115 Lng pair Tune set DW LM 0(f|e) lex(f|e) 0(e|f) lex(elf) PP WP Fr-En Europarl 0.0801 0.1397 0.0431 0.0625 0.1463 0.0638 -0.0670 -0.3975 CLEF 0.0015 0.0795 -0.0046 0.0348 0.1977 0.0208 -0.2904 0.3707 De-En Europarl 0.0588 0.1341 0.0380 0.0181 0.1382 0.0398 -0.0904 -0.4822 CLEF 0.3568 0.1151 0.1168 0.0549 0.0932 0.0805 0.0391 -0.1434 En-Fr Europarl 0.0789 0.1373 0.0002 0.0766 0.1798 0.0293 -0.0978 -0.4002 CLEF 0.0322 0.1251 0.0350 0.1023 0.0534 0.0365 -0.3182 -0.2972 En-De Europarl 0.0584 0.1396 0.0092 0.0821 0.1823 0.0437 -0.1613 -0.3233 CLE</context>
</contexts>
<marker>Brun, Nikoulina, Lagos, 2012</marker>
<rawString>Caroline Brun, Vassilina Nikoulina, and Nikolaos Lagos. 2012. Linguistically-adapted structural query annotation for digital libraries in the social sciences. In Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, Avignon, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--28</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="13583" citStr="Callison-Burch et al., 2009" startWordPosition="2166" endWordPosition="2169">First of all the average query length is ˜5 words, which means that the Nbest list of the translations is more diverse than in the case of general phrase translation (average length 25-30 words). Moreover, the retrieval precision is more naturally integrated into the reranking framework than standard MT evaluation metrics such as BLEU. The main reason is that the notion of Average Retrieval Precision is well defined for a single query translation, while BLEU is defined on the corpus level and correlates poorly with human quality judgements for the individual translations (Specia et al., 2009; Callison-Burch et al., 2009). Finally, the reranking framework allows a lot of flexibility. Thus, it allows enriching the baseline translation model with new complex features which might be difficult to introduce into the translation model directly. Other works applied the reranking framework to different NLP tasks such as Named Entities Extraction (Collins, 2001), parsing (Collins and Roark, 2004), and language modelling (Roark et al., 2004). Most of these works used the reranking framework to combine generative and discriminative methods when both approaches aim at solving the same problem: the generative model produce</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 1–28, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>224--233</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16415" citStr="Chiang et al., 2008" startWordPosition="2630" endWordPosition="2633"> each query qi, i = 1..K. • Each translation t E GEN(qi) is used to perform a retrieval from a target document collection, and an Average Precision score (AP(t)) is computed for each t E GEN(qi) by comparing its retrieval to the relevance annotations done during the CLEF campaign. The weights λ are learned with the objective of maximizing MAP for all the queries of the training set, and, therefore, are optimized for retrieval quality. The weights optimization is done with the Margin Infused Relaxed Algorithm (MIRA)(Crammer and Singer, 2003), which was applied to SMT by (Watanabe et al., 2007; Chiang et al., 2008). MIRA is an online learning algorithm where each weights update is done to keep the new weights as close as possible to the old weights (first term), and score oracle translation (the translation giving the best retrieval score : tz = arg maxt AP(t)) higher than each non-oracle translation (tij) by a margin at least as wide as the loss lij (second term): λ= minλ&apos; 12I Iλ&apos; − λI I2 + \ ) C �K i=1 maxj=1..N lij − λ&apos; - (F(t� i) − F(tij) The loss lij is defined as the difference in the retrieval average precision between the oracle and non-oracle translations: lij = AP(tz) − AP(tij). C is the regul</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 224–233. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane Clinchant</author>
<author>Jean-Michel Renders</author>
</authors>
<title>Query translation through dictionary adaptation.</title>
<date>2007</date>
<booktitle>In CLEF’07,</booktitle>
<pages>182--187</pages>
<contexts>
<context position="7900" citStr="Clinchant and Renders, 2007" startWordPosition="1253" endWordPosition="1256">nd Federico, 2009; Wu et al., 2008; Zhao et al., 2004; Koehn and Schroeder, 2007), or small parallel data used for tuning the SMT parameters (Zheng et al., 2010; Pecina et al., 2011). 3 Our approach This work is based on the hypothesis that the general-purpose SMT system needs to be adapted for query translation. Although in (Ferro and Peters, 2009) it has been mentioned that using Google translate (general-purpose MT) for query translation allowed to CLEF participants to obtain the best CLIR performance, there is still 10% gap between monolingual and cross-lingual IR. We believe that, as in (Clinchant and Renders, 2007), more adapted query translation, possibly further combined with query expansion techniques, can lead to improved retrieval. The problem of the SMT adaptation for querygenre translation has different quality aspects. On the one hand, we want our model to produce a “good” translation (well-formed and transmitting the information contained in the source query) of an input query. On the other hand, we want to obtain good retrieval performance using 110 the proposed translation. These two aspects are not necessarily correlated: a bag-of-word translation can lead to good retrieval performance, even</context>
</contexts>
<marker>Clinchant, Renders, 2007</marker>
<rawString>St´ephane Clinchant and Jean-Michel Renders. 2007. Query translation through dictionary adaptation. In CLEF’07, pages 182–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="13956" citStr="Collins and Roark, 2004" startWordPosition="2221" endWordPosition="2224">verage Retrieval Precision is well defined for a single query translation, while BLEU is defined on the corpus level and correlates poorly with human quality judgements for the individual translations (Specia et al., 2009; Callison-Burch et al., 2009). Finally, the reranking framework allows a lot of flexibility. Thus, it allows enriching the baseline translation model with new complex features which might be difficult to introduce into the translation model directly. Other works applied the reranking framework to different NLP tasks such as Named Entities Extraction (Collins, 2001), parsing (Collins and Roark, 2004), and language modelling (Roark et al., 2004). Most of these works used the reranking framework to combine generative and discriminative methods when both approaches aim at solving the same problem: the generative model produces a set of hypotheses, and the best hypothesis is chosen afterwards via the discriminative reranking model, which allows to enrich the baseline model with the new complex and heterogeneous features. We suggest using the reranking framework to combine two different tasks: Machine Translation and Cross-lingual Information Retrieval. In this context the reranking framework </context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking algorithms for named-entity extraction: boosting and the voted perceptron.</title>
<date>2001</date>
<booktitle>In ACL’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>489--496</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="13921" citStr="Collins, 2001" startWordPosition="2218" endWordPosition="2219">n is that the notion of Average Retrieval Precision is well defined for a single query translation, while BLEU is defined on the corpus level and correlates poorly with human quality judgements for the individual translations (Specia et al., 2009; Callison-Burch et al., 2009). Finally, the reranking framework allows a lot of flexibility. Thus, it allows enriching the baseline translation model with new complex features which might be difficult to introduce into the translation model directly. Other works applied the reranking framework to different NLP tasks such as Named Entities Extraction (Collins, 2001), parsing (Collins and Roark, 2004), and language modelling (Roark et al., 2004). Most of these works used the reranking framework to combine generative and discriminative methods when both approaches aim at solving the same problem: the generative model produces a set of hypotheses, and the best hypothesis is chosen afterwards via the discriminative reranking model, which allows to enrich the baseline model with the new complex and heterogeneous features. We suggest using the reranking framework to combine two different tasks: Machine Translation and Cross-lingual Information Retrieval. In th</context>
</contexts>
<marker>Collins, 2001</marker>
<rawString>Michael Collins. 2001. Ranking algorithms for named-entity extraction: boosting and the voted perceptron. In ACL’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 489–496, Philadelphia, Pennsylvania. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="16341" citStr="Crammer and Singer, 2003" startWordPosition="2616" endWordPosition="2619"> take N = 1000) translations (GEN(qi)) is produced by the baseline MT model for each query qi, i = 1..K. • Each translation t E GEN(qi) is used to perform a retrieval from a target document collection, and an Average Precision score (AP(t)) is computed for each t E GEN(qi) by comparing its retrieval to the relevance annotations done during the CLEF campaign. The weights λ are learned with the objective of maximizing MAP for all the queries of the training set, and, therefore, are optimized for retrieval quality. The weights optimization is done with the Margin Infused Relaxed Algorithm (MIRA)(Crammer and Singer, 2003), which was applied to SMT by (Watanabe et al., 2007; Chiang et al., 2008). MIRA is an online learning algorithm where each weights update is done to keep the new weights as close as possible to the old weights (first term), and score oracle translation (the translation giving the best retrieval score : tz = arg maxt AP(t)) higher than each non-oracle translation (tij) by a margin at least as wide as the loss lij (second term): λ= minλ&apos; 12I Iλ&apos; − λI I2 + \ ) C �K i=1 maxj=1..N lij − λ&apos; - (F(t� i) − F(tij) The loss lij is defined as the difference in the retrieval average precision between the </context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of Machine Translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Diego, California.</location>
<contexts>
<context position="9349" citStr="Doddington, 2002" startWordPosition="1475" endWordPosition="1476">atisation, PoS tagging) which in interaction with badly-formed translations might bring some noise. A couple of works studied the correlation between the standard MT evaluation metrics and the retrieval precision. Thus, (Fujii et al., 2009) showed a good correlation of the BLEU scores with the MAP scores for Cross-Lingual Patent Retrieval. However, the topics in patent search (long and well structured) are very different from standard queries. (Kettunen, 2009) also found a pretty high correlation ( 0.8 − 0.9) between standard MT evaluation metrics (METEOR(Banerjee and Lavie, 2005), BLEU, NIST(Doddington, 2002)) and retrieval precision for long queries. However, the same work shows that the correlation decreases ( 0.6 − 0.7) for short queries. In this paper we propose two approaches to SMT adaptation for queries. The first one optimizes BLEU, while the second one optimizes Mean Average Precision (MAP), a standard metric in information retrieval. We’ll address the issue of the correlation between BLEU and MAP in Section 4. Both of the proposed approaches rely on the phrase-based SMT (PBMT) model (Koehn et al., 2003) implemented in the Open Source SMT toolkit MOSES (Koehn et al., 2007). 3.1 Tuning for</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of Machine Translation quality using n-gram cooccurrence statistics. In Proceedings of the second international conference on Human Language Technology Research, pages 138–145, San Diego, California. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ferro</author>
<author>Carol Peters</author>
</authors>
<title>CLEF 2009 ad hoc track overview: TEL and persian tasks.</title>
<date>2009</date>
<booktitle>In Working Notes for the CLEF 2009 Workshop,</booktitle>
<location>Corfu, Greece.</location>
<contexts>
<context position="2274" citStr="Ferro and Peters, 2009" startWordPosition="344" endWordPosition="347">ls, nor to provide access to their document collection by a third-party external service. The work presented in this paper assumes such a context of use, where a query translation service allows translating queries posed to the search engine of a content provider into several target languages, without requiring changes to the undelying IR system used and without accessing, at translation time, the content provider’s document set. Keeping in mind these constraints, we present two approaches on query translation optimisation. One of the important observations done during the CLEF 2009 campaign (Ferro and Peters, 2009) related to CLIR was that the usage of Statistical Machine Translation (SMT) systems (eg. Google Translate) for query translation led to important improvements in the cross-lingual retrieval performance (the best CLIR performance increased from ˜55% of the monolingual baseline in 2008 to more than 90% in 2009 for French and German target languages). However, generalpurpose SMT systems are not necessarily adapted for query translation. That is because SMT systems trained on a corpus of standard parallel phrases take into account the phrase structure implicitly. The structure of queries is very </context>
<context position="7623" citStr="Ferro and Peters, 2009" startWordPosition="1211" endWordPosition="1214">ur knowledge, there is not much work addressing explicitly the problem of genre adaptation for SMT. Some work done on domain adaptation could be applied to genre adaptation, such as incorporating available in-domain corpora in the SMT model: either monolingual (Bertoldi and Federico, 2009; Wu et al., 2008; Zhao et al., 2004; Koehn and Schroeder, 2007), or small parallel data used for tuning the SMT parameters (Zheng et al., 2010; Pecina et al., 2011). 3 Our approach This work is based on the hypothesis that the general-purpose SMT system needs to be adapted for query translation. Although in (Ferro and Peters, 2009) it has been mentioned that using Google translate (general-purpose MT) for query translation allowed to CLEF participants to obtain the best CLIR performance, there is still 10% gap between monolingual and cross-lingual IR. We believe that, as in (Clinchant and Renders, 2007), more adapted query translation, possibly further combined with query expansion techniques, can lead to improved retrieval. The problem of the SMT adaptation for querygenre translation has different quality aspects. On the one hand, we want our model to produce a “good” translation (well-formed and transmitting the infor</context>
<context position="21963" citStr="Ferro and Peters, 2009" startWordPosition="3570" endWordPosition="3573">all the CLEF tasks (size of the tuning set). Bottom: number of queries extracted from the tasks for which the human relevance judgements were availble (size of the reranking training set). performed with the lemur4 toolkit (Ogilvie and Callan, 2001). The preprocessing includes lemmatisation (with the Xerox Incremental ParserXIP (A¨ıt-Mokhtar et al., 2002)) and filtering out the function words (based on XIP PoS tagging). Table 2 shows the performance of the monolingual retrieval model for each collection. The monolingual retrieval results are comparable to the CLEF AdHoc-TEL 2009 participants (Ferro and Peters, 2009). Let us note here that it is not the case for our CLIR results since we didn’t exploit the fact that each of the collections could actually contain the entries in a language other than the official language of the collection. The cross-lingual retrieval is performed as follows : • the input query (eg. in English) is first translated into the language of the collection (eg. German); • this translation is used to search the target collection (eg. Austrian National Library for German ) . The baseline translation is produced with Moses trained on Europarl. Table 2 reports the baseline performance</context>
</contexts>
<marker>Ferro, Peters, 2009</marker>
<rawString>Nicola Ferro and Carol Peters. 2009. CLEF 2009 ad hoc track overview: TEL and persian tasks. In Working Notes for the CLEF 2009 Workshop, Corfu, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Masao Utiyama</author>
<author>Mikio Yamamoto</author>
<author>Takehito Utsuro</author>
</authors>
<title>Evaluating effects of machine translation accuracy on cross-lingual patent retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’09,</booktitle>
<pages>674--675</pages>
<contexts>
<context position="8972" citStr="Fujii et al., 2009" startWordPosition="1416" endWordPosition="1419"> the proposed translation. These two aspects are not necessarily correlated: a bag-of-word translation can lead to good retrieval performance, even though it won’t be syntactically well-formed; at the same time a well-formed translation can lead to worse retrieval if the wrong lexical choice is done. Moreover, often the retrieval demands some linguistic preprocessing (eg. lemmatisation, PoS tagging) which in interaction with badly-formed translations might bring some noise. A couple of works studied the correlation between the standard MT evaluation metrics and the retrieval precision. Thus, (Fujii et al., 2009) showed a good correlation of the BLEU scores with the MAP scores for Cross-Lingual Patent Retrieval. However, the topics in patent search (long and well structured) are very different from standard queries. (Kettunen, 2009) also found a pretty high correlation ( 0.8 − 0.9) between standard MT evaluation metrics (METEOR(Banerjee and Lavie, 2005), BLEU, NIST(Doddington, 2002)) and retrieval precision for long queries. However, the same work shows that the correlation decreases ( 0.6 − 0.7) for short queries. In this paper we propose two approaches to SMT adaptation for queries. The first one op</context>
</contexts>
<marker>Fujii, Utiyama, Yamamoto, Utsuro, 2009</marker>
<rawString>Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and Takehito Utsuro. 2009. Evaluating effects of machine translation accuracy on cross-lingual patent retrieval. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’09, pages 674–675.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Jian-Yun Nie</author>
<author>Ming Zhou</author>
</authors>
<title>Statistical query translation models for crosslanguage information retrieval.</title>
<date>2006</date>
<pages>5--323</pages>
<contexts>
<context position="5822" citStr="Gao et al., 2006" startWordPosition="922" endWordPosition="925">e different translation resources such as bilingual dictionaries, parallel corpora and/or machine translation. The aspect of disambiguation is important for the first two techniques. Different methods were proposed to deal with disambiguation issues, often relying on the document collection or embedding the translation step directly into the retrieval model (Hiemstra and Jong, 1999; Berger et al., 1999; Kraaij et al., 2003). Other methods rely on external resources like query logs (Gao et al., 2010), Wikipedia (Jadidinejad and Mahmoudi, 2009) or the web (Nie and Chen, 2002; Hu et al., 2008). (Gao et al., 2006) proposes syntax-based translation models to deal with the disambiguation issues (NP-based, dependency-based). The candidate translations proposed by these models are then reranked with the model learned to minimize the translation er2Standard MT evaluation metric ror on the training data. To our knowledge, existing work that use MTbased techniques for query translation use an outof-the-box MT system, without adapting it for query translation in particular (Jones et al., 1999; Wu et al., 2008) (although some query expansion techniques might be applied to the produced translation afterwards (Wu</context>
</contexts>
<marker>Gao, Nie, Zhou, 2006</marker>
<rawString>Jianfeng Gao, Jian-Yun Nie, and Ming Zhou. 2006. Statistical query translation models for crosslanguage information retrieval. 5:323–359, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Gao</author>
<author>Cheng Niu</author>
<author>Jian-Yun Nie</author>
<author>Ming Zhou</author>
<author>KamFai Wong</author>
<author>Hsiao-Wuen Hon</author>
</authors>
<title>Exploiting query logs for cross-lingual query suggestions.</title>
<date>2010</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="5709" citStr="Gao et al., 2010" startWordPosition="901" endWordPosition="904">concentrate on the second group which is more relevant to our settings. The standard query translation methods use different translation resources such as bilingual dictionaries, parallel corpora and/or machine translation. The aspect of disambiguation is important for the first two techniques. Different methods were proposed to deal with disambiguation issues, often relying on the document collection or embedding the translation step directly into the retrieval model (Hiemstra and Jong, 1999; Berger et al., 1999; Kraaij et al., 2003). Other methods rely on external resources like query logs (Gao et al., 2010), Wikipedia (Jadidinejad and Mahmoudi, 2009) or the web (Nie and Chen, 2002; Hu et al., 2008). (Gao et al., 2006) proposes syntax-based translation models to deal with the disambiguation issues (NP-based, dependency-based). The candidate translations proposed by these models are then reranked with the model learned to minimize the translation er2Standard MT evaluation metric ror on the training data. To our knowledge, existing work that use MTbased techniques for query translation use an outof-the-box MT system, without adapting it for query translation in particular (Jones et al., 1999; Wu et</context>
</contexts>
<marker>Gao, Niu, Nie, Zhou, Wong, Hon, 2010</marker>
<rawString>Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, KamFai Wong, and Hsiao-Wuen Hon. 2010. Exploiting query logs for cross-lingual query suggestions. ACM Trans. Inf. Syst., 28(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djoerd Hiemstra</author>
<author>Franciska de Jong</author>
</authors>
<title>Disambiguation strategies for cross-language information retrieval.</title>
<date>1999</date>
<booktitle>In Proceedings of the Third European Conference on Research and Advanced Technology for Digital Libraries,</booktitle>
<pages>274--293</pages>
<marker>Hiemstra, de Jong, 1999</marker>
<rawString>Djoerd Hiemstra and Franciska de Jong. 1999. Disambiguation strategies for cross-language information retrieval. In Proceedings of the Third European Conference on Research and Advanced Technology for Digital Libraries, pages 274–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Hu</author>
<author>Weizhu Chen</author>
<author>Peng Bai</author>
<author>Yansheng Lu</author>
<author>Zheng Chen</author>
<author>Qiang Yang</author>
</authors>
<title>Web query translation via web log mining.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’08,</booktitle>
<pages>749--750</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5802" citStr="Hu et al., 2008" startWordPosition="918" endWordPosition="921">nslation methods use different translation resources such as bilingual dictionaries, parallel corpora and/or machine translation. The aspect of disambiguation is important for the first two techniques. Different methods were proposed to deal with disambiguation issues, often relying on the document collection or embedding the translation step directly into the retrieval model (Hiemstra and Jong, 1999; Berger et al., 1999; Kraaij et al., 2003). Other methods rely on external resources like query logs (Gao et al., 2010), Wikipedia (Jadidinejad and Mahmoudi, 2009) or the web (Nie and Chen, 2002; Hu et al., 2008). (Gao et al., 2006) proposes syntax-based translation models to deal with the disambiguation issues (NP-based, dependency-based). The candidate translations proposed by these models are then reranked with the model learned to minimize the translation er2Standard MT evaluation metric ror on the training data. To our knowledge, existing work that use MTbased techniques for query translation use an outof-the-box MT system, without adapting it for query translation in particular (Jones et al., 1999; Wu et al., 2008) (although some query expansion techniques might be applied to the produced transl</context>
</contexts>
<marker>Hu, Chen, Bai, Lu, Chen, Yang, 2008</marker>
<rawString>Rong Hu, Weizhu Chen, Peng Bai, Yansheng Lu, Zheng Chen, and Qiang Yang. 2008. Web query translation via web log mining. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’08, pages 749–750. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Hossein Jadidinejad</author>
<author>Fariborz Mahmoudi</author>
</authors>
<title>Cross-language information retrieval using meta-language index construction and structural queries.</title>
<date>2009</date>
<booktitle>In Proceedings of the 10th cross-language evaluation forum conference on Multilingual information access evaluation: text retrieval experiments, CLEF’09,</booktitle>
<pages>70--77</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="5753" citStr="Jadidinejad and Mahmoudi, 2009" startWordPosition="906" endWordPosition="910">p which is more relevant to our settings. The standard query translation methods use different translation resources such as bilingual dictionaries, parallel corpora and/or machine translation. The aspect of disambiguation is important for the first two techniques. Different methods were proposed to deal with disambiguation issues, often relying on the document collection or embedding the translation step directly into the retrieval model (Hiemstra and Jong, 1999; Berger et al., 1999; Kraaij et al., 2003). Other methods rely on external resources like query logs (Gao et al., 2010), Wikipedia (Jadidinejad and Mahmoudi, 2009) or the web (Nie and Chen, 2002; Hu et al., 2008). (Gao et al., 2006) proposes syntax-based translation models to deal with the disambiguation issues (NP-based, dependency-based). The candidate translations proposed by these models are then reranked with the model learned to minimize the translation er2Standard MT evaluation metric ror on the training data. To our knowledge, existing work that use MTbased techniques for query translation use an outof-the-box MT system, without adapting it for query translation in particular (Jones et al., 1999; Wu et al., 2008) (although some query expansion t</context>
</contexts>
<marker>Jadidinejad, Mahmoudi, 2009</marker>
<rawString>Amir Hossein Jadidinejad and Fariborz Mahmoudi. 2009. Cross-language information retrieval using meta-language index construction and structural queries. In Proceedings of the 10th cross-language evaluation forum conference on Multilingual information access evaluation: text retrieval experiments, CLEF’09, pages 70–77, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gareth Jones</author>
<author>Sakai Tetsuya</author>
<author>Nigel Collier</author>
<author>Akira Kumano</author>
<author>Kazuo Sumita</author>
</authors>
<title>Exploring the use of machine translation resources for englishjapanese cross-language information retrieval.</title>
<date>1999</date>
<booktitle>In In Proceedings of MT Summit VII Workshop on Machine Translation for Cross Language Information Retrieval,</booktitle>
<pages>181--188</pages>
<contexts>
<context position="6302" citStr="Jones et al., 1999" startWordPosition="993" endWordPosition="996">y logs (Gao et al., 2010), Wikipedia (Jadidinejad and Mahmoudi, 2009) or the web (Nie and Chen, 2002; Hu et al., 2008). (Gao et al., 2006) proposes syntax-based translation models to deal with the disambiguation issues (NP-based, dependency-based). The candidate translations proposed by these models are then reranked with the model learned to minimize the translation er2Standard MT evaluation metric ror on the training data. To our knowledge, existing work that use MTbased techniques for query translation use an outof-the-box MT system, without adapting it for query translation in particular (Jones et al., 1999; Wu et al., 2008) (although some query expansion techniques might be applied to the produced translation afterwards (Wu and He, 2010)). There is a number of works done for domain adaptation in Statistical Machine Translation. However, we want to distinguish between genre and domain adaptation in this work. Generally, genre can be seen as a sub-problem of domain. Thus, we consider genre to be the general style of the text e.g. conversation, news, blog, query (responsible mostly for the text structure) while the domain reflects more what the text is about – eg. social science, healthcare, histo</context>
</contexts>
<marker>Jones, Tetsuya, Collier, Kumano, Sumita, 1999</marker>
<rawString>Gareth Jones, Sakai Tetsuya, Nigel Collier, Akira Kumano, and Kazuo Sumita. 1999. Exploring the use of machine translation resources for englishjapanese cross-language information retrieval. In In Proceedings of MT Summit VII Workshop on Machine Translation for Cross Language Information Retrieval, pages 181–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Kettunen</author>
</authors>
<title>Choosing the best mt programs for clir purposes — can mt metrics be helpful?</title>
<date>2009</date>
<booktitle>In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, ECIR ’09,</booktitle>
<pages>706--712</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="9196" citStr="Kettunen, 2009" startWordPosition="1452" endWordPosition="1453">nslation can lead to worse retrieval if the wrong lexical choice is done. Moreover, often the retrieval demands some linguistic preprocessing (eg. lemmatisation, PoS tagging) which in interaction with badly-formed translations might bring some noise. A couple of works studied the correlation between the standard MT evaluation metrics and the retrieval precision. Thus, (Fujii et al., 2009) showed a good correlation of the BLEU scores with the MAP scores for Cross-Lingual Patent Retrieval. However, the topics in patent search (long and well structured) are very different from standard queries. (Kettunen, 2009) also found a pretty high correlation ( 0.8 − 0.9) between standard MT evaluation metrics (METEOR(Banerjee and Lavie, 2005), BLEU, NIST(Doddington, 2002)) and retrieval precision for long queries. However, the same work shows that the correlation decreases ( 0.6 − 0.7) for short queries. In this paper we propose two approaches to SMT adaptation for queries. The first one optimizes BLEU, while the second one optimizes Mean Average Precision (MAP), a standard metric in information retrieval. We’ll address the issue of the correlation between BLEU and MAP in Section 4. Both of the proposed approa</context>
<context position="24966" citStr="Kettunen, 2009" startWordPosition="4060" endWordPosition="4061"> model as for the baseline (Europarl-based). The weights were then tuned on the CLEF topics described in section 4.1.1. We then tested the system obtained on 50 parallel queries from the CLEF AdHoc-TEL 2009 task. Table 3 describes the results of the evaluation. We observe consistent 1-best MAP improvements, but unstable BLEU (3-gramm) (improvements for English-German, and degradation for other language pairs), although one would have expected BLEU to be improved in this experimental setting given that BLEU was the objective function for MERT. These results, on one side, confirm the remark of (Kettunen, 2009) that there is a correlation (although low) between BLEU and MAP scores. The unstable BLEU scores 114 MAP MAP MAP BLEU BLEU 1-best 5-best 4-gramm 3-gramm Monolingual IR Bilingual IR English 0.3159 French-English 0.1828 0.2186 0.1199 0.1568 German-English 0.0941 0.0942 0.2351 0.2923 French 0.2386 English-French 0.1504 0.1543 0.2863 0.3423 German 0.2162 English-German 0.1009 0.1157 0.0000 0.1218 Table 2: Baseline MAP scores for monolingual and bilingual CLEF AdHoc TEL 2009 task. MAP MAP BLEU BLEU 1-best 5-best 4-gramm 3-gramm Fr-En 0.1954 0.2229 0.1062 0.1489 De-En 0.1018 0.1078 0.2240 0.2486 En</context>
</contexts>
<marker>Kettunen, 2009</marker>
<rawString>Kimmo Kettunen. 2009. Choosing the best mt programs for clir purposes — can mt metrics be helpful? In Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, ECIR ’09, pages 706–712, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>224--227</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7353" citStr="Koehn and Schroeder, 2007" startWordPosition="1165" endWordPosition="1168"> e.g. conversation, news, blog, query (responsible mostly for the text structure) while the domain reflects more what the text is about – eg. social science, healthcare, history, so domain adaptation involves lexical disambiguation and extra lexical coverage problems. To our knowledge, there is not much work addressing explicitly the problem of genre adaptation for SMT. Some work done on domain adaptation could be applied to genre adaptation, such as incorporating available in-domain corpora in the SMT model: either monolingual (Bertoldi and Federico, 2009; Wu et al., 2008; Zhao et al., 2004; Koehn and Schroeder, 2007), or small parallel data used for tuning the SMT parameters (Zheng et al., 2010; Pecina et al., 2011). 3 Our approach This work is based on the hypothesis that the general-purpose SMT system needs to be adapted for query translation. Although in (Ferro and Peters, 2009) it has been mentioned that using Google translate (general-purpose MT) for query translation allowed to CLEF participants to obtain the best CLIR performance, there is still 10% gap between monolingual and cross-lingual IR. We believe that, as in (Clinchant and Renders, 2007), more adapted query translation, possibly further co</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 224–227. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9863" citStr="Koehn et al., 2003" startWordPosition="1561" endWordPosition="1564">9) between standard MT evaluation metrics (METEOR(Banerjee and Lavie, 2005), BLEU, NIST(Doddington, 2002)) and retrieval precision for long queries. However, the same work shows that the correlation decreases ( 0.6 − 0.7) for short queries. In this paper we propose two approaches to SMT adaptation for queries. The first one optimizes BLEU, while the second one optimizes Mean Average Precision (MAP), a standard metric in information retrieval. We’ll address the issue of the correlation between BLEU and MAP in Section 4. Both of the proposed approaches rely on the phrase-based SMT (PBMT) model (Koehn et al., 2003) implemented in the Open Source SMT toolkit MOSES (Koehn et al., 2007). 3.1 Tuning for genre adaptation First, we propose to adapt the PBMT model by tuning the model’s weights on a parallel set of queries. This approach addresses the first aspect of the problem, which is producing a “good” translation. The PBMT model combines different types of features via a log-linear model. The standard features include (Koehn, 2010, Chapter 5): language model, word penalty, distortion, different translation models, etc. The weights of these features are learned during the tuning step with the MERT (Och, 20</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL ’07: Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9933" citStr="Koehn et al., 2007" startWordPosition="1573" endWordPosition="1576">2005), BLEU, NIST(Doddington, 2002)) and retrieval precision for long queries. However, the same work shows that the correlation decreases ( 0.6 − 0.7) for short queries. In this paper we propose two approaches to SMT adaptation for queries. The first one optimizes BLEU, while the second one optimizes Mean Average Precision (MAP), a standard metric in information retrieval. We’ll address the issue of the correlation between BLEU and MAP in Section 4. Both of the proposed approaches rely on the phrase-based SMT (PBMT) model (Koehn et al., 2003) implemented in the Open Source SMT toolkit MOSES (Koehn et al., 2007). 3.1 Tuning for genre adaptation First, we propose to adapt the PBMT model by tuning the model’s weights on a parallel set of queries. This approach addresses the first aspect of the problem, which is producing a “good” translation. The PBMT model combines different types of features via a log-linear model. The standard features include (Koehn, 2010, Chapter 5): language model, word penalty, distortion, different translation models, etc. The weights of these features are learned during the tuning step with the MERT (Och, 2003) algorithm. Roughly the MERT algorithm tunes feature weights one by</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In ACL ’07: Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10285" citStr="Koehn, 2010" startWordPosition="1634" endWordPosition="1635">ation retrieval. We’ll address the issue of the correlation between BLEU and MAP in Section 4. Both of the proposed approaches rely on the phrase-based SMT (PBMT) model (Koehn et al., 2003) implemented in the Open Source SMT toolkit MOSES (Koehn et al., 2007). 3.1 Tuning for genre adaptation First, we propose to adapt the PBMT model by tuning the model’s weights on a parallel set of queries. This approach addresses the first aspect of the problem, which is producing a “good” translation. The PBMT model combines different types of features via a log-linear model. The standard features include (Koehn, 2010, Chapter 5): language model, word penalty, distortion, different translation models, etc. The weights of these features are learned during the tuning step with the MERT (Och, 2003) algorithm. Roughly the MERT algorithm tunes feature weights one by one and optimizes them according to the BLEU score obtained. Our hypothesis is that the impact of different features should be different depending on whether we translate a full sentence, or a query-genre entry. Thus, one would expect that in the case of query-genre the language model or the distortion features should get less importance than in the</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philip Koehn. 2010. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wessel Kraaij</author>
<author>Jian-Yun Nie</author>
<author>Michel Simard</author>
</authors>
<title>Embedding web-based statistical translation models in cross-language information retrieval.</title>
<date>2003</date>
<booktitle>Computational Linguistiques,</booktitle>
<pages>29--381</pages>
<contexts>
<context position="5632" citStr="Kraaij et al., 2003" startWordPosition="888" endWordPosition="891">in groups of approaches to CLIR: document translation and query translation. We concentrate on the second group which is more relevant to our settings. The standard query translation methods use different translation resources such as bilingual dictionaries, parallel corpora and/or machine translation. The aspect of disambiguation is important for the first two techniques. Different methods were proposed to deal with disambiguation issues, often relying on the document collection or embedding the translation step directly into the retrieval model (Hiemstra and Jong, 1999; Berger et al., 1999; Kraaij et al., 2003). Other methods rely on external resources like query logs (Gao et al., 2010), Wikipedia (Jadidinejad and Mahmoudi, 2009) or the web (Nie and Chen, 2002; Hu et al., 2008). (Gao et al., 2006) proposes syntax-based translation models to deal with the disambiguation issues (NP-based, dependency-based). The candidate translations proposed by these models are then reranked with the model learned to minimize the translation er2Standard MT evaluation metric ror on the training data. To our knowledge, existing work that use MTbased techniques for query translation use an outof-the-box MT system, witho</context>
</contexts>
<marker>Kraaij, Nie, Simard, 2003</marker>
<rawString>Wessel Kraaij, Jian-Yun Nie, and Michel Simard. 2003. Embedding web-based statistical translation models in cross-language information retrieval. Computational Linguistiques, 29:381–419, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian-yun Nie</author>
<author>Jiang Chen</author>
</authors>
<title>Exploiting the web as parallel corpora for cross-language information retrieval. Web Intelligence,</title>
<date>2002</date>
<pages>218--239</pages>
<contexts>
<context position="5784" citStr="Nie and Chen, 2002" startWordPosition="914" endWordPosition="917">e standard query translation methods use different translation resources such as bilingual dictionaries, parallel corpora and/or machine translation. The aspect of disambiguation is important for the first two techniques. Different methods were proposed to deal with disambiguation issues, often relying on the document collection or embedding the translation step directly into the retrieval model (Hiemstra and Jong, 1999; Berger et al., 1999; Kraaij et al., 2003). Other methods rely on external resources like query logs (Gao et al., 2010), Wikipedia (Jadidinejad and Mahmoudi, 2009) or the web (Nie and Chen, 2002; Hu et al., 2008). (Gao et al., 2006) proposes syntax-based translation models to deal with the disambiguation issues (NP-based, dependency-based). The candidate translations proposed by these models are then reranked with the model learned to minimize the translation er2Standard MT evaluation metric ror on the training data. To our knowledge, existing work that use MTbased techniques for query translation use an outof-the-box MT system, without adapting it for query translation in particular (Jones et al., 1999; Wu et al., 2008) (although some query expansion techniques might be applied to t</context>
</contexts>
<marker>Nie, Chen, 2002</marker>
<rawString>Jian-yun Nie and Jiang Chen. 2002. Exploiting the web as parallel corpora for cross-language information retrieval. Web Intelligence, pages 218–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian-Yun Nie</author>
</authors>
<title>Cross-Language Information Retrieval.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="12313" citStr="Nie, 2010" startWordPosition="1963" endWordPosition="1964">observe whether and to what extent changes in BLEU scores are correlated to changes in MAP scores. 3.2 Reranking framework for query translation The second approach addresses the retrieval quality problem. An SMT system is usually trained to optimize the quality of the translation (eg. BLEU score for SMT), which is not necessarily correlated with the retrieval quality (especially for the short queries). Thus, for example, the word order which is crucial for translation quality (and is taken into account by most MT evaluation metrics) is often ignored by IR models. Our second approach follows (Nie, 2010, pp.106) argument that “the translation problem is an integral part of the whole CLIR problem, and unified CLIR models integrating translation should be defined”. We propose integrating the IR metric (MAP) into the translation model optimisation step via the reranking framework. Previous attempts to apply the reranking approach to SMT did not show significant improvements in terms of MT evaluation metrics (Och et al., 2003; Nikoulina and Dymetman, 2008). One of the reasons being the poor diversity of the Nbest list of the translations. However, we be111 lieve that this approach has more poten</context>
</contexts>
<marker>Nie, 2010</marker>
<rawString>Jian-Yun Nie. 2010. Cross-Language Information Retrieval. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vassilina Nikoulina</author>
<author>Marc Dymetman</author>
</authors>
<title>Experiments in discriminating phrase-based translations on the basis of syntactic coupling features.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2),</booktitle>
<pages>55--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="12771" citStr="Nikoulina and Dymetman, 2008" startWordPosition="2032" endWordPosition="2035">er which is crucial for translation quality (and is taken into account by most MT evaluation metrics) is often ignored by IR models. Our second approach follows (Nie, 2010, pp.106) argument that “the translation problem is an integral part of the whole CLIR problem, and unified CLIR models integrating translation should be defined”. We propose integrating the IR metric (MAP) into the translation model optimisation step via the reranking framework. Previous attempts to apply the reranking approach to SMT did not show significant improvements in terms of MT evaluation metrics (Och et al., 2003; Nikoulina and Dymetman, 2008). One of the reasons being the poor diversity of the Nbest list of the translations. However, we be111 lieve that this approach has more potential in the context of query translation. First of all the average query length is ˜5 words, which means that the Nbest list of the translations is more diverse than in the case of general phrase translation (average length 25-30 words). Moreover, the retrieval precision is more naturally integrated into the reranking framework than standard MT evaluation metrics such as BLEU. The main reason is that the notion of Average Retrieval Precision is well defi</context>
<context position="17412" citStr="Nikoulina and Dymetman, 2008" startWordPosition="2802" endWordPosition="2805">2I Iλ&apos; − λI I2 + \ ) C �K i=1 maxj=1..N lij − λ&apos; - (F(t� i) − F(tij) The loss lij is defined as the difference in the retrieval average precision between the oracle and non-oracle translations: lij = AP(tz) − AP(tij). C is the regularization parameter which is chosen via 5-fold cross-validation. 112 3.2.2 Features One of the advantages of the reranking framework is that new complex features can be easily integrated. We suggest to enrich the reranking model with different syntax-based features, such as: • features relying on dependency structures: called therein coupling features (proposed by (Nikoulina and Dymetman, 2008)); • features relying on Part of Speech Tagging: called therein PoS mapping features. By integrating the syntax-based features we have a double goal: showing the potential of the reranking framework with more complex features, and examining whether the integration of syntactic information could be useful for query translation. Coupling features. The goal of the coupling features is to measure the similarity between source and target dependency structures. The initial hypothesis is that a better translation should have a dependency structure closer to the one of the source query. In this work w</context>
</contexts>
<marker>Nikoulina, Dymetman, 2008</marker>
<rawString>Vassilina Nikoulina and Marc Dymetman. 2008. Experiments in discriminating phrase-based translations on the basis of syntactic coupling features. In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 55–60. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="21035" citStr="Och and Ney, 2003" startWordPosition="3409" endWordPosition="3412"> access to the relevance judgements. We didn’t have access to all relevance judgements of the previously desribed tracks. Thus we used only a subset of the previously extracted parallel set, which includes CLEF 2000-2008 topics from the AdHoc-main, AdHocTEL and GeoCLEF tracks. The number of queries obtained altogether is shown in (Table 1). 4.1.2 Baseline We tested our approaches on the CLEF AdHocTEL 2009 task (50 topics). This task dealt with monolingual and cross-lingual search in a library catalog. The monolingual retrieval is 3This alignment can be either produced by a toolkit like GIZA++(Och and Ney, 2003) or obtained directly by a system that produced the Nbest list of the translations (Moses). 113 Language pair Number of queries Total queries En - Fr, Fr - En 470 En - De, De - En 714 Annotated queries En - Fr, Fr - En 400 En - De, De - En 350 Table 1: Top: total number of parallel queries gathered from all the CLEF tasks (size of the tuning set). Bottom: number of queries extracted from the tasks for which the human relevance judgements were availble (size of the reranking training set). performed with the lemur4 toolkit (Ogilvie and Callan, 2001). The preprocessing includes lemmatisation (wi</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>John Hopkins University.</institution>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2003</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syntax for Statistical Machine Translation: Final report of John Hopkins 2003 Summer Workshop. Technical report, John Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10466" citStr="Och, 2003" startWordPosition="1662" endWordPosition="1663">, 2003) implemented in the Open Source SMT toolkit MOSES (Koehn et al., 2007). 3.1 Tuning for genre adaptation First, we propose to adapt the PBMT model by tuning the model’s weights on a parallel set of queries. This approach addresses the first aspect of the problem, which is producing a “good” translation. The PBMT model combines different types of features via a log-linear model. The standard features include (Koehn, 2010, Chapter 5): language model, word penalty, distortion, different translation models, etc. The weights of these features are learned during the tuning step with the MERT (Och, 2003) algorithm. Roughly the MERT algorithm tunes feature weights one by one and optimizes them according to the BLEU score obtained. Our hypothesis is that the impact of different features should be different depending on whether we translate a full sentence, or a query-genre entry. Thus, one would expect that in the case of query-genre the language model or the distortion features should get less importance than in the case of the full-sentence translation. MERT tuning on a genre-adapted parallel corpus should leverage this information from the data, adapting the SMT model to the query-genre. We </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160– 167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Ogilvie</author>
<author>James P Callan</author>
</authors>
<title>Experiments using the lemur toolkit.</title>
<date>2001</date>
<booktitle>In TREC.</booktitle>
<contexts>
<context position="21589" citStr="Ogilvie and Callan, 2001" startWordPosition="3514" endWordPosition="3517">ment can be either produced by a toolkit like GIZA++(Och and Ney, 2003) or obtained directly by a system that produced the Nbest list of the translations (Moses). 113 Language pair Number of queries Total queries En - Fr, Fr - En 470 En - De, De - En 714 Annotated queries En - Fr, Fr - En 400 En - De, De - En 350 Table 1: Top: total number of parallel queries gathered from all the CLEF tasks (size of the tuning set). Bottom: number of queries extracted from the tasks for which the human relevance judgements were availble (size of the reranking training set). performed with the lemur4 toolkit (Ogilvie and Callan, 2001). The preprocessing includes lemmatisation (with the Xerox Incremental ParserXIP (A¨ıt-Mokhtar et al., 2002)) and filtering out the function words (based on XIP PoS tagging). Table 2 shows the performance of the monolingual retrieval model for each collection. The monolingual retrieval results are comparable to the CLEF AdHoc-TEL 2009 participants (Ferro and Peters, 2009). Let us note here that it is not the case for our CLIR results since we didn’t exploit the fact that each of the collections could actually contain the entries in a language other than the official language of the collection.</context>
</contexts>
<marker>Ogilvie, Callan, 2001</marker>
<rawString>Paul Ogilvie and James P. Callan. 2001. Experiments using the lemur toolkit. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<contexts>
<context position="3953" citStr="Papineni et al., 2001" startWordPosition="620" endWordPosition="623">ch corpora in order to adapt the SMT model parameters for query translation. In our approach the parameters of the SMT models are optimized on the basis of the parallel queries set. This is achieved either directly in the SMT system using the MERT (Minimum Error Rate Training) algorithm and optimiz1Insufficient for a full SMT system training (˜500 entries) 109 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 109–119, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics ing according to the BLEU2(Papineni et al., 2001) score, or via reranking the Nbest translation candidates generated by a baseline system based on new parameters (and possibly new features) that aim to optimize a retrieval metric. It is important to note that both of the proposed approaches allow keeping the MT system independent of the document collection and indexing, and thus suitable for a query translation service. These two approaches can also be combined by using the model produced with the first approach as a baseline that produces the Nbest list of translations that is then given to the reranking approach. The remainder of this pape</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001. Bleu: a method for automatic evaluation of machine translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Pecina</author>
<author>Antonio Toral</author>
<author>Andy Way</author>
<author>Vassilis Papavassiliou</author>
<author>Prokopis Prokopidis</author>
<author>Maria Giagkou</author>
</authors>
<title>Towards using web-crawled data for domain adaptation in statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Annual Conference of the European Associtation forMachine Translation,</booktitle>
<pages>297--304</pages>
<location>Leuven,</location>
<contexts>
<context position="7454" citStr="Pecina et al., 2011" startWordPosition="1183" endWordPosition="1186"> more what the text is about – eg. social science, healthcare, history, so domain adaptation involves lexical disambiguation and extra lexical coverage problems. To our knowledge, there is not much work addressing explicitly the problem of genre adaptation for SMT. Some work done on domain adaptation could be applied to genre adaptation, such as incorporating available in-domain corpora in the SMT model: either monolingual (Bertoldi and Federico, 2009; Wu et al., 2008; Zhao et al., 2004; Koehn and Schroeder, 2007), or small parallel data used for tuning the SMT parameters (Zheng et al., 2010; Pecina et al., 2011). 3 Our approach This work is based on the hypothesis that the general-purpose SMT system needs to be adapted for query translation. Although in (Ferro and Peters, 2009) it has been mentioned that using Google translate (general-purpose MT) for query translation allowed to CLEF participants to obtain the best CLIR performance, there is still 10% gap between monolingual and cross-lingual IR. We believe that, as in (Clinchant and Renders, 2007), more adapted query translation, possibly further combined with query expansion techniques, can lead to improved retrieval. The problem of the SMT adapta</context>
</contexts>
<marker>Pecina, Toral, Way, Papavassiliou, Prokopidis, Giagkou, 2011</marker>
<rawString>Pavel Pecina, Antonio Toral, Andy Way, Vassilis Papavassiliou, Prokopis Prokopidis, and Maria Giagkou. 2011. Towards using web-crawled data for domain adaptation in statistical machine translation. In Proceedings of the 15th Annual Conference of the European Associtation forMachine Translation, pages 297–304, Leuven, Belgium. European Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
<author>Mark Johnson</author>
</authors>
<title>Discriminative language modeling with conditional random fields and the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL’04),</booktitle>
<contexts>
<context position="14001" citStr="Roark et al., 2004" startWordPosition="2228" endWordPosition="2231">ingle query translation, while BLEU is defined on the corpus level and correlates poorly with human quality judgements for the individual translations (Specia et al., 2009; Callison-Burch et al., 2009). Finally, the reranking framework allows a lot of flexibility. Thus, it allows enriching the baseline translation model with new complex features which might be difficult to introduce into the translation model directly. Other works applied the reranking framework to different NLP tasks such as Named Entities Extraction (Collins, 2001), parsing (Collins and Roark, 2004), and language modelling (Roark et al., 2004). Most of these works used the reranking framework to combine generative and discriminative methods when both approaches aim at solving the same problem: the generative model produces a set of hypotheses, and the best hypothesis is chosen afterwards via the discriminative reranking model, which allows to enrich the baseline model with the new complex and heterogeneous features. We suggest using the reranking framework to combine two different tasks: Machine Translation and Cross-lingual Information Retrieval. In this context the reranking framework doesn’t only allow enriching the baseline tra</context>
</contexts>
<marker>Roark, Saraclar, Collins, Johnson, 2004</marker>
<rawString>Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson. 2004. Discriminative language modeling with conditional random fields and the perceptron algorithm. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL’04), July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Marco Turchi</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
<author>Nello Cristianini</author>
</authors>
<title>Estimating the sentence-level quality of machine translation systems.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Annual Conference of the EAMT,</booktitle>
<pages>28--35</pages>
<location>Barcelona,</location>
<contexts>
<context position="13553" citStr="Specia et al., 2009" startWordPosition="2162" endWordPosition="2165">f query translation. First of all the average query length is ˜5 words, which means that the Nbest list of the translations is more diverse than in the case of general phrase translation (average length 25-30 words). Moreover, the retrieval precision is more naturally integrated into the reranking framework than standard MT evaluation metrics such as BLEU. The main reason is that the notion of Average Retrieval Precision is well defined for a single query translation, while BLEU is defined on the corpus level and correlates poorly with human quality judgements for the individual translations (Specia et al., 2009; Callison-Burch et al., 2009). Finally, the reranking framework allows a lot of flexibility. Thus, it allows enriching the baseline translation model with new complex features which might be difficult to introduce into the translation model directly. Other works applied the reranking framework to different NLP tasks such as Named Entities Extraction (Collins, 2001), parsing (Collins and Roark, 2004), and language modelling (Roark et al., 2004). Most of these works used the reranking framework to combine generative and discriminative methods when both approaches aim at solving the same problem</context>
</contexts>
<marker>Specia, Turchi, Cancedda, Dymetman, Cristianini, 2009</marker>
<rawString>Lucia Specia, Marco Turchi, Nicola Cancedda, Marc Dymetman, and Nello Cristianini. 2009. Estimating the sentence-level quality of machine translation systems. In Proceedings of the 13th Annual Conference of the EAMT, page 28–35, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>764--773</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="16393" citStr="Watanabe et al., 2007" startWordPosition="2626" endWordPosition="2629">e baseline MT model for each query qi, i = 1..K. • Each translation t E GEN(qi) is used to perform a retrieval from a target document collection, and an Average Precision score (AP(t)) is computed for each t E GEN(qi) by comparing its retrieval to the relevance annotations done during the CLEF campaign. The weights λ are learned with the objective of maximizing MAP for all the queries of the training set, and, therefore, are optimized for retrieval quality. The weights optimization is done with the Margin Infused Relaxed Algorithm (MIRA)(Crammer and Singer, 2003), which was applied to SMT by (Watanabe et al., 2007; Chiang et al., 2008). MIRA is an online learning algorithm where each weights update is done to keep the new weights as close as possible to the old weights (first term), and score oracle translation (the translation giving the best retrieval score : tz = arg maxt AP(t)) higher than each non-oracle translation (tij) by a margin at least as wide as the loss lij (second term): λ= minλ&apos; 12I Iλ&apos; − λI I2 + \ ) C �K i=1 maxj=1..N lij − λ&apos; - (F(t� i) − F(tij) The loss lij is defined as the difference in the retrieval average precision between the oracle and non-oracle translations: lij = AP(tz) − A</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 764–773, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Wu</author>
<author>Daqing He</author>
</authors>
<title>A study of query translation using google machine translation system.</title>
<date>2010</date>
<journal>Computational Intelligence and Software Engineering (CiSE).</journal>
<contexts>
<context position="6436" citStr="Wu and He, 2010" startWordPosition="1015" endWordPosition="1018">6) proposes syntax-based translation models to deal with the disambiguation issues (NP-based, dependency-based). The candidate translations proposed by these models are then reranked with the model learned to minimize the translation er2Standard MT evaluation metric ror on the training data. To our knowledge, existing work that use MTbased techniques for query translation use an outof-the-box MT system, without adapting it for query translation in particular (Jones et al., 1999; Wu et al., 2008) (although some query expansion techniques might be applied to the produced translation afterwards (Wu and He, 2010)). There is a number of works done for domain adaptation in Statistical Machine Translation. However, we want to distinguish between genre and domain adaptation in this work. Generally, genre can be seen as a sub-problem of domain. Thus, we consider genre to be the general style of the text e.g. conversation, news, blog, query (responsible mostly for the text structure) while the domain reflects more what the text is about – eg. social science, healthcare, history, so domain adaptation involves lexical disambiguation and extra lexical coverage problems. To our knowledge, there is not much work</context>
</contexts>
<marker>Wu, He, 2010</marker>
<rawString>Dan Wu and Daqing He. 2010. A study of query translation using google machine translation system. Computational Intelligence and Software Engineering (CiSE).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Chengqing Zong</author>
</authors>
<title>Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling2008),</booktitle>
<pages>993--100</pages>
<contexts>
<context position="6320" citStr="Wu et al., 2008" startWordPosition="997" endWordPosition="1000">2010), Wikipedia (Jadidinejad and Mahmoudi, 2009) or the web (Nie and Chen, 2002; Hu et al., 2008). (Gao et al., 2006) proposes syntax-based translation models to deal with the disambiguation issues (NP-based, dependency-based). The candidate translations proposed by these models are then reranked with the model learned to minimize the translation er2Standard MT evaluation metric ror on the training data. To our knowledge, existing work that use MTbased techniques for query translation use an outof-the-box MT system, without adapting it for query translation in particular (Jones et al., 1999; Wu et al., 2008) (although some query expansion techniques might be applied to the produced translation afterwards (Wu and He, 2010)). There is a number of works done for domain adaptation in Statistical Machine Translation. However, we want to distinguish between genre and domain adaptation in this work. Generally, genre can be seen as a sub-problem of domain. Thus, we consider genre to be the general style of the text e.g. conversation, news, blog, query (responsible mostly for the text structure) while the domain reflects more what the text is about – eg. social science, healthcare, history, so domain adap</context>
</contexts>
<marker>Wu, Wang, Zong, 2008</marker>
<rawString>Hua Wu, Haifeng Wang, and Chengqing Zong. 2008. Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling2008), pages 993–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
</authors>
<title>Language model adaptation for statistical machine translation with structured query models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7325" citStr="Zhao et al., 2004" startWordPosition="1161" endWordPosition="1164">l style of the text e.g. conversation, news, blog, query (responsible mostly for the text structure) while the domain reflects more what the text is about – eg. social science, healthcare, history, so domain adaptation involves lexical disambiguation and extra lexical coverage problems. To our knowledge, there is not much work addressing explicitly the problem of genre adaptation for SMT. Some work done on domain adaptation could be applied to genre adaptation, such as incorporating available in-domain corpora in the SMT model: either monolingual (Bertoldi and Federico, 2009; Wu et al., 2008; Zhao et al., 2004; Koehn and Schroeder, 2007), or small parallel data used for tuning the SMT parameters (Zheng et al., 2010; Pecina et al., 2011). 3 Our approach This work is based on the hypothesis that the general-purpose SMT system needs to be adapted for query translation. Although in (Ferro and Peters, 2009) it has been mentioned that using Google translate (general-purpose MT) for query translation allowed to CLEF participants to obtain the best CLIR performance, there is still 10% gap between monolingual and cross-lingual IR. We believe that, as in (Clinchant and Renders, 2007), more adapted query tran</context>
</contexts>
<marker>Zhao, Eck, Vogel, 2004</marker>
<rawString>Bing Zhao, Matthias Eck, and Stephan Vogel. 2004. Language model adaptation for statistical machine translation with structured query models. In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguang Zheng</author>
<author>Zhongjun He</author>
<author>Yao Meng</author>
<author>Hao Yu</author>
</authors>
<title>Domain adaptation for statistical machine translation in development corpus selection.</title>
<date>2010</date>
<booktitle>In Universal Communication Symposium (IUCS), 2010 4th International,</booktitle>
<pages>2--7</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7432" citStr="Zheng et al., 2010" startWordPosition="1179" endWordPosition="1182"> the domain reflects more what the text is about – eg. social science, healthcare, history, so domain adaptation involves lexical disambiguation and extra lexical coverage problems. To our knowledge, there is not much work addressing explicitly the problem of genre adaptation for SMT. Some work done on domain adaptation could be applied to genre adaptation, such as incorporating available in-domain corpora in the SMT model: either monolingual (Bertoldi and Federico, 2009; Wu et al., 2008; Zhao et al., 2004; Koehn and Schroeder, 2007), or small parallel data used for tuning the SMT parameters (Zheng et al., 2010; Pecina et al., 2011). 3 Our approach This work is based on the hypothesis that the general-purpose SMT system needs to be adapted for query translation. Although in (Ferro and Peters, 2009) it has been mentioned that using Google translate (general-purpose MT) for query translation allowed to CLEF participants to obtain the best CLIR performance, there is still 10% gap between monolingual and cross-lingual IR. We believe that, as in (Clinchant and Renders, 2007), more adapted query translation, possibly further combined with query expansion techniques, can lead to improved retrieval. The pro</context>
<context position="11170" citStr="Zheng et al., 2010" startWordPosition="1776" endWordPosition="1779"> them according to the BLEU score obtained. Our hypothesis is that the impact of different features should be different depending on whether we translate a full sentence, or a query-genre entry. Thus, one would expect that in the case of query-genre the language model or the distortion features should get less importance than in the case of the full-sentence translation. MERT tuning on a genre-adapted parallel corpus should leverage this information from the data, adapting the SMT model to the query-genre. We would also like to note that the tuning approach (proposed for domain adaptation by (Zheng et al., 2010)) seems to be more appropriate for genre adaptation than for domain adaptation where the problem of lexical ambiguity is encoded in the translation model and re-weighting the main features might not be sufficient. We use the MERT implementation provided with the Moses toolkit with default settings. Our assumption is that this procedure although not explicitly aimed at improving retrieval performance will nevertheless lead to “better” query translations when compared to the baseline. The results of this apporach allow us also to observe whether and to what extent changes in BLEU scores are corr</context>
</contexts>
<marker>Zheng, He, Meng, Yu, 2010</marker>
<rawString>Zhongguang Zheng, Zhongjun He, Yao Meng, and Hao Yu. 2010. Domain adaptation for statistical machine translation in development corpus selection. In Universal Communication Symposium (IUCS), 2010 4th International, pages 2–7. IEEE.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>