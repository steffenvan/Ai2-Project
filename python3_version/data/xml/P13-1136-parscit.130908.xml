<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.998695">
A Sentence Compression Based Framework to Query-Focused
Multi-Document Summarization
</title>
<author confidence="0.999336">
Lu Wang1 Hema Raghavan2 Vittorio Castelli2 Radu Florian2 Claire Cardie1
</author>
<affiliation confidence="0.99902">
1Department of Computer Science, Cornell University, Ithaca, NY 14853, USA
</affiliation>
<email confidence="0.978187">
{luwang, cardie}@cs.cornell.edu
</email>
<note confidence="0.59417">
2IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA
</note>
<email confidence="0.975929">
{hraghav, vittorio, raduf}@us.ibm.com
</email>
<sectionHeader confidence="0.993836" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998112368421053">
We consider the problem of using sentence
compression techniques to facilitate query-
focused multi-document summarization. We
present a sentence-compression-based frame-
work for the task, and design a series of
learning-based compression models built on
parse trees. An innovative beam search de-
coder is proposed to efficiently find highly
probable compressions. Under this frame-
work, we show how to integrate various in-
dicative metrics such as linguistic motivation
and query relevance into the compression pro-
cess by deriving a novel formulation of a com-
pression scoring function. Our best model
achieves statistically significant improvement
over the state-of-the-art systems on several
metrics (e.g. 8.0% and 5.4% improvements in
ROUGE-2 respectively) for the DUC 2006 and
2007 summarization task.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998390846153846">
The explosion of the Internet clearly warrants
the development of techniques for organizing and
presenting information to users in an effective
way. Query-focused multi-document summariza-
tion (MDS) methods have been proposed as one
such technique and have attracted significant at-
tention in recent years. The goal of query-focused
MDS is to synthesize a brief (often fixed-length)
and well-organized summary from a set of topic-
related documents that answer a complex ques-
tion or address a topic statement. The result-
ing summaries, in turn, can support a number of
information analysis applications including open-
ended question answering, recommender systems,
and summarization of search engine results. As
further evidence of its importance, the Document
Understanding Conference (DUC) has used query-
focused MDS as its main task since 2004 to foster
new research on automatic summarization in the
context of users’ needs.
To date, most top-performing systems for
multi-document summarization—whether query-
specific or not—remain largely extractive: their
summaries are comprised exclusively of sen-
tences selected directly from the documents
to be summarized (Erkan and Radev, 2004;
Haghighi and Vanderwende, 2009; Celikyilmaz
and Hakkani-T¨ur, 2011). Despite their simplicity,
extractive approaches have some disadvantages.
First, lengthy sentences that are partly relevant
are either excluded from the summary or (if se-
lected) can block the selection of other important
sentences, due to summary length constraints.
In addition, when people write summaries, they
tend to abstract the content and seldom use
entire sentences taken verbatim from the original
documents. In news articles, for example, most
sentences are lengthy and contain both potentially
useful information for a summary as well as un-
necessary details that are better omitted. Consider
the following DUC query as input for a MDS
system:1 “In what ways have stolen artworks
been recovered? How often are suspects arrested
or prosecuted for the thefts?” One manually gen-
erated summary includes the following sentence
but removes the bracketed words in gray:
A man suspected of stealing a million-dollar collection
of [hundreds of ancient] Nepalese and Tibetan art objects in
New York [11 years ago] was arrested [Thursday at his South
Los Angeles home, where he had been hiding the antiquities,
police said].
In this example, the compressed sentence is rela-
</bodyText>
<footnote confidence="0.983052">
1From DUC 2005, query for topic d422g.
</footnote>
<page confidence="0.818754">
1384
</page>
<note confidence="0.9177625">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1384–1394,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999846875">
tively more succinct and readable than the origi-
nal (e.g. in terms of Flesch-Kincaid Reading Ease
Score (Kincaid et al., 1975)). Likewise, removing
information irrelevant to the query (e.g. “11 years
ago”, “police said”) is crucial for query-focused
MDS.
Sentence compression techniques (Knight and
Marcu, 2000; Clarke and Lapata, 2008) are the
standard for producing a compact and grammat-
ical version of a sentence while preserving rel-
evance, and prior research (e.g. Lin (2003)) has
demonstrated their potential usefulness for generic
document summarization. Similarly, strides have
been made to incorporate sentence compression
into query-focused MDS systems (Zajic et al.,
2006). Most attempts, however, fail to produce
better results than those of the best systems built
on pure extraction-based approaches that use no
sentence compression.
In this paper we investigate the role of sentence
compression techniques for query-focused MDS.
We extend existing work in the area first by inves-
tigating the role of learning-based sentence com-
pression techniques. In addition, we design three
types of approaches to sentence-compression—
rule-based, sequence-based and tree-based—and
examine them within our compression-based
framework for query-specific MDS. Our top-
performing sentence compression algorithm in-
corporates measures of query relevance, con-
tent importance, redundancy and language qual-
ity, among others. Our tree-based methods rely on
a scoring function that allows for easy and flexi-
ble tailoring of sentence compression to the sum-
marization task, ultimately resulting in significant
improvements for MDS, while at the same time
remaining competitive with existing methods in
terms of sentence compression, as discussed next.
We evaluate the summarization models on
the standard Document Understanding Confer-
ence (DUC) 2006 and 2007 corpora 2 for query-
focused MDS and find that all of our compression-
based summarization models achieve statistically
significantly better performance than the best
DUC 2006 systems. Our best-performing sys-
tem yields an 11.02 ROUGE-2 score (Lin and
Hovy, 2003), a 8.0% improvement over the best
reported score (10.2 (Davis et al., 2012)) on the
</bodyText>
<footnote confidence="0.828727">
2We believe that we can easily adapt our system for tasks
(e.g. TAC-08’s opinion summarization or TAC-09’s update
summarization) or domains (e.g. web pages or wikipedia
pages). We reserve that for future work.
</footnote>
<bodyText confidence="0.995281933333333">
DUC 2006 dataset, and an 13.49 ROUGE-2, a
5.4% improvement over the best score in DUC
2007 (12.8 (Davis et al., 2012)). We also ob-
serve substantial improvements over previous sys-
tems w.r.t. the manual Pyramid (Nenkova and
Passonneau, 2004) evaluation measure (26.4 vs.
22.9 (Jagarlamudi et al., 2006)); human annota-
tors furthermore rate our system-generated sum-
maries as having less redundancy and compara-
ble quality w.r.t. other linguistic quality metrics.
With these results we believe we are the first
to successfully show that sentence compression
can provide statistically significant improvements
over pure extraction-based approaches for query-
focused MDS.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999816147058824">
Existing research on query-focused multi-
document summarization (MDS) largely relies
on extractive approaches, where systems usually
take as input a set of documents and select
the top relevant sentences for inclusion in the
final summary. A wide range of methods have
been employed for this task. For unsupervised
methods, sentence importance can be estimated
by calculating topic signature words (Lin and
Hovy, 2000; Conroy et al., 2006), combining
query similarity and document centrality within
a graph-based model (Otterbacher et al., 2005),
or using a Bayesian model with sophisticated
inference (Daum´e and Marcu, 2006). Davis et
al. (2012) first learn the term weights by Latent
Semantic Analysis, and then greedily select
sentences that cover the maximum combined
weights. Supervised approaches have mainly
focused on applying discriminative learning for
ranking sentences (Fuentes et al., 2007). Lin and
Bilmes (2011) use a class of carefully designed
submodular functions to reward the diversity of
the summaries and select sentences greedily.
Our work is more related to the less studied
area of sentence compression as applied to (sin-
gle) document summarization. Zajic et al. (2006)
tackle the query-focused MDS problem using a
compress-first strategy: they develop heuristics to
generate multiple alternative compressions of all
sentences in the original document; these then be-
come the candidates for extraction. This approach,
however, does not outperform some extraction-
based approaches. A similar idea has been stud-
ied for MDS (Lin, 2003; Gillick and Favre, 2009),
</bodyText>
<page confidence="0.984947">
1385
</page>
<bodyText confidence="0.999987652173913">
but limited improvement is observed over extrac-
tive baselines with simple compression rules. Fi-
nally, although learning-based compression meth-
ods are promising (Martins and Smith, 2009;
Berg-Kirkpatrick et al., 2011), it is unclear how
well they handle issues of redundancy.
Our research is also inspired by probabilis-
tic sentence-compression approaches, such as the
noisy-channel model (Knight and Marcu, 2000;
Turner and Charniak, 2005), and its extension via
synchronous context-free grammars (SCFG) (Aho
and Ullman, 1969; Lewis and Stearns, 1968) for
robust probability estimation (Galley and McKe-
own, 2007). Rather than attempt to derive a new
parse tree like Knight and Marcu (2000) and Gal-
ley and McKeown (2007), we learn to safely re-
move a set of constituents in our parse tree-based
compression model while preserving grammati-
cal structure and essential content. Sentence-level
compression has also been examined via a dis-
criminative model McDonald (2006), and Clarke
and Lapata (2008) also incorporate discourse in-
formation by using integer linear programming.
</bodyText>
<sectionHeader confidence="0.996243" genericHeader="method">
3 The Framework
</sectionHeader>
<bodyText confidence="0.950434">
We now present our query-focused MDS frame-
work consisting of three steps: Sentence Rank-
ing, Sentence Compression and Post-processing.
First, sentence ranking determines the importance
of each sentence given the query. Then, a sen-
tence compressor iteratively generates the most
likely succinct versions of the ranked sentences,
which are cumulatively added to the summary, un-
til a length limit is reached. Finally, the post-
processing stage applies coreference resolution
and sentence reordering to build the summary.
Sentence Ranking. This stage aims to rank sen-
tences in order of relevance to the query. Un-
surprisingly, ranking algorithms have been suc-
cessfully applied to this task. We experimented
with two of them – Support Vector Regres-
sion (SVR) (Mozer et al., 1997) and Lamb-
daMART (Burges et al., 2007). The former
has been used previously for MDS (Ouyang et
al., 2011). LambdaMart on the other hand has
shown considerable success in information re-
trieval tasks (Burges, 2010); we are the first to
apply it to summarization. For training, we use
40 topics (i.e. queries) from the DUC 2005 cor-
pus (Dang, 2005) along with their manually gener-
ated abstracts. As in previous work (Shen and Li,
Basic Features
relative/absolute position
is among the first 1/3/5 sentences?
number of words (with/without stopwords)
number of words more than 5/10 (with/without stopwords)
</bodyText>
<table confidence="0.968637823529412">
Query-Relevant Features
unigram/bigram/skip bigram (at most four words apart) overlap
unigram/bigram TF/TF-IDF similarity
mention overlap
subject/object/indirect object overlap
semantic role overlap
relation overlap
Query-Independent Features
average/total unigram/bigram IDF/TF-IDF
unigram/bigram TF/TF-IDF similarity with the centroid of the cluster
average/sum of sumBasic/SumFocus (Toutanova et al., 2007)
average/sum of mutual information
average/sum of number of topic signature words (Lin and Hovy, 2000)
basic/improved sentence scorers from Conroy et al. (2006)
Content Features
contains verb/web link/phone number?
contains/portion of words between parentheses
</table>
<tableCaption confidence="0.999897">
Table 1: Sentence-level features for sentence ranking.
</tableCaption>
<bodyText confidence="0.993181176470588">
2011; Ouyang et al., 2011), we use the ROUGE-
2 score, which measures bigram overlap between
a sentence and the abstracts, as the objective for
regression.
While space limitations preclude a longer dis-
cussion of the full feature set (ref. Table 1), we
describe next the query-relevant features used for
sentence ranking as these are the most impor-
tant for our summarization setting. The goal of
this feature subset is to determine the similarity
between the query and each candidate sentence.
When computing similarity, we remove stopwords
as well as the words “discuss, describe, specify,
explain, identify, include, involve, note” that are
adopted and extended from Conroy et al. (2006).
Then we conduct simple query expansion based
on the title of the topic and cross-document coref-
erence resolution. Specifically, we first add the
words from the topic title to the query. And for
each mention in the query, we add other mentions
within the set of documents that corefer with this
mention. Finally, we compute two versions of the
features—one based on the original query and an-
other on the expanded one. We also derive the
semantic role overlap and relation instance over-
lap between the query and each sentence. Cross-
document coreference resolution, semantic role la-
beling and relation extraction are accomplished
via the methods described in Section 5.
Sentence Compression. As the main focus of
this paper, we propose three types of compression
methods, described in detail in Section 4 below.
Post-processing. Post-processing performs
coreference resolution and sentence ordering.
</bodyText>
<page confidence="0.963579">
1386
</page>
<table confidence="0.985641823529412">
Basic Features Syntactic Tree Features
first 1/3/5 tokens (toks)? POS tag
last 1/3/5 toks? parent/grandparent label
first letter/all letters capitalized? leftmost child of parent?
is negation? second leftmost child of parent?
is stopword? is headword?
Dependency Tree Features in NP/VP/ADVP/ADJP chunk?
Semantic Features
dependency relation (dep rel)
parent/grandparent dep rel
is the root?
has a depth larger than 3/5?
is a predicate?
semantic role label
Rule-Based Features
For each rule in Table 2 , we construct a corresponding feature to
indicate whether the token is identified by the rule.
</table>
<tableCaption confidence="0.9812565">
Table 3: Token-level features for sequence-based com-
pression.
</tableCaption>
<bodyText confidence="0.99996025">
We replace each pronoun with its referent unless
they appear in the same sentence. For sentence
ordering, each compressed sentence is assigned
to the most similar (tf-idf) query sentence. Then
a Chronological Ordering algorithm (Barzilay et
al., 2002) sorts the sentences for each query based
first on the time stamp, and then the position in
the source document.
</bodyText>
<sectionHeader confidence="0.984692" genericHeader="method">
4 Sentence Compression
</sectionHeader>
<bodyText confidence="0.999330166666667">
Sentence compression is typically formulated as
the problem of removing secondary information
from a sentence while maintaining its grammati-
cality and semantic structure (Knight and Marcu,
2000; McDonald, 2006; Galley and McKeown,
2007; Clarke and Lapata, 2008). We leave other
rewrite operations, such as paraphrasing and re-
ordering, for future work. Below we describe
the sentence compression approaches developed
in this research: RULE-BASED COMPRESSION,
SEQUENCE-BASED COMPRESSION, and TREE-
BASED COMPRESSION.
</bodyText>
<subsectionHeader confidence="0.976412">
4.1 Rule-based Compression
</subsectionHeader>
<bodyText confidence="0.999973333333333">
Turner and Charniak (2005) have shown that ap-
plying hand-crafted rules for trimming sentences
can improve both content and linguistic qual-
ity. Our rule-based approach extends existing
work (Conroy et al., 2006; Toutanova et al., 2007)
to create the linguistically-motivated compression
rules of Table 2. To avoid ill-formed output, we
disallow compressions of more than 10 words by
each rule.
</bodyText>
<subsectionHeader confidence="0.982013">
4.2 Sequence-based Compression
</subsectionHeader>
<bodyText confidence="0.99314225">
As in McDonald (2006) and Clarke and Lapata
(2008), our sequence-based compression model
makes a binary “keep-or-delete” decision for each
word in the sentence. In contrast, however, we
</bodyText>
<figureCaption confidence="0.9873625">
Figure 1: Diagram of tree-based compression. The
nodes to be dropped are grayed out. In this example,
the root of the gray subtree (a “PP”) would be labeled
REMOVE. Its siblings and parent are labeled RETAIN
and PARTIAL, respectively. The trimmed tree is real-
ized as “Malaria causes millions of deaths.”
</figureCaption>
<bodyText confidence="0.999953620689655">
view compression as a sequential tagging problem
and make use of linear-chain Conditional Ran-
dom Fields (CRFs) (Lafferty et al., 2001) to se-
lect the most likely compression. We represent
each sentence as a sequence of tokens, X =
xoxi ... xn, and generate a sequence of labels,
Y = y0y1 . . . yn, that encode which tokens are
kept, using a BIO label format: {B-RETAIN de-
notes the beginning of a retained sequence, I-
RETAIN indicates tokens “inside” the retained se-
quence, O marks tokens to be removed}.
The CRF model is built using the features
shown in Table 3. “Dependency Tree Features”
encode the grammatical relations in which each
word is involved as a dependent. For the “Syntac-
tic Tree”, “Dependency Tree” and “Rule-Based”
features, we also include features for the two
words that precede and the two that follow the cur-
rent word. Detailed descriptions of the training
data and experimental setup are in Section 5.
During inference, we find the maximally likely
sequence Y according to a CRF with parameter
θ (Y = arg maxY , P(Y &apos;|X; θ)), while simulta-
neously enforcing the rules of Table 2 to reduce
the hypothesis space and encourage grammatical
compression. To do this, we encode these rules as
features for each token, and whenever these fea-
ture functions fire, we restrict the possible label
for that token to “O”.
</bodyText>
<subsectionHeader confidence="0.991136">
4.3 Tree-based Compression
</subsectionHeader>
<bodyText confidence="0.999847333333333">
Our tree-based compression methods are in line
with syntax-driven approaches (Galley and McK-
eown, 2007), where operations are carried out
on parse tree constituents. Unlike previous
work (Knight and Marcu, 2000; Galley and McK-
eown, 2007), we do not produce a new parse tree,
</bodyText>
<page confidence="0.976244">
1387
</page>
<table confidence="0.9994436">
Rule Example
Header [MOSCOW, October 19 ( Xinhua ) –] Russian federal troops Tuesday continued...
Relative dates ...Centers for Disease Control confirmed [Tuesday] that there was...
Intra-sentential attribution ...fueling the La Nina weather phenomenon, [the U.N. weather agency said].
Lead adverbials [Interestingly], while the Democrats tend to talk about...
Noun appositives Wayne County Prosecutor [John O’Hara] wanted to send a message...
Nonrestrictive relative clause Putin, [who was born on October 7, 1952 in Leningrad], was elected in the presidential election...
Adverbial clausal modifiers [Starting in 1998], California will require 2 per cent of a manufacturer...
(Lead sentence) [Given the short time], car makers see electric vehicles as...
Within Parentheses ...to Christian home schoolers in the early 1990s [(www.homecomputermarket.com)].
</table>
<tableCaption confidence="0.998532">
Table 2: Linguistically-motivated rules for sentence compression. The grayed-out words in brackets are removed.
</tableCaption>
<bodyText confidence="0.99980176">
but focus on learning to identify the proper set of
constituents to be removed. In particular, when a
node is dropped from the tree, all words it sub-
sumes will be deleted from the sentence.
Formally, given a parse tree T of the sentence
to be compressed and a tree traversal algorithm,
T can be presented as a list of ordered constituent
nodes, T = t0t1 ... tm. Our objective is to find a
set of labels, L = l0l1 ... lm, where li E {RETAIN,
REMOVE, PARTIAL}. RETAIN (RET) and RE-
MOVE (REM) denote whether the node ti is re-
tained or removed. PARTIAL (PAR) means ti is
partly removed, i.e. at least one child subtree of ti
is dropped.
Labels are identified, in order, according to the
tree traversal algorithm. Every node label needs
to be compatible with the labeling history: given
a node ti, and a set of labels l0 ... li_1 predicted
for nodes t0 ... ti_1, li =RET or li =REM is com-
patible with the history when all children of ti are
labeled as RET or REM, respectively; li =PAR is
compatible when ti has at least two descendents
tj and tk (j &lt; i and k &lt; i), one of which is
RETained and the other, REMoved. As such, the
root of the gray subtree in Figure 1 is labeled as
REM; its left siblings as RET; its parent as PAR.
As the space of possible compressions is expo-
nential in the number of leaves in the parse tree,
instead of looking for the globally optimal solu-
tion, we use beam search to find a set of highly
likely compressions and employ a language model
trained on a large corpus for evaluation.
A Beam Search Decoder. The beam search de-
coder (see Algorithm 1) takes as input the sen-
tence’s parse tree T = t0t1 ... tm, an order-
ing O for traversing T (e.g. postorder) as a se-
quence of nodes in T, the set L of possible
node labels, a scoring function S for evaluat-
ing each sentence compression hypothesis, and
a beam size N. Specifically, O is a permuta-
tion on the set {0, 1, ... , m}—each element an
index onto T. Following O, T is re-ordered as
tO0tO1 ... tOm, and the decoder considers each or-
dered constituent tOi in turn. In iteration i, all
existing sentence compression hypotheses are ex-
panded by one node, tOi, labeling it with all com-
patible labels. The new hypotheses (usually sub-
sentences) are ranked by the scorer S and the top
N are preserved to be extended in the next itera-
tion. See Figure 2 for an example.
</bodyText>
<table confidence="0.945976482758621">
Input : parse tree T, ordering O = O0O1 ... Om,
L =fRET, REM, PAR}, hypothesis scorer S,
beam size N
Output: N best compressions
stack +- Φ (empty set);
foreach node tO, in T = tO0 ... tOm do
if i == 0 (first node visited) then
foreach label lO0 in L do
newHypothesis h&apos; 1— [lO0];
put h&apos; into Stack;
end
else
newStack +- Φ (empty set);
foreach hypothesis h in stack do
foreach label lO, in L do
if lO, is compatible then
newHypothesis h&apos; �-- h + [lO,];
put h&apos; into newStack;
end
end
end
stack �-- newStack;
end
Apply S to sort hypotheses in stack in descending
order;
Keep the N best hypotheses in stack;
end
Algorithm 1: Beam search decoder.
Our BASIC Tree-based Compression in-
</table>
<bodyText confidence="0.940251375">
stantiates the beam search decoder with
postorder traversal and a hypothesis scorer
that takes a possible sentence compression—
a sequence of nodes (e.g. tO0 ... tOk) and
their labels (e.g. lO0 ... lOk)—and returns
Ekj=1 log P(lOj|tOj) (denoted later as
ScoreBasic). The probability is estimated by
a Maximum Entropy classifier (Berger et al.,
</bodyText>
<page confidence="0.987446">
1388
</page>
<figureCaption confidence="0.778799555555556">
Figure 2: Example of beam search decoding. For
postorder traversal, the three nodes are visited in a
bottom-up order. The associated compression hypothe-
ses (boxed) are ranked based on the scores in parenthe-
ses. Beam scores for other nodes are omitted.
Table 4: Constituent-level features for tree-based com-
pression. * or † denote features that are concatenated
with every Syntactic Tree feature to compose a new
one.
</figureCaption>
<bodyText confidence="0.999671705882353">
1996) trained at the constituent level using the
features in Table 4. We also apply the rules of
Table 2 during the decoding process. Concretely,
if the words subsumed by a node are identified
by any rule, we only consider REM as the node’s
label.
Given the N-best compressions from the de-
coder, we evaluate the yield of the trimmed trees
using a language model trained on the Giga-
word (Graff, 2003) corpus and return the compres-
sion with the highest probability. Thus, the de-
coder is quite flexible — its learned scoring func-
tion allows us to incorporate features salient for
sentence compression while its language model
guarantees the linguistic quality of the compressed
string. In the sections below we consider addi-
tional improvements.
</bodyText>
<subsectionHeader confidence="0.936797">
4.3.1 Improving Beam Search
</subsectionHeader>
<bodyText confidence="0.99437675">
CONTEXT-aware search is based on the intu-
ition that predictions on preceding context can
be leveraged to facilitate the prediction of the
current node. For example, parent nodes with
children that have all been removed (retained)
should have a label of REM (RET). In light of
this, we encode these contextual predictions as
additional features of S, that is, ALL-CHILDREN-
REMOVED/RETAINED, ANY-LEFTSIBLING-
REMOVED/RETAINED/PARTLY REMOVED,
LABEL-OF-LEFT-SIBLING/HEAD-NODE.
HEAD-driven search modifies the BASIC pos-
torder tree traversal by visiting the head node first
at each level, leaving other orders unchanged. In
a nutshell, if the head node is dropped, then its
modifiers need not be preserved. We adopt the
same features as CONTEXT-aware search, but re-
move those involving left siblings. We also add
one more feature: LABEL-OF-THE-HEAD-NODE-
IT-MODIFIES.
</bodyText>
<subsubsectionHeader confidence="0.512628">
4.3.2 Task-Specific Sentence Compression
</subsubsectionHeader>
<bodyText confidence="0.856570641509434">
The current scorer ScoreBasic is still fairly naive
in that it focuses only on features of the sen-
tence to be compressed. However extra-sentential
knowledge can also be important for query-
focused MDS. For example, information regard-
ing relevance to the query might lead the de-
coder to produce compressions better suited for
the summary. Towards this goal, we construct
a compression scoring function—the multi-scorer
(MULTI)—that allows the incorporation of mul-
tiple task-specific scorers. Given a hypothesis at
any stage of decoding, which yields a sequence of
words W = w0w1...wj, we propose the following
component scorers.
Query Relevance. Query information ought to
guide the compressor to identify the relevant con-
tent. The query Q is expanded as described in
Section 3. Let |W n Q |denote the number of
unique overlapping words between W and Q, then
scoreq = |W n Q|/|W|.
Importance. A query-independent impor-
tance score is defined as the average Sum-
Basic (Toutanova et al., 2007) value in W,
i.e. scoreim = Eji=1 SumBasic(wi)/|W|.
Language Model. We let scorelm be the proba-
bility of W computed by a language model.
Cross-Sentence Redundancy. To encourage di-
versified content, we define a redundancy score to
discount replicated content: scorered = 1 − |W n
C|/|W|, where C is the words already selected for
the summary.
Basic Features Syntactic Tree Features
projection falls w/in first 1/3/5 toks?* constituent label
projection falls w/in last 1/3/5 toks?* parent left/right sibling label
subsumes first 1/3/5 toks?* grandparent left/right sibling label
subsumes last 1/3/5 toks?* is leftmost child of parent?
number of words larger than 5/10?* is second leftmost child of parent?
is leaf node?* is head node of parent?
is root of parsing tree?* label of its head node
has word with first letter capitalized? has a depth greater than 3/5/10?
has word with all letters capitalized? Dependency Tree Features
has negation?
has stopwords?
dep rel of head node†
dep rel of parent’s head node†
Semantic Features dep rel of grandparent’s head node†
contain root of dep tree?†
has a depth larger than 3/5?†
the head node has predicate?
semantic roles of head node
Rule-Based Features
For each rule in Table 2 , we construct a corresponding feature to indicate
whether the token is identified by the rule.
</bodyText>
<page confidence="0.940482">
1389
</page>
<bodyText confidence="0.99815">
The multi-scorer is defined as a linear
combination of the component scorers: Let
</bodyText>
<equation confidence="0.9950486">
(α0, ... , α4), 0 &lt; αi &lt; 1, −−−�
score =
(scoreBasic, scoreQ, scoreim, scorelm, scorered),
S = scoremulti = α~ · −−−�
score (1)
</equation>
<bodyText confidence="0.989944">
The parameters α~ are tuned on a held-out tuning
set by grid search. We linearly normalize the score
of each metric, where the minimum and maximum
values are estimated from the tuning data.
</bodyText>
<sectionHeader confidence="0.99812" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.992616297297298">
We evaluate our methods on the DUC 2005, 2006
and 2007 datasets (Dang, 2005; Dang, 2006;
Dang, 2007), each of which is a collection of
newswire articles. 50 complex queries (topics) are
provided for DUC 2005 and 2006, 35 are collected
for DUC 2007 main task. Relevant documents for
each query are provided along with 4 to 9 human
MDS abstracts. The task is to generate a summary
within 250 words to address the query. We split
DUC 2005 into two parts: 40 topics to train the
sentence ranking models, and 10 for ranking algo-
rithm selection and parameter tuning for the multi-
scorer. DUC 2006 and DUC 2007 are reserved as
held out test sets.
Sentence Compression. The dataset
from Clarke and Lapata (2008) is used to
train the CRF and MaxEnt classifiers (Section 4).
It includes 82 newswire articles with one manually
produced compression aligned to each sentence.
Preprocessing. Documents are processed by a
full NLP pipeline, including token and sentence
segmentation, parsing, semantic role labeling,
and an information extraction pipeline consist-
ing of mention detection, NP coreference, cross-
document resolution, and relation detection (Flo-
rian et al., 2004; Luo et al., 2004; Luo and Zitouni,
2005).
Learning for Sentence Ranking and Compres-
sion. We use Weka (Hall et al., 2009) to train a
support vector regressor and experiment with var-
ious rankers in RankLib (Dang, 2011)3. As Lamb-
daMART has an edge over other rankers on the
held-out dataset, we selected it to produce ranked
sentences for further processing. For sequence-
based compression using CRFs, we employ Mal-
let (McCallum, 2002) and integrate the Table 2
rules during inference. NLTK (Bird et al., 2009)
</bodyText>
<footnote confidence="0.8199175">
3Default parameters are used. If an algorithm needs a val-
idation set, we use 10 out of 40 topics.
</footnote>
<bodyText confidence="0.9984978">
MaxEnt classifiers are used for tree-based com-
pression. Beam size is fixed at 2000.4 Sen-
tence compressions are evaluated by a 5-gram lan-
guage model trained on Gigaword (Graff, 2003)
by SRILM (Stolcke, 2002).
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999914974358975">
The results in Table 5 use the official ROUGE soft-
ware with standard options5 and report ROUGE-
2 (R-2) (measures bigram overlap) and ROUGE-
SU4 (R-SU4) (measures unigram and skip-bigram
separated by up to four words). We compare our
sentence-compression-based methods to the best
performing systems based on ROUGE in DUC
2006 and 2007 (Jagarlamudi et al., 2006; Pingali
et al., 2007), system by Davis et al. (2012) that
report the best R-2 score on DUC 2006 and 2007
thus far, and to the purely extractive methods of
SVR and LambdaMART.
Our sentence-compression-based systems
(marked with †) show statistically significant
improvements over pure extractive summarization
for both R-2 and R-SU4 (paired t-test, p &lt; 0.01).
This means our systems can effectively remove
redundancy within the summary through compres-
sion. Furthermore, our HEAD-driven beam search
method with MULTI-scorer beats all systems on
DUC 20066 and all systems on DUC 2007 except
the best system in terms of R-2 (p &lt; 0.01). Its
R-SU4 score is also significantly (p &lt; 0.01)
better than extractive methods, rule-based and
sequence-based compression methods on both
DUC 2006 and 2007. Moreover, our systems with
learning-based compression have considerable
compression rates, indicating their capability to
remove superfluous words as well as improve
summary quality.
Human Evaluation. The Pyramid (Nenkova
and Passonneau, 2004) evaluation was developed
to manually assess how many relevant facts or
Summarization Content Units (SCUs) are cap-
tured by system summaries. We ask a professional
annotator (who is not one of the authors, is highly
experienced in annotating for various NLP tasks,
and is fluent in English) to carry out a Pyramid
evaluation on 10 randomly selected topics from
</bodyText>
<footnote confidence="0.995848833333333">
4We looked at various beam sizes on the heldout data, and
observed that the performance peaks around this value.
5ROUGE-1.5.5.pl -n 4 -w 1.2 -m -2 4 -u -c 95 -r 1000 -f
A -p 0.5 -t 0 -a -d
6The system output from Davis et al. (2012) is not avail-
able, so significance tests are not conducted on it.
</footnote>
<equation confidence="0.743239">
α~ =
</equation>
<page confidence="0.819939">
1390
</page>
<table confidence="0.99972775">
DUC 2006 DUC 2007
System C Rate R-2 R-SU4 C Rate R-2 R-SU4
Best DUC system – 9.56 15.53 – 12.62 17.90
Davis et al. (2012) – 10.2 15.2 – 12.8 17.5
SVR 100% 7.78 13.02 100% 9.53 14.69
LambdaMART 100% 9.84 14.63 100% 12.34 15.62
Rule-based 78.99% 10.62 *† 15.73 † 78.11% 13.18† 18.15†
Sequence 76.34% 10.49 † 15.60 † 77.20% 13.25† 18.23†
Tree (BASIC + ScoreBasic) 70.48% 10.49 † 15.86 † 69.27% 13.00† 18.29†
Tree (CONTEXT + ScoreBasic) 65.21% 10.55 *† 16.10 † 63.44% 12.75 18.07†
Tree (HEAD + ScoreBasic) 66.70% 10.66 *† 16.18 † 65.05% 12.93 18.15†
Tree (HEAD + MULTI) 70.20% 11.02 *† 16.25 † 73.40% 13.49† 18.46†
</table>
<tableCaption confidence="0.774215857142857">
Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
preserved. R-2 (ROUGE-2) and R-SU4 (ROUGE-SU4) scores are multiplied by 100. “–” indicates that data is
unavailable. BASIC, CONTEXT and HEAD represent the basic beam search decoder, context-aware and head-driven
search extensions respectively. ScoreBasic and MULTI refer to the type of scorer used. Statistically significant
improvements (p &lt; 0.01) over the best system in DUC 06 and 07 are marked with ∗. † indicates statistical
significance (p &lt; 0.01) over extractive approaches (SVR or LambdaMART). HEAD + MULTI outperforms all the
other extract- and compression-based systems in R-2.
</tableCaption>
<table confidence="0.9996865">
System Pyr Gra Non-Red Ref Foc Coh
Best DUC system (ROUGE) 22.9±8.2 3.5±0.9 3.5±1.0 3.5±1.1 3.6±1.0 2.9±1.1
Best DUC system (LQ) – 4.0±0.8 4.2±0.7 3.8±0.7 3.6±0.9 3.4±0.9
Our System 26.4±10.3 3.0±0.9 4.0±1.1 3.6±1.0 3.4±0.9 2.8±1.0
</table>
<tableCaption confidence="0.9740385">
Table 6: Human evaluation on our multi-scorer based system, Jagarlamudi et al. (2006) (Best DUC system
(ROUGE)), and Lacatusu et al. (2006) (Best DUC system (LQ)). Our system can synthesize more relevant content
according to Pyramid (×100). We also examine linguistic quality (LQ) in Grammaticality (Gra), Non-redundancy
(Non-Red), Referential clarity (Ref), Focus (Foc), and Structure and Coherence (Coh) like Dang (2006), each rated
from 1 (very poor) to 5 (very good). Our system has better non-redundancy than Jagarlamudi et al. (2006) and is
comparable to Jagarlamudi et al. (2006) and Lacatusu et al. (2006) in other metrics except grammaticality.
</tableCaption>
<bodyText confidence="0.999891952380952">
the DUC 2006 task with gold-standard SCU an-
notation in abstracts. The Pyramid score (see Ta-
ble 6) is re-calculated for the system with best
ROUGE scores in DUC 2006 (Jagarlamudi et al.,
2006) along with our system by the same annota-
tor to make a meaningful comparison.
We further evaluate the linguistic quality (LQ)
of the summaries for the same 10 topics in ac-
cordance with the measurement in Dang (2006).
Four native speakers who are undergraduate stu-
dents in computer science (none are authors) per-
formed the task, We compare our system based
on HEAD-driven beam search with MULTI-scorer
to the best systems in DUC 2006 achieving top
ROUGE scores (Jagarlamudi et al., 2006) (Best
DUC system (ROUGE)) and top linguistic quality
scores (Lacatusu et al., 2006) (Best DUC system
(LQ))7. The average score and standard deviation
for each metric is displayed in Table 6. Our sys-
tem achieves a higher Pyramid score, an indication
that it captures more of the salient facts. We also
</bodyText>
<footnote confidence="0.969848666666667">
7Lacatusu et al. (2006) obtain the best scores in three lin-
guistic quality metrics (i.e. grammaticality, focus, structure
and coherence), and overall responsiveness on DUC 2006.
</footnote>
<bodyText confidence="0.995671875">
attain better non-redundancy than Jagarlamudi et
al. (2006), meaning that human raters perceive
less replicative content in our summaries. Scores
for other metrics are comparable to Jagarlamudi
et al. (2006) and Lacatusu et al. (2006), which
either uses minimal non-learning-based compres-
sion rules or is a pure extractive system. However,
our compression system sometimes generates less
grammatical sentences, and those are mostly due
to parsing errors. For example, parsing a clause
starting with a past tense verb as an adverbial
clausal modifier can lead to an ill-formed com-
pression. Those issues can be addressed by an-
alyzing k-best parse trees and we leave it in the
future work. A sample summary from our multi-
scorer based system is in Figure 3.
</bodyText>
<subsectionHeader confidence="0.677352">
Sentence Compression Evaluation. We
</subsectionHeader>
<bodyText confidence="0.999330142857143">
also evaluate sentence compression separately
on (Clarke and Lapata, 2008), adopting the same
partitions as (Martins and Smith, 2009), i.e. 1,188
sentences for training and 441 for testing. Our
compression models are compared with Hedge
Trimmer (Dorr et al., 2003), a discriminative
model proposed by McDonald (2006) and a
</bodyText>
<page confidence="0.961672">
1391
</page>
<table confidence="0.999885111111111">
System C Rate Uni-Prec Uni-Rec Uni-F1 Rel-F1
HedgeTrimmer 57.64% 0.72 0.65 0.64 0.50
McDonald (2006) 70.95% 0.77 0.78 0.77 0.55
Martins and Smith (2009) 71.35% 0.77 0.78 0.77 0.56
Rule-based 87.65% 0.74 0.91 0.80 0.63
Sequence 70.79% 0.77 0.80 0.76 0.58
Tree (BASIC) 69.65% 0.77 0.79 0.75 0.56
Tree (CONTEXT) 67.01% 0.79 0.78 0.76 0.57
Tree (HEAD) 68.06% 0.79 0.80 0.77 0.59
</table>
<tableCaption confidence="0.989963">
Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
all use single-scorer. Our context-aware and head-driven tree-based approaches outperform all the other systems
significantly (p &lt; 0.01) in precision (Uni-Prec) without sacrificing the recalls (i.e. there is no statistically signifi-
cant difference between our models and McDonald (2006) / M &amp; S (2009) with p &gt; 0.05). Italicized numbers for
unigram F1 (Uni-F1) are statistically indistinguishable (p &gt; 0.05). Our head-driven tree-based approach also pro-
duces significantly better grammatical relations F1 scores (Rel-F1) than all the other systems except the rule-based
method (p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.996319318181818">
Topic D0626H: How were the bombings of the US em-
bassies in Kenya and Tanzania conducted? What terror-
ist groups and individuals were responsible? How and
where were the attacks planned?
WASHINGTON, August 13 (Xinhua) – President Bill
Clinton Thursday condemned terrorist bomb attacks at
U.S. embassies in Kenya and Tanzania and vowed to find
the bombers and bring them to justice. Clinton met with
his top aides Wednesday in the White House to assess the
situation following the twin bombings at U.S. embassies
in Kenya and Tanzania, which have killed more than 250
people and injured over 5,000, most of them Kenyans and
Tanzanians. Local sources said the plan to bomb U.S. em-
bassies in Kenya and Tanzania took three months to com-
plete and bombers destined for Kenya were dispatched
through Somali and Rwanda. FBI Director Louis Freeh,
Attorney General Janet Reno and other senior U.S. gov-
ernment officials will hold a news conference at 1 p.m.
EDT (1700GMT) at FBI headquarters in Washington “to
announce developments in the investigation of the bomb-
ings of the U.S. embassies in Kenya and Tanzania,” the
FBI said in a statement....
</bodyText>
<figureCaption confidence="0.7584998">
Figure 3: Part of the summary generated by the multi-
scorer based summarizer for topic D0626H (DUC
2006). Grayed out words are removed. Query-
irrelevant phrases, such as temporal information or
source of the news, have been removed.
</figureCaption>
<bodyText confidence="0.999898666666667">
dependency-tree based compressor (Martins and
Smith, 2009)8. We adopt the metrics in Martins
and Smith (2009) to measure the unigram-level
macro precision, recall, and F1-measure with
respect to human annotated compression. In
addition, we also compute the F1 scores of
grammatical relations which are annotated by
RASP (Briscoe and Carroll, 2002) according
to Clarke and Lapata (2008).
In Table 7, our context-aware and head-driven
tree-based compression systems show statistically
significantly (p &lt; 0.01) higher precisions (Uni-
</bodyText>
<footnote confidence="0.396485">
8Thanks to Andr´e F.T. Martins for system outputs.
</footnote>
<bodyText confidence="0.9989461">
Prec) than all the other systems, without decreas-
ing the recalls (Uni-Rec) significantly (p &gt; 0.05)
based on a paired t-test. Unigram F1 scores (Uni-
F1) in italics indicate that the corresponding sys-
tems are not statistically distinguishable (p &gt;
0.05). For grammatical relation evaluation, our
head-driven tree-based system obtains statistically
significantly (p &lt; 0.01) better F1 score (Rel-F1
than all the other systems except the rule-based
system).
</bodyText>
<sectionHeader confidence="0.998376" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999819142857143">
We have presented a framework for query-focused
multi-document summarization based on sentence
compression. We propose three types of com-
pression approaches. Our tree-based compres-
sion method can easily incorporate measures of
query relevance, content importance, redundancy
and language quality into the compression pro-
cess. By testing on a standard dataset using the
automatic metric ROUGE, our models show sub-
stantial improvement over pure extraction-based
methods and state-of-the-art systems. Our best
system also yields better results for human eval-
uation based on Pyramid and achieves comparable
linguistic quality scores.
</bodyText>
<sectionHeader confidence="0.999205" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.5989705">
This work was supported in part by National Sci-
ence Foundation Grant IIS-0968450 and a gift
from Boeing. We thank Ding-Jung Han, Young-
Suk Lee, Xiaoqiang Luo, Sameer Maskey, Myle
Ott, Salim Roukos, Yiye Ruan, Ming Tan, Todd
Ward, Bowen Zhou, and the ACL reviewers for
valuable suggestions and advice on various as-
pects of this work.
</reference>
<page confidence="0.99669">
1392
</page>
<sectionHeader confidence="0.982695" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997620936936938">
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax directed
translations and the pushdown assembler. J. Comput. Syst.
Sci., 3(1):37–56.
Regina Barzilay, Noemie Elhadad, and Kathleen R. McKe-
own. 2002. Inferring strategies for sentence ordering in
multidocument news summarization. J. Artif. Int. Res.,
17(1):35–55, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011.
Jointly learning to extract and compress. ACL ’11, pages
481–490, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput. Linguist.,
22(1):39–71, March.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural
Language Processing with Python. O’Reilly Media.
T. Briscoe and J. Carroll. 2002. Robust accurate statistical
annotation of general text.
Christopher J.C. Burges, Robert Ragno, and Quoc Viet Le.
2007. Learning to rank with nonsmooth cost functions. In
B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances
in Neural Information Processing Systems 19, pages 193–
200. MIT Press, Cambridge, MA.
Christopher J. C. Burges. 2010. From RankNet to Lamb-
daRank to LambdaMART: An overview. Technical report,
Microsoft Research.
Asli Celikyilmaz and Dilek Hakkani-T¨ur. 2011. Discovery
of topically coherent sentences for extractive summariza-
tion. ACL ’11, pages 491–499, Stroudsburg, PA, USA.
Association for Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global inference
for sentence compression an integer linear programming
approach. J. Artif. Int. Res., 31(1):399–429, March.
John M. Conroy, Judith D. Schlesinger, Dianne P. O’Leary,
and Jade Goldstein, 2006. Back to Basics: CLASSY 2006.
U.S. National Inst. of Standards and Technology.
Hoa T. Dang. 2005. Overview of DUC 2005. In Document
Understanding Conference.
Hoa Tran Dang. 2006. Overview of DUC 2006. In
Proc. Document Understanding Workshop, page 10 pages.
NIST.
Hoa T. Dang. 2007. Overview of DUC 2007. In Document
Understanding Conference.
Van Dang. 2011. RankLib. Online.
Hal Daum´e, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. ACL ’06, pages 305–312,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Sashka T. Davis, John M. Conroy, and Judith D. Schlesinger.
2012. Occams - an optimal combinatorial covering algo-
rithm for multi-document summarization. In ICDM Work-
shops, pages 454–463.
Bonnie J Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: a parse-and-trim approach to headline
generation. In Proceedings of the HLT-NAACL 03 on
Text summarization workshop - Volume 5, HLT-NAACL-
DUC ’03, pages 1 – 8, Stroudsburg, PA, USA. Association
for Computational Linguistics, Association for Computa-
tional Linguistics.
G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank: graph-
based lexical centrality as salience in text summarization.
J. Artif. Int. Res., 22(1):457–479, December.
Radu Florian, Hany Hassan, Abraham Ittycheriah, Hongyan
Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov,
and Salim Roukos. 2004. A statistical model for multilin-
gual entity detection and tracking. In HLT-NAACL, pages
1–8.
Maria Fuentes, Enrique Alfonseca, and Horacio Rodriguez.
2007. Support vector machines for query-focused sum-
marization trained and evaluated on pyramid data. In Pro-
ceedings of the 45th Annual Meeting of the ACL on In-
teractive Poster and Demonstration Sessions, ACL ’07,
pages 57–60, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lexicalized
Markov grammars for sentence compression. NAACL
’07, pages 180–187, Rochester, New York, April. Asso-
ciation for Computational Linguistics.
Dan Gillick and Benoit Favre. 2009. A scalable global model
for summarization. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Langauge Process-
ing, ILP ’09, pages 10–18, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
David Graff. 2003. English Gigaword.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summarization.
NAACL ’09, pages 362–370, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten. 2009.
The weka data mining software: an update. SIGKDD Ex-
plor. Newsl., 11(1):10–18, November.
Jagadeesh Jagarlamudi, Prasad Pingali, and Vasudeva Varma,
2006. Query Independent Sentence Scoring approach to
DUC 2006.
J. Peter Kincaid, Robert P. Fishburne, Richard L. Rogers, and
Brad S. Chissom. 1975. Derivation of New Readability
Formulas (Automated Readability Index, Fog Count and
Flesch Reading Ease Formula) for Navy Enlisted Person-
nel. Technical report, February.
Kevin Knight and Daniel Marcu. 2000. Statistics-based sum-
marization - step one: Sentence compression. AAAI ’00,
pages 703–710. AAAI Press.
Finley Lacatusu, Andrew Hickl, Kirk Roberts, Ying Shi,
Jeremy Bensley, Bryan Rink, Patrick Wang, and Lara Tay-
lor, 2006. LCCs gistexter at duc 2006: Multi-strategy
multi-document summarization.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
</reference>
<page confidence="0.586764">
1393
</page>
<reference confidence="0.999500377777778">
Proceedings of the Eighteenth International Conference
on Machine Learning, ICML ’01, pages 282–289, San
Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
P. M. Lewis, II and R. E. Stearns. 1968. Syntax-directed
transduction. J. ACM, 15(3):465–488, July.
Hui Lin and Jeff Bilmes. 2011. A class of submodular func-
tions for document summarization. In Proceedings of the
49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume 1,
HLT ’11, pages 510–520, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2000. The automated ac-
quisition of topic signatures for text summarization. In
Proceedings of the 18th conference on Computational
linguistics - Volume 1, COLING ’00, pages 495–501,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic eval-
uation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computational
Linguistics on Human Language Technology - Volume 1,
pages 71–78.
Chin-Yew Lin. 2003. Improving summarization perfor-
mance by sentence compression: a pilot study. In Pro-
ceedings of the sixth international workshop on Informa-
tion retrieval with Asian languages - Volume 11, AsianIR
’03, pages 1–8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual
coreference resolution with syntactic features. In
HLT/EMNLP.
Xiaoqiang Luo, Abraham Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based on
the bell tree. In ACL, pages 135–142.
Andr´e F. T. Martins and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and com-
pression. In Proceedings of the Workshop on Integer Lin-
ear Programming for Natural Langauge Processing, ILP
’09, pages 1–9, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.cs.umass.edu.
Ryan McDonald. 2006. Discriminative Sentence Compres-
sion with Soft Syntactic Constraints. In Proceedings of
the 11th˜EACL, Trento, Italy, April.
Michael Mozer, Michael I. Jordan, and Thomas Petsche, ed-
itors. 1997. Advances in Neural Information Processing
Systems 9, NIPS, Denver, CO, USA, December 2-5, 1996.
MIT Press.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluating
content selection in summarization: The pyramid method.
In Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, HLT-NAACL 2004: Main Proceedings, pages 145–
152, Boston, Massachusetts, USA, May 2 - May 7. Asso-
ciation for Computational Linguistics.
Jahna Otterbacher, G¨unes¸ Erkan, and Dragomir R. Radev.
2005. Using random walks for question-focused sentence
retrieval. In Proceedings of the conference on Human
Language Technology and Empirical Methods in Natural
Language Processing, HLT ’05, pages 915–922, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.
You Ouyang, Wenjie Li, Sujian Li, and Qin Lu. 2011.
Applying regression models to query-focused multi-
document summarization. Inf. Process. Manage.,
47(2):227–237, March.
Prasad Pingali, Rahul K, and Vasudeva Varma, 2007. IIIT
Hyderabad at DUC 2007. U.S. National Inst. of Standards
and Technology.
Chao Shen and Tao Li. 2011. Learning to rank for query-
focused multi-document summarization. In Diane J.
Cook, Jian Pei, Wei Wang 0010, Osmar R. Zaane, and
Xindong Wu, editors, ICDM, pages 626–634. IEEE.
Andreas Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In Proceedings of ICSLP, volume 2,
pages 901–904, Denver, USA.
Kristina Toutanova, Chris Brockett, Michael Gamon, Ja-
gadeesh Jagarlamudi, Hisami Suzuki, and Lucy Vander-
wende. 2007. The PYTHY Summarization System: Mi-
crosoft Research at DUC 2007. In Proc. of DUC.
Jenine Turner and Eugene Charniak. 2005. Supervised and
unsupervised learning for sentence compression. ACL
’05, pages 290–297, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David Zajic, Bonnie J Dorr, Jimmy Lin, and R. Schwartz.
2006. Sentence compression as a component of a multi-
document summarization system. Proceedings of the
2006 Document Understanding Workshop, New York.
</reference>
<page confidence="0.994911">
1394
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.364455">
<title confidence="0.999196">A Sentence Compression Based Framework to Multi-Document Summarization</title>
<author confidence="0.995268">Hema Vittorio Radu Claire</author>
<affiliation confidence="0.56428">of Computer Science, Cornell University, Ithaca, NY 14853,</affiliation>
<address confidence="0.829729">T. J. Watson Research Center, Yorktown Heights, NY 10598,</address>
<email confidence="0.724173">vittorio,</email>
<abstract confidence="0.9967927">We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This work was supported in part by National Science Foundation Grant IIS-0968450 and a gift from Boeing. We thank Ding-Jung Han, YoungSuk Lee, Xiaoqiang Luo, Sameer Maskey, Myle Ott, Salim Roukos,</title>
<location>Yiye Ruan, Ming Tan, Todd Ward, Bowen</location>
<marker></marker>
<rawString>This work was supported in part by National Science Foundation Grant IIS-0968450 and a gift from Boeing. We thank Ding-Jung Han, YoungSuk Lee, Xiaoqiang Luo, Sameer Maskey, Myle Ott, Salim Roukos, Yiye Ruan, Ming Tan, Todd Ward, Bowen Zhou, and the ACL reviewers for valuable suggestions and advice on various aspects of this work.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Syntax directed translations and the pushdown assembler.</title>
<date>1969</date>
<journal>J. Comput. Syst. Sci.,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="9035" citStr="Aho and Ullman, 1969" startWordPosition="1326" endWordPosition="1329"> approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by using integer linear programming. 3 The Framework We now present our query-focused MDS </context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax directed translations and the pushdown assembler. J. Comput. Syst. Sci., 3(1):37–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Inferring strategies for sentence ordering in multidocument news summarization.</title>
<date>2002</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="14194" citStr="Barzilay et al., 2002" startWordPosition="2103" endWordPosition="2106">VP/ADJP chunk? Semantic Features dependency relation (dep rel) parent/grandparent dep rel is the root? has a depth larger than 3/5? is a predicate? semantic role label Rule-Based Features For each rule in Table 2 , we construct a corresponding feature to indicate whether the token is identified by the rule. Table 3: Token-level features for sequence-based compression. We replace each pronoun with its referent unless they appear in the same sentence. For sentence ordering, each compressed sentence is assigned to the most similar (tf-idf) query sentence. Then a Chronological Ordering algorithm (Barzilay et al., 2002) sorts the sentences for each query based first on the time stamp, and then the position in the source document. 4 Sentence Compression Sentence compression is typically formulated as the problem of removing secondary information from a sentence while maintaining its grammaticality and semantic structure (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008). We leave other rewrite operations, such as paraphrasing and reordering, for future work. Below we describe the sentence compression approaches developed in this research: RULE-BASED COMPRESSION, SEQUEN</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>Regina Barzilay, Noemie Elhadad, and Kathleen R. McKeown. 2002. Inferring strategies for sentence ordering in multidocument news summarization. J. Artif. Int. Res., 17(1):35–55, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<journal>ACL</journal>
<volume>11</volume>
<pages>481--490</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8727" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="1281" endWordPosition="1284">. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while </context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. ACL ’11, pages 481–490, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
</authors>
<location>and Stephen</location>
<marker>Berger, Pietra, </marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Comput. Linguist.,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Pietra, 1996</marker>
<rawString>A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Comput. Linguist., 22(1):39–71, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media.</booktitle>
<contexts>
<context position="28232" citStr="Bird et al., 2009" startWordPosition="4439" endWordPosition="4442">g of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3. As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40 topics. MaxEnt classifiers are used for tree-based compression. Beam size is fixed at 2000.4 Sentence compressions are evaluated by a 5-gram language model trained on Gigaword (Graff, 2003) by SRILM (Stolcke, 2002). 6 Results The results in Table 5 use the official ROUGE software with standard options5 and report ROUGE2 (R-2) (measures bigram overlap) and ROUGESU4 (R-SU4) (measures unigram and skip-bigram separated by up to four words). We compare our sentence-compression-based methods to the best perfor</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<contexts>
<context position="37858" citStr="Briscoe and Carroll, 2002" startWordPosition="5993" endWordPosition="5996"> and Tanzania,” the FBI said in a statement.... Figure 3: Part of the summary generated by the multiscorer based summarizer for topic D0626H (DUC 2006). Grayed out words are removed. Queryirrelevant phrases, such as temporal information or source of the news, have been removed. dependency-tree based compressor (Martins and Smith, 2009)8. We adopt the metrics in Martins and Smith (2009) to measure the unigram-level macro precision, recall, and F1-measure with respect to human annotated compression. In addition, we also compute the F1 scores of grammatical relations which are annotated by RASP (Briscoe and Carroll, 2002) according to Clarke and Lapata (2008). In Table 7, our context-aware and head-driven tree-based compression systems show statistically significantly (p &lt; 0.01) higher precisions (Uni8Thanks to Andr´e F.T. Martins for system outputs. Prec) than all the other systems, without decreasing the recalls (Uni-Rec) significantly (p &gt; 0.05) based on a paired t-test. Unigram F1 scores (UniF1) in italics indicate that the corresponding systems are not statistically distinguishable (p &gt; 0.05). For grammatical relation evaluation, our head-driven tree-based system obtains statistically significantly (p &lt; 0</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>T. Briscoe and J. Carroll. 2002. Robust accurate statistical annotation of general text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher J C Burges</author>
<author>Robert Ragno</author>
<author>Quoc Viet Le</author>
</authors>
<title>Learning to rank with nonsmooth cost functions.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>193--200</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="10406" citStr="Burges et al., 2007" startWordPosition="1541" endWordPosition="1544">ach sentence given the query. Then, a sentence compressor iteratively generates the most likely succinct versions of the ranked sentences, which are cumulatively added to the summary, until a length limit is reached. Finally, the postprocessing stage applies coreference resolution and sentence reordering to build the summary. Sentence Ranking. This stage aims to rank sentences in order of relevance to the query. Unsurprisingly, ranking algorithms have been successfully applied to this task. We experimented with two of them – Support Vector Regression (SVR) (Mozer et al., 1997) and LambdaMART (Burges et al., 2007). The former has been used previously for MDS (Ouyang et al., 2011). LambdaMart on the other hand has shown considerable success in information retrieval tasks (Burges, 2010); we are the first to apply it to summarization. For training, we use 40 topics (i.e. queries) from the DUC 2005 corpus (Dang, 2005) along with their manually generated abstracts. As in previous work (Shen and Li, Basic Features relative/absolute position is among the first 1/3/5 sentences? number of words (with/without stopwords) number of words more than 5/10 (with/without stopwords) Query-Relevant Features unigram/bigra</context>
</contexts>
<marker>Burges, Ragno, Le, 2007</marker>
<rawString>Christopher J.C. Burges, Robert Ragno, and Quoc Viet Le. 2007. Learning to rank with nonsmooth cost functions. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 193– 200. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher J C Burges</author>
</authors>
<title>From RankNet to LambdaRank to LambdaMART: An overview.</title>
<date>2010</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="10580" citStr="Burges, 2010" startWordPosition="1571" endWordPosition="1572">y, until a length limit is reached. Finally, the postprocessing stage applies coreference resolution and sentence reordering to build the summary. Sentence Ranking. This stage aims to rank sentences in order of relevance to the query. Unsurprisingly, ranking algorithms have been successfully applied to this task. We experimented with two of them – Support Vector Regression (SVR) (Mozer et al., 1997) and LambdaMART (Burges et al., 2007). The former has been used previously for MDS (Ouyang et al., 2011). LambdaMart on the other hand has shown considerable success in information retrieval tasks (Burges, 2010); we are the first to apply it to summarization. For training, we use 40 topics (i.e. queries) from the DUC 2005 corpus (Dang, 2005) along with their manually generated abstracts. As in previous work (Shen and Li, Basic Features relative/absolute position is among the first 1/3/5 sentences? number of words (with/without stopwords) number of words more than 5/10 (with/without stopwords) Query-Relevant Features unigram/bigram/skip bigram (at most four words apart) overlap unigram/bigram TF/TF-IDF similarity mention overlap subject/object/indirect object overlap semantic role overlap relation ove</context>
</contexts>
<marker>Burges, 2010</marker>
<rawString>Christopher J. C. Burges. 2010. From RankNet to LambdaRank to LambdaMART: An overview. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Discovery of topically coherent sentences for extractive summarization.</title>
<date>2011</date>
<journal>ACL</journal>
<volume>11</volume>
<pages>491--499</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Celikyilmaz, Hakkani-T¨ur, 2011</marker>
<rawString>Asli Celikyilmaz and Dilek Hakkani-T¨ur. 2011. Discovery of topically coherent sentences for extractive summarization. ACL ’11, pages 491–499, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression an integer linear programming approach.</title>
<date>2008</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="4184" citStr="Clarke and Lapata, 2008" startWordPosition="606" endWordPosition="609">said]. In this example, the compressed sentence is rela1From DUC 2005, query for topic d422g. 1384 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1384–1394, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tively more succinct and readable than the original (e.g. in terms of Flesch-Kincaid Reading Ease Score (Kincaid et al., 1975)). Likewise, removing information irrelevant to the query (e.g. “11 years ago”, “police said”) is crucial for query-focused MDS. Sentence compression techniques (Knight and Marcu, 2000; Clarke and Lapata, 2008) are the standard for producing a compact and grammatical version of a sentence while preserving relevance, and prior research (e.g. Lin (2003)) has demonstrated their potential usefulness for generic document summarization. Similarly, strides have been made to incorporate sentence compression into query-focused MDS systems (Zajic et al., 2006). Most attempts, however, fail to produce better results than those of the best systems built on pure extraction-based approaches that use no sentence compression. In this paper we investigate the role of sentence compression techniques for query-focused</context>
<context position="9505" citStr="Clarke and Lapata (2008)" startWordPosition="1401" endWordPosition="1404">sy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by using integer linear programming. 3 The Framework We now present our query-focused MDS framework consisting of three steps: Sentence Ranking, Sentence Compression and Post-processing. First, sentence ranking determines the importance of each sentence given the query. Then, a sentence compressor iteratively generates the most likely succinct versions of the ranked sentences, which are cumulatively added to the summary, until a length limit is reached. Finally, the postprocessing stage applies coreference resolution and sentence reordering to build the </context>
<context position="14591" citStr="Clarke and Lapata, 2008" startWordPosition="2163" endWordPosition="2166">with its referent unless they appear in the same sentence. For sentence ordering, each compressed sentence is assigned to the most similar (tf-idf) query sentence. Then a Chronological Ordering algorithm (Barzilay et al., 2002) sorts the sentences for each query based first on the time stamp, and then the position in the source document. 4 Sentence Compression Sentence compression is typically formulated as the problem of removing secondary information from a sentence while maintaining its grammaticality and semantic structure (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008). We leave other rewrite operations, such as paraphrasing and reordering, for future work. Below we describe the sentence compression approaches developed in this research: RULE-BASED COMPRESSION, SEQUENCE-BASED COMPRESSION, and TREEBASED COMPRESSION. 4.1 Rule-based Compression Turner and Charniak (2005) have shown that applying hand-crafted rules for trimming sentences can improve both content and linguistic quality. Our rule-based approach extends existing work (Conroy et al., 2006; Toutanova et al., 2007) to create the linguistically-motivated compression rules of Table 2. To avoid ill-form</context>
<context position="27267" citStr="Clarke and Lapata (2008)" startWordPosition="4287" endWordPosition="4290">ang, 2006; Dang, 2007), each of which is a collection of newswire articles. 50 complex queries (topics) are provided for DUC 2005 and 2006, 35 are collected for DUC 2007 main task. Relevant documents for each query are provided along with 4 to 9 human MDS abstracts. The task is to generate a summary within 250 words to address the query. We split DUC 2005 into two parts: 40 topics to train the sentence ranking models, and 10 for ranking algorithm selection and parameter tuning for the multiscorer. DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke and Lapata (2008) is used to train the CRF and MaxEnt classifiers (Section 4). It includes 82 newswire articles with one manually produced compression aligned to each sentence. Preprocessing. Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support v</context>
<context position="34820" citStr="Clarke and Lapata, 2008" startWordPosition="5509" endWordPosition="5512">h either uses minimal non-learning-based compression rules or is a pure extractive system. However, our compression system sometimes generates less grammatical sentences, and those are mostly due to parsing errors. For example, parsing a clause starting with a past tense verb as an adverbial clausal modifier can lead to an ill-formed compression. Those issues can be addressed by analyzing k-best parse trees and we leave it in the future work. A sample summary from our multiscorer based system is in Figure 3. Sentence Compression Evaluation. We also evaluate sentence compression separately on (Clarke and Lapata, 2008), adopting the same partitions as (Martins and Smith, 2009), i.e. 1,188 sentences for training and 441 for testing. Our compression models are compared with Hedge Trimmer (Dorr et al., 2003), a discriminative model proposed by McDonald (2006) and a 1391 System C Rate Uni-Prec Uni-Rec Uni-F1 Rel-F1 HedgeTrimmer 57.64% 0.72 0.65 0.64 0.50 McDonald (2006) 70.95% 0.77 0.78 0.77 0.55 Martins and Smith (2009) 71.35% 0.77 0.78 0.77 0.56 Rule-based 87.65% 0.74 0.91 0.80 0.63 Sequence 70.79% 0.77 0.80 0.76 0.58 Tree (BASIC) 69.65% 0.77 0.79 0.75 0.56 Tree (CONTEXT) 67.01% 0.79 0.78 0.76 0.57 Tree (HEAD</context>
<context position="37896" citStr="Clarke and Lapata (2008)" startWordPosition="5999" endWordPosition="6002">ent.... Figure 3: Part of the summary generated by the multiscorer based summarizer for topic D0626H (DUC 2006). Grayed out words are removed. Queryirrelevant phrases, such as temporal information or source of the news, have been removed. dependency-tree based compressor (Martins and Smith, 2009)8. We adopt the metrics in Martins and Smith (2009) to measure the unigram-level macro precision, recall, and F1-measure with respect to human annotated compression. In addition, we also compute the F1 scores of grammatical relations which are annotated by RASP (Briscoe and Carroll, 2002) according to Clarke and Lapata (2008). In Table 7, our context-aware and head-driven tree-based compression systems show statistically significantly (p &lt; 0.01) higher precisions (Uni8Thanks to Andr´e F.T. Martins for system outputs. Prec) than all the other systems, without decreasing the recalls (Uni-Rec) significantly (p &gt; 0.05) based on a paired t-test. Unigram F1 scores (UniF1) in italics indicate that the corresponding systems are not statistically distinguishable (p &gt; 0.05). For grammatical relation evaluation, our head-driven tree-based system obtains statistically significantly (p &lt; 0.01) better F1 score (Rel-F1 than all </context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression an integer linear programming approach. J. Artif. Int. Res., 31(1):399–429, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>Dianne P O’Leary</author>
<author>Jade Goldstein</author>
</authors>
<date>2006</date>
<booktitle>Back to Basics: CLASSY 2006. U.S. National Inst. of Standards and Technology.</booktitle>
<marker>Conroy, Schlesinger, O’Leary, Goldstein, 2006</marker>
<rawString>John M. Conroy, Judith D. Schlesinger, Dianne P. O’Leary, and Jade Goldstein, 2006. Back to Basics: CLASSY 2006. U.S. National Inst. of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa T Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2005</date>
<booktitle>In Document Understanding Conference.</booktitle>
<contexts>
<context position="10712" citStr="Dang, 2005" startWordPosition="1596" endWordPosition="1597">the summary. Sentence Ranking. This stage aims to rank sentences in order of relevance to the query. Unsurprisingly, ranking algorithms have been successfully applied to this task. We experimented with two of them – Support Vector Regression (SVR) (Mozer et al., 1997) and LambdaMART (Burges et al., 2007). The former has been used previously for MDS (Ouyang et al., 2011). LambdaMart on the other hand has shown considerable success in information retrieval tasks (Burges, 2010); we are the first to apply it to summarization. For training, we use 40 topics (i.e. queries) from the DUC 2005 corpus (Dang, 2005) along with their manually generated abstracts. As in previous work (Shen and Li, Basic Features relative/absolute position is among the first 1/3/5 sentences? number of words (with/without stopwords) number of words more than 5/10 (with/without stopwords) Query-Relevant Features unigram/bigram/skip bigram (at most four words apart) overlap unigram/bigram TF/TF-IDF similarity mention overlap subject/object/indirect object overlap semantic role overlap relation overlap Query-Independent Features average/total unigram/bigram IDF/TF-IDF unigram/bigram TF/TF-IDF similarity with the centroid of the</context>
<context position="26640" citStr="Dang, 2005" startWordPosition="4178" endWordPosition="4179">e in Table 2 , we construct a corresponding feature to indicate whether the token is identified by the rule. 1389 The multi-scorer is defined as a linear combination of the component scorers: Let (α0, ... , α4), 0 &lt; αi &lt; 1, −−−� score = (scoreBasic, scoreQ, scoreim, scorelm, scorered), S = scoremulti = α~ · −−−� score (1) The parameters α~ are tuned on a held-out tuning set by grid search. We linearly normalize the score of each metric, where the minimum and maximum values are estimated from the tuning data. 5 Experimental Setup We evaluate our methods on the DUC 2005, 2006 and 2007 datasets (Dang, 2005; Dang, 2006; Dang, 2007), each of which is a collection of newswire articles. 50 complex queries (topics) are provided for DUC 2005 and 2006, 35 are collected for DUC 2007 main task. Relevant documents for each query are provided along with 4 to 9 human MDS abstracts. The task is to generate a summary within 250 words to address the query. We split DUC 2005 into two parts: 40 topics to train the sentence ranking models, and 10 for ranking algorithm selection and parameter tuning for the multiscorer. DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset fr</context>
</contexts>
<marker>Dang, 2005</marker>
<rawString>Hoa T. Dang. 2005. Overview of DUC 2005. In Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Tran Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2006</date>
<booktitle>In Proc. Document Understanding Workshop,</booktitle>
<pages>10</pages>
<contexts>
<context position="26652" citStr="Dang, 2006" startWordPosition="4180" endWordPosition="4181"> , we construct a corresponding feature to indicate whether the token is identified by the rule. 1389 The multi-scorer is defined as a linear combination of the component scorers: Let (α0, ... , α4), 0 &lt; αi &lt; 1, −−−� score = (scoreBasic, scoreQ, scoreim, scorelm, scorered), S = scoremulti = α~ · −−−� score (1) The parameters α~ are tuned on a held-out tuning set by grid search. We linearly normalize the score of each metric, where the minimum and maximum values are estimated from the tuning data. 5 Experimental Setup We evaluate our methods on the DUC 2005, 2006 and 2007 datasets (Dang, 2005; Dang, 2006; Dang, 2007), each of which is a collection of newswire articles. 50 complex queries (topics) are provided for DUC 2005 and 2006, 35 are collected for DUC 2007 main task. Relevant documents for each query are provided along with 4 to 9 human MDS abstracts. The task is to generate a summary within 250 words to address the query. We split DUC 2005 into two parts: 40 topics to train the sentence ranking models, and 10 for ranking algorithm selection and parameter tuning for the multiscorer. DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke an</context>
<context position="32567" citStr="Dang (2006)" startWordPosition="5145" endWordPosition="5146"> system (ROUGE) 22.9±8.2 3.5±0.9 3.5±1.0 3.5±1.1 3.6±1.0 2.9±1.1 Best DUC system (LQ) – 4.0±0.8 4.2±0.7 3.8±0.7 3.6±0.9 3.4±0.9 Our System 26.4±10.3 3.0±0.9 4.0±1.1 3.6±1.0 3.4±0.9 2.8±1.0 Table 6: Human evaluation on our multi-scorer based system, Jagarlamudi et al. (2006) (Best DUC system (ROUGE)), and Lacatusu et al. (2006) (Best DUC system (LQ)). Our system can synthesize more relevant content according to Pyramid (×100). We also examine linguistic quality (LQ) in Grammaticality (Gra), Non-redundancy (Non-Red), Referential clarity (Ref), Focus (Foc), and Structure and Coherence (Coh) like Dang (2006), each rated from 1 (very poor) to 5 (very good). Our system has better non-redundancy than Jagarlamudi et al. (2006) and is comparable to Jagarlamudi et al. (2006) and Lacatusu et al. (2006) in other metrics except grammaticality. the DUC 2006 task with gold-standard SCU annotation in abstracts. The Pyramid score (see Table 6) is re-calculated for the system with best ROUGE scores in DUC 2006 (Jagarlamudi et al., 2006) along with our system by the same annotator to make a meaningful comparison. We further evaluate the linguistic quality (LQ) of the summaries for the same 10 topics in accordan</context>
</contexts>
<marker>Dang, 2006</marker>
<rawString>Hoa Tran Dang. 2006. Overview of DUC 2006. In Proc. Document Understanding Workshop, page 10 pages. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa T Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2007</date>
<booktitle>In Document Understanding Conference.</booktitle>
<contexts>
<context position="26665" citStr="Dang, 2007" startWordPosition="4182" endWordPosition="4183">uct a corresponding feature to indicate whether the token is identified by the rule. 1389 The multi-scorer is defined as a linear combination of the component scorers: Let (α0, ... , α4), 0 &lt; αi &lt; 1, −−−� score = (scoreBasic, scoreQ, scoreim, scorelm, scorered), S = scoremulti = α~ · −−−� score (1) The parameters α~ are tuned on a held-out tuning set by grid search. We linearly normalize the score of each metric, where the minimum and maximum values are estimated from the tuning data. 5 Experimental Setup We evaluate our methods on the DUC 2005, 2006 and 2007 datasets (Dang, 2005; Dang, 2006; Dang, 2007), each of which is a collection of newswire articles. 50 complex queries (topics) are provided for DUC 2005 and 2006, 35 are collected for DUC 2007 main task. Relevant documents for each query are provided along with 4 to 9 human MDS abstracts. The task is to generate a summary within 250 words to address the query. We split DUC 2005 into two parts: 40 topics to train the sentence ranking models, and 10 for ranking algorithm selection and parameter tuning for the multiscorer. DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke and Lapata (200</context>
</contexts>
<marker>Dang, 2007</marker>
<rawString>Hoa T. Dang. 2007. Overview of DUC 2007. In Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Van Dang</author>
</authors>
<date>2011</date>
<publisher>RankLib. Online.</publisher>
<marker>Van Dang, 2011</marker>
<rawString>Van Dang. 2011. RankLib. Online.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Bayesian query-focused summarization.</title>
<date>2006</date>
<journal>ACL</journal>
<volume>06</volume>
<pages>305--312</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e, III and Daniel Marcu. 2006. Bayesian query-focused summarization. ACL ’06, pages 305–312, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sashka T Davis</author>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
</authors>
<title>Occams - an optimal combinatorial covering algorithm for multi-document summarization.</title>
<date>2012</date>
<booktitle>In ICDM Workshops,</booktitle>
<pages>454--463</pages>
<contexts>
<context position="6027" citStr="Davis et al., 2012" startWordPosition="876" endWordPosition="879">ultimately resulting in significant improvements for MDS, while at the same time remaining competitive with existing methods in terms of sentence compression, as discussed next. We evaluate the summarization models on the standard Document Understanding Conference (DUC) 2006 and 2007 corpora 2 for queryfocused MDS and find that all of our compressionbased summarization models achieve statistically significantly better performance than the best DUC 2006 systems. Our best-performing system yields an 11.02 ROUGE-2 score (Lin and Hovy, 2003), a 8.0% improvement over the best reported score (10.2 (Davis et al., 2012)) on the 2We believe that we can easily adapt our system for tasks (e.g. TAC-08’s opinion summarization or TAC-09’s update summarization) or domains (e.g. web pages or wikipedia pages). We reserve that for future work. DUC 2006 dataset, and an 13.49 ROUGE-2, a 5.4% improvement over the best score in DUC 2007 (12.8 (Davis et al., 2012)). We also observe substantial improvements over previous systems w.r.t. the manual Pyramid (Nenkova and Passonneau, 2004) evaluation measure (26.4 vs. 22.9 (Jagarlamudi et al., 2006)); human annotators furthermore rate our system-generated summaries as having les</context>
<context position="7570" citStr="Davis et al. (2012)" startWordPosition="1110" endWordPosition="1113">focused multidocument summarization (MDS) largely relies on extractive approaches, where systems usually take as input a set of documents and select the top relevant sentences for inclusion in the final summary. A wide range of methods have been employed for this task. For unsupervised methods, sentence importance can be estimated by calculating topic signature words (Lin and Hovy, 2000; Conroy et al., 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al., 2005), or using a Bayesian model with sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compre</context>
<context position="28960" citStr="Davis et al. (2012)" startWordPosition="4562" endWordPosition="4565">ssifiers are used for tree-based compression. Beam size is fixed at 2000.4 Sentence compressions are evaluated by a 5-gram language model trained on Gigaword (Graff, 2003) by SRILM (Stolcke, 2002). 6 Results The results in Table 5 use the official ROUGE software with standard options5 and report ROUGE2 (R-2) (measures bigram overlap) and ROUGESU4 (R-SU4) (measures unigram and skip-bigram separated by up to four words). We compare our sentence-compression-based methods to the best performing systems based on ROUGE in DUC 2006 and 2007 (Jagarlamudi et al., 2006; Pingali et al., 2007), system by Davis et al. (2012) that report the best R-2 score on DUC 2006 and 2007 thus far, and to the purely extractive methods of SVR and LambdaMART. Our sentence-compression-based systems (marked with †) show statistically significant improvements over pure extractive summarization for both R-2 and R-SU4 (paired t-test, p &lt; 0.01). This means our systems can effectively remove redundancy within the summary through compression. Furthermore, our HEAD-driven beam search method with MULTI-scorer beats all systems on DUC 20066 and all systems on DUC 2007 except the best system in terms of R-2 (p &lt; 0.01). Its R-SU4 score is a</context>
<context position="30533" citStr="Davis et al. (2012)" startWordPosition="4816" endWordPosition="4819">nkova and Passonneau, 2004) evaluation was developed to manually assess how many relevant facts or Summarization Content Units (SCUs) are captured by system summaries. We ask a professional annotator (who is not one of the authors, is highly experienced in annotating for various NLP tasks, and is fluent in English) to carry out a Pyramid evaluation on 10 randomly selected topics from 4We looked at various beam sizes on the heldout data, and observed that the performance peaks around this value. 5ROUGE-1.5.5.pl -n 4 -w 1.2 -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -a -d 6The system output from Davis et al. (2012) is not available, so significance tests are not conducted on it. α~ = 1390 DUC 2006 DUC 2007 System C Rate R-2 R-SU4 C Rate R-2 R-SU4 Best DUC system – 9.56 15.53 – 12.62 17.90 Davis et al. (2012) – 10.2 15.2 – 12.8 17.5 SVR 100% 7.78 13.02 100% 9.53 14.69 LambdaMART 100% 9.84 14.63 100% 12.34 15.62 Rule-based 78.99% 10.62 *† 15.73 † 78.11% 13.18† 18.15† Sequence 76.34% 10.49 † 15.60 † 77.20% 13.25† 18.23† Tree (BASIC + ScoreBasic) 70.48% 10.49 † 15.86 † 69.27% 13.00† 18.29† Tree (CONTEXT + ScoreBasic) 65.21% 10.55 *† 16.10 † 63.44% 12.75 18.07† Tree (HEAD + ScoreBasic) 66.70% 10.66 *† 16.18 </context>
</contexts>
<marker>Davis, Conroy, Schlesinger, 2012</marker>
<rawString>Sashka T. Davis, John M. Conroy, and Judith D. Schlesinger. 2012. Occams - an optimal combinatorial covering algorithm for multi-document summarization. In ICDM Workshops, pages 454–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
<author>David Zajic</author>
<author>Richard Schwartz</author>
</authors>
<title>Hedge trimmer: a parse-and-trim approach to headline generation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 03 on Text summarization workshop - Volume 5, HLT-NAACLDUC ’03,</booktitle>
<tech>1 – 8,</tech>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics, Association for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="35010" citStr="Dorr et al., 2003" startWordPosition="5539" endWordPosition="5542">to parsing errors. For example, parsing a clause starting with a past tense verb as an adverbial clausal modifier can lead to an ill-formed compression. Those issues can be addressed by analyzing k-best parse trees and we leave it in the future work. A sample summary from our multiscorer based system is in Figure 3. Sentence Compression Evaluation. We also evaluate sentence compression separately on (Clarke and Lapata, 2008), adopting the same partitions as (Martins and Smith, 2009), i.e. 1,188 sentences for training and 441 for testing. Our compression models are compared with Hedge Trimmer (Dorr et al., 2003), a discriminative model proposed by McDonald (2006) and a 1391 System C Rate Uni-Prec Uni-Rec Uni-F1 Rel-F1 HedgeTrimmer 57.64% 0.72 0.65 0.64 0.50 McDonald (2006) 70.95% 0.77 0.78 0.77 0.55 Martins and Smith (2009) 71.35% 0.77 0.78 0.77 0.56 Rule-based 87.65% 0.74 0.91 0.80 0.63 Sequence 70.79% 0.77 0.80 0.76 0.58 Tree (BASIC) 69.65% 0.77 0.79 0.75 0.56 Tree (CONTEXT) 67.01% 0.79 0.78 0.76 0.57 Tree (HEAD) 68.06% 0.79 0.80 0.77 0.59 Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches all use single-scorer. Our context-aware and head-dri</context>
</contexts>
<marker>Dorr, Zajic, Schwartz, 2003</marker>
<rawString>Bonnie J Dorr, David Zajic, and Richard Schwartz. 2003. Hedge trimmer: a parse-and-trim approach to headline generation. In Proceedings of the HLT-NAACL 03 on Text summarization workshop - Volume 5, HLT-NAACLDUC ’03, pages 1 – 8, Stroudsburg, PA, USA. Association for Computational Linguistics, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: graphbased lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="2372" citStr="Erkan and Radev, 2004" startWordPosition="333" endWordPosition="336">er of information analysis applications including openended question answering, recommender systems, and summarization of search engine results. As further evidence of its importance, the Document Understanding Conference (DUC) has used queryfocused MDS as its main task since 2004 to foster new research on automatic summarization in the context of users’ needs. To date, most top-performing systems for multi-document summarization—whether queryspecific or not—remain largely extractive: their summaries are comprised exclusively of sentences selected directly from the documents to be summarized (Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-T¨ur, 2011). Despite their simplicity, extractive approaches have some disadvantages. First, lengthy sentences that are partly relevant are either excluded from the summary or (if selected) can block the selection of other important sentences, due to summary length constraints. In addition, when people write summaries, they tend to abstract the content and seldom use entire sentences taken verbatim from the original documents. In news articles, for example, most sentences are lengthy and contain both potentially useful information for a</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank: graphbased lexical centrality as salience in text summarization. J. Artif. Int. Res., 22(1):457–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Hany Hassan</author>
<author>Abraham Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Xiaoqiang Luo</author>
<author>Nicolas Nicolov</author>
<author>Salim Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="27724" citStr="Florian et al., 2004" startWordPosition="4352" endWordPosition="4356">and parameter tuning for the multiscorer. DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke and Lapata (2008) is used to train the CRF and MaxEnt classifiers (Section 4). It includes 82 newswire articles with one manually produced compression aligned to each sentence. Preprocessing. Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3. As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40 t</context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>Radu Florian, Hany Hassan, Abraham Ittycheriah, Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov, and Salim Roukos. 2004. A statistical model for multilingual entity detection and tracking. In HLT-NAACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Fuentes</author>
<author>Enrique Alfonseca</author>
<author>Horacio Rodriguez</author>
</authors>
<title>Support vector machines for query-focused summarization trained and evaluated on pyramid data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7827" citStr="Fuentes et al., 2007" startWordPosition="1146" endWordPosition="1149">for this task. For unsupervised methods, sentence importance can be estimated by calculating topic signature words (Lin and Hovy, 2000; Conroy et al., 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al., 2005), or using a Bayesian model with sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches.</context>
</contexts>
<marker>Fuentes, Alfonseca, Rodriguez, 2007</marker>
<rawString>Maria Fuentes, Enrique Alfonseca, and Horacio Rodriguez. 2007. Support vector machines for query-focused summarization trained and evaluated on pyramid data. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 57–60, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Lexicalized Markov grammars for sentence compression.</title>
<date>2007</date>
<journal>NAACL</journal>
<volume>07</volume>
<pages>180--187</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="9122" citStr="Galley and McKeown, 2007" startWordPosition="1338" endWordPosition="1342">, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by using integer linear programming. 3 The Framework We now present our query-focused MDS framework consisting of three steps: Sentence Ranking, Sentence Compression and Post-pr</context>
<context position="14565" citStr="Galley and McKeown, 2007" startWordPosition="2159" endWordPosition="2162">. We replace each pronoun with its referent unless they appear in the same sentence. For sentence ordering, each compressed sentence is assigned to the most similar (tf-idf) query sentence. Then a Chronological Ordering algorithm (Barzilay et al., 2002) sorts the sentences for each query based first on the time stamp, and then the position in the source document. 4 Sentence Compression Sentence compression is typically formulated as the problem of removing secondary information from a sentence while maintaining its grammaticality and semantic structure (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008). We leave other rewrite operations, such as paraphrasing and reordering, for future work. Below we describe the sentence compression approaches developed in this research: RULE-BASED COMPRESSION, SEQUENCE-BASED COMPRESSION, and TREEBASED COMPRESSION. 4.1 Rule-based Compression Turner and Charniak (2005) have shown that applying hand-crafted rules for trimming sentences can improve both content and linguistic quality. Our rule-based approach extends existing work (Conroy et al., 2006; Toutanova et al., 2007) to create the linguistically-motivated compression rules of </context>
<context position="17241" citStr="Galley and McKeown, 2007" startWordPosition="2589" endWordPosition="2593">word. Detailed descriptions of the training data and experimental setup are in Section 5. During inference, we find the maximally likely sequence Y according to a CRF with parameter θ (Y = arg maxY , P(Y &apos;|X; θ)), while simultaneously enforcing the rules of Table 2 to reduce the hypothesis space and encourage grammatical compression. To do this, we encode these rules as features for each token, and whenever these feature functions fire, we restrict the possible label for that token to “O”. 4.3 Tree-based Compression Our tree-based compression methods are in line with syntax-driven approaches (Galley and McKeown, 2007), where operations are carried out on parse tree constituents. Unlike previous work (Knight and Marcu, 2000; Galley and McKeown, 2007), we do not produce a new parse tree, 1387 Rule Example Header [MOSCOW, October 19 ( Xinhua ) –] Russian federal troops Tuesday continued... Relative dates ...Centers for Disease Control confirmed [Tuesday] that there was... Intra-sentential attribution ...fueling the La Nina weather phenomenon, [the U.N. weather agency said]. Lead adverbials [Interestingly], while the Democrats tend to talk about... Noun appositives Wayne County Prosecutor [John O’Hara] wanted </context>
</contexts>
<marker>Galley, McKeown, 2007</marker>
<rawString>Michel Galley and Kathleen McKeown. 2007. Lexicalized Markov grammars for sentence compression. NAACL ’07, pages 180–187, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
</authors>
<title>A scalable global model for summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP ’09,</booktitle>
<pages>10--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8504" citStr="Gillick and Favre, 2009" startWordPosition="1250" endWordPosition="1253">gned submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley </context>
</contexts>
<marker>Gillick, Favre, 2009</marker>
<rawString>Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP ’09, pages 10–18, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<date>2003</date>
<note>English Gigaword.</note>
<contexts>
<context position="22563" citStr="Graff, 2003" startWordPosition="3525" endWordPosition="3526">heses. Beam scores for other nodes are omitted. Table 4: Constituent-level features for tree-based compression. * or † denote features that are concatenated with every Syntactic Tree feature to compose a new one. 1996) trained at the constituent level using the features in Table 4. We also apply the rules of Table 2 during the decoding process. Concretely, if the words subsumed by a node are identified by any rule, we only consider REM as the node’s label. Given the N-best compressions from the decoder, we evaluate the yield of the trimmed trees using a language model trained on the Gigaword (Graff, 2003) corpus and return the compression with the highest probability. Thus, the decoder is quite flexible — its learned scoring function allows us to incorporate features salient for sentence compression while its language model guarantees the linguistic quality of the compressed string. In the sections below we consider additional improvements. 4.3.1 Improving Beam Search CONTEXT-aware search is based on the intuition that predictions on preceding context can be leveraged to facilitate the prediction of the current node. For example, parent nodes with children that have all been removed (retained)</context>
<context position="28512" citStr="Graff, 2003" startWordPosition="4490" endWordPosition="4491">th various rankers in RankLib (Dang, 2011)3. As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40 topics. MaxEnt classifiers are used for tree-based compression. Beam size is fixed at 2000.4 Sentence compressions are evaluated by a 5-gram language model trained on Gigaword (Graff, 2003) by SRILM (Stolcke, 2002). 6 Results The results in Table 5 use the official ROUGE software with standard options5 and report ROUGE2 (R-2) (measures bigram overlap) and ROUGESU4 (R-SU4) (measures unigram and skip-bigram separated by up to four words). We compare our sentence-compression-based methods to the best performing systems based on ROUGE in DUC 2006 and 2007 (Jagarlamudi et al., 2006; Pingali et al., 2007), system by Davis et al. (2012) that report the best R-2 score on DUC 2006 and 2007 thus far, and to the purely extractive methods of SVR and LambdaMART. Our sentence-compression-base</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>David Graff. 2003. English Gigaword.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<journal>NAACL</journal>
<volume>09</volume>
<pages>362--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2404" citStr="Haghighi and Vanderwende, 2009" startWordPosition="337" endWordPosition="340">sis applications including openended question answering, recommender systems, and summarization of search engine results. As further evidence of its importance, the Document Understanding Conference (DUC) has used queryfocused MDS as its main task since 2004 to foster new research on automatic summarization in the context of users’ needs. To date, most top-performing systems for multi-document summarization—whether queryspecific or not—remain largely extractive: their summaries are comprised exclusively of sentences selected directly from the documents to be summarized (Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-T¨ur, 2011). Despite their simplicity, extractive approaches have some disadvantages. First, lengthy sentences that are partly relevant are either excluded from the summary or (if selected) can block the selection of other important sentences, due to summary length constraints. In addition, when people write summaries, they tend to abstract the content and seldom use entire sentences taken verbatim from the original documents. In news articles, for example, most sentences are lengthy and contain both potentially useful information for a summary as well as unnecessary </context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. NAACL ’09, pages 362–370, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="27846" citStr="Hall et al., 2009" startWordPosition="4375" endWordPosition="4378">e dataset from Clarke and Lapata (2008) is used to train the CRF and MaxEnt classifiers (Section 4). It includes 82 newswire articles with one manually produced compression aligned to each sentence. Preprocessing. Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3. As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40 topics. MaxEnt classifiers are used for tree-based compression. Beam size is fixed at 2000.4 Sentence compressions are eval</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: an update. SIGKDD Explor. Newsl., 11(1):10–18, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jagadeesh Jagarlamudi</author>
<author>Prasad Pingali</author>
<author>Vasudeva Varma</author>
</authors>
<title>Query Independent Sentence Scoring approach to DUC</title>
<date>2006</date>
<contexts>
<context position="6546" citStr="Jagarlamudi et al., 2006" startWordPosition="960" endWordPosition="963">E-2 score (Lin and Hovy, 2003), a 8.0% improvement over the best reported score (10.2 (Davis et al., 2012)) on the 2We believe that we can easily adapt our system for tasks (e.g. TAC-08’s opinion summarization or TAC-09’s update summarization) or domains (e.g. web pages or wikipedia pages). We reserve that for future work. DUC 2006 dataset, and an 13.49 ROUGE-2, a 5.4% improvement over the best score in DUC 2007 (12.8 (Davis et al., 2012)). We also observe substantial improvements over previous systems w.r.t. the manual Pyramid (Nenkova and Passonneau, 2004) evaluation measure (26.4 vs. 22.9 (Jagarlamudi et al., 2006)); human annotators furthermore rate our system-generated summaries as having less redundancy and comparable quality w.r.t. other linguistic quality metrics. With these results we believe we are the first to successfully show that sentence compression can provide statistically significant improvements over pure extraction-based approaches for queryfocused MDS. 2 Related Work Existing research on query-focused multidocument summarization (MDS) largely relies on extractive approaches, where systems usually take as input a set of documents and select the top relevant sentences for inclusion in th</context>
<context position="28906" citStr="Jagarlamudi et al., 2006" startWordPosition="4552" endWordPosition="4555">ds a validation set, we use 10 out of 40 topics. MaxEnt classifiers are used for tree-based compression. Beam size is fixed at 2000.4 Sentence compressions are evaluated by a 5-gram language model trained on Gigaword (Graff, 2003) by SRILM (Stolcke, 2002). 6 Results The results in Table 5 use the official ROUGE software with standard options5 and report ROUGE2 (R-2) (measures bigram overlap) and ROUGESU4 (R-SU4) (measures unigram and skip-bigram separated by up to four words). We compare our sentence-compression-based methods to the best performing systems based on ROUGE in DUC 2006 and 2007 (Jagarlamudi et al., 2006; Pingali et al., 2007), system by Davis et al. (2012) that report the best R-2 score on DUC 2006 and 2007 thus far, and to the purely extractive methods of SVR and LambdaMART. Our sentence-compression-based systems (marked with †) show statistically significant improvements over pure extractive summarization for both R-2 and R-SU4 (paired t-test, p &lt; 0.01). This means our systems can effectively remove redundancy within the summary through compression. Furthermore, our HEAD-driven beam search method with MULTI-scorer beats all systems on DUC 20066 and all systems on DUC 2007 except the best s</context>
<context position="32230" citStr="Jagarlamudi et al. (2006)" startWordPosition="5095" endWordPosition="5098">pe of scorer used. Statistically significant improvements (p &lt; 0.01) over the best system in DUC 06 and 07 are marked with ∗. † indicates statistical significance (p &lt; 0.01) over extractive approaches (SVR or LambdaMART). HEAD + MULTI outperforms all the other extract- and compression-based systems in R-2. System Pyr Gra Non-Red Ref Foc Coh Best DUC system (ROUGE) 22.9±8.2 3.5±0.9 3.5±1.0 3.5±1.1 3.6±1.0 2.9±1.1 Best DUC system (LQ) – 4.0±0.8 4.2±0.7 3.8±0.7 3.6±0.9 3.4±0.9 Our System 26.4±10.3 3.0±0.9 4.0±1.1 3.6±1.0 3.4±0.9 2.8±1.0 Table 6: Human evaluation on our multi-scorer based system, Jagarlamudi et al. (2006) (Best DUC system (ROUGE)), and Lacatusu et al. (2006) (Best DUC system (LQ)). Our system can synthesize more relevant content according to Pyramid (×100). We also examine linguistic quality (LQ) in Grammaticality (Gra), Non-redundancy (Non-Red), Referential clarity (Ref), Focus (Foc), and Structure and Coherence (Coh) like Dang (2006), each rated from 1 (very poor) to 5 (very good). Our system has better non-redundancy than Jagarlamudi et al. (2006) and is comparable to Jagarlamudi et al. (2006) and Lacatusu et al. (2006) in other metrics except grammaticality. the DUC 2006 task with gold-sta</context>
<context position="33476" citStr="Jagarlamudi et al., 2006" startWordPosition="5297" endWordPosition="5300">in abstracts. The Pyramid score (see Table 6) is re-calculated for the system with best ROUGE scores in DUC 2006 (Jagarlamudi et al., 2006) along with our system by the same annotator to make a meaningful comparison. We further evaluate the linguistic quality (LQ) of the summaries for the same 10 topics in accordance with the measurement in Dang (2006). Four native speakers who are undergraduate students in computer science (none are authors) performed the task, We compare our system based on HEAD-driven beam search with MULTI-scorer to the best systems in DUC 2006 achieving top ROUGE scores (Jagarlamudi et al., 2006) (Best DUC system (ROUGE)) and top linguistic quality scores (Lacatusu et al., 2006) (Best DUC system (LQ))7. The average score and standard deviation for each metric is displayed in Table 6. Our system achieves a higher Pyramid score, an indication that it captures more of the salient facts. We also 7Lacatusu et al. (2006) obtain the best scores in three linguistic quality metrics (i.e. grammaticality, focus, structure and coherence), and overall responsiveness on DUC 2006. attain better non-redundancy than Jagarlamudi et al. (2006), meaning that human raters perceive less replicative content</context>
</contexts>
<marker>Jagarlamudi, Pingali, Varma, 2006</marker>
<rawString>Jagadeesh Jagarlamudi, Prasad Pingali, and Vasudeva Varma, 2006. Query Independent Sentence Scoring approach to DUC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Peter Kincaid</author>
<author>Robert P Fishburne</author>
<author>Richard L Rogers</author>
<author>Brad S Chissom</author>
</authors>
<title>Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel.</title>
<date>1975</date>
<tech>Technical report,</tech>
<contexts>
<context position="3974" citStr="Kincaid et al., 1975" startWordPosition="577" endWordPosition="580">-dollar collection of [hundreds of ancient] Nepalese and Tibetan art objects in New York [11 years ago] was arrested [Thursday at his South Los Angeles home, where he had been hiding the antiquities, police said]. In this example, the compressed sentence is rela1From DUC 2005, query for topic d422g. 1384 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1384–1394, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tively more succinct and readable than the original (e.g. in terms of Flesch-Kincaid Reading Ease Score (Kincaid et al., 1975)). Likewise, removing information irrelevant to the query (e.g. “11 years ago”, “police said”) is crucial for query-focused MDS. Sentence compression techniques (Knight and Marcu, 2000; Clarke and Lapata, 2008) are the standard for producing a compact and grammatical version of a sentence while preserving relevance, and prior research (e.g. Lin (2003)) has demonstrated their potential usefulness for generic document summarization. Similarly, strides have been made to incorporate sentence compression into query-focused MDS systems (Zajic et al., 2006). Most attempts, however, fail to produce be</context>
</contexts>
<marker>Kincaid, Fishburne, Rogers, Chissom, 1975</marker>
<rawString>J. Peter Kincaid, Robert P. Fishburne, Richard L. Rogers, and Brad S. Chissom. 1975. Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel. Technical report, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistics-based summarization - step one: Sentence compression.</title>
<date>2000</date>
<journal>AAAI</journal>
<volume>00</volume>
<pages>703--710</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="4158" citStr="Knight and Marcu, 2000" startWordPosition="602" endWordPosition="605">the antiquities, police said]. In this example, the compressed sentence is rela1From DUC 2005, query for topic d422g. 1384 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1384–1394, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tively more succinct and readable than the original (e.g. in terms of Flesch-Kincaid Reading Ease Score (Kincaid et al., 1975)). Likewise, removing information irrelevant to the query (e.g. “11 years ago”, “police said”) is crucial for query-focused MDS. Sentence compression techniques (Knight and Marcu, 2000; Clarke and Lapata, 2008) are the standard for producing a compact and grammatical version of a sentence while preserving relevance, and prior research (e.g. Lin (2003)) has demonstrated their potential usefulness for generic document summarization. Similarly, strides have been made to incorporate sentence compression into query-focused MDS systems (Zajic et al., 2006). Most attempts, however, fail to produce better results than those of the best systems built on pure extraction-based approaches that use no sentence compression. In this paper we investigate the role of sentence compression te</context>
<context position="8921" citStr="Knight and Marcu, 2000" startWordPosition="1310" endWordPosition="1313">t; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporat</context>
<context position="14523" citStr="Knight and Marcu, 2000" startWordPosition="2153" endWordPosition="2156"> features for sequence-based compression. We replace each pronoun with its referent unless they appear in the same sentence. For sentence ordering, each compressed sentence is assigned to the most similar (tf-idf) query sentence. Then a Chronological Ordering algorithm (Barzilay et al., 2002) sorts the sentences for each query based first on the time stamp, and then the position in the source document. 4 Sentence Compression Sentence compression is typically formulated as the problem of removing secondary information from a sentence while maintaining its grammaticality and semantic structure (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008). We leave other rewrite operations, such as paraphrasing and reordering, for future work. Below we describe the sentence compression approaches developed in this research: RULE-BASED COMPRESSION, SEQUENCE-BASED COMPRESSION, and TREEBASED COMPRESSION. 4.1 Rule-based Compression Turner and Charniak (2005) have shown that applying hand-crafted rules for trimming sentences can improve both content and linguistic quality. Our rule-based approach extends existing work (Conroy et al., 2006; Toutanova et al., 2007) to create the ling</context>
<context position="17348" citStr="Knight and Marcu, 2000" startWordPosition="2606" endWordPosition="2609"> find the maximally likely sequence Y according to a CRF with parameter θ (Y = arg maxY , P(Y &apos;|X; θ)), while simultaneously enforcing the rules of Table 2 to reduce the hypothesis space and encourage grammatical compression. To do this, we encode these rules as features for each token, and whenever these feature functions fire, we restrict the possible label for that token to “O”. 4.3 Tree-based Compression Our tree-based compression methods are in line with syntax-driven approaches (Galley and McKeown, 2007), where operations are carried out on parse tree constituents. Unlike previous work (Knight and Marcu, 2000; Galley and McKeown, 2007), we do not produce a new parse tree, 1387 Rule Example Header [MOSCOW, October 19 ( Xinhua ) –] Russian federal troops Tuesday continued... Relative dates ...Centers for Disease Control confirmed [Tuesday] that there was... Intra-sentential attribution ...fueling the La Nina weather phenomenon, [the U.N. weather agency said]. Lead adverbials [Interestingly], while the Democrats tend to talk about... Noun appositives Wayne County Prosecutor [John O’Hara] wanted to send a message... Nonrestrictive relative clause Putin, [who was born on October 7, 1952 in Leningrad], </context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statistics-based summarization - step one: Sentence compression. AAAI ’00, pages 703–710. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finley Lacatusu</author>
<author>Andrew Hickl</author>
<author>Kirk Roberts</author>
<author>Ying Shi</author>
<author>Jeremy Bensley</author>
<author>Bryan Rink</author>
<author>Patrick Wang</author>
<author>Lara Taylor</author>
</authors>
<title>LCCs gistexter at duc 2006: Multi-strategy multi-document summarization.</title>
<date>2006</date>
<contexts>
<context position="32284" citStr="Lacatusu et al. (2006)" startWordPosition="5104" endWordPosition="5107"> (p &lt; 0.01) over the best system in DUC 06 and 07 are marked with ∗. † indicates statistical significance (p &lt; 0.01) over extractive approaches (SVR or LambdaMART). HEAD + MULTI outperforms all the other extract- and compression-based systems in R-2. System Pyr Gra Non-Red Ref Foc Coh Best DUC system (ROUGE) 22.9±8.2 3.5±0.9 3.5±1.0 3.5±1.1 3.6±1.0 2.9±1.1 Best DUC system (LQ) – 4.0±0.8 4.2±0.7 3.8±0.7 3.6±0.9 3.4±0.9 Our System 26.4±10.3 3.0±0.9 4.0±1.1 3.6±1.0 3.4±0.9 2.8±1.0 Table 6: Human evaluation on our multi-scorer based system, Jagarlamudi et al. (2006) (Best DUC system (ROUGE)), and Lacatusu et al. (2006) (Best DUC system (LQ)). Our system can synthesize more relevant content according to Pyramid (×100). We also examine linguistic quality (LQ) in Grammaticality (Gra), Non-redundancy (Non-Red), Referential clarity (Ref), Focus (Foc), and Structure and Coherence (Coh) like Dang (2006), each rated from 1 (very poor) to 5 (very good). Our system has better non-redundancy than Jagarlamudi et al. (2006) and is comparable to Jagarlamudi et al. (2006) and Lacatusu et al. (2006) in other metrics except grammaticality. the DUC 2006 task with gold-standard SCU annotation in abstracts. The Pyramid score (</context>
<context position="33560" citStr="Lacatusu et al., 2006" startWordPosition="5310" endWordPosition="5313"> ROUGE scores in DUC 2006 (Jagarlamudi et al., 2006) along with our system by the same annotator to make a meaningful comparison. We further evaluate the linguistic quality (LQ) of the summaries for the same 10 topics in accordance with the measurement in Dang (2006). Four native speakers who are undergraduate students in computer science (none are authors) performed the task, We compare our system based on HEAD-driven beam search with MULTI-scorer to the best systems in DUC 2006 achieving top ROUGE scores (Jagarlamudi et al., 2006) (Best DUC system (ROUGE)) and top linguistic quality scores (Lacatusu et al., 2006) (Best DUC system (LQ))7. The average score and standard deviation for each metric is displayed in Table 6. Our system achieves a higher Pyramid score, an indication that it captures more of the salient facts. We also 7Lacatusu et al. (2006) obtain the best scores in three linguistic quality metrics (i.e. grammaticality, focus, structure and coherence), and overall responsiveness on DUC 2006. attain better non-redundancy than Jagarlamudi et al. (2006), meaning that human raters perceive less replicative content in our summaries. Scores for other metrics are comparable to Jagarlamudi et al. (20</context>
</contexts>
<marker>Lacatusu, Hickl, Roberts, Shi, Bensley, Rink, Wang, Taylor, 2006</marker>
<rawString>Finley Lacatusu, Andrew Hickl, Kirk Roberts, Ying Shi, Jeremy Bensley, Bryan Rink, Patrick Wang, and Lara Taylor, 2006. LCCs gistexter at duc 2006: Multi-strategy multi-document summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="15918" citStr="Lafferty et al., 2001" startWordPosition="2362" endWordPosition="2365"> in McDonald (2006) and Clarke and Lapata (2008), our sequence-based compression model makes a binary “keep-or-delete” decision for each word in the sentence. In contrast, however, we Figure 1: Diagram of tree-based compression. The nodes to be dropped are grayed out. In this example, the root of the gray subtree (a “PP”) would be labeled REMOVE. Its siblings and parent are labeled RETAIN and PARTIAL, respectively. The trimmed tree is realized as “Malaria causes millions of deaths.” view compression as a sequential tagging problem and make use of linear-chain Conditional Random Fields (CRFs) (Lafferty et al., 2001) to select the most likely compression. We represent each sentence as a sequence of tokens, X = xoxi ... xn, and generate a sequence of labels, Y = y0y1 . . . yn, that encode which tokens are kept, using a BIO label format: {B-RETAIN denotes the beginning of a retained sequence, IRETAIN indicates tokens “inside” the retained sequence, O marks tokens to be removed}. The CRF model is built using the features shown in Table 3. “Dependency Tree Features” encode the grammatical relations in which each word is involved as a dependent. For the “Syntactic Tree”, “Dependency Tree” and “Rule-Based” feat</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M Lewis</author>
<author>R E Stearns</author>
</authors>
<title>Syntax-directed transduction.</title>
<date>1968</date>
<journal>J. ACM,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="9061" citStr="Lewis and Stearns, 1968" startWordPosition="1330" endWordPosition="1333"> idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by using integer linear programming. 3 The Framework We now present our query-focused MDS framework consisting of th</context>
</contexts>
<marker>Lewis, Stearns, 1968</marker>
<rawString>P. M. Lewis, II and R. E. Stearns. 1968. Syntax-directed transduction. J. ACM, 15(3):465–488, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>A class of submodular functions for document summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>510--520</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7850" citStr="Lin and Bilmes (2011)" startWordPosition="1150" endWordPosition="1153">pervised methods, sentence importance can be estimated by calculating topic signature words (Lin and Hovy, 2000; Conroy et al., 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al., 2005), or using a Bayesian model with sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has bee</context>
</contexts>
<marker>Lin, Bilmes, 2011</marker>
<rawString>Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 510–520, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics - Volume 1, COLING ’00,</booktitle>
<pages>495--501</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7340" citStr="Lin and Hovy, 2000" startWordPosition="1076" endWordPosition="1079"> we believe we are the first to successfully show that sentence compression can provide statistically significant improvements over pure extraction-based approaches for queryfocused MDS. 2 Related Work Existing research on query-focused multidocument summarization (MDS) largely relies on extractive approaches, where systems usually take as input a set of documents and select the top relevant sentences for inclusion in the final summary. A wide range of methods have been employed for this task. For unsupervised methods, sentence importance can be estimated by calculating topic signature words (Lin and Hovy, 2000; Conroy et al., 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al., 2005), or using a Bayesian model with sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the sum</context>
<context position="11480" citStr="Lin and Hovy, 2000" startWordPosition="1690" endWordPosition="1693">sentences? number of words (with/without stopwords) number of words more than 5/10 (with/without stopwords) Query-Relevant Features unigram/bigram/skip bigram (at most four words apart) overlap unigram/bigram TF/TF-IDF similarity mention overlap subject/object/indirect object overlap semantic role overlap relation overlap Query-Independent Features average/total unigram/bigram IDF/TF-IDF unigram/bigram TF/TF-IDF similarity with the centroid of the cluster average/sum of sumBasic/SumFocus (Toutanova et al., 2007) average/sum of mutual information average/sum of number of topic signature words (Lin and Hovy, 2000) basic/improved sentence scorers from Conroy et al. (2006) Content Features contains verb/web link/phone number? contains/portion of words between parentheses Table 1: Sentence-level features for sentence ranking. 2011; Ouyang et al., 2011), we use the ROUGE2 score, which measures bigram overlap between a sentence and the abstracts, as the objective for regression. While space limitations preclude a longer discussion of the full feature set (ref. Table 1), we describe next the query-relevant features used for sentence ranking as these are the most important for our summarization setting. The g</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th conference on Computational linguistics - Volume 1, COLING ’00, pages 495–501, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -</booktitle>
<volume>1</volume>
<pages>71--78</pages>
<contexts>
<context position="5951" citStr="Lin and Hovy, 2003" startWordPosition="863" endWordPosition="866">y and flexible tailoring of sentence compression to the summarization task, ultimately resulting in significant improvements for MDS, while at the same time remaining competitive with existing methods in terms of sentence compression, as discussed next. We evaluate the summarization models on the standard Document Understanding Conference (DUC) 2006 and 2007 corpora 2 for queryfocused MDS and find that all of our compressionbased summarization models achieve statistically significantly better performance than the best DUC 2006 systems. Our best-performing system yields an 11.02 ROUGE-2 score (Lin and Hovy, 2003), a 8.0% improvement over the best reported score (10.2 (Davis et al., 2012)) on the 2We believe that we can easily adapt our system for tasks (e.g. TAC-08’s opinion summarization or TAC-09’s update summarization) or domains (e.g. web pages or wikipedia pages). We reserve that for future work. DUC 2006 dataset, and an 13.49 ROUGE-2, a 5.4% improvement over the best score in DUC 2007 (12.8 (Davis et al., 2012)). We also observe substantial improvements over previous systems w.r.t. the manual Pyramid (Nenkova and Passonneau, 2004) evaluation measure (26.4 vs. 22.9 (Jagarlamudi et al., 2006)); hu</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Improving summarization performance by sentence compression: a pilot study.</title>
<date>2003</date>
<booktitle>In Proceedings of the sixth international workshop on Information retrieval with Asian languages - Volume 11, AsianIR ’03,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4327" citStr="Lin (2003)" startWordPosition="632" endWordPosition="633">for Computational Linguistics, pages 1384–1394, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tively more succinct and readable than the original (e.g. in terms of Flesch-Kincaid Reading Ease Score (Kincaid et al., 1975)). Likewise, removing information irrelevant to the query (e.g. “11 years ago”, “police said”) is crucial for query-focused MDS. Sentence compression techniques (Knight and Marcu, 2000; Clarke and Lapata, 2008) are the standard for producing a compact and grammatical version of a sentence while preserving relevance, and prior research (e.g. Lin (2003)) has demonstrated their potential usefulness for generic document summarization. Similarly, strides have been made to incorporate sentence compression into query-focused MDS systems (Zajic et al., 2006). Most attempts, however, fail to produce better results than those of the best systems built on pure extraction-based approaches that use no sentence compression. In this paper we investigate the role of sentence compression techniques for query-focused MDS. We extend existing work in the area first by investigating the role of learning-based sentence compression techniques. In addition, we de</context>
<context position="8478" citStr="Lin, 2003" startWordPosition="1248" endWordPosition="1249">efully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust proba</context>
</contexts>
<marker>Lin, 2003</marker>
<rawString>Chin-Yew Lin. 2003. Improving summarization performance by sentence compression: a pilot study. In Proceedings of the sixth international workshop on Information retrieval with Asian languages - Volume 11, AsianIR ’03, pages 1–8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Imed Zitouni</author>
</authors>
<title>Multi-lingual coreference resolution with syntactic features.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="27766" citStr="Luo and Zitouni, 2005" startWordPosition="4361" endWordPosition="4364">. DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke and Lapata (2008) is used to train the CRF and MaxEnt classifiers (Section 4). It includes 82 newswire articles with one manually produced compression aligned to each sentence. Preprocessing. Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3. As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40 topics. MaxEnt classifiers are used for tre</context>
</contexts>
<marker>Luo, Zitouni, 2005</marker>
<rawString>Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual coreference resolution with syntactic features. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abraham Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>A mentionsynchronous coreference resolution algorithm based on the bell tree.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>135--142</pages>
<contexts>
<context position="27742" citStr="Luo et al., 2004" startWordPosition="4357" endWordPosition="4360">or the multiscorer. DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke and Lapata (2008) is used to train the CRF and MaxEnt classifiers (Section 4). It includes 82 newswire articles with one manually produced compression aligned to each sentence. Preprocessing. Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3. As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40 topics. MaxEnt clas</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Xiaoqiang Luo, Abraham Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A mentionsynchronous coreference resolution algorithm based on the bell tree. In ACL, pages 135–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP ’09,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8695" citStr="Martins and Smith, 2009" startWordPosition="1277" endWordPosition="1280">e) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tre</context>
<context position="34879" citStr="Martins and Smith, 2009" startWordPosition="5518" endWordPosition="5521">or is a pure extractive system. However, our compression system sometimes generates less grammatical sentences, and those are mostly due to parsing errors. For example, parsing a clause starting with a past tense verb as an adverbial clausal modifier can lead to an ill-formed compression. Those issues can be addressed by analyzing k-best parse trees and we leave it in the future work. A sample summary from our multiscorer based system is in Figure 3. Sentence Compression Evaluation. We also evaluate sentence compression separately on (Clarke and Lapata, 2008), adopting the same partitions as (Martins and Smith, 2009), i.e. 1,188 sentences for training and 441 for testing. Our compression models are compared with Hedge Trimmer (Dorr et al., 2003), a discriminative model proposed by McDonald (2006) and a 1391 System C Rate Uni-Prec Uni-Rec Uni-F1 Rel-F1 HedgeTrimmer 57.64% 0.72 0.65 0.64 0.50 McDonald (2006) 70.95% 0.77 0.78 0.77 0.55 Martins and Smith (2009) 71.35% 0.77 0.78 0.77 0.56 Rule-based 87.65% 0.74 0.91 0.80 0.63 Sequence 70.79% 0.77 0.80 0.76 0.58 Tree (BASIC) 69.65% 0.77 0.79 0.75 0.56 Tree (CONTEXT) 67.01% 0.79 0.78 0.76 0.57 Tree (HEAD) 68.06% 0.79 0.80 0.77 0.59 Table 7: Sentence compression </context>
<context position="37569" citStr="Martins and Smith, 2009" startWordPosition="5949" endWordPosition="5952">nd Rwanda. FBI Director Louis Freeh, Attorney General Janet Reno and other senior U.S. government officials will hold a news conference at 1 p.m. EDT (1700GMT) at FBI headquarters in Washington “to announce developments in the investigation of the bombings of the U.S. embassies in Kenya and Tanzania,” the FBI said in a statement.... Figure 3: Part of the summary generated by the multiscorer based summarizer for topic D0626H (DUC 2006). Grayed out words are removed. Queryirrelevant phrases, such as temporal information or source of the news, have been removed. dependency-tree based compressor (Martins and Smith, 2009)8. We adopt the metrics in Martins and Smith (2009) to measure the unigram-level macro precision, recall, and F1-measure with respect to human annotated compression. In addition, we also compute the F1 scores of grammatical relations which are annotated by RASP (Briscoe and Carroll, 2002) according to Clarke and Lapata (2008). In Table 7, our context-aware and head-driven tree-based compression systems show statistically significantly (p &lt; 0.01) higher precisions (Uni8Thanks to Andr´e F.T. Martins for system outputs. Prec) than all the other systems, without decreasing the recalls (Uni-Rec) si</context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>Andr´e F. T. Martins and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP ’09, pages 1–9, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="28157" citStr="McCallum, 2002" startWordPosition="4428" endWordPosition="4429">semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3. As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40 topics. MaxEnt classifiers are used for tree-based compression. Beam size is fixed at 2000.4 Sentence compressions are evaluated by a 5-gram language model trained on Gigaword (Graff, 2003) by SRILM (Stolcke, 2002). 6 Results The results in Table 5 use the official ROUGE software with standard options5 and report ROUGE2 (R-2) (measures bigram overlap) and ROUGESU4 (R-SU4) (measures unigram and skip-bigram separated by up to four w</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative Sentence Compression with Soft Syntactic Constraints.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th˜EACL,</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="9475" citStr="McDonald (2006)" startWordPosition="1398" endWordPosition="1399">ches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by using integer linear programming. 3 The Framework We now present our query-focused MDS framework consisting of three steps: Sentence Ranking, Sentence Compression and Post-processing. First, sentence ranking determines the importance of each sentence given the query. Then, a sentence compressor iteratively generates the most likely succinct versions of the ranked sentences, which are cumulatively added to the summary, until a length limit is reached. Finally, the postprocessing stage applies coreference resolution and sen</context>
<context position="14539" citStr="McDonald, 2006" startWordPosition="2157" endWordPosition="2158">ased compression. We replace each pronoun with its referent unless they appear in the same sentence. For sentence ordering, each compressed sentence is assigned to the most similar (tf-idf) query sentence. Then a Chronological Ordering algorithm (Barzilay et al., 2002) sorts the sentences for each query based first on the time stamp, and then the position in the source document. 4 Sentence Compression Sentence compression is typically formulated as the problem of removing secondary information from a sentence while maintaining its grammaticality and semantic structure (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008). We leave other rewrite operations, such as paraphrasing and reordering, for future work. Below we describe the sentence compression approaches developed in this research: RULE-BASED COMPRESSION, SEQUENCE-BASED COMPRESSION, and TREEBASED COMPRESSION. 4.1 Rule-based Compression Turner and Charniak (2005) have shown that applying hand-crafted rules for trimming sentences can improve both content and linguistic quality. Our rule-based approach extends existing work (Conroy et al., 2006; Toutanova et al., 2007) to create the linguistically-motiv</context>
<context position="35062" citStr="McDonald (2006)" startWordPosition="5548" endWordPosition="5549">ng with a past tense verb as an adverbial clausal modifier can lead to an ill-formed compression. Those issues can be addressed by analyzing k-best parse trees and we leave it in the future work. A sample summary from our multiscorer based system is in Figure 3. Sentence Compression Evaluation. We also evaluate sentence compression separately on (Clarke and Lapata, 2008), adopting the same partitions as (Martins and Smith, 2009), i.e. 1,188 sentences for training and 441 for testing. Our compression models are compared with Hedge Trimmer (Dorr et al., 2003), a discriminative model proposed by McDonald (2006) and a 1391 System C Rate Uni-Prec Uni-Rec Uni-F1 Rel-F1 HedgeTrimmer 57.64% 0.72 0.65 0.64 0.50 McDonald (2006) 70.95% 0.77 0.78 0.77 0.55 Martins and Smith (2009) 71.35% 0.77 0.78 0.77 0.56 Rule-based 87.65% 0.74 0.91 0.80 0.63 Sequence 70.79% 0.77 0.80 0.76 0.58 Tree (BASIC) 69.65% 0.77 0.79 0.75 0.56 Tree (CONTEXT) 67.01% 0.79 0.78 0.76 0.57 Tree (HEAD) 68.06% 0.79 0.80 0.77 0.59 Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches all use single-scorer. Our context-aware and head-driven tree-based approaches outperform all the other s</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative Sentence Compression with Soft Syntactic Constraints. In Proceedings of the 11th˜EACL, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mozer</author>
<author>Michael I Jordan</author>
<author>Thomas Petsche</author>
<author>editors</author>
</authors>
<date>1997</date>
<booktitle>Advances in Neural Information Processing Systems 9,</booktitle>
<publisher>MIT Press.</publisher>
<location>NIPS, Denver, CO, USA,</location>
<contexts>
<context position="10369" citStr="Mozer et al., 1997" startWordPosition="1534" endWordPosition="1537">nking determines the importance of each sentence given the query. Then, a sentence compressor iteratively generates the most likely succinct versions of the ranked sentences, which are cumulatively added to the summary, until a length limit is reached. Finally, the postprocessing stage applies coreference resolution and sentence reordering to build the summary. Sentence Ranking. This stage aims to rank sentences in order of relevance to the query. Unsurprisingly, ranking algorithms have been successfully applied to this task. We experimented with two of them – Support Vector Regression (SVR) (Mozer et al., 1997) and LambdaMART (Burges et al., 2007). The former has been used previously for MDS (Ouyang et al., 2011). LambdaMart on the other hand has shown considerable success in information retrieval tasks (Burges, 2010); we are the first to apply it to summarization. For training, we use 40 topics (i.e. queries) from the DUC 2005 corpus (Dang, 2005) along with their manually generated abstracts. As in previous work (Shen and Li, Basic Features relative/absolute position is among the first 1/3/5 sentences? number of words (with/without stopwords) number of words more than 5/10 (with/without stopwords) </context>
</contexts>
<marker>Mozer, Jordan, Petsche, editors, 1997</marker>
<rawString>Michael Mozer, Michael I. Jordan, and Thomas Petsche, editors. 1997. Advances in Neural Information Processing Systems 9, NIPS, Denver, CO, USA, December 2-5, 1996. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>145--152</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="6485" citStr="Nenkova and Passonneau, 2004" startWordPosition="951" endWordPosition="954">DUC 2006 systems. Our best-performing system yields an 11.02 ROUGE-2 score (Lin and Hovy, 2003), a 8.0% improvement over the best reported score (10.2 (Davis et al., 2012)) on the 2We believe that we can easily adapt our system for tasks (e.g. TAC-08’s opinion summarization or TAC-09’s update summarization) or domains (e.g. web pages or wikipedia pages). We reserve that for future work. DUC 2006 dataset, and an 13.49 ROUGE-2, a 5.4% improvement over the best score in DUC 2007 (12.8 (Davis et al., 2012)). We also observe substantial improvements over previous systems w.r.t. the manual Pyramid (Nenkova and Passonneau, 2004) evaluation measure (26.4 vs. 22.9 (Jagarlamudi et al., 2006)); human annotators furthermore rate our system-generated summaries as having less redundancy and comparable quality w.r.t. other linguistic quality metrics. With these results we believe we are the first to successfully show that sentence compression can provide statistically significant improvements over pure extraction-based approaches for queryfocused MDS. 2 Related Work Existing research on query-focused multidocument summarization (MDS) largely relies on extractive approaches, where systems usually take as input a set of docume</context>
<context position="29941" citStr="Nenkova and Passonneau, 2004" startWordPosition="4709" endWordPosition="4712"> redundancy within the summary through compression. Furthermore, our HEAD-driven beam search method with MULTI-scorer beats all systems on DUC 20066 and all systems on DUC 2007 except the best system in terms of R-2 (p &lt; 0.01). Its R-SU4 score is also significantly (p &lt; 0.01) better than extractive methods, rule-based and sequence-based compression methods on both DUC 2006 and 2007. Moreover, our systems with learning-based compression have considerable compression rates, indicating their capability to remove superfluous words as well as improve summary quality. Human Evaluation. The Pyramid (Nenkova and Passonneau, 2004) evaluation was developed to manually assess how many relevant facts or Summarization Content Units (SCUs) are captured by system summaries. We ask a professional annotator (who is not one of the authors, is highly experienced in annotating for various NLP tasks, and is fluent in English) to carry out a Pyramid evaluation on 10 randomly selected topics from 4We looked at various beam sizes on the heldout data, and observed that the performance peaks around this value. 5ROUGE-1.5.5.pl -n 4 -w 1.2 -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -a -d 6The system output from Davis et al. (2012) is not </context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 145– 152, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Using random walks for question-focused sentence retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>915--922</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7468" citStr="Otterbacher et al., 2005" startWordPosition="1094" endWordPosition="1097">ments over pure extraction-based approaches for queryfocused MDS. 2 Related Work Existing research on query-focused multidocument summarization (MDS) largely relies on extractive approaches, where systems usually take as input a set of documents and select the top relevant sentences for inclusion in the final summary. A wide range of methods have been employed for this task. For unsupervised methods, sentence importance can be estimated by calculating topic signature words (Lin and Hovy, 2000; Conroy et al., 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al., 2005), or using a Bayesian model with sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (s</context>
</contexts>
<marker>Otterbacher, Erkan, Radev, 2005</marker>
<rawString>Jahna Otterbacher, G¨unes¸ Erkan, and Dragomir R. Radev. 2005. Using random walks for question-focused sentence retrieval. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 915–922, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>You Ouyang</author>
<author>Wenjie Li</author>
<author>Sujian Li</author>
<author>Qin Lu</author>
</authors>
<title>Applying regression models to query-focused multidocument summarization.</title>
<date>2011</date>
<journal>Inf. Process. Manage.,</journal>
<volume>47</volume>
<issue>2</issue>
<contexts>
<context position="10473" citStr="Ouyang et al., 2011" startWordPosition="1553" endWordPosition="1556">ly generates the most likely succinct versions of the ranked sentences, which are cumulatively added to the summary, until a length limit is reached. Finally, the postprocessing stage applies coreference resolution and sentence reordering to build the summary. Sentence Ranking. This stage aims to rank sentences in order of relevance to the query. Unsurprisingly, ranking algorithms have been successfully applied to this task. We experimented with two of them – Support Vector Regression (SVR) (Mozer et al., 1997) and LambdaMART (Burges et al., 2007). The former has been used previously for MDS (Ouyang et al., 2011). LambdaMart on the other hand has shown considerable success in information retrieval tasks (Burges, 2010); we are the first to apply it to summarization. For training, we use 40 topics (i.e. queries) from the DUC 2005 corpus (Dang, 2005) along with their manually generated abstracts. As in previous work (Shen and Li, Basic Features relative/absolute position is among the first 1/3/5 sentences? number of words (with/without stopwords) number of words more than 5/10 (with/without stopwords) Query-Relevant Features unigram/bigram/skip bigram (at most four words apart) overlap unigram/bigram TF/</context>
<context position="11720" citStr="Ouyang et al., 2011" startWordPosition="1721" endWordPosition="1724">verlap subject/object/indirect object overlap semantic role overlap relation overlap Query-Independent Features average/total unigram/bigram IDF/TF-IDF unigram/bigram TF/TF-IDF similarity with the centroid of the cluster average/sum of sumBasic/SumFocus (Toutanova et al., 2007) average/sum of mutual information average/sum of number of topic signature words (Lin and Hovy, 2000) basic/improved sentence scorers from Conroy et al. (2006) Content Features contains verb/web link/phone number? contains/portion of words between parentheses Table 1: Sentence-level features for sentence ranking. 2011; Ouyang et al., 2011), we use the ROUGE2 score, which measures bigram overlap between a sentence and the abstracts, as the objective for regression. While space limitations preclude a longer discussion of the full feature set (ref. Table 1), we describe next the query-relevant features used for sentence ranking as these are the most important for our summarization setting. The goal of this feature subset is to determine the similarity between the query and each candidate sentence. When computing similarity, we remove stopwords as well as the words “discuss, describe, specify, explain, identify, include, involve, n</context>
</contexts>
<marker>Ouyang, Li, Li, Lu, 2011</marker>
<rawString>You Ouyang, Wenjie Li, Sujian Li, and Qin Lu. 2011. Applying regression models to query-focused multidocument summarization. Inf. Process. Manage., 47(2):227–237, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prasad Pingali</author>
<author>K Rahul</author>
<author>Vasudeva Varma</author>
</authors>
<date>2007</date>
<booktitle>IIIT Hyderabad at DUC 2007. U.S. National Inst. of Standards and Technology.</booktitle>
<contexts>
<context position="28929" citStr="Pingali et al., 2007" startWordPosition="4556" endWordPosition="4559">e 10 out of 40 topics. MaxEnt classifiers are used for tree-based compression. Beam size is fixed at 2000.4 Sentence compressions are evaluated by a 5-gram language model trained on Gigaword (Graff, 2003) by SRILM (Stolcke, 2002). 6 Results The results in Table 5 use the official ROUGE software with standard options5 and report ROUGE2 (R-2) (measures bigram overlap) and ROUGESU4 (R-SU4) (measures unigram and skip-bigram separated by up to four words). We compare our sentence-compression-based methods to the best performing systems based on ROUGE in DUC 2006 and 2007 (Jagarlamudi et al., 2006; Pingali et al., 2007), system by Davis et al. (2012) that report the best R-2 score on DUC 2006 and 2007 thus far, and to the purely extractive methods of SVR and LambdaMART. Our sentence-compression-based systems (marked with †) show statistically significant improvements over pure extractive summarization for both R-2 and R-SU4 (paired t-test, p &lt; 0.01). This means our systems can effectively remove redundancy within the summary through compression. Furthermore, our HEAD-driven beam search method with MULTI-scorer beats all systems on DUC 20066 and all systems on DUC 2007 except the best system in terms of R-2 (</context>
</contexts>
<marker>Pingali, Rahul, Varma, 2007</marker>
<rawString>Prasad Pingali, Rahul K, and Vasudeva Varma, 2007. IIIT Hyderabad at DUC 2007. U.S. National Inst. of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Shen</author>
<author>Tao Li</author>
</authors>
<title>Learning to rank for queryfocused multi-document summarization.</title>
<date>2011</date>
<pages>626--634</pages>
<editor>In Diane J. Cook, Jian Pei, Wei Wang 0010, Osmar R. Zaane, and Xindong Wu, editors, ICDM,</editor>
<publisher>IEEE.</publisher>
<marker>Shen, Li, 2011</marker>
<rawString>Chao Shen and Tao Li. 2011. Learning to rank for queryfocused multi-document summarization. In Diane J. Cook, Jian Pei, Wei Wang 0010, Osmar R. Zaane, and Xindong Wu, editors, ICDM, pages 626–634. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, USA.</location>
<contexts>
<context position="28537" citStr="Stolcke, 2002" startWordPosition="4494" endWordPosition="4495">ankLib (Dang, 2011)3. As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40 topics. MaxEnt classifiers are used for tree-based compression. Beam size is fixed at 2000.4 Sentence compressions are evaluated by a 5-gram language model trained on Gigaword (Graff, 2003) by SRILM (Stolcke, 2002). 6 Results The results in Table 5 use the official ROUGE software with standard options5 and report ROUGE2 (R-2) (measures bigram overlap) and ROUGESU4 (R-SU4) (measures unigram and skip-bigram separated by up to four words). We compare our sentence-compression-based methods to the best performing systems based on ROUGE in DUC 2006 and 2007 (Jagarlamudi et al., 2006; Pingali et al., 2007), system by Davis et al. (2012) that report the best R-2 score on DUC 2006 and 2007 thus far, and to the purely extractive methods of SVR and LambdaMART. Our sentence-compression-based systems (marked with †)</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of ICSLP, volume 2, pages 901–904, Denver, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Chris Brockett</author>
<author>Michael Gamon</author>
<author>Jagadeesh Jagarlamudi</author>
<author>Hisami Suzuki</author>
<author>Lucy Vanderwende</author>
</authors>
<date>2007</date>
<booktitle>The PYTHY Summarization System: Microsoft Research at DUC</booktitle>
<contexts>
<context position="11378" citStr="Toutanova et al., 2007" startWordPosition="1675" endWordPosition="1678">cts. As in previous work (Shen and Li, Basic Features relative/absolute position is among the first 1/3/5 sentences? number of words (with/without stopwords) number of words more than 5/10 (with/without stopwords) Query-Relevant Features unigram/bigram/skip bigram (at most four words apart) overlap unigram/bigram TF/TF-IDF similarity mention overlap subject/object/indirect object overlap semantic role overlap relation overlap Query-Independent Features average/total unigram/bigram IDF/TF-IDF unigram/bigram TF/TF-IDF similarity with the centroid of the cluster average/sum of sumBasic/SumFocus (Toutanova et al., 2007) average/sum of mutual information average/sum of number of topic signature words (Lin and Hovy, 2000) basic/improved sentence scorers from Conroy et al. (2006) Content Features contains verb/web link/phone number? contains/portion of words between parentheses Table 1: Sentence-level features for sentence ranking. 2011; Ouyang et al., 2011), we use the ROUGE2 score, which measures bigram overlap between a sentence and the abstracts, as the objective for regression. While space limitations preclude a longer discussion of the full feature set (ref. Table 1), we describe next the query-relevant f</context>
<context position="15104" citStr="Toutanova et al., 2007" startWordPosition="2235" endWordPosition="2238">semantic structure (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008). We leave other rewrite operations, such as paraphrasing and reordering, for future work. Below we describe the sentence compression approaches developed in this research: RULE-BASED COMPRESSION, SEQUENCE-BASED COMPRESSION, and TREEBASED COMPRESSION. 4.1 Rule-based Compression Turner and Charniak (2005) have shown that applying hand-crafted rules for trimming sentences can improve both content and linguistic quality. Our rule-based approach extends existing work (Conroy et al., 2006; Toutanova et al., 2007) to create the linguistically-motivated compression rules of Table 2. To avoid ill-formed output, we disallow compressions of more than 10 words by each rule. 4.2 Sequence-based Compression As in McDonald (2006) and Clarke and Lapata (2008), our sequence-based compression model makes a binary “keep-or-delete” decision for each word in the sentence. In contrast, however, we Figure 1: Diagram of tree-based compression. The nodes to be dropped are grayed out. In this example, the root of the gray subtree (a “PP”) would be labeled REMOVE. Its siblings and parent are labeled RETAIN and PARTIAL, res</context>
<context position="24819" citStr="Toutanova et al., 2007" startWordPosition="3870" endWordPosition="3873">ruct a compression scoring function—the multi-scorer (MULTI)—that allows the incorporation of multiple task-specific scorers. Given a hypothesis at any stage of decoding, which yields a sequence of words W = w0w1...wj, we propose the following component scorers. Query Relevance. Query information ought to guide the compressor to identify the relevant content. The query Q is expanded as described in Section 3. Let |W n Q |denote the number of unique overlapping words between W and Q, then scoreq = |W n Q|/|W|. Importance. A query-independent importance score is defined as the average SumBasic (Toutanova et al., 2007) value in W, i.e. scoreim = Eji=1 SumBasic(wi)/|W|. Language Model. We let scorelm be the probability of W computed by a language model. Cross-Sentence Redundancy. To encourage diversified content, we define a redundancy score to discount replicated content: scorered = 1 − |W n C|/|W|, where C is the words already selected for the summary. Basic Features Syntactic Tree Features projection falls w/in first 1/3/5 toks?* constituent label projection falls w/in last 1/3/5 toks?* parent left/right sibling label subsumes first 1/3/5 toks?* grandparent left/right sibling label subsumes last 1/3/5 tok</context>
</contexts>
<marker>Toutanova, Brockett, Gamon, Jagarlamudi, Suzuki, Vanderwende, 2007</marker>
<rawString>Kristina Toutanova, Chris Brockett, Michael Gamon, Jagadeesh Jagarlamudi, Hisami Suzuki, and Lucy Vanderwende. 2007. The PYTHY Summarization System: Microsoft Research at DUC 2007. In Proc. of DUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenine Turner</author>
<author>Eugene Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<journal>ACL</journal>
<volume>05</volume>
<pages>290--297</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8949" citStr="Turner and Charniak, 2005" startWordPosition="1314" endWordPosition="1317"> candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by u</context>
<context position="14896" citStr="Turner and Charniak (2005)" startWordPosition="2203" endWordPosition="2206">the position in the source document. 4 Sentence Compression Sentence compression is typically formulated as the problem of removing secondary information from a sentence while maintaining its grammaticality and semantic structure (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008). We leave other rewrite operations, such as paraphrasing and reordering, for future work. Below we describe the sentence compression approaches developed in this research: RULE-BASED COMPRESSION, SEQUENCE-BASED COMPRESSION, and TREEBASED COMPRESSION. 4.1 Rule-based Compression Turner and Charniak (2005) have shown that applying hand-crafted rules for trimming sentences can improve both content and linguistic quality. Our rule-based approach extends existing work (Conroy et al., 2006; Toutanova et al., 2007) to create the linguistically-motivated compression rules of Table 2. To avoid ill-formed output, we disallow compressions of more than 10 words by each rule. 4.2 Sequence-based Compression As in McDonald (2006) and Clarke and Lapata (2008), our sequence-based compression model makes a binary “keep-or-delete” decision for each word in the sentence. In contrast, however, we Figure 1: Diagra</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. ACL ’05, pages 290–297, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Zajic</author>
<author>Bonnie J Dorr</author>
<author>Jimmy Lin</author>
<author>R Schwartz</author>
</authors>
<title>Sentence compression as a component of a multidocument summarization system.</title>
<date>2006</date>
<booktitle>Proceedings of the</booktitle>
<location>New York.</location>
<contexts>
<context position="4530" citStr="Zajic et al., 2006" startWordPosition="656" endWordPosition="659">rms of Flesch-Kincaid Reading Ease Score (Kincaid et al., 1975)). Likewise, removing information irrelevant to the query (e.g. “11 years ago”, “police said”) is crucial for query-focused MDS. Sentence compression techniques (Knight and Marcu, 2000; Clarke and Lapata, 2008) are the standard for producing a compact and grammatical version of a sentence while preserving relevance, and prior research (e.g. Lin (2003)) has demonstrated their potential usefulness for generic document summarization. Similarly, strides have been made to incorporate sentence compression into query-focused MDS systems (Zajic et al., 2006). Most attempts, however, fail to produce better results than those of the best systems built on pure extraction-based approaches that use no sentence compression. In this paper we investigate the role of sentence compression techniques for query-focused MDS. We extend existing work in the area first by investigating the role of learning-based sentence compression techniques. In addition, we design three types of approaches to sentence-compression— rule-based, sequence-based and tree-based—and examine them within our compression-based framework for query-specific MDS. Our topperforming sentenc</context>
<context position="8118" citStr="Zajic et al. (2006)" startWordPosition="1193" endWordPosition="1196">h sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et a</context>
</contexts>
<marker>Zajic, Dorr, Lin, Schwartz, 2006</marker>
<rawString>David Zajic, Bonnie J Dorr, Jimmy Lin, and R. Schwartz. 2006. Sentence compression as a component of a multidocument summarization system. Proceedings of the 2006 Document Understanding Workshop, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>