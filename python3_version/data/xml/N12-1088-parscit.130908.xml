<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.860686">
Low-Dimensional Discriminative Reranking
</title>
<author confidence="0.98557">
Jagadeesh Jagarlamudi
</author>
<affiliation confidence="0.997456">
Department of Computer Science
University of Maryland
</affiliation>
<address confidence="0.838682">
College Park, MD 20742, USA
</address>
<email confidence="0.988149">
jags@umiacs.umd.edu
</email>
<author confidence="0.992991">
Hal Daum´e III
</author>
<affiliation confidence="0.997479">
Department of Computer Science
University of Maryland
</affiliation>
<address confidence="0.841463">
College Park, MD 20742, USA
</address>
<email confidence="0.992987">
hal@umiacs.umd.edu
</email>
<sectionHeader confidence="0.984124" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999586157894737">
The accuracy of many natural language pro-
cessing tasks can be improved by a reranking
step, which involves selecting a single output
from a list of candidate outputs generated by
a baseline system. We propose a novel fam-
ily of reranking algorithms based on learning
separate low-dimensional embeddings of the
task’s input and output spaces. This embed-
ding is learned in such a way that prediction
becomes a low-dimensional nearest-neighbor
search, which can be done computationally ef-
ficiently. A key quality of our approach is that
feature engineering can be done separately on
the input and output spaces; the relationship
between inputs and outputs is learned auto-
matically. Experiments on part-of-speech tag-
ging task in four languages show significant
improvements over a baseline decoder and ex-
isting reranking approaches.
</bodyText>
<sectionHeader confidence="0.992488" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999875680851064">
Mapping inputs to outputs lies at the heart of many
Natural Language Processing applications. For ex-
ample, given a sentence as input: part-of-speech
(POS) tagging involves finding the appropriate POS
tag sequence (Thede and Harper, 1999); pars-
ing involves finding the appropriate tree structure
(Kubler et al., 2009) and statistical machine trans-
lation (SMT) involves finding correct target lan-
guage translation (Brown et al., 1993). The accuracy
achieved on such tasks can often be improved signif-
icantly with the help of a discriminative reranking
step (Collins and Koo, 2005; Charniak and John-
son, 2005; Shen et al., 2004; Watanabe et al., 2007).
For the POS tagging, reranking is relative less ex-
plored due to the already higher accuracies in En-
glish (Collins, 2002), but it is shown to improve ac-
curacies in other languages such as Chinese (Huang
et al., 2007). In this paper, we propose a novel ap-
proach to discriminative reranking and show its ef-
fectiveness in POS tagging. Reranking allows us to
use arbitrary features defined jointly on input and
output spaces that are often difficult to incorporate
into the baseline decoder due to the computational
tractability issues. The effectiveness of reranking
depends on the joint features defined over both input
and output spaces. This has led the community to
spend substantial efforts in defining joint features for
reranking (Fraser et al., 2009; Chiang et al., 2009).
Unfortunately, developing joint features over the
input and output space can be challenging, espe-
cially in problems for which the exact mapping be-
tween the input and the output is unclear (for in-
stance, in automatic caption generation for images,
semantic parsing or non-literal translation). In con-
trast to prior work, our approach uses features de-
fined separately within the input and output spaces,
and learns a mapping function that can map an ob-
ject from one space into the other. Since our ap-
proach requires within-space features, it makes the
feature engineering relatively easy.
For clarity, we will discuss our approach in the
context of POS tagging, though of course it gener-
alizes to any reranking problem. At test time, in
POS tagging, we receive a sentence and a list of
candidate output POS sequences as input. We run
a feature extractor on the input sentence to obtain
a representation x ∈ Rd1; we run an independent
</bodyText>
<page confidence="0.53007">
699
</page>
<note confidence="0.9753205">
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 699–709,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999974379310345">
feature extractor on each of the m-many outputs
to obtain representations y1, ... , ym E Rd2. We
will project all of these points down to a low k-
dimensional space by means of matrices A E Rd1xk
(for x as AT x) and B E Rd2xk (for y� as BT y).
We then select as the output the yj that maximizes
cosine similar to x in the lower-dimensional space:
maxj cos(ATx, BT yj). The goal is to learn the pro-
jection matrices A and B so that the result of this
operation is a low-loss output.
Given training data of sentences and their refer-
ence tag sequences, our approach implicitly uses all
possible pairwise feature combinations across the
views and learns the matrices A and B that can map
a given sentence (as its feature vector) to its cor-
responding tag sequence. Considering all possible
pairwise combinations enables our model to auto-
matically handle long range dependencies such as
a word at a position effecting the tag choice at any
other position.
Experiments performed on four languages (En-
glish, Chinese, French and Swedish) show the ef-
fectiveness of our approach in comparison to the
baseline decoder and to the existing reranking ap-
proaches (Sec. 4). Using only the within-space fea-
tures, our models are able to beat reranking ap-
proaches that use more informative joint features.
While it is possible to include joint features into our
models, we leave this for future work.
</bodyText>
<sectionHeader confidence="0.987949" genericHeader="method">
2 Models for Low-Dimensional Reranking
</sectionHeader>
<bodyText confidence="0.999951">
In this section, we describe our approach to learning
low-dimensional representations for reranking. We
first fix some notation, then discuss the intuition be-
hind the problem we wish to solve. We propose both
generative-style and discriminative-style approaches
to formalizing this intuition, as well as a softened
variant of the discriminative model. In the subse-
quent section, we discuss computational issues re-
lated to these models.
</bodyText>
<subsectionHeader confidence="0.760277">
2.1 Notation
</subsectionHeader>
<bodyText confidence="0.999310538461539">
Let xi E Rd1 and yi E Rd2 be the feature vectors
representing the ith(1 · · · n) sentence and its refer-
ence tag sequence from the training data. Each sen-
tence is also associated with mi number of candi-
date tag sequences, output by the baseline decoder,
and are represented as yij E Rd2 j = 1 · · · mi. Each
candidate tag sequence (yij) is also associated with
a non-negative loss Lij. Note that we place abso-
lutely no constraints on the loss function. Moreover,
let X (d1 xn) and Y (d2 xn) denote the data matri-
ces with xi and yi as columns respectively. Finally,
let (u, v) denote the dot product of the two vectors
u and v.
</bodyText>
<subsectionHeader confidence="0.988676">
2.2 Intuition
</subsectionHeader>
<bodyText confidence="0.99999225">
As stated in the introduction, our goal is to learn
projections A E Rd1xk and B E Rd2xk in such a
way that test-time predictions are made with high
accuracy (or low loss). At test time, the output will
be chosen by maximizing cosine similarity between
the input and the output, after projecting these vec-
tors into a low-dimensional space using A and B,
respectively. The cosine similarity in our context is:
</bodyText>
<equation confidence="0.9866635">
� �
xT AAT x �yT j BBT yj
</equation>
<bodyText confidence="0.999590166666667">
Our goal is to learn A and B in such a way that the
yj with maximum cosine similarity to an x is ac-
tually the correct output. In what follows, we will
describe our models to find one-dimensional projec-
tion vectors a E Rd1 and b E Rd2, but the general-
ization to matrices A and B is very trivial.
</bodyText>
<subsectionHeader confidence="0.999188">
2.3 A Generative-Style Model
</subsectionHeader>
<bodyText confidence="0.999992722222222">
The first model we propose is akin to a gener-
ative probabilistic model, in the sense that it at-
tempts to model the relationship between an input
and its desired output, without taking alternate pos-
sible outputs into account. In the context of the in-
tuition sketched in the previous section, the idea is
to choose A and B so as to maximize the cosine
similarities on the training data between each input
and it’s correct (or minimal-loss) output. This model
intentionally ignores the information present in the
alternative, incorrect outputs. The hope is that by
making the cosine similarities with the best output
as high as possible, all the alternate outputs will look
bad in comparison.
Given a training data of sentences and their
reference tag sequences represented as X and Y
(Sec. 2.1), our generative model finds projection di-
rections, in word and tag spaces, along which the
</bodyText>
<equation confidence="0.904679666666667">
xT ABT yj
(1)
700
</equation>
<bodyText confidence="0.877331">
aligned sentence and tag sequence pairs have maxi-
mum cosine similarity. In the one-dimensional set-
ting, it finds directions a E Rd1 and b E Rd2 such
that the correlation as defined in Eq. 2 is maximized.
</bodyText>
<equation confidence="0.9955345">
aT XYT b (2)
-\/aT X XT a-\/bTY YT b
</equation>
<bodyText confidence="0.939136">
Since the objective is invariant to the scaling of vec-
tors a and b, it can be rewritten as:
</bodyText>
<equation confidence="0.9947135">
aT XYT b (3)
s.t. aT XXT a = 1 and bT YYT b = 1(4)
</equation>
<bodyText confidence="0.999976666666667">
We refer to the constraints in Eq. 4 as length con-
straints in the rest of this paper.
To understand why maximizing this objective
function learns a good mapping function between
the sentence and the tag sequence, consider decom-
posing the objective function as follows:
</bodyText>
<equation confidence="0.645924">
�ymi bm
�
l
xial ymi bm
</equation>
<bodyText confidence="0.981212684210527">
where we replaced the scalars xliym i and albm with
φlm
i and wlm respectively. So finally, the objective
can be expressed as aT XYT b = Pi(w, φ(xi, yi))
where w is the weight vector and φ(xi, yi) is a vec-
tor of size (d1 x d2) and is given by the Kronecker
product of the two feature vectors xi and yi.
In this form, the generative objective function
bears similarity to the linear boundary surface
widely used in machine learning, except that the
weights are restricted to be the outer product of two
vectors. From the reduced expressions, it is clear
that our generative model considers all possible pair-
wise combinations of the input features (d1xd2) and
learns which of them are more important than others.
Intuitively, it puts higher weight on a word and tag
pair that co-occur frequently in the training data, at
the same time each of these are infrequent in their
own views.
</bodyText>
<subsectionHeader confidence="0.99413">
2.4 A Discriminative-Style Model
</subsectionHeader>
<bodyText confidence="0.999991074074074">
The primary disadvantage of our generative model is
that it only uses input sentences and their reference
tag sequences and does not use the incorrect candi-
date tag sequences of a given sentence at all. In what
follows, we describe a model that utilize the incor-
rect candidate tag sequences as negative examples
to improve the projection directions (a and b). Our
goal is to address this by adding constraints to our
model that explicitly penalize ranking high-loss out-
puts higher than low-loss outputs, as is often done in
the context of maximum-margin structure prediction
techniques (Taskar et al., 2004).
In this section, we describe a discriminative
model that keeps track of the margin deviations and
finds the projection directions iteratively. Intuitively,
after the projection into the lower dimensional sub-
space, the cosine similarity of a sentence to its refer-
ence tag sequence must be greater than that of its
incorrect candidate tag sequences. Moreover, the
margin between these similarities should be propor-
tional to the loss of the candidate translation, i.e. the
more dissimilar a candidate tag sequence to its ref-
erence is, the farther it should be from the reference
in the projected space.
From the decomposition shown in Eq. 5, for a
given pair of source sentence xi and a tag sequence
yj, the generative model assigns a score of :
</bodyText>
<equation confidence="0.865599">
(a, xi)(b, yj) = aTxiyTj
b
</equation>
<bodyText confidence="0.961412384615385">
Each input sentence is also associated with a list
of candidate tag sequences and since each of these
candidate sequences are incorrect they should be as-
signed a score less than that of the reference tag se-
quence. Drawing ideas from structure prediction lit-
erature (Bakir et al., 2007), we modify the objec-
tive function in order to include these terms. This
idea can be captured using a loss augmented mar-
gin constraint for each sentence, tag sequence pair
(Tsochantaridis et al., 2004). Let ξi denote a non-
negative slack variable, then we define our new op-
timization problem as:
arg max 1 − λaTXY Tb −
</bodyText>
<equation confidence="0.997470375">
a,b,ξ&gt;0 λ
i
s.t. aT XXT a = 1 and bT YYT b = 1
bi bj aTxiyTi b − aTxiyT ijb &gt; 1 − ξi
Lij
arg max
a,b
aTXYTb = Xn (xi, a)(yi, b)
i=1
= Xn � Xd1 xlial · Xd2
i=1 l=1 m=1
= Xn � Xd1 Xd2
i=1 l=1 m=1
~ X
l,m=1
wlmφim/ (5)
= Xn
i=1
d1,d2
ξi (6)
701
�
LijrT b (8)
ij
</equation>
<bodyText confidence="0.999977571428571">
where 0 &lt; A &lt; 1 is a weight parameter. This ob-
jective function is ensuring that the margin between
the reference and the candidate tag sequences in the
projected space (as given by aTxiyTi b − aTxiyTijb)
is proportional to its loss (Lij). Notice that the slack
is defined for each sentence and it remains the same
for all of its candidate tag sequences.
</bodyText>
<subsectionHeader confidence="0.995094">
2.5 A Softened Discriminative Model
</subsectionHeader>
<bodyText confidence="0.997425888888889">
One disadvantage of the discriminative model de-
scribed in the previous section is that it cannot be
optimized in closed form (as discussed in the next
section). In this section, we consider a model that
lies between the generative model and the (fully)
discriminative model. This softened model has at-
tractive computational properties (it is easy to com-
pute) and will also form a building block for the op-
timization of the full discriminative model.
For each sentence xi, its reference tag sequence
yi should be assigned a higher score than any of its
candidate tag sequences yij i.e. we want to maxi-
mize aTxiyTi b−aTxiyTijb. In the fully discrimina-
tive model, we enforce that this is at least one (mod-
ulo slack). In the relaxed version, we instead require
that this hold on average. In order to achieve this
we add the following terms to the objective function:
bj = 1 mi
</bodyText>
<equation confidence="0.459924">
aT xiyTi b − aTxikTijb = aTxirTijb (7)
</equation>
<bodyText confidence="0.9987768">
where rij = yi − yij is the residual vector between
the reference and the candidate sequences. Now,
we simply sum all these terms for a given sentence
weighted by their loss and encourage it to be as high
as possible, i.e. we maximize
</bodyText>
<equation confidence="0.977613">
� � � 1 Xmi
Lij aTxirT ijb = aT xi mi
j=1
</equation>
<bodyText confidence="0.934313818181818">
The normalization by mi takes care of unequal num-
bers of candidate tag sequences that often arises be-
cause of the difference in the lengths of the input
sentences. Now let R denote a matrix of the same
size as that of Y (i.e. d2 x n) with its ith column as
Pmi
given by 1 j=1 Lijrij, then we add the following
mi
term to the generative objective function:
Finally, the projection directions are obtained by
solving the following optimization problem:
</bodyText>
<equation confidence="0.966833666666667">
arg max (1 − A)aTXYTb + A aTXRTb (10)
a,b
s.t. aT XXT a = 1 and bT YYT b = 1
</equation>
<bodyText confidence="0.999969">
where 0 &lt; A &lt; 1 is the weight parameter to be
tuned on the development set.
</bodyText>
<sectionHeader confidence="0.992651" genericHeader="method">
3 Optimization
</sectionHeader>
<bodyText confidence="0.999987">
In this section, we describe how we solve the opti-
mization problems associated with our models. First
we discuss the solution of the generative model.
Next, we discuss the softened discriminative model,
since its solution will be used as a subroutine in our
final discussion of the fully discriminative model.
</bodyText>
<subsectionHeader confidence="0.99956">
3.1 Optimizing the Generative Model
</subsectionHeader>
<bodyText confidence="0.966294375">
The optimization problem corresponding to the gen-
erative model turns out to be identical to that of
canonical correlation analysis (CCA) (Hotelling,
1936; Hardoon et al., 2004), which immediately
suggests a solution by solving an eigensystem. In
particular, the projection directions are obtained by
solving the following generalized eigensystem:
� 0 Cxy � ~a � �Cxx 0 � ~a~
</bodyText>
<equation confidence="0.7788045">
= (11)
Cyx 0 b 0 Cyy b
</equation>
<bodyText confidence="0.815675">
where Cxx
</bodyText>
<equation confidence="0.7996124">
= (1 − T)XXT + TI, Cyy = (1 −
T)YYT + TI are autocovariance matrices,
=
XYT is the cross-covariance matrix,
=
</equation>
<bodyText confidence="0.999131571428571">
T is a regularization parameter and I is the identity
matrix of appropriate size. Using these eigenvectors
as columns, we form projection matrices A and B.
These projection matrices are used to project sen-
tences and tag sequences into a common lower di-
mensional subspace. In general, using all the eigen-
vectors is sub-optimal fr
</bodyText>
<figure confidence="0.668956647058823">
Cxy
Cyx
CTxy,
om the generalization per-
spective so we retain only top k eigenvectors.
1
mi
Xmi
j=1
Xn ~ 1 Xmi � 3.2 Optimizing the Softened Model
i=1 aTxi LijrT b = aTXRTb (9) In the softened discriminative version, the summa-
mi ij tion of all the difference terms over all candidate tag
j=1 sequences and sentences (Eq. 9), enables a simpler
objective function whose optimum can be derived
by following a procedure very
similar to that of the
702
</figure>
<bodyText confidence="0.996387">
generative model. In particular, the projection direc-
tions are obtained by solving Eq. 11 except that Cxy
is replaced with X((1 − λ)Y T + λRT).
</bodyText>
<subsectionHeader confidence="0.999231">
3.3 Optimizing the Discriminative Model
</subsectionHeader>
<bodyText confidence="0.9999476">
To solve the discriminative model, we begin by con-
structing the Lagrange dual. Let β1, β2 and αij
be the Lagrangian multipliers corresponding to the
length and the margin constraints respectively, then
the Lagrangian of Eq. 6 is given by:
</bodyText>
<equation confidence="0.927379333333333">
L = 1λλaTXY Tb −
y/ (ba)
(C 0 Cyy/ (ba)
</equation>
<bodyText confidence="0.977153238095238">
and R is a ma-
trix of size d2 x n with ith column as given by
matrix to indicate that it is dependent on
the Lagrangian multipliers
In other words, the
solution is similar to that of the previous formulation
except that the residual vectors are weighted by the
Lagrangian multipliers instead of the loss function.
Unlike the max margin formulations of SVM, it is
not easy to rewrite the parameters a, b in terms of
the Lagrangian multipliers
as
itself depends
on
Hence, rewriting the parameters in terms
of the Lagrangian multipliers and then solving the
dual is not amenable in this case.
In order to solve this optimization problem, we
resort to an alternate optimization technique in the
primal space. It proceeds in two stages. In the first
stage, we keep the Lagrangian multipliers
</bodyText>
<figure confidence="0.922211115789474">
fixed
and then solve for the parameters a, b,
and
Projection directions a, b and their Lagrangian
multipliers
�1�λ
λ Y T + RT~
where Cα xy = X
Pmi
1 j=1 αijrij. We use superscript α on the cross-
mi
covariance
αij.
αij
Cαxy
αij’s.
αij
β1,β2
ξi.
β1,β2 are obtained by solving the gen-
eralized eigenvalue problem given in Eq. 12. Using
Algorithm 1 Altern
Input: X, Y,
L,
Output: A, B
Vi, j
=
2:
=
Cxx
(1
+
Cyy = (1
+
3: repeat
6: Solve for the eigenvectors of Eq. 12. .
7: Form matrices A, B with top k eigenvectors
as columns; k is determined using dev. set.
8: Let An &amp; Bn be normalized versions of A
and B s.t. they follow the length constraints.
9: for each sentence i = 1
n do
10: j =
mi,
= (1
12: if
&gt; 0 then
=
15: end if
16: end for
17: until slack values
chan
Y�,
λ,τ
1:
αij
Lij;
rij
yi−
ij;
=
−τ)XXT
τI;
−τ)Y YT
τI
Pmi
4: Form R with ith column as 1 j=1 αijrij
λ YT + RT~ mi
�1�λ
5: Cα xy = X
···
1···
ψij
−xT i AnBT
11: ξi = min ~0 , ψij  |s.t. ψij &gt; 0 n rij�Lij
ξi
�
13: dij = xT i AnBT n rij − �1 − ξi
Lij
αij
αij−γdij
doesn’t
ge
18: return A, B
Differentiating the Lagrangian with respect to the
parameters a, b and setting them to zero yields
the solution for the parameters in terms of the La-
grangian multipliers
αij as follows:
0
CC
α
x0
=
(12)
</figure>
<bodyText confidence="0.992122391304348">
ate optimization algorithm for
solving the parameters of Discriminative Model.
these projection directions, we determine the slack
variable
for each sentence. In the second stage
of the alternate optimization, we fix a, b and
and
take a gradient descent step along
to minimize
the function. We repeat this process until conver-
gence. In our experiments, we noticed that this al-
gorithm converges within five iterations, so we only
run it for five iterations.
The pseudocode of our approach is shown in
Alg. 1. First we initialize the Lagrangian multipli-
ers proportional to the loss of the candidate tag se-
quences (step 1). This ensures that the eigenvectors
solved in step 6 are same as the output given by the
softened model (Sec. 2.5). In general, in our experi-
ments, we observed that this is a good starting point.
After solving the generalized eigenvalue problem in
step. 6, we consider the top k eigenvectors, as de-
termined by the error on the development set an
</bodyText>
<equation confidence="0.649519666666667">
ξi
ξi
αij’s
</equation>
<bodyText confidence="0.986372">
d
normalize them so that they follow the length con-
straints (steps 7 and 8). In the rest of the algorithm,
</bodyText>
<equation confidence="0.956039461538462">
ξi
Xn
i=1
� � � �
− β1 aTXXTa − 1 − β2 bTY YTb − 1
n,miX
+
i=1,j=1
!
aTxirT ijb − 1 + ξi
αij
Lij
703
</equation>
<bodyText confidence="0.999979162162162">
we use these normalized projection directions to find
the slack values which are in turn used to find the up-
date direction for the Lagrangian variables.
In step 10, we compute the potential slack value
(ψij) for each constraint so that it is satisfied and
then choose the minimum of the positive ψij val-
ues as the slack for this sentence (step 11). If the
chosen slack value is equal to zero, it implies that
ψij &lt; 0 Vj = 1 · · · mi which in turn implies that
all the constraints of a given input sentence are sat-
isfied by the current projection directions and hence
there is no need to update the Lagrangian multipli-
ers. Otherwise, some of the constraints are still not
satisfied and hence we will update their correspond-
ing Lagrangian multipliers in steps 13 and 14. In
specific, step 13 computes the deviation of the mar-
gin constraints with the new slack value and step 14
updates the Lagrangian multipliers along the gradi-
ent direction.
In principle, our approach is similar to the cutting
plane algorithm used to optimize slack re-scaling
version of Structured SVM (Tsochantaridis et al.,
2004), but it differs in selecting the slack variable
(step 11). The cutting plane method chooses ξi as
the maximum of 10, ψij} where as we choose the
minimum of the positive ψij values as the slack. In-
tuitively, this means that the cutting plane algorithm
chooses a constraint that is most violated which re-
sults in fewer constraints. This is crucial in struc-
tured SVM, because solving the dual problem is cu-
bic in terms of the number of examples and con-
straints. In contrast, our approach selects the slack
such that at least one of the constraints is satisfied
and adds all the remaining constraints to the active
set. Since step 6 considers a weighted average of all
these constraints the complexity depends only on the
number of training examples and not the constraints.
</bodyText>
<subsectionHeader confidence="0.999388">
3.4 Combining with Viterbi Decoding Score
</subsectionHeader>
<bodyText confidence="0.999991266666667">
All the three formulations discussed until now do not
consider the Viterbi decoding score assigned to each
candidate tag sequence. As explained in Collins and
Koo (2005), the decoding score plays an important
role in reranking the candidate sentences. Here, we
describe a simple linear combination of the Viterbi
decoding score and the score obtained by projecting
into the low-dimensional subspace, using projection
directions obtained by any of the above models.
For a given sentence xi and candidate tag se-
quence pair yij, let sij and pij (Eq. 1) be the scores
assigned by Viterbi decoding and the lower dimen-
sional projections respectively. Then we define the
final score for this pair as a simple linear combina-
tion of these two scores as:
</bodyText>
<equation confidence="0.910876">
Score(xi, yij) = sij + w pij (13)
</equation>
<bodyText confidence="0.99997275">
The weight w is optimized using a grid search on
the development data set, we search for w from 0 to
100 with an increment of 1 and choose the value for
which the error is minimum on the development set.
</bodyText>
<subsectionHeader confidence="0.777431">
3.5 Reranking for POS Tagging
</subsectionHeader>
<bodyText confidence="0.9999739">
To summarize our approach, we convert the train-
ing data into feature vectors and use any of the
three methods discussed above to find the lower di-
mensional projection directions (a and b). Each of
those approaches involve solving a similar general-
ized eigenvalue problem (Eq. 11) with the cross co-
variance matrix Cxy defined differently in the three
approaches. This problem can be solved in differ-
ent ways, but we use the following approach since it
reduces the size of the eigenvalue problem.
</bodyText>
<equation confidence="0.9552568">
C�1
yy CT xyC�1
xx Cxy b = ω b (14)
I/ CXX Cxy b (1 5)
w
</equation>
<bodyText confidence="0.999959125">
where ω is the eigenvalue. Assuming that d2 « d1,
which is usually true in POS tagging because of
the smaller tag vocabulary, these equations solve
a smaller eigenvalue problem. After solving the
eigenvalue problem, we form matrices A and B with
columns as the top k eigenvectors a and b respec-
tively. Given a new sentence and candidate tag se-
quence pair (xi, yij), their similarity is obtained us-
ing Eq. 1. Now, based on the development data set
we find the weight (w) for the linear combination of
the projection and Viterbi decoding scores (Eq. 13).
During the reranking stage, we first use Eq. 1 to
compute the projection score for all the candidate
tag sequences and then use Eq. 13 to combine this
scores with the decoding score. The candidate tag
sequences are reranked based on this final score.
</bodyText>
<sectionHeader confidence="0.998636" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9982555">
In this section, we report POS tagging experiments
on four languages: English, Chinese, French and
</bodyText>
<figure confidence="0.659462">
1
a =
</figure>
<page confidence="0.485564">
704
</page>
<table confidence="0.995370333333333">
Train. Dev. Test
English (En.) # sent. 15K 2K 1791
# words 362K 47K 43K
Chinese (Zh.) # sent. 50K 4K 3647
# words 292K 26K 25K
# sent. 9K 2K 1351
French (Fr.) # words 254K 57K 40K
Swedish (Sv.) # sent. 8K 2K 1431
# words 137K 31K 28K
</table>
<tableCaption confidence="0.999922">
Table 1: Training and test data statistics.
</tableCaption>
<bodyText confidence="0.9999642">
Swedish. The data in all these languages is obtained
from the CoNLL 2006 shared task on multilingual
dependency parsing (Buchholz and Marsi, 2006).
We only consider the word and its fine grained POS
tag (columns 2 and 5 respectively) and ignore the
dependency links in the data. Table 1 shows the data
statistics in each of these languages.
We use a second order Hidden Markov Model
(Thede and Harper, 1999) based tagger as a baseline
tagger in our experiments. This model uses trigram
transition and emission probabilities and is shown
to achieve good accuracies in English and other lan-
guages (Huang et al., 2007). We refer to this as the
baseline tagger in the rest of this paper and is used to
produce n-best list for each candidate sentence. The
n-best list for training data is produced using multi-
fold cross-validation like Collins and Koo (2005)
and Charniak and Johnson (2005). The first block of
Table 2 shows the accuracies of the top-ranked tag
sequence (according to the Viterbi decoding score)
and the oracle accuracies on the 10-best list. As
expected the accuracies on English and French are
high and are on par with the state-of-the-art systems.
From the oracle scores, it is clear that though there is
a chance for improvement using reranking, the scope
for improvement in English is less compared to the
5 point improvement reported for parsing (Charniak
and Johnson, 2005). This indicates the difficulty
of the reranking problem for POS tagging in well-
resourced languages.
</bodyText>
<subsectionHeader confidence="0.999924">
4.1 Reranking Features and Baselines
</subsectionHeader>
<bodyText confidence="0.999991">
In this paper, except for Chinese, we use suffixes of
length two to four as features in the word view and
unigram and bigram tag sequences as features in the
tag view. That is, we convert each word of the sen-
tence into suffixes of length two to four and then
treat each sentence as a bag of suffixes. Similarly,
we treat a candidate POS tag sequence as a bag of
unigram and bigram tag features. For Chinese, we
use character sequences of length one and two as
features for the sentences and use unigram and bi-
gram POS tag sequences on the tag view. We did
not include any alignment based features, i.e. fea-
tures that depend on the position.
We compare our models with a boosting-based
discriminative approach (Collins and Koo, 2005)
and its regularized version (Huang et al., 2007). In
order to enable a fair comparison, we use suffix and
tag pairs as features for both these models. For ex-
ample, we would generate the following features for
the word ‘selling’ in the phrase “the/DT selling/NN
pressure/NN”: (ng, NN), (ng, DT NN), (ing,NN),
(ing,DT NN), (ling,NN), (ling,DT NN). For com-
parison purposes, we also show results by running
the baseline rerankers with n-gram features.
</bodyText>
<subsectionHeader confidence="0.697447">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.9999874">
There are following hyper parameters in each of our
models, regularization parameter T, weight parame-
ter A in the discriminative and softened discrimina-
tive models, the linear combination weight w with
the Viterbi decoding score, and finally, the size of
the lower dimensional subspace (k). We use grid
search to tune these parameters based on the devel-
opment data set. The optimal hyperparameter values
differ based on the model and the language, but the
tagging accuracy is relatively robust with respect to
these parameter values. For English, the best values
for the discriminative model are T = 0.95, A = 0.3
and k = 75. For the same language, Fig. 1 shows
the performance with respect to T and A parameters,
respectively, with other parameters fixed to their op-
timal values. Notice that, although the performance
varies it is always more than the accuracy of the
baseline tagger (96.74%).
Table 2 shows the results of different models on
the development and test data sets. On the test data
set, the baseline reranking approaches perform bet-
ter than the HMM decoder in Chinese and Swedish
languages, but they underperform in English and
French languages. This is justifiable because the in-
dividual characters are good indicators of POS tag
</bodyText>
<figure confidence="0.96449275">
705
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
A
0.78 0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.96 0.98 1
T
96.86
96.84
96.82
96.78
96.76
96.74
96.8
Discriminative
Softened-Disc
Generative
Discriminative
Softened-Disc
96.86
96.84
96.82
96.8
96.78
96.76
96.74
</figure>
<figureCaption confidence="0.99893">
Figure 1: Tagging accuracy with hyperparameters T and A on English development data set.
</figureCaption>
<table confidence="0.999954181818182">
Development Set Test set
English Chinese French Swedish English Chinese French Swedish
Baseline 96.74 92.55 96.94 93.22 96.15 92.31 97.41 93.23
Oracle 98.85 98.41 98.61 96.96 98.39 98.19 99.00 96.48
Collins (Sufx) 96.66 93.00 96.87 93.50 96.06 92.81 97.35 93.44
Regularized (Sufx) 96.60 93.12 96.90 93.36 96.00 92.88 97.38 93.35
Generative 96.82 93.14 96.97 93.46 96.24 92.95 97.43 93.26
Softened-Disc 96.85 93.14 97.04 93.49 96.32 92.87 97.53 93.24
Discriminative 96.85 93.17 97.03 93.50 96.3 92.91 97.53 93.36
Collins (n-gm) 96.74 93.14 97.06 93.44 96.13 92.74 97.54 93.45
Regularized (n-gm) 96.78 93.14 97.01 93.45 96.14 92.80 97.52 93.40
</table>
<tableCaption confidence="0.969619333333333">
Table 2: Accuracy of the baseline HMM tagger and different reranking approaches. For comparison purposes, we also
showed the results of Collins and Koo (2005) its regularized versions with n-gram features. The improvements of our
discriminative models are statistically significant at p = 0.01 and p = 0.05 levels on Chinese and English respectively.
</tableCaption>
<bodyText confidence="0.99918125">
information for Chinese and this additional informa-
tion is being exploited by the reranking approaches.
Swedish, on the other hand, is a Germanic language
with compound word phenomenon which makes the
baseline HMM decoder weaker compared to English
and French.
The fourth block shows the performance of our
models. Except in Swedish, one of our models out-
perform the baseline decoder and the other rerank-
ing approaches. The fact that our models outperform
the baseline system and other reranking approaches
indicate that, by considering all the pairwise com-
binations of the input features our models capture
dependencies that are left by other models. Among
the different formulations of our approach, maxi-
mizing the margin between the correct and incorrect
candidates performed better than generative, and en-
suring that the margin is proportional to the loss of
the candidate sequence (discriminative) led to even
more improved results. Except in Chinese, our dis-
criminative version performed at least as well as the
other variants. Compared to the baseline decoder,
the discriminative version achieves a maximum im-
provement of 0.6 points in Chinese while achieving
0.15, 0.12 and 0.13 points of improvement in En-
glish, French and Swedish languages respectively.
We also reported the results of the baseline
rerankers with n-gram features in the fifth block of
</bodyText>
<tableCaption confidence="0.783547">
Table 2. We remind the reader that our models use
only suffix features, so for a fair comparison the
</tableCaption>
<table confidence="0.9542768">
706
En. Zh. Fr. Sv.
Generative 94.83 89.89 96.1 91.89
Softened-Disc 95.04 89.61 95.97 91.95
Discriminative 94.95 89.76 95.82 92.11
</table>
<tableCaption confidence="0.996742">
Table 3: Accuracies without combining with Viterbi de-
coding score.
</tableCaption>
<bodyText confidence="0.9998855">
reader should compare our results with the baseline
rerankers run with the suffix features. The perfor-
mance of these baseline rankers improved when we
include the n-gram features but it is still less than
the discriminative model in most cases.
Finally, Table 3 shows the performance of our
models without combining with the Viterbi decod-
ing score. As shown, the performance drops signif-
icantly and is in accordance with the behavior ob-
served elsewhere (Collins and Koo, 2005).
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99998708">
In this section, we discuss approaches that are most
relevant to our problem and the approach.
In NLP literature, discriminative reranking has
been well explored for parsing (Collins and Koo,
2005; Charniak and Johnson, 2005; Shen and Joshi,
2003; McDonald et al., 2005; Johnson and Ural,
2010) and statistical machine translation (Shen et
al., 2004; Watanabe et al., 2007; Liang et al., 2006).
Collins (2002) proposed two reranking approaches,
namely boosting algorithm and a voted perceptron,
for the POS tagging task. Later Huang et al. (2007)
propose a regularized version of the objective used
by Collins (2002) and show an improved perfor-
mance for Chinese. In all of the above reranking
approaches, the feature functions are defined jointly
on the input and output, whereas in our approach,
the features are defined separately within each view
and the algorithm learns the relationship between
them automatically. This is the primary difference
between our approach and the existing rerankers.
In principle, our margin formulations are similar
to the max margin formulations of CCA (Szedmak
et al., 2007) and maximum margin regression (Szed-
mak et al., 2006; Wang et al., 2007). These ap-
proaches solve the following optimization problem:
</bodyText>
<equation confidence="0.959979">
min IIW II2 + C1T� (16)
s.t. (yi, Wφ(x)i) &gt; 1 − �i Vi = 1··· n
</equation>
<bodyText confidence="0.999993421052632">
Our approach differs from these formulations in two
main ways: the score assigned by our generative
model (equivalent to CCA) for an input-output pair
(xTi abTyi) can be converted into this format by
substituting W +— baT but in doing so we are
ignoring the rank constraint. It is often observed
that, dimensionality reduction leads to an improved
performance and thus the rank constraint becomes
crucial. Another major difference is that, the con-
straints in Eq. 16 represent that any input and out-
put pair should have at least a margin of 1 (modulo
slack), whereas in our approach, the constraints in-
clude incorrect outputs along with their loss value.
In other words, our formulation is more suitable for
the reranking problem while Eq. 16 is more suitable
for regression or classification tasks. Our genera-
tive model is very similar to the supervised semantic
hashing work (Bai et al., 2010) but the way we opti-
mize is completely different from theirs.
</bodyText>
<sectionHeader confidence="0.999363" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9994972">
In this paper, we proposed a novel family of mod-
els for discriminative reranking problem and showed
improvements for the POS tagging task in four dif-
ferent languages. Here, we restricted our scope to
showing the utility of our technique and, hence, did
not experiment with different features, though it is
an important direction. By using only within space
features, our models are able to beat the rerank-
ing approaches that use potentially more informa-
tive alignment-based features. It is also possible to
include alignment-based features into our models by
posing the problem as a feature selection problem on
the covariance matrices (Jagarlamudi et al., 2011).
Our approach involves an inverse computation and
an eigenvalue problem. Although our models scale
to medium size data sets (our Chinese data set has
50K examples and 33K features), these operations
can be expensive. But there are alternative approx-
imation techniques that scale well to large data sets
(Halko et al., 2009). We leave this for future work.
</bodyText>
<sectionHeader confidence="0.995473" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9983866">
We thank Zhongqiang Huang for providing the code
for the baseline systems, Raghavendra Udupa and
the anonymous reviewers for their insightful com-
ments. This work is partially funded by NSF grants
IIS-1153487 and IIS-1139909.
</bodyText>
<page confidence="0.703356">
707
</page>
<sectionHeader confidence="0.972601" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999913076923077">
Bing Bai, Jason Weston, David Grangier, Ronan
Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier
Chapelle, and Kilian Weinberger. 2010. Learning
to rank with (a lot of) word features. Inf. Retr.,
13(3):291–314, June.
G¨ukhan H. Bakir, Thomas Hofmann, Bernhard
Sch¨olkopf, Alexander J. Smola, Ben Taskar, and
S. V. N. Vishwanathan. 2007. Predicting Structured
Data (Neural Information Processing). The MIT
Press.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19:263–311, June.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning, CoNLL-X ’06, pages 149–
164, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ’05,
pages 173–180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, NAACL ’09, pages 218–226, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31:25–70, March.
Michael Collins. 2002. Ranking algorithms for named-
entity extraction: boosting and the voted perceptron.
In Proceedings of the 40th Annual Meeting on As-
sociation for Computational Linguistics, ACL ’02,
pages 489–496, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Alexander Fraser, Renjing Wang, and Hinrich Sch¨utze.
2009. Rich bitext projection features for parse rerank-
ing. In Proceedings of the 12th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, EACL ’09, pages 282–290, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Nathan Halko, Per-Gunnar. Martinsson, and A. Joel
Tropp. 2009. Finding structure with randomness:
Stochastic algorithms for constructing approximate
matrix decompositions. Technical report, California
Institute of Technology.
David R. Hardoon, Sandor R. Szedmak, and John R.
Shawe-taylor. 2004. Canonical correlation analy-
sis: An overview with application to learning methods.
Neural Comput., 16:2639–2664, December.
Harold Hotelling. 1936. Relation between two sets of
variables. Biometrica, 28:322–377.
Zhongqiang Huang, Mary Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and discrim-
inative reranking. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1093–1102,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Jagadeesh Jagarlamudi, Raghavendra Udupa, Hal
Daum´e III, and Abhijit Bhole. 2011. Improving
bilingual projections via sparse covariance matrices.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
930–940, Edinburgh, Scotland, UK., July. Association
for Computational Linguistics.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown parsers. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, HLT ’10, pages 665–668,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Sandra Kubler, Ryan McDonald, Joakim Nivre, and
Graeme Hirst. 2009. Dependency Parsing. Morgan
and Claypool Publishers.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 761–768, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ’05,
pages 91–98, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Libin Shen and Aravind K. Joshi. 2003. An SVM based
voting algorithm with application to parse reranking.
In Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003 - Volume 4,
CONLL ’03, pages 9–16, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.499829">
708
</page>
<reference confidence="0.999862043478261">
Libin Shen, Anoop Sarkar, and Franz Och. 2004. Dis-
criminative reranking for machine translation. In Hu-
man Language Technology Conference and the 5th
Meeting of the North American Association for Com-
putational Linguistics: HLT-NAACL 2004, Boston,
USA, May.
S. Szedmak, J. Shawe-Taylor, and E. Parado-Hernandez.
2006. Learning via linear operators: Maximum mar-
gin regression; multiclass and multiview learning at
one-class complexity. Technical report, University of
Southampton.
Sandor Szedmak, Tijl De Bie, and David R. Hardoon.
2007. A metamorphosis of canonical correlation anal-
ysis into multivariate maximum margin learning. In
Proceedings of the fifteenth European Symposium on
Artificial Neural Networks.
Ben Taskar, Carlos. Guestrin, and Daphne Koller. 2004.
Max margin markov networks. In Proceedings of
NIPS 16.
Scott M. Thede and Mary P. Harper. 1999. A second-
order Hidden Markov Model for part-of-speech tag-
ging. In Proceedings of the Annual Meeting on Asso-
ciation for Computational Linguistics, pages 175–182.
Association for Computational Linguistics.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the twenty-first inter-
national conference on Machine learning, ICML ’04,
pages 104–, New York, NY, USA. ACM.
Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-
mak. 2007. Kernel regression based machine trans-
lation. In Human Language Technologies 2007: The
Conference of the North American Chapter of the As-
sociation for Computational Linguistics; Companion
Volume, Short Papers, NAACL-Short ’07, pages 185–
188, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online Large-Margin Training for Sta-
tistical Machine Translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764–
773, Prague, Czech Republic, June. Association for
Computational Linguistics.
</reference>
<page confidence="0.935506">
709
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.934994">
<title confidence="0.996562">Low-Dimensional Discriminative Reranking</title>
<author confidence="0.958641">Jagadeesh</author>
<affiliation confidence="0.999923">Department of Computer University of</affiliation>
<address confidence="0.99771">College Park, MD 20742,</address>
<email confidence="0.999842">jags@umiacs.umd.edu</email>
<author confidence="0.998202">Hal Daum´e</author>
<affiliation confidence="0.999937">Department of Computer University of</affiliation>
<address confidence="0.996437">College Park, MD 20742,</address>
<email confidence="0.999845">hal@umiacs.umd.edu</email>
<abstract confidence="0.9992254">The accuracy of many natural language processing tasks can be improved by a reranking step, which involves selecting a single output from a list of candidate outputs generated by a baseline system. We propose a novel family of reranking algorithms based on learning embeddings of the task’s input and output spaces. This embedding is learned in such a way that prediction becomes a low-dimensional nearest-neighbor search, which can be done computationally efficiently. A key quality of our approach is that engineering can be done the input and output spaces; the relationship between inputs and outputs is learned automatically. Experiments on part-of-speech tagging task in four languages show significant improvements over a baseline decoder and existing reranking approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bing Bai</author>
<author>Jason Weston</author>
<author>David Grangier</author>
<author>Ronan Collobert</author>
<author>Kunihiko Sadamasa</author>
<author>Yanjun Qi</author>
<author>Olivier Chapelle</author>
<author>Kilian Weinberger</author>
</authors>
<title>Learning to rank with (a lot of) word features.</title>
<date>2010</date>
<journal>Inf. Retr.,</journal>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="33583" citStr="Bai et al., 2010" startWordPosition="5861" endWordPosition="5864"> is often observed that, dimensionality reduction leads to an improved performance and thus the rank constraint becomes crucial. Another major difference is that, the constraints in Eq. 16 represent that any input and output pair should have at least a margin of 1 (modulo slack), whereas in our approach, the constraints include incorrect outputs along with their loss value. In other words, our formulation is more suitable for the reranking problem while Eq. 16 is more suitable for regression or classification tasks. Our generative model is very similar to the supervised semantic hashing work (Bai et al., 2010) but the way we optimize is completely different from theirs. 6 Discussion In this paper, we proposed a novel family of models for discriminative reranking problem and showed improvements for the POS tagging task in four different languages. Here, we restricted our scope to showing the utility of our technique and, hence, did not experiment with different features, though it is an important direction. By using only within space features, our models are able to beat the reranking approaches that use potentially more informative alignment-based features. It is also possible to include alignment-</context>
</contexts>
<marker>Bai, Weston, Grangier, Collobert, Sadamasa, Qi, Chapelle, Weinberger, 2010</marker>
<rawString>Bing Bai, Jason Weston, David Grangier, Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier Chapelle, and Kilian Weinberger. 2010. Learning to rank with (a lot of) word features. Inf. Retr., 13(3):291–314, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨ukhan H Bakir</author>
<author>Thomas Hofmann</author>
<author>Bernhard Sch¨olkopf</author>
<author>Alexander J Smola</author>
<author>Ben Taskar</author>
<author>S V N Vishwanathan</author>
</authors>
<title>Predicting Structured Data (Neural Information Processing).</title>
<date>2007</date>
<publisher>The MIT Press.</publisher>
<marker>Bakir, Hofmann, Sch¨olkopf, Smola, Taskar, Vishwanathan, 2007</marker>
<rawString>G¨ukhan H. Bakir, Thomas Hofmann, Bernhard Sch¨olkopf, Alexander J. Smola, Ben Taskar, and S. V. N. Vishwanathan. 2007. Predicting Structured Data (Neural Information Processing). The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<contexts>
<context position="1565" citStr="Brown et al., 1993" startWordPosition="227" endWordPosition="230">earned automatically. Experiments on part-of-speech tagging task in four languages show significant improvements over a baseline decoder and existing reranking approaches. 1 Introduction Mapping inputs to outputs lies at the heart of many Natural Language Processing applications. For example, given a sentence as input: part-of-speech (POS) tagging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). For the POS tagging, reranking is relative less explored due to the already higher accuracies in English (Collins, 2002), but it is shown to improve accuracies in other languages such as Chinese (Huang et al., 2007). In this paper, we propose a novel approach to discriminative reranking and show its effectiveness in POS tagging. Reranking allows us to use arbitrary features defin</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Comput. Linguist., 19:263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06,</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="24076" citStr="Buchholz and Marsi, 2006" startWordPosition="4299" endWordPosition="4302">ng score. The candidate tag sequences are reranked based on this final score. 4 Experiments In this section, we report POS tagging experiments on four languages: English, Chinese, French and 1 a = 704 Train. Dev. Test English (En.) # sent. 15K 2K 1791 # words 362K 47K 43K Chinese (Zh.) # sent. 50K 4K 3647 # words 292K 26K 25K # sent. 9K 2K 1351 French (Fr.) # words 254K 57K 40K Swedish (Sv.) # sent. 8K 2K 1431 # words 137K 31K 28K Table 1: Training and test data statistics. Swedish. The data in all these languages is obtained from the CoNLL 2006 shared task on multilingual dependency parsing (Buchholz and Marsi, 2006). We only consider the word and its fine grained POS tag (columns 2 and 5 respectively) and ignore the dependency links in the data. Table 1 shows the data statistics in each of these languages. We use a second order Hidden Markov Model (Thede and Harper, 1999) based tagger as a baseline tagger in our experiments. This model uses trigram transition and emission probabilities and is shown to achieve good accuracies in English and other languages (Huang et al., 2007). We refer to this as the baseline tagger in the rest of this paper and is used to produce n-best list for each candidate sentence.</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06, pages 149– 164, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1738" citStr="Charniak and Johnson, 2005" startWordPosition="255" endWordPosition="259">ches. 1 Introduction Mapping inputs to outputs lies at the heart of many Natural Language Processing applications. For example, given a sentence as input: part-of-speech (POS) tagging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). For the POS tagging, reranking is relative less explored due to the already higher accuracies in English (Collins, 2002), but it is shown to improve accuracies in other languages such as Chinese (Huang et al., 2007). In this paper, we propose a novel approach to discriminative reranking and show its effectiveness in POS tagging. Reranking allows us to use arbitrary features defined jointly on input and output spaces that are often difficult to incorporate into the baseline decoder due to the computational tractability issues. The effectiveness of re</context>
<context position="24815" citStr="Charniak and Johnson (2005)" startWordPosition="4427" endWordPosition="4430">y links in the data. Table 1 shows the data statistics in each of these languages. We use a second order Hidden Markov Model (Thede and Harper, 1999) based tagger as a baseline tagger in our experiments. This model uses trigram transition and emission probabilities and is shown to achieve good accuracies in English and other languages (Huang et al., 2007). We refer to this as the baseline tagger in the rest of this paper and is used to produce n-best list for each candidate sentence. The n-best list for training data is produced using multifold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). The first block of Table 2 shows the accuracies of the top-ranked tag sequence (according to the Viterbi decoding score) and the oracle accuracies on the 10-best list. As expected the accuracies on English and French are high and are on par with the state-of-the-art systems. From the oracle scores, it is clear that though there is a chance for improvement using reranking, the scope for improvement in English is less compared to the 5 point improvement reported for parsing (Charniak and Johnson, 2005). This indicates the difficulty of the reranking problem for POS tagging in wellresourced lan</context>
<context position="31607" citStr="Charniak and Johnson, 2005" startWordPosition="5534" endWordPosition="5537"> The performance of these baseline rankers improved when we include the n-gram features but it is still less than the discriminative model in most cases. Finally, Table 3 shows the performance of our models without combining with the Viterbi decoding score. As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). 5 Related Work In this section, we discuss approaches that are most relevant to our problem and the approach. In NLP literature, discriminative reranking has been well explored for parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen and Joshi, 2003; McDonald et al., 2005; Johnson and Ural, 2010) and statistical machine translation (Shen et al., 2004; Watanabe et al., 2007; Liang et al., 2006). Collins (2002) proposed two reranking approaches, namely boosting algorithm and a voted perceptron, for the POS tagging task. Later Huang et al. (2007) propose a regularized version of the objective used by Collins (2002) and show an improved performance for Chinese. In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined se</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 173–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>218--226</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2558" citStr="Chiang et al., 2009" startWordPosition="392" endWordPosition="395">cies in other languages such as Chinese (Huang et al., 2007). In this paper, we propose a novel approach to discriminative reranking and show its effectiveness in POS tagging. Reranking allows us to use arbitrary features defined jointly on input and output spaces that are often difficult to incorporate into the baseline decoder due to the computational tractability issues. The effectiveness of reranking depends on the joint features defined over both input and output spaces. This has led the community to spend substantial efforts in defining joint features for reranking (Fraser et al., 2009; Chiang et al., 2009). Unfortunately, developing joint features over the input and output space can be challenging, especially in problems for which the exact mapping between the input and the output is unclear (for instance, in automatic caption generation for images, semantic parsing or non-literal translation). In contrast to prior work, our approach uses features defined separately within the input and output spaces, and learns a mapping function that can map an object from one space into the other. Since our approach requires within-space features, it makes the feature engineering relatively easy. For clarity</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 218–226, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--25</pages>
<contexts>
<context position="1710" citStr="Collins and Koo, 2005" startWordPosition="251" endWordPosition="254">isting reranking approaches. 1 Introduction Mapping inputs to outputs lies at the heart of many Natural Language Processing applications. For example, given a sentence as input: part-of-speech (POS) tagging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). For the POS tagging, reranking is relative less explored due to the already higher accuracies in English (Collins, 2002), but it is shown to improve accuracies in other languages such as Chinese (Huang et al., 2007). In this paper, we propose a novel approach to discriminative reranking and show its effectiveness in POS tagging. Reranking allows us to use arbitrary features defined jointly on input and output spaces that are often difficult to incorporate into the baseline decoder due to the computational tractability iss</context>
<context position="21329" citStr="Collins and Koo (2005)" startWordPosition="3803" endWordPosition="3806">SVM, because solving the dual problem is cubic in terms of the number of examples and constraints. In contrast, our approach selects the slack such that at least one of the constraints is satisfied and adds all the remaining constraints to the active set. Since step 6 considers a weighted average of all these constraints the complexity depends only on the number of training examples and not the constraints. 3.4 Combining with Viterbi Decoding Score All the three formulations discussed until now do not consider the Viterbi decoding score assigned to each candidate tag sequence. As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences. Here, we describe a simple linear combination of the Viterbi decoding score and the score obtained by projecting into the low-dimensional subspace, using projection directions obtained by any of the above models. For a given sentence xi and candidate tag sequence pair yij, let sij and pij (Eq. 1) be the scores assigned by Viterbi decoding and the lower dimensional projections respectively. Then we define the final score for this pair as a simple linear combination of these two scores as: Score(xi, yij) = sij + w</context>
<context position="24783" citStr="Collins and Koo (2005)" startWordPosition="4422" endWordPosition="4425">y) and ignore the dependency links in the data. Table 1 shows the data statistics in each of these languages. We use a second order Hidden Markov Model (Thede and Harper, 1999) based tagger as a baseline tagger in our experiments. This model uses trigram transition and emission probabilities and is shown to achieve good accuracies in English and other languages (Huang et al., 2007). We refer to this as the baseline tagger in the rest of this paper and is used to produce n-best list for each candidate sentence. The n-best list for training data is produced using multifold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). The first block of Table 2 shows the accuracies of the top-ranked tag sequence (according to the Viterbi decoding score) and the oracle accuracies on the 10-best list. As expected the accuracies on English and French are high and are on par with the state-of-the-art systems. From the oracle scores, it is clear that though there is a chance for improvement using reranking, the scope for improvement in English is less compared to the 5 point improvement reported for parsing (Charniak and Johnson, 2005). This indicates the difficulty of the reranking problem for </context>
<context position="26192" citStr="Collins and Koo, 2005" startWordPosition="4665" endWordPosition="4668">ram and bigram tag sequences as features in the tag view. That is, we convert each word of the sentence into suffixes of length two to four and then treat each sentence as a bag of suffixes. Similarly, we treat a candidate POS tag sequence as a bag of unigram and bigram tag features. For Chinese, we use character sequences of length one and two as features for the sentences and use unigram and bigram POS tag sequences on the tag view. We did not include any alignment based features, i.e. features that depend on the position. We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al., 2007). In order to enable a fair comparison, we use suffix and tag pairs as features for both these models. For example, we would generate the following features for the word ‘selling’ in the phrase “the/DT selling/NN pressure/NN”: (ng, NN), (ng, DT NN), (ing,NN), (ing,DT NN), (ling,NN), (ling,DT NN). For comparison purposes, we also show results by running the baseline rerankers with n-gram features. 4.2 Results There are following hyper parameters in each of our models, regularization parameter T, weight parameter A in the discriminative and soften</context>
<context position="29036" citStr="Collins and Koo (2005)" startWordPosition="5126" endWordPosition="5129">39 98.19 99.00 96.48 Collins (Sufx) 96.66 93.00 96.87 93.50 96.06 92.81 97.35 93.44 Regularized (Sufx) 96.60 93.12 96.90 93.36 96.00 92.88 97.38 93.35 Generative 96.82 93.14 96.97 93.46 96.24 92.95 97.43 93.26 Softened-Disc 96.85 93.14 97.04 93.49 96.32 92.87 97.53 93.24 Discriminative 96.85 93.17 97.03 93.50 96.3 92.91 97.53 93.36 Collins (n-gm) 96.74 93.14 97.06 93.44 96.13 92.74 97.54 93.45 Regularized (n-gm) 96.78 93.14 97.01 93.45 96.14 92.80 97.52 93.40 Table 2: Accuracy of the baseline HMM tagger and different reranking approaches. For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features. The improvements of our discriminative models are statistically significant at p = 0.01 and p = 0.05 levels on Chinese and English respectively. information for Chinese and this additional information is being exploited by the reranking approaches. Swedish, on the other hand, is a Germanic language with compound word phenomenon which makes the baseline HMM decoder weaker compared to English and French. The fourth block shows the performance of our models. Except in Swedish, one of our models outperform the baseline decoder and the other reranking</context>
<context position="31366" citStr="Collins and Koo, 2005" startWordPosition="5496" endWordPosition="5499">9 Softened-Disc 95.04 89.61 95.97 91.95 Discriminative 94.95 89.76 95.82 92.11 Table 3: Accuracies without combining with Viterbi decoding score. reader should compare our results with the baseline rerankers run with the suffix features. The performance of these baseline rankers improved when we include the n-gram features but it is still less than the discriminative model in most cases. Finally, Table 3 shows the performance of our models without combining with the Viterbi decoding score. As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). 5 Related Work In this section, we discuss approaches that are most relevant to our problem and the approach. In NLP literature, discriminative reranking has been well explored for parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen and Joshi, 2003; McDonald et al., 2005; Johnson and Ural, 2010) and statistical machine translation (Shen et al., 2004; Watanabe et al., 2007; Liang et al., 2006). Collins (2002) proposed two reranking approaches, namely boosting algorithm and a voted perceptron, for the POS tagging task. Later Huang et al. (2007) propose a regularized version of the</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31:25–70, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking algorithms for namedentity extraction: boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>489--496</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1903" citStr="Collins, 2002" startWordPosition="287" endWordPosition="288">agging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). For the POS tagging, reranking is relative less explored due to the already higher accuracies in English (Collins, 2002), but it is shown to improve accuracies in other languages such as Chinese (Huang et al., 2007). In this paper, we propose a novel approach to discriminative reranking and show its effectiveness in POS tagging. Reranking allows us to use arbitrary features defined jointly on input and output spaces that are often difficult to incorporate into the baseline decoder due to the computational tractability issues. The effectiveness of reranking depends on the joint features defined over both input and output spaces. This has led the community to spend substantial efforts in defining joint features f</context>
<context position="31792" citStr="Collins (2002)" startWordPosition="5566" endWordPosition="5567">ur models without combining with the Viterbi decoding score. As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). 5 Related Work In this section, we discuss approaches that are most relevant to our problem and the approach. In NLP literature, discriminative reranking has been well explored for parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen and Joshi, 2003; McDonald et al., 2005; Johnson and Ural, 2010) and statistical machine translation (Shen et al., 2004; Watanabe et al., 2007; Liang et al., 2006). Collins (2002) proposed two reranking approaches, namely boosting algorithm and a voted perceptron, for the POS tagging task. Later Huang et al. (2007) propose a regularized version of the objective used by Collins (2002) and show an improved performance for Chinese. In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined separately within each view and the algorithm learns the relationship between them automatically. This is the primary difference between our approach and the existing rerankers. In princi</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Ranking algorithms for namedentity extraction: boosting and the voted perceptron. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 489–496, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Renjing Wang</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Rich bitext projection features for parse reranking.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09,</booktitle>
<pages>282--290</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Fraser, Wang, Sch¨utze, 2009</marker>
<rawString>Alexander Fraser, Renjing Wang, and Hinrich Sch¨utze. 2009. Rich bitext projection features for parse reranking. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09, pages 282–290, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martinsson</author>
<author>A Joel Tropp</author>
</authors>
<title>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions.</title>
<date>2009</date>
<tech>Technical report,</tech>
<institution>California Institute of Technology.</institution>
<marker>Martinsson, Tropp, 2009</marker>
<rawString>Nathan Halko, Per-Gunnar. Martinsson, and A. Joel Tropp. 2009. Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions. Technical report, California Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Hardoon</author>
<author>Sandor R Szedmak</author>
<author>John R Shawe-taylor</author>
</authors>
<title>Canonical correlation analysis: An overview with application to learning methods.</title>
<date>2004</date>
<journal>Neural Comput.,</journal>
<pages>16--2639</pages>
<contexts>
<context position="14439" citStr="Hardoon et al., 2004" startWordPosition="2521" endWordPosition="2524">T b = 1 where 0 &lt; A &lt; 1 is the weight parameter to be tuned on the development set. 3 Optimization In this section, we describe how we solve the optimization problems associated with our models. First we discuss the solution of the generative model. Next, we discuss the softened discriminative model, since its solution will be used as a subroutine in our final discussion of the fully discriminative model. 3.1 Optimizing the Generative Model The optimization problem corresponding to the generative model turns out to be identical to that of canonical correlation analysis (CCA) (Hotelling, 1936; Hardoon et al., 2004), which immediately suggests a solution by solving an eigensystem. In particular, the projection directions are obtained by solving the following generalized eigensystem: � 0 Cxy � ~a � �Cxx 0 � ~a~ = (11) Cyx 0 b 0 Cyy b where Cxx = (1 − T)XXT + TI, Cyy = (1 − T)YYT + TI are autocovariance matrices, = XYT is the cross-covariance matrix, = T is a regularization parameter and I is the identity matrix of appropriate size. Using these eigenvectors as columns, we form projection matrices A and B. These projection matrices are used to project sentences and tag sequences into a common lower dimensio</context>
</contexts>
<marker>Hardoon, Szedmak, Shawe-taylor, 2004</marker>
<rawString>David R. Hardoon, Sandor R. Szedmak, and John R. Shawe-taylor. 2004. Canonical correlation analysis: An overview with application to learning methods. Neural Comput., 16:2639–2664, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Hotelling</author>
</authors>
<title>Relation between two sets of variables.</title>
<date>1936</date>
<journal>Biometrica,</journal>
<pages>28--322</pages>
<contexts>
<context position="14416" citStr="Hotelling, 1936" startWordPosition="2519" endWordPosition="2520">T a = 1 and bT YYT b = 1 where 0 &lt; A &lt; 1 is the weight parameter to be tuned on the development set. 3 Optimization In this section, we describe how we solve the optimization problems associated with our models. First we discuss the solution of the generative model. Next, we discuss the softened discriminative model, since its solution will be used as a subroutine in our final discussion of the fully discriminative model. 3.1 Optimizing the Generative Model The optimization problem corresponding to the generative model turns out to be identical to that of canonical correlation analysis (CCA) (Hotelling, 1936; Hardoon et al., 2004), which immediately suggests a solution by solving an eigensystem. In particular, the projection directions are obtained by solving the following generalized eigensystem: � 0 Cxy � ~a � �Cxx 0 � ~a~ = (11) Cyx 0 b 0 Cyy b where Cxx = (1 − T)XXT + TI, Cyy = (1 − T)YYT + TI are autocovariance matrices, = XYT is the cross-covariance matrix, = T is a regularization parameter and I is the identity matrix of appropriate size. Using these eigenvectors as columns, we form projection matrices A and B. These projection matrices are used to project sentences and tag sequences into </context>
</contexts>
<marker>Hotelling, 1936</marker>
<rawString>Harold Hotelling. 1936. Relation between two sets of variables. Biometrica, 28:322–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
<author>Wen Wang</author>
</authors>
<title>Mandarin part-of-speech tagging and discriminative reranking.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1093--1102</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1998" citStr="Huang et al., 2007" startWordPosition="303" endWordPosition="306"> involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). For the POS tagging, reranking is relative less explored due to the already higher accuracies in English (Collins, 2002), but it is shown to improve accuracies in other languages such as Chinese (Huang et al., 2007). In this paper, we propose a novel approach to discriminative reranking and show its effectiveness in POS tagging. Reranking allows us to use arbitrary features defined jointly on input and output spaces that are often difficult to incorporate into the baseline decoder due to the computational tractability issues. The effectiveness of reranking depends on the joint features defined over both input and output spaces. This has led the community to spend substantial efforts in defining joint features for reranking (Fraser et al., 2009; Chiang et al., 2009). Unfortunately, developing joint featur</context>
<context position="24545" citStr="Huang et al., 2007" startWordPosition="4380" endWordPosition="4383">cs. Swedish. The data in all these languages is obtained from the CoNLL 2006 shared task on multilingual dependency parsing (Buchholz and Marsi, 2006). We only consider the word and its fine grained POS tag (columns 2 and 5 respectively) and ignore the dependency links in the data. Table 1 shows the data statistics in each of these languages. We use a second order Hidden Markov Model (Thede and Harper, 1999) based tagger as a baseline tagger in our experiments. This model uses trigram transition and emission probabilities and is shown to achieve good accuracies in English and other languages (Huang et al., 2007). We refer to this as the baseline tagger in the rest of this paper and is used to produce n-best list for each candidate sentence. The n-best list for training data is produced using multifold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). The first block of Table 2 shows the accuracies of the top-ranked tag sequence (according to the Viterbi decoding score) and the oracle accuracies on the 10-best list. As expected the accuracies on English and French are high and are on par with the state-of-the-art systems. From the oracle scores, it is clear that though ther</context>
<context position="26241" citStr="Huang et al., 2007" startWordPosition="4673" endWordPosition="4676">view. That is, we convert each word of the sentence into suffixes of length two to four and then treat each sentence as a bag of suffixes. Similarly, we treat a candidate POS tag sequence as a bag of unigram and bigram tag features. For Chinese, we use character sequences of length one and two as features for the sentences and use unigram and bigram POS tag sequences on the tag view. We did not include any alignment based features, i.e. features that depend on the position. We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al., 2007). In order to enable a fair comparison, we use suffix and tag pairs as features for both these models. For example, we would generate the following features for the word ‘selling’ in the phrase “the/DT selling/NN pressure/NN”: (ng, NN), (ng, DT NN), (ing,NN), (ing,DT NN), (ling,NN), (ling,DT NN). For comparison purposes, we also show results by running the baseline rerankers with n-gram features. 4.2 Results There are following hyper parameters in each of our models, regularization parameter T, weight parameter A in the discriminative and softened discriminative models, the linear combination </context>
<context position="31929" citStr="Huang et al. (2007)" startWordPosition="5585" endWordPosition="5588"> the behavior observed elsewhere (Collins and Koo, 2005). 5 Related Work In this section, we discuss approaches that are most relevant to our problem and the approach. In NLP literature, discriminative reranking has been well explored for parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen and Joshi, 2003; McDonald et al., 2005; Johnson and Ural, 2010) and statistical machine translation (Shen et al., 2004; Watanabe et al., 2007; Liang et al., 2006). Collins (2002) proposed two reranking approaches, namely boosting algorithm and a voted perceptron, for the POS tagging task. Later Huang et al. (2007) propose a regularized version of the objective used by Collins (2002) and show an improved performance for Chinese. In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined separately within each view and the algorithm learns the relationship between them automatically. This is the primary difference between our approach and the existing rerankers. In principle, our margin formulations are similar to the max margin formulations of CCA (Szedmak et al., 2007) and maximum margin regression (Szed</context>
</contexts>
<marker>Huang, Harper, Wang, 2007</marker>
<rawString>Zhongqiang Huang, Mary Harper, and Wen Wang. 2007. Mandarin part-of-speech tagging and discriminative reranking. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1093–1102, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jagadeesh Jagarlamudi</author>
<author>Raghavendra Udupa</author>
<author>Hal Daum´e</author>
<author>Abhijit Bhole</author>
</authors>
<title>Improving bilingual projections via sparse covariance matrices.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>930--940</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<marker>Jagarlamudi, Udupa, Daum´e, Bhole, 2011</marker>
<rawString>Jagadeesh Jagarlamudi, Raghavendra Udupa, Hal Daum´e III, and Abhijit Bhole. 2011. Improving bilingual projections via sparse covariance matrices. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 930–940, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Ahmet Engin Ural</author>
</authors>
<title>Reranking the Berkeley and Brown parsers.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>665--668</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="31677" citStr="Johnson and Ural, 2010" startWordPosition="5546" endWordPosition="5549">-gram features but it is still less than the discriminative model in most cases. Finally, Table 3 shows the performance of our models without combining with the Viterbi decoding score. As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). 5 Related Work In this section, we discuss approaches that are most relevant to our problem and the approach. In NLP literature, discriminative reranking has been well explored for parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen and Joshi, 2003; McDonald et al., 2005; Johnson and Ural, 2010) and statistical machine translation (Shen et al., 2004; Watanabe et al., 2007; Liang et al., 2006). Collins (2002) proposed two reranking approaches, namely boosting algorithm and a voted perceptron, for the POS tagging task. Later Huang et al. (2007) propose a regularized version of the objective used by Collins (2002) and show an improved performance for Chinese. In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined separately within each view and the algorithm learns the relationship be</context>
</contexts>
<marker>Johnson, Ural, 2010</marker>
<rawString>Mark Johnson and Ahmet Engin Ural. 2010. Reranking the Berkeley and Brown parsers. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 665–668, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Kubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
<author>Graeme Hirst</author>
</authors>
<title>Dependency Parsing.</title>
<date>2009</date>
<publisher>Morgan and Claypool Publishers.</publisher>
<contexts>
<context position="1449" citStr="Kubler et al., 2009" startWordPosition="210" endWordPosition="213">e engineering can be done separately on the input and output spaces; the relationship between inputs and outputs is learned automatically. Experiments on part-of-speech tagging task in four languages show significant improvements over a baseline decoder and existing reranking approaches. 1 Introduction Mapping inputs to outputs lies at the heart of many Natural Language Processing applications. For example, given a sentence as input: part-of-speech (POS) tagging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). For the POS tagging, reranking is relative less explored due to the already higher accuracies in English (Collins, 2002), but it is shown to improve accuracies in other languages such as Chinese (Huang et al., 2007). In this paper, we propose a novel approach to dis</context>
</contexts>
<marker>Kubler, McDonald, Nivre, Hirst, 2009</marker>
<rawString>Sandra Kubler, Ryan McDonald, Joakim Nivre, and Graeme Hirst. 2009. Dependency Parsing. Morgan and Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>761--768</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 761–768, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>91--98</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="31652" citStr="McDonald et al., 2005" startWordPosition="5542" endWordPosition="5545">d when we include the n-gram features but it is still less than the discriminative model in most cases. Finally, Table 3 shows the performance of our models without combining with the Viterbi decoding score. As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). 5 Related Work In this section, we discuss approaches that are most relevant to our problem and the approach. In NLP literature, discriminative reranking has been well explored for parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen and Joshi, 2003; McDonald et al., 2005; Johnson and Ural, 2010) and statistical machine translation (Shen et al., 2004; Watanabe et al., 2007; Liang et al., 2006). Collins (2002) proposed two reranking approaches, namely boosting algorithm and a voted perceptron, for the POS tagging task. Later Huang et al. (2007) propose a regularized version of the objective used by Collins (2002) and show an improved performance for Chinese. In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined separately within each view and the algorithm l</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 91–98, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>An SVM based voting algorithm with application to parse reranking.</title>
<date>2003</date>
<booktitle>In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003 - Volume 4, CONLL ’03,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="31629" citStr="Shen and Joshi, 2003" startWordPosition="5538" endWordPosition="5541">seline rankers improved when we include the n-gram features but it is still less than the discriminative model in most cases. Finally, Table 3 shows the performance of our models without combining with the Viterbi decoding score. As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). 5 Related Work In this section, we discuss approaches that are most relevant to our problem and the approach. In NLP literature, discriminative reranking has been well explored for parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen and Joshi, 2003; McDonald et al., 2005; Johnson and Ural, 2010) and statistical machine translation (Shen et al., 2004; Watanabe et al., 2007; Liang et al., 2006). Collins (2002) proposed two reranking approaches, namely boosting algorithm and a voted perceptron, for the POS tagging task. Later Huang et al. (2007) propose a regularized version of the objective used by Collins (2002) and show an improved performance for Chinese. In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined separately within each v</context>
</contexts>
<marker>Shen, Joshi, 2003</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2003. An SVM based voting algorithm with application to parse reranking. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003 - Volume 4, CONLL ’03, pages 9–16, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Franz Och</author>
</authors>
<title>Discriminative reranking for machine translation.</title>
<date>2004</date>
<booktitle>In Human Language Technology Conference and the 5th Meeting of the North American Association for Computational Linguistics: HLT-NAACL 2004,</booktitle>
<location>Boston, USA,</location>
<contexts>
<context position="1757" citStr="Shen et al., 2004" startWordPosition="260" endWordPosition="263"> inputs to outputs lies at the heart of many Natural Language Processing applications. For example, given a sentence as input: part-of-speech (POS) tagging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). For the POS tagging, reranking is relative less explored due to the already higher accuracies in English (Collins, 2002), but it is shown to improve accuracies in other languages such as Chinese (Huang et al., 2007). In this paper, we propose a novel approach to discriminative reranking and show its effectiveness in POS tagging. Reranking allows us to use arbitrary features defined jointly on input and output spaces that are often difficult to incorporate into the baseline decoder due to the computational tractability issues. The effectiveness of reranking depends on </context>
<context position="31732" citStr="Shen et al., 2004" startWordPosition="5554" endWordPosition="5557">odel in most cases. Finally, Table 3 shows the performance of our models without combining with the Viterbi decoding score. As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). 5 Related Work In this section, we discuss approaches that are most relevant to our problem and the approach. In NLP literature, discriminative reranking has been well explored for parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen and Joshi, 2003; McDonald et al., 2005; Johnson and Ural, 2010) and statistical machine translation (Shen et al., 2004; Watanabe et al., 2007; Liang et al., 2006). Collins (2002) proposed two reranking approaches, namely boosting algorithm and a voted perceptron, for the POS tagging task. Later Huang et al. (2007) propose a regularized version of the objective used by Collins (2002) and show an improved performance for Chinese. In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined separately within each view and the algorithm learns the relationship between them automatically. This is the primary differenc</context>
</contexts>
<marker>Shen, Sarkar, Och, 2004</marker>
<rawString>Libin Shen, Anoop Sarkar, and Franz Och. 2004. Discriminative reranking for machine translation. In Human Language Technology Conference and the 5th Meeting of the North American Association for Computational Linguistics: HLT-NAACL 2004, Boston, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Szedmak</author>
<author>J Shawe-Taylor</author>
<author>E Parado-Hernandez</author>
</authors>
<title>Learning via linear operators: Maximum margin regression; multiclass and multiview learning at one-class complexity.</title>
<date>2006</date>
<tech>Technical report,</tech>
<institution>University of Southampton.</institution>
<contexts>
<context position="32545" citStr="Szedmak et al., 2006" startWordPosition="5681" endWordPosition="5685">007) propose a regularized version of the objective used by Collins (2002) and show an improved performance for Chinese. In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined separately within each view and the algorithm learns the relationship between them automatically. This is the primary difference between our approach and the existing rerankers. In principle, our margin formulations are similar to the max margin formulations of CCA (Szedmak et al., 2007) and maximum margin regression (Szedmak et al., 2006; Wang et al., 2007). These approaches solve the following optimization problem: min IIW II2 + C1T� (16) s.t. (yi, Wφ(x)i) &gt; 1 − �i Vi = 1··· n Our approach differs from these formulations in two main ways: the score assigned by our generative model (equivalent to CCA) for an input-output pair (xTi abTyi) can be converted into this format by substituting W +— baT but in doing so we are ignoring the rank constraint. It is often observed that, dimensionality reduction leads to an improved performance and thus the rank constraint becomes crucial. Another major difference is that, the constraints </context>
</contexts>
<marker>Szedmak, Shawe-Taylor, Parado-Hernandez, 2006</marker>
<rawString>S. Szedmak, J. Shawe-Taylor, and E. Parado-Hernandez. 2006. Learning via linear operators: Maximum margin regression; multiclass and multiview learning at one-class complexity. Technical report, University of Southampton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandor Szedmak</author>
<author>Tijl De Bie</author>
<author>David R Hardoon</author>
</authors>
<title>A metamorphosis of canonical correlation analysis into multivariate maximum margin learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the fifteenth European Symposium on Artificial Neural Networks.</booktitle>
<marker>Szedmak, De Bie, Hardoon, 2007</marker>
<rawString>Sandor Szedmak, Tijl De Bie, and David R. Hardoon. 2007. A metamorphosis of canonical correlation analysis into multivariate maximum margin learning. In Proceedings of the fifteenth European Symposium on Artificial Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guestrin</author>
<author>Daphne Koller</author>
</authors>
<title>Max margin markov networks.</title>
<date>2004</date>
<booktitle>In Proceedings of NIPS 16.</booktitle>
<marker>Guestrin, Koller, 2004</marker>
<rawString>Ben Taskar, Carlos. Guestrin, and Daphne Koller. 2004. Max margin markov networks. In Proceedings of NIPS 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott M Thede</author>
<author>Mary P Harper</author>
</authors>
<title>A secondorder Hidden Markov Model for part-of-speech tagging.</title>
<date>1999</date>
<booktitle>In Proceedings of the Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>175--182</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1370" citStr="Thede and Harper, 1999" startWordPosition="198" endWordPosition="201"> be done computationally efficiently. A key quality of our approach is that feature engineering can be done separately on the input and output spaces; the relationship between inputs and outputs is learned automatically. Experiments on part-of-speech tagging task in four languages show significant improvements over a baseline decoder and existing reranking approaches. 1 Introduction Mapping inputs to outputs lies at the heart of many Natural Language Processing applications. For example, given a sentence as input: part-of-speech (POS) tagging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). For the POS tagging, reranking is relative less explored due to the already higher accuracies in English (Collins, 2002), but it is shown to improve accuracies in other languages such as </context>
<context position="24337" citStr="Thede and Harper, 1999" startWordPosition="4346" endWordPosition="4349">K 47K 43K Chinese (Zh.) # sent. 50K 4K 3647 # words 292K 26K 25K # sent. 9K 2K 1351 French (Fr.) # words 254K 57K 40K Swedish (Sv.) # sent. 8K 2K 1431 # words 137K 31K 28K Table 1: Training and test data statistics. Swedish. The data in all these languages is obtained from the CoNLL 2006 shared task on multilingual dependency parsing (Buchholz and Marsi, 2006). We only consider the word and its fine grained POS tag (columns 2 and 5 respectively) and ignore the dependency links in the data. Table 1 shows the data statistics in each of these languages. We use a second order Hidden Markov Model (Thede and Harper, 1999) based tagger as a baseline tagger in our experiments. This model uses trigram transition and emission probabilities and is shown to achieve good accuracies in English and other languages (Huang et al., 2007). We refer to this as the baseline tagger in the rest of this paper and is used to produce n-best list for each candidate sentence. The n-best list for training data is produced using multifold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). The first block of Table 2 shows the accuracies of the top-ranked tag sequence (according to the Viterbi decoding score)</context>
</contexts>
<marker>Thede, Harper, 1999</marker>
<rawString>Scott M. Thede and Mary P. Harper. 1999. A secondorder Hidden Markov Model for part-of-speech tagging. In Proceedings of the Annual Meeting on Association for Computational Linguistics, pages 175–182. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the twenty-first international conference on Machine learning, ICML ’04,</booktitle>
<pages>104</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11343" citStr="Tsochantaridis et al., 2004" startWordPosition="1932" endWordPosition="1935"> in Eq. 5, for a given pair of source sentence xi and a tag sequence yj, the generative model assigns a score of : (a, xi)(b, yj) = aTxiyTj b Each input sentence is also associated with a list of candidate tag sequences and since each of these candidate sequences are incorrect they should be assigned a score less than that of the reference tag sequence. Drawing ideas from structure prediction literature (Bakir et al., 2007), we modify the objective function in order to include these terms. This idea can be captured using a loss augmented margin constraint for each sentence, tag sequence pair (Tsochantaridis et al., 2004). Let ξi denote a nonnegative slack variable, then we define our new optimization problem as: arg max 1 − λaTXY Tb − a,b,ξ&gt;0 λ i s.t. aT XXT a = 1 and bT YYT b = 1 bi bj aTxiyTi b − aTxiyT ijb &gt; 1 − ξi Lij arg max a,b aTXYTb = Xn (xi, a)(yi, b) i=1 = Xn � Xd1 xlial · Xd2 i=1 l=1 m=1 = Xn � Xd1 Xd2 i=1 l=1 m=1 ~ X l,m=1 wlmφim/ (5) = Xn i=1 d1,d2 ξi (6) 701 � LijrT b (8) ij where 0 &lt; A &lt; 1 is a weight parameter. This objective function is ensuring that the margin between the reference and the candidate tag sequences in the projected space (as given by aTxiyTi b − aTxiyTijb) is proportional to i</context>
<context position="20346" citStr="Tsochantaridis et al., 2004" startWordPosition="3635" endWordPosition="3638">straints of a given input sentence are satisfied by the current projection directions and hence there is no need to update the Lagrangian multipliers. Otherwise, some of the constraints are still not satisfied and hence we will update their corresponding Lagrangian multipliers in steps 13 and 14. In specific, step 13 computes the deviation of the margin constraints with the new slack value and step 14 updates the Lagrangian multipliers along the gradient direction. In principle, our approach is similar to the cutting plane algorithm used to optimize slack re-scaling version of Structured SVM (Tsochantaridis et al., 2004), but it differs in selecting the slack variable (step 11). The cutting plane method chooses ξi as the maximum of 10, ψij} where as we choose the minimum of the positive ψij values as the slack. Intuitively, this means that the cutting plane algorithm chooses a constraint that is most violated which results in fewer constraints. This is crucial in structured SVM, because solving the dual problem is cubic in terms of the number of examples and constraints. In contrast, our approach selects the slack such that at least one of the constraints is satisfied and adds all the remaining constraints to</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the twenty-first international conference on Machine learning, ICML ’04, pages 104–, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuoran Wang</author>
<author>John Shawe-Taylor</author>
<author>Sandor Szedmak</author>
</authors>
<title>Kernel regression based machine translation.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, NAACL-Short ’07,</booktitle>
<pages>185--188</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="32565" citStr="Wang et al., 2007" startWordPosition="5686" endWordPosition="5689">ized version of the objective used by Collins (2002) and show an improved performance for Chinese. In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined separately within each view and the algorithm learns the relationship between them automatically. This is the primary difference between our approach and the existing rerankers. In principle, our margin formulations are similar to the max margin formulations of CCA (Szedmak et al., 2007) and maximum margin regression (Szedmak et al., 2006; Wang et al., 2007). These approaches solve the following optimization problem: min IIW II2 + C1T� (16) s.t. (yi, Wφ(x)i) &gt; 1 − �i Vi = 1··· n Our approach differs from these formulations in two main ways: the score assigned by our generative model (equivalent to CCA) for an input-output pair (xTi abTyi) can be converted into this format by substituting W +— baT but in doing so we are ignoring the rank constraint. It is often observed that, dimensionality reduction leads to an improved performance and thus the rank constraint becomes crucial. Another major difference is that, the constraints in Eq. 16 represent </context>
</contexts>
<marker>Wang, Shawe-Taylor, Szedmak, 2007</marker>
<rawString>Zhuoran Wang, John Shawe-Taylor, and Sandor Szedmak. 2007. Kernel regression based machine translation. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, NAACL-Short ’07, pages 185– 188, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online Large-Margin Training for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>764--773</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1781" citStr="Watanabe et al., 2007" startWordPosition="264" endWordPosition="267">lies at the heart of many Natural Language Processing applications. For example, given a sentence as input: part-of-speech (POS) tagging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). For the POS tagging, reranking is relative less explored due to the already higher accuracies in English (Collins, 2002), but it is shown to improve accuracies in other languages such as Chinese (Huang et al., 2007). In this paper, we propose a novel approach to discriminative reranking and show its effectiveness in POS tagging. Reranking allows us to use arbitrary features defined jointly on input and output spaces that are often difficult to incorporate into the baseline decoder due to the computational tractability issues. The effectiveness of reranking depends on the joint features defin</context>
<context position="31755" citStr="Watanabe et al., 2007" startWordPosition="5558" endWordPosition="5561"> Finally, Table 3 shows the performance of our models without combining with the Viterbi decoding score. As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). 5 Related Work In this section, we discuss approaches that are most relevant to our problem and the approach. In NLP literature, discriminative reranking has been well explored for parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen and Joshi, 2003; McDonald et al., 2005; Johnson and Ural, 2010) and statistical machine translation (Shen et al., 2004; Watanabe et al., 2007; Liang et al., 2006). Collins (2002) proposed two reranking approaches, namely boosting algorithm and a voted perceptron, for the POS tagging task. Later Huang et al. (2007) propose a regularized version of the objective used by Collins (2002) and show an improved performance for Chinese. In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined separately within each view and the algorithm learns the relationship between them automatically. This is the primary difference between our approach </context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online Large-Margin Training for Statistical Machine Translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 764– 773, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>