<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000047">
<title confidence="0.989956">
Cross-Lingual Mixture Model for Sentiment Classification
</title>
<author confidence="0.992246">
Xinfan Mengt * Furu Weit Xiaohua Liut Ming Zhout Ge Xut Houfeng Wangt
</author>
<affiliation confidence="0.971084">
WOE Key Lab of Computational Linguistics, Peking University
tMicrosoft Research Asia
</affiliation>
<email confidence="0.982499">
t{mxf, xuge, wanghf}@pku.edu.cn
t{fuwei,xiaoliu,mingzhou}@microsoft.com
</email>
<sectionHeader confidence="0.998578" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99621904">
The amount of labeled sentiment data in En-
glish is much larger than that in other lan-
guages. Such a disproportion arouse interest
in cross-lingual sentiment classification, which
aims to conduct sentiment classification in the
target language (e.g. Chinese) using labeled
data in the source language (e.g. English).
Most existing work relies on machine trans-
lation engines to directly adapt labeled data
from the source language to the target lan-
guage. This approach suffers from the limited
coverage of vocabulary in the machine transla-
tion results. In this paper, we propose a gen-
erative cross-lingual mixture model (CLMM)
to leverage unlabeled bilingual parallel data.
By fitting parameters to maximize the likeli-
hood of the bilingual parallel data, the pro-
posed model learns previously unseen senti-
ment words from the large bilingual parallel
data and improves vocabulary coverage signifi-
cantly. Experiments on multiple data sets show
that CLMM is consistently effective in two set-
tings: (1) labeled data in the target language are
unavailable; and (2) labeled data in the target
language are also available.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99466287804878">
Sentiment Analysis (also known as opinion min-
ing), which aims to extract the sentiment informa-
tion from text, has attracted extensive attention in
recent years. Sentiment classification, the task of
determining the sentiment orientation (positive, neg-
ative or neutral) of text, has been the most exten-
sively studied task in sentiment analysis. There is
*Contribution during internship at Microsoft Research Asia.
already a large amount of work on sentiment classi-
fication of text in various genres and in many lan-
guages. For example, Pang et al. (2002) focus on
sentiment classification of movie reviews in English,
and Zagibalov and Carroll (2008) study the problem
of classifying product reviews in Chinese. During
the past few years, NTCIR1 organized several pi-
lot tasks for sentiment classification of news articles
written in English, Chinese and Japanese (Seki et
al., 2007; Seki et al., 2008).
For English sentiment classification, there are sev-
eral labeled corpora available (Hu and Liu, 2004;
Pang et al., 2002; Wiebe et al., 2005). However, la-
beled resources in other languages are often insuf-
ficient or even unavailable. Therefore, it is desir-
able to use the English labeled data to improve senti-
ment classification of documents in other languages.
One direct approach to leveraging the labeled data
in English is to use machine translation engines as a
black box to translate the labeled data from English
to the target language (e.g. Chinese), and then us-
ing the translated training data directly for the devel-
opment of the sentiment classifier in the target lan-
guage (Wan, 2009; Pan et al., 2011).
Although the machine-translation-based methods
are intuitive, they have certain limitations. First,
the vocabulary covered by the translated labeled
data is limited, hence many sentiment indicative
words can not be learned from the translated labeled
data. Duh et al. (2011) report low overlapping
between vocabulary of natural English documents
and the vocabulary of documents translated to En-
glish from Japanese, and the experiments of Duh
</bodyText>
<footnote confidence="0.980121">
1http://research.nii.ac.jp/ntcir/index-en.html
</footnote>
<page confidence="0.804905">
572
</page>
<note confidence="0.986721">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 572–581,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99998088">
et al. (2011) show that vocabulary coverage has a
strong correlation with sentiment classification ac-
curacy. Second, machine translation may change the
sentiment polarity of the original text. For exam-
ple, the negative English sentence “It is too good to
be true” is translated to a positive sentence in Chi-
nese “I;%H4%;%JA3�-W by Google Translate
(http://translate.google.com/), which literally means
“It is good and true”.
In this paper we propose a cross-lingual mixture
model (CLMM) for cross-lingual sentiment classifi-
cation. Instead of relying on the unreliable machine
translated labeled data, CLMM leverages bilingual
parallel data to bridge the language gap between the
source language and the target language. CLMM is
a generative model that treats the source language
and target language words in parallel data as gener-
ated simultaneously by a set of mixture components.
By “synchronizing” the generation of words in the
source language and the target language in a parallel
corpus, the proposed model can (1) improve vocabu-
lary coverage by learning sentiment words from the
unlabeled parallel corpus; (2) transfer polarity label
information between the source language and target
language using a parallel corpus. Besides, CLMM
can improve the accuracy of cross-lingual sentiment
classification consistently regardless of whether la-
beled data in the target language are present or not.
We evaluate the model on sentiment classification
of Chinese using English labeled data. The exper-
iment results show that CLMM yields 71% in accu-
racy when no Chinese labeled data are used, which
significantly improves Chinese sentiment classifica-
tion and is superior to the SVM and co-training based
methods. When Chinese labeled data are employed,
CLMM yields 83% in accuracy, which is remarkably
better than the SVM and achieve state-of-the-art per-
formance.
This paper makes two contributions: (1) we pro-
pose a model to effectively leverage large bilin-
gual parallel data for improving vocabulary cover-
age; and (2) the proposed model is applicable in both
settings of cross-lingual sentiment classification, ir-
respective of the availability of labeled data in the
target language.
The paper is organized as follows. We review re-
lated work in Section 2, and present the cross-lingual
mixture model in Section 3. Then we present the ex-
perimental studies in Section 4, and finally conclude
the paper and outline the future plan in Section 5.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999913">
In this section, we present a brief review of the re-
lated work on monolingual sentiment classification
and cross-lingual sentiment classification.
</bodyText>
<subsectionHeader confidence="0.996307">
2.1 Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.999953058823529">
Early work of sentiment classification focuses on
English product reviews or movie reviews (Pang et
al., 2002; Turney, 2002; Hu and Liu, 2004). Since
then, sentiment classification has been investigated
in various domains and different languages (Zag-
ibalov and Carroll, 2008; Seki et al., 2007; Seki et
al., 2008; Davidov et al., 2010). There exist two
main approaches to extracting sentiment orientation
automatically. The Dictionary-based approach (Tur-
ney, 2002; Taboada et al., 2011) aims to aggregate
the sentiment orientation of a sentence (or docu-
ment) from the sentiment orientations of words or
phrases found in the sentence (or document), while
the corpus-based approach (Pang et al., 2002) treats
the sentiment orientation detection as a conventional
classification task and focuses on building classifier
from a set of sentences (or documents) labeled with
sentiment orientations.
Dictionary-based methods involve in creating or
using sentiment lexicons. Turney (2002) derives
sentiment scores for phrases by measuring the mu-
tual information between the given phrase and the
words “excellent” and “poor”, and then uses the av-
erage scores of the phrases in a document as the
sentiment of the document. Corpus-based meth-
ods are often built upon machine learning mod-
els. Pang et al. (2002) compare the performance
of three commonly used machine learning models
(Naive Bayes, Maximum Entropy and SVM). Ga-
mon (2004) shows that introducing deeper linguistic
features into SVM can help to improve the perfor-
mance. The interested readers are referred to (Pang
and Lee, 2008) for a comprehensive review of senti-
ment classification.
</bodyText>
<subsectionHeader confidence="0.99942">
2.2 Cross-Lingual Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.997006666666667">
Cross-lingual sentiment classification, which aims
to conduct sentiment classification in the target lan-
guage (e.g. Chinese) with labeled data in the source
</bodyText>
<page confidence="0.997557">
573
</page>
<bodyText confidence="0.999705770833333">
language (e.g. English), has been extensively stud-
ied in the very recent years. The basic idea is to ex-
plore the abundant labeled sentiment data in source
language to alleviate the shortage of labeled data in
the target language.
Most existing work relies on machine translation
engines to directly adapt labeled data from the source
language to target language. Wan (2009) proposes
to use ensemble method to train better Chinese sen-
timent classification model on English labeled data
and their Chinese translation. English Labeled data
are first translated to Chinese, and then two SVM
classifiers are trained on English and Chinese labeled
data respectively. After that, co-training (Blum and
Mitchell, 1998) approach is adopted to leverage Chi-
nese unlabeled data and their English translation to
improve the SVM classifier for Chinese sentiment
classification. The same idea is used in (Wan, 2008),
but the ensemble techniques used are various vot-
ing methods and the individual classifiers used are
dictionary-based classifiers.
Instead of ensemble methods, Pan et al. (2011) use
matrix factorization formulation. They extend Non-
negative Matrix Tri-Factorization model (Li et al.,
2009) to bilingual view setting. Their bilingual view
is also constructed by using machine translation en-
gines to translate original documents. Prettenhofer
and Stein (2011) use machine translation engines in
a different way. They generalize Structural Corre-
spondence Learning (Blitzer et al., 2006) to multi-
lingual setting. Instead of using machine translation
engines to translate labeled text, the authors use it to
construct the word translation oracle for pivot words
translation.
Lu et al. (2011) focus on the task of jointly im-
proving the performance of sentiment classification
on two languages (e.g. English and Chinese) . the
authors use an unlabeled parallel corpus instead of
machine translation engines. They assume paral-
lel sentences in the corpus should have the same
sentiment polarity. Besides, they assume labeled
data in both language are available. They propose
a method of training two classifiers based on maxi-
mum entropy formulation to maximize their predic-
tion agreement on the parallel corpus. However, this
method requires labeled data in both the source lan-
guage and the target language, which are not always
readily available.
</bodyText>
<sectionHeader confidence="0.98575" genericHeader="method">
3 Cross-Lingual Mixture Model for
</sectionHeader>
<subsectionHeader confidence="0.883721">
Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.999464">
In this section we present the cross-lingual mix-
ture model (CLMM) for sentiment classification.
We first formalize the task of cross-lingual sentiment
classification. Then we describe the CLMM model
and present the parameter estimation algorithm for
CLMM.
</bodyText>
<subsectionHeader confidence="0.999647">
3.1 Cross-lingual Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.999994636363636">
Formally, the task we are concerned about is to de-
velop a sentiment classifier for the target language T
(e.g. Chinese), given labeled sentiment data DS in
the source language S (e.g. English), unlabeled par-
allel corpus U of the source language and the target
language, and optional labeled data DT in target lan-
guage T. Aligning with previous work (Wan, 2008;
Wan, 2009), we only consider binary sentiment clas-
sification scheme (positive or negative) in this paper,
but the proposed method can be used in other classi-
fication schemes with minor modifications.
</bodyText>
<subsectionHeader confidence="0.999363">
3.2 The Cross-Lingual Mixture Model
</subsectionHeader>
<bodyText confidence="0.999995375">
The basic idea underlying CLMM is to enlarge
the vocabulary by learning sentiment words from the
parallel corpus. CLMM defines an intuitive genera-
tion process as follows. Suppose we are going to
generate a positive or negative Chinese sentence, we
have two ways of generating words. The first way
is to directly generate a Chinese word according to
the polarity of the sentence. The other way is to first
generate an English word with the same polarity and
meaning, and then translate it to a Chinese word.
More formally, CLMM defines a generative mix-
ture model for generating a parallel corpus. The un-
observed polarities of the unlabeled parallel corpus
are modeled as hidden variables, and the observed
words in parallel corpus are modeled as generated by
a set of words generation distributions conditioned
on the hidden variables. Given a parallel corpus, we
fit CLMM model by maximizing the likelihood of
generating this parallel corpus. By maximizing the
likelihood, CLMM can estimate words generation
probabilities for words unseen in the labeled data but
present in the parallel corpus, hence expand the vo-
cabulary. In addition, CLMM can utilize words in
both the source language and target language for de-
</bodyText>
<page confidence="0.990002">
574
</page>
<bodyText confidence="0.625094">
termining polarity classes of the parallel sentences.
</bodyText>
<figureCaption confidence="0.9994515">
Figure 1: The generation process of the
cross-lingual mixture model
</figureCaption>
<bodyText confidence="0.963411333333333">
Figure 1 illustrates the detailed process of gener-
ating words in the source language and target lan-
guage respectively for the parallel corpus U, from
the four mixture components in CLMM. Particu-
larly, for each pair of parallel sentences ui ∈ U, we
generate the words as follows.
</bodyText>
<listItem confidence="0.996067611111111">
1. Document class generation: Generating the
polarity class.
(a) Generating a polarity class cs from a
Bernoulli distribution Ps(C).
(b) Generating a polarity class ct from a
Bernoulli distribution Pt(C)
2. Words generation: Generating the words
(a) Generating source language words ws from
a Multinomial distribution P(ws|cs)
(b) Generating target language words wt from
a Multinomial distribution P(wt|ct)
3. Words projection: Projecting the words onto
the other language
(a) Projecting the source language words ws to
target language words wt by word projec-
tion probability P (wt|ws)
(b) Projecting the target language words wt to
source language words ws by word projec-
</listItem>
<subsubsectionHeader confidence="0.624611">
tion probability P(ws|wt)
</subsubsectionHeader>
<bodyText confidence="0.987752076923077">
CLMM finds parameters by using MLE (Maxi-
mum Likelihood Estimation). The parameters to be
estimated include conditional probabilities of word
to class, P(ws|c) and P(wt|c), and word projection
probabilities, P(ws|wt) and P (wt|ws). We will de-
scribe the log-likelihood function and then show how
to estimate the parameters in subsection 3.3. The
obtained word-class conditional probability P(wt|c)
can then be used to classify text in the target lan-
guages using Bayes Theorem and the Naive Bayes
independence assumption.
Formally, we have the following log-likelihood
function for a parallel corpus U2.
</bodyText>
<equation confidence="0.9996795">
L(θ|U) =
(1)
</equation>
<bodyText confidence="0.999943">
where θ is the model parameters; Nsi (Nti) is the oc-
currences of the word ws (wt) in document di; |Ds |is
the number of documents; |C |is the number of class
labels; Vs and Vt are the vocabulary in the source lan-
guage and the vocabulary in the target language.|Us|
and |Ut |are the number of unlabeled sentences in the
source language and target language.
Meanwhile, we have the following log-likelihood
function for labeled data in the source language Ds.
</bodyText>
<equation confidence="0.645156">
Nsi logP(ws|cj)δij (2)
</equation>
<bodyText confidence="0.99960025">
where δij = 1 if the label of di is cj, and 0 otherwise.
In addition, when labeled data in the target lan-
guage is available, we have the following log-
likelihood function.
</bodyText>
<equation confidence="0.779005">
Nti logP(wt|cj)δij (3)
</equation>
<bodyText confidence="0.999328">
Combining the above three likelihood functions
together, we have the following likelihood function.
</bodyText>
<equation confidence="0.9894765">
L(θ|Dt, Ds, U) = L(θ|U) + L(θ|Ds) + L(θ|Dt)
(4)
</equation>
<bodyText confidence="0.7944135">
Note that the third term on the right hand side
(L(θ|Dt)) is optional.
2For simplicity, we assume the prior distribution P(C) is
uniform and drop it from the formulas.
</bodyText>
<figure confidence="0.865926818181818">
Source
Target
NEG
NEG
POS
POS
ws
wt
u
U
[N9i log (P(w9|cj) + P(w9|wt)P(wt|cj))]
[Nti log (P(wt|cj) + P(wt|w9)P(w9|cj))]
E |U-|
i=1
E |V.|
9=1
|C|
E
j=1
+
|Vt|
E
</figure>
<equation confidence="0.956287043478261">
t=1
E |Ut|
i=1
|C|
E
j=1
E |D3|
i=1
L(θ|Ds) =
|V-|
E
s=1
|C|
E
j=1
E |Dt|
i=1
L(θ|Dt) =
E |Vt|
t=1
|C|
E
j=1
</equation>
<page confidence="0.993226">
575
</page>
<subsectionHeader confidence="0.987626">
3.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.99621452173913">
Instead of estimating word projection probability
(P(ws|wt) and P(wt|ws)) and conditional proba-
bility of word to class (P(wt|c) and P(ws|c)) si-
multaneously in the training procedure, we estimate
them separately since the word projection probabil-
ity stays invariant when estimating other parame-
ters. We estimate word projection probability using
word alignment probability generated by the Berke-
ley aligner (Liang et al., 2006). The word align-
ment probabilities serves two purposes. First, they
connect the corresponding words between the source
language and the target language. Second, they ad-
just the strength of influences between the corre-
sponding words. Figure 2 gives an example of word
alignment probability. As is shown, the three words
“tour de force” altogether express a positive mean-
ing, while in Chinese the same meaning is expressed
with only one word “杰作” (masterpiece). CLMM
use word alignment probability to decrease the in-
fluences from “杰作” (masterpiece) to “tour”, “de”
and “force” individually, using the word projection
probability (i.e. word alignment probability), which
is 0.3 in this case.
</bodyText>
<figure confidence="0.589858">
Herman Melville&apos;s Moby Dick was a tour de force.
赫尔曼 梅尔维尔 的 “白鲸记” 是 一篇 杰作。
</figure>
<figureCaption confidence="0.99508">
Figure 2: Word Alignment Probability
</figureCaption>
<bodyText confidence="0.827505666666667">
We use Expectation-Maximization (EM) algo-
rithm (Dempster et al., 1977) to estimate the con-
ditional probability of word ws and wt given class
c, P(ws|c) and P(wt|c) respectively. We derive the
equations for EM algorithm, using notations similar
to (Nigam et al., 2000).
In the E-step, the distribution of hidden variables
(i.e. class label for unlabeled parallel sentences) is
computed according to the following equations.
</bodyText>
<equation confidence="0.987751875">
P(cj|usi) = Z(cusi = cj) =
∏ws∈usi[P(ws|cj) + ∑P(ws|wt)&gt;0 P(ws|wt)P(wt|cj)]
∑ ∏ ws∈usi[P(ws|cj) + ∑P(ws|wt)&gt;0 P(ws|wt)P(wt|cj)]
cj
P(cj|uti) = Z(cuti = cj) =
∏ wt∈uti[P(wt|cj) + ∑P(wt|ws)&gt;0 P(wt|ws)P(ws|cj)]
∑ ∏ wt∈uti[P(wt|cj) + ∑P(wt|ws)&gt;0 P(wt|ws)P(ws|cj)]
cj
</equation>
<bodyText confidence="0.9989646">
where Z(cusi = cj) (Z(cuti) = cj)is the probability
of the source (target) language sentence usi (uti) in
the i-th pair of sentences ui having class label cj.
In the M-step, the parameters are computed by the
following equations.
</bodyText>
<equation confidence="0.999912333333333">
1 + EI |As(i)NsiP(cj  |di)
P(ws|cj) = |1|(7)
 |V  |+ Es=1 A(i)NsiP(cj|di)
P(w I c) = 1 + EPt |At(i)NtiP(cj  |di) (8)
t j
|V  |+ E|Vt |t=1 A(i)NtiP(cj|di)
</equation>
<bodyText confidence="0.998630142857143">
where As(i) and At(i) are weighting factor to con-
trol the influence of the unlabeled data. We set As(i)
(At(i)) to As (At) when di belongs to unlabeled
data, 1 otherwise. When di belongs to labeled data,
P(cj|di) is 1 when its label is cj and 0 otherwise.
When di belongs to unlabeled data, P(cj|di) is com-
puted according to Equation 5 or 6.
</bodyText>
<sectionHeader confidence="0.999932" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.999914">
4.1 Experiment Setup and Data Sets
</subsectionHeader>
<bodyText confidence="0.990962461538461">
Experiment setup: We conduct experiments on
two common cross-lingual sentiment classification
settings. In the first setting, no labeled data in the
target language are available. This setting has real-
istic significance, since in some situations we need to
quickly develop a sentiment classifier for languages
that we do not have labeled data in hand. In this
case, we classify text in the target language using
only labeled data in the source language. In the sec-
ond setting, labeled data in the target language are
also available. In this case, a more reasonable strat-
egy is to make full use of both labeled data in the
source language and target language to develop the
sentiment classifier for the target language. In our
experiments, we consider English as the source lan-
guage and Chinese as the target language.
Data sets: For Chinese sentiment classification,
we use the same data set described in (Lu et al.,
2011). The labeled data sets consist of two English
data sets and one Chinese data set. The English data
set is from the Multi-Perspective Question Answer-
ing (MPQA) corpus (Wiebe et al., 2005) and the NT-
CIR Opinion Analysis Pilot Task data set (Seki et
al., 2008; Seki et al., 2007). The Chinese data set
also comes from the NTCIR Opinion Analysis Pi-
lot Task data set. The unlabeled parallel sentences
</bodyText>
<equation confidence="0.948089666666667">
1 1 .3 .3 .3
1
1 .5 .5
</equation>
<page confidence="0.992589">
576
</page>
<bodyText confidence="0.9491667">
are selected from ISI Chinese-English parallel cor-
pus (Munteanu and Marcu, 2005). Following the
description in (Lu et al., 2011), we remove neutral
sentences and keep only high confident positive and
negative sentences as predicted by a maximum en-
tropy classifier trained on the labeled data. Table 1
shows the statistics for the data sets used in the ex-
periments. We conduct experiments on two data set-
tings: (1) MPQA + NTCIR-CH and (2) NTCIR-EN
+ NTCIR-CH.
</bodyText>
<table confidence="0.9815325">
MPQA NTCIR-EN NTCIR-CH
Positive 1,471(30%) 528 (30%) 2,378 (55%)
Negative 3,487(70%) 1,209(70%) 1,916(44%)
Total 4,958 1,737 4,294
</table>
<tableCaption confidence="0.999955">
Table 1: Statistics about the Data
</tableCaption>
<bodyText confidence="0.999796533333334">
CLMM includes two hyper-parameters (λs and
λt) controlling the contribution of unlabeled parallel
data. Larger weights indicate larger influence from
the unlabeled data. We set the hyper-parameters
by conducting cross validations on the labeled data.
When Chinese labeled data are unavailable, we set λt
to 1 and λs to 0.1, since no Chinese labeled data are
used and the contribution of target language to the
source language is limited. When Chinese labeled
data are available, we set λs and λt to 0.2.
To prevent long sentences from dominating the pa-
rameter estimation, we preprocess the data set by
normalizing the length of all sentences to the same
constant (Nigam et al., 2000), the average length of
the sentences.
</bodyText>
<subsectionHeader confidence="0.686561">
4.2 Baseline Methods
</subsectionHeader>
<bodyText confidence="0.999941457142857">
For the purpose of comparison, we implement the
following baseline methods.
MT-SVM: We translate the English labeled data to
Chinese using Google Translate and use the transla-
tion results to train the SVM classifier for Chinese.
SVM: We train a SVM classifier on the Chinese
labeled data.
MT-Cotrain: This is the co-training based ap-
proach described in (Wan, 2009). We summarize
the main steps as follows. First, two monolingual
SVM classifiers are trained on English labeled data
and Chinese data translated from English labeled
data. Second, the two classifiers make prediction on
Chinese unlabeled data and their English translation,
respectively. Third, the 100 most confidently pre-
dicted English and Chinese sentences are added to
the training set and the two monolingual SVM classi-
fiers are re-trained on the expanded training set. The
second and the third steps are repeated for 100 times
to obtain the final classifiers.
Para-Cotrain: The training process is the same as
MT-Cotrain. However, we use a different set of En-
glish unlabeled sentences. Instead of using the corre-
sponding machine translation of Chinese unlabeled
sentences, we use the parallel English sentences of
the Chinese unlabeled sentences.
Joint-Train: This is the state-of-the-art method de-
scribed in (Lu et al., 2011). This model use En-
glish labeled data and Chinese labeled data to obtain
initial parameters for two maximum entropy clas-
sifiers (for English documents and Chinese docu-
ments), and then conduct EM-iterations to update
the parameters to gradually improve the agreement
of the two monolingual classifiers on the unlabeled
parallel sentences.
</bodyText>
<subsectionHeader confidence="0.998689">
4.3 Classification Using Only English Labeled
Data
</subsectionHeader>
<bodyText confidence="0.999235166666667">
The first set of experiments are conducted on us-
ing only English labeled data to create the sentiment
classifier for Chinese. This is a challenging task,
since we do not use any Chinese labeled data. And
MPQA and NTCIR data sets are compiled by differ-
ent groups using different annotation guidelines.
</bodyText>
<table confidence="0.996668875">
Method NTCIR-EN MPQA-EN
NTCIR-CH NTCIR-CH
MT-SVM 62.34 54.33
SVM N/A N/A
MT-Cotrain 65.13 59.11
Para-Cotrain 67.21 60.71
Joint-Train N/A N/A
CLMM 70.96 71.52
</table>
<tableCaption confidence="0.8715425">
Table 2: Classification Accuracy Using Only
English Labeled Data
</tableCaption>
<bodyText confidence="0.99787">
Table 2 shows the accuracy of the baseline sys-
tems as well as the proposed model (CLMM). As
is shown, sentiment classification does not bene-
fit much from the direct machine translation. For
NTCIR-EN+NTCIR-CH, the accuracy of MT-SVM
</bodyText>
<page confidence="0.992214">
577
</page>
<bodyText confidence="0.999883190476191">
is only 62.34%. For MPQA-EN+NTCIR-CH, the
accuracy is 54.33%, even lower than a trivial
method, which achieves 55.4% by predicting all sen-
tences to be positive. The underlying reason is that
the vocabulary coverage in machine translated data
is low, therefore the classifier learned from the la-
beled data is unable to generalize well on the test
data. Meanwhile, the accuracy of MT-SVM on
NTCIR-EN+NTCIR-CH data set is much better than
that on MPQA+NTCIR-CH data set. That is be-
cause NTCIR-EN and NTCIR-CH cover similar top-
ics. The other two methods using machine translated
data, MT-Cotrain and Para-Cotrain also do not per-
form very well. This result is reasonable, because the
initial Chinese classifier trained on machine trans-
lated data (MT-SVM) is relatively weak. We also
observe that using a parallel corpus instead of ma-
chine translations can improve classification accu-
racy. It should be noted that we do not have the result
for Joint-Train model in this setting, since it requires
both English labeled data and Chinese labeled data.
</bodyText>
<subsectionHeader confidence="0.999569">
4.4 Classification Using English and Chinese
Labeled Data
</subsectionHeader>
<bodyText confidence="0.999461666666667">
The second set of experiments are conducted on
using both English labeled data and Chinese labeled
data to develop the Chinese sentiment classifier. We
conduct 5-fold cross validations on Chinese labeled
data. We use the same baseline methods as described
in Section 4.2, but we use natural Chinese sentences
instead of translated Chinese sentences as labeled
data in MT-Cotrain and Para-Cotrain. Table 3 shows
the accuracy of baseline systems as well as CLMM.
</bodyText>
<table confidence="0.998352875">
Method NTCIR-EN MPQA-EN
NTCIR-CH NTCIR-CH
MT-SVM 62.34 54.33
SVM 80.58 80.58
MT-Cotrain 82.28 80.93
Para-Cotrain 82.35 82.18
Joint-Train 83.11 83.42
CLMM 82.73 83.02
</table>
<tableCaption confidence="0.895376">
Table 3: Classification Accuracy Using English and
Chinese Labeled Data
</tableCaption>
<bodyText confidence="0.9977724375">
As is seen, SVM performs significantly better than
MT-SVM. One reason is that we use natural Chi-
nese labeled data instead of translated Chinese la-
beled data. Another reason is that we use 5-fold
cross validations in this setting, while the previous
setting is an open test setting. In this setting, SVM
is a strong baseline with 80.6% accuracy. Never-
theless, all three methods which leverage an unla-
beled parallel corpus, namely Para-Cotrain, Joint-
Train and CLMM, still show big improvements over
the SVM baseline. Their results are comparable and
all achieve state-of-the-art accuracy of about 83%,
but in terms of training speed, CLMM is the fastest
method (Table 4). Similar to the previous setting, We
also have the same observation that using a parallel
corpus is better than using translations.
</bodyText>
<table confidence="0.999554">
Method Iterations Total Time
Para-Cotrain 100 6 hours
Joint-Train 10 55 seconds
CLMM 10 30 seconds
</table>
<tableCaption confidence="0.99931">
Table 4: Training Speed Comparison
</tableCaption>
<subsectionHeader confidence="0.985706">
4.5 The Influence of Unlabeled Parallel Data
</subsectionHeader>
<bodyText confidence="0.995897291666667">
We investigate how the size of the unlabeled par-
allel data affects the sentiment classification in this
subsection. We vary the number of sentences in the
unlabeled parallel from 2,000 to 20,000. We use
only English labeled data in this experiment, since
this more directly reflects the effectiveness of each
model in utilizing unlabeled parallel data. From Fig-
ure 3 and Figure 4, we can see that when more unla-
beled parallel data are added, the accuracy of CLMM
consistently improves. The performance of CLMM
is remarkably superior than Para-Cotrain and MT-
Cotrain. When we have 10,000 parallel sentences,
the accuracy of CLMM on the two data sets quickly
increases to 68.77% and 68.91%, respectively. By
contrast, we observe that the performance of Para-
Cotrain and MT-Cotrain is able to obtain accuracy
improvement only after about 10,000 sentences are
added. The reason is that the two methods use ma-
chine translated labeled data to create initial Chinese
classifiers. As is depicted in Table 2, these classifiers
are relatively weak. As a result, in the initial itera-
tions of co-training based methods, the predictions
made by the Chinese classifiers are inaccurate, and
co-training based methods need to see more parallel
</bodyText>
<page confidence="0.988914">
578
</page>
<figure confidence="0.919862">
Number of Sentences
</figure>
<figureCaption confidence="0.982265">
Figure 3: Accuracy with different size of
unlabeled data for NTICR-EN+NTCIR-CH
</figureCaption>
<figure confidence="0.930291">
Number of Sentences
</figure>
<figureCaption confidence="0.9846905">
Figure 5: Accuracy with different size of
labeled data for NTCIR-EN+NTCIR-CH
</figureCaption>
<figure confidence="0.91609">
Number of Sentences
</figure>
<figureCaption confidence="0.975535">
Figure 4: Accuracy with different size of
unlabeled data for MPQA+NTCIR-CH
</figureCaption>
<figure confidence="0.9981662">
Model
● CLMM Joint−Train Para−Cotrain SVM
●
●
●
●
●
●
500 1000 1500 2000 2500 3000 3500
Number of Sentences
</figure>
<figureCaption confidence="0.974674">
Figure 6: Accuracy with different size of
labeled data for MPQA+NTCIR-CH
</figureCaption>
<figure confidence="0.999712169811321">
5000 10000 15000 20000
Accuracy
68
66
64
62
70
●
●
Model
● CLMM MT−Cotrain Para−Cotrain
●
●
●
●
●
●
● ●
500 1000 1500 2000 2500 3000 3500
Accuracy
80
65
70
5
●
Model
● CLMM Joint−Train Para−Cotrain SVM
●
●
●
●
●
●
5000 10000 15000 20000
Accuracy
65
60
55
70
●
●
Model
● CLMM MT−Cotrain Para−Cotrain
●
●
●
●
●
● ● ●
80
5
70
Accuracy
</figure>
<page confidence="0.320426">
65
</page>
<bodyText confidence="0.465487">
sentences to refine the initial classifiers.
</bodyText>
<subsectionHeader confidence="0.945887">
4.6 The Influence of Chinese Labeled Data
</subsectionHeader>
<bodyText confidence="0.999972411764706">
In this subsection, we investigate how the size of
the Chinese labeled data affects the sentiment classi-
fication. As is shown in Figure 5 and Figure 6, when
only 500 labeled sentences are used, CLMM is capa-
ble of achieving 72.52% and 74.48% in accuracy on
the two data sets, obtaining 10% and 8% improve-
ments over the SVM baseline, respectively. This
indicates that our method leverages the unlabeled
data effectively. When more sentences are used,
CLMM consistently shows further improvement in
accuracy. Para-Cotrain and Joint-Train show simi-
lar trends. When 3500 labeled sentences are used,
SVM achieves 80.58%, a relatively high accuracy
for sentiment classification. However, CLMM and
the other two models can still gain improvements.
This further demonstrates the advantages of expand-
ing vocabulary using bilingual parallel data.
</bodyText>
<sectionHeader confidence="0.997383" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.96005019047619">
In this paper, we propose a cross-lingual mix-
ture model (CLMM) to tackle the problem of cross-
lingual sentiment classification. This method has
two advantages over the existing methods. First, the
proposed model can learn previously unseen senti-
ment words from large unlabeled data, which are not
covered by the limited vocabulary in machine trans-
lation of the labeled data. Second, CLMM can ef-
fectively utilize unlabeled parallel data regardless of
whether labeled data in the target language are used
or not. Extensive experiments suggest that CLMM
consistently improve classification accuracy in both
settings. In the future, we will work on leverag-
ing parallel sentences and word alignments for other
tasks in sentiment analysis, such as building multi-
lingual sentiment lexicons.
Acknowledgment We thank Bin Lu and Lei Wang for
their help. This research was partly supported by National High
Technology Research and Development Program of China (863
Program) (No. 2012AA011101) and National Natural Science
Foundation of China (No.91024009, No.60973053)
</bodyText>
<page confidence="0.997742">
579
</page>
<sectionHeader confidence="0.996332" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999575923809524">
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings ofthe 2006 Conference
on Empirical Methods in Natural Language Process-
ing, page 120–128.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the eleventh annual conference on Computa-
tional learning theory, page 92–100.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
page 241–249.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), page 1–38.
Kevin Duh, Akinori Fujino, and Masaaki Nagata. 2011.
Is machine translation ripe for Cross-Lingual sentiment
classification? In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, page 429–433,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vectors,
and the role of linguistic analysis. In Proceedings ofthe
20th international conference on Computational Lin-
guistics, page 841.
Mingqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACMSIGKDD international conference on Knowledge
discovery and data mining, page 168–177.
Tao Li, Yi Zhang, and Vikas Sindhwani. 2009. A non-
negative matrix tri-factorization approach to sentiment
classification with lexical prior knowledge. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, page 244–252, Suntec, Singapore, August.
Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main con-
ference on Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, page 104–111.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K.
Tsou. 2011. Joint bilingual sentiment classification
with unlabeled parallel corpora. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies-
Volume 1, page 320–330.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477–504.
Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classification
from labeled and unlabeled documents using EM. Ma-
chine learning, 39(2):103–134.
Junfeng Pan, Gui-Rong Xue, Yong Yu, and Yang Wang.
2011. Cross-lingual sentiment classification via bi-
view non-negative matrix tri-factorization. Advances
in Knowledge Discovery and Data Mining, page
289–300.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1–135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing-Volume 10, page 79–86.
Peter Prettenhofer and Benno Stein. 2011. Cross-lingual
adaptation using structural correspondence learning.
ACM Transactions on Intelligent Systems and Technol-
ogy (TIST), 3(1):13.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi
Chen, Noriko Kando, and Chin-Yew Lin. 2007.
Overview of opinion analysis pilot task at NTCIR-6.
In Proceedings of NTCIR-6 Workshop Meeting, page
265–278.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,
Hsin-Hsi Chen, Noriko Kando, and Chin-Yew Lin.
2008. Overview of multilingual opinion analysis task
at NTCIR-7. In Proc. of the Seventh NTCIR Workshop.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-Based meth-
ods for sentiment analysis. Comput. Linguist., page to
appear.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguistics,
page 417–424.
Xiaojun Wan. 2008. Using bilingual knowledge and en-
semble techniques for unsupervised chinese sentiment
analysis. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, EMNLP
’08, page 553–561, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Xiaojun Wan. 2009. Co-training for cross-lingual senti-
ment classification. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
</reference>
<page confidence="0.970971">
580
</page>
<reference confidence="0.99762375">
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume 1,
page 235–243.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation,
39(2):165–210.
Taras Zagibalov and John Carroll. 2008. Automatic seed
word selection for unsupervised sentiment classifica-
tion of chinese text. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics-
Volume 1, page 1073–1080.
</reference>
<page confidence="0.998227">
581
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.086227">
<title confidence="0.904684">Cross-Lingual Mixture Model for Sentiment Classification *</title>
<keyword confidence="0.669366">Key Lab of Computational Linguistics, Peking</keyword>
<pubnum confidence="0.176969">Research</pubnum>
<email confidence="0.858617">xuge,</email>
<abstract confidence="0.999779692307693">The amount of labeled sentiment data in English is much larger than that in other languages. Such a disproportion arouse interest in cross-lingual sentiment classification, which aims to conduct sentiment classification in the target language (e.g. Chinese) using labeled data in the source language (e.g. English). Most existing work relies on machine translation engines to directly adapt labeled data from the source language to the target language. This approach suffers from the limited coverage of vocabulary in the machine translation results. In this paper, we propose a gencross-lingual mixture model to leverage unlabeled bilingual parallel data. By fitting parameters to maximize the likelihood of the bilingual parallel data, the proposed model learns previously unseen sentiment words from the large bilingual parallel data and improves vocabulary coverage significantly. Experiments on multiple data sets show that CLMM is consistently effective in two settings: (1) labeled data in the target language are unavailable; and (2) labeled data in the target language are also available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings ofthe 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>120--128</pages>
<contexts>
<context position="9670" citStr="Blitzer et al., 2006" startWordPosition="1467" endWordPosition="1470">. The same idea is used in (Wan, 2008), but the ensemble techniques used are various voting methods and the individual classifiers used are dictionary-based classifiers. Instead of ensemble methods, Pan et al. (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual setting. Instead of using machine translation engines to translate labeled text, the authors use it to construct the word translation oracle for pivot words translation. Lu et al. (2011) focus on the task of jointly improving the performance of sentiment classification on two languages (e.g. English and Chinese) . the authors use an unlabeled parallel corpus instead of machine translation engines. They assume parallel sentences in the corpus should have the same sentiment polarity. Besides, they assume labeled data in both language are available. They propose a method of train</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings ofthe 2006 Conference on Empirical Methods in Natural Language Processing, page 120–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the eleventh annual conference on Computational learning theory,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="8897" citStr="Blum and Mitchell, 1998" startWordPosition="1354" endWordPosition="1357">sic idea is to explore the abundant labeled sentiment data in source language to alleviate the shortage of labeled data in the target language. Most existing work relies on machine translation engines to directly adapt labeled data from the source language to target language. Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. English Labeled data are first translated to Chinese, and then two SVM classifiers are trained on English and Chinese labeled data respectively. After that, co-training (Blum and Mitchell, 1998) approach is adopted to leverage Chinese unlabeled data and their English translation to improve the SVM classifier for Chinese sentiment classification. The same idea is used in (Wan, 2008), but the ensemble techniques used are various voting methods and the individual classifiers used are dictionary-based classifiers. Instead of ensemble methods, Pan et al. (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate or</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, page 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>241--249</pages>
<contexts>
<context position="6684" citStr="Davidov et al., 2010" startWordPosition="1018" endWordPosition="1021">studies in Section 4, and finally conclude the paper and outline the future plan in Section 5. 2 Related Work In this section, we present a brief review of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment orientation detection as a conventional classification task and focuses on building classifier from a set of sentences (or documents) labeled with sentiment orientations. Dictionary-based methods involve in creatin</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, page 241–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Dempster</author>
<author>Nan Laird</author>
<author>Donald Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<pages>1--38</pages>
<contexts>
<context position="16999" citStr="Dempster et al., 1977" startWordPosition="2648" endWordPosition="2651">n example of word alignment probability. As is shown, the three words “tour de force” altogether express a positive meaning, while in Chinese the same meaning is expressed with only one word “杰作” (masterpiece). CLMM use word alignment probability to decrease the influences from “杰作” (masterpiece) to “tour”, “de” and “force” individually, using the word projection probability (i.e. word alignment probability), which is 0.3 in this case. Herman Melville&apos;s Moby Dick was a tour de force. 赫尔曼 梅尔维尔 的 “白鲸记” 是 一篇 杰作。 Figure 2: Word Alignment Probability We use Expectation-Maximization (EM) algorithm (Dempster et al., 1977) to estimate the conditional probability of word ws and wt given class c, P(ws|c) and P(wt|c) respectively. We derive the equations for EM algorithm, using notations similar to (Nigam et al., 2000). In the E-step, the distribution of hidden variables (i.e. class label for unlabeled parallel sentences) is computed according to the following equations. P(cj|usi) = Z(cusi = cj) = ∏ws∈usi[P(ws|cj) + ∑P(ws|wt)&gt;0 P(ws|wt)P(wt|cj)] ∑ ∏ ws∈usi[P(ws|cj) + ∑P(ws|wt)&gt;0 P(ws|wt)P(wt|cj)] cj P(cj|uti) = Z(cuti = cj) = ∏ wt∈uti[P(wt|cj) + ∑P(wt|ws)&gt;0 P(wt|ws)P(ws|cj)] ∑ ∏ wt∈uti[P(wt|cj) + ∑P(wt|ws)&gt;0 P(wt|</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur Dempster, Nan Laird, and Donald Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), page 1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Akinori Fujino</author>
<author>Masaaki Nagata</author>
</authors>
<title>Is machine translation ripe for Cross-Lingual sentiment classification?</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>429--433</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="3307" citStr="Duh et al. (2011)" startWordPosition="506" endWordPosition="509">t approach to leveraging the labeled data in English is to use machine translation engines as a black box to translate the labeled data from English to the target language (e.g. Chinese), and then using the translated training data directly for the development of the sentiment classifier in the target language (Wan, 2009; Pan et al., 2011). Although the machine-translation-based methods are intuitive, they have certain limitations. First, the vocabulary covered by the translated labeled data is limited, hence many sentiment indicative words can not be learned from the translated labeled data. Duh et al. (2011) report low overlapping between vocabulary of natural English documents and the vocabulary of documents translated to English from Japanese, and the experiments of Duh 1http://research.nii.ac.jp/ntcir/index-en.html 572 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 572–581, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics et al. (2011) show that vocabulary coverage has a strong correlation with sentiment classification accuracy. Second, machine translation may change the sentiment polarity of the original </context>
</contexts>
<marker>Duh, Fujino, Nagata, 2011</marker>
<rawString>Kevin Duh, Akinori Fujino, and Masaaki Nagata. 2011. Is machine translation ripe for Cross-Lingual sentiment classification? In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, page 429–433, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis.</title>
<date>2004</date>
<booktitle>In Proceedings ofthe 20th international conference on Computational Linguistics,</booktitle>
<pages>841</pages>
<contexts>
<context position="7770" citStr="Gamon (2004)" startWordPosition="1184" endWordPosition="1186">sifier from a set of sentences (or documents) labeled with sentiment orientations. Dictionary-based methods involve in creating or using sentiment lexicons. Turney (2002) derives sentiment scores for phrases by measuring the mutual information between the given phrase and the words “excellent” and “poor”, and then uses the average scores of the phrases in a document as the sentiment of the document. Corpus-based methods are often built upon machine learning models. Pang et al. (2002) compare the performance of three commonly used machine learning models (Naive Bayes, Maximum Entropy and SVM). Gamon (2004) shows that introducing deeper linguistic features into SVM can help to improve the performance. The interested readers are referred to (Pang and Lee, 2008) for a comprehensive review of sentiment classification. 2.2 Cross-Lingual Sentiment Classification Cross-lingual sentiment classification, which aims to conduct sentiment classification in the target language (e.g. Chinese) with labeled data in the source 573 language (e.g. English), has been extensively studied in the very recent years. The basic idea is to explore the abundant labeled sentiment data in source language to alleviate the sh</context>
</contexts>
<marker>Gamon, 2004</marker>
<rawString>Michael Gamon. 2004. Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis. In Proceedings ofthe 20th international conference on Computational Linguistics, page 841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACMSIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="2421" citStr="Hu and Liu, 2004" startWordPosition="363" endWordPosition="366"> at Microsoft Research Asia. already a large amount of work on sentiment classification of text in various genres and in many languages. For example, Pang et al. (2002) focus on sentiment classification of movie reviews in English, and Zagibalov and Carroll (2008) study the problem of classifying product reviews in Chinese. During the past few years, NTCIR1 organized several pilot tasks for sentiment classification of news articles written in English, Chinese and Japanese (Seki et al., 2007; Seki et al., 2008). For English sentiment classification, there are several labeled corpora available (Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is desirable to use the English labeled data to improve sentiment classification of documents in other languages. One direct approach to leveraging the labeled data in English is to use machine translation engines as a black box to translate the labeled data from English to the target language (e.g. Chinese), and then using the translated training data directly for the development of the sentiment classifier in the target language (Wan, 2009; Pan et </context>
<context position="6491" citStr="Hu and Liu, 2004" startWordPosition="988" endWordPosition="991">a in the target language. The paper is organized as follows. We review related work in Section 2, and present the cross-lingual mixture model in Section 3. Then we present the experimental studies in Section 4, and finally conclude the paper and outline the future plan in Section 5. 2 Related Work In this section, we present a brief review of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment orientation detection</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Mingqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACMSIGKDD international conference on Knowledge discovery and data mining, page 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Li</author>
<author>Yi Zhang</author>
<author>Vikas Sindhwani</author>
</authors>
<title>A nonnegative matrix tri-factorization approach to sentiment classification with lexical prior knowledge.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>244--252</pages>
<institution>Suntec, Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="9376" citStr="Li et al., 2009" startWordPosition="1425" endWordPosition="1428"> then two SVM classifiers are trained on English and Chinese labeled data respectively. After that, co-training (Blum and Mitchell, 1998) approach is adopted to leverage Chinese unlabeled data and their English translation to improve the SVM classifier for Chinese sentiment classification. The same idea is used in (Wan, 2008), but the ensemble techniques used are various voting methods and the individual classifiers used are dictionary-based classifiers. Instead of ensemble methods, Pan et al. (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual setting. Instead of using machine translation engines to translate labeled text, the authors use it to construct the word translation oracle for pivot words translation. Lu et al. (2011) focus on the task of jointly improving the performance of sentiment classification on two languages (e</context>
</contexts>
<marker>Li, Zhang, Sindhwani, 2009</marker>
<rawString>Tao Li, Yi Zhang, and Vikas Sindhwani. 2009. A nonnegative matrix tri-factorization approach to sentiment classification with lexical prior knowledge. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, page 244–252, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="16128" citStr="Liang et al., 2006" startWordPosition="2510" endWordPosition="2513">w9)P(w9|cj))] E |U-| i=1 E |V.| 9=1 |C| E j=1 + |Vt| E t=1 E |Ut| i=1 |C| E j=1 E |D3| i=1 L(θ|Ds) = |V-| E s=1 |C| E j=1 E |Dt| i=1 L(θ|Dt) = E |Vt| t=1 |C| E j=1 575 3.3 Parameter Estimation Instead of estimating word projection probability (P(ws|wt) and P(wt|ws)) and conditional probability of word to class (P(wt|c) and P(ws|c)) simultaneously in the training procedure, we estimate them separately since the word projection probability stays invariant when estimating other parameters. We estimate word projection probability using word alignment probability generated by the Berkeley aligner (Liang et al., 2006). The word alignment probabilities serves two purposes. First, they connect the corresponding words between the source language and the target language. Second, they adjust the strength of influences between the corresponding words. Figure 2 gives an example of word alignment probability. As is shown, the three words “tour de force” altogether express a positive meaning, while in Chinese the same meaning is expressed with only one word “杰作” (masterpiece). CLMM use word alignment probability to decrease the influences from “杰作” (masterpiece) to “tour”, “de” and “force” individually, using the w</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, page 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Lu</author>
<author>Chenhao Tan</author>
<author>Claire Cardie</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Joint bilingual sentiment classification with unlabeled parallel corpora.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1,</booktitle>
<pages>320--330</pages>
<contexts>
<context position="9873" citStr="Lu et al. (2011)" startWordPosition="1499" endWordPosition="1502">. (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual setting. Instead of using machine translation engines to translate labeled text, the authors use it to construct the word translation oracle for pivot words translation. Lu et al. (2011) focus on the task of jointly improving the performance of sentiment classification on two languages (e.g. English and Chinese) . the authors use an unlabeled parallel corpus instead of machine translation engines. They assume parallel sentences in the corpus should have the same sentiment polarity. Besides, they assume labeled data in both language are available. They propose a method of training two classifiers based on maximum entropy formulation to maximize their prediction agreement on the parallel corpus. However, this method requires labeled data in both the source language and the targ</context>
<context position="19306" citStr="Lu et al., 2011" startWordPosition="3032" endWordPosition="3035">ges that we do not have labeled data in hand. In this case, we classify text in the target language using only labeled data in the source language. In the second setting, labeled data in the target language are also available. In this case, a more reasonable strategy is to make full use of both labeled data in the source language and target language to develop the sentiment classifier for the target language. In our experiments, we consider English as the source language and Chinese as the target language. Data sets: For Chinese sentiment classification, we use the same data set described in (Lu et al., 2011). The labeled data sets consist of two English data sets and one Chinese data set. The English data set is from the Multi-Perspective Question Answering (MPQA) corpus (Wiebe et al., 2005) and the NTCIR Opinion Analysis Pilot Task data set (Seki et al., 2008; Seki et al., 2007). The Chinese data set also comes from the NTCIR Opinion Analysis Pilot Task data set. The unlabeled parallel sentences 1 1 .3 .3 .3 1 1 .5 .5 576 are selected from ISI Chinese-English parallel corpus (Munteanu and Marcu, 2005). Following the description in (Lu et al., 2011), we remove neutral sentences and keep only high</context>
<context position="22393" citStr="Lu et al., 2011" startWordPosition="3536" endWordPosition="3539">tly predicted English and Chinese sentences are added to the training set and the two monolingual SVM classifiers are re-trained on the expanded training set. The second and the third steps are repeated for 100 times to obtain the final classifiers. Para-Cotrain: The training process is the same as MT-Cotrain. However, we use a different set of English unlabeled sentences. Instead of using the corresponding machine translation of Chinese unlabeled sentences, we use the parallel English sentences of the Chinese unlabeled sentences. Joint-Train: This is the state-of-the-art method described in (Lu et al., 2011). This model use English labeled data and Chinese labeled data to obtain initial parameters for two maximum entropy classifiers (for English documents and Chinese documents), and then conduct EM-iterations to update the parameters to gradually improve the agreement of the two monolingual classifiers on the unlabeled parallel sentences. 4.3 Classification Using Only English Labeled Data The first set of experiments are conducted on using only English labeled data to create the sentiment classifier for Chinese. This is a challenging task, since we do not use any Chinese labeled data. And MPQA an</context>
</contexts>
<marker>Lu, Tan, Cardie, Tsou, 2011</marker>
<rawString>Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K. Tsou. 2011. Joint bilingual sentiment classification with unlabeled parallel corpora. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1, page 320–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="19810" citStr="Munteanu and Marcu, 2005" startWordPosition="3124" endWordPosition="3127">target language. Data sets: For Chinese sentiment classification, we use the same data set described in (Lu et al., 2011). The labeled data sets consist of two English data sets and one Chinese data set. The English data set is from the Multi-Perspective Question Answering (MPQA) corpus (Wiebe et al., 2005) and the NTCIR Opinion Analysis Pilot Task data set (Seki et al., 2008; Seki et al., 2007). The Chinese data set also comes from the NTCIR Opinion Analysis Pilot Task data set. The unlabeled parallel sentences 1 1 .3 .3 .3 1 1 .5 .5 576 are selected from ISI Chinese-English parallel corpus (Munteanu and Marcu, 2005). Following the description in (Lu et al., 2011), we remove neutral sentences and keep only high confident positive and negative sentences as predicted by a maximum entropy classifier trained on the labeled data. Table 1 shows the statistics for the data sets used in the experiments. We conduct experiments on two data settings: (1) MPQA + NTCIR-CH and (2) NTCIR-EN + NTCIR-CH. MPQA NTCIR-EN NTCIR-CH Positive 1,471(30%) 528 (30%) 2,378 (55%) Negative 3,487(70%) 1,209(70%) 1,916(44%) Total 4,958 1,737 4,294 Table 1: Statistics about the Data CLMM includes two hyper-parameters (λs and λt) controll</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31(4):477–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew Kachites McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="17196" citStr="Nigam et al., 2000" startWordPosition="2681" endWordPosition="2684">terpiece). CLMM use word alignment probability to decrease the influences from “杰作” (masterpiece) to “tour”, “de” and “force” individually, using the word projection probability (i.e. word alignment probability), which is 0.3 in this case. Herman Melville&apos;s Moby Dick was a tour de force. 赫尔曼 梅尔维尔 的 “白鲸记” 是 一篇 杰作。 Figure 2: Word Alignment Probability We use Expectation-Maximization (EM) algorithm (Dempster et al., 1977) to estimate the conditional probability of word ws and wt given class c, P(ws|c) and P(wt|c) respectively. We derive the equations for EM algorithm, using notations similar to (Nigam et al., 2000). In the E-step, the distribution of hidden variables (i.e. class label for unlabeled parallel sentences) is computed according to the following equations. P(cj|usi) = Z(cusi = cj) = ∏ws∈usi[P(ws|cj) + ∑P(ws|wt)&gt;0 P(ws|wt)P(wt|cj)] ∑ ∏ ws∈usi[P(ws|cj) + ∑P(ws|wt)&gt;0 P(ws|wt)P(wt|cj)] cj P(cj|uti) = Z(cuti = cj) = ∏ wt∈uti[P(wt|cj) + ∑P(wt|ws)&gt;0 P(wt|ws)P(ws|cj)] ∑ ∏ wt∈uti[P(wt|cj) + ∑P(wt|ws)&gt;0 P(wt|ws)P(ws|cj)] cj where Z(cusi = cj) (Z(cuti) = cj)is the probability of the source (target) language sentence usi (uti) in the i-th pair of sentences ui having class label cj. In the M-step, the par</context>
<context position="21038" citStr="Nigam et al., 2000" startWordPosition="3324" endWordPosition="3327">ontribution of unlabeled parallel data. Larger weights indicate larger influence from the unlabeled data. We set the hyper-parameters by conducting cross validations on the labeled data. When Chinese labeled data are unavailable, we set λt to 1 and λs to 0.1, since no Chinese labeled data are used and the contribution of target language to the source language is limited. When Chinese labeled data are available, we set λs and λt to 0.2. To prevent long sentences from dominating the parameter estimation, we preprocess the data set by normalizing the length of all sentences to the same constant (Nigam et al., 2000), the average length of the sentences. 4.2 Baseline Methods For the purpose of comparison, we implement the following baseline methods. MT-SVM: We translate the English labeled data to Chinese using Google Translate and use the translation results to train the SVM classifier for Chinese. SVM: We train a SVM classifier on the Chinese labeled data. MT-Cotrain: This is the co-training based approach described in (Wan, 2009). We summarize the main steps as follows. First, two monolingual SVM classifiers are trained on English labeled data and Chinese data translated from English labeled data. Seco</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew Kachites McCallum, Sebastian Thrun, and Tom Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine learning, 39(2):103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junfeng Pan</author>
<author>Gui-Rong Xue</author>
<author>Yong Yu</author>
<author>Yang Wang</author>
</authors>
<title>Cross-lingual sentiment classification via biview non-negative matrix tri-factorization.</title>
<date>2011</date>
<booktitle>Advances in Knowledge Discovery and Data Mining,</booktitle>
<pages>289--300</pages>
<contexts>
<context position="3031" citStr="Pan et al., 2011" startWordPosition="467" endWordPosition="470">u, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is desirable to use the English labeled data to improve sentiment classification of documents in other languages. One direct approach to leveraging the labeled data in English is to use machine translation engines as a black box to translate the labeled data from English to the target language (e.g. Chinese), and then using the translated training data directly for the development of the sentiment classifier in the target language (Wan, 2009; Pan et al., 2011). Although the machine-translation-based methods are intuitive, they have certain limitations. First, the vocabulary covered by the translated labeled data is limited, hence many sentiment indicative words can not be learned from the translated labeled data. Duh et al. (2011) report low overlapping between vocabulary of natural English documents and the vocabulary of documents translated to English from Japanese, and the experiments of Duh 1http://research.nii.ac.jp/ntcir/index-en.html 572 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 572–581, J</context>
<context position="9265" citStr="Pan et al. (2011)" startWordPosition="1410" endWordPosition="1413">on English labeled data and their Chinese translation. English Labeled data are first translated to Chinese, and then two SVM classifiers are trained on English and Chinese labeled data respectively. After that, co-training (Blum and Mitchell, 1998) approach is adopted to leverage Chinese unlabeled data and their English translation to improve the SVM classifier for Chinese sentiment classification. The same idea is used in (Wan, 2008), but the ensemble techniques used are various voting methods and the individual classifiers used are dictionary-based classifiers. Instead of ensemble methods, Pan et al. (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual setting. Instead of using machine translation engines to translate labeled text, the authors use it to construct the word translation oracle for pivot words translation. Lu et al</context>
</contexts>
<marker>Pan, Xue, Yu, Wang, 2011</marker>
<rawString>Junfeng Pan, Gui-Rong Xue, Yong Yu, and Yang Wang. 2011. Cross-lingual sentiment classification via biview non-negative matrix tri-factorization. Advances in Knowledge Discovery and Data Mining, page 289–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<journal>Found. Trends Inf. Retr.,</journal>
<pages>2--1</pages>
<contexts>
<context position="7926" citStr="Pang and Lee, 2008" startWordPosition="1208" endWordPosition="1211">xicons. Turney (2002) derives sentiment scores for phrases by measuring the mutual information between the given phrase and the words “excellent” and “poor”, and then uses the average scores of the phrases in a document as the sentiment of the document. Corpus-based methods are often built upon machine learning models. Pang et al. (2002) compare the performance of three commonly used machine learning models (Naive Bayes, Maximum Entropy and SVM). Gamon (2004) shows that introducing deeper linguistic features into SVM can help to improve the performance. The interested readers are referred to (Pang and Lee, 2008) for a comprehensive review of sentiment classification. 2.2 Cross-Lingual Sentiment Classification Cross-lingual sentiment classification, which aims to conduct sentiment classification in the target language (e.g. Chinese) with labeled data in the source 573 language (e.g. English), has been extensively studied in the very recent years. The basic idea is to explore the abundant labeled sentiment data in source language to alleviate the shortage of labeled data in the target language. Most existing work relies on machine translation engines to directly adapt labeled data from the source langu</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1–135, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1973" citStr="Pang et al. (2002)" startWordPosition="294" endWordPosition="297">ed data in the target language are also available. 1 Introduction Sentiment Analysis (also known as opinion mining), which aims to extract the sentiment information from text, has attracted extensive attention in recent years. Sentiment classification, the task of determining the sentiment orientation (positive, negative or neutral) of text, has been the most extensively studied task in sentiment analysis. There is *Contribution during internship at Microsoft Research Asia. already a large amount of work on sentiment classification of text in various genres and in many languages. For example, Pang et al. (2002) focus on sentiment classification of movie reviews in English, and Zagibalov and Carroll (2008) study the problem of classifying product reviews in Chinese. During the past few years, NTCIR1 organized several pilot tasks for sentiment classification of news articles written in English, Chinese and Japanese (Seki et al., 2007; Seki et al., 2008). For English sentiment classification, there are several labeled corpora available (Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is des</context>
<context position="6458" citStr="Pang et al., 2002" startWordPosition="982" endWordPosition="985">f the availability of labeled data in the target language. The paper is organized as follows. We review related work in Section 2, and present the cross-lingual mixture model in Section 3. Then we present the experimental studies in Section 4, and finally conclude the paper and outline the future plan in Section 5. 2 Related Work In this section, we present a brief review of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats th</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL02 conference on Empirical methods in natural language processing-Volume 10, page 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Prettenhofer</author>
<author>Benno Stein</author>
</authors>
<title>Cross-lingual adaptation using structural correspondence learning.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>3--1</pages>
<contexts>
<context position="9544" citStr="Prettenhofer and Stein (2011)" startWordPosition="1449" endWordPosition="1452">ed to leverage Chinese unlabeled data and their English translation to improve the SVM classifier for Chinese sentiment classification. The same idea is used in (Wan, 2008), but the ensemble techniques used are various voting methods and the individual classifiers used are dictionary-based classifiers. Instead of ensemble methods, Pan et al. (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual setting. Instead of using machine translation engines to translate labeled text, the authors use it to construct the word translation oracle for pivot words translation. Lu et al. (2011) focus on the task of jointly improving the performance of sentiment classification on two languages (e.g. English and Chinese) . the authors use an unlabeled parallel corpus instead of machine translation engines. They assume parallel sentences in the corpus should have</context>
</contexts>
<marker>Prettenhofer, Stein, 2011</marker>
<rawString>Peter Prettenhofer and Benno Stein. 2011. Cross-lingual adaptation using structural correspondence learning. ACM Transactions on Intelligent Systems and Technology (TIST), 3(1):13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>David Kirk Evans</author>
<author>Lun-Wei Ku</author>
<author>Hsin-Hsi Chen</author>
<author>Noriko Kando</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Overview of opinion analysis pilot task at NTCIR-6.</title>
<date>2007</date>
<booktitle>In Proceedings of NTCIR-6 Workshop Meeting,</booktitle>
<pages>265--278</pages>
<contexts>
<context position="2300" citStr="Seki et al., 2007" startWordPosition="344" endWordPosition="347">utral) of text, has been the most extensively studied task in sentiment analysis. There is *Contribution during internship at Microsoft Research Asia. already a large amount of work on sentiment classification of text in various genres and in many languages. For example, Pang et al. (2002) focus on sentiment classification of movie reviews in English, and Zagibalov and Carroll (2008) study the problem of classifying product reviews in Chinese. During the past few years, NTCIR1 organized several pilot tasks for sentiment classification of news articles written in English, Chinese and Japanese (Seki et al., 2007; Seki et al., 2008). For English sentiment classification, there are several labeled corpora available (Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is desirable to use the English labeled data to improve sentiment classification of documents in other languages. One direct approach to leveraging the labeled data in English is to use machine translation engines as a black box to translate the labeled data from English to the target language (e.g. Chinese), and then using the tra</context>
<context position="6642" citStr="Seki et al., 2007" startWordPosition="1010" endWordPosition="1013">n 3. Then we present the experimental studies in Section 4, and finally conclude the paper and outline the future plan in Section 5. 2 Related Work In this section, we present a brief review of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment orientation detection as a conventional classification task and focuses on building classifier from a set of sentences (or documents) labeled with sentiment orientations. D</context>
<context position="19583" citStr="Seki et al., 2007" startWordPosition="3082" endWordPosition="3085">ake full use of both labeled data in the source language and target language to develop the sentiment classifier for the target language. In our experiments, we consider English as the source language and Chinese as the target language. Data sets: For Chinese sentiment classification, we use the same data set described in (Lu et al., 2011). The labeled data sets consist of two English data sets and one Chinese data set. The English data set is from the Multi-Perspective Question Answering (MPQA) corpus (Wiebe et al., 2005) and the NTCIR Opinion Analysis Pilot Task data set (Seki et al., 2008; Seki et al., 2007). The Chinese data set also comes from the NTCIR Opinion Analysis Pilot Task data set. The unlabeled parallel sentences 1 1 .3 .3 .3 1 1 .5 .5 576 are selected from ISI Chinese-English parallel corpus (Munteanu and Marcu, 2005). Following the description in (Lu et al., 2011), we remove neutral sentences and keep only high confident positive and negative sentences as predicted by a maximum entropy classifier trained on the labeled data. Table 1 shows the statistics for the data sets used in the experiments. We conduct experiments on two data settings: (1) MPQA + NTCIR-CH and (2) NTCIR-EN + NTCI</context>
</contexts>
<marker>Seki, Evans, Ku, Chen, Kando, Lin, 2007</marker>
<rawString>Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi Chen, Noriko Kando, and Chin-Yew Lin. 2007. Overview of opinion analysis pilot task at NTCIR-6. In Proceedings of NTCIR-6 Workshop Meeting, page 265–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>David Kirk Evans</author>
<author>Lun-Wei Ku</author>
<author>Hsin-Hsi Chen Le Sun</author>
<author>Noriko Kando</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Overview of multilingual opinion analysis task at NTCIR-7.</title>
<date>2008</date>
<booktitle>In Proc. of the Seventh NTCIR Workshop.</booktitle>
<marker>Seki, Evans, Ku, Le Sun, Kando, Lin, 2008</marker>
<rawString>Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, Noriko Kando, and Chin-Yew Lin. 2008. Overview of multilingual opinion analysis task at NTCIR-7. In Proc. of the Seventh NTCIR Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexicon-Based methods for sentiment analysis.</title>
<date>2011</date>
<journal>Comput. Linguist., page</journal>
<note>to appear.</note>
<contexts>
<context position="6835" citStr="Taboada et al., 2011" startWordPosition="1038" endWordPosition="1041">w of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment orientation detection as a conventional classification task and focuses on building classifier from a set of sentences (or documents) labeled with sentiment orientations. Dictionary-based methods involve in creating or using sentiment lexicons. Turney (2002) derives sentiment scores for phrases by measuring the mutual information between the given phrase and the </context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexicon-Based methods for sentiment analysis. Comput. Linguist., page to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="6472" citStr="Turney, 2002" startWordPosition="986" endWordPosition="987">of labeled data in the target language. The paper is organized as follows. We review related work in Section 2, and present the cross-lingual mixture model in Section 3. Then we present the experimental studies in Section 4, and finally conclude the paper and outline the future plan in Section 5. 2 Related Work In this section, we present a brief review of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment or</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, page 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Using bilingual knowledge and ensemble techniques for unsupervised chinese sentiment analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>553--561</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9087" citStr="Wan, 2008" startWordPosition="1386" endWordPosition="1387">directly adapt labeled data from the source language to target language. Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. English Labeled data are first translated to Chinese, and then two SVM classifiers are trained on English and Chinese labeled data respectively. After that, co-training (Blum and Mitchell, 1998) approach is adopted to leverage Chinese unlabeled data and their English translation to improve the SVM classifier for Chinese sentiment classification. The same idea is used in (Wan, 2008), but the ensemble techniques used are various voting methods and the individual classifiers used are dictionary-based classifiers. Instead of ensemble methods, Pan et al. (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual </context>
<context position="11243" citStr="Wan, 2008" startWordPosition="1714" endWordPosition="1715">ure model (CLMM) for sentiment classification. We first formalize the task of cross-lingual sentiment classification. Then we describe the CLMM model and present the parameter estimation algorithm for CLMM. 3.1 Cross-lingual Sentiment Classification Formally, the task we are concerned about is to develop a sentiment classifier for the target language T (e.g. Chinese), given labeled sentiment data DS in the source language S (e.g. English), unlabeled parallel corpus U of the source language and the target language, and optional labeled data DT in target language T. Aligning with previous work (Wan, 2008; Wan, 2009), we only consider binary sentiment classification scheme (positive or negative) in this paper, but the proposed method can be used in other classification schemes with minor modifications. 3.2 The Cross-Lingual Mixture Model The basic idea underlying CLMM is to enlarge the vocabulary by learning sentiment words from the parallel corpus. CLMM defines an intuitive generation process as follows. Suppose we are going to generate a positive or negative Chinese sentence, we have two ways of generating words. The first way is to directly generate a Chinese word according to the polarity </context>
</contexts>
<marker>Wan, 2008</marker>
<rawString>Xiaojun Wan. 2008. Using bilingual knowledge and ensemble techniques for unsupervised chinese sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, page 553–561, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Co-training for cross-lingual sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>235--243</pages>
<contexts>
<context position="3012" citStr="Wan, 2009" startWordPosition="465" endWordPosition="466"> (Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is desirable to use the English labeled data to improve sentiment classification of documents in other languages. One direct approach to leveraging the labeled data in English is to use machine translation engines as a black box to translate the labeled data from English to the target language (e.g. Chinese), and then using the translated training data directly for the development of the sentiment classifier in the target language (Wan, 2009; Pan et al., 2011). Although the machine-translation-based methods are intuitive, they have certain limitations. First, the vocabulary covered by the translated labeled data is limited, hence many sentiment indicative words can not be learned from the translated labeled data. Duh et al. (2011) report low overlapping between vocabulary of natural English documents and the vocabulary of documents translated to English from Japanese, and the experiments of Duh 1http://research.nii.ac.jp/ntcir/index-en.html 572 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistic</context>
<context position="8560" citStr="Wan (2009)" startWordPosition="1306" endWordPosition="1307">iew of sentiment classification. 2.2 Cross-Lingual Sentiment Classification Cross-lingual sentiment classification, which aims to conduct sentiment classification in the target language (e.g. Chinese) with labeled data in the source 573 language (e.g. English), has been extensively studied in the very recent years. The basic idea is to explore the abundant labeled sentiment data in source language to alleviate the shortage of labeled data in the target language. Most existing work relies on machine translation engines to directly adapt labeled data from the source language to target language. Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. English Labeled data are first translated to Chinese, and then two SVM classifiers are trained on English and Chinese labeled data respectively. After that, co-training (Blum and Mitchell, 1998) approach is adopted to leverage Chinese unlabeled data and their English translation to improve the SVM classifier for Chinese sentiment classification. The same idea is used in (Wan, 2008), but the ensemble techniques used are various voting methods and the ind</context>
<context position="11255" citStr="Wan, 2009" startWordPosition="1716" endWordPosition="1717">CLMM) for sentiment classification. We first formalize the task of cross-lingual sentiment classification. Then we describe the CLMM model and present the parameter estimation algorithm for CLMM. 3.1 Cross-lingual Sentiment Classification Formally, the task we are concerned about is to develop a sentiment classifier for the target language T (e.g. Chinese), given labeled sentiment data DS in the source language S (e.g. English), unlabeled parallel corpus U of the source language and the target language, and optional labeled data DT in target language T. Aligning with previous work (Wan, 2008; Wan, 2009), we only consider binary sentiment classification scheme (positive or negative) in this paper, but the proposed method can be used in other classification schemes with minor modifications. 3.2 The Cross-Lingual Mixture Model The basic idea underlying CLMM is to enlarge the vocabulary by learning sentiment words from the parallel corpus. CLMM defines an intuitive generation process as follows. Suppose we are going to generate a positive or negative Chinese sentence, we have two ways of generating words. The first way is to directly generate a Chinese word according to the polarity of the sente</context>
<context position="21462" citStr="Wan, 2009" startWordPosition="3394" endWordPosition="3395">o 0.2. To prevent long sentences from dominating the parameter estimation, we preprocess the data set by normalizing the length of all sentences to the same constant (Nigam et al., 2000), the average length of the sentences. 4.2 Baseline Methods For the purpose of comparison, we implement the following baseline methods. MT-SVM: We translate the English labeled data to Chinese using Google Translate and use the translation results to train the SVM classifier for Chinese. SVM: We train a SVM classifier on the Chinese labeled data. MT-Cotrain: This is the co-training based approach described in (Wan, 2009). We summarize the main steps as follows. First, two monolingual SVM classifiers are trained on English labeled data and Chinese data translated from English labeled data. Second, the two classifiers make prediction on Chinese unlabeled data and their English translation, respectively. Third, the 100 most confidently predicted English and Chinese sentences are added to the training set and the two monolingual SVM classifiers are re-trained on the expanded training set. The second and the third steps are repeated for 100 times to obtain the final classifiers. Para-Cotrain: The training process </context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Xiaojun Wan. 2009. Co-training for cross-lingual sentiment classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, page 235–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="2461" citStr="Wiebe et al., 2005" startWordPosition="371" endWordPosition="374">a large amount of work on sentiment classification of text in various genres and in many languages. For example, Pang et al. (2002) focus on sentiment classification of movie reviews in English, and Zagibalov and Carroll (2008) study the problem of classifying product reviews in Chinese. During the past few years, NTCIR1 organized several pilot tasks for sentiment classification of news articles written in English, Chinese and Japanese (Seki et al., 2007; Seki et al., 2008). For English sentiment classification, there are several labeled corpora available (Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is desirable to use the English labeled data to improve sentiment classification of documents in other languages. One direct approach to leveraging the labeled data in English is to use machine translation engines as a black box to translate the labeled data from English to the target language (e.g. Chinese), and then using the translated training data directly for the development of the sentiment classifier in the target language (Wan, 2009; Pan et al., 2011). Although the machine-transla</context>
<context position="19493" citStr="Wiebe et al., 2005" startWordPosition="3064" endWordPosition="3067">in the target language are also available. In this case, a more reasonable strategy is to make full use of both labeled data in the source language and target language to develop the sentiment classifier for the target language. In our experiments, we consider English as the source language and Chinese as the target language. Data sets: For Chinese sentiment classification, we use the same data set described in (Lu et al., 2011). The labeled data sets consist of two English data sets and one Chinese data set. The English data set is from the Multi-Perspective Question Answering (MPQA) corpus (Wiebe et al., 2005) and the NTCIR Opinion Analysis Pilot Task data set (Seki et al., 2008; Seki et al., 2007). The Chinese data set also comes from the NTCIR Opinion Analysis Pilot Task data set. The unlabeled parallel sentences 1 1 .3 .3 .3 1 1 .5 .5 576 are selected from ISI Chinese-English parallel corpus (Munteanu and Marcu, 2005). Following the description in (Lu et al., 2011), we remove neutral sentences and keep only high confident positive and negative sentences as predicted by a maximum entropy classifier trained on the labeled data. Table 1 shows the statistics for the data sets used in the experiments</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taras Zagibalov</author>
<author>John Carroll</author>
</authors>
<title>Automatic seed word selection for unsupervised sentiment classification of chinese text.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational LinguisticsVolume 1,</booktitle>
<pages>1073--1080</pages>
<contexts>
<context position="2069" citStr="Zagibalov and Carroll (2008)" startWordPosition="308" endWordPosition="311">also known as opinion mining), which aims to extract the sentiment information from text, has attracted extensive attention in recent years. Sentiment classification, the task of determining the sentiment orientation (positive, negative or neutral) of text, has been the most extensively studied task in sentiment analysis. There is *Contribution during internship at Microsoft Research Asia. already a large amount of work on sentiment classification of text in various genres and in many languages. For example, Pang et al. (2002) focus on sentiment classification of movie reviews in English, and Zagibalov and Carroll (2008) study the problem of classifying product reviews in Chinese. During the past few years, NTCIR1 organized several pilot tasks for sentiment classification of news articles written in English, Chinese and Japanese (Seki et al., 2007; Seki et al., 2008). For English sentiment classification, there are several labeled corpora available (Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is desirable to use the English labeled data to improve sentiment classification of documents in other</context>
<context position="6623" citStr="Zagibalov and Carroll, 2008" startWordPosition="1005" endWordPosition="1009">ngual mixture model in Section 3. Then we present the experimental studies in Section 4, and finally conclude the paper and outline the future plan in Section 5. 2 Related Work In this section, we present a brief review of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment orientation detection as a conventional classification task and focuses on building classifier from a set of sentences (or documents) labeled with sentim</context>
</contexts>
<marker>Zagibalov, Carroll, 2008</marker>
<rawString>Taras Zagibalov and John Carroll. 2008. Automatic seed word selection for unsupervised sentiment classification of chinese text. In Proceedings of the 22nd International Conference on Computational LinguisticsVolume 1, page 1073–1080.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>