<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008331">
<title confidence="0.995194">
Demonstration of Joshua: An Open Source Toolkit
for Parsing-based Machine Translation∗
</title>
<author confidence="0.9752905">
Zhifei Li, Chris Callison-Burch, Chris Dyer†, Juri Ganitkevitch+, Sanjeev Khudanpur,
Lane Schwartz*, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan
</author>
<affiliation confidence="0.58520975">
Center for Language and Speech Processing, Johns Hopkins University
† Computational Linguistics and Information Processing Lab, University of Maryland
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University
* Natural Language Processing Lab, University of Minnesota
</affiliation>
<sectionHeader confidence="0.94931" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999636235294118">
We describe Joshua (Li et al., 2009a)1,
an open source toolkit for statistical ma-
chine translation. Joshua implements all
of the algorithms required for transla-
tion via synchronous context free gram-
mars (SCFGs): chart-parsing, n-gram lan-
guage model integration, beam- and cube-
pruning, and k-best extraction. The toolkit
also implements suffix-array grammar ex-
traction and minimum error rate training.
It uses parallel and distributed computing
techniques for scalability. We also pro-
vide a demonstration outline for illustrat-
ing the toolkit’s features to potential users,
whether they be newcomers to the field
or power users interested in extending the
toolkit.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998646071428571">
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al.
(2005), Galley et al. (2006), and Liu et al. (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high barrier
to entry for other researchers, and makes experi-
ments difficult to duplicate and compare. In this
paper, we describe Joshua, a Java-based general-
purpose open source toolkit for parsing-based ma-
chine translation, serving the same role as Moses
(Koehn et al., 2007) does for regular phrase-based
machine translation.
</bodyText>
<footnote confidence="0.989125428571429">
∗This research was supported in part by the Defense Ad-
vanced Research Projects Agency’s GALE program under
Contract No. HR0011-06-2-0001 and the National Science
Foundation under grants No. 0713448 and 0840112. The
views and findings are the authors’ alone.
1Please cite Li et al. (2009a) if you use Joshua in your
research, and not this demonstration description paper.
</footnote>
<sectionHeader confidence="0.88866" genericHeader="method">
2 Joshua Toolkit
</sectionHeader>
<bodyText confidence="0.998966025641026">
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: Joshua’s codebase consists of
a separate Java package for each major aspect
of functionality. This way, researchers can focus
on a single package of their choosing. Fuur-
thermore, extensible components are defined by
Java interfaces to minimize unintended inter-
actions and unseen dependencies, a common hin-
drance to extensibility in large projects. Where
there is a clear point of departure for research,
a basic implementation of each interface is
provided as an abstract class to minimize
work necessary for extensions.
End-to-end Cohesion: An MT pipeline con-
sists of many diverse components, often designed
by separate groups that have different file formats
and interaction requirements. This leads to a large
number of scripts for format conversion and to
facilitate interaction between the components, re-
sulting in untenable and non-portable projects, and
hindering repeatability of experiments. Joshua, on
the other hand, integrates the critical components
of an MT pipeline seamlessly. Still, each compo-
nent can be used as a stand-alone tool that does not
rely on the rest of the toolkit.
Scalability: Joshua, especially the decoder, is
scalable to large models and data sets. For ex-
ample, the parsing and pruning algorithms are im-
plemented with dynamic programming strategies
and efficient data structures. We also utilize suffix-
array grammar extraction, parallel/distributed de-
coding, and bloom filter language models.
Joshua offers state-of-the-art quality, having
been ranked 4th out of 16 systems in the French-
English task of the 2009 WMT evaluation, both in
automatic (Table 1) and human evaluation.
</bodyText>
<page confidence="0.986323">
25
</page>
<note confidence="0.958543">
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25–28,
Suntec, Singapore, 3 August 2009. c�2009 ACL and AFNLP
</note>
<table confidence="0.6087556">
System BLEU-4
google 31.14
lium 26.89
dcu 26.86
joshua 26.52
uka 25.96
limsi 25.51
uedin 25.44
rwth 24.89
cmu-statxfer 23.65
</table>
<tableCaption confidence="0.954137">
Table 1: BLEU scores for top primary systems on
</tableCaption>
<bodyText confidence="0.835431666666667">
the WMT-09 French-English Task from Callison-
Burch et al. (2009), who also provide human eval-
uation results.
</bodyText>
<subsectionHeader confidence="0.990901">
2.1 Joshua Toolkit Features
</subsectionHeader>
<bodyText confidence="0.9996225">
Here is a short description of Joshua’s main fea-
tures, described in more detail in Li et al. (2009a):
</bodyText>
<listItem confidence="0.930606818181818">
• Training Corpus Sub-sampling: We sup-
port inducing a grammar from a subset
of the training data, that consists of sen-
tences needed to translate a particular test
set. To accomplish this, we make use of the
method proposed by Kishore Papineni (per-
sonal communication), outlined in further de-
tail in (Li et al., 2009a). The method achieves
a 90% reduction in training corpus size while
maintaining state-of-the-art performance.
• Suffix-array Grammar Extraction: Gram-
</listItem>
<bodyText confidence="0.808144545454546">
mars extracted from large training corpora
are often far too large to fit into available
memory. Instead, we follow Callison-Burch
et al. (2005) and Lopez (2007), and use a
source language suffix array to extract only
rules that will actually be used in translating
a particular test set. Direct access to the suffix
array is incorporated into the decoder, allow-
ing rule extraction to be performed for each
input sentence individually, but it can also be
executed as a standalone pre-processing step.
</bodyText>
<listItem confidence="0.997877813953489">
• Grammar formalism: Our decoder as-
sumes a probabilistic synchronous context-
free grammar (SCFG). It handles SCFGs
of the kind extracted by Hiero (Chiang,
2007), but is easily extensible to more gen-
eral SCFGs (as in Galley et al. (2006)) and
closely related formalisms like synchronous
tree substitution grammars (Eisner, 2003).
• Pruning: We incorporate beam- and cube-
pruning (Chiang, 2007) to make decoding
feasible for large SCFGs.
• k-best extraction: Given a source sentence,
the chart-parsing algorithm produces a hy-
pergraph representing an exponential num-
ber of derivation hypotheses. We implement
the extraction algorithm of Huang and Chi-
ang (2005) to extract the k most likely deriva-
tions from the hypergraph.
• Oracle Extraction: Even within the large
set of translations represented by a hyper-
graph, some desired translations (e.g. the ref-
erences) may not be contained due to pruning
or inherent modeling deficiency. We imple-
ment an efficient dynamic programming al-
gorithm (Li and Khudanpur, 2009) for find-
ing the oracle translations, which are most
similar to the desired translations, as mea-
sured by a metric such as BLEU.
• Parallel and distributed decoding: We
support parallel decoding and a distributed
language model that exploit multi-core and
multi-processor architectures and distributed
computing (Li and Khudanpur, 2008).
• Language Models: We implement three lo-
cal n-gram language models: a straightfor-
ward implementation of the n-gram scoring
function in Java, capable of reading stan-
dard ARPA backoff n-gram models; a na-
tive code bridge that allows the decoder to
use the SRILM toolkit to read and score n-
grams2; and finally a Bloom Filter implemen-
tation following Talbot and Osborne (2007).
• Minimum Error Rate Training: Joshua’s
</listItem>
<bodyText confidence="0.880279625">
MERT module optimizes parameter weights
so as to maximize performance on a develop-
ment set as measured by an automatic evalu-
ation metric, such as BLEU. The optimization
consists of a series of line-optimizations us-
ing the efficient method of Och (2003). More
details on the MERT method and the imple-
mentation can be found in Zaidan (2009).3
</bodyText>
<footnote confidence="0.999417">
2The first implementation allows users to easily try the
Joshua toolkit without installing SRILM. However, users
should note that the basic Java LM implementation is not as
scalable as the SRILM native bridge code.
3The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
</footnote>
<page confidence="0.996326">
26
</page>
<listItem confidence="0.990975">
• Variational Decoding: spurious ambiguity
</listItem>
<bodyText confidence="0.743328">
causes the probability of an output string
among to be split among many derivations.
The goodness of a string is measured by
the total probability of its derivations, which
means that finding the best output string is
computationally intractable. The standard
Viterbi approximation is based on the most
probable derivation, but we also implement
a variational approximation, which considers
all the derivations but still allows tractable
decoding (Li et al., 2009b).
</bodyText>
<sectionHeader confidence="0.985506" genericHeader="method">
3 Demonstration Outline
</sectionHeader>
<bodyText confidence="0.999386888888889">
The purpose of the demonstration is 4-fold: 1) to
give newcomers to the field of statistical machine
translation an idea of the state-of-the-art; 2) to
show actual, live, end-to-end operation of the sys-
tem, highlighting its main components, targeting
potential users; 3) to illustrate, through visual aids,
the underlying algorithms, for those interested in
the technical details; and 4) to explain how those
components can be extended, for potential power
users who want to be familiar with the code itself.
The first component of the demonstration will
be an interactive user interface, where arbitrary
user input in a source language is entered into a
web form and then translated into a target lan-
guage by the system. This component specifically
targets newcomers to SMT, and demonstrates the
current state of the art in the field. We will have
trained multiple systems (for multiple language
pairs), hosted on a remote server, which will be
queried with the sample source sentences.
Potential users of the system would be inter-
ested in seeing an actual operation of the system,
in a similar fashion to what they would observe
on their own machines when using the toolkit. For
this purpose, we will demonstrate three main mod-
ules of the toolkit: the rule extraction module, the
MERT module, and the decoding module. Each
module will have a separate terminal window ex-
ecuting it, hence demonstrating both the module’s
expected output as well as its speed of operation.
In addition to demonstrating the functionality
of each module, we will also provide accompa-
nying visual aids that illustrate the underlying al-
gorithms and the technical operational details. We
will provide visualization of the search graph and
(Software and documentation at: http://cs.jhu.edu/
˜ozaidan/zmert.)
the 1-best derivation, which would illustrate the
functionality of the decoder, as well as alterna-
tive translations for phrases of the source sentence,
and where they were learned in the parallel cor-
pus, illustrating the functionality of the grammar
rule extraction. For the MERT module, we will
provide figures that illustrate Och’s efficient line
search method.
</bodyText>
<sectionHeader confidence="0.972473" genericHeader="method">
4 Demonstration Requirements
</sectionHeader>
<bodyText confidence="0.99999375">
The different components of the demonstration
will be spread across at most 3 machines (Fig-
ure 1): one for the live “instant translation” user
interface, one for demonstrating the different com-
ponents of the system and algorithmic visualiza-
tions, and one designated for technical discussion
of the code. We will provide the machines our-
selves and ensure the proper software is installed
and configured. However, we are requesting that
large LCD monitors be made available, if possi-
ble, since that would allow more space to demon-
strate the different components with clarity than
our laptop displays would provide. We will also
require Internet connectivity for the live demon-
stration, in order to gain access to remote servers
where trained models will be hosted.
</bodyText>
<sectionHeader confidence="0.985274" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983570130434783">
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1–28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
ofACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
</reference>
<page confidence="0.993543">
27
</page>
<figure confidence="0.993174333333333">
MERT
Remote server
hosting trained
translation models
Grammar extraction
Decoder
</figure>
<bodyText confidence="0.999566083333333">
We will rely on 3 workstations:
one for the instant translation
demo, where arbitrary input is
translated from/to a language pair
of choice (top); one for runtime
demonstration of the system, with
a terminal window for each of the
three main components of the
systems, as well as visual aids,
such as derivation trees (left); and
one (not shown) designated for
technical discussion of the code.
</bodyText>
<figureCaption confidence="0.986722">
Figure 1: Proposed setup of our demonstration. When this paper is viewed as a PDF, the reader may
zoom in further to see more details.
</figureCaption>
<reference confidence="0.999398239130435">
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 135–139, Athens, Greece, March. As-
sociation for Computational Linguistics.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In Proceedings of ACL.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
ofACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.
</reference>
<page confidence="0.999071">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.619713">
<title confidence="0.9899495">Demonstration of Joshua: An Open Source Toolkit Parsing-based Machine</title>
<author confidence="0.8743585">Chris Callison-Burch Li</author>
<author confidence="0.8743585">Chris Juri Sanjeev Khudanpur</author>
<author confidence="0.8743585">Wren N G Thornton</author>
<author confidence="0.8743585">Jonathan Weese</author>
<author confidence="0.8743585">F Zaidan</author>
<affiliation confidence="0.9719715">Center for Language and Speech Processing, Johns Hopkins University Linguistics and Information Processing Lab, University of Maryland Language Technology and Pattern Recognition Group, RWTH Aachen University * Natural Language Processing Lab, University of Minnesota</affiliation>
<abstract confidence="0.997200833333333">describe et al., an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for translation via synchronous context free gram- (SCFGs): chart-parsing, language model integration, beamand cubeand extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Colin Bannard</author>
<author>Josh Schroeder</author>
</authors>
<title>Scaling phrase-based statistical machine translation to larger corpora and longer phrases.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5210" citStr="Callison-Burch et al. (2005)" startWordPosition="784" endWordPosition="787">i et al. (2009a): • Training Corpus Sub-sampling: We support inducing a grammar from a subset of the training data, that consists of sentences needed to translate a particular test set. To accomplish this, we make use of the method proposed by Kishore Papineni (personal communication), outlined in further detail in (Li et al., 2009a). The method achieves a 90% reduction in training corpus size while maintaining state-of-the-art performance. • Suffix-array Grammar Extraction: Grammars extracted from large training corpora are often far too large to fit into available memory. Instead, we follow Callison-Burch et al. (2005) and Lopez (2007), and use a source language suffix array to extract only rules that will actually be used in translating a particular test set. Direct access to the suffix array is incorporated into the decoder, allowing rule extraction to be performed for each input sentence individually, but it can also be executed as a standalone pre-processing step. • Grammar formalism: Our decoder assumes a probabilistic synchronous contextfree grammar (SCFG). It handles SCFGs of the kind extracted by Hiero (Chiang, 2007), but is easily extensible to more general SCFGs (as in Galley et al. (2006)) and cl</context>
</contexts>
<marker>Callison-Burch, Bannard, Schroeder, 2005</marker>
<rawString>Chris Callison-Burch, Colin Bannard, and Josh Schroeder. 2005. Scaling phrase-based statistical machine translation to larger corpora and longer phrases. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--28</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 1–28, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1301" citStr="Chiang (2007)" startWordPosition="180" endWordPosition="181">uired for translation via synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗This research was suppor</context>
<context position="5726" citStr="Chiang, 2007" startWordPosition="870" endWordPosition="871">often far too large to fit into available memory. Instead, we follow Callison-Burch et al. (2005) and Lopez (2007), and use a source language suffix array to extract only rules that will actually be used in translating a particular test set. Direct access to the suffix array is incorporated into the decoder, allowing rule extraction to be performed for each input sentence individually, but it can also be executed as a standalone pre-processing step. • Grammar formalism: Our decoder assumes a probabilistic synchronous contextfree grammar (SCFG). It handles SCFGs of the kind extracted by Hiero (Chiang, 2007), but is easily extensible to more general SCFGs (as in Galley et al. (2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). • Pruning: We incorporate beam- and cubepruning (Chiang, 2007) to make decoding feasible for large SCFGs. • k-best extraction: Given a source sentence, the chart-parsing algorithm produces a hypergraph representing an exponential number of derivation hypotheses. We implement the extraction algorithm of Huang and Chiang (2005) to extract the k most likely derivations from the hypergraph. • Oracle Extraction: Even within the larg</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="5893" citStr="Eisner, 2003" startWordPosition="896" endWordPosition="897">nly rules that will actually be used in translating a particular test set. Direct access to the suffix array is incorporated into the decoder, allowing rule extraction to be performed for each input sentence individually, but it can also be executed as a standalone pre-processing step. • Grammar formalism: Our decoder assumes a probabilistic synchronous contextfree grammar (SCFG). It handles SCFGs of the kind extracted by Hiero (Chiang, 2007), but is easily extensible to more general SCFGs (as in Galley et al. (2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). • Pruning: We incorporate beam- and cubepruning (Chiang, 2007) to make decoding feasible for large SCFGs. • k-best extraction: Given a source sentence, the chart-parsing algorithm produces a hypergraph representing an exponential number of derivation hypotheses. We implement the extraction algorithm of Huang and Chiang (2005) to extract the k most likely derivations from the hypergraph. • Oracle Extraction: Even within the large set of translations represented by a hypergraph, some desired translations (e.g. the references) may not be contained due to pruning or inherent modeling deficiency.</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/Coling.</booktitle>
<contexts>
<context position="1344" citStr="Galley et al. (2006)" startWordPosition="186" endWordPosition="189">s context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗This research was supported in part by the Defense Advanced Researc</context>
<context position="5802" citStr="Galley et al. (2006)" startWordPosition="883" endWordPosition="886">Callison-Burch et al. (2005) and Lopez (2007), and use a source language suffix array to extract only rules that will actually be used in translating a particular test set. Direct access to the suffix array is incorporated into the decoder, allowing rule extraction to be performed for each input sentence individually, but it can also be executed as a standalone pre-processing step. • Grammar formalism: Our decoder assumes a probabilistic synchronous contextfree grammar (SCFG). It handles SCFGs of the kind extracted by Hiero (Chiang, 2007), but is easily extensible to more general SCFGs (as in Galley et al. (2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). • Pruning: We incorporate beam- and cubepruning (Chiang, 2007) to make decoding feasible for large SCFGs. • k-best extraction: Given a source sentence, the chart-parsing algorithm produces a hypergraph representing an exponential number of derivation hypotheses. We implement the extraction algorithm of Huang and Chiang (2005) to extract the k most likely derivations from the hypergraph. • Oracle Extraction: Even within the large set of translations represented by a hypergraph, some desired translations</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the ACL/Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="6222" citStr="Huang and Chiang (2005)" startWordPosition="943" endWordPosition="947"> assumes a probabilistic synchronous contextfree grammar (SCFG). It handles SCFGs of the kind extracted by Hiero (Chiang, 2007), but is easily extensible to more general SCFGs (as in Galley et al. (2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). • Pruning: We incorporate beam- and cubepruning (Chiang, 2007) to make decoding feasible for large SCFGs. • k-best extraction: Given a source sentence, the chart-parsing algorithm produces a hypergraph representing an exponential number of derivation hypotheses. We implement the extraction algorithm of Huang and Chiang (2005) to extract the k most likely derivations from the hypergraph. • Oracle Extraction: Even within the large set of translations represented by a hypergraph, some desired translations (e.g. the references) may not be contained due to pruning or inherent modeling deficiency. We implement an efficient dynamic programming algorithm (Li and Khudanpur, 2009) for finding the oracle translations, which are most similar to the desired translations, as measured by a metric such as BLEU. • Parallel and distributed decoding: We support parallel decoding and a distributed language model that exploit multi-co</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-2007 Demo</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="1824" citStr="Koehn et al., 2007" startWordPosition="264" endWordPosition="267">t. 1 Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗This research was supported in part by the Defense Advanced Research Projects Agency’s GALE program under Contract No. HR0011-06-2-0001 and the National Science Foundation under grants No. 0713448 and 0840112. The views and findings are the authors’ alone. 1Please cite Li et al. (2009a) if you use Joshua in your research, and not this demonstration description paper. 2 Joshua Toolkit When designing our toolkit, we applied general principles of software engineering to achieve three major goals: Extensibility, end-to-end coherence, and scalabi</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the ACL-2007 Demo and Poster Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
<date>2008</date>
<booktitle>In Proceedings Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<contexts>
<context position="6909" citStr="Li and Khudanpur, 2008" startWordPosition="1051" endWordPosition="1054"> Oracle Extraction: Even within the large set of translations represented by a hypergraph, some desired translations (e.g. the references) may not be contained due to pruning or inherent modeling deficiency. We implement an efficient dynamic programming algorithm (Li and Khudanpur, 2009) for finding the oracle translations, which are most similar to the desired translations, as measured by a metric such as BLEU. • Parallel and distributed decoding: We support parallel decoding and a distributed language model that exploit multi-core and multi-processor architectures and distributed computing (Li and Khudanpur, 2008). • Language Models: We implement three local n-gram language models: a straightforward implementation of the n-gram scoring function in Java, capable of reading standard ARPA backoff n-gram models; a native code bridge that allows the decoder to use the SRILM toolkit to read and score ngrams2; and finally a Bloom Filter implementation following Talbot and Osborne (2007). • Minimum Error Rate Training: Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a se</context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. A scalable decoder for parsing-based machine translation with equivalent language model state maintenance. In Proceedings Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Efficient extraction of oracle-best translations from hypergraphs.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="6574" citStr="Li and Khudanpur, 2009" startWordPosition="1001" endWordPosition="1004"> 2007) to make decoding feasible for large SCFGs. • k-best extraction: Given a source sentence, the chart-parsing algorithm produces a hypergraph representing an exponential number of derivation hypotheses. We implement the extraction algorithm of Huang and Chiang (2005) to extract the k most likely derivations from the hypergraph. • Oracle Extraction: Even within the large set of translations represented by a hypergraph, some desired translations (e.g. the references) may not be contained due to pruning or inherent modeling deficiency. We implement an efficient dynamic programming algorithm (Li and Khudanpur, 2009) for finding the oracle translations, which are most similar to the desired translations, as measured by a metric such as BLEU. • Parallel and distributed decoding: We support parallel decoding and a distributed language model that exploit multi-core and multi-processor architectures and distributed computing (Li and Khudanpur, 2008). • Language Models: We implement three local n-gram language models: a straightforward implementation of the n-gram scoring function in Java, capable of reading standard ARPA backoff n-gram models; a native code bridge that allows the decoder to use the SRILM tool</context>
</contexts>
<marker>Li, Khudanpur, 2009</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2009. Efficient extraction of oracle-best translations from hypergraphs. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsingbased machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>135--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="2162" citStr="Li et al. (2009" startWordPosition="315" endWordPosition="318"> high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗This research was supported in part by the Defense Advanced Research Projects Agency’s GALE program under Contract No. HR0011-06-2-0001 and the National Science Foundation under grants No. 0713448 and 0840112. The views and findings are the authors’ alone. 1Please cite Li et al. (2009a) if you use Joshua in your research, and not this demonstration description paper. 2 Joshua Toolkit When designing our toolkit, we applied general principles of software engineering to achieve three major goals: Extensibility, end-to-end coherence, and scalability. Extensibility: Joshua’s codebase consists of a separate Java package for each major aspect of functionality. This way, researchers can focus on a single package of their choosing. Fuurthermore, extensible components are defined by Java interfaces to minimize unintended interactions and unseen dependencies, a common hindrance to ex</context>
<context position="4596" citStr="Li et al. (2009" startWordPosition="687" endWordPosition="690">f the 2009 WMT evaluation, both in automatic (Table 1) and human evaluation. 25 Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25–28, Suntec, Singapore, 3 August 2009. c�2009 ACL and AFNLP System BLEU-4 google 31.14 lium 26.89 dcu 26.86 joshua 26.52 uka 25.96 limsi 25.51 uedin 25.44 rwth 24.89 cmu-statxfer 23.65 Table 1: BLEU scores for top primary systems on the WMT-09 French-English Task from CallisonBurch et al. (2009), who also provide human evaluation results. 2.1 Joshua Toolkit Features Here is a short description of Joshua’s main features, described in more detail in Li et al. (2009a): • Training Corpus Sub-sampling: We support inducing a grammar from a subset of the training data, that consists of sentences needed to translate a particular test set. To accomplish this, we make use of the method proposed by Kishore Papineni (personal communication), outlined in further detail in (Li et al., 2009a). The method achieves a 90% reduction in training corpus size while maintaining state-of-the-art performance. • Suffix-array Grammar Extraction: Grammars extracted from large training corpora are often far too large to fit into available memory. Instead, we follow Callison-Burch</context>
<context position="8496" citStr="Li et al., 2009" startWordPosition="1308" endWordPosition="1311">The module is also available as a standalone application, Z-MERT, that can be used with other MT systems. 26 • Variational Decoding: spurious ambiguity causes the probability of an output string among to be split among many derivations. The goodness of a string is measured by the total probability of its derivations, which means that finding the best output string is computationally intractable. The standard Viterbi approximation is based on the most probable derivation, but we also implement a variational approximation, which considers all the derivations but still allows tractable decoding (Li et al., 2009b). 3 Demonstration Outline The purpose of the demonstration is 4-fold: 1) to give newcomers to the field of statistical machine translation an idea of the state-of-the-art; 2) to show actual, live, end-to-end operation of the system, highlighting its main components, targeting potential users; 3) to illustrate, through visual aids, the underlying algorithms, for those interested in the technical details; and 4) to explain how those components can be extended, for potential power users who want to be familiar with the code itself. The first component of the demonstration will be an interactive</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009a. Joshua: An open source toolkit for parsingbased machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 135–139, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Variational decoding for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2162" citStr="Li et al. (2009" startWordPosition="315" endWordPosition="318"> high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗This research was supported in part by the Defense Advanced Research Projects Agency’s GALE program under Contract No. HR0011-06-2-0001 and the National Science Foundation under grants No. 0713448 and 0840112. The views and findings are the authors’ alone. 1Please cite Li et al. (2009a) if you use Joshua in your research, and not this demonstration description paper. 2 Joshua Toolkit When designing our toolkit, we applied general principles of software engineering to achieve three major goals: Extensibility, end-to-end coherence, and scalability. Extensibility: Joshua’s codebase consists of a separate Java package for each major aspect of functionality. This way, researchers can focus on a single package of their choosing. Fuurthermore, extensible components are defined by Java interfaces to minimize unintended interactions and unseen dependencies, a common hindrance to ex</context>
<context position="4596" citStr="Li et al. (2009" startWordPosition="687" endWordPosition="690">f the 2009 WMT evaluation, both in automatic (Table 1) and human evaluation. 25 Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25–28, Suntec, Singapore, 3 August 2009. c�2009 ACL and AFNLP System BLEU-4 google 31.14 lium 26.89 dcu 26.86 joshua 26.52 uka 25.96 limsi 25.51 uedin 25.44 rwth 24.89 cmu-statxfer 23.65 Table 1: BLEU scores for top primary systems on the WMT-09 French-English Task from CallisonBurch et al. (2009), who also provide human evaluation results. 2.1 Joshua Toolkit Features Here is a short description of Joshua’s main features, described in more detail in Li et al. (2009a): • Training Corpus Sub-sampling: We support inducing a grammar from a subset of the training data, that consists of sentences needed to translate a particular test set. To accomplish this, we make use of the method proposed by Kishore Papineni (personal communication), outlined in further detail in (Li et al., 2009a). The method achieves a 90% reduction in training corpus size while maintaining state-of-the-art performance. • Suffix-array Grammar Extraction: Grammars extracted from large training corpora are often far too large to fit into available memory. Instead, we follow Callison-Burch</context>
<context position="8496" citStr="Li et al., 2009" startWordPosition="1308" endWordPosition="1311">The module is also available as a standalone application, Z-MERT, that can be used with other MT systems. 26 • Variational Decoding: spurious ambiguity causes the probability of an output string among to be split among many derivations. The goodness of a string is measured by the total probability of its derivations, which means that finding the best output string is computationally intractable. The standard Viterbi approximation is based on the most probable derivation, but we also implement a variational approximation, which considers all the derivations but still allows tractable decoding (Li et al., 2009b). 3 Demonstration Outline The purpose of the demonstration is 4-fold: 1) to give newcomers to the field of statistical machine translation an idea of the state-of-the-art; 2) to show actual, live, end-to-end operation of the system, highlighting its main components, targeting potential users; 3) to illustrate, through visual aids, the underlying algorithms, for those interested in the technical details; and 4) to explain how those components can be extended, for potential power users who want to be familiar with the code itself. The first component of the demonstration will be an interactive</context>
</contexts>
<marker>Li, Eisner, Khudanpur, 2009</marker>
<rawString>Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009b. Variational decoding for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment templates for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/Coling.</booktitle>
<contexts>
<context position="1367" citStr="Liu et al. (2006)" startWordPosition="191" endWordPosition="194">CFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗This research was supported in part by the Defense Advanced Research Projects Agency’s GAL</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment templates for statistical machine translation. In Proceedings of the ACL/Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoLing.</booktitle>
<contexts>
<context position="5227" citStr="Lopez (2007)" startWordPosition="789" endWordPosition="790">us Sub-sampling: We support inducing a grammar from a subset of the training data, that consists of sentences needed to translate a particular test set. To accomplish this, we make use of the method proposed by Kishore Papineni (personal communication), outlined in further detail in (Li et al., 2009a). The method achieves a 90% reduction in training corpus size while maintaining state-of-the-art performance. • Suffix-array Grammar Extraction: Grammars extracted from large training corpora are often far too large to fit into available memory. Instead, we follow Callison-Burch et al. (2005) and Lopez (2007), and use a source language suffix array to extract only rules that will actually be used in translating a particular test set. Direct access to the suffix array is incorporated into the decoder, allowing rule extraction to be performed for each input sentence individually, but it can also be executed as a standalone pre-processing step. • Grammar formalism: Our decoder assumes a probabilistic synchronous contextfree grammar (SCFG). It handles SCFGs of the kind extracted by Hiero (Chiang, 2007), but is easily extensible to more general SCFGs (as in Galley et al. (2006)) and closely related for</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In Proceedings of EMNLPCoLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="7576" citStr="Och (2003)" startWordPosition="1164" endWordPosition="1165">uage models: a straightforward implementation of the n-gram scoring function in Java, capable of reading standard ARPA backoff n-gram models; a native code bridge that allows the decoder to use the SRILM toolkit to read and score ngrams2; and finally a Bloom Filter implementation following Talbot and Osborne (2007). • Minimum Error Rate Training: Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optimizations using the efficient method of Och (2003). More details on the MERT method and the implementation can be found in Zaidan (2009).3 2The first implementation allows users to easily try the Joshua toolkit without installing SRILM. However, users should note that the basic Java LM implementation is not as scalable as the SRILM native bridge code. 3The module is also available as a standalone application, Z-MERT, that can be used with other MT systems. 26 • Variational Decoding: spurious ambiguity causes the probability of an output string among to be split among many derivations. The goodness of a string is measured by the total probabil</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1322" citStr="Quirk et al. (2005)" startWordPosition="182" endWordPosition="185">lation via synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗This research was supported in part by the De</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7282" citStr="Talbot and Osborne (2007)" startWordPosition="1114" endWordPosition="1117">tions, as measured by a metric such as BLEU. • Parallel and distributed decoding: We support parallel decoding and a distributed language model that exploit multi-core and multi-processor architectures and distributed computing (Li and Khudanpur, 2008). • Language Models: We implement three local n-gram language models: a straightforward implementation of the n-gram scoring function in Java, capable of reading standard ARPA backoff n-gram models; a native code bridge that allows the decoder to use the SRILM toolkit to read and score ngrams2; and finally a Bloom Filter implementation following Talbot and Osborne (2007). • Minimum Error Rate Training: Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optimizations using the efficient method of Och (2003). More details on the MERT method and the implementation can be found in Zaidan (2009).3 2The first implementation allows users to easily try the Joshua toolkit without installing SRILM. However, users should note that the basic Java LM implementation is not as scalable as the SRILM native bridge code. 3T</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Randomised language modelling for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
</authors>
<title>Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--79</pages>
<contexts>
<context position="7662" citStr="Zaidan (2009)" startWordPosition="1180" endWordPosition="1181">a, capable of reading standard ARPA backoff n-gram models; a native code bridge that allows the decoder to use the SRILM toolkit to read and score ngrams2; and finally a Bloom Filter implementation following Talbot and Osborne (2007). • Minimum Error Rate Training: Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optimizations using the efficient method of Och (2003). More details on the MERT method and the implementation can be found in Zaidan (2009).3 2The first implementation allows users to easily try the Joshua toolkit without installing SRILM. However, users should note that the basic Java LM implementation is not as scalable as the SRILM native bridge code. 3The module is also available as a standalone application, Z-MERT, that can be used with other MT systems. 26 • Variational Decoding: spurious ambiguity causes the probability of an output string among to be split among many derivations. The goodness of a string is measured by the total probability of its derivations, which means that finding the best output string is computation</context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Omar F. Zaidan. 2009. Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91:79–88.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>