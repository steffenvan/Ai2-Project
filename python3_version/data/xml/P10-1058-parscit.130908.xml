<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002410">
<title confidence="0.988011">
Automatic Generation of Story Highlights
</title>
<author confidence="0.995085">
Kristian Woodsend and Mirella Lapata
</author>
<affiliation confidence="0.999849">
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.874258">
Edinburgh EH8 9AB, United Kingdom
</address>
<email confidence="0.990207">
k.woodsend@ed.ac.uk,mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997289" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999281842105263">
In this paper we present a joint con-
tent selection and compression model
for single-document summarization. The
model operates over a phrase-based rep-
resentation of the source document which
we obtain by merging information from
PCFG parse trees and dependency graphs.
Using an integer linear programming for-
mulation, the model learns to select and
combine phrases subject to length, cover-
age and grammar constraints. We evalu-
ate the approach on the task of generat-
ing “story highlights”—a small number of
brief, self-contained sentences that allow
readers to quickly gather information on
news stories. Experimental results show
that the model’s output is comparable to
human-written highlights in terms of both
grammaticality and content.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99934855">
Summarization is the process of condensing a
source text into a shorter version while preserving
its information content. Humans summarize on
a daily basis and effortlessly, but producing high
quality summaries automatically remains a chal-
lenge. The difficulty lies primarily in the nature
of the task which is complex, must satisfy many
constraints (e.g., summary length, informative-
ness, coherence, grammaticality) and ultimately
requires wide-coverage text understanding. Since
the latter is beyond the capabilities of current NLP
technology, most work today focuses on extractive
summarization, where a summary is created sim-
ply by identifying and subsequently concatenating
the most important sentences in a document.
Without a great deal of linguistic analysis, it
is possible to create summaries for a wide range
of documents. Unfortunately, extracts are of-
ten documents of low readability and text quality
and contain much redundant information. This is
in marked contrast with hand-written summaries
which often combine several pieces of informa-
tion from the original document (Jing, 2002) and
exhibit many rewrite operations such as substitu-
tions, insertions, deletions, or reorderings.
Sentence compression is often regarded as a
promising first step towards ameliorating some of
the problems associated with extractive summa-
rization. The task is commonly expressed as a
word deletion problem. It involves creating a short
grammatical summary of a single sentence, by re-
moving elements that are considered extraneous,
while retaining the most important information
(Knight and Marcu, 2002). Interfacing extractive
summarization with a sentence compression mod-
ule could improve the conciseness of the gener-
ated summaries and render them more informative
(Jing, 2000; Lin, 2003; Zajic et al., 2007).
Despite the bulk of work on sentence compres-
sion and summarization (see Clarke and Lapata
2008 and Mani 2001 for overviews) only a handful
of approaches attempt to do both in a joint model
(Daum´e III and Marcu, 2002; Daum´e III, 2006;
Lin, 2003; Martins and Smith, 2009). One rea-
son for this might be the performance of sentence
compression systems which falls short of attaining
grammaticality levels of human output. For ex-
ample, Clarke and Lapata (2008) evaluate a range
of state-of-the-art compression systems across dif-
ferent domains and show that machine generated
compressions are consistently perceived as worse
than the human gold standard. Another reason is
the summarization objective itself. If our goal is
to summarize news articles, then we may be bet-
ter off selecting the first n sentences of the docu-
ment. This “lead” baseline may err on the side of
verbosity but at least will be grammatical, and it
has indeed proved extremely hard to outperform
by more sophisticated methods (Nenkova, 2005).
In this paper we propose a model for sum-
</bodyText>
<page confidence="0.975271">
565
</page>
<note confidence="0.943408">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565–574,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999928818181818">
marization that incorporates compression into the
task. A key insight in our approach is to formulate
summarization as a phrase rather than sentence
extraction problem. Compression falls naturally
out of this formulation as only phrases deemed
important should appear in the summary. Ob-
viously, our output summaries must meet addi-
tional requirements such as sentence length, over-
all length, topic coverage and, importantly, gram-
maticality. We combine phrase and dependency
information into a single data structure, which al-
lows us to express grammaticality as constraints
across phrase dependencies. We encode these con-
straints through the use of integer linear program-
ming (ILP), a well-studied optimization frame-
work that is able to search the entire solution space
efficiently.
We apply our model to the task of generat-
ing highlights for a single document. Examples
of CNN news articles with human-authored high-
lights are shown in Table 1. Highlights give a
brief overview of the article to allow readers to
quickly gather information on stories, and usually
appear as bullet points. Importantly, they repre-
sent the gist of the entire document and thus of-
ten differ substantially from the first n sentences
in the article (Svore et al., 2007). They are also
highly compressed, written in a telegraphic style
and thus provide an excellent testbed for models
that generate compressed summaries. Experimen-
tal results show that our model’s output is compa-
rable to hand-written highlights both in terms of
grammaticality and informativeness.
</bodyText>
<sectionHeader confidence="0.999896" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999976865671642">
Much effort in automatic summarization has been
devoted to sentence extraction which is often for-
malized as a classification task (Kupiec et al.,
1995). Given appropriately annotated training
data, a binary classifier learns to predict for
each document sentence if it is worth extracting.
Surface-level features are typically used to sin-
gle out important sentences. These include the
presence of certain key phrases, the position of
a sentence in the original document, the sentence
length, the words in the title, the presence of
proper nouns, etc. (Mani, 2001; Sparck Jones,
1999).
Relatively little work has focused on extraction
methods for units smaller than sentences. Jing and
McKeown (2000) first extract sentences, then re-
move redundant phrases, and use (manual) recom-
bination rules to produce coherent output. Wan
and Paris (2008) segment sentences heuristically
into clauses before extraction takes place, and
show that this improves summarization quality.
In the context of multiple-document summariza-
tion, heuristics have also been used to remove par-
enthetical information (Conroy et al., 2004; Sid-
dharthan et al., 2004). Witten et al. (1999) (among
others) extract keyphrases to capture the gist of the
document, without however attempting to recon-
struct sentences or generate summaries.
A few previous approaches have attempted to
interface sentence compression with summariza-
tion. A straightforward way to achieve this is by
adopting a two-stage architecture (e.g., Lin 2003)
where the sentences are first extracted and then
compressed or the other way round. Other work
implements a joint model where words and sen-
tences are deleted simultaneously from a docu-
ment. Using a noisy-channel model, Daum´e III
and Marcu (2002) exploit the discourse structure
of a document and the syntactic structure of its
sentences in order to decide which constituents to
drop but also which discourse units are unimpor-
tant. Martins and Smith (2009) formulate a joint
sentence extraction and summarization model as
an ILP. The latter optimizes an objective func-
tion consisting of two parts: an extraction com-
ponent, essentially a non-greedy variant of max-
imal marginal relevance (McDonald, 2007), and
a sentence compression component, a more com-
pact reformulation of Clarke and Lapata (2008)
based on the output of a dependency parser. Com-
pression and extraction models are trained sepa-
rately in a max-margin framework and then inter-
polated. In the context of multi-document summa-
rization, Daum´e III’s (2006) vine-growth model
creates summaries incrementally, either by start-
ing a new sentence or by growing already existing
ones.
Our own work is closest to Martins and Smith
(2009). We also develop an ILP-based compres-
sion and summarization model, however, several
key differences set our approach apart. Firstly,
content selection is performed at the phrase rather
than sentence level. Secondly, the combination of
phrase and dependency information into a single
data structure is new, and important in allowing
us to express grammaticality as constraints across
phrase dependencies, rather than resorting to a lan-
</bodyText>
<page confidence="0.996499">
566
</page>
<subsectionHeader confidence="0.92236">
Most blacks say MLK’s vision fulfilled, poll finds
</subsectionHeader>
<bodyText confidence="0.991396">
WASHINGTON (CNN) – More than two-thirds of African-
Americans believe Martin Luther King Jr.’s vision for race
relations has been fulfilled, a CNN poll found – a figure up
sharply from a survey in early 2008.
The CNN-Opinion Research Corp. survey was released
Monday, a federal holiday honoring the slain civil rights
leader and a day before Barack Obama is to be sworn in as
the first black U.S. president.
The poll found 69 percent of blacks said King’s vision has
been fulfilled in the more than 45 years since his 1963 ’I have
a dream’ speech – roughly double the 34 percent who agreed
with that assessment in a similar poll taken last March.
But whites remain less optimistic, the survey found.
</bodyText>
<listItem confidence="0.9987795">
• 69 percent of blacks polled say Martin Luther King Jr’s
vision realized.
• Slim majority of whites say King’s vision not fulfilled.
• King gave his “I have a dream” speech in 1963.
</listItem>
<subsectionHeader confidence="0.630614">
9/11 billboard draws flak from Florida Democrats, GOP
</subsectionHeader>
<bodyText confidence="0.968167083333333">
(CNN) – A Florida man is using billboards with an image of
the burning World Trade Center to encourage votes for a Re-
publican presidential candidate, drawing criticism for politi-
cizing the 9/11 attacks.
‘Please Don’t Vote for a Democrat’ reads the type over the
picture of the twin towers after hijacked airliners hit them on
September, 11, 2001.
Mike Meehan, a St. Cloud, Florida, businessman who paid to
post the billboards in the Orlando area, said former President
Clinton should have put a stop to Osama bin Laden and al
Qaeda before 9/11. He said a Republican president would
have done so.
</bodyText>
<listItem confidence="0.99698675">
• Billboards use image from 9/11 to encourage GOP votes.
• 9/11 image wrong for ad, say Florida political parties.
• Floridian praises President Bush, says ex-President Clin-
ton failed to stop al Qaeda.
</listItem>
<tableCaption confidence="0.9832615">
Table 1: Two example CNN news articles, showing the title and the first few paragraphs, and below, the
original highlights that accompanied each story.
</tableCaption>
<bodyText confidence="0.999830416666667">
guage model. Lastly, our model is more com-
pact, has fewer parameters, and does not require
two training procedures. Our approach bears some
resemblance to headline generation (Dorr et al.,
2003; Banko et al., 2000), although we output sev-
eral sentences rather than a single one. Head-
line generation models typically extract individual
words from a document to produce a very short
summary, whereas we extract phrases and ensure
that they are combined into grammatical sentences
through our ILP constraints.
Svore et al. (2007) were the first to foreground
the highlight generation task which we adopt as an
evaluation testbed for our model. Their approach
is however a purely extractive one. Using an al-
gorithm based on neural networks and third-party
resources (e.g., news query logs and Wikipedia en-
tries) they rank sentences and select the three high-
est scoring ones as story highlights. In contrast,
we aim to generate rather than extract highlights.
As a first step we focus on deleting extraneous ma-
terial, but other more sophisticated rewrite opera-
tions (e.g., Cohn and Lapata 2009) could be incor-
porated into our framework.
</bodyText>
<sectionHeader confidence="0.989514" genericHeader="method">
3 The Task
</sectionHeader>
<bodyText confidence="0.9999194">
Given a document, we aim to produce three or four
short sentences covering its main topics, much like
the “Story Highlights” accompanying the (online)
CNN news articles. CNN highlights are written by
humans; we aim to do this automatically.
</bodyText>
<table confidence="0.98497">
Documents Highlights
Sentences 37.2 f 39.6 3.5 f 0.5
Tokens 795.0 f 744.8 47.0 f 9.6
Tokens/sentence 22.4 f 4.2 13.3 f 1.7
</table>
<tableCaption confidence="0.980009">
Table 2: Overview statistics on the corpus of doc-
</tableCaption>
<bodyText confidence="0.999306333333333">
uments and highlights (mean and standard devia-
tion). A minority of documents are transcripts of
interviews and speeches, and can be very long; this
accounts for the very large standard deviation.
Two examples of a news story and its associ-
ated highlights, are shown in Table 1. As can be
seen, the highlights are written in a compressed,
almost telegraphic manner. Articles, auxiliaries
and forms of the verb be are often deleted. Com-
pression is also achieved through paraphrasing,
e.g., substitutions and reorderings. For example,
the document sentence “The poll found 69 percent
of blacks said King’s vision has been fulfilled.” is
rephrased in the highlight as “69 percent of blacks
polled say Martin Luther King Jr’s vision real-
ized.”. In general, there is a fair amount of lexi-
cal overlap between document sentences and high-
lights (42.44%) but the correspondence between
document sentences and highlights is not always
one-to-one. In the first example in Table 1, the sec-
ond paragraph gives rise to two highlights. Also
note that the highlights need not form a coherent
summary, each of them is relatively stand-alone,
and there is little co-referencing between them.
</bodyText>
<page confidence="0.993655">
567
</page>
<figureCaption confidence="0.9441535">
Figure 1: An example phrase structure (a) and dependency (b) tree for the sentence “But whites remain
less optimistic, the survey found.”.
</figureCaption>
<figure confidence="0.997278405405405">
CC
NP
VP
DT
NN
VBD
But
NNS
VBP
ADJP
the
survey
found
whites
remain
RBR
JJ
optimistic
survey
less
the
whites remain less
(b)
(a)
optimistic
S
TOP
found
S
,
,
NP
VP
cop
det
.
.
</figure>
<bodyText confidence="0.999754411764706">
In order to train and evaluate the model pre-
sented in the following sections we created a cor-
pus of document-highlight pairs (approximately
9,000) which we downloaded from the CNN.com
website.1 The articles were randomly sampled
from the years 2007–2009 and covered a wide
range of topics such as business, crime, health,
politics, showbiz, etc. The majority were news
articles, but the set also contained a mixture of
editorials, commentary, interviews and reviews.
Some overview statistics of the corpus are shown
in Table 2. Overall, we observe a high degree of
compression both at the document and sentence
level. The highlights summary tends to be ten
times shorter than the corresponding article. Fur-
thermore, individual highlights have almost half
the length of document sentences.
</bodyText>
<sectionHeader confidence="0.990764" genericHeader="method">
4 Modeling
</sectionHeader>
<bodyText confidence="0.999984357142857">
The objective of our model is to create the most in-
formative story highlights possible, subject to con-
straints relating to sentence length, overall sum-
mary length, topic coverage, and grammaticality.
These constraints are global in their scope, and
cannot be adequately satisfied by optimizing each
one of them individually. Our approach therefore
uses an ILP formulation which will provide a glob-
ally optimal solution, and which can be efficiently
solved using standard optimization tools. Specif-
ically, the model selects phrases from which to
form the highlights, and each highlight is created
from a single sentence through phrase deletion.
The model operates on parse trees augmented with
</bodyText>
<footnote confidence="0.902743">
1The corpus is available from http://homepages.inf.
ed.ac.uk/mlap/resources/index.html.
</footnote>
<bodyText confidence="0.998929542857143">
dependency labels. We first describe how we ob-
tain this representation and then move on to dis-
cuss the model in more detail.
Sentence Representation We obtain syntactic
information by parsing every sentence twice, once
with a phrase structure parser and once with a
dependency parser. The phrase structure and
dependency-based representations for the sen-
tence “But whites remain less optimistic, the sur-
vey found.” (from Table 1) are shown in Fig-
ures 1(a) and 1(b), respectively.
We then combine the output from the two
parsers, by mapping the dependencies to the edges
of the phrase structure tree in a greedy fashion,
shown in Figure 2(a). Starting at the top node of
the dependency graph, we choose a node i and a
dependency arc to node j. We locate the corre-
sponding words i and j on the phrase structure
tree, and locate their nearest shared ancestor p. We
assign the label of the dependency i —* j to the first
unlabeled edge from p to j in the phrase structure
tree. Edges assigned with dependency labels are
shown as dashed lines. These edges are important
to our formulation, as they will be represented by
binary decision variables in the ILP. Further edges
from p to j, and all the edges from p to i, are
marked as fixed and shown as solid lines. In this
way we keep the correct ordering of leaf nodes.
Finally, leaf nodes are merged into parent phrases,
until each phrase node contains a minimum of two
tokens, shown in Figure 2(b). Because of this min-
imum length rule, it is possible for a merged node
to be a clause rather than a phrase, but in the sub-
sequent description we will use the term phrase
rather loosely to describe any merged leaf node.
</bodyText>
<page confidence="0.993501">
568
</page>
<figureCaption confidence="0.992488">
Figure 2: Dependencies are mapped onto phrase structure tree (a) and leaf nodes are merged with parent
phrases (b).
</figureCaption>
<figure confidence="0.999643536585366">
S
.
NP
,
VP
But
NNS
whites
(a)
JJ
optimistic
RBR
less
ADJP
the
survey
found
,
NP
VBD
S
nsubj
VBP
remain
VBD
(b)
.
S
But whites remain
less optimistic
,
the survey
nsubj
,
found.
NP
CC
VP
DT
NN
S
</figure>
<bodyText confidence="0.998734944444445">
ILP model The merged phrase structure tree,
such as shown in Figure 2(b), is the actual input to
our model. Each phrase in the document is given
a salience score. We obtain these scores from the
output of a supervised machine learning algorithm
that predicts for each phrase whether it should be
included in the highlights or not (see Section 5 for
details). Let S be the set of sentences in a docu-
ment, P be the set of phrases, and Ps ⊂ P be the
set of phrases in each sentences ∈ S. T is the set
of words with the highest tf.idf scores, and Pt ⊂ P
is the set of phrases containing the token t ∈ T .
Let fi denote the salience score for phrase i, deter-
mined by the machine learning algorithm, and li is
its length in tokens.
We use a vector of binary variables x ∈ {0,1}|P|
to indicate if each phrase is to be within a high-
light. These are either top-level nodes in our
merged tree representation, or nodes whose edge
to the parent has a dependency label (the dashed
lines). Referring to our example in Figure 2(b), bi-
nary variables would be allocated to the top-level S
node, the child S node and the NP node. The vec-
tor of auxiliary binary variables y ∈ {0,1}|S |in-
dicates from which sentences the chosen phrases
come (see Equations (1i) and (1j)). Let the sets
Di ⊂ P, ∀i ∈ P capture the phrase dependency in-
formation for each phrase i, where each set Di
contains the phrases that depend on the presence
of i. Our objective function function is given in
Equation (1a): it is the sum of the salience scores
of all the phrases chosen to form the highlights
of a given document, subject to the constraints
in Equations (1b)–(1j). The latter provide a nat-
ural way of describing the requirements the output
must meet.
</bodyText>
<equation confidence="0.413502">
lixi ≥ Lmys ∀s ∈ S (1d)
∀t ∈ T
∀i ∈ P, j ∈ Di
∀s ∈ S,i ∈ Ps
∀i ∈ P
∀s ∈ S.
</equation>
<bodyText confidence="0.981701181818182">
Constraint (1b) ensures that the generated high-
lights do not exceed a total budget of LT tokens.
This constraint may vary depending on the appli-
cation or task at hand. Highlights on a small screen
device would presumably be shorter than high-
lights for news articles on the web. It is also possi-
ble to set the length of each highlight to be within
the range [Lm,LM]. Constraints (1c) and (1d) en-
force this requirement. In particular, these con-
straints stop highlights formed from sentences at
the beginning of the document (which tend to have
</bodyText>
<figure confidence="0.9412525">
max E fixi (1a)
x i∈P
s.t. E lixi ≤ LT (1b)
i∈P
E lixi ≤ LMys ∀s ∈ S (1c)
i∈Ps
E
i∈Ps
E xi ≥ 1
i∈Pt
xj → xi
xi → ys
E ys ≤ NS
s∈S
xi ∈ {0,1}
ys ∈ {0,1}
</figure>
<page confidence="0.995224">
569
</page>
<bodyText confidence="0.9989359375">
high salience scores) from being too long. Equa-
tion (1e) is a set-covering constraint, requiring that
each of the words in T appears at least once in
the highlights. We assume that words with high
tf.idf scores reveal to a certain extent what the doc-
ument is about. Constraint (1e) ensures that some
of these words will be present in the highlights.
We enforce grammatical correctness through
constraint (1f) which ensures that the phrase de-
pendencies are respected. Phrases that depend on
phrase i are contained in the set Di. Variable xi is
true, and therefore phrase i will be included, if any
of its dependents xj E Di are true. The phrase de-
pendency constraints, contained in the set Di and
enforced by (1f), are the result of two rules based
on the typed dependency information:
</bodyText>
<listItem confidence="0.981845066666667">
1. Any child node j of the current node i,
whose connecting edge i —* j is of type
nsubj (nominal subject), nsubjpass (passive
nominal subject), dobj (direct object), pobj
(preposition object), infmod (infinitival mod-
ifier), ccomp (clausal complement), xcomp
(open clausal complement), measure (mea-
sure phrase modifier) and num (numeric
modifier) must be included if node i is in-
cluded.
2. The parent node p of the current node i must
always be included if i is, unless the edge
p —* i is of type ccomp (clausal complement)
or advcl (adverbial clause), in which case it
is possible to include i without including p.
</listItem>
<bodyText confidence="0.999972461538462">
Consider again the example in Figure 2(b).
There are only two possible outputs from this sen-
tence. If the phrase “the survey” is chosen, then
the parent node “found” will be included, and from
our first rule the ccomp phrase must also be in-
cluded, which results in the output: “But whites
remain less optimistic, the survey found.” If, on
the other hand, the clause “But whites remain less
optimistic” is chosen, then due to our second rule
there is no constraint that forces the parent phrase
“found” to be included in the highlights. Without
other factors influencing the decision, this would
give the output: “But whites remain less opti-
mistic.” We can see from this example that encod-
ing the possible outputs as decisions on branches
of the phrase structure tree provides a more com-
pact representation of many options than would be
possible with an explicit enumeration of all possi-
ble compressions. Which output is chosen (if any)
depends on the scores of the phrases involved, and
the influence of the other constraints.
Constraint (1g) tells the ILP to create a highlight
if one of its constituent phrases is chosen. Finally,
note that a maximum number of highlights NS can
be set beforehand, and (1h) limits the highlights to
this maximum.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="method">
5 Experimental Set-up
</sectionHeader>
<bodyText confidence="0.999786619047619">
Training We obtained phrase-based salience
scores using a supervised machine learning algo-
rithm. 210 document-highlight pairs were chosen
randomly from our corpus (see Section 3). Two
annotators manually aligned the highlights and
document sentences. Specifically, each sentence
in the document was assigned one of three align-
ment labels: must be in the summary (1), could be
in the summary (2), and is not in the summary (3).
The annotators were asked to label document sen-
tences whose content was identical to the high-
lights as “must be in the summary”, sentences
with partially overlapping content as “could be in
the summary” and the remainder as “should not
be in the summary”. Inter-annotator agreement
was .82 (p &lt; 0.01, using Spearman’s p rank corre-
lation). The mapping of sentence labels to phrase
labels was unsupervised: if the phrase came from
a sentence labeled (1), and there was a unigram
overlap (excluding stop words) between the phrase
and any of the original highlights, we marked this
phrase with a positive label. All other phrases
were marked negative.
Our feature set comprised surface features such
as sentence and paragraph position information,
POS tags, unigram and bigram overlap with the
title, and whether high-scoring tf.idf words were
present in the phrase (66 features in total). The
210 documents produced a training set of 42,684
phrases (3,334 positive and 39,350 negative). We
learned the feature weights with a linear SVM,
using the software SVM-OOPS (Woodsend and
Gondzio, 2009). This tool gave us directly the fea-
ture weights as well as support vector values, and
it allowed different penalties to be applied to pos-
itive and negative misclassifications, enabling us
to compensate for the unbalanced data set. The
penalty hyper-parameters chosen were the ones
that gave the best F-scores, using 10-fold valida-
tion.
Highlight generation We generated highlights
for a test set of 600 documents. We created and
</bodyText>
<page confidence="0.988219">
570
</page>
<bodyText confidence="0.999980808510639">
solved an ILP for each document. Sentences were
first tokenized to separate words and punctuation,
then parsed to obtain phrases and dependencies as
described in Section 4 using the Stanford parser
(Klein and Manning, 2003). For each phrase, fea-
tures were extracted and salience scores calcu-
lated from the feature weights determined through
SVM training. The distance from the SVM hyper-
plane represents the salience score. The ILP model
(see Equation (1)) was parametrized as follows:
the maximum number of highlights NS was 4,
the overall limit on length LT was 75 tokens, the
length of each highlight was in the range of [8,28]
tokens, and the topic coverage set T contained the
top 5 tf.idf words. These parameters were chosen
to capture the properties seen in the majority of
the training set; they were also relaxed enough to
allow a feasible solution of the ILP model (with
hard constraints) for all the documents in the test
set. To solve the ILP model we used the ZIB Opti-
mization Suite software (Achterberg, 2007; Koch,
2004; Wunderling, 1996). The solution was con-
verted into highlights by concatenating the chosen
leaf nodes in order. The ILP problems we created
had on average 290 binary variables and 380 con-
straints. The mean solve time was 0.03 seconds.
Summarization In order to examine the gen-
erality of our model and compare with previous
work, we also evaluated our system on a vanilla
summarization task. Specifically, we used the
same model (trained on the CNN corpus) to gen-
erate summaries for the DUC-2002 corpus2. We
report results on the entire dataset and on a subset
containing 140 documents. This is the same parti-
tion used by Martins and Smith (2009) to evaluate
their ILP model.3
Baselines We compared the output of our model
to two baselines. The first one simply selects
the “leading” three sentences from each document
(without any compression). The second baseline
is the output of a sentence-based ILP model, sim-
ilar to our own, but simpler. The model is given
in (2). The binary decision variables x ∈ {0,1}|S|
now represent sentences, and fi the salience score
for each sentence. The objective again is to max-
imize the total score, but now subject only to
tf.idf coverage (2b) and a limit on the number of
</bodyText>
<footnote confidence="0.98829575">
2http://www-nlpir.nist.gov/projects/duc/
guidelines/2002.html
3We are grateful to Andr´e Martins for providing us with
details of their testing partition.
</footnote>
<bodyText confidence="0.997176170731707">
highlights (2c) which we set to 3. There are no
sentence length or grammaticality constraints, as
there is no sentence compression.
xi ∈ {0,1} ∀i ∈ S. (2d)
The SVM was trained with the same features used
to obtain phrase-based salience scores, but with
sentence-level labels (labels (1) and (2) positive,
(3) negative).
Evaluation We evaluated summarization qual-
ity using ROUGE (Lin and Hovy, 2003). For the
highlight generation task, the original CNN high-
lights were used as the reference. We report un-
igram overlap (ROUGE-1) as a means of assess-
ing informativeness and the longest common sub-
sequence (ROUGE-L) as a means of assessing flu-
ency.
In addition, we evaluated the generated high-
lights by eliciting human judgments. Participants
were presented with a news article and its corre-
sponding highlights and were asked to rate the lat-
ter along three dimensions: informativeness (do
the highlights represent the article’s main topics?),
grammaticality (are they fluent?), and verbosity
(are they overly wordy and repetitive?). The sub-
jects used a seven point rating scale. An ideal
system would receive high numbers for grammat-
icality and informativeness and a low number for
verbosity. We randomly selected nine documents
from the test set and generated highlights with our
model and the sentence-based ILP baseline. We
also included the original highlights as a gold stan-
dard. We thus obtained ratings for 27 (9 × 3)
document-highlights pairs.4 The study was con-
ducted over the Internet using WebExp (Keller
et al., 2009) and was completed by 34 volunteers,
all self reported native English speakers.
With regard to the summarization task, follow-
ing Martins and Smith (2009), we used ROUGE-1
and ROUGE-2 to evaluate our system’s output.
We also report results with ROUGE-L. Each doc-
ument in the DUC-2002 dataset is paired with
</bodyText>
<footnote confidence="0.9460475">
4A Latin square design ensured that subjects did not see
two different highlights of the same document.
</footnote>
<figure confidence="0.778343">
max E fixi (2a)
x i∈S
s.t. E xi ≥ 1 ∀t ∈ T (2b)
i∈St
E xi ≤ NS (2c)
i∈S
571
Recall Precision F-score Recall Precision F-score
Rouge-1 Rouge-L
</figure>
<figureCaption confidence="0.923486333333333">
Figure 3: ROUGE-1 and ROUGE-L results for
phrase-based ILP model and two baselines, with
error bars showing 95% confidence levels.
</figureCaption>
<bodyText confidence="0.9900625">
a human-authored summary (approximately 100
words) which we used as reference.
</bodyText>
<sectionHeader confidence="0.999773" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999793551724138">
We report results on the highlight generation task
in Figure 3 with ROUGE-1 and ROUGE-L (error
bars indicate the 95% confidence interval). In
both measures, the ILP sentence baseline has the
best recall, while the ILP phrase model has the
best precision (the differences are statistically sig-
nificant). F-score is higher for the phrase-based
system but not significantly. This can be at-
tributed to the fact that the longer output of the
sentence-based model makes the recall task easier.
Average highlight lengths are shown in Table 3,
and the compression rates they represent. Our
phrase model achieves the highest compression
rates, whereas the sentence-based model tends to
select long sentences even in comparison to the
lead baseline. The sentence ILP model outper-
forms the lead baseline with respect to recall but
not precision or F-score. The phrase ILP achieves
a significantly better F-score over the lead baseline
with both ROUGE-1 and ROUGE-L.
The results of our human evaluation study are
summarized in Table 4. There was no sta-
tistically significant difference in the grammat-
icality between the highlights generated by the
phrase ILP system and the original CNN high-
lights (means differences were compared using a
Post-hoc Tukey test). The grammaticality of the
sentence ILP was significantly higher overall as
no compression took place (a &lt; 0.05). All three
</bodyText>
<table confidence="0.999334666666667">
s toks/s C.R.
Articles 36.5 22.2 f 4.0 100%
CNN highlights 3.5 13.3 f 1.7 5.8%
ILP phrase 3.8 18.0 f 2.9 8.4%
Leading-3 3.0 25.1 f 7.4 9.3%
ILP sentence 3.0 31.3 f 7.9 11.6%
</table>
<tableCaption confidence="0.826235">
Table 3: Comparison of output lengths: number
of sentences, tokens per sentence, and compres-
sion rate, for CNN articles, their highlights, the
ILP phrase model, and two baselines.
</tableCaption>
<table confidence="0.999932">
Model Grammar Importance Verbosity
CNN highlights 4.85 4.88 3.14
ILP sentence 6.41 5.47 3.97
ILP phrase 5.53 5.05 3.38
</table>
<tableCaption confidence="0.861055">
Table 4: Average human ratings for original CNN
highlights, and two ILP models.
</tableCaption>
<bodyText confidence="0.999719172413793">
systems performed on a similar level with respect
to importance (differences in the means were not
significant). The highlights created by the sen-
tence ILP were considered significantly more ver-
bose (a &lt; 0.05) than those created by the phrase-
based system and the CNN abstractors. Overall,
the highlights generated by the phrase ILP model
were not significantly different from those written
by humans. They capture the same content as the
full sentences, albeit in a more succinct manner.
Table 5 shows the output of the phrase-based sys-
tem for the documents in Table 1.
Our results on the complete DUC-2002 cor-
pus are shown in Table 6. Despite the fact that
our model has not been optimized for the original
task of generating 100-word summaries—instead
it is trained on the CNN corpus, and generates
highlights—the results are comparable with the
best of the original participants5 in each of the
ROUGE measures. Our model is also significantly
better than the lead sentences baseline.
Table 7 presents our results on the same
DUC-2002 partition (140 documents) used by
Martins and Smith (2009). The phrase ILP model
achieves a significantly better F-score (for both
ROUGE-1 and ROUGE-2) over the lead baseline,
the sentence ILP model, and Martins and Smith.
We should point out that the latter model is not a
straw man. It significantly outperforms a pipeline
</bodyText>
<footnote confidence="0.984952666666667">
5The list of participants is on page 12 of the slides
available from http://duc.nist.gov/pubs/2002slides/
overview.02.pdf.
</footnote>
<figure confidence="0.993033333333333">
Leading-3
ILP sentence
ILP phrase
Score 0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
</figure>
<page confidence="0.849225">
572
</page>
<listItem confidence="0.912735466666667">
• More than two-thirds of African-Americans believe
Martin Luther King Jr.’s vision for race relations has
been fulfilled.
• 69 percent of blacks said King’s vision has been ful-
filled in the more than 45 years since his 1963 ‘I have a
dream’ speech.
• But whites remain less optimistic, the survey found.
• A Florida man is using billboards with an image of the
burning World Trade Center to encourage votes for a
Republican presidential candidate, drawing criticism.
• ‘Please Don’t Vote for a Democrat’ reads the type over
the picture of the twin towers.
• Mike Meehan said former President Clinton should
have put a stop to Osama bin Laden and al Qaeda be-
fore 9/11.
</listItem>
<tableCaption confidence="0.9929215">
Table 5: Generated highlights for the stories in Ta-
ble 1 using the phrase ILP model.
</tableCaption>
<table confidence="0.99992225">
Participant ROUGE-1 ROUGE-2 ROUGE-L
28 0.464 0.222 0.432
19 0.459 0.221 0.431
21 0.458 0.216 0.426
29 0.449 0.208 0.419
27 0.445 0.209 0.417
Leading-3 0.416 0.200 0.390
ILP phrase 0.454 0.213 0.428
</table>
<tableCaption confidence="0.866285">
Table 6: ROUGE results on the complete
</tableCaption>
<bodyText confidence="0.9876294">
DUC-2002 corpus, including the top 5 original
participants. For all results, the 95% confidence
interval is ±0.008.
approach that first creates extracts and then com-
presses them. Furthermore, as a standalone sen-
tence compression system it yields state of the art
performance, comparable to McDonald’s (2006)
discriminative model and superior to Hedge Trim-
mer (Zajic et al., 2007), a less sophisticated deter-
ministic system.
</bodyText>
<sectionHeader confidence="0.999242" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.998748833333333">
In this paper we proposed a joint content selection
and compression model for single-document sum-
marization. A key aspect of our approach is the
representation of content by phrases rather than
entire sentences. Salient phrases are selected to
form the summary. Grammaticality, length and
coverage requirements are encoded as constraints
in an integer linear program. Applying the model
to the generation of “story highlights” (and sin-
gle document summaries) shows that it is a vi-
able alternative to extraction-based systems. Both
ROUGE scores and the results of our human study
</bodyText>
<table confidence="0.9983488">
ROUGE-1 ROUGE-2 ROUGE-L
Leading-3 .400 ± .018 .184 ± .015 .374 ± .017
M&amp;S (2009) .403 ± .076 .180 ± .076 —
ILP sentence .430 ± .014 .191 ± .015 .401 ± .014
ILP phrase .445 ± .014 .200 ± .014 .419 ± .014
</table>
<tableCaption confidence="0.951523">
Table 7: ROUGE results on DUC-2002 cor-
</tableCaption>
<bodyText confidence="0.992568954545455">
pus (140 documents). —: only ROUGE-1 and
ROUGE-2 results are given in Martins and Smith
(2009).
confirm that our system manages to create sum-
maries at a high compression rate and yet maintain
the informativeness and grammaticality of a com-
petitive extractive system. The model itself is rel-
atively simple and knowledge-lean, and achieves
good performance without reference to any re-
sources outside the corpus collection.
Future extensions are many and varied. An ob-
vious next step is to examine how the model gen-
eralizes to other domains and text genres. Al-
though coherence is not so much of an issue for
highlights, it certainly plays a role when generat-
ing standard summaries. The ILP model can be
straightforwardly augmented with discourse con-
straints similar to those proposed in Clarke and
Lapata (2007). We would also like to generalize
the model to arbitrary rewrite operations, as our
results indicate that compression rates are likely
to improve with more sophisticated paraphrasing.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999953">
We would like to thank Andreas Grothey and
members of ICCS at the School of Informatics for
the valuable discussions and comments through-
out this work. We acknowledge the support of EP-
SRC through project grants EP/F055765/1 and
GR/T04540/01.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9984443125">
Achterberg, Tobias. 2007. Constraint Integer Programming.
Ph.D. thesis, Technische Universit¨at Berlin.
Banko, Michele, Vibhu O. Mittal, and Michael J. Witbrock.
2000. Headline generation based on statistical translation.
In Proceedings of the 38th ACL. Hong Kong, pages 318–
325.
Clarke, James and Mirella Lapata. 2007. Modelling com-
pression with discourse constraints. In Proceedings of
EMNLP-CoNLL. Prague, Czech Republic, pages 1–11.
Clarke, James and Mirella Lapata. 2008. Global inference
for sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Research
31:399–429.
Cohn, Trevor and Mirella Lapata. 2009. Sentence compres-
sion as tree transduction. Journal of Artificial Intelligence
Research 34:637–674.
</reference>
<page confidence="0.986565">
573
</page>
<reference confidence="0.999753073684211">
Conroy, J. M., J. D. Schlesinger, J. Goldstein, and D. P.
O’Leary. 2004. Left-brain/right-brain multi-document
summarization. In DUC 2004 Conference Proceedings.
Daum´e III, Hal. 2006. Practical Structured Learning Tech-
niques for Natural Language Processing. Ph.D. thesis,
University of Southern California.
Daum´e III, Hal and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of the
40th ACL. Philadelphia, PA, pages 449–456.
Dorr, Bonnie, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to headline
generation. In Proceedings of the HLT-NAACL 2003
Workshop on Text Summarization. pages 1–8.
Jing, Hongyan. 2000. Sentence reduction for automatic text
summarization. In Proceedings of the 6th ANLP. Seattle,
WA, pages 310–315.
Jing, Hongyan. 2002. Using hidden Markov modeling to de-
compose human-written summaries. Computational Lin-
guistics 28(4):527–544.
Jing, Hongyan and Kathleen McKeown. 2000. Cut and paste
summarization. In Proceedings of the 1st NAACL. Seattle,
WA, pages 178–185.
Keller, Frank, Subahshini Gunasekharan, Neil Mayo, and
Martin Corley. 2009. Timing accuracy of web experi-
ments: A case study using the WebExp software package.
Behavior Research Methods 41(1):1–12.
Klein, Dan and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of the 41st ACL. Sap-
poro, Japan, pages 423–430.
Knight, Kevin and Daniel Marcu. 2002. Summarization be-
yond sentence extraction: a probabilistic approach to sen-
tence compression. Artificial Intelligence 139(1):91–107.
Koch, Thorsten. 2004. Rapid Mathematical Prototyping.
Ph.D. thesis, Technische Universit¨at Berlin.
Kupiec, Julian, Jan O. Pedersen, and Francine Chen. 1995. A
trainable document summarizer. In Proceedings of SIGIR-
95. Seattle, WA, pages 68–73.
Lin, Chin-Yew. 2003. Improving summarization performance
by sentence compression — a pilot study. In Proceed-
ings of the 6th International Workshop on Information Re-
trieval with Asian Languages. Sapporo, Japan, pages 1–8.
Lin, Chin-Yew and Eduard H. Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statistics.
In Proceedings of HLT NAACL. Edmonton, Canada, pages
71–78.
Mani, Inderjeet. 2001. Automatic Summarization. John Ben-
jamins Pub Co.
Martins, Andr´e and Noah A. Smith. 2009. Summarization
with a joint model for sentence extraction and compres-
sion. In Proceedings of the Workshop on Integer Linear
Programming for Natural Language Processing. Boulder,
Colorado, pages 1–9.
McDonald, Ryan. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of the
11th EACL. Trento, Italy.
McDonald, Ryan. 2007. A study of global inference algo-
rithms in multi-document summarization. In Proceedings
of the 29th ECIR. Rome, Italy.
Nenkova, Ani. 2005. Automatic text summarization of
newswire: Lessons learned from the Document Under-
standing Conference. In Proceedings of the 20th AAAI.
Pittsburgh, PA, pages 1436–1441.
Siddharthan, Advaith, Ani Nenkova, and Kathleen McKe-
own. 2004. Syntactic simplification for improving con-
tent selection in multi-document summarization. In Pro-
ceedings of the 20th International Conference on Compu-
tational Linguistics (COLING 2004). pages 896–902.
Sparck Jones, Karen. 1999. Automatic summarizing: Factors
and directions. In Inderjeet Mani and Mark T. Maybury,
editors, Advances in Automatic Text Summarization, MIT
Press, Cambridge, pages 1–33.
Svore, Krysta, Lucy Vanderwende, and Christopher Burges.
2007. Enhancing single-document summarization by
combining RankNet and third-party sources. In Proceed-
ings of EMNLP-CoNLL. Prague, Czech Republic, pages
448–457.
Wan, Stephen and C´ecile Paris. 2008. Experimenting with
clause segmentation for text summarization. In Proceed-
ings of the 1st TAC. Gaithersburg, MD.
Witten, Ian H., Gordon Paynter, Eibe Frank, Carl Gutwin, and
Craig G. Nevill-Manning. 1999. KEA: Practical automatic
keyphrase extraction. In Proceedings of the 4th ACM
International Conference on Digital Libraries. Berkeley,
CA, pages 254–255.
Woodsend, Kristian and Jacek Gondzio. 2009. Exploiting
separability in large-scale linear support vector machine
training. Computational Optimization and Applications.
Wunderling, Roland. 1996. Paralleler und objektorientierter
Simplex-Algorithmus. Ph.D. thesis, Technische Univer-
sit¨at Berlin.
Zajic, David, Bonnie J. Door, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization tasks.
Information Processing Management Special Issue on
Summarization 43(6):1549–1570.
</reference>
<page confidence="0.998251">
574
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.818326">
<title confidence="0.999666">Automatic Generation of Story Highlights</title>
<author confidence="0.891528">Woodsend Lapata</author>
<affiliation confidence="0.999898">School of Informatics, University of Edinburgh</affiliation>
<address confidence="0.990053">Edinburgh EH8 9AB, United Kingdom</address>
<abstract confidence="0.9963286">In this paper we present a joint content selection and compression model for single-document summarization. The model operates over a phrase-based representation of the source document which we obtain by merging information from PCFG parse trees and dependency graphs. Using an integer linear programming formulation, the model learns to select and combine phrases subject to length, coverage and grammar constraints. We evaluate the approach on the task of generating “story highlights”—a small number of brief, self-contained sentences that allow readers to quickly gather information on news stories. Experimental results show that the model’s output is comparable to human-written highlights in terms of both grammaticality and content.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tobias Achterberg</author>
</authors>
<title>Constraint Integer Programming.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Technische Universit¨at Berlin.</institution>
<contexts>
<context position="25366" citStr="Achterberg, 2007" startWordPosition="4211" endWordPosition="4212">e salience score. The ILP model (see Equation (1)) was parametrized as follows: the maximum number of highlights NS was 4, the overall limit on length LT was 75 tokens, the length of each highlight was in the range of [8,28] tokens, and the topic coverage set T contained the top 5 tf.idf words. These parameters were chosen to capture the properties seen in the majority of the training set; they were also relaxed enough to allow a feasible solution of the ILP model (with hard constraints) for all the documents in the test set. To solve the ILP model we used the ZIB Optimization Suite software (Achterberg, 2007; Koch, 2004; Wunderling, 1996). The solution was converted into highlights by concatenating the chosen leaf nodes in order. The ILP problems we created had on average 290 binary variables and 380 constraints. The mean solve time was 0.03 seconds. Summarization In order to examine the generality of our model and compare with previous work, we also evaluated our system on a vanilla summarization task. Specifically, we used the same model (trained on the CNN corpus) to generate summaries for the DUC-2002 corpus2. We report results on the entire dataset and on a subset containing 140 documents. T</context>
</contexts>
<marker>Achterberg, 2007</marker>
<rawString>Achterberg, Tobias. 2007. Constraint Integer Programming. Ph.D. thesis, Technische Universit¨at Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Vibhu O Mittal</author>
<author>Michael J Witbrock</author>
</authors>
<title>Headline generation based on statistical translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th ACL. Hong Kong,</booktitle>
<pages>318--325</pages>
<contexts>
<context position="10841" citStr="Banko et al., 2000" startWordPosition="1691" endWordPosition="1694">He said a Republican president would have done so. • Billboards use image from 9/11 to encourage GOP votes. • 9/11 image wrong for ad, say Florida political parties. • Floridian praises President Bush, says ex-President Clinton failed to stop al Qaeda. Table 1: Two example CNN news articles, showing the title and the first few paragraphs, and below, the original highlights that accompanied each story. guage model. Lastly, our model is more compact, has fewer parameters, and does not require two training procedures. Our approach bears some resemblance to headline generation (Dorr et al., 2003; Banko et al., 2000), although we output several sentences rather than a single one. Headline generation models typically extract individual words from a document to produce a very short summary, whereas we extract phrases and ensure that they are combined into grammatical sentences through our ILP constraints. Svore et al. (2007) were the first to foreground the highlight generation task which we adopt as an evaluation testbed for our model. Their approach is however a purely extractive one. Using an algorithm based on neural networks and third-party resources (e.g., news query logs and Wikipedia entries) they r</context>
</contexts>
<marker>Banko, Mittal, Witbrock, 2000</marker>
<rawString>Banko, Michele, Vibhu O. Mittal, and Michael J. Witbrock. 2000. Headline generation based on statistical translation. In Proceedings of the 38th ACL. Hong Kong, pages 318– 325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Modelling compression with discourse constraints.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<pages>1--11</pages>
<location>Prague, Czech Republic,</location>
<marker>Clarke, Lapata, 2007</marker>
<rawString>Clarke, James and Mirella Lapata. 2007. Modelling compression with discourse constraints. In Proceedings of EMNLP-CoNLL. Prague, Czech Republic, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research</journal>
<pages>31--399</pages>
<contexts>
<context position="2865" citStr="Clarke and Lapata 2008" startWordPosition="414" endWordPosition="417">g some of the problems associated with extractive summarization. The task is commonly expressed as a word deletion problem. It involves creating a short grammatical summary of a single sentence, by removing elements that are considered extraneous, while retaining the most important information (Knight and Marcu, 2002). Interfacing extractive summarization with a sentence compression module could improve the conciseness of the generated summaries and render them more informative (Jing, 2000; Lin, 2003; Zajic et al., 2007). Despite the bulk of work on sentence compression and summarization (see Clarke and Lapata 2008 and Mani 2001 for overviews) only a handful of approaches attempt to do both in a joint model (Daum´e III and Marcu, 2002; Daum´e III, 2006; Lin, 2003; Martins and Smith, 2009). One reason for this might be the performance of sentence compression systems which falls short of attaining grammaticality levels of human output. For example, Clarke and Lapata (2008) evaluate a range of state-of-the-art compression systems across different domains and show that machine generated compressions are consistently perceived as worse than the human gold standard. Another reason is the summarization objecti</context>
<context position="7860" citStr="Clarke and Lapata (2008)" startWordPosition="1194" endWordPosition="1197">rom a document. Using a noisy-channel model, Daum´e III and Marcu (2002) exploit the discourse structure of a document and the syntactic structure of its sentences in order to decide which constituents to drop but also which discourse units are unimportant. Martins and Smith (2009) formulate a joint sentence extraction and summarization model as an ILP. The latter optimizes an objective function consisting of two parts: an extraction component, essentially a non-greedy variant of maximal marginal relevance (McDonald, 2007), and a sentence compression component, a more compact reformulation of Clarke and Lapata (2008) based on the output of a dependency parser. Compression and extraction models are trained separately in a max-margin framework and then interpolated. In the context of multi-document summarization, Daum´e III’s (2006) vine-growth model creates summaries incrementally, either by starting a new sentence or by growing already existing ones. Our own work is closest to Martins and Smith (2009). We also develop an ILP-based compression and summarization model, however, several key differences set our approach apart. Firstly, content selection is performed at the phrase rather than sentence level. S</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>Clarke, James and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. Journal of Artificial Intelligence Research 31:399–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression as tree transduction.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research</journal>
<pages>34--637</pages>
<contexts>
<context position="11716" citStr="Cohn and Lapata 2009" startWordPosition="1833" endWordPosition="1836">sentences through our ILP constraints. Svore et al. (2007) were the first to foreground the highlight generation task which we adopt as an evaluation testbed for our model. Their approach is however a purely extractive one. Using an algorithm based on neural networks and third-party resources (e.g., news query logs and Wikipedia entries) they rank sentences and select the three highest scoring ones as story highlights. In contrast, we aim to generate rather than extract highlights. As a first step we focus on deleting extraneous material, but other more sophisticated rewrite operations (e.g., Cohn and Lapata 2009) could be incorporated into our framework. 3 The Task Given a document, we aim to produce three or four short sentences covering its main topics, much like the “Story Highlights” accompanying the (online) CNN news articles. CNN highlights are written by humans; we aim to do this automatically. Documents Highlights Sentences 37.2 f 39.6 3.5 f 0.5 Tokens 795.0 f 744.8 47.0 f 9.6 Tokens/sentence 22.4 f 4.2 13.3 f 1.7 Table 2: Overview statistics on the corpus of documents and highlights (mean and standard deviation). A minority of documents are transcripts of interviews and speeches, and can be v</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>Cohn, Trevor and Mirella Lapata. 2009. Sentence compression as tree transduction. Journal of Artificial Intelligence Research 34:637–674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Conroy</author>
<author>J D Schlesinger</author>
<author>J Goldstein</author>
<author>D P O’Leary</author>
</authors>
<title>Left-brain/right-brain multi-document summarization.</title>
<date>2004</date>
<booktitle>In DUC 2004 Conference Proceedings.</booktitle>
<marker>Conroy, Schlesinger, Goldstein, O’Leary, 2004</marker>
<rawString>Conroy, J. M., J. D. Schlesinger, J. Goldstein, and D. P. O’Leary. 2004. Left-brain/right-brain multi-document summarization. In DUC 2004 Conference Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daum´e Hal</author>
</authors>
<title>Practical Structured Learning Techniques for Natural Language Processing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern California.</institution>
<marker>Hal, 2006</marker>
<rawString>Daum´e III, Hal. 2006. Practical Structured Learning Techniques for Natural Language Processing. Ph.D. thesis, University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daum´e Hal</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel model for document compression.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th ACL.</booktitle>
<pages>449--456</pages>
<location>Philadelphia, PA,</location>
<marker>Hal, Marcu, 2002</marker>
<rawString>Daum´e III, Hal and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proceedings of the 40th ACL. Philadelphia, PA, pages 449–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
<author>David Zajic</author>
<author>Richard Schwartz</author>
</authors>
<title>Hedge trimmer: A parse-and-trim approach to headline generation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003 Workshop on Text Summarization.</booktitle>
<pages>1--8</pages>
<contexts>
<context position="10820" citStr="Dorr et al., 2003" startWordPosition="1687" endWordPosition="1690">Qaeda before 9/11. He said a Republican president would have done so. • Billboards use image from 9/11 to encourage GOP votes. • 9/11 image wrong for ad, say Florida political parties. • Floridian praises President Bush, says ex-President Clinton failed to stop al Qaeda. Table 1: Two example CNN news articles, showing the title and the first few paragraphs, and below, the original highlights that accompanied each story. guage model. Lastly, our model is more compact, has fewer parameters, and does not require two training procedures. Our approach bears some resemblance to headline generation (Dorr et al., 2003; Banko et al., 2000), although we output several sentences rather than a single one. Headline generation models typically extract individual words from a document to produce a very short summary, whereas we extract phrases and ensure that they are combined into grammatical sentences through our ILP constraints. Svore et al. (2007) were the first to foreground the highlight generation task which we adopt as an evaluation testbed for our model. Their approach is however a purely extractive one. Using an algorithm based on neural networks and third-party resources (e.g., news query logs and Wiki</context>
</contexts>
<marker>Dorr, Zajic, Schwartz, 2003</marker>
<rawString>Dorr, Bonnie, David Zajic, and Richard Schwartz. 2003. Hedge trimmer: A parse-and-trim approach to headline generation. In Proceedings of the HLT-NAACL 2003 Workshop on Text Summarization. pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th ANLP.</booktitle>
<pages>310--315</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="2737" citStr="Jing, 2000" startWordPosition="394" endWordPosition="395">ons, deletions, or reorderings. Sentence compression is often regarded as a promising first step towards ameliorating some of the problems associated with extractive summarization. The task is commonly expressed as a word deletion problem. It involves creating a short grammatical summary of a single sentence, by removing elements that are considered extraneous, while retaining the most important information (Knight and Marcu, 2002). Interfacing extractive summarization with a sentence compression module could improve the conciseness of the generated summaries and render them more informative (Jing, 2000; Lin, 2003; Zajic et al., 2007). Despite the bulk of work on sentence compression and summarization (see Clarke and Lapata 2008 and Mani 2001 for overviews) only a handful of approaches attempt to do both in a joint model (Daum´e III and Marcu, 2002; Daum´e III, 2006; Lin, 2003; Martins and Smith, 2009). One reason for this might be the performance of sentence compression systems which falls short of attaining grammaticality levels of human output. For example, Clarke and Lapata (2008) evaluate a range of state-of-the-art compression systems across different domains and show that machine gene</context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>Jing, Hongyan. 2000. Sentence reduction for automatic text summarization. In Proceedings of the 6th ANLP. Seattle, WA, pages 310–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Using hidden Markov modeling to decompose human-written summaries.</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="2060" citStr="Jing, 2002" startWordPosition="296" endWordPosition="297">tter is beyond the capabilities of current NLP technology, most work today focuses on extractive summarization, where a summary is created simply by identifying and subsequently concatenating the most important sentences in a document. Without a great deal of linguistic analysis, it is possible to create summaries for a wide range of documents. Unfortunately, extracts are often documents of low readability and text quality and contain much redundant information. This is in marked contrast with hand-written summaries which often combine several pieces of information from the original document (Jing, 2002) and exhibit many rewrite operations such as substitutions, insertions, deletions, or reorderings. Sentence compression is often regarded as a promising first step towards ameliorating some of the problems associated with extractive summarization. The task is commonly expressed as a word deletion problem. It involves creating a short grammatical summary of a single sentence, by removing elements that are considered extraneous, while retaining the most important information (Knight and Marcu, 2002). Interfacing extractive summarization with a sentence compression module could improve the concis</context>
</contexts>
<marker>Jing, 2002</marker>
<rawString>Jing, Hongyan. 2002. Using hidden Markov modeling to decompose human-written summaries. Computational Linguistics 28(4):527–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen McKeown</author>
</authors>
<title>Cut and paste summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st NAACL.</booktitle>
<pages>178--185</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="6268" citStr="Jing and McKeown (2000)" startWordPosition="950" endWordPosition="953">on which is often formalized as a classification task (Kupiec et al., 1995). Given appropriately annotated training data, a binary classifier learns to predict for each document sentence if it is worth extracting. Surface-level features are typically used to single out important sentences. These include the presence of certain key phrases, the position of a sentence in the original document, the sentence length, the words in the title, the presence of proper nouns, etc. (Mani, 2001; Sparck Jones, 1999). Relatively little work has focused on extraction methods for units smaller than sentences. Jing and McKeown (2000) first extract sentences, then remove redundant phrases, and use (manual) recombination rules to produce coherent output. Wan and Paris (2008) segment sentences heuristically into clauses before extraction takes place, and show that this improves summarization quality. In the context of multiple-document summarization, heuristics have also been used to remove parenthetical information (Conroy et al., 2004; Siddharthan et al., 2004). Witten et al. (1999) (among others) extract keyphrases to capture the gist of the document, without however attempting to reconstruct sentences or generate summari</context>
</contexts>
<marker>Jing, McKeown, 2000</marker>
<rawString>Jing, Hongyan and Kathleen McKeown. 2000. Cut and paste summarization. In Proceedings of the 1st NAACL. Seattle, WA, pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Subahshini Gunasekharan</author>
<author>Neil Mayo</author>
<author>Martin Corley</author>
</authors>
<title>Timing accuracy of web experiments: A case study using the WebExp software package.</title>
<date>2009</date>
<journal>Behavior Research Methods</journal>
<volume>41</volume>
<issue>1</issue>
<contexts>
<context position="28268" citStr="Keller et al., 2009" startWordPosition="4682" endWordPosition="4685">sent the article’s main topics?), grammaticality (are they fluent?), and verbosity (are they overly wordy and repetitive?). The subjects used a seven point rating scale. An ideal system would receive high numbers for grammaticality and informativeness and a low number for verbosity. We randomly selected nine documents from the test set and generated highlights with our model and the sentence-based ILP baseline. We also included the original highlights as a gold standard. We thus obtained ratings for 27 (9 × 3) document-highlights pairs.4 The study was conducted over the Internet using WebExp (Keller et al., 2009) and was completed by 34 volunteers, all self reported native English speakers. With regard to the summarization task, following Martins and Smith (2009), we used ROUGE-1 and ROUGE-2 to evaluate our system’s output. We also report results with ROUGE-L. Each document in the DUC-2002 dataset is paired with 4A Latin square design ensured that subjects did not see two different highlights of the same document. max E fixi (2a) x i∈S s.t. E xi ≥ 1 ∀t ∈ T (2b) i∈St E xi ≤ NS (2c) i∈S 571 Recall Precision F-score Recall Precision F-score Rouge-1 Rouge-L Figure 3: ROUGE-1 and ROUGE-L results for phrase</context>
</contexts>
<marker>Keller, Gunasekharan, Mayo, Corley, 2009</marker>
<rawString>Keller, Frank, Subahshini Gunasekharan, Neil Mayo, and Martin Corley. 2009. Timing accuracy of web experiments: A case study using the WebExp software package. Behavior Research Methods 41(1):1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st ACL.</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="24568" citStr="Klein and Manning, 2003" startWordPosition="4071" endWordPosition="4074">feature weights as well as support vector values, and it allowed different penalties to be applied to positive and negative misclassifications, enabling us to compensate for the unbalanced data set. The penalty hyper-parameters chosen were the ones that gave the best F-scores, using 10-fold validation. Highlight generation We generated highlights for a test set of 600 documents. We created and 570 solved an ILP for each document. Sentences were first tokenized to separate words and punctuation, then parsed to obtain phrases and dependencies as described in Section 4 using the Stanford parser (Klein and Manning, 2003). For each phrase, features were extracted and salience scores calculated from the feature weights determined through SVM training. The distance from the SVM hyperplane represents the salience score. The ILP model (see Equation (1)) was parametrized as follows: the maximum number of highlights NS was 4, the overall limit on length LT was 75 tokens, the length of each highlight was in the range of [8,28] tokens, and the topic coverage set T contained the top 5 tf.idf words. These parameters were chosen to capture the properties seen in the majority of the training set; they were also relaxed en</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, Dan and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st ACL. Sapporo, Japan, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: a probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="2562" citStr="Knight and Marcu, 2002" startWordPosition="367" endWordPosition="370">ast with hand-written summaries which often combine several pieces of information from the original document (Jing, 2002) and exhibit many rewrite operations such as substitutions, insertions, deletions, or reorderings. Sentence compression is often regarded as a promising first step towards ameliorating some of the problems associated with extractive summarization. The task is commonly expressed as a word deletion problem. It involves creating a short grammatical summary of a single sentence, by removing elements that are considered extraneous, while retaining the most important information (Knight and Marcu, 2002). Interfacing extractive summarization with a sentence compression module could improve the conciseness of the generated summaries and render them more informative (Jing, 2000; Lin, 2003; Zajic et al., 2007). Despite the bulk of work on sentence compression and summarization (see Clarke and Lapata 2008 and Mani 2001 for overviews) only a handful of approaches attempt to do both in a joint model (Daum´e III and Marcu, 2002; Daum´e III, 2006; Lin, 2003; Martins and Smith, 2009). One reason for this might be the performance of sentence compression systems which falls short of attaining grammatica</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Knight, Kevin and Daniel Marcu. 2002. Summarization beyond sentence extraction: a probabilistic approach to sentence compression. Artificial Intelligence 139(1):91–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Koch</author>
</authors>
<date>2004</date>
<booktitle>Rapid Mathematical Prototyping. Ph.D. thesis,</booktitle>
<institution>Technische Universit¨at Berlin.</institution>
<contexts>
<context position="25378" citStr="Koch, 2004" startWordPosition="4213" endWordPosition="4214">The ILP model (see Equation (1)) was parametrized as follows: the maximum number of highlights NS was 4, the overall limit on length LT was 75 tokens, the length of each highlight was in the range of [8,28] tokens, and the topic coverage set T contained the top 5 tf.idf words. These parameters were chosen to capture the properties seen in the majority of the training set; they were also relaxed enough to allow a feasible solution of the ILP model (with hard constraints) for all the documents in the test set. To solve the ILP model we used the ZIB Optimization Suite software (Achterberg, 2007; Koch, 2004; Wunderling, 1996). The solution was converted into highlights by concatenating the chosen leaf nodes in order. The ILP problems we created had on average 290 binary variables and 380 constraints. The mean solve time was 0.03 seconds. Summarization In order to examine the generality of our model and compare with previous work, we also evaluated our system on a vanilla summarization task. Specifically, we used the same model (trained on the CNN corpus) to generate summaries for the DUC-2002 corpus2. We report results on the entire dataset and on a subset containing 140 documents. This is the s</context>
</contexts>
<marker>Koch, 2004</marker>
<rawString>Koch, Thorsten. 2004. Rapid Mathematical Prototyping. Ph.D. thesis, Technische Universit¨at Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan O Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of SIGIR95.</booktitle>
<pages>68--73</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="5720" citStr="Kupiec et al., 1995" startWordPosition="866" endWordPosition="869"> Importantly, they represent the gist of the entire document and thus often differ substantially from the first n sentences in the article (Svore et al., 2007). They are also highly compressed, written in a telegraphic style and thus provide an excellent testbed for models that generate compressed summaries. Experimental results show that our model’s output is comparable to hand-written highlights both in terms of grammaticality and informativeness. 2 Related work Much effort in automatic summarization has been devoted to sentence extraction which is often formalized as a classification task (Kupiec et al., 1995). Given appropriately annotated training data, a binary classifier learns to predict for each document sentence if it is worth extracting. Surface-level features are typically used to single out important sentences. These include the presence of certain key phrases, the position of a sentence in the original document, the sentence length, the words in the title, the presence of proper nouns, etc. (Mani, 2001; Sparck Jones, 1999). Relatively little work has focused on extraction methods for units smaller than sentences. Jing and McKeown (2000) first extract sentences, then remove redundant phra</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Kupiec, Julian, Jan O. Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of SIGIR95. Seattle, WA, pages 68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Improving summarization performance by sentence compression — a pilot study.</title>
<date>2003</date>
<booktitle>In Proceedings of the 6th International Workshop on Information Retrieval with Asian Languages.</booktitle>
<pages>1--8</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="2748" citStr="Lin, 2003" startWordPosition="396" endWordPosition="397">ns, or reorderings. Sentence compression is often regarded as a promising first step towards ameliorating some of the problems associated with extractive summarization. The task is commonly expressed as a word deletion problem. It involves creating a short grammatical summary of a single sentence, by removing elements that are considered extraneous, while retaining the most important information (Knight and Marcu, 2002). Interfacing extractive summarization with a sentence compression module could improve the conciseness of the generated summaries and render them more informative (Jing, 2000; Lin, 2003; Zajic et al., 2007). Despite the bulk of work on sentence compression and summarization (see Clarke and Lapata 2008 and Mani 2001 for overviews) only a handful of approaches attempt to do both in a joint model (Daum´e III and Marcu, 2002; Daum´e III, 2006; Lin, 2003; Martins and Smith, 2009). One reason for this might be the performance of sentence compression systems which falls short of attaining grammaticality levels of human output. For example, Clarke and Lapata (2008) evaluate a range of state-of-the-art compression systems across different domains and show that machine generated compr</context>
<context position="7061" citStr="Lin 2003" startWordPosition="1069" endWordPosition="1070">ses before extraction takes place, and show that this improves summarization quality. In the context of multiple-document summarization, heuristics have also been used to remove parenthetical information (Conroy et al., 2004; Siddharthan et al., 2004). Witten et al. (1999) (among others) extract keyphrases to capture the gist of the document, without however attempting to reconstruct sentences or generate summaries. A few previous approaches have attempted to interface sentence compression with summarization. A straightforward way to achieve this is by adopting a two-stage architecture (e.g., Lin 2003) where the sentences are first extracted and then compressed or the other way round. Other work implements a joint model where words and sentences are deleted simultaneously from a document. Using a noisy-channel model, Daum´e III and Marcu (2002) exploit the discourse structure of a document and the syntactic structure of its sentences in order to decide which constituents to drop but also which discourse units are unimportant. Martins and Smith (2009) formulate a joint sentence extraction and summarization model as an ILP. The latter optimizes an objective function consisting of two parts: a</context>
</contexts>
<marker>Lin, 2003</marker>
<rawString>Lin, Chin-Yew. 2003. Improving summarization performance by sentence compression — a pilot study. In Proceedings of the 6th International Workshop on Information Retrieval with Asian Languages. Sapporo, Japan, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT NAACL.</booktitle>
<pages>71--78</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="27141" citStr="Lin and Hovy, 2003" startWordPosition="4501" endWordPosition="4504">subject only to tf.idf coverage (2b) and a limit on the number of 2http://www-nlpir.nist.gov/projects/duc/ guidelines/2002.html 3We are grateful to Andr´e Martins for providing us with details of their testing partition. highlights (2c) which we set to 3. There are no sentence length or grammaticality constraints, as there is no sentence compression. xi ∈ {0,1} ∀i ∈ S. (2d) The SVM was trained with the same features used to obtain phrase-based salience scores, but with sentence-level labels (labels (1) and (2) positive, (3) negative). Evaluation We evaluated summarization quality using ROUGE (Lin and Hovy, 2003). For the highlight generation task, the original CNN highlights were used as the reference. We report unigram overlap (ROUGE-1) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency. In addition, we evaluated the generated highlights by eliciting human judgments. Participants were presented with a news article and its corresponding highlights and were asked to rate the latter along three dimensions: informativeness (do the highlights represent the article’s main topics?), grammaticality (are they fluent?), and verbosity (are they </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Lin, Chin-Yew and Eduard H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of HLT NAACL. Edmonton, Canada, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Automatic Summarization.</title>
<date>2001</date>
<publisher>John Benjamins Pub Co.</publisher>
<contexts>
<context position="2879" citStr="Mani 2001" startWordPosition="419" endWordPosition="420">ciated with extractive summarization. The task is commonly expressed as a word deletion problem. It involves creating a short grammatical summary of a single sentence, by removing elements that are considered extraneous, while retaining the most important information (Knight and Marcu, 2002). Interfacing extractive summarization with a sentence compression module could improve the conciseness of the generated summaries and render them more informative (Jing, 2000; Lin, 2003; Zajic et al., 2007). Despite the bulk of work on sentence compression and summarization (see Clarke and Lapata 2008 and Mani 2001 for overviews) only a handful of approaches attempt to do both in a joint model (Daum´e III and Marcu, 2002; Daum´e III, 2006; Lin, 2003; Martins and Smith, 2009). One reason for this might be the performance of sentence compression systems which falls short of attaining grammaticality levels of human output. For example, Clarke and Lapata (2008) evaluate a range of state-of-the-art compression systems across different domains and show that machine generated compressions are consistently perceived as worse than the human gold standard. Another reason is the summarization objective itself. If </context>
<context position="6131" citStr="Mani, 2001" startWordPosition="932" endWordPosition="933">mmaticality and informativeness. 2 Related work Much effort in automatic summarization has been devoted to sentence extraction which is often formalized as a classification task (Kupiec et al., 1995). Given appropriately annotated training data, a binary classifier learns to predict for each document sentence if it is worth extracting. Surface-level features are typically used to single out important sentences. These include the presence of certain key phrases, the position of a sentence in the original document, the sentence length, the words in the title, the presence of proper nouns, etc. (Mani, 2001; Sparck Jones, 1999). Relatively little work has focused on extraction methods for units smaller than sentences. Jing and McKeown (2000) first extract sentences, then remove redundant phrases, and use (manual) recombination rules to produce coherent output. Wan and Paris (2008) segment sentences heuristically into clauses before extraction takes place, and show that this improves summarization quality. In the context of multiple-document summarization, heuristics have also been used to remove parenthetical information (Conroy et al., 2004; Siddharthan et al., 2004). Witten et al. (1999) (amon</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Mani, Inderjeet. 2001. Automatic Summarization. John Benjamins Pub Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing.</booktitle>
<pages>1--9</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="3042" citStr="Martins and Smith, 2009" startWordPosition="446" endWordPosition="449"> a single sentence, by removing elements that are considered extraneous, while retaining the most important information (Knight and Marcu, 2002). Interfacing extractive summarization with a sentence compression module could improve the conciseness of the generated summaries and render them more informative (Jing, 2000; Lin, 2003; Zajic et al., 2007). Despite the bulk of work on sentence compression and summarization (see Clarke and Lapata 2008 and Mani 2001 for overviews) only a handful of approaches attempt to do both in a joint model (Daum´e III and Marcu, 2002; Daum´e III, 2006; Lin, 2003; Martins and Smith, 2009). One reason for this might be the performance of sentence compression systems which falls short of attaining grammaticality levels of human output. For example, Clarke and Lapata (2008) evaluate a range of state-of-the-art compression systems across different domains and show that machine generated compressions are consistently perceived as worse than the human gold standard. Another reason is the summarization objective itself. If our goal is to summarize news articles, then we may be better off selecting the first n sentences of the document. This “lead” baseline may err on the side of verb</context>
<context position="7518" citStr="Martins and Smith (2009)" startWordPosition="1142" endWordPosition="1145">hes have attempted to interface sentence compression with summarization. A straightforward way to achieve this is by adopting a two-stage architecture (e.g., Lin 2003) where the sentences are first extracted and then compressed or the other way round. Other work implements a joint model where words and sentences are deleted simultaneously from a document. Using a noisy-channel model, Daum´e III and Marcu (2002) exploit the discourse structure of a document and the syntactic structure of its sentences in order to decide which constituents to drop but also which discourse units are unimportant. Martins and Smith (2009) formulate a joint sentence extraction and summarization model as an ILP. The latter optimizes an objective function consisting of two parts: an extraction component, essentially a non-greedy variant of maximal marginal relevance (McDonald, 2007), and a sentence compression component, a more compact reformulation of Clarke and Lapata (2008) based on the output of a dependency parser. Compression and extraction models are trained separately in a max-margin framework and then interpolated. In the context of multi-document summarization, Daum´e III’s (2006) vine-growth model creates summaries inc</context>
<context position="26024" citStr="Martins and Smith (2009)" startWordPosition="4321" endWordPosition="4324">he solution was converted into highlights by concatenating the chosen leaf nodes in order. The ILP problems we created had on average 290 binary variables and 380 constraints. The mean solve time was 0.03 seconds. Summarization In order to examine the generality of our model and compare with previous work, we also evaluated our system on a vanilla summarization task. Specifically, we used the same model (trained on the CNN corpus) to generate summaries for the DUC-2002 corpus2. We report results on the entire dataset and on a subset containing 140 documents. This is the same partition used by Martins and Smith (2009) to evaluate their ILP model.3 Baselines We compared the output of our model to two baselines. The first one simply selects the “leading” three sentences from each document (without any compression). The second baseline is the output of a sentence-based ILP model, similar to our own, but simpler. The model is given in (2). The binary decision variables x ∈ {0,1}|S| now represent sentences, and fi the salience score for each sentence. The objective again is to maximize the total score, but now subject only to tf.idf coverage (2b) and a limit on the number of 2http://www-nlpir.nist.gov/projects/</context>
<context position="28421" citStr="Martins and Smith (2009)" startWordPosition="4706" endWordPosition="4709"> point rating scale. An ideal system would receive high numbers for grammaticality and informativeness and a low number for verbosity. We randomly selected nine documents from the test set and generated highlights with our model and the sentence-based ILP baseline. We also included the original highlights as a gold standard. We thus obtained ratings for 27 (9 × 3) document-highlights pairs.4 The study was conducted over the Internet using WebExp (Keller et al., 2009) and was completed by 34 volunteers, all self reported native English speakers. With regard to the summarization task, following Martins and Smith (2009), we used ROUGE-1 and ROUGE-2 to evaluate our system’s output. We also report results with ROUGE-L. Each document in the DUC-2002 dataset is paired with 4A Latin square design ensured that subjects did not see two different highlights of the same document. max E fixi (2a) x i∈S s.t. E xi ≥ 1 ∀t ∈ T (2b) i∈St E xi ≤ NS (2c) i∈S 571 Recall Precision F-score Recall Precision F-score Rouge-1 Rouge-L Figure 3: ROUGE-1 and ROUGE-L results for phrase-based ILP model and two baselines, with error bars showing 95% confidence levels. a human-authored summary (approximately 100 words) which we used as re</context>
<context position="32060" citStr="Martins and Smith (2009)" startWordPosition="5309" endWordPosition="5312">succinct manner. Table 5 shows the output of the phrase-based system for the documents in Table 1. Our results on the complete DUC-2002 corpus are shown in Table 6. Despite the fact that our model has not been optimized for the original task of generating 100-word summaries—instead it is trained on the CNN corpus, and generates highlights—the results are comparable with the best of the original participants5 in each of the ROUGE measures. Our model is also significantly better than the lead sentences baseline. Table 7 presents our results on the same DUC-2002 partition (140 documents) used by Martins and Smith (2009). The phrase ILP model achieves a significantly better F-score (for both ROUGE-1 and ROUGE-2) over the lead baseline, the sentence ILP model, and Martins and Smith. We should point out that the latter model is not a straw man. It significantly outperforms a pipeline 5The list of participants is on page 12 of the slides available from http://duc.nist.gov/pubs/2002slides/ overview.02.pdf. Leading-3 ILP sentence ILP phrase Score 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 572 • More than two-thirds of African-Americans believe Martin Luther King Jr.’s vision for race relations has been fulfilled. • 6</context>
<context position="34877" citStr="Martins and Smith (2009)" startWordPosition="5781" endWordPosition="5784">overage requirements are encoded as constraints in an integer linear program. Applying the model to the generation of “story highlights” (and single document summaries) shows that it is a viable alternative to extraction-based systems. Both ROUGE scores and the results of our human study ROUGE-1 ROUGE-2 ROUGE-L Leading-3 .400 ± .018 .184 ± .015 .374 ± .017 M&amp;S (2009) .403 ± .076 .180 ± .076 — ILP sentence .430 ± .014 .191 ± .015 .401 ± .014 ILP phrase .445 ± .014 .200 ± .014 .419 ± .014 Table 7: ROUGE results on DUC-2002 corpus (140 documents). —: only ROUGE-1 and ROUGE-2 results are given in Martins and Smith (2009). confirm that our system manages to create summaries at a high compression rate and yet maintain the informativeness and grammaticality of a competitive extractive system. The model itself is relatively simple and knowledge-lean, and achieves good performance without reference to any resources outside the corpus collection. Future extensions are many and varied. An obvious next step is to examine how the model generalizes to other domains and text genres. Although coherence is not so much of an issue for highlights, it certainly plays a role when generating standard summaries. The ILP model c</context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>Martins, Andr´e and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing. Boulder, Colorado, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic constraints.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th EACL.</booktitle>
<location>Trento, Italy.</location>
<marker>McDonald, 2006</marker>
<rawString>McDonald, Ryan. 2006. Discriminative sentence compression with soft syntactic constraints. In Proceedings of the 11th EACL. Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 29th ECIR.</booktitle>
<location>Rome, Italy.</location>
<contexts>
<context position="7764" citStr="McDonald, 2007" startWordPosition="1181" endWordPosition="1182">er work implements a joint model where words and sentences are deleted simultaneously from a document. Using a noisy-channel model, Daum´e III and Marcu (2002) exploit the discourse structure of a document and the syntactic structure of its sentences in order to decide which constituents to drop but also which discourse units are unimportant. Martins and Smith (2009) formulate a joint sentence extraction and summarization model as an ILP. The latter optimizes an objective function consisting of two parts: an extraction component, essentially a non-greedy variant of maximal marginal relevance (McDonald, 2007), and a sentence compression component, a more compact reformulation of Clarke and Lapata (2008) based on the output of a dependency parser. Compression and extraction models are trained separately in a max-margin framework and then interpolated. In the context of multi-document summarization, Daum´e III’s (2006) vine-growth model creates summaries incrementally, either by starting a new sentence or by growing already existing ones. Our own work is closest to Martins and Smith (2009). We also develop an ILP-based compression and summarization model, however, several key differences set our app</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>McDonald, Ryan. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings of the 29th ECIR. Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
</authors>
<title>Automatic text summarization of newswire: Lessons learned from the Document Understanding Conference.</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th AAAI.</booktitle>
<pages>1436--1441</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="3781" citStr="Nenkova, 2005" startWordPosition="567" endWordPosition="568">levels of human output. For example, Clarke and Lapata (2008) evaluate a range of state-of-the-art compression systems across different domains and show that machine generated compressions are consistently perceived as worse than the human gold standard. Another reason is the summarization objective itself. If our goal is to summarize news articles, then we may be better off selecting the first n sentences of the document. This “lead” baseline may err on the side of verbosity but at least will be grammatical, and it has indeed proved extremely hard to outperform by more sophisticated methods (Nenkova, 2005). In this paper we propose a model for sum565 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565–574, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics marization that incorporates compression into the task. A key insight in our approach is to formulate summarization as a phrase rather than sentence extraction problem. Compression falls naturally out of this formulation as only phrases deemed important should appear in the summary. Obviously, our output summaries must meet additional requirements such as sentence </context>
</contexts>
<marker>Nenkova, 2005</marker>
<rawString>Nenkova, Ani. 2005. Automatic text summarization of newswire: Lessons learned from the Document Understanding Conference. In Proceedings of the 20th AAAI. Pittsburgh, PA, pages 1436–1441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>Syntactic simplification for improving content selection in multi-document summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING</booktitle>
<pages>896--902</pages>
<contexts>
<context position="6703" citStr="Siddharthan et al., 2004" startWordPosition="1013" endWordPosition="1017">title, the presence of proper nouns, etc. (Mani, 2001; Sparck Jones, 1999). Relatively little work has focused on extraction methods for units smaller than sentences. Jing and McKeown (2000) first extract sentences, then remove redundant phrases, and use (manual) recombination rules to produce coherent output. Wan and Paris (2008) segment sentences heuristically into clauses before extraction takes place, and show that this improves summarization quality. In the context of multiple-document summarization, heuristics have also been used to remove parenthetical information (Conroy et al., 2004; Siddharthan et al., 2004). Witten et al. (1999) (among others) extract keyphrases to capture the gist of the document, without however attempting to reconstruct sentences or generate summaries. A few previous approaches have attempted to interface sentence compression with summarization. A straightforward way to achieve this is by adopting a two-stage architecture (e.g., Lin 2003) where the sentences are first extracted and then compressed or the other way round. Other work implements a joint model where words and sentences are deleted simultaneously from a document. Using a noisy-channel model, Daum´e III and Marcu (</context>
</contexts>
<marker>Siddharthan, Nenkova, McKeown, 2004</marker>
<rawString>Siddharthan, Advaith, Ani Nenkova, and Kathleen McKeown. 2004. Syntactic simplification for improving content selection in multi-document summarization. In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004). pages 896–902.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparck Jones</author>
<author>Karen</author>
</authors>
<title>Automatic summarizing: Factors and directions.</title>
<date>1999</date>
<booktitle>In Inderjeet Mani and</booktitle>
<pages>1--33</pages>
<editor>Mark T. Maybury, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge,</location>
<marker>Jones, Karen, 1999</marker>
<rawString>Sparck Jones, Karen. 1999. Automatic summarizing: Factors and directions. In Inderjeet Mani and Mark T. Maybury, editors, Advances in Automatic Text Summarization, MIT Press, Cambridge, pages 1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krysta Svore</author>
<author>Lucy Vanderwende</author>
<author>Christopher Burges</author>
</authors>
<title>Enhancing single-document summarization by combining RankNet and third-party sources.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<pages>448--457</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5259" citStr="Svore et al., 2007" startWordPosition="796" endWordPosition="799">s through the use of integer linear programming (ILP), a well-studied optimization framework that is able to search the entire solution space efficiently. We apply our model to the task of generating highlights for a single document. Examples of CNN news articles with human-authored highlights are shown in Table 1. Highlights give a brief overview of the article to allow readers to quickly gather information on stories, and usually appear as bullet points. Importantly, they represent the gist of the entire document and thus often differ substantially from the first n sentences in the article (Svore et al., 2007). They are also highly compressed, written in a telegraphic style and thus provide an excellent testbed for models that generate compressed summaries. Experimental results show that our model’s output is comparable to hand-written highlights both in terms of grammaticality and informativeness. 2 Related work Much effort in automatic summarization has been devoted to sentence extraction which is often formalized as a classification task (Kupiec et al., 1995). Given appropriately annotated training data, a binary classifier learns to predict for each document sentence if it is worth extracting. </context>
<context position="11153" citStr="Svore et al. (2007)" startWordPosition="1740" endWordPosition="1743">nd the first few paragraphs, and below, the original highlights that accompanied each story. guage model. Lastly, our model is more compact, has fewer parameters, and does not require two training procedures. Our approach bears some resemblance to headline generation (Dorr et al., 2003; Banko et al., 2000), although we output several sentences rather than a single one. Headline generation models typically extract individual words from a document to produce a very short summary, whereas we extract phrases and ensure that they are combined into grammatical sentences through our ILP constraints. Svore et al. (2007) were the first to foreground the highlight generation task which we adopt as an evaluation testbed for our model. Their approach is however a purely extractive one. Using an algorithm based on neural networks and third-party resources (e.g., news query logs and Wikipedia entries) they rank sentences and select the three highest scoring ones as story highlights. In contrast, we aim to generate rather than extract highlights. As a first step we focus on deleting extraneous material, but other more sophisticated rewrite operations (e.g., Cohn and Lapata 2009) could be incorporated into our frame</context>
</contexts>
<marker>Svore, Vanderwende, Burges, 2007</marker>
<rawString>Svore, Krysta, Lucy Vanderwende, and Christopher Burges. 2007. Enhancing single-document summarization by combining RankNet and third-party sources. In Proceedings of EMNLP-CoNLL. Prague, Czech Republic, pages 448–457.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>C´ecile Paris</author>
</authors>
<title>Experimenting with clause segmentation for text summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 1st TAC.</booktitle>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="6410" citStr="Wan and Paris (2008)" startWordPosition="972" endWordPosition="975">rns to predict for each document sentence if it is worth extracting. Surface-level features are typically used to single out important sentences. These include the presence of certain key phrases, the position of a sentence in the original document, the sentence length, the words in the title, the presence of proper nouns, etc. (Mani, 2001; Sparck Jones, 1999). Relatively little work has focused on extraction methods for units smaller than sentences. Jing and McKeown (2000) first extract sentences, then remove redundant phrases, and use (manual) recombination rules to produce coherent output. Wan and Paris (2008) segment sentences heuristically into clauses before extraction takes place, and show that this improves summarization quality. In the context of multiple-document summarization, heuristics have also been used to remove parenthetical information (Conroy et al., 2004; Siddharthan et al., 2004). Witten et al. (1999) (among others) extract keyphrases to capture the gist of the document, without however attempting to reconstruct sentences or generate summaries. A few previous approaches have attempted to interface sentence compression with summarization. A straightforward way to achieve this is by</context>
</contexts>
<marker>Wan, Paris, 2008</marker>
<rawString>Wan, Stephen and C´ecile Paris. 2008. Experimenting with clause segmentation for text summarization. In Proceedings of the 1st TAC. Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Gordon Paynter</author>
<author>Eibe Frank</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>KEA: Practical automatic keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the 4th ACM International Conference on Digital Libraries.</booktitle>
<pages>254--255</pages>
<location>Berkeley, CA,</location>
<contexts>
<context position="6725" citStr="Witten et al. (1999)" startWordPosition="1018" endWordPosition="1021">er nouns, etc. (Mani, 2001; Sparck Jones, 1999). Relatively little work has focused on extraction methods for units smaller than sentences. Jing and McKeown (2000) first extract sentences, then remove redundant phrases, and use (manual) recombination rules to produce coherent output. Wan and Paris (2008) segment sentences heuristically into clauses before extraction takes place, and show that this improves summarization quality. In the context of multiple-document summarization, heuristics have also been used to remove parenthetical information (Conroy et al., 2004; Siddharthan et al., 2004). Witten et al. (1999) (among others) extract keyphrases to capture the gist of the document, without however attempting to reconstruct sentences or generate summaries. A few previous approaches have attempted to interface sentence compression with summarization. A straightforward way to achieve this is by adopting a two-stage architecture (e.g., Lin 2003) where the sentences are first extracted and then compressed or the other way round. Other work implements a joint model where words and sentences are deleted simultaneously from a document. Using a noisy-channel model, Daum´e III and Marcu (2002) exploit the disc</context>
</contexts>
<marker>Witten, Paynter, Frank, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Witten, Ian H., Gordon Paynter, Eibe Frank, Carl Gutwin, and Craig G. Nevill-Manning. 1999. KEA: Practical automatic keyphrase extraction. In Proceedings of the 4th ACM International Conference on Digital Libraries. Berkeley, CA, pages 254–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Jacek Gondzio</author>
</authors>
<title>Exploiting separability in large-scale linear support vector machine training. Computational Optimization and Applications.</title>
<date>2009</date>
<contexts>
<context position="23911" citStr="Woodsend and Gondzio, 2009" startWordPosition="3966" endWordPosition="3969">was a unigram overlap (excluding stop words) between the phrase and any of the original highlights, we marked this phrase with a positive label. All other phrases were marked negative. Our feature set comprised surface features such as sentence and paragraph position information, POS tags, unigram and bigram overlap with the title, and whether high-scoring tf.idf words were present in the phrase (66 features in total). The 210 documents produced a training set of 42,684 phrases (3,334 positive and 39,350 negative). We learned the feature weights with a linear SVM, using the software SVM-OOPS (Woodsend and Gondzio, 2009). This tool gave us directly the feature weights as well as support vector values, and it allowed different penalties to be applied to positive and negative misclassifications, enabling us to compensate for the unbalanced data set. The penalty hyper-parameters chosen were the ones that gave the best F-scores, using 10-fold validation. Highlight generation We generated highlights for a test set of 600 documents. We created and 570 solved an ILP for each document. Sentences were first tokenized to separate words and punctuation, then parsed to obtain phrases and dependencies as described in Sect</context>
</contexts>
<marker>Woodsend, Gondzio, 2009</marker>
<rawString>Woodsend, Kristian and Jacek Gondzio. 2009. Exploiting separability in large-scale linear support vector machine training. Computational Optimization and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Wunderling</author>
</authors>
<title>Paralleler und objektorientierter Simplex-Algorithmus.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Technische Universit¨at Berlin.</institution>
<contexts>
<context position="25397" citStr="Wunderling, 1996" startWordPosition="4215" endWordPosition="4216">l (see Equation (1)) was parametrized as follows: the maximum number of highlights NS was 4, the overall limit on length LT was 75 tokens, the length of each highlight was in the range of [8,28] tokens, and the topic coverage set T contained the top 5 tf.idf words. These parameters were chosen to capture the properties seen in the majority of the training set; they were also relaxed enough to allow a feasible solution of the ILP model (with hard constraints) for all the documents in the test set. To solve the ILP model we used the ZIB Optimization Suite software (Achterberg, 2007; Koch, 2004; Wunderling, 1996). The solution was converted into highlights by concatenating the chosen leaf nodes in order. The ILP problems we created had on average 290 binary variables and 380 constraints. The mean solve time was 0.03 seconds. Summarization In order to examine the generality of our model and compare with previous work, we also evaluated our system on a vanilla summarization task. Specifically, we used the same model (trained on the CNN corpus) to generate summaries for the DUC-2002 corpus2. We report results on the entire dataset and on a subset containing 140 documents. This is the same partition used </context>
</contexts>
<marker>Wunderling, 1996</marker>
<rawString>Wunderling, Roland. 1996. Paralleler und objektorientierter Simplex-Algorithmus. Ph.D. thesis, Technische Universit¨at Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Zajic</author>
<author>Bonnie J Door</author>
<author>Jimmy Lin</author>
<author>Richard Schwartz</author>
</authors>
<title>Multi-candidate reduction: Sentence compression as a tool for document summarization tasks.</title>
<date>2007</date>
<journal>Information Processing Management Special Issue on Summarization</journal>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="2769" citStr="Zajic et al., 2007" startWordPosition="398" endWordPosition="401">derings. Sentence compression is often regarded as a promising first step towards ameliorating some of the problems associated with extractive summarization. The task is commonly expressed as a word deletion problem. It involves creating a short grammatical summary of a single sentence, by removing elements that are considered extraneous, while retaining the most important information (Knight and Marcu, 2002). Interfacing extractive summarization with a sentence compression module could improve the conciseness of the generated summaries and render them more informative (Jing, 2000; Lin, 2003; Zajic et al., 2007). Despite the bulk of work on sentence compression and summarization (see Clarke and Lapata 2008 and Mani 2001 for overviews) only a handful of approaches attempt to do both in a joint model (Daum´e III and Marcu, 2002; Daum´e III, 2006; Lin, 2003; Martins and Smith, 2009). One reason for this might be the performance of sentence compression systems which falls short of attaining grammaticality levels of human output. For example, Clarke and Lapata (2008) evaluate a range of state-of-the-art compression systems across different domains and show that machine generated compressions are consisten</context>
<context position="33904" citStr="Zajic et al., 2007" startWordPosition="5616" endWordPosition="5619">l. Participant ROUGE-1 ROUGE-2 ROUGE-L 28 0.464 0.222 0.432 19 0.459 0.221 0.431 21 0.458 0.216 0.426 29 0.449 0.208 0.419 27 0.445 0.209 0.417 Leading-3 0.416 0.200 0.390 ILP phrase 0.454 0.213 0.428 Table 6: ROUGE results on the complete DUC-2002 corpus, including the top 5 original participants. For all results, the 95% confidence interval is ±0.008. approach that first creates extracts and then compresses them. Furthermore, as a standalone sentence compression system it yields state of the art performance, comparable to McDonald’s (2006) discriminative model and superior to Hedge Trimmer (Zajic et al., 2007), a less sophisticated deterministic system. 7 Conclusions In this paper we proposed a joint content selection and compression model for single-document summarization. A key aspect of our approach is the representation of content by phrases rather than entire sentences. Salient phrases are selected to form the summary. Grammaticality, length and coverage requirements are encoded as constraints in an integer linear program. Applying the model to the generation of “story highlights” (and single document summaries) shows that it is a viable alternative to extraction-based systems. Both ROUGE scor</context>
</contexts>
<marker>Zajic, Door, Lin, Schwartz, 2007</marker>
<rawString>Zajic, David, Bonnie J. Door, Jimmy Lin, and Richard Schwartz. 2007. Multi-candidate reduction: Sentence compression as a tool for document summarization tasks. Information Processing Management Special Issue on Summarization 43(6):1549–1570.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>