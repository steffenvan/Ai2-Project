<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000144">
<title confidence="0.993498">
Optimal Parsing Strategies for Linear Context-Free Rewriting Systems
</title>
<author confidence="0.998786">
Daniel Gildea
</author>
<affiliation confidence="0.9970935">
Computer Science Department
University of Rochester
</affiliation>
<address confidence="0.316351">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.955397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.96467375">
Factorization is the operation of transforming
a production in a Linear Context-Free Rewrit-
ing System (LCFRS) into two simpler produc-
tions by factoring out a subset of the nontermi-
nals on the production’s righthand side. Fac-
torization lowers the rank of a production but
may increase its fan-out. We show how to
apply factorization in order to minimize the
parsing complexity of the resulting grammar,
and study the relationship between rank, fan-
out, and parsing complexity. We show that it
is always possible to obtain optimum parsing
complexity with rank two. However, among
transformed grammars of rank two, minimum
parsing complexity is not always possible with
minimum fan-out. Applying our factorization
algorithm to LCFRS rules extracted from de-
pendency treebanks allows us to find the most
efficient parsing strategy for the syntactic phe-
nomena found in non-projective trees.
</bodyText>
<sectionHeader confidence="0.999076" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960857142857">
Gómez-Rodríguez et al. (2009a) recently examined
the problem of transforming arbitrary grammars in
the Linear Context-Free Rewriting System (LCFRS)
formalism (Vijay-Shankar et al., 1987) in order to
reduce the rank of a grammar to 2 while minimiz-
ing its fan-out. The work was motivated by the
desire to develop efficient chart-parsing algorithms
for non-projective dependency trees (Kuhlmann and
Nivre, 2006) that do not rely on the independence
assumptions of spanning tree algorithms (McDon-
ald et al., 2005). Efficient parsing algorithms for
general LCFRS are also relevant in the context of
Synchronous Context-Free Grammars (SCFGs) as a
formalism for machine translation, as well as the de-
sire to handle even more general synchronous gram-
mar formalisms which allow nonterminals to cover
discontinuous spans in either language (Melamed et
al., 2004; Wellington et al., 2006). LCFRS provides
a very general formalism which subsumes SCFGs,
the Multitext Grammars of Melamed et al. (2004),
as well as mildly context-sensitive monolingual for-
malisms such as Tree Adjoining Grammar (Joshi
and Schabes, 1997). Thus, work on transforming
general LCFRS grammars promises to be widely ap-
plicable in both understanding how these formalisms
interrelate, and, from a more practical viewpoint, de-
riving efficient parsing algorithms for them.
In this paper, we focus on the problem of trans-
forming an LCFRS grammar into an equivalent
grammar for which straightforward application of
dynamic programming to each rule yields a tabular
parsing algorithm with minimum complexity. This
is closely related, but not equivalent, to the prob-
lem considered by Gómez-Rodríguez et al. (2009a),
who minimize the fan-out, rather than the parsing
complexity, of the resulting grammar. In Section 4,
we show that restricting our attention to factorized
grammars with rank no greater than 2 comes at no
cost in parsing complexity. This result may be sur-
prising, as Gómez-Rodríguez et al. (2009a) com-
ment that “there may be cases in which one has to
find an optimal trade-off between rank and fan-out”
in order to minimize parsing complexity – in fact,
no such trade-off is necessary, as rank 2 is always
sufficient for optimal parsing complexity. Given
this fact, we show how to adapt the factorization al-
gorithm of Gómez-Rodríguez et al. (2009a) to re-
turn a transformed grammar with minimal parsing
complexity and rank 2. In Section 5, we give a
</bodyText>
<page confidence="0.963178">
769
</page>
<subsubsectionHeader confidence="0.576747">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 769–776,
</subsubsectionHeader>
<subsectionHeader confidence="0.274506">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.992462">
counterexample to the conjecture that minimal pars-
ing complexity is possible among binarizations with
minimal fan-out.
</bodyText>
<sectionHeader confidence="0.94349" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.989262375">
A linear context-free rewriting system (LCFRS) is
defined as a tuple G = (VN, VT, P, 5), where VT is
a set of terminal symbols, VN is a set of nonterminal
symbols, P is a set of productions, and 5 E VN is
a distinguished start symbol. Associated with each
nonterminal B is a fan-out cp(B), which tell how
many discontinuous spans B covers. Productions
p E P take the form:
</bodyText>
<equation confidence="0.968096333333333">
p : A , g(B1, B2,.. . , Br) (1)
where A, B1,... Br E VN, and g is a function
g: (VT∗)ϕ(B1) X ... X (VT∗)ϕ(B,) , (VT∗)ϕ(A)
</equation>
<bodyText confidence="0.9998876">
which specifies how to assemble the Eri=1 cp(Bi)
spans of the righthand side nonterminals into the
cp(A) spans of the lefthand side nonterminal. The
function g must be linear, non-erasing, which
means that if we write
</bodyText>
<equation confidence="0.9570415">
g((x1,1, ... , x1,ϕ(B1)), ... , (x1,1, ... , x1,ϕ(B,)))
= (t1,...,tϕ(A))
</equation>
<bodyText confidence="0.999938222222222">
the tuple of strings (t1, ... , tϕ(A)) on the righthand
side contains each variable xi,j from the lefthand
side exactly once, and may also contain terminals
from VT.
We call r, the number of nonterminals on the
righthand side of a production p, the rank of p, p(p).
The fan-out of a production, cp(p) is the fan-out of its
lefthand side, cp(A). The rank of a grammar is the
maximum rank of its rules,
</bodyText>
<equation confidence="0.984376">
p(G) = maxp(p)
p∈P
</equation>
<bodyText confidence="0.996477666666667">
and similarly the fan-out of a grammar is the maxi-
mum fan-out of its rules, or equivalently, of its non-
terminals:
</bodyText>
<equation confidence="0.7109565">
cp(G) = max cp(B)
B∈VN
</equation>
<sectionHeader confidence="0.969795" genericHeader="method">
3 Parsing LCFRS
</sectionHeader>
<bodyText confidence="0.999823772727273">
A bottom-up dynamic programming parser can be
produced from an LCFRS grammar by generaliz-
ing the CYK algorithm for context-free grammars.
We convert each production of the LCFRS into a
deduction rule with variables for the left and right
endpoints of each of the cp(Bi) spans of each of the
nonterminals Bi, i E [r] in the righthand side of the
production.
The computational complexity of the resulting
parser is polynomial in the length of the input string,
with the degree of the polynomial being the number
of distinct endpoints in the most complex produc-
tion. Thus, for input of length n, the complexity
is O(nc) for some constant c which depends on the
grammar.
For a given rule, each of the r nonterminals has
cp(Bi) spans, and each span has a left and right end-
point, giving an upper bound of c &lt; 2 Eri=1 cp(Bi).
However, some of these endpoints may be shared
between nonterminals on the righthand side. The
exact number of distinct variables for the dynamic
programming deduction rule can the written
</bodyText>
<equation confidence="0.989574">
r
c(p) = cp(A) + cp(Bi) (2)
i=1
</equation>
<bodyText confidence="0.9998937">
where c(p) is the parsing complexity of a produc-
tion p of the form of eq. 1 (Seki et al., 1991). To
see this, consider counting the left endpoint of each
span on the lefthand side of the production, and the
right endpoint of each span on the righthand side of
the production. Any variable corresponding to the
left endpoint of a span of a righthand side nonter-
minal will either be shared with the right endpoint
of another span if two spans are being joined by the
production, or, alternatively, will form the left end-
point of a span of A. Thus, each distinct endpoint in
the production is counted exactly once by eq. 2.
The parsing complexity of a grammar, c(G), is
the maximum parsing complexity of its rules. From
eq. 2, we see that c(G) &lt; (p(G) + 1)cp(G). While
we focus on the time complexity of parsing, it is in-
teresting to note the space complexity of the DP al-
gorithm is O(n2ϕ(G)), since the DP table for each
nonterminal is indexed by at most 2cp(G) positions
in the input string.
</bodyText>
<page confidence="0.915894">
770
</page>
<figure confidence="0.441854">
4 Binarization Minimizes Parsing and applying this inequality to the definition of c(p2)
Complexity we have:
</figure>
<bodyText confidence="0.991557">
An LCFRS production of rank r can be factorized
into two productions of the form:
</bodyText>
<equation confidence="0.997244">
p1 : A → g1(B1, ... , Br−2, X)
p2 : X → g2(Br−1, Br)
</equation>
<bodyText confidence="0.999252666666667">
This operation results in new productions that have
lower rank, but possibly higher fan-out, than the
original production.
If we examine the DP deduction rules correspond-
ing to the original production p, and the first new
production p1 we find that
</bodyText>
<equation confidence="0.975526">
c(p1) ≤ c(p)
</equation>
<bodyText confidence="0.999662">
regardless of the function g of the original produc-
tion, or the fan-out of the production’s nonterminals.
This is because
</bodyText>
<equation confidence="0.99558">
ϕ(X) ≤ ϕ(Br−1) + ϕ(Br)
</equation>
<bodyText confidence="0.991088333333333">
that is, our newly created nonterminal X may join
spans from Br−1 and Br, but can never introduce
new spans. Thus,
</bodyText>
<equation confidence="0.9901994">
�ϕ(Bi) + ϕ(X)
r
≤ ϕ(A) + ϕ(Bi)
i=1
= c(p)
</equation>
<bodyText confidence="0.8663505">
As similar result holds for the second newly cre-
ated production:
</bodyText>
<equation confidence="0.973994">
c(p2) ≤ c(p)
</equation>
<bodyText confidence="0.999860333333333">
In this case, the fan-out of the newly created nonter-
minal, ϕ(X) may be greater than ϕ(A). Let us con-
sider the left endpoints of the spans of X. Each left
endpoint is either also the left endpoint of a span of
A, or is the right endpoint of some nonterminal not
included in X, that is, one of B1,... Br−2. Thus,
</bodyText>
<equation confidence="0.990513">
ϕ(X) ≤ ϕ(A) +
c(p2) = ϕ(X) + ϕ(Br−1) + ϕ(Br−2)
r
≤ ϕ(A) + ϕ(Bi)
i=1
= c(p)
</equation>
<bodyText confidence="0.999962666666667">
For notational convenience, we have defined the
factorization operation as factoring out the last two
nonterminals of a rule; however, the same operation
can be applied to factor out any subset of the orig-
inal nonterminals. The same argument that parsing
complexity cannot increase still applies.
We may apply the factorization operation repeat-
edly until all rules have rank 2; we refer to the re-
sulting grammar as a binarization of the original
LCFRS. The factorization operation may increase
the fan-out of a grammar, but never increases its
parsing complexity. This guarantees that, if we wish
to find the transformation of the original grammar
having the lowest parsing complexity, it is sufficient
to consider only binarizations. This is because any
transformed grammar having more than two nonter-
minals on the righthand side can be binarized with-
out increasing its parsing complexity.
</bodyText>
<sectionHeader confidence="0.7977005" genericHeader="method">
5 The relationship between fan-out and
parsing complexity
</sectionHeader>
<bodyText confidence="0.999605611111111">
Gómez-Rodríguez et al. (2009a) provide an algo-
rithm for finding the binarization of an LCFRS hav-
ing minimal fan-out. The key idea is to search over
ways of combining subsets of a rule’s righthand side
nonterminals such that subsets with low fan-out are
considered first; this results in an algorithm with
complexity polynomial in the rank of the input rule,
with the exponent depending on the resulting mini-
mum fan-out.
This algorithm can be adapted to find the binariza-
tion with minimum parsing complexity, rather than
minimum fan-out. We simply use c(p) rather than
ϕ(p) as the score for new productions, controlling
both which binarizations we prefer and the order in
which they are explored.
An interesting question then arises: does the bina-
rization with minimal parsing complexity also have
minimal fan-out? A binarization into a grammar of
</bodyText>
<equation confidence="0.963531777777778">
c(p1) = ϕ(A) + r−2�
i=1
r−2�
i=1
ϕ(Bi)
771
A → g(B1, B2, B3, B4)
g(hx1,1, x1,2i, hx2,1, x2,2, x2,3i, hx3,1, x3,2, x3,3, x3,4, x3,5i, hx4,1, x4,2, x4,3i) �
hx4,1x3,1, x2,1, x4,2x1,1x2,2x4,3x3,2x2,3x3,3, x1,2x3,4, x3,5i
</equation>
<figureCaption confidence="0.960109">
Figure 2: A production for which minimizing fan-out and minimizing parsing complexity are mutually exclusive.
</figureCaption>
<equation confidence="0.825871888888889">
{B4}
{B3}
{B3, B4}
{B1}
{B3, B4}
{B1, B3, B4}
{B2}
{B1, B3, B4}
{B1, B2, B3, B4}
</equation>
<figureCaption confidence="0.960104666666667">
Figure 3: The binarization of the rule from Figure 2 that minimizes parsing complexity. In each of the three steps,
we show the spans of each of the two subsets of the rule’s righthand-side nonterms being combined, with the spans of
their union (corresponding to a nonterminal created by the binarization) below.
</figureCaption>
<page confidence="0.971063">
772
</page>
<listItem confidence="0.930684545454545">
1: function MINIMAL-BINARIZATION(p, ≺)
2: workingSet ← ∅;
3: agenda ← priorityQueue(≺);
4: for i from 1 to ρ(p) do
5: workingSet ← workingSet ∪{BZ};
6: agenda ← agenda ∪{BZ};
7: while agenda =6 ∅ do
8: p′ ← pop minimum from agenda;
9: if nonterms(p′) = {B1,... BP(P)} then
10: return p′;
11: for p1 ∈ workingSet do
</listItem>
<figure confidence="0.983079666666667">
12: p2 ← newProd(p′, p1);
13: find p′2 ∈ workingSet �
14: nonterms(p′2) = nonterms(p2);
15: if p2 ≺ p′2 then
16: workingSet ← workingSet ∪{p2}\{p′2};
17: push(agenda, p2);
</figure>
<figureCaption confidence="0.998073">
Figure 1: Algorithm to compute best binarization accord-
ing to a user-specified ordering ≺ over productions.
</figureCaption>
<bodyText confidence="0.999421172413793">
fan-out f′ cannot have parsing complexity higher
than 3f′, according to eq. 2. Thus, minimizing fan-
out puts an upper bound on parsing complexity, but
is not guaranteed to minimize it absolutely. Bina-
rizations with the same fan-out may in fact vary
in their parsing complexity; similarly binarizations
with the same parsing complexity may vary in their
fan-out. It is not immediately apparent whether, in
order to find a binarization of minimal parsing com-
plexity, it is sufficient to consider only binarizations
of minimal fan-out.
To test this conjecture, we adapted the algorithm
of Gómez-Rodríguez et al. (2009a) to use a prior-
ity queue as the agenda, as shown in Figure 1. The
algorithm takes as an argument an arbitrary partial
ordering relation on productions, and explores pos-
sible binarized rules in the order specified by this re-
lation. In Figure 1, “workingSet” is a set of single-
ton nonterminals and binarized productions which
are guaranteed to be optimal for the subset of non-
terminals that they cover. The function “nonterms”
returns, for a newly created production, the subset
of the original nonterminals B1,... Br that it gen-
erates, and returns subsets of singleton nonterminals
directly.
To find the binarization with the minimum fan-out
f′ and the lowest parsing complexity among bina-
rizations with fan-out f′, we use the following com-
parison operation in the binarization algorithm:
</bodyText>
<equation confidence="0.9966555">
p1 ≺cpc p2 iff ϕ(p1) &lt; ϕ(p2) ∨
(ϕ(p1) = ϕ(p2) ∧ c(p1) &lt; c(p2))
</equation>
<bodyText confidence="0.998645571428572">
guaranteeing that we explore binarizations with
lower fan-out first, and, among binarizations with
equal fan-out, those with lower parsing complexity
first. Similarly, we can search for the binarization
with the lowest parsing complexity c′ and the lowest
fan-out among binarizations with complexity c′, we
use
</bodyText>
<equation confidence="0.9977485">
p1 ≺ccp p2 iff c(p1) &lt; c(p2) ∨
(c(p1) = c(p2) ∧ ϕ(p1) &lt; ϕ(p2))
</equation>
<bodyText confidence="0.999975">
We find that, in fact, it is sometimes necessary to
sacrifice minimum fan-out in order to achieve mini-
mum parsing complexity. An example of an LCFRS
rule for which this is the case is shown in Figure 2.
This production can be binarized to produce a set of
productions with parsing complexity 14 (Figure 3);
among binarizations with this complexity the mini-
mum fan-out is 6. However, an alternative binariza-
tion with fan-out 5 is also possible; among binariza-
tions with this fan-out, the minimum parsing com-
plexity is 15. This binarization (not pictured) first
joins B1 and B2, then adds B4, and finally adds B3.
Given the incompatibility of optimizing time
complexity and fan-out, which corresponds to space
complexity, which should we prefer? In some sit-
uations, it may be desirable to find some trade-off
between the two. It is important to note, however,
that if optimization of space complexity is the sole
objective, factorization is unnecessary, as one can
never improve on the fan-out required by the origi-
nal grammar nonterminals.
</bodyText>
<sectionHeader confidence="0.977107" genericHeader="method">
6 A Note on Generative Capacity
</sectionHeader>
<bodyText confidence="0.999883714285714">
Rambow and Satta (1999) categorize the genera-
tive capacity of LCFRS grammars according to their
rank and fan-out. In particular, they show that
grammars can be arranged in a two-dimensional
grid, with languages of rank r and fan-out f having
greater generative capacity than both grammars of
rank r and fan-out f −1 and grammars of rank r −1
</bodyText>
<page confidence="0.986007">
773
</page>
<bodyText confidence="0.5465885">
nmod sbj root vc pp nmod np tmp
A hearing is scheduled on the issue today
</bodyText>
<equation confidence="0.9987605">
nmod g1 g1 = (A)
sbj g2(nmod, pp) g2((x1,1), (x2,1)) = (x1,1 hearing, x2,1)
root g3(sbj, vc) g3((x1,1, x1,2), (x2,1, x2,2)) = (x1,1 is x2,1x1,2x2,2)
vc g4(tmp) g4((x1,1)) = (scheduled, x1,1)
pp g5(tmp) g5((x1,1)) = ( on x1,1)
nmod g6 g6 = ( the)
np g7(nmod) g7((x1,1)) = (x1,1 issue)
tmp g8 g8 = ( today)
</equation>
<figureCaption confidence="0.999432">
Figure 4: A dependency tree with the LCFRS rules extracted for each word (Kuhlmann and Satta, 2009).
</figureCaption>
<bodyText confidence="0.999898441176471">
and fan-out f, with two exceptions: with fan-out 1,
all ranks greater than one are equivalent (context-
free languages), and with fan-out 2, rank 2 and rank
3 are equivalent.
This classification is somewhat unsatisfying be-
cause minor changes to a grammar can change both
its rank and fan-out. In particular, through factor-
izing rules, it is always possible to decrease rank,
potentially at the cost of increasing fan-out, until a
binarized grammar of rank 2 is achieved.
Parsing complexity, as defined above, also pro-
vides a method to compare the generative capacity
of LCFRS grammars. From Rambow and Satta’s
result that grammars of rank two and increasing
fan-out provide an infinite hierarchy of increasing
generative capacity, we see that parsing complexity
also provides such an infinite hierarchy. Compar-
ing grammars according to the parsing complexity
amounts to specifying a normalized binarization for
grammars of arbitrary rank and fan-out, and compar-
ing the resulting binarized grammars. This allows us
to arrange LCFRS grammars into total ordering over
generative capacity, that is a one-dimensional hier-
archy, rather than a two-dimensional grid. It also
gives a way of categorizing generative capacity that
is more closely tied to algorithmic complexity.
It is important to note, however, that parsing com-
plexity as calculated by our algorithm remains a
function of the grammar, rather than an intrinsic
function of the language. One can produce arbitrar-
ily complex grammars that generate the simple lan-
guage a∗. Thus the parsing complexity of a gram-
mar, like its rank and fan-out, can be said to catego-
rize its strong generative capacity.
</bodyText>
<sectionHeader confidence="0.998118" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.9998849375">
A number of recent papers have examined dynamic
programming algorithms for parsing non-projective
dependency structures by exploring how well vari-
ous categories of polynomially-parsable grammars
cover the structures found in dependency treebanks
for various languages (Kuhlmann and Nivre, 2006;
Gómez-Rodríguez et al., 2009b).
Kuhlmann and Satta (2009) give an algorithm for
extracting LCFRS rules from dependency structures.
One rule is extracted for each word in the depen-
dency tree. The rank of the rule is the number of
children that the word has in the dependency tree,
as shown by the example in Figure 4. The fan-out
of the symbol corresponding to a word is the num-
ber of continuous intervals in the sentence formed
by the word and its descendants in the tree. Projec-
</bodyText>
<page confidence="0.992576">
774
</page>
<figure confidence="0.67726875">
complexity arabic czech danish dutch german port swedish
20 1
18 1
16 1
15 1
13 1
12 2 3
11 1 1 1
10 2 6 16 3
9 7 4 1
8 4 7 129 65 10
7 3 12 89 30 18
6 178 11 362 1811 492 59
5 48 1132 93 411 1848 172 201
4 250 18269 1026 6678 18124 2643 1736
3 10942 265202 18306 39362 154948 41075 41245
</figure>
<tableCaption confidence="0.989151">
Table 1: Number of productions with specified parsing complexity
</tableCaption>
<bodyText confidence="0.999940214285715">
tive trees yield LCFRS rules of fan-out one and pars-
ing complexity three, while the fan-out and parsing
complexity from non-projective trees are in princi-
ple unbounded.
Extracting LCFRS rules from treebanks allows us
to study how many of the rules fall within certain
constraints. Kuhlmann and Satta (2009) give an al-
gorithm for binarizing LCRFS rules without increas-
ing the rules’ fan-out; however, this is not always
possible, and the algorithm does not succeed even in
some cases for which such a binarization is possible.
Kuhlmann and Satta (2009) find that all but 0.02%
of productions in the CoNLL 2006 training data,
which includes various languages, can be binarized
by their algorithm, but they do not give the fan-out
or parsing complexity of the resulting rules. In re-
lated work, Gómez-Rodríguez et al. (2009b) define
the class of mildly ill-nested dependency structures
of varying gap degrees; gap degree is essentially fan-
out minus one. For a given gap degree k, this class of
grammars can be parsing in time O(n3k+4) for lexi-
calized grammars. Gómez-Rodríguez et al. (2009b)
study dependency treebanks for nine languages and
find that all dependency structures meet the mildly
ill-nested condition in the dependency treebanks for
some gap degree. However, they do not report the
maximum gap degree or parsing complexity.
We extracted LCFRS rules from dependency tree-
banks using the same procedure as Kuhlmann and
Satta (2009), and applied the algorithm of Figure 1
directly to calculate their minimum parsing com-
plexity. This allows us to characterize the pars-
ing complexity of the rules found in the treebank
without needing to define specific conditions on
the rules, such as well-nestedness (Kuhlmann and
Nivre, 2006) or mildly ill-nestedness, that may not
be necessary for all efficiently parsable grammars.
The numbers of rules of different complexities are
shown in Table 1.
As found by previous studies, the vast major-
ity of productions are context-free (projective trees,
parsable in O(n3)). Of non-projective rules, the
vast majority can be parsed in O(n6), including the
well-nested structures of gap degree one defined by
Kuhlmann and Nivre (2006). The single most com-
plex rule had parsing complexity of O(n20), and was
derived from a Swedish sentence which turns out to
be so garbled as to be incomprehensible (taken from
the high school essay portion of the Swedish tree-
bank). It is interesting to note that, while the bina-
rization algorithm is exponential in the worst case, it
is practical for real data: analyzing all the rules ex-
tracted from the various treebanks takes only a few
minutes. We did not find any cases in rules extracted
from Treebank data of rules where minimizing pars-
ing complexity is inconsistent with minimizing fan-
</bodyText>
<page confidence="0.993812">
775
</page>
<bodyText confidence="0.964868">
out, as is the case for the rule of Figure 2.
</bodyText>
<sectionHeader confidence="0.968694" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999794153846154">
We give an algorithm for finding the optimum pars-
ing complexity for an LCFRS among grammars ob-
tained by binarization. We find that minimum pars-
ing complexity is always achievable with rank 2, but
is not always achievable with minimum fan-out. By
applying the binarization algorithm to productions
found in dependency treebanks, we can completely
characterize the parsing complexity of the extracted
LCFRS grammar.
Acknowledgments This work was funded by NSF
grants IIS-0546554 and IIS-0910611. We are grate-
ful to Joakim Nivre for assistance with the Swedish
treebank.
</bodyText>
<sectionHeader confidence="0.999422" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999591111111111">
Carlos Gómez-Rodríguez, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009a. Optimal reduction of
rule length in linear conext-free rewriting systems. In
Proceedings of the 2009 Meeting of the North Ameri-
can chapter of the Association for Computational Lin-
guistics (NAACL-09), pages 539–547.
Carlos Gómez-Rodríguez, David Weir, and John Car-
roll. 2009b. Parsing mildly non-projective depen-
dency structures. In Proceedings of the 12th Confer-
ence of the European Chapter of the ACL (EACL-09),
pages 291–299.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining gram-
mars. In G. Rozenberg and A. Salomaa, editors,
Handbook ofFormal Languages, volume 3, pages 69–
124. Springer, Berlin.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06), pages 507–514.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL-09), pages 478–
486.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (HLT/EMNLP).
I. Dan Melamed, Giorgio Satta, and Ben Wellington.
2004. Generalized multitext grammars. In Proceed-
ings of the 42nd Annual Conference of the Association
for Computational Linguistics (ACL-04), Barcelona,
Spain.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theor. Comput. Sci., 223(1-2):87–120.
H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991.
On multiple context-free grammars. Theoretical Com-
puter Science, 88:191–229.
K. Vijay-Shankar, D. L. Weir, and A. K. Joshi. 1987.
Characterizing structural descriptions produced by
various grammatical formalisms. In Proceedings of
the 25th Annual Conference of the Association for
Computational Linguistics (ACL-87).
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the
complexity of translational equivalence. In Proceed-
ings of the International Conference on Computa-
tional Linguistics/Association for Computational Lin-
guistics (COLING/ACL-06), pages 977–984, Sydney,
Australia.
</reference>
<page confidence="0.998523">
776
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.972760">
<title confidence="0.999987">Optimal Parsing Strategies for Linear Context-Free Rewriting Systems</title>
<author confidence="0.989468">Daniel</author>
<affiliation confidence="0.9998385">Computer Science University of</affiliation>
<address confidence="0.999065">Rochester, NY 14627</address>
<abstract confidence="0.999249285714286">Factorization is the operation of transforming a production in a Linear Context-Free Rewriting System (LCFRS) into two simpler productions by factoring out a subset of the nonterminals on the production’s righthand side. Factorization lowers the rank of a production but may increase its fan-out. We show how to apply factorization in order to minimize the complexity the resulting grammar, and study the relationship between rank, fanout, and parsing complexity. We show that it is always possible to obtain optimum parsing complexity with rank two. However, among transformed grammars of rank two, minimum parsing complexity is not always possible with minimum fan-out. Applying our factorization algorithm to LCFRS rules extracted from dependency treebanks allows us to find the most efficient parsing strategy for the syntactic phenomena found in non-projective trees.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Carlos Gómez-Rodríguez</author>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
<author>David Weir</author>
</authors>
<title>Optimal reduction of rule length in linear conext-free rewriting systems.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-09),</booktitle>
<pages>539--547</pages>
<contexts>
<context position="1091" citStr="Gómez-Rodríguez et al. (2009" startWordPosition="160" endWordPosition="163"> show how to apply factorization in order to minimize the parsing complexity of the resulting grammar, and study the relationship between rank, fanout, and parsing complexity. We show that it is always possible to obtain optimum parsing complexity with rank two. However, among transformed grammars of rank two, minimum parsing complexity is not always possible with minimum fan-out. Applying our factorization algorithm to LCFRS rules extracted from dependency treebanks allows us to find the most efficient parsing strategy for the syntactic phenomena found in non-projective trees. 1 Introduction Gómez-Rodríguez et al. (2009a) recently examined the problem of transforming arbitrary grammars in the Linear Context-Free Rewriting System (LCFRS) formalism (Vijay-Shankar et al., 1987) in order to reduce the rank of a grammar to 2 while minimizing its fan-out. The work was motivated by the desire to develop efficient chart-parsing algorithms for non-projective dependency trees (Kuhlmann and Nivre, 2006) that do not rely on the independence assumptions of spanning tree algorithms (McDonald et al., 2005). Efficient parsing algorithms for general LCFRS are also relevant in the context of Synchronous Context-Free Grammars </context>
<context position="2730" citStr="Gómez-Rodríguez et al. (2009" startWordPosition="409" endWordPosition="412">ormalisms such as Tree Adjoining Grammar (Joshi and Schabes, 1997). Thus, work on transforming general LCFRS grammars promises to be widely applicable in both understanding how these formalisms interrelate, and, from a more practical viewpoint, deriving efficient parsing algorithms for them. In this paper, we focus on the problem of transforming an LCFRS grammar into an equivalent grammar for which straightforward application of dynamic programming to each rule yields a tabular parsing algorithm with minimum complexity. This is closely related, but not equivalent, to the problem considered by Gómez-Rodríguez et al. (2009a), who minimize the fan-out, rather than the parsing complexity, of the resulting grammar. In Section 4, we show that restricting our attention to factorized grammars with rank no greater than 2 comes at no cost in parsing complexity. This result may be surprising, as Gómez-Rodríguez et al. (2009a) comment that “there may be cases in which one has to find an optimal trade-off between rank and fan-out” in order to minimize parsing complexity – in fact, no such trade-off is necessary, as rank 2 is always sufficient for optimal parsing complexity. Given this fact, we show how to adapt the factor</context>
<context position="9461" citStr="Gómez-Rodríguez et al. (2009" startWordPosition="1605" endWordPosition="1608">until all rules have rank 2; we refer to the resulting grammar as a binarization of the original LCFRS. The factorization operation may increase the fan-out of a grammar, but never increases its parsing complexity. This guarantees that, if we wish to find the transformation of the original grammar having the lowest parsing complexity, it is sufficient to consider only binarizations. This is because any transformed grammar having more than two nonterminals on the righthand side can be binarized without increasing its parsing complexity. 5 The relationship between fan-out and parsing complexity Gómez-Rodríguez et al. (2009a) provide an algorithm for finding the binarization of an LCFRS having minimal fan-out. The key idea is to search over ways of combining subsets of a rule’s righthand side nonterminals such that subsets with low fan-out are considered first; this results in an algorithm with complexity polynomial in the rank of the input rule, with the exponent depending on the resulting minimum fan-out. This algorithm can be adapted to find the binarization with minimum parsing complexity, rather than minimum fan-out. We simply use c(p) rather than ϕ(p) as the score for new productions, controlling both whic</context>
<context position="12214" citStr="Gómez-Rodríguez et al. (2009" startWordPosition="2062" endWordPosition="2065">r productions. fan-out f′ cannot have parsing complexity higher than 3f′, according to eq. 2. Thus, minimizing fanout puts an upper bound on parsing complexity, but is not guaranteed to minimize it absolutely. Binarizations with the same fan-out may in fact vary in their parsing complexity; similarly binarizations with the same parsing complexity may vary in their fan-out. It is not immediately apparent whether, in order to find a binarization of minimal parsing complexity, it is sufficient to consider only binarizations of minimal fan-out. To test this conjecture, we adapted the algorithm of Gómez-Rodríguez et al. (2009a) to use a priority queue as the agenda, as shown in Figure 1. The algorithm takes as an argument an arbitrary partial ordering relation on productions, and explores possible binarized rules in the order specified by this relation. In Figure 1, “workingSet” is a set of singleton nonterminals and binarized productions which are guaranteed to be optimal for the subset of nonterminals that they cover. The function “nonterms” returns, for a newly created production, the subset of the original nonterminals B1,... Br that it generates, and returns subsets of singleton nonterminals directly. To find</context>
<context position="17326" citStr="Gómez-Rodríguez et al., 2009" startWordPosition="2902" endWordPosition="2905">ns a function of the grammar, rather than an intrinsic function of the language. One can produce arbitrarily complex grammars that generate the simple language a∗. Thus the parsing complexity of a grammar, like its rank and fan-out, can be said to categorize its strong generative capacity. 7 Experiments A number of recent papers have examined dynamic programming algorithms for parsing non-projective dependency structures by exploring how well various categories of polynomially-parsable grammars cover the structures found in dependency treebanks for various languages (Kuhlmann and Nivre, 2006; Gómez-Rodríguez et al., 2009b). Kuhlmann and Satta (2009) give an algorithm for extracting LCFRS rules from dependency structures. One rule is extracted for each word in the dependency tree. The rank of the rule is the number of children that the word has in the dependency tree, as shown by the example in Figure 4. The fan-out of the symbol corresponding to a word is the number of continuous intervals in the sentence formed by the word and its descendants in the tree. Projec774 complexity arabic czech danish dutch german port swedish 20 1 18 1 16 1 15 1 13 1 12 2 3 11 1 1 1 10 2 6 16 3 9 7 4 1 8 4 7 129 65 10 7 3 12 89 3</context>
<context position="18954" citStr="Gómez-Rodríguez et al. (2009" startWordPosition="3206" endWordPosition="3209"> from treebanks allows us to study how many of the rules fall within certain constraints. Kuhlmann and Satta (2009) give an algorithm for binarizing LCRFS rules without increasing the rules’ fan-out; however, this is not always possible, and the algorithm does not succeed even in some cases for which such a binarization is possible. Kuhlmann and Satta (2009) find that all but 0.02% of productions in the CoNLL 2006 training data, which includes various languages, can be binarized by their algorithm, but they do not give the fan-out or parsing complexity of the resulting rules. In related work, Gómez-Rodríguez et al. (2009b) define the class of mildly ill-nested dependency structures of varying gap degrees; gap degree is essentially fanout minus one. For a given gap degree k, this class of grammars can be parsing in time O(n3k+4) for lexicalized grammars. Gómez-Rodríguez et al. (2009b) study dependency treebanks for nine languages and find that all dependency structures meet the mildly ill-nested condition in the dependency treebanks for some gap degree. However, they do not report the maximum gap degree or parsing complexity. We extracted LCFRS rules from dependency treebanks using the same procedure as Kuhlma</context>
</contexts>
<marker>Gómez-Rodríguez, Kuhlmann, Satta, Weir, 2009</marker>
<rawString>Carlos Gómez-Rodríguez, Marco Kuhlmann, Giorgio Satta, and David Weir. 2009a. Optimal reduction of rule length in linear conext-free rewriting systems. In Proceedings of the 2009 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-09), pages 539–547.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Gómez-Rodríguez</author>
<author>David Weir</author>
<author>John Carroll</author>
</authors>
<title>Parsing mildly non-projective dependency structures.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL-09),</booktitle>
<pages>291--299</pages>
<contexts>
<context position="1091" citStr="Gómez-Rodríguez et al. (2009" startWordPosition="160" endWordPosition="163"> show how to apply factorization in order to minimize the parsing complexity of the resulting grammar, and study the relationship between rank, fanout, and parsing complexity. We show that it is always possible to obtain optimum parsing complexity with rank two. However, among transformed grammars of rank two, minimum parsing complexity is not always possible with minimum fan-out. Applying our factorization algorithm to LCFRS rules extracted from dependency treebanks allows us to find the most efficient parsing strategy for the syntactic phenomena found in non-projective trees. 1 Introduction Gómez-Rodríguez et al. (2009a) recently examined the problem of transforming arbitrary grammars in the Linear Context-Free Rewriting System (LCFRS) formalism (Vijay-Shankar et al., 1987) in order to reduce the rank of a grammar to 2 while minimizing its fan-out. The work was motivated by the desire to develop efficient chart-parsing algorithms for non-projective dependency trees (Kuhlmann and Nivre, 2006) that do not rely on the independence assumptions of spanning tree algorithms (McDonald et al., 2005). Efficient parsing algorithms for general LCFRS are also relevant in the context of Synchronous Context-Free Grammars </context>
<context position="2730" citStr="Gómez-Rodríguez et al. (2009" startWordPosition="409" endWordPosition="412">ormalisms such as Tree Adjoining Grammar (Joshi and Schabes, 1997). Thus, work on transforming general LCFRS grammars promises to be widely applicable in both understanding how these formalisms interrelate, and, from a more practical viewpoint, deriving efficient parsing algorithms for them. In this paper, we focus on the problem of transforming an LCFRS grammar into an equivalent grammar for which straightforward application of dynamic programming to each rule yields a tabular parsing algorithm with minimum complexity. This is closely related, but not equivalent, to the problem considered by Gómez-Rodríguez et al. (2009a), who minimize the fan-out, rather than the parsing complexity, of the resulting grammar. In Section 4, we show that restricting our attention to factorized grammars with rank no greater than 2 comes at no cost in parsing complexity. This result may be surprising, as Gómez-Rodríguez et al. (2009a) comment that “there may be cases in which one has to find an optimal trade-off between rank and fan-out” in order to minimize parsing complexity – in fact, no such trade-off is necessary, as rank 2 is always sufficient for optimal parsing complexity. Given this fact, we show how to adapt the factor</context>
<context position="9461" citStr="Gómez-Rodríguez et al. (2009" startWordPosition="1605" endWordPosition="1608">until all rules have rank 2; we refer to the resulting grammar as a binarization of the original LCFRS. The factorization operation may increase the fan-out of a grammar, but never increases its parsing complexity. This guarantees that, if we wish to find the transformation of the original grammar having the lowest parsing complexity, it is sufficient to consider only binarizations. This is because any transformed grammar having more than two nonterminals on the righthand side can be binarized without increasing its parsing complexity. 5 The relationship between fan-out and parsing complexity Gómez-Rodríguez et al. (2009a) provide an algorithm for finding the binarization of an LCFRS having minimal fan-out. The key idea is to search over ways of combining subsets of a rule’s righthand side nonterminals such that subsets with low fan-out are considered first; this results in an algorithm with complexity polynomial in the rank of the input rule, with the exponent depending on the resulting minimum fan-out. This algorithm can be adapted to find the binarization with minimum parsing complexity, rather than minimum fan-out. We simply use c(p) rather than ϕ(p) as the score for new productions, controlling both whic</context>
<context position="12214" citStr="Gómez-Rodríguez et al. (2009" startWordPosition="2062" endWordPosition="2065">r productions. fan-out f′ cannot have parsing complexity higher than 3f′, according to eq. 2. Thus, minimizing fanout puts an upper bound on parsing complexity, but is not guaranteed to minimize it absolutely. Binarizations with the same fan-out may in fact vary in their parsing complexity; similarly binarizations with the same parsing complexity may vary in their fan-out. It is not immediately apparent whether, in order to find a binarization of minimal parsing complexity, it is sufficient to consider only binarizations of minimal fan-out. To test this conjecture, we adapted the algorithm of Gómez-Rodríguez et al. (2009a) to use a priority queue as the agenda, as shown in Figure 1. The algorithm takes as an argument an arbitrary partial ordering relation on productions, and explores possible binarized rules in the order specified by this relation. In Figure 1, “workingSet” is a set of singleton nonterminals and binarized productions which are guaranteed to be optimal for the subset of nonterminals that they cover. The function “nonterms” returns, for a newly created production, the subset of the original nonterminals B1,... Br that it generates, and returns subsets of singleton nonterminals directly. To find</context>
<context position="17326" citStr="Gómez-Rodríguez et al., 2009" startWordPosition="2902" endWordPosition="2905">ns a function of the grammar, rather than an intrinsic function of the language. One can produce arbitrarily complex grammars that generate the simple language a∗. Thus the parsing complexity of a grammar, like its rank and fan-out, can be said to categorize its strong generative capacity. 7 Experiments A number of recent papers have examined dynamic programming algorithms for parsing non-projective dependency structures by exploring how well various categories of polynomially-parsable grammars cover the structures found in dependency treebanks for various languages (Kuhlmann and Nivre, 2006; Gómez-Rodríguez et al., 2009b). Kuhlmann and Satta (2009) give an algorithm for extracting LCFRS rules from dependency structures. One rule is extracted for each word in the dependency tree. The rank of the rule is the number of children that the word has in the dependency tree, as shown by the example in Figure 4. The fan-out of the symbol corresponding to a word is the number of continuous intervals in the sentence formed by the word and its descendants in the tree. Projec774 complexity arabic czech danish dutch german port swedish 20 1 18 1 16 1 15 1 13 1 12 2 3 11 1 1 1 10 2 6 16 3 9 7 4 1 8 4 7 129 65 10 7 3 12 89 3</context>
<context position="18954" citStr="Gómez-Rodríguez et al. (2009" startWordPosition="3206" endWordPosition="3209"> from treebanks allows us to study how many of the rules fall within certain constraints. Kuhlmann and Satta (2009) give an algorithm for binarizing LCRFS rules without increasing the rules’ fan-out; however, this is not always possible, and the algorithm does not succeed even in some cases for which such a binarization is possible. Kuhlmann and Satta (2009) find that all but 0.02% of productions in the CoNLL 2006 training data, which includes various languages, can be binarized by their algorithm, but they do not give the fan-out or parsing complexity of the resulting rules. In related work, Gómez-Rodríguez et al. (2009b) define the class of mildly ill-nested dependency structures of varying gap degrees; gap degree is essentially fanout minus one. For a given gap degree k, this class of grammars can be parsing in time O(n3k+4) for lexicalized grammars. Gómez-Rodríguez et al. (2009b) study dependency treebanks for nine languages and find that all dependency structures meet the mildly ill-nested condition in the dependency treebanks for some gap degree. However, they do not report the maximum gap degree or parsing complexity. We extracted LCFRS rules from dependency treebanks using the same procedure as Kuhlma</context>
</contexts>
<marker>Gómez-Rodríguez, Weir, Carroll, 2009</marker>
<rawString>Carlos Gómez-Rodríguez, David Weir, and John Carroll. 2009b. Parsing mildly non-projective dependency structures. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL-09), pages 291–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-adjoining grammars.</title>
<date>1997</date>
<booktitle>Handbook ofFormal Languages,</booktitle>
<volume>3</volume>
<pages>69--124</pages>
<editor>In G. Rozenberg and A. Salomaa, editors,</editor>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="2168" citStr="Joshi and Schabes, 1997" startWordPosition="323" endWordPosition="326">thms (McDonald et al., 2005). Efficient parsing algorithms for general LCFRS are also relevant in the context of Synchronous Context-Free Grammars (SCFGs) as a formalism for machine translation, as well as the desire to handle even more general synchronous grammar formalisms which allow nonterminals to cover discontinuous spans in either language (Melamed et al., 2004; Wellington et al., 2006). LCFRS provides a very general formalism which subsumes SCFGs, the Multitext Grammars of Melamed et al. (2004), as well as mildly context-sensitive monolingual formalisms such as Tree Adjoining Grammar (Joshi and Schabes, 1997). Thus, work on transforming general LCFRS grammars promises to be widely applicable in both understanding how these formalisms interrelate, and, from a more practical viewpoint, deriving efficient parsing algorithms for them. In this paper, we focus on the problem of transforming an LCFRS grammar into an equivalent grammar for which straightforward application of dynamic programming to each rule yields a tabular parsing algorithm with minimum complexity. This is closely related, but not equivalent, to the problem considered by Gómez-Rodríguez et al. (2009a), who minimize the fan-out, rather t</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>A.K. Joshi and Y. Schabes. 1997. Tree-adjoining grammars. In G. Rozenberg and A. Salomaa, editors, Handbook ofFormal Languages, volume 3, pages 69– 124. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Joakim Nivre</author>
</authors>
<title>Mildly non-projective dependency structures.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<pages>507--514</pages>
<contexts>
<context position="1471" citStr="Kuhlmann and Nivre, 2006" startWordPosition="216" endWordPosition="219"> Applying our factorization algorithm to LCFRS rules extracted from dependency treebanks allows us to find the most efficient parsing strategy for the syntactic phenomena found in non-projective trees. 1 Introduction Gómez-Rodríguez et al. (2009a) recently examined the problem of transforming arbitrary grammars in the Linear Context-Free Rewriting System (LCFRS) formalism (Vijay-Shankar et al., 1987) in order to reduce the rank of a grammar to 2 while minimizing its fan-out. The work was motivated by the desire to develop efficient chart-parsing algorithms for non-projective dependency trees (Kuhlmann and Nivre, 2006) that do not rely on the independence assumptions of spanning tree algorithms (McDonald et al., 2005). Efficient parsing algorithms for general LCFRS are also relevant in the context of Synchronous Context-Free Grammars (SCFGs) as a formalism for machine translation, as well as the desire to handle even more general synchronous grammar formalisms which allow nonterminals to cover discontinuous spans in either language (Melamed et al., 2004; Wellington et al., 2006). LCFRS provides a very general formalism which subsumes SCFGs, the Multitext Grammars of Melamed et al. (2004), as well as mildly </context>
<context position="17296" citStr="Kuhlmann and Nivre, 2006" startWordPosition="2898" endWordPosition="2901">ted by our algorithm remains a function of the grammar, rather than an intrinsic function of the language. One can produce arbitrarily complex grammars that generate the simple language a∗. Thus the parsing complexity of a grammar, like its rank and fan-out, can be said to categorize its strong generative capacity. 7 Experiments A number of recent papers have examined dynamic programming algorithms for parsing non-projective dependency structures by exploring how well various categories of polynomially-parsable grammars cover the structures found in dependency treebanks for various languages (Kuhlmann and Nivre, 2006; Gómez-Rodríguez et al., 2009b). Kuhlmann and Satta (2009) give an algorithm for extracting LCFRS rules from dependency structures. One rule is extracted for each word in the dependency tree. The rank of the rule is the number of children that the word has in the dependency tree, as shown by the example in Figure 4. The fan-out of the symbol corresponding to a word is the number of continuous intervals in the sentence formed by the word and its descendants in the tree. Projec774 complexity arabic czech danish dutch german port swedish 20 1 18 1 16 1 15 1 13 1 12 2 3 11 1 1 1 10 2 6 16 3 9 7 4</context>
<context position="19868" citStr="Kuhlmann and Nivre, 2006" startWordPosition="3351" endWordPosition="3354">ebanks for nine languages and find that all dependency structures meet the mildly ill-nested condition in the dependency treebanks for some gap degree. However, they do not report the maximum gap degree or parsing complexity. We extracted LCFRS rules from dependency treebanks using the same procedure as Kuhlmann and Satta (2009), and applied the algorithm of Figure 1 directly to calculate their minimum parsing complexity. This allows us to characterize the parsing complexity of the rules found in the treebank without needing to define specific conditions on the rules, such as well-nestedness (Kuhlmann and Nivre, 2006) or mildly ill-nestedness, that may not be necessary for all efficiently parsable grammars. The numbers of rules of different complexities are shown in Table 1. As found by previous studies, the vast majority of productions are context-free (projective trees, parsable in O(n3)). Of non-projective rules, the vast majority can be parsed in O(n6), including the well-nested structures of gap degree one defined by Kuhlmann and Nivre (2006). The single most complex rule had parsing complexity of O(n20), and was derived from a Swedish sentence which turns out to be so garbled as to be incomprehensibl</context>
</contexts>
<marker>Kuhlmann, Nivre, 2006</marker>
<rawString>Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-projective dependency structures. In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06), pages 507–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
</authors>
<title>Treebank grammar techniques for non-projective dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL-09),</booktitle>
<pages>478--486</pages>
<contexts>
<context position="15336" citStr="Kuhlmann and Satta, 2009" startWordPosition="2594" endWordPosition="2597">h languages of rank r and fan-out f having greater generative capacity than both grammars of rank r and fan-out f −1 and grammars of rank r −1 773 nmod sbj root vc pp nmod np tmp A hearing is scheduled on the issue today nmod g1 g1 = (A) sbj g2(nmod, pp) g2((x1,1), (x2,1)) = (x1,1 hearing, x2,1) root g3(sbj, vc) g3((x1,1, x1,2), (x2,1, x2,2)) = (x1,1 is x2,1x1,2x2,2) vc g4(tmp) g4((x1,1)) = (scheduled, x1,1) pp g5(tmp) g5((x1,1)) = ( on x1,1) nmod g6 g6 = ( the) np g7(nmod) g7((x1,1)) = (x1,1 issue) tmp g8 g8 = ( today) Figure 4: A dependency tree with the LCFRS rules extracted for each word (Kuhlmann and Satta, 2009). and fan-out f, with two exceptions: with fan-out 1, all ranks greater than one are equivalent (contextfree languages), and with fan-out 2, rank 2 and rank 3 are equivalent. This classification is somewhat unsatisfying because minor changes to a grammar can change both its rank and fan-out. In particular, through factorizing rules, it is always possible to decrease rank, potentially at the cost of increasing fan-out, until a binarized grammar of rank 2 is achieved. Parsing complexity, as defined above, also provides a method to compare the generative capacity of LCFRS grammars. From Rambow an</context>
<context position="17355" citStr="Kuhlmann and Satta (2009)" startWordPosition="2906" endWordPosition="2909">ther than an intrinsic function of the language. One can produce arbitrarily complex grammars that generate the simple language a∗. Thus the parsing complexity of a grammar, like its rank and fan-out, can be said to categorize its strong generative capacity. 7 Experiments A number of recent papers have examined dynamic programming algorithms for parsing non-projective dependency structures by exploring how well various categories of polynomially-parsable grammars cover the structures found in dependency treebanks for various languages (Kuhlmann and Nivre, 2006; Gómez-Rodríguez et al., 2009b). Kuhlmann and Satta (2009) give an algorithm for extracting LCFRS rules from dependency structures. One rule is extracted for each word in the dependency tree. The rank of the rule is the number of children that the word has in the dependency tree, as shown by the example in Figure 4. The fan-out of the symbol corresponding to a word is the number of continuous intervals in the sentence formed by the word and its descendants in the tree. Projec774 complexity arabic czech danish dutch german port swedish 20 1 18 1 16 1 15 1 13 1 12 2 3 11 1 1 1 10 2 6 16 3 9 7 4 1 8 4 7 129 65 10 7 3 12 89 30 18 6 178 11 362 1811 492 59</context>
<context position="18686" citStr="Kuhlmann and Satta (2009)" startWordPosition="3161" endWordPosition="3164">75 41245 Table 1: Number of productions with specified parsing complexity tive trees yield LCFRS rules of fan-out one and parsing complexity three, while the fan-out and parsing complexity from non-projective trees are in principle unbounded. Extracting LCFRS rules from treebanks allows us to study how many of the rules fall within certain constraints. Kuhlmann and Satta (2009) give an algorithm for binarizing LCRFS rules without increasing the rules’ fan-out; however, this is not always possible, and the algorithm does not succeed even in some cases for which such a binarization is possible. Kuhlmann and Satta (2009) find that all but 0.02% of productions in the CoNLL 2006 training data, which includes various languages, can be binarized by their algorithm, but they do not give the fan-out or parsing complexity of the resulting rules. In related work, Gómez-Rodríguez et al. (2009b) define the class of mildly ill-nested dependency structures of varying gap degrees; gap degree is essentially fanout minus one. For a given gap degree k, this class of grammars can be parsing in time O(n3k+4) for lexicalized grammars. Gómez-Rodríguez et al. (2009b) study dependency treebanks for nine languages and find that all</context>
</contexts>
<marker>Kuhlmann, Satta, 2009</marker>
<rawString>Marco Kuhlmann and Giorgio Satta. 2009. Treebank grammar techniques for non-projective dependency parsing. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL-09), pages 478– 486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP).</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Giorgio Satta</author>
<author>Ben Wellington</author>
</authors>
<title>Generalized multitext grammars.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Conference of the Association for Computational Linguistics (ACL-04),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1914" citStr="Melamed et al., 2004" startWordPosition="285" endWordPosition="288">2 while minimizing its fan-out. The work was motivated by the desire to develop efficient chart-parsing algorithms for non-projective dependency trees (Kuhlmann and Nivre, 2006) that do not rely on the independence assumptions of spanning tree algorithms (McDonald et al., 2005). Efficient parsing algorithms for general LCFRS are also relevant in the context of Synchronous Context-Free Grammars (SCFGs) as a formalism for machine translation, as well as the desire to handle even more general synchronous grammar formalisms which allow nonterminals to cover discontinuous spans in either language (Melamed et al., 2004; Wellington et al., 2006). LCFRS provides a very general formalism which subsumes SCFGs, the Multitext Grammars of Melamed et al. (2004), as well as mildly context-sensitive monolingual formalisms such as Tree Adjoining Grammar (Joshi and Schabes, 1997). Thus, work on transforming general LCFRS grammars promises to be widely applicable in both understanding how these formalisms interrelate, and, from a more practical viewpoint, deriving efficient parsing algorithms for them. In this paper, we focus on the problem of transforming an LCFRS grammar into an equivalent grammar for which straightfo</context>
</contexts>
<marker>Melamed, Satta, Wellington, 2004</marker>
<rawString>I. Dan Melamed, Giorgio Satta, and Ben Wellington. 2004. Generalized multitext grammars. In Proceedings of the 42nd Annual Conference of the Association for Computational Linguistics (ACL-04), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>Giorgio Satta</author>
</authors>
<title>Independent parallelism in finite copying parallel rewriting systems.</title>
<date>1999</date>
<journal>Theor. Comput. Sci.,</journal>
<pages>223--1</pages>
<contexts>
<context position="14535" citStr="Rambow and Satta (1999)" startWordPosition="2449" endWordPosition="2452">is fan-out, the minimum parsing complexity is 15. This binarization (not pictured) first joins B1 and B2, then adds B4, and finally adds B3. Given the incompatibility of optimizing time complexity and fan-out, which corresponds to space complexity, which should we prefer? In some situations, it may be desirable to find some trade-off between the two. It is important to note, however, that if optimization of space complexity is the sole objective, factorization is unnecessary, as one can never improve on the fan-out required by the original grammar nonterminals. 6 A Note on Generative Capacity Rambow and Satta (1999) categorize the generative capacity of LCFRS grammars according to their rank and fan-out. In particular, they show that grammars can be arranged in a two-dimensional grid, with languages of rank r and fan-out f having greater generative capacity than both grammars of rank r and fan-out f −1 and grammars of rank r −1 773 nmod sbj root vc pp nmod np tmp A hearing is scheduled on the issue today nmod g1 g1 = (A) sbj g2(nmod, pp) g2((x1,1), (x2,1)) = (x1,1 hearing, x2,1) root g3(sbj, vc) g3((x1,1, x1,2), (x2,1, x2,2)) = (x1,1 is x2,1x1,2x2,2) vc g4(tmp) g4((x1,1)) = (scheduled, x1,1) pp g5(tmp) g</context>
</contexts>
<marker>Rambow, Satta, 1999</marker>
<rawString>Owen Rambow and Giorgio Satta. 1999. Independent parallelism in finite copying parallel rewriting systems. Theor. Comput. Sci., 223(1-2):87–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Seki</author>
<author>T Matsumura</author>
<author>M Fujii</author>
<author>T Kasami</author>
</authors>
<title>On multiple context-free grammars.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<pages>88--191</pages>
<contexts>
<context position="6311" citStr="Seki et al., 1991" startWordPosition="1045" endWordPosition="1048">nct endpoints in the most complex production. Thus, for input of length n, the complexity is O(nc) for some constant c which depends on the grammar. For a given rule, each of the r nonterminals has cp(Bi) spans, and each span has a left and right endpoint, giving an upper bound of c &lt; 2 Eri=1 cp(Bi). However, some of these endpoints may be shared between nonterminals on the righthand side. The exact number of distinct variables for the dynamic programming deduction rule can the written r c(p) = cp(A) + cp(Bi) (2) i=1 where c(p) is the parsing complexity of a production p of the form of eq. 1 (Seki et al., 1991). To see this, consider counting the left endpoint of each span on the lefthand side of the production, and the right endpoint of each span on the righthand side of the production. Any variable corresponding to the left endpoint of a span of a righthand side nonterminal will either be shared with the right endpoint of another span if two spans are being joined by the production, or, alternatively, will form the left endpoint of a span of A. Thus, each distinct endpoint in the production is counted exactly once by eq. 2. The parsing complexity of a grammar, c(G), is the maximum parsing complexi</context>
</contexts>
<marker>Seki, Matsumura, Fujii, Kasami, 1991</marker>
<rawString>H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991. On multiple context-free grammars. Theoretical Computer Science, 88:191–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shankar</author>
<author>D L Weir</author>
<author>A K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Conference of the Association for Computational Linguistics (ACL-87).</booktitle>
<contexts>
<context position="1249" citStr="Vijay-Shankar et al., 1987" startWordPosition="180" endWordPosition="183">sing complexity. We show that it is always possible to obtain optimum parsing complexity with rank two. However, among transformed grammars of rank two, minimum parsing complexity is not always possible with minimum fan-out. Applying our factorization algorithm to LCFRS rules extracted from dependency treebanks allows us to find the most efficient parsing strategy for the syntactic phenomena found in non-projective trees. 1 Introduction Gómez-Rodríguez et al. (2009a) recently examined the problem of transforming arbitrary grammars in the Linear Context-Free Rewriting System (LCFRS) formalism (Vijay-Shankar et al., 1987) in order to reduce the rank of a grammar to 2 while minimizing its fan-out. The work was motivated by the desire to develop efficient chart-parsing algorithms for non-projective dependency trees (Kuhlmann and Nivre, 2006) that do not rely on the independence assumptions of spanning tree algorithms (McDonald et al., 2005). Efficient parsing algorithms for general LCFRS are also relevant in the context of Synchronous Context-Free Grammars (SCFGs) as a formalism for machine translation, as well as the desire to handle even more general synchronous grammar formalisms which allow nonterminals to c</context>
</contexts>
<marker>Vijay-Shankar, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shankar, D. L. Weir, and A. K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In Proceedings of the 25th Annual Conference of the Association for Computational Linguistics (ACL-87).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Wellington</author>
<author>Sonjia Waxmonsky</author>
<author>I Dan Melamed</author>
</authors>
<title>Empirical lower bounds on the complexity of translational equivalence.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<pages>977--984</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1940" citStr="Wellington et al., 2006" startWordPosition="289" endWordPosition="292"> fan-out. The work was motivated by the desire to develop efficient chart-parsing algorithms for non-projective dependency trees (Kuhlmann and Nivre, 2006) that do not rely on the independence assumptions of spanning tree algorithms (McDonald et al., 2005). Efficient parsing algorithms for general LCFRS are also relevant in the context of Synchronous Context-Free Grammars (SCFGs) as a formalism for machine translation, as well as the desire to handle even more general synchronous grammar formalisms which allow nonterminals to cover discontinuous spans in either language (Melamed et al., 2004; Wellington et al., 2006). LCFRS provides a very general formalism which subsumes SCFGs, the Multitext Grammars of Melamed et al. (2004), as well as mildly context-sensitive monolingual formalisms such as Tree Adjoining Grammar (Joshi and Schabes, 1997). Thus, work on transforming general LCFRS grammars promises to be widely applicable in both understanding how these formalisms interrelate, and, from a more practical viewpoint, deriving efficient parsing algorithms for them. In this paper, we focus on the problem of transforming an LCFRS grammar into an equivalent grammar for which straightforward application of dynam</context>
</contexts>
<marker>Wellington, Waxmonsky, Melamed, 2006</marker>
<rawString>Benjamin Wellington, Sonjia Waxmonsky, and I. Dan Melamed. 2006. Empirical lower bounds on the complexity of translational equivalence. In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06), pages 977–984, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>