<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.552862" genericHeader="method">
COMMONSENSE METAPHYSICS AND LEXICAL SEMANTICS
</sectionHeader>
<author confidence="0.905693">
Jerry R. Hobbs, William Croft, Todd Davies,
Douglas Edwards, and Kenneth Laws
</author>
<sectionHeader confidence="0.917893" genericHeader="method">
Artificial Intelligence Center
SRI International
</sectionHeader>
<bodyText confidence="0.9932852">
In the TACITUS project for using commonsense knowledge in the understanding of texts about
mechanical devices and their failures, we have been developing various commonsense theories that are
needed to mediate between the way we talk about the behavior of such devices and causal models of their
operation. Of central importance in this effort is the axiomatization of what might be called
&amp;quot;commonsense metaphysics&amp;quot;. This includes a number of areas that figure in virtually every domain of
discourse, such as granularity, scales, time, space, material, physical objects, shape, causality,
functionality, and force. Our effort has been to construct core theories of each of these areas, and then
to define, or at least characterize, a large number of lexical items in terms provided by the core theories.
In this paper we discuss our methodological principles and describe the key ideas in the various domains
we are investigating.
</bodyText>
<sectionHeader confidence="0.998403" genericHeader="method">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999880641509434">
In the TACITUS project for using commonsense knowl-
edge in the understanding of texts about mechanical
devices and their failures, we have been developing
various commonsense theories that are needed to me-
diate between the way we talk about the behavior of
such devices and causal models of their operation. Of
central importance in this effort is the axiomatization of
what might be called &amp;quot;commonsense metaphysics&amp;quot;.
This includes a number of areas that figure in virtually
every domain of discourse, such as scalar notions,
granularity, time, space, material, physical objects,
causality, functionality, force, and shape. Our approach
to lexical semantics is to construct core theories of each
of these areas, and then to define, or at least character-
ize, a large number of lexical items in terms provided by
the core theories. In the TACITUS system, processes
for solving pragmatics problems posed by a text will use
the knowledge base consisting of these theories, in
conjunction with the logical forms of the sentences in
the text, to produce an interpretation. In this paper we
do not stress these interpretation processes; this is
another, important aspect of the TACITUS project, and
it will be described in subsequent papers (Hobbs and
Martin, 1987).
This work represents a convergence of research in
lexical semantics in linguistics and efforts in artificial
intelligence to encode commonsense knowledge. Over
the years, lexical semanticists have developed formal-
isms of increasing adequacy for encoding word mean-
ing, progressing from simple sets of features (Katz and
Fodor, 1963) to notations for predicate-argument struc-
ture (Lakoff, 1972; Miller and Johnson-Laird, 1976), but
the early attempts still limited access to world knowl-
edge and assumed only very restricted sorts of process-
ing. Workers in computational linguistics introduced
inference (Rieger, 1974; Schank, 1975) and other com-
plex cognitive processes (Herskovits, 1982) into our
understanding of the role of word meaning. Recently
linguists have given greater attention to the cognitive
processes that would operate on their representations
(e.g., Talmy, 1983; Croft, 1986). Independently, in arti-
ficial intelligence an effort arose to encode large amounts
of commonsense knowledge (Hayes, 1979; Hobbs and
Moore, 1985; Hobbs et al. 1985). The research reported
here represents a convergence of these various devel-
opments. By constructing core theories of certain fun-
damental phenomena and defining lexical items within
these theories, using the full power of predicate calcu-
lus, we are able to cope with complexities of word
meaning that have hitherto escaped lexical semanticists.
Moreover, we can do this within a framework that gives
full scope to the planning and reasoning processes that
manipulate representations of word meaning.
</bodyText>
<footnote confidence="0.860262">
Copyright 1987 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided
that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To
copy otherwise, or to republish, requires a fee and/or specific permission.
0362-613X/ 87 /030241-250$03.00
</footnote>
<note confidence="0.821188">
Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 241
Jerry R. Hobbs et al. Commonsense Metaphysics and Lexical Semantics
</note>
<bodyText confidence="0.9916715">
In constructing the core theories we are attempting to
adhere to several methodological principles:
I. One should aim for characterization of concepts,
rather than definition. One cannot generally expect to
find necessary and sufficient conditions for a concept.
The most we can hope for is to find a number of
necessary conditions and a number of sufficient condi-
tions. This amounts to saying that a great many predi-
cates are primitives, but they are primitives that are
highly interrelated with the rest of the knowledge base.
</bodyText>
<listItem confidence="0.929646">
2. One should determine the minimal structure nec-
essary for a concept to make sense. In efforts to
</listItem>
<bodyText confidence="0.987840107142857">
axiomatize an area, there are two positions one may
take, exemplified by set theory and by group theory. In
axiomatizing set theory, one attempts to capture exactly
some concept that one has strong intuitions about. If the
axiomatization turns out to have unexpected models,
this exposes an inadequacy. In group theory, by con-
trast, one characterizes an abstract class of structures.
If it turns out that there are unexpected models, this is
a serendipitous discovery of a new phenomenon that we
can reason about using an old theory. The pervasive
character of metaphor in natural language discourse
shows that our commonsense theories of the world
ought to be much more like group theory than set
theory. By seeking minimal structures in axiomatizing
concepts, we optimize the possibilities of using the
theories in metaphorical and analogical contexts. This
principle is illustrated below in the section on regions.
One consequence of this principle is that our approach
will seem more syntactic than semantic. We have
concentrated more on specifying axioms than on con-
structing models. Our view is that the chief role of
models in our effort is for proving the consistency and
independence of sets of axioms, and for showing their
adequacy. As an example of the last point, many of the
spatial and temporal theories we construct are intended
at least to have Euclidean space or the real numbers as
one model, and a subclass of graph-theoretical struc-
tures as other models.
</bodyText>
<listItem confidence="0.9736345">
3. A balance must be struck between attempting to
cover all cases and aiming only for the prototypical
</listItem>
<bodyText confidence="0.9972922">
cases. In general, we have tried to cover as many cases
as possible with an elegant axiomatization, in line with
the two previous principles, but where the formalization
begins to look baroque, we assume that higher pro-
cesses will block some inferences in the marginal cases.
We assume that inferences will be drawn in a controlled
fashion. Thus, every outré, highly context-dependent
counterexample need not be accounted for, and to a
certain extent, definitions can be geared specifically to a
prototype.
</bodyText>
<listItem confidence="0.914756447368421">
4. Where competing ontologies suggest themselves in
a domain, one should try to construct a theory that
accommodates both. Rather than commit oneself to
adopting one set of primitives rather than another, one
should show how either set can be characterized in
terms of the other. Generally, each of the ontologies is
useful for different purposes, and it is convenient to be
able to appeal to both. Our treatment of time illustrates
this.
5. The theories one constructs should be richer in
axioms than in theorems. In mathematics, one expects
to state half a dozen axioms and prove dozens of
theorems from them. In encoding commonsense knowl-
edge, it seems to be just the opposite. The theorems we
seek to prove on the basis of these axioms are theorems
about specific situations that are to be interpreted, in
particular, theorems about a text that the system is
attempting to understand.
6. One should avoid falling into &amp;quot;black holes&amp;quot;. There
are a few &amp;quot;mysterious&amp;quot; concepts that crop up repeat-
edly in the formalization of commonsense metaphysics.
Among these are &amp;quot;relevant&amp;quot; (that is, relevant to the
task at hand) and &amp;quot;normative&amp;quot; (that is, conforming to
some norm or pattern). To insist upon giving a satisfac-
tory analysis of these before using them in analyzing
other concepts is to cross the event horizon that sepa-
rates lexical semantics from philosophy. On the other
hand, our experience suggests that to avoid their use
entirely is crippling; the lexical semantics of a wide
variety of other terms depends upon them. Instead, we
have decided to leave them minimally analyzed for the
moment and use them without scruple in the analysis of
other commonsense concepts. This approach will allow
us to accumulate many examples of the use of these
mysterious concepts, and in the end, contribute to their
successful analysis. The use of these concepts appears
below in the discussions of the words &amp;quot;immediately&amp;quot;,
&amp;quot;sample&amp;quot;, and &amp;quot;operate&amp;quot;.
</listItem>
<bodyText confidence="0.999968727272727">
We chose as an initial target the problem of encoding
the commonsense knowledge that underlies the concept
of &amp;quot;wear&amp;quot;, as in a part of a device wearing out. Our aim
was to define &amp;quot;wear&amp;quot; in terms of predicates character-
ized elsewhere in the knowledge base and to be able to
infer some consequences of wear. For something to
wear, we decided, is for it to lose imperceptible bits of
material from its surface due to abrasive action over
time. One goal, which we have not yet achieved, is to be
able to prove as a theorem that, since the shape of a part
of a mechanical device is often functional and since loss
of material can result in a change of shape, wear of a
part of a device can cause the failure of the device as a
whole. In addition, as we have proceeded, we have
characterized a number of words found in a set of target
texts, as it has become possible.
We are encoding the knowledge as axioms in what is
for the most part a first-order logic, described by Hobbs
(1985a), although quantification over predicates is
sometimes convenient. In the formalism there is a
nominalization operator &amp;quot; &apos; &amp;quot; for reifying events and
conditions, as expressed in the following axiom schema:
</bodyText>
<equation confidence="0.61121">
(Vx)p(x) (3e)pi(e,x) A Exist(e)
That is, p is true of x if and only if there is a condition
e of p&apos;s being true of x and e exists in the real world.
</equation>
<page confidence="0.898698">
242 Computational Linguistics Volume 13, Numbers 3-4, July-December 1987
</page>
<note confidence="0.674607">
Jerry R. Hobbs et al. Commonsense Metaphysics and Lexical Semantics
</note>
<bodyText confidence="0.999854">
In our implementation so far, we have been proving
simple theorems from our axioms using the CG5 theo-
rem-prover developed by Mark Stickel (1982), and we
are now beginning to use the knowledge base in text
processing.
</bodyText>
<sectionHeader confidence="0.992933" genericHeader="method">
2 REQUIREMENTS ON ARGUMENTS OF PREDICATES
</sectionHeader>
<bodyText confidence="0.999947571428571">
There is a notational convention used below that de-
serves some explanation. It has frequently been noted
that relational words in natural language can take only
certain types of words as their arguments. These are
usually described as selectional constraints. The same is
true of predicates in our knowledge base. The con-
straints are expressed below by rules of the form
</bodyText>
<equation confidence="0.994557">
p(x,y) : r(x,y)
</equation>
<bodyText confidence="0.99574825">
This means that for p even to make sense applied to x
and y, it must be the case that r is true of x and y. The
logical import of this rule is that wherever there is an
axiom of the form
</bodyText>
<equation confidence="0.984393333333333">
(V x,y)p(x,y) D q(x,y)
this is really to be read as
(V x,y)p(x,y) A r(x,y) D q(x,y)
</equation>
<bodyText confidence="0.999771083333333">
The checking of selectional constraints, therefore,
emerges as a by-product of other logical operations: the
constraint r(x,y) must be verified if anything else is to be
proved from p(x,y).
The simplest example of such an r(x,y) is a conjunc-
tion of sort constraints r1(x) A r2(y). Our approach is a
generalization of this, because much more complex
requirements can be placed on the arguments. Con-
sider, for example, the verb &amp;quot;range&amp;quot;. If x ranges from y
to z, there must be a scale s that includes y and z, and x
must be a set of entities that are located at various
places on the scale. This can be represented as follows:
</bodyText>
<equation confidence="0.9316275">
range(x,y,z) : (3 s) [scale(s) A y E sA zEsA set(x)
A (V u)[u E x D (3 v) vEsA at(u,v)]]
</equation>
<sectionHeader confidence="0.97779" genericHeader="method">
3 THE KNOWLEDGE BASE
</sectionHeader>
<subsectionHeader confidence="0.967757">
3.1 SETS AND GRANULARITY
</subsectionHeader>
<bodyText confidence="0.999855318181818">
At the foundation of the knowledge base is an axioma-
tization of set theory. It follows the standard Zermelo-
Fraenkel approach, except that there is no axiom of
infinity.
Since so many concepts used in discourse are grain-
dependent, a theory of granularity is also fundamental
(see Hobbs 1985b). A grain is defined in terms of an
indistinguishability relation, which is reflexive and sym-
metric, but not necessarily transitive. One grain can be
a refinement of another, with the obvious definition.
The most refined grain is the identity grain, i.e., the one
in which every two distinct elements are distinguish-
able. One possible relationship between two grains, one
of which is a refinement of the other, is what we call an
&amp;quot;Archimedean relation&amp;quot;, after the Archimedean prop-
erty of real numbers. Intuitively, if enough events occur
that are imperceptible at the coarser grain g2 but per-
ceptible at the finer grain go the aggregate will eventu-
ally be perceptible at the coarser grain. This is an
important property in phenomena subject to the heap
paradox. Wear, for instance, eventually has significant
consequences.
</bodyText>
<subsectionHeader confidence="0.999145">
3.2 SCALES
</subsectionHeader>
<bodyText confidence="0.9999749">
A great many of the most common words in English
have scales as their subject matter. This includes many
prepositions, the most common adverbs, comparatives,
and many abstract verbs. When spatial vocabulary is
used metaphorically, it is generally the scalar aspect of
space that carries over to the target domain. A scale is
defined as a set of elements, together with a partial
ordering and a granularity (or an indistinguishability
relation). The partial ordering and the indistinguishabil-
ity relation are consistent with each other:
</bodyText>
<equation confidence="0.5408165">
(Vx,y,z)x&lt;yAy—zDx&lt;zVx—z
That is, if x is less than y and y is indistinguishable from
z, then either x is less than z or x is indistinguishable
from z.
</equation>
<bodyText confidence="0.986873571428571">
It is useful to have an adjacency relation between
points on a scale, and there are a number of ways we
could introduce it. We could simply take it to be
primitive; in a scale having a distance function, we
could define two points to be adjacent when the distance
between them is less than some E; finally, we could
define adjacency in terms of the grain size for the scale:
</bodyText>
<equation confidence="0.995769">
(V x,y,$) adj(x,y,$)
(3z) z x A z y A -1 yl,
</equation>
<bodyText confidence="0.995782">
That is, distinguishable elements x and y are adjacent on
scale s if and only if there is an element z which is
indistinguishable from both.
Two important possible properties of scales are con-
nectedness and denseness. We can say that two ele-
ments of a scale are connected by a chain of adj
relations:
</bodyText>
<equation confidence="0.9994295">
(Vx,y,$)connected(x,y,$) =--
adj(x,y,$) V (3z)adj(x,z,$) A connected(z,y,$)
</equation>
<bodyText confidence="0.99971225">
A scale is connected (sconnected) if all pairs of elements
are connected. A scale is dense if between any two
points there is a third point, until the two points are so
close together that the grain size no longer allows us to
determine whether such an intermediate point exists.
Cranking up the magnification could well resolve the
continuous space into a discrete set, as objects into
atoms.
</bodyText>
<equation confidence="0.394597333333333">
(Vs)dense(s)
(Vx,y)xEsAyEsAx&lt;sy
DOZXX &lt;, z A z &lt;s y) V (3z)(x —s z A z Y)
</equation>
<bodyText confidence="0.949279">
This expresses the commonsense notion of continuity.
Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 243
Jerry R. Hobbs et al. Commonsense Metaphysics and Lexical Semantics
A subscale of a scale has as its elements a subset of
the elements of the scale and has as its partial ordering
and its grain the partial ordering and the grain of the
scale.
</bodyText>
<equation confidence="0.7799228">
(V si,s2)subscale(s2,si)E--- subset(s2,s1)
A (Vx,y)[[x &lt;$1 y = x &lt;$2 y] A [x —si Y = x =s2 Y]]
An interval can be defined as a connected subscale:
(Vi)interval(i) = (3s)scale(s)
A subscale(i,$) A sconnected(i)
</equation>
<bodyText confidence="0.999930411764706">
The relations between time intervals that Allen and
Kautz (1985) have defined can be defined in a straight-
forward manner in the approach presented here, but for
intervals in general.
A concept closely related to scales is that of a
&amp;quot;cycle&amp;quot;. This is a system that has a natural ordering
locally but contains a loop globally. Examples are the
color wheel, clock times, and geographical locations
ordered by &amp;quot;east of&apos;. We have axiomatized cycles in
terms of a ternary between relation whose axioms
parallel those for a partial ordering.
The figure-ground relationship is of fundamental im-
portance in language. We encode it with the primitive
predicate at. It is possible that the minimal structure
necessary for something to be a ground is that of a scale;
hence, this is a selectional constraint on the arguments
of at.&apos;
</bodyText>
<equation confidence="0.939792">
at(x,y) : (3s)y E s A scale(s)
</equation>
<bodyText confidence="0.999543333333333">
At this point, we are already in a position to define some
fairly complex words. As an illustration, we give the
example of &amp;quot;range&amp;quot; as in &amp;quot;x ranges from y to z&amp;quot;:
</bodyText>
<equation confidence="0.8292734">
(Vx,y,z)range(x,Y,z)
(3s,s1,u1,u2)scale(s) A subscale(st,$)
A bottom(y,si) A top(z,s1)
Au, E x A at(ut,y) A u2 E x A at(u2,z)
A (Vu)[u E x D (3v)v E si A at(u,v)}
</equation>
<bodyText confidence="0.997476466666667">
That is, x ranges from y to z if and only if y and z are the
bottom and top of a subscale s1 of some scale s and x is
a set which has elements at y and z and all of whose
elements are located at points on sr
A very important scale is the linearly ordered scale of
numbers. We do not plan to reason axiomatically about
numbers, but it is useful in natural language processing
to have encoded a few facts about numbers. For exam-
ple, a set has a cardinality which is an element of the
number scale.
Verticality is a concept that would most properly be
analyzed in the section on space, but it is a property that
many other scales have acquired metaphorically, for
whatever reason. The number scale is one of these.
Even in the absence of an analysis of verticality, it is a
</bodyText>
<footnote confidence="0.701619333333333">
1 However, we are currently examining an approach in which a more
abstract concept, &amp;quot;system&amp;quot;, discussed in Section 3.6.3, is taken to be
the minimal structure for expressing location.
</footnote>
<bodyText confidence="0.999611173913044">
useful property to have as a primitive in lexical seman-
tics.
The word &amp;quot;high&amp;quot; is a vague term asserting that an
entity is in the upper region of some scale. It requires
that the scale be a vertical one, such as the number
scale. The verticality requirement distinguishes &amp;quot;high&amp;quot;
from the more general term &amp;quot;very&amp;quot;; we can say &amp;quot;very
hard&amp;quot; but not &amp;quot;highly hard&amp;quot;. The phrase &amp;quot;highly
planar&amp;quot; sounds all right because the high register of
&amp;quot;planar&amp;quot; suggests a quantifiable, scientific accuracy,
whereas the low register of &amp;quot;flat&amp;quot; makes &amp;quot;highly flat&amp;quot;
sound much worse.
The test of any definition is whether it allows one to
draw the appropriate inferences. In our target texts, the
phrase &amp;quot;high usage&amp;quot; occurs. Usage is a set of using
events, and the verticality requirement on &amp;quot;high&amp;quot;
forces us to coerce the phrase into &amp;quot;a high or large
number of using events&amp;quot;. Combining this with an axiom
stating that the use of a mechanical device involves the
likelihood of abrasive events, as defined below, and
with the definition of &amp;quot;wear&amp;quot; in terms of abrasive
events, we should be able to conclude the likelihood of
wear.
</bodyText>
<subsectionHeader confidence="0.997653">
3.3 TIME: TWO ONTOLOGIES
</subsectionHeader>
<bodyText confidence="0.999230818181818">
There are two possible ontologies for time. In the first,
the one most acceptable to the mathematically minded,
there is a time line, which is a scale having some
topological structure. We can stipulate the time line to
be linearly ordered (although it is not in approaches that
build ignorance of relative times into the representation
of time (e.g., Hobbs, 1974) nor in approaches employing
branching futures (e.g., McDermott, 1985)), and we can
stipulate it to be dense (although it is not in the situation
calculus). We take before to be the ordering on the time
line:
</bodyText>
<equation confidence="0.936778">
(Vt1,t2)before(t1 ,t2) ------
(3T)Time-line(T) A t1 E T A t2 E T A ti &lt; T t2
</equation>
<bodyText confidence="0.999969444444444">
We allow both instants and intervals of time. Most
events occur at some instant or during some interval. In
this approach, nearly every predicate takes a time
argument.
In the second ontology, the one that seems to be
more deeply rooted in language, the world consists of a
large number of more or less independent processes, or
histories, or sequences of events. There is a primitive
relation change between conditions. Thus,
</bodyText>
<equation confidence="0.822405">
chartge(ei,e2) A pi(el,x) A q&apos;(e2,x)
</equation>
<bodyText confidence="0.999876444444444">
says that there is a change from the condition el of p&apos;s
being true of x to the condition e2 of q&apos;s being true of x.
The time line in this ontology is then an artificial
construct, a regular sequence of imagined abstract
events (think of them as ticks of a clock in the National
Bureau of Standards) to which other events can be
related. The change ontology seems to correspond to
the way we experience the world. We recognize rela-
tions of causality, change of state, and copresence
</bodyText>
<page confidence="0.808425">
244 Computational Linguistics Volume 13, Numbers 3-4, July-December 1987
</page>
<note confidence="0.872264">
Jerry R. Hobbs et al. Commonsense Metaphysics and Lexical Semantics
</note>
<bodyText confidence="0.999876133333333">
among events and conditions. When events are not
related in these ways, judgments of relative time must
be mediated by copresence relations between the events
and events on a clock and change of state relations on
the clock.
The predicate change possesses a limited transitiv-
ity. There has been a change from Reagan&apos;s being an
actor to Reagan&apos;s being president, even though he was
governor in between. But we probably do not want to
say there has been a change from Reagan&apos;s being an
actor to Margaret Thatcher&apos;s being prime minister, even
though the second event comes after the first.
In this ontology, we can say that any two times,
viewed as events, always have a change relation be-
tween them.
</bodyText>
<equation confidence="0.93028325">
(Vt1,t2)before(t1,t2) D change(tt,t2)
The predicate change is related to before by the axiom
(Ve1,e2) change(e1,e2) D
(3t1,t2) at(ei,ti) A at(e2,t2) A before(ti,t2)
</equation>
<bodyText confidence="0.9982684">
That is, if there is a change from el to e2, then there is
a time t1 at which el occurred and a time t2 at which e2
occurred, and ti is before t2. This does not allow us to
derive change of state from temporal succession. For
this, we would need axioms of the form
</bodyText>
<equation confidence="0.935370666666667">
(V ei,e2,t ht2,x) p&apos;(ei,x) A at(e
A q&apos;(e2,x) A at(e2,t2) A before(ti,t2)
D change(e1,e2)
</equation>
<bodyText confidence="0.980799">
That is, if x is p at time 11 and q at a later time t2, then
there has been a change of state from one to the other.
This axiom would not necessarily be true for all p&apos;s and
q&apos;s. Time arguments in predications can be viewed as
abbreviations:
</bodyText>
<equation confidence="0.955553">
(V x,t)p(x,t) (3e)p&apos;(e,x) A at(e,t)
</equation>
<bodyText confidence="0.999402666666667">
The word &amp;quot;move&amp;quot;, or the predicate move, (as in &amp;quot;x
moves from y to z&amp;quot;) can then be defined equivalently in
terms of change,
</bodyText>
<equation confidence="0.9476352">
(V x,y,z)move(x,y,z) =-
(2e1 ,e2) change(e1,e2) A at&apos; (ei,x,y) A at&apos; (e2,x,z)
or in terms of the time line,
(V x,y,z) move(x,y,z)
(3t1,t2) at(x,y,t1) A at(x,z,t2) A before(t1,t2)
</equation>
<bodyText confidence="0.999820393939394">
(The latter definition has to be complicated a bit to
accommodate cyclic motion. The former axiom is all
right as it stands, provided there is also an axiom saying
that for there to be a change from a state to the same
state, there must be an intermediate different state.)
In English and apparently all other natural languages,
both ontologies are represented in the lexicon. The time
line ontology is found in clock and calendar terms, tense
systems of verbs, and in the deictic temporal locatives
such as &amp;quot;yesterday&amp;quot;, &amp;quot;today&amp;quot;, &amp;quot;tomorrow&amp;quot;, &amp;quot;last
night&amp;quot;, and so on. The change ontology is exhibited in
most verbs, and in temporal clausal connectives. The
universal presence in natural languages of both classes
of lexical items and grammatical markers requires a
theory that can accommodate both ontologies, illustrat-
ing the importance of methodological principle 4.
Among temporal connectives, the word &amp;quot;while&amp;quot;
presents interesting problems. In &amp;quot;el while e2&amp;quot;, e2 must
be an event occurring over a time interval; el must be an
event and may occur either at a point or over an
interval. One&apos;s first guess is that the point or interval for
el must be included in the interval for e2. However,
there are cases, such as
The electricity should be off while the switch is being
repaired.
which suggest the reading &amp;quot;e2 is included in e1&amp;quot;. We
came to the conclusion that one can infer no more than
that el and e2 overlap, and any tighter constraints result
from implicatures from background knowledge.
The word &amp;quot;immediately&amp;quot;, as in &amp;quot;immediately after
the alarm&amp;quot;, also presents a number of problems. It
requires its argument e to be an ordering relation
between two entities x and y on some scale s.
</bodyText>
<equation confidence="0.994934">
immediate(e) : (3x,y,$)less-than&apos;(e,x,y,$)
</equation>
<bodyText confidence="0.939605714285714">
It is not clear what the constraints on the scale are.
Temporal and spatial scales are acceptable, as in &amp;quot;im-
mediately after the alarm&amp;quot; and &amp;quot;immediately to the
left&amp;quot;, but the size scale is not:
* John is immediately larger than Bill.
Etymologically, it means that there are no intermediate
entities between x and y on s. Thus,
</bodyText>
<equation confidence="0.997191">
(V e,x,y,$) immediate(e) A less-than&apos;(e,x,y,$)
D (3z)less-than(x,z,$) A less-than(z,y,$)
</equation>
<bodyText confidence="0.999681">
However, this will only work if we restrict z to be a
relevant entity. For example, in the sentence
We disengaged the compressor immediately after the
alarm.
the implication is that no event that could damage the
compressor occurred between the alarm and the disen-
gagement, since the text is about equipment failure.
</bodyText>
<subsectionHeader confidence="0.965836">
3.4 SPACES AND DIMENSION: THE MINIMAL STRUCTURE
</subsectionHeader>
<bodyText confidence="0.9998412">
The notion of dimension has been made precise in linear
algebra. Since the concept of a region is used metaphor-
ically as well as in the spatial sense, however, we were
concerned to determine the minimal structure a system
requires for it to make sense to call it a space of more
than one dimension. For a two-dimensional space, there
must be a scale, or partial ordering, for each dimension.
Moreover, the two scales must be independent, in that
the order of elements on one scale can not be deter-
mined from their order on the other. Formally,
</bodyText>
<figure confidence="0.921568">
(V sp)space(sp)
(3s1,s2) scalei(si,sp) A scale2(s2,sp)
Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 245
Jerry R. Hobbs et al. Commonsense Metaphysics and Lexical Semantics
.0&amp;quot;
..••••
••••
•••••
A ■••••
.,••••
•
•
</figure>
<figureCaption confidence="0.999269">
Figure 1.1 The Simplest Space.
</figureCaption>
<bodyText confidence="0.995059421052632">
A (3x)[(3y1) [x &lt;,, yi A x &lt;2yi]
A (3y2)[x &lt;s, A y2 A y2 &lt;,2 x]]
Note that this does not allow &lt;S2 to be simply the
reverse of &lt;si. An unsurprising consequence of this
definition is that the minimal example of a two-dimen-
sional space consists of three points (three points deter-
mine a plane), e.g., the points A, B, and C, where
A &lt; B, A &lt; C, C &lt; 2 A, A &lt; 2 B.
This is illustrated in Figure 1.
The dimensional scales are apparently found in all
natural languages in relevant domains. The familiar
three-dimensional space of common sense can be de-
fined by the three scale pairs &amp;quot;up-down&amp;quot;, &amp;quot;front-
back&amp;quot;, and &amp;quot;left-right&amp;quot;; the two-dimensional plane of
the commonsense conception of the earth&apos;s surface is
represented by the two scale pairs &amp;quot;north-south&amp;quot; and
&amp;quot;east-west&amp;quot;.
The simplest, although not the only, way to define
adjacency in the space is as adjacency on both scales:
</bodyText>
<equation confidence="0.916507666666667">
(Vx,y,sp)adj(x,y,sp)
(3s1,s2) scalei(si,sp) A scale2(s2,sp)
A adj(x,y,s1) A adj(x,y,s2)
</equation>
<bodyText confidence="0.996681">
A region is a subset of a space. The surface and interior
of a region can be defined in terms of adjacency, in a
manner paralleling the definition of a boundary in point-
set topology. In the following, s is the boundary or
surface of a two- or three-dimensional region r embed-
ded in a space sp.
</bodyText>
<equation confidence="0.987627">
(V s,r,sp)surface(s,r,sp)
(V x)x E r j [x E s -=-
(Ey)(y E sp A 7 (y E r) A adj(x,y,sp))]
</equation>
<bodyText confidence="0.996092">
Finally, we can define the notion of &amp;quot;contact&amp;quot; in terms
of points in different regions being adjacent:
</bodyText>
<equation confidence="0.9847585">
(V r1,r2,sp)contact(r1, r2,sp)
disjoint(r1,r2) A (3 x,y)(x E r1 A y E r2 A adj(x,y,sp))
</equation>
<bodyText confidence="0.998760428571429">
By picking the scales and defining adjacency right, we
can talk about points of contact between communica-
tion networks, systems of knowledge, and other meta-
phorical domains. By picking the scales to be the real
line and defining adjacency in terms of &amp;neighborhoods,
we get Euclidean space and can talk about contact
between physical objects.
</bodyText>
<subsectionHeader confidence="0.925934">
3.5 MATERIAL
</subsectionHeader>
<bodyText confidence="0.999986807692308">
Physical objects and materials must be distinguished,
just as they are in apparently every natural language, by
means of the count noun-mass noun distinction. A
physical object is not a bit of material, but rather is
composed of a bit of material at any given time. Thus,
rivers and human bodies are physical objects, even
though their material constitution changes over time.
This distinction also allows us to talk about an object&apos;s
losing material through wear and still remaining the
same object.
We will say that an entity b is a bit of material by
means of the expression material(b). Bits of material are
characterized by both extension and cohesion. The
primitive predication occupies(b,r,t) encodes extension,
saying that a bit of material b occupies a region r at time
t. The topology of a bit of material is then parasitic on
the topology of the region it occupies. A part 61 of a bit
of material b is a bit of material whose occupied region
is always a subregion of the region occupied by b.
Point-like particles (particle) are defined in terms of
points in the occupied region, disjoint bits (disjointbit)
in terms of the disjointness of regions, and contact
between bits in terms of contact between their regions.
We can then state as follows the principle of non-joint-
occupancy that two bits of material cannot occupy the
same place at the same time:
</bodyText>
<equation confidence="0.999042">
(V bl, b2)(disjointbit(b 1,62)
D(V x,y,b3,b4) interior(b3,b1) A interior(b4,62)
A particle(x,b3) A particle(y,b4)
D (3z)(at(x, z) A at(y, z))
</equation>
<bodyText confidence="0.999166611111111">
That is, if bits 61 and b2 are disjoint, then there is no
entity z that is at interior points in both b1 and b2. At
some future point in our work, this may emerge as a
consequence of a richer theory of cohesion and force.
The cohesion of materials is also a primitive prop-
erty, for we must distinguish between a bump on the
surface of an object and a chip merely lying on the
surface. Cohesion depends on a primitive relation bond
between particles of material, paralleling the role of adj
in regions. The relation attached is defined as the
transitive closure of bond. A topology of cohesion is
built up in a manner analogous to the topology of
regions. In addition, we have encoded the relation that
bond bears to motion, i.e., that bonded bits remain
adjacent and that one moves when the other does, and
the relation of bond to force, i.e. that there is a
characteristic force that breaks a bond in a given
material.
</bodyText>
<page confidence="0.915796">
246 Computational Linguistics Volume 13, Numbers 3-4, July-December 1987
</page>
<note confidence="0.827145">
Jerry R. Hobbs et at. Commonsense Metaphysics and Lexical Semantics
</note>
<bodyText confidence="0.999751025641025">
Different materials react in different ways to forces of
various strengths. Materials subjected to force exhibit
or fail to exhibit several invariance properties, proposed
by Hager (1985). If the material is shape-invariant with
respect to a particular force, its shape remains the same.
If it is topologically invariant, particles that are adjacent
remain adjacent. Shape invariance implies topological
invariance. If subjected to forces of a certain strength or
degree d1, a material ceases being shape-invariant. At a
force of strength d2 di, it ceases being topologically
invariant, and at a force of strength d3 d2, it simply
breaks. Metals exhibit the full range of possibilities, that
is, 0 &lt; di &lt;d2 &lt; d3 &lt; cc. For forces of strength d &lt; di,
the material is &amp;quot;hard&amp;quot;; for forces of strength d where di
&lt;d &lt; d2, it is &amp;quot;flexible&amp;quot;; for forces of strength d where
d2 &lt; d &lt; d3, it is &amp;quot;malleable&amp;quot;. Words such as &amp;quot;ductile&amp;quot;
and &amp;quot;elastic&amp;quot; can be defined in terms of this vocabu-
lary, together with predicates about the geometry of the
bit of material. Words such as &amp;quot;brittle&amp;quot; (di = d2 = d3)
and &amp;quot;fluid&amp;quot; (d2 = 0, d3 = cc) can also be defined in these
terms. While we should not expect to be able to define
various material terms, like &amp;quot;metal&amp;quot; and &amp;quot;ceramic&amp;quot;,
we can certainly characterize many of their properties
with this vocabulary.
Because of its invariance properties, material inter-
acts with containment and motion. The word &amp;quot;clog&amp;quot;
illustrates this. The predicate clog is a three-place
relation: x clogs y against the flow of z. It is the
obstruction by x of z&apos;s motion through y, but with the
selectional restriction that z must be something that can
flow, such as a liquid, gas, or powder. If a rope is
passing through a hole in a board, and a knot in the rope
prevents it from going through, we do not say that the
hole is clogged. On the other hand, there do not seem to
be any selectional constraints on x. In particular, x can
be identical with z: glue, sand, or molasses can clog a
passageway against its own flow. We can speak of
clogging where the obstruction of flow is not complete,
but it must be thought of as &amp;quot;nearly&amp;quot; complete.
</bodyText>
<subsectionHeader confidence="0.99877">
3.6 OTHER DOMAINS
3.6.1 CAUSAL CONNECTION
</subsectionHeader>
<bodyText confidence="0.999913352941176">
Attachment within materials is one variety of causal
connection. In general, if two entities x and y are
causally connected with respect to some behavior p of
x, then whenever p happens to x, there is some corre-
sponding behavior q that happens to y. In the case of
attachment, p and q are both move. A particularly
common kind of causal connection between two entities
is one mediated by the motion of a third entity from one
to the other. (This might be called a &amp;quot;vector boson&amp;quot;
connection.) Photons mediating the connection between
the sun and our eyes, raindrops connecting a state of the
clouds with the wetness of our skin and clothes, a virus
being transmitted from one person to another, and
utterances passing between people are all examples of
such causal connections. Barriers, openings, and pene-
tration are all defined with respect to paths of causal
connection.
</bodyText>
<subsectionHeader confidence="0.932024">
3.6.2 FORCE
</subsectionHeader>
<bodyText confidence="0.999955846153846">
The concept of &amp;quot;force&amp;quot; is axiomatized, in a way
consistent with Talmy&apos;s treatment (1985), in terms of
the predications force(a,b,di) and resist(b,a,d2) — a
forces against b with strength di and b resists a&apos;s action
with strength d2. We can infer motion from facts about
relative strength. This treatment can also be specialized
to Newtonian force, where we have not merely move-
ment, but acceleration. In addition, in spaces in which
orientation is defined, forces can have an orientation,
and a version of the &amp;quot;parallelogram of forces&amp;quot; law can
be encoded. Finally, force interacts with shape in ways
characterized by words like &amp;quot;stretch&amp;quot;, &amp;quot;compress&amp;quot;,
&amp;quot;bend&amp;quot;, &amp;quot;twist&amp;quot;, and &amp;quot;shear&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.926983">
3.6.3 SYSTEMS AND FUNCTIONALITY
</subsectionHeader>
<bodyText confidence="0.9986899">
An important concept is the notion of a &amp;quot;system&amp;quot;,
which is a set of entities, a set of their properties, and a
set of relations among them. A common kind of system
is one in which the entities are events and conditions
and the relations are causal and enabling relations. A
mechanical device can be described as such a system —
in a sense, in terms of the plan it executes in its
operation. The function of various parts and of condi-
tions of those parts is then the role they play in this
system, or plan.
The intransitive sense of &amp;quot;operate&amp;quot;, as in
The diesel was operating.
involves systems and functionality. If an entity x oper-
ates, there must be a larger systems of which xis a part.
The entity x itself is a system with parts. These parts
undergo normative state changes, thereby causing x to
undergo normative state changes, thereby causing x to
produce an effect with a normative function in the larger
system s. The concept of &amp;quot;normative&amp;quot; is discussed
below.
</bodyText>
<subsectionHeader confidence="0.966218">
3.6.4 SHAPE
</subsectionHeader>
<bodyText confidence="0.99990445">
We have been approaching the problem of characteriz-
ing shape from a number of different angles. The
classical treatment of shape is via the notion of &amp;quot;simi-
larity&amp;quot; in Euclidean geometry, and in Hilbert&apos;s formal
reconstruction of Euclidean geometry (Hilbert, 1902)
the key primitive concept seems to be that of &amp;quot;con-
gruent angles&amp;quot;. Therefore, we first sought to develop a
theory of &amp;quot;orientation&amp;quot;. The shape of an object can
then be characterized in terms of changes in orientation
of a tangent as one moves about on the surface of the
object, as is done in some vision research (e.g., Zahn
and Roskies, 1972). In all of this, since &amp;quot;shape&amp;quot; can be
used loosely and metaphorically, one question we are
asking is whether some minimal, abstract structure can
be found in which the notion of &amp;quot;shape&amp;quot; makes sense.
Consider, for instance, a graph in which one scale is
discrete, or even unordered. Accordingly, we have been
examining a number of examples, asking when it seems
right to say two structures have different shapes.
We have also examined the interactions of shape and
</bodyText>
<note confidence="0.4751875">
Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 247
Jerry R. Hobbs et al. Commonsense Metaphysics and Lexical Semantics
</note>
<bodyText confidence="0.999374875">
functionality (see Davis, 1984). What seems to be
crucial is how the shape of an obstacle constrains the
motion of a substance or of an object of a particular
shape (see Shoham, 1985). Thus, a funnel concentrates
the flow of a liquid, and similarly, a wedge concentrates
force. A box pushed against a ridge in the floor will
topple, and a rotating wheel is a limiting case of contin-
uous toppling.
</bodyText>
<subsectionHeader confidence="0.90906">
3.7 HITTING, ABRASION, WEAR, AND RELATED CONCEPTS
</subsectionHeader>
<bodyText confidence="0.999228166666667">
For x to hit y is for x to move into contact with y with
some force.
The basic scenario for an abrasive event is that there
is an impinging bit of material m that hits an object o and
by doing so removes a pointlike bit of material bo from
the surface of o:
</bodyText>
<equation confidence="0.851823857142857">
abr-evene(e,m,o,b0) : material(m)
A (Vt) at(e,t) D topologically-invariant(o,t)
(V e,m,o,b0)abr-evene(e,m,o,b0)
(3 t,b,s,e1,e2,e3) at(e,t) A consists-of(o,b,t)
A surface(s,b) A particle(bo,$) A change&apos;(e,e1,e2)
A attached&apos;(eobo,b) A nott(e2,e1) A cause(e3,e)
A hie(e3,m,b0)
</equation>
<bodyText confidence="0.9986564">
That is, e is an abrasive event of a material m impinging
on a topologically invariant object o and detaching 1)0 if
and only if bo is a particle of the surface s of the bit of
material material b of which o consists at the time t at
which e occurs, and e is a change from the condition el
of bo&apos;s being attached to b to the negation e2 of that
condition, where the change is caused by the hitting e3
of m against bo.
After the abrasive event, the pointlike bit bo is no
longer a part of the object o:
</bodyText>
<figure confidence="0.51476575">
(Ve,m,o,b0,e1,e2,12)abr-evene(e,m,o,b0)
A change&apos;(e,ei,e2) A at(e2,t2)
A consists-of(o,b2,t2)
D part(b0,b2)
</figure>
<bodyText confidence="0.99837">
That is, if e is an abrasive event of m impinging against
o and detaching bo, and e is a change from el to e2, and
e2 holds at time t2, then bo is not part of the bit of
material b2 of which o consists at t2. It is necessary to
state this explicitly since objects and bits of material can
be discontinuous.
An abrasion is a large set of abrasive events widely
distributed through some nonpointlike region on the
surface of an object:
</bodyText>
<equation confidence="0.7805635">
(V e,m,o) abrade&apos;(e,m,o)
(3 bs)large(bs)
</equation>
<construct confidence="0.831783666666667">
A [(Vei)[ei E e D (3 bo)bo e bs A abr-evene(e4,m,o,b0)]
A (Vb,s,t)[at(e,t) A consists-of(o,b,t) A surface(s,b)
D (3r) subregion(r,$) A widely-distributed(bs,r)]]
</construct>
<bodyText confidence="0.999157571428571">
That is, e is an abrasion by m of o if and only if there is
a large set bs of bits of material and e is a set of abrasive
events in which m impinges on o and removes a bit bo,
an element in bs, from o, and if e occurs at time t and o
consists of material b at time t, then there is a subregion
r of the surface s of b over which bs is widely distrib-
uted.
Wear can result from a large collection of abrasive
events distributed over time as well as space (so that
there may be no instant at which enough abrasive
events occur to count as an abrasion). Thus, the link
between wear and abrasion is via the common notion of
abrasive events, not via a definition of wear in terms of
abrasion.
</bodyText>
<equation confidence="0.5867274">
(V e,m,o) wear&apos;(e,m,o)
(3bs) large(bs)
A [(Ved[ei E e
D (3 bo)bo E bs A abr-evene(e1,m,o,b0)]
A (3i)[interval(i) A widely-distributed(e,i)]]
</equation>
<bodyText confidence="0.999692454545455">
That is, e is a wearing by x of o if and only if there is a
large set bs of bits of material and e is a set of abrasive
events in which m impinges on o and removes a bit bo,
an element in bs, from o, and e is widely distributed
over some time interval i.
We have not yet characterized the concept &amp;quot;large&amp;quot;,
but we anticipate that it would be similar to &amp;quot;high&amp;quot;. The
concept &amp;quot;widely distributed&amp;quot; concerns systems. If x is
distributed in y, then y is a system and x is a set of
entities which are located at components of y. For the
distribution to be wide, most of the elements of a
partition of y, determined independently of the distribu-
tion, must contain components which have elements of
x at them.
The word &amp;quot;wear&amp;quot; is one of a large class of other
events involving cumulative, gradual loss of material —
events described by words like &amp;quot;chip&amp;quot;, &amp;quot;corrode&amp;quot;,
&amp;quot;file&amp;quot;, &amp;quot;erode&amp;quot;, &amp;quot;sand&amp;quot;, &amp;quot;grind&amp;quot;, &amp;quot;weather&amp;quot;, &amp;quot;rust&amp;quot;,
&amp;quot;tarnish&amp;quot;, &amp;quot;eat away&amp;quot;, &amp;quot;rot&amp;quot;, and &amp;quot;decay&amp;quot;. All of
these lexical items can now be defined as variations on
the definition of &amp;quot;wear&amp;quot;, since we have built up the
axiomatizations underlying &amp;quot;wear&amp;quot;. We are now in a
position to characterize the entire class. We will illus-
trate this by defining two different types of variants of
&amp;quot;wear&amp;quot; — &amp;quot;chip&amp;quot; and &amp;quot;corrode&amp;quot;.
&amp;quot;Chip&amp;quot; differs from &amp;quot;wear&amp;quot; in three ways: the bit of
material removed in one abrasive event is larger (it need
not be point-like), it need not happen because of a
material hitting against the object, and &amp;quot;chip&amp;quot; does not
require (though it does permit) a large collection of such
events: one can say that some object is chipped even if
there is one chip in it. Thus, we slightly alter the
definition of abr-event to accommodate these changes:
</bodyText>
<figure confidence="0.655362166666667">
(V e,m,o,b0)chip&apos;(e,m,o,b0)=---
(3t,b,s,e1,e2,e3)at(e,t) A consists-ofto,b,t)
248 Computational Linguistics Volume 13, Numbers 3-4, July-December 1987
Jerry R. Hobbs et at. Commonsense Metaphysics and Lexical Semantics
A surface(s,b) A part(bo,$) A change&apos;(e,e1,e2)
A attached(e1,b0,b) A noe(e2,ei)
</figure>
<bodyText confidence="0.989755666666667">
That is, e is a chipping event by a material m of a bit of
material bo from an object o if and only if bo is a part of
the surface s of the bit of material material b of which o
consists at the time t at which e occurs, and e is a
change from the condition el of bo&apos;s being attached to b
to the negation e2 of that condition.
&amp;quot;Corrode&amp;quot; differs from &amp;quot;wear&amp;quot; in that the bit of
material is chemically transformed as well as being
detached by the contact event; in fact, in some way the
chemical transformation causes the detachment. This
can be captured by adding a condition to the abrasive
event that renders it a (single) corrode event:
</bodyText>
<figure confidence="0.808065714285714">
corrode-event(m,o,b0) : fluid(m)
A contact(m,b0)
(V e,m,o,b0) corrode-event&apos;(e,m,o,b0)
t,b,s,ei,e2,e3) at(e,t) A consists-of(o,b,t)
A surface(s,b) A particle(bo,$) A change&apos;(e,e1,e2)
A attached&apos;(e1,b0,b) A noe(e2,e1) A cause(e3,e)
A chemical-change&apos;(e3,m,b0)
</figure>
<bodyText confidence="0.996022866666667">
That is, e is a corrosive event by a fluid m of a bit of
material bo with which it is in contact if and only if 60 is
a particle of the surface s of the bit of material b of
which o consists at the time t at which e occurs, and e
is a change from the condition ei of bo&apos;s being attached
to b to the negation e2 of that condition, where the
change is caused by a chemical reaction e3 of m with bo.
&amp;quot;Corrode&amp;quot; itself may be defined in a parallel fashion
to &amp;quot;wear&amp;quot;, by substituting corrode-event for abr-event.
All of this suggests the generalization that abrasive
events, chipping and corrode events all detach the bit in
question, and that we may describe all of these as
detaching events. We can then generalize the above
axiom about abrasive events that result in loss of
material to the following axiom about detaching:
</bodyText>
<equation confidence="0.812445333333333">
(V e,m,o,b0,e1,e2,t2) detach&apos;(e,m,o,b0)
A change&apos;(e,e1,e2) A at(e242) A consists-ofto,t2,b2)
D 7 part(b0,b2)
</equation>
<bodyText confidence="0.9894825">
That is, if e is a detaching event by m of bo from o, and
e is a change from el to e2, and e2 holds at time t2, then
1)0 is not part of the bit of material b2 of which o consists
at t2.
</bodyText>
<sectionHeader confidence="0.983217" genericHeader="method">
4 RELEVANCE AND THE NORMATIVE
</sectionHeader>
<bodyText confidence="0.999941022222222">
Many of the concepts we are investigating have driven
us inexorably to the problems of what is meant by
&amp;quot;relevant&amp;quot; and by &amp;quot;normative&amp;quot;. We do not pretend to
have solved these problems. But for each of these
concepts we do have the beginnings of an account that
can play a role in analysis, if not yet in implementation.
Our view of relevance, briefly stated, is that some-
thing is relevant to some goal if it is a part of a plan to
achieve that goal. (A formal treatment of a similar view
is given in Davies, forthcoming.) We can illustrate this
with an example involving the word &amp;quot;sample&amp;quot;. If a bit
of material x is a sample of another bit of material y,
then x is a part of y, and moreover, there are relevant
properties p and q such that it is believed that if p is true
of x then q is true of y. That is, looking at the properties
of the sample tells us something important about the
properties of the whole. Frequently, p and q are the
same property. In our target texts, the following sen-
tence occurs:
We retained an oil sample for future inspection.
The oil in the sample is a part of the total lube oil in
the lube oil system, and it is believed that a property of
the sample, such as &amp;quot;contaminated with metal parti-
cles&amp;quot;, will be true of all the lube oil as well, and that this
will provide information about possible wear on the
bearings. It is therefore relevant to the goal of maintain-
ing the machinery in good working order.
We have arrived at the following provisional account
of what it means to be &amp;quot;normative&amp;quot;. For an entity to
exhibit a normative condition or behavior, it must first
of all be a component of a larger system. This system
has structure in the form of relations among its compo-
nents. A pattern is a property of the system, namely, the
property of a subset of these stuctural relations holding.
A norm is a pattern established either by conventional
stipulation or by statistical regularity. An entity behaves
in a normative fashion if it is a component of a system
and instantiates a norm within that system. The word
&amp;quot;operate&amp;quot;, discussed in Section 3.6.3, illustrates this.
When we say that an engine is operating, we have in
mind a larger system — i.e., the device the engine
drives — to which the engine may bear various possible
relations. A subset of these relations is stipulated to be
the norm — the way it is supposed to work. We say it is
operating when it is instantiating this norm.
</bodyText>
<sectionHeader confidence="0.998413" genericHeader="conclusions">
5 CONCLUSION
</sectionHeader>
<bodyText confidence="0.9999188">
The research we have been engaged in has forced us to
explicate a complex set of commonsense concepts.
Since we have done it in as general a fashion as
possible, we expect to be able, building on this founda-
tion, to axiomatize a large number of other areas,
including areas unrelated to mechanical devices. The
very fact that we have been able to characterize words
as diverse as &amp;quot;range&amp;quot;, &amp;quot;immediately&amp;quot;, &amp;quot;brittle&amp;quot;, &amp;quot;ope-
rate&amp;quot;, and &amp;quot;wear&amp;quot; shows the promising nature of this
approach.
</bodyText>
<sectionHeader confidence="0.978248" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.761568777777778">
The research reported here was funded by the Defense
Advanced Research Projects Agency under Office of
Naval Research contract N00014-85-C-0013. It builds
Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 249
Jerry R. Hobbs et at. Commonsense Metaphysics and Lexical Semantics
on work supported by NIH Grant LM03611 from the
National Library of Medicine, by Grant IST-8209346
from the National Science Foundation, and by a gift
from the Systems Development Foundation.
</bodyText>
<sectionHeader confidence="0.998525" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999910308641976">
Allen, James F., and Henry A. Kautz. 1985. A Model of Naive
Temporal Reasoning. In: Jerry R. Hobbs and Robert C. Moore,
Eds., Formal Theories of the Commonsense World, Ablex Pub-
lishing Corp., Norwood, New Jersey: 251-268.
Croft, William. 1986. Categories and Relations in Syntax: The Clause-
Level Organization of Information. Ph.D. dissertation, Depart-
ment of Linguistics, Stanford University, Stanford, California.
Davies, Todd R. Forthcoming. Determination Rules for Generaliza-
tion and Analogical Inference. In: David H. Helman, Ed., Ana-
logical Reasoning. D. Reidel, Dordrecht, Netherlands.
Davis, Ernest. 1984. Shape and Function of Solid Objects: Some
Examples. Computer Science Technical Report 137, New York
University, New York, New York.
Hager, Greg. 1985. Naive Physics of Materials: A Recon Mission. In:
Commonsense Summer: Final Report, Report No. CSL1-85-35,
Center for the Study of Language and Information, Stanford
University, Stanford, California.
Hayes, Patrick J. 1979. Naive Physics Manifesto. In: Donald Michie,
Ed., Expert Systems in the Micro-electronic Age, Edinburgh
University Press, Edinburgh, Scotland: 242-270.
Herskovits, Annette. 1982. Space and the Prepositions in English:
Regularities and Irregularities in a Complex Domain. Ph.D.
dissertation, Department of Linguistics, Stanford University,
Stanford, California.
Hilbert, David. 1902. The Foundations of Geometry. The Open Court
Publishing Company.
Hobbs, Jerry R. 1974. A Model for Natural Language Semantics, Part
I: The Model. Research Report #36, Department of Computer
Science, Yale University, New Haven, Connecticut.
Hobbs, Jerry R. 1985a. Ontological Promiscuity. Proceedings, 23rd
Annual Meeting of the Association for Computational Linguistics,
Chicago, Illinois, 61-69.
Hobbs, Jerry R. 1985b. Granularity. Proceedings of the Ninth Inter-
national Joint Conference on Artificial Intelligence, Los Angeles,
California, 432-435.
Hobbs, Jerry R. and Robert C. Moore, Eds. 1985. Formal Theories of
the Commonsense World. Ablex Publishing Corp., Norwood,
New Jersey.
Hobbs, Jerry R., Tom Blenko, Bill Croft, Greg Hager, Henry A.
Kautz, Paul Kube, and Yoav Shoham. 1985. Commonsense Sum-
mer: Final Report, Report No. CSLI-85-35, Center for the Study
of Language and Information, Stanford University, Stanford,
California.
Hobbs, Jerry R., and Paul A. Martin. 1987. Local Pragmatics.
Proceedings of the Tenth International Joint Conference on Arti-
ficial Intelligence, Milano, Italy, 520-523.
Katz, Jerrold J. and Jerry A. Fodor. 1963. The Structure of a
Semantic Theory. Language, Vol. 39: 170-210.
Lakoff, George. 1972. Linguistics and Natural Logic. In: Donald
Davidson and Gilbert Harman, Eds., Semantics of Natural Lan-
guage: 545-665.
McDermott, Drew. 1985. Reasoning about Plans. In: Jerry R. Hobbs
and Robert C. Moore, Eds., Formal Theories of the Commonsense
World, Ablex Publishing Corp., Norwood, New Jersey: 269-318.
Miller, George A. and Philip N. Johnson-Laird. 1976. Language and
Perception, Belknap Press.
Rieger, Charles J. 1974. Conceptual Memory: A Theory and Com-
puter Program for Processing and Meaning Content of Natural
Language Utterances. Stanford AIM-233, Department of Com-
puter Science, Stanford University, Stanford, California.
Schank, Roger. 1975. Conceptual Information Processing. Elsevier
Publishing Company.
Shoham, Yoav. 1985. Naive Kinematics: Two Aspects of Shape. In:
Commonsense Summer: Final Report, Report No. CSLI-85-35,
Center for the Study of Language and Information, Stanford
University, Stanford, California.
Stickel, Mark E. 1982. A Nonclausal Connection-Graph Resolution
Theorem-Proving Program. Proceedings of the AAAI-82 National
Conference on Artificial Intelligence, Pittsburgh, Pennsylvania:
229-233.
Talmy, Leonard. 1983. How Language Structures Space. In: Herbert
Pick and Linda Acredolo, Eds., Spatial Orientation: Theory,
Research, and Application, Plenum Press.
Talmy, Leonard. 1985. Force Dynamics in Language and Thought. In:
William H. Effort, Paul D. Kroeber, and Karen L. Peterson, Eds.,
Proceedings from the Parasession on Causatives and Agentivity,
21st Regional Meeting, Chicago Linguistic Society, Chicago,
Illinois.
Zahn, C. T., and R. Z. Roskies. 1972. Fourier Descriptors for Plane
Closed Curves. IEEE Transactions on Computers, Vol. C-21, No.
3: 269-281.
</reference>
<page confidence="0.85556">
250 Computational Linguistics Volume 13, Numbers 3-4, July-December 1987
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.537172">
<title confidence="0.998701">COMMONSENSE METAPHYSICS AND LEXICAL SEMANTICS</title>
<author confidence="0.773968">Jerry R Hobbs</author>
<author confidence="0.773968">William Croft</author>
<author confidence="0.773968">Todd Davies</author>
<author confidence="0.773968">Douglas Edwards</author>
<author confidence="0.773968">Kenneth</author>
<affiliation confidence="0.998422">Artificial Intelligence SRI International</affiliation>
<abstract confidence="0.9984296">In the TACITUS project for using commonsense knowledge in the understanding of texts about mechanical devices and their failures, we have been developing various commonsense theories that are needed to mediate between the way we talk about the behavior of such devices and causal models of their operation. Of central importance in this effort is the axiomatization of what might be called &amp;quot;commonsense metaphysics&amp;quot;. This includes a number of areas that figure in virtually every domain of discourse, such as granularity, scales, time, space, material, physical objects, shape, causality, functionality, and force. Our effort has been to construct core theories of each of these areas, and then to define, or at least characterize, a large number of lexical items in terms provided by the core theories. In this paper we discuss our methodological principles and describe the key ideas in the various domains we are investigating.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James F Allen</author>
<author>Henry A Kautz</author>
</authors>
<title>A Model of Naive Temporal Reasoning. In:</title>
<date>1985</date>
<pages>251--268</pages>
<location>Norwood, New Jersey:</location>
<contexts>
<context position="15991" citStr="Allen and Kautz (1985)" startWordPosition="2648" endWordPosition="2651">mmonsense notion of continuity. Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 243 Jerry R. Hobbs et al. Commonsense Metaphysics and Lexical Semantics A subscale of a scale has as its elements a subset of the elements of the scale and has as its partial ordering and its grain the partial ordering and the grain of the scale. (V si,s2)subscale(s2,si)E--- subset(s2,s1) A (Vx,y)[[x &lt;$1 y = x &lt;$2 y] A [x —si Y = x =s2 Y]] An interval can be defined as a connected subscale: (Vi)interval(i) = (3s)scale(s) A subscale(i,$) A sconnected(i) The relations between time intervals that Allen and Kautz (1985) have defined can be defined in a straightforward manner in the approach presented here, but for intervals in general. A concept closely related to scales is that of a &amp;quot;cycle&amp;quot;. This is a system that has a natural ordering locally but contains a loop globally. Examples are the color wheel, clock times, and geographical locations ordered by &amp;quot;east of&apos;. We have axiomatized cycles in terms of a ternary between relation whose axioms parallel those for a partial ordering. The figure-ground relationship is of fundamental importance in language. We encode it with the primitive predicate at. It is possi</context>
</contexts>
<marker>Allen, Kautz, 1985</marker>
<rawString>Allen, James F., and Henry A. Kautz. 1985. A Model of Naive Temporal Reasoning. In: Jerry R. Hobbs and Robert C. Moore, Eds., Formal Theories of the Commonsense World, Ablex Publishing Corp., Norwood, New Jersey: 251-268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Croft</author>
</authors>
<title>Categories and Relations in Syntax: The ClauseLevel Organization of Information.</title>
<date>1986</date>
<institution>Department of Linguistics, Stanford University,</institution>
<location>Stanford, California.</location>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="3269" citStr="Croft, 1986" startWordPosition="497" endWordPosition="498">g from simple sets of features (Katz and Fodor, 1963) to notations for predicate-argument structure (Lakoff, 1972; Miller and Johnson-Laird, 1976), but the early attempts still limited access to world knowledge and assumed only very restricted sorts of processing. Workers in computational linguistics introduced inference (Rieger, 1974; Schank, 1975) and other complex cognitive processes (Herskovits, 1982) into our understanding of the role of word meaning. Recently linguists have given greater attention to the cognitive processes that would operate on their representations (e.g., Talmy, 1983; Croft, 1986). Independently, in artificial intelligence an effort arose to encode large amounts of commonsense knowledge (Hayes, 1979; Hobbs and Moore, 1985; Hobbs et al. 1985). The research reported here represents a convergence of these various developments. By constructing core theories of certain fundamental phenomena and defining lexical items within these theories, using the full power of predicate calculus, we are able to cope with complexities of word meaning that have hitherto escaped lexical semanticists. Moreover, we can do this within a framework that gives full scope to the planning and reaso</context>
</contexts>
<marker>Croft, 1986</marker>
<rawString>Croft, William. 1986. Categories and Relations in Syntax: The ClauseLevel Organization of Information. Ph.D. dissertation, Department of Linguistics, Stanford University, Stanford, California.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Todd R Forthcoming Davies</author>
</authors>
<title>Determination Rules for Generalization and Analogical Inference. In:</title>
<location>Dordrecht, Netherlands.</location>
<marker>Davies, </marker>
<rawString>Davies, Todd R. Forthcoming. Determination Rules for Generalization and Analogical Inference. In: David H. Helman, Ed., Analogical Reasoning. D. Reidel, Dordrecht, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ernest Davis</author>
</authors>
<title>Shape and Function of Solid Objects: Some Examples. Computer Science</title>
<date>1984</date>
<tech>Technical Report 137,</tech>
<institution>New York University,</institution>
<location>New York, New York.</location>
<contexts>
<context position="36511" citStr="Davis, 1984" startWordPosition="6200" endWordPosition="6201">shape&amp;quot; can be used loosely and metaphorically, one question we are asking is whether some minimal, abstract structure can be found in which the notion of &amp;quot;shape&amp;quot; makes sense. Consider, for instance, a graph in which one scale is discrete, or even unordered. Accordingly, we have been examining a number of examples, asking when it seems right to say two structures have different shapes. We have also examined the interactions of shape and Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 247 Jerry R. Hobbs et al. Commonsense Metaphysics and Lexical Semantics functionality (see Davis, 1984). What seems to be crucial is how the shape of an obstacle constrains the motion of a substance or of an object of a particular shape (see Shoham, 1985). Thus, a funnel concentrates the flow of a liquid, and similarly, a wedge concentrates force. A box pushed against a ridge in the floor will topple, and a rotating wheel is a limiting case of continuous toppling. 3.7 HITTING, ABRASION, WEAR, AND RELATED CONCEPTS For x to hit y is for x to move into contact with y with some force. The basic scenario for an abrasive event is that there is an impinging bit of material m that hits an object o and </context>
</contexts>
<marker>Davis, 1984</marker>
<rawString>Davis, Ernest. 1984. Shape and Function of Solid Objects: Some Examples. Computer Science Technical Report 137, New York University, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Hager</author>
</authors>
<title>Naive Physics of Materials: A Recon Mission. In: Commonsense Summer:</title>
<date>1985</date>
<tech>Final Report, Report No. CSL1-85-35,</tech>
<institution>Center for the Study of Language and Information, Stanford University,</institution>
<location>Stanford, California.</location>
<contexts>
<context position="30696" citStr="Hager (1985)" startWordPosition="5194" endWordPosition="5195">pology of regions. In addition, we have encoded the relation that bond bears to motion, i.e., that bonded bits remain adjacent and that one moves when the other does, and the relation of bond to force, i.e. that there is a characteristic force that breaks a bond in a given material. 246 Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 Jerry R. Hobbs et at. Commonsense Metaphysics and Lexical Semantics Different materials react in different ways to forces of various strengths. Materials subjected to force exhibit or fail to exhibit several invariance properties, proposed by Hager (1985). If the material is shape-invariant with respect to a particular force, its shape remains the same. If it is topologically invariant, particles that are adjacent remain adjacent. Shape invariance implies topological invariance. If subjected to forces of a certain strength or degree d1, a material ceases being shape-invariant. At a force of strength d2 di, it ceases being topologically invariant, and at a force of strength d3 d2, it simply breaks. Metals exhibit the full range of possibilities, that is, 0 &lt; di &lt;d2 &lt; d3 &lt; cc. For forces of strength d &lt; di, the material is &amp;quot;hard&amp;quot;; for forces of </context>
</contexts>
<marker>Hager, 1985</marker>
<rawString>Hager, Greg. 1985. Naive Physics of Materials: A Recon Mission. In: Commonsense Summer: Final Report, Report No. CSL1-85-35, Center for the Study of Language and Information, Stanford University, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick J Hayes</author>
</authors>
<title>Naive Physics Manifesto. In: Donald Michie, Ed., Expert Systems in the Micro-electronic Age,</title>
<date>1979</date>
<pages>242--270</pages>
<publisher>University Press,</publisher>
<location>Edinburgh</location>
<contexts>
<context position="3390" citStr="Hayes, 1979" startWordPosition="514" endWordPosition="515">and Johnson-Laird, 1976), but the early attempts still limited access to world knowledge and assumed only very restricted sorts of processing. Workers in computational linguistics introduced inference (Rieger, 1974; Schank, 1975) and other complex cognitive processes (Herskovits, 1982) into our understanding of the role of word meaning. Recently linguists have given greater attention to the cognitive processes that would operate on their representations (e.g., Talmy, 1983; Croft, 1986). Independently, in artificial intelligence an effort arose to encode large amounts of commonsense knowledge (Hayes, 1979; Hobbs and Moore, 1985; Hobbs et al. 1985). The research reported here represents a convergence of these various developments. By constructing core theories of certain fundamental phenomena and defining lexical items within these theories, using the full power of predicate calculus, we are able to cope with complexities of word meaning that have hitherto escaped lexical semanticists. Moreover, we can do this within a framework that gives full scope to the planning and reasoning processes that manipulate representations of word meaning. Copyright 1987 by the Association for Computational Lingu</context>
</contexts>
<marker>Hayes, 1979</marker>
<rawString>Hayes, Patrick J. 1979. Naive Physics Manifesto. In: Donald Michie, Ed., Expert Systems in the Micro-electronic Age, Edinburgh University Press, Edinburgh, Scotland: 242-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annette Herskovits</author>
</authors>
<title>Space and the Prepositions in English: Regularities and Irregularities in a Complex Domain.</title>
<date>1982</date>
<institution>Department of Linguistics, Stanford University,</institution>
<location>Stanford, California.</location>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="3065" citStr="Herskovits, 1982" startWordPosition="467" endWordPosition="468">n linguistics and efforts in artificial intelligence to encode commonsense knowledge. Over the years, lexical semanticists have developed formalisms of increasing adequacy for encoding word meaning, progressing from simple sets of features (Katz and Fodor, 1963) to notations for predicate-argument structure (Lakoff, 1972; Miller and Johnson-Laird, 1976), but the early attempts still limited access to world knowledge and assumed only very restricted sorts of processing. Workers in computational linguistics introduced inference (Rieger, 1974; Schank, 1975) and other complex cognitive processes (Herskovits, 1982) into our understanding of the role of word meaning. Recently linguists have given greater attention to the cognitive processes that would operate on their representations (e.g., Talmy, 1983; Croft, 1986). Independently, in artificial intelligence an effort arose to encode large amounts of commonsense knowledge (Hayes, 1979; Hobbs and Moore, 1985; Hobbs et al. 1985). The research reported here represents a convergence of these various developments. By constructing core theories of certain fundamental phenomena and defining lexical items within these theories, using the full power of predicate </context>
</contexts>
<marker>Herskovits, 1982</marker>
<rawString>Herskovits, Annette. 1982. Space and the Prepositions in English: Regularities and Irregularities in a Complex Domain. Ph.D. dissertation, Department of Linguistics, Stanford University, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hilbert</author>
</authors>
<title>The Foundations of Geometry.</title>
<date>1902</date>
<publisher>The Open Court Publishing Company.</publisher>
<contexts>
<context position="35531" citStr="Hilbert, 1902" startWordPosition="6037" endWordPosition="6038">perates, there must be a larger systems of which xis a part. The entity x itself is a system with parts. These parts undergo normative state changes, thereby causing x to undergo normative state changes, thereby causing x to produce an effect with a normative function in the larger system s. The concept of &amp;quot;normative&amp;quot; is discussed below. 3.6.4 SHAPE We have been approaching the problem of characterizing shape from a number of different angles. The classical treatment of shape is via the notion of &amp;quot;similarity&amp;quot; in Euclidean geometry, and in Hilbert&apos;s formal reconstruction of Euclidean geometry (Hilbert, 1902) the key primitive concept seems to be that of &amp;quot;congruent angles&amp;quot;. Therefore, we first sought to develop a theory of &amp;quot;orientation&amp;quot;. The shape of an object can then be characterized in terms of changes in orientation of a tangent as one moves about on the surface of the object, as is done in some vision research (e.g., Zahn and Roskies, 1972). In all of this, since &amp;quot;shape&amp;quot; can be used loosely and metaphorically, one question we are asking is whether some minimal, abstract structure can be found in which the notion of &amp;quot;shape&amp;quot; makes sense. Consider, for instance, a graph in which one scale is dis</context>
</contexts>
<marker>Hilbert, 1902</marker>
<rawString>Hilbert, David. 1902. The Foundations of Geometry. The Open Court Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>A Model for Natural Language Semantics, Part I: The Model.</title>
<date>1974</date>
<tech>Research Report #36,</tech>
<institution>Department of Computer Science, Yale University,</institution>
<location>New Haven, Connecticut.</location>
<contexts>
<context position="19550" citStr="Hobbs, 1974" startWordPosition="3280" endWordPosition="3281">stating that the use of a mechanical device involves the likelihood of abrasive events, as defined below, and with the definition of &amp;quot;wear&amp;quot; in terms of abrasive events, we should be able to conclude the likelihood of wear. 3.3 TIME: TWO ONTOLOGIES There are two possible ontologies for time. In the first, the one most acceptable to the mathematically minded, there is a time line, which is a scale having some topological structure. We can stipulate the time line to be linearly ordered (although it is not in approaches that build ignorance of relative times into the representation of time (e.g., Hobbs, 1974) nor in approaches employing branching futures (e.g., McDermott, 1985)), and we can stipulate it to be dense (although it is not in the situation calculus). We take before to be the ordering on the time line: (Vt1,t2)before(t1 ,t2) ------ (3T)Time-line(T) A t1 E T A t2 E T A ti &lt; T t2 We allow both instants and intervals of time. Most events occur at some instant or during some interval. In this approach, nearly every predicate takes a time argument. In the second ontology, the one that seems to be more deeply rooted in language, the world consists of a large number of more or less independent</context>
</contexts>
<marker>Hobbs, 1974</marker>
<rawString>Hobbs, Jerry R. 1974. A Model for Natural Language Semantics, Part I: The Model. Research Report #36, Department of Computer Science, Yale University, New Haven, Connecticut.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<date>1985</date>
<booktitle>Ontological Promiscuity. Proceedings, 23rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>61--69</pages>
<location>Chicago, Illinois,</location>
<contexts>
<context position="10098" citStr="Hobbs (1985" startWordPosition="1629" endWordPosition="1630">ptible bits of material from its surface due to abrasive action over time. One goal, which we have not yet achieved, is to be able to prove as a theorem that, since the shape of a part of a mechanical device is often functional and since loss of material can result in a change of shape, wear of a part of a device can cause the failure of the device as a whole. In addition, as we have proceeded, we have characterized a number of words found in a set of target texts, as it has become possible. We are encoding the knowledge as axioms in what is for the most part a first-order logic, described by Hobbs (1985a), although quantification over predicates is sometimes convenient. In the formalism there is a nominalization operator &amp;quot; &apos; &amp;quot; for reifying events and conditions, as expressed in the following axiom schema: (Vx)p(x) (3e)pi(e,x) A Exist(e) That is, p is true of x if and only if there is a condition e of p&apos;s being true of x and e exists in the real world. 242 Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 Jerry R. Hobbs et al. Commonsense Metaphysics and Lexical Semantics In our implementation so far, we have been proving simple theorems from our axioms using the CG5 theore</context>
<context position="12560" citStr="Hobbs 1985" startWordPosition="2068" endWordPosition="2069">x ranges from y to z, there must be a scale s that includes y and z, and x must be a set of entities that are located at various places on the scale. This can be represented as follows: range(x,y,z) : (3 s) [scale(s) A y E sA zEsA set(x) A (V u)[u E x D (3 v) vEsA at(u,v)]] 3 THE KNOWLEDGE BASE 3.1 SETS AND GRANULARITY At the foundation of the knowledge base is an axiomatization of set theory. It follows the standard ZermeloFraenkel approach, except that there is no axiom of infinity. Since so many concepts used in discourse are graindependent, a theory of granularity is also fundamental (see Hobbs 1985b). A grain is defined in terms of an indistinguishability relation, which is reflexive and symmetric, but not necessarily transitive. One grain can be a refinement of another, with the obvious definition. The most refined grain is the identity grain, i.e., the one in which every two distinct elements are distinguishable. One possible relationship between two grains, one of which is a refinement of the other, is what we call an &amp;quot;Archimedean relation&amp;quot;, after the Archimedean property of real numbers. Intuitively, if enough events occur that are imperceptible at the coarser grain g2 but perceptib</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>Hobbs, Jerry R. 1985a. Ontological Promiscuity. Proceedings, 23rd Annual Meeting of the Association for Computational Linguistics, Chicago, Illinois, 61-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<date>1985</date>
<booktitle>Granularity. Proceedings of the Ninth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>432--435</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="10098" citStr="Hobbs (1985" startWordPosition="1629" endWordPosition="1630">ptible bits of material from its surface due to abrasive action over time. One goal, which we have not yet achieved, is to be able to prove as a theorem that, since the shape of a part of a mechanical device is often functional and since loss of material can result in a change of shape, wear of a part of a device can cause the failure of the device as a whole. In addition, as we have proceeded, we have characterized a number of words found in a set of target texts, as it has become possible. We are encoding the knowledge as axioms in what is for the most part a first-order logic, described by Hobbs (1985a), although quantification over predicates is sometimes convenient. In the formalism there is a nominalization operator &amp;quot; &apos; &amp;quot; for reifying events and conditions, as expressed in the following axiom schema: (Vx)p(x) (3e)pi(e,x) A Exist(e) That is, p is true of x if and only if there is a condition e of p&apos;s being true of x and e exists in the real world. 242 Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 Jerry R. Hobbs et al. Commonsense Metaphysics and Lexical Semantics In our implementation so far, we have been proving simple theorems from our axioms using the CG5 theore</context>
<context position="12560" citStr="Hobbs 1985" startWordPosition="2068" endWordPosition="2069">x ranges from y to z, there must be a scale s that includes y and z, and x must be a set of entities that are located at various places on the scale. This can be represented as follows: range(x,y,z) : (3 s) [scale(s) A y E sA zEsA set(x) A (V u)[u E x D (3 v) vEsA at(u,v)]] 3 THE KNOWLEDGE BASE 3.1 SETS AND GRANULARITY At the foundation of the knowledge base is an axiomatization of set theory. It follows the standard ZermeloFraenkel approach, except that there is no axiom of infinity. Since so many concepts used in discourse are graindependent, a theory of granularity is also fundamental (see Hobbs 1985b). A grain is defined in terms of an indistinguishability relation, which is reflexive and symmetric, but not necessarily transitive. One grain can be a refinement of another, with the obvious definition. The most refined grain is the identity grain, i.e., the one in which every two distinct elements are distinguishable. One possible relationship between two grains, one of which is a refinement of the other, is what we call an &amp;quot;Archimedean relation&amp;quot;, after the Archimedean property of real numbers. Intuitively, if enough events occur that are imperceptible at the coarser grain g2 but perceptib</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>Hobbs, Jerry R. 1985b. Granularity. Proceedings of the Ninth International Joint Conference on Artificial Intelligence, Los Angeles, California, 432-435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Robert C Moore</author>
<author>Eds</author>
</authors>
<date>1985</date>
<booktitle>Formal Theories of the Commonsense World. Ablex Publishing Corp.,</booktitle>
<location>Norwood, New Jersey.</location>
<contexts>
<context position="3433" citStr="Hobbs et al. 1985" startWordPosition="520" endWordPosition="523">rly attempts still limited access to world knowledge and assumed only very restricted sorts of processing. Workers in computational linguistics introduced inference (Rieger, 1974; Schank, 1975) and other complex cognitive processes (Herskovits, 1982) into our understanding of the role of word meaning. Recently linguists have given greater attention to the cognitive processes that would operate on their representations (e.g., Talmy, 1983; Croft, 1986). Independently, in artificial intelligence an effort arose to encode large amounts of commonsense knowledge (Hayes, 1979; Hobbs and Moore, 1985; Hobbs et al. 1985). The research reported here represents a convergence of these various developments. By constructing core theories of certain fundamental phenomena and defining lexical items within these theories, using the full power of predicate calculus, we are able to cope with complexities of word meaning that have hitherto escaped lexical semanticists. Moreover, we can do this within a framework that gives full scope to the planning and reasoning processes that manipulate representations of word meaning. Copyright 1987 by the Association for Computational Linguistics. Permission to copy without fee all </context>
</contexts>
<marker>Hobbs, Moore, Eds, 1985</marker>
<rawString>Hobbs, Jerry R. and Robert C. Moore, Eds. 1985. Formal Theories of the Commonsense World. Ablex Publishing Corp., Norwood, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Tom Blenko</author>
<author>Bill Croft</author>
<author>Greg Hager</author>
<author>Henry A Kautz</author>
<author>Paul Kube</author>
<author>Yoav Shoham</author>
</authors>
<title>Commonsense Summer:</title>
<date>1985</date>
<tech>Final Report, Report No. CSLI-85-35,</tech>
<institution>Center for the Study of Language and Information, Stanford University,</institution>
<location>Stanford, California.</location>
<contexts>
<context position="3433" citStr="Hobbs et al. 1985" startWordPosition="520" endWordPosition="523">rly attempts still limited access to world knowledge and assumed only very restricted sorts of processing. Workers in computational linguistics introduced inference (Rieger, 1974; Schank, 1975) and other complex cognitive processes (Herskovits, 1982) into our understanding of the role of word meaning. Recently linguists have given greater attention to the cognitive processes that would operate on their representations (e.g., Talmy, 1983; Croft, 1986). Independently, in artificial intelligence an effort arose to encode large amounts of commonsense knowledge (Hayes, 1979; Hobbs and Moore, 1985; Hobbs et al. 1985). The research reported here represents a convergence of these various developments. By constructing core theories of certain fundamental phenomena and defining lexical items within these theories, using the full power of predicate calculus, we are able to cope with complexities of word meaning that have hitherto escaped lexical semanticists. Moreover, we can do this within a framework that gives full scope to the planning and reasoning processes that manipulate representations of word meaning. Copyright 1987 by the Association for Computational Linguistics. Permission to copy without fee all </context>
</contexts>
<marker>Hobbs, Blenko, Croft, Hager, Kautz, Kube, Shoham, 1985</marker>
<rawString>Hobbs, Jerry R., Tom Blenko, Bill Croft, Greg Hager, Henry A. Kautz, Paul Kube, and Yoav Shoham. 1985. Commonsense Summer: Final Report, Report No. CSLI-85-35, Center for the Study of Language and Information, Stanford University, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Paul A Martin</author>
</authors>
<title>Local Pragmatics.</title>
<date>1987</date>
<booktitle>Proceedings of the Tenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>520--523</pages>
<location>Milano, Italy,</location>
<contexts>
<context position="2377" citStr="Hobbs and Martin, 1987" startWordPosition="367" endWordPosition="370">h to lexical semantics is to construct core theories of each of these areas, and then to define, or at least characterize, a large number of lexical items in terms provided by the core theories. In the TACITUS system, processes for solving pragmatics problems posed by a text will use the knowledge base consisting of these theories, in conjunction with the logical forms of the sentences in the text, to produce an interpretation. In this paper we do not stress these interpretation processes; this is another, important aspect of the TACITUS project, and it will be described in subsequent papers (Hobbs and Martin, 1987). This work represents a convergence of research in lexical semantics in linguistics and efforts in artificial intelligence to encode commonsense knowledge. Over the years, lexical semanticists have developed formalisms of increasing adequacy for encoding word meaning, progressing from simple sets of features (Katz and Fodor, 1963) to notations for predicate-argument structure (Lakoff, 1972; Miller and Johnson-Laird, 1976), but the early attempts still limited access to world knowledge and assumed only very restricted sorts of processing. Workers in computational linguistics introduced inferen</context>
</contexts>
<marker>Hobbs, Martin, 1987</marker>
<rawString>Hobbs, Jerry R., and Paul A. Martin. 1987. Local Pragmatics. Proceedings of the Tenth International Joint Conference on Artificial Intelligence, Milano, Italy, 520-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold J Katz</author>
<author>Jerry A Fodor</author>
</authors>
<title>The Structure of a Semantic Theory.</title>
<date>1963</date>
<journal>Language,</journal>
<volume>39</volume>
<pages>170--210</pages>
<contexts>
<context position="2710" citStr="Katz and Fodor, 1963" startWordPosition="415" endWordPosition="418">n conjunction with the logical forms of the sentences in the text, to produce an interpretation. In this paper we do not stress these interpretation processes; this is another, important aspect of the TACITUS project, and it will be described in subsequent papers (Hobbs and Martin, 1987). This work represents a convergence of research in lexical semantics in linguistics and efforts in artificial intelligence to encode commonsense knowledge. Over the years, lexical semanticists have developed formalisms of increasing adequacy for encoding word meaning, progressing from simple sets of features (Katz and Fodor, 1963) to notations for predicate-argument structure (Lakoff, 1972; Miller and Johnson-Laird, 1976), but the early attempts still limited access to world knowledge and assumed only very restricted sorts of processing. Workers in computational linguistics introduced inference (Rieger, 1974; Schank, 1975) and other complex cognitive processes (Herskovits, 1982) into our understanding of the role of word meaning. Recently linguists have given greater attention to the cognitive processes that would operate on their representations (e.g., Talmy, 1983; Croft, 1986). Independently, in artificial intelligen</context>
</contexts>
<marker>Katz, Fodor, 1963</marker>
<rawString>Katz, Jerrold J. and Jerry A. Fodor. 1963. The Structure of a Semantic Theory. Language, Vol. 39: 170-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
</authors>
<title>Linguistics and Natural Logic. In: Donald Davidson and Gilbert Harman, Eds., Semantics of Natural Language:</title>
<date>1972</date>
<pages>545--665</pages>
<contexts>
<context position="2770" citStr="Lakoff, 1972" startWordPosition="425" endWordPosition="426"> produce an interpretation. In this paper we do not stress these interpretation processes; this is another, important aspect of the TACITUS project, and it will be described in subsequent papers (Hobbs and Martin, 1987). This work represents a convergence of research in lexical semantics in linguistics and efforts in artificial intelligence to encode commonsense knowledge. Over the years, lexical semanticists have developed formalisms of increasing adequacy for encoding word meaning, progressing from simple sets of features (Katz and Fodor, 1963) to notations for predicate-argument structure (Lakoff, 1972; Miller and Johnson-Laird, 1976), but the early attempts still limited access to world knowledge and assumed only very restricted sorts of processing. Workers in computational linguistics introduced inference (Rieger, 1974; Schank, 1975) and other complex cognitive processes (Herskovits, 1982) into our understanding of the role of word meaning. Recently linguists have given greater attention to the cognitive processes that would operate on their representations (e.g., Talmy, 1983; Croft, 1986). Independently, in artificial intelligence an effort arose to encode large amounts of commonsense kn</context>
</contexts>
<marker>Lakoff, 1972</marker>
<rawString>Lakoff, George. 1972. Linguistics and Natural Logic. In: Donald Davidson and Gilbert Harman, Eds., Semantics of Natural Language: 545-665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drew McDermott</author>
</authors>
<title>Reasoning about Plans. In:</title>
<date>1985</date>
<pages>269--318</pages>
<location>Norwood, New Jersey:</location>
<contexts>
<context position="19620" citStr="McDermott, 1985" startWordPosition="3289" endWordPosition="3290">d of abrasive events, as defined below, and with the definition of &amp;quot;wear&amp;quot; in terms of abrasive events, we should be able to conclude the likelihood of wear. 3.3 TIME: TWO ONTOLOGIES There are two possible ontologies for time. In the first, the one most acceptable to the mathematically minded, there is a time line, which is a scale having some topological structure. We can stipulate the time line to be linearly ordered (although it is not in approaches that build ignorance of relative times into the representation of time (e.g., Hobbs, 1974) nor in approaches employing branching futures (e.g., McDermott, 1985)), and we can stipulate it to be dense (although it is not in the situation calculus). We take before to be the ordering on the time line: (Vt1,t2)before(t1 ,t2) ------ (3T)Time-line(T) A t1 E T A t2 E T A ti &lt; T t2 We allow both instants and intervals of time. Most events occur at some instant or during some interval. In this approach, nearly every predicate takes a time argument. In the second ontology, the one that seems to be more deeply rooted in language, the world consists of a large number of more or less independent processes, or histories, or sequences of events. There is a primitive</context>
</contexts>
<marker>McDermott, 1985</marker>
<rawString>McDermott, Drew. 1985. Reasoning about Plans. In: Jerry R. Hobbs and Robert C. Moore, Eds., Formal Theories of the Commonsense World, Ablex Publishing Corp., Norwood, New Jersey: 269-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Philip N Johnson-Laird</author>
</authors>
<title>Language and Perception,</title>
<date>1976</date>
<publisher>Belknap Press.</publisher>
<contexts>
<context position="2803" citStr="Miller and Johnson-Laird, 1976" startWordPosition="427" endWordPosition="430">terpretation. In this paper we do not stress these interpretation processes; this is another, important aspect of the TACITUS project, and it will be described in subsequent papers (Hobbs and Martin, 1987). This work represents a convergence of research in lexical semantics in linguistics and efforts in artificial intelligence to encode commonsense knowledge. Over the years, lexical semanticists have developed formalisms of increasing adequacy for encoding word meaning, progressing from simple sets of features (Katz and Fodor, 1963) to notations for predicate-argument structure (Lakoff, 1972; Miller and Johnson-Laird, 1976), but the early attempts still limited access to world knowledge and assumed only very restricted sorts of processing. Workers in computational linguistics introduced inference (Rieger, 1974; Schank, 1975) and other complex cognitive processes (Herskovits, 1982) into our understanding of the role of word meaning. Recently linguists have given greater attention to the cognitive processes that would operate on their representations (e.g., Talmy, 1983; Croft, 1986). Independently, in artificial intelligence an effort arose to encode large amounts of commonsense knowledge (Hayes, 1979; Hobbs and M</context>
</contexts>
<marker>Miller, Johnson-Laird, 1976</marker>
<rawString>Miller, George A. and Philip N. Johnson-Laird. 1976. Language and Perception, Belknap Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Rieger</author>
</authors>
<title>Conceptual Memory: A Theory and Computer Program for Processing and Meaning Content of Natural Language Utterances.</title>
<date>1974</date>
<tech>Stanford AIM-233,</tech>
<institution>Department of Computer Science, Stanford University,</institution>
<location>Stanford, California.</location>
<contexts>
<context position="2993" citStr="Rieger, 1974" startWordPosition="457" endWordPosition="458">is work represents a convergence of research in lexical semantics in linguistics and efforts in artificial intelligence to encode commonsense knowledge. Over the years, lexical semanticists have developed formalisms of increasing adequacy for encoding word meaning, progressing from simple sets of features (Katz and Fodor, 1963) to notations for predicate-argument structure (Lakoff, 1972; Miller and Johnson-Laird, 1976), but the early attempts still limited access to world knowledge and assumed only very restricted sorts of processing. Workers in computational linguistics introduced inference (Rieger, 1974; Schank, 1975) and other complex cognitive processes (Herskovits, 1982) into our understanding of the role of word meaning. Recently linguists have given greater attention to the cognitive processes that would operate on their representations (e.g., Talmy, 1983; Croft, 1986). Independently, in artificial intelligence an effort arose to encode large amounts of commonsense knowledge (Hayes, 1979; Hobbs and Moore, 1985; Hobbs et al. 1985). The research reported here represents a convergence of these various developments. By constructing core theories of certain fundamental phenomena and defining</context>
</contexts>
<marker>Rieger, 1974</marker>
<rawString>Rieger, Charles J. 1974. Conceptual Memory: A Theory and Computer Program for Processing and Meaning Content of Natural Language Utterances. Stanford AIM-233, Department of Computer Science, Stanford University, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Schank</author>
</authors>
<title>Conceptual Information Processing.</title>
<date>1975</date>
<publisher>Elsevier Publishing Company.</publisher>
<contexts>
<context position="3008" citStr="Schank, 1975" startWordPosition="459" endWordPosition="460">ents a convergence of research in lexical semantics in linguistics and efforts in artificial intelligence to encode commonsense knowledge. Over the years, lexical semanticists have developed formalisms of increasing adequacy for encoding word meaning, progressing from simple sets of features (Katz and Fodor, 1963) to notations for predicate-argument structure (Lakoff, 1972; Miller and Johnson-Laird, 1976), but the early attempts still limited access to world knowledge and assumed only very restricted sorts of processing. Workers in computational linguistics introduced inference (Rieger, 1974; Schank, 1975) and other complex cognitive processes (Herskovits, 1982) into our understanding of the role of word meaning. Recently linguists have given greater attention to the cognitive processes that would operate on their representations (e.g., Talmy, 1983; Croft, 1986). Independently, in artificial intelligence an effort arose to encode large amounts of commonsense knowledge (Hayes, 1979; Hobbs and Moore, 1985; Hobbs et al. 1985). The research reported here represents a convergence of these various developments. By constructing core theories of certain fundamental phenomena and defining lexical items </context>
</contexts>
<marker>Schank, 1975</marker>
<rawString>Schank, Roger. 1975. Conceptual Information Processing. Elsevier Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Shoham</author>
</authors>
<title>Naive Kinematics: Two Aspects of Shape. In: Commonsense Summer:</title>
<date>1985</date>
<tech>Final Report, Report No. CSLI-85-35,</tech>
<institution>Center for the Study of Language and Information, Stanford University,</institution>
<location>Stanford, California.</location>
<contexts>
<context position="36663" citStr="Shoham, 1985" startWordPosition="6229" endWordPosition="6230">of &amp;quot;shape&amp;quot; makes sense. Consider, for instance, a graph in which one scale is discrete, or even unordered. Accordingly, we have been examining a number of examples, asking when it seems right to say two structures have different shapes. We have also examined the interactions of shape and Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 247 Jerry R. Hobbs et al. Commonsense Metaphysics and Lexical Semantics functionality (see Davis, 1984). What seems to be crucial is how the shape of an obstacle constrains the motion of a substance or of an object of a particular shape (see Shoham, 1985). Thus, a funnel concentrates the flow of a liquid, and similarly, a wedge concentrates force. A box pushed against a ridge in the floor will topple, and a rotating wheel is a limiting case of continuous toppling. 3.7 HITTING, ABRASION, WEAR, AND RELATED CONCEPTS For x to hit y is for x to move into contact with y with some force. The basic scenario for an abrasive event is that there is an impinging bit of material m that hits an object o and by doing so removes a pointlike bit of material bo from the surface of o: abr-evene(e,m,o,b0) : material(m) A (Vt) at(e,t) D topologically-invariant(o,t</context>
</contexts>
<marker>Shoham, 1985</marker>
<rawString>Shoham, Yoav. 1985. Naive Kinematics: Two Aspects of Shape. In: Commonsense Summer: Final Report, Report No. CSLI-85-35, Center for the Study of Language and Information, Stanford University, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark E Stickel</author>
</authors>
<title>A Nonclausal Connection-Graph Resolution Theorem-Proving Program.</title>
<date>1982</date>
<booktitle>Proceedings of the AAAI-82 National Conference on Artificial Intelligence,</booktitle>
<pages>229--233</pages>
<location>Pittsburgh, Pennsylvania:</location>
<contexts>
<context position="10739" citStr="Stickel (1982)" startWordPosition="1735" endWordPosition="1736"> over predicates is sometimes convenient. In the formalism there is a nominalization operator &amp;quot; &apos; &amp;quot; for reifying events and conditions, as expressed in the following axiom schema: (Vx)p(x) (3e)pi(e,x) A Exist(e) That is, p is true of x if and only if there is a condition e of p&apos;s being true of x and e exists in the real world. 242 Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 Jerry R. Hobbs et al. Commonsense Metaphysics and Lexical Semantics In our implementation so far, we have been proving simple theorems from our axioms using the CG5 theorem-prover developed by Mark Stickel (1982), and we are now beginning to use the knowledge base in text processing. 2 REQUIREMENTS ON ARGUMENTS OF PREDICATES There is a notational convention used below that deserves some explanation. It has frequently been noted that relational words in natural language can take only certain types of words as their arguments. These are usually described as selectional constraints. The same is true of predicates in our knowledge base. The constraints are expressed below by rules of the form p(x,y) : r(x,y) This means that for p even to make sense applied to x and y, it must be the case that r is true of</context>
</contexts>
<marker>Stickel, 1982</marker>
<rawString>Stickel, Mark E. 1982. A Nonclausal Connection-Graph Resolution Theorem-Proving Program. Proceedings of the AAAI-82 National Conference on Artificial Intelligence, Pittsburgh, Pennsylvania: 229-233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Talmy</author>
</authors>
<title>How Language Structures Space. In: Herbert Pick and Linda Acredolo, Eds., Spatial Orientation: Theory, Research, and Application,</title>
<date>1983</date>
<publisher>Plenum Press.</publisher>
<contexts>
<context position="3255" citStr="Talmy, 1983" startWordPosition="495" endWordPosition="496">g, progressing from simple sets of features (Katz and Fodor, 1963) to notations for predicate-argument structure (Lakoff, 1972; Miller and Johnson-Laird, 1976), but the early attempts still limited access to world knowledge and assumed only very restricted sorts of processing. Workers in computational linguistics introduced inference (Rieger, 1974; Schank, 1975) and other complex cognitive processes (Herskovits, 1982) into our understanding of the role of word meaning. Recently linguists have given greater attention to the cognitive processes that would operate on their representations (e.g., Talmy, 1983; Croft, 1986). Independently, in artificial intelligence an effort arose to encode large amounts of commonsense knowledge (Hayes, 1979; Hobbs and Moore, 1985; Hobbs et al. 1985). The research reported here represents a convergence of these various developments. By constructing core theories of certain fundamental phenomena and defining lexical items within these theories, using the full power of predicate calculus, we are able to cope with complexities of word meaning that have hitherto escaped lexical semanticists. Moreover, we can do this within a framework that gives full scope to the plan</context>
</contexts>
<marker>Talmy, 1983</marker>
<rawString>Talmy, Leonard. 1983. How Language Structures Space. In: Herbert Pick and Linda Acredolo, Eds., Spatial Orientation: Theory, Research, and Application, Plenum Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Talmy</author>
</authors>
<title>Force Dynamics in Language</title>
<date>1985</date>
<booktitle>Eds., Proceedings from the Parasession on Causatives and Agentivity, 21st Regional Meeting, Chicago Linguistic Society,</booktitle>
<location>Chicago, Illinois.</location>
<marker>Talmy, 1985</marker>
<rawString>Talmy, Leonard. 1985. Force Dynamics in Language and Thought. In: William H. Effort, Paul D. Kroeber, and Karen L. Peterson, Eds., Proceedings from the Parasession on Causatives and Agentivity, 21st Regional Meeting, Chicago Linguistic Society, Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Zahn</author>
<author>R Z Roskies</author>
</authors>
<title>Fourier Descriptors for Plane Closed Curves.</title>
<date>1972</date>
<journal>IEEE Transactions on Computers,</journal>
<volume>21</volume>
<pages>269--281</pages>
<contexts>
<context position="35874" citStr="Zahn and Roskies, 1972" startWordPosition="6097" endWordPosition="6100">elow. 3.6.4 SHAPE We have been approaching the problem of characterizing shape from a number of different angles. The classical treatment of shape is via the notion of &amp;quot;similarity&amp;quot; in Euclidean geometry, and in Hilbert&apos;s formal reconstruction of Euclidean geometry (Hilbert, 1902) the key primitive concept seems to be that of &amp;quot;congruent angles&amp;quot;. Therefore, we first sought to develop a theory of &amp;quot;orientation&amp;quot;. The shape of an object can then be characterized in terms of changes in orientation of a tangent as one moves about on the surface of the object, as is done in some vision research (e.g., Zahn and Roskies, 1972). In all of this, since &amp;quot;shape&amp;quot; can be used loosely and metaphorically, one question we are asking is whether some minimal, abstract structure can be found in which the notion of &amp;quot;shape&amp;quot; makes sense. Consider, for instance, a graph in which one scale is discrete, or even unordered. Accordingly, we have been examining a number of examples, asking when it seems right to say two structures have different shapes. We have also examined the interactions of shape and Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 247 Jerry R. Hobbs et al. Commonsense Metaphysics and Lexical Sema</context>
</contexts>
<marker>Zahn, Roskies, 1972</marker>
<rawString>Zahn, C. T., and R. Z. Roskies. 1972. Fourier Descriptors for Plane Closed Curves. IEEE Transactions on Computers, Vol. C-21, No. 3: 269-281.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>