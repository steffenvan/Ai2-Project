<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000054">
<title confidence="0.927961">
Polynomial to Linear: Efficient Classification with Conjunctive Features
</title>
<author confidence="0.993409">
Naoki Yoshinaga
</author>
<affiliation confidence="0.998504">
Institute of Industrial Science
University of Tokyo
</affiliation>
<address confidence="0.957601">
4-6-1 Komaba, Meguro-ku, Tokyo
</address>
<email confidence="0.999017">
ynaga@tkl.iis.u-tokyo.ac.jp
</email>
<author confidence="0.977479">
Masaru Kitsuregawa
</author>
<affiliation confidence="0.998362">
Institute of Industrial Science
University of Tokyo
</affiliation>
<address confidence="0.957552">
4-6-1 Komaba, Meguro-ku, Tokyo
</address>
<email confidence="0.998782">
kitsure@tkl.iis.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.994798" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999833588235294">
This paper proposes a method that speeds
up a classifier trained with many con-
junctive features: combinations of (prim-
itive) features. The key idea is to pre-
compute as partial results the weights of
primitive feature vectors that appear fre-
quently in the target NLP task. A trie
compactly stores the primitive feature vec-
tors with their weights, and it enables the
classifier to find for a given feature vec-
tor its longest prefix feature vector whose
weight has already been computed. Ex-
perimental results for a Japanese depen-
dency parsing task show that our method
speeded up the SVM and LLM classifiers
of the parsers, which achieved accuracy of
90.84/90.71%, by a factor of 10.7/11.6.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994385483871">
Deep and accurate text analysis based on discrimi-
native models is not yet efficient enough as a com-
ponent of real-time applications, and it is inade-
quate to process Web-scale corpora for knowledge
acquisition (Pantel, 2007; Saeger et al., 2009) or
semi-supervised learning (McClosky et al., 2006;
Spoustová et al., 2009). One of the main reasons
for this inefficiency is attributed to the inefficiency
of core classifiers trained with many feature com-
binations (e.g., word n-grams). Hereafter, we refer
to features that explicitly represent combinations
of features as conjunctive features and the other
atomic features as primitive features.
The feature combinations play an essential role
in obtaining a classifier with state-of-the-art ac-
curacy for several NLP tasks; recent examples in-
clude dependency parsing (Koo et al., 2008), parse
re-ranking (McClosky et al., 2006), pronoun reso-
lution (Nguyen and Kim, 2008), and semantic role
labeling (Liu and Sarkar, 2007). However, ‘ex-
plicit’ feature combinations significantly increase
the feature space, which slows down not only
training but also testing of the classifier.
Kernel-based methods such as support vector
machines (SVMs) consider feature combinations
space-efficiently by using a polynomial kernel
function (Cortes and Vapnik, 1995). The kernel-
based classification is, however, known to be very
slow in NLP tasks, so efficient classifiers should
sum up the weights of the explicit conjunctive fea-
tures (Isozaki and Kazawa, 2002; Kudo and Mat-
sumoto, 2003; Goldberg and Elhadad, 2008).
E1-regularized log-linear models (E1-LLMs), on
the other hand, provide sparse solutions, in which
weights of irrelevant features are exactly zero, by
assuming a Laplacian prior on the weights (Tibshi-
rani, 1996; Kazama and Tsujii, 2003; Goodman,
2004; Gao et al., 2007). However, as Kazama and
Tsujii (2005) have reported in a text categorization
task and we later confirm in a dependency pars-
ing task, when most features regarded as irrelevant
during training E1-LLMs appear rarely in the task,
we cannot greatly reduce the number of active fea-
tures in each classification. In the end, when effi-
ciency is a major concern, we must use exhaustive
feature selection (Wu et al., 2007; Okanohara and
Tsujii, 2009) or even restrict the order of conjunc-
tive features at the expense of accuracy.
In this study, we provide a simple, but effective
solution to the inefficiency of classifiers trained
with higher-order conjunctive features (or polyno-
mial kernel), by exploiting the Zipfian nature of
language data. The key idea is to precompute the
weights of primitive feature vectors and use them
as partial results to compute the weight of a given
feature vector. We use a trie called the feature
sequence trie to efficiently find for a given fea-
ture vector its longest prefix feature vector whose
weight has been computed. The trie is built from
feature vectors generated by applying the classifier
to actual data in the classification task. The time
complexity of the classifier approaches time that
</bodyText>
<page confidence="0.962178">
1542
</page>
<note confidence="0.996599">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1542–1551,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999648857142857">
is linear with respect to the number of primitive
features when the retrieved feature vector covers
most of the features in the input feature vector.
We implemented our algorithm for SVM and
LLM classifiers and evaluated the performance of
the resulting classifiers in a Japanese dependency
parsing task. Experimental results show that it
successfully speeded up classifiers trained with
higher-order conjunctive features by a factor of 10.
The rest of this paper is organized as follows.
Section 2 introduces LLMs and SVMs. Section 3
proposes our classification algorithm. Section 4
presents experimental results. Section 5 concludes
with a summary and addresses future directions.
</bodyText>
<sectionHeader confidence="0.994567" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.995228590909091">
In this paper, we focus on linear classifiers that cal-
culate the probability (or score) by summing up
weights of individual features. Examples include
not only log-linear models but also support vec-
tor machines with kernel expansion (Isozaki and
Kazawa, 2002; Kudo and Matsumoto, 2003). Be-
low, we introduce these two classifiers and their
ways to consider feature combinations.
In classification-based NLP, the target task is
modeled as one or more classification steps. For
example in part-of-speech (POS) tagging, each
classification decides whether to assign a partic-
ular label (POS tag) to a given sample (each word
in a given sentence). Each sample is then repre-
sented by a feature vector x, whose element xi is
a value of a feature function fi ∈ F.
Here, we assume a binary feature function
fi(x) ∈ {0, 1}, in which a non-zero value means
that particular context data appears in the sample.
We say that a feature fi is active in sample x when
xi = fi(x) = 1 and |x |represents the number of
active features in x (|x |= |{fi|fi(x) = 1}|).
</bodyText>
<subsectionHeader confidence="0.844573">
2.1 Log-Linear Models
</subsectionHeader>
<bodyText confidence="0.997540409090909">
The log-linear model (LLM), or also known as
maximum-entropy model (Berger et al., 1996), is
a linear classifier widely used in the NLP literature.
Let the training data of LLMs be {hxi, yii}Li=1,
where xi ∈ {0,1}n is a feature vector and yi is a
class label associated with xi. We assume a binary
label yi ∈ {±1} here to simplify the argument.
The classifier provides conditional probability
p(y|x) for a given feature vector x and a label y:
where fi,y(x, y) is a feature function that returns
a non-zero value when fi(x) = 1 and the label is
y, wi,y ∈ R is a weight associated with fi,y, and
Z(x) = Ey exp Ei wi,yfi,y(x, y) is the partition
function. We can consider feature combinations in
LLMs by explicitly introducing a new conjunctive
feature fF,,y(x, y) that is activated when a partic-
ular set of features F0 ⊆ F to be combined is acti-
vated (namely, fF,,y(x, y) = Afi,y∈F, fi,y(x, y)).
We then introduce an `1-regularized LLM (`1-
LLM), in which the weight vector w is tuned so
as to maximize the logarithm of the a posteriori
probability of the training data:
</bodyText>
<equation confidence="0.998588">
L
L(w) = log p(yi|xi) − Ckwk1. (2)
i=1
</equation>
<bodyText confidence="0.99553225">
Hyper-parameter C thereby controls the degree of
over-fitting (solution sparseness). Interested read-
ers may refer to the cited literature (Andrew and
Gao, 2007) for the optimization procedures.
</bodyText>
<subsectionHeader confidence="0.999767">
2.2 Support Vector Machines
</subsectionHeader>
<bodyText confidence="0.924503333333333">
A support vector machine (SVM) is a binary clas-
sifier (Cortes and Vapnik, 1995). Training with
samples {hxi,yii}Li=1 where xi ∈ {0,1}n and
</bodyText>
<equation confidence="0.96651125">
yi ∈ {±1} yields the following decision function:
y(x) = sgn(g(x) + b)
g(x) = � yjαjφ(xj)Tφ(x), (3)
xj∈SV
</equation>
<bodyText confidence="0.999744615384615">
where b ∈ R, φ : Rn 7→ RH and support vec-
tors xj ∈ SV (subset of training samples), each
of which is associated with weight αj ∈ R. We
hereafter call g(x) the weight function. Nonlinear
mapping function φ is chosen to make the train-
ing samples linearly separable in RH space. Ker-
nel function k(xj, x) = φ(xj)Tφ(x) is then in-
troduced to compute the dot product in RH space
without mapping x to φ(x).
To consider combinations of primitive features
fj ∈ F, we use a polynomial kernel kd(xj, x) =
(xTj x + 1)d. From Eq. 3, we obtain the weight
function for the polynomial kernel as:
</bodyText>
<equation confidence="0.9916465">
g(x) = � yjαj(xTj x + 1)d. (4)
xj∈SV
</equation>
<bodyText confidence="0.99915725">
Since we assumed that xi is a binary value repre-
senting whether a (primitive) feature fi is active
in the sample, the polynomial kernel of degree d
implies a mapping φd from x to φd(x) that has
</bodyText>
<equation confidence="0.8269244">
1 �
p(y|x) = Z(x) exp wi,yfi,y(x, y), (1)
i
1543
H = Ed= (k) dimensions. Each dimension rep-
</equation>
<bodyText confidence="0.9973032">
resents a (weighted) conjunction of d features in
the original sample x.1
Kernel Expansion (SVM-KE) The time com-
plexity of Eq. 4 is O(|x |· |SV|). This cost is usu-
ally high for classifiers used in NLP tasks because
they often have many support vectors (|SV |&gt;
10, 000). Kernel expansion (KE) was proposed
by Isozaki and Kazawa (2002) to convert Eq. 4
into the linear sum of the weights in the mapped
feature space as in LLM (p(y|x) in Eq. 1):
</bodyText>
<equation confidence="0.9933895">
�g(x) = wTxd = wixdi , (5)
i
</equation>
<bodyText confidence="0.98020405">
where xd is a binary feature vector whose element
xdi has a non-zero value when (φd(x))i &gt; 0, w
is the weight vector for xd in the expanded fea-
ture space Fd and is precalculated from the sup-
port vectors xj and their weights αj. Interested
readers may refer to Kudo and Matsumoto (2003)
for the detailed computation for obtaining w.
The time complexity of Eq. 5 (and Eq. 1) is
O(|xd|), which is linear with respect to the num-
ber of active features in xd within the expanded
feature space Fd.
Heuristic Kernel Expansion (SVM-HKE) To
make the weight vector sparse, Kudo and Mat-
sumoto (2003) proposed a heuristic method that
filters out less useful features whose absolute
weight values are less than a pre-defined threshold
σ.2 They reported that increased threshold value σ
resulted in a dramatically sparse feature space Fd,
which had the side-effects of accuracy degradation
and classifier speed-up.
</bodyText>
<sectionHeader confidence="0.987751" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.99815225">
In this section, we propose a method that speeds
up a classifier trained with many conjunctive fea-
tures. Below, we focus on a kernel-based classifier
trained with a polynomial kernel of degree d (here,
</bodyText>
<footnote confidence="0.998259909090909">
1For example, given an input vector x = (x1, x2)T
and a support vector x0 = (x01, x02)T, the 2nd-order
polynomial kernel returns k2(x0, x) = (x01x1 + x02x2 +
1)2 = 3x01x1 + 3x02x2 + 2x01x1x02x2 + 1 (∵ ,x0i, xi E
{0, 11). This function thus implies a mapping 02(x) =
(1, y 3x1, y 3x2, y 2x1x2)T. In the following argument, we
ignore the dimension of the constant in the mapped space and
assume constant b is set to include it.
2Precisely speaking, they set different thresholds to posi-
tive (αj &gt; 0) and negative (αj &lt; 0) support vectors, consid-
ering the proportion of positive and negative support vectors.
</footnote>
<figureCaption confidence="0.999661">
Figure 1: Efficient computation of g(x).
</figureCaption>
<bodyText confidence="0.995219833333333">
SVMs), but an analogous argument is possible for
linear classifiers (e.g., LLMs).3
We hereafter represent a binary feature vector x
as a set of active features {fi|fi(x) = 1}. x can
thereby be represented as an element of the power
set 2-77 of the set of features F.
</bodyText>
<subsectionHeader confidence="0.96679">
3.1 Idea
</subsectionHeader>
<bodyText confidence="0.998045692307692">
Let us remember that weight function g(x) in
Eq. 5 maps x ∈ 2-77 to W ∈ R. If we could cal-
culate Wx = g(x) for all possible x in advance,
we could obtain g(x) by simply checking |x |ele-
ments, namely, in O(|x|) time. However, because
|{x|x ∈ 2-77} |= 2|-77 |and |F |is likely to be very
large (often |F |&gt; 10, 000 in NLP tasks), this cal-
culation is impractical.
We then compute and store weight Wx0 =
g(x&apos;) for x&apos; ∈ Vc(⊂ 2-77), a certain subset of
the possible value space, and compute g(x) for
x ∈/ Vc by using precalculated weight Wxc for
xc ⊆4x in the following way:
</bodyText>
<equation confidence="0.83863">
g(x) = Wxc + � wi. (6)
fiExd−xdc
</equation>
<bodyText confidence="0.999856857142857">
Intuitively speaking, starting from partial weight
Wxc, we add up remaining weights of primitive
features f ∈ F that are not active in xc but active
in x and conjunctive features that combine f and
the other active features in x.
An example of this computation (d = 2) is de-
picted in Figure 1. We can efficiently compute
g(x) for a vector x that has four active features
f1, f2, f3, and f4 (and x2 has their six conjunc-
tive features) using precalculated weight W11,2,31;
we should first check the three features f1, f2, and
f3 to retrieve W11,2,31 and next check the remain-
ing four features related to f4, namely f4, f1,4,
f2,4, and f3,4, in order to add up the remaining
</bodyText>
<footnote confidence="0.6712232">
3When a feature vector x includes (explicit) conjunctive
features f E Fd, we assume weight function g0(y|x0) =
g(y|x), where x0 is a projection of x (by φ−1
d : Fd � F).
4This means that all active features in xc are active in x.
</footnote>
<page confidence="0.995458">
1544
</page>
<bodyText confidence="0.995770333333333">
weights, while the normal computation in Eq. 5
should check the four primitive and six conjunc-
tive features to get the individual weights.
Expected time complexity Counting the num-
ber of features to be checked in the computation,
we obtain the time complexity f(x, d) of Eq. 6 as:
</bodyText>
<equation confidence="0.9539515">
f(x,d) = O(|xc |+ |xd |_ |xdc|), (7)
where  |xd  |=1: I I k  |l (8)
k=1 \ )
(e.g., |x2 |= |x|2+|x|
2 and |x3 |= |x|3+5|x|
6 ).5 Note
</equation>
<bodyText confidence="0.921584833333333">
that when |xc |becomes close to |x|, this time
complexity actually approaches O(|x|).
Thus, to minimize this computational cost, xc
is to be chosen from Vc as follows:
xc = argmin (|x0 |+ |xd |_ |x0d|). (9)
x&apos;∈V,,x&apos;⊆x
</bodyText>
<subsectionHeader confidence="0.999915">
3.2 Construction of Feature Sequence Trie
</subsectionHeader>
<bodyText confidence="0.999309379310345">
There are two issues with speeding up the classi-
fier by the computation shown in Eq. 6. First, since
we can store weights for only a small fraction of
possible feature vectors (namely, |Vc |« 2|F|), we
should choose Vc so as to maximize its impact on
the speed-up. Second, we should quickly find an
optimal xc from Vc for a given feature vector x.
The solution to the first problem is to enumer-
ate partial feature vectors that frequently appear in
the target task. Note that typical linguistic features
used in NLP tasks usually consist of disjunctive
sets of features (e.g., word surface and POS), in
which each set is likely to follow Zipf’s law (Zipf,
1949) and correlate with each other. We can ex-
pect the distribution of feature vectors, the mixture
of Zipf distributions, to be Zipfian. This has been
confirmed for word n-grams (Egghe, 2000) and
itemset support distribution (Chuang et al., 2008).
We can thereby expect that a small set of partial
feature vectors commonly appear in the task.
To solve the second problem, we introduce a
feature sequence trie (fstrie), which represents a
hierarchy of feature vectors, to enable the clas-
sifier to efficiently retrieve (sub-)optimal xc (in
Eq. 9) for a given feature vector x. We build an
fstrie in the following steps:
Step 1: Apply the target classifier to actual (raw)
data in the task to enumerate possible feature
vectors (hereafter, source feature vectors).
</bodyText>
<footnote confidence="0.978792">
5This is the maximum number of conjunctive features.
</footnote>
<figureCaption confidence="0.990591">
Figure 2: Feature sequence trie and completion of
prefix feature vector weights.
</figureCaption>
<bodyText confidence="0.991406230769231">
Step 2: Sort the features in each source feature
vector according to their frequency in the
training data (in descending order).
Step 3: Build a trie from the source feature vec-
tors by regarding feature indices as characters
and store weights of all prefix feature vectors.
An fstrie built from six source feature vectors is
shown in Figure 2. In fstries, a path from the root
to another node represents a feature vector. An
important point here is that the fstrie stores the
weights of all prefix feature vectors of the source
feature vectors, and the trie structure enables us to
retrieve for a given feature vector x the weight of
its longest prefix vector xc C x in O(|xc|) time.
To handle feature functions in LLMs (Eq. 1), we
store partial weight Wx,,y = Ei wi,yfi,y(xc, y)
for each label y on the node that expresses xc.
Since we sort the features in the source fea-
ture vectors according to their frequency, the pre-
fix feature vectors exclude less frequent features
in the source feature vectors. Lexical features or
finer-grained features (e.g., POS-subcategory) are
usually less frequent than coarse-grained features
(e.g., POS), so they lie in the latter part of the
feature vectors. This sorting helps us to retrieve
longer feature vector xc for input feature vector x
that will have diverse infrequent features. It also
minimizes the size of fstrie by sharing the com-
mon frequent prefix (e.g., {f1, f2} in Figure 2).
Pruning nodes from fstrie We have so far de-
scribed the way to construct an fstrie from the
source feature vectors. However, a naive enumer-
ation of source feature vectors will result in the
explosion of the fstrie size, and we want to have
a principled way to control the fstrie size rather
than reducing the processed data size. Below, we
present a method that prunes useless prefix feature
vectors (nodes) from the constructed fstrie to max-
imize its impact on the classifier efficiency.
</bodyText>
<page confidence="0.969176">
1545
</page>
<table confidence="0.421698666666667">
Algorithm 1 PRUNE NODES FROM FSTRIE
Input: fstrie T, node_limit N E N
Output: fstrie T
</table>
<listItem confidence="0.803247333333333">
1: while # of nodes in T &gt; N do
2: xc +— argmin u(x0)
x,∈leaf(T)
3: remove xc, T
4: end while
5: return T
</listItem>
<bodyText confidence="0.999652818181818">
We adopt a greedy strategy that iteratively
prunes a leaf node (one prefix feature vector and
its weight) from the fstrie built from all the source
feature vectors, according to a certain utility score
calculated for each node. In this study, we con-
sider two metrics for each prefix feature vector xc
to calculate its utility score.
Probability p(xc), which denotes how often the
stored weight Wxc will be used in the tar-
get task. The maximum-likelihood estima-
tion provides probability:
</bodyText>
<equation confidence="0.963093">
p (xc) = �x&apos;�x� nx,
Ex nx
</equation>
<bodyText confidence="0.99986075">
where nx E N is the frequency count of a
source feature vector x in the processed data.
Computation reduction Δd(xc), which denotes
how much computation is reduced by Wxc to
calculate a weight of x _Q xc. This can be es-
timated by counting the number of conjunc-
tive features we additionally have to check
when we remove xc. Since the fstrie stores
the weight of a prefix feature vector xc- C xc
such that |xc- |= |xc |− 1 (e.g., in Figure 2,
xc- = lf1, f2} for xc = lf1, f2, f4}), we
can define the computation reduction as:
</bodyText>
<equation confidence="0.987765285714286">
Δd(xc) = (|xd c |− |xd c-|) − (|xc |− |xc-|)
�
(|k|)− � (|xck − 1)
k=2 k=2
(∵ Eq. 8).
Δ2(xc) = |xc |− 1 and Δ3(xc) = |xc|2−|xc|
2 .
</equation>
<bodyText confidence="0.953501272727273">
We calculate utility score of each node xc in the
fstrie as u(xc) = p(xc) · Δd(xc), which means
the expected computation reduction by xc in the
target task, and prune the lowest-utility-score leaf
nodes from the fstrie one by one (Algorithm 1). If
several prefix vectors have the same utility score,
we eliminate them in numerical descending order.
Algorithm 2 COMPUTE WEIGHT WITH FSTRIE
Input: fstrie T, weight vector w E R|Fd|
feature vector x E 2F
Output: weight W = g(x) E R
</bodyText>
<listItem confidence="0.989318571428571">
1: x +— sort(x)
2: (xc, Wxc) +— prefix_search(T, x)
3: W +— Wxc
4: for all feature fj E xd − xdc do
5: W +— W + wj
6: end for
7: return W
</listItem>
<subsectionHeader confidence="0.997007">
3.3 Classification Algorithm
</subsectionHeader>
<bodyText confidence="0.999947411764706">
Our classification algorithm is shown in detail in
Algorithm 2. The classifier first sorts the active
features in input feature vector x according to their
frequency in the training data. Then, for x, it re-
trieves the longest common prefix vector xc from
the fstrie (line 2 in Algorithm 2). It then adds the
weights of the remaining features to partial weight
Wxc (line 5 in Algorithm 2).
Note that the remaining features whose weights
we sum up (line 4 in Algorithm 2) are primitive
and conjunctive features that relate to f E x − xc,
which appear less frequently than f0 E xc in the
training data. Thus, when we apply our algorithm
to classifiers with the sparse solution (e.g., SVM-
HKEs or `1-LLMs), |xd|−|xd c |can be much smaller
than the theoretical expectation (Eq. 8). We con-
firmed this in the following experiments.
</bodyText>
<sectionHeader confidence="0.999092" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9999865">
We applied our algorithm to SVM-KE, SVM-HKE,
and `1-LLM classifiers and evaluated the resulting
classifiers in a Japanese dependency parsing task.
To the best of our knowledge, there are no previous
reports of an exact weight calculation faster than
linear summation (Eqs. 1 and 5). We also com-
pared our SVM classifier with a classifier called
polynomial kernel inverted (PKI: Kudo and Mat-
sumoto (2003)), which uses the polynomial kernel
(Eq. 4) and inverted indexing to support vectors.
</bodyText>
<subsectionHeader confidence="0.970209">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.998303333333333">
A Japanese dependency parser inputs bunsetsu-
segmented sentences and outputs the correct head
(bunsetsu) for each bunsetsu; here, a bunsetsu is
a grammatical unit in Japanese consisting of one
or more content words followed by zero or more
function words. A parser generates a feature vec-
</bodyText>
<equation confidence="0.92834">
, (10)
</equation>
<page confidence="0.927467">
1546
</page>
<table confidence="0.931967714285714">
Modifier, head word (surface-form, POS, POS-subcategory,
modifiee inflection form), functional word (surface-form,
bunsetsu POS, POS-subcategory, inflection form), brackets,
quotation marks, punctuation marks, position in
sentence (beginning, end)
Between distance (1, 2–5, 6–), case-particles, brackets,
bunsetsus quotation marks, punctuation marks
</table>
<tableCaption confidence="0.999807">
Table 1: Feature set used for experiments.
</tableCaption>
<bodyText confidence="0.999931923076923">
tor for a particular pair of bunsetsus (modifier and
modifiee candidates) by exploiting the head-final
and projective (Nivre, 2003) nature of dependency
relations in Japanese. The classifier then outputs
label y = ‘+1’ (dependent) or ‘−1’ (independent).
Since our classifier is independent of individ-
ual parsing algorithms, we targeted speeding up
(a classifier in) the shift-reduce parser proposed
by Sassano (2004), which has been reported to be
the most efficient for this task, with almost state-
of-the-art accuracy (Iwatate et al., 2008). This
parser decreases the number of classification steps
by using the fact that a bunsetsu is likely to modify
a bunsetsu close to itself. Due to space limitations,
we omit the details of the parsing algorithm.
We used the standard feature set tailored for this
task (Kudo and Matsumoto, 2002; Sassano, 2004;
Iwatate et al., 2008) (Table 1). Note that features
listed in the ‘Between bunsetsus’ row represent
contexts between the target pair of bunsetsus and
appear independently from other features, which
will become an obstacle to finding the longest pre-
fix vector. This task is therefore a better measure
of our method than simple sequential labeling such
as POS tagging or named-entity recognition.
For evaluation, we used Kyoto Text Corpus Ver-
sion 4.0 (Kurohashi and Nagao, 2003), Mainichi
news articles in 1995 that have been manually an-
notated with dependency relations.6 The train-
ing, development, and test sets included 24,283,
4833, and 9284 sentences, and 234,685, 47,571,
and 89,874 bunsetsus, respectively. The training
samples generated from the training set included
150,064 positive and 146,712 negative samples.
The following experiments were performed on
a server with an Intel ® XeonTM 3.20-GHz CPU.
We used TinySVM7 and a simple C++ library for
maximum entropy classification8 to train SVMs
and E1-LLMs, respectively. We used Darts-Clone,9
</bodyText>
<footnote confidence="0.99995175">
6http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus-e.html
7http://chasen.org/˜taku/software/TinySVM/
8http://www-tsujii.is.s.u-tokyo.ac.jp/˜tsuruoka/maxent/
9http://code.google.com/p/darts-clone/
</footnote>
<table confidence="0.997682176470588">
Model type Model statistics Dep. Sent.
Model d w / v |Fd ||Xd |acc. acc.
SVM-KE 1 0 39712 27.3 88.29 46.49
SVM-KE 2 0 1478109 380.6 90.76 53.83
SVM-KE 3 0 26194354 3286.7 90.93&amp;quot;54.43&amp;quot;
SVM-HKE 3 0.001 13247675 2725.9 90.92&amp;quot;54.39&amp;quot;
SVM-HKE 3 0.002 2514385 2238.1 90.91»54.32&gt;
SVM-HKE 3 0.003 793195 1855.4 90.83 54.21
SVM-KE 4 0 293416102 20395.4 90.91&amp;quot;54.69&amp;quot;
SVM-HKE 4 0.0002 96522236 15282.1 90.93&amp;quot;54.53&gt;
SVM-HKE 4 0.0004 19245076 11565.0 90.96&amp;quot;54.64&amp;quot;
SVM-HKE 4 0.0006 7277592 8958.2 90.84 54.48&gt;
i1-LLM 1 1.0 9268 26.5 88.22 46.06
i1-LLM 2 2.0 32575 309.8 90.62 53.46
i1-LLM 3 3.0 129503 2088.3 90.71 54.09&gt;
i1-LLM 3 4.0 85419 1803.0 90.61 53.79
i1-LLM 3 5.0 63046 1699.5 90.59 53.55
</table>
<tableCaption confidence="0.988332">
Table 2: Specifications of LLMs and SVMs. The
</tableCaption>
<bodyText confidence="0.991078555555555">
accuracy marked with ‘»’ or ‘&gt;’ was signifi-
cantly better than the d = 2 counterpart (p &lt; 0.01
or 0.01 &lt; p &lt; 0.05 by McNemar’s test).
a double-array trie (Aoe, 1989; Yata et al., 2008),
as a compact trie implementation. All these li-
braries and algorithms are implemented in C++.
The code for building fstries occupies 100 lines,
while the code for the classifier occupies 20 lines
(except those for kernel expansion).
</bodyText>
<sectionHeader confidence="0.655969" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.992570363636364">
Specifications of SVMs and LLMs used here are
shown in Table 2; |Fd |is the number of active fea-
tures, while |xd |is the average number of active
features in each classification for the test corpus.
Dependency accuracy is the ratio of dependency
relations correctly identified by the parser, while
sentence accuracy is the exact match accuracy of
complete dependency relations in a sentence.
For LLM training, we designed explicit conjunc-
tive features for all the d or lower-order feature
combinations to make the results comparable to
those of SVMs. We could not train d = 4 LLMs
due to parameter explosion. We varied SVM soft
margin parameter c from 0.1 to 0.000001 and LLM
width factor parameter w,10 which controls the im-
pact of the prior, from 1.0 to 5.0, and adjusted
the values to maximize dependency accuracy for
the development set: (d, c) = (1, 0.1), (2, 0.005),
(3, 0.0001), (4, 0.000005) for SVMs and (d, w) =
(1, 1.0), (2, 2.0), (3, 4.0) for E1-LLMs.
The accuracy of around 90.9% (SVM-KE, d =
3,4) is close to the performance of state-of-the-
</bodyText>
<footnote confidence="0.9482385">
10The parameter C of i1-LLM in Eq. 2 was set to w/L
(referred to in Kazama and Tsujii (2003) as ‘single width’).
</footnote>
<page confidence="0.972239">
1547
</page>
<table confidence="0.999159714285714">
Model PKI Baseline Proposed w/ fstrieS Proposed w/ fstrieM Proposed w/ fstrieL Speed
type d classify Mem. Time [ms/sent.] Mem. Time [ms/sent.] Mem. Time [ms/sent.] Mem. Time [ms/sent.] up
[ms/sent.] (MB) classify (total) (MB) classify (total) (MB) classify (total) (MB) classify (total)
SVM-KE 1 13.480 0.2 0.003 (0.015) +0.6 0.006 (0.018) +20.2 0.007 (0.018) +662.9 0.016 (0.029) NA
SVM-KE 2 10.313 13.5 0.041 (0.054) +0.5 0.020 (0.032) +18.0 0.021 (0.034) +662.4 0.023 (0.036) 2.1
SVM-KE 3 10.945 142.2 0.345 (0.361) +0.5 0.163 (0.178) +18.2 0.108 (0.123) +667.0 0.079 (0.093) 4.4
SVM-KE 4 12.603 648.0 2.338 (2.363) +0.5 1.156 (1.178) +18.6 0.671 (0.690) +675.9 0.415 (0.432) 5.6
</table>
<tableCaption confidence="0.999845">
Table 3: Parsing results for test corpus: SVM-KE classifiers with dense feature space.
</tableCaption>
<bodyText confidence="0.99975625">
art parsers (Iwatate et al., 2008), and the model
statistics are considered to be complex (or re-
alistic) enough to evaluate our classifier’s util-
ity. The number of support vectors of SVMs was
71, 766 f 9.2%, which is twice as many as those
used by Kudo and Matsumoto (2003) (34,996) in
their experiments on the same task.
We could clearly observe that the number of ac-
tive features |xd |increased dramatically according
to the order d of feature combinations. The den-
sity of |xd |for SVMs was very high (e.g., |x3 |=
3286.7, close to the maximum shown in Eq. 8:
</bodyText>
<equation confidence="0.649446">
(27.33 + 5 x 27.3)/6 ^_ 3414.
</equation>
<bodyText confidence="0.999922233333333">
For d &gt; 3 models, we attempted to control
the size of the feature space |Fd |by changing
the model’s hyper-parameters: threshold Q for the
SVM-HKE and width factor w for the E1-LLM. Al-
though we successfully reduced the size of the fea-
ture space |Fd|, we could not dramatically reduce
the average number of active features |xd |in each
classification while keeping the accuracy advan-
tage. This confirms that the solution sparseness
does not suffice to obtain an efficient classifier.
We obtained source feature vectors to build
fstries by applying parsers with the target clas-
sifiers to a raw corpus in the target domain,
3,258,313 sentences of 1991–94 Mainichi news
articles that were morphologically analyzed by
JUMAN6 and segmented into bunsetsus by KNP.6
We first built fstrieL using all the source feature
vectors. We then attempted to reduce the number
of prefix feature vectors in fstrieL to 1/2n the size
by Algorithm 1. We refer to fstries built from 1/32
and 1/1024 of the prefix feature vectors in fstrieL
as fstrieM and fstrieS in the following experiments.
Because we exploited Algorithm 2 to calcu-
late the weights of the prefix feature vectors, it
took less than one hour (59 min. 29 sec.) on the
3.20-GHz server to build fstrieL (and calculate the
utility score for all the nodes in it) for the slow-
est SVM-KE (d = 4) from the 40,409,190 source
feature vectors (62,654,549 prefix feature vectors)
generated by parsing the 3,258,313 sentences.
</bodyText>
<figure confidence="0.952925333333333">
Ave. classification time [ms/sent.]
0 100 200 300 400 500 600 700
Size of fstrie [MB]
</figure>
<figureCaption confidence="0.998009">
Figure 3: Average classification time per sentence
plotted against size of fstrie: SVM-KE.
</figureCaption>
<bodyText confidence="0.995699307692308">
Results for SVM-KE with dense feature space
The performances of parsers having SVM-KE clas-
sifiers with and without the fstrie are given in Ta-
ble 3. The ‘speed-up’ column shows the speed-up
factor of the most efficient classifier (bold) ver-
sus the baseline classifier without fstries. Since
each classifier solved a slightly different num-
ber of classification steps (112, 853 f 0.15%), we
show the (average) cumulative classification time
for a sentence. The Mem. columns show the size
of weight vectors for SVM-KE classifiers and the
size of fstriesS, fstriesM, and fstriesL, respectively.
The fstries successfully speeded up SVM-KE
classifiers with the dense feature space.11 The
SVM-KE classifiers without fstries were still faster
than PKI, but as expected from a large |xd |value,
the classifiers with higher conjunctive features
were much slower than the classifier with only
primitive features by factors of 13 (d = 2), 109
(d = 3) and 738 (d = 4) and the classification
time accounted for most of the parsing time.
The average classification time of our classifiers
plotted against fstrie size is shown in Figure 3.
Surprisingly, we obtained a significant speed-up
even with tiny fstrie sizes of &lt; 1 MB. Further-
more, we naively controlled the fstrie size by sim-
</bodyText>
<footnote confidence="0.985302">
11The inefficiency of the classifier (d = 1) results from the
cost of the additional sort function (line 1 in Algorithm 2) and
CPU cache failure due to random accesses to the huge fstries.
</footnote>
<figure confidence="0.7478111">
2.5
0.5
1.5
2
0
1
SVM-KE (d = 1)
SVM-KE (d = 2)
SVM-KE (d = 3)
SVM-KE (d = 4)
</figure>
<page confidence="0.924148">
1548
</page>
<table confidence="0.999161571428571">
Model Baseline Proposed w/ fstrieS Proposed w/ fstrieM Proposed w/ fstrieL Speed
type d or / w Mem. Time [ms/sent.] Mem. Time [ms/sent.] Mem. Time [ms/sent.] Mem. Time [ms/sent.] up
(MB) classify (total) (MB) classify (total) (MB) classify (total) (MB) classify (total)
SVM-HKE 3 0.001 64.6 0.348 (0.363) +0.5 0.151 (0.166) +17.6 0.097 (0.111) +638.0 0.070 (0.084) 5.0
SVM-HKE 3 0.002 13.9 0.332 (0.346) +0.5 0.123 (0.137) +17.0 0.074 (0.088) +612.2 0.053 (0.067) 6.2
SVM-HKE 3 0.003 4.2 0.314 (0.328) +0.4 0.102 (0.115) +14.7 0.057 (0.070) +526.2 0.041 (0.054) 7.8
SVM-HKE 4 0.0002 235.0 2.258 (2.280) +0.5 1.022 (1.042) +17.7 0.558 (0.575) +637.1 0.330 (0.346) 6.8
SVM-HKE 4 0.0004 82.8 2.038 (2.058) +0.5 0.816 (0.835) +16.8 0.414 (0.430) +601.7 0.234 (0.249) 8.7
SVM-HKE 4 0.0006 32.2 1.802 (1.820) +0.4 0.646 (0.662) +15.7 0.311 (0.326) +558.9 0.168 (0.183) 10.7
ii-LLM 1 1.0 0.1 0.004 (0.016) +0.8 0.006 (0.018) +25.0 0.007 (0.019) +787.7 0.016 (0.029) NA
ii-LLM 2 2.0 0.4 0.043 (0.055) +0.6 0.016 (0.028) +20.5 0.015 (0.027) +698.0 0.018 (0.030) 2.9
ii-LLM 3 3.0 1.0 0.314 (0.326) +0.5 0.091 (0.103) +17.8 0.041 (0.054) +601.0 0.027 (0.040) 11.6
ii-LLM 3 4.0 0.7 0.300 (0.313) +0.5 0.082 (0.094) +16.3 0.036 (0.049) +550.1 0.024 (0.037) 12.4
ii-LLM 3 5.0 0.5 0.290 (0.302) +0.5 0.076 (0.088) +15.1 0.032 (0.045) +510.7 0.022 (0.035) 13.3
</table>
<tableCaption confidence="0.999774">
Table 4: Parsing results for test corpus: SVM-HKE and 1-LLM classifiers with sparse feature space.
</tableCaption>
<figure confidence="0.998603566666666">
0 10 20 30 40 50 60 70 80
Size of fstrie [MB]
Ave. classification time [ms/sent.]
2.5
0.5
1.5
2
0
1
0.671 ms/sent.
(18.6 MB)
naive
utility score
0.680 ms/sent.
(67.1 MB)
0.4
SVM-KE (d = 3)
0.35 SVM-HKE (d = 3, σ = 0.001)
SVM-HKE (d = 3, σ = 0.002)
0.3
SVM-HKE (d = 3, σ = 0.003)
0.25
0.2
0.15
0.1
0.05
0
0 100 200 300 400 500 600 700
Size of fstrie [MB]
Ave. classification time [ms/sent.]
</figure>
<figureCaption confidence="0.979827">
Figure 4: Fstrie reduction: utility score vs. pro-
cessed sentence reduction for SVM-KE (d = 4).
</figureCaption>
<bodyText confidence="0.999681333333333">
ply reducing the number of sentences processed to
1/2n. The impact on the speed-up of the resulting
fstries (naive) and the fstries constructed by our
utility score (utility-score) on SVM-KE (d = 4)
is shown in Figure 4. The Zipfian nature of lan-
guage data let us obtain a substantial speed-up
even when we naively reduced the fstrie size, and
the utility score further decreased the fstrie size
required to obtain the same speed-up. We needed
less than 1/3 size fstries to achieve the same speed-
up: 0.671 ms./sent. (18.6 MB) (utility-score) vs.
0.680 ms./sent. (67.1 MB) (naive).
</bodyText>
<subsectionHeader confidence="0.98758">
Results for SVM-HKE and 1-LLM classifiers
</subsectionHeader>
<bodyText confidence="0.996164454545454">
with sparse feature space The performances of
parsers having SVM-HKE and 1-LLM classifiers
with and without the fstrie are given in Table 4.
The fstries successfully speeded up the SVM-HKE
and 1-LLM classifiers by factors of 10.7 (SVM-
HKE, d = 4, Q = 0.0006) and 11.6 ( 1-LLM,
d = 3, w = 3.0). We obtained more speed-
up when we used fstries for classifiers with more
sparse feature space Fd (Figures 5 and 6). The
parsing speed with d = 3 models are now compa-
rable to the parsing speed with d = 2 models.
</bodyText>
<figureCaption confidence="0.99457225">
Figure 5: Average classification time per sentence
plotted against size of fstrie: SVM-HKE (d = 3).
Figure 6: Average classification time per sentence
plotted against size of fstrie: 1-LLM (d = 3).
</figureCaption>
<bodyText confidence="0.999296090909091">
Without fstries, little speed-up of SVM-HKE
classifiers versus the SVM-KE classifiers (in Ta-
ble 3) was obtained due to the mild reduction in
the average number of active features |xd |in the
classification. This result conforms to the results
reported in (Kudo and Matsumoto, 2003).
The parsing speed reached 14,937 sentences
per second with accuracy of 90.91% (SVM-HKE,
d = 3, Q = 0.002). We used this parser to pro-
cess 1,005,918 sentences (5,934,184 bunsetsus)
randomly extracted from Japanese weblog feeds
</bodyText>
<figure confidence="0.995622357142857">
0.4
i1-LLM (d = 3, w = 3.0)
0.35 i1-LLM (d = 3, w = 4.0)
i1-LLM (d = 3, w = 5.0)
0.25
0.2
0.15
0.1
0.05
0
0 100 200 300 400 500 600 700
Size of fstrie [MB]
Ave. classification time [ms/sent.]
0.3
</figure>
<page confidence="0.993461">
1549
</page>
<bodyText confidence="0.9996194375">
updated in November 2008, to see how much the
impact of fstries lessens when the test data and
the data processed to build fstries mismatch. The
parsing time was 156.4 sec. without fstrieL, while
it was just 35.9 sec. with fstrieL. The speed-up
factor of 4.4 on weblog feeds was slightly worse
than that on news articles (0.346/0.067 = 5.2)
but still evident. This implies that sorting features
in building fstries yielded prefix features vectors
that commonly appear in this task, by excluding
domain-specific features such as lexical features.
In summary, our algorithm successfully mini-
mized the efficiency gap among classifiers with
different degrees of feature combinations and
made accurate classifiers trained with higher-order
feature combinations practical.
</bodyText>
<sectionHeader confidence="0.994828" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999985333333333">
Our simple method speeds up a classifier trained
with many conjunctive features by using precal-
culated weights of (partial) feature vectors stored
in a feature sequence trie (fstrie). We experimen-
tally demonstrated that it speeded up SVM and
LLM classifiers for a Japanese dependency pars-
ing task by a factor of 10. We also confirmed that
the sparse feature space provided by E1-LLMs and
SVM-HKEs contributed much to size reduction of
the fstrie required to achieve the same speed-up.
The implementations of the proposed algorithm
for LLMs and SVMs (with a polynomial kernel) and
the Japanese dependency parser will be available
at http://www.tkl.iis.u-tokyo.ac.jp/˜ynaga/.
We plan to apply our method to wider range of
classifiers used in various NLP tasks. To speed up
classifiers used in a real-time application, we can
build fstries incrementally by using feature vec-
tors generated from user inputs. When we run our
classifiers on resource-tight environments such as
cell-phones, we can use a random feature mix-
ing technique (Ganchev and Dredze, 2008) or a
memory-efficient trie implementation based on a
succinct data structure (Jacobson, 1989; Delpratt
et al., 2006) to reduce required memory usage.
We will combine our method with other tech-
niques that provide sparse solutions, for example,
kernel methods on a budget (Dekel and Singer,
2007; Dekel et al., 2008; Orabona et al., 2008) or
kernel approximation (surveyed in Kashima et al.
(2009)). It is also easy to combine our method
with SVMs with partial kernel expansion (Gold-
berg and Elhadad, 2008), which will yield slower
but more space-efficient classifiers. We will in
the future consider an issue of speeding up decod-
ing with structured models (Lafferty et al., 2001;
Miyao and Tsujii, 2002; Sutton et al., 2004).
Acknowledgment The authors wish to thank
Susumu Yata and Yoshimasa Tsuruoka for letting
the authors to use their pre-release libraries. The
authors also thank Nobuhiro Kaji and the anony-
mous reviewers for their valuable comments.
</bodyText>
<sectionHeader confidence="0.997846" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999735871794872">
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of Ll-regularized log-linear models. In Proc.
ICML 2007, pages 33–40.
Jun’ichi Aoe. 1989. An efficient digital search al-
gorithm by using a double-array structure. IEEE
Transactions on Software Engineering, 15(9):1066–
1077, September.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39–71, March.
Kun-Ta Chuang, Jiun-Long Huang, and Ming-Syan
Chen. 2008. Power-law relationship and
self-similarity in the itemset support distribution:
analysis and applications. The VLDB Journal,
17(5):1121–1141, August.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273–
297, September.
Ofer Dekel and Yoram Singer. 2007. Support vec-
tor machines on a budget. In Bernhard Schölkopf,
John Platt, and Thomas Hofmann, editors, Advances
in Neural Information Processing Systems 19, pages
345–352. The MIT Press.
Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer.
2008. The forgetron: A kernel-based perceptron on
a budget. SIAM Journal on Computing, 37(5):1342–
1372, January.
O’Neil Delpratt, Naila Rahman, and Rajeev Raman.
2006. Engineering the LOUDS succinct tree rep-
resentation. In Proc. WEA 2006, pages 134–145.
Leo Egghe. 2000. The distribution of n-grams. Scien-
tometrics, 47(2):237–252, February.
Kuzman Ganchev and Mark Dredze. 2008. Small sta-
tistical models by random feature mixing. In Proc.
ACL 2008 Workshop on Mobile Language Process-
ing, pages 19–20.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study
</reference>
<page confidence="0.786304">
1550
</page>
<reference confidence="0.999247699029126">
of parameter estimation methods for statistical natu-
ral language processing. In Proc. ACL 2007, pages
824–831.
Yoav Goldberg and Michael Elhadad. 2008.
splitSVM: Fast, space-efficient, non-heuristic, poly-
nomial kernel computation for NLP applications. In
Proc. ACL 2008, Short Papers, pages 237–240.
Joshua Goodman. 2004. Exponential priors for max-
imum entropy models. In Proc. HLT-NAACL 2004,
pages 305–311.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recogni-
tion. In Proc. COLING 2002, pages 1–7.
Masakazu Iwatate, Masayuki Asahara, and Yuji Mat-
sumoto. 2008. Japanese dependency parsing using
a tournament model. In Proc. COLING 2008, pages
361–368.
Guy Jacobson. 1989. Space-efficient static trees and
graphs. In Proc. FOCS 1989, pages 549–554.
Hisashi Kashima, Tsuyoshi Idé, Tsuyoshi Kato, and
Masashi Sugiyama. 2009. Recent advances and
trends in large-scale kernel methods. IEICE Trans-
actions on on Information and Systems, E92-D. to
appear.
Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proc. EMNLP 2003, pages
137–144.
Jun’ichi Kazama and Jun’ichi Tsujii. 2005. Maxi-
mum entropy models with inequality constraints: A
case study on text categorization. Machine Learn-
ing, 60(1-3):159–194.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proc. ACL 2008, pages 595–603.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. CoNLL 2002, pages 1–7.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods
for kernel-based text analysis. In Proc. ACL 2003,
pages 24–31.
Sadao Kurohashi and Makoto Nagao. 2003. Build-
ing a Japanese parsed corpus. In Anne Abeillé, edi-
tor, Treebank: Building and Using Parsed Corpora,
pages 249–260. Kluwer Academic Publishers.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proc. ICML 2001, pages 282–289.
Yudong Liu and Anoop Sarkar. 2007. Experimental
evaluation of LTAG-based features for semantic role
labeling. In Proc. EMNLP 2007, pages 590–599.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proc. HLT-NAACL 2006, pages 152–159.
Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc. HLT
2002, pages 292–297.
Ngan L.T. Nguyen and Jin-Dong Kim. 2008. Explor-
ing domain differences for the design of a pronoun
resolution system for biomedical texts. In Proc.
COLING 2008, pages 625–632.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proc. IWPT 2003,
pages 149–160.
Daisuke Okanohara and Jun’ichi Tsujii. 2009. Learn-
ing combination features with Ll regularization. In
Proc. HLT-NAACL 2009, Short Papers, pages 97–
100.
Francesco Orabona, Joseph Keshet, and Barbara Ca-
puto. 2008. The projectron: abounded kernel-based
perceptron. In Proc. ICML 2008, pages 720–727.
Patrick Pantel. 2007. Data catalysis: Facilitating large-
scale natural language data processing. In Proc.
ISUC, pages 201–204.
Stijn De Saeger, Kentaro Torisawa, and Jun’ichi
Kazama. 2009. Mining web-scale treebanks. In
Proc. NLP 2009, pages 837–840.
Manabu Sassano. 2004. Linear-time dependency anal-
ysis for Japanese. In Proc. COLING 2004, pages
8–14.
Drahomíra “Johanka” Spoustová, Jan Hajiˇc, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised
training for the averaged perceptron POS tagger. In
Proc. EACL 2009, pages 763–771.
Charles Sutton, Khashayar Rohanimanesh, and An-
drew McCallum. 2004. Dynamic conditional ran-
dom fields: factorized probabilistic models for label-
ing and segmenting sequence data. In Proc. ICML
2004, pages 783–790.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B, 58(1):267–288, April.
Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee. 2007.
An approximate approach for training polynomial
kernel SVMs in linear time. In Proc. ACL 2007
Poster and Demo Sessions, pages 65–68.
Susumu Yata, Kazuhiro Morita, Masao Fuketa, and
Jun’ichi Aoe. 2008. Fast string matching with
space-efficient word graphs. In Proc. Innovations
in Information Technology 2008, pages 79–83.
George K. Zipf. 1949. Human Behavior and the Prin-
ciple of Least-Effort. Addison-Wesley.
</reference>
<page confidence="0.994065">
1551
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.245853">
<title confidence="0.988905">Polynomial to Linear: Efficient Classification with Conjunctive Features</title>
<author confidence="0.896052">Naoki</author>
<affiliation confidence="0.9995315">Institute of Industrial University of</affiliation>
<address confidence="0.912881">4-6-1 Komaba, Meguro-ku,</address>
<email confidence="0.974169">ynaga@tkl.iis.u-tokyo.ac.jp</email>
<author confidence="0.369636">Masaru</author>
<affiliation confidence="0.999409">Institute of Industrial University of</affiliation>
<address confidence="0.932">4-6-1 Komaba, Meguro-ku,</address>
<email confidence="0.984529">kitsure@tkl.iis.u-tokyo.ac.jp</email>
<abstract confidence="0.993671277777778">This paper proposes a method that speeds up a classifier trained with many conjunctive features: combinations of (primitive) features. The key idea is to precompute as partial results the weights of primitive feature vectors that appear frein the target A trie compactly stores the primitive feature vectors with their weights, and it enables the classifier to find for a given feature vector its longest prefix feature vector whose weight has already been computed. Experimental results for a Japanese dependency parsing task show that our method up the of the parsers, which achieved accuracy of 90.84/90.71%, by a factor of 10.7/11.6.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Jianfeng Gao</author>
</authors>
<title>Scalable training of Ll-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In Proc. ICML</booktitle>
<pages>33--40</pages>
<contexts>
<context position="7278" citStr="Andrew and Gao, 2007" startWordPosition="1167" endWordPosition="1170">tion. We can consider feature combinations in LLMs by explicitly introducing a new conjunctive feature fF,,y(x, y) that is activated when a particular set of features F0 ⊆ F to be combined is activated (namely, fF,,y(x, y) = Afi,y∈F, fi,y(x, y)). We then introduce an `1-regularized LLM (`1- LLM), in which the weight vector w is tuned so as to maximize the logarithm of the a posteriori probability of the training data: L L(w) = log p(yi|xi) − Ckwk1. (2) i=1 Hyper-parameter C thereby controls the degree of over-fitting (solution sparseness). Interested readers may refer to the cited literature (Andrew and Gao, 2007) for the optimization procedures. 2.2 Support Vector Machines A support vector machine (SVM) is a binary classifier (Cortes and Vapnik, 1995). Training with samples {hxi,yii}Li=1 where xi ∈ {0,1}n and yi ∈ {±1} yields the following decision function: y(x) = sgn(g(x) + b) g(x) = � yjαjφ(xj)Tφ(x), (3) xj∈SV where b ∈ R, φ : Rn 7→ RH and support vectors xj ∈ SV (subset of training samples), each of which is associated with weight αj ∈ R. We hereafter call g(x) the weight function. Nonlinear mapping function φ is chosen to make the training samples linearly separable in RH space. Kernel function k</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Galen Andrew and Jianfeng Gao. 2007. Scalable training of Ll-regularized log-linear models. In Proc. ICML 2007, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Aoe</author>
</authors>
<title>An efficient digital search algorithm by using a double-array structure.</title>
<date>1989</date>
<journal>IEEE Transactions on Software Engineering,</journal>
<volume>15</volume>
<issue>9</issue>
<pages>1077</pages>
<contexts>
<context position="23698" citStr="Aoe, 1989" startWordPosition="4020" endWordPosition="4021">.003 793195 1855.4 90.83 54.21 SVM-KE 4 0 293416102 20395.4 90.91&amp;quot;54.69&amp;quot; SVM-HKE 4 0.0002 96522236 15282.1 90.93&amp;quot;54.53&gt; SVM-HKE 4 0.0004 19245076 11565.0 90.96&amp;quot;54.64&amp;quot; SVM-HKE 4 0.0006 7277592 8958.2 90.84 54.48&gt; i1-LLM 1 1.0 9268 26.5 88.22 46.06 i1-LLM 2 2.0 32575 309.8 90.62 53.46 i1-LLM 3 3.0 129503 2088.3 90.71 54.09&gt; i1-LLM 3 4.0 85419 1803.0 90.61 53.79 i1-LLM 3 5.0 63046 1699.5 90.59 53.55 Table 2: Specifications of LLMs and SVMs. The accuracy marked with ‘»’ or ‘&gt;’ was significantly better than the d = 2 counterpart (p &lt; 0.01 or 0.01 &lt; p &lt; 0.05 by McNemar’s test). a double-array trie (Aoe, 1989; Yata et al., 2008), as a compact trie implementation. All these libraries and algorithms are implemented in C++. The code for building fstries occupies 100 lines, while the code for the classifier occupies 20 lines (except those for kernel expansion). 4.2 Results Specifications of SVMs and LLMs used here are shown in Table 2; |Fd |is the number of active features, while |xd |is the average number of active features in each classification for the test corpus. Dependency accuracy is the ratio of dependency relations correctly identified by the parser, while sentence accuracy is the exact match</context>
</contexts>
<marker>Aoe, 1989</marker>
<rawString>Jun’ichi Aoe. 1989. An efficient digital search algorithm by using a double-array structure. IEEE Transactions on Software Engineering, 15(9):1066– 1077, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="6097" citStr="Berger et al., 1996" startWordPosition="955" endWordPosition="958">n decides whether to assign a particular label (POS tag) to a given sample (each word in a given sentence). Each sample is then represented by a feature vector x, whose element xi is a value of a feature function fi ∈ F. Here, we assume a binary feature function fi(x) ∈ {0, 1}, in which a non-zero value means that particular context data appears in the sample. We say that a feature fi is active in sample x when xi = fi(x) = 1 and |x |represents the number of active features in x (|x |= |{fi|fi(x) = 1}|). 2.1 Log-Linear Models The log-linear model (LLM), or also known as maximum-entropy model (Berger et al., 1996), is a linear classifier widely used in the NLP literature. Let the training data of LLMs be {hxi, yii}Li=1, where xi ∈ {0,1}n is a feature vector and yi is a class label associated with xi. We assume a binary label yi ∈ {±1} here to simplify the argument. The classifier provides conditional probability p(y|x) for a given feature vector x and a label y: where fi,y(x, y) is a feature function that returns a non-zero value when fi(x) = 1 and the label is y, wi,y ∈ R is a weight associated with fi,y, and Z(x) = Ey exp Ei wi,yfi,y(x, y) is the partition function. We can consider feature combinatio</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun-Ta Chuang</author>
<author>Jiun-Long Huang</author>
<author>Ming-Syan Chen</author>
</authors>
<title>Power-law relationship and self-similarity in the itemset support distribution: analysis and applications.</title>
<date>2008</date>
<journal>The VLDB Journal,</journal>
<volume>17</volume>
<issue>5</issue>
<contexts>
<context position="14083" citStr="Chuang et al., 2008" startWordPosition="2417" endWordPosition="2420"> quickly find an optimal xc from Vc for a given feature vector x. The solution to the first problem is to enumerate partial feature vectors that frequently appear in the target task. Note that typical linguistic features used in NLP tasks usually consist of disjunctive sets of features (e.g., word surface and POS), in which each set is likely to follow Zipf’s law (Zipf, 1949) and correlate with each other. We can expect the distribution of feature vectors, the mixture of Zipf distributions, to be Zipfian. This has been confirmed for word n-grams (Egghe, 2000) and itemset support distribution (Chuang et al., 2008). We can thereby expect that a small set of partial feature vectors commonly appear in the task. To solve the second problem, we introduce a feature sequence trie (fstrie), which represents a hierarchy of feature vectors, to enable the classifier to efficiently retrieve (sub-)optimal xc (in Eq. 9) for a given feature vector x. We build an fstrie in the following steps: Step 1: Apply the target classifier to actual (raw) data in the task to enumerate possible feature vectors (hereafter, source feature vectors). 5This is the maximum number of conjunctive features. Figure 2: Feature sequence trie</context>
</contexts>
<marker>Chuang, Huang, Chen, 2008</marker>
<rawString>Kun-Ta Chuang, Jiun-Long Huang, and Ming-Syan Chen. 2008. Power-law relationship and self-similarity in the itemset support distribution: analysis and applications. The VLDB Journal, 17(5):1121–1141, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Supportvector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<volume>20</volume>
<issue>3</issue>
<pages>297</pages>
<contexts>
<context position="2339" citStr="Cortes and Vapnik, 1995" startWordPosition="343" endWordPosition="346">y an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks; recent examples include dependency parsing (Koo et al., 2008), parse re-ranking (McClosky et al., 2006), pronoun resolution (Nguyen and Kim, 2008), and semantic role labeling (Liu and Sarkar, 2007). However, ‘explicit’ feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier. Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). E1-regularized log-linear models (E1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization ta</context>
<context position="7419" citStr="Cortes and Vapnik, 1995" startWordPosition="1189" endWordPosition="1192">a particular set of features F0 ⊆ F to be combined is activated (namely, fF,,y(x, y) = Afi,y∈F, fi,y(x, y)). We then introduce an `1-regularized LLM (`1- LLM), in which the weight vector w is tuned so as to maximize the logarithm of the a posteriori probability of the training data: L L(w) = log p(yi|xi) − Ckwk1. (2) i=1 Hyper-parameter C thereby controls the degree of over-fitting (solution sparseness). Interested readers may refer to the cited literature (Andrew and Gao, 2007) for the optimization procedures. 2.2 Support Vector Machines A support vector machine (SVM) is a binary classifier (Cortes and Vapnik, 1995). Training with samples {hxi,yii}Li=1 where xi ∈ {0,1}n and yi ∈ {±1} yields the following decision function: y(x) = sgn(g(x) + b) g(x) = � yjαjφ(xj)Tφ(x), (3) xj∈SV where b ∈ R, φ : Rn 7→ RH and support vectors xj ∈ SV (subset of training samples), each of which is associated with weight αj ∈ R. We hereafter call g(x) the weight function. Nonlinear mapping function φ is chosen to make the training samples linearly separable in RH space. Kernel function k(xj, x) = φ(xj)Tφ(x) is then introduced to compute the dot product in RH space without mapping x to φ(x). To consider combinations of primiti</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine Learning, 20(3):273– 297, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ofer Dekel</author>
<author>Yoram Singer</author>
</authors>
<title>Support vector machines on a budget.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>345--352</pages>
<editor>In Bernhard Schölkopf, John Platt, and Thomas Hofmann, editors,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="35780" citStr="Dekel and Singer, 2007" startWordPosition="6062" endWordPosition="6065">iers used in various NLP tasks. To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs. When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage. We will combine our method with other techniques that provide sparse solutions, for example, kernel methods on a budget (Dekel and Singer, 2007; Dekel et al., 2008; Orabona et al., 2008) or kernel approximation (surveyed in Kashima et al. (2009)). It is also easy to combine our method with SVMs with partial kernel expansion (Goldberg and Elhadad, 2008), which will yield slower but more space-efficient classifiers. We will in the future consider an issue of speeding up decoding with structured models (Lafferty et al., 2001; Miyao and Tsujii, 2002; Sutton et al., 2004). Acknowledgment The authors wish to thank Susumu Yata and Yoshimasa Tsuruoka for letting the authors to use their pre-release libraries. The authors also thank Nobuhiro </context>
</contexts>
<marker>Dekel, Singer, 2007</marker>
<rawString>Ofer Dekel and Yoram Singer. 2007. Support vector machines on a budget. In Bernhard Schölkopf, John Platt, and Thomas Hofmann, editors, Advances in Neural Information Processing Systems 19, pages 345–352. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ofer Dekel</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>The forgetron: A kernel-based perceptron on a budget.</title>
<date>2008</date>
<journal>SIAM Journal on Computing,</journal>
<volume>37</volume>
<issue>5</issue>
<pages>1372</pages>
<contexts>
<context position="35800" citStr="Dekel et al., 2008" startWordPosition="6066" endWordPosition="6069"> tasks. To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs. When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage. We will combine our method with other techniques that provide sparse solutions, for example, kernel methods on a budget (Dekel and Singer, 2007; Dekel et al., 2008; Orabona et al., 2008) or kernel approximation (surveyed in Kashima et al. (2009)). It is also easy to combine our method with SVMs with partial kernel expansion (Goldberg and Elhadad, 2008), which will yield slower but more space-efficient classifiers. We will in the future consider an issue of speeding up decoding with structured models (Lafferty et al., 2001; Miyao and Tsujii, 2002; Sutton et al., 2004). Acknowledgment The authors wish to thank Susumu Yata and Yoshimasa Tsuruoka for letting the authors to use their pre-release libraries. The authors also thank Nobuhiro Kaji and the anonymo</context>
</contexts>
<marker>Dekel, Shalev-Shwartz, Singer, 2008</marker>
<rawString>Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer. 2008. The forgetron: A kernel-based perceptron on a budget. SIAM Journal on Computing, 37(5):1342– 1372, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O’Neil Delpratt</author>
<author>Naila Rahman</author>
<author>Rajeev Raman</author>
</authors>
<title>Engineering the LOUDS succinct tree representation.</title>
<date>2006</date>
<booktitle>In Proc. WEA</booktitle>
<pages>134--145</pages>
<contexts>
<context position="35603" citStr="Delpratt et al., 2006" startWordPosition="6033" endWordPosition="6036">with a polynomial kernel) and the Japanese dependency parser will be available at http://www.tkl.iis.u-tokyo.ac.jp/˜ynaga/. We plan to apply our method to wider range of classifiers used in various NLP tasks. To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs. When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage. We will combine our method with other techniques that provide sparse solutions, for example, kernel methods on a budget (Dekel and Singer, 2007; Dekel et al., 2008; Orabona et al., 2008) or kernel approximation (surveyed in Kashima et al. (2009)). It is also easy to combine our method with SVMs with partial kernel expansion (Goldberg and Elhadad, 2008), which will yield slower but more space-efficient classifiers. We will in the future consider an issue of speeding up decoding with structured models (Lafferty et al., 2001; Miyao and Tsujii, 2002; Sutton et al.</context>
</contexts>
<marker>Delpratt, Rahman, Raman, 2006</marker>
<rawString>O’Neil Delpratt, Naila Rahman, and Rajeev Raman. 2006. Engineering the LOUDS succinct tree representation. In Proc. WEA 2006, pages 134–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Egghe</author>
</authors>
<title>The distribution of n-grams.</title>
<date>2000</date>
<journal>Scientometrics,</journal>
<volume>47</volume>
<issue>2</issue>
<contexts>
<context position="14028" citStr="Egghe, 2000" startWordPosition="2411" endWordPosition="2412">e its impact on the speed-up. Second, we should quickly find an optimal xc from Vc for a given feature vector x. The solution to the first problem is to enumerate partial feature vectors that frequently appear in the target task. Note that typical linguistic features used in NLP tasks usually consist of disjunctive sets of features (e.g., word surface and POS), in which each set is likely to follow Zipf’s law (Zipf, 1949) and correlate with each other. We can expect the distribution of feature vectors, the mixture of Zipf distributions, to be Zipfian. This has been confirmed for word n-grams (Egghe, 2000) and itemset support distribution (Chuang et al., 2008). We can thereby expect that a small set of partial feature vectors commonly appear in the task. To solve the second problem, we introduce a feature sequence trie (fstrie), which represents a hierarchy of feature vectors, to enable the classifier to efficiently retrieve (sub-)optimal xc (in Eq. 9) for a given feature vector x. We build an fstrie in the following steps: Step 1: Apply the target classifier to actual (raw) data in the task to enumerate possible feature vectors (hereafter, source feature vectors). 5This is the maximum number o</context>
</contexts>
<marker>Egghe, 2000</marker>
<rawString>Leo Egghe. 2000. The distribution of n-grams. Scientometrics, 47(2):237–252, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Mark Dredze</author>
</authors>
<title>Small statistical models by random feature mixing.</title>
<date>2008</date>
<booktitle>In Proc. ACL 2008 Workshop on Mobile Language Processing,</booktitle>
<pages>pages</pages>
<contexts>
<context position="35486" citStr="Ganchev and Dredze, 2008" startWordPosition="6016" endWordPosition="6019">n of the fstrie required to achieve the same speed-up. The implementations of the proposed algorithm for LLMs and SVMs (with a polynomial kernel) and the Japanese dependency parser will be available at http://www.tkl.iis.u-tokyo.ac.jp/˜ynaga/. We plan to apply our method to wider range of classifiers used in various NLP tasks. To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs. When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage. We will combine our method with other techniques that provide sparse solutions, for example, kernel methods on a budget (Dekel and Singer, 2007; Dekel et al., 2008; Orabona et al., 2008) or kernel approximation (surveyed in Kashima et al. (2009)). It is also easy to combine our method with SVMs with partial kernel expansion (Goldberg and Elhadad, 2008), which will yield slower but more space-efficient classifiers. We will in the future consider </context>
</contexts>
<marker>Ganchev, Dredze, 2008</marker>
<rawString>Kuzman Ganchev and Mark Dredze. 2008. Small statistical models by random feature mixing. In Proc. ACL 2008 Workshop on Mobile Language Processing, pages 19–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Galen Andrew</author>
<author>Mark Johnson</author>
<author>Kristina Toutanova</author>
</authors>
<title>A comparative study of parameter estimation methods for statistical natural language processing.</title>
<date>2007</date>
<booktitle>In Proc. ACL</booktitle>
<pages>824--831</pages>
<contexts>
<context position="2859" citStr="Gao et al., 2007" startWordPosition="425" endWordPosition="428">re combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). E1-regularized log-linear models (E1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training E1-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification. In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy. In this study, we provide a simple, but effective solution to the inefficiency o</context>
</contexts>
<marker>Gao, Andrew, Johnson, Toutanova, 2007</marker>
<rawString>Jianfeng Gao, Galen Andrew, Mark Johnson, and Kristina Toutanova. 2007. A comparative study of parameter estimation methods for statistical natural language processing. In Proc. ACL 2007, pages 824–831.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>splitSVM: Fast, space-efficient, non-heuristic, polynomial kernel computation for NLP applications.</title>
<date>2008</date>
<booktitle>In Proc. ACL 2008, Short Papers,</booktitle>
<pages>237--240</pages>
<contexts>
<context position="2589" citStr="Goldberg and Elhadad, 2008" startWordPosition="384" endWordPosition="387"> and semantic role labeling (Liu and Sarkar, 2007). However, ‘explicit’ feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier. Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). E1-regularized log-linear models (E1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training E1-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification. In the end, when efficie</context>
</contexts>
<marker>Goldberg, Elhadad, 2008</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2008. splitSVM: Fast, space-efficient, non-heuristic, polynomial kernel computation for NLP applications. In Proc. ACL 2008, Short Papers, pages 237–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Exponential priors for maximum entropy models.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL</booktitle>
<pages>305--311</pages>
<contexts>
<context position="2840" citStr="Goodman, 2004" startWordPosition="423" endWordPosition="424"> consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). E1-regularized log-linear models (E1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training E1-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification. In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy. In this study, we provide a simple, but effective solution to</context>
</contexts>
<marker>Goodman, 2004</marker>
<rawString>Joshua Goodman. 2004. Exponential priors for maximum entropy models. In Proc. HLT-NAACL 2004, pages 305–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Hideto Kazawa</author>
</authors>
<title>Efficient support vector classifiers for named entity recognition.</title>
<date>2002</date>
<booktitle>In Proc. COLING</booktitle>
<pages>1--7</pages>
<contexts>
<context position="2534" citStr="Isozaki and Kazawa, 2002" startWordPosition="375" endWordPosition="378">., 2006), pronoun resolution (Nguyen and Kim, 2008), and semantic role labeling (Liu and Sarkar, 2007). However, ‘explicit’ feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier. Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). E1-regularized log-linear models (E1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training E1-LLMs appear rarely in the task, we cannot greatly reduce the number of active fe</context>
<context position="5201" citStr="Isozaki and Kazawa, 2002" startWordPosition="797" endWordPosition="800">at it successfully speeded up classifiers trained with higher-order conjunctive features by a factor of 10. The rest of this paper is organized as follows. Section 2 introduces LLMs and SVMs. Section 3 proposes our classification algorithm. Section 4 presents experimental results. Section 5 concludes with a summary and addresses future directions. 2 Preliminaries In this paper, we focus on linear classifiers that calculate the probability (or score) by summing up weights of individual features. Examples include not only log-linear models but also support vector machines with kernel expansion (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003). Below, we introduce these two classifiers and their ways to consider feature combinations. In classification-based NLP, the target task is modeled as one or more classification steps. For example in part-of-speech (POS) tagging, each classification decides whether to assign a particular label (POS tag) to a given sample (each word in a given sentence). Each sample is then represented by a feature vector x, whose element xi is a value of a feature function fi ∈ F. Here, we assume a binary feature function fi(x) ∈ {0, 1}, in which a non-zero value means that particul</context>
<context position="8819" citStr="Isozaki and Kazawa (2002)" startWordPosition="1453" endWordPosition="1456"> xj∈SV Since we assumed that xi is a binary value representing whether a (primitive) feature fi is active in the sample, the polynomial kernel of degree d implies a mapping φd from x to φd(x) that has 1 � p(y|x) = Z(x) exp wi,yfi,y(x, y), (1) i 1543 H = Ed= (k) dimensions. Each dimension represents a (weighted) conjunction of d features in the original sample x.1 Kernel Expansion (SVM-KE) The time complexity of Eq. 4 is O(|x |· |SV|). This cost is usually high for classifiers used in NLP tasks because they often have many support vectors (|SV |&gt; 10, 000). Kernel expansion (KE) was proposed by Isozaki and Kazawa (2002) to convert Eq. 4 into the linear sum of the weights in the mapped feature space as in LLM (p(y|x) in Eq. 1): �g(x) = wTxd = wixdi , (5) i where xd is a binary feature vector whose element xdi has a non-zero value when (φd(x))i &gt; 0, w is the weight vector for xd in the expanded feature space Fd and is precalculated from the support vectors xj and their weights αj. Interested readers may refer to Kudo and Matsumoto (2003) for the detailed computation for obtaining w. The time complexity of Eq. 5 (and Eq. 1) is O(|xd|), which is linear with respect to the number of active features in xd within t</context>
</contexts>
<marker>Isozaki, Kazawa, 2002</marker>
<rawString>Hideki Isozaki and Hideto Kazawa. 2002. Efficient support vector classifiers for named entity recognition. In Proc. COLING 2002, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masakazu Iwatate</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency parsing using a tournament model.</title>
<date>2008</date>
<booktitle>In Proc. COLING</booktitle>
<pages>361--368</pages>
<contexts>
<context position="21247" citStr="Iwatate et al., 2008" startWordPosition="3646" endWordPosition="3649">s quotation marks, punctuation marks Table 1: Feature set used for experiments. tor for a particular pair of bunsetsus (modifier and modifiee candidates) by exploiting the head-final and projective (Nivre, 2003) nature of dependency relations in Japanese. The classifier then outputs label y = ‘+1’ (dependent) or ‘−1’ (independent). Since our classifier is independent of individual parsing algorithms, we targeted speeding up (a classifier in) the shift-reduce parser proposed by Sassano (2004), which has been reported to be the most efficient for this task, with almost stateof-the-art accuracy (Iwatate et al., 2008). This parser decreases the number of classification steps by using the fact that a bunsetsu is likely to modify a bunsetsu close to itself. Due to space limitations, we omit the details of the parsing algorithm. We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1). Note that features listed in the ‘Between bunsetsus’ row represent contexts between the target pair of bunsetsus and appear independently from other features, which will become an obstacle to finding the longest prefix vector. This task is therefore a bett</context>
<context position="25940" citStr="Iwatate et al., 2008" startWordPosition="4393" endWordPosition="4396">] up [ms/sent.] (MB) classify (total) (MB) classify (total) (MB) classify (total) (MB) classify (total) SVM-KE 1 13.480 0.2 0.003 (0.015) +0.6 0.006 (0.018) +20.2 0.007 (0.018) +662.9 0.016 (0.029) NA SVM-KE 2 10.313 13.5 0.041 (0.054) +0.5 0.020 (0.032) +18.0 0.021 (0.034) +662.4 0.023 (0.036) 2.1 SVM-KE 3 10.945 142.2 0.345 (0.361) +0.5 0.163 (0.178) +18.2 0.108 (0.123) +667.0 0.079 (0.093) 4.4 SVM-KE 4 12.603 648.0 2.338 (2.363) +0.5 1.156 (1.178) +18.6 0.671 (0.690) +675.9 0.415 (0.432) 5.6 Table 3: Parsing results for test corpus: SVM-KE classifiers with dense feature space. art parsers (Iwatate et al., 2008), and the model statistics are considered to be complex (or realistic) enough to evaluate our classifier’s utility. The number of support vectors of SVMs was 71, 766 f 9.2%, which is twice as many as those used by Kudo and Matsumoto (2003) (34,996) in their experiments on the same task. We could clearly observe that the number of active features |xd |increased dramatically according to the order d of feature combinations. The density of |xd |for SVMs was very high (e.g., |x3 |= 3286.7, close to the maximum shown in Eq. 8: (27.33 + 5 x 27.3)/6 ^_ 3414. For d &gt; 3 models, we attempted to control </context>
</contexts>
<marker>Iwatate, Asahara, Matsumoto, 2008</marker>
<rawString>Masakazu Iwatate, Masayuki Asahara, and Yuji Matsumoto. 2008. Japanese dependency parsing using a tournament model. In Proc. COLING 2008, pages 361–368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Jacobson</author>
</authors>
<title>Space-efficient static trees and graphs.</title>
<date>1989</date>
<booktitle>In Proc. FOCS</booktitle>
<pages>549--554</pages>
<contexts>
<context position="35579" citStr="Jacobson, 1989" startWordPosition="6031" endWordPosition="6032"> LLMs and SVMs (with a polynomial kernel) and the Japanese dependency parser will be available at http://www.tkl.iis.u-tokyo.ac.jp/˜ynaga/. We plan to apply our method to wider range of classifiers used in various NLP tasks. To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs. When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage. We will combine our method with other techniques that provide sparse solutions, for example, kernel methods on a budget (Dekel and Singer, 2007; Dekel et al., 2008; Orabona et al., 2008) or kernel approximation (surveyed in Kashima et al. (2009)). It is also easy to combine our method with SVMs with partial kernel expansion (Goldberg and Elhadad, 2008), which will yield slower but more space-efficient classifiers. We will in the future consider an issue of speeding up decoding with structured models (Lafferty et al., 2001; Miyao and Tsu</context>
</contexts>
<marker>Jacobson, 1989</marker>
<rawString>Guy Jacobson. 1989. Space-efficient static trees and graphs. In Proc. FOCS 1989, pages 549–554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hisashi Kashima</author>
<author>Tsuyoshi Idé</author>
<author>Tsuyoshi Kato</author>
<author>Masashi Sugiyama</author>
</authors>
<title>Recent advances and trends in large-scale kernel methods.</title>
<date>2009</date>
<journal>IEICE Transactions on on Information and Systems,</journal>
<volume>92</volume>
<note>to appear.</note>
<marker>Kashima, Idé, Kato, Sugiyama, 2009</marker>
<rawString>Hisashi Kashima, Tsuyoshi Idé, Tsuyoshi Kato, and Masashi Sugiyama. 2009. Recent advances and trends in large-scale kernel methods. IEICE Transactions on on Information and Systems, E92-D. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluation and extension of maximum entropy models with inequality constraints.</title>
<date>2003</date>
<booktitle>In Proc. EMNLP</booktitle>
<pages>137--144</pages>
<contexts>
<context position="2825" citStr="Kazama and Tsujii, 2003" startWordPosition="419" endWordPosition="422">rt vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). E1-regularized log-linear models (E1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training E1-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification. In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy. In this study, we provide a simple, but effect</context>
<context position="25110" citStr="Kazama and Tsujii (2003)" startWordPosition="4264" endWordPosition="4267">comparable to those of SVMs. We could not train d = 4 LLMs due to parameter explosion. We varied SVM soft margin parameter c from 0.1 to 0.000001 and LLM width factor parameter w,10 which controls the impact of the prior, from 1.0 to 5.0, and adjusted the values to maximize dependency accuracy for the development set: (d, c) = (1, 0.1), (2, 0.005), (3, 0.0001), (4, 0.000005) for SVMs and (d, w) = (1, 1.0), (2, 2.0), (3, 4.0) for E1-LLMs. The accuracy of around 90.9% (SVM-KE, d = 3,4) is close to the performance of state-of-the10The parameter C of i1-LLM in Eq. 2 was set to w/L (referred to in Kazama and Tsujii (2003) as ‘single width’). 1547 Model PKI Baseline Proposed w/ fstrieS Proposed w/ fstrieM Proposed w/ fstrieL Speed type d classify Mem. Time [ms/sent.] Mem. Time [ms/sent.] Mem. Time [ms/sent.] Mem. Time [ms/sent.] up [ms/sent.] (MB) classify (total) (MB) classify (total) (MB) classify (total) (MB) classify (total) SVM-KE 1 13.480 0.2 0.003 (0.015) +0.6 0.006 (0.018) +20.2 0.007 (0.018) +662.9 0.016 (0.029) NA SVM-KE 2 10.313 13.5 0.041 (0.054) +0.5 0.020 (0.032) +18.0 0.021 (0.034) +662.4 0.023 (0.036) 2.1 SVM-KE 3 10.945 142.2 0.345 (0.361) +0.5 0.163 (0.178) +18.2 0.108 (0.123) +667.0 0.079 (0.</context>
</contexts>
<marker>Kazama, Tsujii, 2003</marker>
<rawString>Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation and extension of maximum entropy models with inequality constraints. In Proc. EMNLP 2003, pages 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Maximum entropy models with inequality constraints: A case study on text categorization.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="2897" citStr="Kazama and Tsujii (2005)" startWordPosition="431" endWordPosition="434">ly by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). E1-regularized log-linear models (E1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training E1-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification. In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy. In this study, we provide a simple, but effective solution to the inefficiency of classifiers trained with higher-orde</context>
</contexts>
<marker>Kazama, Tsujii, 2005</marker>
<rawString>Jun’ichi Kazama and Jun’ichi Tsujii. 2005. Maximum entropy models with inequality constraints: A case study on text categorization. Machine Learning, 60(1-3):159–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. ACL</booktitle>
<pages>595--603</pages>
<contexts>
<context position="1876" citStr="Koo et al., 2008" startWordPosition="278" endWordPosition="281">l, 2007; Saeger et al., 2009) or semi-supervised learning (McClosky et al., 2006; Spoustová et al., 2009). One of the main reasons for this inefficiency is attributed to the inefficiency of core classifiers trained with many feature combinations (e.g., word n-grams). Hereafter, we refer to features that explicitly represent combinations of features as conjunctive features and the other atomic features as primitive features. The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks; recent examples include dependency parsing (Koo et al., 2008), parse re-ranking (McClosky et al., 2006), pronoun resolution (Nguyen and Kim, 2008), and semantic role labeling (Liu and Sarkar, 2007). However, ‘explicit’ feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier. Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of t</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proc. ACL 2008, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Proc. CoNLL</booktitle>
<pages>1--7</pages>
<contexts>
<context position="21541" citStr="Kudo and Matsumoto, 2002" startWordPosition="3696" endWordPosition="3699"> = ‘+1’ (dependent) or ‘−1’ (independent). Since our classifier is independent of individual parsing algorithms, we targeted speeding up (a classifier in) the shift-reduce parser proposed by Sassano (2004), which has been reported to be the most efficient for this task, with almost stateof-the-art accuracy (Iwatate et al., 2008). This parser decreases the number of classification steps by using the fact that a bunsetsu is likely to modify a bunsetsu close to itself. Due to space limitations, we omit the details of the parsing algorithm. We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1). Note that features listed in the ‘Between bunsetsus’ row represent contexts between the target pair of bunsetsus and appear independently from other features, which will become an obstacle to finding the longest prefix vector. This task is therefore a better measure of our method than simple sequential labeling such as POS tagging or named-entity recognition. For evaluation, we used Kyoto Text Corpus Version 4.0 (Kurohashi and Nagao, 2003), Mainichi news articles in 1995 that have been manually annotated with dependency relations.6 The training</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Proc. CoNLL 2002, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2003</date>
<booktitle>In Proc. ACL</booktitle>
<pages>24--31</pages>
<contexts>
<context position="2560" citStr="Kudo and Matsumoto, 2003" startWordPosition="379" endWordPosition="383">on (Nguyen and Kim, 2008), and semantic role labeling (Liu and Sarkar, 2007). However, ‘explicit’ feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier. Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). E1-regularized log-linear models (E1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training E1-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classificat</context>
<context position="5228" citStr="Kudo and Matsumoto, 2003" startWordPosition="801" endWordPosition="804"> up classifiers trained with higher-order conjunctive features by a factor of 10. The rest of this paper is organized as follows. Section 2 introduces LLMs and SVMs. Section 3 proposes our classification algorithm. Section 4 presents experimental results. Section 5 concludes with a summary and addresses future directions. 2 Preliminaries In this paper, we focus on linear classifiers that calculate the probability (or score) by summing up weights of individual features. Examples include not only log-linear models but also support vector machines with kernel expansion (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003). Below, we introduce these two classifiers and their ways to consider feature combinations. In classification-based NLP, the target task is modeled as one or more classification steps. For example in part-of-speech (POS) tagging, each classification decides whether to assign a particular label (POS tag) to a given sample (each word in a given sentence). Each sample is then represented by a feature vector x, whose element xi is a value of a feature function fi ∈ F. Here, we assume a binary feature function fi(x) ∈ {0, 1}, in which a non-zero value means that particular context data appears in </context>
<context position="9243" citStr="Kudo and Matsumoto (2003)" startWordPosition="1538" endWordPosition="1541">O(|x |· |SV|). This cost is usually high for classifiers used in NLP tasks because they often have many support vectors (|SV |&gt; 10, 000). Kernel expansion (KE) was proposed by Isozaki and Kazawa (2002) to convert Eq. 4 into the linear sum of the weights in the mapped feature space as in LLM (p(y|x) in Eq. 1): �g(x) = wTxd = wixdi , (5) i where xd is a binary feature vector whose element xdi has a non-zero value when (φd(x))i &gt; 0, w is the weight vector for xd in the expanded feature space Fd and is precalculated from the support vectors xj and their weights αj. Interested readers may refer to Kudo and Matsumoto (2003) for the detailed computation for obtaining w. The time complexity of Eq. 5 (and Eq. 1) is O(|xd|), which is linear with respect to the number of active features in xd within the expanded feature space Fd. Heuristic Kernel Expansion (SVM-HKE) To make the weight vector sparse, Kudo and Matsumoto (2003) proposed a heuristic method that filters out less useful features whose absolute weight values are less than a pre-defined threshold σ.2 They reported that increased threshold value σ resulted in a dramatically sparse feature space Fd, which had the side-effects of accuracy degradation and classi</context>
<context position="19902" citStr="Kudo and Matsumoto (2003)" startWordPosition="3452" endWordPosition="3456">apply our algorithm to classifiers with the sparse solution (e.g., SVMHKEs or `1-LLMs), |xd|−|xd c |can be much smaller than the theoretical expectation (Eq. 8). We confirmed this in the following experiments. 4 Evaluation We applied our algorithm to SVM-KE, SVM-HKE, and `1-LLM classifiers and evaluated the resulting classifiers in a Japanese dependency parsing task. To the best of our knowledge, there are no previous reports of an exact weight calculation faster than linear summation (Eqs. 1 and 5). We also compared our SVM classifier with a classifier called polynomial kernel inverted (PKI: Kudo and Matsumoto (2003)), which uses the polynomial kernel (Eq. 4) and inverted indexing to support vectors. 4.1 Experimental Settings A Japanese dependency parser inputs bunsetsusegmented sentences and outputs the correct head (bunsetsu) for each bunsetsu; here, a bunsetsu is a grammatical unit in Japanese consisting of one or more content words followed by zero or more function words. A parser generates a feature vec, (10) 1546 Modifier, head word (surface-form, POS, POS-subcategory, modifiee inflection form), functional word (surface-form, bunsetsu POS, POS-subcategory, inflection form), brackets, quotation marks</context>
<context position="26179" citStr="Kudo and Matsumoto (2003)" startWordPosition="4437" endWordPosition="4440"> +0.5 0.020 (0.032) +18.0 0.021 (0.034) +662.4 0.023 (0.036) 2.1 SVM-KE 3 10.945 142.2 0.345 (0.361) +0.5 0.163 (0.178) +18.2 0.108 (0.123) +667.0 0.079 (0.093) 4.4 SVM-KE 4 12.603 648.0 2.338 (2.363) +0.5 1.156 (1.178) +18.6 0.671 (0.690) +675.9 0.415 (0.432) 5.6 Table 3: Parsing results for test corpus: SVM-KE classifiers with dense feature space. art parsers (Iwatate et al., 2008), and the model statistics are considered to be complex (or realistic) enough to evaluate our classifier’s utility. The number of support vectors of SVMs was 71, 766 f 9.2%, which is twice as many as those used by Kudo and Matsumoto (2003) (34,996) in their experiments on the same task. We could clearly observe that the number of active features |xd |increased dramatically according to the order d of feature combinations. The density of |xd |for SVMs was very high (e.g., |x3 |= 3286.7, close to the maximum shown in Eq. 8: (27.33 + 5 x 27.3)/6 ^_ 3414. For d &gt; 3 models, we attempted to control the size of the feature space |Fd |by changing the model’s hyper-parameters: threshold Q for the SVM-HKE and width factor w for the E1-LLM. Although we successfully reduced the size of the feature space |Fd|, we could not dramatically redu</context>
<context position="33206" citStr="Kudo and Matsumoto, 2003" startWordPosition="5645" endWordPosition="5648">th more sparse feature space Fd (Figures 5 and 6). The parsing speed with d = 3 models are now comparable to the parsing speed with d = 2 models. Figure 5: Average classification time per sentence plotted against size of fstrie: SVM-HKE (d = 3). Figure 6: Average classification time per sentence plotted against size of fstrie: 1-LLM (d = 3). Without fstries, little speed-up of SVM-HKE classifiers versus the SVM-KE classifiers (in Table 3) was obtained due to the mild reduction in the average number of active features |xd |in the classification. This result conforms to the results reported in (Kudo and Matsumoto, 2003). The parsing speed reached 14,937 sentences per second with accuracy of 90.91% (SVM-HKE, d = 3, Q = 0.002). We used this parser to process 1,005,918 sentences (5,934,184 bunsetsus) randomly extracted from Japanese weblog feeds 0.4 i1-LLM (d = 3, w = 3.0) 0.35 i1-LLM (d = 3, w = 4.0) i1-LLM (d = 3, w = 5.0) 0.25 0.2 0.15 0.1 0.05 0 0 100 200 300 400 500 600 700 Size of fstrie [MB] Ave. classification time [ms/sent.] 0.3 1549 updated in November 2008, to see how much the impact of fstries lessens when the test data and the data processed to build fstries mismatch. The parsing time was 156.4 sec</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proc. ACL 2003, pages 24–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Building a Japanese parsed corpus.</title>
<date>2003</date>
<booktitle>In Anne Abeillé, editor, Treebank: Building and Using Parsed Corpora,</booktitle>
<pages>249--260</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="22034" citStr="Kurohashi and Nagao, 2003" startWordPosition="3774" endWordPosition="3777">ions, we omit the details of the parsing algorithm. We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1). Note that features listed in the ‘Between bunsetsus’ row represent contexts between the target pair of bunsetsus and appear independently from other features, which will become an obstacle to finding the longest prefix vector. This task is therefore a better measure of our method than simple sequential labeling such as POS tagging or named-entity recognition. For evaluation, we used Kyoto Text Corpus Version 4.0 (Kurohashi and Nagao, 2003), Mainichi news articles in 1995 that have been manually annotated with dependency relations.6 The training, development, and test sets included 24,283, 4833, and 9284 sentences, and 234,685, 47,571, and 89,874 bunsetsus, respectively. The training samples generated from the training set included 150,064 positive and 146,712 negative samples. The following experiments were performed on a server with an Intel ® XeonTM 3.20-GHz CPU. We used TinySVM7 and a simple C++ library for maximum entropy classification8 to train SVMs and E1-LLMs, respectively. We used Darts-Clone,9 6http://nlp.kuee.kyoto-u</context>
</contexts>
<marker>Kurohashi, Nagao, 2003</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 2003. Building a Japanese parsed corpus. In Anne Abeillé, editor, Treebank: Building and Using Parsed Corpora, pages 249–260. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ICML</booktitle>
<pages>282--289</pages>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ICML 2001, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yudong Liu</author>
<author>Anoop Sarkar</author>
</authors>
<title>Experimental evaluation of LTAG-based features for semantic role labeling.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP</booktitle>
<pages>590--599</pages>
<contexts>
<context position="2012" citStr="Liu and Sarkar, 2007" startWordPosition="299" endWordPosition="302">or this inefficiency is attributed to the inefficiency of core classifiers trained with many feature combinations (e.g., word n-grams). Hereafter, we refer to features that explicitly represent combinations of features as conjunctive features and the other atomic features as primitive features. The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks; recent examples include dependency parsing (Koo et al., 2008), parse re-ranking (McClosky et al., 2006), pronoun resolution (Nguyen and Kim, 2008), and semantic role labeling (Liu and Sarkar, 2007). However, ‘explicit’ feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier. Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). E1-regularized log-li</context>
</contexts>
<marker>Liu, Sarkar, 2007</marker>
<rawString>Yudong Liu and Anoop Sarkar. 2007. Experimental evaluation of LTAG-based features for semantic role labeling. In Proc. EMNLP 2007, pages 590–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proc. HLT-NAACL</booktitle>
<pages>152--159</pages>
<contexts>
<context position="1339" citStr="McClosky et al., 2006" startWordPosition="198" endWordPosition="201">o find for a given feature vector its longest prefix feature vector whose weight has already been computed. Experimental results for a Japanese dependency parsing task show that our method speeded up the SVM and LLM classifiers of the parsers, which achieved accuracy of 90.84/90.71%, by a factor of 10.7/11.6. 1 Introduction Deep and accurate text analysis based on discriminative models is not yet efficient enough as a component of real-time applications, and it is inadequate to process Web-scale corpora for knowledge acquisition (Pantel, 2007; Saeger et al., 2009) or semi-supervised learning (McClosky et al., 2006; Spoustová et al., 2009). One of the main reasons for this inefficiency is attributed to the inefficiency of core classifiers trained with many feature combinations (e.g., word n-grams). Hereafter, we refer to features that explicitly represent combinations of features as conjunctive features and the other atomic features as primitive features. The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks; recent examples include dependency parsing (Koo et al., 2008), parse re-ranking (McClosky et al., 2006), pronoun resolution </context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proc. HLT-NAACL 2006, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proc. HLT</booktitle>
<pages>292--297</pages>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum entropy estimation for feature forests. In Proc. HLT 2002, pages 292–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ngan L T Nguyen</author>
<author>Jin-Dong Kim</author>
</authors>
<title>Exploring domain differences for the design of a pronoun resolution system for biomedical texts.</title>
<date>2008</date>
<booktitle>In Proc. COLING</booktitle>
<pages>625--632</pages>
<contexts>
<context position="1961" citStr="Nguyen and Kim, 2008" startWordPosition="291" endWordPosition="294"> Spoustová et al., 2009). One of the main reasons for this inefficiency is attributed to the inefficiency of core classifiers trained with many feature combinations (e.g., word n-grams). Hereafter, we refer to features that explicitly represent combinations of features as conjunctive features and the other atomic features as primitive features. The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks; recent examples include dependency parsing (Koo et al., 2008), parse re-ranking (McClosky et al., 2006), pronoun resolution (Nguyen and Kim, 2008), and semantic role labeling (Liu and Sarkar, 2007). However, ‘explicit’ feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier. Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003;</context>
</contexts>
<marker>Nguyen, Kim, 2008</marker>
<rawString>Ngan L.T. Nguyen and Jin-Dong Kim. 2008. Exploring domain differences for the design of a pronoun resolution system for biomedical texts. In Proc. COLING 2008, pages 625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proc. IWPT</booktitle>
<pages>149--160</pages>
<contexts>
<context position="20837" citStr="Nivre, 2003" startWordPosition="3585" endWordPosition="3586">ds followed by zero or more function words. A parser generates a feature vec, (10) 1546 Modifier, head word (surface-form, POS, POS-subcategory, modifiee inflection form), functional word (surface-form, bunsetsu POS, POS-subcategory, inflection form), brackets, quotation marks, punctuation marks, position in sentence (beginning, end) Between distance (1, 2–5, 6–), case-particles, brackets, bunsetsus quotation marks, punctuation marks Table 1: Feature set used for experiments. tor for a particular pair of bunsetsus (modifier and modifiee candidates) by exploiting the head-final and projective (Nivre, 2003) nature of dependency relations in Japanese. The classifier then outputs label y = ‘+1’ (dependent) or ‘−1’ (independent). Since our classifier is independent of individual parsing algorithms, we targeted speeding up (a classifier in) the shift-reduce parser proposed by Sassano (2004), which has been reported to be the most efficient for this task, with almost stateof-the-art accuracy (Iwatate et al., 2008). This parser decreases the number of classification steps by using the fact that a bunsetsu is likely to modify a bunsetsu close to itself. Due to space limitations, we omit the details of </context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proc. IWPT 2003, pages 149–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Learning combination features with Ll regularization.</title>
<date>2009</date>
<booktitle>In Proc. HLT-NAACL 2009, Short Papers,</booktitle>
<pages>97--100</pages>
<contexts>
<context position="3299" citStr="Okanohara and Tsujii, 2009" startWordPosition="499" endWordPosition="502">utions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training E1-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification. In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy. In this study, we provide a simple, but effective solution to the inefficiency of classifiers trained with higher-order conjunctive features (or polynomial kernel), by exploiting the Zipfian nature of language data. The key idea is to precompute the weights of primitive feature vectors and use them as partial results to compute the weight of a given feature vector. We use a trie called the feature sequence trie to efficiently find for a given feature vector its longest prefix feature vector whose weight has been co</context>
</contexts>
<marker>Okanohara, Tsujii, 2009</marker>
<rawString>Daisuke Okanohara and Jun’ichi Tsujii. 2009. Learning combination features with Ll regularization. In Proc. HLT-NAACL 2009, Short Papers, pages 97– 100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Orabona</author>
<author>Joseph Keshet</author>
<author>Barbara Caputo</author>
</authors>
<title>The projectron: abounded kernel-based perceptron.</title>
<date>2008</date>
<booktitle>In Proc. ICML</booktitle>
<pages>720--727</pages>
<contexts>
<context position="35823" citStr="Orabona et al., 2008" startWordPosition="6070" endWordPosition="6073">classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs. When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage. We will combine our method with other techniques that provide sparse solutions, for example, kernel methods on a budget (Dekel and Singer, 2007; Dekel et al., 2008; Orabona et al., 2008) or kernel approximation (surveyed in Kashima et al. (2009)). It is also easy to combine our method with SVMs with partial kernel expansion (Goldberg and Elhadad, 2008), which will yield slower but more space-efficient classifiers. We will in the future consider an issue of speeding up decoding with structured models (Lafferty et al., 2001; Miyao and Tsujii, 2002; Sutton et al., 2004). Acknowledgment The authors wish to thank Susumu Yata and Yoshimasa Tsuruoka for letting the authors to use their pre-release libraries. The authors also thank Nobuhiro Kaji and the anonymous reviewers for their </context>
</contexts>
<marker>Orabona, Keshet, Caputo, 2008</marker>
<rawString>Francesco Orabona, Joseph Keshet, and Barbara Caputo. 2008. The projectron: abounded kernel-based perceptron. In Proc. ICML 2008, pages 720–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Data catalysis: Facilitating largescale natural language data processing.</title>
<date>2007</date>
<booktitle>In Proc. ISUC,</booktitle>
<pages>201--204</pages>
<contexts>
<context position="1266" citStr="Pantel, 2007" startWordPosition="189" endWordPosition="190">ture vectors with their weights, and it enables the classifier to find for a given feature vector its longest prefix feature vector whose weight has already been computed. Experimental results for a Japanese dependency parsing task show that our method speeded up the SVM and LLM classifiers of the parsers, which achieved accuracy of 90.84/90.71%, by a factor of 10.7/11.6. 1 Introduction Deep and accurate text analysis based on discriminative models is not yet efficient enough as a component of real-time applications, and it is inadequate to process Web-scale corpora for knowledge acquisition (Pantel, 2007; Saeger et al., 2009) or semi-supervised learning (McClosky et al., 2006; Spoustová et al., 2009). One of the main reasons for this inefficiency is attributed to the inefficiency of core classifiers trained with many feature combinations (e.g., word n-grams). Hereafter, we refer to features that explicitly represent combinations of features as conjunctive features and the other atomic features as primitive features. The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks; recent examples include dependency parsing (Koo et </context>
</contexts>
<marker>Pantel, 2007</marker>
<rawString>Patrick Pantel. 2007. Data catalysis: Facilitating largescale natural language data processing. In Proc. ISUC, pages 201–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stijn De Saeger</author>
<author>Kentaro Torisawa</author>
<author>Jun’ichi Kazama</author>
</authors>
<title>Mining web-scale treebanks.</title>
<date>2009</date>
<booktitle>In Proc. NLP</booktitle>
<pages>837--840</pages>
<marker>De Saeger, Torisawa, Kazama, 2009</marker>
<rawString>Stijn De Saeger, Kentaro Torisawa, and Jun’ichi Kazama. 2009. Mining web-scale treebanks. In Proc. NLP 2009, pages 837–840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Sassano</author>
</authors>
<title>Linear-time dependency analysis for Japanese.</title>
<date>2004</date>
<booktitle>In Proc. COLING</booktitle>
<pages>8--14</pages>
<contexts>
<context position="21122" citStr="Sassano (2004)" startWordPosition="3627" endWordPosition="3628">tuation marks, position in sentence (beginning, end) Between distance (1, 2–5, 6–), case-particles, brackets, bunsetsus quotation marks, punctuation marks Table 1: Feature set used for experiments. tor for a particular pair of bunsetsus (modifier and modifiee candidates) by exploiting the head-final and projective (Nivre, 2003) nature of dependency relations in Japanese. The classifier then outputs label y = ‘+1’ (dependent) or ‘−1’ (independent). Since our classifier is independent of individual parsing algorithms, we targeted speeding up (a classifier in) the shift-reduce parser proposed by Sassano (2004), which has been reported to be the most efficient for this task, with almost stateof-the-art accuracy (Iwatate et al., 2008). This parser decreases the number of classification steps by using the fact that a bunsetsu is likely to modify a bunsetsu close to itself. Due to space limitations, we omit the details of the parsing algorithm. We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1). Note that features listed in the ‘Between bunsetsus’ row represent contexts between the target pair of bunsetsus and appear indepen</context>
</contexts>
<marker>Sassano, 2004</marker>
<rawString>Manabu Sassano. 2004. Linear-time dependency analysis for Japanese. In Proc. COLING 2004, pages 8–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahomíra “Johanka” Spoustová</author>
<author>Jan Hajiˇc</author>
<author>Jan Raab</author>
<author>Miroslav Spousta</author>
</authors>
<title>Semi-supervised training for the averaged perceptron POS tagger.</title>
<date>2009</date>
<booktitle>In Proc. EACL</booktitle>
<pages>763--771</pages>
<marker>Spoustová, Hajiˇc, Raab, Spousta, 2009</marker>
<rawString>Drahomíra “Johanka” Spoustová, Jan Hajiˇc, Jan Raab, and Miroslav Spousta. 2009. Semi-supervised training for the averaged perceptron POS tagger. In Proc. EACL 2009, pages 763–771.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Khashayar Rohanimanesh</author>
<author>Andrew McCallum</author>
</authors>
<title>Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data. In</title>
<date>2004</date>
<booktitle>Proc. ICML</booktitle>
<pages>783--790</pages>
<marker>Sutton, Rohanimanesh, McCallum, 2004</marker>
<rawString>Charles Sutton, Khashayar Rohanimanesh, and Andrew McCallum. 2004. Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data. In Proc. ICML 2004, pages 783–790.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1996</date>
<journal>Journal of the Royal Statistical Society. Series B,</journal>
<volume>58</volume>
<issue>1</issue>
<contexts>
<context position="2800" citStr="Tibshirani, 1996" startWordPosition="416" endWordPosition="418">hods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). E1-regularized log-linear models (E1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training E1-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification. In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy. In this study, we pro</context>
</contexts>
<marker>Tibshirani, 1996</marker>
<rawString>Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B, 58(1):267–288, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Chieh Wu</author>
<author>Jie-Chi Yang</author>
<author>Yue-Shi Lee</author>
</authors>
<title>An approximate approach for training polynomial kernel SVMs in linear time.</title>
<date>2007</date>
<booktitle>In Proc. ACL 2007 Poster and Demo Sessions,</booktitle>
<pages>65--68</pages>
<contexts>
<context position="3270" citStr="Wu et al., 2007" startWordPosition="495" endWordPosition="498">rovide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training E1-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification. In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy. In this study, we provide a simple, but effective solution to the inefficiency of classifiers trained with higher-order conjunctive features (or polynomial kernel), by exploiting the Zipfian nature of language data. The key idea is to precompute the weights of primitive feature vectors and use them as partial results to compute the weight of a given feature vector. We use a trie called the feature sequence trie to efficiently find for a given feature vector its longest prefix feature ve</context>
</contexts>
<marker>Wu, Yang, Lee, 2007</marker>
<rawString>Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee. 2007. An approximate approach for training polynomial kernel SVMs in linear time. In Proc. ACL 2007 Poster and Demo Sessions, pages 65–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susumu Yata</author>
<author>Kazuhiro Morita</author>
<author>Masao Fuketa</author>
<author>Jun’ichi Aoe</author>
</authors>
<title>Fast string matching with space-efficient word graphs.</title>
<date>2008</date>
<booktitle>In Proc. Innovations in Information Technology</booktitle>
<pages>79--83</pages>
<contexts>
<context position="23718" citStr="Yata et al., 2008" startWordPosition="4022" endWordPosition="4025"> 1855.4 90.83 54.21 SVM-KE 4 0 293416102 20395.4 90.91&amp;quot;54.69&amp;quot; SVM-HKE 4 0.0002 96522236 15282.1 90.93&amp;quot;54.53&gt; SVM-HKE 4 0.0004 19245076 11565.0 90.96&amp;quot;54.64&amp;quot; SVM-HKE 4 0.0006 7277592 8958.2 90.84 54.48&gt; i1-LLM 1 1.0 9268 26.5 88.22 46.06 i1-LLM 2 2.0 32575 309.8 90.62 53.46 i1-LLM 3 3.0 129503 2088.3 90.71 54.09&gt; i1-LLM 3 4.0 85419 1803.0 90.61 53.79 i1-LLM 3 5.0 63046 1699.5 90.59 53.55 Table 2: Specifications of LLMs and SVMs. The accuracy marked with ‘»’ or ‘&gt;’ was significantly better than the d = 2 counterpart (p &lt; 0.01 or 0.01 &lt; p &lt; 0.05 by McNemar’s test). a double-array trie (Aoe, 1989; Yata et al., 2008), as a compact trie implementation. All these libraries and algorithms are implemented in C++. The code for building fstries occupies 100 lines, while the code for the classifier occupies 20 lines (except those for kernel expansion). 4.2 Results Specifications of SVMs and LLMs used here are shown in Table 2; |Fd |is the number of active features, while |xd |is the average number of active features in each classification for the test corpus. Dependency accuracy is the ratio of dependency relations correctly identified by the parser, while sentence accuracy is the exact match accuracy of complet</context>
</contexts>
<marker>Yata, Morita, Fuketa, Aoe, 2008</marker>
<rawString>Susumu Yata, Kazuhiro Morita, Masao Fuketa, and Jun’ichi Aoe. 2008. Fast string matching with space-efficient word graphs. In Proc. Innovations in Information Technology 2008, pages 79–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George K Zipf</author>
</authors>
<title>Human Behavior and the Principle of Least-Effort.</title>
<date>1949</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="13841" citStr="Zipf, 1949" startWordPosition="2380" endWordPosition="2381">ier by the computation shown in Eq. 6. First, since we can store weights for only a small fraction of possible feature vectors (namely, |Vc |« 2|F|), we should choose Vc so as to maximize its impact on the speed-up. Second, we should quickly find an optimal xc from Vc for a given feature vector x. The solution to the first problem is to enumerate partial feature vectors that frequently appear in the target task. Note that typical linguistic features used in NLP tasks usually consist of disjunctive sets of features (e.g., word surface and POS), in which each set is likely to follow Zipf’s law (Zipf, 1949) and correlate with each other. We can expect the distribution of feature vectors, the mixture of Zipf distributions, to be Zipfian. This has been confirmed for word n-grams (Egghe, 2000) and itemset support distribution (Chuang et al., 2008). We can thereby expect that a small set of partial feature vectors commonly appear in the task. To solve the second problem, we introduce a feature sequence trie (fstrie), which represents a hierarchy of feature vectors, to enable the classifier to efficiently retrieve (sub-)optimal xc (in Eq. 9) for a given feature vector x. We build an fstrie in the fol</context>
</contexts>
<marker>Zipf, 1949</marker>
<rawString>George K. Zipf. 1949. Human Behavior and the Principle of Least-Effort. Addison-Wesley.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>