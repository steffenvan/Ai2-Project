<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.074049">
<title confidence="0.921589">
On Minimizing Training Corpus for Parser Acquisition *
</title>
<author confidence="0.995473">
Rebecca Hwa
</author>
<affiliation confidence="0.9981785">
Institute for Advanced Computer Studies,
University of Maryland
</affiliation>
<address confidence="0.842588">
College Park, MD 20742 USA
</address>
<email confidence="0.857396">
hwa©umiacs.umd.edu
</email>
<sectionHeader confidence="0.968662" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899518518519">
Many corpus-based natural lan-
guage processing systems rely on
using large quantities of annotated
text as their training examples.
Building this kind of resource is
an expensive and labor-intensive
project. To minimize effort spent
on annotating examples that are not
helpful the training process, recent
research efforts have begun to ap-
ply active learning techniques to se-
lectively choose data to be anno-
tated. In this work, we consider se-
lecting training examples with the
tree-entropy metric. Our goal is to
assess how well this selection tech-
nique can be applied for training dif-
ferent types of parsers. We find that
tree-entropy can significantly reduce
the amount of training annotation
for both a history-based parser and
EM-based parser. Moreover, the
examples selected for the history-
based parser are also good for train-
ing the EM-based parser, suggesting
that the technique is parser indepen-
dent.
</bodyText>
<sectionHeader confidence="0.997026" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9898045">
In recent years, large collections of text in
machine readable format have become readily
</bodyText>
<footnote confidence="0.801542833333333">
This material is based upon work supported by
the National Science Foundation under Grant No. IRI
9712068 and DARPA contract N6600197C8540. We
thank Michael Collins for the use of his parser; and Ric
Crabbe and the anonymous reviewers for their com-
ments on the paper.
</footnote>
<bodyText confidence="0.999905423076924">
available. These ought be valuable resources
for training natural language processing sys-
tem. Unfortunately, most systems cannot
take advantage of the data in their raw text
form; typically, the data must be annotated
by a human to become effective training ex-
amples. For instance, consider the task of in-
ducing a grammar to parse English sentences.
Studies have shown that a grammar trained
OIL sentences annotated with their constituent
trees produces much better parses than one
trained on just the sentences alone (Pereira
and Schabes, 1992). Recent state-of-the-
art parsers developed by Collins (1997) and
Charniak (1999) are all trained from hand-
annotated corpora such as those from the
Penn Treebank Project (Marcus et al., 1993).
However, building an annotated corpus is a
human labor-intensive project; therefore, it is
important to find ways to minimize the size
of the corpus.
Out of a large pool of raw text, what sub-
set should be annotated and added to the
training set? Recent studies have begun to
address this question using sample selection,
in which training process is seen as interac-
tive session between the learning system and
the human annotator (Lewis and Gale, 1994),
(Engelson and Dagan, 1996), (Fujii et al.,
1998), (Thompson et al., 1999), and (Ngai
and Yarowsky, 2000). The system actively
influences its learning progress by evaluating
potential candidates from the pool of raw text
and selecting those with high Training Utility
Values (TUV) for humans to annotate. As the
learning process continues, the system should
become better at identifying good training
candidates so that the annotators would not
need to waste time ort processing uninforma-
tive examples.
This work considers the problem of apply-
ing sample selection techniques to the task of
training statistical parsers. Our primary chal-
lenge is in designing a function that can accu-
rately estimate art unlabeled candidate&apos;s po-
tential utility for training a parser. lit a previ-
ous study (Hwa, 2000b), we have applied sam-
ple selection to art induction algorithm based
on the expectation-maximization (EM) prin-
ciple that induces Probabilistic Lexicalized
Tree Insertion Grammars (PLTIGs). In that
work, we proposed art uncertainty-based eval-
uation function to estimate the TUV of unla-
beled candidates called tree entropy. We have
empirically shown that sample selection with
tree entropy can reduce the size of the training
corpus significantly. However, because only
art EM-based learner was used, it is unknown
whether the evaluation function would be gen-
eral enough to be applicable to other types of
learners. The goal of this work is to assess
the robustness of the tree-entropy evaluation
function. We have performed experiments to
evaluate how well the metric selects training
examples for different types of parsers and
to determine whether examples selected for
one type of parser might be good for training
a different type of parser. Our experimen-
tal results show that the tree-entropy metric
can reduce the amount of training annotation
by 23% for a history-based lexical statisti-
cal parser, the Model 2 parser described by
Coffins (1997). Moreover, we found that the
data selected for training the Collins Parser
also make good training examples for induc-
ing the EM-based PLTIG parser, suggesting
that the tree-entropy evaluation function is
parser independent.
</bodyText>
<sectionHeader confidence="0.961943" genericHeader="method">
2 The Learning Framework
</sectionHeader>
<bodyText confidence="0.999951142857143">
There are two types of sample selection al-
gorithms • committee based or single learner.
A committee-based selection algorithm works
with multiple learners, each maintaining a dif-
ferent hypothesis (perhaps pertaining to dif-
ferent aspects of the problem). The candidate
examples that lead to the most disagreements
</bodyText>
<listItem confidence="0.56835075">
U is a set of unlabeled candidates.
L is a set of labeled training examples.
Al is the current model.
Initialize:
</listItem>
<figure confidence="0.803356375">
Al Train(L).
Repeat
N Select(n,U,M, f).
U U — N.
L L U Label(N).
Al Train(L).
Until (Al Altrue) or
(U = 0) or (human stops).
</figure>
<figureCaption confidence="0.9633095">
Figure 1: The pseudo-code for the sample se-
lection learning algorithm
</figureCaption>
<bodyText confidence="0.99865244">
among the different learners are considered to
have the highest TUV. (Cohn et al., 1994; Fre-
und et al., 1997). For computationally inten-
sive problems, keeping multiple learners may
be impractical. In this work, we focus ort sam-
ple selection algorithms that use only a single
learner that keeps just one working hypoth-
esis. Without access to multiple hypotheses,
the selection algorithm can nonetheless esti-
mate the TUV of art example. We categorize
some possible ranking criteria into the follow-
ing three classes:
Problem-space: Knowledge about the
problem-space may help to locate
good training canidates. For example,
knowing the distribution of the pool,
we might select the most frequently
occuring instances.
Performance of the hypothesis: Testing
the candidates ort the current hypothesis
may show the type of data ort which the
hypothesis performs weakly (Lewis and
Catlett, 1994).
Parameters of the hypothesis:
Estimating the potential impact of
the candidates will have ort the param-
eters of the current working hypothesis
locates those examples that will change
the current hypothesis the most.
Figure 1 outlines the single-learner sample
selection training loop in pseudo-code. Ini-
tially, the training set, L, consists of a small
number of labeled examples. The learner uses
L to train an initial model Al. Also avail-
able to the learner is a large pool of unlabeled
training candidates, U. In each iteration, the
selection algorithm, S elect(n,U, Al, f), uses
art evaluation function f to compute the ex-
pected TUV of each candidate in U and re-
turns the n candidates with the highest val-
ues. The set of the n chosen candidates are
then labeled by human experts and added to
the existing training set. Training on the up-
dated set L, the system modifies the model
so that it is consistent with all the examples
seen thus far. The loop continues until one of
the stopping conditions is met: the model is
considered to be good enough, all candidates
are labeled, or all human resources are used
up.
</bodyText>
<subsectionHeader confidence="0.956915">
2.1 The Evaluation Function
</subsectionHeader>
<bodyText confidence="0.997709">
At the heart of the sample selection algorithm
is the evaluation function that predicts each
unlabeled candidate&apos;s training utility. Our
proposed function ranks candidates based on
the &amp;quot;performance of hypothesis.&amp;quot; In other
words, we wish to find the set of sentences
that the current parsing model is the most
uncertain about. One way to measure the
parser&apos;s uncertainty is to compute the tree
entropy over the distribution of parsing prob-
abilities of the set of trees produced by the
parser. More specifically, the tree entropy for
a sentence u, is:
</bodyText>
<equation confidence="0.9733825">
T E(u, M) = Pr(t1u,m)10g2Pr(tlu,m),
ter.
</equation>
<bodyText confidence="0.999869625">
where T is the set of possible trees that Al
generated for u. Details of computing tree
entropy have been discussed previously (Hwa,
2000b). Our proposed function evaluates each
candidate by measuring the similarity be-
tween the tree entropy of the candidate and
the uniform distribution for the same number
of trees. That is,
</bodyText>
<equation confidence="0.967029333333333">
T Feu, M)
fte(u, =
log2 ITI
</equation>
<subsectionHeader confidence="0.991421">
2.2 Parsing Models
</subsectionHeader>
<bodyText confidence="0.9999226875">
To test the robustness of the tree-entropy
evaluation function, we use it to select train-
ing examples for the Collins Parser and the
PLTIG parser. Although both are lexical-
ized and statistical parsers, their learning al-
gorithms are different. The Collins Parser is
a fully-supervised, history-based learner that
models the parameters of the parser by tak-
ing statistics directly from the training data.
In contrast, PLTIG&apos;s EM-based induction al-
gorithm (Hwa, 2000a) is partially-supervised;
the model&apos;s parameters are estimated indi-
rectly from the training data. Our goal for
this study is to determine whether the suc-
cess of the tree-entropy metric is learner de-
pendent.
</bodyText>
<sectionHeader confidence="0.989598" genericHeader="method">
3 Experimental Setup and Results
</sectionHeader>
<bodyText confidence="0.999976125">
Two experiments are performed. The first
experiment assesses whether the tree-entropy
evaluation function can select good examples
for a history-based learner. The second exper-
iment is a preliminary study on whether the
examples selected for a history-based learner
are also good training examples for a EM-
based learner.
</bodyText>
<subsectionHeader confidence="0.995855">
3.1 Experiment 1
</subsectionHeader>
<bodyText confidence="0.999974368421053">
We use the Collins Parser as the basic learning
model Al in the sample selection framework
described in Figure 1. To simulate the in-
teractive process, we create a large unlabeled
candidate pool U by stripping all annotated
information from sections 02 through 21 of the
Wall Street Journal corpus. Initially, L, the
set of labeled training data, consists of 500
parsed sentences. In each iteration, n = 1000
new sentences are picked from U to be added
to L. Then, a new parser is trained from the
updated L and tested on section 00 to chart
the learning progress.
We compare the learning rate of the parser
trained on examples selected by the tree en-
tropy evaluation function, ft, with a baseline
in which the model was trained with exam-
ples sequentially selected. The experimen-
tal results are graphed in Figure 2(a). The
</bodyText>
<figure confidence="0.998478112903226">
0 100000 200000 300000 400000 500000 600000 700000
Number of constituents in the training set
700000
baseline
tree entropy
600000
Number of constituents in the training set
100000
0
500000
400000
300000
200000
85.5 86 86.5 87 87.5 88 88.5 89
Parsing performance of the test set
90
88
86
84
n
�
Parsing performance of the test set
�
�
82
80
78
76
baseline
tree entropy
�
&apos;baseline&apos;
&apos;Tree entropy (for Model 2)&apos;
&apos;Tree entropy (for PLTIG)&apos;
�
0 5000 10000 15000 20000 25000 30000 35000 40000 45000
Number of constituents in the training set
30000
25000
20000
15000
10000
5000
0
78 78.5 79 79.5 80 80.5 81 81.5
&apos;baseline&apos;
&apos;Tree entropy (for Model 2)&apos;
&apos;Tree entropy for PLTIG&apos;
Parsing performance of the test set
82
81
80
79
Parsing performance of the test set
n
�
78
77
76
75
74
Number of constituents in the training set
</figure>
<reference confidence="0.998583590163934">
David Cohn, Les Atlas, and Richard Ladner.
1994. Improving generalization with active
learning. Machine Learning, 15(2):201-221.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the
ACL, pages 16-23, Madrid, Spain.
Sean P. Engelson and Ido Dagan. 1996. Min-
imizing manual annotation cost in supervised
training from copora. In Proceedings of the 34th
Annual Meeting of the ACL, pages 319-326.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and
Naftali Tishby. 1997. Selective sampling using
the query by committee algorithm. Machine
Learning, 28(2-3):133-168.
Atsushi Fujii, Kentaro Inui, Takenobu Tokunaga,
and Hozumi Tanaka. 1998. Selective sampling
for example-based word sense disambiguation.
Computational Linguistics, 24(4):573-598, De-
cember.
Rebecca Hwa. 2000a. Learning Probabilistic Lex-
icalized Grammars for Natural Language Pro-
cessing. Ph.D. thesis, Harvard University.
Rebecca Hwa. 2000b. Sample selection for sta-
tistical grammar induction. In Proceedings of
the 2000 Joint SIGDAT Conference on EMNLP
and V L C, pages 45-52, Hong Kong, China, Oc-
tober.
David D. Lewis and Jason Catlett. 1994. Het-
erogeneous uncertainty sampling for supervised
learning. In Proceedings of the Eleventh In-
ternational Conference on Machine Learning,
pages 148-156.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers.
In Proceedings of the 17th Annual International
ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 3-
12.
Mitchell Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: the
Penn Treebank. Computational Linguistics,
19(2):313-330.
Grace Ngai and David Yarowsky. 2000. Rule writ-
ing or annotation: Cost-efficient resource usage
for base noun phrase chunking. In Proceedings
of the 38th Annual Meeting of the ACL, pages
117-125, Hong Kong, China, October.
Fernando Pereira and Yves Schabes. 1992. Inside-
Outside reestimation from partially bracketed
corpora. In Proceedings of the 30th Annual
Meeting of the ACL, pages 128-135, Newark,
Delaware.
Cynthia A. Thompson, Mary Elaine Califf, and
Raymond J. Mooney. 1999. Active learning
for natural language parsing and information
extraction. In Proceedings of ICML-99, pages
406-414, Bled, Slovenia.
C. J. Van Rijsbergen. 1979. Information Re-
trieval. Butterworth.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.733303">
<title confidence="0.999644">On Minimizing Training Corpus for Parser Acquisition *</title>
<author confidence="0.998919">Rebecca Hwa</author>
<affiliation confidence="0.999606">Institute for Advanced Computer University of</affiliation>
<address confidence="0.999747">College Park, MD 20742</address>
<email confidence="0.9995">hwa©umiacs.umd.edu</email>
<abstract confidence="0.990453035714286">Many corpus-based natural language processing systems rely on using large quantities of annotated text as their training examples. Building this kind of resource is an expensive and labor-intensive project. To minimize effort spent on annotating examples that are not helpful the training process, recent research efforts have begun to apply active learning techniques to selectively choose data to be annotated. In this work, we consider selecting training examples with the Our goal is to assess how well this selection technique can be applied for training different types of parsers. We find that tree-entropy can significantly reduce the amount of training annotation for both a history-based parser and EM-based parser. Moreover, the examples selected for the historybased parser are also good for training the EM-based parser, suggesting that the technique is parser independent.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Cohn</author>
<author>Les Atlas</author>
<author>Richard Ladner</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>1994</date>
<booktitle>Machine Learning,</booktitle>
<pages>15--2</pages>
<contexts>
<context position="5563" citStr="Cohn et al., 1994" startWordPosition="882" endWordPosition="885">mmittee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem). The candidate examples that lead to the most disagreements U is a set of unlabeled candidates. L is a set of labeled training examples. Al is the current model. Initialize: Al Train(L). Repeat N Select(n,U,M, f). U U — N. L L U Label(N). Al Train(L). Until (Al Altrue) or (U = 0) or (human stops). Figure 1: The pseudo-code for the sample selection learning algorithm among the different learners are considered to have the highest TUV. (Cohn et al., 1994; Freund et al., 1997). For computationally intensive problems, keeping multiple learners may be impractical. In this work, we focus ort sample selection algorithms that use only a single learner that keeps just one working hypothesis. Without access to multiple hypotheses, the selection algorithm can nonetheless estimate the TUV of art example. We categorize some possible ranking criteria into the following three classes: Problem-space: Knowledge about the problem-space may help to locate good training canidates. For example, knowing the distribution of the pool, we might select the most freq</context>
</contexts>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>David Cohn, Les Atlas, and Richard Ladner. 1994. Improving generalization with active learning. Machine Learning, 15(2):201-221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL,</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="2072" citStr="Collins (1997)" startWordPosition="322" endWordPosition="323">e paper. available. These ought be valuable resources for training natural language processing system. Unfortunately, most systems cannot take advantage of the data in their raw text form; typically, the data must be annotated by a human to become effective training examples. For instance, consider the task of inducing a grammar to parse English sentences. Studies have shown that a grammar trained OIL sentences annotated with their constituent trees produces much better parses than one trained on just the sentences alone (Pereira and Schabes, 1992). Recent state-of-theart parsers developed by Collins (1997) and Charniak (1999) are all trained from handannotated corpora such as those from the Penn Treebank Project (Marcus et al., 1993). However, building an annotated corpus is a human labor-intensive project; therefore, it is important to find ways to minimize the size of the corpus. Out of a large pool of raw text, what subset should be annotated and added to the training set? Recent studies have begun to address this question using sample selection, in which training process is seen as interactive session between the learning system and the human annotator (Lewis and Gale, 1994), (Engelson and </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the ACL, pages 16-23, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean P Engelson</author>
<author>Ido Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from copora.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL,</booktitle>
<pages>319--326</pages>
<contexts>
<context position="2684" citStr="Engelson and Dagan, 1996" startWordPosition="423" endWordPosition="426">ollins (1997) and Charniak (1999) are all trained from handannotated corpora such as those from the Penn Treebank Project (Marcus et al., 1993). However, building an annotated corpus is a human labor-intensive project; therefore, it is important to find ways to minimize the size of the corpus. Out of a large pool of raw text, what subset should be annotated and added to the training set? Recent studies have begun to address this question using sample selection, in which training process is seen as interactive session between the learning system and the human annotator (Lewis and Gale, 1994), (Engelson and Dagan, 1996), (Fujii et al., 1998), (Thompson et al., 1999), and (Ngai and Yarowsky, 2000). The system actively influences its learning progress by evaluating potential candidates from the pool of raw text and selecting those with high Training Utility Values (TUV) for humans to annotate. As the learning process continues, the system should become better at identifying good training candidates so that the annotators would not need to waste time ort processing uninformative examples. This work considers the problem of applying sample selection techniques to the task of training statistical parsers. Our pri</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>Sean P. Engelson and Ido Dagan. 1996. Minimizing manual annotation cost in supervised training from copora. In Proceedings of the 34th Annual Meeting of the ACL, pages 319-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>H Sebastian Seung</author>
<author>Eli Shamir</author>
<author>Naftali Tishby</author>
</authors>
<title>Selective sampling using the query by committee algorithm.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>28--2</pages>
<contexts>
<context position="5585" citStr="Freund et al., 1997" startWordPosition="886" endWordPosition="890">tion algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem). The candidate examples that lead to the most disagreements U is a set of unlabeled candidates. L is a set of labeled training examples. Al is the current model. Initialize: Al Train(L). Repeat N Select(n,U,M, f). U U — N. L L U Label(N). Al Train(L). Until (Al Altrue) or (U = 0) or (human stops). Figure 1: The pseudo-code for the sample selection learning algorithm among the different learners are considered to have the highest TUV. (Cohn et al., 1994; Freund et al., 1997). For computationally intensive problems, keeping multiple learners may be impractical. In this work, we focus ort sample selection algorithms that use only a single learner that keeps just one working hypothesis. Without access to multiple hypotheses, the selection algorithm can nonetheless estimate the TUV of art example. We categorize some possible ranking criteria into the following three classes: Problem-space: Knowledge about the problem-space may help to locate good training canidates. For example, knowing the distribution of the pool, we might select the most frequently occuring instan</context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1997</marker>
<rawString>Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali Tishby. 1997. Selective sampling using the query by committee algorithm. Machine Learning, 28(2-3):133-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Kentaro Inui</author>
<author>Takenobu Tokunaga</author>
<author>Hozumi Tanaka</author>
</authors>
<title>Selective sampling for example-based word sense disambiguation.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--4</pages>
<contexts>
<context position="2706" citStr="Fujii et al., 1998" startWordPosition="427" endWordPosition="430">1999) are all trained from handannotated corpora such as those from the Penn Treebank Project (Marcus et al., 1993). However, building an annotated corpus is a human labor-intensive project; therefore, it is important to find ways to minimize the size of the corpus. Out of a large pool of raw text, what subset should be annotated and added to the training set? Recent studies have begun to address this question using sample selection, in which training process is seen as interactive session between the learning system and the human annotator (Lewis and Gale, 1994), (Engelson and Dagan, 1996), (Fujii et al., 1998), (Thompson et al., 1999), and (Ngai and Yarowsky, 2000). The system actively influences its learning progress by evaluating potential candidates from the pool of raw text and selecting those with high Training Utility Values (TUV) for humans to annotate. As the learning process continues, the system should become better at identifying good training candidates so that the annotators would not need to waste time ort processing uninformative examples. This work considers the problem of applying sample selection techniques to the task of training statistical parsers. Our primary challenge is in d</context>
</contexts>
<marker>Fujii, Inui, Tokunaga, Tanaka, 1998</marker>
<rawString>Atsushi Fujii, Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka. 1998. Selective sampling for example-based word sense disambiguation. Computational Linguistics, 24(4):573-598, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Learning Probabilistic Lexicalized Grammars for Natural Language Processing.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="3453" citStr="Hwa, 2000" startWordPosition="547" endWordPosition="548">didates from the pool of raw text and selecting those with high Training Utility Values (TUV) for humans to annotate. As the learning process continues, the system should become better at identifying good training candidates so that the annotators would not need to waste time ort processing uninformative examples. This work considers the problem of applying sample selection techniques to the task of training statistical parsers. Our primary challenge is in designing a function that can accurately estimate art unlabeled candidate&apos;s potential utility for training a parser. lit a previous study (Hwa, 2000b), we have applied sample selection to art induction algorithm based on the expectation-maximization (EM) principle that induces Probabilistic Lexicalized Tree Insertion Grammars (PLTIGs). In that work, we proposed art uncertainty-based evaluation function to estimate the TUV of unlabeled candidates called tree entropy. We have empirically shown that sample selection with tree entropy can reduce the size of the training corpus significantly. However, because only art EM-based learner was used, it is unknown whether the evaluation function would be general enough to be applicable to other type</context>
<context position="8238" citStr="Hwa, 2000" startWordPosition="1325" endWordPosition="1326">ing utility. Our proposed function ranks candidates based on the &amp;quot;performance of hypothesis.&amp;quot; In other words, we wish to find the set of sentences that the current parsing model is the most uncertain about. One way to measure the parser&apos;s uncertainty is to compute the tree entropy over the distribution of parsing probabilities of the set of trees produced by the parser. More specifically, the tree entropy for a sentence u, is: T E(u, M) = Pr(t1u,m)10g2Pr(tlu,m), ter. where T is the set of possible trees that Al generated for u. Details of computing tree entropy have been discussed previously (Hwa, 2000b). Our proposed function evaluates each candidate by measuring the similarity between the tree entropy of the candidate and the uniform distribution for the same number of trees. That is, T Feu, M) fte(u, = log2 ITI 2.2 Parsing Models To test the robustness of the tree-entropy evaluation function, we use it to select training examples for the Collins Parser and the PLTIG parser. Although both are lexicalized and statistical parsers, their learning algorithms are different. The Collins Parser is a fully-supervised, history-based learner that models the parameters of the parser by taking statis</context>
</contexts>
<marker>Hwa, 2000</marker>
<rawString>Rebecca Hwa. 2000a. Learning Probabilistic Lexicalized Grammars for Natural Language Processing. Ph.D. thesis, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample selection for statistical grammar induction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT Conference on EMNLP and V L C,</booktitle>
<pages>45--52</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="3453" citStr="Hwa, 2000" startWordPosition="547" endWordPosition="548">didates from the pool of raw text and selecting those with high Training Utility Values (TUV) for humans to annotate. As the learning process continues, the system should become better at identifying good training candidates so that the annotators would not need to waste time ort processing uninformative examples. This work considers the problem of applying sample selection techniques to the task of training statistical parsers. Our primary challenge is in designing a function that can accurately estimate art unlabeled candidate&apos;s potential utility for training a parser. lit a previous study (Hwa, 2000b), we have applied sample selection to art induction algorithm based on the expectation-maximization (EM) principle that induces Probabilistic Lexicalized Tree Insertion Grammars (PLTIGs). In that work, we proposed art uncertainty-based evaluation function to estimate the TUV of unlabeled candidates called tree entropy. We have empirically shown that sample selection with tree entropy can reduce the size of the training corpus significantly. However, because only art EM-based learner was used, it is unknown whether the evaluation function would be general enough to be applicable to other type</context>
<context position="8238" citStr="Hwa, 2000" startWordPosition="1325" endWordPosition="1326">ing utility. Our proposed function ranks candidates based on the &amp;quot;performance of hypothesis.&amp;quot; In other words, we wish to find the set of sentences that the current parsing model is the most uncertain about. One way to measure the parser&apos;s uncertainty is to compute the tree entropy over the distribution of parsing probabilities of the set of trees produced by the parser. More specifically, the tree entropy for a sentence u, is: T E(u, M) = Pr(t1u,m)10g2Pr(tlu,m), ter. where T is the set of possible trees that Al generated for u. Details of computing tree entropy have been discussed previously (Hwa, 2000b). Our proposed function evaluates each candidate by measuring the similarity between the tree entropy of the candidate and the uniform distribution for the same number of trees. That is, T Feu, M) fte(u, = log2 ITI 2.2 Parsing Models To test the robustness of the tree-entropy evaluation function, we use it to select training examples for the Collins Parser and the PLTIG parser. Although both are lexicalized and statistical parsers, their learning algorithms are different. The Collins Parser is a fully-supervised, history-based learner that models the parameters of the parser by taking statis</context>
</contexts>
<marker>Hwa, 2000</marker>
<rawString>Rebecca Hwa. 2000b. Sample selection for statistical grammar induction. In Proceedings of the 2000 Joint SIGDAT Conference on EMNLP and V L C, pages 45-52, Hong Kong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Jason Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>In Proceedings of the Eleventh International Conference on Machine Learning,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="6363" citStr="Lewis and Catlett, 1994" startWordPosition="1005" endWordPosition="1008">ly a single learner that keeps just one working hypothesis. Without access to multiple hypotheses, the selection algorithm can nonetheless estimate the TUV of art example. We categorize some possible ranking criteria into the following three classes: Problem-space: Knowledge about the problem-space may help to locate good training canidates. For example, knowing the distribution of the pool, we might select the most frequently occuring instances. Performance of the hypothesis: Testing the candidates ort the current hypothesis may show the type of data ort which the hypothesis performs weakly (Lewis and Catlett, 1994). Parameters of the hypothesis: Estimating the potential impact of the candidates will have ort the parameters of the current working hypothesis locates those examples that will change the current hypothesis the most. Figure 1 outlines the single-learner sample selection training loop in pseudo-code. Initially, the training set, L, consists of a small number of labeled examples. The learner uses L to train an initial model Al. Also available to the learner is a large pool of unlabeled training candidates, U. In each iteration, the selection algorithm, S elect(n,U, Al, f), uses art evaluation f</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>David D. Lewis and Jason Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Proceedings of the Eleventh International Conference on Machine Learning, pages 148-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>3--12</pages>
<contexts>
<context position="2656" citStr="Lewis and Gale, 1994" startWordPosition="419" endWordPosition="422">t parsers developed by Collins (1997) and Charniak (1999) are all trained from handannotated corpora such as those from the Penn Treebank Project (Marcus et al., 1993). However, building an annotated corpus is a human labor-intensive project; therefore, it is important to find ways to minimize the size of the corpus. Out of a large pool of raw text, what subset should be annotated and added to the training set? Recent studies have begun to address this question using sample selection, in which training process is seen as interactive session between the learning system and the human annotator (Lewis and Gale, 1994), (Engelson and Dagan, 1996), (Fujii et al., 1998), (Thompson et al., 1999), and (Ngai and Yarowsky, 2000). The system actively influences its learning progress by evaluating potential candidates from the pool of raw text and selecting those with high Training Utility Values (TUV) for humans to annotate. As the learning process continues, the system should become better at identifying good training candidates so that the annotators would not need to waste time ort processing uninformative examples. This work considers the problem of applying sample selection techniques to the task of training </context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and William A. Gale. 1994. A sequential algorithm for training text classifiers. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="2202" citStr="Marcus et al., 1993" startWordPosition="342" endWordPosition="345">ystems cannot take advantage of the data in their raw text form; typically, the data must be annotated by a human to become effective training examples. For instance, consider the task of inducing a grammar to parse English sentences. Studies have shown that a grammar trained OIL sentences annotated with their constituent trees produces much better parses than one trained on just the sentences alone (Pereira and Schabes, 1992). Recent state-of-theart parsers developed by Collins (1997) and Charniak (1999) are all trained from handannotated corpora such as those from the Penn Treebank Project (Marcus et al., 1993). However, building an annotated corpus is a human labor-intensive project; therefore, it is important to find ways to minimize the size of the corpus. Out of a large pool of raw text, what subset should be annotated and added to the training set? Recent studies have begun to address this question using sample selection, in which training process is seen as interactive session between the learning system and the human annotator (Lewis and Gale, 1994), (Engelson and Dagan, 1996), (Fujii et al., 1998), (Thompson et al., 1999), and (Ngai and Yarowsky, 2000). The system actively influences its lea</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>David Yarowsky</author>
</authors>
<title>Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the ACL,</booktitle>
<pages>117--125</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="2762" citStr="Ngai and Yarowsky, 2000" startWordPosition="436" endWordPosition="439">ch as those from the Penn Treebank Project (Marcus et al., 1993). However, building an annotated corpus is a human labor-intensive project; therefore, it is important to find ways to minimize the size of the corpus. Out of a large pool of raw text, what subset should be annotated and added to the training set? Recent studies have begun to address this question using sample selection, in which training process is seen as interactive session between the learning system and the human annotator (Lewis and Gale, 1994), (Engelson and Dagan, 1996), (Fujii et al., 1998), (Thompson et al., 1999), and (Ngai and Yarowsky, 2000). The system actively influences its learning progress by evaluating potential candidates from the pool of raw text and selecting those with high Training Utility Values (TUV) for humans to annotate. As the learning process continues, the system should become better at identifying good training candidates so that the annotators would not need to waste time ort processing uninformative examples. This work considers the problem of applying sample selection techniques to the task of training statistical parsers. Our primary challenge is in designing a function that can accurately estimate art unl</context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>Grace Ngai and David Yarowsky. 2000. Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking. In Proceedings of the 38th Annual Meeting of the ACL, pages 117-125, Hong Kong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>InsideOutside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the ACL,</booktitle>
<pages>128--135</pages>
<location>Newark, Delaware.</location>
<contexts>
<context position="2012" citStr="Pereira and Schabes, 1992" startWordPosition="312" endWordPosition="315">ser; and Ric Crabbe and the anonymous reviewers for their comments on the paper. available. These ought be valuable resources for training natural language processing system. Unfortunately, most systems cannot take advantage of the data in their raw text form; typically, the data must be annotated by a human to become effective training examples. For instance, consider the task of inducing a grammar to parse English sentences. Studies have shown that a grammar trained OIL sentences annotated with their constituent trees produces much better parses than one trained on just the sentences alone (Pereira and Schabes, 1992). Recent state-of-theart parsers developed by Collins (1997) and Charniak (1999) are all trained from handannotated corpora such as those from the Penn Treebank Project (Marcus et al., 1993). However, building an annotated corpus is a human labor-intensive project; therefore, it is important to find ways to minimize the size of the corpus. Out of a large pool of raw text, what subset should be annotated and added to the training set? Recent studies have begun to address this question using sample selection, in which training process is seen as interactive session between the learning system an</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. InsideOutside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the ACL, pages 128-135, Newark, Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Mary Elaine Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Active learning for natural language parsing and information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of ICML-99,</booktitle>
<pages>406--414</pages>
<location>Bled, Slovenia.</location>
<contexts>
<context position="2731" citStr="Thompson et al., 1999" startWordPosition="431" endWordPosition="434">from handannotated corpora such as those from the Penn Treebank Project (Marcus et al., 1993). However, building an annotated corpus is a human labor-intensive project; therefore, it is important to find ways to minimize the size of the corpus. Out of a large pool of raw text, what subset should be annotated and added to the training set? Recent studies have begun to address this question using sample selection, in which training process is seen as interactive session between the learning system and the human annotator (Lewis and Gale, 1994), (Engelson and Dagan, 1996), (Fujii et al., 1998), (Thompson et al., 1999), and (Ngai and Yarowsky, 2000). The system actively influences its learning progress by evaluating potential candidates from the pool of raw text and selecting those with high Training Utility Values (TUV) for humans to annotate. As the learning process continues, the system should become better at identifying good training candidates so that the annotators would not need to waste time ort processing uninformative examples. This work considers the problem of applying sample selection techniques to the task of training statistical parsers. Our primary challenge is in designing a function that </context>
</contexts>
<marker>Thompson, Califf, Mooney, 1999</marker>
<rawString>Cynthia A. Thompson, Mary Elaine Califf, and Raymond J. Mooney. 1999. Active learning for natural language parsing and information extraction. In Proceedings of ICML-99, pages 406-414, Bled, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<location>Butterworth.</location>
<marker>Van Rijsbergen, 1979</marker>
<rawString>C. J. Van Rijsbergen. 1979. Information Retrieval. Butterworth.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>