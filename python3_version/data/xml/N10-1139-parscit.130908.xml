<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.991603">
Expected Sequence Similarity Maximization
</title>
<author confidence="0.985564">
Cyril Allauzen1, Shankar Kumar1, Wolfgang Macherey1, Mehryar Mohri2,1 and Michael Riley1
</author>
<affiliation confidence="0.9547235">
1Google Research, 76 Ninth Avenue, New York, NY 10011
2Courant Institute of Mathematical Sciences, 251 Mercer Street, New York, NY 10012
</affiliation>
<sectionHeader confidence="0.989137" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999807318181818">
This paper presents efficient algorithms for
expected similarity maximization, which co-
incides with minimum Bayes decoding for a
similarity-based loss function. Our algorithms
are designed for similarity functions that are
sequence kernels in a general class of posi-
tive definite symmetric kernels. We discuss
both a general algorithm and a more efficient
algorithm applicable in a common unambigu-
ous scenario. We also describe the applica-
tion of our algorithms to machine translation
and report the results of experiments with sev-
eral translation data sets which demonstrate a
substantial speed-up. In particular, our results
show a speed-up by two orders of magnitude
with respect to the original method of Tromble
et al. (2008) and by a factor of 3 or more
even with respect to an approximate algorithm
specifically designed for that task. These re-
sults open the path for the exploration of more
appropriate or optimal kernels for the specific
tasks considered.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999882982758621">
The output of many complex natural language pro-
cessing systems such as information extraction,
speech recognition, or machine translation systems
is a probabilistic automaton. Exploiting the full in-
formation provided by this probabilistic automaton
can lead to more accurate results than just using the
one-best sequence.
Different techniques have been explored in the
past to take advantage of the full lattice, some based
on the use of a more complex model applied to
the automaton as in rescoring, others using addi-
tional data or information for reranking the hypothe-
ses represented by the automaton. One method for
using these probabilistic automata that has been suc-
cessful in large-vocabulary speech recognition (Goel
and Byrne, 2000) and machine translation (Kumar
and Byrne, 2004; Tromble et al., 2008) applications
and that requires no additional data or other com-
plex models is the minimum Bayes risk (MBR) de-
coding technique. This returns that sequence of the
automaton having the minimum expected loss with
respect to all sequences accepted by the automaton
(Bickel and Doksum, 2001). Often, minimizing the
loss function L can be equivalently viewed as max-
imizing a similarity function K between sequences,
which corresponds to a kernel function when it is
positive definite symmetric (Berg et al., 1984). The
technique can then be thought of as an expected se-
quence similarity maximization.
This paper considers this expected similarity max-
imization view. Since different similarity functions
can be used within this framework, one may wish to
select the one that is the most appropriate or relevant
to the task considered. However, a crucial require-
ment for this choice to be realistic is to ensure that
for the family of similarity functions considered the
expected similarity maximization is efficiently com-
putable. Thus, we primarily focus on this algorith-
mic problem in this paper, leaving it to future work
to study the question of determining how to select
the similarity function and report on the benefits of
this choice.
A general family of sequence kernels including
the sequence kernels used in computational biology,
text categorization, spoken-dialog classification, and
many other tasks is that of rational kernels (Cortes
et al., 2004). We show how the expected similarity
maximization can be efficiently computed for these
kernels. In section 3, we describe more specifically
the framework of expected similarity maximization
in the case of rational kernels and the correspond-
ing algorithmic problem. In Section 4, we describe
both a general method for the computation of the ex-
pected similarity maximization, and a more efficient
method that can be used with a broad sub-family
of rational kernels that verify a condition of non-
ambiguity. This latter family includes the class of
n-gram kernels which have been previously used to
</bodyText>
<page confidence="0.953303">
957
</page>
<note confidence="0.7537475">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 957–965,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998958">
apply MBR to machine translation (Tromble et al.,
2008). We examine in more detail the use and ap-
plication of our algorithms to machine translation
in Section 5. Section 6 reports the results of ex-
periments applying our algorithms in several large
data sets in machine translation. These experiments
demonstrate the efficiency of our algorithm which
is shown empirically to be two orders of magnitude
faster than Tromble et al. (2008) and more than 3
times faster than even an approximation algorithm
specifically designed for this problem (Kumar et al.,
2009). We start with some preliminary definitions
and algorithms related to weighted automata and
transducers, following the definitions and terminol-
ogy of Cortes et al. (2004).
</bodyText>
<sectionHeader confidence="0.998059" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.999959212121212">
Weighted transducers are finite-state transducers in
which each transition carries some weight in addi-
tion to the input and output labels. The weight set
has the structure of a semiring.
A semiring (K, ®, ®, 0,1) verifies all the axioms
of a ring except from the existence of a negative el-
ement −x for each x E K, which it may verify or
not. Thus, roughly speaking, a semiring is a ring
that may lack negation. It is specified by a set of
values K, two binary operations ® and ®, and two
designated values 0 and 1. When ® is commutative,
the semiring is said to be commutative.
The real semiring (R+, +, x, 0,1) is used when
the weights represent probabilities. The log
semiring (R U {−oo, +oo}, ®log, +, oo, 0) is iso-
morphic to the real semiring via the negative-
log mapping and is often used in practice
for numerical stability. The tropical semiring
(RU, {−oo, +oo}, min, +, oo, 0) is derived from
the log semiring via the Viterbi approximation and
is often used in shortest-path applications.
Figure 1(a) shows an example of a weighted
finite-state transducer over the real semiring
(R+, +, x, 0, 1). In this figure, the input and out-
put labels of a transition are separated by a colon
delimiter and the weight is indicated after the slash
separator. A weighted transducer has a set of initial
states represented in the figure by a bold circle and
a set of final states, represented by double circles. A
path from an initial state to a final state is an accept-
ing path.
The weight of an accepting path is obtained by
first ®-multiplying the weights of its constituent
</bodyText>
<figureCaption confidence="0.9375804">
Figure 1: (a) Example of weighted transducer T over the
real semiring (R+, +, x, 0, 1). (b) Example of weighted
automaton A. A can be obtained from T by projection on
the output and T (aab, bba) = A(bba) = 1 x 2 x 6 x 8 +
2 x 4 x 5 x 8.
</figureCaption>
<bodyText confidence="0.994738470588235">
transitions and ®-multiplying this product on the left
by the weight of the initial state of the path (which
equals 1 in our work) and on the right by the weight
of the final state of the path (displayed after the slash
in the figure). The weight associated by a weighted
transducer T to a pair of strings (x, y) E E* x E* is
denoted by T (x, y) and is obtained by ®-summing
the weights of all accepting paths with input label x
and output label y.
For any transducer T, T−1 denotes its inverse,
that is the transducer obtained from T by swapping
the input and output labels of each transition. For all
x, y E E*, we have T−1(x, y) = T (y, x).
The composition of two weighted transducers T1
and T2 with matching input and output alphabets E,
is a weighted transducer denoted by T1 o T2 when
the semiring is commutative and the sum:
</bodyText>
<equation confidence="0.994654">
(T1 o T2)(x, y) = � T1(x, z) ® T2(z, y) (1)
ZEE∗
</equation>
<bodyText confidence="0.999015375">
is well-defined and in K for all x, y (Salomaa and
Soittola, 1978).
Weighted automata can be defined as weighted
transducers A with identical input and output labels,
for any transition. Since only pairs of the form (x, x)
can have a non-zero weight associated to them by
A, we denote the weight associated by A to (x, x)
by A(x) and call it the weight associated by A to
x. Similarly, in the graph representation of weighted
automata, the output (or input) label is omitted. Fig-
ure 1(b) shows an example of a weighted automa-
ton. When A and B are weighted automata, A o B
is called the intersection of A and B. Omitting the
input labels of a weighted transducer T results in a
weighted automaton which is said to be the output
projection of T.
</bodyText>
<figure confidence="0.99836152631579">
(a) (b)
3/8
a/5
a/6
1
b/4
2/1
a/3
0 b/1
b/2
3/8
b:a/5
b:a/6
1
a:b/4
2/1
b:a/3
0 a:b/1
a:b/2
</figure>
<page confidence="0.993601">
958
</page>
<sectionHeader confidence="0.998843" genericHeader="method">
3 General Framework
</sectionHeader>
<bodyText confidence="0.988060833333333">
Let X be a probabilistic automaton representing the
output of a complex model for a specific query input.
The model may be for example a speech recognition
system, an information extraction system, or a ma-
chine translation system (which originally motivated
our study). For machine translation, the sequences
accepted by X are the potential translations of the
input sentence, each with some probability given by
X.
Let E be the alphabet for the task considered, e.g.,
words of the target language in machine translation,
and let L: E* x E* —* R denote a loss function
defined over the sequences on that alphabet. Given
a reference or hypothesis set H C_ E*, minimum
Bayes risk (MBR) decoding consists of selecting a
hypothesis x E H with minimum expected loss with
respect to the probability distribution X (Bickel and
Doksum, 2001; Tromble et al., 2008):
</bodyText>
<equation confidence="0.997895">
E [L(x, x′)]. (2)
x′�X
</equation>
<bodyText confidence="0.99947575">
Here, we shall consider the case, frequent in prac-
tice, where minimizing the loss L is equivalent to
maximizing a similarity measure K : E* xE* —* R.
When K is a sequence kernel that can be repre-
sented by weighted transducers, it is a rational ker-
nel (Cortes et al., 2004). The problem is then equiv-
alent to the following expected similarity maximiza-
tion:
</bodyText>
<equation confidence="0.942707166666667">
E [K(x, x′)]. (3)
x′�X
When K is a positive definite symmetric rational
kernel, it can often be rewritten as K(x, y) = (T o
T−1)(x, y), where T is a weighted transducer over
the semiring (R+U{+oo}, +, x, 0, 1). Equation (3)
</equation>
<bodyText confidence="0.982776333333333">
can then be rewritten as
can compute a composition based on an automa-
ton accepting all sequences in H, A(H). This leads
to a straightforward method for determining the se-
quence maximizing the expected similarity having
the following steps:
</bodyText>
<listItem confidence="0.9597872">
1. compute the composition X o T, project on
the output and optimize (epsilon-remove, de-
terminize, minimize (Mohri, 2009)) and let Y2
be the result;1
2. compute the composition Y1 = A(H) o T;
3. compute Y1 o Y2 and project on the input, let Z
be the result;2
4. determinize Z;
5. find the maximum weight path with the label of
that path giving 5.
</listItem>
<bodyText confidence="0.999892636363636">
While this method can be efficient in various scenar-
ios, in some instances the weighted determinization
yielding Z can be both space- and time-consuming,
even though the input is acyclic. The next two sec-
tions describe more efficient algorithms.
Note that in practice, for numerical stability, all
of these computations are done in the log semiring
which is isomorphic to (R+ U{+oo}, +, x, 0, 1). In
particular, the maximum weight path in the last step
is then obtained by using a standard single-source
shortest-path algorithm.
</bodyText>
<subsectionHeader confidence="0.988225">
4.2 Efficient method for n-gram kernels
</subsectionHeader>
<bodyText confidence="0.999047833333333">
A common family of rational kernels is the family
of n-gram kernels. These kernels are widely use as
a similarity measure in natural language processing
and computational biology applications, see (Leslie
et al., 2002; Lodhi et al., 2002) for instance.
The n-gram kernel Kn of order n is defined as
</bodyText>
<equation confidence="0.9884631">
5 = argmin
xEH
x = argmax
xEH
x = argmax
EX[(T o T−1)(x, x′)] (4)
xEH
= argmax I IA(x) o T o T−1 o XI I, (5) � cx(z)cy(z), (6)
xEH Kn(x, y) =
|z|=n
</equation>
<bodyText confidence="0.999933666666667">
where we denote by A(x) an automaton accepting
(only) the string x and by I I·I I the sum of the weights
of all accepted paths of a transducer.
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="method">
4 Algorithms
</sectionHeader>
<subsectionHeader confidence="0.99435">
4.1 General method
</subsectionHeader>
<bodyText confidence="0.993487625">
Equation (5) could suggest computing A(x) o T o
T−1 o X for each possible x E H. Instead, we
where cx(z) is the number of occurrences of z in
x. Kn is a positive definite symmetric rational ker-
nel since it corresponds to the weighted transducer
Tn o T −1
n where the transducer Tn is defined such
that Tn(x, z) = cx(z) for all x, z E E* with JzJ = n.
</bodyText>
<footnote confidence="0.997878333333333">
1Equivalent to computing T−1 o X and projecting on the
input.
2Z is then the projection on the input of A(H)oT oT −1oX.
</footnote>
<page confidence="0.993818">
959
</page>
<figure confidence="0.999789279569893">
(a) (b)
7
a
b
8
b
(c) (d)
0
b/0
a/0
2
1
b/1.5
b/0.5
4
3
a/1.8
a/1.8
5
6
b/1.5
b/0.5
a/0.2
7
8
b/1.5
a/1.8
9/0
b
a
b
1
3
5
9
a
a
0
b
b
a/0
a
2
4
6
a/0.2
b/0.5
b/1.5
a/0
a/1.8
b/0
b/0
ε
a/0.2
1
b/1.5
a/1
2
b/0.5
0 b/1 a/1.8 3/1
b:e
b:e
a:e
a:e
0
a:a
b:b
1
a:a
b:b
2
0
b/0.5
a/0.5
2
1
b/1
b/1
4
3
a/1
a/1
5
6
b/0.6
a/0.4
b/1
7
8
b/1
a/1
9/1
(e) (f)
</figure>
<figureCaption confidence="0.68070675">
Figure 2: Efficient method for bigram kernel: (a) Counting transducer T2 for E = {a, b} (over the real semiring). (b)
Probabilistic automaton X (over the real semiring). (c) The hypothesis automaton A(H) (unweighted). (d) Automaton
Y2 representing the expected bigram counts in X (over the real semiring). (e) Automaton Y1: the context dependency
model derived from Y2 (over the tropical semiring). (f) The composition A(H) ◦ Y1 (over the tropical semiring).
</figureCaption>
<bodyText confidence="0.9786665">
nal; the set of transitions E contains all 4-tuple
(origin, label, weight, destination) of the form:
</bodyText>
<listItem confidence="0.99668725">
• (w, a, 0, wa) with wa ∈ Q and |w |≤ n −
2 and
• (aw, b, Y2(awb), wb) with Y2(awb) =6 0
and |w |= n − 2
</listItem>
<bodyText confidence="0.823914">
where a, b ∈ E and w ∈ E*. Observe that
</bodyText>
<equation confidence="0.8267985">
w ∈ Q when wa ∈ Q and that aw, wb ∈ Q
when Y2(awb) =6 0. Given a string x, we have
�Y1(x) = cX(w)cx(w). (8)
|w|=n
</equation>
<bodyText confidence="0.982297">
Observe that Y1 is a deterministic automaton,
hence Y1(x) can be computed in O(|x|) time.
</bodyText>
<listItem confidence="0.781081">
3. Compute x: We compute the composition
</listItem>
<bodyText confidence="0.991619666666667">
A(H) ◦ Y1. x is then the label of the accepting
path with the largest weight in this transducer
and can be obtained by applying a shortest-path
algorithm to −A(H) ◦ Y1 in the tropical semir-
ing.
The main computational advantage of this method
is that it avoids the determinization of Z in the
The transducer T2 for E = {a, b} is shown in Fig-
ure 2(a).
Taking advantage of the special structure of n-
gram kernels and of the fact that A(H) is an un-
weighted automaton, we can devise a new and sig-
nificantly more efficient method for computing x
based on the following steps.
1. Compute the expected n-gram counts in X: We
compute the composition X ◦T, project on out-
put and optimize (epsilon-remove, determinize,
minimize) and let Y2 be the result. Observe that
the weighted automaton Y2 is a compact repre-
sentation of the expected n-gram counts in X,
i.e. for an n-gram w (i.e. |w |= n):
</bodyText>
<equation confidence="0.9843345">
�Y2(w) = X(x)cx(w)
xEE∗ (7)
= � �cx(w)� = cX(w).
x�X
</equation>
<footnote confidence="0.6147574">
2. Construct a context-dependency model: We
compute the weighted automaton Y1 over the
tropical semiring as follow: the set of states is
Q = {w ∈ E* ||w |≤ n and w occurs in X},
the initial state being ǫ and every state being fi-
</footnote>
<figure confidence="0.999235621621622">
b/0
b
b
a
3/c2
b/0
3
b/0
ε /c2
3’/0
a
0 1
b
a
2/c1
0
a
b
1
b
a
a
0
a/0
1
a/0
a/0
a/1 a/c1
0 1 2/1
b/c2
3/c2
2/c1
2
b/0 a/0
ε /c1
2’/0
(a) (b) (c) (d)
</figure>
<figureCaption confidence="0.997080333333333">
Figure 3: Illustration of the construction of Y1 in the unambiguous case. (a) Weighted automaton Y2 (over the real
semiring). (b) Deterministic tree automaton Y2′ accepting {aa, ab} (over the tropical semiring). (c) Result of deter-
minization of E∗Y2′ (over the tropical semiring). (d) Weighted automaton Y1 (over the tropical semiring).
</figureCaption>
<bodyText confidence="0.9990874">
(+, x) semiring, which can sometimes be costly.
The method has also been shown empirically to be
significantly faster than the one described in the pre-
vious section.
The algorithm is illustrated in Figure 2. The al-
phabet is E = {a, b} and the counting transducer
corresponding to the bigram kernel is given in Fig-
ure 2(a). The evidence probabilistic automaton X
is given in Figure 2(b) and we use as hypothesis
set the set of strings that were assigned a non-zero
probability by X; this set is represented by the deter-
ministic finite automaton A(H) given in Figure 2(c).
The result of step 1 of the algorithm is the weighted
automaton Y2 over the real semiring given in Fig-
ure 2(d). The result of step 2 is the weighted au-
tomaton Y1 over the tropical semiring is given in
Figure 2(e). Finally, the result of the composition
A(H) o Y1 (step 3) is the weighted automaton over
the tropical semiring given in Figure 2(f). The re-
sult of the expected similarity maximization is the
string x = ababa, which is obtained by applying
a shortest-path algorithm to −A(H) o Y1. Observe
that the string x with the largest probability in X is
x = bbaba and is hence different from x = ababa in
this example.
</bodyText>
<subsectionHeader confidence="0.994894">
4.3 Efficient method for the unambiguous case
</subsectionHeader>
<bodyText confidence="0.9995437">
The algorithm presented in the previous section for
n-gram kernels can be generalized to handle a wide
variety of rational kernels.
Let K be an arbitrary rational kernel defined by a
weighted transducer T. Let XT denote the regular
language of the strings output by T. We shall as-
sume that XT is a finite language, though the results
of this section generalize to the infinite case. Let
E denote a new alphabet defined by E = {#x: x E
XT} and consider the simple grammar G of context-
</bodyText>
<equation confidence="0.836532">
dependent batch rules:
E → #x/x E. (9)
</equation>
<bodyText confidence="0.993464828571429">
Each such rule inserts the symbol #x immediately
after an occurrence x in the input string. For batch
context-dependent rules, the context of the applica-
tion for all rules is determined at once before their
application (Kaplan and Kay, 1994). Assume that
this grammar is unambiguous for a parallel applica-
tion of the rules. This condition means that there is
a unique way of parsing an input string using the
strings of XT. The assumption holds for n-gram
sequences, for example, since the rules applicable
are uniquely determined by the n-grams (making the
previous section a special case).
Given an acyclic weighted automaton Y2 over the
tropical semiring accepting a subset of XT, we can
construct a deterministic weighted automaton Y1 for
E∗L(Y2) when this grammar is unambiguous. The
weight assigned by Y1 to an input string is then the
sum of the weights of the substrings accepted by Y2.
This can be achieved using weighted determiniza-
tion.
This suggests a new method for generalizing Step
2 of the algorithm described in the previous section
as follows (see illustration in Figure 3):
(i) use Y2 to construct a deterministic weighted
tree Y2′ defined on the tropical semiring ac-
cepting the same strings as Y2 with the same
weights, with the final weights equal to the to-
tal weight given by Y2 to the string ending at
that leaf;
(ii) let Y1 be the weighted automaton obtained by
first adding self-loops labeled with all elements
of E at the initial state of Y2′ and then deter-
minizing it, and then inserting new transitions
leaving final states as described in (Mohri and
Sproat, 1996).
</bodyText>
<page confidence="0.995681">
961
</page>
<bodyText confidence="0.99996525">
Step (ii) consists of computing a deterministic
weighted automaton for E∗Y′�. This step corre-
sponds to the Aho-Corasick construction (Aho and
Corasick, 1975) and can be done in time linear in
the size of Y′�.
This approach assumes that the grammar G of
batch context-dependent rules inferred by XT is un-
ambiguous. This can be tested by constructing the
finite automaton corresponding to all rules in G. The
grammar G is unambiguous iff the resulting automa-
ton is unambiguous (which can be tested using a
classical algorithm). An alternative and more ef-
ficient test consists of checking the presence of a
failure or default transition to a final state during
the Aho-Corasick construction, which occurs if and
only if there is ambiguity.
</bodyText>
<sectionHeader confidence="0.968068" genericHeader="method">
5 Application to Machine Translation
</sectionHeader>
<bodyText confidence="0.999303222222222">
In machine translation, the BLEU score (Papineni et
al., 2001) is typically used as an evaluation metric.
In (Tromble et al., 2008), a Minimum Bayes-Risk
decoding approach for MT lattices was introduced.3
The loss function used in that approach was an ap-
proximation of the log-BLEU score by a linear func-
tion of n-gram matches and candidate length. This
loss function corresponds to the following similarity
measure:
</bodyText>
<equation confidence="0.830866">
XKLB(x, x′) = 00|x′ |+ 0|w|cx(w)1x′(w).
|w|≤n
(10)
</equation>
<bodyText confidence="0.994177444444445">
where 1x(w) is 1 if w occurs in x and 0 otherwise.
(Tromble et al., 2008) implements the MBR de-
coder using weighted automata operations. First,
the set of n-grams is extracted from the lat-
tice. Next, the posterior probability p(w|X) of
each n-gram is computed. Starting with the un-
weighted lattice A(H), the contribution of each n-
gram w to (10) is applied by iteratively compos-
ing with the weighted automaton corresponding to
w(w/(0|w|p(w|X))w)∗ where w = E∗ \ (E∗wE∗).
Finally, the MBR hypothesis is extracted as the best
path in the automaton. The above steps are carried
out one n-gram at a time. For a moderately large lat-
tice, there can be several thousands of n-grams and
the procedure becomes expensive. This leads us to
investigate methods that do not require processing
the n-grams one at a time in order to achieve greater
efficiency.
</bodyText>
<footnote confidence="0.939618">
3Related approaches were presented in (DeNero et al., 2009;
Kumar et al., 2009; Li et al., 2009).
</footnote>
<figureCaption confidence="0.94855">
Figure 4: Transducer T1 over the real semiring for the
alphabet {a, b}.
</figureCaption>
<bodyText confidence="0.9950468">
The first idea is to approximate the KLB similar-
ity measure using a weighted sum of n-gram ker-
nels. This corresponds to approximating 1x′(w) by
cx′(w) in (10). This leads us to the following simi-
larity measure:
</bodyText>
<equation confidence="0.996263666666667">
XKNG(x, x′) = 00|x′ |+ 0|w|cx(w)cx′(w)
|w|≤n
= 00|x′ |+ X 0iKi(x, x′)
</equation>
<bodyText confidence="0.90986325">
1≤i≤n
Intuitively, the larger the length of w the less likely
it is that cx(w) =� 1x(w), which suggests comput-
ing the contribution to KLB(x, x′) of lower-order
n-grams (|w |G k) exactly, but using the approxima-
tion by n-gram kernels for the higher-order n-grams
(|w |&gt; k). This gives the following similarity mea-
sure:
</bodyText>
<equation confidence="0.93057">
XKk NG(x, x′) = 00|x′ |+ 0|w|cx(w)1x′(w)
1≤|w|≤k
+ X 0|w|cx(w)cx′(w)
k&lt;|w|≤n
Observe that K0NG = KNG and KnNG = KLB.
</equation>
<bodyText confidence="0.99823">
All these similarity measures can still be com-
puted using the framework described in Section 4.
Indeed, there exists a transducer Tn over the real
semiring such that Tn(x, z) = 1x(z) for all x E E∗
and z E En. The transducer T1 for E = {a, b} is
given by Figure 4. Let us define the similarity mea-
sure Kn as:
</bodyText>
<equation confidence="0.995987">
Kn(x, x′) = (TnoT −1
n )(x, x′) = X cx(w)1x′(w).
</equation>
<bodyText confidence="0.994261">
|w|=n
Observe that the framework described in Section 4
can still be applied even though Kn is not symmet-
ric. The similarity measures KLB, KNG and KkNG
</bodyText>
<figure confidence="0.992810818181818">
b:ε
a:ε
ε:ε
1
a:a
0
ε:ε
a:ε b:b
3
2
b:ε
</figure>
<page confidence="0.982411">
962
</page>
<table confidence="0.998980285714286">
zhen aren
nist02 nist04 nist05 nist06 nist08 nist02 nist04 nist05 nist06 nist08
no mbr 38.7 39.2 38.3 33.5 26.5 64.0 51.8 57.3 45.5 43.8
exact 37.0 39.2 38.6 34.3 27.5 65.2 51.4 58.1 45.2 45.0
approx 39.0 39.9 38.6 34.4 27.4 65.2 52.5 58.1 46.2 45.0
ngram 36.6 39.1 38.1 34.4 27.7 64.3 50.1 56.7 44.1 42.8
ngram1 37.1 39.2 38.5 34.4 27.5 65.2 51.4 58.0 45.2 44.8
</table>
<tableCaption confidence="0.987791">
Table 1: BLEU score (%)
</tableCaption>
<table confidence="0.999247666666667">
zhen aren
nist02 nist04 nist05 nist06 nist08 nist02 nist04 nist05 nist06 nist08
exact 3560 7863 5553 6313 5738 12341 23266 11152 11417 11405
approx 168 422 279 335 328 504 1296 528 619 808
ngram 28 72 34 70 43 85 368 105 63 66
ngram1 58 175 96 99 89 368 943 308 167 191
</table>
<tableCaption confidence="0.999159">
Table 2: MBR Time (in seconds)
</tableCaption>
<bodyText confidence="0.9938195">
can then be expressed as the relevant linear combi-
nation of Ki and Ki.
</bodyText>
<sectionHeader confidence="0.995482" genericHeader="method">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.9998163">
Lattices were generated using a phrase-based MT
system similar to the alignment template system de-
scribed in (Och and Ney, 2004). Given a source sen-
tence, the system produces a word lattice A that is a
compact representation of a very large N-best list of
translation hypotheses for that source sentence and
their likelihoods. The lattice A is converted into a
lattice X that represents a probability distribution
(i.e. the posterior probability distribution given the
source sentence) following:
</bodyText>
<equation confidence="0.995819">
exp(αA(x))
X(x) = Ey∈E∗ exp(αA(y)) (14)
</equation>
<bodyText confidence="0.998133428571429">
where the scaling factor α E [0, oc) flattens the dis-
tribution when α &lt; 1 and sharpens it when α &gt; 1.
We then applied the methods described in Section 5
to the lattice X using as hypothesis set H the un-
weighted lattice obtained from X.
The following parameters for the n-gram factors
were used:
</bodyText>
<equation confidence="0.997548">
−1 and θn = 1 for n ≥ 1. (15)
T 4Tprn−1
</equation>
<bodyText confidence="0.998797">
Experiments were conducted on two language
pairs Arabic-English (aren) and Chinese-English
(zhen) and for a variety of datasets from the NIST
Open Machine Translation (OpenMT) Evaluation.4
The values of α, p and r used for each pair are given
</bodyText>
<footnote confidence="0.954556">
4http://www.nist.gov/speech/tests/mt
</footnote>
<table confidence="0.998096333333333">
α p r
aren 0.2 0.85 0.72
zhen 0.1 0.80 0.62
</table>
<tableCaption confidence="0.999956">
Table 3: Parameters used for performing MBR.
</tableCaption>
<bodyText confidence="0.99888125">
in Table 3. We used the IBM implementation of the
BLEU score (Papineni et al., 2001).
We implemented the following methods using the
OpenFst library (Allauzen et al., 2007):
</bodyText>
<listItem confidence="0.997133888888889">
• exact: uses the similarity measure KLB based
on the linearized log-BLEU, implemented as
described in (Tromble et al., 2008);
• approx: uses the approximation to KLB from
(Kumar et al., 2009) and described in the ap-
pendix;
• ngram: uses the similarity measure KNG im-
plemented using the algorithm of Section 4.2;
• ngram1: uses the similarity measure K1NG
</listItem>
<bodyText confidence="0.886922333333333">
also implemented using the algorithm of Sec-
tion 4.2.
The results from Tables 1-2 show that ngram1
performs as well as exact on all datasets5 while be-
ing two orders of magnitude faster than exact and
overall more than 3 times faster than approx.
</bodyText>
<sectionHeader confidence="0.999286" genericHeader="method">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9957315">
We showed that for broad families of transducers
T and thus rational kernels, the expected similar-
</bodyText>
<footnote confidence="0.807102666666667">
5We consider BLEU score differences of less than 0.4% not
significant (Koehn, 2004).
θ0 =
</footnote>
<page confidence="0.995615">
963
</page>
<bodyText confidence="0.999986125">
ity maximization problem can be solved efficiently.
This opens up the option of seeking the most appro-
priate rational kernel or transducer T for the spe-
cific task considered. In particular, the kernel K
used in our machine translation applications might
not be optimal. One may well imagine for exam-
ple that some n-grams should be further emphasized
and others de-emphasized in the definition of the
similarity. This can be easily accommodated in the
framework of rational kernels by modifying the tran-
sition weights of T. But, ideally, one would wish
to select those weights in an optimal fashion. As
mentioned earlier, we leave this question to future
work. However, we can offer a brief look at how
one could tackle this question. One method for de-
termining an optimal kernel for the expected sim-
ilarity maximization problem consists of solving a
problem similar to that of learning kernels in classi-
fication or regression. Let X1, ... , Xm be m lattices
with Ref(X1), ... , Ref(Xm) the associated refer-
ences and let bx(K, Xi) be the solution of the ex-
pected similarity maximization for lattice Xi when
using kernel K. Then, the kernel learning optimiza-
tion problem can be formulated as follows:
</bodyText>
<equation confidence="0.5448575">
L(bx(K, Xi), Ref(Xi))
s. t. K = T o T−1 ∧ Tr[K] G C,
</equation>
<bodyText confidence="0.999519428571429">
where K is a convex family of rational kernels and
Tr[K] denotes the trace of the kernel matrix. In
particular, we could choose K as a family of linear
combinations of base rational kernels. Techniques
and ideas similar to those discussed by Cortes et al.
(2008) for learning sequence kernels could be di-
rectly relevant to this problem.
</bodyText>
<sectionHeader confidence="0.676297" genericHeader="conclusions">
A Appendix
</sectionHeader>
<bodyText confidence="0.999811833333333">
We describe here the approximation of the KLB
similarity measure from Kumar et al. (2009). We
assume in this section that the lattice X is determin-
istic in order to simplify the notations. The posterior
probability of n-gram w in the lattice X can be for-
mulated as:
</bodyText>
<equation confidence="0.9785645">
p(w|X) = 1x(w)P(x|s) =
X X 1x(w)X(x)
xEE* xEE*
(16)
</equation>
<bodyText confidence="0.998801">
where s denotes the source sentence. When using
the similarity measure KLB defined Equation (10),
</bodyText>
<equation confidence="0.786851">
Equation (3) can then be reformulated as:
θjwjcx′(w)p(w|X). (17)
</equation>
<bodyText confidence="0.943890346153846">
The key idea behind this new approximation algo-
rithm is to rewrite the n-gram posterior probability
(Equation 16) as follows:
f(e, w, πx)X(x) (18)
where EX is the set of transitions of X, πx is
the unique accepting path labeled by x in X and
f(e, w, π) is a score assigned to transition e on path
π containing n-gram w:
1 if w E e, p(e|X) &gt; p(e′|X),
and e′ precedes e on π
0 otherwise.
(19)
In other words, for each path π, we count the tran-
sition that contributes n-gram w and has the highest
transition posterior probability relative to its prede-
cessors on the path π; there is exactly one such tran-
sition on each lattice path π.
We note that f(e, w, π) relies on the full path π
which means that it cannot be computed based on
local statistics. We therefore approximate the quan-
tity f(e, w, π) with f*(e, w, X) that counts the tran-
sition e with n-gram w that has the highest arc poste-
rior probability relative to predecessors in the entire
lattice X. f*(e, w, X) can be computed locally, and
the n-gram posterior probability based on f* can be
determined as follows:
</bodyText>
<equation confidence="0.853705714285714">
X
p(w|9) =
xEE*
X= X1wEef*(e, w, X) 1πx(e)X(x)
eEEx xEE*
X= 1wEef*(e, w, X)P(e|X),
eEEx
</equation>
<bodyText confidence="0.9931715">
(20)
where P(e|X) is the posterior probability of a lat-
tice transition e E EX. The algorithm to perform
Lattice MBR is given in Algorithm 1. For each state
t in the lattice, we maintain a quantity Score(w, t)
for each n-gram w that lies on a path from the initial
state to t. Score(w, t) is the highest posterior prob-
ability among all transitions on the paths that termi-
nate on t and contain n-gram w. The forward pass
requires computing the n-grams introduced by each
transition; to do this, we propagate n-grams (up to
maximum order −1) terminating on each state.
</bodyText>
<figure confidence="0.995377695652174">
1
m
Xm
i=1
min
KE1C
b
x = argmax
x′EH
X
θ0|x′ |+
w
X
p(w|X) =
xEE*
X
eEEx
f(e, w, π) =



X f*(e, w, X)X(x)
eEEx
</figure>
<page confidence="0.950089">
964
</page>
<reference confidence="0.9869472">
Algorithm 1 MBR Decoding on Lattices
1: Sort the lattice states topologically.
2: Compute backward probabilities of each state.
3: Compute posterior prob. of each n-gram:
4: for each transition e do
5: Compute transition posterior probability P(e|X).
6: Compute n-gram posterior probs. P(w|X):
7: for each n-gram w introduced by e do
8: Propagate n − 1 gram suffix to he.
9: if p(e|X) &gt; Score(w,T(e)) then
10: Update posterior probs. and scores:
p(w|X) += p(e|X) − Score(w, T(e)).
Score(w, he) = p(e|X).
11: else
12: Score(w, he) = Score(w, T(e)).
13: end if
14: end for
15: end for
16: Assign scores to transitions (given by Equation 17).
17: Find best path in the lattice (Equation 17).
</reference>
<sectionHeader confidence="0.95945" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999732082191781">
Alfred V. Aho and Margaret J. Corasick. 1975. Efficient
String Matching: An Aid to Bibliographic Search.
Communications of the ACM, 18(6):333–340.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: a
general and efficient weighted finite-state transducer
library. In CIAA 2007, volume 4783 of LNCS, pages
11–23. Springer. http://www.openfst.org.
Christian Berg, Jens Peter Reus Christensen, and Paul
Ressel. 1984. Harmonic Analysis on Semigroups.
Springer-Verlag: Berlin-New York.
Peter J. Bickel and Kjell A. Doksum. 2001. Mathemati-
cal Statistics, vol. I. Prentice Hall.
Corinna Cortes, Patrick Haffner, and Mehryar Mohri.
2004. Rational Kernels: Theory and Algorithms.
Journal ofMachine Learning Research, 5:1035–1062.
Corinna Cortes, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2008. Learning sequence kernels. In Pro-
ceedings ofMLSP 2008, October.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings ofACL and IJCNLP, pages 567–575.
Vaibhava Goel and William J. Byrne. 2000. Minimum
Bayes-risk automatic speech recognition. Computer
Speech and Language, 14(2):115–135.
Ronald M. Kaplan and Martin Kay. 1994. Regular mod-
els of phonological rule systems. Computational Lin-
guistics, 20(3).
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In EMNLP,
Barcelona, Spain.
Shankar Kumar and William J. Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL, Boston, MA, USA.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Asso-
ciation for Computational Linguistics and IJCNLP.
Christina S. Leslie, Eleazar Eskin, and William Stafford
Noble. 2002. The Spectrum Kernel: A String Kernel
for SVM Protein Classification. In Pacific Symposium
on Biocomputing, pages 566–575.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proceedings ofACL and IJCNLP, pages 593–
601.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watskins. 2002. Text classifica-
tion using string kernels. Journal ofMachine Learning
Research, 2:419–44.
Mehryar Mohri and Richard Sproat. 1996. An Efficient
Compiler for Weighted Rewrite Rules. In Proceedings
ofACL ’96, Santa Cruz, California.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Manfred Droste, Werner Kuich, and Heiko Vogler,
editors, Handbook of Weighted Automata, chapter 6,
pages 213–254. Springer.
Franz J. Och and Hermann Ney. 2004. The align-
ment template approach to statistical mchine transla-
tion. Computational Linguistics, 30(4):417–449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Report
RC22176 (W0109-022), IBM Research Division.
Arto Salomaa and Matti Soittola. 1978. Automata-
Theoretic Aspects ofFormal Power Series. Springer.
Roy W. Tromble, Shankar Kumar, Franz J. Och, and
Wolfgang Macherey. 2008. Lattice minimum Bayes-
risk decoding for statistical machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 620–
629.
</reference>
<page confidence="0.998702">
965
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.292470">
<title confidence="0.997752">Expected Sequence Similarity Maximization</title>
<author confidence="0.941735">Shankar Wolfgang Mehryar</author>
<author confidence="0.941735">Michael</author>
<affiliation confidence="0.4639645">Research, 76 Ninth Avenue, New York, NY 10011 Institute of Mathematical Sciences, 251 Mercer Street, New York, NY 10012</affiliation>
<abstract confidence="0.996566869565217">This paper presents efficient algorithms for expected similarity maximization, which coincides with minimum Bayes decoding for a similarity-based loss function. Our algorithms are designed for similarity functions that are sequence kernels in a general class of positive definite symmetric kernels. We discuss both a general algorithm and a more efficient algorithm applicable in a common unambiguous scenario. We also describe the application of our algorithms to machine translation and report the results of experiments with several translation data sets which demonstrate a substantial speed-up. In particular, our results show a speed-up by two orders of magnitude with respect to the original method of Tromble et al. (2008) and by a factor of 3 or more even with respect to an approximate algorithm specifically designed for that task. These results open the path for the exploration of more appropriate or optimal kernels for the specific tasks considered.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Algorithm 1 MBR Decoding on Lattices 1: Sort the lattice states topologically. 2: Compute backward probabilities of each state. 3: Compute posterior prob. of each n-gram: 4: for each transition e do 5: Compute transition posterior probability P(e|X). 6: Compute n-gram posterior probs. P(w|X): 7: for each n-gram w introduced by e do 8: Propagate n − 1 gram suffix to he. 9: if p(e|X)</title>
<journal>Score(w,T(e)) then</journal>
<volume>11</volume>
<marker></marker>
<rawString>Algorithm 1 MBR Decoding on Lattices 1: Sort the lattice states topologically. 2: Compute backward probabilities of each state. 3: Compute posterior prob. of each n-gram: 4: for each transition e do 5: Compute transition posterior probability P(e|X). 6: Compute n-gram posterior probs. P(w|X): 7: for each n-gram w introduced by e do 8: Propagate n − 1 gram suffix to he. 9: if p(e|X) &gt; Score(w,T(e)) then 10: Update posterior probs. and scores: p(w|X) += p(e|X) − Score(w, T(e)). Score(w, he) = p(e|X). 11: else 12: Score(w, he) = Score(w, T(e)). 13: end if 14: end for 15: end for 16: Assign scores to transitions (given by Equation 17). 17: Find best path in the lattice (Equation 17).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Margaret J Corasick</author>
</authors>
<title>Efficient String Matching: An Aid to Bibliographic Search.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="18795" citStr="Aho and Corasick, 1975" startWordPosition="3352" endWordPosition="3355"> weighted tree Y2′ defined on the tropical semiring accepting the same strings as Y2 with the same weights, with the final weights equal to the total weight given by Y2 to the string ending at that leaf; (ii) let Y1 be the weighted automaton obtained by first adding self-loops labeled with all elements of E at the initial state of Y2′ and then determinizing it, and then inserting new transitions leaving final states as described in (Mohri and Sproat, 1996). 961 Step (ii) consists of computing a deterministic weighted automaton for E∗Y′�. This step corresponds to the Aho-Corasick construction (Aho and Corasick, 1975) and can be done in time linear in the size of Y′�. This approach assumes that the grammar G of batch context-dependent rules inferred by XT is unambiguous. This can be tested by constructing the finite automaton corresponding to all rules in G. The grammar G is unambiguous iff the resulting automaton is unambiguous (which can be tested using a classical algorithm). An alternative and more efficient test consists of checking the presence of a failure or default transition to a final state during the Aho-Corasick construction, which occurs if and only if there is ambiguity. 5 Application to Mac</context>
</contexts>
<marker>Aho, Corasick, 1975</marker>
<rawString>Alfred V. Aho and Margaret J. Corasick. 1975. Efficient String Matching: An Aid to Bibliographic Search. Communications of the ACM, 18(6):333–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: a general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In CIAA 2007,</booktitle>
<volume>4783</volume>
<pages>11--23</pages>
<publisher>Springer. http://www.openfst.org.</publisher>
<contexts>
<context position="24360" citStr="Allauzen et al., 2007" startWordPosition="4339" endWordPosition="4342">meters for the n-gram factors were used: −1 and θn = 1 for n ≥ 1. (15) T 4Tprn−1 Experiments were conducted on two language pairs Arabic-English (aren) and Chinese-English (zhen) and for a variety of datasets from the NIST Open Machine Translation (OpenMT) Evaluation.4 The values of α, p and r used for each pair are given 4http://www.nist.gov/speech/tests/mt α p r aren 0.2 0.85 0.72 zhen 0.1 0.80 0.62 Table 3: Parameters used for performing MBR. in Table 3. We used the IBM implementation of the BLEU score (Papineni et al., 2001). We implemented the following methods using the OpenFst library (Allauzen et al., 2007): • exact: uses the similarity measure KLB based on the linearized log-BLEU, implemented as described in (Tromble et al., 2008); • approx: uses the approximation to KLB from (Kumar et al., 2009) and described in the appendix; • ngram: uses the similarity measure KNG implemented using the algorithm of Section 4.2; • ngram1: uses the similarity measure K1NG also implemented using the algorithm of Section 4.2. The results from Tables 1-2 show that ngram1 performs as well as exact on all datasets5 while being two orders of magnitude faster than exact and overall more than 3 times faster than appro</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: a general and efficient weighted finite-state transducer library. In CIAA 2007, volume 4783 of LNCS, pages 11–23. Springer. http://www.openfst.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Berg</author>
<author>Jens Peter Reus Christensen</author>
<author>Paul Ressel</author>
</authors>
<title>Harmonic Analysis on Semigroups. Springer-Verlag:</title>
<date>1984</date>
<location>Berlin-New York.</location>
<contexts>
<context position="2573" citStr="Berg et al., 1984" startWordPosition="393" endWordPosition="396"> speech recognition (Goel and Byrne, 2000) and machine translation (Kumar and Byrne, 2004; Tromble et al., 2008) applications and that requires no additional data or other complex models is the minimum Bayes risk (MBR) decoding technique. This returns that sequence of the automaton having the minimum expected loss with respect to all sequences accepted by the automaton (Bickel and Doksum, 2001). Often, minimizing the loss function L can be equivalently viewed as maximizing a similarity function K between sequences, which corresponds to a kernel function when it is positive definite symmetric (Berg et al., 1984). The technique can then be thought of as an expected sequence similarity maximization. This paper considers this expected similarity maximization view. Since different similarity functions can be used within this framework, one may wish to select the one that is the most appropriate or relevant to the task considered. However, a crucial requirement for this choice to be realistic is to ensure that for the family of similarity functions considered the expected similarity maximization is efficiently computable. Thus, we primarily focus on this algorithmic problem in this paper, leaving it to fu</context>
</contexts>
<marker>Berg, Christensen, Ressel, 1984</marker>
<rawString>Christian Berg, Jens Peter Reus Christensen, and Paul Ressel. 1984. Harmonic Analysis on Semigroups. Springer-Verlag: Berlin-New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter J Bickel</author>
<author>Kjell A Doksum</author>
</authors>
<date>2001</date>
<journal>Mathematical Statistics,</journal>
<volume>vol.</volume>
<publisher>I. Prentice Hall.</publisher>
<contexts>
<context position="2352" citStr="Bickel and Doksum, 2001" startWordPosition="358" endWordPosition="361">he automaton as in rescoring, others using additional data or information for reranking the hypotheses represented by the automaton. One method for using these probabilistic automata that has been successful in large-vocabulary speech recognition (Goel and Byrne, 2000) and machine translation (Kumar and Byrne, 2004; Tromble et al., 2008) applications and that requires no additional data or other complex models is the minimum Bayes risk (MBR) decoding technique. This returns that sequence of the automaton having the minimum expected loss with respect to all sequences accepted by the automaton (Bickel and Doksum, 2001). Often, minimizing the loss function L can be equivalently viewed as maximizing a similarity function K between sequences, which corresponds to a kernel function when it is positive definite symmetric (Berg et al., 1984). The technique can then be thought of as an expected sequence similarity maximization. This paper considers this expected similarity maximization view. Since different similarity functions can be used within this framework, one may wish to select the one that is the most appropriate or relevant to the task considered. However, a crucial requirement for this choice to be reali</context>
<context position="9450" citStr="Bickel and Doksum, 2001" startWordPosition="1604" endWordPosition="1607">e translation system (which originally motivated our study). For machine translation, the sequences accepted by X are the potential translations of the input sentence, each with some probability given by X. Let E be the alphabet for the task considered, e.g., words of the target language in machine translation, and let L: E* x E* —* R denote a loss function defined over the sequences on that alphabet. Given a reference or hypothesis set H C_ E*, minimum Bayes risk (MBR) decoding consists of selecting a hypothesis x E H with minimum expected loss with respect to the probability distribution X (Bickel and Doksum, 2001; Tromble et al., 2008): E [L(x, x′)]. (2) x′�X Here, we shall consider the case, frequent in practice, where minimizing the loss L is equivalent to maximizing a similarity measure K : E* xE* —* R. When K is a sequence kernel that can be represented by weighted transducers, it is a rational kernel (Cortes et al., 2004). The problem is then equivalent to the following expected similarity maximization: E [K(x, x′)]. (3) x′�X When K is a positive definite symmetric rational kernel, it can often be rewritten as K(x, y) = (T o T−1)(x, y), where T is a weighted transducer over the semiring (R+U{+oo}</context>
</contexts>
<marker>Bickel, Doksum, 2001</marker>
<rawString>Peter J. Bickel and Kjell A. Doksum. 2001. Mathematical Statistics, vol. I. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Patrick Haffner</author>
<author>Mehryar Mohri</author>
</authors>
<title>Rational Kernels: Theory and Algorithms.</title>
<date>2004</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>5--1035</pages>
<contexts>
<context position="3521" citStr="Cortes et al., 2004" startWordPosition="542" endWordPosition="545">, a crucial requirement for this choice to be realistic is to ensure that for the family of similarity functions considered the expected similarity maximization is efficiently computable. Thus, we primarily focus on this algorithmic problem in this paper, leaving it to future work to study the question of determining how to select the similarity function and report on the benefits of this choice. A general family of sequence kernels including the sequence kernels used in computational biology, text categorization, spoken-dialog classification, and many other tasks is that of rational kernels (Cortes et al., 2004). We show how the expected similarity maximization can be efficiently computed for these kernels. In section 3, we describe more specifically the framework of expected similarity maximization in the case of rational kernels and the corresponding algorithmic problem. In Section 4, we describe both a general method for the computation of the expected similarity maximization, and a more efficient method that can be used with a broad sub-family of rational kernels that verify a condition of nonambiguity. This latter family includes the class of n-gram kernels which have been previously used to 957</context>
<context position="5051" citStr="Cortes et al. (2004)" startWordPosition="781" endWordPosition="784">ur algorithms to machine translation in Section 5. Section 6 reports the results of experiments applying our algorithms in several large data sets in machine translation. These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al. (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al., 2009). We start with some preliminary definitions and algorithms related to weighted automata and transducers, following the definitions and terminology of Cortes et al. (2004). 2 Preliminaries Weighted transducers are finite-state transducers in which each transition carries some weight in addition to the input and output labels. The weight set has the structure of a semiring. A semiring (K, ®, ®, 0,1) verifies all the axioms of a ring except from the existence of a negative element −x for each x E K, which it may verify or not. Thus, roughly speaking, a semiring is a ring that may lack negation. It is specified by a set of values K, two binary operations ® and ®, and two designated values 0 and 1. When ® is commutative, the semiring is said to be commutative. The </context>
<context position="9770" citStr="Cortes et al., 2004" startWordPosition="1665" endWordPosition="1668"> E* x E* —* R denote a loss function defined over the sequences on that alphabet. Given a reference or hypothesis set H C_ E*, minimum Bayes risk (MBR) decoding consists of selecting a hypothesis x E H with minimum expected loss with respect to the probability distribution X (Bickel and Doksum, 2001; Tromble et al., 2008): E [L(x, x′)]. (2) x′�X Here, we shall consider the case, frequent in practice, where minimizing the loss L is equivalent to maximizing a similarity measure K : E* xE* —* R. When K is a sequence kernel that can be represented by weighted transducers, it is a rational kernel (Cortes et al., 2004). The problem is then equivalent to the following expected similarity maximization: E [K(x, x′)]. (3) x′�X When K is a positive definite symmetric rational kernel, it can often be rewritten as K(x, y) = (T o T−1)(x, y), where T is a weighted transducer over the semiring (R+U{+oo}, +, x, 0, 1). Equation (3) can then be rewritten as can compute a composition based on an automaton accepting all sequences in H, A(H). This leads to a straightforward method for determining the sequence maximizing the expected similarity having the following steps: 1. compute the composition X o T, project on the out</context>
</contexts>
<marker>Cortes, Haffner, Mohri, 2004</marker>
<rawString>Corinna Cortes, Patrick Haffner, and Mehryar Mohri. 2004. Rational Kernels: Theory and Algorithms. Journal ofMachine Learning Research, 5:1035–1062.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Mehryar Mohri</author>
<author>Afshin Rostamizadeh</author>
</authors>
<title>Learning sequence kernels.</title>
<date>2008</date>
<booktitle>In Proceedings ofMLSP</booktitle>
<contexts>
<context position="26682" citStr="Cortes et al. (2008)" startWordPosition="4740" endWordPosition="4743">in classification or regression. Let X1, ... , Xm be m lattices with Ref(X1), ... , Ref(Xm) the associated references and let bx(K, Xi) be the solution of the expected similarity maximization for lattice Xi when using kernel K. Then, the kernel learning optimization problem can be formulated as follows: L(bx(K, Xi), Ref(Xi)) s. t. K = T o T−1 ∧ Tr[K] G C, where K is a convex family of rational kernels and Tr[K] denotes the trace of the kernel matrix. In particular, we could choose K as a family of linear combinations of base rational kernels. Techniques and ideas similar to those discussed by Cortes et al. (2008) for learning sequence kernels could be directly relevant to this problem. A Appendix We describe here the approximation of the KLB similarity measure from Kumar et al. (2009). We assume in this section that the lattice X is deterministic in order to simplify the notations. The posterior probability of n-gram w in the lattice X can be formulated as: p(w|X) = 1x(w)P(x|s) = X X 1x(w)X(x) xEE* xEE* (16) where s denotes the source sentence. When using the similarity measure KLB defined Equation (10), Equation (3) can then be reformulated as: θjwjcx′(w)p(w|X). (17) The key idea behind this new appr</context>
</contexts>
<marker>Cortes, Mohri, Rostamizadeh, 2008</marker>
<rawString>Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. 2008. Learning sequence kernels. In Proceedings ofMLSP 2008, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>David Chiang</author>
<author>Kevin Knight</author>
</authors>
<title>Fast consensus decoding over translation forests.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL and IJCNLP,</booktitle>
<pages>567--575</pages>
<contexts>
<context position="20783" citStr="DeNero et al., 2009" startWordPosition="3686" endWordPosition="3689">ghted lattice A(H), the contribution of each ngram w to (10) is applied by iteratively composing with the weighted automaton corresponding to w(w/(0|w|p(w|X))w)∗ where w = E∗ \ (E∗wE∗). Finally, the MBR hypothesis is extracted as the best path in the automaton. The above steps are carried out one n-gram at a time. For a moderately large lattice, there can be several thousands of n-grams and the procedure becomes expensive. This leads us to investigate methods that do not require processing the n-grams one at a time in order to achieve greater efficiency. 3Related approaches were presented in (DeNero et al., 2009; Kumar et al., 2009; Li et al., 2009). Figure 4: Transducer T1 over the real semiring for the alphabet {a, b}. The first idea is to approximate the KLB similarity measure using a weighted sum of n-gram kernels. This corresponds to approximating 1x′(w) by cx′(w) in (10). This leads us to the following similarity measure: XKNG(x, x′) = 00|x′ |+ 0|w|cx(w)cx′(w) |w|≤n = 00|x′ |+ X 0iKi(x, x′) 1≤i≤n Intuitively, the larger the length of w the less likely it is that cx(w) =� 1x(w), which suggests computing the contribution to KLB(x, x′) of lower-order n-grams (|w |G k) exactly, but using the approx</context>
</contexts>
<marker>DeNero, Chiang, Knight, 2009</marker>
<rawString>John DeNero, David Chiang, and Kevin Knight. 2009. Fast consensus decoding over translation forests. In Proceedings ofACL and IJCNLP, pages 567–575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vaibhava Goel</author>
<author>William J Byrne</author>
</authors>
<title>Minimum Bayes-risk automatic speech recognition.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="1997" citStr="Goel and Byrne, 2000" startWordPosition="301" endWordPosition="304">achine translation systems is a probabilistic automaton. Exploiting the full information provided by this probabilistic automaton can lead to more accurate results than just using the one-best sequence. Different techniques have been explored in the past to take advantage of the full lattice, some based on the use of a more complex model applied to the automaton as in rescoring, others using additional data or information for reranking the hypotheses represented by the automaton. One method for using these probabilistic automata that has been successful in large-vocabulary speech recognition (Goel and Byrne, 2000) and machine translation (Kumar and Byrne, 2004; Tromble et al., 2008) applications and that requires no additional data or other complex models is the minimum Bayes risk (MBR) decoding technique. This returns that sequence of the automaton having the minimum expected loss with respect to all sequences accepted by the automaton (Bickel and Doksum, 2001). Often, minimizing the loss function L can be equivalently viewed as maximizing a similarity function K between sequences, which corresponds to a kernel function when it is positive definite symmetric (Berg et al., 1984). The technique can then</context>
</contexts>
<marker>Goel, Byrne, 2000</marker>
<rawString>Vaibhava Goel and William J. Byrne. 2000. Minimum Bayes-risk automatic speech recognition. Computer Speech and Language, 14(2):115–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Martin Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="17281" citStr="Kaplan and Kay, 1994" startWordPosition="3096" endWordPosition="3099">n arbitrary rational kernel defined by a weighted transducer T. Let XT denote the regular language of the strings output by T. We shall assume that XT is a finite language, though the results of this section generalize to the infinite case. Let E denote a new alphabet defined by E = {#x: x E XT} and consider the simple grammar G of contextdependent batch rules: E → #x/x E. (9) Each such rule inserts the symbol #x immediately after an occurrence x in the input string. For batch context-dependent rules, the context of the application for all rules is determined at once before their application (Kaplan and Kay, 1994). Assume that this grammar is unambiguous for a parallel application of the rules. This condition means that there is a unique way of parsing an input string using the strings of XT. The assumption holds for n-gram sequences, for example, since the rules applicable are uniquely determined by the n-grams (making the previous section a special case). Given an acyclic weighted automaton Y2 over the tropical semiring accepting a subset of XT, we can construct a deterministic weighted automaton Y1 for E∗L(Y2) when this grammar is unambiguous. The weight assigned by Y1 to an input string is then the</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Ronald M. Kaplan and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation. In EMNLP,</title>
<date>2004</date>
<location>Barcelona,</location>
<contexts>
<context position="25157" citStr="Koehn, 2004" startWordPosition="4476" endWordPosition="4477">l., 2009) and described in the appendix; • ngram: uses the similarity measure KNG implemented using the algorithm of Section 4.2; • ngram1: uses the similarity measure K1NG also implemented using the algorithm of Section 4.2. The results from Tables 1-2 show that ngram1 performs as well as exact on all datasets5 while being two orders of magnitude faster than exact and overall more than 3 times faster than approx. 7 Conclusion We showed that for broad families of transducers T and thus rational kernels, the expected similar5We consider BLEU score differences of less than 0.4% not significant (Koehn, 2004). θ0 = 963 ity maximization problem can be solved efficiently. This opens up the option of seeking the most appropriate rational kernel or transducer T for the specific task considered. In particular, the kernel K used in our machine translation applications might not be optimal. One may well imagine for example that some n-grams should be further emphasized and others de-emphasized in the definition of the similarity. This can be easily accommodated in the framework of rational kernels by modifying the transition weights of T. But, ideally, one would wish to select those weights in an optimal</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In EMNLP, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William J Byrne</author>
</authors>
<title>Minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<location>Boston, MA, USA.</location>
<contexts>
<context position="2044" citStr="Kumar and Byrne, 2004" startWordPosition="308" endWordPosition="311">utomaton. Exploiting the full information provided by this probabilistic automaton can lead to more accurate results than just using the one-best sequence. Different techniques have been explored in the past to take advantage of the full lattice, some based on the use of a more complex model applied to the automaton as in rescoring, others using additional data or information for reranking the hypotheses represented by the automaton. One method for using these probabilistic automata that has been successful in large-vocabulary speech recognition (Goel and Byrne, 2000) and machine translation (Kumar and Byrne, 2004; Tromble et al., 2008) applications and that requires no additional data or other complex models is the minimum Bayes risk (MBR) decoding technique. This returns that sequence of the automaton having the minimum expected loss with respect to all sequences accepted by the automaton (Bickel and Doksum, 2001). Often, minimizing the loss function L can be equivalently viewed as maximizing a similarity function K between sequences, which corresponds to a kernel function when it is positive definite symmetric (Berg et al., 1984). The technique can then be thought of as an expected sequence similari</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William J. Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In HLT-NAACL, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics and IJCNLP.</booktitle>
<contexts>
<context position="4880" citStr="Kumar et al., 2009" startWordPosition="756" endWordPosition="759">une 2010. c�2010 Association for Computational Linguistics apply MBR to machine translation (Tromble et al., 2008). We examine in more detail the use and application of our algorithms to machine translation in Section 5. Section 6 reports the results of experiments applying our algorithms in several large data sets in machine translation. These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al. (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al., 2009). We start with some preliminary definitions and algorithms related to weighted automata and transducers, following the definitions and terminology of Cortes et al. (2004). 2 Preliminaries Weighted transducers are finite-state transducers in which each transition carries some weight in addition to the input and output labels. The weight set has the structure of a semiring. A semiring (K, ®, ®, 0,1) verifies all the axioms of a ring except from the existence of a negative element −x for each x E K, which it may verify or not. Thus, roughly speaking, a semiring is a ring that may lack negation. </context>
<context position="20803" citStr="Kumar et al., 2009" startWordPosition="3690" endWordPosition="3693">he contribution of each ngram w to (10) is applied by iteratively composing with the weighted automaton corresponding to w(w/(0|w|p(w|X))w)∗ where w = E∗ \ (E∗wE∗). Finally, the MBR hypothesis is extracted as the best path in the automaton. The above steps are carried out one n-gram at a time. For a moderately large lattice, there can be several thousands of n-grams and the procedure becomes expensive. This leads us to investigate methods that do not require processing the n-grams one at a time in order to achieve greater efficiency. 3Related approaches were presented in (DeNero et al., 2009; Kumar et al., 2009; Li et al., 2009). Figure 4: Transducer T1 over the real semiring for the alphabet {a, b}. The first idea is to approximate the KLB similarity measure using a weighted sum of n-gram kernels. This corresponds to approximating 1x′(w) by cx′(w) in (10). This leads us to the following similarity measure: XKNG(x, x′) = 00|x′ |+ 0|w|cx(w)cx′(w) |w|≤n = 00|x′ |+ X 0iKi(x, x′) 1≤i≤n Intuitively, the larger the length of w the less likely it is that cx(w) =� 1x(w), which suggests computing the contribution to KLB(x, x′) of lower-order n-grams (|w |G k) exactly, but using the approximation by n-gram ke</context>
<context position="24554" citStr="Kumar et al., 2009" startWordPosition="4371" endWordPosition="4374">of datasets from the NIST Open Machine Translation (OpenMT) Evaluation.4 The values of α, p and r used for each pair are given 4http://www.nist.gov/speech/tests/mt α p r aren 0.2 0.85 0.72 zhen 0.1 0.80 0.62 Table 3: Parameters used for performing MBR. in Table 3. We used the IBM implementation of the BLEU score (Papineni et al., 2001). We implemented the following methods using the OpenFst library (Allauzen et al., 2007): • exact: uses the similarity measure KLB based on the linearized log-BLEU, implemented as described in (Tromble et al., 2008); • approx: uses the approximation to KLB from (Kumar et al., 2009) and described in the appendix; • ngram: uses the similarity measure KNG implemented using the algorithm of Section 4.2; • ngram1: uses the similarity measure K1NG also implemented using the algorithm of Section 4.2. The results from Tables 1-2 show that ngram1 performs as well as exact on all datasets5 while being two orders of magnitude faster than exact and overall more than 3 times faster than approx. 7 Conclusion We showed that for broad families of transducers T and thus rational kernels, the expected similar5We consider BLEU score differences of less than 0.4% not significant (Koehn, 20</context>
<context position="26857" citStr="Kumar et al. (2009)" startWordPosition="4769" endWordPosition="4772">ty maximization for lattice Xi when using kernel K. Then, the kernel learning optimization problem can be formulated as follows: L(bx(K, Xi), Ref(Xi)) s. t. K = T o T−1 ∧ Tr[K] G C, where K is a convex family of rational kernels and Tr[K] denotes the trace of the kernel matrix. In particular, we could choose K as a family of linear combinations of base rational kernels. Techniques and ideas similar to those discussed by Cortes et al. (2008) for learning sequence kernels could be directly relevant to this problem. A Appendix We describe here the approximation of the KLB similarity measure from Kumar et al. (2009). We assume in this section that the lattice X is deterministic in order to simplify the notations. The posterior probability of n-gram w in the lattice X can be formulated as: p(w|X) = 1x(w)P(x|s) = X X 1x(w)X(x) xEE* xEE* (16) where s denotes the source sentence. When using the similarity measure KLB defined Equation (10), Equation (3) can then be reformulated as: θjwjcx′(w)p(w|X). (17) The key idea behind this new approximation algorithm is to rewrite the n-gram posterior probability (Equation 16) as follows: f(e, w, πx)X(x) (18) where EX is the set of transitions of X, πx is the unique acc</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proceedings of the Association for Computational Linguistics and IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina S Leslie</author>
<author>Eleazar Eskin</author>
<author>William Stafford Noble</author>
</authors>
<title>The Spectrum Kernel: A String Kernel for SVM Protein Classification.</title>
<date>2002</date>
<booktitle>In Pacific Symposium on Biocomputing,</booktitle>
<pages>566--575</pages>
<contexts>
<context position="11451" citStr="Leslie et al., 2002" startWordPosition="1952" endWordPosition="1955">he input is acyclic. The next two sections describe more efficient algorithms. Note that in practice, for numerical stability, all of these computations are done in the log semiring which is isomorphic to (R+ U{+oo}, +, x, 0, 1). In particular, the maximum weight path in the last step is then obtained by using a standard single-source shortest-path algorithm. 4.2 Efficient method for n-gram kernels A common family of rational kernels is the family of n-gram kernels. These kernels are widely use as a similarity measure in natural language processing and computational biology applications, see (Leslie et al., 2002; Lodhi et al., 2002) for instance. The n-gram kernel Kn of order n is defined as 5 = argmin xEH x = argmax xEH x = argmax EX[(T o T−1)(x, x′)] (4) xEH = argmax I IA(x) o T o T−1 o XI I, (5) � cx(z)cy(z), (6) xEH Kn(x, y) = |z|=n where we denote by A(x) an automaton accepting (only) the string x and by I I·I I the sum of the weights of all accepted paths of a transducer. 4 Algorithms 4.1 General method Equation (5) could suggest computing A(x) o T o T−1 o X for each possible x E H. Instead, we where cx(z) is the number of occurrences of z in x. Kn is a positive definite symmetric rational kern</context>
</contexts>
<marker>Leslie, Eskin, Noble, 2002</marker>
<rawString>Christina S. Leslie, Eleazar Eskin, and William Stafford Noble. 2002. The Spectrum Kernel: A String Kernel for SVM Protein Classification. In Pacific Symposium on Biocomputing, pages 566–575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Variational decoding for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL and IJCNLP,</booktitle>
<pages>593--601</pages>
<contexts>
<context position="20821" citStr="Li et al., 2009" startWordPosition="3694" endWordPosition="3697">ach ngram w to (10) is applied by iteratively composing with the weighted automaton corresponding to w(w/(0|w|p(w|X))w)∗ where w = E∗ \ (E∗wE∗). Finally, the MBR hypothesis is extracted as the best path in the automaton. The above steps are carried out one n-gram at a time. For a moderately large lattice, there can be several thousands of n-grams and the procedure becomes expensive. This leads us to investigate methods that do not require processing the n-grams one at a time in order to achieve greater efficiency. 3Related approaches were presented in (DeNero et al., 2009; Kumar et al., 2009; Li et al., 2009). Figure 4: Transducer T1 over the real semiring for the alphabet {a, b}. The first idea is to approximate the KLB similarity measure using a weighted sum of n-gram kernels. This corresponds to approximating 1x′(w) by cx′(w) in (10). This leads us to the following similarity measure: XKNG(x, x′) = 00|x′ |+ 0|w|cx(w)cx′(w) |w|≤n = 00|x′ |+ X 0iKi(x, x′) 1≤i≤n Intuitively, the larger the length of w the less likely it is that cx(w) =� 1x(w), which suggests computing the contribution to KLB(x, x′) of lower-order n-grams (|w |G k) exactly, but using the approximation by n-gram kernels for the high</context>
</contexts>
<marker>Li, Eisner, Khudanpur, 2009</marker>
<rawString>Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009. Variational decoding for statistical machine translation. In Proceedings ofACL and IJCNLP, pages 593– 601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watskins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="11472" citStr="Lodhi et al., 2002" startWordPosition="1956" endWordPosition="1959">The next two sections describe more efficient algorithms. Note that in practice, for numerical stability, all of these computations are done in the log semiring which is isomorphic to (R+ U{+oo}, +, x, 0, 1). In particular, the maximum weight path in the last step is then obtained by using a standard single-source shortest-path algorithm. 4.2 Efficient method for n-gram kernels A common family of rational kernels is the family of n-gram kernels. These kernels are widely use as a similarity measure in natural language processing and computational biology applications, see (Leslie et al., 2002; Lodhi et al., 2002) for instance. The n-gram kernel Kn of order n is defined as 5 = argmin xEH x = argmax xEH x = argmax EX[(T o T−1)(x, x′)] (4) xEH = argmax I IA(x) o T o T−1 o XI I, (5) � cx(z)cy(z), (6) xEH Kn(x, y) = |z|=n where we denote by A(x) an automaton accepting (only) the string x and by I I·I I the sum of the weights of all accepted paths of a transducer. 4 Algorithms 4.1 General method Equation (5) could suggest computing A(x) o T o T−1 o X for each possible x E H. Instead, we where cx(z) is the number of occurrences of z in x. Kn is a positive definite symmetric rational kernel since it correspon</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watskins, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watskins. 2002. Text classification using string kernels. Journal ofMachine Learning Research, 2:419–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Richard Sproat</author>
</authors>
<title>An Efficient Compiler for Weighted Rewrite Rules.</title>
<date>1996</date>
<booktitle>In Proceedings ofACL ’96,</booktitle>
<location>Santa Cruz, California.</location>
<contexts>
<context position="18632" citStr="Mohri and Sproat, 1996" startWordPosition="3328" endWordPosition="3331">ethod for generalizing Step 2 of the algorithm described in the previous section as follows (see illustration in Figure 3): (i) use Y2 to construct a deterministic weighted tree Y2′ defined on the tropical semiring accepting the same strings as Y2 with the same weights, with the final weights equal to the total weight given by Y2 to the string ending at that leaf; (ii) let Y1 be the weighted automaton obtained by first adding self-loops labeled with all elements of E at the initial state of Y2′ and then determinizing it, and then inserting new transitions leaving final states as described in (Mohri and Sproat, 1996). 961 Step (ii) consists of computing a deterministic weighted automaton for E∗Y′�. This step corresponds to the Aho-Corasick construction (Aho and Corasick, 1975) and can be done in time linear in the size of Y′�. This approach assumes that the grammar G of batch context-dependent rules inferred by XT is unambiguous. This can be tested by constructing the finite automaton corresponding to all rules in G. The grammar G is unambiguous iff the resulting automaton is unambiguous (which can be tested using a classical algorithm). An alternative and more efficient test consists of checking the pres</context>
</contexts>
<marker>Mohri, Sproat, 1996</marker>
<rawString>Mehryar Mohri and Richard Sproat. 1996. An Efficient Compiler for Weighted Rewrite Rules. In Proceedings ofACL ’96, Santa Cruz, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Weighted automata algorithms.</title>
<date>2009</date>
<booktitle>Handbook of Weighted Automata, chapter 6,</booktitle>
<pages>213--254</pages>
<editor>In Manfred Droste, Werner Kuich, and Heiko Vogler, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="10439" citStr="Mohri, 2009" startWordPosition="1781" endWordPosition="1782">d similarity maximization: E [K(x, x′)]. (3) x′�X When K is a positive definite symmetric rational kernel, it can often be rewritten as K(x, y) = (T o T−1)(x, y), where T is a weighted transducer over the semiring (R+U{+oo}, +, x, 0, 1). Equation (3) can then be rewritten as can compute a composition based on an automaton accepting all sequences in H, A(H). This leads to a straightforward method for determining the sequence maximizing the expected similarity having the following steps: 1. compute the composition X o T, project on the output and optimize (epsilon-remove, determinize, minimize (Mohri, 2009)) and let Y2 be the result;1 2. compute the composition Y1 = A(H) o T; 3. compute Y1 o Y2 and project on the input, let Z be the result;2 4. determinize Z; 5. find the maximum weight path with the label of that path giving 5. While this method can be efficient in various scenarios, in some instances the weighted determinization yielding Z can be both space- and time-consuming, even though the input is acyclic. The next two sections describe more efficient algorithms. Note that in practice, for numerical stability, all of these computations are done in the log semiring which is isomorphic to (R</context>
</contexts>
<marker>Mohri, 2009</marker>
<rawString>Mehryar Mohri. 2009. Weighted automata algorithms. In Manfred Droste, Werner Kuich, and Heiko Vogler, editors, Handbook of Weighted Automata, chapter 6, pages 213–254. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical mchine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="23075" citStr="Och and Ney, 2004" startWordPosition="4120" endWordPosition="4123">6.7 44.1 42.8 ngram1 37.1 39.2 38.5 34.4 27.5 65.2 51.4 58.0 45.2 44.8 Table 1: BLEU score (%) zhen aren nist02 nist04 nist05 nist06 nist08 nist02 nist04 nist05 nist06 nist08 exact 3560 7863 5553 6313 5738 12341 23266 11152 11417 11405 approx 168 422 279 335 328 504 1296 528 619 808 ngram 28 72 34 70 43 85 368 105 63 66 ngram1 58 175 96 99 89 368 943 308 167 191 Table 2: MBR Time (in seconds) can then be expressed as the relevant linear combination of Ki and Ki. 6 Experimental Results Lattices were generated using a phrase-based MT system similar to the alignment template system described in (Och and Ney, 2004). Given a source sentence, the system produces a word lattice A that is a compact representation of a very large N-best list of translation hypotheses for that source sentence and their likelihoods. The lattice A is converted into a lattice X that represents a probability distribution (i.e. the posterior probability distribution given the source sentence) following: exp(αA(x)) X(x) = Ey∈E∗ exp(αA(y)) (14) where the scaling factor α E [0, oc) flattens the distribution when α &lt; 1 and sharpens it when α &gt; 1. We then applied the methods described in Section 5 to the lattice X using as hypothesis s</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical mchine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<tech>Technical Report RC22176 (W0109-022),</tech>
<institution>IBM Research Division.</institution>
<contexts>
<context position="19474" citStr="Papineni et al., 2001" startWordPosition="3466" endWordPosition="3469">approach assumes that the grammar G of batch context-dependent rules inferred by XT is unambiguous. This can be tested by constructing the finite automaton corresponding to all rules in G. The grammar G is unambiguous iff the resulting automaton is unambiguous (which can be tested using a classical algorithm). An alternative and more efficient test consists of checking the presence of a failure or default transition to a final state during the Aho-Corasick construction, which occurs if and only if there is ambiguity. 5 Application to Machine Translation In machine translation, the BLEU score (Papineni et al., 2001) is typically used as an evaluation metric. In (Tromble et al., 2008), a Minimum Bayes-Risk decoding approach for MT lattices was introduced.3 The loss function used in that approach was an approximation of the log-BLEU score by a linear function of n-gram matches and candidate length. This loss function corresponds to the following similarity measure: XKLB(x, x′) = 00|x′ |+ 0|w|cx(w)1x′(w). |w|≤n (10) where 1x(w) is 1 if w occurs in x and 0 otherwise. (Tromble et al., 2008) implements the MBR decoder using weighted automata operations. First, the set of n-grams is extracted from the lattice. </context>
<context position="24272" citStr="Papineni et al., 2001" startWordPosition="4326" endWordPosition="4329">e X using as hypothesis set H the unweighted lattice obtained from X. The following parameters for the n-gram factors were used: −1 and θn = 1 for n ≥ 1. (15) T 4Tprn−1 Experiments were conducted on two language pairs Arabic-English (aren) and Chinese-English (zhen) and for a variety of datasets from the NIST Open Machine Translation (OpenMT) Evaluation.4 The values of α, p and r used for each pair are given 4http://www.nist.gov/speech/tests/mt α p r aren 0.2 0.85 0.72 zhen 0.1 0.80 0.62 Table 3: Parameters used for performing MBR. in Table 3. We used the IBM implementation of the BLEU score (Papineni et al., 2001). We implemented the following methods using the OpenFst library (Allauzen et al., 2007): • exact: uses the similarity measure KLB based on the linearized log-BLEU, implemented as described in (Tromble et al., 2008); • approx: uses the approximation to KLB from (Kumar et al., 2009) and described in the appendix; • ngram: uses the similarity measure KNG implemented using the algorithm of Section 4.2; • ngram1: uses the similarity measure K1NG also implemented using the algorithm of Section 4.2. The results from Tables 1-2 show that ngram1 performs as well as exact on all datasets5 while being t</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a Method for Automatic Evaluation of Machine Translation. Technical Report RC22176 (W0109-022), IBM Research Division.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arto Salomaa</author>
<author>Matti Soittola</author>
</authors>
<title>AutomataTheoretic Aspects ofFormal Power Series.</title>
<date>1978</date>
<publisher>Springer.</publisher>
<contexts>
<context position="7824" citStr="Salomaa and Soittola, 1978" startWordPosition="1312" endWordPosition="1315">E* is denoted by T (x, y) and is obtained by ®-summing the weights of all accepting paths with input label x and output label y. For any transducer T, T−1 denotes its inverse, that is the transducer obtained from T by swapping the input and output labels of each transition. For all x, y E E*, we have T−1(x, y) = T (y, x). The composition of two weighted transducers T1 and T2 with matching input and output alphabets E, is a weighted transducer denoted by T1 o T2 when the semiring is commutative and the sum: (T1 o T2)(x, y) = � T1(x, z) ® T2(z, y) (1) ZEE∗ is well-defined and in K for all x, y (Salomaa and Soittola, 1978). Weighted automata can be defined as weighted transducers A with identical input and output labels, for any transition. Since only pairs of the form (x, x) can have a non-zero weight associated to them by A, we denote the weight associated by A to (x, x) by A(x) and call it the weight associated by A to x. Similarly, in the graph representation of weighted automata, the output (or input) label is omitted. Figure 1(b) shows an example of a weighted automaton. When A and B are weighted automata, A o B is called the intersection of A and B. Omitting the input labels of a weighted transducer T re</context>
</contexts>
<marker>Salomaa, Soittola, 1978</marker>
<rawString>Arto Salomaa and Matti Soittola. 1978. AutomataTheoretic Aspects ofFormal Power Series. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy W Tromble</author>
<author>Shankar Kumar</author>
<author>Franz J Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice minimum Bayesrisk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>620--629</pages>
<contexts>
<context position="1007" citStr="Tromble et al. (2008)" startWordPosition="145" endWordPosition="148">ayes decoding for a similarity-based loss function. Our algorithms are designed for similarity functions that are sequence kernels in a general class of positive definite symmetric kernels. We discuss both a general algorithm and a more efficient algorithm applicable in a common unambiguous scenario. We also describe the application of our algorithms to machine translation and report the results of experiments with several translation data sets which demonstrate a substantial speed-up. In particular, our results show a speed-up by two orders of magnitude with respect to the original method of Tromble et al. (2008) and by a factor of 3 or more even with respect to an approximate algorithm specifically designed for that task. These results open the path for the exploration of more appropriate or optimal kernels for the specific tasks considered. 1 Introduction The output of many complex natural language processing systems such as information extraction, speech recognition, or machine translation systems is a probabilistic automaton. Exploiting the full information provided by this probabilistic automaton can lead to more accurate results than just using the one-best sequence. Different techniques have be</context>
<context position="4375" citStr="Tromble et al., 2008" startWordPosition="674" endWordPosition="677">orresponding algorithmic problem. In Section 4, we describe both a general method for the computation of the expected similarity maximization, and a more efficient method that can be used with a broad sub-family of rational kernels that verify a condition of nonambiguity. This latter family includes the class of n-gram kernels which have been previously used to 957 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 957–965, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics apply MBR to machine translation (Tromble et al., 2008). We examine in more detail the use and application of our algorithms to machine translation in Section 5. Section 6 reports the results of experiments applying our algorithms in several large data sets in machine translation. These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al. (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al., 2009). We start with some preliminary definitions and algorithms related to weighted automata and tr</context>
<context position="9473" citStr="Tromble et al., 2008" startWordPosition="1608" endWordPosition="1611">ch originally motivated our study). For machine translation, the sequences accepted by X are the potential translations of the input sentence, each with some probability given by X. Let E be the alphabet for the task considered, e.g., words of the target language in machine translation, and let L: E* x E* —* R denote a loss function defined over the sequences on that alphabet. Given a reference or hypothesis set H C_ E*, minimum Bayes risk (MBR) decoding consists of selecting a hypothesis x E H with minimum expected loss with respect to the probability distribution X (Bickel and Doksum, 2001; Tromble et al., 2008): E [L(x, x′)]. (2) x′�X Here, we shall consider the case, frequent in practice, where minimizing the loss L is equivalent to maximizing a similarity measure K : E* xE* —* R. When K is a sequence kernel that can be represented by weighted transducers, it is a rational kernel (Cortes et al., 2004). The problem is then equivalent to the following expected similarity maximization: E [K(x, x′)]. (3) x′�X When K is a positive definite symmetric rational kernel, it can often be rewritten as K(x, y) = (T o T−1)(x, y), where T is a weighted transducer over the semiring (R+U{+oo}, +, x, 0, 1). Equation</context>
<context position="19543" citStr="Tromble et al., 2008" startWordPosition="3478" endWordPosition="3481">nferred by XT is unambiguous. This can be tested by constructing the finite automaton corresponding to all rules in G. The grammar G is unambiguous iff the resulting automaton is unambiguous (which can be tested using a classical algorithm). An alternative and more efficient test consists of checking the presence of a failure or default transition to a final state during the Aho-Corasick construction, which occurs if and only if there is ambiguity. 5 Application to Machine Translation In machine translation, the BLEU score (Papineni et al., 2001) is typically used as an evaluation metric. In (Tromble et al., 2008), a Minimum Bayes-Risk decoding approach for MT lattices was introduced.3 The loss function used in that approach was an approximation of the log-BLEU score by a linear function of n-gram matches and candidate length. This loss function corresponds to the following similarity measure: XKLB(x, x′) = 00|x′ |+ 0|w|cx(w)1x′(w). |w|≤n (10) where 1x(w) is 1 if w occurs in x and 0 otherwise. (Tromble et al., 2008) implements the MBR decoder using weighted automata operations. First, the set of n-grams is extracted from the lattice. Next, the posterior probability p(w|X) of each n-gram is computed. St</context>
<context position="24487" citStr="Tromble et al., 2008" startWordPosition="4359" endWordPosition="4362">s Arabic-English (aren) and Chinese-English (zhen) and for a variety of datasets from the NIST Open Machine Translation (OpenMT) Evaluation.4 The values of α, p and r used for each pair are given 4http://www.nist.gov/speech/tests/mt α p r aren 0.2 0.85 0.72 zhen 0.1 0.80 0.62 Table 3: Parameters used for performing MBR. in Table 3. We used the IBM implementation of the BLEU score (Papineni et al., 2001). We implemented the following methods using the OpenFst library (Allauzen et al., 2007): • exact: uses the similarity measure KLB based on the linearized log-BLEU, implemented as described in (Tromble et al., 2008); • approx: uses the approximation to KLB from (Kumar et al., 2009) and described in the appendix; • ngram: uses the similarity measure KNG implemented using the algorithm of Section 4.2; • ngram1: uses the similarity measure K1NG also implemented using the algorithm of Section 4.2. The results from Tables 1-2 show that ngram1 performs as well as exact on all datasets5 while being two orders of magnitude faster than exact and overall more than 3 times faster than approx. 7 Conclusion We showed that for broad families of transducers T and thus rational kernels, the expected similar5We consider </context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy W. Tromble, Shankar Kumar, Franz J. Och, and Wolfgang Macherey. 2008. Lattice minimum Bayesrisk decoding for statistical machine translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 620– 629.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>