<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999031">
A FrameNet-based Semantic Role Labeler for Swedish
</title>
<author confidence="0.931817">
Richard Johansson and Pierre Nugues
</author>
<affiliation confidence="0.733448">
Department of Computer Science, LTH
Lund University, Sweden
</affiliation>
<email confidence="0.995649">
{richard, pierre}@cs.lth.se
</email>
<sectionHeader confidence="0.993799" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999877764705882">
We present a FrameNet-based semantic
role labeling system for Swedish text. As
training data for the system, we used an
annotated corpus that we produced by
transferring FrameNet annotation from the
English side to the Swedish side in a par-
allel corpus. In addition, we describe two
frame element bracketing algorithms that
are suitable when no robust constituent
parsers are available.
We evaluated the system on a part of the
FrameNet example corpus that we trans-
lated manually, and obtained an accuracy
score of 0.75 on the classification of pre-
segmented frame elements, and precision
and recall scores of 0.67 and 0.47 for the
complete task.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999816633333333">
Semantic role labeling (SRL), the process of auto-
matically identifying arguments of a predicate in
a sentence and assigning them semantic roles, has
received much attention during the recent years.
SRL systems have been used in a number of
projects in Information Extraction and Question
Answering, and are believed to be applicable in
other domains as well.
Building SRL systems for English has been
studied widely (Gildea and Jurafsky, 2002;
Litkowski, 2004), inter alia. However, all these
works rely on corpora that have been produced at
the cost of a large effort by human annotators. For
instance, the current FrameNet corpus (Baker et
al., 1998) consists of 130,000 manually annotated
sentences. For smaller languages such as Swedish,
such corpora are not available.
In this work, we describe a FrameNet-based se-
mantic role labeler for Swedish text. Since there
was no existing training corpus available — no
FrameNet-annotated Swedish corpus of substan-
tial size exists — we used an English-Swedish
parallel corpus whose English part was annotated
with semantic roles using the FrameNet annota-
tion scheme. We then applied a cross-language
transfer to derive an annotated Swedish part. To
evaluate the performance of the Swedish SRL
system, we applied it to a small portion of the
FrameNet example corpus that we translated man-
ually.
</bodyText>
<subsectionHeader confidence="0.993751">
1.1 FrameNet: an Introduction
</subsectionHeader>
<bodyText confidence="0.998894789473684">
FrameNet (Baker et al., 1998) is a lexical database
that describes English words using Frame Seman-
tics (Fillmore, 1976). In this framework, predi-
cates (or in FrameNet terminology, target words)
and their arguments are linked by means of seman-
tic frames. A frame can intuitively be thought of
as a template that defines a set of slots, frame ele-
ments (FEs), that represent parts of the conceptual
structure and typically correspond to prototypical
participants or properties.
Figure 1 shows an example sentence annotated
with FrameNet information. In this example, the
target word statements belongs to (“evokes”) the
frame STATEMENT. Two constituents that fill slots
of the frame (SPEAKER and TOPIC) are annotated
as well.
As usual in these cases, [both parties]SPEAKER
agreed to make no further statements [on the
matter]TOPIC.
</bodyText>
<figureCaption confidence="0.964129">
Figure 1: A sentence from the FrameNet example
corpus.
</figureCaption>
<page confidence="0.993901">
436
</page>
<note confidence="0.7247915">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 436–443,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.995020125">
The initial versions of FrameNet were focused
on describing situations and events, i.e. typically
verbs and their nominalizations. Currently, how-
ever, FrameNet defines frames for a wider range of
semantic relations that can be thought of as predi-
cate/argument structures, including descriptions of
events, states, properties, and objects.
FrameNet consists of the following main parts:
</bodyText>
<listItem confidence="0.934544375">
• An ontology consisting of a set of frames,
frame elements for each frame, and rela-
tions (such as inheritance and causative-of)
between frames.
• A list of lexical units, that is word forms
paired with their corresponding frames. The
frame is used to distinguish between differ-
ent senses of the word, although the treatment
of polysemy in FrameNet is relatively coarse-
grained.
• A collection of example sentences that pro-
vide lexical evidence for the frames and the
corresponding lexical units. Although this
corpus is not intended to be representative, it
is typically used as a training corpus when
contructing automatic FrameNet labelers.
</listItem>
<sectionHeader confidence="0.754644" genericHeader="related work">
1.2 Related Work
</sectionHeader>
<bodyText confidence="0.999911607142857">
Since training data is often a scarce resource for
most languages other than English, a wide range
of methods have been proposed to reduce the need
for manual annotation. Many of these have relied
on existing resources for English and a transfer
method based on word alignment in a parallel cor-
pus to automatically create an annotated corpus in
a new language. Although these data are typically
quite noisy, they have been used to train automatic
systems.
For the particular case of transfer of FrameNet
annotation, there have been a few projects that
have studied transfer methods and evaluated the
quality of the automatically produced corpus. Jo-
hansson and Nugues (2005) applied the word-
based methods of Yarowsky et al. (2001) and ob-
tained promising results. Another recent effort
(Padó and Lapata, 2005) demonstrates that deeper
linguistic information, such as parse trees in the
source and target language, is very beneficial for
the process of FrameNet annotation transfer.
A rather different method to construct bilingual
semantic role annotation is the approach taken by
BiFrameNet (Fung and Chen, 2004). In that work,
annotated structures in a new language (in that
case Chinese) are produced by mining for similar
structures rather than projecting them via parallel
corpora.
</bodyText>
<sectionHeader confidence="0.7080835" genericHeader="method">
2 Automatic Annotation of a Swedish
Training Corpus
</sectionHeader>
<subsectionHeader confidence="0.9810665">
2.1 Training an English Semantic Role
Labeler
</subsectionHeader>
<bodyText confidence="0.9997386">
We selected the 150 most frequent frames in
FrameNet and applied the Collins parser (Collins,
1999) to the example sentences for these frames.
We built a conventional FrameNet parser for En-
glish using 100,000 of these sentences as a train-
ing set and 8,000 as a development set. The classi-
fiers were based on Support Vector Machines that
we trained using LIBSVM (Chang and Lin, 2001)
with the Gaussian kernel. When testing the sys-
tem, we did not assume that the frame was known
a priori. We used the available semantic roles for
all senses of the target word as features for the
classifier.
On a test set from FrameNet, we estimated that
the system had a precision of 0.71 and a recall of
0.65 using a strict scoring method. The result is
slightly lower than the best systems at Senseval-
3 (Litkowski, 2004), possibly because we used a
larger set of frames, and we did not assume that
the frame was known a priori.
</bodyText>
<subsectionHeader confidence="0.999439">
2.2 Transferring the Annotation
</subsectionHeader>
<bodyText confidence="0.99998625">
We produced a Swedish-language corpus anno-
tated with FrameNet information by applying
the SRL system to the English side of Europarl
(Koehn, 2005), which is a parallel corpus that is
derived from the proceedings of the European Par-
liament. We projected the bracketing of the target
words and the frame elements onto the Swedish
side of the corpus by using the Giza++ word
aligner (Och and Ney, 2003). Each word on the
English side was mapped by the aligner onto a
(possibly empty) set of words on the Swedish side.
We used the maximal span method to infer the
bracketing on the Swedish side, which means that
the span of a projected entity was set to the range
from the leftmost projected token to the rightmost.
Figure 2 shows an example of this process.
To make the brackets conform to the FrameNet
annotation practices, we applied a small set of
heuristics. The FrameNet conventions specify that
linking words such as prepositions and subordinat-
</bodyText>
<page confidence="0.996706">
437
</page>
<figure confidence="0.9304906">
[We] wanted to [our perplexity as regards these points] [by abstaining in committee]MEANS
express
SPEAKER MESSAGE
[Genom att avstå från att rösta i utskottet] har [vi] velat uttrycka [denna vår tveksamhet] MESSAGE
MEANS SPEAKER
</figure>
<figureCaption confidence="0.999355">
Figure 2: Example of projection of FrameNet annotation.
</figureCaption>
<bodyText confidence="0.999890875">
ing conjunctions should be included in the brack-
eting. However, since constructions are not iso-
morphic in the sentence pair, a linking word on
the target side may be missed by the projection
method since it is not present on the source side.
For example, the sentence the doctor was answer-
ing an emergency phone call is translated into
Swedish as doktorn svarade på ett larmsamtal,
which uses a construction with a preposition på
‘to/at/on’ that has no counterpart in the English
sentence. The heuristics that we used are spe-
cific for Swedish, although they would probably
be very similar for any other language that uses
a similar set of prepositions and connectives, i.e.
most European languages.
We used the following heuristics:
</bodyText>
<listItem confidence="0.984073416666667">
• When there was only a linking word (preposi-
tion, subordinating conjunction, or infinitive
marker) between the FE and the target word,
it was merged with the FE.
• When a Swedish FE was preceded by a link-
ing word, and the English FE starts with such
a word, it was merged with the FE.
• We used a chunker and adjusted the FE
brackets to include only complete chunks.
• When a Swedish FE crossed the target word,
we used only the part of the FE that was on
the right side of the target.
</listItem>
<bodyText confidence="0.999955037037037">
In addition, some bad annotation was discarded
because we obviously could not use sentences
where no counterpart for the target word could be
found. Additionally, we used only the sentences
where the target word was mapped to a noun, verb,
or an adjective on the Swedish side.
Because of homonymy and polysemy problems,
applying a SRL system without knowing target
words and frames a priori necessarily introduces
noise into the automatically created training cor-
pus. There are two kinds of word sense ambigu-
ity that are problematic in this case: the “internal”
ambiguity, or the fact that there may be more than
one frame for a given target word; and the “exter-
nal” ambiguity, where frequently occurring word
senses are not listed in FrameNet. To sidestep the
problem of internal ambiguity, we used the avail-
able semantic roles for all senses of the target word
as features for the classifier (as described above).
Solving the problem of external ambiguity was
outside the scope of this work.
Some potential target words had to be ignored
since their sense ambiguity was too difficult to
overcome. This category includes auxiliaries such
as be and have, as well as verbs such as take and
make, which frequently appear as support verbs
for nominal predicates.
</bodyText>
<subsectionHeader confidence="0.99819">
2.3 Motivation
</subsectionHeader>
<bodyText confidence="0.905299444444444">
Although the meaning of the two sentences in
a sentence pair in a parallel corpus should be
roughly the same, a fundamental question is
whether it is meaningful to project semantic
markup of text across languages. Equivalent
words in two different languages sometimes ex-
hibit subtle but significant semantic differences.
However, we believe that a transfer makes sense,
since the nature of FrameNet is rather coarse-
grained. Even though the words that evoke a frame
may not have exact counterparts, it is probable that
the frame itself has.
For the projection method to be meaningful, we
must make the following assumptions:
• The complete frame ontology in the English
FrameNet is meaningful in Swedish as well,
and each frame has the same set of semantic
roles and the same relations to other frames.
</bodyText>
<listItem confidence="0.971261333333333">
• When a target word evokes a certain frame in
English, it has a counterpart in Swedish that
evokes the same frame.
• Some of the FEs on the English side have
counterparts with the same semantic roles on
the Swedish side.
</listItem>
<page confidence="0.996233">
438
</page>
<bodyText confidence="0.999990631578947">
In addition, we made the (obviously simplistic)
assumption that the contiguous entities we project
are also contiguous on the target side.
These assumptions may all be put into ques-
tion. Above all, the second assumption will fail
in many cases because the translations are not lit-
eral, which means that the sentences in the pair
may express slightly different information. The
third assumption may be invalid if the information
expressed is realized by radically different con-
structions, which means that an argument may be-
long to another predicate or change its semantic
role on the Swedish side. Padó and Lapata (2005)
avoid this problem by using heuristics based on a
target-language FrameNet to select sentences that
are close in meaning. Since we have no such re-
source to rely on, we are forced to accept that this
problem introduces a certain amount of noise into
the automatically annotated corpus.
</bodyText>
<sectionHeader confidence="0.849156" genericHeader="method">
3 Training a Swedish SRL System
</sectionHeader>
<bodyText confidence="0.9986447">
Using the transferred FrameNet annotation, we
trained a SRL system for Swedish text. Like most
previous systems, it consists of two parts: a FE
bracketer and a classifier that assigns semantic
roles to FEs. Both parts are implemented as SVM
classifiers trained using LIBSVM. The semantic
role classifier is rather conventional and is not de-
scribed in this paper.
To construct the features used by the classifiers,
we used the following tools:
</bodyText>
<listItem confidence="0.962221">
• An HMM-based POS tagger,
• A rule-based chunker,
• A rule-based time expression detector,
• Two clause identifiers, of which one is rule-
based and one is statistical,
• The MALTPARSER dependency parser
(Nivre et al., 2004), trained on a 100,000-
word Swedish treebank.
</listItem>
<bodyText confidence="0.9996185">
We constructed shallow parse trees using the
clause trees and the chunks. Dependency and shal-
low parse trees for a fragment of a sentence from
our test corpus are shown in Figures 3 and 4, re-
spectively. This sentence, which was translated
from an English sentence that read the doctor was
answering an emergency phone call, comes from
the English FrameNet example corpus.
</bodyText>
<note confidence="0.367812">
PR
</note>
<figureCaption confidence="0.757248">
doktorn svarade på ett larmsamtal
Figure 3: Example dependency parse tree.
</figureCaption>
<figure confidence="0.848358">
[ [ doktorn]NG_nom [ svarade ]VG_fin [ på] PP [ ett larmsamtal ]NG_nom] Clause
</figure>
<figureCaption confidence="0.997055">
Figure 4: Example shallow parse tree.
</figureCaption>
<subsectionHeader confidence="0.997961">
3.1 Frame Element Bracketing Methods
</subsectionHeader>
<bodyText confidence="0.99994582051282">
We created two redundancy-based FE bracket-
ing algorithms based on binary classification of
chunks as starting or ending the FE. This is some-
what similar to the chunk-based system described
by Pradhan et al. (2005a), which uses a segmenta-
tion strategy based on IOB2 bracketing. However,
our system still exploits the dependency parse tree
during classification.
We first tried the conventional approach to the
problem of FE bracketing: applying a parser to the
sentence, and classifying each node in the parse
tree as being an FE or not. We used a dependency
parser since there is no constituent-based parser
available for Swedish. This proved unsuccessful
because the spans of the dependency subtrees fre-
quently were incompatible with the spans defined
by the FrameNet annotations. This was especially
the case for non-verbal target words and when the
head of the argument was above the target word in
the dependency tree. To be usable, this approach
would require some sort of transformation, possi-
bly a conversion into a phrase-structure tree, to be
applied to the dependency trees to align the spans
with the FEs. Preliminary investigations were un-
successful, and we left this to future work.
We believe that the methods we developed are
more suitable in our case, since they base their
decisions on several parse trees (in our case, two
clause-chunk trees and one dependency tree). This
redundancy is valuable because the dependency
parsing model was trained on a treebank of just
100,000 words, which makes it less robust than
Collins’ or Charniak’s parsers for English. In ad-
dition, the methods do not implicitly rely on the
common assumption that every FE has a counter-
part in a parse tree. Recent work in semantic role
labeling, see for example Pradhan et al. (2005b),
has focused on combining the results of SRL sys-
tems based on different types of syntax. Still, all
</bodyText>
<figure confidence="0.5589925">
SUB ADV
DET
</figure>
<page confidence="0.996499">
439
</page>
<bodyText confidence="0.9998983">
systems exploiting recursive parse trees are based
on binary classification of nodes as being an argu-
ment or not.
The training sets used to train the final classi-
fiers consisted of one million training instances for
the start classifier, 500,000 for the end classifier,
and 272,000 for the role classifier. The features
used by the classifiers are described in Subsec-
tion 3.2, and the performance of the two FE brack-
eting algorithms compared in Subsection 4.2.
</bodyText>
<subsectionHeader confidence="0.558865">
3.1.1 Greedy start-end
</subsectionHeader>
<bodyText confidence="0.999941545454545">
The first FE bracketing algorithm, the greedy
start-end method, proceeds through the sequence
of chunks in one pass from left to right. For each
chunk opening bracket, a binary classifier decides
if an FE starts there or not. Similarly, another bi-
nary classifier tests chunk end brackets for ends
of FEs. To ensure compliance to the FrameNet
annotation standard (bracket matching, and no FE
crossing the target word), the algorithm inserts ad-
ditional end brackets where appropriate. Pseu-
docode is given in Algorithm 1.
</bodyText>
<table confidence="0.246974117647059">
Algorithm 1 Greedy Bracketing
Input: A list L of chunks and a target word t
Binary classifiers starts and ends
Output: The sets S and E of start and end brackets
Split L into the sublists Lbefore, Ltarget, and Lafter, which correspond
to the parts of the list that is before, at, and after the target word, respectively.
Initialize chunk-open to FALSE
for Lsub in {Lbefore, Ltarget, Lafter} do
for c in Lsub do
if starts(c) then
if chunk-open then
Add an end bracket before c to E
end if
chunk-open ← TRUE
Add a start bracket before c to S
end if
if chunk-open ∧ (ends(c) ∨ c is final in Lsub) then
</table>
<tableCaption confidence="0.7734368">
chunk-open ← FALSE
Add an end bracket after c to E
end if
end for
end for
</tableCaption>
<bodyText confidence="0.999764625">
Figure 5 shows an example of this algorithm,
applied to the example fragment. The small brack-
ets correspond to chunk boundaries, and the large
brackets to FE boundaries that the algorithm in-
serts. In the example, the algorithm inserts an end
bracket after the word doktorn ‘the doctor’, since
no end bracket was found before the target word
svarade ‘was answering’.
</bodyText>
<subsectionHeader confidence="0.836185">
3.1.2 Globally optimized start-end
</subsectionHeader>
<bodyText confidence="0.99942325">
The second algorithm, the globally optimized
start-end method, maximizes a global probability
score over each sentence. For each chunk open-
ing and closing bracket, probability models assign
</bodyText>
<figureCaption confidence="0.943308">
Figure 5: Illustration of the greedy start-end
method.
</figureCaption>
<bodyText confidence="0.9463239">
the probability of an FE starting (or ending, re-
spectively) at that chunk. The probabilities are
estimated using the built-in sigmoid fitting meth-
ods of LIBSVM. Making the somewhat unrealis-
tic assumption of independence of the brackets,
the global probability score to maximize is de-
fined as the product of all start and end proba-
bilities. We added a set of constraints to ensure
that the segmentation conforms to the FrameNet
annotation standard. The constrained optimiza-
tion problem is then solved using the JACOP fi-
nite domain constraint solver (Kuchcinski, 2003).
We believe that an n-best beam search method
would produce similar results. The pseudocode
for the method can be seen in Algorithm 2. The
definitions of the predicates no-nesting and
no-crossing, which should be obvious, are
omitted.
Algorithm 2 Globally Optimized Bracketing
Input: A list L of chunks and a target word t
</bodyText>
<subsectionHeader confidence="0.628999">
Probability models ˆPstarts and ˆPends
</subsectionHeader>
<bodyText confidence="0.8779">
Output: The sets Smax and Emax of start and end brackets
</bodyText>
<equation confidence="0.997932875">
legal(S, E) ← |S |= |E|
∧ max(E) &gt; max(S) ∧ min(S) &lt; min(E)
∧ Ino-nesting(S, E) ∧ no-crossing(t, S, E)
IL
I
score(S, E) ← j j�7cES Pstarts(c) · LEL\S(1 − Pstarts(c))
ILEE Pends (c) · ICEL\E(1 − ˆPends(c))
(Smax, Emax) ← argmaxllegal(S,E)Iscore(S, E)
</equation>
<bodyText confidence="0.999257333333333">
Figure 6 shows an example of the globally op-
timized start-end method. In the example, the
global probability score is maximized by a brack-
eting that is illegal because the FE starting at dok-
torn is not closed before the target (0.8 · 0.6 · 0.6 ·
0.7 · 0.8 · 0.7 = 0.11). The solution of the con-
strained problem is a bracketing that contains an
end bracket before the target (0.8 · 0.4 · 0.6 · 0.7 ·
0.8 · 0.7 = 0.075)
</bodyText>
<subsectionHeader confidence="0.984811">
3.2 Features Used by the Classifiers
</subsectionHeader>
<bodyText confidence="0.935734">
Table 1 summarizes the feature sets used by
the greedy start-end (GSE), optimized start-end
(OSE), and semantic role classification (SRC).
</bodyText>
<figure confidence="0.880114333333333">
START START
... [[doktorn] ] svarade [[på] [ett larmsamtal] ] . . .
Additional END inserted END
</figure>
<page confidence="0.879158">
440
</page>
<figureCaption confidence="0.9955725">
Figure 6: Illustration of the globally optimized
start-end method.
</figureCaption>
<table confidence="0.8006795">
GSE OSE SRC
Target lemma + + +
Target POS + + +
Voice + + +
Allowed role labels + + +
Position + + +
Head word (HW) + + +
Head POS + + +
Phrase/chunk type (PT) + + +
HW/POS/PT, f2 chunk window + + -
Dep-tree &amp; shallow path target + + +
Starting paths target + + -
Ending paths target + + -
Path start + - -
</table>
<tableCaption confidence="0.997458">
Table 1: Features used by the classifiers.
</tableCaption>
<subsectionHeader confidence="0.52363">
3.2.1 Conventional Features
</subsectionHeader>
<bodyText confidence="0.999226">
Most of the features that we use have been used
by almost every system since the first well-known
description (Gildea and Jurafsky, 2002). The fol-
lowing of them are used by all classifiers:
</bodyText>
<listItem confidence="0.9978352">
• Target word (predicate) lemma and POS
• Voice (when the target word is a verb)
• Position (before or after the target)
• Headword and POS
• Phrase or chunk type
</listItem>
<bodyText confidence="0.999927958333333">
In addition, all classifiers use the set of allowed
semantic role labels as a set of boolean features.
This is needed to constrain the output to a la-
bel that is allowed by FrameNet for the current
frame. In addition, this feature has proven use-
ful for the FE bracketing classifiers to distinguish
between event-type and object-type frames. For
event-type frames, dependencies are often long-
distance, while for object-type frames, they are
typically restricted to chunks very near the target
word. The part of speech of the target word alone
is not enough to distinguish these two classes,
since many nouns belong to event-type frames.
For the phrase/chunk type feature, we use
slightly different values for the bracketing case
and the role assignment case: for bracketing, the
value of this feature is simply the type of the cur-
rent chunk; for classification, it is the type of the
largest chunk or clause that starts at the leftmost
token of the FE. For prepositional phrases, the
preposition is attached to the phrase type (for ex-
ample, the second FE in the example fragment
starts with the preposition på ‘at/on’, which causes
the value of the phrase type feature to be PP-på).
</bodyText>
<subsectionHeader confidence="0.835835">
3.2.2 Chunk Context Features
</subsectionHeader>
<bodyText confidence="0.999955166666667">
Similarly to the chunk-based PropBank ar-
gument bracketer described by Pradhan et al.
(2005a), the start-end methods use the head word,
head POS, and chunk type of chunks in a window
of size 2 on both sides of the current chunk to clas-
sify it as being the start or end of an FE.
</bodyText>
<subsectionHeader confidence="0.96764">
3.2.3 Parse Tree Path Features
</subsectionHeader>
<bodyText confidence="0.9995615">
Parse tree path features have been shown to be
very important for argument bracketing in several
studies. All classifiers used here use a set of such
features:
</bodyText>
<listItem confidence="0.501543363636364">
• Dependency tree path from the head to the
target word. In the example text, the first
chunk (consisting of the word doktorn), has
the value SUB-T for this feature. This means
that to go from the head of the chunk to the
target in the dependency graph (Figure 3),
you traverse a SUB (subject) link upwards.
Similarly, the last chunk (ett larmsamtal) has
the value PR-T-ADV-T.
• Shallow path from the chunk containing the
head to the target word. For the same chunks
</listItem>
<bodyText confidence="0.98432175">
as above, these values are both NG_nom-T-
Clause-I-VG_fin, which means that to tra-
verse the shallow parse tree (Figure 4) from
the chunk to the target, you start with a
NG_nom node, go upwards to a Clause
node, and finally down to the VG_fin node.
The start-end classifiers additionally use the full
set of paths (dependency and shallow paths) to the
target word from each node starting (or ending, re-
spectively) at the current chunk, and the greedy
end classifier also uses the path from the current
chunk to the start chunk.
</bodyText>
<figure confidence="0.995962863636364">
^
Pends
^
=0.4 Pends=0.3
^
Pends
=0.7
P^starts=0.8 P^starts=0.6 P^starts=0.2
^
1−Ptarts= 0.2 1−
^
Ptarts=0.4
... [[doktorn] ] svarade [[på] [ett larmsamtal] ] . . .
^ ^
1−Pends=0.6 1−Pends =0.7 1−
1−
= 0.8
^
Ptarts
^
=0.3
Pends
</figure>
<page confidence="0.979852">
441
</page>
<sectionHeader confidence="0.743115" genericHeader="evaluation">
4 Evaluation of the System
</sectionHeader>
<subsectionHeader confidence="0.987432">
4.1 Evaluation Corpus
</subsectionHeader>
<bodyText confidence="0.9997629">
To evaluate the system, we manually translated
150 sentences from the FrameNet example corpus.
These sentences were selected randomly from the
English development set. Some sentences were re-
moved, typically because we found the annotation
dubious or the meaning of the sentence difficult to
comprehend precisely. The translation was mostly
straightforward. Because of the extensive use of
compounding in Swedish, some frame elements
were merged with target words.
</bodyText>
<subsectionHeader confidence="0.995552">
4.2 Comparison of FE Bracketing Methods
</subsectionHeader>
<bodyText confidence="0.999346166666667">
We compared the performance of the two methods
for FE bracketing on the test set. Because of lim-
ited time, we used smaller training sets than for the
full evaluation below (100,000 training instances
for all classifiers). Table 2 shows the result of this
comparison.
</bodyText>
<table confidence="0.84142425">
Greedy Optimized
Precision 0.70 0.76
Recall 0.50 0.44
F0=1 0.58 0.55
</table>
<tableCaption confidence="0.999838">
Table 2: Comparison of FE bracketing methods.
</tableCaption>
<bodyText confidence="0.9999685">
As we can see from the Table 2, the globally op-
timized start-end method increased the precision
somewhat, but decreased the recall and made the
overall F-measure lower. We therefore used the
greedy start-end method for our final evaluation
that is described in the next section.
</bodyText>
<subsectionHeader confidence="0.99901">
4.3 Final System Performance
</subsectionHeader>
<bodyText confidence="0.999949058823529">
We applied the Swedish semantic role labeler to
the translated sentences and evaluated the result.
We used the conventional experimental setting
where the frame and the target word were given
in advance. The results, with approximate 95%
confidence intervals included, are presented in Ta-
ble 3. The figures are precision and recall for the
full task, classification accuracy of pre-segmented
arguments, precision and recall for the bracket-
ing task, full task precision and recall using the
Senseval-3 scoring metrics, and finally the propor-
tion of full sentences whose FEs were correctly
bracketed and classified. The Senseval-3 method
uses a more lenient scoring scheme that counts a
FE as correctly identified if it overlaps with the
gold standard FE and has the correct label. Al-
though the strict measures are more interesting,
we include these figures for comparison with the
systems participating in the Senseval-3 Restricted
task (Litkowski, 2004).
We include baseline scores for the argument
bracketing and classification tasks, respectively.
The bracketing baseline method considers non-
punctuation subtrees dependent of the target word.
When the target word is a verb, the baseline puts
FE brackets around the words included in each of
these subtrees1. When the target is a noun, we also
bracket the target word token itself, and when it is
an adjective, we additionally bracket its parent to-
ken. As a baseline for the argument classification
task, every argument is assigned the most frequent
semantic role in the frame. As can be seen from
the table, all scores except the argument bracket-
ing recall are well above the baselines.
</bodyText>
<table confidence="0.999945416666667">
Precision (Strict scoring method) 0.67 f 0.064
Recall 0.47 f 0.057
Argument Classification Accuracy 0.75 f 0.050
Baseline 0.41 f 0.056
Argument Bracketing Precision 0.80 f 0.055
Baseline Precision 0.50 f 0.055
Argument Bracketing Recall 0.57 f 0.057
Baseline Recall 0.55 f 0.057
Precision (Senseval-3 scoring method) 0.77 f 0.057
Overlap 0.75 f 0.039
Recall 0.55 f 0.057
Complete Sentence Accuracy 0.29 f 0.073
</table>
<tableCaption confidence="0.9151075">
Table 3: Results on the Swedish test set with ap-
proximate 95% confidence intervals.
</tableCaption>
<bodyText confidence="0.959109052631579">
Although the performance figures are better
than the baselines, they are still lower than for
most English systems (although higher than some
of the systems at Senseval-3). We believe that
the main reason for the performance is the qual-
ity of the data that were used to train the system,
since the results are consistent with the hypoth-
esis that the quality of the transferred data was
roughly equal to the performance of the English
system multiplied by the figures for the transfer
method (Johansson and Nugues, 2005). In that
experiment, the transfer method had a precision
of 0.84, a recall of 0.81, and an F-measure of
0.82. If we assume that the transfer performance
is similar for Swedish, we arrive at a precision of
0.71 · 0.84 = 0.60, a recall of 0.65 · 0.81 = 0.53,
1This is possible because MALTPARSER produces projec-
tive trees, i.e. the words in each subtree form a contiguous
substring of the sentence.
</bodyText>
<page confidence="0.995646">
442
</page>
<bodyText confidence="0.99989375">
and an F-measure of 0.56. For the F-measure,
0.55 for the system and 0.56 for the product, the
figures match closely. For the precision, the sys-
tem performance (0.67) is significantly higher than
the product (0.60), which suggests that the SVM
learning method handles the noisy training set
rather well for this task. The recall (0.47) is lower
than the corresponding product (0.53), but the dif-
ference is not statistically significant at the 95%
level. These figures suggest that the main effort
towards improving the system should be spent on
improving the training data.
</bodyText>
<sectionHeader confidence="0.999198" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999986774193548">
We have described the design and implementa-
tion of a Swedish FrameNet-based SRL system
that was trained using a corpus that was anno-
tated using cross-language transfer from English
to Swedish. With no manual effort except for
translating sentences for evaluation, we were able
to reach promising results. To our knowledge, the
system is the first SRL system for Swedish in liter-
ature. We believe that the methods described could
be applied to any language, as long as there ex-
ists a parallel corpus where one of the languages
is English. However, the relatively close relation-
ship between English and Swedish probably made
the task comparatively easy in our case.
As we can see, the figures (especially the FE
bracketing recall) leave room for improvement for
the system to be useful in a fully automatic set-
ting. Apart from the noisy training set, proba-
ble reasons for this include the lower robustness
of the Swedish parsers compared to those avail-
able for English. In addition, we have noticed
that the European Parliament corpus is somewhat
biased. For instance, a very large proportion of
the target words evoke the STATEMENT or DIS-
CUSSION frames, but there are very few instances
of the BEING—WET and MAKING—FACES frames.
While training, we tried to balance the selection
somewhat, but applying the projection methods
on other types of parallel corpora (such as novels
available in both languages) may produce a better
training corpus.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999811438596492">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of COLING-ACL’98, pages 86–90, Montréal,
Canada.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines.
Michael J. Collins. 1999. Head-driven statistical mod-
els for natural language parsing. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia.
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language, 280:20–32.
Pascale Fung and Benfeng Chen. 2004. BiFrameNet:
Bilingual frame semantics resource construction
by cross-lingual induction. In Proceedings of
COLING-2004.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Richard Johansson and Pierre Nugues. 2005. Using
parallel corpora for automatic transfer of FrameNet
annotation. In Proceedings of the 1st ROMANCE
FrameNet Workshop, Cluj-Napoca, Romania, 26-28
July.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit 2005.
Krzysztof Kuchcinski. 2003. Constraints-driven
scheduling and resource assignment. ACM Transac-
tions on Design Automation of Electronic Systems,
8(3):355–383.
Ken Litkowski. 2004. Senseval-3 task: Automatic la-
beling of semantic roles. In Senseval-3: Third Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text, pages 9–12.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL-2004, pages 49–56.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Sebastian Padó and Mirella Lapata. 2005. Cross-
lingual projection of role-semantic information. In
Proceedings of HLT/EMNLP 2005.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Dan Jurafsky.
2005a. Support vector learning for semantic argu-
ment classification. Machine Learning, 60(1):11–
39.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Daniel Jurafsky. 2005b. Semantic role
labeling using different syntactic views. In Proceed-
ings of ACL-2005.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of HLT 2001.
</reference>
<page confidence="0.999342">
443
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875392">
<title confidence="0.999983">A FrameNet-based Semantic Role Labeler for Swedish</title>
<author confidence="0.999724">Richard Johansson</author>
<author confidence="0.999724">Pierre Nugues</author>
<affiliation confidence="0.9981355">Department of Computer Science, LTH Lund University, Sweden</affiliation>
<email confidence="0.884468">richard@cs.lth.se</email>
<email confidence="0.884468">pierre@cs.lth.se</email>
<abstract confidence="0.999585388888889">We present a FrameNet-based semantic role labeling system for Swedish text. As training data for the system, we used an annotated corpus that we produced by transferring FrameNet annotation from the English side to the Swedish side in a parallel corpus. In addition, we describe two frame element bracketing algorithms that are suitable when no robust constituent parsers are available. We evaluated the system on a part of the FrameNet example corpus that we translated manually, and obtained an accuracy score of 0.75 on the classification of presegmented frame elements, and precision and recall scores of 0.67 and 0.47 for the complete task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL’98,</booktitle>
<pages>86--90</pages>
<location>Montréal, Canada.</location>
<contexts>
<context position="1497" citStr="Baker et al., 1998" startWordPosition="232" endWordPosition="235"> process of automatically identifying arguments of a predicate in a sentence and assigning them semantic roles, has received much attention during the recent years. SRL systems have been used in a number of projects in Information Extraction and Question Answering, and are believed to be applicable in other domains as well. Building SRL systems for English has been studied widely (Gildea and Jurafsky, 2002; Litkowski, 2004), inter alia. However, all these works rely on corpora that have been produced at the cost of a large effort by human annotators. For instance, the current FrameNet corpus (Baker et al., 1998) consists of 130,000 manually annotated sentences. For smaller languages such as Swedish, such corpora are not available. In this work, we describe a FrameNet-based semantic role labeler for Swedish text. Since there was no existing training corpus available — no FrameNet-annotated Swedish corpus of substantial size exists — we used an English-Swedish parallel corpus whose English part was annotated with semantic roles using the FrameNet annotation scheme. We then applied a cross-language transfer to derive an annotated Swedish part. To evaluate the performance of the Swedish SRL system, we ap</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of COLING-ACL’98, pages 86–90, Montréal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2001</date>
<contexts>
<context position="6065" citStr="Chang and Lin, 2001" startWordPosition="951" endWordPosition="954">es in a new language (in that case Chinese) are produced by mining for similar structures rather than projecting them via parallel corpora. 2 Automatic Annotation of a Swedish Training Corpus 2.1 Training an English Semantic Role Labeler We selected the 150 most frequent frames in FrameNet and applied the Collins parser (Collins, 1999) to the example sentences for these frames. We built a conventional FrameNet parser for English using 100,000 of these sentences as a training set and 8,000 as a development set. The classifiers were based on Support Vector Machines that we trained using LIBSVM (Chang and Lin, 2001) with the Gaussian kernel. When testing the system, we did not assume that the frame was known a priori. We used the available semantic roles for all senses of the target word as features for the classifier. On a test set from FrameNet, we estimated that the system had a precision of 0.71 and a recall of 0.65 using a strict scoring method. The result is slightly lower than the best systems at Senseval3 (Litkowski, 2004), possibly because we used a larger set of frames, and we did not assume that the frame was known a priori. 2.2 Transferring the Annotation We produced a Swedish-language corpus</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="5782" citStr="Collins, 1999" startWordPosition="903" endWordPosition="904">se trees in the source and target language, is very beneficial for the process of FrameNet annotation transfer. A rather different method to construct bilingual semantic role annotation is the approach taken by BiFrameNet (Fung and Chen, 2004). In that work, annotated structures in a new language (in that case Chinese) are produced by mining for similar structures rather than projecting them via parallel corpora. 2 Automatic Annotation of a Swedish Training Corpus 2.1 Training an English Semantic Role Labeler We selected the 150 most frequent frames in FrameNet and applied the Collins parser (Collins, 1999) to the example sentences for these frames. We built a conventional FrameNet parser for English using 100,000 of these sentences as a training set and 8,000 as a development set. The classifiers were based on Support Vector Machines that we trained using LIBSVM (Chang and Lin, 2001) with the Gaussian kernel. When testing the system, we did not assume that the frame was known a priori. We used the available semantic roles for all senses of the target word as features for the classifier. On a test set from FrameNet, we estimated that the system had a precision of 0.71 and a recall of 0.65 using </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael J. Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frame semantics and the nature of language. Annals of the New York Academy</title>
<date>1976</date>
<booktitle>of Sciences: Conference on the Origin and Development of Language,</booktitle>
<pages>280--20</pages>
<contexts>
<context position="2334" citStr="Fillmore, 1976" startWordPosition="365" endWordPosition="366">o existing training corpus available — no FrameNet-annotated Swedish corpus of substantial size exists — we used an English-Swedish parallel corpus whose English part was annotated with semantic roles using the FrameNet annotation scheme. We then applied a cross-language transfer to derive an annotated Swedish part. To evaluate the performance of the Swedish SRL system, we applied it to a small portion of the FrameNet example corpus that we translated manually. 1.1 FrameNet: an Introduction FrameNet (Baker et al., 1998) is a lexical database that describes English words using Frame Semantics (Fillmore, 1976). In this framework, predicates (or in FrameNet terminology, target words) and their arguments are linked by means of semantic frames. A frame can intuitively be thought of as a template that defines a set of slots, frame elements (FEs), that represent parts of the conceptual structure and typically correspond to prototypical participants or properties. Figure 1 shows an example sentence annotated with FrameNet information. In this example, the target word statements belongs to (“evokes”) the frame STATEMENT. Two constituents that fill slots of the frame (SPEAKER and TOPIC) are annotated as we</context>
</contexts>
<marker>Fillmore, 1976</marker>
<rawString>Charles J. Fillmore. 1976. Frame semantics and the nature of language. Annals of the New York Academy of Sciences: Conference on the Origin and Development of Language, 280:20–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Benfeng Chen</author>
</authors>
<title>BiFrameNet: Bilingual frame semantics resource construction by cross-lingual induction.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-2004.</booktitle>
<contexts>
<context position="5411" citStr="Fung and Chen, 2004" startWordPosition="843" endWordPosition="846"> of FrameNet annotation, there have been a few projects that have studied transfer methods and evaluated the quality of the automatically produced corpus. Johansson and Nugues (2005) applied the wordbased methods of Yarowsky et al. (2001) and obtained promising results. Another recent effort (Padó and Lapata, 2005) demonstrates that deeper linguistic information, such as parse trees in the source and target language, is very beneficial for the process of FrameNet annotation transfer. A rather different method to construct bilingual semantic role annotation is the approach taken by BiFrameNet (Fung and Chen, 2004). In that work, annotated structures in a new language (in that case Chinese) are produced by mining for similar structures rather than projecting them via parallel corpora. 2 Automatic Annotation of a Swedish Training Corpus 2.1 Training an English Semantic Role Labeler We selected the 150 most frequent frames in FrameNet and applied the Collins parser (Collins, 1999) to the example sentences for these frames. We built a conventional FrameNet parser for English using 100,000 of these sentences as a training set and 8,000 as a development set. The classifiers were based on Support Vector Machi</context>
</contexts>
<marker>Fung, Chen, 2004</marker>
<rawString>Pascale Fung and Benfeng Chen. 2004. BiFrameNet: Bilingual frame semantics resource construction by cross-lingual induction. In Proceedings of COLING-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1287" citStr="Gildea and Jurafsky, 2002" startWordPosition="197" endWordPosition="200">y, and obtained an accuracy score of 0.75 on the classification of presegmented frame elements, and precision and recall scores of 0.67 and 0.47 for the complete task. 1 Introduction Semantic role labeling (SRL), the process of automatically identifying arguments of a predicate in a sentence and assigning them semantic roles, has received much attention during the recent years. SRL systems have been used in a number of projects in Information Extraction and Question Answering, and are believed to be applicable in other domains as well. Building SRL systems for English has been studied widely (Gildea and Jurafsky, 2002; Litkowski, 2004), inter alia. However, all these works rely on corpora that have been produced at the cost of a large effort by human annotators. For instance, the current FrameNet corpus (Baker et al., 1998) consists of 130,000 manually annotated sentences. For smaller languages such as Swedish, such corpora are not available. In this work, we describe a FrameNet-based semantic role labeler for Swedish text. Since there was no existing training corpus available — no FrameNet-annotated Swedish corpus of substantial size exists — we used an English-Swedish parallel corpus whose English part w</context>
<context position="20368" citStr="Gildea and Jurafsky, 2002" startWordPosition="3399" endWordPosition="3402">al] ] . . . Additional END inserted END 440 Figure 6: Illustration of the globally optimized start-end method. GSE OSE SRC Target lemma + + + Target POS + + + Voice + + + Allowed role labels + + + Position + + + Head word (HW) + + + Head POS + + + Phrase/chunk type (PT) + + + HW/POS/PT, f2 chunk window + + - Dep-tree &amp; shallow path target + + + Starting paths target + + - Ending paths target + + - Path start + - - Table 1: Features used by the classifiers. 3.2.1 Conventional Features Most of the features that we use have been used by almost every system since the first well-known description (Gildea and Jurafsky, 2002). The following of them are used by all classifiers: • Target word (predicate) lemma and POS • Voice (when the target word is a verb) • Position (before or after the target) • Headword and POS • Phrase or chunk type In addition, all classifiers use the set of allowed semantic role labels as a set of boolean features. This is needed to constrain the output to a label that is allowed by FrameNet for the current frame. In addition, this feature has proven useful for the FE bracketing classifiers to distinguish between event-type and object-type frames. For event-type frames, dependencies are ofte</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Using parallel corpora for automatic transfer of FrameNet annotation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 1st ROMANCE FrameNet Workshop,</booktitle>
<location>Cluj-Napoca, Romania,</location>
<contexts>
<context position="4973" citStr="Johansson and Nugues (2005)" startWordPosition="775" endWordPosition="779"> for most languages other than English, a wide range of methods have been proposed to reduce the need for manual annotation. Many of these have relied on existing resources for English and a transfer method based on word alignment in a parallel corpus to automatically create an annotated corpus in a new language. Although these data are typically quite noisy, they have been used to train automatic systems. For the particular case of transfer of FrameNet annotation, there have been a few projects that have studied transfer methods and evaluated the quality of the automatically produced corpus. Johansson and Nugues (2005) applied the wordbased methods of Yarowsky et al. (2001) and obtained promising results. Another recent effort (Padó and Lapata, 2005) demonstrates that deeper linguistic information, such as parse trees in the source and target language, is very beneficial for the process of FrameNet annotation transfer. A rather different method to construct bilingual semantic role annotation is the approach taken by BiFrameNet (Fung and Chen, 2004). In that work, annotated structures in a new language (in that case Chinese) are produced by mining for similar structures rather than projecting them via parall</context>
<context position="27384" citStr="Johansson and Nugues, 2005" startWordPosition="4572" endWordPosition="4575">e Accuracy 0.29 f 0.073 Table 3: Results on the Swedish test set with approximate 95% confidence intervals. Although the performance figures are better than the baselines, they are still lower than for most English systems (although higher than some of the systems at Senseval-3). We believe that the main reason for the performance is the quality of the data that were used to train the system, since the results are consistent with the hypothesis that the quality of the transferred data was roughly equal to the performance of the English system multiplied by the figures for the transfer method (Johansson and Nugues, 2005). In that experiment, the transfer method had a precision of 0.84, a recall of 0.81, and an F-measure of 0.82. If we assume that the transfer performance is similar for Swedish, we arrive at a precision of 0.71 · 0.84 = 0.60, a recall of 0.65 · 0.81 = 0.53, 1This is possible because MALTPARSER produces projective trees, i.e. the words in each subtree form a contiguous substring of the sentence. 442 and an F-measure of 0.56. For the F-measure, 0.55 for the system and 0.56 for the product, the figures match closely. For the precision, the system performance (0.67) is significantly higher than th</context>
</contexts>
<marker>Johansson, Nugues, 2005</marker>
<rawString>Richard Johansson and Pierre Nugues. 2005. Using parallel corpora for automatic transfer of FrameNet annotation. In Proceedings of the 1st ROMANCE FrameNet Workshop, Cluj-Napoca, Romania, 26-28 July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit</booktitle>
<contexts>
<context position="6774" citStr="Koehn, 2005" startWordPosition="1079" endWordPosition="1080">priori. We used the available semantic roles for all senses of the target word as features for the classifier. On a test set from FrameNet, we estimated that the system had a precision of 0.71 and a recall of 0.65 using a strict scoring method. The result is slightly lower than the best systems at Senseval3 (Litkowski, 2004), possibly because we used a larger set of frames, and we did not assume that the frame was known a priori. 2.2 Transferring the Annotation We produced a Swedish-language corpus annotated with FrameNet information by applying the SRL system to the English side of Europarl (Koehn, 2005), which is a parallel corpus that is derived from the proceedings of the European Parliament. We projected the bracketing of the target words and the frame elements onto the Swedish side of the corpus by using the Giza++ word aligner (Och and Ney, 2003). Each word on the English side was mapped by the aligner onto a (possibly empty) set of words on the Swedish side. We used the maximal span method to infer the bracketing on the Swedish side, which means that the span of a projected entity was set to the range from the leftmost projected token to the rightmost. Figure 2 shows an example of this</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krzysztof Kuchcinski</author>
</authors>
<title>Constraints-driven scheduling and resource assignment.</title>
<date>2003</date>
<journal>ACM Transactions on Design Automation of Electronic Systems,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="18422" citStr="Kuchcinski, 2003" startWordPosition="3040" endWordPosition="3041">re 5: Illustration of the greedy start-end method. the probability of an FE starting (or ending, respectively) at that chunk. The probabilities are estimated using the built-in sigmoid fitting methods of LIBSVM. Making the somewhat unrealistic assumption of independence of the brackets, the global probability score to maximize is defined as the product of all start and end probabilities. We added a set of constraints to ensure that the segmentation conforms to the FrameNet annotation standard. The constrained optimization problem is then solved using the JACOP finite domain constraint solver (Kuchcinski, 2003). We believe that an n-best beam search method would produce similar results. The pseudocode for the method can be seen in Algorithm 2. The definitions of the predicates no-nesting and no-crossing, which should be obvious, are omitted. Algorithm 2 Globally Optimized Bracketing Input: A list L of chunks and a target word t Probability models ˆPstarts and ˆPends Output: The sets Smax and Emax of start and end brackets legal(S, E) ← |S |= |E| ∧ max(E) &gt; max(S) ∧ min(S) &lt; min(E) ∧ Ino-nesting(S, E) ∧ no-crossing(t, S, E) IL I score(S, E) ← j j�7cES Pstarts(c) · LEL\S(1 − Pstarts(c)) ILEE Pends (c)</context>
</contexts>
<marker>Kuchcinski, 2003</marker>
<rawString>Krzysztof Kuchcinski. 2003. Constraints-driven scheduling and resource assignment. ACM Transactions on Design Automation of Electronic Systems, 8(3):355–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Litkowski</author>
</authors>
<title>Senseval-3 task: Automatic labeling of semantic roles.</title>
<date>2004</date>
<booktitle>In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>9--12</pages>
<contexts>
<context position="1305" citStr="Litkowski, 2004" startWordPosition="201" endWordPosition="202"> score of 0.75 on the classification of presegmented frame elements, and precision and recall scores of 0.67 and 0.47 for the complete task. 1 Introduction Semantic role labeling (SRL), the process of automatically identifying arguments of a predicate in a sentence and assigning them semantic roles, has received much attention during the recent years. SRL systems have been used in a number of projects in Information Extraction and Question Answering, and are believed to be applicable in other domains as well. Building SRL systems for English has been studied widely (Gildea and Jurafsky, 2002; Litkowski, 2004), inter alia. However, all these works rely on corpora that have been produced at the cost of a large effort by human annotators. For instance, the current FrameNet corpus (Baker et al., 1998) consists of 130,000 manually annotated sentences. For smaller languages such as Swedish, such corpora are not available. In this work, we describe a FrameNet-based semantic role labeler for Swedish text. Since there was no existing training corpus available — no FrameNet-annotated Swedish corpus of substantial size exists — we used an English-Swedish parallel corpus whose English part was annotated with </context>
<context position="6488" citStr="Litkowski, 2004" startWordPosition="1031" endWordPosition="1032">glish using 100,000 of these sentences as a training set and 8,000 as a development set. The classifiers were based on Support Vector Machines that we trained using LIBSVM (Chang and Lin, 2001) with the Gaussian kernel. When testing the system, we did not assume that the frame was known a priori. We used the available semantic roles for all senses of the target word as features for the classifier. On a test set from FrameNet, we estimated that the system had a precision of 0.71 and a recall of 0.65 using a strict scoring method. The result is slightly lower than the best systems at Senseval3 (Litkowski, 2004), possibly because we used a larger set of frames, and we did not assume that the frame was known a priori. 2.2 Transferring the Annotation We produced a Swedish-language corpus annotated with FrameNet information by applying the SRL system to the English side of Europarl (Koehn, 2005), which is a parallel corpus that is derived from the proceedings of the European Parliament. We projected the bracketing of the target words and the frame elements onto the Swedish side of the corpus by using the Giza++ word aligner (Och and Ney, 2003). Each word on the English side was mapped by the aligner ont</context>
<context position="25683" citStr="Litkowski, 2004" startWordPosition="4295" endWordPosition="4296">he full task, classification accuracy of pre-segmented arguments, precision and recall for the bracketing task, full task precision and recall using the Senseval-3 scoring metrics, and finally the proportion of full sentences whose FEs were correctly bracketed and classified. The Senseval-3 method uses a more lenient scoring scheme that counts a FE as correctly identified if it overlaps with the gold standard FE and has the correct label. Although the strict measures are more interesting, we include these figures for comparison with the systems participating in the Senseval-3 Restricted task (Litkowski, 2004). We include baseline scores for the argument bracketing and classification tasks, respectively. The bracketing baseline method considers nonpunctuation subtrees dependent of the target word. When the target word is a verb, the baseline puts FE brackets around the words included in each of these subtrees1. When the target is a noun, we also bracket the target word token itself, and when it is an adjective, we additionally bracket its parent token. As a baseline for the argument classification task, every argument is assigned the most frequent semantic role in the frame. As can be seen from the</context>
</contexts>
<marker>Litkowski, 2004</marker>
<rawString>Ken Litkowski. 2004. Senseval-3 task: Automatic labeling of semantic roles. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 9–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="12991" citStr="Nivre et al., 2004" startWordPosition="2131" endWordPosition="2134">rained a SRL system for Swedish text. Like most previous systems, it consists of two parts: a FE bracketer and a classifier that assigns semantic roles to FEs. Both parts are implemented as SVM classifiers trained using LIBSVM. The semantic role classifier is rather conventional and is not described in this paper. To construct the features used by the classifiers, we used the following tools: • An HMM-based POS tagger, • A rule-based chunker, • A rule-based time expression detector, • Two clause identifiers, of which one is rulebased and one is statistical, • The MALTPARSER dependency parser (Nivre et al., 2004), trained on a 100,000- word Swedish treebank. We constructed shallow parse trees using the clause trees and the chunks. Dependency and shallow parse trees for a fragment of a sentence from our test corpus are shown in Figures 3 and 4, respectively. This sentence, which was translated from an English sentence that read the doctor was answering an emergency phone call, comes from the English FrameNet example corpus. PR doktorn svarade på ett larmsamtal Figure 3: Example dependency parse tree. [ [ doktorn]NG_nom [ svarade ]VG_fin [ på] PP [ ett larmsamtal ]NG_nom] Clause Figure 4: Example shallo</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Proceedings of CoNLL-2004, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="7027" citStr="Och and Ney, 2003" startWordPosition="1122" endWordPosition="1125">e result is slightly lower than the best systems at Senseval3 (Litkowski, 2004), possibly because we used a larger set of frames, and we did not assume that the frame was known a priori. 2.2 Transferring the Annotation We produced a Swedish-language corpus annotated with FrameNet information by applying the SRL system to the English side of Europarl (Koehn, 2005), which is a parallel corpus that is derived from the proceedings of the European Parliament. We projected the bracketing of the target words and the frame elements onto the Swedish side of the corpus by using the Giza++ word aligner (Och and Ney, 2003). Each word on the English side was mapped by the aligner onto a (possibly empty) set of words on the Swedish side. We used the maximal span method to infer the bracketing on the Swedish side, which means that the span of a projected entity was set to the range from the leftmost projected token to the rightmost. Figure 2 shows an example of this process. To make the brackets conform to the FrameNet annotation practices, we applied a small set of heuristics. The FrameNet conventions specify that linking words such as prepositions and subordinat437 [We] wanted to [our perplexity as regards these</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Padó</author>
<author>Mirella Lapata</author>
</authors>
<title>Crosslingual projection of role-semantic information.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<contexts>
<context position="5107" citStr="Padó and Lapata, 2005" startWordPosition="798" endWordPosition="801">have relied on existing resources for English and a transfer method based on word alignment in a parallel corpus to automatically create an annotated corpus in a new language. Although these data are typically quite noisy, they have been used to train automatic systems. For the particular case of transfer of FrameNet annotation, there have been a few projects that have studied transfer methods and evaluated the quality of the automatically produced corpus. Johansson and Nugues (2005) applied the wordbased methods of Yarowsky et al. (2001) and obtained promising results. Another recent effort (Padó and Lapata, 2005) demonstrates that deeper linguistic information, such as parse trees in the source and target language, is very beneficial for the process of FrameNet annotation transfer. A rather different method to construct bilingual semantic role annotation is the approach taken by BiFrameNet (Fung and Chen, 2004). In that work, annotated structures in a new language (in that case Chinese) are produced by mining for similar structures rather than projecting them via parallel corpora. 2 Automatic Annotation of a Swedish Training Corpus 2.1 Training an English Semantic Role Labeler We selected the 150 most</context>
<context position="12007" citStr="Padó and Lapata (2005)" startWordPosition="1967" endWordPosition="1970">8 In addition, we made the (obviously simplistic) assumption that the contiguous entities we project are also contiguous on the target side. These assumptions may all be put into question. Above all, the second assumption will fail in many cases because the translations are not literal, which means that the sentences in the pair may express slightly different information. The third assumption may be invalid if the information expressed is realized by radically different constructions, which means that an argument may belong to another predicate or change its semantic role on the Swedish side. Padó and Lapata (2005) avoid this problem by using heuristics based on a target-language FrameNet to select sentences that are close in meaning. Since we have no such resource to rely on, we are forced to accept that this problem introduces a certain amount of noise into the automatically annotated corpus. 3 Training a Swedish SRL System Using the transferred FrameNet annotation, we trained a SRL system for Swedish text. Like most previous systems, it consists of two parts: a FE bracketer and a classifier that assigns semantic roles to FEs. Both parts are implemented as SVM classifiers trained using LIBSVM. The sem</context>
</contexts>
<marker>Padó, Lapata, 2005</marker>
<rawString>Sebastian Padó and Mirella Lapata. 2005. Crosslingual projection of role-semantic information. In Proceedings of HLT/EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<volume>60</volume>
<issue>1</issue>
<pages>39</pages>
<contexts>
<context position="13854" citStr="Pradhan et al. (2005" startWordPosition="2273" endWordPosition="2276">ively. This sentence, which was translated from an English sentence that read the doctor was answering an emergency phone call, comes from the English FrameNet example corpus. PR doktorn svarade på ett larmsamtal Figure 3: Example dependency parse tree. [ [ doktorn]NG_nom [ svarade ]VG_fin [ på] PP [ ett larmsamtal ]NG_nom] Clause Figure 4: Example shallow parse tree. 3.1 Frame Element Bracketing Methods We created two redundancy-based FE bracketing algorithms based on binary classification of chunks as starting or ending the FE. This is somewhat similar to the chunk-based system described by Pradhan et al. (2005a), which uses a segmentation strategy based on IOB2 bracketing. However, our system still exploits the dependency parse tree during classification. We first tried the conventional approach to the problem of FE bracketing: applying a parser to the sentence, and classifying each node in the parse tree as being an FE or not. We used a dependency parser since there is no constituent-based parser available for Swedish. This proved unsuccessful because the spans of the dependency subtrees frequently were incompatible with the spans defined by the FrameNet annotations. This was especially the case f</context>
<context position="15415" citStr="Pradhan et al. (2005" startWordPosition="2531" endWordPosition="2534">ccessful, and we left this to future work. We believe that the methods we developed are more suitable in our case, since they base their decisions on several parse trees (in our case, two clause-chunk trees and one dependency tree). This redundancy is valuable because the dependency parsing model was trained on a treebank of just 100,000 words, which makes it less robust than Collins’ or Charniak’s parsers for English. In addition, the methods do not implicitly rely on the common assumption that every FE has a counterpart in a parse tree. Recent work in semantic role labeling, see for example Pradhan et al. (2005b), has focused on combining the results of SRL systems based on different types of syntax. Still, all SUB ADV DET 439 systems exploiting recursive parse trees are based on binary classification of nodes as being an argument or not. The training sets used to train the final classifiers consisted of one million training instances for the start classifier, 500,000 for the end classifier, and 272,000 for the role classifier. The features used by the classifiers are described in Subsection 3.2, and the performance of the two FE bracketing algorithms compared in Subsection 4.2. 3.1.1 Greedy start-e</context>
<context position="21884" citStr="Pradhan et al. (2005" startWordPosition="3659" endWordPosition="3662">ghtly different values for the bracketing case and the role assignment case: for bracketing, the value of this feature is simply the type of the current chunk; for classification, it is the type of the largest chunk or clause that starts at the leftmost token of the FE. For prepositional phrases, the preposition is attached to the phrase type (for example, the second FE in the example fragment starts with the preposition på ‘at/on’, which causes the value of the phrase type feature to be PP-på). 3.2.2 Chunk Context Features Similarly to the chunk-based PropBank argument bracketer described by Pradhan et al. (2005a), the start-end methods use the head word, head POS, and chunk type of chunks in a window of size 2 on both sides of the current chunk to classify it as being the start or end of an FE. 3.2.3 Parse Tree Path Features Parse tree path features have been shown to be very important for argument bracketing in several studies. All classifiers used here use a set of such features: • Dependency tree path from the head to the target word. In the example text, the first chunk (consisting of the word doktorn), has the value SUB-T for this feature. This means that to go from the head of the chunk to the</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James Martin, and Dan Jurafsky. 2005a. Support vector learning for semantic argument classification. Machine Learning, 60(1):11– 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Semantic role labeling using different syntactic views.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005.</booktitle>
<contexts>
<context position="13854" citStr="Pradhan et al. (2005" startWordPosition="2273" endWordPosition="2276">ively. This sentence, which was translated from an English sentence that read the doctor was answering an emergency phone call, comes from the English FrameNet example corpus. PR doktorn svarade på ett larmsamtal Figure 3: Example dependency parse tree. [ [ doktorn]NG_nom [ svarade ]VG_fin [ på] PP [ ett larmsamtal ]NG_nom] Clause Figure 4: Example shallow parse tree. 3.1 Frame Element Bracketing Methods We created two redundancy-based FE bracketing algorithms based on binary classification of chunks as starting or ending the FE. This is somewhat similar to the chunk-based system described by Pradhan et al. (2005a), which uses a segmentation strategy based on IOB2 bracketing. However, our system still exploits the dependency parse tree during classification. We first tried the conventional approach to the problem of FE bracketing: applying a parser to the sentence, and classifying each node in the parse tree as being an FE or not. We used a dependency parser since there is no constituent-based parser available for Swedish. This proved unsuccessful because the spans of the dependency subtrees frequently were incompatible with the spans defined by the FrameNet annotations. This was especially the case f</context>
<context position="15415" citStr="Pradhan et al. (2005" startWordPosition="2531" endWordPosition="2534">ccessful, and we left this to future work. We believe that the methods we developed are more suitable in our case, since they base their decisions on several parse trees (in our case, two clause-chunk trees and one dependency tree). This redundancy is valuable because the dependency parsing model was trained on a treebank of just 100,000 words, which makes it less robust than Collins’ or Charniak’s parsers for English. In addition, the methods do not implicitly rely on the common assumption that every FE has a counterpart in a parse tree. Recent work in semantic role labeling, see for example Pradhan et al. (2005b), has focused on combining the results of SRL systems based on different types of syntax. Still, all SUB ADV DET 439 systems exploiting recursive parse trees are based on binary classification of nodes as being an argument or not. The training sets used to train the final classifiers consisted of one million training instances for the start classifier, 500,000 for the end classifier, and 272,000 for the role classifier. The features used by the classifiers are described in Subsection 3.2, and the performance of the two FE bracketing algorithms compared in Subsection 4.2. 3.1.1 Greedy start-e</context>
<context position="21884" citStr="Pradhan et al. (2005" startWordPosition="3659" endWordPosition="3662">ghtly different values for the bracketing case and the role assignment case: for bracketing, the value of this feature is simply the type of the current chunk; for classification, it is the type of the largest chunk or clause that starts at the leftmost token of the FE. For prepositional phrases, the preposition is attached to the phrase type (for example, the second FE in the example fragment starts with the preposition på ‘at/on’, which causes the value of the phrase type feature to be PP-på). 3.2.2 Chunk Context Features Similarly to the chunk-based PropBank argument bracketer described by Pradhan et al. (2005a), the start-end methods use the head word, head POS, and chunk type of chunks in a window of size 2 on both sides of the current chunk to classify it as being the start or end of an FE. 3.2.3 Parse Tree Path Features Parse tree path features have been shown to be very important for argument bracketing in several studies. All classifiers used here use a set of such features: • Dependency tree path from the head to the target word. In the example text, the first chunk (consisting of the word doktorn), has the value SUB-T for this feature. This means that to go from the head of the chunk to the</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Daniel Jurafsky. 2005b. Semantic role labeling using different syntactic views. In Proceedings of ACL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora. In</title>
<date>2001</date>
<booktitle>Proceedings of HLT</booktitle>
<contexts>
<context position="5029" citStr="Yarowsky et al. (2001)" startWordPosition="786" endWordPosition="789">ds have been proposed to reduce the need for manual annotation. Many of these have relied on existing resources for English and a transfer method based on word alignment in a parallel corpus to automatically create an annotated corpus in a new language. Although these data are typically quite noisy, they have been used to train automatic systems. For the particular case of transfer of FrameNet annotation, there have been a few projects that have studied transfer methods and evaluated the quality of the automatically produced corpus. Johansson and Nugues (2005) applied the wordbased methods of Yarowsky et al. (2001) and obtained promising results. Another recent effort (Padó and Lapata, 2005) demonstrates that deeper linguistic information, such as parse trees in the source and target language, is very beneficial for the process of FrameNet annotation transfer. A rather different method to construct bilingual semantic role annotation is the approach taken by BiFrameNet (Fung and Chen, 2004). In that work, annotated structures in a new language (in that case Chinese) are produced by mining for similar structures rather than projecting them via parallel corpora. 2 Automatic Annotation of a Swedish Training</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of HLT 2001.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>