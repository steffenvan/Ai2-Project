<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000192">
<title confidence="0.9425725">
Reducing Approximation and Estimation Errors for Chinese Lexical
Processing with Heterogeneous Annotations
</title>
<author confidence="0.981703">
Weiwei Sun† and Xiaojun Want *
</author>
<affiliation confidence="0.98901">
†tInstitute of Computer Science and Technology, Peking University
†Saarbr¨ucken Graduate School of Computer Science
†Department of Computational Linguistics, Saarland University
†Language Technology Lab, DFKI GmbH
</affiliation>
<email confidence="0.99732">
{ws,wanxiaojun}@pku.edu.cn
</email>
<sectionHeader confidence="0.997369" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999893789473684">
We address the issue of consuming heteroge-
neous annotation data for Chinese word seg-
mentation and part-of-speech tagging. We em-
pirically analyze the diversity between two
representative corpora, i.e. Penn Chinese
Treebank (CTB) and PKU’s People’s Daily
(PPD), on manually mapped data, and show
that their linguistic annotations are systemat-
ically different and highly compatible. The
analysis is further exploited to improve pro-
cessing accuracy by (1) integrating systems
that are respectively trained on heterogeneous
annotations to reduce the approximation error,
and (2) re-training models with high quality
automatically converted data to reduce the es-
timation error. Evaluation on the CTB and
PPD data shows that our novel model achieves
a relative error reduction of 11% over the best
reported result in the literature.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99879302">
A majority of data-driven NLP systems rely on
large-scale, manually annotated corpora that are im-
portant to train statistical models but very expensive
to build. Nowadays, for many tasks, multiple het-
erogeneous annotated corpora have been built and
publicly available. For example, the Penn Treebank
is popular to train PCFG-based parsers, while the
Redwoods Treebank is well known for HPSG re-
search; the Propbank is favored to build general se-
mantic role labeling systems, while the FrameNet is
attractive for predicate-specific labeling. The anno-
This work is mainly finished when the first author was
in Saarland University and DFKI. Both authors are the corre-
sponding authors.
tation schemes in different projects are usually dif-
ferent, since the underlying linguistic theories vary
and have different ways to explain the same lan-
guage phenomena. Though statistical NLP systems
usually are not bound to specific annotation stan-
dards, almost all of them assume homogeneous an-
notation in the training corpus. The co-existence of
heterogeneous annotation data therefore presents a
new challenge to the consumers of such resources.
There are two essential characteristics of hetero-
geneous annotations that can be utilized to reduce
two main types of errors in statistical NLP, i.e. the
approximation error that is due to the intrinsic sub-
optimality of a model and the estimation error that is
due to having only finite training data. First, hetero-
geneous annotations are (similar but) different as a
result of different annotation schemata. Systems re-
spectively trained on heterogeneous annotation data
can produce different but relevant linguistic analy-
sis. This suggests that complementary features from
heterogeneous analysis can be derived for disam-
biguation, and therefore the approximation error can
be reduced. Second, heterogeneous annotations are
(different but) similar because their linguistic analy-
sis is highly correlated. This implies that appropriate
conversions between heterogeneous corpora could
be reasonably accurate, and therefore the estimation
error can be reduced by reason of the increase of re-
liable training data.
This paper explores heterogeneous annotations
to reduce both approximation and estimation errors
for Chinese word segmentation and part-of-speech
(POS) tagging, which are fundamental steps for
more advanced Chinese language processing tasks.
We empirically analyze the diversity between two
representative popular heterogeneous corpora, i.e.
</bodyText>
<page confidence="0.914953">
232
</page>
<note confidence="0.9862345">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 232–241,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.995881421052632">
Penn Chinese Treebank (CTB) and PKU’s People’s
Daily (PPD). To that end, we manually label 200
sentences from CTB with PPD-style annotations.1
Our analysis confirms the aforementioned two prop-
erties of heterogeneous annotations. Inspired by
the sub-word tagging method introduced in (Sun,
2011), we propose a structure-based stacking model
to fully utilize heterogeneous word structures to re-
duce the approximation error. In particular, joint
word segmentation and POS tagging is addressed
as a two step process. First, character-based tag-
gers are respectively trained on heterogeneous an-
notations to produce multiple analysis. The outputs
of these taggers are then merged into sub-word se-
quences, which are further re-segmented and tagged
by a sub-word tagger. The sub-word tagger is de-
signed to refine the tagging result with the help of
heterogeneous annotations. To reduce the estima-
tion error, we employ a learning-based approach to
convert complementary heterogeneous data to in-
crease labeled training data for the target task. Both
the character-based tagger and the sub-word tagger
can be refined by re-training with automatically con-
verted data.
We conduct experiments on the CTB and PPD
data, and compare our system with state-of-the-
art systems. Our structure-based stacking model
achieves an f-score of 94.36, which is superior to
a feature-based stacking model introduced in (Jiang
et al., 2009). The converted data can also enhance
the baseline model. A simple character-based model
can be improved from 93.41 to 94.11. Since the
two treatments are concerned with reducing differ-
ent types of errors and thus not fully overlapping, the
combination of them gives a further improvement.
Our final system achieves an f-score of 94.68, which
yields a relative error reduction of 11% over the best
published result (94.02).
</bodyText>
<sectionHeader confidence="0.8715915" genericHeader="introduction">
2 Joint Chinese Word Segmentation and
POS Tagging
</sectionHeader>
<bodyText confidence="0.988305">
Different from English and other Western languages,
Chinese is written without explicit word delimiters
such as space characters. To find and classify the
</bodyText>
<footnote confidence="0.669428">
1The first 200 sentences of the development data for experi-
ments are selected. This data set is submitted as a supplemental
material for research purposes.
</footnote>
<bodyText confidence="0.999947875">
basic language units, i.e. words, word segmentation
and POS tagging are important initial steps for Chi-
nese language processing. Supervised learning with
specifically defined training data has become a dom-
inant paradigm. Joint approaches that resolve the
two tasks simultaneously have received much atten-
tion in recent research. Previous work has shown
that joint solutions led to accuracy improvements
over pipelined systems by avoiding segmentation er-
ror propagation and exploiting POS information to
help segmentation (Ng and Low, 2004; Jiang et al.,
2008a; Zhang and Clark, 2008; Sun, 2011).
Two kinds of approaches are popular for joint
word segmentation and POS tagging. The first is the
“character-based” approach, where basic processing
units are characters which compose words (Jiang et
al., 2008a). In this kind of approach, the task is for-
mulated as the classification of characters into POS
tags with boundary information. For example, the
label B-NN indicates that a character is located at the
begging of a noun. Using this method, POS infor-
mation is allowed to interact with segmentation. The
second kind of solution is the “word-based” method,
also known as semi-Markov tagging (Zhang and
Clark, 2008; Zhang and Clark, 2010), where the ba-
sic predicting units are words themselves. This kind
of solver sequentially decides whether the local se-
quence of characters makes up a word as well as its
possible POS tag. Solvers may use previously pre-
dicted words and their POS information as clues to
process a new word.
In addition, we proposed an effective and efficient
stacked sub-word tagging model, which combines
strengths of both character-based and word-based
approaches (Sun, 2011). First, different character-
based and word-based models are trained to produce
multiple segmentation and tagging results. Sec-
ond, the outputs of these coarse-grained models are
merged into sub-word sequences, which are fur-
ther bracketed and labeled with POS tags by a fine-
grained sub-word tagger. Their solution can be
viewed as utilizing stacked learning to integrate het-
erogeneous models.
Supervised segmentation and tagging can be im-
proved by exploiting rich linguistic resources. Jiang
et al. (2009) presented a preliminary study for an-
notation ensemble, which motivates our research as
well as similar investigations for other NLP tasks,
</bodyText>
<page confidence="0.997142">
233
</page>
<bodyText confidence="0.999919153846154">
e.g. parsing (Niu et al., 2009; Sun et al., 2010). In
their solution, heterogeneous data is used to train an
auxiliary segmentation and tagging system to pro-
duce informative features for target prediction. Our
previous work (Sun and Xu, 2011) and Wang et al.
(2011) explored unlabeled data to enhance strong
supervised segmenters and taggers. Both of their
work fall into the category of feature induction based
semi-supervised learning. In brief, their methods
harvest useful string knowledge from unlabeled or
automatically analyzed data, and apply the knowl-
edge to design new features for discriminative learn-
ing.
</bodyText>
<sectionHeader confidence="0.993631" genericHeader="method">
3 About Heterogeneous Annotations
</sectionHeader>
<bodyText confidence="0.999918676470588">
For Chinese word segmentation and POS tag-
ging, supervised learning has become a dominant
paradigm. Much of the progress is due to the devel-
opment of both corpora and machine learning tech-
niques. Although several institutions to date have
released their segmented and POS tagged data, ac-
quiring sufficient quantities of high quality training
examples is still a major bottleneck. The annotation
schemes of existing lexical resources are different,
since the underlying linguistic theories vary. Despite
the existence of multiple resources, such data cannot
be simply put together for training systems, because
almost all of statistical NLP systems assume homo-
geneous annotation. Therefore, it is not only inter-
esting but also important to study how to fully utilize
heterogeneous resources to improve Chinese lexical
processing.
There are two main types of errors in statistical
NLP: (1) the approximation error that is due to the
intrinsic suboptimality of a model and (2) the esti-
mation error that is due to having only finite train-
ing data. Take Chinese word segmentation for ex-
ample. Our previous analysis (Sun, 2010) shows
that one main intrinsic disadvantage of character-
based model is the difficulty in incorporating the
whole word information, while one main disadvan-
tage of word-based model is the weak ability to ex-
press word formation. In both models, the signifi-
cant decrease of the prediction accuracy of out-of-
vocabulary (OOV) words indicates the impact of the
estimation error. The two essential characteristics
about systematic diversity of heterogeneous annota-
tions can be utilized to reduce both approximation
and estimation errors.
</bodyText>
<subsectionHeader confidence="0.999487">
3.1 Analysis of the CTB and PPD Standards
</subsectionHeader>
<bodyText confidence="0.999989317073171">
This paper focuses on two representative popular
corpora for Chinese lexical processing: (1) the Penn
Chinese Treebank (CTB) and (2) the PKU’s Peo-
ple’s Daily data (PPD). To analyze the diversity be-
tween their annotation standards, we pick up 200
sentences from CTB and manually label them ac-
cording to the PPD standard. Specially, we employ a
PPD-style segmentation and tagging system to auto-
matically label these 200 sentences. A linguistic ex-
pert who deeply understands the PPD standard then
manually checks the automatic analysis and corrects
its errors.
These 200 sentences are segmented as 3886 and
3882 words respectively according to the CTB and
PPD standards. The average lengths of word tokens
are almost the same. However, the word bound-
aries or the definitions of words are different. 3561
word tokens are consistently segmented by both
standards. In other words, 91.7% CTB word tokens
share the same word boundaries with 91.6% PPD
word tokens. Among these 3561 words, there are
552 punctuations that are simply consistently seg-
mented. If punctuations are filtered out to avoid
overestimation of consistency, 90.4% CTB words
have same boundaries with 90.3% PPD words. The
boundaries of words that are differently segmented
are compatible. Among all annotations, only one
cross-bracketing occurs. The statistics indicates that
the two heterogenous segmented corpora are sys-
tematically different, and confirms the aforemen-
tioned two properties of heterogeneous annotations.
Table 1 is the mapping between CTB-style tags
and PPD-style tags. For the definition and illus-
tration of these tags, please refers to the annotation
guidelines2. The statistics after colons are how many
times this POS tag pair appears among the 3561
words that are consistently segmented. From this ta-
ble, we can see that (1) there is no one-to-one map-
ping between their heterogeneous word classifica-
tion but (2) the mapping between heterogeneous tags
is not very uncertain. This simple analysis indicates
</bodyText>
<footnote confidence="0.994966666666667">
2Available at http://www.cis.upenn.edu/
˜chinese/posguide.3rd.ch.pdf and http://www.
icl.pku.edu.cn/icl_groups/corpus/spec.htm.
</footnote>
<page confidence="0.996984">
234
</page>
<bodyText confidence="0.999904363636363">
that the two POS tagged corpora also hold the two
properties of heterogeneous annotations. The dif-
ferences between the POS annotation standards are
systematic. The annotations in CTB are treebank-
driven, and thus consider more functional (dynamic)
information of basic lexical categories. The annota-
tions in PPD are lexicon-driven, and thus focus on
more static properties of words. Limited to the doc-
ument length, we only illustrate the annotation of
verbs and nouns for better understanding of the dif-
ferences.
</bodyText>
<listItem confidence="0.99802625">
• The CTB tag VV indicates common verbs that
are mainly labeled as verbs (v) too according
to the PPD standard. However, these words can
be also tagged as nominal categories (a, vn, n).
The main reason is that there are a large num-
ber of Chinese adjectives and nouns that can be
realized as predicates without linking verbs.
• The tag NN indicates common nouns in CTB.
Some of them are labeled as verbal categories
(vn, v). The main reason is that a majority of
Chinese verbs could be realized as subjects and
objects without form changes.
</listItem>
<sectionHeader confidence="0.995346" genericHeader="method">
4 Structure-based Stacking
</sectionHeader>
<subsectionHeader confidence="0.988095">
4.1 Reducing the Approximation Error via
Stacking
</subsectionHeader>
<bodyText confidence="0.999495777777778">
Each annotation data set alone can yield a predictor
that can be taken as a mechanism to produce struc-
tured texts. With different training data, we can con-
struct multiple heterogeneous systems. These sys-
tems produce similar linguistic analysis which holds
the same high level linguistic principles but differ in
details. A very simple idea to take advantage of het-
erogeneous structures is to design a predictor which
can predict a more accurate target structure based
on the input, the less accurate target structure and
complementary structures. This idea is very close
to stacked learning (Wolpert, 1992), which is well
developed for ensemble learning, and successfully
applied to some NLP tasks, e.g. dependency parsing
(Nivre and McDonald, 2008; Torres Martins et al.,
2008).
Formally speaking, our idea is to include two
“levels” of processing. The first level includes one
</bodyText>
<tableCaption confidence="0.997659">
Table 1: Mapping between CTB and PPD POS Tags.
</tableCaption>
<bodyText confidence="0.999906166666667">
or more base predictors fl, ..., fK that are indepen-
dently built on different training data. The second
level processing consists of an inference function h
that takes as input (x, fl(x),..., fK(x))3 and out-
puts a final prediction h(x, f1(x),..., fK(x)). The
only difference between model ensemble and anno-
tation ensemble is that the output spaces of model
ensemble are the same while the output spaces of an-
notation ensemble are different. This framework is
general and flexible, in the sense that it assumes al-
most nothing about the individual systems and take
them as black boxes.
</bodyText>
<subsectionHeader confidence="0.997407">
4.2 A Character-based Tagger
</subsectionHeader>
<bodyText confidence="0.999994357142857">
With IOB2 representation (Ramshaw and Marcus,
1995), the problem of joint segmentation and tag-
ging can be regarded as a character classification
task. Previous work shows that the character-based
approach is an effective method for Chinese lexical
processing. Both of our feature- and structure-based
stacking models employ base character-based tag-
gers to generate multiple segmentation and tagging
results. Our base tagger use a discriminative sequen-
tial classifier to predict the POS tag with positional
information for each character. Each character can
be assigned one of two possible boundary tags: “B”
for a character that begins a word and “I” for a char-
acter that occurs in the middle of a word. We denote
</bodyText>
<footnote confidence="0.991611">
3x is a given Chinese sentence.
</footnote>
<equation confidence="0.724970653846154">
AS u:44; CD m:134;
DEC u:83; DEV u:7;
DEG u:123; ETC u:9;
LB p:1; NT t:98;
OD m:41; PU w:552;
SP u:1; VC v:32;
VE v:13; BA p:2; d:1;
CS c:3; d:1; DT r:15; b:1;
MSP c:2; u:1; PN r:53; n:2;
CC c:73; p:5; v:2; M q:101; n:11; v:1;
LC f:51; Ng:3; v:1; u:1; P p:133; v:4; c:2; Vg:1;
VA a:57; i:4; z:2; ad:1;
b:1;
VV v:382; i:5; a:3; Vg:2;
vn:2; n:2; p:2; w:1;
AD d:149; c:11; ad:6; z:4;
a:3; v:2; n:1; r:1; m:1; f:1;
t:1;
NR ns:170; nr:65; j:23;
nt:21; nz:7; n:2; s:1;
JJ a:43; b:13; n:3; vn:3;
d:2; j:2; f:2; t:2; z:1;
NN n:738; vn:135; v:26;
j:19; Ng:5; an:5; a:3; r:3; s:3;
Ag:2; nt:2; f:2; q:2; i:1; t:1;
nz:1; b:1;
</equation>
<page confidence="0.97849">
235
</page>
<bodyText confidence="0.993229333333333">
a candidate character token ci with a fixed window
ci_2ci_1cici+1ci+2. The following features are used
for classification:
</bodyText>
<listItem confidence="0.999951">
• Character unigrams: ck (i − l &lt; k &lt; i + l)
• Character bigrams: ckck+1 (i − l &lt; k &lt; i + l)
</listItem>
<subsectionHeader confidence="0.998689">
4.3 Feature-based Stacking
</subsectionHeader>
<bodyText confidence="0.997841875">
Jiang et al. (2009) introduced a feature-based stack-
ing solution for annotation ensemble. In their so-
lution, an auxiliary tagger CTagppd is trained on a
complementary corpus, i.e. PPD, to assist the tar-
get CTB-style tagging. To refine the character-based
tagger CTagctb, PPD-style character labels are di-
rectly incorporated as new features. The stacking
model relies on the ability of discriminative learning
method to explore informative features, which play
central role to boost the tagging performance. To
compare their feature-based stacking model and our
structure-based model, we implement a similar sys-
tem CTagppd,ctb. Apart from character uni/bigram
features, the PPD-style character labels are used to
derive the following features to enhance our CTB-
style tagger:
</bodyText>
<listItem confidence="0.994935">
• Character label unigrams: cppd
</listItem>
<equation confidence="0.934376">
k (i−lppd &lt; k &lt;
i + lppd) — —
</equation>
<listItem confidence="0.911978">
• Character label bigrams: cppd
</listItem>
<equation confidence="0.963816666666667">
k cppd
k+1 (i − lppd &lt;
k &lt; i + lppd)
</equation>
<bodyText confidence="0.999764333333333">
In the above descriptions, l and lppd are the win-
dow sizes of features, which can be tuned on devel-
opment data.
</bodyText>
<subsectionHeader confidence="0.999353">
4.4 Structure-based Stacking
</subsectionHeader>
<bodyText confidence="0.999836769230769">
We propose a novel structured-based stacking model
for the task, in which heterogeneous word struc-
tures are used not only to generate features but also
to derive a sub-word structure. Our work is in-
spired by the stacked sub-word tagging model in-
troduced in (Sun, 2011). Their work is motivated
by the diversity of heterogeneous models, while
our work is motivated by the diversity of heteroge-
neous annotations. The workflow of our new sys-
tem is shown in Figure 1. In the first phase, one
character-based CTB-style tagger (CTagctb) and
one character-based PPD-style tagger (CTagppd)
are respectively trained to produce heterogenous
</bodyText>
<figureCaption confidence="0.9978055">
Figure 1: Sub-word tagging based on heterogeneous tag-
gers.
</figureCaption>
<bodyText confidence="0.998823064516129">
word boundaries. In the second phase, this system
first combines the two segmentation and tagging re-
sults to get sub-words which maximize the agree-
ment about word boundaries. Finally, a fine-grained
sub-word tagger (STagctb) is applied to bracket sub-
words into words and also to label their POS tags.
We can also apply a PPD-style sub-word tagger. To
compare with previous work, we specially concen-
trate on the PPD-to-CTB adaptation.
Following (Sun, 2011), the intermediate sub-word
structures is defined to maximize the agreement of
CTagctb and CTagppd. In other words, the goal is
to make merged sub-words as large as possible but
not overlap with any predicted word produced by
the two taggers. If the position between two con-
tinuous characters is predicted as a word boundary
by any segmenter, this position is taken as a separa-
tion position of the sub-word sequence. This strat-
egy makes sure that it is still possible to correctly
re-segment the strings of which the boundaries are
disagreed with by the heterogeneous segmenters in
the sub-word tagging stage.
To train the sub-word tagger STagctb, features
are formed making use of both CTB-style and PPD-
style POS tags provided by the character-based tag-
gers. In the following description, “C” refers to the
content of a sub-word; “Tctb” and “Tppd” refers to
the positional POS tags generated from CTagctb and
CTagppd; lC, lctb
T and lppd Tare the window sizes.
For convenience, we denote a sub-word with its con-
</bodyText>
<figure confidence="0.9982552">
Merging
Sub-word
sequences
CTB-style
sub-word tag-
ger STagctb
CTB-style character
tagger CTagctb
Segmented and
tagged sentences
Raw sentences
PPD-style character
tagger CTagppd
Segmented and
tagged sentences
</figure>
<page confidence="0.993318">
236
</page>
<bodyText confidence="0.834605">
Character-based Conversion The feature-based
stacking model CTagppd→ctb maps the input char-
acter sequence c and its PPD-style character label
sequence to the corresponding CTB-style character
label sequence. This model by itself can be taken as
a corpus conversion model to transform a PPD-style
analysis to a CTB-style analysis. By processing the
auxiliary corpus Dppd with CTagppd→ctb, we ac-
quire a new labeled data set D0ctb = DCT ag�����tb
ppd→ctb .
We can re-train the CTagctb model with both origi-
nal and converted data Dctb ∪ D0ctb.
Sub-word-based Conversion Similarly, the
structure-based stacking model can be also taken
as a corpus conversion model. By processing the
auxiliary corpus Dppd with STagctb, we acquire
s
</bodyText>
<equation confidence="0.3783045">
00
et D
</equation>
<bodyText confidence="0.9598405">
text ...si−1sisi+1..., where si is the current token.
The following features are applied:
</bodyText>
<listItem confidence="0.8322495">
• Unigram features: C(sk) (i − lC ≤ k ≤ +lC),
Tctb(sk) (i − lctb
</listItem>
<equation confidence="0.962670117647059">
T ≤ k ≤ i + lctb
T ), Tppd(sk)
(i − lTd≤ k ≤ i + lTd)
• Bigram features: C(sk)C(sk+1) (i − lC ≤ k &lt;
i + lC), Tctb(sk)Tctb(sk+1) (i − lctb
T ≤ k &lt;
i + lctb
T ), Tppd(sk)Tppd(sk+1) (i − lppd
T ≤ k &lt;
i + lppd
T
)
• C(si−1)C(si+1) (if lC ≥ 1),
Tctb(si−1)Tctb(si+1) (if lctb ≥ 1),
T
Tppd(si−1)Tppd(si+1) (if lppd
T ≥ 1)
</equation>
<bodyText confidence="0.942062272727273">
• Word formation features: character n-gram
prefixes and suffixes for n up to 3.
Cross-validation CTagctb and CTagppd are di-
rectly trained on the original training data, i.e. the
CTB and PPD data. Cross-validation technique has
been proved necessary to generate the training data
for sub-word tagging, since it deals with the train-
ing/test mismatch problem (Sun, 2011). To con-
struct training data for the new heterogeneous sub-
word tagger, a 10-fold cross-validation on the origi-
nal CTB data is performed too.
</bodyText>
<sectionHeader confidence="0.992482" genericHeader="method">
5 Data-driven Annotation Conversion
</sectionHeader>
<bodyText confidence="0.979910357142857">
It is possible to acquire high quality labeled data
for a specific annotation standard by exploring ex-
isting heterogeneous corpora, since the annotations
are normally highly compatible. Moreover, the ex-
ploitation of additional (pseudo) labeled data aims to
reduce the estimation error and enhances a NLP sys-
tem in a different way from stacking. We therefore
expect the improvements are not much overlapping
and the combination of them can give a further im-
provement.
The stacking models can be viewed as annota-
tion converters: They take as input complementary
structures and produce as output target structures.
In other words, the stacking models actually learn
statistical models to transform the lexical represen-
tations. We can acquire informative extra samples
by processing the PPD data with our stacking mod-
els. Though the converted annotations are imperfect,
they are still helpful to reduce the estimation error.
a new labeled data
re-train the STagctb model with Dctb ∪ D00 . If
ctb
we use the gold PPD-style labels of D00b to extract
ct
sub-words, the new model will overfit to the gold
PPD-style labels, which are unavailable at test time.
To avoid this training/test mismatch problem, we
also employ a 10-fold cross validation procedure to
add noise.
It is not a new topic to convert corpus from one
formalism to another. A well known work is trans-
forming Penn Treebank into resources for various
deep linguistic processing, including LTAG (Xia,
1999), CCG (Hockenmaier and Steedman, 2007),
HPSG (Miyao et al., 2004) and LFG (Cahill et al.,
2002). Such work for corpus conversion mainly
leverages rich sets of hand-crafted rules to convert
corpora. The construction of linguistic rules is usu-
ally time-consuming and the rules are not full cover-
age. Compared to rule-based conversion, our statis-
tical converters are much easier to built and empiri-
cally perform well.
</bodyText>
<sectionHeader confidence="0.998247" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99851">
6.1 Setting
</subsectionHeader>
<bodyText confidence="0.999311571428572">
Previous studies on joint Chinese word segmenta-
tion and POS tagging have used the CTB in experi-
ments. We follow this setting in this paper. We use
CTB 5.0 as our main corpus and define the train-
ing, development and test sets according to (Jiang
et al., 2008a; Jiang et al., 2008b; Kruengkrai et al.,
2009; Zhang and Clark, 2010; Sun, 2011). Jiang et
</bodyText>
<equation confidence="0.8079945">
DSTag�tb
ctb = ppd→ctb. We can
</equation>
<page confidence="0.987496">
237
</page>
<bodyText confidence="0.999870666666667">
al. (2009) present a preliminary study for the annota-
tion adaptation topic, and conduct experiments with
the extra PPD data4. In other words, the CTB-sytle
annotation is the target analysis while the PPD-style
annotation is the complementary/auxiliary analysis.
Our experiments for annotation ensemble follows
their setting to lead to a fair comparison of our sys-
tem and theirs. A CRF learning toolkit, wapiti5
(Lavergne et al., 2010), is used to resolve sequence
labeling problems. Among several parameter esti-
mation methods provided by wapiti, our auxiliary
experiments indicate that the “rprop-” method works
best. Three metrics are used for evaluation: preci-
sion (P), recall (R) and balanced f-score (F) defined
by 2PR/(P+R). Precision is the relative amount of
correct words in the system output. Recall is the rel-
ative amount of correct words compared to the gold
standard annotations. A token is considered to be
correct if its boundaries match the boundaries of a
word in the gold standard and their POS tags are
identical.
</bodyText>
<subsectionHeader confidence="0.998563">
6.2 Results of Stacking
</subsectionHeader>
<bodyText confidence="0.961079521739131">
Table 2 summarizes the segmentation and tagging
performance of the baseline and different stacking
models. The baseline of the character-based joint
solver (CTagctb) is competitive, and achieves an
f-score of 92.93. By using the character labels
from a heterogeneous solver (CTagppd), which is
trained on the PPD data set, the performance of this
character-based system (CTagppd—�ctb) is improved
to 93.67. This result confirms the importance of a
heterogeneous structure. Our structure-based stack-
ing solution is effective and outperforms the feature-
based stacking. By better exploiting the heteroge-
neous word boundary structures, our sub-word tag-
ging model achieves an f-score of 94.03 (lctb Tand
lppd
T are tuned on the development data and both set
to 1).
The contribution of the auxiliary tagger is two-
fold. On one hand, the heterogeneous solver pro-
vides structural information, which is the basis to
construct the sub-word sequence. On the other
hand, this tagger provides additional POS informa-
tion, which is helpful for disambiguation. To eval-
</bodyText>
<footnote confidence="0.9999235">
4http://icl.pku.edu.cn/icl_res/
5http://wapiti.limsi.fr/
</footnote>
<table confidence="0.99965475">
Devel. P R F
CTagctb 93.28% 92.58% 92.93
CTagppd�ctb 93.89% 93.46% 93.67
STagctb 94.07% 93.99% 94.03
</table>
<tableCaption confidence="0.99423">
Table 2: Performance of different stacking models on the
development data.
</tableCaption>
<bodyText confidence="0.9999365">
uate these two contributions, we do another experi-
ment by just using the heterogeneous word boundary
structures without the POS information. The f-score
of this type of sub-word tagging is 93.73. This re-
sult indicates that both the word boundary and POS
information are helpful.
</bodyText>
<subsectionHeader confidence="0.999592">
6.3 Learning Curves
</subsectionHeader>
<bodyText confidence="0.9992574">
We do additional experiments to evaluate the effect
of heterogeneous features as the amount of PPD data
is varied. Table 3 summarizes the f-score change.
The feature-based model works well only when a
considerable amount of heterogeneous data is avail-
able. When a small set is added, the performance is
even lower than the baseline (92.93). The structure-
based stacking model is more robust and obtains
consistent gains regardless of the size of the com-
plementary data.
</bodyText>
<table confidence="0.99927175">
#CTB #PPD PPD → CTB
CTag STag
18104 7381 92.21 93.26
18104 14545 93.22 93.82
18104 21745 93.58 93.96
18104 28767 93.55 93.87
18104 35996 93.67 94.03
9052 9052 92.10 92.40
</table>
<tableCaption confidence="0.983169">
Table 3: F-scores relative to sizes of training data. Sizes
(shown in column #CTB and #PPD) are numbers of sen-
tences in each training corpus.
</tableCaption>
<subsectionHeader confidence="0.993069">
6.4 Results of Annotation Conversion
</subsectionHeader>
<bodyText confidence="0.999989285714286">
The stacking models can be viewed as data-driven
annotation converting models. However they are not
trained on “real” labeled samples. Although the tar-
get representation (CTB-style analysis in our case)
is gold standard, the input representation (PPD-style
analysis in our case) is labeled by a automatic tag-
ger CTagppd. To make clear whether these stacking
</bodyText>
<page confidence="0.990915">
238
</page>
<bodyText confidence="0.999948555555556">
models trained with noisy inputs can tolerate per-
fect inputs, we evaluate the two stacking models on
our manually converted data. The accuracies pre-
sented in Table 4 indicate that though the conver-
sion models are learned by applying noisy data, they
can refine target tagging with gold auxiliary tagging.
Another interesting thing is that the gold PPD-style
analysis does not help the sub-word tagging model
as much as the character tagging model.
</bodyText>
<table confidence="0.997585">
Auto PPD Gold PPD
CTagppd→ctb 93.69 95.19
STagctb 94.14 94.70
</table>
<tableCaption confidence="0.994186">
Table 4: F-scores with gold PPD-style tagging on the
manually converted data.
</tableCaption>
<bodyText confidence="0.99889175">
data, the performance goes up to 94.11 and 94.68.
These results are also better than the best published
result on the same data set that is reported in (Jiang
et al., 2009).
</bodyText>
<table confidence="0.999877125">
Test P R F
(Sun, 2011) - - - - 94.02
(Jiang et al., 2009) - - - - 94.02
(Wang et al., 2011) - - - - 94.186
Character model 93.31% 93.51% 93.41
+Re-training 93.93% 94.29% 94.11
Sub-word model 94.10% 94.62% 94.36
+Re-training 94.42% 94.93% 94.68
</table>
<tableCaption confidence="0.9426535">
Table 6: Performance of different systems on the test
data.
</tableCaption>
<subsectionHeader confidence="0.916379">
6.5 Results of Re-training
</subsectionHeader>
<bodyText confidence="0.999933545454545">
Table 5 shows accuracies of re-trained models. Note
that a sub-word tagger is built on character taggers,
so when we re-train a sub-word system, we should
consider whether or not re-training base character
taggers. The error rates decrease as automatically
converted data is added to the training pool, espe-
cially for the character-based tagger CTagctb. When
the base CTB-style tagging is improved, the final
tagging is improved in the end. The re-training does
not help the sub-word tagging much; the improve-
ment is very modest.
</bodyText>
<table confidence="0.998590285714286">
CTagctb STagctb P(%) R(%) F
Dctb U D0 - - 94.46 94.06 94.26
ctb Dctb 94.61 94.43 94.52
Dctb U D0 b Dctb U D00ctb 94.05 94.08 94.06
ct Dctb U D00ctb 94.71 94.53 94.62
Dctb
Dctb U D0 ctb
</table>
<tableCaption confidence="0.9938185">
Table 5: Performance of re-trained models on the devel-
opment data.
</tableCaption>
<subsectionHeader confidence="0.986728">
6.6 Comparison to the State-of-the-Art
</subsectionHeader>
<bodyText confidence="0.99992">
Table 6 summarizes the tagging performance of
different systems. The baseline of the character-
based tagger is competitive, and achieve an f-score
of 93.41. By better using the heterogeneous word
boundary structures, our sub-word tagging model
achieves an f-score of 94.36. Both character and
sub-word tagging model can be enhanced with auto-
matically converted corpus. With the pseudo labeled
</bodyText>
<sectionHeader confidence="0.999466" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999982705882353">
Our theoretical and empirical analysis of two rep-
resentative popular corpora highlights two essential
characteristics of heterogeneous annotations which
are explored to reduce approximation and estima-
tion errors for Chinese word segmentation and POS
tagging. We employ stacking models to incorporate
features derived from heterogeneous analysis and
apply them to convert heterogeneous labeled data for
re-training. The appropriate application of hetero-
geneous annotations leads to a significant improve-
ment (a relative error reduction of 11%) over the best
performance for this task. Although our discussion
is for a specific task, the key idea to leverage het-
erogeneous annotations to reduce the approximation
error with stacking models and the estimation error
with automatically converted corpora is very general
and applicable to other NLP tasks.
</bodyText>
<sectionHeader confidence="0.957235" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.995548714285714">
This work is mainly finished when the first author
was in Saarland University and DFKI. At that time,
this author was funded by DFKI and German Aca-
demic Exchange Service (DAAD). While working
in Peking University, both author are supported by
NSFC (61170166) and National High-Tech R&amp;D
Program (2012AA011101).
</bodyText>
<footnote confidence="0.9931845">
6This result is achieved with much unlabeled data, which is
different from our setting.
</footnote>
<page confidence="0.997461">
239
</page>
<sectionHeader confidence="0.996272" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999316104761905">
Aoife Cahill, Mairead Mccarthy, Josef Van Genabith, and
Andy Way. 2002. Automatic annotation of the penn
treebank with lfg f-structure information. In Proceed-
ings of the LREC Workshop on Linguistic Knowledge
Acquisition and Representation: Bootstrapping Anno-
tated Language Data, Las Palmas, Canary Islands,
pages 8–15.
Julia Hockenmaier and Mark Steedman. 2007. Ccgbank:
A corpus of ccg derivations and dependency structures
extracted from the penn treebank. Computational Lin-
guistics, 33(3):355–396.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u.
2008a. A cascaded linear model for joint Chinese
word segmentation and part-of-speech tagging. In
Proceedings ofACL-08: HLT, pages 897–904, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for Chinese word segmentation and
part-of-speech tagging. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 385–392, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging – a case study. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 522–530, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and pos
tagging. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 513–521, Suntec, Singapore,
August. Association for Computational Linguistics.
Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon.
2010. Practical very large scale CRFs. pages 504–
513, July.
Yusuke Miyao, Takashi Ninomiya, and Jun ichi Tsujii.
2004. Corpus-oriented grammar development for ac-
quiring a head-driven phrase structure grammar from
the penn treebank. In IJCNLP, pages 684–693.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 277–
284, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Ex-
ploiting heterogeneous treebanks for parsing. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 46–54, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950–958,
Columbus, Ohio, June. Association for Computational
Linguistics.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarowsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
82–94, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese word
segmentation using unlabeled data. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 970–979, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Weiwei Sun, Rui Wang, and Yi Zhang. 2010. Dis-
criminative parse reranking for Chinese with homoge-
neous and heterogeneous annotations. In Proceedings
of Joint Conference on Chinese Language Processing
(CIPS-SIGHAN), Beijing, China, August.
Weiwei Sun. 2010. Word-based and character-based
word segmentation models: Comparison and combi-
nation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 1211–1219, Beijing, China, August. Coling
2010 Organizing Committee.
Weiwei Sun. 2011. A stacked sub-word model for joint
Chinese word segmentation and part-of-speech tag-
ging. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1385–1394, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Andr´e Filipe Torres Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking dependency
parsers. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 157–166, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.
2011. Improving chinese word segmentation and
pos tagging with semi-supervised methods using large
</reference>
<page confidence="0.949147">
240
</page>
<reference confidence="0.999226809523809">
auto-analyzed data. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 309–317, Chiang Mai, Thailand, Novem-
ber. Asian Federation of Natural Language Processing.
David H. Wolpert. 1992. Original contribution: Stacked
generalization. Neural Netw., 5:241–259, February.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of Natural Lan-
guage Processing Pacific Rim Symposium, pages 398–
403.
Yue Zhang and Stephen Clark. 2008. Joint word segmen-
tation and POS tagging using a single perceptron. In
Proceedings ofACL-08: HLT, pages 888–896, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and POS-tagging using a sin-
gle discriminative model. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 843–852, Cambridge, MA,
October. Association for Computational Linguistics.
</reference>
<page confidence="0.998107">
241
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.574748">
<title confidence="0.997253">Reducing Approximation and Estimation Errors for Chinese Processing with Heterogeneous Annotations</title>
<affiliation confidence="0.9739295">of Computer Science and Technology, Peking Graduate School of Computer</affiliation>
<address confidence="0.8464755">of Computational Linguistics, Saarland Technology Lab, DFKI</address>
<email confidence="0.918595">ws@pku.edu.cn</email>
<email confidence="0.918595">wanxiaojun@pku.edu.cn</email>
<abstract confidence="0.9957013">We address the issue of consuming heterogeneous annotation data for Chinese word segmentation and part-of-speech tagging. We empirically analyze the diversity between two representative corpora, i.e. Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible. The analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) re-training models with high quality automatically converted data to reduce the estimation error. Evaluation on the CTB and PPD data shows that our novel model achieves a relative error reduction of 11% over the best reported result in the literature.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Mairead Mccarthy</author>
<author>Josef Van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Automatic annotation of the penn treebank with lfg f-structure information.</title>
<date>2002</date>
<booktitle>In Proceedings of the LREC Workshop on Linguistic Knowledge Acquisition</booktitle>
<pages>8--15</pages>
<marker>Cahill, Mccarthy, Van Genabith, Way, 2002</marker>
<rawString>Aoife Cahill, Mairead Mccarthy, Josef Van Genabith, and Andy Way. 2002. Automatic annotation of the penn treebank with lfg f-structure information. In Proceedings of the LREC Workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Annotated Language Data, Las Palmas, Canary Islands, pages 8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Ccgbank: A corpus of ccg derivations and dependency structures extracted from the penn treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="23852" citStr="Hockenmaier and Steedman, 2007" startWordPosition="3773" endWordPosition="3776"> are still helpful to reduce the estimation error. a new labeled data re-train the STagctb model with Dctb ∪ D00 . If ctb we use the gold PPD-style labels of D00b to extract ct sub-words, the new model will overfit to the gold PPD-style labels, which are unavailable at test time. To avoid this training/test mismatch problem, we also employ a 10-fold cross validation procedure to add noise. It is not a new topic to convert corpus from one formalism to another. A well known work is transforming Penn Treebank into resources for various deep linguistic processing, including LTAG (Xia, 1999), CCG (Hockenmaier and Steedman, 2007), HPSG (Miyao et al., 2004) and LFG (Cahill et al., 2002). Such work for corpus conversion mainly leverages rich sets of hand-crafted rules to convert corpora. The construction of linguistic rules is usually time-consuming and the rules are not full coverage. Compared to rule-based conversion, our statistical converters are much easier to built and empirically perform well. 6 Experiments 6.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the CTB in experiments. We follow this setting in this paper. We use CTB 5.0 as our main corpus and define the training</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. Ccgbank: A corpus of ccg derivations and dependency structures extracted from the penn treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan L¨u</author>
</authors>
<title>A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>897--904</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<marker>Jiang, Huang, Liu, L¨u, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u. 2008a. A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. In Proceedings ofACL-08: HLT, pages 897–904, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
</authors>
<title>Word lattice reranking for Chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>385--392</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="6659" citStr="Jiang et al., 2008" startWordPosition="981" endWordPosition="984">submitted as a supplemental material for research purposes. basic language units, i.e. words, word segmentation and POS tagging are important initial steps for Chinese language processing. Supervised learning with specifically defined training data has become a dominant paradigm. Joint approaches that resolve the two tasks simultaneously have received much attention in recent research. Previous work has shown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008; Sun, 2011). Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words (Jiang et al., 2008a). In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. The second kind of solution is the “word-based” method</context>
<context position="24512" citStr="Jiang et al., 2008" startWordPosition="3886" endWordPosition="3889">l et al., 2002). Such work for corpus conversion mainly leverages rich sets of hand-crafted rules to convert corpora. The construction of linguistic rules is usually time-consuming and the rules are not full coverage. Compared to rule-based conversion, our statistical converters are much easier to built and empirically perform well. 6 Experiments 6.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the CTB in experiments. We follow this setting in this paper. We use CTB 5.0 as our main corpus and define the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011). Jiang et DSTag�tb ctb = ppd→ctb. We can 237 al. (2009) present a preliminary study for the annotation adaptation topic, and conduct experiments with the extra PPD data4. In other words, the CTB-sytle annotation is the target analysis while the PPD-style annotation is the complementary/auxiliary analysis. Our experiments for annotation ensemble follows their setting to lead to a fair comparison of our system and theirs. A CRF learning toolkit, wapiti5 (Lavergne et al., 2010), is used to resolve sequence labeling</context>
</contexts>
<marker>Jiang, Mi, Liu, 2008</marker>
<rawString>Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word lattice reranking for Chinese word segmentation and part-of-speech tagging. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 385–392, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging – a case study.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>522--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="5316" citStr="Jiang et al., 2009" startWordPosition="774" endWordPosition="777">igned to refine the tagging result with the help of heterogeneous annotations. To reduce the estimation error, we employ a learning-based approach to convert complementary heterogeneous data to increase labeled training data for the target task. Both the character-based tagger and the sub-word tagger can be refined by re-training with automatically converted data. We conduct experiments on the CTB and PPD data, and compare our system with state-of-theart systems. Our structure-based stacking model achieves an f-score of 94.36, which is superior to a feature-based stacking model introduced in (Jiang et al., 2009). The converted data can also enhance the baseline model. A simple character-based model can be improved from 93.41 to 94.11. Since the two treatments are concerned with reducing different types of errors and thus not fully overlapping, the combination of them gives a further improvement. Our final system achieves an f-score of 94.68, which yields a relative error reduction of 11% over the best published result (94.02). 2 Joint Chinese Word Segmentation and POS Tagging Different from English and other Western languages, Chinese is written without explicit word delimiters such as space characte</context>
<context position="8303" citStr="Jiang et al. (2009)" startWordPosition="1241" endWordPosition="1244">acked sub-word tagging model, which combines strengths of both character-based and word-based approaches (Sun, 2011). First, different characterbased and word-based models are trained to produce multiple segmentation and tagging results. Second, the outputs of these coarse-grained models are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a finegrained sub-word tagger. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, 233 e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their me</context>
<context position="17205" citStr="Jiang et al. (2009)" startWordPosition="2678" endWordPosition="2681">v:4; c:2; Vg:1; VA a:57; i:4; z:2; ad:1; b:1; VV v:382; i:5; a:3; Vg:2; vn:2; n:2; p:2; w:1; AD d:149; c:11; ad:6; z:4; a:3; v:2; n:1; r:1; m:1; f:1; t:1; NR ns:170; nr:65; j:23; nt:21; nz:7; n:2; s:1; JJ a:43; b:13; n:3; vn:3; d:2; j:2; f:2; t:2; z:1; NN n:738; vn:135; v:26; j:19; Ng:5; an:5; a:3; r:3; s:3; Ag:2; nt:2; f:2; q:2; i:1; t:1; nz:1; b:1; 235 a candidate character token ci with a fixed window ci_2ci_1cici+1ci+2. The following features are used for classification: • Character unigrams: ck (i − l &lt; k &lt; i + l) • Character bigrams: ckck+1 (i − l &lt; k &lt; i + l) 4.3 Feature-based Stacking Jiang et al. (2009) introduced a feature-based stacking solution for annotation ensemble. In their solution, an auxiliary tagger CTagppd is trained on a complementary corpus, i.e. PPD, to assist the target CTB-style tagging. To refine the character-based tagger CTagctb, PPD-style character labels are directly incorporated as new features. The stacking model relies on the ability of discriminative learning method to explore informative features, which play central role to boost the tagging performance. To compare their feature-based stacking model and our structure-based model, we implement a similar system CTagp</context>
<context position="29220" citStr="Jiang et al., 2009" startWordPosition="4637" endWordPosition="4640">ies presented in Table 4 indicate that though the conversion models are learned by applying noisy data, they can refine target tagging with gold auxiliary tagging. Another interesting thing is that the gold PPD-style analysis does not help the sub-word tagging model as much as the character tagging model. Auto PPD Gold PPD CTagppd→ctb 93.69 95.19 STagctb 94.14 94.70 Table 4: F-scores with gold PPD-style tagging on the manually converted data. data, the performance goes up to 94.11 and 94.68. These results are also better than the best published result on the same data set that is reported in (Jiang et al., 2009). Test P R F (Sun, 2011) - - - - 94.02 (Jiang et al., 2009) - - - - 94.02 (Wang et al., 2011) - - - - 94.186 Character model 93.31% 93.51% 93.41 +Re-training 93.93% 94.29% 94.11 Sub-word model 94.10% 94.62% 94.36 +Re-training 94.42% 94.93% 94.68 Table 6: Performance of different systems on the test data. 6.5 Results of Re-training Table 5 shows accuracies of re-trained models. Note that a sub-word tagger is built on character taggers, so when we re-train a sub-word system, we should consider whether or not re-training base character taggers. The error rates decrease as automatically converted </context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging – a case study. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 522–530, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint Chinese word segmentation and pos tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>513--521</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="24559" citStr="Kruengkrai et al., 2009" startWordPosition="3894" endWordPosition="3897">ersion mainly leverages rich sets of hand-crafted rules to convert corpora. The construction of linguistic rules is usually time-consuming and the rules are not full coverage. Compared to rule-based conversion, our statistical converters are much easier to built and empirically perform well. 6 Experiments 6.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the CTB in experiments. We follow this setting in this paper. We use CTB 5.0 as our main corpus and define the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011). Jiang et DSTag�tb ctb = ppd→ctb. We can 237 al. (2009) present a preliminary study for the annotation adaptation topic, and conduct experiments with the extra PPD data4. In other words, the CTB-sytle annotation is the target analysis while the PPD-style annotation is the complementary/auxiliary analysis. Our experiments for annotation ensemble follows their setting to lead to a fair comparison of our system and theirs. A CRF learning toolkit, wapiti5 (Lavergne et al., 2010), is used to resolve sequence labeling problems. Among several parameter estimation m</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and pos tagging. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 513–521, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lavergne</author>
<author>Olivier Capp´e</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Practical very large scale CRFs.</title>
<date>2010</date>
<pages>504--513</pages>
<marker>Lavergne, Capp´e, Yvon, 2010</marker>
<rawString>Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon. 2010. Practical very large scale CRFs. pages 504– 513, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun ichi Tsujii</author>
</authors>
<title>Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the penn treebank.</title>
<date>2004</date>
<booktitle>In IJCNLP,</booktitle>
<pages>684--693</pages>
<contexts>
<context position="23879" citStr="Miyao et al., 2004" startWordPosition="3778" endWordPosition="3781">tion error. a new labeled data re-train the STagctb model with Dctb ∪ D00 . If ctb we use the gold PPD-style labels of D00b to extract ct sub-words, the new model will overfit to the gold PPD-style labels, which are unavailable at test time. To avoid this training/test mismatch problem, we also employ a 10-fold cross validation procedure to add noise. It is not a new topic to convert corpus from one formalism to another. A well known work is transforming Penn Treebank into resources for various deep linguistic processing, including LTAG (Xia, 1999), CCG (Hockenmaier and Steedman, 2007), HPSG (Miyao et al., 2004) and LFG (Cahill et al., 2002). Such work for corpus conversion mainly leverages rich sets of hand-crafted rules to convert corpora. The construction of linguistic rules is usually time-consuming and the rules are not full coverage. Compared to rule-based conversion, our statistical converters are much easier to built and empirically perform well. 6 Experiments 6.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the CTB in experiments. We follow this setting in this paper. We use CTB 5.0 as our main corpus and define the training, development and test sets</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2004</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Jun ichi Tsujii. 2004. Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the penn treebank. In IJCNLP, pages 684–693.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based?</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>277--284</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="6639" citStr="Ng and Low, 2004" startWordPosition="977" endWordPosition="980"> This data set is submitted as a supplemental material for research purposes. basic language units, i.e. words, word segmentation and POS tagging are important initial steps for Chinese language processing. Supervised learning with specifically defined training data has become a dominant paradigm. Joint approaches that resolve the two tasks simultaneously have received much attention in recent research. Previous work has shown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008; Sun, 2011). Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words (Jiang et al., 2008a). In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. The second kind of solution is the</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based? In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 277– 284, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng-Yu Niu</author>
<author>Haifeng Wang</author>
<author>Hua Wu</author>
</authors>
<title>Exploiting heterogeneous treebanks for parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>46--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="8477" citStr="Niu et al., 2009" startWordPosition="1269" endWordPosition="1272">rained to produce multiple segmentation and tagging results. Second, the outputs of these coarse-grained models are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a finegrained sub-word tagger. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, 233 e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their methods harvest useful string knowledge from unlabeled or automatically analyzed data, and apply the knowledge to design new features for discriminative learning. 3 About Heter</context>
</contexts>
<marker>Niu, Wang, Wu, 2009</marker>
<rawString>Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Exploiting heterogeneous treebanks for parsing. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 46–54, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>950--958</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="14788" citStr="Nivre and McDonald, 2008" startWordPosition="2254" endWordPosition="2257">erent training data, we can construct multiple heterogeneous systems. These systems produce similar linguistic analysis which holds the same high level linguistic principles but differ in details. A very simple idea to take advantage of heterogeneous structures is to design a predictor which can predict a more accurate target structure based on the input, the less accurate target structure and complementary structures. This idea is very close to stacked learning (Wolpert, 1992), which is well developed for ensemble learning, and successfully applied to some NLP tasks, e.g. dependency parsing (Nivre and McDonald, 2008; Torres Martins et al., 2008). Formally speaking, our idea is to include two “levels” of processing. The first level includes one Table 1: Mapping between CTB and PPD POS Tags. or more base predictors fl, ..., fK that are independently built on different training data. The second level processing consists of an inference function h that takes as input (x, fl(x),..., fK(x))3 and outputs a final prediction h(x, f1(x),..., fK(x)). The only difference between model ensemble and annotation ensemble is that the output spaces of model ensemble are the same while the output spaces of annotation ensem</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of ACL-08: HLT, pages 950–958, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance Ramshaw</author>
<author>Mitch Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<editor>In David Yarowsky and Kenneth Church, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="15630" citStr="Ramshaw and Marcus, 1995" startWordPosition="2393" endWordPosition="2396">t are independently built on different training data. The second level processing consists of an inference function h that takes as input (x, fl(x),..., fK(x))3 and outputs a final prediction h(x, f1(x),..., fK(x)). The only difference between model ensemble and annotation ensemble is that the output spaces of model ensemble are the same while the output spaces of annotation ensemble are different. This framework is general and flexible, in the sense that it assumes almost nothing about the individual systems and take them as black boxes. 4.2 A Character-based Tagger With IOB2 representation (Ramshaw and Marcus, 1995), the problem of joint segmentation and tagging can be regarded as a character classification task. Previous work shows that the character-based approach is an effective method for Chinese lexical processing. Both of our feature- and structure-based stacking models employ base character-based taggers to generate multiple segmentation and tagging results. Our base tagger use a discriminative sequential classifier to predict the POS tag with positional information for each character. Each character can be assigned one of two possible boundary tags: “B” for a character that begins a word and “I” </context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance Ramshaw and Mitch Marcus. 1995. Text chunking using transformation-based learning. In David Yarowsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora, pages 82–94, Somerset, New Jersey. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Jia Xu</author>
</authors>
<title>Enhancing Chinese word segmentation using unlabeled data.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>970--979</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="8689" citStr="Sun and Xu, 2011" startWordPosition="1303" endWordPosition="1306">grained sub-word tagger. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, 233 e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their methods harvest useful string knowledge from unlabeled or automatically analyzed data, and apply the knowledge to design new features for discriminative learning. 3 About Heterogeneous Annotations For Chinese word segmentation and POS tagging, supervised learning has become a dominant paradigm. Much of the progress is due to the development of both corpora and machine learning techniqu</context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Weiwei Sun and Jia Xu. 2011. Enhancing Chinese word segmentation using unlabeled data. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 970–979, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Rui Wang</author>
<author>Yi Zhang</author>
</authors>
<title>Discriminative parse reranking for Chinese with homogeneous and heterogeneous annotations.</title>
<date>2010</date>
<booktitle>In Proceedings of Joint Conference on Chinese Language Processing (CIPS-SIGHAN),</booktitle>
<location>Beijing, China,</location>
<contexts>
<context position="8496" citStr="Sun et al., 2010" startWordPosition="1273" endWordPosition="1276">multiple segmentation and tagging results. Second, the outputs of these coarse-grained models are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a finegrained sub-word tagger. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, 233 e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their methods harvest useful string knowledge from unlabeled or automatically analyzed data, and apply the knowledge to design new features for discriminative learning. 3 About Heterogeneous Annotation</context>
</contexts>
<marker>Sun, Wang, Zhang, 2010</marker>
<rawString>Weiwei Sun, Rui Wang, and Yi Zhang. 2010. Discriminative parse reranking for Chinese with homogeneous and heterogeneous annotations. In Proceedings of Joint Conference on Chinese Language Processing (CIPS-SIGHAN), Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>Word-based and character-based word segmentation models: Comparison and combination.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1211--1219</pages>
<location>Beijing, China,</location>
<contexts>
<context position="10218" citStr="Sun, 2010" startWordPosition="1542" endWordPosition="1543">tence of multiple resources, such data cannot be simply put together for training systems, because almost all of statistical NLP systems assume homogeneous annotation. Therefore, it is not only interesting but also important to study how to fully utilize heterogeneous resources to improve Chinese lexical processing. There are two main types of errors in statistical NLP: (1) the approximation error that is due to the intrinsic suboptimality of a model and (2) the estimation error that is due to having only finite training data. Take Chinese word segmentation for example. Our previous analysis (Sun, 2010) shows that one main intrinsic disadvantage of characterbased model is the difficulty in incorporating the whole word information, while one main disadvantage of word-based model is the weak ability to express word formation. In both models, the significant decrease of the prediction accuracy of out-ofvocabulary (OOV) words indicates the impact of the estimation error. The two essential characteristics about systematic diversity of heterogeneous annotations can be utilized to reduce both approximation and estimation errors. 3.1 Analysis of the CTB and PPD Standards This paper focuses on two re</context>
</contexts>
<marker>Sun, 2010</marker>
<rawString>Weiwei Sun. 2010. Word-based and character-based word segmentation models: Comparison and combination. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1211–1219, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1385--1394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="4203" citStr="Sun, 2011" startWordPosition="605" endWordPosition="606">processing tasks. We empirically analyze the diversity between two representative popular heterogeneous corpora, i.e. 232 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 232–241, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD). To that end, we manually label 200 sentences from CTB with PPD-style annotations.1 Our analysis confirms the aforementioned two properties of heterogeneous annotations. Inspired by the sub-word tagging method introduced in (Sun, 2011), we propose a structure-based stacking model to fully utilize heterogeneous word structures to reduce the approximation error. In particular, joint word segmentation and POS tagging is addressed as a two step process. First, character-based taggers are respectively trained on heterogeneous annotations to produce multiple analysis. The outputs of these taggers are then merged into sub-word sequences, which are further re-segmented and tagged by a sub-word tagger. The sub-word tagger is designed to refine the tagging result with the help of heterogeneous annotations. To reduce the estimation er</context>
<context position="6695" citStr="Sun, 2011" startWordPosition="989" endWordPosition="990">earch purposes. basic language units, i.e. words, word segmentation and POS tagging are important initial steps for Chinese language processing. Supervised learning with specifically defined training data has become a dominant paradigm. Joint approaches that resolve the two tasks simultaneously have received much attention in recent research. Previous work has shown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008; Sun, 2011). Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words (Jiang et al., 2008a). In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. The second kind of solution is the “word-based” method, also known as semi-Markov tagging </context>
<context position="18501" citStr="Sun, 2011" startWordPosition="2894" endWordPosition="2895">sed to derive the following features to enhance our CTBstyle tagger: • Character label unigrams: cppd k (i−lppd &lt; k &lt; i + lppd) — — • Character label bigrams: cppd k cppd k+1 (i − lppd &lt; k &lt; i + lppd) In the above descriptions, l and lppd are the window sizes of features, which can be tuned on development data. 4.4 Structure-based Stacking We propose a novel structured-based stacking model for the task, in which heterogeneous word structures are used not only to generate features but also to derive a sub-word structure. Our work is inspired by the stacked sub-word tagging model introduced in (Sun, 2011). Their work is motivated by the diversity of heterogeneous models, while our work is motivated by the diversity of heterogeneous annotations. The workflow of our new system is shown in Figure 1. In the first phase, one character-based CTB-style tagger (CTagctb) and one character-based PPD-style tagger (CTagppd) are respectively trained to produce heterogenous Figure 1: Sub-word tagging based on heterogeneous taggers. word boundaries. In the second phase, this system first combines the two segmentation and tagging results to get sub-words which maximize the agreement about word boundaries. Fin</context>
<context position="22174" citStr="Sun, 2011" startWordPosition="3505" endWordPosition="3506">C(sk)C(sk+1) (i − lC ≤ k &lt; i + lC), Tctb(sk)Tctb(sk+1) (i − lctb T ≤ k &lt; i + lctb T ), Tppd(sk)Tppd(sk+1) (i − lppd T ≤ k &lt; i + lppd T ) • C(si−1)C(si+1) (if lC ≥ 1), Tctb(si−1)Tctb(si+1) (if lctb ≥ 1), T Tppd(si−1)Tppd(si+1) (if lppd T ≥ 1) • Word formation features: character n-gram prefixes and suffixes for n up to 3. Cross-validation CTagctb and CTagppd are directly trained on the original training data, i.e. the CTB and PPD data. Cross-validation technique has been proved necessary to generate the training data for sub-word tagging, since it deals with the training/test mismatch problem (Sun, 2011). To construct training data for the new heterogeneous subword tagger, a 10-fold cross-validation on the original CTB data is performed too. 5 Data-driven Annotation Conversion It is possible to acquire high quality labeled data for a specific annotation standard by exploring existing heterogeneous corpora, since the annotations are normally highly compatible. Moreover, the exploitation of additional (pseudo) labeled data aims to reduce the estimation error and enhances a NLP system in a different way from stacking. We therefore expect the improvements are not much overlapping and the combinat</context>
<context position="24594" citStr="Sun, 2011" startWordPosition="3902" endWordPosition="3903">d rules to convert corpora. The construction of linguistic rules is usually time-consuming and the rules are not full coverage. Compared to rule-based conversion, our statistical converters are much easier to built and empirically perform well. 6 Experiments 6.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the CTB in experiments. We follow this setting in this paper. We use CTB 5.0 as our main corpus and define the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011). Jiang et DSTag�tb ctb = ppd→ctb. We can 237 al. (2009) present a preliminary study for the annotation adaptation topic, and conduct experiments with the extra PPD data4. In other words, the CTB-sytle annotation is the target analysis while the PPD-style annotation is the complementary/auxiliary analysis. Our experiments for annotation ensemble follows their setting to lead to a fair comparison of our system and theirs. A CRF learning toolkit, wapiti5 (Lavergne et al., 2010), is used to resolve sequence labeling problems. Among several parameter estimation methods provided by wapiti, our auxi</context>
<context position="29244" citStr="Sun, 2011" startWordPosition="4645" endWordPosition="4646"> that though the conversion models are learned by applying noisy data, they can refine target tagging with gold auxiliary tagging. Another interesting thing is that the gold PPD-style analysis does not help the sub-word tagging model as much as the character tagging model. Auto PPD Gold PPD CTagppd→ctb 93.69 95.19 STagctb 94.14 94.70 Table 4: F-scores with gold PPD-style tagging on the manually converted data. data, the performance goes up to 94.11 and 94.68. These results are also better than the best published result on the same data set that is reported in (Jiang et al., 2009). Test P R F (Sun, 2011) - - - - 94.02 (Jiang et al., 2009) - - - - 94.02 (Wang et al., 2011) - - - - 94.186 Character model 93.31% 93.51% 93.41 +Re-training 93.93% 94.29% 94.11 Sub-word model 94.10% 94.62% 94.36 +Re-training 94.42% 94.93% 94.68 Table 6: Performance of different systems on the test data. 6.5 Results of Re-training Table 5 shows accuracies of re-trained models. Note that a sub-word tagger is built on character taggers, so when we re-train a sub-word system, we should consider whether or not re-training base character taggers. The error rates decrease as automatically converted data is added to the tra</context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Weiwei Sun. 2011. A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1385–1394, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Filipe Torres Martins</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>157--166</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="14818" citStr="Martins et al., 2008" startWordPosition="2259" endWordPosition="2262">ruct multiple heterogeneous systems. These systems produce similar linguistic analysis which holds the same high level linguistic principles but differ in details. A very simple idea to take advantage of heterogeneous structures is to design a predictor which can predict a more accurate target structure based on the input, the less accurate target structure and complementary structures. This idea is very close to stacked learning (Wolpert, 1992), which is well developed for ensemble learning, and successfully applied to some NLP tasks, e.g. dependency parsing (Nivre and McDonald, 2008; Torres Martins et al., 2008). Formally speaking, our idea is to include two “levels” of processing. The first level includes one Table 1: Mapping between CTB and PPD POS Tags. or more base predictors fl, ..., fK that are independently built on different training data. The second level processing consists of an inference function h that takes as input (x, fl(x),..., fK(x))3 and outputs a final prediction h(x, f1(x),..., fK(x)). The only difference between model ensemble and annotation ensemble is that the output spaces of model ensemble are the same while the output spaces of annotation ensemble are different. This framew</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>Andr´e Filipe Torres Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiou Wang</author>
<author>Jun’ichi Kazama</author>
<author>Yoshimasa Tsuruoka</author>
<author>Wenliang Chen</author>
<author>Yujie Zhang</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto-analyzed data.</title>
<date>2011</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>309--317</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="8712" citStr="Wang et al. (2011)" startWordPosition="1308" endWordPosition="1311">r. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, 233 e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their methods harvest useful string knowledge from unlabeled or automatically analyzed data, and apply the knowledge to design new features for discriminative learning. 3 About Heterogeneous Annotations For Chinese word segmentation and POS tagging, supervised learning has become a dominant paradigm. Much of the progress is due to the development of both corpora and machine learning techniques. Although several in</context>
<context position="29313" citStr="Wang et al., 2011" startWordPosition="4661" endWordPosition="4664">isy data, they can refine target tagging with gold auxiliary tagging. Another interesting thing is that the gold PPD-style analysis does not help the sub-word tagging model as much as the character tagging model. Auto PPD Gold PPD CTagppd→ctb 93.69 95.19 STagctb 94.14 94.70 Table 4: F-scores with gold PPD-style tagging on the manually converted data. data, the performance goes up to 94.11 and 94.68. These results are also better than the best published result on the same data set that is reported in (Jiang et al., 2009). Test P R F (Sun, 2011) - - - - 94.02 (Jiang et al., 2009) - - - - 94.02 (Wang et al., 2011) - - - - 94.186 Character model 93.31% 93.51% 93.41 +Re-training 93.93% 94.29% 94.11 Sub-word model 94.10% 94.62% 94.36 +Re-training 94.42% 94.93% 94.68 Table 6: Performance of different systems on the test data. 6.5 Results of Re-training Table 5 shows accuracies of re-trained models. Note that a sub-word tagger is built on character taggers, so when we re-train a sub-word system, we should consider whether or not re-training base character taggers. The error rates decrease as automatically converted data is added to the training pool, especially for the character-based tagger CTagctb. When t</context>
</contexts>
<marker>Wang, Kazama, Tsuruoka, Chen, Zhang, Torisawa, 2011</marker>
<rawString>Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang, and Kentaro Torisawa. 2011. Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto-analyzed data. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 309–317, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Wolpert</author>
</authors>
<title>Original contribution: Stacked generalization.</title>
<date>1992</date>
<journal>Neural Netw.,</journal>
<pages>5--241</pages>
<contexts>
<context position="14646" citStr="Wolpert, 1992" startWordPosition="2235" endWordPosition="2236">tacking Each annotation data set alone can yield a predictor that can be taken as a mechanism to produce structured texts. With different training data, we can construct multiple heterogeneous systems. These systems produce similar linguistic analysis which holds the same high level linguistic principles but differ in details. A very simple idea to take advantage of heterogeneous structures is to design a predictor which can predict a more accurate target structure based on the input, the less accurate target structure and complementary structures. This idea is very close to stacked learning (Wolpert, 1992), which is well developed for ensemble learning, and successfully applied to some NLP tasks, e.g. dependency parsing (Nivre and McDonald, 2008; Torres Martins et al., 2008). Formally speaking, our idea is to include two “levels” of processing. The first level includes one Table 1: Mapping between CTB and PPD POS Tags. or more base predictors fl, ..., fK that are independently built on different training data. The second level processing consists of an inference function h that takes as input (x, fl(x),..., fK(x))3 and outputs a final prediction h(x, f1(x),..., fK(x)). The only difference betwe</context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>David H. Wolpert. 1992. Original contribution: Stacked generalization. Neural Netw., 5:241–259, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Extracting tree adjoining grammars from bracketed corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>398--403</pages>
<contexts>
<context position="23814" citStr="Xia, 1999" startWordPosition="3770" endWordPosition="3771">e imperfect, they are still helpful to reduce the estimation error. a new labeled data re-train the STagctb model with Dctb ∪ D00 . If ctb we use the gold PPD-style labels of D00b to extract ct sub-words, the new model will overfit to the gold PPD-style labels, which are unavailable at test time. To avoid this training/test mismatch problem, we also employ a 10-fold cross validation procedure to add noise. It is not a new topic to convert corpus from one formalism to another. A well known work is transforming Penn Treebank into resources for various deep linguistic processing, including LTAG (Xia, 1999), CCG (Hockenmaier and Steedman, 2007), HPSG (Miyao et al., 2004) and LFG (Cahill et al., 2002). Such work for corpus conversion mainly leverages rich sets of hand-crafted rules to convert corpora. The construction of linguistic rules is usually time-consuming and the rules are not full coverage. Compared to rule-based conversion, our statistical converters are much easier to built and empirically perform well. 6 Experiments 6.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the CTB in experiments. We follow this setting in this paper. We use CTB 5.0 as o</context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>Fei Xia. 1999. Extracting tree adjoining grammars from bracketed corpora. In Proceedings of Natural Language Processing Pacific Rim Symposium, pages 398– 403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>888--896</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="6683" citStr="Zhang and Clark, 2008" startWordPosition="985" endWordPosition="988">mental material for research purposes. basic language units, i.e. words, word segmentation and POS tagging are important initial steps for Chinese language processing. Supervised learning with specifically defined training data has become a dominant paradigm. Joint approaches that resolve the two tasks simultaneously have received much attention in recent research. Previous work has shown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008; Sun, 2011). Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words (Jiang et al., 2008a). In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. The second kind of solution is the “word-based” method, also known as semi-Mar</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Proceedings ofACL-08: HLT, pages 888–896, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and POS-tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>843--852</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="7341" citStr="Zhang and Clark, 2010" startWordPosition="1091" endWordPosition="1094">s are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words (Jiang et al., 2008a). In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. The second kind of solution is the “word-based” method, also known as semi-Markov tagging (Zhang and Clark, 2008; Zhang and Clark, 2010), where the basic predicting units are words themselves. This kind of solver sequentially decides whether the local sequence of characters makes up a word as well as its possible POS tag. Solvers may use previously predicted words and their POS information as clues to process a new word. In addition, we proposed an effective and efficient stacked sub-word tagging model, which combines strengths of both character-based and word-based approaches (Sun, 2011). First, different characterbased and word-based models are trained to produce multiple segmentation and tagging results. Second, the outputs</context>
<context position="24582" citStr="Zhang and Clark, 2010" startWordPosition="3898" endWordPosition="3901">ich sets of hand-crafted rules to convert corpora. The construction of linguistic rules is usually time-consuming and the rules are not full coverage. Compared to rule-based conversion, our statistical converters are much easier to built and empirically perform well. 6 Experiments 6.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the CTB in experiments. We follow this setting in this paper. We use CTB 5.0 as our main corpus and define the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011). Jiang et DSTag�tb ctb = ppd→ctb. We can 237 al. (2009) present a preliminary study for the annotation adaptation topic, and conduct experiments with the extra PPD data4. In other words, the CTB-sytle annotation is the target analysis while the PPD-style annotation is the complementary/auxiliary analysis. Our experiments for annotation ensemble follows their setting to lead to a fair comparison of our system and theirs. A CRF learning toolkit, wapiti5 (Lavergne et al., 2010), is used to resolve sequence labeling problems. Among several parameter estimation methods provided by wapi</context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 843–852, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>