<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004535">
<title confidence="0.796054">
Investigation of Question Classifier in Question Answering
</title>
<author confidence="0.763539">
Zhiheng Huang Marcus Thint Asli Celikyilmaz
</author>
<affiliation confidence="0.625535">
EECS Department Intelligent Systems Research Center EECS Department
University of California British Telecom Group University of California
at Berkeley Chief Technology Office at Berkeley
</affiliation>
<address confidence="0.419833">
CA 94720-1776, USA marcus.2.thint@bt.com CA 94720-1776, USA
</address>
<email confidence="0.938609">
zhiheng@cs.berkeley.edu asli@cs.berkeley.edu
</email>
<sectionHeader confidence="0.99672" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999768294117647">
In this paper, we investigate how an ac-
curate question classifier contributes to
a question answering system. We first
present a Maximum Entropy (ME) based
question classifier which makes use of
head word features and their WordNet hy-
pernyms. We show that our question clas-
sifier can achieve the state of the art per-
formance in the standard UIUC question
dataset. We then investigate quantitatively
the contribution of this question classifier
to a feature driven question answering sys-
tem. With our accurate question classifier
and some standard question answer fea-
tures, our question answering system per-
forms close to the state of the art using
TREC corpus.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958967213115">
Question answering has drawn significant atten-
tion from the last decade (Prager, 2006). It at-
tempts to answer the question posed in natural
language by providing the answer phrase rather
than the whole documents. An important step in
question answering (QA) is to classify the ques-
tion to the anticipated type of the answer. For
example, the question of Who discovered x-rays
should be classified into the type of human (indi-
vidual). This information would narrow down the
search space to identify the correct answer string.
In addition, this information can suggest different
strategies to search and verify a candidate answer.
In fact, the combination of question classification
and the named entity recognition is a key approach
in modern question answering systems (Voorhees
and Dang, 2005).
The question classification is by no means triv-
ial: Simply using question wh-words can not
achieve satisfactory results. The difficulty lies
in classifying the what and which type questions.
Considering the example What is the capital of Yu-
goslavia, it is of location (city) type, while What
is the pH scale is of definition type. As with
the previous work of (Li and Roth, 2002; Li and
Roth, 2006; Krishnan et al., 2005; Moschitti et
al., 2007), we propose a feature driven statistical
question classifier (Huang et al., 2008). In partic-
ular, we propose head word feature and augment
semantic features of such head words using Word-
Net. In addition, Lesk’s word sense disambigua-
tion (WSD) algorithm is adapted and the depth of
hypernym feature is optimized. With further aug-
ment of other standard features such as unigrams,
we can obtain accuracy of 89.0% using ME model
for 50 fine classes over UIUC dataset.
In addition to building an accurate question
classifier, we investigate the contribution of this
question classifier to a feature driven question an-
swering rank model. It is worth noting that, most
of the features we used in question answering rank
model, depend on the question type information.
For instance, if a question is classified as a type of
sport, we then only care about whether there are
sport entities existing in the candidate sentences.
It is expected that a fine grained named entity rec-
ognizer (NER) should make good use of the accu-
rate question type information. However, due to
the lack of a fine grained NER tool at hand, we
employ the Stanford NER package (Finkel et al.,
2005) which identifies only four types of named
entities. Even with such a coarse named entity
recognizer, the experiments show that the question
classifier plays an important role in determining
the performance of a question answering system.
The rest of the paper is organized as follow-
ing. Section 2 reviews the maximum entropy
model which are used in both question classifica-
tion and question answering ranking. Section 3
presents the features used in question classifica-
tion. Section 4 presents the question classification
</bodyText>
<page confidence="0.982342">
543
</page>
<note confidence="0.9966055">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 543–550,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.99946075">
accuracy over UIUC question dataset. Section 5
presents the question answer features. Section 6
illustrates the results based on TREC question an-
swer dataset. And Section 7 draws the conclusion.
</bodyText>
<sectionHeader confidence="0.983056" genericHeader="method">
2 Maximum Entropy Models
</sectionHeader>
<bodyText confidence="0.999594933333333">
Maximum entropy (ME) models (Berger et al.,
1996; Manning and Klein, 2003), also known as
log-linear and exponential learning models, pro-
vide a general purpose machine learning technique
for classification and prediction which has been
successfully applied to natural language process-
ing including part of speech tagging, named entity
recognition etc. Maximum entropy models can in-
tegrate features from many heterogeneous infor-
mation sources for classification. Each feature
corresponds to a constraint on the model. Given
a training set of (C, D), where C is a set of class
labels and D is a set of feature represented data
points, the maximal entropy model attempts to
maximize the log likelihood
</bodyText>
<equation confidence="0.987400333333333">
exp Ei Aifi(c, d)
E c′exp Ej Ajfi(c, d),
(1)
</equation>
<bodyText confidence="0.9999252">
where fi(c, d) are feature indicator functions. We
use ME models for both question classification
and question answer ranking. In question answer
context, such function, for instance, could be the
presence or absence of dictionary entities (as pre-
sented in Section 5.2) associated with a particular
class type (either true or false, indicating a sen-
tence can or cannot answer the question). Ai are
the parameters need to be estimated which reflects
the importance of fi(c, d) in prediction.
</bodyText>
<sectionHeader confidence="0.986467" genericHeader="method">
3 Question Classification Features
</sectionHeader>
<bodyText confidence="0.995992166666667">
Li and Roth (2002) have developed a machine
learning approach which uses the SNoW learning
architecture. They have compiled the UIUC ques-
tion classification dataset1 which consists of 5500
training and 500 test questions.2 All questions in
the dataset have been manually labeled according
to the coarse and fine grained categories as shown
in Table 1, with coarse classes (in bold) followed
by their fine classes.
The UIUC dataset has laid a platform for the
follow-up research including (Hacioglu and Ward,
2003; Zhang and Lee, 2003; Li and Roth, 2006;
</bodyText>
<tableCaption confidence="0.9542295">
Table 1: 6 coarse and 50 fine Question types de-
fined in UIUC question dataset.
</tableCaption>
<table confidence="0.985186071428571">
ABBR letter desc NUM
abb other manner code
exp plant reason count
ENTITY product HUMAN date
animal religion group distance
body sport individual money
color substance title order
creative symbol desc other
currency technique LOC period
dis.med. term city percent
event vehicle country speed
food word mountain temp
instrument DESC other size
lang definition state weight
</table>
<bodyText confidence="0.99699440625">
Krishnan et al., 2005; Moschitti et al., 2007). In
contrast to Li and Roth (2006)’s approach which
makes use of a very rich feature set, we propose
to use a compact yet effective feature set. The fea-
tures are briefly described as following. More de-
tailed information can be found at (Huang et al.,
2008).
Question wh-word The wh-word feature is the
question wh-word in given questions. For ex-
ample, the wh-word of question What is the
population of China is what.
Head Word head word is defined as one single
word specifying the object that the question
seeks. For example the head word of What
is a group of turkeys called, is turkeys. This
is different to previous work including (Li
and Roth, 2002; Krishnan et al., 2005) which
has suggested a contiguous span of words
(a group of turkeys in this example). The
single word definition effectively avoids the
noisy information brought by non-head word
of the span (group in this case). A syntac-
tic parser (Petrov and Klein, 2007) and the
Collins rules (Collins, 1999) are modified to
extract such head words.
WordNet Hypernym WordNet hypernyms are
extracted for the head word of a given ques-
tion. The classic Lesk algorithm (Lesk, 1986)
is used to compute the most probable sense
for a head word in the question context, and
then the hypernyms are extracted based on
that sense. The depth of hypernyms is set to
</bodyText>
<footnote confidence="0.9996165">
1Available at http://12r.cs.uiuc.edu/∼cogcomp/Data/QA/QC.
2Test questions are from TREC 10.
</footnote>
<equation confidence="0.98865">
log P(C|D, A) = 11 log
(c,d)∈(C,D)
</equation>
<page confidence="0.982459">
544
</page>
<bodyText confidence="0.999505266666667">
six with trial and error.3 Hypernyms features
capture the general terms of extracted head
word. For instance, the head word of ques-
tion What is the proper name for a female
walrus is extracted as walrus and its direct
hypernyms such as mammal and animal are
extracted as informative features to predict
the correct question type of ENTY:animal.
Unigram words Bag of words features. Such
features provide useful question context in-
formation.
Word shape Five word shape features, namely all
upper case, all lower case, mixed case, all
digits, and other are used to serve as a coarse
named entity recognizer.
</bodyText>
<sectionHeader confidence="0.994655" genericHeader="method">
4 Question Classification Experiments
</sectionHeader>
<bodyText confidence="0.938370744186046">
We train a Maximum Entropy model using the
UIUC 5500 training questions and test over the
500 test questions. Tables 2 shows the accuracy of
6 coarse class and 50 fine grained class, with fea-
tures being fed incrementally. The question classi-
fication performance is measured by accuracy, i.e.,
the proportion of the correctly classified questions
among all test questions. The baseline using the
Table 2: Question classification accuracy using in-
cremental feature sets for 6 and 50 classes over
UIUC split.
6 class 50 class
wh-word 46.0 46.8
+ head word 92.2 82.0
+ hypernym 91.8 85.6
+ unigram 93.0 88.4
+ word shape 93.6 89.0
wh-head word results in 46.0% and 46.8% respec-
tively for 6 coarse and 50 fine class classification.
The incremental use of head word boosts the accu-
racy significantly to 92.2% and 82.0% for 6 and 50
classes. This reflects the informativeness of such
feature. The inclusion of hypernym feature within
6 depths boosts 3.6% for 50 classes, while result-
ing in slight loss for 6 coarse classes. The further
use of unigram feature leads to 2.8% gain in 50
classes. Finally, the use of word shape leads to
0.6% accuracy increase for 50 classes. The best
3We performed 10 cross validation experiment over train-
ing data and tried various depths of 1, 3, 6, 9 and ∞, with ∞
signifies that no depth constraint is imposed.
accuracies achieved are 93.6% and 89.0% for 6
and 50 classes respectively.
The individual feature contributions were dis-
cussed in greater detail in (Huang et al., 2008).
Also, The SVM (rathern than ME model) was em-
ployed using the same feature set and the results
were very close (93.4% for 6 class and 89.2% for
50 class). Table 3 shows the feature ablation ex-
periment4 which is missing in that paper. The
experiment shows that the proposed head word
and its hypernym features play an essential role
in building an accurate question classifier.
</bodyText>
<tableCaption confidence="0.734322333333333">
Table 3: Question classification accuracy by re-
moving one feature at a time for 6 and 50 classes
over UIUC split.
</tableCaption>
<table confidence="0.386975857142857">
6 class 50 class
overall 93.6 89.0
- wh-word 93.6 89.0
- head word 92.8 88.2
- hypernym 90.8 84.2
- unigram 93.6 86.8
- word shape 93.0 88.4
</table>
<bodyText confidence="0.999569">
Our best result feature space only consists of
13’697 binary features and each question has 10
to 30 active features. Compared to the over feature
size of 200’000 in Li and Roth (2002), our feature
space is much more compact, yet turned out to be
more informative as suggested by the experiments.
Table 4 shows the summary of the classification
accuracy of all question classifiers which were ap-
plied to UIUC dataset.5 Our results are summa-
rized in the last row.
In addition, we have performed the 10 cross
validation experiment over the 5500 UIUC train-
ing corpus using our best model. The result is
89.05±1.25 and 83.73±1.61 for 6 and 50 classes,6
which outperforms the best result of 86.1±1.1 for
6 classes as reported in (Moschitti et al., 2007).
</bodyText>
<sectionHeader confidence="0.922618" genericHeader="method">
5 Question Answer Features
</sectionHeader>
<bodyText confidence="0.998232">
For a pair of a question and a candidate sentence,
we extract binary features which include CoNLL
named entities presence feature (NE), dictionary
</bodyText>
<footnote confidence="0.995478222222222">
4Remove one feature at a time from the entire feature set.
5Note (1) that SNoW accuracy without the related word
dictionary was not reported. With the semantically related
word dictionary, it achieved 91%. Note (2) that SNoW with a
semantically related word dictionary achieved 84.2% but the
other algorithms did not use it.
6These results are worse than the result over UIUC split;
as the UIUC test data includes a larger percentage of easily
classified question types.
</footnote>
<page confidence="0.998408">
545
</page>
<tableCaption confidence="0.9980285">
Table 4: Accuracy of all question classifiers which
were applied to UIUC dataset.
</tableCaption>
<table confidence="0.99645675">
Algorithm 6 class 50 class
Li and Roth, SNoW −(1) 78.8(2)
Hacioglu et al., SVM+ECOC − 80.2-82
Zhang &amp; Lee, Linear SVM 87.4 79.2
Zhang &amp; Lee, Tree SVM 90.0 −
Krishnan et al., SVM+CRF 93.4 86.2
Moschitti et al., Kernel 91.8 −
Maximum Entropy Model 93.6 89.0
</table>
<bodyText confidence="0.748384666666667">
entities presence feature (DIC), numerical entities
presence feature (NUM), question specific feature
(SPE), and dependency validity feature (DEP).
</bodyText>
<subsectionHeader confidence="0.969215">
5.1 CoNLL named entities presence feature
</subsectionHeader>
<bodyText confidence="0.999936529411765">
We use Stanford named entity recognizer (NER)
(Finkel et al., 2005) to identify CoNLL style NEs7
as possible answer strings in a candidate sentence
for a given type of question. In particular, if the
question is ABBR type, we tag CoNLL LOC,
ORG and MISC entities as candidate answers; If
the question is HUMAN type, we tag CoNLL PER
and ORG entities; And if the question is LOC
type, we tag CoNLL LOC and MISC entities. For
other types of questions, we assume there is no
candidate CoNLL NEs to tag. We create a binary
feature NE to indicate the presence or absence of
tagged CoNLL entities. Further more, we cre-
ate four binary features NE-PER, NE-LOC, NE-
ORG, and NE-MISC to indicate the presence of
tagged CoNLL PER, LOC, ORG and MISC enti-
ties.
</bodyText>
<subsectionHeader confidence="0.996467">
5.2 Dictionary entities presence feature
</subsectionHeader>
<bodyText confidence="0.999990538461538">
As four types of CoNLL named entities are not
enough to cover 50 question types, we include the
101 dictionary files compiled in the Ephyra project
(Schlaefer et al., 2007). These dictionary files con-
tain names for specific semantic types. For exam-
ple, the actor dictionary comprises a list of actor
names such as Tom Hanks and Kevin Spacey. For
each question, if the head word of such question
(see Section 3) matches the name of a dictionary
file, then each noun phrase in a candidate sentence
is looked up to check its presence in the dictio-
nary. If so, a binary DIC feature is created. For
example, for the question What rank did Chester
</bodyText>
<footnote confidence="0.9241195">
7Person (PER), location (LOC), organization (ORG), and
miscellaneous (MISC).
</footnote>
<bodyText confidence="0.99917224">
Nimitz reach, as there is a military rank dictionary
matches the head word rank, then all the noun
phrases in a candidate sentence are looked up in
the military rank dictionary. As a result, a sen-
tence contains word Admiral will result in the DIC
feature being activated, as such word is present in
the military rank dictionary.
Note that an implementation tip is to allow the
proximity match in the dictionary look up. Con-
sider the question What film introduced Jar Jar
Binks. As there is a match between the ques-
tion head word film and the dictionary named
film, each noun phrase in the candidate sentence
is checked. However, no dictionary entities have
been found from the candidate sentence Best plays
Jar Jar Binks, a floppy-eared, two-legged creature
in “Star Wars: Episode I – The Phantom Men-
ace”, although there is movie entitled Star Wars
Episode I: The Phantom Menace in the dictionary.
Notice that Star Wars: Episode I – The Phantom
Menace in the sentence and the dictionary entity
Star Wars Episode I: The Phantom Menace do not
have exactly identical spelling. The use of prox-
imity look up which allows edit distance being less
than 10% error can resolve this.
</bodyText>
<subsectionHeader confidence="0.990974">
5.3 Numerical entities presence feature
</subsectionHeader>
<bodyText confidence="0.999754588235294">
There are so far no match for question types of
NUM (as shown in Table 1) including NUM:count
and NUM:date etc. These types of questions
seek the numerical answers such as the amount of
money and the duration of period. It is natural to
compile regular expression patterns to match such
entities. For example, for a NUM:money typed
question What is Rohm and Haas’s annual rev-
enue, we compile NUM:money regular expression
pattern which matches the strings of number fol-
lowed by a currency sign ($ and dollars etc). Such
pattern is able to identify 4 billion $ as a candidate
answer in the candidate sentence Rohm and Haas,
with 4 billion $ in annual sales... There are 13 pat-
terns compiled to cover all numerical types. We
create a binary feature NUM to indicate the pres-
ence of possible numerical answers in a sentence.
</bodyText>
<subsectionHeader confidence="0.970826">
5.4 Specific features
</subsectionHeader>
<bodyText confidence="0.999852">
Specific features are question dependent. For ex-
ample, for question When was James Dean born,
any candidate sentence matches the pattern James
Dean (number - number) is likely to answer such
question. We create a binary feature SPE to indi-
cate the presence of such match between a ques-
</bodyText>
<page confidence="0.992022">
546
</page>
<bodyText confidence="0.999199806451613">
tion and a candidate sentence. We list all question
and sentence match patterns which are used in our
experiments as following:
when born feature 1 The question begins with when is/was
and follows by a person name and then follows by key
word born; The candidate sentence contains such per-
son name which follows by the pattern of (number -
number).
when born feature 2 The question begins with when is/was
and follows by a person name and then follows by key
word born; The candidate sentence contains such per-
son name, a NUM:date entity, and a key word born.
where born feature 1 The question begins with where
is/was and follows by a person name and then follows
by key word born; The candidate sentence contains
such person name, a NER LOC entity, and a key word
born.
when die feature 1 The question begins with when did and
follows by a person name and then follows by key word
die; The candidate sentence contains such person name
which follows by the pattern of (number - number).
when die feature 2 The question begins with when did and
follows by a person name and then follows by key
word die; The candidate sentence contains such person
name, a NUM:date entity, and a key word died.
how many feature The question begins with how many and
follows by a noun; The candidate sentence contains a
number and then follows by such noun.
cooccurrent Feature This feature takes two phrase argu-
ments, if the question contains the first phrase and the
candidate sentence contains the second, such feature
would be activated.
Note that the construction of specific features
require the access to aforementioned extracted
named entities. For example, the when born fea-
ture 2 pattern needs the information whether a
candidate sentence contains a NUM:date entity
and where born feature 1 pattern needs the in-
formation whether a candidate sentence contains
a NER LOC entity. Note also that the patterns of
when born feature and when die feature have
similar structure and thus can be simplified in im-
plementation. How many feature can be used
to identify the sentence Amtrak annually serves
about 21 million passengers for question How
many passengers does Amtrak serve annually. The
cooccurrent feature is the most general one. An
example of cooccurrent feature would take the
arguments of marry and husband, or marry and
wife. Such feature would be activated for ques-
tion Whom did Eileen Marie Collins marry and
candidate sentence ... were Collins’ husband,
Pat Youngs, an airline pilot... It is worth noting
that the two arguments are not necessarily differ-
ent. For example, they could be both established,
which makes such feature activated for question
When was the IFC established and candidate sen-
tence IFC was established in 1956 as a member of
the World Bank Group. The reason why we use the
cooccurrence of the word established is due to its
main verb role, which may carry more information
than other words.
</bodyText>
<subsectionHeader confidence="0.943631">
5.5 Dependency validity features
</subsectionHeader>
<bodyText confidence="0.999973">
Like (Cui et al., 2004), we extract the dependency
path from the question word to the common word
(existing in both question and sentence), and the
path from candidate answer (such as CoNLL NE
and numerical entity) to the common word for
each pair of question and candidate sentence using
Stanford dependency parser (Klein and Manning,
2003; Marneffe et al., 2006). For example, for
question When did James Dean die and candidate
sentence In 1955, actor James Dean was killed in
a two-car collision near Cholame, Calif., we ex-
tract the pathes of When:advmod:nsubj:Dean and
1955:prep-in:nsubjpass:Dean for question and
sentence respectively, where advmod and nsubj
etc. are grammatical relations. We propose the
dependency validity feature (DEP) as following.
For all paired paths between a question and a can-
didate sentence, if at least one pair of path in which
all pairs of grammatical relations have been seen
in the training, then the DEP feature is set to be
true, false otherwise. That is, the true validity fea-
ture indicates that at least one pair of path between
the question and candidate sentence is possible to
be a true pair (ie, the candidate noun phrase in the
sentence path is the true answer).
</bodyText>
<sectionHeader confidence="0.978661" genericHeader="method">
6 Question Answer Experiments
</sectionHeader>
<bodyText confidence="0.999829">
Recall that most of the question answer features
depend on the question classifier. For instance,
the NE feature checks the presence or absence of
CoNLL style named entities subject to the clas-
sified question type. In this section, we evaluate
how the quality of question classifiers affects the
question answering performance.
</bodyText>
<subsectionHeader confidence="0.994634">
6.1 Experiment setup
</subsectionHeader>
<bodyText confidence="0.996182666666667">
We use TREC99-03 factoid questions for training
and TREC04 factoid questions for testing. To fa-
cilitate the comparison to others work (Cui et al.,
2004; Shen and Klakow, 2006), we first retrieve
all relevant documents which are compiled by Ken
Litkowski8 to create training and test datasets. We
</bodyText>
<footnote confidence="0.998867">
8Available at http://trec.nist.gov/data/qa.html.
</footnote>
<page confidence="0.995651">
547
</page>
<bodyText confidence="0.999885045454546">
then apply key word search for each question and
retrieve the top 20 relevant sentences. We create
a feature represented data point using each pair of
question and candidate sentence and label it either
true or false depending on whether the sentence
can answer the given question or not. The labeling
is conducted by matching the gold factoid answer
pattern against the candidate sentence.
There are two extra steps performed for train-
ing set but not for test data. In order to construct
a high quality training set, we manually check the
correctness of the training data points and remove
the false positive ones which cannot support the
question although there is a match to gold answer.
In addition, in order to keep the training data well
balanced, we keep maximum four false data points
(question answer pair) for each question but no
limit over the true label data points. In doing so,
we use 1458 questions to compile 8712 training
data points and among them 1752 have true labels.
Similarly, we use 202 questions to compile 4008
test data points and among them 617 have true la-
bels.
We use the training data to train a maximum
entropy model and use such model to rank test
data set. Compared with a classification task (such
as the question classifier), the ranking process re-
quires one extra step: For data points which share
the same question, the probabilities of being pre-
dicted as true label are used to rank the data points.
In align with the previous work, performance is
evaluated using mean reciprocal rank (MRR), top
1 prediction accuracy (top1) and top 5 prediction
accuracy (top5). For the test data set, 157 among
the 202 questions have correct answers found in
retrieved sentences. This leads to the upper bound
of MRR score being 77.8%.
To evaluate how the quality of question clas-
sifiers affects the question answering, we have
created three question classifiers: QC1, QC2
and QC3. The features which are used to train
these question classifiers and their performance
are shown in Table 5. Note that QC3 is the best
question classifier we obtained in Section 4.
</bodyText>
<tableCaption confidence="0.9414165">
Table 5: Features used to train and the perfor-
mance of three question classifiers.
</tableCaption>
<table confidence="0.97382475">
Name features 6 class 50 class
QC1 wh-word 46.0 46.8
QC2 wh-word+ head 92.2 82.0
QC3 All 93.6 89.0
</table>
<subsectionHeader confidence="0.999667">
6.2 Experiment results
</subsectionHeader>
<bodyText confidence="0.9983212">
The first experiment is to evaluate the individ-
ual contribution of various features derived using
three question classifiers. Table 6 shows the base-
line result and results using DIC, NE, NE-4, REG,
SPE, and DEP features. The baseline is the key
word search without the use of maximum entropy
model. As can be seen, the question classifiers
do not affect the DIC feature at all, as DIC fea-
ture does not depend on question classifiers. Bet-
ter question classifier boosts considerable gain for
NE, NE-4 and REG in their contribution to ques-
tion answering. For example, the best question
classifier QC3 outperforms the worst one (QC1)
by 1.5%, 2.0%, and 2.0% MRR scores for NE,
NE-4 and REG respectively. However, it is sur-
prising that the MRR and top5 contribution of NE
and NE-4 decreases if QC1 is replaced by QC2, al-
though the top1 score results in performance gain
slightly. This unexpected results can be partially
explained as follows. For some questions, even
QC2 produces correct predictions, the errors of
NE and NE-4 features may cause over-confident
scores for certain candidate sentences. As SPE and
DEP are not directly dependent on question clas-
sifier, their individual contribution only changes
slightly or remains the same for different ques-
tion classifiers. If the best question classifier is
used, the most important features are SPE and
REG, which can individually boost the MRR score
over 54%, while the others result in less significant
gains.
We now incrementally use various features and
the results are show in Table 6 as well. As can
be seen, the more features and the better question
classifier are used, the higher performance the ME
model has. The inclusion of REG and SPE results
in significant boost for the performance. For ex-
ample, if the best question classifier QC3 is used,
the REG results in 6.9% and 8% gain for MRR
and top1 scores respectively. This is due to a large
portion of NUM type questions in test dataset. The
SPE feature contributes significantly to the per-
formance due to its high precision in answering
birth/death time/location questions. NE and NE-4
result in reasonable gains while DEP feature con-
tributes little. However, this does not mean that
DEP is not important, as once the model reaches a
high MRR score, it becomes hard to improve.
Table 6 clearly shows that the question type
classifier plays an essential role in a high perfor-
</bodyText>
<page confidence="0.99818">
548
</page>
<tableCaption confidence="0.998094">
Table 6: Performance of individual and incremental feature sets for three question classifiers.
</tableCaption>
<table confidence="0.999843222222222">
Individual
Feature MRR Top1 Top5
QC1 QC2 QC3 QC1 QC2 QC3 QC1 QC2 QC3
Baseline 49.9 49.9 49.9 40.1 40.1 40.1 59.4 59.4 59.4
DIC 49.5 49.5 49.5 42.6 42.6 42.6 60.4 60.4 60.4
NE 48.5 47.5 50.0 40.6 40.6 42.6 61.9 60.9 63.4
NE-4 49.5 48.5 51.5 41.6 42.1 44.6 62.4 61.9 64.4
REG 52.0 54.0 54.0 44.1 47.0 47.5 64.4 65.3 65.3
SPE 55.0 55.0 55.0 48.5 48.5 48.5 64.4 64.4 64.4
DEP 51.0 51.5 52.0 43.6 44.1 44.6 65.3 65.8 65.8
Incremental
Baseline 49.9 49.9 49.9 40.1 40.1 40.1 59.4 59.4 59.4
+DIC 49.5 49.5 49.5 42.6 42.6 42.6 60.4 60.4 60.4
+NE 50.0 48.5 51.0 43.1 42.1 44.6 62.9 61.4 64.4
+NE-4 51.5 50.0 53.0 44.1 43.6 46.0 63.4 62.9 65.8
+REG 55.0 56.9 59.9 48.0 51.0 54.0 68.3 68.8 71.8
+SPE 60.4 62.4 65.3 55.4 58.4 61.4 70.8 70.8 73.8
+DEP 61.4 62.9 66.3 55.9 58.4 62.4 71.8 71.8 73.8
</table>
<bodyText confidence="0.991287878787879">
mance question answer system. Assume all the
features are used, the better question classifier sig-
nificantly boosts the overall performance. For ex-
ample, the best question classifier QC3 outper-
forms the worst QC1 by 4.9%, 6.5%, and 2.0%
for MRR, top1 and top5 scores respectively. Even
compared to a good question classifier QC2, the
gain of using QC3 is still 3.4%, 4.0% and 2.0%
for MRR, top1 and top5 scores respectively. One
can imagine that if a fine grained NER is available
(rather than the current four type coarse NER), the
potential gain is much significant.
The reason that the question classifier affects
the question answering performance is straightfor-
ward. As a upstream source, the incorrect classi-
fication of question type would confuse the down-
stream answer search process. For example, for
question What is Rohm and Haas’s annual rev-
enue, our best question classifier is able to clas-
sify it into the correct type of NUM:money and
thus would put $ 4 billion as a candidate answer.
However, the inferior question classifiers misclas-
sify it into HUM:ind type and thereby could not
return a correct answer. Figure 1 shows the indi-
vidual MRR scores for the 42 questions (among
the 202 test questions) which have different pre-
dicted question types using QC3 and QC2. For al-
most all test questions, the accurate question clas-
sifier QC3 achieves higher MRR scores compared
to QC2.
Table 7 shows performance of various question
answer systems including (Tanev et al., 2004; Wu
et al., 2005; Cui et al., 2004; Shen and Klakow,
</bodyText>
<figure confidence="0.9417935">
0 5 10 15 20 25 30 35 40 45
question id
</figure>
<figureCaption confidence="0.844736666666667">
Figure 1: Individual MRR scores for questions
which have different predicted question types us-
ing QC3 and QC2.
</figureCaption>
<bodyText confidence="0.998743285714286">
2006) and this work which were applied to the
same training and test datasets. Among all the sys-
tems, our model can achieve the best MRR score
of 66.3%, which is close to the state of the art of
67.0%. Considering the question answer features
used in this paper are quite standard, the boost is
mainly due to our accurate question classifier.
</bodyText>
<tableCaption confidence="0.997311">
Table 7: Various system performance comparison.
</tableCaption>
<table confidence="0.9799064">
System MRR Top1 Top5
Tanev et al. 2004 57.0 49.0 67.0
Cui et al. 2004 60.0 53.0 70.0
Shen and Klakow, 2006 67.0 62.0 74.0
This work 66.3 62.4 73.8
</table>
<figure confidence="0.978079">
MRR
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
QC3
QC2
</figure>
<page confidence="0.993305">
549
</page>
<sectionHeader confidence="0.998872" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999986615384615">
In this paper, we have presented a question clas-
sifier which makes use of a compact yet effi-
cient feature set. The question classifier outper-
forms previous question classifiers over the stan-
dard UIUC question dataset. We further investi-
gated quantitatively how the quality of question
classifier impacts the performance of question an-
swer system. The experiments showed that an ac-
curate question classifier plays an essential role
in question answering system. With our accurate
question classifier and some standard question an-
swer features, our question answering system per-
forms close to the state of the art.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998758">
We wish to thank the three anonymous review-
ers for their invaluable comments. This re-
search was supported by British Telecom grant
CT1080028046 and BISC Program of UC Berke-
ley.
</bodyText>
<sectionHeader confidence="0.999432" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999585026666667">
A. L. Berger, S. A. D. Pietra, and V. J. D. Pietra. 1996.
A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39–
71.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. PhD thesis, University of
Pennsylvania.
H. Cui, K Li, R. Sun, T. Chua, and M. Kan. 2004. Na-
tional university of singapore at the trec-13 question
answering. In Proc. of TREC 2004, NIST.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Proc. of
ACL, pages 363-370.
D. Hacioglu and W. Ward. 2003. Question classifica-
tion with support vector machines and error correct-
ing codes. In Proc. of the ACL/HLT, vol. 2, pages
28–30.
Z. Huang, M. Thint, and Z. Qin. 2008. Question clas-
sification using head words and their hypernyms. In
Proc. of the EMNLP.
D. Klein and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In Proc. of ACL 2003, vol. 1, pages
423–430.
V. Krishnan, S. Das, and S. Chakrabarti. 2005. En-
hanced answer type inference from questions using
sequential models. In Proc. of the HLT/EMNLP.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In ACM Special Inter-
est Group for Design of Communication Proceed-
ings of the 5th annual international conference on
Systems documentation, pages 24–26.
X. Li and D. Roth. 2002. Learning question classi-
fiers. In the 19th international conference on Com-
putational linguistics, vol. 1, pages 1-7.
X. Li and D. Roth. 2006. Learning question classifiers:
the role of semantic information. Natural Language
Engineering, 12(3):229–249.
C. D. Manning and D. Klein. 2003. Optimization,
maxent models, and conditional estimation with-
out magic. Tutorial at HLT-NAACL 2003 and ACL
2003.
M. D. Marneffe, B. MacCartney and C. D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC 2006.
A. Moschitti, S. Quarteroni, R. Basili and S. Manand-
har 2007. Exploiting syntactic and shallow seman-
tic kernels for question answer classification. In
Proc. of ACL 2007, pages 776-783.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of the HLT-NAACL.
J. Prager. 2006. Open-domain question-answering.
In Foundations and Trends in Information Retrieval,
vol. 1, pages 91-231, 2006.
N. Schlaefer, J. Ko, J. Betteridge, G. Sautter, M. Pathak
and E. Nyberg. 2007. Semantic extensions of the
Ephyra QA system for TREC 2007. In Proc. of the
TREC 2007.
D. Shen and D. Klakow. 2006. Exploring correlation
of dependency relation paths for answer extraction.
In Proc. of the ACL 2006.
H. Tanev, M. Kouylekov, and B. Magnini. 2004.
Combining linguistic processing and web mining for
question answering: Itc-irst at TREC-2004. In Proc.
of the TREC 2004, NIST.
E. M. Voorhees and H. T. Dang. 2005. Overview of
the TREC 2005 question answering track. In Proc.
of the TREC 2005, NIST.
M. Wu, M. Duan, S. Shaikh, S. Small, and T. Strza-
lkowski. 2005. University at Albanys ILQUA in
TREC 2005. In Proc. of the TREC 2005, NIST.
D. Zhang and W. S. Lee. 2003. Question classification
using support vector machines. In The ACM SIGIR
conference in information retrieval, pages 26–32.
</reference>
<page confidence="0.997289">
550
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.536323">
<title confidence="0.999134">Investigation of Question Classifier in Question Answering</title>
<author confidence="0.99918">Zhiheng Marcus Asli</author>
<affiliation confidence="0.857787333333333">EECS Intelligent Systems Research EECS University of British Telecom University of at Chief Technology Office at</affiliation>
<address confidence="0.983844">CA 94720-1776, USA marcus.2.thint@bt.com CA 94720-1776, USA</address>
<email confidence="0.998908">zhiheng@cs.berkeley.eduasli@cs.berkeley.edu</email>
<abstract confidence="0.996115722222222">In this paper, we investigate how an accurate question classifier contributes to a question answering system. We first present a Maximum Entropy (ME) based question classifier which makes use of head word features and their WordNet hypernyms. We show that our question classifier can achieve the state of the art performance in the standard UIUC question dataset. We then investigate quantitatively the contribution of this question classifier to a feature driven question answering system. With our accurate question classifier and some standard question answer features, our question answering system performs close to the state of the art using TREC corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>71</pages>
<contexts>
<context position="4433" citStr="Berger et al., 1996" startWordPosition="698" endWordPosition="701">del which are used in both question classification and question answering ranking. Section 3 presents the features used in question classification. Section 4 presents the question classification 543 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 543–550, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP accuracy over UIUC question dataset. Section 5 presents the question answer features. Section 6 illustrates the results based on TREC question answer dataset. And Section 7 draws the conclusion. 2 Maximum Entropy Models Maximum entropy (ME) models (Berger et al., 1996; Manning and Klein, 2003), also known as log-linear and exponential learning models, provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging, named entity recognition etc. Maximum entropy models can integrate features from many heterogeneous information sources for classification. Each feature corresponds to a constraint on the model. Given a training set of (C, D), where C is a set of class labels and D is a set of feature represented data points, the maximal ent</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. D. Pietra, and V. J. D. Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39– 71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="7677" citStr="Collins, 1999" startWordPosition="1234" endWordPosition="1235">h-word of question What is the population of China is what. Head Word head word is defined as one single word specifying the object that the question seeks. For example the head word of What is a group of turkeys called, is turkeys. This is different to previous work including (Li and Roth, 2002; Krishnan et al., 2005) which has suggested a contiguous span of words (a group of turkeys in this example). The single word definition effectively avoids the noisy information brought by non-head word of the span (group in this case). A syntactic parser (Petrov and Klein, 2007) and the Collins rules (Collins, 1999) are modified to extract such head words. WordNet Hypernym WordNet hypernyms are extracted for the head word of a given question. The classic Lesk algorithm (Lesk, 1986) is used to compute the most probable sense for a head word in the question context, and then the hypernyms are extracted based on that sense. The depth of hypernyms is set to 1Available at http://12r.cs.uiuc.edu/∼cogcomp/Data/QA/QC. 2Test questions are from TREC 10. log P(C|D, A) = 11 log (c,d)∈(C,D) 544 six with trial and error.3 Hypernyms features capture the general terms of extracted head word. For instance, the head word </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-driven statistical models for natural language parsing. PhD thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>K Li</author>
<author>R Sun</author>
<author>T Chua</author>
<author>M Kan</author>
</authors>
<title>National university of singapore at the trec-13 question answering.</title>
<date>2004</date>
<booktitle>In Proc. of TREC</booktitle>
<pages>NIST.</pages>
<contexts>
<context position="19683" citStr="Cui et al., 2004" startWordPosition="3291" endWordPosition="3294">ed for question Whom did Eileen Marie Collins marry and candidate sentence ... were Collins’ husband, Pat Youngs, an airline pilot... It is worth noting that the two arguments are not necessarily different. For example, they could be both established, which makes such feature activated for question When was the IFC established and candidate sentence IFC was established in 1956 as a member of the World Bank Group. The reason why we use the cooccurrence of the word established is due to its main verb role, which may carry more information than other words. 5.5 Dependency validity features Like (Cui et al., 2004), we extract the dependency path from the question word to the common word (existing in both question and sentence), and the path from candidate answer (such as CoNLL NE and numerical entity) to the common word for each pair of question and candidate sentence using Stanford dependency parser (Klein and Manning, 2003; Marneffe et al., 2006). For example, for question When did James Dean die and candidate sentence In 1955, actor James Dean was killed in a two-car collision near Cholame, Calif., we extract the pathes of When:advmod:nsubj:Dean and 1955:prep-in:nsubjpass:Dean for question and sente</context>
<context position="21400" citStr="Cui et al., 2004" startWordPosition="3571" endWordPosition="3574"> a true pair (ie, the candidate noun phrase in the sentence path is the true answer). 6 Question Answer Experiments Recall that most of the question answer features depend on the question classifier. For instance, the NE feature checks the presence or absence of CoNLL style named entities subject to the classified question type. In this section, we evaluate how the quality of question classifiers affects the question answering performance. 6.1 Experiment setup We use TREC99-03 factoid questions for training and TREC04 factoid questions for testing. To facilitate the comparison to others work (Cui et al., 2004; Shen and Klakow, 2006), we first retrieve all relevant documents which are compiled by Ken Litkowski8 to create training and test datasets. We 8Available at http://trec.nist.gov/data/qa.html. 547 then apply key word search for each question and retrieve the top 20 relevant sentences. We create a feature represented data point using each pair of question and candidate sentence and label it either true or false depending on whether the sentence can answer the given question or not. The labeling is conducted by matching the gold factoid answer pattern against the candidate sentence. There are t</context>
<context position="28676" citStr="Cui et al., 2004" startWordPosition="4821" endWordPosition="4824">ssify it into the correct type of NUM:money and thus would put $ 4 billion as a candidate answer. However, the inferior question classifiers misclassify it into HUM:ind type and thereby could not return a correct answer. Figure 1 shows the individual MRR scores for the 42 questions (among the 202 test questions) which have different predicted question types using QC3 and QC2. For almost all test questions, the accurate question classifier QC3 achieves higher MRR scores compared to QC2. Table 7 shows performance of various question answer systems including (Tanev et al., 2004; Wu et al., 2005; Cui et al., 2004; Shen and Klakow, 0 5 10 15 20 25 30 35 40 45 question id Figure 1: Individual MRR scores for questions which have different predicted question types using QC3 and QC2. 2006) and this work which were applied to the same training and test datasets. Among all the systems, our model can achieve the best MRR score of 66.3%, which is close to the state of the art of 67.0%. Considering the question answer features used in this paper are quite standard, the boost is mainly due to our accurate question classifier. Table 7: Various system performance comparison. System MRR Top1 Top5 Tanev et al. 2004 </context>
</contexts>
<marker>Cui, Li, Sun, Chua, Kan, 2004</marker>
<rawString>H. Cui, K Li, R. Sun, T. Chua, and M. Kan. 2004. National university of singapore at the trec-13 question answering. In Proc. of TREC 2004, NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="3486" citStr="Finkel et al., 2005" startWordPosition="552" endWordPosition="555">tribution of this question classifier to a feature driven question answering rank model. It is worth noting that, most of the features we used in question answering rank model, depend on the question type information. For instance, if a question is classified as a type of sport, we then only care about whether there are sport entities existing in the candidate sentences. It is expected that a fine grained named entity recognizer (NER) should make good use of the accurate question type information. However, due to the lack of a fine grained NER tool at hand, we employ the Stanford NER package (Finkel et al., 2005) which identifies only four types of named entities. Even with such a coarse named entity recognizer, the experiments show that the question classifier plays an important role in determining the performance of a question answering system. The rest of the paper is organized as following. Section 2 reviews the maximum entropy model which are used in both question classification and question answering ranking. Section 3 presents the features used in question classification. Section 4 presents the question classification 543 Proceedings of the 2009 Conference on Empirical Methods in Natural Langua</context>
<context position="12930" citStr="Finkel et al., 2005" startWordPosition="2122" endWordPosition="2125">fied question types. 545 Table 4: Accuracy of all question classifiers which were applied to UIUC dataset. Algorithm 6 class 50 class Li and Roth, SNoW −(1) 78.8(2) Hacioglu et al., SVM+ECOC − 80.2-82 Zhang &amp; Lee, Linear SVM 87.4 79.2 Zhang &amp; Lee, Tree SVM 90.0 − Krishnan et al., SVM+CRF 93.4 86.2 Moschitti et al., Kernel 91.8 − Maximum Entropy Model 93.6 89.0 entities presence feature (DIC), numerical entities presence feature (NUM), question specific feature (SPE), and dependency validity feature (DEP). 5.1 CoNLL named entities presence feature We use Stanford named entity recognizer (NER) (Finkel et al., 2005) to identify CoNLL style NEs7 as possible answer strings in a candidate sentence for a given type of question. In particular, if the question is ABBR type, we tag CoNLL LOC, ORG and MISC entities as candidate answers; If the question is HUMAN type, we tag CoNLL PER and ORG entities; And if the question is LOC type, we tag CoNLL LOC and MISC entities. For other types of questions, we assume there is no candidate CoNLL NEs to tag. We create a binary feature NE to indicate the presence or absence of tagged CoNLL entities. Further more, we create four binary features NE-PER, NE-LOC, NEORG, and NE-</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proc. of ACL, pages 363-370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hacioglu</author>
<author>W Ward</author>
</authors>
<title>Question classification with support vector machines and error correcting codes.</title>
<date>2003</date>
<booktitle>In Proc. of the ACL/HLT,</booktitle>
<volume>2</volume>
<pages>28--30</pages>
<contexts>
<context position="6167" citStr="Hacioglu and Ward, 2003" startWordPosition="975" endWordPosition="978">ed to be estimated which reflects the importance of fi(c, d) in prediction. 3 Question Classification Features Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture. They have compiled the UIUC question classification dataset1 which consists of 5500 training and 500 test questions.2 All questions in the dataset have been manually labeled according to the coarse and fine grained categories as shown in Table 1, with coarse classes (in bold) followed by their fine classes. The UIUC dataset has laid a platform for the follow-up research including (Hacioglu and Ward, 2003; Zhang and Lee, 2003; Li and Roth, 2006; Table 1: 6 coarse and 50 fine Question types defined in UIUC question dataset. ABBR letter desc NUM abb other manner code exp plant reason count ENTITY product HUMAN date animal religion group distance body sport individual money color substance title order creative symbol desc other currency technique LOC period dis.med. term city percent event vehicle country speed food word mountain temp instrument DESC other size lang definition state weight Krishnan et al., 2005; Moschitti et al., 2007). In contrast to Li and Roth (2006)’s approach which makes use</context>
</contexts>
<marker>Hacioglu, Ward, 2003</marker>
<rawString>D. Hacioglu and W. Ward. 2003. Question classification with support vector machines and error correcting codes. In Proc. of the ACL/HLT, vol. 2, pages 28–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Huang</author>
<author>M Thint</author>
<author>Z Qin</author>
</authors>
<title>Question classification using head words and their hypernyms.</title>
<date>2008</date>
<booktitle>In Proc. of the EMNLP.</booktitle>
<contexts>
<context position="2404" citStr="Huang et al., 2008" startWordPosition="367" endWordPosition="370">cognition is a key approach in modern question answering systems (Voorhees and Dang, 2005). The question classification is by no means trivial: Simply using question wh-words can not achieve satisfactory results. The difficulty lies in classifying the what and which type questions. Considering the example What is the capital of Yugoslavia, it is of location (city) type, while What is the pH scale is of definition type. As with the previous work of (Li and Roth, 2002; Li and Roth, 2006; Krishnan et al., 2005; Moschitti et al., 2007), we propose a feature driven statistical question classifier (Huang et al., 2008). In particular, we propose head word feature and augment semantic features of such head words using WordNet. In addition, Lesk’s word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, we can obtain accuracy of 89.0% using ME model for 50 fine classes over UIUC dataset. In addition to building an accurate question classifier, we investigate the contribution of this question classifier to a feature driven question answering rank model. It is worth noting that, most of the features we </context>
<context position="6962" citStr="Huang et al., 2008" startWordPosition="1109" endWordPosition="1112">ason count ENTITY product HUMAN date animal religion group distance body sport individual money color substance title order creative symbol desc other currency technique LOC period dis.med. term city percent event vehicle country speed food word mountain temp instrument DESC other size lang definition state weight Krishnan et al., 2005; Moschitti et al., 2007). In contrast to Li and Roth (2006)’s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. The features are briefly described as following. More detailed information can be found at (Huang et al., 2008). Question wh-word The wh-word feature is the question wh-word in given questions. For example, the wh-word of question What is the population of China is what. Head Word head word is defined as one single word specifying the object that the question seeks. For example the head word of What is a group of turkeys called, is turkeys. This is different to previous work including (Li and Roth, 2002; Krishnan et al., 2005) which has suggested a contiguous span of words (a group of turkeys in this example). The single word definition effectively avoids the noisy information brought by non-head word </context>
<context position="10303" citStr="Huang et al., 2008" startWordPosition="1675" endWordPosition="1678">he inclusion of hypernym feature within 6 depths boosts 3.6% for 50 classes, while resulting in slight loss for 6 coarse classes. The further use of unigram feature leads to 2.8% gain in 50 classes. Finally, the use of word shape leads to 0.6% accuracy increase for 50 classes. The best 3We performed 10 cross validation experiment over training data and tried various depths of 1, 3, 6, 9 and ∞, with ∞ signifies that no depth constraint is imposed. accuracies achieved are 93.6% and 89.0% for 6 and 50 classes respectively. The individual feature contributions were discussed in greater detail in (Huang et al., 2008). Also, The SVM (rathern than ME model) was employed using the same feature set and the results were very close (93.4% for 6 class and 89.2% for 50 class). Table 3 shows the feature ablation experiment4 which is missing in that paper. The experiment shows that the proposed head word and its hypernym features play an essential role in building an accurate question classifier. Table 3: Question classification accuracy by removing one feature at a time for 6 and 50 classes over UIUC split. 6 class 50 class overall 93.6 89.0 - wh-word 93.6 89.0 - head word 92.8 88.2 - hypernym 90.8 84.2 - unigram </context>
</contexts>
<marker>Huang, Thint, Qin, 2008</marker>
<rawString>Z. Huang, M. Thint, and Z. Qin. 2008. Question classification using head words and their hypernyms. In Proc. of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. of ACL 2003,</booktitle>
<volume>1</volume>
<pages>423--430</pages>
<contexts>
<context position="20000" citStr="Klein and Manning, 2003" startWordPosition="3343" endWordPosition="3346">e IFC established and candidate sentence IFC was established in 1956 as a member of the World Bank Group. The reason why we use the cooccurrence of the word established is due to its main verb role, which may carry more information than other words. 5.5 Dependency validity features Like (Cui et al., 2004), we extract the dependency path from the question word to the common word (existing in both question and sentence), and the path from candidate answer (such as CoNLL NE and numerical entity) to the common word for each pair of question and candidate sentence using Stanford dependency parser (Klein and Manning, 2003; Marneffe et al., 2006). For example, for question When did James Dean die and candidate sentence In 1955, actor James Dean was killed in a two-car collision near Cholame, Calif., we extract the pathes of When:advmod:nsubj:Dean and 1955:prep-in:nsubjpass:Dean for question and sentence respectively, where advmod and nsubj etc. are grammatical relations. We propose the dependency validity feature (DEP) as following. For all paired paths between a question and a candidate sentence, if at least one pair of path in which all pairs of grammatical relations have been seen in the training, then the D</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In Proc. of ACL 2003, vol. 1, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Krishnan</author>
<author>S Das</author>
<author>S Chakrabarti</author>
</authors>
<title>Enhanced answer type inference from questions using sequential models.</title>
<date>2005</date>
<booktitle>In Proc. of the HLT/EMNLP.</booktitle>
<contexts>
<context position="2297" citStr="Krishnan et al., 2005" startWordPosition="351" endWordPosition="354">ch and verify a candidate answer. In fact, the combination of question classification and the named entity recognition is a key approach in modern question answering systems (Voorhees and Dang, 2005). The question classification is by no means trivial: Simply using question wh-words can not achieve satisfactory results. The difficulty lies in classifying the what and which type questions. Considering the example What is the capital of Yugoslavia, it is of location (city) type, while What is the pH scale is of definition type. As with the previous work of (Li and Roth, 2002; Li and Roth, 2006; Krishnan et al., 2005; Moschitti et al., 2007), we propose a feature driven statistical question classifier (Huang et al., 2008). In particular, we propose head word feature and augment semantic features of such head words using WordNet. In addition, Lesk’s word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, we can obtain accuracy of 89.0% using ME model for 50 fine classes over UIUC dataset. In addition to building an accurate question classifier, we investigate the contribution of this question clas</context>
<context position="6680" citStr="Krishnan et al., 2005" startWordPosition="1058" endWordPosition="1061"> classes. The UIUC dataset has laid a platform for the follow-up research including (Hacioglu and Ward, 2003; Zhang and Lee, 2003; Li and Roth, 2006; Table 1: 6 coarse and 50 fine Question types defined in UIUC question dataset. ABBR letter desc NUM abb other manner code exp plant reason count ENTITY product HUMAN date animal religion group distance body sport individual money color substance title order creative symbol desc other currency technique LOC period dis.med. term city percent event vehicle country speed food word mountain temp instrument DESC other size lang definition state weight Krishnan et al., 2005; Moschitti et al., 2007). In contrast to Li and Roth (2006)’s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. The features are briefly described as following. More detailed information can be found at (Huang et al., 2008). Question wh-word The wh-word feature is the question wh-word in given questions. For example, the wh-word of question What is the population of China is what. Head Word head word is defined as one single word specifying the object that the question seeks. For example the head word of What is a group of turkeys call</context>
</contexts>
<marker>Krishnan, Das, Chakrabarti, 2005</marker>
<rawString>V. Krishnan, S. Das, and S. Chakrabarti. 2005. Enhanced answer type inference from questions using sequential models. In Proc. of the HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In ACM Special Interest Group for Design of Communication Proceedings of the 5th annual international conference on Systems documentation,</booktitle>
<pages>24--26</pages>
<contexts>
<context position="7846" citStr="Lesk, 1986" startWordPosition="1262" endWordPosition="1263"> head word of What is a group of turkeys called, is turkeys. This is different to previous work including (Li and Roth, 2002; Krishnan et al., 2005) which has suggested a contiguous span of words (a group of turkeys in this example). The single word definition effectively avoids the noisy information brought by non-head word of the span (group in this case). A syntactic parser (Petrov and Klein, 2007) and the Collins rules (Collins, 1999) are modified to extract such head words. WordNet Hypernym WordNet hypernyms are extracted for the head word of a given question. The classic Lesk algorithm (Lesk, 1986) is used to compute the most probable sense for a head word in the question context, and then the hypernyms are extracted based on that sense. The depth of hypernyms is set to 1Available at http://12r.cs.uiuc.edu/∼cogcomp/Data/QA/QC. 2Test questions are from TREC 10. log P(C|D, A) = 11 log (c,d)∈(C,D) 544 six with trial and error.3 Hypernyms features capture the general terms of extracted head word. For instance, the head word of question What is the proper name for a female walrus is extracted as walrus and its direct hypernyms such as mammal and animal are extracted as informative features t</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In ACM Special Interest Group for Design of Communication Proceedings of the 5th annual international conference on Systems documentation, pages 24–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In the 19th international conference on Computational linguistics,</booktitle>
<volume>1</volume>
<pages>1--7</pages>
<contexts>
<context position="2255" citStr="Li and Roth, 2002" startWordPosition="343" endWordPosition="346">n suggest different strategies to search and verify a candidate answer. In fact, the combination of question classification and the named entity recognition is a key approach in modern question answering systems (Voorhees and Dang, 2005). The question classification is by no means trivial: Simply using question wh-words can not achieve satisfactory results. The difficulty lies in classifying the what and which type questions. Considering the example What is the capital of Yugoslavia, it is of location (city) type, while What is the pH scale is of definition type. As with the previous work of (Li and Roth, 2002; Li and Roth, 2006; Krishnan et al., 2005; Moschitti et al., 2007), we propose a feature driven statistical question classifier (Huang et al., 2008). In particular, we propose head word feature and augment semantic features of such head words using WordNet. In addition, Lesk’s word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, we can obtain accuracy of 89.0% using ME model for 50 fine classes over UIUC dataset. In addition to building an accurate question classifier, we investig</context>
<context position="5673" citStr="Li and Roth (2002)" startWordPosition="897" endWordPosition="900"> maximize the log likelihood exp Ei Aifi(c, d) E c′exp Ej Ajfi(c, d), (1) where fi(c, d) are feature indicator functions. We use ME models for both question classification and question answer ranking. In question answer context, such function, for instance, could be the presence or absence of dictionary entities (as presented in Section 5.2) associated with a particular class type (either true or false, indicating a sentence can or cannot answer the question). Ai are the parameters need to be estimated which reflects the importance of fi(c, d) in prediction. 3 Question Classification Features Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture. They have compiled the UIUC question classification dataset1 which consists of 5500 training and 500 test questions.2 All questions in the dataset have been manually labeled according to the coarse and fine grained categories as shown in Table 1, with coarse classes (in bold) followed by their fine classes. The UIUC dataset has laid a platform for the follow-up research including (Hacioglu and Ward, 2003; Zhang and Lee, 2003; Li and Roth, 2006; Table 1: 6 coarse and 50 fine Question types defined in UIUC que</context>
<context position="7359" citStr="Li and Roth, 2002" startWordPosition="1179" endWordPosition="1182"> approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. The features are briefly described as following. More detailed information can be found at (Huang et al., 2008). Question wh-word The wh-word feature is the question wh-word in given questions. For example, the wh-word of question What is the population of China is what. Head Word head word is defined as one single word specifying the object that the question seeks. For example the head word of What is a group of turkeys called, is turkeys. This is different to previous work including (Li and Roth, 2002; Krishnan et al., 2005) which has suggested a contiguous span of words (a group of turkeys in this example). The single word definition effectively avoids the noisy information brought by non-head word of the span (group in this case). A syntactic parser (Petrov and Klein, 2007) and the Collins rules (Collins, 1999) are modified to extract such head words. WordNet Hypernym WordNet hypernyms are extracted for the head word of a given question. The classic Lesk algorithm (Lesk, 1986) is used to compute the most probable sense for a head word in the question context, and then the hypernyms are e</context>
<context position="11120" citStr="Li and Roth (2002)" startWordPosition="1824" endWordPosition="1827">nt4 which is missing in that paper. The experiment shows that the proposed head word and its hypernym features play an essential role in building an accurate question classifier. Table 3: Question classification accuracy by removing one feature at a time for 6 and 50 classes over UIUC split. 6 class 50 class overall 93.6 89.0 - wh-word 93.6 89.0 - head word 92.8 88.2 - hypernym 90.8 84.2 - unigram 93.6 86.8 - word shape 93.0 88.4 Our best result feature space only consists of 13’697 binary features and each question has 10 to 30 active features. Compared to the over feature size of 200’000 in Li and Roth (2002), our feature space is much more compact, yet turned out to be more informative as suggested by the experiments. Table 4 shows the summary of the classification accuracy of all question classifiers which were applied to UIUC dataset.5 Our results are summarized in the last row. In addition, we have performed the 10 cross validation experiment over the 5500 UIUC training corpus using our best model. The result is 89.05±1.25 and 83.73±1.61 for 6 and 50 classes,6 which outperforms the best result of 86.1±1.1 for 6 classes as reported in (Moschitti et al., 2007). 5 Question Answer Features For a p</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>X. Li and D. Roth. 2002. Learning question classifiers. In the 19th international conference on Computational linguistics, vol. 1, pages 1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning question classifiers: the role of semantic information.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="2274" citStr="Li and Roth, 2006" startWordPosition="347" endWordPosition="350"> strategies to search and verify a candidate answer. In fact, the combination of question classification and the named entity recognition is a key approach in modern question answering systems (Voorhees and Dang, 2005). The question classification is by no means trivial: Simply using question wh-words can not achieve satisfactory results. The difficulty lies in classifying the what and which type questions. Considering the example What is the capital of Yugoslavia, it is of location (city) type, while What is the pH scale is of definition type. As with the previous work of (Li and Roth, 2002; Li and Roth, 2006; Krishnan et al., 2005; Moschitti et al., 2007), we propose a feature driven statistical question classifier (Huang et al., 2008). In particular, we propose head word feature and augment semantic features of such head words using WordNet. In addition, Lesk’s word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, we can obtain accuracy of 89.0% using ME model for 50 fine classes over UIUC dataset. In addition to building an accurate question classifier, we investigate the contributio</context>
<context position="6207" citStr="Li and Roth, 2006" startWordPosition="983" endWordPosition="986">ce of fi(c, d) in prediction. 3 Question Classification Features Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture. They have compiled the UIUC question classification dataset1 which consists of 5500 training and 500 test questions.2 All questions in the dataset have been manually labeled according to the coarse and fine grained categories as shown in Table 1, with coarse classes (in bold) followed by their fine classes. The UIUC dataset has laid a platform for the follow-up research including (Hacioglu and Ward, 2003; Zhang and Lee, 2003; Li and Roth, 2006; Table 1: 6 coarse and 50 fine Question types defined in UIUC question dataset. ABBR letter desc NUM abb other manner code exp plant reason count ENTITY product HUMAN date animal religion group distance body sport individual money color substance title order creative symbol desc other currency technique LOC period dis.med. term city percent event vehicle country speed food word mountain temp instrument DESC other size lang definition state weight Krishnan et al., 2005; Moschitti et al., 2007). In contrast to Li and Roth (2006)’s approach which makes use of a very rich feature set, we propose </context>
</contexts>
<marker>Li, Roth, 2006</marker>
<rawString>X. Li and D. Roth. 2006. Learning question classifiers: the role of semantic information. Natural Language Engineering, 12(3):229–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>D Klein</author>
</authors>
<title>Optimization, maxent models, and conditional estimation without magic.</title>
<date>2003</date>
<booktitle>Tutorial at HLT-NAACL 2003 and ACL</booktitle>
<contexts>
<context position="4459" citStr="Manning and Klein, 2003" startWordPosition="702" endWordPosition="705"> both question classification and question answering ranking. Section 3 presents the features used in question classification. Section 4 presents the question classification 543 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 543–550, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP accuracy over UIUC question dataset. Section 5 presents the question answer features. Section 6 illustrates the results based on TREC question answer dataset. And Section 7 draws the conclusion. 2 Maximum Entropy Models Maximum entropy (ME) models (Berger et al., 1996; Manning and Klein, 2003), also known as log-linear and exponential learning models, provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging, named entity recognition etc. Maximum entropy models can integrate features from many heterogeneous information sources for classification. Each feature corresponds to a constraint on the model. Given a training set of (C, D), where C is a set of class labels and D is a set of feature represented data points, the maximal entropy model attempts to max</context>
</contexts>
<marker>Manning, Klein, 2003</marker>
<rawString>C. D. Manning and D. Klein. 2003. Optimization, maxent models, and conditional estimation without magic. Tutorial at HLT-NAACL 2003 and ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Marneffe</author>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proc. of LREC</booktitle>
<contexts>
<context position="20024" citStr="Marneffe et al., 2006" startWordPosition="3347" endWordPosition="3350">didate sentence IFC was established in 1956 as a member of the World Bank Group. The reason why we use the cooccurrence of the word established is due to its main verb role, which may carry more information than other words. 5.5 Dependency validity features Like (Cui et al., 2004), we extract the dependency path from the question word to the common word (existing in both question and sentence), and the path from candidate answer (such as CoNLL NE and numerical entity) to the common word for each pair of question and candidate sentence using Stanford dependency parser (Klein and Manning, 2003; Marneffe et al., 2006). For example, for question When did James Dean die and candidate sentence In 1955, actor James Dean was killed in a two-car collision near Cholame, Calif., we extract the pathes of When:advmod:nsubj:Dean and 1955:prep-in:nsubjpass:Dean for question and sentence respectively, where advmod and nsubj etc. are grammatical relations. We propose the dependency validity feature (DEP) as following. For all paired paths between a question and a candidate sentence, if at least one pair of path in which all pairs of grammatical relations have been seen in the training, then the DEP feature is set to be </context>
</contexts>
<marker>Marneffe, MacCartney, Manning, 2006</marker>
<rawString>M. D. Marneffe, B. MacCartney and C. D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proc. of LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>S Quarteroni</author>
<author>R Basili</author>
<author>S Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question answer classification.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>776--783</pages>
<contexts>
<context position="2322" citStr="Moschitti et al., 2007" startWordPosition="355" endWordPosition="358">te answer. In fact, the combination of question classification and the named entity recognition is a key approach in modern question answering systems (Voorhees and Dang, 2005). The question classification is by no means trivial: Simply using question wh-words can not achieve satisfactory results. The difficulty lies in classifying the what and which type questions. Considering the example What is the capital of Yugoslavia, it is of location (city) type, while What is the pH scale is of definition type. As with the previous work of (Li and Roth, 2002; Li and Roth, 2006; Krishnan et al., 2005; Moschitti et al., 2007), we propose a feature driven statistical question classifier (Huang et al., 2008). In particular, we propose head word feature and augment semantic features of such head words using WordNet. In addition, Lesk’s word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, we can obtain accuracy of 89.0% using ME model for 50 fine classes over UIUC dataset. In addition to building an accurate question classifier, we investigate the contribution of this question classifier to a feature drive</context>
<context position="6705" citStr="Moschitti et al., 2007" startWordPosition="1062" endWordPosition="1065">set has laid a platform for the follow-up research including (Hacioglu and Ward, 2003; Zhang and Lee, 2003; Li and Roth, 2006; Table 1: 6 coarse and 50 fine Question types defined in UIUC question dataset. ABBR letter desc NUM abb other manner code exp plant reason count ENTITY product HUMAN date animal religion group distance body sport individual money color substance title order creative symbol desc other currency technique LOC period dis.med. term city percent event vehicle country speed food word mountain temp instrument DESC other size lang definition state weight Krishnan et al., 2005; Moschitti et al., 2007). In contrast to Li and Roth (2006)’s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. The features are briefly described as following. More detailed information can be found at (Huang et al., 2008). Question wh-word The wh-word feature is the question wh-word in given questions. For example, the wh-word of question What is the population of China is what. Head Word head word is defined as one single word specifying the object that the question seeks. For example the head word of What is a group of turkeys called, is turkeys. This is d</context>
<context position="11684" citStr="Moschitti et al., 2007" startWordPosition="1921" endWordPosition="1924">d to the over feature size of 200’000 in Li and Roth (2002), our feature space is much more compact, yet turned out to be more informative as suggested by the experiments. Table 4 shows the summary of the classification accuracy of all question classifiers which were applied to UIUC dataset.5 Our results are summarized in the last row. In addition, we have performed the 10 cross validation experiment over the 5500 UIUC training corpus using our best model. The result is 89.05±1.25 and 83.73±1.61 for 6 and 50 classes,6 which outperforms the best result of 86.1±1.1 for 6 classes as reported in (Moschitti et al., 2007). 5 Question Answer Features For a pair of a question and a candidate sentence, we extract binary features which include CoNLL named entities presence feature (NE), dictionary 4Remove one feature at a time from the entire feature set. 5Note (1) that SNoW accuracy without the related word dictionary was not reported. With the semantically related word dictionary, it achieved 91%. Note (2) that SNoW with a semantically related word dictionary achieved 84.2% but the other algorithms did not use it. 6These results are worse than the result over UIUC split; as the UIUC test data includes a larger p</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>A. Moschitti, S. Quarteroni, R. Basili and S. Manandhar 2007. Exploiting syntactic and shallow semantic kernels for question answer classification. In Proc. of ACL 2007, pages 776-783.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proc. of the HLT-NAACL.</booktitle>
<contexts>
<context position="7639" citStr="Petrov and Klein, 2007" startWordPosition="1226" endWordPosition="1229"> wh-word in given questions. For example, the wh-word of question What is the population of China is what. Head Word head word is defined as one single word specifying the object that the question seeks. For example the head word of What is a group of turkeys called, is turkeys. This is different to previous work including (Li and Roth, 2002; Krishnan et al., 2005) which has suggested a contiguous span of words (a group of turkeys in this example). The single word definition effectively avoids the noisy information brought by non-head word of the span (group in this case). A syntactic parser (Petrov and Klein, 2007) and the Collins rules (Collins, 1999) are modified to extract such head words. WordNet Hypernym WordNet hypernyms are extracted for the head word of a given question. The classic Lesk algorithm (Lesk, 1986) is used to compute the most probable sense for a head word in the question context, and then the hypernyms are extracted based on that sense. The depth of hypernyms is set to 1Available at http://12r.cs.uiuc.edu/∼cogcomp/Data/QA/QC. 2Test questions are from TREC 10. log P(C|D, A) = 11 log (c,d)∈(C,D) 544 six with trial and error.3 Hypernyms features capture the general terms of extracted h</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved inference for unlexicalized parsing. In Proc. of the HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
</authors>
<title>Open-domain question-answering.</title>
<date>2006</date>
<booktitle>In Foundations and Trends in Information Retrieval,</booktitle>
<volume>1</volume>
<pages>91--231</pages>
<contexts>
<context position="1167" citStr="Prager, 2006" startWordPosition="167" endWordPosition="168"> based question classifier which makes use of head word features and their WordNet hypernyms. We show that our question classifier can achieve the state of the art performance in the standard UIUC question dataset. We then investigate quantitatively the contribution of this question classifier to a feature driven question answering system. With our accurate question classifier and some standard question answer features, our question answering system performs close to the state of the art using TREC corpus. 1 Introduction Question answering has drawn significant attention from the last decade (Prager, 2006). It attempts to answer the question posed in natural language by providing the answer phrase rather than the whole documents. An important step in question answering (QA) is to classify the question to the anticipated type of the answer. For example, the question of Who discovered x-rays should be classified into the type of human (individual). This information would narrow down the search space to identify the correct answer string. In addition, this information can suggest different strategies to search and verify a candidate answer. In fact, the combination of question classification and t</context>
</contexts>
<marker>Prager, 2006</marker>
<rawString>J. Prager. 2006. Open-domain question-answering. In Foundations and Trends in Information Retrieval, vol. 1, pages 91-231, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Schlaefer</author>
<author>J Ko</author>
<author>J Betteridge</author>
<author>G Sautter</author>
<author>M Pathak</author>
<author>E Nyberg</author>
</authors>
<date>2007</date>
<booktitle>Semantic extensions of the Ephyra QA system for TREC</booktitle>
<contexts>
<context position="13822" citStr="Schlaefer et al., 2007" startWordPosition="2281" endWordPosition="2284"> ORG entities; And if the question is LOC type, we tag CoNLL LOC and MISC entities. For other types of questions, we assume there is no candidate CoNLL NEs to tag. We create a binary feature NE to indicate the presence or absence of tagged CoNLL entities. Further more, we create four binary features NE-PER, NE-LOC, NEORG, and NE-MISC to indicate the presence of tagged CoNLL PER, LOC, ORG and MISC entities. 5.2 Dictionary entities presence feature As four types of CoNLL named entities are not enough to cover 50 question types, we include the 101 dictionary files compiled in the Ephyra project (Schlaefer et al., 2007). These dictionary files contain names for specific semantic types. For example, the actor dictionary comprises a list of actor names such as Tom Hanks and Kevin Spacey. For each question, if the head word of such question (see Section 3) matches the name of a dictionary file, then each noun phrase in a candidate sentence is looked up to check its presence in the dictionary. If so, a binary DIC feature is created. For example, for the question What rank did Chester 7Person (PER), location (LOC), organization (ORG), and miscellaneous (MISC). Nimitz reach, as there is a military rank dictionary </context>
</contexts>
<marker>Schlaefer, Ko, Betteridge, Sautter, Pathak, Nyberg, 2007</marker>
<rawString>N. Schlaefer, J. Ko, J. Betteridge, G. Sautter, M. Pathak and E. Nyberg. 2007. Semantic extensions of the Ephyra QA system for TREC 2007. In Proc. of the TREC 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>D Klakow</author>
</authors>
<title>Exploring correlation of dependency relation paths for answer extraction.</title>
<date>2006</date>
<booktitle>In Proc. of the ACL</booktitle>
<contexts>
<context position="21424" citStr="Shen and Klakow, 2006" startWordPosition="3575" endWordPosition="3578">the candidate noun phrase in the sentence path is the true answer). 6 Question Answer Experiments Recall that most of the question answer features depend on the question classifier. For instance, the NE feature checks the presence or absence of CoNLL style named entities subject to the classified question type. In this section, we evaluate how the quality of question classifiers affects the question answering performance. 6.1 Experiment setup We use TREC99-03 factoid questions for training and TREC04 factoid questions for testing. To facilitate the comparison to others work (Cui et al., 2004; Shen and Klakow, 2006), we first retrieve all relevant documents which are compiled by Ken Litkowski8 to create training and test datasets. We 8Available at http://trec.nist.gov/data/qa.html. 547 then apply key word search for each question and retrieve the top 20 relevant sentences. We create a feature represented data point using each pair of question and candidate sentence and label it either true or false depending on whether the sentence can answer the given question or not. The labeling is conducted by matching the gold factoid answer pattern against the candidate sentence. There are two extra steps performed</context>
<context position="29343" citStr="Shen and Klakow, 2006" startWordPosition="4944" endWordPosition="4947"> question id Figure 1: Individual MRR scores for questions which have different predicted question types using QC3 and QC2. 2006) and this work which were applied to the same training and test datasets. Among all the systems, our model can achieve the best MRR score of 66.3%, which is close to the state of the art of 67.0%. Considering the question answer features used in this paper are quite standard, the boost is mainly due to our accurate question classifier. Table 7: Various system performance comparison. System MRR Top1 Top5 Tanev et al. 2004 57.0 49.0 67.0 Cui et al. 2004 60.0 53.0 70.0 Shen and Klakow, 2006 67.0 62.0 74.0 This work 66.3 62.4 73.8 MRR 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 1 QC3 QC2 549 7 Conclusion In this paper, we have presented a question classifier which makes use of a compact yet efficient feature set. The question classifier outperforms previous question classifiers over the standard UIUC question dataset. We further investigated quantitatively how the quality of question classifier impacts the performance of question answer system. The experiments showed that an accurate question classifier plays an essential role in question answering system. With our accurate question cl</context>
</contexts>
<marker>Shen, Klakow, 2006</marker>
<rawString>D. Shen and D. Klakow. 2006. Exploring correlation of dependency relation paths for answer extraction. In Proc. of the ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tanev</author>
<author>M Kouylekov</author>
<author>B Magnini</author>
</authors>
<title>Combining linguistic processing and web mining for question answering: Itc-irst at TREC-2004.</title>
<date>2004</date>
<booktitle>In Proc. of the TREC 2004,</booktitle>
<location>NIST.</location>
<contexts>
<context position="28641" citStr="Tanev et al., 2004" startWordPosition="4813" endWordPosition="4816">st question classifier is able to classify it into the correct type of NUM:money and thus would put $ 4 billion as a candidate answer. However, the inferior question classifiers misclassify it into HUM:ind type and thereby could not return a correct answer. Figure 1 shows the individual MRR scores for the 42 questions (among the 202 test questions) which have different predicted question types using QC3 and QC2. For almost all test questions, the accurate question classifier QC3 achieves higher MRR scores compared to QC2. Table 7 shows performance of various question answer systems including (Tanev et al., 2004; Wu et al., 2005; Cui et al., 2004; Shen and Klakow, 0 5 10 15 20 25 30 35 40 45 question id Figure 1: Individual MRR scores for questions which have different predicted question types using QC3 and QC2. 2006) and this work which were applied to the same training and test datasets. Among all the systems, our model can achieve the best MRR score of 66.3%, which is close to the state of the art of 67.0%. Considering the question answer features used in this paper are quite standard, the boost is mainly due to our accurate question classifier. Table 7: Various system performance comparison. Syst</context>
</contexts>
<marker>Tanev, Kouylekov, Magnini, 2004</marker>
<rawString>H. Tanev, M. Kouylekov, and B. Magnini. 2004. Combining linguistic processing and web mining for question answering: Itc-irst at TREC-2004. In Proc. of the TREC 2004, NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
<author>H T Dang</author>
</authors>
<title>question answering track.</title>
<date>2005</date>
<journal>Overview of the TREC</journal>
<booktitle>In Proc. of the TREC</booktitle>
<pages>NIST.</pages>
<contexts>
<context position="1875" citStr="Voorhees and Dang, 2005" startWordPosition="278" endWordPosition="281">er phrase rather than the whole documents. An important step in question answering (QA) is to classify the question to the anticipated type of the answer. For example, the question of Who discovered x-rays should be classified into the type of human (individual). This information would narrow down the search space to identify the correct answer string. In addition, this information can suggest different strategies to search and verify a candidate answer. In fact, the combination of question classification and the named entity recognition is a key approach in modern question answering systems (Voorhees and Dang, 2005). The question classification is by no means trivial: Simply using question wh-words can not achieve satisfactory results. The difficulty lies in classifying the what and which type questions. Considering the example What is the capital of Yugoslavia, it is of location (city) type, while What is the pH scale is of definition type. As with the previous work of (Li and Roth, 2002; Li and Roth, 2006; Krishnan et al., 2005; Moschitti et al., 2007), we propose a feature driven statistical question classifier (Huang et al., 2008). In particular, we propose head word feature and augment semantic feat</context>
</contexts>
<marker>Voorhees, Dang, 2005</marker>
<rawString>E. M. Voorhees and H. T. Dang. 2005. Overview of the TREC 2005 question answering track. In Proc. of the TREC 2005, NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wu</author>
<author>M Duan</author>
<author>S Shaikh</author>
<author>S Small</author>
<author>T Strzalkowski</author>
</authors>
<date>2005</date>
<booktitle>University at Albanys ILQUA in TREC 2005. In Proc. of the TREC</booktitle>
<pages>NIST.</pages>
<contexts>
<context position="28658" citStr="Wu et al., 2005" startWordPosition="4817" endWordPosition="4820">er is able to classify it into the correct type of NUM:money and thus would put $ 4 billion as a candidate answer. However, the inferior question classifiers misclassify it into HUM:ind type and thereby could not return a correct answer. Figure 1 shows the individual MRR scores for the 42 questions (among the 202 test questions) which have different predicted question types using QC3 and QC2. For almost all test questions, the accurate question classifier QC3 achieves higher MRR scores compared to QC2. Table 7 shows performance of various question answer systems including (Tanev et al., 2004; Wu et al., 2005; Cui et al., 2004; Shen and Klakow, 0 5 10 15 20 25 30 35 40 45 question id Figure 1: Individual MRR scores for questions which have different predicted question types using QC3 and QC2. 2006) and this work which were applied to the same training and test datasets. Among all the systems, our model can achieve the best MRR score of 66.3%, which is close to the state of the art of 67.0%. Considering the question answer features used in this paper are quite standard, the boost is mainly due to our accurate question classifier. Table 7: Various system performance comparison. System MRR Top1 Top5 </context>
</contexts>
<marker>Wu, Duan, Shaikh, Small, Strzalkowski, 2005</marker>
<rawString>M. Wu, M. Duan, S. Shaikh, S. Small, and T. Strzalkowski. 2005. University at Albanys ILQUA in TREC 2005. In Proc. of the TREC 2005, NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhang</author>
<author>W S Lee</author>
</authors>
<title>Question classification using support vector machines.</title>
<date>2003</date>
<booktitle>In The ACM SIGIR conference in information retrieval,</booktitle>
<pages>26--32</pages>
<contexts>
<context position="6188" citStr="Zhang and Lee, 2003" startWordPosition="979" endWordPosition="982">reflects the importance of fi(c, d) in prediction. 3 Question Classification Features Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture. They have compiled the UIUC question classification dataset1 which consists of 5500 training and 500 test questions.2 All questions in the dataset have been manually labeled according to the coarse and fine grained categories as shown in Table 1, with coarse classes (in bold) followed by their fine classes. The UIUC dataset has laid a platform for the follow-up research including (Hacioglu and Ward, 2003; Zhang and Lee, 2003; Li and Roth, 2006; Table 1: 6 coarse and 50 fine Question types defined in UIUC question dataset. ABBR letter desc NUM abb other manner code exp plant reason count ENTITY product HUMAN date animal religion group distance body sport individual money color substance title order creative symbol desc other currency technique LOC period dis.med. term city percent event vehicle country speed food word mountain temp instrument DESC other size lang definition state weight Krishnan et al., 2005; Moschitti et al., 2007). In contrast to Li and Roth (2006)’s approach which makes use of a very rich featu</context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>D. Zhang and W. S. Lee. 2003. Question classification using support vector machines. In The ACM SIGIR conference in information retrieval, pages 26–32.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>