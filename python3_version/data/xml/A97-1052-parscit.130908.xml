<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010613">
<title confidence="0.990245">
Automatic Extraction of Subcategorization from Corpora
</title>
<author confidence="0.996926">
Ted Briscoe John Carroll
</author>
<affiliation confidence="0.9989545">
Computer Laboratory Cognitive and Computing Sciences
University of Cambridge University of Sussex
</affiliation>
<address confidence="0.72172">
Pembroke Street, Cambridge CB2 3QG, UK Brighton BN1 9QH, UK
</address>
<email confidence="0.998798">
ejb@cl.cam.ac.uk john.carroll@cogs.susx.ac.uk
</email>
<sectionHeader confidence="0.997385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999626875">
We describe a novel technique and imple-
mented system for constructing a subcate-
gorization dictionary from textual corpora.
Each dictionary entry encodes the relative
frequency of occurrence of a comprehen-
sive set of subcategorization classes for En-
glish. An initial experiment, on a sample
of 14 verbs which exhibit multiple comple-
mentation patterns, demonstrates that the
technique achieves accuracy comparable to
previous approaches, which are all limited
to a highly restricted set of subcategoriza-
tion classes. We also demonstrate that a
subcategorization dictionary built with the
system improves the accuracy of a parser
by an appreciable amount&apos;.
</bodyText>
<sectionHeader confidence="0.99309" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999913266666667">
Predicate subcategorization is a key component of
a lexical entry, because most, if not all, recent syn-
tactic theories &apos;project&apos; syntactic structure from the
lexicon. Therefore, a wide-coverage parser utilizing
such a lexicalist grammar must have access to an
accurate and comprehensive dictionary encoding (at
a minimum) the number and category of a predi-
cate&apos;s arguments and ideally also information about
control with predicative arguments, semantic selec-
tion preferences on arguments, and so forth, to allow
the recovery of the correct predicate-argument struc-
ture. If the parser uses statistical techniques to rank
analyses, it is also critical that the dictionary encode
the relative frequency of distinct subcategorization
classes for each predicate.
</bodyText>
<footnote confidence="0.982475111111111">
1This work was supported by UK DTI/SALT
project 41/5808 &apos;Integrated Language Database&apos;, CEC
Telematics Applications Programme project LE1-2111
&apos;SPARKLE: Shallow PARsing and Knowledge extraction
for Language Engineering&apos;, and by SERC/EPSRC Ad-
vanced Fellowships to both authors. We would like to
thank the COMLEX Syntax development team for al-
lowing us access to pre-release data (for an early exper-
iment), and for useful feedback.
</footnote>
<bodyText confidence="0.99971175">
Several substantial machine-readable subcatego-
rization dictionaries exist for English, either built
largely automatically from machine-readable ver-
sions of conventional learners&apos; dictionaries, or manu-
ally by (computational) linguists (e.g. the Alvey NL
Tools (ANLT) dictionary, Boguraev et al. (1987);
the COMLEX Syntax dictionary, Grishman et al.
(1994)). Unfortunately, neither approach can yield a
genuinely accurate or comprehensive computational
lexicon, because both rest ultimately on the manual
efforts of lexicographers / linguists and are, there-
fore, prone to errors of omission and commission
which are hard or impossible to detect automatically
(e.g. Boguraev &amp; Briscoe, 1989; see also section 3.1
below for an example). Furthermore, manual encod-
ing is labour intensive and, therefore, it is costly to
extend it to neologisms, information not currently
encoded (such as relative frequency of different sub-
categorizations), or other (sub)languages. These
problems are compounded by the fact that predi-
cate subcategorization is closely associated to lexical
sense and the senses of a word change between cor-
pora, sublanguages and/or subject domains (Jensen,
1991).
In a recent experiment with a wide-coverage pars-
ing system utilizing a lexicalist grammatical frame-
work, Briscoe &amp; Carroll (1993) observed that half
of parse failures on unseen test data were caused
by inaccurate subcategorization information in the
ANLT dictionary. The close connection between
sense and subcategorization and between subject do-
main and sense makes it likely that a fully accurate
&apos;static&apos; subcategorization dictionary of a language is
unattainable in any case. Moreover, although Sch-
abes (1992) and others have proposed `lexicalized&apos;
probabilistic grammars to improve the accuracy of
parse ranking, no wide-coverage parser has yet been
constructed incorporating probabilities of different
subcategorizations for individual predicates, because
of the problems of accurately estimating them.
These problems suggest that automatic construc-
tion or updating of subcategorization dictionaries
from textual corpora is a more promising avenue
to pursue. Preliminary experiments acquiring a few
</bodyText>
<page confidence="0.997589">
356
</page>
<bodyText confidence="0.999830740740741">
verbal subcategorization classes have been reported
by Brent (1991, 1993), Manning (1993), and Ush-
ioda et at. (1993). In these experiments the max-
imum number of distinct subcategorization classes
recognized is sixteen, and only Ushioda et at. at-
tempt to derive relative subcategorization frequency
for individual predicates.
We describe a new system capable of distinguish-
ing 160 verbal subcategorization classesâ€”a superset
of those found in the ANLT and COMLEX Syn-
tax dictionaries. The classes also incorporate infor-
mation about control of predicative arguments and
alternations such as particle movement and extra-
position. We report an initial experiment which
demonstrates that this system is capable of acquir-
ing the subcategorization classes of verbs and the
relative frequencies of these classes with compara-
ble accuracy to the less ambitious extant systems.
We achieve this performance by exploiting a more
sophisticated robust statistical parser which yields
complete though &apos;shallow&apos; parses, a more compre-
hensive subcategorization class classifier, and a pri-
ori estimates of the probability of membership of
these classes. We also describe a small-scale ex-
periment which demonstrates that subcategorization
class frequency information for individual verbs can
be used to improve parsing accuracy.
</bodyText>
<sectionHeader confidence="0.455334" genericHeader="introduction">
2 Description of the System
</sectionHeader>
<subsectionHeader confidence="0.914917">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.99970325">
The system consists of the following six components
which are applied in sequence to sentences contain-
ing a specific predicate in order to retrieve a set of
subcategorization classes for that predicate:
</bodyText>
<listItem confidence="0.993096285714286">
1. A tagger, a first-order HMM part-of-speech
(PoS) and punctuation tag disambiguator, is
used to assign and rank tags for each word and
punctuation token in sequences of sentences (El-
worthy, 1994).
2. A lemmatizer is used to replace word-tag
pairs with lemma-tag pairs, where a lemma is
the morphological base or dictionary headword
form appropriate for the word, given the PoS
assignment made by the tagger. We use an en-
hanced version of the GATE project stemmer
(Cunningham et at., 1995).
3. A probabilistic LR parser, trained on a tree-
bank, returns ranked analyses (Briscoe &amp; Car-
roll, 1993; Carroll, 1993, 1994), using a gram-
mar written in a feature-based unification gram-
mar formalism which assigns &apos;shallow&apos; phrase
structure analyses to tag networks (or &apos;lattices&apos;)
returned by the tagger (Briscoe &amp; Carroll, 1994,
1995; Carroll Sz Briscoe, 1996).
4. A patternset extractor which extracts sub-
categorization patterns, including the syntac-
tic categories and head lemmas of constituents,
from sentence subanalyses which begin/end at
the boundaries of (specified) predicates.
5. A pattern classifier which assigns patterns in
patternsets to subcategorization classes or re-
jects patterns as unclassifiable on the basis of
the feature values of syntactic categories and
the head lemmas in each pattern.
6. A patternsets evaluator which evaluates sets
of patternsets gathered for a (single) predicate,
constructing putative subcategorization entries
and filtering the latter on the basis of their re-
liability and likelihood.
</listItem>
<bodyText confidence="0.988680666666667">
For example, building entries for attribute, and
given that one of the sentences in our data was (la),
the tagger and lemmatizer return (lb).
</bodyText>
<equation confidence="0.836819">
(1) a He attributed his failure, he said, to
no&lt;blank&gt;one buying his books.
b he_PPHS1 attribute_VVD his_APPS fail-
ure_NN1 he_PPHS1 say_VVD to_II
no&lt;blank&gt; one_PN buy_V VG his_APP$
book_NN2
</equation>
<bodyText confidence="0.996057071428572">
(lb) is parsed successfully by the probabilistic LR
parser, and the ranked analyses are returned. Then
the patternset extractor locates the subanalyses con-
taining attribute and constructs a patternset. The
highest ranked analysis and pattern for this example
are shown in Figure 12. Patterns encode the value
of the VSUBCAT feature from the VP rule and the
head lemma(s) of each argument. In the case of PP
(P2) arguments, the pattern also encodes the value of
PSUBCAT from the PP rule and the head lemma(s)
of its complement(s). In the next stage of process-
ing, patterns are classified, in this case giving the
subcategorization class corresponding to transitive
plus PP with non-finite clausal complement.
The system could be applied to corpus data by
first sorting sentences into groups containing in-
stances of a specified predicate, but we use a different
strategy since it is more efficient to tag, lemmatize
and parse a corpus just once, extracting patternsets
for all predicates in each sentence; then to classify
the patterns in all patternsets; and finally, to sort
and recombine patternsets into sets of patternsets,
one set for each distinct predicate containing pat-
ternsets of just the patterns relevant to that predi-
cate. The tagger, lemmatizer, grammar and parser
have been described elsewhere (see previous refer-
ences), so we provide only brief relevant details here,
concentrating on the description of the components
</bodyText>
<footnote confidence="0.999004666666667">
2The analysis shows only category aliases rather than
sets of feature-value pairs. Ta represents a text adjunct
delimited by commas (Nunberg 1990; Briscoe 86 Carroll,
1994). Tokens in the patternset are indexed by sequen-
tial position in the sentence so that two or more tokens
of the same type can be kept distinct in patterns.
</footnote>
<page confidence="0.976645">
357
</page>
<equation confidence="0.993385260869565">
(Tp
(V2 (N2 he_PPHS1)
(V1 (VO attribute_VVD)
(N2 (DT his_APP8)
(Ni
(NO (NO failure_NN1)
(Ta (Pu ,_,)
(V2 (N2 he_PPHS1)
(V1 (VO say_VVD))) (Pu
(P2
(P1 (PO to_II)
(N2 no&lt;blank&gt;one_PN)
(V1 (VO buy_VVG) (N2 (DT his_APP$) (Ni
(1 ((((he:1 PPHS1))
(VSUBCAT NP_PP)
((attribute:6 VVD))
((failure:8 NN1))
((PSUBCAT SING)
((to:9 II))
((no&lt;blank&gt;one:10 PN))
((buy:11 VVG))))
1))
(NO book_NN2)))))))))
</equation>
<figureCaption confidence="0.998219">
Figure 1: Highest-ranked analysis and patternset for (lb)
</figureCaption>
<bodyText confidence="0.999383323529412">
of the system that are new: the extractor, classifier
and evaluator.
The grammar consists of 455 phrase structure
rule schemata in the format accepted by the parser
(a syntactic variant of a Definite Clause Grammar
with iterative (Kleene) operators). It is &apos;shallow&apos; in
that no atof which thetempt is made to fully anal-
yse unbounded dependencies. However, the distinc-
tion between arguments and adjuncts is expressed,
following X-bar theory (e.g. Jackendoff, 1977), by
Chomsky-adjunction to maximal projections of ad-
juncts (XP XP Adjunct) as opposed to &apos;govern-
ment&apos; of arguments (i.e. arguments are sisters within
X1 projections; X1 XO Argl... ArgN). Further-
more, all analyses are rooted (in S) so the grammar
assigns global, shallow and often &apos;spurious&apos; analy-
ses to many sentences. There are 29 distinct val-
ues for VSUBCAT and 10 for PSUBCAT; these are
analysed in patterns along with specific closed-class
head lemmas of arguments, such as it (dummy sub-
jects), whether (wh-complements), and so forth, to
classify patterns as evidence for one of the 160 sub-
categorization classes. Each of these classes can be
parameterized for specific predicates by, for exam-
ple, different prepositions or particles. Currently,
the coverage of this grammarâ€”the proportion of sen-
tences for which at least one analysis is foundâ€”is
79% when applied to the Susanne corpus (Sampson,
1995), a 138K word treebanked and balanced subset
of the Brown corpus. Wide coverage is important
since information is acquired only from successful
parses. The combined throughput of the parsing
components on a Sun UltraSparc 1/140 is around
50 words per CPU second.
</bodyText>
<subsectionHeader confidence="0.971178">
2.2 The Extractor, Classifier and Evaluator
</subsectionHeader>
<bodyText confidence="0.999994891304348">
The extractor takes as input the ranked analyses
from the probabilistic parser. It locates the subanal-
yses around the predicate, finding the constituents
identified as complements inside each subanalysis,
and the subject clause preceding it. Instances of
passive constructions are recognized and treated spe-
cially. The extractor returns the predicate, the
VSUBCAT value, and just the heads of the comple-
ments (except in the case of PPs, where it returns
the PSUBCAT value, the preposition head, and the
heads of the PP&apos;s complements).
The subcategorization classes recognized by the
classifier were obtained by manually merging the
classes exemplified in the COMLEX Syntax and
ANLT dictionaries and adding around 30 classes
found by manual inspection of unclassifiable pat-
terns for corpus examples during development of the
system. These consisted of some extra patterns for
phrasal verbs with complex complementation and
with flexible ordering of the preposition/particle,
some for non-passivizable patterns with a surface
direct object, and some for rarer combinations of
governed preposition and complementizer combina-
tions. The classifier filters out as unclassifiable
around 15% of patterns found by the extractor when
run on all the patternsets extracted from the Su-
sanne corpus. This demonstrates the value of the
classifier as a filter of spurious analyses, as well as
providing both translation between extracted pat-
terns and two existing subcategorization dictionar-
ies and a definition of the target subcategorization
dictionary.
The evaluator builds entries by taking the pat-
terns for a given predicate built from successful
parses and records the number of observations of
each subcategorization class. Patterns provide sev-
eral types of information which can be used to rank
or select between patterns in the patternset for a
given sentence exemplifying an instance of a pred-
icate, such as the ranking of the parse from which
it was extracted or the proportion of subanalyses
supporting a specific pattern. Currently, we simply
select the pattern supported by the highest ranked
parse. However, we are experimenting with alterna-
tive approaches. The resulting set of putative classes
for a predicate are filtered, following Brent (1993),
</bodyText>
<page confidence="0.993064">
358
</page>
<bodyText confidence="0.99982636">
by hypothesis testing on binomial frequency data.
Evaluating putative entries on binomial frequency
data requires that we record the total number of
patternsets n for a given predicate, and the number
of these patternsets containing a pattern support-
ing an entry for given class m. These figures are
straightforwardly computed from the output of the
classifier; however, we also require an estimate of the
probability that a pattern for class i will occur with
a verb which is not a member of subcategorization
class i. Brent proposes estimating these probabili-
ties experimentally on the basis of the behaviour of
the extractor. We estimate this probability more di-
rectly by first extracting the number of verbs which
are members of each class in the ANLT dictionary
(with intuitive estimates for the membership of the
novel classes) and converting this to a probability of
class membership by dividing by the total number of
verbs in the dictionary; and secondly, by multiplying
the complement of these probabilities by the proba-
bility of a pattern for class i, defined as the number
of patterns for i extracted from the Susanne corpus
divided by the total number of patterns. So, p(v -i),
the probability of verb v not of class i occurring with
a pattern for class i is:
</bodyText>
<equation confidence="0.976945">
p(v -i) = (1 I anIt_verbs_in_class_il Ipatterns_f or_i
n!
P(m, n, p) = mi(p Pm(1 P)n-m
n im
</equation>
<bodyText confidence="0.972121">
The probability of the event happening m or more
times is:
</bodyText>
<equation confidence="0.9097255">
P(m+, n,p) = E P(i,n,p)
i=m
</equation>
<bodyText confidence="0.999167285714286">
Thus P(m,n,p(v -i)) is the probability that m or
more occurrences of patterns for i will occur with
a verb which is not a member of i, given n occur-
rences of that verb. Setting a threshold of less than
or equal to 0.05 yields a 95% or better confidence
that a high enough proportion of patterns for i have
been observed for the verb to be in class i3.
</bodyText>
<subsectionHeader confidence="0.850723">
2.3 Discussion
</subsectionHeader>
<bodyText confidence="0.99795">
Our approach to acquiring subcategorization classes
is predicated on the following assumptions:
</bodyText>
<listItem confidence="0.997707">
â€¢ most sentences will not allow the application of
all possible rules of English complementation;
â€¢ some sentences will be unambiguous even given
the indeterminacy of the grammar4;
</listItem>
<footnote confidence="0.95150575">
3Brent (1993:249-253) provides a detailed explana-
tion and justification for the use of this measure.
41n fact, 5% of sentences in Susanne are assigned only
a single analysis by the grammar.
</footnote>
<listItem confidence="0.9996168">
â€¢ many incorrect analyses will yield patterns
which are unclassifiable, and are thus filtered
out;
â€¢ arguments of a specific verb will occur with
greater frequency than adjuncts (in potential
argument positions);
â€¢ the patternset generator will incorrectly output
patterns for certain classes more often than oth-
ers; and
â€¢ even a highest ranked paâ–ª ttern for i is only a
</listItem>
<bodyText confidence="0.999716419354839">
probabilistic cue for membership of i, so mem-
bership should only be inferred if there are
enough occurrences of patterns for i in the data
to outweigh the error probability for i.
This simple automated, hybrid linguis-
tic/statistical approach contrasts with the manual
linguistic analysis of the COMLEX Syntax lexicog-
raphers (Meyers et at., 1994), who propose five cri-
teria and five heuristics for argument-hood and six
criteria and two heuristics for adjunct-hood, culled
mostly from the linguistics literature. Many of these
are not exploitable automatically because they rest
on semantic judgements which cannot (yet) be made
automatically: for example, optional arguments are
often &apos;understood&apos; or implied if missing. Others are
syntactic tests involving diathesis alternation possi-
bilities (e.g. passive, dative movement, Levin (1993))
which require recognition that the &apos;same&apos; argument,
defined usually by semantic class / thematic role, is
occurring across argument positions. We hope to ex-
ploit this information where possible at a later stage
in the development of our approach. However, recog-
nizing same/similar arguments requires considerable
quantities of lexical data or the ability to back-off to
lexical semantic classes. At the moment, we exploit
linguistic information about the syntactic type, obli-
gatoriness and position of arguments, as well as the
set of possible subcategorization classes, and com-
bine this with statistical inference based on the prob-
ability of class membership and the frequency and
reliability of patterns for classes.
</bodyText>
<sectionHeader confidence="0.998006" genericHeader="method">
3 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.990443">
3.1 Lexicon Evaluation â€” Method
</subsectionHeader>
<bodyText confidence="0.999989769230769">
In order to test the accuracy of our system (as de-
veloped so far) and to provide empirical feedback
for further development, we took the Susanne, SEC
(Taylor &amp; Knowles, 1988) and LOB corpora (Gar-
side et at., 1987)â€”a total of 1.2 million wordsâ€”and
extracted all sentences containing an occurrence of
one of fourteen verbs, up to a maximum of 1000
citations of each. These verbs, listed in Figure 2,
were chosen at random, subject to the constraint
that they exhibited multiple complementation pat-
terns. The sentences containing these verbs were
tagged and parsed automatically, and the extractor,
classifier and evaluator were applied to the resulting
</bodyText>
<equation confidence="0.655916">
lanit_verbs1 &apos;patterns&apos;
</equation>
<bodyText confidence="0.893887">
The binomial distribution gives the probability of an
event with probability p happening exactly m times
out of n attempts:
</bodyText>
<page confidence="0.995452">
359
</page>
<bodyText confidence="0.999818106382979">
successful analyses. The citations from which entries
were derived totaled approximately 70K words.
The results were evaluated against a merged entry
for these verbs from the ANLT and COMLEX Syn-
tax dictionaries, and also against a manual analysis
of the corpus data for seven of the verbs. The process
of evaluating the performance of the system relative
to the dictionaries could, in principle, be reduced to
an automated report of type precision (percentage of
correct subcategorization classes to all classes found)
and recall (percentage of correct classes found in the
dictionary entry). However, since there are disagree-
ments between the dictionaries and there are classes
found in the corpus data that are not contained in
either dictionary, we report results relative both to a
manually merged entry from ANLT and COMLEX,
and also, for seven of the verbs, to a manual anal-
ysis of the actual corpus data. The latter analysis
is necessary because precision and recall measures
against the merged entry will still tend to yield in-
accurate results as the system cannot acquire classes
not exemplified in the data, and may acquire classes
incorrectly absent from the dictionaries.
We illustrate these problems with reference to
seem, where there is overlap, but not agreement
between the COMLEX and ANLT entries. Thus,
both predict that seem will occur with a sentential
complement and dummy subject, but only ANLT
predicts the possibility of a `wh&apos; complement and
only COMLEX predicts the (optional) presence of
a PP[to] argument with the sentential complement.
One ANLT entry covers two COMLEX entries given
the different treatment of the relevant complements
but the classifier keeps them distinct. The corpus
data for seem contains examples of further classes
which we judge valid, in which seem can take a
PP[tol and infinitive complement, as in he seems to
me to be insane, and a passive participle, as in he
seemed depressed. This comparison illustrates the
problem of errors of omission common to computa-
tional lexicons constructed manually and also from
machine-readable dictionaries. All classes for seem
are exemplified in the corpus data, but for ask, for
example, eight classes (out of a possible 27 in the
merged entry) are not present, so comparison only
to the merged entry would give an unreasonably low
estimate of recall.
</bodyText>
<subsectionHeader confidence="0.998893">
3.2 Lexicon Evaluation â€” Results
</subsectionHeader>
<bodyText confidence="0.9998808">
Figure 2 gives the raw results for the merged en-
tries and corpus analysis on each verb. It shows the
number of true positives (TP), correct classes pro-
posed by our system, false positives (FP), incorrect
classes proposed by our system, and false negatives
(FN), correct classes not proposed by our system,
as judged against the merged entry, and, for seven
of the verbs, against the corpus analysis. It also
shows, in the final column, the number of sentences
from which classes were extracted.
</bodyText>
<table confidence="0.972512642857143">
Dictionary Corpus
(14 verbs) (7 verbs)
Precision 65.7% 76.6%
Recall 35.5% 43.4%
Figure 3: Type precision and recall
Ranking Accuracy
ask 75.0%
begin 100.0%
believe 66.7%
cause 100.0%
give 70.0%
seem 75.0%
swing 83.3%
Mean 81.4%
</table>
<figureCaption confidence="0.998641">
Figure 4: Ranking accuracy of classes
</figureCaption>
<bodyText confidence="0.9955635">
Figure 3 gives the type precision and recall of
our system&apos;s recognition of subcategorization classes
as evaluated against the merged dictionary entries
(14 verbs) and against the manually analysed cor-
pus data (7 verbs). The frequency distribution of
the classes is highly skewed: for example for believe,
there are 107 instances of the most common class in
the corpus data, but only 6 instances in total of the
least common four classes. More generally, for the
manually analysed verbs, almost 60% of the false
negatives have only one or two exemplars each in
the corpus citations. None of them are returned by
the system because the binomial filter always rejects
classes hypothesised on the basis of such little evi-
dence.
In Figure 4 we estimate the accuracy with which
our system ranks true positive classes against the
correct ranking for the seven verbs whose corpus in-
put was manually analysed. We compute this mea-
sure by calculating the percentage of pairs of classes
at positions (n, m) s.t. n &lt; m in the system rank-
ing that are ordered the same in the correct ranking.
This gives us an estimate of the accuracy of the rel-
ative frequencies of classes output by the system.
For each of the seven verbs for which we under-
took a corpus analysis, we calculate the token recall
of our system as the percentage (over all exemplars)
of true positives in the corpus. This gives us an es-
timate of the parsing performance that would result
from providing a parser with entries built using the
system, shown in Figure 5.
Further evaluation of the results for these seven
verbs reveals that the filtering phase is the weak
link in the systeni. There are only 13 true negatives
which the system failed to propose, each exemplified
in the data by a mean of 4.5 examples. On the other
hand, there are 67 false negatives supported by an
estimated mean of 7.1 examples which should, ide-
</bodyText>
<page confidence="0.9943">
360
</page>
<table confidence="0.99840137037037">
Merged FP Entry Corpus FP Data No. of
TP FN TP FN Sentences
ask 9 0 18 9 0 10 390
begin 4 1 7 4 1 7 311
believe 4 4 11 4 4 8 230
cause 2 3 6 2 3 5 95
expect 6 5 3 - - - 223
find 5 7 15 - - - 645
give 5 2 11 5 2 5 639
help 6 3 8 - - - 223
like 3 2 7 - - - 228
move 4 3 9 - - - 217
produce 2 1 3 - - 152
provide 3 2 6 - - - 217
seem 8 1 4 8 1 4 534
swing 4 0 10 4 0 8 45
Totals 65 34 118 36 11 47 4149
Figure 2: Raw results for test of 14 verbs
Token Recall
ask 78.5%
begin 73.8%
believe 34.5%
cause 92.1%
give 92.2%
seem 84.7%
swing 39.2%
Mean 80.9%
</table>
<figureCaption confidence="0.99774">
Figure 5: Token recall
</figureCaption>
<bodyText confidence="0.9999859">
ally, have been accepted by the filter, and 11 false
positives which should have been rejected. The per-
formance of the filter for classes with less than 10
exemplars is around chance, and a simple heuris-
tic of accepting all classes with more than 10 exem-
plars would have produced broadly similar results
for these verbs. The filter may well be performing
poorly because the probability of generating a sub-
categorization class for a given verb is often lower
than the error probability for that class.
</bodyText>
<subsectionHeader confidence="0.999859">
3.3 Parsing Evaluation
</subsectionHeader>
<bodyText confidence="0.999269166666667">
In addition to evaluating the acquired subcategoriza-
tion information against existing lexical resources,
we have also evaluated the information in the con-
text of an actual parsing system. In particular we
wanted to establish whether the subcategorization
frequency information for individual verbs could be
used to improve the accuracy of a parser that uses
statistical techniques to rank analyses.
The experiment used the same probabilistic parser
and tag sequence grammar as are present in the
acquisition system (see references above)â€”although
the experiment does not in any way rely on the
</bodyText>
<table confidence="0.9960045">
Mean Recall Precision
crossings
&apos;Baseline&apos; 1.00 70.7% 72.3%
Lexicalised 0.93 71.4% 72.9%
</table>
<figureCaption confidence="0.9138095">
Figure 6: GEIG evaluation metrics for parser against
Susanne bracketings
</figureCaption>
<bodyText confidence="0.999797666666667">
parsers or grammars being the same. We ran-
domly selected a test set of 250 in-coverage sen-
tences (of lengths 3-56 tokens, mean 18.2) from the
Susanne treebank, retagged with possibly multiple
tags per word, and measured the &apos;baseline&apos; accu-
racy of the unlexicalized parser on the sentences us-
ing the now standard PARSEVAL/GEIG evaluation
metrics of mean crossing brackets per sentence and
(unlabelled) bracket recall and precision (e.g. Gr-
ishman et al., 1992); see figure 65. Next, we col-
lected all words in the test corpus tagged as possi-
bly being verbs (giving a total of 356 distinct lem-
mas) and retrieved all citations of them in the LOB
corpus, plus Susanne with the 250 test sentences
excluded. We acquired subcategorization and as-
sociated frequency information from the citations,
in the process successfully parsing 380K words. We
then parsed the test set, with each verb subcate-
gorization possibility weighted by its raw frequency
score, and using the naive add-one smoothing tech-
nique to allow for omitted possibilities. The GEIG
measures for the lexicalized parser show a 7% im-
provement in the crossing bracket score (figure 6).
Over the existing test corpus this is not statisti-
</bodyText>
<footnote confidence="0.89542325">
&apos;Carroll &amp; Briscoe (1996) use the same test set, al-
though the baseline results reported here differ slightly
due to differences in the mapping from parse trees to
Susanne-compatible bracketings.
</footnote>
<page confidence="0.997622">
361
</page>
<bodyText confidence="0.999765909090909">
cally significant at the 95% level (paired t-test, 1.21,
249 df, p = 0.11)â€”although if the pattern of differ-
ences were maintained over a larger test set of 470
sentences it would be significant. We expect that
a more sophisticated smoothing technique, a larger
acquisition corpus, and extensions to the system to
deal with nominal and adjectival predicates would
improve accuracy still further. Nevertheless, this
experiment demonstrates that lexicalizing a gram-
mar/parser with subcategorization frequencies can
appreciably improve the accuracy of parse ranking.
</bodyText>
<sectionHeader confidence="0.999581" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999884155555556">
Brent&apos;s (1993) approach to acquiring subcategoriza-
tion is based on a philosophy of only exploiting un-
ambiguous and determinate information in unanal-
ysed corpora. He defines a number of lexical pat-
terns (mostly involving closed class items, such as
pronouns) which reliably cue one of five subcatego-
rization classes. Brent does not report comprehen-
sive results, but for one class, sentential complement
verbs, he achieves 96% precision and 76% recall at
classifying individual tokens of 63 distinct verbs as
exemplars or non-exemplars of this class. He does
not attempt to rank different classes for a given verb.
Ushioda et al. (1993) utilise a PoS tagged corpus
and finite-state NP parser to recognize and calcu-
late the relative frequency of six subcategorization
classes. They report an accuracy rate of 83% (254
errors) at classifying 1565 classifiable tokens of 33
distinct verbs in running text and suggest that in-
correct noun phrase boundary detection accounts for
the majority of errors. They report that for 32 verbs
their system correctly predicts the most frequent
class, and for 30 verbs it correctly predicts the sec-
ond most frequent class, if there was one. Our sys-
tem rankings include all classes for each verb, from
a total of 160 classes, and average 81.4% correct.
Manning (1993) conducts a larger experiment,
also using a PoS tagged corpus and a finite-state
NP parser, attempting to recognize sixteen distinct
complementation patterns. He reports that for a test
sample of 200 tokens of 40 verbs in running text, the
acquired subcategorization dictionary listed the ap-
propriate entry for 163 cases, giving a token recall of
82% (as compared with 80.9% in our experiment).
He also reports a comparison of acquired entries for
the verbs to the entries given in the Oxford Advanced
Learner&apos;s Dictionary of Current English (Hornby,
1989) on which his system achieves a precision of
90% and a recall of 43%. His system averages 3.48
subentries (maximum 10)â€”less then half the num-
ber produced in our experiment. It is not clear what
level of evidence the performance of Manning&apos;s sys-
tem is based on, but the system was applied to 4.1
million words of text (c.f. our 1.2 million words) and
the verbs are all common, so it is likely that consid-
erably more exemplars of each verb were available.
</bodyText>
<sectionHeader confidence="0.973327" genericHeader="conclusions">
5 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.999987555555556">
The experiment and comparison reported above sug-
gests that our more comprehensive subcategoriza-
tion class extractor is able both to assign classes
to individual verbal predicates and also to rank
them according to relative frequency with compa-
rable accuracy to extant systems. We have also
demonstrated that a subcategorization dictionary
built with the system can improve the accuracy of a
probabilistic parser by an appreciable amount.
The system we have developed is straightfor-
wardly extensible to nominal and adjectival pred-
icates; the existing grammar distinguishes nominal
and adjectival arguments from adjuncts structurally,
so all that is required is extension of the classi-
fier. Developing an analogous system for another
language would be harder but not infeasible; sim-
ilar taggers and parsers have been developed for a
number of languages, but no extant subcategoriza-
tion dictionaries exist to our knowledge, therefore
the lexical statistics we utilize for statistical filter-
ing would need to be estimated, perhaps using the
technique described by Brent (1993). However, the
entire approach to filtering needs improvement, as
evaluation of our results demonstrates that it is the
weakest link in our current system.
Our system needs further refinement to nar-
row some subcategorization classes, for example, to
choose between differing control options with pred-
icative complements. It also needs supplementing
with information about diathesis alternation pos-
sibilities (e.g. Levin, 1993) and semantic selection
preferences on argument heads. Grishman &amp; Ster-
ling (1992), Poznanski &amp; Sanfilippo (1993), Resnik
(1993), Ribas (1994) and others have shown that it
is possible to acquire selection preferences from (par-
tially) parsed data. Our system already gathers head
lemmas in patterns, so any of these approaches could
be applied, in principle. In future work, we intend to
extend the system in this direction. The ability to
recognize that argument slots of different subcatego-
rization classes for the same predicate share seman-
tic restrictions/preferences would assist recognition
that the predicate undergoes specific alternations,
this in turn assisting inferences about control, equi
and raising (e.g. Boguraev &amp; Briscoe, 1987).
</bodyText>
<sectionHeader confidence="0.986855" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.960723">
Boguraev, B. &amp; Briscoe, E. 1987. Large lexicons
for natural language processing: utilising the gram-
mar coding system of the Longman Dictionary of
</bodyText>
<reference confidence="0.978745285714286">
Contemporary English. Computational Linguistics
13.4: 219-240.
Boguraev, B. &amp; Briscoe, E. 1989. Introduction. In
Boguraev, B. &amp; Briscoe, E. eds. Computational Lex-
icography for Natural Language Processing. Long-
man, London: 1-40.
Boguraev, B., Briscoe, E., Carroll, J., Carter, D.
</reference>
<page confidence="0.650365">
362
</page>
<reference confidence="0.997378796610169">
&amp; Grover, C. 1987. The derivation of a gram-
matically-indexed lexicon from the Longman Dictio-
nary of Contemporary English. In Proceedings of the
25th Annual Meeting of the Association for Compu-
tational Linguistics, Stanford, CA. 193-200.
Brent, M. 1991. Automatic acquisition of subcatego-
rization frames from untagged text. In Proceedings
of the 29th Annual Meeting of the Association for
Computational Linguistics, Berkeley, CA. 209-214.
Brent, M. 1993. From grammar to lexicon: unsu-
pervised learning of lexical syntax. Computational
Linguistics 19.3: 243-262.
Briscoe, E. &amp; Carroll, J. 1993. Generalised proba-
bilistic LR parsing for unification-based grammars.
Computational Linguistics 19.1: 25-60.
Briscoe, E. &amp; Carroll, J. 1994. Parsing (with) punc-
tuation. Rank Xerox Research Centre, Grenoble,
MLTT-TR-007.
Briscoe, E. &amp; Carroll, J. 1995. Developing and eval-
uating a probabilistic LR parser of part-of-speech
and punctuation labels. In Proceedings of the 4th
ACL/SIGPARSE International Workshop on Pars-
ing Technologies, Prague, Czech Republic. 48-58.
Carroll, J. 1993. Practical unification-based parsing
of natural language. Cambridge University Com-
puter Laboratory, TR-224.
Carroll, J. 1994. Relating complexity to practical
performance in parsing with wide-coverage unifica-
tion grammars. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Lin-
guistics, NMSU, Las Cruces, NM. 287-294.
Carroll, J. &amp; Briscoe, E. 1996. Apportioning de-
velopment effort in a probabilistic LR parsing sys-
tem through evaluation. In Proceedings of the ACL
SIGDAT Conference on Empirical Methods in Natu-
ral Language Processing, University of Pensylvania,
Philadelphia, PA. 92-100.
Carroll, J. &amp; Grover, C. 1989. The derivation
of a large computational lexicon for English from
LDOCE. In Boguraev, B. and Briscoe, E. eds. Com-
putational Lexicography for Natural Language Pro-
cessing. Longman, London: 117-134.
Cunningham, H., Gaizauskas, R. &amp; Wilks, Y. 1995.
A general architecture for text engineering (GATE)
- a new approach to language R&amp;D. Research memo
CS-95-21, Department of Computer Science, Univer-
sity of Sheffield, UK.
de Marcken, C. 1990. Parsing the LOB corpus. In
Proceedings of the 28th Annual Meeting of the As-
sociation for Computational Linguistics, Pittsburgh,
PA. 243-251.
Elworthy, D. 1994. Does Baum-Welch re-estimation
help taggers?. In Proceedings of the 4th Conf. Ap-
plied NLP, Stuttgart, Germany.
Garside, R., Leech, G. &amp; Sampson, G. 1987. The
computational analysis of English: A corpus-based
approach. Longman, London.
Grishman, R., Macleod, C. &amp; Meyers, A. 1994.
Comlex syntax: building a computational lexi-
con. In Proceedings of the International Conference
on Computational Linguistics, COLING-94, Kyoto,
Japan. 268-272.
Grishman, R., Macleod, C. &amp; Sterling, J. 1992.
Evaluating parsing strategies using standardized
parse files. In Proceedings of the 3rd ACL
Conference on Applied Natural Language Process-
ing, Trento, Italy. 156-161.
Grishman, R. &amp; Sterling, J. 1992. Acquisition of
selectional patterns. In Proceedings of the Inter-
national Conference on Computational Linguistics,
COLING-92, Nantes, France. 658-664.
Jackendoff, R. 1977. X-bar syntax. MIT Press;
Cambridge, MA..
Jensen, K. 1991. A broad-coverage natural language
analysis system. In M. Tomita eds. Current Issues
in Parsing Technology. Kluwer, Dordrecht.
Levin, B. 1993. Towards a lexical organization of
English verbs. Chicago University Press, Chicago.
Manning, C. 1993. Automatic acquisition of a large
subcategorisation dictionary from corpora. In Pro-
ceedings of the 31st Annual Meeting of the Asso-
ciation for Computational Linguistics, Columbus,
Ohio. 235-242.
Meyers, A., Macleod, C. &amp; Grishman, R. 1994.
Standardization of the complement adjunct distinc-
tion. New York University, Ms.
Nunberg, G. 1990. The linguistics of punctuation.
CSLI Lecture Notes 18, Stanford, CA.
Poznanski, V. &amp; Sanfilippo, A. 1993. Detecting de-
pendencies between semantic verb subclasses and
subcategorization frames in text corpora. In Pro-
ceedings of the SIGLEX ACL Workshop on the Ac-
quisition of Lexical Knowledge from Text, Boguraev,
B. &amp; Pustejovsky, J. eds.
Resnik, P. 1993. Selection and information: a class-
based approach to lexical relationships. University of
Pennsylvania, CIS Dept, PhD thesis.
Ribas, P. 1994. An experiment on learning ap-
propriate selection restrictions from a parsed cor-
pus. In Proceedings of the International Conference
on Computational Linguistics, COLING-94, Kyoto,
Japan.
Sampson, G. 1995. English for the computer. Ox-
ford, UK: Oxford University Press.
Schabes, Y. 1992. Stochastic lexicalized tree ad-
joining grammars. In Proceedings of the Inter-
national Conference on Computational Linguistics,
COLING-92, Nantes, France. 426-432.
Taylor, L. &amp; Knowles, G. 1988. Manual of informa-
tion to accompany the SEC corpus: the machine-
readable corpus of spoken English. University of
Lancaster, UK, Ms.
Ushioda, A., Evans, D., Gibson, T. &amp; Waibel, A.
1993. The automatic acquisition of frequencies of
verb subcategorization frames from tagged corpora.
In Boguraev, B. &amp; Pustejovsky, J. eds. SIGLEX
ACL Workshop on the Acquisition of Lexical Knowl-
edge from Text. Columbus, Ohio: 95-106.
</reference>
<page confidence="0.999364">
363
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.050000">
<title confidence="0.999857">Automatic Extraction of Subcategorization from Corpora</title>
<author confidence="0.999876">Ted Briscoe John Carroll</author>
<affiliation confidence="0.99958">Computer Laboratory Cognitive and Computing Sciences University of Cambridge University of Sussex</affiliation>
<address confidence="0.99872">Pembroke Street, Cambridge CB2 3QG, UK Brighton BN1 9QH, UK</address>
<email confidence="0.988923">ejb@cl.cam.ac.ukjohn.carroll@cogs.susx.ac.uk</email>
<abstract confidence="0.998740934782608">We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount&apos;. 1 Motivation Predicate subcategorization is a key component of a lexical entry, because most, if not all, recent syntactic theories &apos;project&apos; syntactic structure from the lexicon. Therefore, a wide-coverage parser utilizing such a lexicalist grammar must have access to an accurate and comprehensive dictionary encoding (at a minimum) the number and category of a predicate&apos;s arguments and ideally also information about control with predicative arguments, semantic selection preferences on arguments, and so forth, to allow the recovery of the correct predicate-argument structure. If the parser uses statistical techniques to rank analyses, it is also critical that the dictionary encode the relative frequency of distinct subcategorization classes for each predicate. work was supported by UK DTI/SALT project 41/5808 &apos;Integrated Language Database&apos;, CEC Telematics Applications Programme project LE1-2111 &apos;SPARKLE: Shallow PARsing and Knowledge extraction for Language Engineering&apos;, and by SERC/EPSRC Advanced Fellowships to both authors. We would like to thank the COMLEX Syntax development team for allowing us access to pre-release data (for an early experiment), and for useful feedback. Several substantial machine-readable subcategorization dictionaries exist for English, either built largely automatically from machine-readable versions of conventional learners&apos; dictionaries, or manually by (computational) linguists (e.g. the Alvey NL (ANLT) dictionary, Boguraev al. COMLEX Syntax dictionary, Grishman al. (1994)). Unfortunately, neither approach can yield a genuinely accurate or comprehensive computational lexicon, because both rest ultimately on the manual efforts of lexicographers / linguists and are, therefore, prone to errors of omission and commission which are hard or impossible to detect automatically (e.g. Boguraev &amp; Briscoe, 1989; see also section 3.1 below for an example). Furthermore, manual encoding is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency of different subcategorizations), or other (sub)languages. These problems are compounded by the fact that predicate subcategorization is closely associated to lexical sense and the senses of a word change between corpora, sublanguages and/or subject domains (Jensen, 1991). In a recent experiment with a wide-coverage parsing system utilizing a lexicalist grammatical framework, Briscoe &amp; Carroll (1993) observed that half of parse failures on unseen test data were caused by inaccurate subcategorization information in the ANLT dictionary. The close connection between sense and subcategorization and between subject domain and sense makes it likely that a fully accurate &apos;static&apos; subcategorization dictionary of a language is unattainable in any case. Moreover, although Schabes (1992) and others have proposed `lexicalized&apos; probabilistic grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations for individual predicates, because of the problems of accurately estimating them. These problems suggest that automatic construction or updating of subcategorization dictionaries from textual corpora is a more promising avenue to pursue. Preliminary experiments acquiring a few 356 verbal subcategorization classes have been reported Brent (1991, 1993), Manning (1993), and Ushat. In these experiments the maximum number of distinct subcategorization classes is sixteen, and only Ushioda at. attempt to derive relative subcategorization frequency for individual predicates. We describe a new system capable of distinguishing 160 verbal subcategorization classesâ€”a superset of those found in the ANLT and COMLEX Syntax dictionaries. The classes also incorporate information about control of predicative arguments and alternations such as particle movement and extraposition. We report an initial experiment which demonstrates that this system is capable of acquiring the subcategorization classes of verbs and the relative frequencies of these classes with comparable accuracy to the less ambitious extant systems. We achieve this performance by exploiting a more sophisticated robust statistical parser which yields complete though &apos;shallow&apos; parses, a more compresubcategorization class classifier, and a priof the probability of membership of these classes. We also describe a small-scale experiment which demonstrates that subcategorization class frequency information for individual verbs can be used to improve parsing accuracy. 2 Description of the System 2.1 Overview The system consists of the following six components which are applied in sequence to sentences containing a specific predicate in order to retrieve a set of subcategorization classes for that predicate: 1. A tagger, a first-order HMM part-of-speech (PoS) and punctuation tag disambiguator, is used to assign and rank tags for each word and punctuation token in sequences of sentences (Elworthy, 1994). 2. A lemmatizer is used to replace word-tag pairs with lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. We use an enhanced version of the GATE project stemmer at., A probabilistic trained on a treereturns ranked analyses (Briscoe Carroll, 1993; Carroll, 1993, 1994), using a grammar written in a feature-based unification grammar formalism which assigns &apos;shallow&apos; phrase structure analyses to tag networks (or &apos;lattices&apos;)</abstract>
<note confidence="0.7844545">returned by the tagger (Briscoe &amp; Carroll, 1994, Carroll 1996).</note>
<abstract confidence="0.998941655172414">4. A patternset extractor which extracts subcategorization patterns, including the syntactic categories and head lemmas of constituents, from sentence subanalyses which begin/end at the boundaries of (specified) predicates. 5. A pattern classifier which assigns patterns in patternsets to subcategorization classes or rejects patterns as unclassifiable on the basis of the feature values of syntactic categories and the head lemmas in each pattern. 6. A patternsets evaluator which evaluates sets of patternsets gathered for a (single) predicate, constructing putative subcategorization entries and filtering the latter on the basis of their reliability and likelihood. example, building entries for given that one of the sentences in our data was (la), the tagger and lemmatizer return (lb). a attributed his failure, he said, to no&lt;blank&gt;one buying his books. attribute_VVD his_APPS failure_NN1 he_PPHS1 say_VVD to_II no&lt;blank&gt; one_PN buy_V VG his_APP$ book_NN2 (lb) is parsed successfully by the probabilistic LR parser, and the ranked analyses are returned. Then the patternset extractor locates the subanalyses conconstructs a patternset. The highest ranked analysis and pattern for this example shown in Figure Patterns encode the value of the VSUBCAT feature from the VP rule and the head lemma(s) of each argument. In the case of PP (P2) arguments, the pattern also encodes the value of PSUBCAT from the PP rule and the head lemma(s) of its complement(s). In the next stage of processing, patterns are classified, in this case giving the subcategorization class corresponding to transitive plus PP with non-finite clausal complement. The system could be applied to corpus data by first sorting sentences into groups containing instances of a specified predicate, but we use a different strategy since it is more efficient to tag, lemmatize and parse a corpus just once, extracting patternsets for all predicates in each sentence; then to classify the patterns in all patternsets; and finally, to sort and recombine patternsets into sets of patternsets, one set for each distinct predicate containing patternsets of just the patterns relevant to that predicate. The tagger, lemmatizer, grammar and parser have been described elsewhere (see previous references), so we provide only brief relevant details here, concentrating on the description of the components analysis shows only category aliases rather than sets of feature-value pairs. Ta represents a text adjunct delimited by commas (Nunberg 1990; Briscoe 86 Carroll, 1994). Tokens in the patternset are indexed by sequential position in the sentence so that two or more tokens of the same type can be kept distinct in patterns.</abstract>
<note confidence="0.964422875">357 (Tp (V2 (N2 he_PPHS1) (V1 (VO attribute_VVD) (N2 (DT his_APP8) (Ni (NO (NO failure_NN1) (Ta (Pu ,_,) (V2 (N2 he_PPHS1) (V1 (VO say_VVD))) (Pu (P2 (P1 (PO to_II) (N2 no&lt;blank&gt;one_PN) (V1 (VO buy_VVG) (N2 (DT his_APP$) (Ni PPHS1)) (VSUBCAT NP_PP) ((attribute:6 VVD)) ((failure:8 NN1)) ((PSUBCAT SING) ((to:9 II)) ((no&lt;blank&gt;one:10 PN)) ((buy:11 VVG)))) 1)) Figure 1: Highest-ranked analysis and patternset for (lb)</note>
<abstract confidence="0.997204402834009">of the system that are new: the extractor, classifier and evaluator. The grammar consists of 455 phrase structure rule schemata in the format accepted by the parser (a syntactic variant of a Definite Clause Grammar with iterative (Kleene) operators). It is &apos;shallow&apos; in that no atof which thetempt is made to fully analyse unbounded dependencies. However, the distinction between arguments and adjuncts is expressed, following X-bar theory (e.g. Jackendoff, 1977), by Chomsky-adjunction to maximal projections of adjuncts (XP XP Adjunct) as opposed to &apos;government&apos; of arguments (i.e. arguments are sisters within projections; X1 XO Argl... ArgN). more, all analyses are rooted (in S) so the grammar assigns global, shallow and often &apos;spurious&apos; analyses to many sentences. There are 29 distinct values for VSUBCAT and 10 for PSUBCAT; these are analysed in patterns along with specific closed-class lemmas of arguments, such as suband so forth, to classify patterns as evidence for one of the 160 subcategorization classes. Each of these classes can be parameterized for specific predicates by, for example, different prepositions or particles. Currently, the coverage of this grammarâ€”the proportion of sentences for which at least one analysis is foundâ€”is 79% when applied to the Susanne corpus (Sampson, 1995), a 138K word treebanked and balanced subset of the Brown corpus. Wide coverage is important since information is acquired only from successful parses. The combined throughput of the parsing components on a Sun UltraSparc 1/140 is around 50 words per CPU second. 2.2 The Extractor, Classifier and Evaluator The extractor takes as input the ranked analyses from the probabilistic parser. It locates the subanalyses around the predicate, finding the constituents identified as complements inside each subanalysis, and the subject clause preceding it. Instances of passive constructions are recognized and treated specially. The extractor returns the predicate, the VSUBCAT value, and just the heads of the complements (except in the case of PPs, where it returns the PSUBCAT value, the preposition head, and the heads of the PP&apos;s complements). The subcategorization classes recognized by the classifier were obtained by manually merging the classes exemplified in the COMLEX Syntax and ANLT dictionaries and adding around 30 classes found by manual inspection of unclassifiable patterns for corpus examples during development of the system. These consisted of some extra patterns for phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some for non-passivizable patterns with a surface direct object, and some for rarer combinations of governed preposition and complementizer combinations. The classifier filters out as unclassifiable around 15% of patterns found by the extractor when run on all the patternsets extracted from the Susanne corpus. This demonstrates the value of the classifier as a filter of spurious analyses, as well as providing both translation between extracted patterns and two existing subcategorization dictionaries and a definition of the target subcategorization dictionary. The evaluator builds entries by taking the patterns for a given predicate built from successful parses and records the number of observations of each subcategorization class. Patterns provide several types of information which can be used to rank or select between patterns in the patternset for a given sentence exemplifying an instance of a predicate, such as the ranking of the parse from which it was extracted or the proportion of subanalyses supporting a specific pattern. Currently, we simply select the pattern supported by the highest ranked parse. However, we are experimenting with alternative approaches. The resulting set of putative classes for a predicate are filtered, following Brent (1993), 358 by hypothesis testing on binomial frequency data. Evaluating putative entries on binomial frequency data requires that we record the total number of patternsets n for a given predicate, and the number of these patternsets containing a pattern supporting an entry for given class m. These figures are straightforwardly computed from the output of the classifier; however, we also require an estimate of the probability that a pattern for class i will occur with a verb which is not a member of subcategorization class i. Brent proposes estimating these probabilities experimentally on the basis of the behaviour of the extractor. We estimate this probability more directly by first extracting the number of verbs which are members of each class in the ANLT dictionary (with intuitive estimates for the membership of the novel classes) and converting this to a probability of class membership by dividing by the total number of verbs in the dictionary; and secondly, by multiplying the complement of these probabilities by the probability of a pattern for class i, defined as the number of patterns for i extracted from the Susanne corpus divided by the total number of patterns. So, p(v -i), the probability of verb v not of class i occurring with a pattern for class i is: -i) = (1 Ipatterns_f or_i n! n, p) = mi(p im The probability of the event happening m or more times is: n,p) = i=m -i)) the probability that m or more occurrences of patterns for i will occur with a verb which is not a member of i, given n occurrences of that verb. Setting a threshold of less than or equal to 0.05 yields a 95% or better confidence that a high enough proportion of patterns for i have observed for the verb to be in class 2.3 Discussion Our approach to acquiring subcategorization classes is predicated on the following assumptions: â€¢ most sentences will not allow the application of all possible rules of English complementation; â€¢ some sentences will be unambiguous even given indeterminacy of the (1993:249-253) provides a detailed explanation and justification for the use of this measure. fact, 5% of sentences in Susanne are assigned only a single analysis by the grammar. â€¢ many incorrect analyses will yield patterns which are unclassifiable, and are thus filtered out; â€¢ arguments of a specific verb will occur with greater frequency than adjuncts (in potential argument positions); â€¢ the patternset generator will incorrectly output for certain classes more often than others; and even a highest ranked for i is only a probabilistic cue for membership of i, so membership should only be inferred if there are enough occurrences of patterns for i in the data to outweigh the error probability for i. This simple automated, hybrid linguistic/statistical approach contrasts with the manual linguistic analysis of the COMLEX Syntax lexicog- (Meyers at., who propose five criteria and five heuristics for argument-hood and six criteria and two heuristics for adjunct-hood, culled mostly from the linguistics literature. Many of these are not exploitable automatically because they rest on semantic judgements which cannot (yet) be made automatically: for example, optional arguments are often &apos;understood&apos; or implied if missing. Others are syntactic tests involving diathesis alternation possibilities (e.g. passive, dative movement, Levin (1993)) which require recognition that the &apos;same&apos; argument, defined usually by semantic class / thematic role, is occurring across argument positions. We hope to exploit this information where possible at a later stage in the development of our approach. However, recognizing same/similar arguments requires considerable quantities of lexical data or the ability to back-off to lexical semantic classes. At the moment, we exploit linguistic information about the syntactic type, obligatoriness and position of arguments, as well as the set of possible subcategorization classes, and combine this with statistical inference based on the probability of class membership and the frequency and reliability of patterns for classes. 3 Experimental Evaluation 3.1 Lexicon Evaluation â€” Method In order to test the accuracy of our system (as developed so far) and to provide empirical feedback for further development, we took the Susanne, SEC (Taylor &amp; Knowles, 1988) and LOB corpora (Garat., total of 1.2 million wordsâ€”and extracted all sentences containing an occurrence of one of fourteen verbs, up to a maximum of 1000 citations of each. These verbs, listed in Figure 2, were chosen at random, subject to the constraint that they exhibited multiple complementation patterns. The sentences containing these verbs were tagged and parsed automatically, and the extractor, classifier and evaluator were applied to the resulting lanit_verbs1 &apos;patterns&apos; The binomial distribution gives the probability of an with probability exactly m times out of n attempts: 359 successful analyses. The citations from which entries were derived totaled approximately 70K words. The results were evaluated against a merged entry for these verbs from the ANLT and COMLEX Syntax dictionaries, and also against a manual analysis of the corpus data for seven of the verbs. The process of evaluating the performance of the system relative to the dictionaries could, in principle, be reduced to automated report of precision of correct subcategorization classes to all classes found) of correct classes found in the dictionary entry). However, since there are disagreements between the dictionaries and there are classes found in the corpus data that are not contained in either dictionary, we report results relative both to a manually merged entry from ANLT and COMLEX, and also, for seven of the verbs, to a manual analysis of the actual corpus data. The latter analysis is necessary because precision and recall measures against the merged entry will still tend to yield inaccurate results as the system cannot acquire classes not exemplified in the data, and may acquire classes incorrectly absent from the dictionaries. We illustrate these problems with reference to there is overlap, but not agreement between the COMLEX and ANLT entries. Thus, predict that occur with a sentential complement and dummy subject, but only ANLT predicts the possibility of a `wh&apos; complement and only COMLEX predicts the (optional) presence of a PP[to] argument with the sentential complement. One ANLT entry covers two COMLEX entries given the different treatment of the relevant complements but the classifier keeps them distinct. The corpus for examples of further classes we judge valid, in which take a and infinitive complement, as in seems to to be insane, a passive participle, as in depressed. comparison illustrates the problem of errors of omission common to computational lexicons constructed manually and also from dictionaries. All classes for exemplified in the corpus data, but for example, eight classes (out of a possible 27 in the merged entry) are not present, so comparison only to the merged entry would give an unreasonably low estimate of recall. Lexicon Evaluation Figure 2 gives the raw results for the merged entries and corpus analysis on each verb. It shows the of positives correct classes proby our system, positives incorrect proposed by our system, and negatives classes not proposed by our system, as judged against the merged entry, and, for seven of the verbs, against the corpus analysis. It also shows, in the final column, the number of sentences from which classes were extracted. Dictionary (14 verbs) Corpus (7 verbs) Precision Recall 65.7% 76.6% 35.5% 43.4% Figure 3: Type precision and recall Ranking Accuracy ask 75.0% begin 100.0% believe 66.7% cause 100.0% give 70.0% seem 75.0% swing 83.3% Mean 81.4% Figure 4: Ranking accuracy of classes Figure 3 gives the type precision and recall of our system&apos;s recognition of subcategorization classes as evaluated against the merged dictionary entries (14 verbs) and against the manually analysed corpus data (7 verbs). The frequency distribution of classes is highly skewed: for example for there are 107 instances of the most common class in the corpus data, but only 6 instances in total of the least common four classes. More generally, for the manually analysed verbs, almost 60% of the false negatives have only one or two exemplars each in the corpus citations. None of them are returned by because the binomial filter always rejects classes hypothesised on the basis of such little evidence. In Figure 4 we estimate the accuracy with which our system ranks true positive classes against the correct ranking for the seven verbs whose corpus input was manually analysed. We compute this measure by calculating the percentage of pairs of classes at positions (n, m) s.t. n &lt; m in the system ranking that are ordered the same in the correct ranking. This gives us an estimate of the accuracy of the relative frequencies of classes output by the system. For each of the seven verbs for which we undertook a corpus analysis, we calculate the token recall of our system as the percentage (over all exemplars) of true positives in the corpus. This gives us an estimate of the parsing performance that would result from providing a parser with entries built using the system, shown in Figure 5. Further evaluation of the results for these seven verbs reveals that the filtering phase is the weak in the systeni. There are only 13 negatives which the system failed to propose, each exemplified in the data by a mean of 4.5 examples. On the other there are 67 negatives by an mean of 7.1 examples which should, ide- 360 Merged TP FP Entry Corpus TP FP Data No. of FN FN Sentences ask 9 0 18 9 0 10 390 begin 4 1 7 4 1 7 311 believe 4 4 11 4 4 8 230 cause 2 3 6 2 3 5 95 expect 6 5 3 - - - 223 find 5 7 15 - - - 645 give 5 2 11 5 2 5 639 help 6 3 8 - - - 223 like 3 2 7 - - - 228 move 4 3 9 - - - 217 produce 2 1 3 - - 152 provide 3 2 6 - - - 217 seem 8 1 4 8 1 4 534 swing 4 0 10 4 0 8 45 Totals 65 34 118 36 11 47 4149 Figure 2: Raw results for test of 14 verbs Token Recall ask 78.5% begin 73.8% believe 34.5% cause 92.1% give 92.2% seem 84.7% swing 39.2% Mean 80.9% Figure 5: Token recall have been accepted by the filter, and 11 should have been rejected. The performance of the filter for classes with less than 10 exemplars is around chance, and a simple heuristic of accepting all classes with more than 10 exemplars would have produced broadly similar results for these verbs. The filter may well be performing poorly because the probability of generating a subcategorization class for a given verb is often lower than the error probability for that class. 3.3 Parsing Evaluation In addition to evaluating the acquired subcategorization information against existing lexical resources, we have also evaluated the information in the context of an actual parsing system. In particular we wanted to establish whether the subcategorization frequency information for individual verbs could be used to improve the accuracy of a parser that uses statistical techniques to rank analyses. The experiment used the same probabilistic parser and tag sequence grammar as are present in the acquisition system (see references above)â€”although the experiment does not in any way rely on the Mean Recall Precision crossings &apos;Baseline&apos; Lexicalised 1.00 70.7% 72.3% 0.93 71.4% 72.9% Figure 6: GEIG evaluation metrics for parser against Susanne bracketings parsers or grammars being the same. We randomly selected a test set of 250 in-coverage sentences (of lengths 3-56 tokens, mean 18.2) from the Susanne treebank, retagged with possibly multiple tags per word, and measured the &apos;baseline&apos; accuracy of the unlexicalized parser on the sentences using the now standard PARSEVAL/GEIG evaluation metrics of mean crossing brackets per sentence and (unlabelled) bracket recall and precision (e.g. Gral., see figure Next, we colwords in the test corpus tagged as possibly being verbs (giving a total of 356 distinct lemmas) and retrieved all citations of them in the LOB corpus, plus Susanne with the 250 test sentences excluded. We acquired subcategorization and associated frequency information from the citations, in the process successfully parsing 380K words. We then parsed the test set, with each verb subcategorization possibility weighted by its raw frequency score, and using the naive add-one smoothing technique to allow for omitted possibilities. The GEIG measures for the lexicalized parser show a 7% improvement in the crossing bracket score (figure 6). the existing test corpus this is not statisti- &apos;Carroll &amp; Briscoe (1996) use the same test set, although the baseline results reported here differ slightly due to differences in the mapping from parse trees to Susanne-compatible bracketings. 361 significant at the 95% level p = if the pattern of differences were maintained over a larger test set of 470 sentences it would be significant. We expect that a more sophisticated smoothing technique, a larger acquisition corpus, and extensions to the system to deal with nominal and adjectival predicates would improve accuracy still further. Nevertheless, this experiment demonstrates that lexicalizing a grammar/parser with subcategorization frequencies can appreciably improve the accuracy of parse ranking. 4 Related Work Brent&apos;s (1993) approach to acquiring subcategorization is based on a philosophy of only exploiting unambiguous and determinate information in unanalysed corpora. He defines a number of lexical patterns (mostly involving closed class items, such as pronouns) which reliably cue one of five subcategorization classes. Brent does not report comprehensive results, but for one class, sentential complement verbs, he achieves 96% precision and 76% recall at classifying individual tokens of 63 distinct verbs as exemplars or non-exemplars of this class. He does not attempt to rank different classes for a given verb. al. utilise a PoS tagged corpus and finite-state NP parser to recognize and calculate the relative frequency of six subcategorization classes. They report an accuracy rate of 83% (254 errors) at classifying 1565 classifiable tokens of 33 distinct verbs in running text and suggest that incorrect noun phrase boundary detection accounts for the majority of errors. They report that for 32 verbs their system correctly predicts the most frequent class, and for 30 verbs it correctly predicts the second most frequent class, if there was one. Our system rankings include all classes for each verb, from a total of 160 classes, and average 81.4% correct. Manning (1993) conducts a larger experiment, also using a PoS tagged corpus and a finite-state NP parser, attempting to recognize sixteen distinct complementation patterns. He reports that for a test sample of 200 tokens of 40 verbs in running text, the acquired subcategorization dictionary listed the appropriate entry for 163 cases, giving a token recall of 82% (as compared with 80.9% in our experiment). He also reports a comparison of acquired entries for verbs to the entries given in the Advanced Dictionary of Current English 1989) on which his system achieves a precision of 90% and a recall of 43%. His system averages 3.48 subentries (maximum 10)â€”less then half the number produced in our experiment. It is not clear what level of evidence the performance of Manning&apos;s system is based on, but the system was applied to 4.1 million words of text (c.f. our 1.2 million words) and the verbs are all common, so it is likely that considerably more exemplars of each verb were available. 5 Conclusions and Further Work The experiment and comparison reported above suggests that our more comprehensive subcategorization class extractor is able both to assign classes to individual verbal predicates and also to rank them according to relative frequency with comparable accuracy to extant systems. We have also demonstrated that a subcategorization dictionary built with the system can improve the accuracy of a probabilistic parser by an appreciable amount. The system we have developed is straightforwardly extensible to nominal and adjectival predicates; the existing grammar distinguishes nominal and adjectival arguments from adjuncts structurally, so all that is required is extension of the classifier. Developing an analogous system for another language would be harder but not infeasible; similar taggers and parsers have been developed for a number of languages, but no extant subcategorization dictionaries exist to our knowledge, therefore the lexical statistics we utilize for statistical filtering would need to be estimated, perhaps using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman &amp; Sterling (1992), Poznanski &amp; Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences about control, equi and raising (e.g. Boguraev &amp; Briscoe, 1987).</abstract>
<note confidence="0.811851333333333">References Boguraev, B. &amp; Briscoe, E. 1987. Large lexicons for natural language processing: utilising the gramcoding system of the Dictionary of Contemporary English. Computational Linguistics 13.4: 219-240.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Contemporary English</author>
</authors>
<journal>Computational Linguistics</journal>
<volume>13</volume>
<pages>219--240</pages>
<marker>English, </marker>
<rawString>Contemporary English. Computational Linguistics 13.4: 219-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>E Briscoe</author>
</authors>
<date>1989</date>
<booktitle>Computational Lexicography for Natural Language Processing.</booktitle>
<pages>1--40</pages>
<editor>Introduction. In Boguraev, B. &amp; Briscoe, E. eds.</editor>
<publisher>Longman,</publisher>
<location>London:</location>
<contexts>
<context position="2835" citStr="Boguraev &amp; Briscoe, 1989" startWordPosition="398" endWordPosition="401">ries exist for English, either built largely automatically from machine-readable versions of conventional learners&apos; dictionaries, or manually by (computational) linguists (e.g. the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987); the COMLEX Syntax dictionary, Grishman et al. (1994)). Unfortunately, neither approach can yield a genuinely accurate or comprehensive computational lexicon, because both rest ultimately on the manual efforts of lexicographers / linguists and are, therefore, prone to errors of omission and commission which are hard or impossible to detect automatically (e.g. Boguraev &amp; Briscoe, 1989; see also section 3.1 below for an example). Furthermore, manual encoding is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency of different subcategorizations), or other (sub)languages. These problems are compounded by the fact that predicate subcategorization is closely associated to lexical sense and the senses of a word change between corpora, sublanguages and/or subject domains (Jensen, 1991). In a recent experiment with a wide-coverage parsing system utilizing a lexicalist grammatical framework, Briscoe</context>
</contexts>
<marker>Boguraev, Briscoe, 1989</marker>
<rawString>Boguraev, B. &amp; Briscoe, E. 1989. Introduction. In Boguraev, B. &amp; Briscoe, E. eds. Computational Lexicography for Natural Language Processing. Longman, London: 1-40.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B Boguraev</author>
<author>E Briscoe</author>
<author>J Carroll</author>
<author>D Carter</author>
</authors>
<marker>Boguraev, Briscoe, Carroll, Carter, </marker>
<rawString>Boguraev, B., Briscoe, E., Carroll, J., Carter, D.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Grover</author>
</authors>
<title>The derivation of a grammatically-indexed lexicon from the Longman Dictionary of Contemporary English.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>193--200</pages>
<location>Stanford, CA.</location>
<marker>Grover, 1987</marker>
<rawString>&amp; Grover, C. 1987. The derivation of a grammatically-indexed lexicon from the Longman Dictionary of Contemporary English. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, Stanford, CA. 193-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>Automatic acquisition of subcategorization frames from untagged text.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>209--214</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="4398" citStr="Brent (1991" startWordPosition="624" endWordPosition="625">le in any case. Moreover, although Schabes (1992) and others have proposed `lexicalized&apos; probabilistic grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations for individual predicates, because of the problems of accurately estimating them. These problems suggest that automatic construction or updating of subcategorization dictionaries from textual corpora is a more promising avenue to pursue. Preliminary experiments acquiring a few 356 verbal subcategorization classes have been reported by Brent (1991, 1993), Manning (1993), and Ushioda et at. (1993). In these experiments the maximum number of distinct subcategorization classes recognized is sixteen, and only Ushioda et at. attempt to derive relative subcategorization frequency for individual predicates. We describe a new system capable of distinguishing 160 verbal subcategorization classesâ€”a superset of those found in the ANLT and COMLEX Syntax dictionaries. The classes also incorporate information about control of predicative arguments and alternations such as particle movement and extraposition. We report an initial experiment which dem</context>
</contexts>
<marker>Brent, 1991</marker>
<rawString>Brent, M. 1991. Automatic acquisition of subcategorization frames from untagged text. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, Berkeley, CA. 209-214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>From grammar to lexicon: unsupervised learning of lexical syntax.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>243--262</pages>
<contexts>
<context position="13872" citStr="Brent (1993)" startWordPosition="2089" endWordPosition="2090"> parses and records the number of observations of each subcategorization class. Patterns provide several types of information which can be used to rank or select between patterns in the patternset for a given sentence exemplifying an instance of a predicate, such as the ranking of the parse from which it was extracted or the proportion of subanalyses supporting a specific pattern. Currently, we simply select the pattern supported by the highest ranked parse. However, we are experimenting with alternative approaches. The resulting set of putative classes for a predicate are filtered, following Brent (1993), 358 by hypothesis testing on binomial frequency data. Evaluating putative entries on binomial frequency data requires that we record the total number of patternsets n for a given predicate, and the number of these patternsets containing a pattern supporting an entry for given class m. These figures are straightforwardly computed from the output of the classifier; however, we also require an estimate of the probability that a pattern for class i will occur with a verb which is not a member of subcategorization class i. Brent proposes estimating these probabilities experimentally on the basis </context>
<context position="15986" citStr="Brent (1993" startWordPosition="2453" endWordPosition="2454"> or more occurrences of patterns for i will occur with a verb which is not a member of i, given n occurrences of that verb. Setting a threshold of less than or equal to 0.05 yields a 95% or better confidence that a high enough proportion of patterns for i have been observed for the verb to be in class i3. 2.3 Discussion Our approach to acquiring subcategorization classes is predicated on the following assumptions: â€¢ most sentences will not allow the application of all possible rules of English complementation; â€¢ some sentences will be unambiguous even given the indeterminacy of the grammar4; 3Brent (1993:249-253) provides a detailed explanation and justification for the use of this measure. 41n fact, 5% of sentences in Susanne are assigned only a single analysis by the grammar. â€¢ many incorrect analyses will yield patterns which are unclassifiable, and are thus filtered out; â€¢ arguments of a specific verb will occur with greater frequency than adjuncts (in potential argument positions); â€¢ the patternset generator will incorrectly output patterns for certain classes more often than others; and â€¢ even a highest ranked paâ–ª ttern for i is only a probabilistic cue for membership of i, so membershi</context>
<context position="31202" citStr="Brent (1993)" startWordPosition="4979" endWordPosition="4980">eloped is straightforwardly extensible to nominal and adjectival predicates; the existing grammar distinguishes nominal and adjectival arguments from adjuncts structurally, so all that is required is extension of the classifier. Developing an analogous system for another language would be harder but not infeasible; similar taggers and parsers have been developed for a number of languages, but no extant subcategorization dictionaries exist to our knowledge, therefore the lexical statistics we utilize for statistical filtering would need to be estimated, perhaps using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman &amp; Sterling (1992), Poznanski &amp; Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is</context>
</contexts>
<marker>Brent, 1993</marker>
<rawString>Brent, M. 1993. From grammar to lexicon: unsupervised learning of lexical syntax. Computational Linguistics 19.3: 243-262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalised probabilistic LR parsing for unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>25--60</pages>
<contexts>
<context position="3452" citStr="Briscoe &amp; Carroll (1993)" startWordPosition="491" endWordPosition="494">e, 1989; see also section 3.1 below for an example). Furthermore, manual encoding is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency of different subcategorizations), or other (sub)languages. These problems are compounded by the fact that predicate subcategorization is closely associated to lexical sense and the senses of a word change between corpora, sublanguages and/or subject domains (Jensen, 1991). In a recent experiment with a wide-coverage parsing system utilizing a lexicalist grammatical framework, Briscoe &amp; Carroll (1993) observed that half of parse failures on unseen test data were caused by inaccurate subcategorization information in the ANLT dictionary. The close connection between sense and subcategorization and between subject domain and sense makes it likely that a fully accurate &apos;static&apos; subcategorization dictionary of a language is unattainable in any case. Moreover, although Schabes (1992) and others have proposed `lexicalized&apos; probabilistic grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations fo</context>
<context position="6475" citStr="Briscoe &amp; Carroll, 1993" startWordPosition="942" endWordPosition="946">n classes for that predicate: 1. A tagger, a first-order HMM part-of-speech (PoS) and punctuation tag disambiguator, is used to assign and rank tags for each word and punctuation token in sequences of sentences (Elworthy, 1994). 2. A lemmatizer is used to replace word-tag pairs with lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. We use an enhanced version of the GATE project stemmer (Cunningham et at., 1995). 3. A probabilistic LR parser, trained on a treebank, returns ranked analyses (Briscoe &amp; Carroll, 1993; Carroll, 1993, 1994), using a grammar written in a feature-based unification grammar formalism which assigns &apos;shallow&apos; phrase structure analyses to tag networks (or &apos;lattices&apos;) returned by the tagger (Briscoe &amp; Carroll, 1994, 1995; Carroll Sz Briscoe, 1996). 4. A patternset extractor which extracts subcategorization patterns, including the syntactic categories and head lemmas of constituents, from sentence subanalyses which begin/end at the boundaries of (specified) predicates. 5. A pattern classifier which assigns patterns in patternsets to subcategorization classes or rejects patterns as u</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Briscoe, E. &amp; Carroll, J. 1993. Generalised probabilistic LR parsing for unification-based grammars. Computational Linguistics 19.1: 25-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Parsing (with) punctuation. Rank Xerox Research Centre,</title>
<date>1994</date>
<location>Grenoble, MLTT-TR-007.</location>
<contexts>
<context position="6701" citStr="Briscoe &amp; Carroll, 1994" startWordPosition="977" endWordPosition="980">). 2. A lemmatizer is used to replace word-tag pairs with lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. We use an enhanced version of the GATE project stemmer (Cunningham et at., 1995). 3. A probabilistic LR parser, trained on a treebank, returns ranked analyses (Briscoe &amp; Carroll, 1993; Carroll, 1993, 1994), using a grammar written in a feature-based unification grammar formalism which assigns &apos;shallow&apos; phrase structure analyses to tag networks (or &apos;lattices&apos;) returned by the tagger (Briscoe &amp; Carroll, 1994, 1995; Carroll Sz Briscoe, 1996). 4. A patternset extractor which extracts subcategorization patterns, including the syntactic categories and head lemmas of constituents, from sentence subanalyses which begin/end at the boundaries of (specified) predicates. 5. A pattern classifier which assigns patterns in patternsets to subcategorization classes or rejects patterns as unclassifiable on the basis of the feature values of syntactic categories and the head lemmas in each pattern. 6. A patternsets evaluator which evaluates sets of patternsets gathered for a (single) predicate, constructing putat</context>
</contexts>
<marker>Briscoe, Carroll, 1994</marker>
<rawString>Briscoe, E. &amp; Carroll, J. 1994. Parsing (with) punctuation. Rank Xerox Research Centre, Grenoble, MLTT-TR-007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Developing and evaluating a probabilistic LR parser of part-of-speech and punctuation labels.</title>
<date>1995</date>
<booktitle>In Proceedings of the 4th ACL/SIGPARSE International Workshop on Parsing Technologies,</booktitle>
<pages>48--58</pages>
<location>Prague, Czech Republic.</location>
<marker>Briscoe, Carroll, 1995</marker>
<rawString>Briscoe, E. &amp; Carroll, J. 1995. Developing and evaluating a probabilistic LR parser of part-of-speech and punctuation labels. In Proceedings of the 4th ACL/SIGPARSE International Workshop on Parsing Technologies, Prague, Czech Republic. 48-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
</authors>
<title>Practical unification-based parsing of natural language.</title>
<date>1993</date>
<pages>224</pages>
<institution>Cambridge University Computer Laboratory,</institution>
<contexts>
<context position="3452" citStr="Carroll (1993)" startWordPosition="493" endWordPosition="494">ee also section 3.1 below for an example). Furthermore, manual encoding is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency of different subcategorizations), or other (sub)languages. These problems are compounded by the fact that predicate subcategorization is closely associated to lexical sense and the senses of a word change between corpora, sublanguages and/or subject domains (Jensen, 1991). In a recent experiment with a wide-coverage parsing system utilizing a lexicalist grammatical framework, Briscoe &amp; Carroll (1993) observed that half of parse failures on unseen test data were caused by inaccurate subcategorization information in the ANLT dictionary. The close connection between sense and subcategorization and between subject domain and sense makes it likely that a fully accurate &apos;static&apos; subcategorization dictionary of a language is unattainable in any case. Moreover, although Schabes (1992) and others have proposed `lexicalized&apos; probabilistic grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations fo</context>
<context position="6475" citStr="Carroll, 1993" startWordPosition="944" endWordPosition="946">for that predicate: 1. A tagger, a first-order HMM part-of-speech (PoS) and punctuation tag disambiguator, is used to assign and rank tags for each word and punctuation token in sequences of sentences (Elworthy, 1994). 2. A lemmatizer is used to replace word-tag pairs with lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. We use an enhanced version of the GATE project stemmer (Cunningham et at., 1995). 3. A probabilistic LR parser, trained on a treebank, returns ranked analyses (Briscoe &amp; Carroll, 1993; Carroll, 1993, 1994), using a grammar written in a feature-based unification grammar formalism which assigns &apos;shallow&apos; phrase structure analyses to tag networks (or &apos;lattices&apos;) returned by the tagger (Briscoe &amp; Carroll, 1994, 1995; Carroll Sz Briscoe, 1996). 4. A patternset extractor which extracts subcategorization patterns, including the syntactic categories and head lemmas of constituents, from sentence subanalyses which begin/end at the boundaries of (specified) predicates. 5. A pattern classifier which assigns patterns in patternsets to subcategorization classes or rejects patterns as u</context>
</contexts>
<marker>Carroll, 1993</marker>
<rawString>Carroll, J. 1993. Practical unification-based parsing of natural language. Cambridge University Computer Laboratory, TR-224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
</authors>
<title>Relating complexity to practical performance in parsing with wide-coverage unification grammars.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, NMSU, Las Cruces, NM.</booktitle>
<pages>287--294</pages>
<contexts>
<context position="6701" citStr="Carroll, 1994" startWordPosition="979" endWordPosition="980">mmatizer is used to replace word-tag pairs with lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. We use an enhanced version of the GATE project stemmer (Cunningham et at., 1995). 3. A probabilistic LR parser, trained on a treebank, returns ranked analyses (Briscoe &amp; Carroll, 1993; Carroll, 1993, 1994), using a grammar written in a feature-based unification grammar formalism which assigns &apos;shallow&apos; phrase structure analyses to tag networks (or &apos;lattices&apos;) returned by the tagger (Briscoe &amp; Carroll, 1994, 1995; Carroll Sz Briscoe, 1996). 4. A patternset extractor which extracts subcategorization patterns, including the syntactic categories and head lemmas of constituents, from sentence subanalyses which begin/end at the boundaries of (specified) predicates. 5. A pattern classifier which assigns patterns in patternsets to subcategorization classes or rejects patterns as unclassifiable on the basis of the feature values of syntactic categories and the head lemmas in each pattern. 6. A patternsets evaluator which evaluates sets of patternsets gathered for a (single) predicate, constructing putat</context>
<context position="9343" citStr="Carroll, 1994" startWordPosition="1387" endWordPosition="1388">h sentence; then to classify the patterns in all patternsets; and finally, to sort and recombine patternsets into sets of patternsets, one set for each distinct predicate containing patternsets of just the patterns relevant to that predicate. The tagger, lemmatizer, grammar and parser have been described elsewhere (see previous references), so we provide only brief relevant details here, concentrating on the description of the components 2The analysis shows only category aliases rather than sets of feature-value pairs. Ta represents a text adjunct delimited by commas (Nunberg 1990; Briscoe 86 Carroll, 1994). Tokens in the patternset are indexed by sequential position in the sentence so that two or more tokens of the same type can be kept distinct in patterns. 357 (Tp (V2 (N2 he_PPHS1) (V1 (VO attribute_VVD) (N2 (DT his_APP8) (Ni (NO (NO failure_NN1) (Ta (Pu ,_,) (V2 (N2 he_PPHS1) (V1 (VO say_VVD))) (Pu (P2 (P1 (PO to_II) (N2 no&lt;blank&gt;one_PN) (V1 (VO buy_VVG) (N2 (DT his_APP$) (Ni (1 ((((he:1 PPHS1)) (VSUBCAT NP_PP) ((attribute:6 VVD)) ((failure:8 NN1)) ((PSUBCAT SING) ((to:9 II)) ((no&lt;blank&gt;one:10 PN)) ((buy:11 VVG)))) 1)) (NO book_NN2))))))))) Figure 1: Highest-ranked analysis and patternset fo</context>
</contexts>
<marker>Carroll, 1994</marker>
<rawString>Carroll, J. 1994. Relating complexity to practical performance in parsing with wide-coverage unification grammars. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, NMSU, Las Cruces, NM. 287-294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>E Briscoe</author>
</authors>
<title>Apportioning development effort in a probabilistic LR parsing system through evaluation.</title>
<date>1996</date>
<booktitle>In Proceedings of the ACL SIGDAT Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>92--100</pages>
<institution>University of Pensylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="27041" citStr="Carroll &amp; Briscoe (1996)" startWordPosition="4313" endWordPosition="4317">as) and retrieved all citations of them in the LOB corpus, plus Susanne with the 250 test sentences excluded. We acquired subcategorization and associated frequency information from the citations, in the process successfully parsing 380K words. We then parsed the test set, with each verb subcategorization possibility weighted by its raw frequency score, and using the naive add-one smoothing technique to allow for omitted possibilities. The GEIG measures for the lexicalized parser show a 7% improvement in the crossing bracket score (figure 6). Over the existing test corpus this is not statisti&apos;Carroll &amp; Briscoe (1996) use the same test set, although the baseline results reported here differ slightly due to differences in the mapping from parse trees to Susanne-compatible bracketings. 361 cally significant at the 95% level (paired t-test, 1.21, 249 df, p = 0.11)â€”although if the pattern of differences were maintained over a larger test set of 470 sentences it would be significant. We expect that a more sophisticated smoothing technique, a larger acquisition corpus, and extensions to the system to deal with nominal and adjectival predicates would improve accuracy still further. Nevertheless, this experiment d</context>
</contexts>
<marker>Carroll, Briscoe, 1996</marker>
<rawString>Carroll, J. &amp; Briscoe, E. 1996. Apportioning development effort in a probabilistic LR parsing system through evaluation. In Proceedings of the ACL SIGDAT Conference on Empirical Methods in Natural Language Processing, University of Pensylvania, Philadelphia, PA. 92-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>C Grover</author>
</authors>
<title>The derivation of a large computational lexicon for English from LDOCE.</title>
<date>1989</date>
<booktitle>Computational Lexicography for Natural Language Processing.</booktitle>
<pages>117--134</pages>
<editor>In Boguraev, B. and Briscoe, E. eds.</editor>
<publisher>Longman,</publisher>
<location>London:</location>
<marker>Carroll, Grover, 1989</marker>
<rawString>Carroll, J. &amp; Grover, C. 1989. The derivation of a large computational lexicon for English from LDOCE. In Boguraev, B. and Briscoe, E. eds. Computational Lexicography for Natural Language Processing. Longman, London: 117-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>R Gaizauskas</author>
<author>Y Wilks</author>
</authors>
<title>A general architecture for text engineering (GATE) - a new approach to language R&amp;D.</title>
<date>1995</date>
<institution>Department of Computer Science, University of Sheffield, UK.</institution>
<note>Research memo CS-95-21,</note>
<marker>Cunningham, Gaizauskas, Wilks, 1995</marker>
<rawString>Cunningham, H., Gaizauskas, R. &amp; Wilks, Y. 1995. A general architecture for text engineering (GATE) - a new approach to language R&amp;D. Research memo CS-95-21, Department of Computer Science, University of Sheffield, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C de Marcken</author>
</authors>
<title>Parsing the LOB corpus.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>243--251</pages>
<location>Pittsburgh, PA.</location>
<marker>de Marcken, 1990</marker>
<rawString>de Marcken, C. 1990. Parsing the LOB corpus. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, Pittsburgh, PA. 243-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Does Baum-Welch re-estimation help taggers?.</title>
<date>1994</date>
<booktitle>In Proceedings of the 4th Conf. Applied NLP,</booktitle>
<location>Stuttgart, Germany.</location>
<contexts>
<context position="6079" citStr="Elworthy, 1994" startWordPosition="876" endWordPosition="878">We also describe a small-scale experiment which demonstrates that subcategorization class frequency information for individual verbs can be used to improve parsing accuracy. 2 Description of the System 2.1 Overview The system consists of the following six components which are applied in sequence to sentences containing a specific predicate in order to retrieve a set of subcategorization classes for that predicate: 1. A tagger, a first-order HMM part-of-speech (PoS) and punctuation tag disambiguator, is used to assign and rank tags for each word and punctuation token in sequences of sentences (Elworthy, 1994). 2. A lemmatizer is used to replace word-tag pairs with lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. We use an enhanced version of the GATE project stemmer (Cunningham et at., 1995). 3. A probabilistic LR parser, trained on a treebank, returns ranked analyses (Briscoe &amp; Carroll, 1993; Carroll, 1993, 1994), using a grammar written in a feature-based unification grammar formalism which assigns &apos;shallow&apos; phrase structure analyses to tag networks (or &apos;lattices&apos;) returned by the tagger (B</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>Elworthy, D. 1994. Does Baum-Welch re-estimation help taggers?. In Proceedings of the 4th Conf. Applied NLP, Stuttgart, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garside</author>
<author>G Leech</author>
<author>G Sampson</author>
</authors>
<title>The computational analysis of English: A corpus-based approach.</title>
<date>1987</date>
<publisher>Longman,</publisher>
<location>London.</location>
<marker>Garside, Leech, Sampson, 1987</marker>
<rawString>Garside, R., Leech, G. &amp; Sampson, G. 1987. The computational analysis of English: A corpus-based approach. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>C Macleod</author>
<author>A Meyers</author>
</authors>
<title>Comlex syntax: building a computational lexicon.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics, COLING-94,</booktitle>
<pages>268--272</pages>
<location>Kyoto,</location>
<contexts>
<context position="2502" citStr="Grishman et al. (1994)" startWordPosition="350" endWordPosition="353">ARsing and Knowledge extraction for Language Engineering&apos;, and by SERC/EPSRC Advanced Fellowships to both authors. We would like to thank the COMLEX Syntax development team for allowing us access to pre-release data (for an early experiment), and for useful feedback. Several substantial machine-readable subcategorization dictionaries exist for English, either built largely automatically from machine-readable versions of conventional learners&apos; dictionaries, or manually by (computational) linguists (e.g. the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987); the COMLEX Syntax dictionary, Grishman et al. (1994)). Unfortunately, neither approach can yield a genuinely accurate or comprehensive computational lexicon, because both rest ultimately on the manual efforts of lexicographers / linguists and are, therefore, prone to errors of omission and commission which are hard or impossible to detect automatically (e.g. Boguraev &amp; Briscoe, 1989; see also section 3.1 below for an example). Furthermore, manual encoding is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency of different subcategorizations), or other (sub)langu</context>
</contexts>
<marker>Grishman, Macleod, Meyers, 1994</marker>
<rawString>Grishman, R., Macleod, C. &amp; Meyers, A. 1994. Comlex syntax: building a computational lexicon. In Proceedings of the International Conference on Computational Linguistics, COLING-94, Kyoto, Japan. 268-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>C Macleod</author>
<author>J Sterling</author>
</authors>
<title>Evaluating parsing strategies using standardized parse files.</title>
<date>1992</date>
<booktitle>In Proceedings of the 3rd ACL Conference on Applied Natural Language Processing,</booktitle>
<pages>156--161</pages>
<location>Trento,</location>
<contexts>
<context position="26285" citStr="Grishman et al., 1992" startWordPosition="4187" endWordPosition="4191">e Mean Recall Precision crossings &apos;Baseline&apos; 1.00 70.7% 72.3% Lexicalised 0.93 71.4% 72.9% Figure 6: GEIG evaluation metrics for parser against Susanne bracketings parsers or grammars being the same. We randomly selected a test set of 250 in-coverage sentences (of lengths 3-56 tokens, mean 18.2) from the Susanne treebank, retagged with possibly multiple tags per word, and measured the &apos;baseline&apos; accuracy of the unlexicalized parser on the sentences using the now standard PARSEVAL/GEIG evaluation metrics of mean crossing brackets per sentence and (unlabelled) bracket recall and precision (e.g. Grishman et al., 1992); see figure 65. Next, we collected all words in the test corpus tagged as possibly being verbs (giving a total of 356 distinct lemmas) and retrieved all citations of them in the LOB corpus, plus Susanne with the 250 test sentences excluded. We acquired subcategorization and associated frequency information from the citations, in the process successfully parsing 380K words. We then parsed the test set, with each verb subcategorization possibility weighted by its raw frequency score, and using the naive add-one smoothing technique to allow for omitted possibilities. The GEIG measures for the le</context>
</contexts>
<marker>Grishman, Macleod, Sterling, 1992</marker>
<rawString>Grishman, R., Macleod, C. &amp; Sterling, J. 1992. Evaluating parsing strategies using standardized parse files. In Proceedings of the 3rd ACL Conference on Applied Natural Language Processing, Trento, Italy. 156-161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>J Sterling</author>
</authors>
<title>Acquisition of selectional patterns.</title>
<date>1992</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics, COLING-92,</booktitle>
<pages>658--664</pages>
<location>Nantes,</location>
<contexts>
<context position="31709" citStr="Grishman &amp; Sterling (1992)" startWordPosition="5049" endWordPosition="5053"> we utilize for statistical filtering would need to be estimated, perhaps using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman &amp; Sterling (1992), Poznanski &amp; Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences</context>
</contexts>
<marker>Grishman, Sterling, 1992</marker>
<rawString>Grishman, R. &amp; Sterling, J. 1992. Acquisition of selectional patterns. In Proceedings of the International Conference on Computational Linguistics, COLING-92, Nantes, France. 658-664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>X-bar syntax.</title>
<date>1977</date>
<publisher>MIT Press;</publisher>
<location>Cambridge, MA..</location>
<contexts>
<context position="10413" citStr="Jackendoff, 1977" startWordPosition="1554" endWordPosition="1555">) ((PSUBCAT SING) ((to:9 II)) ((no&lt;blank&gt;one:10 PN)) ((buy:11 VVG)))) 1)) (NO book_NN2))))))))) Figure 1: Highest-ranked analysis and patternset for (lb) of the system that are new: the extractor, classifier and evaluator. The grammar consists of 455 phrase structure rule schemata in the format accepted by the parser (a syntactic variant of a Definite Clause Grammar with iterative (Kleene) operators). It is &apos;shallow&apos; in that no atof which thetempt is made to fully analyse unbounded dependencies. However, the distinction between arguments and adjuncts is expressed, following X-bar theory (e.g. Jackendoff, 1977), by Chomsky-adjunction to maximal projections of adjuncts (XP XP Adjunct) as opposed to &apos;government&apos; of arguments (i.e. arguments are sisters within X1 projections; X1 XO Argl... ArgN). Furthermore, all analyses are rooted (in S) so the grammar assigns global, shallow and often &apos;spurious&apos; analyses to many sentences. There are 29 distinct values for VSUBCAT and 10 for PSUBCAT; these are analysed in patterns along with specific closed-class head lemmas of arguments, such as it (dummy subjects), whether (wh-complements), and so forth, to classify patterns as evidence for one of the 160 subcatego</context>
</contexts>
<marker>Jackendoff, 1977</marker>
<rawString>Jackendoff, R. 1977. X-bar syntax. MIT Press; Cambridge, MA..</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jensen</author>
</authors>
<title>A broad-coverage natural language analysis system.</title>
<date>1991</date>
<booktitle>Current Issues in Parsing Technology.</booktitle>
<editor>In M. Tomita eds.</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="3321" citStr="Jensen, 1991" startWordPosition="473" endWordPosition="474"> prone to errors of omission and commission which are hard or impossible to detect automatically (e.g. Boguraev &amp; Briscoe, 1989; see also section 3.1 below for an example). Furthermore, manual encoding is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency of different subcategorizations), or other (sub)languages. These problems are compounded by the fact that predicate subcategorization is closely associated to lexical sense and the senses of a word change between corpora, sublanguages and/or subject domains (Jensen, 1991). In a recent experiment with a wide-coverage parsing system utilizing a lexicalist grammatical framework, Briscoe &amp; Carroll (1993) observed that half of parse failures on unseen test data were caused by inaccurate subcategorization information in the ANLT dictionary. The close connection between sense and subcategorization and between subject domain and sense makes it likely that a fully accurate &apos;static&apos; subcategorization dictionary of a language is unattainable in any case. Moreover, although Schabes (1992) and others have proposed `lexicalized&apos; probabilistic grammars to improve the accurac</context>
</contexts>
<marker>Jensen, 1991</marker>
<rawString>Jensen, K. 1991. A broad-coverage natural language analysis system. In M. Tomita eds. Current Issues in Parsing Technology. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>Towards a lexical organization of English verbs.</title>
<date>1993</date>
<publisher>Chicago University Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="17373" citStr="Levin (1993)" startWordPosition="2667" endWordPosition="2668">stical approach contrasts with the manual linguistic analysis of the COMLEX Syntax lexicographers (Meyers et at., 1994), who propose five criteria and five heuristics for argument-hood and six criteria and two heuristics for adjunct-hood, culled mostly from the linguistics literature. Many of these are not exploitable automatically because they rest on semantic judgements which cannot (yet) be made automatically: for example, optional arguments are often &apos;understood&apos; or implied if missing. Others are syntactic tests involving diathesis alternation possibilities (e.g. passive, dative movement, Levin (1993)) which require recognition that the &apos;same&apos; argument, defined usually by semantic class / thematic role, is occurring across argument positions. We hope to exploit this information where possible at a later stage in the development of our approach. However, recognizing same/similar arguments requires considerable quantities of lexical data or the ability to back-off to lexical semantic classes. At the moment, we exploit linguistic information about the syntactic type, obligatoriness and position of arguments, as well as the set of possible subcategorization classes, and combine this with stati</context>
<context position="31628" citStr="Levin, 1993" startWordPosition="5040" endWordPosition="5041">ctionaries exist to our knowledge, therefore the lexical statistics we utilize for statistical filtering would need to be estimated, perhaps using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman &amp; Sterling (1992), Poznanski &amp; Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, B. 1993. Towards a lexical organization of English verbs. Chicago University Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
</authors>
<title>Automatic acquisition of a large subcategorisation dictionary from corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>235--242</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="4421" citStr="Manning (1993)" startWordPosition="627" endWordPosition="628">over, although Schabes (1992) and others have proposed `lexicalized&apos; probabilistic grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations for individual predicates, because of the problems of accurately estimating them. These problems suggest that automatic construction or updating of subcategorization dictionaries from textual corpora is a more promising avenue to pursue. Preliminary experiments acquiring a few 356 verbal subcategorization classes have been reported by Brent (1991, 1993), Manning (1993), and Ushioda et at. (1993). In these experiments the maximum number of distinct subcategorization classes recognized is sixteen, and only Ushioda et at. attempt to derive relative subcategorization frequency for individual predicates. We describe a new system capable of distinguishing 160 verbal subcategorization classesâ€”a superset of those found in the ANLT and COMLEX Syntax dictionaries. The classes also incorporate information about control of predicative arguments and alternations such as particle movement and extraposition. We report an initial experiment which demonstrates that this sys</context>
<context position="29089" citStr="Manning (1993)" startWordPosition="4640" endWordPosition="4641"> parser to recognize and calculate the relative frequency of six subcategorization classes. They report an accuracy rate of 83% (254 errors) at classifying 1565 classifiable tokens of 33 distinct verbs in running text and suggest that incorrect noun phrase boundary detection accounts for the majority of errors. They report that for 32 verbs their system correctly predicts the most frequent class, and for 30 verbs it correctly predicts the second most frequent class, if there was one. Our system rankings include all classes for each verb, from a total of 160 classes, and average 81.4% correct. Manning (1993) conducts a larger experiment, also using a PoS tagged corpus and a finite-state NP parser, attempting to recognize sixteen distinct complementation patterns. He reports that for a test sample of 200 tokens of 40 verbs in running text, the acquired subcategorization dictionary listed the appropriate entry for 163 cases, giving a token recall of 82% (as compared with 80.9% in our experiment). He also reports a comparison of acquired entries for the verbs to the entries given in the Oxford Advanced Learner&apos;s Dictionary of Current English (Hornby, 1989) on which his system achieves a precision of</context>
</contexts>
<marker>Manning, 1993</marker>
<rawString>Manning, C. 1993. Automatic acquisition of a large subcategorisation dictionary from corpora. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, Columbus, Ohio. 235-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>C Macleod</author>
<author>R Grishman</author>
</authors>
<title>Standardization of the complement adjunct distinction.</title>
<date>1994</date>
<location>New York University, Ms.</location>
<marker>Meyers, Macleod, Grishman, 1994</marker>
<rawString>Meyers, A., Macleod, C. &amp; Grishman, R. 1994. Standardization of the complement adjunct distinction. New York University, Ms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Nunberg</author>
</authors>
<title>The linguistics of punctuation.</title>
<date>1990</date>
<booktitle>CSLI Lecture Notes 18,</booktitle>
<location>Stanford, CA.</location>
<contexts>
<context position="9316" citStr="Nunberg 1990" startWordPosition="1383" endWordPosition="1384">for all predicates in each sentence; then to classify the patterns in all patternsets; and finally, to sort and recombine patternsets into sets of patternsets, one set for each distinct predicate containing patternsets of just the patterns relevant to that predicate. The tagger, lemmatizer, grammar and parser have been described elsewhere (see previous references), so we provide only brief relevant details here, concentrating on the description of the components 2The analysis shows only category aliases rather than sets of feature-value pairs. Ta represents a text adjunct delimited by commas (Nunberg 1990; Briscoe 86 Carroll, 1994). Tokens in the patternset are indexed by sequential position in the sentence so that two or more tokens of the same type can be kept distinct in patterns. 357 (Tp (V2 (N2 he_PPHS1) (V1 (VO attribute_VVD) (N2 (DT his_APP8) (Ni (NO (NO failure_NN1) (Ta (Pu ,_,) (V2 (N2 he_PPHS1) (V1 (VO say_VVD))) (Pu (P2 (P1 (PO to_II) (N2 no&lt;blank&gt;one_PN) (V1 (VO buy_VVG) (N2 (DT his_APP$) (Ni (1 ((((he:1 PPHS1)) (VSUBCAT NP_PP) ((attribute:6 VVD)) ((failure:8 NN1)) ((PSUBCAT SING) ((to:9 II)) ((no&lt;blank&gt;one:10 PN)) ((buy:11 VVG)))) 1)) (NO book_NN2))))))))) Figure 1: Highest-ranked</context>
</contexts>
<marker>Nunberg, 1990</marker>
<rawString>Nunberg, G. 1990. The linguistics of punctuation. CSLI Lecture Notes 18, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Poznanski</author>
<author>A Sanfilippo</author>
</authors>
<title>Detecting dependencies between semantic verb subclasses and subcategorization frames in text corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the SIGLEX ACL Workshop on the Acquisition of Lexical Knowledge</booktitle>
<editor>from Text, Boguraev, B. &amp; Pustejovsky, J. eds.</editor>
<contexts>
<context position="31740" citStr="Poznanski &amp; Sanfilippo (1993)" startWordPosition="5054" endWordPosition="5057">filtering would need to be estimated, perhaps using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman &amp; Sterling (1992), Poznanski &amp; Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences about control, equi and raisin</context>
</contexts>
<marker>Poznanski, Sanfilippo, 1993</marker>
<rawString>Poznanski, V. &amp; Sanfilippo, A. 1993. Detecting dependencies between semantic verb subclasses and subcategorization frames in text corpora. In Proceedings of the SIGLEX ACL Workshop on the Acquisition of Lexical Knowledge from Text, Boguraev, B. &amp; Pustejovsky, J. eds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selection and information: a classbased approach to lexical relationships.</title>
<date>1993</date>
<tech>Dept, PhD thesis.</tech>
<institution>University of Pennsylvania, CIS</institution>
<contexts>
<context position="31755" citStr="Resnik (1993)" startWordPosition="5058" endWordPosition="5059">mated, perhaps using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman &amp; Sterling (1992), Poznanski &amp; Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences about control, equi and raising (e.g. Bogurae</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Resnik, P. 1993. Selection and information: a classbased approach to lexical relationships. University of Pennsylvania, CIS Dept, PhD thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ribas</author>
</authors>
<title>An experiment on learning appropriate selection restrictions from a parsed corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics, COLING-94,</booktitle>
<location>Kyoto, Japan.</location>
<contexts>
<context position="31769" citStr="Ribas (1994)" startWordPosition="5060" endWordPosition="5061">using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman &amp; Sterling (1992), Poznanski &amp; Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences about control, equi and raising (e.g. Boguraev &amp; Briscoe, 1</context>
</contexts>
<marker>Ribas, 1994</marker>
<rawString>Ribas, P. 1994. An experiment on learning appropriate selection restrictions from a parsed corpus. In Proceedings of the International Conference on Computational Linguistics, COLING-94, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sampson</author>
</authors>
<title>English for the computer.</title>
<date>1995</date>
<publisher>Oxford University Press.</publisher>
<location>Oxford, UK:</location>
<contexts>
<context position="11318" citStr="Sampson, 1995" startWordPosition="1699" endWordPosition="1700">analyses to many sentences. There are 29 distinct values for VSUBCAT and 10 for PSUBCAT; these are analysed in patterns along with specific closed-class head lemmas of arguments, such as it (dummy subjects), whether (wh-complements), and so forth, to classify patterns as evidence for one of the 160 subcategorization classes. Each of these classes can be parameterized for specific predicates by, for example, different prepositions or particles. Currently, the coverage of this grammarâ€”the proportion of sentences for which at least one analysis is foundâ€”is 79% when applied to the Susanne corpus (Sampson, 1995), a 138K word treebanked and balanced subset of the Brown corpus. Wide coverage is important since information is acquired only from successful parses. The combined throughput of the parsing components on a Sun UltraSparc 1/140 is around 50 words per CPU second. 2.2 The Extractor, Classifier and Evaluator The extractor takes as input the ranked analyses from the probabilistic parser. It locates the subanalyses around the predicate, finding the constituents identified as complements inside each subanalysis, and the subject clause preceding it. Instances of passive constructions are recognized a</context>
</contexts>
<marker>Sampson, 1995</marker>
<rawString>Sampson, G. 1995. English for the computer. Oxford, UK: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
</authors>
<title>Stochastic lexicalized tree adjoining grammars.</title>
<date>1992</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics, COLING-92,</booktitle>
<pages>426--432</pages>
<location>Nantes,</location>
<contexts>
<context position="3836" citStr="Schabes (1992)" startWordPosition="549" endWordPosition="551">and the senses of a word change between corpora, sublanguages and/or subject domains (Jensen, 1991). In a recent experiment with a wide-coverage parsing system utilizing a lexicalist grammatical framework, Briscoe &amp; Carroll (1993) observed that half of parse failures on unseen test data were caused by inaccurate subcategorization information in the ANLT dictionary. The close connection between sense and subcategorization and between subject domain and sense makes it likely that a fully accurate &apos;static&apos; subcategorization dictionary of a language is unattainable in any case. Moreover, although Schabes (1992) and others have proposed `lexicalized&apos; probabilistic grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations for individual predicates, because of the problems of accurately estimating them. These problems suggest that automatic construction or updating of subcategorization dictionaries from textual corpora is a more promising avenue to pursue. Preliminary experiments acquiring a few 356 verbal subcategorization classes have been reported by Brent (1991, 1993), Manning (1993), and Ushioda e</context>
</contexts>
<marker>Schabes, 1992</marker>
<rawString>Schabes, Y. 1992. Stochastic lexicalized tree adjoining grammars. In Proceedings of the International Conference on Computational Linguistics, COLING-92, Nantes, France. 426-432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Taylor</author>
<author>G Knowles</author>
</authors>
<title>Manual of information to accompany the SEC corpus: the machinereadable corpus of spoken English.</title>
<date>1988</date>
<location>University of Lancaster, UK, Ms.</location>
<contexts>
<context position="18326" citStr="Taylor &amp; Knowles, 1988" startWordPosition="2813" endWordPosition="2816">exical data or the ability to back-off to lexical semantic classes. At the moment, we exploit linguistic information about the syntactic type, obligatoriness and position of arguments, as well as the set of possible subcategorization classes, and combine this with statistical inference based on the probability of class membership and the frequency and reliability of patterns for classes. 3 Experimental Evaluation 3.1 Lexicon Evaluation â€” Method In order to test the accuracy of our system (as developed so far) and to provide empirical feedback for further development, we took the Susanne, SEC (Taylor &amp; Knowles, 1988) and LOB corpora (Garside et at., 1987)â€”a total of 1.2 million wordsâ€”and extracted all sentences containing an occurrence of one of fourteen verbs, up to a maximum of 1000 citations of each. These verbs, listed in Figure 2, were chosen at random, subject to the constraint that they exhibited multiple complementation patterns. The sentences containing these verbs were tagged and parsed automatically, and the extractor, classifier and evaluator were applied to the resulting lanit_verbs1 &apos;patterns&apos; The binomial distribution gives the probability of an event with probability p happening exactly m </context>
</contexts>
<marker>Taylor, Knowles, 1988</marker>
<rawString>Taylor, L. &amp; Knowles, G. 1988. Manual of information to accompany the SEC corpus: the machinereadable corpus of spoken English. University of Lancaster, UK, Ms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ushioda</author>
<author>D Evans</author>
<author>T Gibson</author>
<author>A Waibel</author>
</authors>
<title>The automatic acquisition of frequencies of verb subcategorization frames from tagged corpora.</title>
<date>1993</date>
<booktitle>SIGLEX ACL Workshop on the Acquisition of Lexical Knowledge from Text.</booktitle>
<pages>95--106</pages>
<editor>In Boguraev, B. &amp; Pustejovsky, J. eds.</editor>
<location>Columbus, Ohio:</location>
<contexts>
<context position="28427" citStr="Ushioda et al. (1993)" startWordPosition="4528" endWordPosition="4531">) approach to acquiring subcategorization is based on a philosophy of only exploiting unambiguous and determinate information in unanalysed corpora. He defines a number of lexical patterns (mostly involving closed class items, such as pronouns) which reliably cue one of five subcategorization classes. Brent does not report comprehensive results, but for one class, sentential complement verbs, he achieves 96% precision and 76% recall at classifying individual tokens of 63 distinct verbs as exemplars or non-exemplars of this class. He does not attempt to rank different classes for a given verb. Ushioda et al. (1993) utilise a PoS tagged corpus and finite-state NP parser to recognize and calculate the relative frequency of six subcategorization classes. They report an accuracy rate of 83% (254 errors) at classifying 1565 classifiable tokens of 33 distinct verbs in running text and suggest that incorrect noun phrase boundary detection accounts for the majority of errors. They report that for 32 verbs their system correctly predicts the most frequent class, and for 30 verbs it correctly predicts the second most frequent class, if there was one. Our system rankings include all classes for each verb, from a t</context>
</contexts>
<marker>Ushioda, Evans, Gibson, Waibel, 1993</marker>
<rawString>Ushioda, A., Evans, D., Gibson, T. &amp; Waibel, A. 1993. The automatic acquisition of frequencies of verb subcategorization frames from tagged corpora. In Boguraev, B. &amp; Pustejovsky, J. eds. SIGLEX ACL Workshop on the Acquisition of Lexical Knowledge from Text. Columbus, Ohio: 95-106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>