<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.086668">
<title confidence="0.9948">
Improving Bitext Word Alignments
via Syntax-based Reordering of English
</title>
<author confidence="0.99486">
Elliott Franco Dr´abek and David Yarowsky
</author>
<affiliation confidence="0.932234">
Department of Computer Science
Johns Hopkins University
</affiliation>
<address confidence="0.847187">
Baltimore, MD 21218, USA
</address>
<email confidence="0.999772">
{edrabek,yarowsky}@cs.jhu.edu
</email>
<sectionHeader confidence="0.995676" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999559764705882">
We present an improved method for automated word
alignment of parallel texts which takes advantage
of knowledge of syntactic divergences, while avoid-
ing the need for syntactic analysis of the less re-
source rich language, and retaining the robustness of
syntactically agnostic approaches such as the IBM
word alignment models. We achieve this by using
simple, easily-elicited knowledge to produce syntax-
based heuristics which transform the target lan-
guage (e.g. English) into a form more closely resem-
bling the source language, and then by using stan-
dard alignment methods to align the transformed
bitext. We present experimental results under vari-
able resource conditions. The method improves
word alignment performance for language pairs such
as English-Korean and English-Hindi, which exhibit
longer-distance syntactic divergences.
</bodyText>
<sectionHeader confidence="0.998523" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979111111111">
Word-level alignment is a key infrastructural tech-
nology for multilingual processing. It is crucial for
the development of translation models and transla-
tion lexica (Tufi¸s, 2002; Melamed, 1998), as well as
for translingual projection (Yarowsky et al., 2001;
Lopez et al., 2002). It has increasingly attracted at-
tention as a task worthy of study in its own right
(Mihalcea and Pedersen, 2003; Och and Ney, 2000).
Syntax-light alignment models such as the five
IBM models (Brown et al., 1993) and their rela-
tives have proved to be very successful and robust
at producing word-level alignments, especially for
closely related languages with similar word order
and mostly local reorderings, which can be cap-
tured via simple models of relative word distortion.
However, these models have been less successful at
modeling syntactic distortions with longer distance
movement. In contrast, more syntactically informed
approaches have been constrained by the often weak
syntactic correspondences typical of real-world par-
allel texts, and by the difficulty of finding or induc-
ing syntactic parsers for any but a few of the world’s
most studied languages.
Our approach uses simple, easily-elicited knowl-
edge of divergences to produce heuristic syntax-
based transformations from English to a form
(English&apos;) more closely resembling the source lan-
</bodyText>
<figureCaption confidence="0.999472">
Figure 1: System Architecture
</figureCaption>
<bodyText confidence="0.999960782608696">
guage, and then using standard alignment methods
to align the transformed version to the target lan-
guage. This approach retains the robustness of syn-
tactically agnostic models, while taking advantage
of syntactic knowledge. Because the approach relies
only on syntactic analysis of English, it can avoid
the difficulty of developing a full parser for a new
low-resource language.
Our method is rapid and low cost. It requires
only coarse-grained knowledge of basic word order,
knowledge which can be rapidly found in even the
briefest grammatical sketches. Because basic word
order changes very slowly with time, word order of
related languages tends to be very similar. For ex-
ample, even if we only know that a language is of
the Northern-Indian/Sanskrit family, we can easily
guess with high confidence that it is systematically
head-final. Because our method can be restricted
to only bi-text pre-processing and post-processing,
it can be used as a wrapper around any existing
word-alignment tool, without modification, to pro-
vide improved performance by minimizing alignment
distortion.
</bodyText>
<sectionHeader confidence="0.623824" genericHeader="introduction">
2 Prior Work
</sectionHeader>
<footnote confidence="0.8298185">
The 2003 HLT-NAACL Workshop on Building and
Using Parallel Texts (Mihalcea and Pedersen, 2003)
reflected the increasing importance of the word-
alignment task, and established standard perfor-
mance measures and some benchmark tasks.
There is prior work studying systematic cross-
</footnote>
<figure confidence="0.999036571428571">
English
Source
 |\|
English
Language-specific
Heuristics
Transform
Retrace
Traces
Run GIZA++
English’
Source
|/ |
English’
Source
English:
NP
VP
NP
NP
PP
the
use of plutonium is to manufacture nuclear weapons
VP
VP
S
Hindi: plutoniyama kaa istemaala paramaanu hathiyaara banaane ke lie hotaa hai
plutonium ’s use nuclear weapons manufacture to is
</figure>
<figureCaption confidence="0.998636">
Figure 3: Transformed Hindi-English&apos; sentence pair with gold-standard word-alignments. Rotated nodes are
marked with an arc.
Figure 2: Original Hindi-English sentence pair with gold-standard word-alignments.
</figureCaption>
<figure confidence="0.980247555555555">
S
VP
NP VP
NP NP
English’: plutonium of the use nuclear weapons manufacture to is
VP
PP
Hindi: plutoniyama kaa istemaala paramaanu hathiyaara banaane ke lie hotaa hai
plutonium ’s use nuclear weapons manufacture to is
</figure>
<bodyText confidence="0.99905996">
linguistic structural divergences, such as the DUSTer
system (Dorr et al., 2002). While the focus on ma-
jor classes of structural variation such as manner-of-
motion verb-phrase transformations have facilitated
both transfer and generation in machine translation,
these divergences have not been integrated into a
system that produces automatic word alignments
and have tended to focus on more local phrasal varia-
tion rather than more comprehensive sentential syn-
tactic reordering.
Complementary prior work (e.g. Wu, 1995) has
also addressed syntactic transduction for bilingual
parsing, translation, and word-alignment. Much of
this work depends on high-quality parsing of both
target and source sentences, which may be unavail-
able for many “lower density” languages of interest.
Tree-to-string models, such as (Yamada and Knight,
2001) remove this dependency, and such models are
well suited for situations with large, cleanly trans-
lated training corpora. By contrast, our method re-
tains the robustness of the underlying aligner to-
wards loose translations, and can if necessary use
knowledge of syntactic divergences even in the ab-
sence of any training corpora whatsoever, using only
a translation lexicon.
</bodyText>
<sectionHeader confidence="0.983726" genericHeader="method">
3 System
</sectionHeader>
<bodyText confidence="0.9981525">
Figure 1 shows the system architecture. We start
by running the Collins parser (Collins, 1999) on the
English side of both training and testing data, and
apply our source-language-specific heuristics to the
</bodyText>
<table confidence="0.999221">
Language VP AP NP
English VO AO AN, NR
Hindi OV OA AN, RN
Korean OV OA AN, RN
Chinese VO AOA AN, RN
Romanian VO AO NA, NR
</table>
<tableCaption confidence="0.998928">
Table 1: Basic word order for three major phrase
</tableCaption>
<bodyText confidence="0.991774954545455">
types – VP: verb phrases with Verb and Object,
AP: appositional (prepositional or postpositional)
phrases with Apposition and Object, and NP: noun
phrases with Noun and Adjective or Relative clause.
Chinese has both prepositions and postpositions.
resulting trees. This yields English&apos; text, along with
traces recording correspondences between English&apos;
words and the English originals. We use GIZA++
(Och and Ney, 2000) to align the English&apos; with the
source language text, yielding alignments in terms
of the English&apos;. Finally, we use the traces to map
these alignments to the original English words.
Figure 2 shows an illustrative Hindi-English sen-
tence pair, with true word alignments, and parse-
tree over the English sentence. Although it is only
a short sentence, the large number of crossing align-
ments clearly show the high-degree of reordering,
and especially long-distance motion, caused by the
syntactic divergences between Hindi and English.
Figure 3 shows the same sentence pair after En-
glish has been transformed into English&apos; by our sys-
tem. Tree nodes whose children have been reordered
</bodyText>
<figure confidence="0.9569415">
3 3.2 3.4 3.6 3.8 4 4.2 4.4 4.6 4.8
log(number of training sentences)
</figure>
<figureCaption confidence="0.9997865">
Figure 4: Hindi alignment performance
Figure 5: Korean alignment performance
</figureCaption>
<bodyText confidence="0.999991166666667">
are marked by a subtended arc. Crossings have been
eliminated, and the alignment is now monotonic.
Table 1 shows the basic word order of three major
phrase types for each of the languages we treated. In
each case, our heuristics transform the English trees
to achieve these same word orders. For the Chinese
case, we apply several more language-specific trans-
formations. Because Chinese has both prepositions
and postpositions, we retain the original preposition
and add an additional bracketing postposition. We
also move verb modifiers other than noun phrases to
the left of the head verb.
</bodyText>
<sectionHeader confidence="0.999668" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999893071428572">
For each language we treated, we assembled
sentence-aligned, tokenized training and test cor-
pora, with hand-annotated gold-standard word
alignments for the latter&apos;. We did not apply any
sort of morphological analysis beyond basic word to-
kenization. We measured system performance with
wa eval align.pl, provided by Rada Mihalcea and
Ted Pedersen.
Each training set provides the aligner with infor-
mation about lexical affinities and reordering pat-
terns. For Hindi, Korean and Chinese, we also tested
our system under the more difficult situation of hav-
ing only a bilingual word list but no bitext available.
This is a plausible low-resource language scenario
</bodyText>
<figure confidence="0.8038435">
3 35 4, 4.5 5
log(number of training sentences)
</figure>
<figureCaption confidence="0.9998">
Figure 6: Chinese alignment performance
Figure 7: Romanian alignment performance
</figureCaption>
<table confidence="0.999971464285714">
# Train Direct English&apos;
Sents
P R F P R F
Hindi
Dict only 16.4 13.8 15.0 18.5 15.6 17.0
1000 26.8 23.0 24.8 28.4 24.4 26.2
3162 35.7 31.6 33.5 38.4 33.5 35.8
10000 46.6 42.7 44.6 50.4 45.2 47.6
31622 60.1 56.0 58.0 63.6 58.5 61.0
63095 64.7 61.7 63.2 66.3 62.2 64.2
Korean
Dict only 26.6 12.3 16.9 27.5 12.9 17.6
1000 9.4 7.3 8.2 11.3 8.7 9.8
3162 13.2 10.2 11.5 16.0 12.4 14.0
10000 15.2 12.0 13.4 17.0 13.3 14.9
30199 21.5 16.9 18.9 21.9 17.2 19.3
Chinese
Dict only 44.4 30.4 36.1 44.5 30.5 36.2
1000 33.0 22.2 26.5 30.8 22.6 26.1
3162 44.6 28.9 35.1 41.7 30.0 34.9
10000 51.1 34.0 40.8 50.7 35.8 42.0
31622 60.4 39.0 47.4 55.7 39.7 46.4
100000 66.0 43.7 52.6 63.7 45.4 53.0
Romanian
1000 49.6 27.7 35.6 50.1 28.0 35.9
3162 57.9 33.4 42.4 57.6 33.0 42.0
10000 72.6 45.5 55.9 71.3 45.0 55.2
48441 84.7 57.8 68.7 83.5 57.1 67.8
</table>
<tableCaption confidence="0.988367">
Table 2: Performance in Precision, Recall, and F-
measure (per cent) of all systems.
</tableCaption>
<figure confidence="0.99020856">
70
65
60
55
50
45
40
35
30
25
20
E’ Method
Direct
3 3.2 n3.4 3.6 3$ 4 4.2 4.4
log(
number of training sentences)
F-measure
25
20
15
10
5
0
E’ Method
Direct
55
50
45
40
35
30
25
E’ Method
Direct
3 3.2 3.4 3.6 3.8 4 4.2 4.4 4.6
log(number of training sentences)
F-measure
75
70
65
60
55
50
45
40
35
E’ Method
Direct
F-measure
F-measure
</figure>
<table confidence="0.987854571428571">
Source # Test Mean Correlation
Language Sents Length
Direct E&apos;
Hindi 46 16.3 54.1 60.1
Korean 100 20.2 10.2 31.6
Chinese 88 26.5 60.2 63.7
Romanian 248 22.7 81.1 80.6
</table>
<tableCaption confidence="0.998704">
Table 3: Test set characteristics, including number
</tableCaption>
<bodyText confidence="0.997258176470588">
of sentence pairs, mean length of English sentences,
and correlation r2 between English and source-
language normalized word positions in gold-standard
data, for direct and English&apos; situations.
and a test of the ability of the system to take sole
responsibility for knowledge of reordering.
Table 3 describes the test sets and shows the cor-
relation in gold standard aligned word pairs between
the position of the English word in the English sen-
tence and the position of the source-language word
in the source-language sentence (normalizing the po-
sitions to fall between 0 and 1). The baseline (di-
rect) correlations give quantitative evidence of dif-
fering degrees of syntactic divergence with English,
and the English&apos; correlations demonstrate that our
heuristics do have the effect of better fitting source
language word order.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999827066666667">
Figures 4, 5, 6 and 7 show learning curves for sys-
tems trained on parallel sentences with and with-
out the English&apos; transforms. Table 2 provides fur-
ther detail, and also shows the performance of sys-
tems trained without any bitext, but only with ac-
cess to a bilingual translation lexicon. Our sys-
tem achieves consistent, substantial performance im-
provement under all situations for English-Hindi
and English-Korean language pairs, which exhibit
longer distance SOV→SVO syntactic divergence.
For English-Romanian and English-Chinese, neither
significant improvement nor degradation is seen, but
these are language pairs with quite similar sentential
word order to English, and hence have less opportu-
nity to benefit from our syntactic transformations.
</bodyText>
<sectionHeader confidence="0.999652" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999117096774194">
We have developed a system to improve the per-
formance of bitext word alignment between English
and a source language by first reordering parsed
English into an order more closely resembling that
&apos;Hindi training: news text from the LDC for the 2003
DARPA TIDES Surprise Language exercise; Hindi testing:
news text from Rebecca Hwa, then at the University of Mary-
land; Hindi dictionary: The Hindi-English Dictionary, v. 2.0
from IIIT (Hyderabad) LTRC; Korean training: Unbound
Bible; Korean testing: half from Penn Korean Treebank and
half from Universal declaration of Human Rights, aligned by
Woosung Kim at the Johns Hopkins University; Korean dic-
tionary: EngDic v. 4; Chinese training: news text from FBIS;
Chinese testing: Penn Chinese Treebank news text aligned by
Rebecca Hwa, then at the University of Maryland; Chinese
dictionary: from the LDC; Romanian training and testing:
(Mihalcea and Pedersen, 2003).
of the source language, based only on knowledge
of the coarse basic word order of the source lan-
guage, such as can be obtained from any cross-
linguistic survey of languages, and requiring no pars-
ing of the source language. We applied the sys-
tem to the task of aligning English with Hindi, Ko-
rean, Chinese and Romanian. Performance improve-
ment is greatest for Hindi and Korean, which exhibit
longer-distance constituent reordering with respect
to English. These properties suggest the proposed
English&apos; word alignment method can be an effective
approach for word alignment to languages with both
greater cross-linguistic word-order divergence and an
absence of available parsers.
</bodyText>
<sectionHeader confidence="0.999283" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994213780487805">
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
B. J. Dorr, L. Pearl, R. Hwa, and N. Habash. 2002.
DUSTer: A method for unraveling cross-language
divergences for statistical word-level alignment.
In Proceedings of AMTA-02, pages 31–43.
A. Lopez, M. Nosal, R. Hwa, and P. Resnik. 2002.
Word-level alignment for multilingual resource ac-
quisition. In Proceedings of the LREC-02 Work-
shop on Linguistic Knowledge Acquisition and
Representation.
I. D. Melamed. 1998. Empirical methods for MT
lexicon development. Lecture Notes in Computer
Science, 1529:18–9999.
R. Mihalcea and T. Pedersen. 2003. An evalua-
tion exercise for word alignment. In Rada Mi-
halcea and Ted Pedersen, editors, Proceedings of
the HLT-NAACL 2003 Workshop on Building and
Using Parallel Texts, pages 1–10.
F. J. Och and H. Ney. 2000. A comparison of align-
ment models for statistical machine translation.
In Proceedings of COLING-00, pages 1086–1090.
D. I. Tufi¸s. 2002. A cheap and fast way to build
useful translation lexicons. In Proceedings of
COLING-02, pages 1030–1036.
D. Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation,
bracketing, and alignment of parallel corpora. In
Proceedings of IJCAI-95, pages 1328–1335.
K. Yamada and K. Knight. 2001. A syntax-based
statistical translation model. In Proceedings of
ACL-01, pages 523–530.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Pro-
ceedings of HLT-01, pages 161–168.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867354">
<title confidence="0.936822">Improving Bitext Word Alignments via Syntax-based Reordering of English</title>
<author confidence="0.99962">Elliott Franco Dr´abek</author>
<author confidence="0.99962">David Yarowsky</author>
<affiliation confidence="0.9995525">Department of Computer Science Johns Hopkins University</affiliation>
<address confidence="0.999958">Baltimore, MD 21218, USA</address>
<abstract confidence="0.999644388888889">We present an improved method for automated word alignment of parallel texts which takes advantage of knowledge of syntactic divergences, while avoiding the need for syntactic analysis of the less resource rich language, and retaining the robustness of syntactically agnostic approaches such as the IBM word alignment models. We achieve this by using simple, easily-elicited knowledge to produce syntaxbased heuristics which transform the target language (e.g. English) into a form more closely resembling the source language, and then by using standard alignment methods to align the transformed bitext. We present experimental results under variable resource conditions. The method improves word alignment performance for language pairs such as English-Korean and English-Hindi, which exhibit longer-distance syntactic divergences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1574" citStr="Brown et al., 1993" startWordPosition="227" endWordPosition="230">or language pairs such as English-Korean and English-Hindi, which exhibit longer-distance syntactic divergences. 1 Introduction Word-level alignment is a key infrastructural technology for multilingual processing. It is crucial for the development of translation models and translation lexica (Tufi¸s, 2002; Melamed, 1998), as well as for translingual projection (Yarowsky et al., 2001; Lopez et al., 2002). It has increasingly attracted attention as a task worthy of study in its own right (Mihalcea and Pedersen, 2003; Och and Ney, 2000). Syntax-light alignment models such as the five IBM models (Brown et al., 1993) and their relatives have proved to be very successful and robust at producing word-level alignments, especially for closely related languages with similar word order and mostly local reorderings, which can be captured via simple models of relative word distortion. However, these models have been less successful at modeling syntactic distortions with longer distance movement. In contrast, more syntactically informed approaches have been constrained by the often weak syntactic correspondences typical of real-world parallel texts, and by the difficulty of finding or inducing syntactic parsers fo</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5907" citStr="Collins, 1999" startWordPosition="882" endWordPosition="883">entences, which may be unavailable for many “lower density” languages of interest. Tree-to-string models, such as (Yamada and Knight, 2001) remove this dependency, and such models are well suited for situations with large, cleanly translated training corpora. By contrast, our method retains the robustness of the underlying aligner towards loose translations, and can if necessary use knowledge of syntactic divergences even in the absence of any training corpora whatsoever, using only a translation lexicon. 3 System Figure 1 shows the system architecture. We start by running the Collins parser (Collins, 1999) on the English side of both training and testing data, and apply our source-language-specific heuristics to the Language VP AP NP English VO AO AN, NR Hindi OV OA AN, RN Korean OV OA AN, RN Chinese VO AOA AN, RN Romanian VO AO NA, NR Table 1: Basic word order for three major phrase types – VP: verb phrases with Verb and Object, AP: appositional (prepositional or postpositional) phrases with Apposition and Object, and NP: noun phrases with Noun and Adjective or Relative clause. Chinese has both prepositions and postpositions. resulting trees. This yields English&apos; text, along with traces record</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Dorr</author>
<author>L Pearl</author>
<author>R Hwa</author>
<author>N Habash</author>
</authors>
<title>DUSTer: A method for unraveling cross-language divergences for statistical word-level alignment.</title>
<date>2002</date>
<booktitle>In Proceedings of AMTA-02,</booktitle>
<pages>31--43</pages>
<contexts>
<context position="4678" citStr="Dorr et al., 2002" startWordPosition="696" endWordPosition="699"> kaa istemaala paramaanu hathiyaara banaane ke lie hotaa hai plutonium ’s use nuclear weapons manufacture to is Figure 3: Transformed Hindi-English&apos; sentence pair with gold-standard word-alignments. Rotated nodes are marked with an arc. Figure 2: Original Hindi-English sentence pair with gold-standard word-alignments. S VP NP VP NP NP English’: plutonium of the use nuclear weapons manufacture to is VP PP Hindi: plutoniyama kaa istemaala paramaanu hathiyaara banaane ke lie hotaa hai plutonium ’s use nuclear weapons manufacture to is linguistic structural divergences, such as the DUSTer system (Dorr et al., 2002). While the focus on major classes of structural variation such as manner-ofmotion verb-phrase transformations have facilitated both transfer and generation in machine translation, these divergences have not been integrated into a system that produces automatic word alignments and have tended to focus on more local phrasal variation rather than more comprehensive sentential syntactic reordering. Complementary prior work (e.g. Wu, 1995) has also addressed syntactic transduction for bilingual parsing, translation, and word-alignment. Much of this work depends on high-quality parsing of both targ</context>
</contexts>
<marker>Dorr, Pearl, Hwa, Habash, 2002</marker>
<rawString>B. J. Dorr, L. Pearl, R. Hwa, and N. Habash. 2002. DUSTer: A method for unraveling cross-language divergences for statistical word-level alignment. In Proceedings of AMTA-02, pages 31–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
<author>M Nosal</author>
<author>R Hwa</author>
<author>P Resnik</author>
</authors>
<title>Word-level alignment for multilingual resource acquisition.</title>
<date>2002</date>
<booktitle>In Proceedings of the LREC-02 Workshop on Linguistic Knowledge Acquisition and Representation.</booktitle>
<contexts>
<context position="1361" citStr="Lopez et al., 2002" startWordPosition="190" endWordPosition="193">e source language, and then by using standard alignment methods to align the transformed bitext. We present experimental results under variable resource conditions. The method improves word alignment performance for language pairs such as English-Korean and English-Hindi, which exhibit longer-distance syntactic divergences. 1 Introduction Word-level alignment is a key infrastructural technology for multilingual processing. It is crucial for the development of translation models and translation lexica (Tufi¸s, 2002; Melamed, 1998), as well as for translingual projection (Yarowsky et al., 2001; Lopez et al., 2002). It has increasingly attracted attention as a task worthy of study in its own right (Mihalcea and Pedersen, 2003; Och and Ney, 2000). Syntax-light alignment models such as the five IBM models (Brown et al., 1993) and their relatives have proved to be very successful and robust at producing word-level alignments, especially for closely related languages with similar word order and mostly local reorderings, which can be captured via simple models of relative word distortion. However, these models have been less successful at modeling syntactic distortions with longer distance movement. In contr</context>
</contexts>
<marker>Lopez, Nosal, Hwa, Resnik, 2002</marker>
<rawString>A. Lopez, M. Nosal, R. Hwa, and P. Resnik. 2002. Word-level alignment for multilingual resource acquisition. In Proceedings of the LREC-02 Workshop on Linguistic Knowledge Acquisition and Representation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Empirical methods for MT lexicon development.</title>
<date>1998</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>1529--18</pages>
<contexts>
<context position="1277" citStr="Melamed, 1998" startWordPosition="178" endWordPosition="179">sform the target language (e.g. English) into a form more closely resembling the source language, and then by using standard alignment methods to align the transformed bitext. We present experimental results under variable resource conditions. The method improves word alignment performance for language pairs such as English-Korean and English-Hindi, which exhibit longer-distance syntactic divergences. 1 Introduction Word-level alignment is a key infrastructural technology for multilingual processing. It is crucial for the development of translation models and translation lexica (Tufi¸s, 2002; Melamed, 1998), as well as for translingual projection (Yarowsky et al., 2001; Lopez et al., 2002). It has increasingly attracted attention as a task worthy of study in its own right (Mihalcea and Pedersen, 2003; Och and Ney, 2000). Syntax-light alignment models such as the five IBM models (Brown et al., 1993) and their relatives have proved to be very successful and robust at producing word-level alignments, especially for closely related languages with similar word order and mostly local reorderings, which can be captured via simple models of relative word distortion. However, these models have been less </context>
</contexts>
<marker>Melamed, 1998</marker>
<rawString>I. D. Melamed. 1998. Empirical methods for MT lexicon development. Lecture Notes in Computer Science, 1529:18–9999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>T Pedersen</author>
</authors>
<title>An evaluation exercise for word alignment.</title>
<date>2003</date>
<booktitle>Proceedings of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts,</booktitle>
<pages>1--10</pages>
<editor>In Rada Mihalcea and Ted Pedersen, editors,</editor>
<contexts>
<context position="1474" citStr="Mihalcea and Pedersen, 2003" startWordPosition="210" endWordPosition="213">nt experimental results under variable resource conditions. The method improves word alignment performance for language pairs such as English-Korean and English-Hindi, which exhibit longer-distance syntactic divergences. 1 Introduction Word-level alignment is a key infrastructural technology for multilingual processing. It is crucial for the development of translation models and translation lexica (Tufi¸s, 2002; Melamed, 1998), as well as for translingual projection (Yarowsky et al., 2001; Lopez et al., 2002). It has increasingly attracted attention as a task worthy of study in its own right (Mihalcea and Pedersen, 2003; Och and Ney, 2000). Syntax-light alignment models such as the five IBM models (Brown et al., 1993) and their relatives have proved to be very successful and robust at producing word-level alignments, especially for closely related languages with similar word order and mostly local reorderings, which can be captured via simple models of relative word distortion. However, these models have been less successful at modeling syntactic distortions with longer distance movement. In contrast, more syntactically informed approaches have been constrained by the often weak syntactic correspondences typ</context>
<context position="3645" citStr="Mihalcea and Pedersen, 2003" startWordPosition="542" endWordPosition="545"> word order changes very slowly with time, word order of related languages tends to be very similar. For example, even if we only know that a language is of the Northern-Indian/Sanskrit family, we can easily guess with high confidence that it is systematically head-final. Because our method can be restricted to only bi-text pre-processing and post-processing, it can be used as a wrapper around any existing word-alignment tool, without modification, to provide improved performance by minimizing alignment distortion. 2 Prior Work The 2003 HLT-NAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003) reflected the increasing importance of the wordalignment task, and established standard performance measures and some benchmark tasks. There is prior work studying systematic crossEnglish Source |\| English Language-specific Heuristics Transform Retrace Traces Run GIZA++ English’ Source |/ | English’ Source English: NP VP NP NP PP the use of plutonium is to manufacture nuclear weapons VP VP S Hindi: plutoniyama kaa istemaala paramaanu hathiyaara banaane ke lie hotaa hai plutonium ’s use nuclear weapons manufacture to is Figure 3: Transformed Hindi-English&apos; sentence pair with gold-standard wor</context>
<context position="12805" citStr="Mihalcea and Pedersen, 2003" startWordPosition="2025" endWordPosition="2028">se; Hindi testing: news text from Rebecca Hwa, then at the University of Maryland; Hindi dictionary: The Hindi-English Dictionary, v. 2.0 from IIIT (Hyderabad) LTRC; Korean training: Unbound Bible; Korean testing: half from Penn Korean Treebank and half from Universal declaration of Human Rights, aligned by Woosung Kim at the Johns Hopkins University; Korean dictionary: EngDic v. 4; Chinese training: news text from FBIS; Chinese testing: Penn Chinese Treebank news text aligned by Rebecca Hwa, then at the University of Maryland; Chinese dictionary: from the LDC; Romanian training and testing: (Mihalcea and Pedersen, 2003). of the source language, based only on knowledge of the coarse basic word order of the source language, such as can be obtained from any crosslinguistic survey of languages, and requiring no parsing of the source language. We applied the system to the task of aligning English with Hindi, Korean, Chinese and Romanian. Performance improvement is greatest for Hindi and Korean, which exhibit longer-distance constituent reordering with respect to English. These properties suggest the proposed English&apos; word alignment method can be an effective approach for word alignment to languages with both grea</context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>R. Mihalcea and T. Pedersen. 2003. An evaluation exercise for word alignment. In Rada Mihalcea and Ted Pedersen, editors, Proceedings of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING-00,</booktitle>
<pages>1086--1090</pages>
<contexts>
<context position="1494" citStr="Och and Ney, 2000" startWordPosition="214" endWordPosition="217"> variable resource conditions. The method improves word alignment performance for language pairs such as English-Korean and English-Hindi, which exhibit longer-distance syntactic divergences. 1 Introduction Word-level alignment is a key infrastructural technology for multilingual processing. It is crucial for the development of translation models and translation lexica (Tufi¸s, 2002; Melamed, 1998), as well as for translingual projection (Yarowsky et al., 2001; Lopez et al., 2002). It has increasingly attracted attention as a task worthy of study in its own right (Mihalcea and Pedersen, 2003; Och and Ney, 2000). Syntax-light alignment models such as the five IBM models (Brown et al., 1993) and their relatives have proved to be very successful and robust at producing word-level alignments, especially for closely related languages with similar word order and mostly local reorderings, which can be captured via simple models of relative word distortion. However, these models have been less successful at modeling syntactic distortions with longer distance movement. In contrast, more syntactically informed approaches have been constrained by the often weak syntactic correspondences typical of real-world p</context>
<context position="6610" citStr="Och and Ney, 2000" startWordPosition="996" endWordPosition="999">-specific heuristics to the Language VP AP NP English VO AO AN, NR Hindi OV OA AN, RN Korean OV OA AN, RN Chinese VO AOA AN, RN Romanian VO AO NA, NR Table 1: Basic word order for three major phrase types – VP: verb phrases with Verb and Object, AP: appositional (prepositional or postpositional) phrases with Apposition and Object, and NP: noun phrases with Noun and Adjective or Relative clause. Chinese has both prepositions and postpositions. resulting trees. This yields English&apos; text, along with traces recording correspondences between English&apos; words and the English originals. We use GIZA++ (Och and Ney, 2000) to align the English&apos; with the source language text, yielding alignments in terms of the English&apos;. Finally, we use the traces to map these alignments to the original English words. Figure 2 shows an illustrative Hindi-English sentence pair, with true word alignments, and parsetree over the English sentence. Although it is only a short sentence, the large number of crossing alignments clearly show the high-degree of reordering, and especially long-distance motion, caused by the syntactic divergences between Hindi and English. Figure 3 shows the same sentence pair after English has been transfo</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. A comparison of alignment models for statistical machine translation. In Proceedings of COLING-00, pages 1086–1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D I Tufi¸s</author>
</authors>
<title>A cheap and fast way to build useful translation lexicons.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING-02,</booktitle>
<pages>1030--1036</pages>
<marker>Tufi¸s, 2002</marker>
<rawString>D. I. Tufi¸s. 2002. A cheap and fast way to build useful translation lexicons. In Proceedings of COLING-02, pages 1030–1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI-95,</booktitle>
<pages>1328--1335</pages>
<contexts>
<context position="5117" citStr="Wu, 1995" startWordPosition="762" endWordPosition="763">nu hathiyaara banaane ke lie hotaa hai plutonium ’s use nuclear weapons manufacture to is linguistic structural divergences, such as the DUSTer system (Dorr et al., 2002). While the focus on major classes of structural variation such as manner-ofmotion verb-phrase transformations have facilitated both transfer and generation in machine translation, these divergences have not been integrated into a system that produces automatic word alignments and have tended to focus on more local phrasal variation rather than more comprehensive sentential syntactic reordering. Complementary prior work (e.g. Wu, 1995) has also addressed syntactic transduction for bilingual parsing, translation, and word-alignment. Much of this work depends on high-quality parsing of both target and source sentences, which may be unavailable for many “lower density” languages of interest. Tree-to-string models, such as (Yamada and Knight, 2001) remove this dependency, and such models are well suited for situations with large, cleanly translated training corpora. By contrast, our method retains the robustness of the underlying aligner towards loose translations, and can if necessary use knowledge of syntactic divergences eve</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>D. Wu. 1995. Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora. In Proceedings of IJCAI-95, pages 1328–1335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL-01,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="5432" citStr="Yamada and Knight, 2001" startWordPosition="805" endWordPosition="808">th transfer and generation in machine translation, these divergences have not been integrated into a system that produces automatic word alignments and have tended to focus on more local phrasal variation rather than more comprehensive sentential syntactic reordering. Complementary prior work (e.g. Wu, 1995) has also addressed syntactic transduction for bilingual parsing, translation, and word-alignment. Much of this work depends on high-quality parsing of both target and source sentences, which may be unavailable for many “lower density” languages of interest. Tree-to-string models, such as (Yamada and Knight, 2001) remove this dependency, and such models are well suited for situations with large, cleanly translated training corpora. By contrast, our method retains the robustness of the underlying aligner towards loose translations, and can if necessary use knowledge of syntactic divergences even in the absence of any training corpora whatsoever, using only a translation lexicon. 3 System Figure 1 shows the system architecture. We start by running the Collins parser (Collins, 1999) on the English side of both training and testing data, and apply our source-language-specific heuristics to the Language VP </context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Proceedings of ACL-01, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>G Ngai</author>
<author>R Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of HLT-01,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="1340" citStr="Yarowsky et al., 2001" startWordPosition="186" endWordPosition="189">e closely resembling the source language, and then by using standard alignment methods to align the transformed bitext. We present experimental results under variable resource conditions. The method improves word alignment performance for language pairs such as English-Korean and English-Hindi, which exhibit longer-distance syntactic divergences. 1 Introduction Word-level alignment is a key infrastructural technology for multilingual processing. It is crucial for the development of translation models and translation lexica (Tufi¸s, 2002; Melamed, 1998), as well as for translingual projection (Yarowsky et al., 2001; Lopez et al., 2002). It has increasingly attracted attention as a task worthy of study in its own right (Mihalcea and Pedersen, 2003; Och and Ney, 2000). Syntax-light alignment models such as the five IBM models (Brown et al., 1993) and their relatives have proved to be very successful and robust at producing word-level alignments, especially for closely related languages with similar word order and mostly local reorderings, which can be captured via simple models of relative word distortion. However, these models have been less successful at modeling syntactic distortions with longer distan</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of HLT-01, pages 161–168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>