<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000147">
<title confidence="0.893859">
Efficient Subsampling for Training Complex Language Models
</title>
<author confidence="0.663396">
Puyang Xu Asela Gunawardana# Sanjeev Khudanpur
</author>
<email confidence="0.864027">
puyangxu@jhu.edu aselag@microsoft.com khudanpur@jhu.edu
</email>
<affiliation confidence="0.839963">
Department of Electrical and Computer Engineering #Microsoft Research
Center for Language and Speech Processing Redmond, WA 98052, USA
Johns Hopkins University
Baltimore, MD 21218, USA
</affiliation>
<sectionHeader confidence="0.982275" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999643882352941">
We propose an efficient way to train maximum
entropy language models (MELM) and neural
network language models (NNLM). The ad-
vantage of the proposed method comes from
a more robust and efficient subsampling tech-
nique. The original multi-class language mod-
eling problem is transformed into a set of bi-
nary problems where each binary classifier
predicts whether or not a particular word will
occur. We show that the binarized model is
as powerful as the standard model and allows
us to aggressively subsample negative training
examples without sacrificing predictive per-
formance. Empirical results show that we can
train MELM and NNLM at 1% — 5% of the
standard complexity with no loss in perfor-
mance.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999609">
Language models (LM) assign probabilities to se-
quences of words. They are widely used in many
natural language processing applications. The prob-
ability of a sequence can be modeled as a product of
local probabilities, as shown in (1), where wi is the
ith word, and hi is the word history preceding wi.
</bodyText>
<equation confidence="0.97592">
l
P(w1, w2, ..., wl) = P(wi|hi) (1)
i=1
</equation>
<bodyText confidence="0.999915564102564">
Therefore the task of language modeling reduces
to estimating a set of conditional distributions
{P(w|h)}. The n-gram LM is a dominant way to
parametrize P(w|h), where it is assumed that w only
depends on the previous n—1 words. More complex
models have also been proposed–MELM (Rosen-
feld, 1996) and NNLM (Bengio et al., 2003) are two
examples.
Modeling P(w|h) can be seen as a multi-class
classification problem. Given the history, we have
to choose a word in the vocabulary, which can eas-
ily be a few hundred thousand words in size. For
complex models such as MELM and NNLM, this
poses a computational challenge for learning, be-
cause the resulting objective functions are expensive
to normalize. In contrast, n-gram LMs do not suf-
fer from this computational challenge. In the web
era, language modelers have access to virtually un-
limited amounts of data, while the computing power
available to process this data is limited. Therefore,
despite the demonstrated effectiveness of complex
LMs, the n-gram is still the predominant approach
for most real world applications.
Subsampling is a simple solution to get around
the constraint of computing resources. For the pur-
pose of language modeling, it amounts to taking
only part of the text corpus to train the LM. For
complex models such as NNLM, it has been shown
that subsampling can speed up training greatly, at
the cost of some degradation in predictive perfor-
mance (Schwenk, 2007), allowing for trade-off be-
tween computational cost and LM quality.
Our contribution is a novel way to train com-
plex LMs such as MELM and NNLM which allows
much more aggressive subsampling without incur-
ring as high a cost in predictive performance. The
key to our approach is reducing the multi-class LM
problem into a set of binary problems. Instead of
training a V-class classifier, where V is the size of
</bodyText>
<page confidence="0.951542">
1128
</page>
<note confidence="0.9576805">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1128–1136,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999941727272727">
the vocabulary, we train V binary classifiers, each
one of which performs a one-against-all classifica-
tion. The V trained binary probabilities are then re-
normalized to obtain a valid distribution over the V
words. Subsampling here can be done in the nega-
tive examples. Since the majority of training exam-
ples are negative for each of the binary classifiers,
we can achieve substantial computational saving by
only keeping subsets of them. We will show that the
binarized LM is as powerful as its multi-class coun-
terpart, while being able to sustain much more ag-
gressive subsampling. For certain types of LMs such
as MELM, there are more benefits–the binarization
leads to a set of completely independent classifiers
to train, which allows easy parallelization and sig-
nificantly lowers the memory requirement.
Similar one-against-all approaches are often used
in the machine learning community, especially by
SVM (support vector machine) practitioners to solve
multi-class problems (Rifkin and Klautau, 2004;
Allwein et al., 2000). The goal of this paper is to
show that a similar technique can also be used for
language modeling and that it enables us to sub-
sample data much more efficiently. We show that
the proposed approach is useful when the dominant
modeling constraint is computing power as opposed
to training data.
The rest of the paper is organized as follows. In
section 2, we describe our binarization and subsam-
pling techniques for language models with MELM
and NNLM as two specific examples. Experimental
results are presented in Section 3, followed by dis-
cussion in Section 4.
</bodyText>
<sectionHeader confidence="0.6672085" genericHeader="method">
2 Approximating Language Models with
Binary Classifiers
</sectionHeader>
<bodyText confidence="0.9987545">
Suppose we have an LM that can be written in the
form
</bodyText>
<equation confidence="0.99804725">
exp aw(h; θ)
P(w|h) =
E (2)
w, exp aw,(h; θ),
</equation>
<bodyText confidence="0.9992696">
where aw(h; θ) is a parametrized history representa-
tion for word w.
Given a training corpus of word history pairs with
empirical distribution P˜(h, w), the regularized log
likelihood of the training set can be written as
</bodyText>
<equation confidence="0.976627666666667">
�
P˜ (h) P˜(w|h) log P(w|h) − r(θ), (3)
w
</equation>
<bodyText confidence="0.998902142857143">
where r(θ) is the regularizing function over the pa-
rameters.
Assuming that r(θ) can be written as a sum over
per-word regularizers, namely r(θ) = Ew rw(θ),
we can take the gradient of the log likelihood w.r.t θ
to show that the regularized MLE for the LM satis-
fies
</bodyText>
<equation confidence="0.9944392">
�
P˜ (h) P(w|h)Vθaw(h; θ)
w
�= P˜(w, h)Vθaw(h; θ) − � Vθrw(θ). (4)
h,w w
For each word w, we can define a binary classifier
that predicts whether the next word is w by
exp aw(h; θ)
Pb(w|h) = (5)
1 + exp aw(h; θ).
</equation>
<bodyText confidence="0.999207285714286">
The regularized training set log likelihood for all
the binary classifiers is given by
where Pb( ¯w|h) = 1 − Pb(w|h) is the probability of
w not occurring. Here we assume the same structure
of the regularizer r(θ).
The regularized MLE for the binary classifiers
satisfies
</bodyText>
<equation confidence="0.97765175">
� P˜(h) � Pb(w|h)Vθaw(h; θ)
h w
�= P˜ (w, h)Vθaw(h; θ) − � Vθrw(θ). (7)
h,w w
</equation>
<bodyText confidence="0.999730545454545">
Notice the right hand sides of (4) and (7) are the
same. Thus, taking P&apos;(w|h) = Pb(w|h) from ML
trained binary classifiers gives an LM that meets the
MLE constraints for language models. Therefore,
if Ew Pb(w|h) = 1, ML training for the language
model is equivalent to ML training of the binary
classifiers and using the probabilities given by the
classifiers as our LM probabilities.
Note that in practice, the probabilities given by
the binary classifiers are not guaranteed to sum up
to one. For tasks such as measuring perplexity,
</bodyText>
<equation confidence="0.979151888888889">
�
L =
h
�
h
�Lb = � �P˜ (h) P˜ (w|h) log Pb(w|h)
w h
+ P˜( ¯w |h) log Pb( ¯w |h)1 − rw(θ), (6)
J w
</equation>
<page confidence="0.975463">
1129
</page>
<bodyText confidence="0.966311666666667">
these probabilities have to be normalized explicitly.
Our hope is that for large enough data sets and rich
enough history representation aw(h; θ), we will get
</bodyText>
<equation confidence="0.9720295">
P
w Pb(w|h) Pz 1 so that renormalizing the classi-
fiers to get
Pb(w|h)
P&apos;(w|h) = (8)
Pw&apos;EV Pb(w&apos;|h)
</equation>
<bodyText confidence="0.898955">
will not change the MLE constraint too much.
</bodyText>
<subsectionHeader confidence="0.99688">
2.1 Stratified Sampling
</subsectionHeader>
<bodyText confidence="0.96714275862069">
We note that iterative estimation of the LM shown
in (2) in general requires enumerating over the T
training cases in the training set and computing the
denominator of (2) for each case at a cost of O(V ).
Thus, each iteration of training takes O(V T) in gen-
eral. The complexity of estimating each of the V
binary classifiers is O(T) per iteration, also giving
O(V T) per iteration in total.
However, as mentioned earlier, we are able to
maximally subsample negative examples for each
classifier. Thus the classifier for w is trained us-
ing the C(w) positive examples and a proportion
α of the T − C(w) negative examples. The total
number of training examples for all V classifiers is
then (1 − α)T + αV T. For large V , we choose
α &gt;&gt; 1
1+V so that this is approximately αV T .
Thus, our complexity for estimating all V classifiers
is O(αV T).
The resulting training set for each binary classi-
fier is a stratified sample (Neyman, 1934), and our
estimate needs to be calibrated to account for this.
Since the training set subsamples negative examples
by α, the resulting classifier will have a likelihood
ratio
= exp aw(h; θ) (9)
that is overestimated by a factor of 1α. This can be
corrected by simply adding log α to the bias (uni-
gram) weight of the classifier.
</bodyText>
<subsectionHeader confidence="0.998751">
2.2 Maximum Entropy LM
</subsectionHeader>
<bodyText confidence="0.999881833333333">
MELM is an effective alternative to the standard n-
gram LM. It provides a flexible framework to incor-
porate different knowledge sources in the form of
feature constraints. Specifically, MELM takes the
form of (2), for word w following history h, we have
the following probability definition,
</bodyText>
<equation confidence="0.887602">
exp Pi θifi(h, w)
P(w|h) = (10)
Pw&apos;EV exp Pi θifi(h, w&apos;) .
</equation>
<bodyText confidence="0.99984315">
fi is the ith feature function defined over the
word-history pair, θi is the feature weight associated
with fi. By defining general features, we have a nat-
ural framework to go beyond n-grams and capture
more complex dependencies that exist in language.
Previous research has shown the benefit of including
various kinds of syntactic and semantic information
into the LM (Khudanpur and Wu, 2000). However,
despite providing a promising avenue for language
modeling, MELM are computationally expensive to
estimate. The bottleneck lies in the denominator
of (10).
To estimate θis, gradient based methods can be
used. The derivative of the likelihood function L
w.r.t θi has a simple form, namely
(11)
where k is the index of word-history pair in the train-
ing corpus. The first term in the derivative is the ob-
served feature count in the training corpus, the sec-
ond term is the expected feature count according to
the model. In order to obtain P(w&apos;|h) in the second
term, we need to compute the normalizer, which in-
volves a very expensive summation over the entire
vocabulary. As described earlier, the complexity for
each iteration of training is at O(V T), where T is
the size of training corpus.
For feature sets that can be expressed hierarchi-
cally, for example n-gram feature set, where higher
order n-grams imply lower order n-grams, Wu and
Khudanpur (2000) exploit the structure of the nor-
malizer, and precompute components that can be
shared by different histories. For arbitrary feature
sets, however, it may not be possible to establish
the required hierarchical relations and the normal-
izer still needs to be computed explicitly. Good-
man (2001) changes the original LM into a class-
based LM, where each one of the two-step predic-
tions only involves a much smaller summation in the
normalizer. In addition, MELM estimation can be
parallelized, with expected count computation done
</bodyText>
<equation confidence="0.998110888888889">
X=
k
Xfi(wk, hk)− X
k w&apos;EV
P(w&apos;|h)fi(w&apos;, hk),
∂L
∂θi
Pb(w|h)
1 − Pb(w|h)
</equation>
<page confidence="0.774624">
1130
</page>
<bodyText confidence="0.999981">
separately for different parts of the training data and
merged together at the end of each iteration. For
models with massive parametrizations, this merge
step can be expensive due to communication costs.
Obviously, a different way to expedite MELM
training is to simply train on less data. We propose a
way to do this without incurring a significant loss of
modeling power, by reframing the problem in terms
of binary classification. As mentioned above, we
build V binary classifiers of the form in (5) to model
the distribution over the V words. The binary clas-
sifiers use the same features as the MELM of (10),
and are given by:
</bodyText>
<equation confidence="0.985614333333333">
exp Ei Oifi(h, w)
Pb(w|h) = (12)
1 + exp Ei Oi fi (h, w).
</equation>
<bodyText confidence="0.9999648">
We assume the features are partitioned over the vo-
cabulary, so that each feature fi has an associated
w such that fi(h, w&apos;) = 0 for all w&apos; =� w. There-
fore, the corresponding Oi affects only the binary
classifier for w. This gives an important advan-
tage in terms of parallelization–we have a set of bi-
nary classifiers with no feature sharing, and can be
trained separately on different machines. The par-
allelized computations are completely independent
and do not require the tedious communication be-
tween machines. Memory-wise, since the compu-
tations are independent, each word trainer only have
to store features that are associated with the word, so
the memory requirement for each individual worker
is significantly reduced.
</bodyText>
<subsectionHeader confidence="0.967082">
2.3 Neural Network LM
</subsectionHeader>
<bodyText confidence="0.999794818181818">
Neural Network Language Models (NNLM) have
gained a lot of interest since their introduction (Ben-
gio et al., 2003). While in standard language mod-
eling, words are treated as discrete symbols, NNLM
map them into a continuous space and learn their
representations automatically. It is often believed
that NNLM can generalize better to sequences that
are not seen in the training data. However, despite
having been shown to outperform standard n-gram
LM (Schwenk, 2007), NNLM are computationally
expensive to train.
</bodyText>
<figureCaption confidence="0.65434425">
Figure 1 shows the standard feed-forward NNLM
architecture. Starting from the left part of the figure,
each word of the n − 1 words history is mapped to a
Figure 1: Feed-forward NNLM
</figureCaption>
<bodyText confidence="0.8843784">
continuous vector and concatenated. Through a non-
linear hidden layer, the neural network constructs a
multinomial distribution at the output layer. Denot-
ing the concatenated d-dimensional word represen-
tations r, we have the following probability defini-
</bodyText>
<equation confidence="0.93987825">
tion:
eak
P(wi = k|wi−1, ..., wi−n+1) = Em eam , (13)
ak = bk +
</equation>
<bodyText confidence="0.9998488">
where h denotes the hidden layer size, b and c are
the bias vectors for the output nodes and hidden
nodes respectively. Note that NNLM also has the
form of (2).
Stochastic gradient descent is often used to max-
imize the training data likelihood under such a
model. The gradient can be computed using the
back-propagation method. To analyze the complex-
ity, computing an n-gram conditional probability re-
quires approximately
</bodyText>
<equation confidence="0.546734">
O((n − 1)dh + h + V h + V ) (15)
</equation>
<bodyText confidence="0.999912375">
operations, where V is the size of the vocabu-
lary. The four terms in the complexity correspond
to computing the hidden layer, applying the nonlin-
earity, computing the output layer and normaliza-
tion, respectively. The error propagation stage can
be analyzed similarly and takes about the same num-
ber of operations. For typical values as used in our
experiments, namely n = 3, d = 50, h = 200,
</bodyText>
<figure confidence="0.496307333333333">
h Wkl tanh(cl + (n−1)d Uljrj), (14)
l=1 �
j=1
</figure>
<page confidence="0.825967">
1131
</page>
<bodyText confidence="0.885070111111111">
V = 10000, the majority of the complexity per iter- ple for only a fraction α of the others. The rest of
ation comes from the term hV . For large scale tasks, the nodes are not computed and do not produce er-
it may be impractical to train an NNLM. ror signal for the hidden representation. We calibrate
A lot of previous research has focused on the classifiers after subsampled training as described
speeding up NNLM training. It usually aims above for MELM.
at removing the computational dependency on V . It is straightforward to show that the dominating
Schwenk (2007) used a short list of frequent words term V h in the complexity is reduced to αV h. We
such that a large number of out-of-list words are want to point out that compared with MELM, sub-
taken care of by a back-off LM. To reduce the sampling the negatives here does not always reduce
gradient computation introduced by the normal- the complexity proportionally. In cases where the
izer, Bengio and Senecal (2008) proposed a dif- vocabulary is very small, as shown in (15), com-
ferent kind of importance sampling technique. A puting the hidden layer can no longer be ignored.
recent work (Mikolov et al., 2011) applied Good- Nonetheless, real world applications such as speech
man’s class MELM trick (2001) to NNLM, in or- recognition, usually involves a vocabulary of consid-
der to avoid the gigantic normalization. A similar erable size, therefore, subsampling in the binary set-
technique has been introduced even earlier which ting can still achieve substantial speedup for NNLM.
took the idea of factorizing output layer to the ex- 3 Experimental Results
treme (Morin, 2005) by replacing the V-way predic- 3.1 MELM
tion by a tree-style hierarchical prediction. The au- We evaluate the proposed technique on two datasets
thors show a theoretical complexity reduction from of different sizes. Our first dataset is obtained
O(V ) to (log V ), but the technique requires a care- from Penn Treebank. Section 00-20 are used for
ful clustering which may not be easily attainable in training(972K tokens), section 21-22 are the val-
practice. idation set(77K), section 23-24(86K) are the test
Subsampling has also been proposed to acceler- set. The vocabulary size of the experiment is
ate NNLM training (Schwenk, 2007). The idea is to 10, 000. This is one of the standard setups on which
select random subsets of the training data in each many researchers have reported perplexity results on
epoch of stochastic gradient descent. After some (Mikolov et al., 2011).
epochs, it is very likely that all of the training exam- The binary MELM is trained using stochastic
ples have been seen by the model. We will show that gradient descent, no explicit regularization is per-
our binary classifier representation leads to a more formed (Zhang, 2004). The learning rate starts at 0.1
robust and promising subsampling strategy. and is halved every time the perplexity on the vali-
As with MELM, we notice that the parameters dation set stops decreasing. It usually takes around
of (14) can be interpreted as also defining a set of 20 iterations before no significant improvement can
V per-word binary classifiers be obtained on the validation set. The training stops
eak at that time.
Pb(wi = k|wi−1, ..., wi−n+1) = 1 + eak , (16) We compare perplexity with both the standard in-
but with a common hidden layer representation. As terpolated Kneser-Ney trigram model and the stan-
in MELM, we will train the classifiers, and renor- dard MELM. The MELM is L2 regularized and es-
malize them to obtain an NNLM over the V words. timated using a variant of generalized iterative scal-
In order to train the classifiers, we need to com- ing, the regularizer is tuned on the validation data.
pute all V output nodes and propagate the errors To demonstrate the effectiveness of our subsampling
back. Since the hidden layer is shared, the classifiers approach, we compare the subsampled versions of
are not independent, and the computations can not the binary MELM and the standard MELM. In order
be easily parallelized to multiple machines. How- to obtain valid perplexities, the binary LMs are first
ever, subsampling can be done differently for each renormalized explicitly according to equation (8) for
</bodyText>
<table confidence="0.918221444444444">
classifier. Each training instance serves as a positive each test history.
example for one classifier and as a negative exam-
1132
Model PPL
KN Trigram 153.0
Standard MELM, Feat-I 154.2
Binary MELM, Feat-I 153.7
Standard MELM, Feat-II 140.2
Binary MELM, Feat-II 141.1
</table>
<tableCaption confidence="0.999928">
Table 1: Binary MELM vs. Standard MELM
</tableCaption>
<bodyText confidence="0.99991597368421">
We consider two kinds of feature sets: Feat-I con-
tains only n-gram features, namely unigram, bigram
and trigram features, with no count cutoff, the total
number of features is 0.9M. Feat-II is augmented
with skip-1 bigrams and skip-1 trigrams (Goodman,
2001), as well as word trigger features as described
in (Rosenfeld, 1996). The total number of features
in this set is 1.9M. Note that the speedup trick de-
scribed in (Wu and Khudanpur, 2000) can be used
for feat-I, but not feat-II.
Table 1 shows the perplexity results when no sub-
sampling is performed. With only n-gram features,
the binary MELM is able to match both standard
MELM and the Kneser-Ney model. We can also see
that by adding features that are known to be able to
improve the standard MELM, we can get the same
improvement in the binary setting.
Figure 2 shows the comparisons of the two types
of MELM when the training data are subsampled.
The standard MELM with n-gram features suffers
drastically as we sample more aggressively. In con-
trast, the binary n-gram MELM(Feat-I) does not
appear to be hurt by aggressive subsampling, even
when 99% of the negative examples are discarded.
The robustness also holds for Feat-II where more
complicated features are added into the model. This
suggests a very efficient way of training MELM–
with only 1% of the computational cost, we are able
to train an LM as powerful as the standard MELM.
We further test our approach on a second dataset
which comes from Wall Street Journal corpus. It
contains 26M training tokens and a test set of 22K
tokens. We also have a held-out validation set to
tune parameters. This set of experiments is intended
to demonstrate that the binary subsampling tech-
nique is useful on a large text corpus where training
a standard MELM is not practical, and gives a better
LM than the commonly used Kneser-Ney baseline.
</bodyText>
<figureCaption confidence="0.8984235">
Figure 2: Subsampled Binary MELM vs. Subsampled
Standard MELM
</figureCaption>
<table confidence="0.999563571428571">
Model PPL
KN Trigram 117.7
Standard MELM, Trigram 116.5
Binary MELM, Feat-III, 10% 110.2
Binary MELM, Feat-III, 5% 110.8
Binary MELM, Feat-III, 2% 112.1
Binary MELM, Feat-III, 1% 112.4
</table>
<tableCaption confidence="0.999169">
Table 2: Binary Subsampled MELM on WSJ
</tableCaption>
<bodyText confidence="0.999621391304348">
The binary MELM is trained in the same way as
described in the previous experiment. Besides un-
igram, bigram and trigram features, we also added
skip-1 bigrams and skip-1 trigrams, this gives us
7.5M features in total. We call this set of features
feat-III. We were unable to train a standard MELM
with feat-III or a binary MELM without subsam-
pling because of the computational cost. However,
with our binary subsampling technique, as shown in
Table 2, we are able to benefit from skip n-gram fea-
tures with only 5% of the standard MELM complex-
ity. Also the performance does not degrade much as
we discard more negative examples.
To show that such improvement in perplexity
translates into gains in practical applications, we
conducted a set of speech recognition experiments.
The task is on Wall Street Journal, the LMs are
trained on 37M tokens and are used to rescore the n-
best list generated by the first pass recognizer with
a trigram LM. The details of the experimental setup
can be found in (Xu et al., 2009). Our baseline LM
is an interpolated Kneser-Ney 4-gram model.
Note that the size of the vocabulary for the task
</bodyText>
<page confidence="0.932875">
1133
</page>
<table confidence="0.9993004">
Model Dev WER Eval WER
KN 4-gram 11.8 17.2
Binary MELM, Feat-IV, 5% 11.0 16.7
Binary MELM, Feat-IV, 2% 11.2 16.7
Binary MELM, Feat-IV, 1% 11.2 16.7
</table>
<tableCaption confidence="0.984112">
Table 3: WSJ WER improvement. Binary MELM are
interpolated with KN 4-gram
</tableCaption>
<bodyText confidence="0.9998664">
is 20K, for the purpose of rescoring, we are only
interested in the words that exist in the n-best list,
therefore, for the binary MELM, we only have to
train about 5300 binary classifiers. For comparison,
the KN 4-gram also uses the same restricted vocabu-
lary. The features for the binary MELM are n-gram
features up to 4-grams plus skip-1 bigrams and skip-
1 trigrams. The total number of features is 10M. We
call this set of features Feat-IV.
Table 3 demonstrates the word error rate(WER)
improvement enabled by our binary subsampling
technique. Note that we can achieve 0.5% abso-
lute WER improvement on the test set at only 1%
of the standard MELM complexity. More specifi-
cally, with only 50 machines, such a reduction in
complexity allows us to train a binary MELM with
skip n-gram features in less than two hours, which is
not possible for the standard MELM on 37M words.
Obviously, with more machines, the estimation
can be even faster, it’s also reasonable to expect that
with more kinds of features, the improvement can
be even larger. We think that the proposed technique
opens the door for the utilization of the modeling
framework provided by MELM at a scale that has
not been possible before.
</bodyText>
<subsectionHeader confidence="0.995685">
3.2 NNLM
</subsectionHeader>
<bodyText confidence="0.999618666666667">
We evaluate our binary subsampling technique on
the same Penn Treebank corpus as described for the
MELM experiments. Taking random subsets of the
training data with the standard model is our primary
baseline to compare with. The NNLM we train is
a trigram LM with tanh hidden units. The size of
word representation and the size of hidden layer are
tuned minimally on the validation set(Hidden layer
size 200; Representation size 50). We adopt the
same learning rate strategy as for training MELM,
and the validation set is used to track perplexity per-
formance and adjust learning rate correspondingly.
</bodyText>
<table confidence="0.9989564">
Model PPL
100% 20% 10% 5%
Standard NNLM 154.3 239.8 297.0 360.3
Binary NNLM - 152.7 160.0 176.2
KN trigram 153.0 - - -
</table>
<tableCaption confidence="0.941493">
Table 4: Binary NNLM vs. Standard NNLM. Fixed ran-
dom subset.
</tableCaption>
<table confidence="0.9996482">
Model Interpolated PPL
100% 20% 10% 5%
Standard NNLM 132.7 145.6 148.6 150.7
Binary NNLM - 132.1 134.2 138.0
KN trigram 153.0 - - -
</table>
<tableCaption confidence="0.9939835">
Table 5: Binary NNLM vs. Standard NNLM. Fixed ran-
dom subset. Interpolated with KN trigram.
</tableCaption>
<bodyText confidence="0.995694548387097">
All parameters are initialized randomly with mean 0
and variance 0.01. As with binary MELM, binary
NNLM are explicitly renormalized to obtain valid
perplexities.
In our first experiment, we keep the subsampled
data fixed as we did for MELM. For the standard
NNLM, it means only a subset of the data is seen by
the model and it does not change through epochs;
For binary NNLM, it means the subset of negative
examples for each binary classifier does not change.
Table 4 shows the perplexity results by NNLM itself
and the interpolated results are shown in Table 5.
We can see that both models exhibit a tendency
to deteriorate as we subsample more aggressively.
However, the standard NNLM is clearly impacted
more severely. With binary NNLM, we are able to
retain all the gain after interpolation with only 20%
of the negative examples.
Notice that with a fixed random subset, we are not
replicating the experiments of Schwenk (Schwenk,
2007) exactly, although it is reasonable to expect
both models are able to benefit from seeing different
random subsets of the training data. This is verified
by results in Table 6 and Table 7.
The standard NNLM benefits quite a lot going
from using a fixed random subset to a variable ran-
dom subset, but still demonstrates a clear tendency
to deteriorate as we discard more and more data. On
the constrast, the binary NNLM maintains all the
performance gain with only 5% of the negative ex-
amples and still clearly outperforms its counterpart.
</bodyText>
<page confidence="0.975582">
1134
</page>
<table confidence="0.99917925">
Model PPL
100% 20% 10% 5%
Standard NNLM 154.3 157.7 172.2 186.5
Binary NNLM - 151.7 150.1 152.1
</table>
<tableCaption confidence="0.940436">
Table 6: Binary NNLM vs. Standard NNLM. Variable
random subset.
</tableCaption>
<table confidence="0.999776">
Model Interpolate PPL
100% 20% 10% 5%
Standard NNLM 132.7 133.9 138.1 141.2
Binary NNLM - 132.2 131.7 132.2
</table>
<tableCaption confidence="0.9977025">
Table 7: Binary NNLM vs. Standard NNLM. Variable
random subset. Interpolated with KN trigram.
</tableCaption>
<sectionHeader confidence="0.999424" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999997053571429">
For the standard models, the amount of existent pat-
terns fed into training heavily depends on the sub-
sampling rate α. For a small α, the models will in-
evitably lose some training patterns given any rea-
sonable number of epochs of training. Taking vari-
able random subsets in each epoch can alleviate this
problem to some extent, but still can not solve the
fundamental problem. In the binary setting, we are
able to do subsampling differently. While the com-
plexity remains the same without subsampling, the
majority of the complexity comes from processing
negatives examples for each binary classifier. There-
fore, we can achieve the same level of speedup as
standard subsampling by only subsampling negative
examples, and most importantly, it allows us to keep
all the existent patterns(positive examples) in the
training data. Of course, negative examples are im-
portant and even in the binary case, we benefit from
including more of them, but since we have so many
of them, they might not be as critical as positive ex-
amples in determining the distribution.
A similar conclusion can be drawn from Google’s
work on large LMs (Brants et al., 2007). Not having
to properly smooth the LM, they are still able to ben-
efit from large volumes of web text as training data.
It is probably more important to have a high n-gram
coverage than having a precise distribution.
The explanation here might lead us to wonder
whether for the multi-class problem, subsampling
the terms in the normalizer would achieve the same
results. More specifically, instead of summing over
all words in the vocabulary, we may choose to only
consider α of them. In fact, the short-list approach
in (Schwenk, 2007) and the adaptive importance
sampling in (Bengio and Senecal, 2008) have ex-
actly this intuition. However, in the multi-class
setup, subsampling like this has to be very careful.
We have to either have a good estimate of how much
probability mass we’ve thrown away, as in the short-
list approach, or have a good estimate of the entire
normalizer, as in the importance sampling approach.
It is very unlikely that an arbitrary random subsam-
pling will not harm the model. Fortunately, in the bi-
nary case, the effect of random subsampling is much
easier to analyze. We know exactly how much nega-
tive examples we’ve discarded, and they can be com-
pensated easily in the end.
It is worth pointing out that the proposed tech-
nique is not restricted to MELM and NNLM. We
have done experiments to binarize the class trick
sometimes used for language modeling (Goodman,
2001; Mikolov et al., 2011), and it also proves to
be useful. We plan to report these results in the fu-
ture. More generally, for many large-scale multi-
class problems, binarization and subsampling can be
an effective combination to consider.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999977142857143">
We propose efficient subsampling techniques for
training large multi-class classifiers such as maxi-
mum entropy language models and neural network
language models. The main idea is to replace a
multi-way decision by a set of binary decisions.
Since most of the training instances in the binary
setting are negatives examples, we can achieve sub-
stantial speedup by subsampling only the negatives.
We show by extensive experiments that this is more
robust than subsampling subsets of training data for
the original multi-class classifier. The proposed
method can be very useful for building large lan-
guage models and solving more general multi-class
problems.
</bodyText>
<sectionHeader confidence="0.999073" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.766315666666667">
This work is partially supported by National Science
Foundation Grant No¯ 0963898, the DARPA GALE
Program and JHU/HLTCOE.
</bodyText>
<page confidence="0.991278">
1135
</page>
<sectionHeader confidence="0.993823" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999790453125">
Allwein, Erin, Robert Schapire, Yoram Singer and Pack
Kaelbling. 2000. Reducing Multiclass to Binary: A
Unifying Approach for Margin Classifiers. Journal of
Machine Learning Research, 1:113-141.
Bengio, Yoshua, Rejean Ducharme and Pascal Vincent
2003. A neural probabilistic language model Journal
of Machine Learning research, 3:1137–1155.
Bengio, Yoshua and J. Senecal 2008. Adaptive impor-
tance sampling to accelerate training of a neural prob-
abilistic language model IEEE Transaction on Neural
Network, Apr. 2008.
Berger, Adam, Stephen A. Della Pietra and Vicent J.
Della Pietra 1996. A Maximum Entropy approach
to Natural Language Processing. Computational Lin-
guistics, 1996, 22:39-71.
Brants, Thorsten, Ashok C. Popat, Peng Xu, Frank J. Och
and Jeffrey Dean 2007. Large language models in ma-
chine translation. In Proceedings of 2007 Conference
on Empirical Methods in Natural Language Process-
ing, 858–867.
Goodman, Joshua 2001. Classes for Fast Maximum
Entropy Training. Proceedings of 2001 IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing.
Goodman, Joshua 2001. A bit of Progress in Language
Modeling. Computer Speech and Language, 403-434.
Khudanpur, Sanjeev and Jun Wu 2000. Maximum En-
tropy Techniques for Exploiting Syntactic, Semantic
and Collocational Dependencies in Language Model-
ing. Computer Speech and Language, 14(4):355-372.
Mikolov, Tomas, Stefan Kombrink, Lukas Burget, Jan
”Honza” Cernocky and Sanjeev Khudanpur 2011. Ex-
tensions of recurrent neural network language model.
Proceedings of 2011 IEEE International Conference
on Acoustics, Speech and Signal Processing.
Morin, Frederic 2005. Hierarchical probabilistic neural
network language model. AISTATS’05, pp. 246-252.
Neyman, Jerzy 1934. On the Two Different Aspects
of the Representative Method: The Method of Strati-
fied Sampling and the Method of Purposive Selection.
Journal of the Royal Statistical Society, 97(4):558-
625.
Rifkin, Ryan and Aldebaro Klautau 2004. In Defense of
One-Vs-All Classification. Journal of Machine Learn-
ing Research.
Rosenfeld, Roni. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech and Language, 10:187–228.
Schwenk, Holger 2007. Continuous space language
model. Computer Speech and Language, 21(3):492-
518.
Wu, Jun and Sanjeev Khudanpur. 2000. Efficient train-
ing methods for maximum entropy language model-
ing. Proceedings of the 6th International Conference
on Spoken Language Technologies, pp. 114–117.
Xu, Puyang, Damianos Karakos and Sanjeev Khudanpur.
2009. Self-supervised discriminative training of statis-
tical language models. Proceedings of 2009 IEEE Au-
tomatic Speech Recognition and Understanding Work-
shop.
Zhang, Tong 2004. Solving large scale linear prediction
problems using stochastic gradient descent algorithms.
Proceedings of 2004 International Conference on Ma-
chine Learnings.
</reference>
<page confidence="0.99434">
1136
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.790835">
<title confidence="0.999837">Efficient Subsampling for Training Complex Language Models</title>
<author confidence="0.99739">Xu Asela Khudanpur</author>
<email confidence="0.888435">puyangxu@jhu.eduaselag@microsoft.comkhudanpur@jhu.edu</email>
<affiliation confidence="0.978491333333333">of Electrical and Computer Engineering Research Center for Language and Speech Processing Redmond, WA 98052, Johns Hopkins University</affiliation>
<address confidence="0.999955">Baltimore, MD 21218, USA</address>
<abstract confidence="0.997203277777778">We propose an efficient way to train maximum entropy language models (MELM) and neural network language models (NNLM). The advantage of the proposed method comes from a more robust and efficient subsampling technique. The original multi-class language modeling problem is transformed into a set of binary problems where each binary classifier predicts whether or not a particular word will occur. We show that the binarized model is as powerful as the standard model and allows us to aggressively subsample negative training examples without sacrificing predictive performance. Empirical results show that we can MELM and NNLM at the standard complexity with no loss in performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Erin Allwein</author>
<author>Robert Schapire</author>
<author>Yoram Singer</author>
<author>Pack Kaelbling</author>
</authors>
<title>Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers.</title>
<date>2000</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>1--113</pages>
<contexts>
<context position="4486" citStr="Allwein et al., 2000" startWordPosition="711" endWordPosition="714"> only keeping subsets of them. We will show that the binarized LM is as powerful as its multi-class counterpart, while being able to sustain much more aggressive subsampling. For certain types of LMs such as MELM, there are more benefits–the binarization leads to a set of completely independent classifiers to train, which allows easy parallelization and significantly lowers the memory requirement. Similar one-against-all approaches are often used in the machine learning community, especially by SVM (support vector machine) practitioners to solve multi-class problems (Rifkin and Klautau, 2004; Allwein et al., 2000). The goal of this paper is to show that a similar technique can also be used for language modeling and that it enables us to subsample data much more efficiently. We show that the proposed approach is useful when the dominant modeling constraint is computing power as opposed to training data. The rest of the paper is organized as follows. In section 2, we describe our binarization and subsampling techniques for language models with MELM and NNLM as two specific examples. Experimental results are presented in Section 3, followed by discussion in Section 4. 2 Approximating Language Models with </context>
</contexts>
<marker>Allwein, Schapire, Singer, Kaelbling, 2000</marker>
<rawString>Allwein, Erin, Robert Schapire, Yoram Singer and Pack Kaelbling. 2000. Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers. Journal of Machine Learning Research, 1:113-141.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Rejean Ducharme and Pascal Vincent 2003. A neural probabilistic language model</title>
<journal>Journal of Machine Learning research,</journal>
<pages>3--1137</pages>
<marker>Bengio, </marker>
<rawString>Bengio, Yoshua, Rejean Ducharme and Pascal Vincent 2003. A neural probabilistic language model Journal of Machine Learning research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>J Senecal</author>
</authors>
<title>Adaptive importance sampling to accelerate training of a neural probabilistic language model</title>
<date>2008</date>
<journal>IEEE Transaction on Neural Network,</journal>
<contexts>
<context position="15190" citStr="Bengio and Senecal (2008)" startWordPosition="2595" endWordPosition="2598">ifiers after subsampled training as described speeding up NNLM training. It usually aims above for MELM. at removing the computational dependency on V . It is straightforward to show that the dominating Schwenk (2007) used a short list of frequent words term V h in the complexity is reduced to αV h. We such that a large number of out-of-list words are want to point out that compared with MELM, subtaken care of by a back-off LM. To reduce the sampling the negatives here does not always reduce gradient computation introduced by the normal- the complexity proportionally. In cases where the izer, Bengio and Senecal (2008) proposed a dif- vocabulary is very small, as shown in (15), comferent kind of importance sampling technique. A puting the hidden layer can no longer be ignored. recent work (Mikolov et al., 2011) applied Good- Nonetheless, real world applications such as speech man’s class MELM trick (2001) to NNLM, in or- recognition, usually involves a vocabulary of considder to avoid the gigantic normalization. A similar erable size, therefore, subsampling in the binary settechnique has been introduced even earlier which ting can still achieve substantial speedup for NNLM. took the idea of factorizing outp</context>
<context position="28068" citStr="Bengio and Senecal, 2008" startWordPosition="4777" endWordPosition="4780"> (Brants et al., 2007). Not having to properly smooth the LM, they are still able to benefit from large volumes of web text as training data. It is probably more important to have a high n-gram coverage than having a precise distribution. The explanation here might lead us to wonder whether for the multi-class problem, subsampling the terms in the normalizer would achieve the same results. More specifically, instead of summing over all words in the vocabulary, we may choose to only consider α of them. In fact, the short-list approach in (Schwenk, 2007) and the adaptive importance sampling in (Bengio and Senecal, 2008) have exactly this intuition. However, in the multi-class setup, subsampling like this has to be very careful. We have to either have a good estimate of how much probability mass we’ve thrown away, as in the shortlist approach, or have a good estimate of the entire normalizer, as in the importance sampling approach. It is very unlikely that an arbitrary random subsampling will not harm the model. Fortunately, in the binary case, the effect of random subsampling is much easier to analyze. We know exactly how much negative examples we’ve discarded, and they can be compensated easily in the end. </context>
</contexts>
<marker>Bengio, Senecal, 2008</marker>
<rawString>Bengio, Yoshua and J. Senecal 2008. Adaptive importance sampling to accelerate training of a neural probabilistic language model IEEE Transaction on Neural Network, Apr. 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vicent J Della Pietra</author>
</authors>
<title>A Maximum Entropy approach to Natural Language Processing. Computational Linguistics,</title>
<date>1996</date>
<pages>22--39</pages>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, Adam, Stephen A. Della Pietra and Vicent J. Della Pietra 1996. A Maximum Entropy approach to Natural Language Processing. Computational Linguistics, 1996, 22:39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Frank J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of 2007 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>858--867</pages>
<contexts>
<context position="27465" citStr="Brants et al., 2007" startWordPosition="4676" endWordPosition="4679">plexity comes from processing negatives examples for each binary classifier. Therefore, we can achieve the same level of speedup as standard subsampling by only subsampling negative examples, and most importantly, it allows us to keep all the existent patterns(positive examples) in the training data. Of course, negative examples are important and even in the binary case, we benefit from including more of them, but since we have so many of them, they might not be as critical as positive examples in determining the distribution. A similar conclusion can be drawn from Google’s work on large LMs (Brants et al., 2007). Not having to properly smooth the LM, they are still able to benefit from large volumes of web text as training data. It is probably more important to have a high n-gram coverage than having a precise distribution. The explanation here might lead us to wonder whether for the multi-class problem, subsampling the terms in the normalizer would achieve the same results. More specifically, instead of summing over all words in the vocabulary, we may choose to only consider α of them. In fact, the short-list approach in (Schwenk, 2007) and the adaptive importance sampling in (Bengio and Senecal, 20</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Brants, Thorsten, Ashok C. Popat, Peng Xu, Frank J. Och and Jeffrey Dean 2007. Large language models in machine translation. In Proceedings of 2007 Conference on Empirical Methods in Natural Language Processing, 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Classes for Fast Maximum Entropy Training.</title>
<date>2001</date>
<booktitle>Proceedings of 2001 IEEE International Conference on Acoustics, Speech and Signal Processing.</booktitle>
<contexts>
<context position="10549" citStr="Goodman (2001)" startWordPosition="1792" endWordPosition="1794">mmation over the entire vocabulary. As described earlier, the complexity for each iteration of training is at O(V T), where T is the size of training corpus. For feature sets that can be expressed hierarchically, for example n-gram feature set, where higher order n-grams imply lower order n-grams, Wu and Khudanpur (2000) exploit the structure of the normalizer, and precompute components that can be shared by different histories. For arbitrary feature sets, however, it may not be possible to establish the required hierarchical relations and the normalizer still needs to be computed explicitly. Goodman (2001) changes the original LM into a classbased LM, where each one of the two-step predictions only involves a much smaller summation in the normalizer. In addition, MELM estimation can be parallelized, with expected count computation done X= k Xfi(wk, hk)− X k w&apos;EV P(w&apos;|h)fi(w&apos;, hk), ∂L ∂θi Pb(w|h) 1 − Pb(w|h) 1130 separately for different parts of the training data and merged together at the end of each iteration. For models with massive parametrizations, this merge step can be expensive due to communication costs. Obviously, a different way to expedite MELM training is to simply train on less da</context>
<context position="19011" citStr="Goodman, 2001" startWordPosition="3225" endWordPosition="3226">ormalized explicitly according to equation (8) for classifier. Each training instance serves as a positive each test history. example for one classifier and as a negative exam1132 Model PPL KN Trigram 153.0 Standard MELM, Feat-I 154.2 Binary MELM, Feat-I 153.7 Standard MELM, Feat-II 140.2 Binary MELM, Feat-II 141.1 Table 1: Binary MELM vs. Standard MELM We consider two kinds of feature sets: Feat-I contains only n-gram features, namely unigram, bigram and trigram features, with no count cutoff, the total number of features is 0.9M. Feat-II is augmented with skip-1 bigrams and skip-1 trigrams (Goodman, 2001), as well as word trigger features as described in (Rosenfeld, 1996). The total number of features in this set is 1.9M. Note that the speedup trick described in (Wu and Khudanpur, 2000) can be used for feat-I, but not feat-II. Table 1 shows the perplexity results when no subsampling is performed. With only n-gram features, the binary MELM is able to match both standard MELM and the Kneser-Ney model. We can also see that by adding features that are known to be able to improve the standard MELM, we can get the same improvement in the binary setting. Figure 2 shows the comparisons of the two type</context>
<context position="28861" citStr="Goodman, 2001" startWordPosition="4918" endWordPosition="4919">e’ve thrown away, as in the shortlist approach, or have a good estimate of the entire normalizer, as in the importance sampling approach. It is very unlikely that an arbitrary random subsampling will not harm the model. Fortunately, in the binary case, the effect of random subsampling is much easier to analyze. We know exactly how much negative examples we’ve discarded, and they can be compensated easily in the end. It is worth pointing out that the proposed technique is not restricted to MELM and NNLM. We have done experiments to binarize the class trick sometimes used for language modeling (Goodman, 2001; Mikolov et al., 2011), and it also proves to be useful. We plan to report these results in the future. More generally, for many large-scale multiclass problems, binarization and subsampling can be an effective combination to consider. 5 Conclusion We propose efficient subsampling techniques for training large multi-class classifiers such as maximum entropy language models and neural network language models. The main idea is to replace a multi-way decision by a set of binary decisions. Since most of the training instances in the binary setting are negatives examples, we can achieve substantia</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Goodman, Joshua 2001. Classes for Fast Maximum Entropy Training. Proceedings of 2001 IEEE International Conference on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>A bit of</title>
<date>2001</date>
<booktitle>Progress in Language Modeling. Computer Speech and Language,</booktitle>
<pages>403--434</pages>
<contexts>
<context position="10549" citStr="Goodman (2001)" startWordPosition="1792" endWordPosition="1794">mmation over the entire vocabulary. As described earlier, the complexity for each iteration of training is at O(V T), where T is the size of training corpus. For feature sets that can be expressed hierarchically, for example n-gram feature set, where higher order n-grams imply lower order n-grams, Wu and Khudanpur (2000) exploit the structure of the normalizer, and precompute components that can be shared by different histories. For arbitrary feature sets, however, it may not be possible to establish the required hierarchical relations and the normalizer still needs to be computed explicitly. Goodman (2001) changes the original LM into a classbased LM, where each one of the two-step predictions only involves a much smaller summation in the normalizer. In addition, MELM estimation can be parallelized, with expected count computation done X= k Xfi(wk, hk)− X k w&apos;EV P(w&apos;|h)fi(w&apos;, hk), ∂L ∂θi Pb(w|h) 1 − Pb(w|h) 1130 separately for different parts of the training data and merged together at the end of each iteration. For models with massive parametrizations, this merge step can be expensive due to communication costs. Obviously, a different way to expedite MELM training is to simply train on less da</context>
<context position="19011" citStr="Goodman, 2001" startWordPosition="3225" endWordPosition="3226">ormalized explicitly according to equation (8) for classifier. Each training instance serves as a positive each test history. example for one classifier and as a negative exam1132 Model PPL KN Trigram 153.0 Standard MELM, Feat-I 154.2 Binary MELM, Feat-I 153.7 Standard MELM, Feat-II 140.2 Binary MELM, Feat-II 141.1 Table 1: Binary MELM vs. Standard MELM We consider two kinds of feature sets: Feat-I contains only n-gram features, namely unigram, bigram and trigram features, with no count cutoff, the total number of features is 0.9M. Feat-II is augmented with skip-1 bigrams and skip-1 trigrams (Goodman, 2001), as well as word trigger features as described in (Rosenfeld, 1996). The total number of features in this set is 1.9M. Note that the speedup trick described in (Wu and Khudanpur, 2000) can be used for feat-I, but not feat-II. Table 1 shows the perplexity results when no subsampling is performed. With only n-gram features, the binary MELM is able to match both standard MELM and the Kneser-Ney model. We can also see that by adding features that are known to be able to improve the standard MELM, we can get the same improvement in the binary setting. Figure 2 shows the comparisons of the two type</context>
<context position="28861" citStr="Goodman, 2001" startWordPosition="4918" endWordPosition="4919">e’ve thrown away, as in the shortlist approach, or have a good estimate of the entire normalizer, as in the importance sampling approach. It is very unlikely that an arbitrary random subsampling will not harm the model. Fortunately, in the binary case, the effect of random subsampling is much easier to analyze. We know exactly how much negative examples we’ve discarded, and they can be compensated easily in the end. It is worth pointing out that the proposed technique is not restricted to MELM and NNLM. We have done experiments to binarize the class trick sometimes used for language modeling (Goodman, 2001; Mikolov et al., 2011), and it also proves to be useful. We plan to report these results in the future. More generally, for many large-scale multiclass problems, binarization and subsampling can be an effective combination to consider. 5 Conclusion We propose efficient subsampling techniques for training large multi-class classifiers such as maximum entropy language models and neural network language models. The main idea is to replace a multi-way decision by a set of binary decisions. Since most of the training instances in the binary setting are negatives examples, we can achieve substantia</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Goodman, Joshua 2001. A bit of Progress in Language Modeling. Computer Speech and Language, 403-434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjeev Khudanpur</author>
<author>Jun Wu</author>
</authors>
<title>Maximum Entropy Techniques for Exploiting Syntactic, Semantic and Collocational Dependencies in Language Modeling. Computer Speech and Language,</title>
<date>2000</date>
<pages>14--4</pages>
<contexts>
<context position="9292" citStr="Khudanpur and Wu, 2000" startWordPosition="1582" endWordPosition="1585">ledge sources in the form of feature constraints. Specifically, MELM takes the form of (2), for word w following history h, we have the following probability definition, exp Pi θifi(h, w) P(w|h) = (10) Pw&apos;EV exp Pi θifi(h, w&apos;) . fi is the ith feature function defined over the word-history pair, θi is the feature weight associated with fi. By defining general features, we have a natural framework to go beyond n-grams and capture more complex dependencies that exist in language. Previous research has shown the benefit of including various kinds of syntactic and semantic information into the LM (Khudanpur and Wu, 2000). However, despite providing a promising avenue for language modeling, MELM are computationally expensive to estimate. The bottleneck lies in the denominator of (10). To estimate θis, gradient based methods can be used. The derivative of the likelihood function L w.r.t θi has a simple form, namely (11) where k is the index of word-history pair in the training corpus. The first term in the derivative is the observed feature count in the training corpus, the second term is the expected feature count according to the model. In order to obtain P(w&apos;|h) in the second term, we need to compute the nor</context>
</contexts>
<marker>Khudanpur, Wu, 2000</marker>
<rawString>Khudanpur, Sanjeev and Jun Wu 2000. Maximum Entropy Techniques for Exploiting Syntactic, Semantic and Collocational Dependencies in Language Modeling. Computer Speech and Language, 14(4):355-372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Lukas Burget</author>
</authors>
<title>Honza” Cernocky and Sanjeev Khudanpur 2011. Extensions of recurrent neural network language model.</title>
<date></date>
<booktitle>Proceedings of 2011 IEEE International Conference on Acoustics, Speech and Signal Processing.</booktitle>
<marker>Mikolov, Kombrink, Burget, </marker>
<rawString>Mikolov, Tomas, Stefan Kombrink, Lukas Burget, Jan ”Honza” Cernocky and Sanjeev Khudanpur 2011. Extensions of recurrent neural network language model. Proceedings of 2011 IEEE International Conference on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>AISTATS’05,</booktitle>
<pages>246--252</pages>
<contexts>
<context position="15852" citStr="Morin, 2005" startWordPosition="2703" endWordPosition="2704">n in (15), comferent kind of importance sampling technique. A puting the hidden layer can no longer be ignored. recent work (Mikolov et al., 2011) applied Good- Nonetheless, real world applications such as speech man’s class MELM trick (2001) to NNLM, in or- recognition, usually involves a vocabulary of considder to avoid the gigantic normalization. A similar erable size, therefore, subsampling in the binary settechnique has been introduced even earlier which ting can still achieve substantial speedup for NNLM. took the idea of factorizing output layer to the ex- 3 Experimental Results treme (Morin, 2005) by replacing the V-way predic- 3.1 MELM tion by a tree-style hierarchical prediction. The au- We evaluate the proposed technique on two datasets thors show a theoretical complexity reduction from of different sizes. Our first dataset is obtained O(V ) to (log V ), but the technique requires a care- from Penn Treebank. Section 00-20 are used for ful clustering which may not be easily attainable in training(972K tokens), section 21-22 are the valpractice. idation set(77K), section 23-24(86K) are the test Subsampling has also been proposed to acceler- set. The vocabulary size of the experiment i</context>
</contexts>
<marker>Morin, 2005</marker>
<rawString>Morin, Frederic 2005. Hierarchical probabilistic neural network language model. AISTATS’05, pp. 246-252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerzy Neyman</author>
</authors>
<title>On the Two Different Aspects of the Representative Method: The Method of Stratified Sampling and the Method of Purposive Selection.</title>
<date>1934</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>97--4</pages>
<contexts>
<context position="8197" citStr="Neyman, 1934" startWordPosition="1397" endWordPosition="1398">ers is O(T) per iteration, also giving O(V T) per iteration in total. However, as mentioned earlier, we are able to maximally subsample negative examples for each classifier. Thus the classifier for w is trained using the C(w) positive examples and a proportion α of the T − C(w) negative examples. The total number of training examples for all V classifiers is then (1 − α)T + αV T. For large V , we choose α &gt;&gt; 1 1+V so that this is approximately αV T . Thus, our complexity for estimating all V classifiers is O(αV T). The resulting training set for each binary classifier is a stratified sample (Neyman, 1934), and our estimate needs to be calibrated to account for this. Since the training set subsamples negative examples by α, the resulting classifier will have a likelihood ratio = exp aw(h; θ) (9) that is overestimated by a factor of 1α. This can be corrected by simply adding log α to the bias (unigram) weight of the classifier. 2.2 Maximum Entropy LM MELM is an effective alternative to the standard ngram LM. It provides a flexible framework to incorporate different knowledge sources in the form of feature constraints. Specifically, MELM takes the form of (2), for word w following history h, we h</context>
</contexts>
<marker>Neyman, 1934</marker>
<rawString>Neyman, Jerzy 1934. On the Two Different Aspects of the Representative Method: The Method of Stratified Sampling and the Method of Purposive Selection. Journal of the Royal Statistical Society, 97(4):558-625.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ryan Rifkin</author>
</authors>
<title>and Aldebaro Klautau 2004. In Defense of One-Vs-All Classification.</title>
<journal>Journal of Machine Learning Research.</journal>
<marker>Rifkin, </marker>
<rawString>Rifkin, Ryan and Aldebaro Klautau 2004. In Defense of One-Vs-All Classification. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roni Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling. Computer Speech and Language,</title>
<date>1996</date>
<pages>10--187</pages>
<contexts>
<context position="1708" citStr="Rosenfeld, 1996" startWordPosition="266" endWordPosition="268">assign probabilities to sequences of words. They are widely used in many natural language processing applications. The probability of a sequence can be modeled as a product of local probabilities, as shown in (1), where wi is the ith word, and hi is the word history preceding wi. l P(w1, w2, ..., wl) = P(wi|hi) (1) i=1 Therefore the task of language modeling reduces to estimating a set of conditional distributions {P(w|h)}. The n-gram LM is a dominant way to parametrize P(w|h), where it is assumed that w only depends on the previous n—1 words. More complex models have also been proposed–MELM (Rosenfeld, 1996) and NNLM (Bengio et al., 2003) are two examples. Modeling P(w|h) can be seen as a multi-class classification problem. Given the history, we have to choose a word in the vocabulary, which can easily be a few hundred thousand words in size. For complex models such as MELM and NNLM, this poses a computational challenge for learning, because the resulting objective functions are expensive to normalize. In contrast, n-gram LMs do not suffer from this computational challenge. In the web era, language modelers have access to virtually unlimited amounts of data, while the computing power available to</context>
<context position="19079" citStr="Rosenfeld, 1996" startWordPosition="3236" endWordPosition="3237">h training instance serves as a positive each test history. example for one classifier and as a negative exam1132 Model PPL KN Trigram 153.0 Standard MELM, Feat-I 154.2 Binary MELM, Feat-I 153.7 Standard MELM, Feat-II 140.2 Binary MELM, Feat-II 141.1 Table 1: Binary MELM vs. Standard MELM We consider two kinds of feature sets: Feat-I contains only n-gram features, namely unigram, bigram and trigram features, with no count cutoff, the total number of features is 0.9M. Feat-II is augmented with skip-1 bigrams and skip-1 trigrams (Goodman, 2001), as well as word trigger features as described in (Rosenfeld, 1996). The total number of features in this set is 1.9M. Note that the speedup trick described in (Wu and Khudanpur, 2000) can be used for feat-I, but not feat-II. Table 1 shows the perplexity results when no subsampling is performed. With only n-gram features, the binary MELM is able to match both standard MELM and the Kneser-Ney model. We can also see that by adding features that are known to be able to improve the standard MELM, we can get the same improvement in the binary setting. Figure 2 shows the comparisons of the two types of MELM when the training data are subsampled. The standard MELM w</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Rosenfeld, Roni. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer Speech and Language, 10:187–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language model.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<pages>21--3</pages>
<contexts>
<context position="2847" citStr="Schwenk, 2007" startWordPosition="456" endWordPosition="457">virtually unlimited amounts of data, while the computing power available to process this data is limited. Therefore, despite the demonstrated effectiveness of complex LMs, the n-gram is still the predominant approach for most real world applications. Subsampling is a simple solution to get around the constraint of computing resources. For the purpose of language modeling, it amounts to taking only part of the text corpus to train the LM. For complex models such as NNLM, it has been shown that subsampling can speed up training greatly, at the cost of some degradation in predictive performance (Schwenk, 2007), allowing for trade-off between computational cost and LM quality. Our contribution is a novel way to train complex LMs such as MELM and NNLM which allows much more aggressive subsampling without incurring as high a cost in predictive performance. The key to our approach is reducing the multi-class LM problem into a set of binary problems. Instead of training a V-class classifier, where V is the size of 1128 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1128–1136, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Li</context>
<context position="12768" citStr="Schwenk, 2007" startWordPosition="2169" endWordPosition="2170">re features that are associated with the word, so the memory requirement for each individual worker is significantly reduced. 2.3 Neural Network LM Neural Network Language Models (NNLM) have gained a lot of interest since their introduction (Bengio et al., 2003). While in standard language modeling, words are treated as discrete symbols, NNLM map them into a continuous space and learn their representations automatically. It is often believed that NNLM can generalize better to sequences that are not seen in the training data. However, despite having been shown to outperform standard n-gram LM (Schwenk, 2007), NNLM are computationally expensive to train. Figure 1 shows the standard feed-forward NNLM architecture. Starting from the left part of the figure, each word of the n − 1 words history is mapped to a Figure 1: Feed-forward NNLM continuous vector and concatenated. Through a nonlinear hidden layer, the neural network constructs a multinomial distribution at the output layer. Denoting the concatenated d-dimensional word representations r, we have the following probability definition: eak P(wi = k|wi−1, ..., wi−n+1) = Em eam , (13) ak = bk + where h denotes the hidden layer size, b and c are the</context>
<context position="14782" citStr="Schwenk (2007)" startWordPosition="2524" endWordPosition="2525">anh(cl + (n−1)d Uljrj), (14) l=1 � j=1 1131 V = 10000, the majority of the complexity per iter- ple for only a fraction α of the others. The rest of ation comes from the term hV . For large scale tasks, the nodes are not computed and do not produce erit may be impractical to train an NNLM. ror signal for the hidden representation. We calibrate A lot of previous research has focused on the classifiers after subsampled training as described speeding up NNLM training. It usually aims above for MELM. at removing the computational dependency on V . It is straightforward to show that the dominating Schwenk (2007) used a short list of frequent words term V h in the complexity is reduced to αV h. We such that a large number of out-of-list words are want to point out that compared with MELM, subtaken care of by a back-off LM. To reduce the sampling the negatives here does not always reduce gradient computation introduced by the normal- the complexity proportionally. In cases where the izer, Bengio and Senecal (2008) proposed a dif- vocabulary is very small, as shown in (15), comferent kind of importance sampling technique. A puting the hidden layer can no longer be ignored. recent work (Mikolov et al., 2</context>
<context position="16487" citStr="Schwenk, 2007" startWordPosition="2805" endWordPosition="2806">ay predic- 3.1 MELM tion by a tree-style hierarchical prediction. The au- We evaluate the proposed technique on two datasets thors show a theoretical complexity reduction from of different sizes. Our first dataset is obtained O(V ) to (log V ), but the technique requires a care- from Penn Treebank. Section 00-20 are used for ful clustering which may not be easily attainable in training(972K tokens), section 21-22 are the valpractice. idation set(77K), section 23-24(86K) are the test Subsampling has also been proposed to acceler- set. The vocabulary size of the experiment is ate NNLM training (Schwenk, 2007). The idea is to 10, 000. This is one of the standard setups on which select random subsets of the training data in each many researchers have reported perplexity results on epoch of stochastic gradient descent. After some (Mikolov et al., 2011). epochs, it is very likely that all of the training exam- The binary MELM is trained using stochastic ples have been seen by the model. We will show that gradient descent, no explicit regularization is perour binary classifier representation leads to a more formed (Zhang, 2004). The learning rate starts at 0.1 robust and promising subsampling strategy.</context>
<context position="25402" citStr="Schwenk, 2007" startWordPosition="4328" endWordPosition="4329">does not change through epochs; For binary NNLM, it means the subset of negative examples for each binary classifier does not change. Table 4 shows the perplexity results by NNLM itself and the interpolated results are shown in Table 5. We can see that both models exhibit a tendency to deteriorate as we subsample more aggressively. However, the standard NNLM is clearly impacted more severely. With binary NNLM, we are able to retain all the gain after interpolation with only 20% of the negative examples. Notice that with a fixed random subset, we are not replicating the experiments of Schwenk (Schwenk, 2007) exactly, although it is reasonable to expect both models are able to benefit from seeing different random subsets of the training data. This is verified by results in Table 6 and Table 7. The standard NNLM benefits quite a lot going from using a fixed random subset to a variable random subset, but still demonstrates a clear tendency to deteriorate as we discard more and more data. On the constrast, the binary NNLM maintains all the performance gain with only 5% of the negative examples and still clearly outperforms its counterpart. 1134 Model PPL 100% 20% 10% 5% Standard NNLM 154.3 157.7 172.</context>
<context position="28001" citStr="Schwenk, 2007" startWordPosition="4769" endWordPosition="4770"> conclusion can be drawn from Google’s work on large LMs (Brants et al., 2007). Not having to properly smooth the LM, they are still able to benefit from large volumes of web text as training data. It is probably more important to have a high n-gram coverage than having a precise distribution. The explanation here might lead us to wonder whether for the multi-class problem, subsampling the terms in the normalizer would achieve the same results. More specifically, instead of summing over all words in the vocabulary, we may choose to only consider α of them. In fact, the short-list approach in (Schwenk, 2007) and the adaptive importance sampling in (Bengio and Senecal, 2008) have exactly this intuition. However, in the multi-class setup, subsampling like this has to be very careful. We have to either have a good estimate of how much probability mass we’ve thrown away, as in the shortlist approach, or have a good estimate of the entire normalizer, as in the importance sampling approach. It is very unlikely that an arbitrary random subsampling will not harm the model. Fortunately, in the binary case, the effect of random subsampling is much easier to analyze. We know exactly how much negative exampl</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Schwenk, Holger 2007. Continuous space language model. Computer Speech and Language, 21(3):492-518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Wu</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Efficient training methods for maximum entropy language modeling.</title>
<date>2000</date>
<booktitle>Proceedings of the 6th International Conference on Spoken Language Technologies,</booktitle>
<pages>114--117</pages>
<contexts>
<context position="10257" citStr="Wu and Khudanpur (2000)" startWordPosition="1745" endWordPosition="1748">air in the training corpus. The first term in the derivative is the observed feature count in the training corpus, the second term is the expected feature count according to the model. In order to obtain P(w&apos;|h) in the second term, we need to compute the normalizer, which involves a very expensive summation over the entire vocabulary. As described earlier, the complexity for each iteration of training is at O(V T), where T is the size of training corpus. For feature sets that can be expressed hierarchically, for example n-gram feature set, where higher order n-grams imply lower order n-grams, Wu and Khudanpur (2000) exploit the structure of the normalizer, and precompute components that can be shared by different histories. For arbitrary feature sets, however, it may not be possible to establish the required hierarchical relations and the normalizer still needs to be computed explicitly. Goodman (2001) changes the original LM into a classbased LM, where each one of the two-step predictions only involves a much smaller summation in the normalizer. In addition, MELM estimation can be parallelized, with expected count computation done X= k Xfi(wk, hk)− X k w&apos;EV P(w&apos;|h)fi(w&apos;, hk), ∂L ∂θi Pb(w|h) 1 − Pb(w|h) </context>
<context position="19196" citStr="Wu and Khudanpur, 2000" startWordPosition="3256" endWordPosition="3259">132 Model PPL KN Trigram 153.0 Standard MELM, Feat-I 154.2 Binary MELM, Feat-I 153.7 Standard MELM, Feat-II 140.2 Binary MELM, Feat-II 141.1 Table 1: Binary MELM vs. Standard MELM We consider two kinds of feature sets: Feat-I contains only n-gram features, namely unigram, bigram and trigram features, with no count cutoff, the total number of features is 0.9M. Feat-II is augmented with skip-1 bigrams and skip-1 trigrams (Goodman, 2001), as well as word trigger features as described in (Rosenfeld, 1996). The total number of features in this set is 1.9M. Note that the speedup trick described in (Wu and Khudanpur, 2000) can be used for feat-I, but not feat-II. Table 1 shows the perplexity results when no subsampling is performed. With only n-gram features, the binary MELM is able to match both standard MELM and the Kneser-Ney model. We can also see that by adding features that are known to be able to improve the standard MELM, we can get the same improvement in the binary setting. Figure 2 shows the comparisons of the two types of MELM when the training data are subsampled. The standard MELM with n-gram features suffers drastically as we sample more aggressively. In contrast, the binary n-gram MELM(Feat-I) d</context>
</contexts>
<marker>Wu, Khudanpur, 2000</marker>
<rawString>Wu, Jun and Sanjeev Khudanpur. 2000. Efficient training methods for maximum entropy language modeling. Proceedings of the 6th International Conference on Spoken Language Technologies, pp. 114–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Puyang Xu</author>
</authors>
<title>Damianos Karakos and Sanjeev Khudanpur.</title>
<date>2009</date>
<booktitle>Proceedings of 2009 IEEE Automatic Speech Recognition and Understanding Workshop.</booktitle>
<marker>Xu, 2009</marker>
<rawString>Xu, Puyang, Damianos Karakos and Sanjeev Khudanpur. 2009. Self-supervised discriminative training of statistical language models. Proceedings of 2009 IEEE Automatic Speech Recognition and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
</authors>
<title>Solving large scale linear prediction problems using stochastic gradient descent algorithms.</title>
<date>2004</date>
<booktitle>Proceedings of 2004 International Conference on Machine Learnings.</booktitle>
<contexts>
<context position="17011" citStr="Zhang, 2004" startWordPosition="2894" endWordPosition="2895">acceler- set. The vocabulary size of the experiment is ate NNLM training (Schwenk, 2007). The idea is to 10, 000. This is one of the standard setups on which select random subsets of the training data in each many researchers have reported perplexity results on epoch of stochastic gradient descent. After some (Mikolov et al., 2011). epochs, it is very likely that all of the training exam- The binary MELM is trained using stochastic ples have been seen by the model. We will show that gradient descent, no explicit regularization is perour binary classifier representation leads to a more formed (Zhang, 2004). The learning rate starts at 0.1 robust and promising subsampling strategy. and is halved every time the perplexity on the valiAs with MELM, we notice that the parameters dation set stops decreasing. It usually takes around of (14) can be interpreted as also defining a set of 20 iterations before no significant improvement can V per-word binary classifiers be obtained on the validation set. The training stops eak at that time. Pb(wi = k|wi−1, ..., wi−n+1) = 1 + eak , (16) We compare perplexity with both the standard inbut with a common hidden layer representation. As terpolated Kneser-Ney tri</context>
</contexts>
<marker>Zhang, 2004</marker>
<rawString>Zhang, Tong 2004. Solving large scale linear prediction problems using stochastic gradient descent algorithms. Proceedings of 2004 International Conference on Machine Learnings.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>