<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005255">
<title confidence="0.9984805">
Generating with a Grammar Based on Tree Descriptions: a
Constraint-Based Approach
</title>
<author confidence="0.805198">
Claire Gardent
</author>
<affiliation confidence="0.34902">
CNRS
</affiliation>
<address confidence="0.600691">
LORIA, BP 239 Campus Scientifique
54506 Vandoeuvre-les-Nancy, France
</address>
<email confidence="0.997555">
claire.gardent@loria.fr
</email>
<author confidence="0.937773">
Stefan Thater
</author>
<affiliation confidence="0.865481">
Computational Linguistics
Universit¨at des Saarlandes
</affiliation>
<address confidence="0.533246">
Saarbr¨ucken, Germany
</address>
<email confidence="0.992701">
stth@coli.uni-sb.de
</email>
<sectionHeader confidence="0.993722" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952615384615">
While the generative view of language
processing builds bigger units out of
smaller ones by means of rewriting
steps, the axiomatic view eliminates in-
valid linguistic structures out of a set of
possible structures by means of well-
formedness principles. We present a
generator based on the axiomatic view
and argue that when combined with a
TAG-like grammar and a flat seman-
tics, this axiomatic view permits avoid-
ing drawbacks known to hold either of
top-down or of bottom-up generators.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999717225352113">
We take the axiomatic view of language and show
that it yields an interestingly new perspective on
the tactical generation task i.e. the task of produc-
ing from a given semantics a string with seman-
tics .
As (Cornell and Rogers, To appear) clearly
shows, there has recently been a surge of interest
in logic based grammars for natural language. In
this branch of research sometimes referred to as
“Model Theoretic Syntax”, a grammar is viewed
as a set of axioms defining the well-formed struc-
tures of natural language.
The motivation for model theoretic grammars
is initially theoretical: the use of logic should sup-
port both a more precise formulation of grammars
and a different perspective on the mathematical
and computational properties of natural language.
But eventually the question must also be ad-
dressed of how such grammars could be put to
work. One obvious answer is to use a model gen-
erator. Given a logical formula , a model genera-
tor is a program which builds some of the models
satisfying this formula. Thus for parsing, a model
generator can be used to enumerate the (minimal)
model(s), that is, the parse trees, satisfying the
conjunction of the lexical categories selected on
the basis of the input string plus any additional
constraints which might be encoded in the gram-
mar. And similarly for generation, a model gener-
ator can be used to enumerate the models satisfy-
ing the bag of lexical items selected by the lexical
look up phase on the basis of the input semantics.
How can we design model generators which
work efficiently on natural language input i.e. on
the type of information delivered by logic based
grammars? (Duchier and Gardent, 1999) shows
that constraint programming can be used to im-
plement a model generator for tree logic (Back-
ofen et al., 1995). Further, (Duchier and Thater,
1999) shows that this model generator can be used
to parse with descriptions based grammars (Ram-
bow et al., 1995; Kallmeyer, 1999) that is, on
logic based grammars where lexical entries are
descriptions of trees expressed in some tree logic.
In this paper, we build on (Duchier and Thater,
1999) and show that modulo some minor modi-
fications, the same model generator can be used
to generate with description based grammars.
We describe the workings of the algorithm and
compare it with standard existing top-down and
bottom-up generation algorithms. In specific, we
argue that the change of perspective offered by
the constraint-based, axiomatic approach to pro-
cessing presents some interesting differences with
the more traditional generative approach usually
pursued in tactical generation and further, that the
combination of this static view with a TAG-like
grammar and a flat semantics results in a system
which combines the positive aspects of both top-
down and bottom-up generators.
The paper is structured as follows. Sec-
tion 2 presents the grammars we are working
with namely, Description Grammars (DG), Sec-
tion 3 summarises the parsing model presented in
(Duchier and Thater, 1999) and Section 4 shows
that this model can be extended to generate with
DGs. In Section 5, we compare our generator
with top-down and bottom-up generators, Section
6 reports on a proof-of-concept implementation
and Section 7 concludes with pointers for further
research.
</bodyText>
<sectionHeader confidence="0.975642" genericHeader="method">
2 Description Grammars
</sectionHeader>
<bodyText confidence="0.968811942857143">
There is a range of grammar formalisms which
depart from Tree Adjoining Grammar (TAG) by
taking as basic building blocks tree descriptions
rather than trees. D-Tree Grammar (DTG) is pro-
posed in (Rambow et al., 1995) to remedy some
empirical and theoretical shortcomings of TAG;
Tree Description Grammar (TDG) is introduced
in (Kallmeyer, 1999) to support syntactic and se-
mantic underspecification and Interaction Gram-
mar is presented in (Perrier, 2000) as an alterna-
tive way of formulating linear logic grammars.
Like all these frameworks, DG uses tree de-
scriptions and thereby benefits first, from the ex-
tended domain of locality which makes TAG par-
ticularly suitable for generation (cf. (Joshi,1987))
and second, from the monotonicity which differ-
entiates descriptions from trees with respect to ad-
junction (cf. (Vijay-Shanker, 1992)).
DG differs from DTG and TDG however in
that it adopts an axiomatic rather than a genera-
tive view of grammar: whereas in DTG and TDG,
derived trees are constructed through a sequence
of rewriting steps, in DG derived trees are mod-
els satisfying a conjunction of elementary tree de-
scriptions. Moreover, DG differs from Interaction
Grammars in that it uses a flat rather than a Mon-
tague style recursive semantics thereby permitting
a simple syntax/semantics interface (see below).
A Description Grammar is a set of lexical en-
tries of the form where is a tree descrip-
tion and is the semantic representation associ-
ated with .
Tree descriptions. A tree description is a con-
junction of literals that specify either the label
of a node or the position of a node relative to
</bodyText>
<figureCaption confidence="0.999227">
Figure 1: Example grammar 1
</figureCaption>
<bodyText confidence="0.995107305555556">
other nodes. As a logical notation quickly be-
comes unwieldy, we use graphics instead. Fig-
ure 1 gives a graphic representation of a small DG
fragment. The following conventions are used.
Nodes represent node variables, plain edges strict
dominance and dotted edges dominance. The la-
bels of the nodes abbreviate a feature structure,
e.g. the label NP: represents the feature struc-
ture , while the anchor represents
the value in the feature structure of the im-
mediately dominating node variable.
Node variables can have positive, negative or
neutral polarity which are represented by black,
white and gray nodes respectively. Intuitively, a
negative node variable can be thought of as an
open valency which must be filled exactly once
by a positive node variable while a neutral node
variable is a variable that may not be identified
with any other node variable. Formally, polari-
ties are used to define the class of saturated mod-
els. A saturated model for a tree description
(written S ) is a model in which each nega-
tive node variable is identified with exactly one
positive node variable, each positive node vari-
able with exactly one negative node variable and
neutral node variables are not identified with any
other node variable. Intuitively, a saturated model
for a given tree description is the smallest tree sat-
isfying this description and such that all syntactic
valencies are filled. In contrast, a free model
for (written, F ) is a model such that ev-
ery node in that model interprets exactly one node
variable in .
In DG, lexical tree descriptions must obey the
following conventions. First, the polarities are
used in a systematic way as follows. Roots of
</bodyText>
<figure confidence="0.9928935625">
S:
NP: VP:
VP:
sees
NP:
John
V
NP:
S:
NP: S:
S:
NP: VP:
VP:
NP:
Mary
sees
</figure>
<bodyText confidence="0.999439533333333">
fragments (fully specified subtrees) are always
positive; except for the anchor, all leaves of frag-
ments are negative, and internal node variables
are neutral. This guarantees that in a saturated
model, tree fragments that belong to the denota-
tion of distinct tree descriptions do not overlap.
Second, we require that every lexical tree descrip-
tion has a single minimal free model, which es-
sentially means that the lexical descriptions must
be tree shaped.
Semantic representation. Following (Stone and
Doran, 1997), we represent meaning using a flat
semantic representation, i.e. as multisets, or con-
junctions, of non-recursive propositions. This
treatment offers a simple syntax-semantics inter-
face in that the meaning of a tree is just the con-
junction of meanings of the lexical tree descrip-
tions used to derive it once the free variables oc-
curring in the propositions are instantiated. A free
variable is instantiated as follows: each free vari-
able labels a syntactic node variable and is uni-
fied with the label of any node variable identified
with . For the purpose of this paper, a simple se-
mantic representation language is adopted which
in particular, does not include “handles” i.e. la-
bels on propositions. For a wider empirical cov-
erage including e.g. quantifiers, a more sophisti-
cated version of flat semantics can be used such as
Minimal Recursion Semantics (Copestake et al.,
1999).
</bodyText>
<sectionHeader confidence="0.854768" genericHeader="method">
3 Parsing with DG
</sectionHeader>
<bodyText confidence="0.999787333333333">
Parsing with DG can be formulated as a model
generation problem, the task of finding models
satisfying a give logical formula. If we restrict
our attention to grammars where every lexical tree
description has exactly one anchor and (unreal-
istically) assuming that each word is associated
</bodyText>
<figure confidence="0.8433115">
—
John
saw
Mary
</figure>
<figureCaption confidence="0.998871">
Figure 3: Example parsing matrix
</figureCaption>
<bodyText confidence="0.9998219">
with exactly one lexical entry, then parsing a sen-
tence consists in finding the saturated
model(s) with yield such that sat-
isfies the conjunction of lexical tree descriptions
with the tree description associ-
ated with the word by the grammar.
Figure 2 illustrates this idea for the sentence
“John loves Mary”. The tree on the right hand
side represents the saturated model satisfying the
conjunction of the descriptions given on the left
and obtained from parsing the sentence “John
sees Mary” (the isolated negative node variable,
the “ROOT description”, is postulated during
parsing to cancel out the negative polarity of the
top-most S-node in the parse tree). The dashed
lines between the left and the right part of the fig-
ure schematise the interpretation function: it indi-
cates which node variables gets mapped to which
node in the model.
As (Duchier and Thater, 1999) shows however,
lexical ambiguity means that the parsing problem
is in fact more complex as it in effect requires that
models be searched for that satisfy a conjunction
of disjunctions (rather than simply a conjunction)
of lexical tree descriptions.
The constraint based encoding of this problem
presented in (Duchier and Thater, 1999) can be
sketched as follows1. To start with, the conjunc-
tion of disjunctions of descriptions obtained on
the basis of the lexical lookup is represented as
a matrix, where each row corresponds to a word
from the input (except for the first row which is
filled with the above mentioned ROOT descrip-
tion) and columns give the lexical entries asso-
ciated by the grammar with these words. Any
matrix entry which is empty is filled with the for-
mula which is true in all models. Figure 3
shows an example parsing matrix for the string
“John saw Mary” given the grammar in Figure 1.2
Given such a matrix, the task of parsing con-
</bodyText>
<footnote confidence="0.99272725">
1For a detailed presentation of this constraint based en-
coding, see the paper itself.
2For lack of space in the remainder of the paper, we omit
the ROOT description in the matrices.
</footnote>
<figure confidence="0.9959672">
VP:
NP:
VP:
VP:
V
NP:
John
V
NP:
NP:
John
Mary
sees
sees
NP:
NP:
Mary
S
S:
S:
</figure>
<figureCaption confidence="0.978044">
Figure 2: S
</figureCaption>
<bodyText confidence="0.661676">
sists in:
</bodyText>
<listItem confidence="0.986858875">
1. selecting exactly one entry per row thereby
producing a conjunction of selected lexical
entries,
2. building a saturated model for this conjunc-
tion of selected entries such that the yield of
that model is equal to the input string and
3. building a free model for each of the remain-
ing (non selected) entries.
</listItem>
<bodyText confidence="0.999972454545454">
The important point about this way of formu-
lating the problem is that it requires all constraints
imposed by the lexical tree descriptions occurring
in the matrix to be satisfied (though not neces-
sarily in the same model). This ensures strong
constraint propagation and thereby reduces non-
determinism. In particular, it avoids the combina-
torial explosion that would result from first gener-
ating the possible conjunctions of lexical descrip-
tions out of the CNF obtained by lexical lookup
and second, testing their satisfiability.
</bodyText>
<sectionHeader confidence="0.974937" genericHeader="method">
4 Generating with DG
</sectionHeader>
<bodyText confidence="0.99992775">
We now show how the parsing model just de-
scribed can be adapted to generate from some se-
mantic representation , one or more sentence(s)
with semantics .
</bodyText>
<subsectionHeader confidence="0.992247">
4.1 Basic Idea
</subsectionHeader>
<bodyText confidence="0.997589806451613">
The parsing model outlined in the previous sec-
tion can directly be adapted for generation as fol-
lows. First, the lexical lookup is modified such
that propositions instead of words are used to de-
termine the relevant lexical tree descriptions: a
lexical tree description is selected if its seman-
tics subsumes part of the input semantics. Sec-
ond, the constraint that the yield of the saturated
model matches the input string is replaced by a
constraint that the sum of the cardinalities of the
multisets of propositions associated with the lex-
ical tree descriptions composing the solution tree
equals the cardinality of the input semantics. To-
gether with the above requirement that only lexi-
cal entries be selected whose semantics subsumes
part of the goal semantics, this ensures that the se-
mantics of the solution trees is identical with the
input semantics.
The following simple example illustrates
this idea. Suppose the input semantics is
and the grammar is as given in Figure 1. The
generating matrix then is:
Given this generating matrix, two matrix mod-
els will be generated, one with a saturated model
satisfying and a free
model satisfying and the other with the sat-
urated model satisfying
and a free model satisfying . The first
solution yields the sentence “John sees Mary”
whereas the second yields the topicalised sen-
tence “Mary, John sees.”
</bodyText>
<subsectionHeader confidence="0.998671">
4.2 Going Further
</subsectionHeader>
<bodyText confidence="0.999243412500001">
The problem with the simple method outlined
above is that it severely restricts the class of gram-
mars that can be used by the generator. Recall that
in (Duchier and Thater, 1999)’s parsing model,
the assumption is made that each lexical entry has
exactly one anchor. In practice this means that the
parser can deal neither with a grammar assign-
ing trees with multiple anchors to idioms (as is
argued for in e.g. (Abeill´e and Schabes, 1989))
nor with a grammar allowing for trace anchored
lexical entries. The mirror restriction for genera-
tion is that each lexical entry must be associated
with exactly one semantic proposition. The re-
sulting shortcomings are that the generator can
deal neither with a lexical entry having an empty
semantics nor with a lexical entry having a multi-
propositional semantics. We first show that these
restrictions are too strong. We then show how to
adapt the generator so as to lift them.
Empty Semantics. Arguably there are words
such as “that” or infinitival “to” whose semantic
contribution is void. As (Shieber, 1988) showed,
the problem with such words is that they cannot
be selected on the basis of the input semantics.
To circumvent this problem, we take advantage
of the TAG extended domain of locality to avoid
having such entries in the grammar. For instance,
complementizer “that” does not anchor a tree de-
scription by itself but occurs in all lexical tree de-
scriptions providing an appropriate syntactic con-
text for it, e.g. in the tree description for “say”.
Multiple Propositions. Lexical entries with a
multi-propositional semantics are also very com-
mon. For instance, a neo-Davidsonian seman-
tics would associate e.g. with
the verb “run” or with the
past tensed “ran”. Similarly, agentless passive
“be” might be represented by an overt quantifi-
cation over the missing agent position (such as
with a variable over
the complement verb semantics). And a gram-
mar with a rich lexical semantics might for in-
stance associate the semantics ,
with “want” (cf. (McCawley, 1979)
which argues for such a semantics to account for
examples such as “Reuters wants the report to-
morrow” where “tomorrow” modifies the “hav-
ing” not the “wanting”).
Because it assumes that each lexical entry is
associated with exactly one semantic proposi-
tion, such cases cannot be dealt with the gener-
ator sketched in the previous section. A simple
method for fixing this problem would be to first
partition the input semantics in as many ways as
are possible and to then use the resulting parti-
tions as the basis for lexical lookup.
The problems with this method are both theo-
retical and computational. On the theoretical side,
the problem is that the partitioning is made in-
dependent of grammatical knowledge. It would
be better for the decomposition of the input se-
mantics to be specified by the lexical lookup
phase, rather than by means of a language in-
dependent partitioning procedure. Computation-
ally, this method is unsatisfactory in that it im-
plements a generate-and-test procedure (first, a
partition is created and second, model genera-
tion is applied to the resulting matrices) which
could rapidly lead to combinatorial explosion and
is contrary in spirit to (Duchier and Thater, 1999)
constraint-based approach.
We therefore propose the following alternative
procedure. We start by marking in each lexi-
cal entry, one proposition in the associated se-
mantics as being the head of this semantic rep-
resentation. The marking is arbitrary: it does
not matter which proposition is the head as long
as each semantic representation has exactly one
head. We then use this head for lexical lookup.
Instead of selecting lexical entries on the basis
</bodyText>
<figureCaption confidence="0.998146">
Figure 4: Example grammar
</figureCaption>
<bodyText confidence="0.997837411764706">
of their whole semantics, we select them on the
basis of their index. That is, a lexical entry is
selected iff its head unifies with a proposition
in the input semantics. To preserve coherence,
we further maintain the additional constraint that
the total semantics of each selected entries sub-
sumes (part of) the input semantics. For instance,
given the grammar in Figure 4 (where seman-
tic heads are underlined) and the input semantics
, the generat-
Given this matrix, two solutions will be found:
the saturated tree for “John ran” satisfying the
conjunction and that for “John did
run” satisfying . No other so-
lution is found as for any other conjunction of de-
scriptions made available by the matrix, no satu-
rated model exists.
</bodyText>
<sectionHeader confidence="0.916786" genericHeader="method">
5 Comparison with related work
</sectionHeader>
<bodyText confidence="0.999927615384615">
Our generator presents three main characteristics:
(i) It is based on an axiomatic rather than a gen-
erative view of grammar, (ii) it uses a TAG-like
grammar in which the basic linguistic units are
trees rather than categories and (iii) it assumes a
flat semantics.
In what follows we show that this combina-
tion of features results in a generator which in-
tegrates the positive aspects of both top-down and
bottom-up generators. In this sense, it is not un-
like (Shieber et al., 1990)’s semantic-head-driven
generation. As will become clear in the follow-
ing section however, it differs from it in that it
</bodyText>
<figure confidence="0.980840764705882">
S:
NP: VP:
VP:
V
NP:
John
run
V
did
S:
NP: VP:
VP:
V
VP:
ran
VP:
ing matrix will be:
</figure>
<bodyText confidence="0.861501">
integrates stronger lexicalist (i.e. bottom-up) in-
formation.
</bodyText>
<subsectionHeader confidence="0.993123">
5.1 Bottom-Up Generation
</subsectionHeader>
<bodyText confidence="0.994857658914729">
Bottom-up or “lexically-driven” generators (e.g.,
(Shieber, 1988; Whitelock, 1992; Kay, 1996; Car-
roll et al., 1999)) start from a bag of lexical items
with instantiated semantics and generates a syn-
tactic tree by applying grammar rules whose right
hand side matches a sequence of phrases in the
current input.
There are two known disadvantages to bottom-
up generators. On the one hand, they require
that the grammar be semantically monotonic that
is, that the semantics of each daughter in a rule
subsumes some portion of the mother semantics.
On the other hand, they are often overly non-
deterministic (though see (Carroll et al., 1999) for
an exception). We now show how these problems
are dealt with in the present algorithm.
Non-determinism. Two main sources of non-
determinism affect the performance of bottom-up
generators: the lack of an indexing scheme and
the presence of intersective modifiers.
In (Shieber, 1988), a chart-based bottom-up
generator is presented which is devoid of an in-
dexing scheme: all word edges leave and enter the
same vertex and as a result, interactions must be
considered explicitly between new edges and all
edges currently in the chart. The standard solution
to this problem (cf. (Kay, 1996)) is to index edges
with semantic indices (for instance, the edge with
category N/x:dog(x) will be indexed with x) and
to restrict edge combination to these edges which
have compatible indices. Specifically, an active
edge with category A(...)/C(c ...) (with c the se-
mantics index of the missing component) is re-
stricted to combine with inactive edges with cate-
gory C(c ...), and vice versa.
Although our generator does not make use of a
chart, the constraint-based processing model de-
scribed in (Duchier and Thater, 1999) imposes a
similar restriction on possible combinations as it
in essence requires that only these nodes pairs be
tried for identification which (i) have opposite po-
larity and (ii) are labeled with the same semantic
index.
Let us now turn to the second known source
of non-determinism for bottom-up generators
namely, intersective modifiers. Within a construc-
tive approach to lexicalist generation, the number
of structures (edges or phrases) built when gener-
ating a phrase with intersective modifiers is
in the case where the grammar imposes a single
linear ordering of these modifiers. For instance,
when generating “The fierce little black cat”, a
naive constructive approach will also build the
subphrases (1) only to find that these cannot be
part of the output as they do not exhaust the input
semantics.
(1) The fierce black cat, The fierce little cat, The little
black cat, The black cat, The fierce cat, The little cat,
The cat.
To remedy this shortcoming, various heuristics
and parsing strategies have been proposed. (Brew,
1992) combines a constraint-propagation mech-
anism with a shift-reduce generator, propagating
constraints after every reduction step. (Carroll et
al., 1999) advocate a two-step generation algo-
rithm in which first, the basic structure of the sen-
tence is generated and second, intersective mod-
ifiers are adjoined in. And (Poznanski et al.,
1995) make use of a tree reconstruction method
which incrementally improves the syntactic tree
until it is accepted by the grammar. In effect,
the constraint-based encoding of the axiomatic
view of generation proposed here takes advantage
of Brew’s observation that constraint propagation
can be very effective in pruning the search space
involved in the generation process.
In constraint programming, the solutions to a
constraint satisfaction problem (CSP) are found
by alternating propagation with distribution steps.
Propagation is a process of deterministic infer-
ence which fills out the consequences of a given
choice by removing all the variable values which
can be inferred to be inconsistent with the prob-
lem constraint while distribution is a search pro-
cess which enumerates possible values for the
problem variables. By specifying global proper-
ties of the output and letting constraint propaga-
tion fill out the consequences of a choice, situa-
tions in which no suitable trees can be built can be
detected early. Specifically, the global constraint
stating that the semantics of a solution tree must
be identical with the goal semantics rules out the
generation of the phrases in (1b). In practice, we
observe that constraint propagation is indeed very
efficient at pruning the search space. As table
5 shows, the number of choice points (for these
specific examples) augments very slowly with the
size of the input.
Semantic monotonicity. Lexical lookup only re-
turns these categories in the grammar whose se-
mantics subsumes some portion of the input se-
mantics. Therefore if some grammar rule involves
a daughter category whose semantics is not part
of the mother semantics i.e. if the grammar is se-
mantically non-monotonic, this rule will never be
applied even though it might need to be. Here is
an example. Suppose the grammar contains the
following rule (where X/Y abbreviates a category
with part-of-speech X and semantics Y):
vp/call up(X,Y) v/call up(X,Y), np/Y, pp/up
And suppose the input semantics is
. On the basis of this
input, lexical lookup will return the categories
V/call up(john,mary), NP/john and NP/mary
(because their semantics subsumes some portion
of the input semantics) but not the category
PP/up. Hence the sentence “John called Mary
up” will fail to be generated.
In short, the semantic monotonicity constraint
makes the generation of collocations and idioms
problematic. Here again the extended domain of
locality provided by TAG is useful as it means
that the basic units are trees rather than categories.
Furthermore, as argued in (Abeill´e and Schabes,
1989), these trees can have multiple lexical an-
chors. As in the case of vestigial semantics dis-
cussed in Section 4 above, this means that phono-
logical material can be generated without its se-
mantics necessarily being part of the input.
</bodyText>
<subsectionHeader confidence="0.982117">
5.2 Top-Down Generation
</subsectionHeader>
<bodyText confidence="0.998338">
As shown in detail in (Shieber et al., 1990), top-
down generators can fail to terminate on certain
grammars because they lack the lexical informa-
tion necessary for their well-foundedness. A sim-
ple example involves the following grammar frag-
ment:
</bodyText>
<footnote confidence="0.835738">
r1. s/S np/NP, vp(NP)/S
r2. np/NP det(N)/NP, n/N
r3. det(N)/NP np/NP0, poss(NP0,NP)/NP
r4. np/john john
r5. poss(NP0,NP)/mod(N,NP0) s
r6. n/father father
r7. vp(NP)/left(NP) left
</footnote>
<bodyText confidence="0.999796266666667">
Given a top-down regime proceeding depth-first,
left-to-right through the search space defined by
the grammar rules, termination may fail to occur
as the intermediate goal semantics NP (in the sec-
ond rule) is uninstantiated and permits an infinite
loop by iterative applications of rules r2 and r3.
Such non-termination problems do not arise
for the present algorithm as it is lexically driven.
So for instance given the corresponding DG frag-
ment for the above grammar and the input seman-
tics ,
the generator will simply select the tree de-
scriptions for “left”, “John”, “s” and “father”
and generate the saturated model satisfying the
conjunction of these descriptions.
</bodyText>
<sectionHeader confidence="0.999481" genericHeader="method">
6 Implementation
</sectionHeader>
<bodyText confidence="0.999903473684211">
The ideas presented here have been implemented
using the concurrent constraint programming lan-
guage Oz (Smolka, 1995). The implementation
includes a model generator for the tree logic pre-
sented in section 2, two lexical lookup modules
(one for parsing, one for generation) and a small
DG fragment for English which has been tested
in parsing and generation mode on a small set of
English sentences.
This implementation can be seen as a proof
of concept for the ideas presented in this paper:
it shows how a constraint-based encoding of the
type of global constraints suggested by an ax-
iomatic view of grammar can help reduce non-
determinism (few choice points cf. table 5) but
performance decreases rapidly with the length of
the input and it remains a matter for further re-
search how efficiency can be improved to scale
up to bigger sentences and larger grammars.
</bodyText>
<sectionHeader confidence="0.998934" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9990348">
We have shown that modulo some minor changes,
the constraint-based approach to parsing pre-
sented in (Duchier and Thater, 1999) could also
be used for generation. Furthermore, we have ar-
gued that the resulting generator, when combined
with a TAG-like grammar and a flat semantics,
had some interesting features: it exhibits the lex-
icalist aspects of bottom-up approaches thereby
avoiding the non-termination problems connected
with top-down approaches; it includes enough
Example CP Time
The cat likes a fox 1 1.2s
The little brown cat likes a yellow fox 2 1.8s
The fierce little brown cat likes a yellow fox 2 5.5s
The fierce little brown cat likes a tame yellow fox 3 8.0s
</bodyText>
<figureCaption confidence="0.990661">
Figure 5: Examples
</figureCaption>
<bodyText confidence="0.999962823529412">
top-down guidance from the TAG trees to avoid
typical bottom-up shortcomings such as the re-
quirement for grammar semantic monotonicity
and by implementing an axiomatic view of gram-
mar, it supports a near-deterministic treatment of
intersective modifiers.
It would be interesting to see whether other
axiomatic constraint-based treatments of gram-
mar could be use to support both parsing and
generation. In particular, we intend to investi-
gate whether the dependency grammar presented
in (Duchier, 1999), once equipped with a se-
mantics, could be used not only for parsing but
also for generating. And similarly, whether the
description based treatment of discourse parsing
sketched in (Duchier and Gardent, 2001) could be
used to generate discourse.
</bodyText>
<sectionHeader confidence="0.999089" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999855971428572">
A. Abeill´e and Y. Schabes. 1989. Parsing idioms
in lexicalised TAGs. In Proceedings of EACL ’89,
Manchester, UK.
R. Backofen, J. Rogers, and K. Vijay-Shanker. 1995.
A first-order axiomatization of the theory of finite
trees. Journal ofLogic, Language and Information,
4(1).
C. Brew. 1992. Letting the cat out of the bag: Gen-
eration for shake-and-bake MT. In Proceedings of
COLING ’92, Nantes, France.
J. Carroll, A. Copestake, D. Flickinger, and
V. Pazna´nski. 1999. An efficient chart generator
for (semi-)lexicalist grammars. In Proceedings of
EWNLG ’99.
A. Copestake, D. Flickinger, I. Sag, and C. Pol-
lard. 1999. Minimal Recursion Seman-
tics: An introduction. URL: http://www-
csli.stanford.edu/ aac/papers.html, September.
T. Cornell and J. Rogers. To appear. Model theo-
retic syntax. In L. Cheng and R. Sybesma, editors,
The GLOT International State of the Article Book 1.
Holland Academic Graphics, The Hague.
D. Duchier and C. Gardent. 1999. A constraint-based
treatment of descriptions. In H.C. Bunt and E.G.C.
Thijsse, editors, Proceedings ofIWCS-3, Tilburg.
D. Duchier and C. Gardent. 2001. Tree descrip-
tions, constraints and incrementality. In Comput-
ing Meaning, volume 2 of Studies in Linguistics and
Philosophy Series. Kluwer Academic Publishers.
D. Duchier and S. Thater. 1999. Parsing with
tree descriptions: a constraint-based approach. In
NLULP’99, Las Cruces, New Mexico.
D. Duchier. 1999. Axiomatizing dependency parsing
using set constraints. In Sixth Meeting on Mathe-
matics ofLanguage, Orlando, Florida.
A. Joshi. 1987. The relevance of Tree Adjoining
Grammar to generation. In Natural Language Gen-
eration, chapter 16. Martinus Jijhoff Publishers,
Dordrecht, Holland.
L. Kallmeyer. 1999. Tree Description Grammars and
Underspeci�ed Representations. Ph.D. thesis, Uni-
versit¨at T¨ubingen.
M. Kay. 1996. Chart generation. In Proceedings of
ACL’96, Santa Cruz, USA.
J. D. McCawley. 1979. Adverbs, Vowels, and other
objects of Wonder. University of Chicago Press,
Chicago, Illinois.
G. Perrier. 2000. Interaction grammars. In In Pro-
ceedings of 18th International Conference on Com-
putational Linguistics (COLING 2000).
V. Poznanski, J. L. Beaven, and P. Whitelock. 1995.
An efficient generation algorithm for lexicalist MT.
In Proceedings ofACL ’95.
O. Rambow, K. Vijay-Shanker, and D. Weir. 1995.
D-tree Grammars. In Proceedings ofACL ’95.
S. Shieber, F. Pereira, G. van Noord, and R. Moore.
1990. Semantic-head-driven generation. Computa-
tional Linguistics, 16(1).
S. Shieber. 1988. A Uniform Architecture for Parsing
and Generation. In Proceedings ofACL ’88.
G. Smolka. 1995. The Oz Programming Model. In
Computer Science Today, volume 1000 ofLNCS.
M. Stone and C. Doran. 1997. Sentence planning
as description using Tree-Adjoining Grammar. In
Proceedings ofACL ’97.
K. Vijay-Shanker. 1992. Using descriptions of trees
in Tree Adjoining Grammars. Computational Lin-
guistics, 18(4):481–518.
P. Whitelock. 1992. Shake-and-bake translation. In
Proceedings of COLING ’92, Nantes, France.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.701527">
<title confidence="0.997649">Generating with a Grammar Based on Tree Descriptions: a Constraint-Based Approach</title>
<author confidence="0.994297">Claire Gardent</author>
<affiliation confidence="0.951525">CNRS</affiliation>
<address confidence="0.893513">LORIA, BP 239 Campus Scientifique 54506 Vandoeuvre-les-Nancy, France</address>
<email confidence="0.992506">claire.gardent@loria.fr</email>
<author confidence="0.996463">Stefan Thater</author>
<affiliation confidence="0.973026">Computational Linguistics Universit¨at des Saarlandes</affiliation>
<address confidence="0.949134">Saarbr¨ucken, Germany</address>
<email confidence="0.999288">stth@coli.uni-sb.de</email>
<abstract confidence="0.999395857142857">the of language processing builds bigger units out of smaller ones by means of rewriting the eliminates invalid linguistic structures out of a set of possible structures by means of wellformedness principles. We present a generator based on the axiomatic view and argue that when combined with a TAG-like grammar and a flat semantics, this axiomatic view permits avoiding drawbacks known to hold either of top-down or of bottom-up generators.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abeill´e</author>
<author>Y Schabes</author>
</authors>
<title>Parsing idioms in lexicalised TAGs.</title>
<date>1989</date>
<booktitle>In Proceedings of EACL ’89,</booktitle>
<location>Manchester, UK.</location>
<marker>Abeill´e, Schabes, 1989</marker>
<rawString>A. Abeill´e and Y. Schabes. 1989. Parsing idioms in lexicalised TAGs. In Proceedings of EACL ’89, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Backofen</author>
<author>J Rogers</author>
<author>K Vijay-Shanker</author>
</authors>
<title>A first-order axiomatization of the theory of finite trees.</title>
<date>1995</date>
<journal>Journal ofLogic, Language and Information,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="2600" citStr="Backofen et al., 1995" startWordPosition="416" endWordPosition="420"> lexical categories selected on the basis of the input string plus any additional constraints which might be encoded in the grammar. And similarly for generation, a model generator can be used to enumerate the models satisfying the bag of lexical items selected by the lexical look up phase on the basis of the input semantics. How can we design model generators which work efficiently on natural language input i.e. on the type of information delivered by logic based grammars? (Duchier and Gardent, 1999) shows that constraint programming can be used to implement a model generator for tree logic (Backofen et al., 1995). Further, (Duchier and Thater, 1999) shows that this model generator can be used to parse with descriptions based grammars (Rambow et al., 1995; Kallmeyer, 1999) that is, on logic based grammars where lexical entries are descriptions of trees expressed in some tree logic. In this paper, we build on (Duchier and Thater, 1999) and show that modulo some minor modifications, the same model generator can be used to generate with description based grammars. We describe the workings of the algorithm and compare it with standard existing top-down and bottom-up generation algorithms. In specific, we a</context>
</contexts>
<marker>Backofen, Rogers, Vijay-Shanker, 1995</marker>
<rawString>R. Backofen, J. Rogers, and K. Vijay-Shanker. 1995. A first-order axiomatization of the theory of finite trees. Journal ofLogic, Language and Information, 4(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brew</author>
</authors>
<title>Letting the cat out of the bag: Generation for shake-and-bake MT.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING ’92,</booktitle>
<location>Nantes, France.</location>
<contexts>
<context position="21754" citStr="Brew, 1992" startWordPosition="3602" endWordPosition="3603">s or phrases) built when generating a phrase with intersective modifiers is in the case where the grammar imposes a single linear ordering of these modifiers. For instance, when generating “The fierce little black cat”, a naive constructive approach will also build the subphrases (1) only to find that these cannot be part of the output as they do not exhaust the input semantics. (1) The fierce black cat, The fierce little cat, The little black cat, The black cat, The fierce cat, The little cat, The cat. To remedy this shortcoming, various heuristics and parsing strategies have been proposed. (Brew, 1992) combines a constraint-propagation mechanism with a shift-reduce generator, propagating constraints after every reduction step. (Carroll et al., 1999) advocate a two-step generation algorithm in which first, the basic structure of the sentence is generated and second, intersective modifiers are adjoined in. And (Poznanski et al., 1995) make use of a tree reconstruction method which incrementally improves the syntactic tree until it is accepted by the grammar. In effect, the constraint-based encoding of the axiomatic view of generation proposed here takes advantage of Brew’s observation that co</context>
</contexts>
<marker>Brew, 1992</marker>
<rawString>C. Brew. 1992. Letting the cat out of the bag: Generation for shake-and-bake MT. In Proceedings of COLING ’92, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>V Pazna´nski</author>
</authors>
<title>An efficient chart generator for (semi-)lexicalist grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of EWNLG ’99.</booktitle>
<marker>Carroll, Copestake, Flickinger, Pazna´nski, 1999</marker>
<rawString>J. Carroll, A. Copestake, D. Flickinger, and V. Pazna´nski. 1999. An efficient chart generator for (semi-)lexicalist grammars. In Proceedings of EWNLG ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>I Sag</author>
<author>C Pollard</author>
</authors>
<title>Minimal Recursion Semantics: An introduction. URL: http://wwwcsli.stanford.edu/ aac/papers.html,</title>
<date>1999</date>
<contexts>
<context position="8881" citStr="Copestake et al., 1999" startWordPosition="1446" endWordPosition="1449">he lexical tree descriptions used to derive it once the free variables occurring in the propositions are instantiated. A free variable is instantiated as follows: each free variable labels a syntactic node variable and is unified with the label of any node variable identified with . For the purpose of this paper, a simple semantic representation language is adopted which in particular, does not include “handles” i.e. labels on propositions. For a wider empirical coverage including e.g. quantifiers, a more sophisticated version of flat semantics can be used such as Minimal Recursion Semantics (Copestake et al., 1999). 3 Parsing with DG Parsing with DG can be formulated as a model generation problem, the task of finding models satisfying a give logical formula. If we restrict our attention to grammars where every lexical tree description has exactly one anchor and (unrealistically) assuming that each word is associated — John saw Mary Figure 3: Example parsing matrix with exactly one lexical entry, then parsing a sentence consists in finding the saturated model(s) with yield such that satisfies the conjunction of lexical tree descriptions with the tree description associated with the word by the grammar. F</context>
</contexts>
<marker>Copestake, Flickinger, Sag, Pollard, 1999</marker>
<rawString>A. Copestake, D. Flickinger, I. Sag, and C. Pollard. 1999. Minimal Recursion Semantics: An introduction. URL: http://wwwcsli.stanford.edu/ aac/papers.html, September.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Cornell</author>
<author>J Rogers</author>
</authors>
<title>To appear. Model theoretic syntax.</title>
<booktitle>The GLOT International State of the Article Book 1. Holland Academic Graphics, The Hague.</booktitle>
<editor>In L. Cheng and R. Sybesma, editors,</editor>
<marker>Cornell, Rogers, </marker>
<rawString>T. Cornell and J. Rogers. To appear. Model theoretic syntax. In L. Cheng and R. Sybesma, editors, The GLOT International State of the Article Book 1. Holland Academic Graphics, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Duchier</author>
<author>C Gardent</author>
</authors>
<title>A constraint-based treatment of descriptions.</title>
<date>1999</date>
<booktitle>Proceedings ofIWCS-3,</booktitle>
<editor>In H.C. Bunt and E.G.C. Thijsse, editors,</editor>
<location>Tilburg.</location>
<contexts>
<context position="2484" citStr="Duchier and Gardent, 1999" startWordPosition="396" endWordPosition="399">l generator can be used to enumerate the (minimal) model(s), that is, the parse trees, satisfying the conjunction of the lexical categories selected on the basis of the input string plus any additional constraints which might be encoded in the grammar. And similarly for generation, a model generator can be used to enumerate the models satisfying the bag of lexical items selected by the lexical look up phase on the basis of the input semantics. How can we design model generators which work efficiently on natural language input i.e. on the type of information delivered by logic based grammars? (Duchier and Gardent, 1999) shows that constraint programming can be used to implement a model generator for tree logic (Backofen et al., 1995). Further, (Duchier and Thater, 1999) shows that this model generator can be used to parse with descriptions based grammars (Rambow et al., 1995; Kallmeyer, 1999) that is, on logic based grammars where lexical entries are descriptions of trees expressed in some tree logic. In this paper, we build on (Duchier and Thater, 1999) and show that modulo some minor modifications, the same model generator can be used to generate with description based grammars. We describe the workings of</context>
</contexts>
<marker>Duchier, Gardent, 1999</marker>
<rawString>D. Duchier and C. Gardent. 1999. A constraint-based treatment of descriptions. In H.C. Bunt and E.G.C. Thijsse, editors, Proceedings ofIWCS-3, Tilburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Duchier</author>
<author>C Gardent</author>
</authors>
<title>Tree descriptions, constraints and incrementality.</title>
<date>2001</date>
<booktitle>In Computing Meaning,</booktitle>
<volume>2</volume>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Duchier, Gardent, 2001</marker>
<rawString>D. Duchier and C. Gardent. 2001. Tree descriptions, constraints and incrementality. In Computing Meaning, volume 2 of Studies in Linguistics and Philosophy Series. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Duchier</author>
<author>S Thater</author>
</authors>
<title>Parsing with tree descriptions: a constraint-based approach.</title>
<date>1999</date>
<booktitle>In NLULP’99, Las</booktitle>
<location>Cruces, New Mexico.</location>
<contexts>
<context position="2637" citStr="Duchier and Thater, 1999" startWordPosition="422" endWordPosition="425">e basis of the input string plus any additional constraints which might be encoded in the grammar. And similarly for generation, a model generator can be used to enumerate the models satisfying the bag of lexical items selected by the lexical look up phase on the basis of the input semantics. How can we design model generators which work efficiently on natural language input i.e. on the type of information delivered by logic based grammars? (Duchier and Gardent, 1999) shows that constraint programming can be used to implement a model generator for tree logic (Backofen et al., 1995). Further, (Duchier and Thater, 1999) shows that this model generator can be used to parse with descriptions based grammars (Rambow et al., 1995; Kallmeyer, 1999) that is, on logic based grammars where lexical entries are descriptions of trees expressed in some tree logic. In this paper, we build on (Duchier and Thater, 1999) and show that modulo some minor modifications, the same model generator can be used to generate with description based grammars. We describe the workings of the algorithm and compare it with standard existing top-down and bottom-up generation algorithms. In specific, we argue that the change of perspective o</context>
<context position="10111" citStr="Duchier and Thater, 1999" startWordPosition="1649" endWordPosition="1652"> 2 illustrates this idea for the sentence “John loves Mary”. The tree on the right hand side represents the saturated model satisfying the conjunction of the descriptions given on the left and obtained from parsing the sentence “John sees Mary” (the isolated negative node variable, the “ROOT description”, is postulated during parsing to cancel out the negative polarity of the top-most S-node in the parse tree). The dashed lines between the left and the right part of the figure schematise the interpretation function: it indicates which node variables gets mapped to which node in the model. As (Duchier and Thater, 1999) shows however, lexical ambiguity means that the parsing problem is in fact more complex as it in effect requires that models be searched for that satisfy a conjunction of disjunctions (rather than simply a conjunction) of lexical tree descriptions. The constraint based encoding of this problem presented in (Duchier and Thater, 1999) can be sketched as follows1. To start with, the conjunction of disjunctions of descriptions obtained on the basis of the lexical lookup is represented as a matrix, where each row corresponds to a word from the input (except for the first row which is filled with t</context>
<context position="13926" citStr="Duchier and Thater, 1999" startWordPosition="2304" endWordPosition="2307">nput semantics is and the grammar is as given in Figure 1. The generating matrix then is: Given this generating matrix, two matrix models will be generated, one with a saturated model satisfying and a free model satisfying and the other with the saturated model satisfying and a free model satisfying . The first solution yields the sentence “John sees Mary” whereas the second yields the topicalised sentence “Mary, John sees.” 4.2 Going Further The problem with the simple method outlined above is that it severely restricts the class of grammars that can be used by the generator. Recall that in (Duchier and Thater, 1999)’s parsing model, the assumption is made that each lexical entry has exactly one anchor. In practice this means that the parser can deal neither with a grammar assigning trees with multiple anchors to idioms (as is argued for in e.g. (Abeill´e and Schabes, 1989)) nor with a grammar allowing for trace anchored lexical entries. The mirror restriction for generation is that each lexical entry must be associated with exactly one semantic proposition. The resulting shortcomings are that the generator can deal neither with a lexical entry having an empty semantics nor with a lexical entry having a m</context>
<context position="16957" citStr="Duchier and Thater, 1999" startWordPosition="2805" endWordPosition="2808">h theoretical and computational. On the theoretical side, the problem is that the partitioning is made independent of grammatical knowledge. It would be better for the decomposition of the input semantics to be specified by the lexical lookup phase, rather than by means of a language independent partitioning procedure. Computationally, this method is unsatisfactory in that it implements a generate-and-test procedure (first, a partition is created and second, model generation is applied to the resulting matrices) which could rapidly lead to combinatorial explosion and is contrary in spirit to (Duchier and Thater, 1999) constraint-based approach. We therefore propose the following alternative procedure. We start by marking in each lexical entry, one proposition in the associated semantics as being the head of this semantic representation. The marking is arbitrary: it does not matter which proposition is the head as long as each semantic representation has exactly one head. We then use this head for lexical lookup. Instead of selecting lexical entries on the basis Figure 4: Example grammar of their whole semantics, we select them on the basis of their index. That is, a lexical entry is selected iff its head u</context>
<context position="20715" citStr="Duchier and Thater, 1999" startWordPosition="3432" endWordPosition="3435"> new edges and all edges currently in the chart. The standard solution to this problem (cf. (Kay, 1996)) is to index edges with semantic indices (for instance, the edge with category N/x:dog(x) will be indexed with x) and to restrict edge combination to these edges which have compatible indices. Specifically, an active edge with category A(...)/C(c ...) (with c the semantics index of the missing component) is restricted to combine with inactive edges with category C(c ...), and vice versa. Although our generator does not make use of a chart, the constraint-based processing model described in (Duchier and Thater, 1999) imposes a similar restriction on possible combinations as it in essence requires that only these nodes pairs be tried for identification which (i) have opposite polarity and (ii) are labeled with the same semantic index. Let us now turn to the second known source of non-determinism for bottom-up generators namely, intersective modifiers. Within a constructive approach to lexicalist generation, the number of structures (edges or phrases) built when generating a phrase with intersective modifiers is in the case where the grammar imposes a single linear ordering of these modifiers. For instance,</context>
<context position="27054" citStr="Duchier and Thater, 1999" startWordPosition="4445" endWordPosition="4448">f English sentences. This implementation can be seen as a proof of concept for the ideas presented in this paper: it shows how a constraint-based encoding of the type of global constraints suggested by an axiomatic view of grammar can help reduce nondeterminism (few choice points cf. table 5) but performance decreases rapidly with the length of the input and it remains a matter for further research how efficiency can be improved to scale up to bigger sentences and larger grammars. 7 Conclusion We have shown that modulo some minor changes, the constraint-based approach to parsing presented in (Duchier and Thater, 1999) could also be used for generation. Furthermore, we have argued that the resulting generator, when combined with a TAG-like grammar and a flat semantics, had some interesting features: it exhibits the lexicalist aspects of bottom-up approaches thereby avoiding the non-termination problems connected with top-down approaches; it includes enough Example CP Time The cat likes a fox 1 1.2s The little brown cat likes a yellow fox 2 1.8s The fierce little brown cat likes a yellow fox 2 5.5s The fierce little brown cat likes a tame yellow fox 3 8.0s Figure 5: Examples top-down guidance from the TAG tr</context>
</contexts>
<marker>Duchier, Thater, 1999</marker>
<rawString>D. Duchier and S. Thater. 1999. Parsing with tree descriptions: a constraint-based approach. In NLULP’99, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Duchier</author>
</authors>
<title>Axiomatizing dependency parsing using set constraints.</title>
<date>1999</date>
<booktitle>In Sixth Meeting on Mathematics ofLanguage,</booktitle>
<location>Orlando, Florida.</location>
<marker>Duchier, 1999</marker>
<rawString>D. Duchier. 1999. Axiomatizing dependency parsing using set constraints. In Sixth Meeting on Mathematics ofLanguage, Orlando, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
</authors>
<title>The relevance of Tree Adjoining Grammar to generation.</title>
<date>1987</date>
<booktitle>In Natural Language Generation, chapter 16. Martinus Jijhoff Publishers,</booktitle>
<location>Dordrecht, Holland.</location>
<marker>Joshi, 1987</marker>
<rawString>A. Joshi. 1987. The relevance of Tree Adjoining Grammar to generation. In Natural Language Generation, chapter 16. Martinus Jijhoff Publishers, Dordrecht, Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kallmeyer</author>
</authors>
<title>Tree Description Grammars and Underspeci�ed Representations.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit¨at T¨ubingen.</institution>
<contexts>
<context position="2762" citStr="Kallmeyer, 1999" startWordPosition="445" endWordPosition="446">el generator can be used to enumerate the models satisfying the bag of lexical items selected by the lexical look up phase on the basis of the input semantics. How can we design model generators which work efficiently on natural language input i.e. on the type of information delivered by logic based grammars? (Duchier and Gardent, 1999) shows that constraint programming can be used to implement a model generator for tree logic (Backofen et al., 1995). Further, (Duchier and Thater, 1999) shows that this model generator can be used to parse with descriptions based grammars (Rambow et al., 1995; Kallmeyer, 1999) that is, on logic based grammars where lexical entries are descriptions of trees expressed in some tree logic. In this paper, we build on (Duchier and Thater, 1999) and show that modulo some minor modifications, the same model generator can be used to generate with description based grammars. We describe the workings of the algorithm and compare it with standard existing top-down and bottom-up generation algorithms. In specific, we argue that the change of perspective offered by the constraint-based, axiomatic approach to processing presents some interesting differences with the more traditio</context>
<context position="4458" citStr="Kallmeyer, 1999" startWordPosition="710" endWordPosition="711">s that this model can be extended to generate with DGs. In Section 5, we compare our generator with top-down and bottom-up generators, Section 6 reports on a proof-of-concept implementation and Section 7 concludes with pointers for further research. 2 Description Grammars There is a range of grammar formalisms which depart from Tree Adjoining Grammar (TAG) by taking as basic building blocks tree descriptions rather than trees. D-Tree Grammar (DTG) is proposed in (Rambow et al., 1995) to remedy some empirical and theoretical shortcomings of TAG; Tree Description Grammar (TDG) is introduced in (Kallmeyer, 1999) to support syntactic and semantic underspecification and Interaction Grammar is presented in (Perrier, 2000) as an alternative way of formulating linear logic grammars. Like all these frameworks, DG uses tree descriptions and thereby benefits first, from the extended domain of locality which makes TAG particularly suitable for generation (cf. (Joshi,1987)) and second, from the monotonicity which differentiates descriptions from trees with respect to adjunction (cf. (Vijay-Shanker, 1992)). DG differs from DTG and TDG however in that it adopts an axiomatic rather than a generative view of gramm</context>
</contexts>
<marker>Kallmeyer, 1999</marker>
<rawString>L. Kallmeyer. 1999. Tree Description Grammars and Underspeci�ed Representations. Ph.D. thesis, Universit¨at T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Chart generation.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL’96,</booktitle>
<location>Santa Cruz, USA.</location>
<contexts>
<context position="19060" citStr="Kay, 1996" startWordPosition="3160" endWordPosition="3161">mantics. In what follows we show that this combination of features results in a generator which integrates the positive aspects of both top-down and bottom-up generators. In this sense, it is not unlike (Shieber et al., 1990)’s semantic-head-driven generation. As will become clear in the following section however, it differs from it in that it S: NP: VP: VP: V NP: John run V did S: NP: VP: VP: V VP: ran VP: ing matrix will be: integrates stronger lexicalist (i.e. bottom-up) information. 5.1 Bottom-Up Generation Bottom-up or “lexically-driven” generators (e.g., (Shieber, 1988; Whitelock, 1992; Kay, 1996; Carroll et al., 1999)) start from a bag of lexical items with instantiated semantics and generates a syntactic tree by applying grammar rules whose right hand side matches a sequence of phrases in the current input. There are two known disadvantages to bottomup generators. On the one hand, they require that the grammar be semantically monotonic that is, that the semantics of each daughter in a rule subsumes some portion of the mother semantics. On the other hand, they are often overly nondeterministic (though see (Carroll et al., 1999) for an exception). We now show how these problems are de</context>
</contexts>
<marker>Kay, 1996</marker>
<rawString>M. Kay. 1996. Chart generation. In Proceedings of ACL’96, Santa Cruz, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D McCawley</author>
</authors>
<title>Adverbs, Vowels, and other objects of Wonder.</title>
<date>1979</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, Illinois.</location>
<contexts>
<context position="15763" citStr="McCawley, 1979" startWordPosition="2609" endWordPosition="2610"> descriptions providing an appropriate syntactic context for it, e.g. in the tree description for “say”. Multiple Propositions. Lexical entries with a multi-propositional semantics are also very common. For instance, a neo-Davidsonian semantics would associate e.g. with the verb “run” or with the past tensed “ran”. Similarly, agentless passive “be” might be represented by an overt quantification over the missing agent position (such as with a variable over the complement verb semantics). And a grammar with a rich lexical semantics might for instance associate the semantics , with “want” (cf. (McCawley, 1979) which argues for such a semantics to account for examples such as “Reuters wants the report tomorrow” where “tomorrow” modifies the “having” not the “wanting”). Because it assumes that each lexical entry is associated with exactly one semantic proposition, such cases cannot be dealt with the generator sketched in the previous section. A simple method for fixing this problem would be to first partition the input semantics in as many ways as are possible and to then use the resulting partitions as the basis for lexical lookup. The problems with this method are both theoretical and computational</context>
</contexts>
<marker>McCawley, 1979</marker>
<rawString>J. D. McCawley. 1979. Adverbs, Vowels, and other objects of Wonder. University of Chicago Press, Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Perrier</author>
</authors>
<title>Interaction grammars. In</title>
<date>2000</date>
<booktitle>In Proceedings of 18th International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="4567" citStr="Perrier, 2000" startWordPosition="726" endWordPosition="727">d bottom-up generators, Section 6 reports on a proof-of-concept implementation and Section 7 concludes with pointers for further research. 2 Description Grammars There is a range of grammar formalisms which depart from Tree Adjoining Grammar (TAG) by taking as basic building blocks tree descriptions rather than trees. D-Tree Grammar (DTG) is proposed in (Rambow et al., 1995) to remedy some empirical and theoretical shortcomings of TAG; Tree Description Grammar (TDG) is introduced in (Kallmeyer, 1999) to support syntactic and semantic underspecification and Interaction Grammar is presented in (Perrier, 2000) as an alternative way of formulating linear logic grammars. Like all these frameworks, DG uses tree descriptions and thereby benefits first, from the extended domain of locality which makes TAG particularly suitable for generation (cf. (Joshi,1987)) and second, from the monotonicity which differentiates descriptions from trees with respect to adjunction (cf. (Vijay-Shanker, 1992)). DG differs from DTG and TDG however in that it adopts an axiomatic rather than a generative view of grammar: whereas in DTG and TDG, derived trees are constructed through a sequence of rewriting steps, in DG derive</context>
</contexts>
<marker>Perrier, 2000</marker>
<rawString>G. Perrier. 2000. Interaction grammars. In In Proceedings of 18th International Conference on Computational Linguistics (COLING 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Poznanski</author>
<author>J L Beaven</author>
<author>P Whitelock</author>
</authors>
<title>An efficient generation algorithm for lexicalist MT.</title>
<date>1995</date>
<booktitle>In Proceedings ofACL ’95.</booktitle>
<contexts>
<context position="22091" citStr="Poznanski et al., 1995" startWordPosition="3650" endWordPosition="3653">f the output as they do not exhaust the input semantics. (1) The fierce black cat, The fierce little cat, The little black cat, The black cat, The fierce cat, The little cat, The cat. To remedy this shortcoming, various heuristics and parsing strategies have been proposed. (Brew, 1992) combines a constraint-propagation mechanism with a shift-reduce generator, propagating constraints after every reduction step. (Carroll et al., 1999) advocate a two-step generation algorithm in which first, the basic structure of the sentence is generated and second, intersective modifiers are adjoined in. And (Poznanski et al., 1995) make use of a tree reconstruction method which incrementally improves the syntactic tree until it is accepted by the grammar. In effect, the constraint-based encoding of the axiomatic view of generation proposed here takes advantage of Brew’s observation that constraint propagation can be very effective in pruning the search space involved in the generation process. In constraint programming, the solutions to a constraint satisfaction problem (CSP) are found by alternating propagation with distribution steps. Propagation is a process of deterministic inference which fills out the consequences</context>
</contexts>
<marker>Poznanski, Beaven, Whitelock, 1995</marker>
<rawString>V. Poznanski, J. L. Beaven, and P. Whitelock. 1995. An efficient generation algorithm for lexicalist MT. In Proceedings ofACL ’95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Rambow</author>
<author>K Vijay-Shanker</author>
<author>D Weir</author>
</authors>
<title>D-tree Grammars.</title>
<date>1995</date>
<booktitle>In Proceedings ofACL ’95.</booktitle>
<contexts>
<context position="2744" citStr="Rambow et al., 1995" startWordPosition="440" endWordPosition="444">for generation, a model generator can be used to enumerate the models satisfying the bag of lexical items selected by the lexical look up phase on the basis of the input semantics. How can we design model generators which work efficiently on natural language input i.e. on the type of information delivered by logic based grammars? (Duchier and Gardent, 1999) shows that constraint programming can be used to implement a model generator for tree logic (Backofen et al., 1995). Further, (Duchier and Thater, 1999) shows that this model generator can be used to parse with descriptions based grammars (Rambow et al., 1995; Kallmeyer, 1999) that is, on logic based grammars where lexical entries are descriptions of trees expressed in some tree logic. In this paper, we build on (Duchier and Thater, 1999) and show that modulo some minor modifications, the same model generator can be used to generate with description based grammars. We describe the workings of the algorithm and compare it with standard existing top-down and bottom-up generation algorithms. In specific, we argue that the change of perspective offered by the constraint-based, axiomatic approach to processing presents some interesting differences with</context>
<context position="4330" citStr="Rambow et al., 1995" startWordPosition="690" endWordPosition="693">namely, Description Grammars (DG), Section 3 summarises the parsing model presented in (Duchier and Thater, 1999) and Section 4 shows that this model can be extended to generate with DGs. In Section 5, we compare our generator with top-down and bottom-up generators, Section 6 reports on a proof-of-concept implementation and Section 7 concludes with pointers for further research. 2 Description Grammars There is a range of grammar formalisms which depart from Tree Adjoining Grammar (TAG) by taking as basic building blocks tree descriptions rather than trees. D-Tree Grammar (DTG) is proposed in (Rambow et al., 1995) to remedy some empirical and theoretical shortcomings of TAG; Tree Description Grammar (TDG) is introduced in (Kallmeyer, 1999) to support syntactic and semantic underspecification and Interaction Grammar is presented in (Perrier, 2000) as an alternative way of formulating linear logic grammars. Like all these frameworks, DG uses tree descriptions and thereby benefits first, from the extended domain of locality which makes TAG particularly suitable for generation (cf. (Joshi,1987)) and second, from the monotonicity which differentiates descriptions from trees with respect to adjunction (cf. (</context>
</contexts>
<marker>Rambow, Vijay-Shanker, Weir, 1995</marker>
<rawString>O. Rambow, K. Vijay-Shanker, and D. Weir. 1995. D-tree Grammars. In Proceedings ofACL ’95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
<author>F Pereira</author>
<author>G van Noord</author>
<author>R Moore</author>
</authors>
<title>Semantic-head-driven generation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<marker>Shieber, Pereira, van Noord, Moore, 1990</marker>
<rawString>S. Shieber, F. Pereira, G. van Noord, and R. Moore. 1990. Semantic-head-driven generation. Computational Linguistics, 16(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>A Uniform Architecture for Parsing and Generation.</title>
<date>1988</date>
<booktitle>In Proceedings ofACL ’88.</booktitle>
<contexts>
<context position="14801" citStr="Shieber, 1988" startWordPosition="2451" endWordPosition="2452">)) nor with a grammar allowing for trace anchored lexical entries. The mirror restriction for generation is that each lexical entry must be associated with exactly one semantic proposition. The resulting shortcomings are that the generator can deal neither with a lexical entry having an empty semantics nor with a lexical entry having a multipropositional semantics. We first show that these restrictions are too strong. We then show how to adapt the generator so as to lift them. Empty Semantics. Arguably there are words such as “that” or infinitival “to” whose semantic contribution is void. As (Shieber, 1988) showed, the problem with such words is that they cannot be selected on the basis of the input semantics. To circumvent this problem, we take advantage of the TAG extended domain of locality to avoid having such entries in the grammar. For instance, complementizer “that” does not anchor a tree description by itself but occurs in all lexical tree descriptions providing an appropriate syntactic context for it, e.g. in the tree description for “say”. Multiple Propositions. Lexical entries with a multi-propositional semantics are also very common. For instance, a neo-Davidsonian semantics would as</context>
<context position="19032" citStr="Shieber, 1988" startWordPosition="3156" endWordPosition="3157">s and (iii) it assumes a flat semantics. In what follows we show that this combination of features results in a generator which integrates the positive aspects of both top-down and bottom-up generators. In this sense, it is not unlike (Shieber et al., 1990)’s semantic-head-driven generation. As will become clear in the following section however, it differs from it in that it S: NP: VP: VP: V NP: John run V did S: NP: VP: VP: V VP: ran VP: ing matrix will be: integrates stronger lexicalist (i.e. bottom-up) information. 5.1 Bottom-Up Generation Bottom-up or “lexically-driven” generators (e.g., (Shieber, 1988; Whitelock, 1992; Kay, 1996; Carroll et al., 1999)) start from a bag of lexical items with instantiated semantics and generates a syntactic tree by applying grammar rules whose right hand side matches a sequence of phrases in the current input. There are two known disadvantages to bottomup generators. On the one hand, they require that the grammar be semantically monotonic that is, that the semantics of each daughter in a rule subsumes some portion of the mother semantics. On the other hand, they are often overly nondeterministic (though see (Carroll et al., 1999) for an exception). We now sh</context>
</contexts>
<marker>Shieber, 1988</marker>
<rawString>S. Shieber. 1988. A Uniform Architecture for Parsing and Generation. In Proceedings ofACL ’88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Smolka</author>
</authors>
<title>The Oz Programming Model.</title>
<date>1995</date>
<journal>In Computer Science Today,</journal>
<volume>1000</volume>
<pages>ofLNCS.</pages>
<contexts>
<context position="26168" citStr="Smolka, 1995" startWordPosition="4296" endWordPosition="4297">e second rule) is uninstantiated and permits an infinite loop by iterative applications of rules r2 and r3. Such non-termination problems do not arise for the present algorithm as it is lexically driven. So for instance given the corresponding DG fragment for the above grammar and the input semantics , the generator will simply select the tree descriptions for “left”, “John”, “s” and “father” and generate the saturated model satisfying the conjunction of these descriptions. 6 Implementation The ideas presented here have been implemented using the concurrent constraint programming language Oz (Smolka, 1995). The implementation includes a model generator for the tree logic presented in section 2, two lexical lookup modules (one for parsing, one for generation) and a small DG fragment for English which has been tested in parsing and generation mode on a small set of English sentences. This implementation can be seen as a proof of concept for the ideas presented in this paper: it shows how a constraint-based encoding of the type of global constraints suggested by an axiomatic view of grammar can help reduce nondeterminism (few choice points cf. table 5) but performance decreases rapidly with the le</context>
</contexts>
<marker>Smolka, 1995</marker>
<rawString>G. Smolka. 1995. The Oz Programming Model. In Computer Science Today, volume 1000 ofLNCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stone</author>
<author>C Doran</author>
</authors>
<title>Sentence planning as description using Tree-Adjoining Grammar.</title>
<date>1997</date>
<booktitle>In Proceedings ofACL ’97.</booktitle>
<contexts>
<context position="8002" citStr="Stone and Doran, 1997" startWordPosition="1301" endWordPosition="1304">tematic way as follows. Roots of S: NP: VP: VP: sees NP: John V NP: S: NP: S: S: NP: VP: VP: NP: Mary sees fragments (fully specified subtrees) are always positive; except for the anchor, all leaves of fragments are negative, and internal node variables are neutral. This guarantees that in a saturated model, tree fragments that belong to the denotation of distinct tree descriptions do not overlap. Second, we require that every lexical tree description has a single minimal free model, which essentially means that the lexical descriptions must be tree shaped. Semantic representation. Following (Stone and Doran, 1997), we represent meaning using a flat semantic representation, i.e. as multisets, or conjunctions, of non-recursive propositions. This treatment offers a simple syntax-semantics interface in that the meaning of a tree is just the conjunction of meanings of the lexical tree descriptions used to derive it once the free variables occurring in the propositions are instantiated. A free variable is instantiated as follows: each free variable labels a syntactic node variable and is unified with the label of any node variable identified with . For the purpose of this paper, a simple semantic representat</context>
</contexts>
<marker>Stone, Doran, 1997</marker>
<rawString>M. Stone and C. Doran. 1997. Sentence planning as description using Tree-Adjoining Grammar. In Proceedings ofACL ’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
</authors>
<title>Using descriptions of trees in Tree Adjoining Grammars.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="4950" citStr="Vijay-Shanker, 1992" startWordPosition="785" endWordPosition="786"> to remedy some empirical and theoretical shortcomings of TAG; Tree Description Grammar (TDG) is introduced in (Kallmeyer, 1999) to support syntactic and semantic underspecification and Interaction Grammar is presented in (Perrier, 2000) as an alternative way of formulating linear logic grammars. Like all these frameworks, DG uses tree descriptions and thereby benefits first, from the extended domain of locality which makes TAG particularly suitable for generation (cf. (Joshi,1987)) and second, from the monotonicity which differentiates descriptions from trees with respect to adjunction (cf. (Vijay-Shanker, 1992)). DG differs from DTG and TDG however in that it adopts an axiomatic rather than a generative view of grammar: whereas in DTG and TDG, derived trees are constructed through a sequence of rewriting steps, in DG derived trees are models satisfying a conjunction of elementary tree descriptions. Moreover, DG differs from Interaction Grammars in that it uses a flat rather than a Montague style recursive semantics thereby permitting a simple syntax/semantics interface (see below). A Description Grammar is a set of lexical entries of the form where is a tree description and is the semantic represent</context>
</contexts>
<marker>Vijay-Shanker, 1992</marker>
<rawString>K. Vijay-Shanker. 1992. Using descriptions of trees in Tree Adjoining Grammars. Computational Linguistics, 18(4):481–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Whitelock</author>
</authors>
<title>Shake-and-bake translation.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING ’92,</booktitle>
<location>Nantes, France.</location>
<contexts>
<context position="19049" citStr="Whitelock, 1992" startWordPosition="3158" endWordPosition="3159">assumes a flat semantics. In what follows we show that this combination of features results in a generator which integrates the positive aspects of both top-down and bottom-up generators. In this sense, it is not unlike (Shieber et al., 1990)’s semantic-head-driven generation. As will become clear in the following section however, it differs from it in that it S: NP: VP: VP: V NP: John run V did S: NP: VP: VP: V VP: ran VP: ing matrix will be: integrates stronger lexicalist (i.e. bottom-up) information. 5.1 Bottom-Up Generation Bottom-up or “lexically-driven” generators (e.g., (Shieber, 1988; Whitelock, 1992; Kay, 1996; Carroll et al., 1999)) start from a bag of lexical items with instantiated semantics and generates a syntactic tree by applying grammar rules whose right hand side matches a sequence of phrases in the current input. There are two known disadvantages to bottomup generators. On the one hand, they require that the grammar be semantically monotonic that is, that the semantics of each daughter in a rule subsumes some portion of the mother semantics. On the other hand, they are often overly nondeterministic (though see (Carroll et al., 1999) for an exception). We now show how these prob</context>
</contexts>
<marker>Whitelock, 1992</marker>
<rawString>P. Whitelock. 1992. Shake-and-bake translation. In Proceedings of COLING ’92, Nantes, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>