<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023112">
<title confidence="0.988771">
Wrapping of Trees
</title>
<author confidence="0.995438">
James Rogers
</author>
<affiliation confidence="0.822118">
Department of Computer Science
Earlham College
Richmond, IN 47374, USA
</affiliation>
<email confidence="0.998827">
jrogers@cs.earlham.edu
</email>
<sectionHeader confidence="0.9939" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999855428571429">
We explore the descriptive power, in terms of syn-
tactic phenomena, of a formalism that extends Tree-
Adjoining Grammar (TAG) by adding a fourth level
of hierarchical decomposition to the three levels
TAG already employs. While extending the descrip-
tive power minimally, the additional level of decom-
position allows us to obtain a uniform account of a
range of phenomena that has heretofore been dif-
ficult to encompass, an account that employs uni-
tary elementary structures and eschews synchro-
nized derivation operations, and which is, in many
respects, closer to the spirit of the intuitions under-
lying TAG-based linguistic theory than previously
considered extensions to TAG.
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921557692308">
Tree-Adjoining Grammar (TAG) (Joshi and Sch-
abes, 1997; Joshi et al., 1975) is a grammar formal-
ism which comes with a well-developed theory of
natural language syntax (Frank, 2002; Frank, 1992;
Kroch and Joshi, 1985). There are, however, a num-
ber of constructions, many in the core of language,
which present difficulties for the linguistic under-
pinnings of TAG systems, although not necessarily
for the implemented systems themselves. Most of
these involve the combining of trees in ways that are
more complicated than the simple embedding pro-
vided by the tree-adjunction operation.
The most widely studied way of addressing these
constructions within TAG-based linguistic theory
(Kroch and Joshi, 1987; Kroch, 1989; Frank, 2002)
has been to assume some sort of multi-component
adjoining (MCTAG (Weir, 1988)), in which elemen-
tary structures are factored into sets of trees that
are adjoined simultaneously at multiple points. De-
pending on the restrictions placed on where this ad-
joining can occur the effect of such extensions range
from no increase in complexity of either the licensed
tree sets or the computational complexity of pars-
ing, to substantial increases in both. In this paper
we explore these issues within the framework of an
extension of TAG that is conservative in the sense
that it preserves the unitary nature of the elemen-
tary structures and of the adjunction operation and
extends the descriptive power minimally.
While the paper is organized around particular
syntactic phenomena, it is not a study of syntax
itself. We make no attempt to provide a compre-
hensive theory of syntax. In fact, we attempt to
simply instantiate the foundations of existing the-
ory (Frank, 2002) in as faithful a way as possible.
Our primary focus is the interplay between the lin-
guistic theory and the formal language theory. All
of the phenomena we consider can be (and in prac-
tice are (Group, 1998)) handled ad hoc with feature-
structure based TAG (FTAG, (Vijay-Shanker and
Joshi, 1991)). From a practical perspective, the
role of the underlying linguistic theory is, at least
in part, to insure consistent and comprehensive im-
plementation of ad hoc mechanisms. From a theo-
retical perspective, the role of the formal language
framework is, at least in part, to insure coherent and
computationally well-grounded theories. Our over-
all goal is to find formal systems that are as close as
possible to being a direct embodiment of the prin-
ciples guiding the linguistic theory and which are
maximally constrained in their formal and compu-
tational complexity.
</bodyText>
<sectionHeader confidence="0.8924095" genericHeader="introduction">
2 Hierarchical Decomposition of Strings
and Trees
</sectionHeader>
<bodyText confidence="0.999931076923077">
Like many approaches to formalization of natural
language syntax, TAG is based on a hierarchical de-
composition of strings which is represented by or-
dered trees. (Figure 1.) These trees are, in essence,
graphs representing two relationships—the left-to-
right ordering of the structural components of the
string and the relationship between a component
and its immediate constituents.
The distinguishing characteristic of TAG is that it
identifies an additional hierarchical decomposition
of these trees. This shows up, for instance when a
clause which has the form of a wh-question is em-
bedded as an argument within another clause. In the
</bodyText>
<figureCaption confidence="0.991027">
Figure 2: Bridge verbs and subj-aux inversion.
</figureCaption>
<figure confidence="0.978315571428571">
IP
CP
I’
DP
like
t
t V DP
</figure>
<figureCaption confidence="0.997733">
Figure 1: Wh-movement and subj-aux inversion.
</figureCaption>
<figure confidence="0.978070779411765">
I
Alice
VP
V
does
DP
IP
I’
I
who
DP
does
DP
C’
I
VP
like
Alice
Bob
CP
IP
DP
who
I
IP
I
DP
does
DP
CP
does
Carol
Carol
I
VP
I
VP
t
V
think
C
IP
DP
who
IP
t
V
that
DP
think
Alice
I
VP
C
DP
does
V
DP
Alice
like
t
I
VP
does
V
DP
like
t
</figure>
<bodyText confidence="0.958947697674419">
wh-form (as in the right-hand tree of Figure 1), one
of the arguments of the verb is fronted as a wh-word
and the inflectional element (does, in this case) pre-
cedes the subject. This is generally known in the lit-
erature as wh-movement and subj-aux inversion, but
TAG does not necessarily assume there is any ac-
tual transformational movement involved, only that
there is a systematic relationship between the wh-
form and the canonical configuration. The ‘ ’s in the
trees mark the position of the corresponding compo-
nents in the canonical trees.1
When such a clause occurs as the argument of
a bridge verb (such as think or believe) it is split,
with the wh-word appearing to the left of the matrix
clause and the rest of the subordinate clause occur-
ring to the right (Figure 2). Standardly, TAG ac-
counts analyze this as insertion of the tree for the
matrix clause between the upper an lower portions
&apos;This systematic relationship between the wh-form and the
canonical configuration has been a fundamental component of
syntactic theories dating back, at least, to the work of Harris in
the ’50’s.
of the tree for the embedded clause, an operation
known as tree-adjunction. In effect, the tree for
the embedded clause is wrapped around that of the
matrix clause. This process may iterate, with ad-
junction of arbitrarily many instances of bridge verb
trees:
Who does Bob believe ... Carol thinks that Al-
ice likes.
One of the key advantages of this approach is that
the wh-word is introduced into the derivation within
the same elementary structure as the verb it is an ar-
gument of. Hence these structures are semantically
coherent—they express all and only the structural
relationships between the elements of a single func-
tional domain (Frank, 2002). The adjoined struc-
tures are similarly coherent and the derivation pre-
serves that coherence at all stages.
Following Rogers (2003) we will represent this
by connecting the adjoined tree to the point at which
it adjoins via a third, “tree constituency” relation as
in the right hand part of Figure 2. This gives us
</bodyText>
<figure confidence="0.999623909090909">
Alice
DP
does
IP
I
seem
V
VP
ro V DP
I
like
VP
Bob
DP
who
does
CP
I
Alice
DP
IP
I
r
seem
V
VP
ro
I
like
V
VP
DP
r
</figure>
<figureCaption confidence="0.999982">
Figure 3: Raising verbs.
</figureCaption>
<bodyText confidence="0.9999895625">
structures that we usually conceptualize as three-
dimensional trees, but which can simply be regarded
as graphs with three sorts of edges, one for each
of the hierarchical relations expressed by the struc-
tures. Within this context, tree-adjunction is a pro-
cess of concatenating these structures, identifying
the root of the adjoined structure with the point at
which it is adjoined.2
The resulting complex structures are formally
equivalent to the derivation trees in standard for-
malizations of TAG. The derived tree is obtained by
concatenating the tree yield of the structure analo-
gously to the way that the string yield of a deriva-
tion tree is concatenated to form the derived string
of a context-free grammar. Note that in this case it is
essential to identify the point in the frontier of each
tree component at which the components it domi-
nates will be attached. This point is referred to as
the foot of the tree and the path to it from the root is
referred to as the (principal) spine of the tree. Here
we have marked the spines by doubling the corre-
sponding edges of the graphs.
Following Rogers (2002), we will treat the sub-
ject of the clause as if it were “adjoined” into the
rest of the clause at the root of the . At this point,
this is for purely theory-internal reasons—it will al-
low us to exploit the additional formal power we
will shortly bring to bear. It should be noted that
it does not represent ordinary adjunction. The sub-
ject originates in the same elementary structure as
the rest of the clause, it is just a somewhat richer
structure than the more standard tree.
</bodyText>
<sectionHeader confidence="0.86039" genericHeader="method">
3 Raising Verbs and Subj-Aux Inversion
</sectionHeader>
<bodyText confidence="0.998947">
A problem arises, for this account, when the matrix
verb is a raising verb, such as seems or appears as
in
2Context-free derivation can be viewed as a similar process
of concatenating trees.
Alice seems to like Bob
Who does Alice seem to like
Here the matrix clause and the embedded clause
share, in some sense, the same subject argument.
(Figure 3.) Raising verbs are distinguished, further,
from the control verbs (such as want or promise) in
the fact that they may realize their subject as an ex-
pletive it:
It seems Alice likes Bob.
Note, in particular, that in each of these cases the
inflection is carried by the matrix clause. In order to
maintain semantic coherence, we will assume that
the subject originates in the elementary structure of
the embedded clause. This, then, interprets the rais-
ing verb as taking an to an , adjoining at the
between the subject and the inflectional element of
the embedded clause (as in the left-hand side of Fig-
ure 3).
For the declarative form this provides a nesting of
the trees similar to that of the bridge verbs; the em-
bedded clause tree is wrapped around that of the ma-
trix clause. For the wh-form, however, the wrapping
pattern is more complex. Since who and Alice must
originate in the same elementary structure as like,
while does must originate in the same elementary
structure as seem, the trees evidently must factor
and be interleaved as shown in the right-hand side of
the figure. Such a wrapping pattern is not possible
in ordinary TAG. The sequences of labels occurring
along the spines of TAG tree sets must form context-
free languages (Weir, 1988). Hence the “center-
embedded” wrapping patterns of the bridge verbs
and the declarative form of the raising verbs are pos-
sible but the “cross-serial” pattern of the wh-form of
the raising verbs is not.
</bodyText>
<figureCaption confidence="0.994166">
Figure 4: An higher-order account.
</figureCaption>
<figure confidence="0.998492847457627">
CP
CP
I
DP
IP
does
who
IP
DP
DP
I
VP
I
who
Alice
t
does
I
V
DP
VP
to
seem
Alice
VP
V
IP
I
DP
like
t
t
I
CP DP
V
does
Alice
seem
DP
I
VP
I
VP
who
t
to
I
VP
V
V
to
DP
seem
like
t
V
DP
like
t
</figure>
<sectionHeader confidence="0.980844" genericHeader="method">
4 Higher-order Decomposition
</sectionHeader>
<bodyText confidence="0.999980833333333">
One approach to obtaining the more complicated
wrapping pattern that occurs in the wh-form of the
raising verb trees is to move to a formalism in which
the spine languages of the derived trees are TALs
(the string languages derived by TAGs), which can
describe such patterns. One such formalism is the
third level of Weir’s Control Language Hierarchy
(Weir, 1992) which admits sets of derivation trees
generated by CFGs which are filtered by a require-
ment that the sequences of labels on the spines oc-
cur in some particular TAL.3 The problem with this
approach is that it abandons the notion of semantic
coherence of the elementary structures.
It turns out, however, that one can generate ex-
actly the same tree sets if one moves to a for-
malism in which another level of hierarchical de-
composition is introduced (Rogers, 2003). This
now gives structures which employ four hierarchical
relations—the fourth representing the constituency
relation encoding a hierarchical decomposition of
the third-level structures. In this framework, the
seem structure can be taken to be inserted between
the subject and the rest of the like structure as shown
in Figure 4. Again, spines are marked by doubling
</bodyText>
<footnote confidence="0.944089">
3TAG is equivalent to the second level of this hierarchy, in
which the spine languages are Context-Free.
</footnote>
<bodyText confidence="0.994345125">
the edges.
The third-order yield of the corresponding de-
rived structure now wraps the third-order like struc-
ture around that of the seem structure, with the frag-
ment of like that contains the subject attaching at
the third-order “foot” node in the tree-yield of the
seem structure (the ) as shown at the bottom of the
figure. The center-embedding wrapping pattern of
these third-order spines guarantees that the wrap-
ping pattern of spines of the tree yield will be a
TAL, in particular, the “cross-serial” pattern needed
by raising of wh-form structures.
The fourth-order structure has the added benefit
of clearly justifying the status of the like structure as
a single elementary structure despite of the apparent
extraction of the subject along the third relation.
</bodyText>
<sectionHeader confidence="0.992443" genericHeader="method">
5 Locality Effects
</sectionHeader>
<bodyText confidence="0.942898666666667">
Note that it is the to recursion along the third-
order spine of the seem structure that actually does
the raising of the subject. One of the consequences
of this is that that-trace violations, such as
Who does Alice seem that does like .
cannot occur. If the complementizer originates in
the seem structure, it will occur under the . If it
originates in the like tree it will occur in a similar
position between the CP and the . In either case,
</bodyText>
<figureCaption confidence="0.999661">
Figure 5: Expletive it.
</figureCaption>
<figure confidence="0.9992443">
CP
IP
DP
IP
it
that
C
DP
Alice
does
I
VP
I
V
seem
does
V
DP
like
Bob
</figure>
<bodyText confidence="0.982282">
the complementizer must precede the raised subject
in the derived string.
If we fill the subject position of the seem struc-
ture with expletive it, as in Figure 5, the position
in the yield of the structure is occupied and we no
longer have to recursion. This motivates analyz-
ing these structures as to recursion, similar to
bridge verbs, rather than to . (Figure 5.) More
importantly the presence of the expletive subject in
the seem tree rules out super-raising violations such
as
Alice does appear it seems does like Bob.
No matter how the seem structure is interpreted, if it
is to raise Alice then the Alice structure will have to
settle somewhere in its yield. Without extending the
seem structure to include the position, none of the
possible positions will yield the correct string (and
all can be ruled out on simple structural grounds). If
the seem structure is extended to include the , the
raising will be ruled out on the assumption that the
structure must attach at .
</bodyText>
<sectionHeader confidence="0.9953" genericHeader="method">
6 Subject-Object Asymmetry
</sectionHeader>
<bodyText confidence="0.998727788461539">
Another phenomenon that has proved problematic
for standard TAG accounts is extraction from nomi-
nals, such as
Who did Alice publish a picture of .
Here the wh-word is an argument of the preposi-
tional phrase in the object nominal picture of. Ap-
parently, the tree structure involves wrapping of the
picture tree around the publish tree. (See Figure 6.)
The problem, as normally analyzed (Frank, 2002;
Kroch, 1989), is that the the publish tree does have
the recursive structure normally assumed for auxil-
iary trees. We will take a somewhat less strict view
and rule out the adjunction of the publish tree sim-
ply on the grounds that it would involve attaching a
structure rooted in (or possibly CP) to a DP node.
The usual way around this difficulty has been to
assume that the who is introduced in the publish
tree, corresponding, presumably, to the as yet miss-
ing DP. The picture tree is then factored into two
components, an isolated DP node which adjoins at
the wh-DP, establishing its connection to the argu-
ment trace, and the picture DP which combines at
the object position of publish.
This seems to at least test the spirit of the seman-
tic coherence requirement. If the who is not extra-
neous in the publish tree then it must be related in
some way to the object position. But the identity of
who is ultimately not the object of publish (a pic-
ture) but rather the object of the embedded preposi-
tion (the person the picture is of).
If we analyze this in terms of a fourth hierarchi-
cal relation, we can allow the who to originate in
the picture structure, which would now be rooted
in CP. This could be allowed to attach at the root
of the publish structure on the assumption that it is
a C-node of some sort, providing the wrapping of
its tree-yield around that of the publish. (See Fig-
ure 6.) Thus we get an account with intact elemen-
tary structures which are unquestionably semanti-
cally coherent.
One of the striking characteristics of extraction of
this sort is the asymmetry between extraction from
the object, which is acceptable, and extraction from
the subject, which is not:
Who did a picture of illustrate the point.
In the account under consideration, we might con-
template a similar combination of structures, but in
this case the picture DP has to somehow migrate up
to combine at the subject position. Under our as-
sumption that the subject structure is attached to the
illustrate tree via the third relation, this would re-
quire the subject structure to, in effect, have two
</bodyText>
<figure confidence="0.988689428571429">
Alice does it seems does like Bob.
CP
CP
DP
DP
DP
IP
who
who
IP
DP
PP
IP
IP
DP
a picture
did
did
P
DP
I
DP
VP
of
t
t
V
VP
DP
IP I
PP
illustrate
the point
DP
CP
IP
DP
a picture
P
t
V
DP
illustrate
t the point
of
I
DP
VP
did
DP
who
t
DP
V
DP
PP
a picture
illustrate
the point
P
DP
of
t
</figure>
<figureCaption confidence="0.998376">
Figure 7: Extraction from subject nominal.
</figureCaption>
<figure confidence="0.999519746031746">
CP
DP
DP
CP
DP
who
who
IP
CP DP IP
PP
IP
IP
DP
a picture
did
did
Alice
P
DP
I
of
DP
Alice
VP
t
t
IP
I
VP
V
t
publish
DP
CP
IP
DP
V
DP
did
Alice
publish
DP
I
VP
DP
PP
who
t
a picture
DP
PP
V
DP
P
apicture
publish
of
DP
P
t
DP
of
t
</figure>
<figureCaption confidence="0.99993">
Figure 6: Extraction from object nominal.
</figureCaption>
<bodyText confidence="0.999965714285714">
feet, an extension that strictly increases the gen-
erative power of the formalism. Alternatively, we
might assume that the picture structure attaches in
the yield of the illustrate structure or between the
main part of the structure and the subject tree, but
either of these would fail to promote the who to the
root of the yield structure.
</bodyText>
<sectionHeader confidence="0.992552" genericHeader="evaluation">
7 Processing
</sectionHeader>
<bodyText confidence="0.999925">
As with any computationally oriented formalism,
the ability to define the correct set of structures is
only one aspect of the problem. Just as important
is the question of the complexity of processing lan-
guage relative to that definition. Fortunately, the
languages of the Control Language Hierarchy are
well understood and recognition algorithms, based
on a CKY-style dynamic programming approach,
are know for each level. The time complexity of
the algorithm for the level, as a function of the
length of the input ( ), is (Palis and
Shende, 1992). In the case of the fourth-order gram-
mars, which correspond to the third level of the
CLH, this gives an upper bound of .
While, strictly speaking, this is a feasible time
complexity, in practice we expect that approaches
with better average-case complexity, such as Early-
style algorithms, will be necessary if these gram-
mars are to be parsed directly. But, as we noted
in the introduction, grammars of this complexity
are not necessarily intended to be used as working
grammars. Rather they are mechanisms for express-
ing the linguistic theory serving as the foundation of
working grammars of more practical complexity.
Since all of our proposed use of the higher-order
relations involve either combining at a root (with-
out properly embedding) or embedding with finitely
bounded depth of nesting, the effect of the higher-
dimensional combining operations are expressible
using a finite set of features. Hence, the sets of
derived trees can be generated by adding finitely
many features to ordinary TAGs and the theory en-
tailed by our accounts of these phenomena (as ex-
pressed in the sets of derived trees) is expressible in
FTAG. Thus, a complete theory of syntax incorpo-
rating them would be (not necessarily not) compati-
ble with implementation within existing TAG-based
systems. A more long term goal is to implement
a compilation mechanism which will translate the
linguistic theory, stated in terms of the hierarchical
relations, directly into grammars stated in terms of
the existing TAG-based systems.
</bodyText>
<sectionHeader confidence="0.995388" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99999508">
In many ways the formalism we have working with
is a minimal extension of ordinary TAGs. Formally,
the step from TAG to add the fourth hierarchical re-
lation is directly analogous to the step from CFG
to TAG. Moreover, while the graphs describing the
derived structures are often rather complicated, con-
ceptually they involve reasoning in terms of only a
single additional relation. The benefit of the added
complexity is a uniform account of a range of phe-
nomena that has heretofore been difficult to encom-
pass, an account that employs unitary elementary
structures and eschews synchronized derivation op-
erations, and which is, in many respects, closer to
the spirit of the intuitions underlying TAG-based
linguistic theory than previously considered exten-
sions to TAG.
While it is impossible to determine how compre-
hensive the coverage of a more fully developed the-
ory of syntax based on this formalism will be with-
out actually completing such a theory, we believe
that the results presented here suggest that the uni-
formity provided by adding this fourth level of de-
composition to our vocabulary is likely to more than
compensate for the added complexity of the fourth
level elementary structures.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999956846153847">
Robert Evan Frank. 1992. Syntactic Locality and
Tree Adjoining Grammar: Grammatical, Acqui-
sition and Processing Perspectives. Ph.D. disser-
tation, Univ. of Penn.
Robert Frank. 2002. Phrase Structure Composition
and Syntactic Dependencies. MIT Press.
The XTAG Research Group. 1998. A lexical-
ized tree adjoining grammar for english. Tech-
nical Report IRCS-98-18, Institute for Research
in Cognitive Science.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Handbook of Formal
Languages and Automata, volume 3, pages 69–
123. Springer-Verlag.
Aravind K. Joshi, Leon Levy, and Masako Taka-
hashi. 1975. Tree adjunct grammars. Journal of
the Computer and Systems Sciences, 10:136–163.
Anthony Kroch and Aravind K. Joshi. 1985. The
linquistic relevance of tree adjoining grammar.
Technical Report MS-CS-85-16, Dept. of Com-
puter and Information Sciences.
Anthony S. Kroch and Aravind K. Joshi. 1987. An-
alyzing extraposition in a tree adjoining grammar.
In Syntax and Semantics, pages 107–149. Aca-
demic Press. Vol. 20.
Anthony Kroch. 1989. Asymmetries in long dis-
tance extraction in a tree adjoining grammar. In
Mark Baltin and Anthony Kroch, editors, Alter-
native Conceptions of Phrase Structure, pages
66–98. University of Chicago Press.
Michael A. Palis and Sunil M. Shende. 1992. Up-
per bounds on recognition of a hierarchy of non-
context-free languages. Theoretical Computer
Science, 98:289–319.
James Rogers. 2002. One more perspective on se-
mantic relations in TAG. In Proceedings of the
Sixth International Workshop on Tree Adjoining
Grammars and Related Frameworks, Venice, IT,
May.
James Rogers. 2003. Syntactic structures as multi-
dimensional trees. Research on Language and
Computation, 1(3–4):265–305.
K. Vijay-Shanker and Aravind K. Joshi. 1991.
Unification based tree adjoining grammars. In
J. Wedekind, editor, Unification-based Gram-
mars. MIT Press, Cambridge, MA.
David J. Weir. 1988. Characterizing Mildly
Context-Sensitive Grammar Formalisms. Ph.D.
thesis, University of Pennsylvania.
David J. Weir. 1992. A geometric hierarchy be-
yond context-free languages. Theoretical Com-
puter Science, 104:235–261.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930487">
<title confidence="0.999844">Wrapping of Trees</title>
<author confidence="0.999737">James Rogers</author>
<affiliation confidence="0.9981985">Department of Computer Science Earlham College</affiliation>
<address confidence="0.999181">Richmond, IN 47374, USA</address>
<email confidence="0.999859">jrogers@cs.earlham.edu</email>
<abstract confidence="0.995525333333333">We explore the descriptive power, in terms of syntactic phenomena, of a formalism that extends Tree- Adjoining Grammar (TAG) by adding a fourth level of hierarchical decomposition to the three levels TAG already employs. While extending the descriptive power minimally, the additional level of decomposition allows us to obtain a uniform account of a range of phenomena that has heretofore been difficult to encompass, an account that employs unitary elementary structures and eschews synchronized derivation operations, and which is, in many respects, closer to the spirit of the intuitions underlying TAG-based linguistic theory than previously considered extensions to TAG.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robert Evan Frank</author>
</authors>
<title>Syntactic Locality and Tree Adjoining Grammar: Grammatical, Acquisition and Processing Perspectives.</title>
<date>1992</date>
<institution>Univ. of Penn.</institution>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="1015" citStr="Frank, 1992" startWordPosition="154" endWordPosition="155">e additional level of decomposition allows us to obtain a uniform account of a range of phenomena that has heretofore been difficult to encompass, an account that employs unitary elementary structures and eschews synchronized derivation operations, and which is, in many respects, closer to the spirit of the intuitions underlying TAG-based linguistic theory than previously considered extensions to TAG. 1 Introduction Tree-Adjoining Grammar (TAG) (Joshi and Schabes, 1997; Joshi et al., 1975) is a grammar formalism which comes with a well-developed theory of natural language syntax (Frank, 2002; Frank, 1992; Kroch and Joshi, 1985). There are, however, a number of constructions, many in the core of language, which present difficulties for the linguistic underpinnings of TAG systems, although not necessarily for the implemented systems themselves. Most of these involve the combining of trees in ways that are more complicated than the simple embedding provided by the tree-adjunction operation. The most widely studied way of addressing these constructions within TAG-based linguistic theory (Kroch and Joshi, 1987; Kroch, 1989; Frank, 2002) has been to assume some sort of multi-component adjoining (MC</context>
</contexts>
<marker>Frank, 1992</marker>
<rawString>Robert Evan Frank. 1992. Syntactic Locality and Tree Adjoining Grammar: Grammatical, Acquisition and Processing Perspectives. Ph.D. dissertation, Univ. of Penn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Frank</author>
</authors>
<title>Phrase Structure Composition and Syntactic Dependencies.</title>
<date>2002</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1002" citStr="Frank, 2002" startWordPosition="152" endWordPosition="153">minimally, the additional level of decomposition allows us to obtain a uniform account of a range of phenomena that has heretofore been difficult to encompass, an account that employs unitary elementary structures and eschews synchronized derivation operations, and which is, in many respects, closer to the spirit of the intuitions underlying TAG-based linguistic theory than previously considered extensions to TAG. 1 Introduction Tree-Adjoining Grammar (TAG) (Joshi and Schabes, 1997; Joshi et al., 1975) is a grammar formalism which comes with a well-developed theory of natural language syntax (Frank, 2002; Frank, 1992; Kroch and Joshi, 1985). There are, however, a number of constructions, many in the core of language, which present difficulties for the linguistic underpinnings of TAG systems, although not necessarily for the implemented systems themselves. Most of these involve the combining of trees in ways that are more complicated than the simple embedding provided by the tree-adjunction operation. The most widely studied way of addressing these constructions within TAG-based linguistic theory (Kroch and Joshi, 1987; Kroch, 1989; Frank, 2002) has been to assume some sort of multi-component </context>
<context position="2516" citStr="Frank, 2002" startWordPosition="395" endWordPosition="396">censed tree sets or the computational complexity of parsing, to substantial increases in both. In this paper we explore these issues within the framework of an extension of TAG that is conservative in the sense that it preserves the unitary nature of the elementary structures and of the adjunction operation and extends the descriptive power minimally. While the paper is organized around particular syntactic phenomena, it is not a study of syntax itself. We make no attempt to provide a comprehensive theory of syntax. In fact, we attempt to simply instantiate the foundations of existing theory (Frank, 2002) in as faithful a way as possible. Our primary focus is the interplay between the linguistic theory and the formal language theory. All of the phenomena we consider can be (and in practice are (Group, 1998)) handled ad hoc with featurestructure based TAG (FTAG, (Vijay-Shanker and Joshi, 1991)). From a practical perspective, the role of the underlying linguistic theory is, at least in part, to insure consistent and comprehensive implementation of ad hoc mechanisms. From a theoretical perspective, the role of the formal language framework is, at least in part, to insure coherent and computationa</context>
<context position="6160" citStr="Frank, 2002" startWordPosition="1031" endWordPosition="1032"> operation known as tree-adjunction. In effect, the tree for the embedded clause is wrapped around that of the matrix clause. This process may iterate, with adjunction of arbitrarily many instances of bridge verb trees: Who does Bob believe ... Carol thinks that Alice likes. One of the key advantages of this approach is that the wh-word is introduced into the derivation within the same elementary structure as the verb it is an argument of. Hence these structures are semantically coherent—they express all and only the structural relationships between the elements of a single functional domain (Frank, 2002). The adjoined structures are similarly coherent and the derivation preserves that coherence at all stages. Following Rogers (2003) we will represent this by connecting the adjoined tree to the point at which it adjoins via a third, “tree constituency” relation as in the right hand part of Figure 2. This gives us Alice DP does IP I seem V VP ro V DP I like VP Bob DP who does CP I Alice DP IP I r seem V VP ro I like V VP DP r Figure 3: Raising verbs. structures that we usually conceptualize as threedimensional trees, but which can simply be regarded as graphs with three sorts of edges, one for </context>
<context position="14326" citStr="Frank, 2002" startWordPosition="2485" endWordPosition="2486">all can be ruled out on simple structural grounds). If the seem structure is extended to include the , the raising will be ruled out on the assumption that the structure must attach at . 6 Subject-Object Asymmetry Another phenomenon that has proved problematic for standard TAG accounts is extraction from nominals, such as Who did Alice publish a picture of . Here the wh-word is an argument of the prepositional phrase in the object nominal picture of. Apparently, the tree structure involves wrapping of the picture tree around the publish tree. (See Figure 6.) The problem, as normally analyzed (Frank, 2002; Kroch, 1989), is that the the publish tree does have the recursive structure normally assumed for auxiliary trees. We will take a somewhat less strict view and rule out the adjunction of the publish tree simply on the grounds that it would involve attaching a structure rooted in (or possibly CP) to a DP node. The usual way around this difficulty has been to assume that the who is introduced in the publish tree, corresponding, presumably, to the as yet missing DP. The picture tree is then factored into two components, an isolated DP node which adjoins at the wh-DP, establishing its connection</context>
</contexts>
<marker>Frank, 2002</marker>
<rawString>Robert Frank. 2002. Phrase Structure Composition and Syntactic Dependencies. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>The XTAG Research Group</author>
</authors>
<title>A lexicalized tree adjoining grammar for english.</title>
<date>1998</date>
<tech>Technical Report IRCS-98-18,</tech>
<institution>Institute for Research in Cognitive Science.</institution>
<contexts>
<context position="2722" citStr="Group, 1998" startWordPosition="433" endWordPosition="434">ense that it preserves the unitary nature of the elementary structures and of the adjunction operation and extends the descriptive power minimally. While the paper is organized around particular syntactic phenomena, it is not a study of syntax itself. We make no attempt to provide a comprehensive theory of syntax. In fact, we attempt to simply instantiate the foundations of existing theory (Frank, 2002) in as faithful a way as possible. Our primary focus is the interplay between the linguistic theory and the formal language theory. All of the phenomena we consider can be (and in practice are (Group, 1998)) handled ad hoc with featurestructure based TAG (FTAG, (Vijay-Shanker and Joshi, 1991)). From a practical perspective, the role of the underlying linguistic theory is, at least in part, to insure consistent and comprehensive implementation of ad hoc mechanisms. From a theoretical perspective, the role of the formal language framework is, at least in part, to insure coherent and computationally well-grounded theories. Our overall goal is to find formal systems that are as close as possible to being a direct embodiment of the principles guiding the linguistic theory and which are maximally cons</context>
</contexts>
<marker>Group, 1998</marker>
<rawString>The XTAG Research Group. 1998. A lexicalized tree adjoining grammar for english. Technical Report IRCS-98-18, Institute for Research in Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Treeadjoining grammars.</title>
<date>1997</date>
<booktitle>In Handbook of Formal Languages and Automata,</booktitle>
<volume>3</volume>
<pages>69--123</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="877" citStr="Joshi and Schabes, 1997" startWordPosition="128" endWordPosition="132">G) by adding a fourth level of hierarchical decomposition to the three levels TAG already employs. While extending the descriptive power minimally, the additional level of decomposition allows us to obtain a uniform account of a range of phenomena that has heretofore been difficult to encompass, an account that employs unitary elementary structures and eschews synchronized derivation operations, and which is, in many respects, closer to the spirit of the intuitions underlying TAG-based linguistic theory than previously considered extensions to TAG. 1 Introduction Tree-Adjoining Grammar (TAG) (Joshi and Schabes, 1997; Joshi et al., 1975) is a grammar formalism which comes with a well-developed theory of natural language syntax (Frank, 2002; Frank, 1992; Kroch and Joshi, 1985). There are, however, a number of constructions, many in the core of language, which present difficulties for the linguistic underpinnings of TAG systems, although not necessarily for the implemented systems themselves. Most of these involve the combining of trees in ways that are more complicated than the simple embedding provided by the tree-adjunction operation. The most widely studied way of addressing these constructions within T</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Aravind K. Joshi and Yves Schabes. 1997. Treeadjoining grammars. In Handbook of Formal Languages and Automata, volume 3, pages 69– 123. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Leon Levy</author>
<author>Masako Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of the Computer and Systems Sciences,</journal>
<pages>10--136</pages>
<contexts>
<context position="898" citStr="Joshi et al., 1975" startWordPosition="133" endWordPosition="136">el of hierarchical decomposition to the three levels TAG already employs. While extending the descriptive power minimally, the additional level of decomposition allows us to obtain a uniform account of a range of phenomena that has heretofore been difficult to encompass, an account that employs unitary elementary structures and eschews synchronized derivation operations, and which is, in many respects, closer to the spirit of the intuitions underlying TAG-based linguistic theory than previously considered extensions to TAG. 1 Introduction Tree-Adjoining Grammar (TAG) (Joshi and Schabes, 1997; Joshi et al., 1975) is a grammar formalism which comes with a well-developed theory of natural language syntax (Frank, 2002; Frank, 1992; Kroch and Joshi, 1985). There are, however, a number of constructions, many in the core of language, which present difficulties for the linguistic underpinnings of TAG systems, although not necessarily for the implemented systems themselves. Most of these involve the combining of trees in ways that are more complicated than the simple embedding provided by the tree-adjunction operation. The most widely studied way of addressing these constructions within TAG-based linguistic t</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Aravind K. Joshi, Leon Levy, and Masako Takahashi. 1975. Tree adjunct grammars. Journal of the Computer and Systems Sciences, 10:136–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Kroch</author>
<author>Aravind K Joshi</author>
</authors>
<title>The linquistic relevance of tree adjoining grammar.</title>
<date>1985</date>
<tech>Technical Report MS-CS-85-16,</tech>
<institution>Dept. of Computer and Information Sciences.</institution>
<contexts>
<context position="1039" citStr="Kroch and Joshi, 1985" startWordPosition="156" endWordPosition="159">level of decomposition allows us to obtain a uniform account of a range of phenomena that has heretofore been difficult to encompass, an account that employs unitary elementary structures and eschews synchronized derivation operations, and which is, in many respects, closer to the spirit of the intuitions underlying TAG-based linguistic theory than previously considered extensions to TAG. 1 Introduction Tree-Adjoining Grammar (TAG) (Joshi and Schabes, 1997; Joshi et al., 1975) is a grammar formalism which comes with a well-developed theory of natural language syntax (Frank, 2002; Frank, 1992; Kroch and Joshi, 1985). There are, however, a number of constructions, many in the core of language, which present difficulties for the linguistic underpinnings of TAG systems, although not necessarily for the implemented systems themselves. Most of these involve the combining of trees in ways that are more complicated than the simple embedding provided by the tree-adjunction operation. The most widely studied way of addressing these constructions within TAG-based linguistic theory (Kroch and Joshi, 1987; Kroch, 1989; Frank, 2002) has been to assume some sort of multi-component adjoining (MCTAG (Weir, 1988)), in wh</context>
</contexts>
<marker>Kroch, Joshi, 1985</marker>
<rawString>Anthony Kroch and Aravind K. Joshi. 1985. The linquistic relevance of tree adjoining grammar. Technical Report MS-CS-85-16, Dept. of Computer and Information Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony S Kroch</author>
<author>Aravind K Joshi</author>
</authors>
<title>Analyzing extraposition in a tree adjoining grammar.</title>
<date>1987</date>
<booktitle>In Syntax and Semantics,</booktitle>
<volume>20</volume>
<pages>107--149</pages>
<publisher>Academic Press.</publisher>
<contexts>
<context position="1526" citStr="Kroch and Joshi, 1987" startWordPosition="230" endWordPosition="233">rammar formalism which comes with a well-developed theory of natural language syntax (Frank, 2002; Frank, 1992; Kroch and Joshi, 1985). There are, however, a number of constructions, many in the core of language, which present difficulties for the linguistic underpinnings of TAG systems, although not necessarily for the implemented systems themselves. Most of these involve the combining of trees in ways that are more complicated than the simple embedding provided by the tree-adjunction operation. The most widely studied way of addressing these constructions within TAG-based linguistic theory (Kroch and Joshi, 1987; Kroch, 1989; Frank, 2002) has been to assume some sort of multi-component adjoining (MCTAG (Weir, 1988)), in which elementary structures are factored into sets of trees that are adjoined simultaneously at multiple points. Depending on the restrictions placed on where this adjoining can occur the effect of such extensions range from no increase in complexity of either the licensed tree sets or the computational complexity of parsing, to substantial increases in both. In this paper we explore these issues within the framework of an extension of TAG that is conservative in the sense that it pre</context>
</contexts>
<marker>Kroch, Joshi, 1987</marker>
<rawString>Anthony S. Kroch and Aravind K. Joshi. 1987. Analyzing extraposition in a tree adjoining grammar. In Syntax and Semantics, pages 107–149. Academic Press. Vol. 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Kroch</author>
</authors>
<title>Asymmetries in long distance extraction in a tree adjoining grammar.</title>
<date>1989</date>
<booktitle>In Mark Baltin and Anthony Kroch, editors, Alternative Conceptions of Phrase Structure,</booktitle>
<pages>66--98</pages>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="1539" citStr="Kroch, 1989" startWordPosition="234" endWordPosition="235">comes with a well-developed theory of natural language syntax (Frank, 2002; Frank, 1992; Kroch and Joshi, 1985). There are, however, a number of constructions, many in the core of language, which present difficulties for the linguistic underpinnings of TAG systems, although not necessarily for the implemented systems themselves. Most of these involve the combining of trees in ways that are more complicated than the simple embedding provided by the tree-adjunction operation. The most widely studied way of addressing these constructions within TAG-based linguistic theory (Kroch and Joshi, 1987; Kroch, 1989; Frank, 2002) has been to assume some sort of multi-component adjoining (MCTAG (Weir, 1988)), in which elementary structures are factored into sets of trees that are adjoined simultaneously at multiple points. Depending on the restrictions placed on where this adjoining can occur the effect of such extensions range from no increase in complexity of either the licensed tree sets or the computational complexity of parsing, to substantial increases in both. In this paper we explore these issues within the framework of an extension of TAG that is conservative in the sense that it preserves the un</context>
<context position="14340" citStr="Kroch, 1989" startWordPosition="2487" endWordPosition="2488">led out on simple structural grounds). If the seem structure is extended to include the , the raising will be ruled out on the assumption that the structure must attach at . 6 Subject-Object Asymmetry Another phenomenon that has proved problematic for standard TAG accounts is extraction from nominals, such as Who did Alice publish a picture of . Here the wh-word is an argument of the prepositional phrase in the object nominal picture of. Apparently, the tree structure involves wrapping of the picture tree around the publish tree. (See Figure 6.) The problem, as normally analyzed (Frank, 2002; Kroch, 1989), is that the the publish tree does have the recursive structure normally assumed for auxiliary trees. We will take a somewhat less strict view and rule out the adjunction of the publish tree simply on the grounds that it would involve attaching a structure rooted in (or possibly CP) to a DP node. The usual way around this difficulty has been to assume that the who is introduced in the publish tree, corresponding, presumably, to the as yet missing DP. The picture tree is then factored into two components, an isolated DP node which adjoins at the wh-DP, establishing its connection to the argume</context>
</contexts>
<marker>Kroch, 1989</marker>
<rawString>Anthony Kroch. 1989. Asymmetries in long distance extraction in a tree adjoining grammar. In Mark Baltin and Anthony Kroch, editors, Alternative Conceptions of Phrase Structure, pages 66–98. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Palis</author>
<author>Sunil M Shende</author>
</authors>
<title>Upper bounds on recognition of a hierarchy of noncontext-free languages.</title>
<date>1992</date>
<journal>Theoretical Computer Science,</journal>
<pages>98--289</pages>
<contexts>
<context position="17919" citStr="Palis and Shende, 1992" startWordPosition="3163" endWordPosition="3166"> fail to promote the who to the root of the yield structure. 7 Processing As with any computationally oriented formalism, the ability to define the correct set of structures is only one aspect of the problem. Just as important is the question of the complexity of processing language relative to that definition. Fortunately, the languages of the Control Language Hierarchy are well understood and recognition algorithms, based on a CKY-style dynamic programming approach, are know for each level. The time complexity of the algorithm for the level, as a function of the length of the input ( ), is (Palis and Shende, 1992). In the case of the fourth-order grammars, which correspond to the third level of the CLH, this gives an upper bound of . While, strictly speaking, this is a feasible time complexity, in practice we expect that approaches with better average-case complexity, such as Earlystyle algorithms, will be necessary if these grammars are to be parsed directly. But, as we noted in the introduction, grammars of this complexity are not necessarily intended to be used as working grammars. Rather they are mechanisms for expressing the linguistic theory serving as the foundation of working grammars of more p</context>
</contexts>
<marker>Palis, Shende, 1992</marker>
<rawString>Michael A. Palis and Sunil M. Shende. 1992. Upper bounds on recognition of a hierarchy of noncontext-free languages. Theoretical Computer Science, 98:289–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Rogers</author>
</authors>
<title>One more perspective on semantic relations in TAG.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth International Workshop on Tree Adjoining Grammars and Related Frameworks,</booktitle>
<location>Venice, IT,</location>
<contexts>
<context position="7723" citStr="Rogers (2002)" startWordPosition="1316" endWordPosition="1317">G. The derived tree is obtained by concatenating the tree yield of the structure analogously to the way that the string yield of a derivation tree is concatenated to form the derived string of a context-free grammar. Note that in this case it is essential to identify the point in the frontier of each tree component at which the components it dominates will be attached. This point is referred to as the foot of the tree and the path to it from the root is referred to as the (principal) spine of the tree. Here we have marked the spines by doubling the corresponding edges of the graphs. Following Rogers (2002), we will treat the subject of the clause as if it were “adjoined” into the rest of the clause at the root of the . At this point, this is for purely theory-internal reasons—it will allow us to exploit the additional formal power we will shortly bring to bear. It should be noted that it does not represent ordinary adjunction. The subject originates in the same elementary structure as the rest of the clause, it is just a somewhat richer structure than the more standard tree. 3 Raising Verbs and Subj-Aux Inversion A problem arises, for this account, when the matrix verb is a raising verb, such a</context>
</contexts>
<marker>Rogers, 2002</marker>
<rawString>James Rogers. 2002. One more perspective on semantic relations in TAG. In Proceedings of the Sixth International Workshop on Tree Adjoining Grammars and Related Frameworks, Venice, IT, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Rogers</author>
</authors>
<title>Syntactic structures as multidimensional trees.</title>
<date>2003</date>
<booktitle>Research on Language and Computation,</booktitle>
<pages>1--3</pages>
<contexts>
<context position="6291" citStr="Rogers (2003)" startWordPosition="1051" endWordPosition="1052"> process may iterate, with adjunction of arbitrarily many instances of bridge verb trees: Who does Bob believe ... Carol thinks that Alice likes. One of the key advantages of this approach is that the wh-word is introduced into the derivation within the same elementary structure as the verb it is an argument of. Hence these structures are semantically coherent—they express all and only the structural relationships between the elements of a single functional domain (Frank, 2002). The adjoined structures are similarly coherent and the derivation preserves that coherence at all stages. Following Rogers (2003) we will represent this by connecting the adjoined tree to the point at which it adjoins via a third, “tree constituency” relation as in the right hand part of Figure 2. This gives us Alice DP does IP I seem V VP ro V DP I like VP Bob DP who does CP I Alice DP IP I r seem V VP ro I like V VP DP r Figure 3: Raising verbs. structures that we usually conceptualize as threedimensional trees, but which can simply be regarded as graphs with three sorts of edges, one for each of the hierarchical relations expressed by the structures. Within this context, tree-adjunction is a process of concatenating </context>
<context position="11124" citStr="Rogers, 2003" startWordPosition="1936" endWordPosition="1937">g languages derived by TAGs), which can describe such patterns. One such formalism is the third level of Weir’s Control Language Hierarchy (Weir, 1992) which admits sets of derivation trees generated by CFGs which are filtered by a requirement that the sequences of labels on the spines occur in some particular TAL.3 The problem with this approach is that it abandons the notion of semantic coherence of the elementary structures. It turns out, however, that one can generate exactly the same tree sets if one moves to a formalism in which another level of hierarchical decomposition is introduced (Rogers, 2003). This now gives structures which employ four hierarchical relations—the fourth representing the constituency relation encoding a hierarchical decomposition of the third-level structures. In this framework, the seem structure can be taken to be inserted between the subject and the rest of the like structure as shown in Figure 4. Again, spines are marked by doubling 3TAG is equivalent to the second level of this hierarchy, in which the spine languages are Context-Free. the edges. The third-order yield of the corresponding derived structure now wraps the third-order like structure around that of</context>
</contexts>
<marker>Rogers, 2003</marker>
<rawString>James Rogers. 2003. Syntactic structures as multidimensional trees. Research on Language and Computation, 1(3–4):265–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>Aravind K Joshi</author>
</authors>
<title>Unification based tree adjoining grammars.</title>
<date>1991</date>
<editor>In J. Wedekind, editor, Unification-based Grammars.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2809" citStr="Vijay-Shanker and Joshi, 1991" startWordPosition="444" endWordPosition="447">s and of the adjunction operation and extends the descriptive power minimally. While the paper is organized around particular syntactic phenomena, it is not a study of syntax itself. We make no attempt to provide a comprehensive theory of syntax. In fact, we attempt to simply instantiate the foundations of existing theory (Frank, 2002) in as faithful a way as possible. Our primary focus is the interplay between the linguistic theory and the formal language theory. All of the phenomena we consider can be (and in practice are (Group, 1998)) handled ad hoc with featurestructure based TAG (FTAG, (Vijay-Shanker and Joshi, 1991)). From a practical perspective, the role of the underlying linguistic theory is, at least in part, to insure consistent and comprehensive implementation of ad hoc mechanisms. From a theoretical perspective, the role of the formal language framework is, at least in part, to insure coherent and computationally well-grounded theories. Our overall goal is to find formal systems that are as close as possible to being a direct embodiment of the principles guiding the linguistic theory and which are maximally constrained in their formal and computational complexity. 2 Hierarchical Decomposition of S</context>
</contexts>
<marker>Vijay-Shanker, Joshi, 1991</marker>
<rawString>K. Vijay-Shanker and Aravind K. Joshi. 1991. Unification based tree adjoining grammars. In J. Wedekind, editor, Unification-based Grammars. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Weir</author>
</authors>
<title>Characterizing Mildly Context-Sensitive Grammar Formalisms.</title>
<date>1988</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1631" citStr="Weir, 1988" startWordPosition="248" endWordPosition="249">ch and Joshi, 1985). There are, however, a number of constructions, many in the core of language, which present difficulties for the linguistic underpinnings of TAG systems, although not necessarily for the implemented systems themselves. Most of these involve the combining of trees in ways that are more complicated than the simple embedding provided by the tree-adjunction operation. The most widely studied way of addressing these constructions within TAG-based linguistic theory (Kroch and Joshi, 1987; Kroch, 1989; Frank, 2002) has been to assume some sort of multi-component adjoining (MCTAG (Weir, 1988)), in which elementary structures are factored into sets of trees that are adjoined simultaneously at multiple points. Depending on the restrictions placed on where this adjoining can occur the effect of such extensions range from no increase in complexity of either the licensed tree sets or the computational complexity of parsing, to substantial increases in both. In this paper we explore these issues within the framework of an extension of TAG that is conservative in the sense that it preserves the unitary nature of the elementary structures and of the adjunction operation and extends the de</context>
<context position="9845" citStr="Weir, 1988" startWordPosition="1692" endWordPosition="1693">ides a nesting of the trees similar to that of the bridge verbs; the embedded clause tree is wrapped around that of the matrix clause. For the wh-form, however, the wrapping pattern is more complex. Since who and Alice must originate in the same elementary structure as like, while does must originate in the same elementary structure as seem, the trees evidently must factor and be interleaved as shown in the right-hand side of the figure. Such a wrapping pattern is not possible in ordinary TAG. The sequences of labels occurring along the spines of TAG tree sets must form contextfree languages (Weir, 1988). Hence the “centerembedded” wrapping patterns of the bridge verbs and the declarative form of the raising verbs are possible but the “cross-serial” pattern of the wh-form of the raising verbs is not. Figure 4: An higher-order account. CP CP I DP IP does who IP DP DP I VP I who Alice t does I V DP VP to seem Alice VP V IP I DP like t t I CP DP V does Alice seem DP I VP I VP who t to I VP V V to DP seem like t V DP like t 4 Higher-order Decomposition One approach to obtaining the more complicated wrapping pattern that occurs in the wh-form of the raising verb trees is to move to a formalism in </context>
</contexts>
<marker>Weir, 1988</marker>
<rawString>David J. Weir. 1988. Characterizing Mildly Context-Sensitive Grammar Formalisms. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Weir</author>
</authors>
<title>A geometric hierarchy beyond context-free languages.</title>
<date>1992</date>
<journal>Theoretical Computer Science,</journal>
<pages>104--235</pages>
<contexts>
<context position="10662" citStr="Weir, 1992" startWordPosition="1855" endWordPosition="1856">e 4: An higher-order account. CP CP I DP IP does who IP DP DP I VP I who Alice t does I V DP VP to seem Alice VP V IP I DP like t t I CP DP V does Alice seem DP I VP I VP who t to I VP V V to DP seem like t V DP like t 4 Higher-order Decomposition One approach to obtaining the more complicated wrapping pattern that occurs in the wh-form of the raising verb trees is to move to a formalism in which the spine languages of the derived trees are TALs (the string languages derived by TAGs), which can describe such patterns. One such formalism is the third level of Weir’s Control Language Hierarchy (Weir, 1992) which admits sets of derivation trees generated by CFGs which are filtered by a requirement that the sequences of labels on the spines occur in some particular TAL.3 The problem with this approach is that it abandons the notion of semantic coherence of the elementary structures. It turns out, however, that one can generate exactly the same tree sets if one moves to a formalism in which another level of hierarchical decomposition is introduced (Rogers, 2003). This now gives structures which employ four hierarchical relations—the fourth representing the constituency relation encoding a hierarch</context>
</contexts>
<marker>Weir, 1992</marker>
<rawString>David J. Weir. 1992. A geometric hierarchy beyond context-free languages. Theoretical Computer Science, 104:235–261.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>