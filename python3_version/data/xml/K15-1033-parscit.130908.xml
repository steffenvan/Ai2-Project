<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011540">
<title confidence="0.991383">
Do dependency parsing metrics correlate with human judgments?
</title>
<author confidence="0.998522">
Barbara Plank,* H´ector Martinez Alonso,* ˇZeljko Agi´c,* Danijela Merkler,† Anders Søgaard*
</author>
<affiliation confidence="0.999357">
*Center for Language Technology, University of Copenhagen, Denmark
†Department of Linguistics, University of Zagreb, Croatia
</affiliation>
<email confidence="0.98634">
bplank@cst.dk
</email>
<sectionHeader confidence="0.997232" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999442">
Using automatic measures such as labeled
and unlabeled attachment scores is com-
mon practice in dependency parser evalu-
ation. In this paper, we examine whether
these measures correlate with human judg-
ments of overall parse quality. We ask lin-
guists with experience in dependency an-
notation to judge system outputs. We mea-
sure the correlation between their judg-
ments and a range of parse evaluation met-
rics across five languages. The human-
metric correlation is lower for dependency
parsing than for other NLP tasks. Also,
inter-annotator agreement is sometimes
higher than the agreement between judg-
ments and metrics, indicating that the stan-
dard metrics fail to capture certain aspects
of parse quality, such as the relevance of
root attachment or the relative importance
of the different parts of speech.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963050847458">
In dependency parser evaluation, the standard ac-
curacy metrics—labeled and unlabeled attachment
scores—are defined simply as averages over cor-
rect attachment decisions. Several authors have
pointed out problems with these metrics; they are
both sensitive to annotation guidelines (Schwartz
et al., 2012; Tsarfaty et al., 2011), and they fail
to say anything about how parsers fare on rare,
but important linguistic constructions (Nivre et al.,
2010). Both criticisms rely on the intuition that
some parsing errors are more important than oth-
ers, and that our metrics should somehow reflect
that. There are sentences that are hard to anno-
tate because they are ambiguous, or because they
contain phenomena peripheral to linguistic theory,
such as punctuation, clitics, or fragments. Man-
ning (2011) discusses similar issues for part-of-
speech tagging.
To measure the variable relevance of parsing
errors, we present experiments with human judg-
ment of parse output quality across five languages:
Croatian, Danish, English, German, and Spanish.
For the human judgments, we asked professional
linguists with dependency annotation experience
to judge which of two parsers produced the bet-
ter parse. Our stance here is that, insofar ex-
perts are able to annotate dependency trees, they
are also able to determine the quality of a pre-
dicted syntactic structure, which we can in turn
use to evaluate parser evaluation metrics. Even
though downstream evaluation is critical in as-
sessing the usefulness of parses, it also presents
non-trivial challenges in choosing the appropriate
downstream tasks (Elming et al., 2013), we see
human judgments as an important supplement to
extrinsic evaluation.
To the best of our knowledge, no prior study
has analyzed the correlation between dependency
parsing metrics and human judgments. For a range
of other NLP tasks, metrics have been evaluated
by how well they correlate with human judgments.
For instance, the standard automatic metrics for
certain tasks—such as BLEU in machine trans-
lation, or ROUGE-N and NIST in summariza-
tion or natural language generation—were evalu-
ated, reaching correlation coefficients well above
.80 (Papineni et al., 2002; Lin, 2004; Belz and Re-
iter, 2006; Callison-Burch et al., 2007).
We find that correlations between evaluation
metrics and human judgments are weaker for
dependency parsing than other NLP tasks—our
correlation coefficients are typically between .35
and .55—and that inter-annotator agreement is
sometimes higher than human-metric agreement.
Moreover, our analysis (§5) reveals that humans
have a preference for attachment over labeling de-
cisions, and that attachments closer to the root are
more important. Our findings suggest that the cur-
rently employed metrics are not fully adequate.
</bodyText>
<page confidence="0.99102">
315
</page>
<note confidence="0.612523">
Proceedings of the 19th Conference on Computational Language Learning, pages 315–320,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999734875">
Contributions We present i) a systematic com-
parison between a range of available dependency
parsing metrics and their correlation with human
judgments; and ii) a novel dataset1 of 984 sen-
tences (up to 200 sentences for each of the 5 lan-
guages) annotated with human judgments for the
preferred automatically parsed dependency tree,
enabling further research in this direction.
</bodyText>
<sectionHeader confidence="0.9896" genericHeader="introduction">
2 Metrics
</sectionHeader>
<bodyText confidence="0.997478466666667">
We evaluate seven dependency parsing metrics,
described in this section.
Given a labeled gold tree G = (V, EG, lG(·))
and a labeled predicted tree P = (V, EP, lP(·)),
let E C V x V be the set of directed edges from
dependents to heads, and let l : V x V —* L be the
edge labeling function, with L the set of depen-
dency labels.
The three most commonly used metrics
are those from the CoNLL 2006–7 shared
tasks (Buchholz and Marsi, 2006): unlabeled at-
tachment score (UAS), label accuracy (LA), both
introduced by Eisner (1996), and labeled attach-
ment score (LAS), the pivotal dependency parsing
metric introduced by Nivre et al. (2004).
</bodyText>
<equation confidence="0.9978525">
UAS= |{e  |e ∈ EG ∩ EP}|
|V |
LAS = |{e  |lG(e) = lP(e), e ∈ EG ∩ EP}|
|V |
LA = |{v  |v ∈ V, lG(v, ·) = lP(v, ·)}|
|V |
</equation>
<bodyText confidence="0.9999798">
We include two further metrics—namely, la-
beled (LCP) and unlabeled (UCP) complete
predications—to give account for the relevance of
correct predicate prediction for parsing quality.
LCP is inspired by the complete predicates met-
ric from the SemEval 2015 shared task on seman-
tic parsing (Oepen et al., 2015).2 LCP is triggered
by a verb (i.e., set of nodes Vverb) and checks
whether all its core arguments match, i.e., all out-
going dependency edges except for punctuation.
Since LCP is a very strict metric, we also evaluate
UCP, its unlabeled variant. Given a function cX(v)
that retrieves the set of child nodes of a node v
from a tree X, we first define UCP as follows, and
then incorporate the label matching for LCP:
</bodyText>
<footnote confidence="0.999608">
1The dataset is publicly available at https://
bitbucket.org/lowlands/release
2http://alt.qcri.org/semeval2015/
</footnote>
<equation confidence="0.958099">
UCP = |{v  |Vverb, cG(v) = cP(v)}|
|Vverb|
LCP = |{v  |Vverb, cG(v) = cP (v) ∧ lG(v,·) = lP (v, ·)}|
|Vverb|
</equation>
<bodyText confidence="0.999314666666667">
For the final figure of seven different parsing
metrics, on top of the previous five, in our exper-
iments we also include the neutral edge direction
metric (NED) (Schwartz et al., 2011), and tree edit
distance (TED) (Tsarfaty et al., 2011; Tsarfaty et
al., 2012).3
</bodyText>
<sectionHeader confidence="0.999102" genericHeader="method">
3 Experiment
</sectionHeader>
<bodyText confidence="0.9993805">
In our analysis, we compare the metrics with hu-
man judgments. We examine how well the auto-
matic metrics correlate with each other, as well
as with human judgments, and whether inter-
annotator agreement exceeds annotator-metric
agreement.
</bodyText>
<table confidence="0.996359666666667">
LANG TYPE SENT SL TD ANN RAW κ
da CDT 200 22.7 8.1 2-3 .77 .53
de UD 200 18.0 4.4 2 .67 .33
en UD 200 23.4 5.4 4 .73 .45
es UD 184 32.5 6.7 4 .60 .20
hr PDT 200 28.5 7.8 2 .80 .59
</table>
<tableCaption confidence="0.9416875">
Table 1: Data characteristics and agreement statis-
tics. TD: tree depth; SL: sentence length.
</tableCaption>
<bodyText confidence="0.999221833333333">
Data In our experiments we use data from five
languages: The English (en), German (de) and
Spanish (es) treebanks from the Universal Depen-
dencies (UD v1.0) project (Nivre et al., 2015), the
Copenhagen Dependency Treebank (da) (Buch-
Kromann, 2003), and the Croatian Dependency
Treebank (hr) (Agi´c and Merkler, 2013). We keep
the original POS tags for all datasets (17 tags in
case of UD, 13 tags for Croatian, and 23 for Dan-
ish). Data characteristics are in Table 1.
For the parsing systems, we follow McDon-
ald and Nivre (2007) and use the second or-
der MST (McDonald et al., 2005), as well as
Malt parser with pseudo-projectivization (Nivre
and Nilsson, 2005) and default parameters. For
each language, we train the parsers on the canoni-
cal training section. We randomly select 200 sen-
tences from the test sections, where our two de-
</bodyText>
<footnote confidence="0.953295">
3http://www.tsarfaty.com/unipar/
</footnote>
<page confidence="0.994709">
316
</page>
<table confidence="0.999531923076923">
LANG PARSER LAS UAS LA NED TED LCP UCP
en Malt 79.17 82.31 87.88 84.34 85.20 41.27 47.17
MST 78.30 82.91 86.80 84.72 83.49 36.05 45.58
es Malt 78.72 82.85 87.34 82.90 84.20 34.00 43.00
MST 79.51 84.97 86.95 85.00 83.16 31.83 44.00
da Malt 79.28 83.40 85.92 83.39 77.50 47.69 55.23
MST 82.75 87.00 88.42 87.01 78.39 52.31 62.31
de Malt 69.09 75.70 82.05 75.54 80.37 19.72 30.45
MST 72.07 80.29 82.22 80.13 78.94 19.38 33.22
hr Malt 63.21 72.34 76.66 71.94 71.64 23.18 31.03
MST 65.98 76.20 79.01 75.89 72.82 24.71 34.29
Avg Malt 73.84 79.32 83.97 76.62 79.78 33.17 43.18
MST 75.72 82.27 84.68 82.55 79.36 32.86 44.08
</table>
<tableCaption confidence="0.99988">
Table 2: Parsing performance of Malt and MST.
</tableCaption>
<bodyText confidence="0.999282894736842">
pendency parsers do not agree on the correct anal-
ysis, after removing punctuation.4 We do not con-
trol for predicted trees matching the gold standard.
Annotation task A total of 7 annotators were
involved in the annotation task. All the annota-
tors are either native or fluent speakers, and well-
versed in dependency syntax analysis.
For each language, we present the selected
200 sentences with their two predicted depen-
dency structures to 2–4 annotators and ask them
to rank which of the two parses is better. They
see graphical representations of the two de-
pendency structures, visualized with the What’s
Wrong With My NLP? tool.5 The annotators
were not informed of what parser produced which
tree, nor had they access to the gold stan-
dard. The dataset of 984 sentences is available
at:https://bitbucket.org/lowlands/
release (folder CoNLL2015).
</bodyText>
<sectionHeader confidence="0.999954" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.997104">
First, we perform a standard evaluation in order to
see how the parsers fare, using our range of depen-
dency evaluation measures. In addition, we com-
pute correlations between metrics to assess their
similarity. Finally, we correlate the measures with
human judgements, and compare average annota-
tor and human-system agreements.
Table 2 presents the parsing performances with
respect to the set of metrics. We see that using
LAS, Malt performs better on English, while MST
performs better on the remaining four languages.
Table 3 presents Spearman’s ρ between metrics
across the 5 languages. Some metrics are strongly
</bodyText>
<footnote confidence="0.996279666666667">
4For Spanish, we had fewer analyses where the two
parsers disagreed, i.e., 184.
5https://code.google.com/p/whatswrong/
</footnote>
<table confidence="0.998947714285714">
ρ UAS LA NED TED LCP UCP
LAS .755 .622 .743 .556 .236 .286
UAS – .436 .869 .512 .211 .342
LA – – .436 .419 .206 .154
NED – – – .499 .216 .339
TED – – – – .175 .219
LCP – – – – – .352
</table>
<tableCaption confidence="0.990566">
Table 3: Correlations between metrics.
</tableCaption>
<table confidence="0.998935375">
ρ en es da de hr All
LAS .547 .478 .297 .466 .540 .457
UAS .541 .437 .331 .453 .397 .425
LA .387* .250* .232 .310 .467 .324*
NED .541 .469 .318 .501 .446 .448
TED .372* .404 .323 .331 .405* .361*
LCP .022* .230* .171 .120* .120* .126*
UCP .249* .195* .223 .190* .143* .195*
</table>
<tableCaption confidence="0.828784">
Table 4: Correlations between human judgments
and metrics (micro avg). * means significantly
different from LAS ρ using Fisher’s z-transform.
Bold: highest correlation per language.
</tableCaption>
<bodyText confidence="0.998096827586207">
correlated, e.g., LAS and LA, and UAS and NED,
but some exhibit very low correlation coefficients.
Next we study correlations with human judg-
ments (Table 4). In order to aggregate over the an-
notations, we use an item-response model (Hovy
et al., 2013). The correlations are relatively weak
compared to similar findings for other NLP tasks.
For instance, ROUGE-1 (Lin, 2004) correlates
strongly with perceived summary quality, with a
coefficient of 0.99. The same holds for BLEU and
human judgments of machine translation quality
(Papineni et al., 2002).
We find that, overall, LAS is the metric that cor-
relates best with human judgments. It is closely
followed by UAS, which does not differ signifi-
cantly from LAS, albeit the correlations for UAS
are slightly lower on average. NED is in turn
highly correlated with UAS. The correlations for
the predicate-based measures (LCP, UCP) are the
lowest, as they are presumably too strict, and very
different to LAS.
Motivated by the fact that people prefer the
parse that gets the overall structure right (§5), we
experimented with weighting edges proportionally
to their log-distance to root. However, the sig-
nal was fairly weak; the correlations were only
slightly higher for English and Danish: .552 and
.338, respectively.
Finally, we compare the mean agreement be-
</bodyText>
<page confidence="0.996457">
317
</page>
<table confidence="0.9962875">
ANN LAS UAS LA NED TED LCP UCP
da .768 .838 .848 .808 .828 .828 .745 .765
de .670 .710 .690 .635 .710 .630 .575 .565
en .728 .715 .705 .660 .700 .658 .525 .600
es .601 .663 .644 .603 .652 .635 .581 .554
hr .800 .755 .700 .735 .730 .705 .570 .580
</table>
<tableCaption confidence="0.7859845">
Table 5: Average mean agreement between anno-
tators, and between annotators and metrics.
</tableCaption>
<bodyText confidence="0.9998065">
tween humans with the mean agreement between
humans and standard metrics, cf. Table 5. For two
languages (English and Croatian), humans agree
more with each other than with the standard met-
rics, suggesting that metrics are not fully adequate.
The mean agreement between humans is .728 for
English, with slightly lower scores for the met-
rics (LAS: .715, UAS:.705, NED:.660). The
difference between mean agreement of annotators
and human-metric was higher for Croatian: .80
vs .755. For Danish, German and Spanish, how-
ever, average agreement between metrics and hu-
man judgments is higher than our inter-annotator
agreement.
</bodyText>
<sectionHeader confidence="0.993195" genericHeader="method">
5 Analysis
</sectionHeader>
<bodyText confidence="0.99983336">
In sum, our experiments show that metrics corre-
late relatively weakly with human judgments, sug-
gesting that some errors are more important to hu-
mans than others, and that the relevance of these
errors are not captured by the metrics.
To better understand this, we first consider the
POS-wise correlations between human judgments
and LAS, cf. Table 6. In English, for example, the
correlation between judgments and LAS is signif-
icantly stronger for content words6 (p, = 0.522)
than for function words (pf = 0.175). This also
holds for the other UD languages, namely Ger-
man (p, = 0.423 vs pf = 0.263) and Spanish
(p, = 0.403 vs pf = 0.228). This is not the
case for the non-UD languages, Croatian and Dan-
ish, where the difference between content-POS
and function-POS correlations is not significantly
different. In Danish, function words head nouns,
and are thus more important than in UD, where
content-content word relations are annotated, and
function words are leaves in the dependency tree.
This difference in dependency formalism is shown
by the higher correlation for pf for Danish.
The greater correlation for content words for
English, German and Spanish suggests that errors
</bodyText>
<footnote confidence="0.989882">
6Tagged as ADJ, NOUN, PROPN, VERB.
</footnote>
<table confidence="0.984867166666667">
p content function
en .522 .175
de .423 .263
es .403 .228
da .148 .173
hr .340 .306
</table>
<tableCaption confidence="0.673587333333333">
Table 6: Correlations between human judgements
and POS-wise LAS (content p, vs function pf pos-
wise LAS correlations).
</tableCaption>
<bodyText confidence="0.89085072972973">
in attaching or labeling content words mean more
to human judges than errors in attaching or label-
ing function words. We also observe that longer
sentences do not compromise annotation quality,
with a p between −0.07 and 0.08 across languages
regarding sentence length and agreement.
For the languages for which we had 4 annota-
tors, we analyzed the subset of trees where hu-
mans and system (by LAS) disagreed, but where
there was majority vote for one tree. We obtained
35 dependency instances for English and 27 for
Spanish (cf. Table 7). Two of the authors deter-
mined whether humans preferred labeling over at-
tachment, or otherwise.
attachment labeling items
en 86% 14% 35
es 67% 33% 27
Table 7: Preference of attachment or labeling for
items where humans and system disagreed and hu-
man agreement ≥ 0.75.
Table 7 shows that there is a prevalent prefer-
ence for attachment over labeling for both lan-
guages. For Spanish, there is proportionally
higher label preference. Out of the attach-
ment preferences, 36% and 28% were related to
root/main predicate attachments, for English and
Spanish respectively. The relevance of the root-
attachment preference indicates that attachment is
more important than labeling for our annotators.
Figure 5 provides three examples from the data
where human and system disagree. Parse i) in-
volves a coordination as well as a (local) adver-
bial, where humans voted for correct coordination
(red) and thus unanimously preferred attachment
over labeling. Yet, LAS was higher for the analy-
sis in blue because “certainly” is attached to “Eu-
ropeans” in the gold standard. Parse ii) is another
</bodyText>
<page confidence="0.998054">
318
</page>
<figureCaption confidence="0.999775">
Figure 1: Examples where human and system (LAS) disagree. Human choice: i) red; ii) red; iii) blue.
</figureCaption>
<bodyText confidence="0.9997086">
example where humans preferred attachment (in
this case root attachment), while iii) shows a Span-
ish example (“waiter is needed”) where the subject
label (nsubj) of “camarero” (“waiter”) was the de-
cisive trait.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999896933333334">
Parsing metrics are sensitive to the choice of an-
notation scheme (Schwartz et al., 2012; Tsarfaty
et al., 2011) and fail to capture how parsers fare
on important linguistic constructions (Nivre et al.,
2010). In other NLP tasks, several studies have
examined how metrics correlate with human judg-
ments, including machine translation, summariza-
tion and natural language generation (Papineni
et al., 2002; Lin, 2004; Belz and Reiter, 2006;
Callison-Burch et al., 2007). Our study is the first
to assess the correlation of human judgments and
dependency parsing metrics. While previous stud-
ies reached correlation coefficients over 0.80, this
is not the case for dependency parsing, where we
observe much lower coefficients.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999974620689655">
We have shown that out of seven metrics, LAS
correlates best with human jugdments. Neverthe-
less, our study shows that there is an amount of
human preference that is not captured with LAS.
Our analysis on human versus system disagree-
ment indicates that attachment is more important
than labeling, and that humans prefer a parse that
gets the overall structure right. For some lan-
guages, inter-annotator agreement is higher than
annotator-metric (LAS) agreement, and content-
POS is more important than function-POS, indi-
cating there is an amount of human preference that
is not captured with our current metrics. These
observations raise the important question on how
to incorporate our observations into parsing met-
rics that provide a better fit to human judgments.
We do not propose a better metric here, but simply
show that while LAS seems to be the most ade-
quate metric, there is still a need for better metrics
to complement downstream evaluation.
We outline a number of extensions for future
research. Among those, we would aim at aug-
menting the annotations by obtaining more de-
tailed judgments from human annotators. The cur-
rent evaluation would ideally encompass more (di-
verse) domains and languages, as well as the many
diverse annotation schemes implemented in vari-
ous publicly available dependency treebanks that
were not included in our experiment.
</bodyText>
<sectionHeader confidence="0.998631" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999409666666667">
We thank Muntsa Padr´o and Miguel Ballesteros
for their help and the three anonymous reviewers
for their valuable feedback.
</bodyText>
<sectionHeader confidence="0.998852" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9752365">
ˇZeljko Agi´c and Danijela Merkler. 2013. Three
Syntactic Formalisms for Data-Driven Dependency
Parsing of Croatian. In Text, Speech, and Dialogue.
Springer.
Anja Belz and Ehud Reiter. 2006. Comparing Auto-
matic and Human Evaluation of NLG Systems. In
EACL.
Matthias Buch-Kromann. 2003. The Danish Depen-
dency Treebank and the DTAG Treebank Tool. In
TLT.
</reference>
<page confidence="0.996264">
319
</page>
<reference confidence="0.99935802739726">
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In CoNLL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing. In COLING.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, H´ector Martinez Alonso, and
Anders Søgaard. 2013. Down-stream effects of
tree-to-dependency conversions. In NAACL.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with MACE. In NAACL.
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In Text summariza-
tion branches out: Proceedings of the ACL-04 work-
shop.
Christopher D Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: is it time for some linguis-
tics? In Computational Linguistics and Intelligent
Text Processing. Springer.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsers.
In EMNLP-CoNLL.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In ACL.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In ACL.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In CoNLL.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Car-
los Gomez-Rodriguez. 2010. Evaluation of depen-
dency parsers on unbounded dependencies. In COL-
ING.
Joakim Nivre, Cristina Bosco, Jinho Choi, Marie-
Catherine de Marneffe, Timothy Dozat, Rich´ard
Farkas, Jennifer Foster, Filip Ginter, Yoav Gold-
berg, Jan Hajiˇc, Jenna Kanerva, Veronika Laippala,
Alessandro Lenci, Teresa Lynn, Christopher Man-
ning, Ryan McDonald, Anna Missil¨a, Simonetta
Montemagni, Slav Petrov, Sampo Pyysalo, Natalia
Silveira, Maria Simi, Aaron Smith, Reut Tsarfaty,
Veronika Vincze, and Daniel Zeman. 2015. Univer-
sal dependencies 1.0.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Silvie Cinkova, Dan Flickinger, Jan
Hajic, and Zdenka Uresova. 2015. Semeval 2015
task 18: Broad-coverage semantic dependency pars-
ing. In Proceedings of the 9th International Work-
shop on Semantic Evaluation (SemEval 2015).
Kishore Papineni, Salim Roukus, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL, Philadel-
phia, Pennsylvania.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In ACL.
Roy Schwartz, Omri Abend, and Ari Rappoport. 2012.
Learnability-based syntactic annotation design. In
COLING.
Reut Tsarfaty, Joakim Nivre, and Evelina Anders-
son. 2011. Evaluating dependency parsing: robust
and heuristics-free cross-annotation evaluation. In
EMNLP.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical
parsing. In EACL.
</reference>
<page confidence="0.99826">
320
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.640607">
<title confidence="0.824114">Do dependency parsing metrics correlate with human judgments?</title>
<author confidence="0.831314">Martinez</author>
<affiliation confidence="0.914969">Center for Language Technology, University of Copenhagen, of Linguistics, University of Zagreb,</affiliation>
<email confidence="0.995807">bplank@cst.dk</email>
<abstract confidence="0.99961819047619">Using automatic measures such as labeled and unlabeled attachment scores is common practice in dependency parser evaluation. In this paper, we examine whether these measures correlate with human judgments of overall parse quality. We ask linguists with experience in dependency annotation to judge system outputs. We measure the correlation between their judgments and a range of parse evaluation metrics across five languages. The humanmetric correlation is lower for dependency parsing than for other NLP tasks. Also, inter-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ˇZeljko Agi´c</author>
<author>Danijela Merkler</author>
</authors>
<title>Three Syntactic Formalisms for Data-Driven Dependency Parsing of Croatian.</title>
<date>2013</date>
<booktitle>In Text, Speech, and Dialogue.</booktitle>
<publisher>Springer.</publisher>
<marker>Agi´c, Merkler, 2013</marker>
<rawString>ˇZeljko Agi´c and Danijela Merkler. 2013. Three Syntactic Formalisms for Data-Driven Dependency Parsing of Croatian. In Text, Speech, and Dialogue. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Ehud Reiter</author>
</authors>
<title>Comparing Automatic and Human Evaluation of NLG Systems.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="3334" citStr="Belz and Reiter, 2006" startWordPosition="503" endWordPosition="507">ng et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. 315 Proceedings of the 19th Conference on Comp</context>
<context position="16810" citStr="Belz and Reiter, 2006" startWordPosition="2769" endWordPosition="2772"> (in this case root attachment), while iii) shows a Spanish example (“waiter is needed”) where the subject label (nsubj) of “camarero” (“waiter”) was the decisive trait. 6 Related Work Parsing metrics are sensitive to the choice of annotation scheme (Schwartz et al., 2012; Tsarfaty et al., 2011) and fail to capture how parsers fare on important linguistic constructions (Nivre et al., 2010). In other NLP tasks, several studies have examined how metrics correlate with human judgments, including machine translation, summarization and natural language generation (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). Our study is the first to assess the correlation of human judgments and dependency parsing metrics. While previous studies reached correlation coefficients over 0.80, this is not the case for dependency parsing, where we observe much lower coefficients. 7 Conclusions We have shown that out of seven metrics, LAS correlates best with human jugdments. Nevertheless, our study shows that there is an amount of human preference that is not captured with LAS. Our analysis on human versus system disagreement indicates that attachment is more important than labeling, and </context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>Anja Belz and Ehud Reiter. 2006. Comparing Automatic and Human Evaluation of NLG Systems. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Buch-Kromann</author>
</authors>
<title>The Danish Dependency Treebank and the DTAG Treebank Tool.</title>
<date>2003</date>
<booktitle>In TLT.</booktitle>
<marker>Buch-Kromann, 2003</marker>
<rawString>Matthias Buch-Kromann. 2003. The Danish Dependency Treebank and the DTAG Treebank Tool. In TLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X Shared Task on Multilingual Dependency Parsing. In CoNLL.</title>
<date>2006</date>
<contexts>
<context position="4882" citStr="Buchholz and Marsi, 2006" startWordPosition="752" endWordPosition="755">p to 200 sentences for each of the 5 languages) annotated with human judgments for the preferred automatically parsed dependency tree, enabling further research in this direction. 2 Metrics We evaluate seven dependency parsing metrics, described in this section. Given a labeled gold tree G = (V, EG, lG(·)) and a labeled predicted tree P = (V, EP, lP(·)), let E C V x V be the set of directed edges from dependents to heads, and let l : V x V —* L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). UAS= |{e |e ∈ EG ∩ EP}| |V | LAS = |{e |lG(e) = lP(e), e ∈ EG ∩ EP}| |V | LA = |{v |v ∈ V, lG(v, ·) = lP(v, ·)}| |V | We include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic pars</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="3364" citStr="Callison-Burch et al., 2007" startWordPosition="508" endWordPosition="511">e human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. 315 Proceedings of the 19th Conference on Computational Language Learning, p</context>
<context position="16840" citStr="Callison-Burch et al., 2007" startWordPosition="2773" endWordPosition="2776">achment), while iii) shows a Spanish example (“waiter is needed”) where the subject label (nsubj) of “camarero” (“waiter”) was the decisive trait. 6 Related Work Parsing metrics are sensitive to the choice of annotation scheme (Schwartz et al., 2012; Tsarfaty et al., 2011) and fail to capture how parsers fare on important linguistic constructions (Nivre et al., 2010). In other NLP tasks, several studies have examined how metrics correlate with human judgments, including machine translation, summarization and natural language generation (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). Our study is the first to assess the correlation of human judgments and dependency parsing metrics. While previous studies reached correlation coefficients over 0.80, this is not the case for dependency parsing, where we observe much lower coefficients. 7 Conclusions We have shown that out of seven metrics, LAS correlates best with human jugdments. Nevertheless, our study shows that there is an amount of human preference that is not captured with LAS. Our analysis on human versus system disagreement indicates that attachment is more important than labeling, and that humans prefer a parse tha</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (meta-) evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing.</title>
<date>1996</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="4971" citStr="Eisner (1996)" startWordPosition="767" endWordPosition="768">matically parsed dependency tree, enabling further research in this direction. 2 Metrics We evaluate seven dependency parsing metrics, described in this section. Given a labeled gold tree G = (V, EG, lG(·)) and a labeled predicted tree P = (V, EP, lP(·)), let E C V x V be the set of directed edges from dependents to heads, and let l : V x V —* L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). UAS= |{e |e ∈ EG ∩ EP}| |V | LAS = |{e |lG(e) = lP(e), e ∈ EG ∩ EP}| |V | LA = |{v |v ∈ V, lG(v, ·) = lP(v, ·)}| |V | We include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set of nodes Vverb) and chec</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Elming</author>
<author>Anders Johannsen</author>
<author>Sigrid Klerke</author>
<author>Emanuele Lapponi</author>
<author>H´ector Martinez Alonso</author>
<author>Anders Søgaard</author>
</authors>
<title>Down-stream effects of tree-to-dependency conversions.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="2729" citStr="Elming et al., 2013" startWordPosition="409" endWordPosition="412">ges: Croatian, Danish, English, German, and Spanish. For the human judgments, we asked professional linguists with dependency annotation experience to judge which of two parsers produced the better parse. Our stance here is that, insofar experts are able to annotate dependency trees, they are also able to determine the quality of a predicted syntactic structure, which we can in turn use to evaluate parser evaluation metrics. Even though downstream evaluation is critical in assessing the usefulness of parses, it also presents non-trivial challenges in choosing the appropriate downstream tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter,</context>
</contexts>
<marker>Elming, Johannsen, Klerke, Lapponi, Alonso, Søgaard, 2013</marker>
<rawString>Jakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi, H´ector Martinez Alonso, and Anders Søgaard. 2013. Down-stream effects of tree-to-dependency conversions. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Ashish Vaswani</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning whom to trust with MACE.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="10978" citStr="Hovy et al., 2013" startWordPosition="1808" endWordPosition="1811">LA .387* .250* .232 .310 .467 .324* NED .541 .469 .318 .501 .446 .448 TED .372* .404 .323 .331 .405* .361* LCP .022* .230* .171 .120* .120* .126* UCP .249* .195* .223 .190* .143* .195* Table 4: Correlations between human judgments and metrics (micro avg). * means significantly different from LAS ρ using Fisher’s z-transform. Bold: highest correlation per language. correlated, e.g., LAS and LA, and UAS and NED, but some exhibit very low correlation coefficients. Next we study correlations with human judgments (Table 4). In order to aggregate over the annotations, we use an item-response model (Hovy et al., 2013). The correlations are relatively weak compared to similar findings for other NLP tasks. For instance, ROUGE-1 (Lin, 2004) correlates strongly with perceived summary quality, with a coefficient of 0.99. The same holds for BLEU and human judgments of machine translation quality (Papineni et al., 2002). We find that, overall, LAS is the metric that correlates best with human judgments. It is closely followed by UAS, which does not differ significantly from LAS, albeit the correlations for UAS are slightly lower on average. NED is in turn highly correlated with UAS. The correlations for the predi</context>
</contexts>
<marker>Hovy, Berg-Kirkpatrick, Vaswani, Hovy, 2013</marker>
<rawString>Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with MACE. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Text summarization branches out: Proceedings of the ACL-04</booktitle>
<pages>workshop.</pages>
<contexts>
<context position="3311" citStr="Lin, 2004" startWordPosition="501" endWordPosition="502">tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. 315 Proceedings of the </context>
<context position="11100" citStr="Lin, 2004" startWordPosition="1828" endWordPosition="1829">20* .120* .126* UCP .249* .195* .223 .190* .143* .195* Table 4: Correlations between human judgments and metrics (micro avg). * means significantly different from LAS ρ using Fisher’s z-transform. Bold: highest correlation per language. correlated, e.g., LAS and LA, and UAS and NED, but some exhibit very low correlation coefficients. Next we study correlations with human judgments (Table 4). In order to aggregate over the annotations, we use an item-response model (Hovy et al., 2013). The correlations are relatively weak compared to similar findings for other NLP tasks. For instance, ROUGE-1 (Lin, 2004) correlates strongly with perceived summary quality, with a coefficient of 0.99. The same holds for BLEU and human judgments of machine translation quality (Papineni et al., 2002). We find that, overall, LAS is the metric that correlates best with human judgments. It is closely followed by UAS, which does not differ significantly from LAS, albeit the correlations for UAS are slightly lower on average. NED is in turn highly correlated with UAS. The correlations for the predicate-based measures (LCP, UCP) are the lowest, as they are presumably too strict, and very different to LAS. Motivated by </context>
<context position="16787" citStr="Lin, 2004" startWordPosition="2767" endWordPosition="2768"> attachment (in this case root attachment), while iii) shows a Spanish example (“waiter is needed”) where the subject label (nsubj) of “camarero” (“waiter”) was the decisive trait. 6 Related Work Parsing metrics are sensitive to the choice of annotation scheme (Schwartz et al., 2012; Tsarfaty et al., 2011) and fail to capture how parsers fare on important linguistic constructions (Nivre et al., 2010). In other NLP tasks, several studies have examined how metrics correlate with human judgments, including machine translation, summarization and natural language generation (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). Our study is the first to assess the correlation of human judgments and dependency parsing metrics. While previous studies reached correlation coefficients over 0.80, this is not the case for dependency parsing, where we observe much lower coefficients. 7 Conclusions We have shown that out of seven metrics, LAS correlates best with human jugdments. Nevertheless, our study shows that there is an amount of human preference that is not captured with LAS. Our analysis on human versus system disagreement indicates that attachment is more import</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: a package for automatic evaluation of summaries. In Text summarization branches out: Proceedings of the ACL-04 workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Part-of-speech tagging from 97% to 100%: is it time for some linguistics?</title>
<date>2011</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="1918" citStr="Manning (2011)" startWordPosition="285" endWordPosition="287">ors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. To measure the variable relevance of parsing errors, we present experiments with human judgment of parse output quality across five languages: Croatian, Danish, English, German, and Spanish. For the human judgments, we asked professional linguists with dependency annotation experience to judge which of two parsers produced the better parse. Our stance here is that, insofar experts are able to annotate dependency trees, they are also able to determine the quality of a predicted syntactic structure, which we can in turn use to evaluate parser </context>
</contexts>
<marker>Manning, 2011</marker>
<rawString>Christopher D Manning. 2011. Part-of-speech tagging from 97% to 100%: is it time for some linguistics? In Computational Linguistics and Intelligent Text Processing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsers.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="7453" citStr="McDonald and Nivre (2007)" startWordPosition="1207" endWordPosition="1211">0 .59 Table 1: Data characteristics and agreement statistics. TD: tree depth; SL: sentence length. Data In our experiments we use data from five languages: The English (en), German (de) and Spanish (es) treebanks from the Universal Dependencies (UD v1.0) project (Nivre et al., 2015), the Copenhagen Dependency Treebank (da) (BuchKromann, 2003), and the Croatian Dependency Treebank (hr) (Agi´c and Merkler, 2013). We keep the original POS tags for all datasets (17 tags in case of UD, 13 tags for Croatian, and 23 for Danish). Data characteristics are in Table 1. For the parsing systems, we follow McDonald and Nivre (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two de3http://www.tsarfaty.com/unipar/ 316 LANG PARSER LAS UAS LA NED TED LCP UCP en Malt 79.17 82.31 87.88 84.34 85.20 41.27 47.17 MST 78.30 82.91 86.80 84.72 83.49 36.05 45.58 es Malt 78.72 82.85 87.34 82.90 84.20 34.00 43.00 MST 79.51 84.97 86.95 85.00 83.16 31.83 44.00 da Malt 79.28 83.40 85.92 8</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsers. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7506" citStr="McDonald et al., 2005" startWordPosition="1219" endWordPosition="1222">tics. TD: tree depth; SL: sentence length. Data In our experiments we use data from five languages: The English (en), German (de) and Spanish (es) treebanks from the Universal Dependencies (UD v1.0) project (Nivre et al., 2015), the Copenhagen Dependency Treebank (da) (BuchKromann, 2003), and the Croatian Dependency Treebank (hr) (Agi´c and Merkler, 2013). We keep the original POS tags for all datasets (17 tags in case of UD, 13 tags for Croatian, and 23 for Danish). Data characteristics are in Table 1. For the parsing systems, we follow McDonald and Nivre (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two de3http://www.tsarfaty.com/unipar/ 316 LANG PARSER LAS UAS LA NED TED LCP UCP en Malt 79.17 82.31 87.88 84.34 85.20 41.27 47.17 MST 78.30 82.91 86.80 84.72 83.49 36.05 45.58 es Malt 78.72 82.85 87.34 82.90 84.20 34.00 43.00 MST 79.51 84.97 86.95 85.00 83.16 31.83 44.00 da Malt 79.28 83.40 85.92 83.39 77.50 47.69 55.23 MST 82.75 87.00 88.42 87.01 78</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudoprojective dependency parsing.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7585" citStr="Nivre and Nilsson, 2005" startWordPosition="1230" endWordPosition="1233">a from five languages: The English (en), German (de) and Spanish (es) treebanks from the Universal Dependencies (UD v1.0) project (Nivre et al., 2015), the Copenhagen Dependency Treebank (da) (BuchKromann, 2003), and the Croatian Dependency Treebank (hr) (Agi´c and Merkler, 2013). We keep the original POS tags for all datasets (17 tags in case of UD, 13 tags for Croatian, and 23 for Danish). Data characteristics are in Table 1. For the parsing systems, we follow McDonald and Nivre (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two de3http://www.tsarfaty.com/unipar/ 316 LANG PARSER LAS UAS LA NED TED LCP UCP en Malt 79.17 82.31 87.88 84.34 85.20 41.27 47.17 MST 78.30 82.91 86.80 84.72 83.49 36.05 45.58 es Malt 78.72 82.85 87.34 82.90 84.20 34.00 43.00 MST 79.51 84.97 86.95 85.00 83.16 31.83 44.00 da Malt 79.28 83.40 85.92 83.39 77.50 47.69 55.23 MST 82.75 87.00 88.42 87.01 78.39 52.31 62.31 de Malt 69.09 75.70 82.05 75.54 80.37 19.72 30.45 MST 72.07 80.</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudoprojective dependency parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="5080" citStr="Nivre et al. (2004)" startWordPosition="782" endWordPosition="785">ven dependency parsing metrics, described in this section. Given a labeled gold tree G = (V, EG, lG(·)) and a labeled predicted tree P = (V, EP, lP(·)), let E C V x V be the set of directed edges from dependents to heads, and let l : V x V —* L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). UAS= |{e |e ∈ EG ∩ EP}| |V | LAS = |{e |lG(e) = lP(e), e ∈ EG ∩ EP}| |V | LA = |{v |v ∈ V, lG(v, ·) = lP(v, ·)}| |V | We include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set of nodes Vverb) and checks whether all its core arguments match, i.e., all outgoing dependency edges except for punctuation. Since LC</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Laura Rimell</author>
<author>Ryan McDonald</author>
<author>Carlos Gomez-Rodriguez</author>
</authors>
<title>Evaluation of dependency parsers on unbounded dependencies.</title>
<date>2010</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="1572" citStr="Nivre et al., 2010" startWordPosition="229" endWordPosition="232">cs fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. To measure the variable relevance of parsing errors, we present experiments with human judgment of parse output quality across five languages: Croatian, Danish, English, German, and Spanish. For the hu</context>
<context position="16581" citStr="Nivre et al., 2010" startWordPosition="2735" endWordPosition="2738">“certainly” is attached to “Europeans” in the gold standard. Parse ii) is another 318 Figure 1: Examples where human and system (LAS) disagree. Human choice: i) red; ii) red; iii) blue. example where humans preferred attachment (in this case root attachment), while iii) shows a Spanish example (“waiter is needed”) where the subject label (nsubj) of “camarero” (“waiter”) was the decisive trait. 6 Related Work Parsing metrics are sensitive to the choice of annotation scheme (Schwartz et al., 2012; Tsarfaty et al., 2011) and fail to capture how parsers fare on important linguistic constructions (Nivre et al., 2010). In other NLP tasks, several studies have examined how metrics correlate with human judgments, including machine translation, summarization and natural language generation (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). Our study is the first to assess the correlation of human judgments and dependency parsing metrics. While previous studies reached correlation coefficients over 0.80, this is not the case for dependency parsing, where we observe much lower coefficients. 7 Conclusions We have shown that out of seven metrics, LAS correlates best with human</context>
</contexts>
<marker>Nivre, Rimell, McDonald, Gomez-Rodriguez, 2010</marker>
<rawString>Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos Gomez-Rodriguez. 2010. Evaluation of dependency parsers on unbounded dependencies. In COLING.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Joakim Nivre</author>
<author>Cristina Bosco</author>
<author>Jinho Choi</author>
<author>MarieCatherine de Marneffe</author>
<author>Timothy Dozat</author>
<author>Rich´ard Farkas</author>
<author>Jennifer Foster</author>
<author>Filip Ginter</author>
<author>Yoav Goldberg</author>
<author>Jan Hajiˇc</author>
<author>Jenna Kanerva</author>
</authors>
<date>2015</date>
<location>Veronika Laippala, Alessandro Lenci, Teresa Lynn, Christopher Manning, Ryan McDonald, Anna Missil¨a, Simonetta Montemagni, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Maria</location>
<note>Universal dependencies 1.0.</note>
<marker>Nivre, Bosco, Choi, de Marneffe, Dozat, Farkas, Foster, Ginter, Goldberg, Hajiˇc, Kanerva, 2015</marker>
<rawString>Joakim Nivre, Cristina Bosco, Jinho Choi, MarieCatherine de Marneffe, Timothy Dozat, Rich´ard Farkas, Jennifer Foster, Filip Ginter, Yoav Goldberg, Jan Hajiˇc, Jenna Kanerva, Veronika Laippala, Alessandro Lenci, Teresa Lynn, Christopher Manning, Ryan McDonald, Anna Missil¨a, Simonetta Montemagni, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Maria Simi, Aaron Smith, Reut Tsarfaty, Veronika Vincze, and Daniel Zeman. 2015. Universal dependencies 1.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Marco Kuhlmann</author>
<author>Yusuke Miyao</author>
<author>Daniel Zeman</author>
<author>Silvie Cinkova</author>
<author>Dan Flickinger</author>
<author>Jan Hajic</author>
<author>Zdenka Uresova</author>
</authors>
<title>task 18: Broad-coverage semantic dependency parsing.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval</booktitle>
<note>Semeval</note>
<contexts>
<context position="5506" citStr="Oepen et al., 2015" startWordPosition="865" endWordPosition="868">abeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). UAS= |{e |e ∈ EG ∩ EP}| |V | LAS = |{e |lG(e) = lP(e), e ∈ EG ∩ EP}| |V | LA = |{v |v ∈ V, lG(v, ·) = lP(v, ·)}| |V | We include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set of nodes Vverb) and checks whether all its core arguments match, i.e., all outgoing dependency edges except for punctuation. Since LCP is a very strict metric, we also evaluate UCP, its unlabeled variant. Given a function cX(v) that retrieves the set of child nodes of a node v from a tree X, we first define UCP as follows, and then incorporate the label matching for LCP: 1The dataset is publicly available at https:// bitbucket.org/lowlands/release 2http://alt.qcri.org/semeval2015/ UCP = |{v |Vverb, cG(v) = cP(v)}| |Vverb| LCP = |{v |Vverb, cG(v) = cP (v</context>
</contexts>
<marker>Oepen, Kuhlmann, Miyao, Zeman, Cinkova, Flickinger, Hajic, Uresova, 2015</marker>
<rawString>Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel Zeman, Silvie Cinkova, Dan Flickinger, Jan Hajic, and Zdenka Uresova. 2015. Semeval 2015 task 18: Broad-coverage semantic dependency parsing. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukus</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="3300" citStr="Papineni et al., 2002" startWordPosition="497" endWordPosition="500">appropriate downstream tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. 315 Proceedi</context>
<context position="11279" citStr="Papineni et al., 2002" startWordPosition="1853" endWordPosition="1856">ρ using Fisher’s z-transform. Bold: highest correlation per language. correlated, e.g., LAS and LA, and UAS and NED, but some exhibit very low correlation coefficients. Next we study correlations with human judgments (Table 4). In order to aggregate over the annotations, we use an item-response model (Hovy et al., 2013). The correlations are relatively weak compared to similar findings for other NLP tasks. For instance, ROUGE-1 (Lin, 2004) correlates strongly with perceived summary quality, with a coefficient of 0.99. The same holds for BLEU and human judgments of machine translation quality (Papineni et al., 2002). We find that, overall, LAS is the metric that correlates best with human judgments. It is closely followed by UAS, which does not differ significantly from LAS, albeit the correlations for UAS are slightly lower on average. NED is in turn highly correlated with UAS. The correlations for the predicate-based measures (LCP, UCP) are the lowest, as they are presumably too strict, and very different to LAS. Motivated by the fact that people prefer the parse that gets the overall structure right (§5), we experimented with weighting edges proportionally to their log-distance to root. However, the s</context>
<context position="16776" citStr="Papineni et al., 2002" startWordPosition="2763" endWordPosition="2766"> where humans preferred attachment (in this case root attachment), while iii) shows a Spanish example (“waiter is needed”) where the subject label (nsubj) of “camarero” (“waiter”) was the decisive trait. 6 Related Work Parsing metrics are sensitive to the choice of annotation scheme (Schwartz et al., 2012; Tsarfaty et al., 2011) and fail to capture how parsers fare on important linguistic constructions (Nivre et al., 2010). In other NLP tasks, several studies have examined how metrics correlate with human judgments, including machine translation, summarization and natural language generation (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). Our study is the first to assess the correlation of human judgments and dependency parsing metrics. While previous studies reached correlation coefficients over 0.80, this is not the case for dependency parsing, where we observe much lower coefficients. 7 Conclusions We have shown that out of seven metrics, LAS correlates best with human jugdments. Nevertheless, our study shows that there is an amount of human preference that is not captured with LAS. Our analysis on human versus system disagreement indicates that attachment is </context>
</contexts>
<marker>Papineni, Roukus, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukus, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Schwartz</author>
<author>Omri Abend</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6324" citStr="Schwartz et al., 2011" startWordPosition="1004" endWordPosition="1007">ric, we also evaluate UCP, its unlabeled variant. Given a function cX(v) that retrieves the set of child nodes of a node v from a tree X, we first define UCP as follows, and then incorporate the label matching for LCP: 1The dataset is publicly available at https:// bitbucket.org/lowlands/release 2http://alt.qcri.org/semeval2015/ UCP = |{v |Vverb, cG(v) = cP(v)}| |Vverb| LCP = |{v |Vverb, cG(v) = cP (v) ∧ lG(v,·) = lP (v, ·)}| |Vverb| For the final figure of seven different parsing metrics, on top of the previous five, in our experiments we also include the neutral edge direction metric (NED) (Schwartz et al., 2011), and tree edit distance (TED) (Tsarfaty et al., 2011; Tsarfaty et al., 2012).3 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. LANG TYPE SENT SL TD ANN RAW κ da CDT 200 22.7 8.1 2-3 .77 .53 de UD 200 18.0 4.4 2 .67 .33 en UD 200 23.4 5.4 4 .73 .45 es UD 184 32.5 6.7 4 .60 .20 hr PDT 200 28.5 7.8 2 .80 .59 Table 1: Data characteristics and agreement statistics. TD: tree depth; SL: sentence lengt</context>
</contexts>
<marker>Schwartz, Abend, Reichart, Rappoport, 2011</marker>
<rawString>Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rappoport. 2011. Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Schwartz</author>
<author>Omri Abend</author>
<author>Ari Rappoport</author>
</authors>
<title>Learnability-based syntactic annotation design.</title>
<date>2012</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="1425" citStr="Schwartz et al., 2012" startWordPosition="205" endWordPosition="208">r NLP tasks. Also, inter-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. To measure the variable relevance of parsing errors, w</context>
<context position="16461" citStr="Schwartz et al., 2012" startWordPosition="2716" endWordPosition="2719">ation (red) and thus unanimously preferred attachment over labeling. Yet, LAS was higher for the analysis in blue because “certainly” is attached to “Europeans” in the gold standard. Parse ii) is another 318 Figure 1: Examples where human and system (LAS) disagree. Human choice: i) red; ii) red; iii) blue. example where humans preferred attachment (in this case root attachment), while iii) shows a Spanish example (“waiter is needed”) where the subject label (nsubj) of “camarero” (“waiter”) was the decisive trait. 6 Related Work Parsing metrics are sensitive to the choice of annotation scheme (Schwartz et al., 2012; Tsarfaty et al., 2011) and fail to capture how parsers fare on important linguistic constructions (Nivre et al., 2010). In other NLP tasks, several studies have examined how metrics correlate with human judgments, including machine translation, summarization and natural language generation (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). Our study is the first to assess the correlation of human judgments and dependency parsing metrics. While previous studies reached correlation coefficients over 0.80, this is not the case for dependency parsing, where w</context>
</contexts>
<marker>Schwartz, Abend, Rappoport, 2012</marker>
<rawString>Roy Schwartz, Omri Abend, and Ari Rappoport. 2012. Learnability-based syntactic annotation design. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Joakim Nivre</author>
<author>Evelina Andersson</author>
</authors>
<title>Evaluating dependency parsing: robust and heuristics-free cross-annotation evaluation.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1449" citStr="Tsarfaty et al., 2011" startWordPosition="209" endWordPosition="212">r-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. To measure the variable relevance of parsing errors, we present experiments wi</context>
<context position="6377" citStr="Tsarfaty et al., 2011" startWordPosition="1013" endWordPosition="1016">n a function cX(v) that retrieves the set of child nodes of a node v from a tree X, we first define UCP as follows, and then incorporate the label matching for LCP: 1The dataset is publicly available at https:// bitbucket.org/lowlands/release 2http://alt.qcri.org/semeval2015/ UCP = |{v |Vverb, cG(v) = cP(v)}| |Vverb| LCP = |{v |Vverb, cG(v) = cP (v) ∧ lG(v,·) = lP (v, ·)}| |Vverb| For the final figure of seven different parsing metrics, on top of the previous five, in our experiments we also include the neutral edge direction metric (NED) (Schwartz et al., 2011), and tree edit distance (TED) (Tsarfaty et al., 2011; Tsarfaty et al., 2012).3 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. LANG TYPE SENT SL TD ANN RAW κ da CDT 200 22.7 8.1 2-3 .77 .53 de UD 200 18.0 4.4 2 .67 .33 en UD 200 23.4 5.4 4 .73 .45 es UD 184 32.5 6.7 4 .60 .20 hr PDT 200 28.5 7.8 2 .80 .59 Table 1: Data characteristics and agreement statistics. TD: tree depth; SL: sentence length. Data In our experiments we use data from five lang</context>
<context position="16485" citStr="Tsarfaty et al., 2011" startWordPosition="2720" endWordPosition="2723">animously preferred attachment over labeling. Yet, LAS was higher for the analysis in blue because “certainly” is attached to “Europeans” in the gold standard. Parse ii) is another 318 Figure 1: Examples where human and system (LAS) disagree. Human choice: i) red; ii) red; iii) blue. example where humans preferred attachment (in this case root attachment), while iii) shows a Spanish example (“waiter is needed”) where the subject label (nsubj) of “camarero” (“waiter”) was the decisive trait. 6 Related Work Parsing metrics are sensitive to the choice of annotation scheme (Schwartz et al., 2012; Tsarfaty et al., 2011) and fail to capture how parsers fare on important linguistic constructions (Nivre et al., 2010). In other NLP tasks, several studies have examined how metrics correlate with human judgments, including machine translation, summarization and natural language generation (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). Our study is the first to assess the correlation of human judgments and dependency parsing metrics. While previous studies reached correlation coefficients over 0.80, this is not the case for dependency parsing, where we observe much lower coe</context>
</contexts>
<marker>Tsarfaty, Nivre, Andersson, 2011</marker>
<rawString>Reut Tsarfaty, Joakim Nivre, and Evelina Andersson. 2011. Evaluating dependency parsing: robust and heuristics-free cross-annotation evaluation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Joakim Nivre</author>
<author>Evelina Andersson</author>
</authors>
<title>Cross-framework evaluation for statistical parsing.</title>
<date>2012</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="6401" citStr="Tsarfaty et al., 2012" startWordPosition="1017" endWordPosition="1020"> retrieves the set of child nodes of a node v from a tree X, we first define UCP as follows, and then incorporate the label matching for LCP: 1The dataset is publicly available at https:// bitbucket.org/lowlands/release 2http://alt.qcri.org/semeval2015/ UCP = |{v |Vverb, cG(v) = cP(v)}| |Vverb| LCP = |{v |Vverb, cG(v) = cP (v) ∧ lG(v,·) = lP (v, ·)}| |Vverb| For the final figure of seven different parsing metrics, on top of the previous five, in our experiments we also include the neutral edge direction metric (NED) (Schwartz et al., 2011), and tree edit distance (TED) (Tsarfaty et al., 2011; Tsarfaty et al., 2012).3 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. LANG TYPE SENT SL TD ANN RAW κ da CDT 200 22.7 8.1 2-3 .77 .53 de UD 200 18.0 4.4 2 .67 .33 en UD 200 23.4 5.4 4 .73 .45 es UD 184 32.5 6.7 4 .60 .20 hr PDT 200 28.5 7.8 2 .80 .59 Table 1: Data characteristics and agreement statistics. TD: tree depth; SL: sentence length. Data In our experiments we use data from five languages: The English (en),</context>
</contexts>
<marker>Tsarfaty, Nivre, Andersson, 2012</marker>
<rawString>Reut Tsarfaty, Joakim Nivre, and Evelina Andersson. 2012. Cross-framework evaluation for statistical parsing. In EACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>