<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9983745">
Lexical Semantic Techniques for Corpus
Analysis
</title>
<author confidence="0.999548">
James Pustejovsky* Sabine Berglert
</author>
<affiliation confidence="0.998894">
Brandeis University Concordia University
</affiliation>
<author confidence="0.841935">
Peter Anickt
</author>
<affiliation confidence="0.685101">
Digital Equipment Corporation
</affiliation>
<bodyText confidence="0.994418">
In this paper we outline a research program for computational linguistics, making extensive use
of text corpora. We demonstrate how a semantic framework for lexical knowledge can suggest
richer relationships among words in text beyond that of simple co-occurrence. The work suggests
how linguistic phenomena such as metonymy and polysemy might be exploitable for semantic
tagging of lexical items. Unlike with purely statistical collocational analyses, the framework
of a semantic theory allows the automatic construction of predictions about deeper semantic
relationships among words appearing in collocational systems. We illustrate the approach for
the acquisition of lexical information for several classes of nominals, and how such techniques can
fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary.
In addition to conventional lexical semantic relations, we show how information concerning
lexical presuppositions and preference relations can also be acquired from corpora, when analyzed
with the appropriate semantic tools. Finally, we discuss the potential that corpus studies have for
enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm
linguistic hypotheses.
</bodyText>
<sectionHeader confidence="0.992155" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999823933333333">
The proliferation of on-line textual information poses an interesting challenge to lin-
guistic researchers for several reasons. First, it provides the linguist with sentence and
word usage information that has been difficult to collect and consequently largely
ignored by linguists. Second, it has intensified the search for efficient automated in-
dexing and retrieval techniques. Full-text indexing, in which all the content words in
a document are used as keywords, is one of the most promising of recent automated
approaches, yet its mediocre precision and recall characteristics indicate that there is
much room for improvement (Croft 1989). The use of domain knowledge can enhance
the effectiveness of a full-text system by providing related terms that can be used to
broaden, narrow, or refocus a query at retrieval time (Debili, Fluhr, and Radasua 1988;
Anick et al. 1989. Likewise, domain knowledge may be applied at indexing time to
do word sense disambiguation (Krovetz and Croft 1989) or content analysis (Jacobs
1991). Unfortunately, for many domains, such knowledge, even in the form of a the-
saurus, is either not available or is incomplete with respect to the vocabulary of the
texts indexed.
</bodyText>
<affiliation confidence="0.863917333333333">
* Computer Science Department, Brandeis University, Waltham MA 02254.
Computer Science Department, Concordia University, Montreal, Quebec H3G 1M8, Canada.
Digital Equipment Corporation, 111 Locke Drive LM02-1/D12, Marlboro MA 01752.
</affiliation>
<note confidence="0.8387185">
© 1993 Association for Computational Linguistics
Computational Linguistics Volume 19, Number 2
</note>
<bodyText confidence="0.999767090909091">
In this paper we examine how linguistic phenomena such as metonymy and po-
lysemy might be exploited for the semantic tagging of lexical items. Unlike purely
statistical collocational analyses, employing a semantic theory allows for the auto-
matic construction of deeper semantic relationships among words appearing in collo-
cational systems. We illustrate the approach for the acquisition of lexical information
for several classes of nominals, and how such techniques can fine-tune the lexical
structures acquired from an initial seeding of a machine-readable dictionary. In addi-
tion to conventional lexical semantic relations, we show how information concerning
lexical presuppositions and preference relations (Wilks 1978) can also be acquired from
corpora, when analyzed with the appropriate semantic tools. Finally, we discuss the
potential that corpus studies have for enriching the data set for theoretical linguistic
research, as well as helping to confirm or disconfirm linguistic hypotheses.
The aim of our research is to discover what kinds of knowledge can be reliably
acquired through the use of these methods, exploiting, as they do, general linguis-
tic knowledge rather than domain knowledge. In this respect, our program is simi-
lar to Zernik&apos;s (1989) work on extracting verb semantics from corpora using lexical
categories. Our research, however, differs in two respects: first, we employ a more
expressive lexical semantics; second, our focus is on all major categories in the lan-
guage, and not just verbs. This is important since for full-text information retrieval,
information about nominals is paramount, as most queries tend to be expressed as
conjunctions of nouns. From a theoretical perspective, we believe that the contribu-
tion of the lexical semantics of nominals to the overall structure of the lexicon has been
somewhat neglected, relative to that of verbs. While Zernik (1989) presents ambiguity
and metonymy as a potential obstacle to effective corpus analysis, we believe that
the existence of motivated metonymic structures actually provides valuable clues for
semantic analysis of nouns in a corpus.
We will assume, for this paper, the general framework of a generative lexicon as
outlined in Pustejovsky (1991). In particular, we make use of the principles of type
coercion and qualia structure. This model of semantic knowledge associated with
words is based on a system of generative devices that is able to recursively define
new word senses for lexical items in the language. These devices and the associated
dictionary make up a generative lexicon, where semantic information is distributed
throughout the lexicon to all categories. The general framework assumes four basic
levels of semantic description: argument structure, qualia structure, lexical inheritance
structure, and event structure.
Connecting these different levels is a set of generative devices that provide for
the compositional interpretation of words in context. The most important of these
devices is a semantic transformation called type coercion—analogous to coercion in
programming languages—which captures the semantic relatedness between syntacti-
cally distinct expressions. As an operation on types within a A-calculus, type coercion
can be seen as transforming a monomorphic language into one with polymorphic
types (cf. Cardelli and Wegner 1985). Argument, event, and qualia types must con-
form to the well-formedness conditions defined by the type system defined by the
lexical inheritance structure when undergoing operations of semantic composition.&apos;
</bodyText>
<footnote confidence="0.695662857142857">
1 The details of type coercion need not concern us here. Briefly, however, whenever there exists a
grammatical environment where more than one syntactic type satisfies the semantic type selected by
the governing element, the governing element can be analyzed as coercing a range of surface types
into a single semantic type. An example of subject type coercion is a causative verb, semantically
selecting an event as subject (as in (i)), but syntactically permitting a nonevent denoting NP (as in (ii)):
i. The flood killed the grass.
ii. The herbicide killed the grass.
</footnote>
<page confidence="0.975508">
332
</page>
<note confidence="0.99834">
James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis
</note>
<bodyText confidence="0.99963425">
One component of this approach, the qualia structure, specifies the different as-
pects of a word&apos;s meaning through the use of subtyping. These include the subtypes
CONSTITUTIVE, FORMAL, TELIC, and AGENTIVE. To illustrate how these are used, the
qualia structure for book is given below.2
</bodyText>
<equation confidence="0.9831768">
book(x,y)
CONST = information(y)
FORMAL = physobj(x)
TELIC = read(T,w,y)
AGENTIVE = write(T,z,y)
</equation>
<bodyText confidence="0.8133425">
This structured representation allows one to use the same lexical entry in different
contexts, where the word refers to different qualia of the noun&apos;s denotation. For ex-
ample, the sentences in (1)—(3) below refer to different aspects (or qualia) of the general
meaning of book.3
Example 1
This book weighs four ounces.
Example 2
John finished a book.
</bodyText>
<subsectionHeader confidence="0.3769">
Example 3
</subsectionHeader>
<bodyText confidence="0.9826585625">
This is an interesting book.
Example 1 makes reference to the formal role, while 3 refers to the constitutive role.
Example 2, however, can refer to either the telic or the agentive aspects given above.
The utility of such knowledge for information retrieval is readily apparent. This theory
claims that noun meanings should make reference to related concepts and the relations
into which they enter. The qualia structure, thus, can be viewed as a kind of generic
template for structuring this knowledge. Such information about how nouns relate to
other lexical items and their concepts might prove to be much more useful in full-text
information retrieval than what has come from standard statistical techniques.
To illustrate how such semantic structuring might be useful, consider the general
class of artifact nouns. A generative view of the lexicon predicts that by classifying
an element into a particular category, we can generate many aspects of its semantic
structure, and hence, its syntactic behavior. For example, the representation above for
book refers to several word senses, all of which are logically related by the semantic
template for an artifactual object. That is, it contains information, it has a material
extension, it serves some function, and it is created by some particular act or event.
</bodyText>
<listItem confidence="0.9158538">
2 Briefly, the qualia can be defined as follows:
• CONSTITUTIVE: the relation between an object and its constituent parts;
• FORMAL: that which distinguishes it within a larger domain;
• TELIC: its purpose and function;
• AGENTIVE: factors involved in its origin or &amp;quot;bringing it about.&amp;quot;
</listItem>
<bodyText confidence="0.959903">
In the qualia structures given below, we adopt the convention that [a, 0] denotes conjunction of
formulas within the feature structure, while [a; 0] will denote disjunction.
</bodyText>
<listItem confidence="0.2215175">
3 A related approach for expressing the different semantic relations of nominals in distinguished contexts
is given in Bierwisch (1983).
</listItem>
<page confidence="0.996865">
333
</page>
<note confidence="0.853291">
Computational Linguistics Volume 19, Number 2
</note>
<bodyText confidence="0.984742">
Such an analysis allows us to minimally structure objects according to these four
qualia.
As an example of how objects cluster according to these dimensions, we will
briefly consider three object types: (1) containers (of information), e.g., book, tape, record;
(2) instruments, e.g., gun, hammer, paintbrush; and (3) figure-ground objects, e.g., door,
room, fireplace. Because of how their qualia structures differ, these classes appear in
vastly different grammatical contexts.
As with containers in general, information containers permit metonymic exten-
sions between the container and the material contained within it. Collocations such
as those in Examples 4 through 7 indicate that this metonymy is grammaticalized
through specific and systematic head-PP constructions.
</bodyText>
<figure confidence="0.677877">
Example 4
read a book
Example 5
read a story in a book
Example 6
read a tape
Example 7
</figure>
<bodyText confidence="0.901898333333333">
read the information on the tape
Instruments, on the other hand, display classic agent—instrument causative alter-
nations, such as those in Examples 8 through 11 (cf. Fillmore 1968; Lakoff 1968, 1970).
</bodyText>
<subsectionHeader confidence="0.689919">
Example 8
</subsectionHeader>
<bodyText confidence="0.990528">
... smash the vase with the hammer
</bodyText>
<subsectionHeader confidence="0.718889">
Example 9
</subsectionHeader>
<bodyText confidence="0.986966">
The hammer smashed the vase.
</bodyText>
<subsectionHeader confidence="0.834458">
Example 10
</subsectionHeader>
<bodyText confidence="0.989467">
... kill him with a gun
</bodyText>
<subsectionHeader confidence="0.895614">
Example 11
</subsectionHeader>
<bodyText confidence="0.9966335">
The gun killed him.
Finally, figure-ground nominals (Pustejovsky and Anick 1988) permit perspective
shifts such as those in Examples 12 through 15. These are nouns that refer to physical
objects as well as the specific enclosure or aperture associated with it.
</bodyText>
<subsectionHeader confidence="0.724621">
Example 12
</subsectionHeader>
<bodyText confidence="0.810002">
John painted the door.
</bodyText>
<subsectionHeader confidence="0.440003">
Example 13
</subsectionHeader>
<bodyText confidence="0.620664">
John walked through the door.
</bodyText>
<page confidence="0.993644">
334
</page>
<note confidence="0.872275">
James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis
Example 14
</note>
<bodyText confidence="0.565164">
John is scrubbing the fireplace.
</bodyText>
<subsectionHeader confidence="0.404008">
Example 15
</subsectionHeader>
<bodyText confidence="0.9782594">
The smoke filled the fireplace.
That is, paint and scrub are actions on physical objects while walk through and
fi// are processes in spaces. These collocational patterns, we argue, are systematically
predictable from the lexical semantics of the noun, and we term such sets of collocated
structures lexical conceptual paradigms (LCPs).4
To make this point clearer, let us consider a specific example of an LCP from
the computer science domain, namely for the noun tape. Because of the particular
metonymy observed for a noun like tape, we will classify it as belonging to the con-
tainer/containee LCP. This general class is represented as follows, where P and Q are
predicate variables:5
</bodyText>
<equation confidence="0.99485025">
[container(x,y)
CONST = P(y)
FORMAL = Q(x)
TELIC = hold(S,x,y)
</equation>
<bodyText confidence="0.983177833333333">
The LCP is a generic qualia structure that captures not only the semantic relationship
between arguments types of a relation, but also, through corpus-tuning, the collocation
relations that realize these roles. The telic function of a container, for example, is the
relation hold, but this underspecifies which spatial prepositions would adequately
satisfy this semantic relation (e.g. in, on, inside, etc.).
In this view, a noun such as tape would have the following qualia structure:
</bodyText>
<equation confidence="0.9945918">
tape(x,y)
CONST = information(y)
FORMAL = physobj(x),2-dimen(x)
TELIC = contain(S,x,y)
AGENTIVE = Write(T,Z,y)
</equation>
<bodyText confidence="0.9998172">
This states that a tape is an &amp;quot;information container&amp;quot; that is also a two-dimensional
physical object, where the information is written onto the object.&apos; With such nouns, a
logical metonymy exists (as the result of type coercion), when the logical argument of
a semantic type, which is selected by a function of some sort, denotes the semantic
type itself. Thus, in this example, the type selected for by a verb such as read refers to
the &amp;quot;information&amp;quot; argument for tape, while a verb such as carry would select for the
&amp;quot;physical object&amp;quot; argument. They are, however, logically related, since the noun itself
denotes a relation.
The representation above simply states that any semantics for tape must logically
make reference to the object itself (formal), what it can contain (const), what purpose
</bodyText>
<footnote confidence="0.981057">
4 This relates to Menuk&apos;s lexical functions and the syntactic structures they associate with an element.
See Men&apos;uk (1988) and references therein. Cruse (1986, 1992) and Nunberg (1978) discuss the
foregrounding and backgrounding of information with respect to similar examples.
5 Within the qualia structure for a term, FORMAL and CONST roles typically refer to the object domain
while TELIC and AGENTIVE refer to events. Hence, the first parameter in the latter two roles refers to an
event sort, i.e., a state (s), process (P), or transition (T).
6 The appropriate selection of a surface spatial preposition will follow from its formal type specification
as a 2-dimen object. Cf. Pustejovsky (in press) for details.
</footnote>
<page confidence="0.994706">
335
</page>
<note confidence="0.365297">
Computational Linguistics Volume 19, Number 2
</note>
<bodyText confidence="0.999955372093023">
it serves (telic), and how it arises (agentive). This provides us with a semantic repre-
sentation that can capture the multiple perspectives a single lexical item may assume
in different contexts. Yet, the qualia for a lexical item such as tape are not isolated
values for that one word, but are integrated into a global knowledge base indicating
how these senses relate to other lexical items and their senses. This is the contribution
of inheritance and the hierarchical structuring of knowledge (cf. Evans and Gazdar
1990; Copestake and Briscoe 1992; Russell et al. 1992). In Pustejovsky (1991) it is sug-
gested that there are two types of relational structures for lexical knowledge; a fixed
inheritance similar to that of an is-a hierarchy (cf. Touretzky 1986); and a dynamic
structure that operates generatively from the qualia structure of a lexical item to create
a relational structure for ad hoc categories.&apos;
Reviewing briefly, the basic idea is that semantics allows for the dynamic cre-
ation of arbitrary concepts through the application of certain transformations to lexical
meanings. Thus for every predicate, Q, we can generate its opposition, -Q. Similarly,
these two predicates can be related temporally to generate the transition events defin-
ing this opposition. These operations include but may not be limited to: negation;
&lt;, temporal precedence; &gt;, temporal succession; =, temporal equivalence; and act, an
operator adding agency to an argument. We will call the concept space generated by
these operations the Projective Conclusion Space of a specific quale for a lexical item. To
return to the example of tape above, the predicates read and copy are related to the telic
value by just such an operation, while predicates such as mount and dismount—i.e. un-
mount—are related to the formal role. Following the previous discussion, with mounted
as the predicate Q, successive applications of the negation and temporal precedence
operators derives the transition verbs mount and dismount.&apos; We return to a discussion
of this in Section 3, and to how this space relates to statistically significant collocations
in text.
It is our view that the approach outlined above for representing lexical knowl-
edge can be put to use in the service of information retrieval tasks. In this respect,
our proposal can be compared to attempts at object classification in information sci-
ence. One approach, known as faceted classification (Vickery 1975) proceeds roughly
as follows: collect all terms lying within a field; then group the terms into facets by
assigning them to categories. Typical examples of this are state, property, reaction, and
device. However, each subject area is likely to have its own sets of categories, which
makes it difficult to re-use a set of facet classifications.9
Even if the relational information provided by the qualia structure and inheri-
tance would improve performance in information retrieval tasks, one problem still
remains, namely that it would be very time-consuming to hand-code such structures
for all nouns in a domain. Since it is our belief that such representations are generic
structures across all domains, it is our long-term goal to develop methods for auto-
matically extracting these relations and values from on-line corpora. In the sections
that follow, we describe several experiments indicating that the qualia structures do, in
fact, correlate with well-behaved collocational patterns, thereby allowing us to perform
structure-matching operations over corpora to find these relations.
</bodyText>
<footnote confidence="0.997161666666667">
7 This is similar to thesauruslike structures, within the IR community, cf. for example Sparck Jones (1981).
8 Details of the derivation are as follows. Let Q be mounted, then —Q gives ---,mounted, and &lt; applied to
these two states gives Q &lt; which is lexicalized as dismount. A similar derivation exists for mount.
Cf. Pustejovsky (1991) for details.
9 This is reflected in the sublanguage work of Grishman, Hirschman, and Nhan (1986), whose automated
discovery procedures are aimed at clustering nouns into categories like diagnosis and symptom.
</footnote>
<page confidence="0.997803">
336
</page>
<note confidence="0.992461">
James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis
</note>
<subsectionHeader confidence="0.754418">
2. Seeding Lexical Structures from MRDs
</subsectionHeader>
<bodyText confidence="0.994427704545455">
In this section we discuss briefly how a lexical semantic theory can help in extract-
ing information from machine-readable dictionaries (MRDs). We describe research on
conversion of a machine-tractable dictionary (Wilks et al. 1993) into a usable lexical
knowledge base (Boguraev 1991). Although the results here are preliminary, it is im-
portant to mention the process of converting an MRD into a lexical knowledge base, so
that the process of corpus-tuning is put into the proper perspective. The initial seed-
ing of lexical structures is being done independently both from the Oxford Advanced
Learners Dictionary (OALD) and from lexical entries in the Longman Dictionary of
Contemporary English (Procter, Ilson, and Ayto 1978). These are then automatically
adapted to the format of generative lexical structures. It is these lexical structures that
are then statistically tuned against the corpus, following the methods outlined in Anick
and Pustejovsky (1990) and Pustejovsky (1992).
Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn
(1985), Byrd et al. (1987), Markowitz, Ahlswede, and Evens (1986), and Nakamura
and Nagao (1988) showed that taxonomic information and certain semantic relations
can be extracted from MRDs using fairly simple techniques. Later work by Veronis
and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et al. (1992)
provides us with a number of techniques for transfering information from MRDs to a
representation language such as that described in the previous section. Our goal is to
automate, to the extent possible, the initial construction of these structures.
Extensive research has been done on the kind of information needed by natural
language programs and on the representation of that information (Wang, Vanden-
dorpe, and Evens 1985; Ahlswede and Evens 1988). Following Boguraev et al. (1989)
and Wilks et al. of 1989), we believe that much of what is needed for NLP lexicons can
be found either explicitly or implicitly in a dictionary, and empirical evidence suggests
that this information gives rise to a sufficiently rich lexical representation for use in
extracting information from texts. Techniques for identifying explicit information in
machine-readable dictionaries have been developed by many researchers (Boguraev
et al. 1989; Slator 1988; Slator and Wilks 1987; Guthrie et al. 1990) and are well under-
stood. Many properties of a word sense or the semantic relationships between word
senses are available in MRDs, but this information can only be identified computa-
tionally through some analysis of the definition text of an entry (Atkins 1991). Some
research has already been done in this area. Alshawi (1987), Boguraev et al. (1989),
Vossen, Meijs, and den Broeder (1989), and the work described in Wilks et al. (1992)
have made explicit some kinds of implicit information found in MRDs. Here we pro-
pose to refine and merge some of the previous techniques to make explicit the implicit
information specified by a theory of generative lexicons.
Given what we described above for the lexical structures for nominals, we can
identify these semantic relations in the OALD and LDOCE by pattern matching on the
parse trees of definitions. To illustrate what specific information can be derived by au-
tomatic seeding from machine-readable dictionaries, consider the following examples.1°
For example, the LDOCE definition for book is:
&amp;quot;a collection of sheets of paper fastened together as a thing to be read,
or to be written in&amp;quot;
</bodyText>
<footnote confidence="0.971739">
10 The following lexical entries, termed gls&apos;s, are taken from the lexical databases derived from the OALD
using tools developed by Peter Dilworth, and from LDOCE using a combination of tools developed by
Louise Guthrie, Gees Stein, and Pete Dilworth.
</footnote>
<page confidence="0.990463">
337
</page>
<note confidence="0.362353">
Computational Linguistics Volume 19, Number 2
</note>
<bodyText confidence="0.997563736842105">
while the OALD provides a somewhat different definition:
&amp;quot;number of sheet of papers, either printed or blank, fastened together
in a cover.&amp;quot;
Note that both definitions are close to, but not identical to the information structure
suggested in the previous section, using a qualia structure for nominals. LDOCE sug-
gests write in rather than write as the value for the telic role, while the OALD suggests
nothing for this role. Furthermore, although the physical contents of a book as &amp;quot;a
collection of sheets of paper&amp;quot; is mentioned, nowhere is information made reference to
in the definition. When the dictionary fails to provide the value for a semantic role,
the information must be either hand-entered or the lexical structure must be tuned
against a large corpus, in the hope of extracting such features automatically. We turn
to this issue in the next two sections.
Although the two dictionaries differ in substantial respects, it is remarkable how
systematic the definition structures are for extracting semantic information, if there is a
clear idea how this information should be structured. For example, from the following
OALD definition for cigarette,
cigarette n roll of shredded tobacco enclosed in thin paper for smok-
ing.
the initial lexical structure below is generated.
</bodyText>
<equation confidence="0.89396825">
gls(cigarette,
syn( [type (n) ,
code (C)] ) ,
qualia( [formal( [roll] ) ,
telic ( [smoking] ) ,
constUtobacco,paperp,
agent ( [enclosed] )] ) ,
cospec ( [] )) .
</equation>
<bodyText confidence="0.8169684">
Parsing the LDOCE entry for the same noun results in a different lexical structure:
cigarette n finely cut shredded tobacco rolled in a narrow tube of
thin paper for smoking.
gls(cigarette,
syn( [type (n) ,
</bodyText>
<equation confidence="0.997425571428571">
code (C) ,
ldoce_id(cigarette_0_1)] ) ,
qualia( [formal( [tube] ) ,
telic( [smoking]),
const ( [tobacco , paper] )
agent ( [rolled] )] ) ,
cospec ( [] )) .
</equation>
<bodyText confidence="0.997788">
One obvious problem with the above representation is that there is no information
indicating how the word being defined binds to the relations in the qualia. Currently,
</bodyText>
<page confidence="0.99901">
338
</page>
<note confidence="0.985699">
James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis
</note>
<bodyText confidence="0.9996508">
subsequent routines providing for argument binding analyze the relational structure
for particular aspects of noun meaning, giving us a lexical structure fairly close to
what we need for representation and retrieval purposes, although the result is in no
way ideal or uniform over all nominal forms. (cf. Cowie, Guthrie, and Pustejovsky
[1992] for details of this operation on LDOCE.):&amp;quot;
</bodyText>
<equation confidence="0.9983928">
cigarette(x)
CONST = tobacco(y),shredded(y),paper(z)
FORMAL = roll(x)
TELIC = smoke(T,w,x)
AGENTIVE = artifact(x)
</equation>
<bodyText confidence="0.9999368125">
In a related set of experiments performed while constructing a large lexical data-
base for data extraction purposes, we seeded a lexicon with 6000 verbs from LDOCE.
This process and the corpus tuning for both argument typing and subcategorization
acquisition are described in Cowie, Guthrie, and Pustejovsky (1992) and Pustejovsky
et al. (1992).
In summary, based on a theory of lexical semantics, we have discussed how an
MRD can be useful as a corpus for automatically seeding lexical structures. Rather than
addressing the specific problems inherent in converting MRDs into useful lexicons,
we have emphasized how it provides us, in a sense, with a generic vocabulary from
which to begin lexical acquisition over corpora. In the next section, we will address
the problem of taking these initial, and often very incomplete lexical structures, and
enriching them with information acquired from corpus analysis. As mentioned in
the previous section, the power of a generative lexicon is that it takes much of the
burden of semantic interpretation off of the verbal system by supplying a much richer
semantics for nouns and adjectives. This makes the lexical structures ideal as an initial
representation for knowledge acquisition and subsequent information retrieval tasks.
</bodyText>
<sectionHeader confidence="0.439698" genericHeader="method">
3. Knowledge Acquisition from Corpora
</sectionHeader>
<bodyText confidence="0.99961047368421">
A machine-readable dictionary provides the raw material from which to construct
computationally useful representations of the generic vocabulary contained within it.
The lexical structures discussed in the previous section are one example of how such
information can be exploited. Many sublanguages, however, are poorly represented in
on-line dictionaries, if represented at all. Vocabularies geared to specialized domains
will be necessary for many applications, such as text categorization and information
retrieval. The second area of our research program that we discuss is aimed at devel-
oping techniques for building sublanguage lexicons via syntactic and statistical corpus
analysis coupled with analytic techniques based on the tenets of generative lexicon
theory.
To understand fully the experiments described in the next two sections, we will
refer to several semantic notions introduced in previous sections. These include type
coercion, where a lexical item requires a specific type specification for its argument, and
11 As one reviewer correctly pointed out, more than simple argument binding is involved here. For
example, the model must know that paper can enclose shredded tobacco, but not the reverse. Such
information, typically part of commonsense knowledge, is well outside the domain of lexical
semantics, as envisioned here. One approach to this problem, consistent with our methodology, is to
examine the corpus and the collocations that result from training on specific qualia relations. Further
work will hopefully clarify the nature of this problem, and whether it is best treated lexically or not.
</bodyText>
<page confidence="0.986991">
339
</page>
<note confidence="0.341358">
Computational Linguistics Volume 19, Number 2
</note>
<bodyText confidence="0.999700833333333">
the argument is able to change type accordingly—this explains the behavior of logical
metonymy and the syntactic variation seen in complements to verbs and nominals;
and cospecification, a semantic tagging of what collocational patterns the lexical item
may enter into.
Metonymy, in this view, can be seen as a case of the &amp;quot;licensed violation&amp;quot; of selec-
tional restrictions. For example, while the verb announce selects for a human subject,
sentences like The Dow Corporation announced third quarter losses are not only an accept-
able paraphrase of the selectionally correct form Mr. Dow Jr. announced third quarter
losses for Dow Corp, but they are the preferred form in the corpora being examined.
This is an example of subject type coercion, where the semantics for Dow Corp as a
company must specify that there is a human typically associated with such official
pronouncements (see Section 5).12
For one set of experiments, we used a corpus of approximately 3,000 articles writ-
ten by Digital Equipment Corporation&apos;s Customer Support Specialists for an on-line
computer troubleshooting library. The articles, each one- to two-page long descriptions
of a problem and its solution, comprise about 1 million words. Our analysis proceeds
in two phases. In the first phase, we pre-process the corpus to build a database of
phrasal relationships. This consists briefly of the following steps:
</bodyText>
<listItem confidence="0.9882325">
1. Perform unknown word resolution to the corpus. The corpus is
searched for strings that are not members of a 25,000 word generic
on-line English lexicon. Morphological analysis is then applied to these
unknown strings to identify candidate citation forms and their likely
morphological paradigms. Unless morphological evidence indicates
otherwise, we enter unknown words into the lexicon as regular nouns; if
there is evidence of some other morphological paradigm, such as verbal
or adjectival suffixes, the word is entered into the lexicon accordingly.
2. Corpus tagging. Once the lexicon is updated to include the new single
word forms in the domain, the corpus is tagged with part-of-speech
</listItem>
<bodyText confidence="0.8312432">
indicators. Any words that are ambiguous with respect to category are
disambiguated according to a set of several dozen ordered
disambiguation heuristics, which choose a category based on the
categories of the words immediately preceding and following the
ambiguous term.
</bodyText>
<listItem confidence="0.961151333333333">
3. Partial parsing. The tagged corpus is then segmented into a flat
sequence of phrasal groupings, using closed class words such as
prepositions and determiners, as well as certain part-of-speech
</listItem>
<bodyText confidence="0.997969272727273">
transitions, to indicate likely phrase boundaries. No attempt is made to
construct a full parse tree or resolve prepositional phrase attachment,
conjunction scoping, etc. A concordance is constructed, identifying, for
each word appearing in the corpus, the set of sentences, phrases, and
phrase locations in which the word appears.
12 Within the current framework, a distinction is made between logical metonymy, where the metonymic
extension or relation is transparent from the lexical semantics of the coerced phrase, and conventional
metonymy, where the relation may not be directly calculated from information provided grammatically.
For example, in the sentence &amp;quot;The Boston office called today,&amp;quot; it is not clear from logical metonymy
what relation Boston bears to office other than location; i.e., it is not obvious that it is a branch office.
This is well beyond lexical semantics (cf. Lakoff 1987 and Martin 1990).
</bodyText>
<page confidence="0.996637">
340
</page>
<note confidence="0.98281">
James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis
</note>
<bodyText confidence="0.961177">
The database of partially parsed sentences provides the raw material for a number
of sublanguage analyses. This begins the second phase of analysis:
</bodyText>
<listItem confidence="0.9950348">
1. Noun compound recognition and bracketing. In technical sublanguages,
noun compounds are often employed to expand the working vocabulary
without the invention of new word forms. It is therefore useful in
applications such as lexicon-assisted full-text information retrieval (Anick
1992) to include such noun compounds as lexical items for both
</listItem>
<bodyText confidence="0.970707906976744">
querying and thesaurus browsing. We construct bracketed noun
compounds from our database of partial parses in a two-step process.
The first simply searches the corpus for (recurring) contiguous sequences
of nouns. Then, to bracket each compound that includes more than two
nouns, we test whether possible subcomponents of the phrase exist on
their own (as complete noun compounds) elsewhere in the corpus.
Sample bracketed compounds derived from the computer
troubleshooting database include [ [syst em management] utility],
[TK50 [tape drive] ], [ [database management] system].
2. Generation of taxonomic relationships on the basis of collocational
information. Technical sublanguages often express subclass relationships
in noun compounds of the form &lt;instance-name&gt; &lt;class-name&gt;, as in
&amp;quot;Unix operating system&amp;quot; and &amp;quot;C language.&amp;quot; Unfortunately, noun
compounds are also employed to express numerous other relationships,
as in &amp;quot;Unix kernel&amp;quot; and &amp;quot;C debugger.&amp;quot; We have found, however, that
collocational evidence can be employed to suggest which noun
compounds reflect taxonomic relationships, using a strategy similar to
that employed by Hindle (1990) for detecting synonyms. Given a term T,
we extract from the phrase database those nouns N, that appear as the
head of any phrase in which T is the immediately preceding term. These
nouns represent candidate classes of which T may be a member. We then
generate the set of verbs that take T as direct object and calculate the
mutual information value for each verb/T collocation (cf. Hindle 1990).
We do the same for each noun N. Under the assumption that instance
and class nouns are likely to co-occur with the same verbs, we compute
a similarity score between T and each noun N„ by summing the product
of the mutual information values for those verbs occurring with both
nouns. (Verbs with negative mutual information values are left out of the
calculation.) The noun with the highest similarity score is often the class
of which T is an instance, as illustrated by the sample results in Figure 1.
For each word displayed in Figure 1, its &amp;quot;class&amp;quot; is the head noun with
the highest similarity score. Other head nouns occurring with the word
as modifier are listed as well.
As with all the automated procedures described here, this algorithm
yields useful, but imperfect results. The class chosen for &amp;quot;VMS,&amp;quot; for
example, is incorrect, and may reflect the fact that in a DEC
troubleshooting database, authors see no need to further specify VMS as
&amp;quot;VMS operating system.&amp;quot; A more interesting observation is that, among
the collocations associated with the terms, there are often several that
might qualify as classes of which the term is an instance, e.g.,
DECWindows could also be classified as &amp;quot;software&amp;quot;; TK50 might also
qualify as &amp;quot;tape.&amp;quot; From a generative lexicon perspective, these
alternative classifications reflect multiple inheritance through the noun&apos;s
</bodyText>
<page confidence="0.979512">
341
</page>
<figure confidence="0.890500466666667">
Computational Linguistics Volume 19, Number 2
word class score other collocations
HSC
BACKUP
RLO2
TK50
ACCVIO
VAX
VMS
upgrade
DCL
CHECKSUM
EDT
TPU
RTL
DEC Windows
controller
operation
disk
cartridge
error
product
support
procedure
level
value
editor
command
error
environment
</figure>
<table confidence="0.866310857142857">
27.69 device, disk, path, message
34.18 disk, tape, process, saveset
15.93 media, kit, pack
39.17 tape, kit, density, format
14.35 problem
23.28 configuration, node, editor, hardware
7.98 product, upgrade, installation
12.27 phase, option, support, prerequisite
9.14 command, procedure, access, error
4.45 character, operation, error
11.58 session, conversion, search, problem
3.62 editor, session, function, debugger
1.58 routine, library
75.46 image, application, intrinsics, software
</table>
<figureCaption confidence="0.942031">
Figure 1
</figureCaption>
<subsectionHeader confidence="0.56786">
Classification of nouns from a computer troubleshooting corpus.
</subsectionHeader>
<bodyText confidence="0.968347615384615">
qualia. That is, &amp;quot;cartridge&amp;quot; is further specifying the formal role of tape
for TK50. DECWindows is functionally an &amp;quot;environment,&amp;quot; its telic role,
while &amp;quot;software&amp;quot; characterizes its formal quale.
3. Extraction of information relating to noun&apos;s qualia. Under certain
circumstances, it may be possible to elicit information about a noun&apos;s
qualia from automated procedures on a corpus. In this line of research,
we hayed employed the notion of &amp;quot;lexical conceptual paradigm&amp;quot;
described above. An LCP relates a set of syntactic behaviors to the
lexical semantic structures of the participating lexical items.
For example, the set of expressions involving the word &amp;quot;tape&amp;quot; in the
context of its use as a secondary storage device suggests that it fits the
container artifact schema of the qualia structure, with &amp;quot;information&amp;quot; and
&amp;quot;file&amp;quot; as its containees:
</bodyText>
<figure confidence="0.7341502">
(a) read information from tape
(b) write file to tape
(c) read information on tape
(d) read tape
(e) write tape
</figure>
<bodyText confidence="0.992704333333333">
As mentioned in Section 1, containers tend to appear as objects of the
prepositions to, from, in, and on as well as in direct object position, in
which case they are typically serving metonymically for the containee.
Thus, the container LCP relates the set of generalized syntactic patterns
V, Ni {to, from, on} Nk
vi N
</bodyText>
<equation confidence="0.902145">
Nk
to the underlying lexical semantic structure given below.
[_
container(x,y)
CONST = P(y)
FORMAL ---- Q(X)
TELIC = hold(S,x,y) _
</equation>
<page confidence="0.991566">
342
</page>
<note confidence="0.56179">
James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis
</note>
<table confidence="0.937021769230769">
verb MI count
unload 5.43 5
position 3.92 5
mount 3.77 29
initialize 3.18 10
dismount 2.88 5
read 1.40 7
load 1.18 4
restore 0.80 3
write —0.24 2
copy —2.55 1
Figure 2
Verbs associated with direct object tape as direct object.
</table>
<bodyText confidence="0.9907046">
This LCP includes a nominal alternation between the container and
containee in the object position of verbs. For tape, this alternation is
manifested for verbs that predicate the telic role of data storage but not
the formal role of physical object, which refers to the object as a whole
regardless of its contents:
</bodyText>
<listItem confidence="0.995856571428571">
• TELIC = data-storage
(a) read (tape/data from tape)
(b) write (tape/data on tape)
(c) copy (tape/data from tape)
• FORMAL= physical object
(a) mount (tape)
(b) dismount (tape)
</listItem>
<bodyText confidence="0.9990283">
We have explored the use of heuristics to distinguish those predicates
that relate to the Telic quale of the noun. Consider the word tape, which
occurs as the direct object in 107 sentences in our corpus. It appears with
a total of 34 different verbs. By applying the mutual information metric
(MI) to the verb—object pairs, we can sort the verbs accordingly, giving us
the table of verbs most highly associated with tape, shown in Figure 2.
While the mutual information statistic does a good job of identifying
verbs that semantically relate to the word tape, it provides no information
about how the verbs relate to the noun&apos;s qualia structure. That is, verbs
such as unload, position, and mount are selecting for the formal quale of
tape, a physical object that can be physically manipulated with respect to
a tape drive. Read, write, and copy, on the other hand, relate to the telic
role, the function of a tape as a medium for storing information.
Our hypothesis was that the nominal alternation can help to
distinguish the two sets of verbs. We reasoned that, if the alternation is
based on the container/containee metonymy, then it will be those verbs
that apply to the telic role of the direct object that participate in the
alternation. We tested this hypothesis as follows.
We generated a candidate set of containees for tape by identifying all
the nouns that appeared in the corpus to the left of the adjunct on tape.
</bodyText>
<page confidence="0.994182">
343
</page>
<figure confidence="0.957110384615385">
Computational Linguistics Volume 19, Number 2
Si Verbs with tape as object
S2 Verbs with a containee of tape as object
Si n S2 {restore, create, write, read, copy, replace}
• — 52 {mount, initialize, unload, position, dismount, load, allocate }
Verbs with disk as object
S2 Verbs with a containee of disk as object
Si n S2 {compress, restore, disable, rebuild, modify, recover, search, copy}
• — 52 {mount, initialize, boot, dismount, serve, }
Si Verbs with directory as object
52 Verbs with containee of directory as object
• n 52 {create, recreate, delete, store, rename, check}
Si — S2 {own, miss, search, review}
</figure>
<figureCaption confidence="0.686523">
Figure 3
</figureCaption>
<bodyText confidence="0.982871583333334">
Intersection and set difference for three container nouns.
Then we took the set of verbs that had one of these containee nouns as a
direct object and compared this set to the set of verbs that had the
container noun tape as a direct object in the corpus. According to our
hypothesis, verbs applying to the telic role should appear in the
intersection of these two sets (as a result of the alternation), while those
applying to the formal role will appear in the set difference {verbs with
containers as direct object}—{verbs with containees as direct object}. The
difference operation should serve to remove any verbs that co-occur with
containee objects. Figure 3 shows the results of intersection and set
difference for three container nouns tape, disk, and directory.
The results indicate that the container LCP is able to differentiate
nouns with respect to their telic and formal qualia, for the nouns tape
and disk but not for directory. The poor discrimination in the latter case
can be attributed to the fact that a directory is a recursive container. A
directory contains files, and a directory is itself a file. Therefore, verbs
that apply to the formal role of directory are likely to apply to the formal
role of objects contained in directories (such as other directories). This
can be seen as a shortcoming of the container LCP for the task at hand,
but may be a useful way of diagnosing when containers contain objects
functionally similar to themselves.
The result of this corpus acquisition procedure is a kind of minimal faceted analysis
for the noun tape, as illustrated below, showing only the qualia that are relevant to the
discussion.&apos;
</bodyText>
<equation confidence="0.9771015">
[tape(x,y)
CONST = information(y);file(y)
FORMAL = mount(z,x);dismount(z,x)
TELIC = read(w,y);write(w,y);copy(w,y);contain(w,y)
</equation>
<bodyText confidence="0.96060025">
13 Because the technique was sensitive to grammatical position of the object NP, the argument can be
bound to the appropriate variable in the relation expressed in the qualia. It should be pointed out that
these qualia values do not carry event place variables, since such discrimination was beyond the scope
of this experiment.
</bodyText>
<page confidence="0.995639">
344
</page>
<note confidence="0.993763">
James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis
</note>
<bodyText confidence="0.996223666666667">
What is interesting about the qualia values is how close they are to the concepts in
the projective conclusion space of tape, as mentioned in Section 1.
To illustrate this procedure on another semantic category, consider the term mouse
in its computer artifact sense. In our corpus, it appears in the object position of the
verb use in a &amp;quot;use NP to&amp;quot; construction, as well as the object of the preposition with
following a transitive verb and its object:
</bodyText>
<listItem confidence="0.99886">
1. use the mouse to set breakpoints
2. use the mouse anywhere
3. move a window with the mouse
4. click on it with the mouse . . .
</listItem>
<bodyText confidence="0.96510625">
These constructions are symptomatic of its role as an instrument; and the VP
complement of to as well as the VP dominating the with-PP identify the telic predicates
for the noun. Other verbs, for which mouse appears as a direct object are currently
defaulted into the formal role, resulting in an entry for mouse as follows:
</bodyText>
<equation confidence="0.99919875">
[mouse(x)
CONST = button(y)
FORMAL = physobj(x)
TELIC = set(x,breakpoint);move(x,window);click-on(x,z)
</equation>
<bodyText confidence="0.999878375">
The above experiments have met with limited success, enough to warrant continu-
ing our application of lexical semantic theory to knowledge acquisition from corpora,
but not enough to remove the human from the loop. As they currently exist, the
algorithms described here can be used as tools to help the knowledge engineer ex-
tract useful information from on-line textual sources, and in some applications (e.g.,
a &amp;quot;related terms&amp;quot; thesaurus for full text information retrieval) may provide a useful
way to heuristically organize sublanguage terminology when human resources are
unavailable.
</bodyText>
<sectionHeader confidence="0.836919" genericHeader="method">
4. Semantic Type Induction from Syntactic Forms
</sectionHeader>
<bodyText confidence="0.9999234">
The purpose of the research described in this section is to experiment with the au-
tomatic acquisition of semantic tags for words in a sublanguage, tags well beyond
that available from the seeding of MRDs. The identification of semantic tags is the
result of type coercion on known syntactic forms, to induce a semantic feature, such
as [+event] or [+object].
</bodyText>
<subsectionHeader confidence="0.996783">
4.1 Coercive Environments in Corpora
</subsectionHeader>
<bodyText confidence="0.9999803">
A pervasive example of type coercion is seen in the complements of aspectual verbs
such as begin and finish, and verbs such as enjoy. That is, in sentences such as &amp;quot;John
began the book,&amp;quot; the normal complement expected is an action or event of some sort,
most often expressed by a gerundive or infinitival phrase: &amp;quot;John began reading the
book,&amp;quot; &amp;quot;John began to read the book.&amp;quot; In Pustejovsky (1991) it was argued that in such
cases, the verb need not have multiple subcategorizations, but only one deep semantic
type, in this case, an event. Thus, the verb coerces its complement (e.g. &amp;quot;the book&amp;quot;)
into an event related to that object. Such information can be represented by means of
a representational schema called qualia structure, which, among other things, specifies
the relations associated with objects.
</bodyText>
<page confidence="0.990114">
345
</page>
<table confidence="0.999506952380952">
Computational Linguistics Volume 19, Number 2
count verb object
205 begin career
176 begin day
159 begin work
140 begin talk
120 begin campaign
113 begin investigation
106 begin process
92 begin program
85 begin operation
85 begin negotiation
66 begin strike
64 begin production
59 begin meeting
59 begin term
50 begin visit
45 begin test
39 begin construction
31 begin debate
29 begin trial
</table>
<figureCaption confidence="0.866913">
Figure 4
</figureCaption>
<bodyText confidence="0.982935909090909">
Counts for objects of begin/V.
In related work being carried out with Mats Rooth of the University of Stuttgart,
we are exploring what the range of coercion types is, and what environments they
may appear in, as discovered in corpora. Some of our initial data suggest that the
hypothesis of deep semantic selection may in fact be correct, as well as indicating
what the nature of the coercion rules may be. Using techniques described in Church
and Hindle (1990), Church and Hanks (1990), and Hindle and Rooth (1991), Figure 4
shows some examples of the most frequent V-0 pairs from the AP corpus.
Corpus studies confirm similar results for &amp;quot;weakly intensional contexts&amp;quot; such as
the complement of coercive verbs such as veto. These are interesting because regard-
less of the noun type appearing as complement, it is embedded within a semantic
interpretation of &amp;quot;the proposal to,&amp;quot; thereby clothing the complement within an inten-
sional context. The examples in Figure 5 with the verb veto indicate two things: first,
that such coercions are regular and pervasive in corpora; second, that almost anything
can be vetoed, but that the most frequently occurring objects are closest to the type
selected by the verb.
What these data show is that the highest count complement types match the type
required by the verb; namely, that one vetoes a bill or proposal to do something, not
the thing itself. These nouns can therefore be used with some predictive certainty for
inducing the semantic type in coercive environments such as &amp;quot;veto the expedition.&amp;quot;
This work is still preliminary, however, and requires further examination (Pustejovsky
and Rooth [unpublished]).
</bodyText>
<subsectionHeader confidence="0.99788">
4.2 Induction of Semantic Relations from Syntactic Forms
</subsectionHeader>
<bodyText confidence="0.790624">
In this section, we present another experiment indicating the feasibility of inducing
semantic tags for lexical items from corpora.&apos; Imagine being able to take the V-0 pairs
</bodyText>
<footnote confidence="0.526163">
14 This section presents an abridged version of material reported on in Pustejovsky (1992).
</footnote>
<page confidence="0.974704">
346
</page>
<table confidence="0.936309">
James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis
count verb object
303 veto bill
84 veto legislation
58 veto measure
35 veto resolution
21 veto law
14 veto item
12 veto decision
9 veto proposal
9 veto plan
</table>
<figure confidence="0.974840615384615">
7 veto package
6 veto increase
5 veto sanction
5 veto penalty
4 veto notice
4 veto idea
4 veto appropriation
4 veto mission
4 veto attempt
3 veto search
3 veto cut
3 veto deal
1 veto expedition
</figure>
<figureCaption confidence="0.971619">
Figure 5
</figureCaption>
<bodyText confidence="0.966750125">
Counts for objects of veto/V.
such as those given in Section 4.1, and then applying semantic tags to the verbs that
are appropriate to the role they play for that object (i.e., induction of the qualia roles
for that noun). This is similar to the experiment reported on in Section 3. Here we
apply a similar technique to a much larger corpus, in order to induce the agentive role
for nouns; that is, the semantic predicate associated with bringing about the object.
In this example we look at the behavior of noun phrases and the prepositional
phrases that follow them. In particular, we look at the co-occurrence of nominals with
between, with, and to. Table 1 shows results of the conflating noun plus preposition
patterns. The percentage shown indicates the ratio of the particular collocation to the
key word. Mutual information (MI) statistics for the two words in collocation are also
shown. What these results indicate is that induction of semantic type from conflating
syntactic patterns is possible. Based on the semantic types for these prepositions, the
syntactic evidence suggests that there is an equivalence class where each preposition
makes reference to a symmetric relation between the arguments in the following two
patterns:
</bodyText>
<listItem confidence="0.999499666666667">
• Z with y = ARzAx3y[Rz(x, y) A Rz(Y, X)1
• Z between x and y =
ARza,y[Rz(x, y) A RZ(y)x)]
</listItem>
<bodyText confidence="0.9661312">
We then take these results and, for those nouns where the association ratios for N
with and N between are similar, we pair them with the set of verbs governing these
&amp;quot;NP PP&amp;quot; combinations in corpus, effectively partitioning the original V-0 set into
[+agentive] predicates and [—agentive] predicates.
These are semantic n-grams rather than direct interpretations of the prepositions.
</bodyText>
<page confidence="0.995326">
347
</page>
<note confidence="0.456368">
Computational Linguistics Volume 19, Number 2
</note>
<tableCaption confidence="0.883388">
Table 1
Mutual information for noun + preposition patterns.
</tableCaption>
<table confidence="0.998165304347826">
Word Word Word Word Word Word Word Word
+ to + with + between + to + with + between
(%)/MI (%)/MI (%)/MI (%)/MI (%)/MI (%)/MI
agreement .117 .159 .028 expansion .013 .007 0
1.512 3.423 3.954 -.666 .381 n/a
announcement .010 .003 0 impasse 0 .064 .096
-.918 -.409 n/a n/a 2.520 5.192
barrier .215 0 .030 interactions 0 0 .250
2.117 n/a 4.046 n/a n/a 6.141
competition .019 .028 .021 market .013 .006 .000
-.269 1.701 3.666 -.637 .240 -.500
confrontation .029 .283 .074 range .005 .002 .020
.141 4.000 4.932 -1.533 -.618 3.663
contest .052 .052 .039 relations .009 .217 .103
.715 2.323 4.301 -1.017 3.736 5.254
contract .066 .060 .002 settlement .013 .091 .012
.947 2.463 1.701 -.626 2.868 3.142
deal .028 .193 .004 talks .029 .218 .030
.086 3.616 2.015 .138 3.740 4.043
dialogue 0 .326 .152 venture .032 .105 .035
n/a 4.140 5.644 .226 3.008 4.185
difference .017 .009 .348 war .010 .041 .015
-.410 .638 6.474 -.937 2.079 3.372
</table>
<bodyText confidence="0.9978092">
What these expressions in effect indicate is the range of semantic environments they
will appear in. That is, in sentences like those in Example 16, the force of the relational
nouns agreement and talks is that they are unsaturated for the predicate bringing about
this relation. In 17, on the other hand, the NPs headed by agreement and talks are
saturated in this respect.
</bodyText>
<listItem confidence="0.7860325">
Example 16
a. John made an agreement with Mary.
b. Apple opened talks with IBM.
Example 17
a. This is an agreement between John and Mary.
b. Those were the first talks between Apple and IBM.
</listItem>
<bodyText confidence="0.998974333333333">
If our hypothesis is correct, we expect that verbs governing nominals collocated with
a with-phrase will be mostly those predicates referring to the agentive quale of the
nominal. This is because the with-phrase is unsaturated as a predicate, and acts to
</bodyText>
<page confidence="0.993925">
348
</page>
<note confidence="0.791401">
James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis
</note>
<figure confidence="0.8845095">
count verb object
19 form venture
3 announce venture
3 enter venture
2 discuss venture
1 be venture
1 abandon venture
1 begin venture
1 complete venture
1 negotiate venture
1 start venture
1 expect venture
</figure>
<figureCaption confidence="0.892074">
Figure 6
</figureCaption>
<subsectionHeader confidence="0.768788">
Verb-object pairs with prep = with.
</subsectionHeader>
<bodyText confidence="0.997841285714286">
identify the agent of the verb as its argument (cf. Nilsen (1973)). This is confirmed by
our data, shown in Figure 6.
Conversely, verbs governing nominals collocating with a between-phrase will not
refer to the agentive since the phrase is saturated already. Indeed, the only verb occur-
ring in this position with any frequency is the copula be, namely with the following
counts: 12 be/V venture/O. Thus, weak semantic types can be induced on the basis
of syntactic behavior.
There is a growing literature on corpus-based acquisition and tuning (Smadja
1991a; Zernik and Jacobs 1991; Brent 1991; as well as Grishman and Sterling 1992).
We share with these researchers a general dependence on well-behaved collocational
patterns and distributional structures. Probably the main distinguishing feature of our
approach is its reliance on a fairly well studied semantic framework to aid and guide
the semantic induction process itself, whether it involves selectional restrictions or
semantic types.
</bodyText>
<sectionHeader confidence="0.576801" genericHeader="method">
5. Lexical Presuppositions and Preferences
</sectionHeader>
<bodyText confidence="0.999982769230769">
In the previous section we presented algorithms for extracting collocational informa-
tion from corpora, in order to supplement and fine-tune the lexical structures seeded
by a machine-readable dictionary. In this section we demonstrate that, in addition to
conventional lexical semantic relations, it is also possible to acquire information con-
cerning lexical presuppositions and preferences from corpora, when analyzed with
the appropriate semantic tools. In particular, we will discuss a phenomenon we call
discourse polarity, and how corpus-based experiments provide clues toward the repre-
sentation of this phenomenon, as well as information on preference relations.
As we have seen, providing a representational system for lexical semantic relations
is a nontrivial task. Representing presuppositional information, however, is even more
daunting. Nevertheless, there are some systematic semantic generalizations associated
with such subtle lexical inferences. To illustrate this, consider the following examples
taken from the Wall Street Journal Corpus, involving the verb insist.
</bodyText>
<footnote confidence="0.407166">
Example 18
But Mr. Fourtou insisted that the restructuring plans hadn&apos;t played a role in his decision.
</footnote>
<page confidence="0.991545">
349
</page>
<figure confidence="0.815793090909091">
Computational Linguistics Volume 19, Number 2
Example 19
But so far, the majority is insisting that a daily paper in the home is an essential
educational resource that Mr. Oshry must have, like it or not.
Example 20
But Mr. Nishi insists there is a common theme to his scattered projects: to improve
and spread personal computers.
Example 21
&amp;quot;Mister, Djemaa is a crazy place for you,&amp;quot; insists the first of many young men, clutching
a visitor&apos;s sleeve.
Example 22
</figure>
<bodyText confidence="0.5445805">
But the BNL sources yesterday insisted that the head office was aware of only a small
portion of the credits to Iraq made by Atlanta.
</bodyText>
<equation confidence="0.38577">
Example 23
</equation>
<bodyText confidence="0.673151666666667">
Mr. Smale, who ordinarily insists on a test market before a national roll-out, told the
team to go ahead—although he said he was skeptical that Pringle&apos;s could survive, Mr.
Tucker says.
</bodyText>
<equation confidence="0.549784">
Example 24
</equation>
<bodyText confidence="0.993393681818182">
The Cantonese insist that their fish be &amp;quot;fresh,&amp;quot; though one whiff of Hong Kong harbor
and the visitor may yearn for something shipped from distant seas.
Example 25
Money isn&apos;t the issue, Mr. Bush insists.
From analyzing these and similar data, a pattern emerges concerning the use of verbs
like insist in discourse; namely, the co-occurrence with discourse markers denoting
negative affect, such as although and but, as well as literal negatives, e.g., no and not.
This is reminiscent of the behavior of negative polarity items such as any more and at all.
Such lexical items occur only in the context of negatives within a certain structural
configuration.&apos; In a similar way, verbs such as insist seem to require an overt or
implicit negation within the immediate discourse context, rather than within the clause.
For this reason, we will call such verbs discourse polarity items.
For our purposes, the significance of such data is twofold: first, experiments on
corpora can test and confirm linguistic intuitions concerning a subtle semantic judg-
ment; second, if such knowledge is in fact so systematic, then it must be at least
partially represented in the lexical semantics of the verb.
To test whether the intuitions supported by the above data could be confirmed
in corpora, Bergler (1991) derived the statistical co-occurrence of insist with discourse
polarity markers in the 7 million-word corpus of Wall Street Journal articles. She derived
the statistics reported in Figure 7.
Let us assume, on the basis of this preliminary date presented in Bergler (1992)
that these verbs in fact do behave as discourse polarity items. The question then
</bodyText>
<page confidence="0.709526333333333">
15 There is a rich literature on this topic. For discussion see Ladusaw (1980) and Linebarger (1980).
16 Overlap between the categories occurs in less than 35 cases.
350
</page>
<note confidence="0.497114">
Lexical Semantic Techniques for Corpus Analysis
James Pustejovsky et al.
</note>
<sectionHeader confidence="0.593342" genericHeader="method">
Keywords Count Comments
</sectionHeader>
<bodyText confidence="0.779415285714286">
insist 586 occurrences throughout the corpus
insist on 109 these have been cleaned by hand and are actually oc-
currences of the idiom insist on rather than accidental
co-occurrences.
insist &amp; but 117 occurrences of both insist and but in the same sentence
insist &amp; negation 186 includes not and n&apos;t
insist Sr subjunctive 159 includes would, could, should, and be
</bodyText>
<figureCaption confidence="0.997981">
Figure 7
</figureCaption>
<bodyText confidence="0.8535685">
Negative markers with insist in WSJC
immediately arises as to how we represent this type of knowledge. Using the language
of the qualia structure discussed above, we can make explicit reference to the polarity
behavior, in the following informal but intuitive representation for the verb insist!&apos;
</bodyText>
<equation confidence="0.999313">
[insist(xind,y:prop)
FORMAL = REPORTING—VERB—LCP
TELIC = say(x,true(y)) &amp; presupposed(0) &amp; y =
</equation>
<bodyText confidence="0.99940052173913">
This entry states that in the REPORTING—VERB sense of the word, insist is a relation
between an individual and a statement that is the negation of a proposition, /p, pre-
supposed in the context of the utterance. As argued in Pustejovsky (1991) and Miller
and Fellbaum (1991), such simple oppositional predicates form a central part of our
lexicalization of concepts. Semantically motivated collocations such as these extracted
from large corpora can provide presuppositional information for words that would
otherwise be missing from the lexical semantics of an entry. While full automatic ex-
traction of semantic collocations is not yet feasible, some recent research in related
areas is promising.
Hindle (1990) reports interesting results of this kind based on literal collocations,
where he parses the corpus (Hindle 1983) into predicate—argument structures and
applies a mutual information measure (Fano 1961; Magerman and Marcus 1990) to weigh
the association between the predicate and each of its arguments. For example, as a
list of the most frequent objects for the verb drink in his corpus, Hindle found beer,
tea, Pepsi, and champagne. Based on the distributional hypothesis that the degree of
shared contexts is a similarity measure for words, he develops a similarity metric
for nouns based on their substitutability in certain verb contexts. Hindle thus finds
sets of semantically similar nouns based on syntactic co-occurrence data. The sets
he extracts are promising; for example, the ten most similar nouns to treaty in his
corpus are agreement, plan, constitution, contract, proposal, accord, amendment, rule, law,
and legislation.
This work is very close in spirit to our own investigation here; the emphasis on
syntactic co-occurrence enables Hindle to extract his similarity lists automatically; they
</bodyText>
<footnote confidence="0.962328666666667">
17 For illustration, we use an abbreviated version of the lexical entries under discussion, highlighting only
certain qualia for the verbs. For the most recent representation of verbal semantics in this framework,
see Pustejovsky (1993).
</footnote>
<page confidence="0.990833">
351
</page>
<note confidence="0.662087">
Computational Linguistics Volume 19, Number 2
</note>
<bodyText confidence="0.9990879">
are therefore easy to compile for different corpora, different sublanguages, etc. Here
we are attempting to use these techniques together with a model of lexical meaning,
to capture deeper lexical semantic collocations; e.g., the generalization that the list of
objects occurring for the word drink contains only liquids.
In the final part of this section, we turn to how the analysis of corpora can provide
lexical semantic preferences for verb selection. As discussed above, there is a growing
body of research on deriving collocations from corpora (cf. Church and Hanks 1990;
Klavans, Chodorow, and Wacholder 1990; Wilks et al. 1993; Smadja 1991a, 1991b; Cal-
zolari and Bindi 1990). Here we employ the tools of semantic analysis from Section 1
to examine the behavior of metonymy with reporting verbs. We will show, on the ba-
sis of corpus analysis, how verbs display marked differences in the ability to license
metonymic operations over their arguments. Such information, we argue, is part of
the preference semantics for a sublanguage, as automatically derived from corpus.
Metonymy can be seen as a case of &amp;quot;licensed violation&amp;quot; of selectional restric-
tions. For example, while the verb announce selects for a human subject, sentences like
The Phantasie Corporation announced third quarter losses are not only an acceptable para-
phrase of the selectionally correct form Mr. Phantasie Jr. announced third quarter losses
for Phantasie Corp, but they are the preferred form in the Wall Street Journal). This is an
example of subject type coercion, as discussed in Section 1. For example, the qualia
structure for a noun such as corporation might be represented as below:
</bodyText>
<equation confidence="0.9976654">
corporation(x)
CONST = group(y),spokesperson(w),executive(z)
FORMAL = organization(x)
TELIC = execute(z,decisions)
AGENTIVE = incorporate(m)
</equation>
<bodyText confidence="0.999947736842105">
The metonymic extension in this example is straightforward: a spokesman, executive,
or otherwise legitimate representative &amp;quot;speaking for&amp;quot; a company or institution can be
metonymically replaced by that company or institution.&apos;
We find that this type of metonymic extension for the subject is natural and indeed
very frequent with reporting verbs Bergler (1991), such as announce, report, release, and
claim, while it is in general not possible with other verbs selecting human subjects, e.g.,
the verbs of contemplation (such as contemplate, consider, and think). However, there
are subtle differences in the occurrence of such metonymies for the different members
of the same semantic verb class that arise from corpus analysis.
A reporting verb is an utterance verb that is used to relate the words of a source. In
a careful study of seven reporting verbs on a 250,000-word corpus of Time magazine
articles from 1963, we found that the preference for different metonymic extensions
varies considerably within this field (Bergler 1991). Figure 8 shows the findings for the
words insist, deny, admit, claim, announce, said, and told for two metonymic extensions,
namely where a group stands for an individual (Analysts said . . .) and where a company
or other institution stands for the individual (IBM announced ... ).19
The difference in patterns of metonymic behavior is quite striking: semantically
similar verbs seem to pattern similarly over all three categories; admit, insist, and deny
show a closer resemblance to each other than to any of the others, while said and
</bodyText>
<footnote confidence="0.9768194">
18 Note, however, that the metonymic extension is not quite as simple as extending from any employee to
the whole company or institution, but that a form of legitimation has to be involved.) For more detail
see Bergler (1992).
19 The data for Figure 8 have been screened to ensure that only occurrences that constitute reporting
contexts were used.
</footnote>
<page confidence="0.984259">
352
</page>
<note confidence="0.853681">
James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis
</note>
<table confidence="0.988606076923077">
person group instit. other
admit 64% 19% 14% 2%
deny 59% 11% 19% 11%
insist 57% 24% 16% 3%
announce 51% 10% 31% 8%
claim 35% 21% 38% 6%
said 83% 6% 4% 8%
told 69% 7% 8% 16%
Figure 8
Preference for different metonymies in subject position.
person group institution other
WSJ 49% 15% 34% 2%
TIME 83% 6% 4% 8%
</table>
<figureCaption confidence="0.897426">
Figure 9
</figureCaption>
<bodyText confidence="0.989391090909091">
Preference for metonymies for said in a 160,000-word fragment of the Wall Street Journal
corpus.
told form a category by themselves. There may be a purely semantic explanation why
said and told seem not to prefer the metonymic use in subject position; e.g., perhaps
these verbs relate more closely to the act of uttering, or perhaps they are too informal,
stylistically. Evidence from other corpora, however, suggests that such information is
accurately characterized as lexical preference. An initial experiment on a subset of the
Wall Street Journal Corpus, for example, shows that said has a quite different metonymic
distribution there, reported in Figure 9.
In this corpus we discovered that subject selection for an individual person ap-
peared in only 50% of the sentences, while a company/institution appeared in 34%
of the cases. This difference could either be attributed to a difference in style between
Time magazine and the Wall Street Journal or perhaps to a difference in general usage
between 1963 and 1989. The statistics presented here can of course not determine the
reason for the difference, but rather help establish the lexical semantic preferences that
exist in a certain corpus and sublanguage.
An important question related to the extraction of preference information is what
the corpus should be. Recent effort has been spent constructing balanced corpora, con-
taining text from different styles and sources, such as novels, newspaper texts, scien-
tific journal articles, etc. The assumption is of course that given a representative mix of
samples of language use, we can extract the general properties and usage of words.
But if we gain access to sophisticated automatic corpus analysis tools such as those
</bodyText>
<page confidence="0.997001">
353
</page>
<note confidence="0.662748">
Computational Linguistics Volume 19, Number 2
</note>
<bodyText confidence="0.999705666666667">
discussed above, and indeed if we have specialized algorithms for sublanguage ex-
traction, then homogeneous corpora might provide better data. The few examples of
lexical preference mentioned in this section might not tell us anything conclusive for
the definitive usage of a word such as said, if there even exists such a notion. Nev-
ertheless the statistics provide an important tool for text analysis within the corpus
from which they are derived. Because we can systematically capture the violation of
selectional restrictions (as semantically predicted), there is no need for a text analy-
sis system to perform extensive commonsense inferencing. Thus, such presupposition
and preference statistics are vital to efficient processing of real text.
</bodyText>
<sectionHeader confidence="0.615842" genericHeader="conclusions">
6. Summary and Discussion
</sectionHeader>
<bodyText confidence="0.999588538461538">
In this paper we have presented a particularly directed program of research for how
text corpora can contribute to linguistics and computational linguistics. We first pre-
sented a representation language for lexical knowledge, the generative lexicon, and
demonstrated how it facilitates the structuring of lexical relations among words, look-
ing in particular at the problems of metonymy and polysemy.
Such a framework for lexical knowledge suggests that there are richer relation-
ships among words in text beyond that of simple co-occurrence that can be extracted
automatically. The work suggests how linguistic phenomena such as metonymy and
polysemy might be exploited for knowledge acquisition for lexical items. Unlike purely
statistical collocational analyses, the framework of a semantic theory allows the auto-
matic construction of predictions about deeper semantic relationships among words
appearing in collocational systems.
We illustrated the approach for the acquisition of lexical information for several
classes of nominals, and how such techniques can fine-tune the lexical structures ac-
quired from an initial seeding of a machine-readable dictionary. In addition to conven-
tional lexical semantic relations, we then showed how information concerning lexical
presuppositions and preference relations can also be acquired from corpora, when
analyzed with the appropriate semantic tools.
In conclusion, we feel that the application of computational resources to the anal-
ysis of text corpora has and will continue to have a profound effect on the direction
of linguistic and computational linguistic research. Unlike previous attempts at cor-
pus research, the current focus is supported and guided by theoretical tools, and not
merely statistical techniques. We should furthermore welcome the ability to expand
the data set used for the confirmation of linguistic hypotheses. At the same time, we
must remember that statistical results themselves reveal nothing, and require careful
and systematic interpretation by the investigator to become linguistic data.
</bodyText>
<sectionHeader confidence="0.894659" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.966954428571429">
This research was supported by DARPA
contract MDA904-91-C-9328. We would like
to thank Scott Waterman for his assistance
in preparing the statistics. We would also
like to thank Mats Rooth, Scott Waterman,
and four anonymous reviewers for useful
comments and discussion.
</bodyText>
<sectionHeader confidence="0.928902" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.921504">
Ahlswede, T., and Evens, M. (1988).
&amp;quot;Generating a relational lexicon from a
machine-readable dictionary.&amp;quot; International
Journal of Lexicography, 1(3), 214-237.
Alshawi, H. (1987). &amp;quot;Processing dictionary
definitions with phrasal pattern
hierarchies.&amp;quot; Computational Linguistics, 13,
3-4.
Alshawi, H.; Boguraev, B.; and Briscoe, T.
(1985). &amp;quot;Towards a dictionary support
environment for real time parsing.&amp;quot; In
Proceedings, European Conference on
Computational Linguistics. Geneva,
Switzerland.
Amsler, R. A. (1980). The structure of the
Merriam-Webster Pocket Dictionary.
Doctoral dissertation, University of Texas.
</reference>
<page confidence="0.998626">
354
</page>
<note confidence="0.981238">
James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis
</note>
<table confidence="0.7256551">
Amsler, R. A. (1989). &amp;quot;Third generation
computational lexicology.&amp;quot; In Proceedings,
First International Lexical Acquisition
Workshop. Detroit, Michigan, August 1989.
Amsler, R. A., and White, J. S. (1979).
&amp;quot;Development of a computational
methodology for deriving natural
language semantic structures via analysis
of machine-readable dictionaries.&amp;quot; NSF
Technical Report MCS77-01315.
Anick, P. (1992). &amp;quot;Lexicon assisted
information retrieval for the help-desk.&amp;quot;
In Proceedings, IEEE CAIA-92 Workshop on
Al and Help-Desks. Monterey, California.
Anick, P.; Brennan, J.; Flynn, R.; Hanssen,
D.; Alvey, B.; and Robbins, J. (1989). &amp;quot;A
direct manipulation interface for Boolean
information retrieval via natural language
query&amp;quot; In Proceedings, SIGIR &apos;89.
Anick, P., and Pustejovsky, J. (1990). &amp;quot;An
application of lexical semantics to
knowledge acquisition from corpora.&amp;quot; In
Proceedings, 13th International Conference of
Computational Linguistics. Helsinki,
Finland.
Atkins, B. T. (1991). &amp;quot;Building a lexicon:
Reconciling anisomorphic sense
differentiations in machine-readable
dictionaries.&amp;quot; International Journal of
Lexicography.
</table>
<reference confidence="0.984562913043478">
Atkins, B. T., and Levin, B. (1991).
&amp;quot;Admitting impediments.&amp;quot; In Lexical
Acquisition: Using On-Line Resources to
Build a Lexicon, edited by U. Zernik. LEA.
Bergler, S. (1991). &amp;quot;The semantics of
collocational patterns for reporting
verbs.&amp;quot; In Proceedings, Fifth Conference of
the European Chapter of the Association for
Computational Linguistics. Berlin, Germany,
April 1991.
Bergler, S. (1992). &amp;quot;Evidential analysis of
reported speech.&amp;quot; Doctoral dissertation,
Brandeis University.
Bierwisch, M. (1983). &amp;quot;Semantische und
konzeptuelle Reprasentationen
lexikalischer Einheiten.&amp;quot; In
Untersuchungen zur Semantik, edited by
R. Ruzicka and W. Motsch.
Akademische-Verlag.
Binot, J.-L., and Jensen, K. (1987). &amp;quot;A
semantic expert using an online standard
dictionary.&amp;quot; In Proceedings, 10th
International Joint Conference on Artificial
Intelligence (IJCAI-87). Milan, Italy,
709-714.
Boguraev, B. (1979). &amp;quot;Automatic resolution
of linguistic ambiguities.&amp;quot; Technical
Report No. 11, University of Cambridge
Computer Laboratory, Cambridge, U.K.
Boguraev, B. (1991). &amp;quot;Building a lexicon: The
contribution of computers.&amp;quot; International
Journal of Lexicography, 4(3).
Boguraev, B., and Briscoe, T. (1989).
&amp;quot;Introduction.&amp;quot; In Computational
Lexicography for Natural Language
Processing, edited by B. Boguraev and
T. Briscoe. Longman Group UK.
Boguraev, B., and Briscoe, T. (1987). &amp;quot;Large
lexicons for natural language processing:
Exploring the grammar coding system of
LDOCE.&amp;quot; Computational Linguistics, 13.
Boguraev, B.; Byrd, R.; Klavans, J.; and Neff,
M. (1989). &amp;quot;From machine readable
dictionaries to a lexical knowledge base.&amp;quot;
In Proceedings, First International Lexical
Acquisition Workshop. Detroit, Michigan,
August 1989.
Boguraev, B., and Pustejovsky, J. (1990).
&amp;quot;Lexical ambiguity and the role of
knowledge representation in lexicon
design.&amp;quot; In Proceedings, 13th International
Conference of Computational Linguistics.
Helsinki, Finland, August 1990.
Brent, M. (1991). &amp;quot;Automatics semantic
classification of verbs from their syntactic
contexts: An implemented classifier for
stativity.&amp;quot; In Proceedings, Fifth Conference of
the European Chapter of the Association for
Computational Linguistics. Berlin, Germany,
April 1991.
Briscoe, E.; Copestake, A.; and Boguraev, B.
(1990). &amp;quot;Enjoy the paper: Lexical
semantics via lexicology.&amp;quot; In Proceedings,
13th International Conference on
Computational Linguistics. Helsinki,
Finland, 42-47.
Byrd, R.; Calzolari, N.; Chodorow, M.;
Klavans, J.; Neff, M.; and Rizk, 0. (1987).
&amp;quot;Tools and methods for computational
lexicology.&amp;quot; Computational Linguistics,
13(3-4), 219-240.
Byrd, R. (1989). &amp;quot;Discovering relationships
among word senses.&amp;quot; In Proceedings, Fifth
Annual Conference of the UW Centre for the
New Oxford English Dictionary. Oxford,
U.K., 67-80.
Calzolari, N. (1984). &amp;quot;Detecting patterns in a
lexical database.&amp;quot; In Proceedings, Seventh
International Conference on Computational
Linguistics (COLING-84). Stanford,
California.
Calzolari, N., and Bindi, R. (1990).
&amp;quot;Acquisition of lexical information from a
large textual Italian corpus.&amp;quot; In
Proceedings, 13th International Conference on
Computational Linguistics (COLING-90).
Helsinki, Finland.
Cardelli, L., and Wegner, P. (1985). &amp;quot;On
understanding types, data abstraction,
and polymorphism.&amp;quot; ACM Computing
Surveys, 17(4), 471-522.
Chodorow, M.; Byrd, R.; and Heidorn, G.
</reference>
<page confidence="0.993962">
355
</page>
<note confidence="0.601119">
Computational Linguistics Volume 19, Number 2
</note>
<reference confidence="0.998688540983607">
(1985). &amp;quot;Extracting semantic hierarchies
from a large on-line dictionary.&amp;quot; In
Proceedings, 23rd Annual Meeting of the
ACL. Chicago, Illinois, 299-304.
Church, K. (1988). &amp;quot;A stochastic parts
program and noun phrase parser for
unrestricted text.&amp;quot; In Proceedings, Second
Conference on Applied Natural Language
Processing. Austin, Texas.
Church, K., and Hanks, P. (1990). &amp;quot;Word
association norms, mutual information,
and lexicography.&amp;quot; Computational
Linguistics, 16(1), 22-29.
Church, K., and Hindle, D. (1990).
&amp;quot;Collocational constraints and
corpus-based linguistics.&amp;quot; In Working
Notes of the AAAI Symposium: Text-Based
Intelligent Systems. Stanford, California.
Copestake, Ann (in press). &amp;quot;Defaults in the
LKB.&amp;quot; In Default Inheritance in the Lexicon,
edited by T. Briscoe and A. Copestake.
Cambridge University Press.
Copestake, A., and Briscoe, E. (1992).
&amp;quot;Lexical operations in a unification-based
framework.&amp;quot; In Lexical Semantics and
Knowledge Representation, edited by
J. Pustejovsky and S. Bergler. Springer
Verlag.
Cowie, J.; Guthrie, L.; and Pustejovsky, J.
(1992). &amp;quot;Description of the MUCBRUCE
system as used for MUC-4.&amp;quot; In
Proceedings, Fourth Message Understanding
Conference (MUC-4). Morgan Kaufmann.
Croft, W. B. (1989). &amp;quot;Automatic indexing.&amp;quot;
In Indexing: The State of Our Knowledge and
the State of Our Ignorance, edited by
B. H. Weinberg, 87-100. Learned
Information, Inc.
Cruse, D. A. (1986). Lexical Semantics.
Cambridge University Press.
Cruse, D. A. (1992). &amp;quot;Polysemy and related
phenomena from a cognitive linguistic
viewpoint.&amp;quot; In Proceedings, Second Toulouse
Workshop on Lexical Semantics, edited by
P. Saint-Dizier and E. Viegas. Toulouse,
France.
Debili, F.; Fluhr, C.; and Radasoa, P. (1988).
&amp;quot;About reformulation in full-text IRS.&amp;quot; In
Proceedings, RIA0-88, 343-357.
Evans, R., and Gazdar, G., editors. (1990).
&amp;quot;The DATR papers: February 1990.&amp;quot;
Cognitive Science Research Paper CSRP
139, School of Cognitive and Computing
Sciences, University of Sussex.
Evens, M. (1987). Relational Models of the
Lexicon. Cambridge University Press.
Fano, R. (1961). Transmission of Information: A
Statistical Theory of Communications. MIT
Press.
Fillmore, C. (1968). &amp;quot;The case for case.&amp;quot; In
Universals in Linguistics Theory, edited by
E. Bach and R. Harms. Holt, Rinehart and
Winston.
Grishman, R.; Hirschman, L.; and Nhan,
N. T. (1986). &amp;quot;Discovery procedures for
sublanguage selectional patterns: Initial
experiments.&amp;quot; Computational Linguistics,
12(3), 205-215.
Grishman, R., and Sterling, J. (1992).
&amp;quot;Acquisition of selectional patterns.&amp;quot; In
Proceedings, 14th International Conference on
Computational Linguistics (COLING-92).
Nantes, France, July 1992.
Guthrie, L.; Slator, B.; Wilks, Y.; and Bruce,
R. (1990). &amp;quot;Is there content in Empty
Heads?&amp;quot; In Proceedings, 13th International
Conference of Computational Linguistics
(COLING-90). Helsinki, Finland.
Hindle, D. 1983. &amp;quot;Deterministic parsing of
syntactic non-fluencies.&amp;quot; In Proceedings,
21st Annual Meeting of the Association for
Computational Linguistics, Cambridge,
Massachusetts, June 1983,123-128.
Hindle, D. (1990). &amp;quot;Noun classification from
predicate-argument structures.&amp;quot; In
Proceedings, 28th Annual Meeting of the
Association for Computational Linguistics,
Pittsburgh, Pennsylvania, June 1990,
268-275.
Hindle, D., and Rooth, M. (1991).
&amp;quot;Structural ambiguity and lexical
relations.&amp;quot; In Proceedings of the ACL.
Jacobs, P. (1991). &amp;quot;Making sense of lexical
acquisition.&amp;quot; In Lexical Acquisition: Using
On-Line Resources to Build a Lexicon, edited
by U. Zernik, LEA.
Klavans, J.; Chodorow, M.; and Wacholder,
N. (1990). &amp;quot;From dictionary to knowledge
base via taxonomy.&amp;quot; In Proceedings, Sixth
Conference of the UW Centre for the New
OED. Waterloo, 110-132.
Krovetz, R., and Croft, W. B. (1989). &amp;quot;Word
sense disambiguation using
machine-readable dictionaries.&amp;quot; In
Proceedings, SIGIR. 127-136.
Ladusaw, W. (1980). Polarity Sensitivity as
Inherent Scope Relations. Indiana University
Linguistics Club.
Lakoff, G. (1968). &amp;quot;Instrumental adverbs
and the concept of deep structure.&amp;quot;
Foundations of Language, 4,4-29.
Lakoff, G. (1970). Irregularity in Syntax. Holt,
Rinehart, and Winston.
Lakoff, G. (1987). Women, Fire, and Dangerous
Objects. University of Chicago Press.
Linebarger, M. (1980). &amp;quot;The grammar of
negative polarity.&amp;quot; Doctoral dissertation,
MIT, Cambridge MA.
Maarek, Y. S., and Smadja, F. Z. (1989). &amp;quot;Full
text indexing based on lexical relations,
an application: Software libraries.&amp;quot; In
Proceedings, SIGIR. 127-136.
</reference>
<page confidence="0.998862">
356
</page>
<note confidence="0.990584">
James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis
</note>
<reference confidence="0.999799639344263">
Markowitz, J.; Ahlswede, T.; and Evens, M.
(1986). &amp;quot;Semantically significant patterns
in dictionary definitions.&amp;quot; In Proceedings,
24th Annual Meeting of the Association for
Computational Linguistics. New York, New
York, 112-119.
Magerman, D., and Marcus, M. (1990).
&amp;quot;Parsing a natural language using mutual
information statistics.&amp;quot; In Proceedings,
Eighth National Conference on Artificial
Intelligence (AAAI-90). Boston,
Massachusetts.
Martin, J. (1990). A Computational Model of
Metaphor Interpretation. Academic Press.
Mertuk, I. (1988). Dependency Syntax. SUNY
Press.
Miller, G., and Fellbaum, C. (1991). &amp;quot;Verbs
in WordNet.&amp;quot; Cognition.
Nakamura, J., and Nagao, M. (1988).
&amp;quot;Extraction of semantic information from
an ordinary English dictionary and its
evaluation.&amp;quot; In Proceedings, COLING-88.
Budapest, Hungary, 459-464.
Nilsen, D. L. F. (1973). The Instrumental Case
in English: Syntactic and Semantic
Considerations. Mouton.
Nirenburg, S., and Nirenburg, I. (1988). &amp;quot;A
framework for lexical selection in natural
language generation.&amp;quot; In Proceedings,
COLING-88. Budapest, Hungary.
Nunberg, G. (1978). The Pragmatics of
Reference. Indiana University Linguistics
Club.
Procter, P.; Ilson, R. F.; and Ayto, J. (1978).
Longman Dictionary of Contemporary
English. Longman Group Limited.
Pustejovsky, J. (1989). &amp;quot;Issues in
computational lexical semantics.&amp;quot; In
Proceedings, Fourth Conference of the
European Chapter of the ACL. Manchester,
England, April 1989.
Pustejovsky, J. (1991). &amp;quot;The generative
lexicon.&amp;quot; Computational Linguistics, 17(4),
409-441.
Pustejovsky, J. (1992). &amp;quot;The acquisition of
lexical semantic knowledge from large
corpora.&amp;quot; In Proceedings, DARPA Spoken
and Written Language Workshop. Morgan
Kaufmann.
Pustejovsky, J. (1993). &amp;quot;Linguistic constraints
on type coercion.&amp;quot; In Computational Lexical
Semantics, edited by P. Saint-Dizier and
E. Viegas. Cambridge University Press.
Pustejovsky, J. (in press). The Generative
Lexicon: A Theory of Computational Lexical
Semantics. MIT Press.
Pustejovsky, J., and Anick, P. (1988). &amp;quot;The
semantic interpretation of nominals.&amp;quot; In
Proceedings, 12th International Conference of
Computational Linguistics. Budapest,
Hungary, August 1988.
Pustejovsky, J., and Bergler, S. (1987). &amp;quot;The
acquisition of conceptual structure for the
lexicon.&amp;quot; In Proceedings, Sixth National
Conference on Artificial Intelligence. Seattle,
Washington.
Pustejovsky, J., and Boguraev, B. (1993).
&amp;quot;Lexical knowledge representation and
natural language processing.&amp;quot; Artificial
Intelligence.
Pustejovsky, J., and Rooth, M.
(unpublished). &amp;quot;Type coercive
environments in corpora.&amp;quot;
Pustejovsky, J.; Waterman, S.; Cowie, J.; and
Stein, G. (1992). &amp;quot;Overview of the
DIDEROT system for the Tipster text
extraction project.&amp;quot; In Proceedings, DARPA
TIPSTER 12-Month Evaluation. San Diego,
California, September 1992.
Rau, L., and Jacobs, P. (1988). &amp;quot;Integrating
top-down and bottom-up strategies in a
text processing system.&amp;quot; In Proceedings,
Second Conference on Applied Natural
Language Processing, Austin Texas,
February 1988,129-135.
Russell, G.; Ballim, A.; Carroll, J.; and
Warwick-Armstrong, S. (1992). &amp;quot;A
practical approach to multiple default
inheritance for unification-based
lexicons.&amp;quot; Computational Linguistics, 18(3),
311-337.
Slator, B. M. (1988). &amp;quot;Constructing
contextually organized lexical semantic
knowledge-bases.&amp;quot; In Proceedings, Third
Annual Rocky Mountain Conference on
Artificial Intelligence. Denver, Colorado,
June 1988,142-148.
Slator, B. M., and Wilks, Y. A. (1987).
&amp;quot;Toward semantic structures from
dictionary entries.&amp;quot; In Proceedings, Second
Annual Rocky Mountain Conference on
Artificial Intelligence. Boulder, Colorado,
85-96.
Smadja, F. (1991a). &amp;quot;Macrocoding the
lexicon with co-occurrence knowledge.&amp;quot;
In Lexical Acquisition: Using On-Line
Resources to Build a Lexicon, edited by
U. Zernik. Lawrence Erlbaum Associates.
Smadja, F. (1991b). &amp;quot;From n-grams to
collocations: an evaluation of xtract.&amp;quot; In
Proceedings, 29th Annual Meeting of the
Association for Computational Linguistics.
Berkeley, California, June 1991,279-284.
Sparck Jones, K., editor. (1981). Information
Retrieval Experiments. Butterworth.
Sparck Jones, K. (1986). Synonymy and
Semantic Classification. Edinburgh
Information Technology Series (EDITS).
Edinburgh University Press.
Talmy, L. (1985). &amp;quot;Lexicalization patterns.&amp;quot;
In Language Typology and Syntactic
Description 3: Grammatical Categories and the
</reference>
<page confidence="0.943917">
357
</page>
<reference confidence="0.989182054545455">
Computational Linguistics Volume 19, Number 2
Lexicon, edited by T. Shopen, 57-149.
Cambridge University Press.
Touretzky, D. S. (1986). The Mathematics of
Inheritance Systems. Morgan Kaufmann.
Veronis, J., and Ide, N. (1991). &amp;quot;An
assessment of semantic information
automatically extracted from machine
readable dictionaries.&amp;quot; In Proceedings, Fifth
Conference of the European Chapter of the
Association for Computational Linguistics.
Berlin, Germany, April 1991.
Vickery, B. C. (1975). Classification and
Indexing in Science. Butterworth and Co.
Walker, D. E., and Amsler, R. A. (1986).
&amp;quot;The use of machine-readable dictionaries
in sublanguage analysis.&amp;quot; In Analyzing
Language in Restricted Domains, edited by
R. Grishman and R. Kittredge. Lawrence
Erlbaum Associates.
Vossen, P.; Meijs, W.; and den Broeder, M.
(1989). &amp;quot;Meaning and structure in
dictionary definitions.&amp;quot; In Computational
Lexicography for Natural Language
Processing, edited by B. Boguraev and
T. Briscoe. Longman.
Wang, Y.-C.; Vandendorpe, J.; and Evens, M.
(1985). &amp;quot;Relational thesauri in information
retrieval.&amp;quot; Journal of the American Society for
Information Science, 36,15-27.
Wilensky, R.; Chin, D. N.; Luria, M.; Martin,
J.; Mayfield, J.; and Wu, D. (1988). &amp;quot;The
Berkeley UNIX consultant project.&amp;quot;
Computational Linguistics, 14(4), 35-84.
Wilks, Y. A. (1978). &amp;quot;Making preferences
more active.&amp;quot; Artificial Intelligence, 10,
75-97.
Wilks, Y. A.; Fass, D.; Guo, C.-M.;
McDonald, J. E.; Plate, T.; and Slator,
B. M. (1989). &amp;quot;A tractable machine
dictionary as a resource for computational
semantics.&amp;quot; In Computational Lexicography
for Natural Language Processing, edited by
B. Boguraev and T. Briscoe, 193-228.
Longman.
Wilks, Y.; Fass, D.; Guo, C.-M.; McDonald,
J. E.; Plate, T.; and Slator, B. M. (1993).
&amp;quot;Providing machine tractable dictionary
tools.&amp;quot; In Semantics and the Lexicon, edited
by J. Pustejovsky. Kluwer Academic
Publishers.
Zernik, U. (1989). &amp;quot;Lexicon acquisition:
Learning from corpus by exploiting
lexical categories.&amp;quot; In Proceedings,
IJCAI-89.
</reference>
<page confidence="0.997814">
358
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.950146">
<title confidence="0.997572">Lexical Semantic Techniques for Corpus Analysis</title>
<author confidence="0.996329">James Pustejovsky Sabine Berglert</author>
<affiliation confidence="0.998294">Brandeis University Concordia University</affiliation>
<author confidence="0.995981">Peter Anickt</author>
<affiliation confidence="0.991293">Digital Equipment Corporation</affiliation>
<abstract confidence="0.997875">In this paper we outline a research program for computational linguistics, making extensive use of text corpora. We demonstrate how a semantic framework for lexical knowledge can suggest richer relationships among words in text beyond that of simple co-occurrence. The work suggests how linguistic phenomena such as metonymy and polysemy might be exploitable for semantic tagging of lexical items. Unlike with purely statistical collocational analyses, the framework of a semantic theory allows the automatic construction of predictions about deeper semantic among words appearing in systems. illustrate the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary. In addition to conventional lexical semantic relations, we show how information concerning lexical presuppositions and preference relations can also be acquired from corpora, when analyzed with the appropriate semantic tools. Finally, we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm linguistic hypotheses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Ahlswede</author>
<author>M Evens</author>
</authors>
<title>Generating a relational lexicon from a machine-readable dictionary.&amp;quot;</title>
<date>1988</date>
<journal>International Journal of Lexicography,</journal>
<volume>1</volume>
<issue>3</issue>
<pages>214--237</pages>
<contexts>
<context position="20525" citStr="Ahlswede and Evens 1988" startWordPosition="3150" endWordPosition="3153">relations can be extracted from MRDs using fairly simple techniques. Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et al. (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section. Our goal is to automate, to the extent possible, the initial construction of these structures. Extensive research has been done on the kind of information needed by natural language programs and on the representation of that information (Wang, Vandendorpe, and Evens 1985; Ahlswede and Evens 1988). Following Boguraev et al. (1989) and Wilks et al. of 1989), we believe that much of what is needed for NLP lexicons can be found either explicitly or implicitly in a dictionary, and empirical evidence suggests that this information gives rise to a sufficiently rich lexical representation for use in extracting information from texts. Techniques for identifying explicit information in machine-readable dictionaries have been developed by many researchers (Boguraev et al. 1989; Slator 1988; Slator and Wilks 1987; Guthrie et al. 1990) and are well understood. Many properties of a word sense or th</context>
</contexts>
<marker>Ahlswede, Evens, 1988</marker>
<rawString>Ahlswede, T., and Evens, M. (1988). &amp;quot;Generating a relational lexicon from a machine-readable dictionary.&amp;quot; International Journal of Lexicography, 1(3), 214-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Processing dictionary definitions with phrasal pattern hierarchies.&amp;quot;</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<volume>13</volume>
<pages>3--4</pages>
<contexts>
<context position="21389" citStr="Alshawi (1987)" startWordPosition="3288" endWordPosition="3289">sufficiently rich lexical representation for use in extracting information from texts. Techniques for identifying explicit information in machine-readable dictionaries have been developed by many researchers (Boguraev et al. 1989; Slator 1988; Slator and Wilks 1987; Guthrie et al. 1990) and are well understood. Many properties of a word sense or the semantic relationships between word senses are available in MRDs, but this information can only be identified computationally through some analysis of the definition text of an entry (Atkins 1991). Some research has already been done in this area. Alshawi (1987), Boguraev et al. (1989), Vossen, Meijs, and den Broeder (1989), and the work described in Wilks et al. (1992) have made explicit some kinds of implicit information found in MRDs. Here we propose to refine and merge some of the previous techniques to make explicit the implicit information specified by a theory of generative lexicons. Given what we described above for the lexical structures for nominals, we can identify these semantic relations in the OALD and LDOCE by pattern matching on the parse trees of definitions. To illustrate what specific information can be derived by automatic seeding</context>
</contexts>
<marker>Alshawi, 1987</marker>
<rawString>Alshawi, H. (1987). &amp;quot;Processing dictionary definitions with phrasal pattern hierarchies.&amp;quot; Computational Linguistics, 13, 3-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>B Boguraev</author>
<author>T Briscoe</author>
</authors>
<title>Towards a dictionary support environment for real time parsing.&amp;quot; In</title>
<date>1985</date>
<booktitle>Proceedings, European Conference on Computational Linguistics.</booktitle>
<location>Geneva, Switzerland.</location>
<marker>Alshawi, Boguraev, Briscoe, 1985</marker>
<rawString>Alshawi, H.; Boguraev, B.; and Briscoe, T. (1985). &amp;quot;Towards a dictionary support environment for real time parsing.&amp;quot; In Proceedings, European Conference on Computational Linguistics. Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Amsler</author>
</authors>
<title>The structure of the Merriam-Webster Pocket Dictionary. Doctoral dissertation,</title>
<date>1980</date>
<institution>University of Texas.</institution>
<contexts>
<context position="19701" citStr="Amsler (1980)" startWordPosition="3026" endWordPosition="3027">l knowledge base, so that the process of corpus-tuning is put into the proper perspective. The initial seeding of lexical structures is being done independently both from the Oxford Advanced Learners Dictionary (OALD) and from lexical entries in the Longman Dictionary of Contemporary English (Procter, Ilson, and Ayto 1978). These are then automatically adapted to the format of generative lexical structures. It is these lexical structures that are then statistically tuned against the corpus, following the methods outlined in Anick and Pustejovsky (1990) and Pustejovsky (1992). Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn (1985), Byrd et al. (1987), Markowitz, Ahlswede, and Evens (1986), and Nakamura and Nagao (1988) showed that taxonomic information and certain semantic relations can be extracted from MRDs using fairly simple techniques. Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et al. (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section. Our goal is to automate, to the extent possible, the initial construction</context>
</contexts>
<marker>Amsler, 1980</marker>
<rawString>Amsler, R. A. (1980). The structure of the Merriam-Webster Pocket Dictionary. Doctoral dissertation, University of Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B T Atkins</author>
<author>B Levin</author>
</authors>
<title>Admitting impediments.&amp;quot; In Lexical Acquisition: Using On-Line Resources to Build a Lexicon, edited by</title>
<date>1991</date>
<marker>Atkins, Levin, 1991</marker>
<rawString>Atkins, B. T., and Levin, B. (1991). &amp;quot;Admitting impediments.&amp;quot; In Lexical Acquisition: Using On-Line Resources to Build a Lexicon, edited by U. Zernik. LEA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bergler</author>
</authors>
<title>The semantics of collocational patterns for reporting verbs.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, Fifth Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<location>Berlin, Germany,</location>
<contexts>
<context position="56205" citStr="Bergler (1991)" startWordPosition="8874" endWordPosition="8875">s such as insist seem to require an overt or implicit negation within the immediate discourse context, rather than within the clause. For this reason, we will call such verbs discourse polarity items. For our purposes, the significance of such data is twofold: first, experiments on corpora can test and confirm linguistic intuitions concerning a subtle semantic judgment; second, if such knowledge is in fact so systematic, then it must be at least partially represented in the lexical semantics of the verb. To test whether the intuitions supported by the above data could be confirmed in corpora, Bergler (1991) derived the statistical co-occurrence of insist with discourse polarity markers in the 7 million-word corpus of Wall Street Journal articles. She derived the statistics reported in Figure 7. Let us assume, on the basis of this preliminary date presented in Bergler (1992) that these verbs in fact do behave as discourse polarity items. The question then 15 There is a rich literature on this topic. For discussion see Ladusaw (1980) and Linebarger (1980). 16 Overlap between the categories occurs in less than 35 cases. 350 Lexical Semantic Techniques for Corpus Analysis James Pustejovsky et al. Ke</context>
<context position="61868" citStr="Bergler (1991)" startWordPosition="9746" endWordPosition="9747">sed in Section 1. For example, the qualia structure for a noun such as corporation might be represented as below: corporation(x) CONST = group(y),spokesperson(w),executive(z) FORMAL = organization(x) TELIC = execute(z,decisions) AGENTIVE = incorporate(m) The metonymic extension in this example is straightforward: a spokesman, executive, or otherwise legitimate representative &amp;quot;speaking for&amp;quot; a company or institution can be metonymically replaced by that company or institution.&apos; We find that this type of metonymic extension for the subject is natural and indeed very frequent with reporting verbs Bergler (1991), such as announce, report, release, and claim, while it is in general not possible with other verbs selecting human subjects, e.g., the verbs of contemplation (such as contemplate, consider, and think). However, there are subtle differences in the occurrence of such metonymies for the different members of the same semantic verb class that arise from corpus analysis. A reporting verb is an utterance verb that is used to relate the words of a source. In a careful study of seven reporting verbs on a 250,000-word corpus of Time magazine articles from 1963, we found that the preference for differe</context>
</contexts>
<marker>Bergler, 1991</marker>
<rawString>Bergler, S. (1991). &amp;quot;The semantics of collocational patterns for reporting verbs.&amp;quot; In Proceedings, Fifth Conference of the European Chapter of the Association for Computational Linguistics. Berlin, Germany, April 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bergler</author>
</authors>
<title>Evidential analysis of reported speech.&amp;quot; Doctoral dissertation,</title>
<date>1992</date>
<institution>Brandeis University.</institution>
<contexts>
<context position="56477" citStr="Bergler (1992)" startWordPosition="8916" endWordPosition="8917">riments on corpora can test and confirm linguistic intuitions concerning a subtle semantic judgment; second, if such knowledge is in fact so systematic, then it must be at least partially represented in the lexical semantics of the verb. To test whether the intuitions supported by the above data could be confirmed in corpora, Bergler (1991) derived the statistical co-occurrence of insist with discourse polarity markers in the 7 million-word corpus of Wall Street Journal articles. She derived the statistics reported in Figure 7. Let us assume, on the basis of this preliminary date presented in Bergler (1992) that these verbs in fact do behave as discourse polarity items. The question then 15 There is a rich literature on this topic. For discussion see Ladusaw (1980) and Linebarger (1980). 16 Overlap between the categories occurs in less than 35 cases. 350 Lexical Semantic Techniques for Corpus Analysis James Pustejovsky et al. Keywords Count Comments insist 586 occurrences throughout the corpus insist on 109 these have been cleaned by hand and are actually occurrences of the idiom insist on rather than accidental co-occurrences. insist &amp; but 117 occurrences of both insist and but in the same sent</context>
<context position="63309" citStr="Bergler (1992)" startWordPosition="9982" endWordPosition="9983">nds for an individual (Analysts said . . .) and where a company or other institution stands for the individual (IBM announced ... ).19 The difference in patterns of metonymic behavior is quite striking: semantically similar verbs seem to pattern similarly over all three categories; admit, insist, and deny show a closer resemblance to each other than to any of the others, while said and 18 Note, however, that the metonymic extension is not quite as simple as extending from any employee to the whole company or institution, but that a form of legitimation has to be involved.) For more detail see Bergler (1992). 19 The data for Figure 8 have been screened to ensure that only occurrences that constitute reporting contexts were used. 352 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis person group instit. other admit 64% 19% 14% 2% deny 59% 11% 19% 11% insist 57% 24% 16% 3% announce 51% 10% 31% 8% claim 35% 21% 38% 6% said 83% 6% 4% 8% told 69% 7% 8% 16% Figure 8 Preference for different metonymies in subject position. person group institution other WSJ 49% 15% 34% 2% TIME 83% 6% 4% 8% Figure 9 Preference for metonymies for said in a 160,000-word fragment of the Wall Street Jo</context>
</contexts>
<marker>Bergler, 1992</marker>
<rawString>Bergler, S. (1992). &amp;quot;Evidential analysis of reported speech.&amp;quot; Doctoral dissertation, Brandeis University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bierwisch</author>
</authors>
<title>Semantische und konzeptuelle Reprasentationen lexikalischer Einheiten.&amp;quot;</title>
<date>1983</date>
<booktitle>In Untersuchungen zur Semantik,</booktitle>
<publisher>Akademische-Verlag.</publisher>
<note>edited by</note>
<contexts>
<context position="9856" citStr="Bierwisch (1983)" startWordPosition="1484" endWordPosition="1485">cular act or event. 2 Briefly, the qualia can be defined as follows: • CONSTITUTIVE: the relation between an object and its constituent parts; • FORMAL: that which distinguishes it within a larger domain; • TELIC: its purpose and function; • AGENTIVE: factors involved in its origin or &amp;quot;bringing it about.&amp;quot; In the qualia structures given below, we adopt the convention that [a, 0] denotes conjunction of formulas within the feature structure, while [a; 0] will denote disjunction. 3 A related approach for expressing the different semantic relations of nominals in distinguished contexts is given in Bierwisch (1983). 333 Computational Linguistics Volume 19, Number 2 Such an analysis allows us to minimally structure objects according to these four qualia. As an example of how objects cluster according to these dimensions, we will briefly consider three object types: (1) containers (of information), e.g., book, tape, record; (2) instruments, e.g., gun, hammer, paintbrush; and (3) figure-ground objects, e.g., door, room, fireplace. Because of how their qualia structures differ, these classes appear in vastly different grammatical contexts. As with containers in general, information containers permit metonym</context>
</contexts>
<marker>Bierwisch, 1983</marker>
<rawString>Bierwisch, M. (1983). &amp;quot;Semantische und konzeptuelle Reprasentationen lexikalischer Einheiten.&amp;quot; In Untersuchungen zur Semantik, edited by R. Ruzicka and W. Motsch. Akademische-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-L Binot</author>
<author>K Jensen</author>
</authors>
<title>A semantic expert using an online standard dictionary.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, 10th International Joint Conference on Artificial Intelligence (IJCAI-87).</booktitle>
<pages>709--714</pages>
<location>Milan, Italy,</location>
<marker>Binot, Jensen, 1987</marker>
<rawString>Binot, J.-L., and Jensen, K. (1987). &amp;quot;A semantic expert using an online standard dictionary.&amp;quot; In Proceedings, 10th International Joint Conference on Artificial Intelligence (IJCAI-87). Milan, Italy, 709-714.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
</authors>
<title>Automatic resolution of linguistic ambiguities.&amp;quot;</title>
<date>1979</date>
<tech>Technical Report No. 11,</tech>
<institution>University of Cambridge Computer Laboratory,</institution>
<location>Cambridge, U.K.</location>
<marker>Boguraev, 1979</marker>
<rawString>Boguraev, B. (1979). &amp;quot;Automatic resolution of linguistic ambiguities.&amp;quot; Technical Report No. 11, University of Cambridge Computer Laboratory, Cambridge, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
</authors>
<title>Building a lexicon: The contribution of computers.&amp;quot;</title>
<date>1991</date>
<journal>International Journal of Lexicography,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="18970" citStr="Boguraev 1991" startWordPosition="2914" endWordPosition="2915">91) for details. 9 This is reflected in the sublanguage work of Grishman, Hirschman, and Nhan (1986), whose automated discovery procedures are aimed at clustering nouns into categories like diagnosis and symptom. 336 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis 2. Seeding Lexical Structures from MRDs In this section we discuss briefly how a lexical semantic theory can help in extracting information from machine-readable dictionaries (MRDs). We describe research on conversion of a machine-tractable dictionary (Wilks et al. 1993) into a usable lexical knowledge base (Boguraev 1991). Although the results here are preliminary, it is important to mention the process of converting an MRD into a lexical knowledge base, so that the process of corpus-tuning is put into the proper perspective. The initial seeding of lexical structures is being done independently both from the Oxford Advanced Learners Dictionary (OALD) and from lexical entries in the Longman Dictionary of Contemporary English (Procter, Ilson, and Ayto 1978). These are then automatically adapted to the format of generative lexical structures. It is these lexical structures that are then statistically tuned agains</context>
</contexts>
<marker>Boguraev, 1991</marker>
<rawString>Boguraev, B. (1991). &amp;quot;Building a lexicon: The contribution of computers.&amp;quot; International Journal of Lexicography, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>T Briscoe</author>
</authors>
<title>Introduction.&amp;quot; In Computational Lexicography for Natural Language Processing,</title>
<date>1989</date>
<publisher>Longman Group UK.</publisher>
<note>edited by</note>
<marker>Boguraev, Briscoe, 1989</marker>
<rawString>Boguraev, B., and Briscoe, T. (1989). &amp;quot;Introduction.&amp;quot; In Computational Lexicography for Natural Language Processing, edited by B. Boguraev and T. Briscoe. Longman Group UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>T Briscoe</author>
</authors>
<title>Large lexicons for natural language processing: Exploring the grammar coding system of LDOCE.&amp;quot;</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<volume>13</volume>
<marker>Boguraev, Briscoe, 1987</marker>
<rawString>Boguraev, B., and Briscoe, T. (1987). &amp;quot;Large lexicons for natural language processing: Exploring the grammar coding system of LDOCE.&amp;quot; Computational Linguistics, 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>R Byrd</author>
<author>J Klavans</author>
<author>M Neff</author>
</authors>
<title>From machine readable dictionaries to a lexical knowledge base.&amp;quot; In</title>
<date>1989</date>
<booktitle>Proceedings, First International Lexical Acquisition Workshop.</booktitle>
<location>Detroit, Michigan,</location>
<contexts>
<context position="20559" citStr="Boguraev et al. (1989)" startWordPosition="3155" endWordPosition="3158"> using fairly simple techniques. Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et al. (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section. Our goal is to automate, to the extent possible, the initial construction of these structures. Extensive research has been done on the kind of information needed by natural language programs and on the representation of that information (Wang, Vandendorpe, and Evens 1985; Ahlswede and Evens 1988). Following Boguraev et al. (1989) and Wilks et al. of 1989), we believe that much of what is needed for NLP lexicons can be found either explicitly or implicitly in a dictionary, and empirical evidence suggests that this information gives rise to a sufficiently rich lexical representation for use in extracting information from texts. Techniques for identifying explicit information in machine-readable dictionaries have been developed by many researchers (Boguraev et al. 1989; Slator 1988; Slator and Wilks 1987; Guthrie et al. 1990) and are well understood. Many properties of a word sense or the semantic relationships between w</context>
</contexts>
<marker>Boguraev, Byrd, Klavans, Neff, 1989</marker>
<rawString>Boguraev, B.; Byrd, R.; Klavans, J.; and Neff, M. (1989). &amp;quot;From machine readable dictionaries to a lexical knowledge base.&amp;quot; In Proceedings, First International Lexical Acquisition Workshop. Detroit, Michigan, August 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>J Pustejovsky</author>
</authors>
<title>Lexical ambiguity and the role of knowledge representation in lexicon design.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 13th International Conference of Computational Linguistics.</booktitle>
<location>Helsinki, Finland,</location>
<marker>Boguraev, Pustejovsky, 1990</marker>
<rawString>Boguraev, B., and Pustejovsky, J. (1990). &amp;quot;Lexical ambiguity and the role of knowledge representation in lexicon design.&amp;quot; In Proceedings, 13th International Conference of Computational Linguistics. Helsinki, Finland, August 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>Automatics semantic classification of verbs from their syntactic contexts: An implemented classifier for stativity.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, Fifth Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<location>Berlin, Germany,</location>
<contexts>
<context position="52468" citStr="Brent 1991" startWordPosition="8293" endWordPosition="8294">ith prep = with. identify the agent of the verb as its argument (cf. Nilsen (1973)). This is confirmed by our data, shown in Figure 6. Conversely, verbs governing nominals collocating with a between-phrase will not refer to the agentive since the phrase is saturated already. Indeed, the only verb occurring in this position with any frequency is the copula be, namely with the following counts: 12 be/V venture/O. Thus, weak semantic types can be induced on the basis of syntactic behavior. There is a growing literature on corpus-based acquisition and tuning (Smadja 1991a; Zernik and Jacobs 1991; Brent 1991; as well as Grishman and Sterling 1992). We share with these researchers a general dependence on well-behaved collocational patterns and distributional structures. Probably the main distinguishing feature of our approach is its reliance on a fairly well studied semantic framework to aid and guide the semantic induction process itself, whether it involves selectional restrictions or semantic types. 5. Lexical Presuppositions and Preferences In the previous section we presented algorithms for extracting collocational information from corpora, in order to supplement and fine-tune the lexical str</context>
</contexts>
<marker>Brent, 1991</marker>
<rawString>Brent, M. (1991). &amp;quot;Automatics semantic classification of verbs from their syntactic contexts: An implemented classifier for stativity.&amp;quot; In Proceedings, Fifth Conference of the European Chapter of the Association for Computational Linguistics. Berlin, Germany, April 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>A Copestake</author>
<author>B Boguraev</author>
</authors>
<title>Enjoy the paper: Lexical semantics via lexicology.&amp;quot; In</title>
<date>1990</date>
<booktitle>Proceedings, 13th International Conference on Computational Linguistics.</booktitle>
<pages>42--47</pages>
<location>Helsinki, Finland,</location>
<marker>Briscoe, Copestake, Boguraev, 1990</marker>
<rawString>Briscoe, E.; Copestake, A.; and Boguraev, B. (1990). &amp;quot;Enjoy the paper: Lexical semantics via lexicology.&amp;quot; In Proceedings, 13th International Conference on Computational Linguistics. Helsinki, Finland, 42-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Byrd</author>
<author>N Calzolari</author>
<author>M Chodorow</author>
<author>J Klavans</author>
<author>M Neff</author>
<author>Rizk</author>
</authors>
<title>Tools and methods for computational lexicology.&amp;quot;</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<volume>13</volume>
<issue>3</issue>
<pages>219--240</pages>
<contexts>
<context position="19775" citStr="Byrd et al. (1987)" startWordPosition="3035" endWordPosition="3038">e proper perspective. The initial seeding of lexical structures is being done independently both from the Oxford Advanced Learners Dictionary (OALD) and from lexical entries in the Longman Dictionary of Contemporary English (Procter, Ilson, and Ayto 1978). These are then automatically adapted to the format of generative lexical structures. It is these lexical structures that are then statistically tuned against the corpus, following the methods outlined in Anick and Pustejovsky (1990) and Pustejovsky (1992). Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn (1985), Byrd et al. (1987), Markowitz, Ahlswede, and Evens (1986), and Nakamura and Nagao (1988) showed that taxonomic information and certain semantic relations can be extracted from MRDs using fairly simple techniques. Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et al. (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section. Our goal is to automate, to the extent possible, the initial construction of these structures. Extensive research has been done on the kind of info</context>
</contexts>
<marker>Byrd, Calzolari, Chodorow, Klavans, Neff, Rizk, 1987</marker>
<rawString>Byrd, R.; Calzolari, N.; Chodorow, M.; Klavans, J.; Neff, M.; and Rizk, 0. (1987). &amp;quot;Tools and methods for computational lexicology.&amp;quot; Computational Linguistics, 13(3-4), 219-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Byrd</author>
</authors>
<title>Discovering relationships among word senses.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, Fifth Annual Conference of the UW Centre for the New Oxford English Dictionary.</booktitle>
<pages>67--80</pages>
<location>Oxford, U.K.,</location>
<marker>Byrd, 1989</marker>
<rawString>Byrd, R. (1989). &amp;quot;Discovering relationships among word senses.&amp;quot; In Proceedings, Fifth Annual Conference of the UW Centre for the New Oxford English Dictionary. Oxford, U.K., 67-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Calzolari</author>
</authors>
<title>Detecting patterns in a lexical database.&amp;quot;</title>
<date>1984</date>
<booktitle>In Proceedings, Seventh International Conference on Computational Linguistics (COLING-84).</booktitle>
<location>Stanford, California.</location>
<contexts>
<context position="19719" citStr="Calzolari (1984)" startWordPosition="3028" endWordPosition="3029">e, so that the process of corpus-tuning is put into the proper perspective. The initial seeding of lexical structures is being done independently both from the Oxford Advanced Learners Dictionary (OALD) and from lexical entries in the Longman Dictionary of Contemporary English (Procter, Ilson, and Ayto 1978). These are then automatically adapted to the format of generative lexical structures. It is these lexical structures that are then statistically tuned against the corpus, following the methods outlined in Anick and Pustejovsky (1990) and Pustejovsky (1992). Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn (1985), Byrd et al. (1987), Markowitz, Ahlswede, and Evens (1986), and Nakamura and Nagao (1988) showed that taxonomic information and certain semantic relations can be extracted from MRDs using fairly simple techniques. Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et al. (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section. Our goal is to automate, to the extent possible, the initial construction of these structur</context>
</contexts>
<marker>Calzolari, 1984</marker>
<rawString>Calzolari, N. (1984). &amp;quot;Detecting patterns in a lexical database.&amp;quot; In Proceedings, Seventh International Conference on Computational Linguistics (COLING-84). Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Calzolari</author>
<author>R Bindi</author>
</authors>
<title>Acquisition of lexical information from a large textual Italian corpus.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 13th International Conference on Computational Linguistics (COLING-90).</booktitle>
<location>Helsinki, Finland.</location>
<contexts>
<context position="60381" citStr="Calzolari and Bindi 1990" startWordPosition="9519" endWordPosition="9523">rent sublanguages, etc. Here we are attempting to use these techniques together with a model of lexical meaning, to capture deeper lexical semantic collocations; e.g., the generalization that the list of objects occurring for the word drink contains only liquids. In the final part of this section, we turn to how the analysis of corpora can provide lexical semantic preferences for verb selection. As discussed above, there is a growing body of research on deriving collocations from corpora (cf. Church and Hanks 1990; Klavans, Chodorow, and Wacholder 1990; Wilks et al. 1993; Smadja 1991a, 1991b; Calzolari and Bindi 1990). Here we employ the tools of semantic analysis from Section 1 to examine the behavior of metonymy with reporting verbs. We will show, on the basis of corpus analysis, how verbs display marked differences in the ability to license metonymic operations over their arguments. Such information, we argue, is part of the preference semantics for a sublanguage, as automatically derived from corpus. Metonymy can be seen as a case of &amp;quot;licensed violation&amp;quot; of selectional restrictions. For example, while the verb announce selects for a human subject, sentences like The Phantasie Corporation announced thir</context>
</contexts>
<marker>Calzolari, Bindi, 1990</marker>
<rawString>Calzolari, N., and Bindi, R. (1990). &amp;quot;Acquisition of lexical information from a large textual Italian corpus.&amp;quot; In Proceedings, 13th International Conference on Computational Linguistics (COLING-90). Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Cardelli</author>
<author>P Wegner</author>
</authors>
<title>On understanding types, data abstraction, and polymorphism.&amp;quot;</title>
<date>1985</date>
<journal>ACM Computing Surveys,</journal>
<volume>17</volume>
<issue>4</issue>
<pages>471--522</pages>
<contexts>
<context position="6342" citStr="Cardelli and Wegner 1985" startWordPosition="927" endWordPosition="930">ription: argument structure, qualia structure, lexical inheritance structure, and event structure. Connecting these different levels is a set of generative devices that provide for the compositional interpretation of words in context. The most important of these devices is a semantic transformation called type coercion—analogous to coercion in programming languages—which captures the semantic relatedness between syntactically distinct expressions. As an operation on types within a A-calculus, type coercion can be seen as transforming a monomorphic language into one with polymorphic types (cf. Cardelli and Wegner 1985). Argument, event, and qualia types must conform to the well-formedness conditions defined by the type system defined by the lexical inheritance structure when undergoing operations of semantic composition.&apos; 1 The details of type coercion need not concern us here. Briefly, however, whenever there exists a grammatical environment where more than one syntactic type satisfies the semantic type selected by the governing element, the governing element can be analyzed as coercing a range of surface types into a single semantic type. An example of subject type coercion is a causative verb, semantical</context>
</contexts>
<marker>Cardelli, Wegner, 1985</marker>
<rawString>Cardelli, L., and Wegner, P. (1985). &amp;quot;On understanding types, data abstraction, and polymorphism.&amp;quot; ACM Computing Surveys, 17(4), 471-522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chodorow</author>
<author>R Byrd</author>
<author>G Heidorn</author>
</authors>
<title>Extracting semantic hierarchies from a large on-line dictionary.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings, 23rd Annual Meeting of the ACL.</booktitle>
<pages>299--304</pages>
<location>Chicago, Illinois,</location>
<marker>Chodorow, Byrd, Heidorn, 1985</marker>
<rawString>Chodorow, M.; Byrd, R.; and Heidorn, G. (1985). &amp;quot;Extracting semantic hierarchies from a large on-line dictionary.&amp;quot; In Proceedings, 23rd Annual Meeting of the ACL. Chicago, Illinois, 299-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, Second Conference on Applied Natural Language Processing.</booktitle>
<location>Austin, Texas.</location>
<marker>Church, 1988</marker>
<rawString>Church, K. (1988). &amp;quot;A stochastic parts program and noun phrase parser for unrestricted text.&amp;quot; In Proceedings, Second Conference on Applied Natural Language Processing. Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.&amp;quot;</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>22--29</pages>
<contexts>
<context position="46043" citStr="Church and Hanks (1990)" startWordPosition="7215" endWordPosition="7218">gin production 59 begin meeting 59 begin term 50 begin visit 45 begin test 39 begin construction 31 begin debate 29 begin trial Figure 4 Counts for objects of begin/V. In related work being carried out with Mats Rooth of the University of Stuttgart, we are exploring what the range of coercion types is, and what environments they may appear in, as discovered in corpora. Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be. Using techniques described in Church and Hindle (1990), Church and Hanks (1990), and Hindle and Rooth (1991), Figure 4 shows some examples of the most frequent V-0 pairs from the AP corpus. Corpus studies confirm similar results for &amp;quot;weakly intensional contexts&amp;quot; such as the complement of coercive verbs such as veto. These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of &amp;quot;the proposal to,&amp;quot; thereby clothing the complement within an intensional context. The examples in Figure 5 with the verb veto indicate two things: first, that such coercions are regular and pervasive in corpora; second, that al</context>
<context position="60275" citStr="Church and Hanks 1990" startWordPosition="9503" endWordPosition="9506">mputational Linguistics Volume 19, Number 2 are therefore easy to compile for different corpora, different sublanguages, etc. Here we are attempting to use these techniques together with a model of lexical meaning, to capture deeper lexical semantic collocations; e.g., the generalization that the list of objects occurring for the word drink contains only liquids. In the final part of this section, we turn to how the analysis of corpora can provide lexical semantic preferences for verb selection. As discussed above, there is a growing body of research on deriving collocations from corpora (cf. Church and Hanks 1990; Klavans, Chodorow, and Wacholder 1990; Wilks et al. 1993; Smadja 1991a, 1991b; Calzolari and Bindi 1990). Here we employ the tools of semantic analysis from Section 1 to examine the behavior of metonymy with reporting verbs. We will show, on the basis of corpus analysis, how verbs display marked differences in the ability to license metonymic operations over their arguments. Such information, we argue, is part of the preference semantics for a sublanguage, as automatically derived from corpus. Metonymy can be seen as a case of &amp;quot;licensed violation&amp;quot; of selectional restrictions. For example, wh</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Church, K., and Hanks, P. (1990). &amp;quot;Word association norms, mutual information, and lexicography.&amp;quot; Computational Linguistics, 16(1), 22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>D Hindle</author>
</authors>
<title>Collocational constraints and corpus-based linguistics.&amp;quot; In Working Notes of the AAAI Symposium: Text-Based Intelligent Systems.</title>
<date>1990</date>
<location>Stanford, California.</location>
<contexts>
<context position="46018" citStr="Church and Hindle (1990)" startWordPosition="7211" endWordPosition="7214">tion 66 begin strike 64 begin production 59 begin meeting 59 begin term 50 begin visit 45 begin test 39 begin construction 31 begin debate 29 begin trial Figure 4 Counts for objects of begin/V. In related work being carried out with Mats Rooth of the University of Stuttgart, we are exploring what the range of coercion types is, and what environments they may appear in, as discovered in corpora. Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be. Using techniques described in Church and Hindle (1990), Church and Hanks (1990), and Hindle and Rooth (1991), Figure 4 shows some examples of the most frequent V-0 pairs from the AP corpus. Corpus studies confirm similar results for &amp;quot;weakly intensional contexts&amp;quot; such as the complement of coercive verbs such as veto. These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of &amp;quot;the proposal to,&amp;quot; thereby clothing the complement within an intensional context. The examples in Figure 5 with the verb veto indicate two things: first, that such coercions are regular and pervasive in</context>
</contexts>
<marker>Church, Hindle, 1990</marker>
<rawString>Church, K., and Hindle, D. (1990). &amp;quot;Collocational constraints and corpus-based linguistics.&amp;quot; In Working Notes of the AAAI Symposium: Text-Based Intelligent Systems. Stanford, California.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Copestake</author>
</authors>
<title>Ann (in press). &amp;quot;Defaults in the LKB.&amp;quot;</title>
<booktitle>In Default Inheritance in the Lexicon, edited</booktitle>
<publisher>Cambridge University Press.</publisher>
<marker>Copestake, </marker>
<rawString>Copestake, Ann (in press). &amp;quot;Defaults in the LKB.&amp;quot; In Default Inheritance in the Lexicon, edited by T. Briscoe and A. Copestake. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>E Briscoe</author>
</authors>
<title>Lexical operations in a unification-based framework.&amp;quot;</title>
<date>1992</date>
<booktitle>In Lexical Semantics and Knowledge Representation,</booktitle>
<publisher>Springer Verlag.</publisher>
<note>edited by</note>
<contexts>
<context position="15033" citStr="Copestake and Briscoe 1992" startWordPosition="2300" endWordPosition="2303"> (in press) for details. 335 Computational Linguistics Volume 19, Number 2 it serves (telic), and how it arises (agentive). This provides us with a semantic representation that can capture the multiple perspectives a single lexical item may assume in different contexts. Yet, the qualia for a lexical item such as tape are not isolated values for that one word, but are integrated into a global knowledge base indicating how these senses relate to other lexical items and their senses. This is the contribution of inheritance and the hierarchical structuring of knowledge (cf. Evans and Gazdar 1990; Copestake and Briscoe 1992; Russell et al. 1992). In Pustejovsky (1991) it is suggested that there are two types of relational structures for lexical knowledge; a fixed inheritance similar to that of an is-a hierarchy (cf. Touretzky 1986); and a dynamic structure that operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories.&apos; Reviewing briefly, the basic idea is that semantics allows for the dynamic creation of arbitrary concepts through the application of certain transformations to lexical meanings. Thus for every predicate, Q, we can generate its opposit</context>
</contexts>
<marker>Copestake, Briscoe, 1992</marker>
<rawString>Copestake, A., and Briscoe, E. (1992). &amp;quot;Lexical operations in a unification-based framework.&amp;quot; In Lexical Semantics and Knowledge Representation, edited by J. Pustejovsky and S. Bergler. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cowie</author>
<author>L Guthrie</author>
<author>J Pustejovsky</author>
</authors>
<title>Description of the MUCBRUCE system as used for MUC-4.&amp;quot; In</title>
<date>1992</date>
<booktitle>Proceedings, Fourth Message Understanding Conference (MUC-4).</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<marker>Cowie, Guthrie, Pustejovsky, 1992</marker>
<rawString>Cowie, J.; Guthrie, L.; and Pustejovsky, J. (1992). &amp;quot;Description of the MUCBRUCE system as used for MUC-4.&amp;quot; In Proceedings, Fourth Message Understanding Conference (MUC-4). Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Croft</author>
</authors>
<title>Automatic indexing.&amp;quot; In Indexing: The State of Our Knowledge and the State of Our Ignorance,</title>
<date>1989</date>
<pages>87--100</pages>
<institution>Learned Information, Inc.</institution>
<note>edited by</note>
<contexts>
<context position="2093" citStr="Croft 1989" startWordPosition="293" endWordPosition="294">xtual information poses an interesting challenge to linguistic researchers for several reasons. First, it provides the linguist with sentence and word usage information that has been difficult to collect and consequently largely ignored by linguists. Second, it has intensified the search for efficient automated indexing and retrieval techniques. Full-text indexing, in which all the content words in a document are used as keywords, is one of the most promising of recent automated approaches, yet its mediocre precision and recall characteristics indicate that there is much room for improvement (Croft 1989). The use of domain knowledge can enhance the effectiveness of a full-text system by providing related terms that can be used to broaden, narrow, or refocus a query at retrieval time (Debili, Fluhr, and Radasua 1988; Anick et al. 1989. Likewise, domain knowledge may be applied at indexing time to do word sense disambiguation (Krovetz and Croft 1989) or content analysis (Jacobs 1991). Unfortunately, for many domains, such knowledge, even in the form of a thesaurus, is either not available or is incomplete with respect to the vocabulary of the texts indexed. * Computer Science Department, Brande</context>
</contexts>
<marker>Croft, 1989</marker>
<rawString>Croft, W. B. (1989). &amp;quot;Automatic indexing.&amp;quot; In Indexing: The State of Our Knowledge and the State of Our Ignorance, edited by B. H. Weinberg, 87-100. Learned Information, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Cruse</author>
</authors>
<title>Lexical Semantics.</title>
<date>1986</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="13872" citStr="Cruse (1986" startWordPosition="2115" endWordPosition="2116">ype itself. Thus, in this example, the type selected for by a verb such as read refers to the &amp;quot;information&amp;quot; argument for tape, while a verb such as carry would select for the &amp;quot;physical object&amp;quot; argument. They are, however, logically related, since the noun itself denotes a relation. The representation above simply states that any semantics for tape must logically make reference to the object itself (formal), what it can contain (const), what purpose 4 This relates to Menuk&apos;s lexical functions and the syntactic structures they associate with an element. See Men&apos;uk (1988) and references therein. Cruse (1986, 1992) and Nunberg (1978) discuss the foregrounding and backgrounding of information with respect to similar examples. 5 Within the qualia structure for a term, FORMAL and CONST roles typically refer to the object domain while TELIC and AGENTIVE refer to events. Hence, the first parameter in the latter two roles refers to an event sort, i.e., a state (s), process (P), or transition (T). 6 The appropriate selection of a surface spatial preposition will follow from its formal type specification as a 2-dimen object. Cf. Pustejovsky (in press) for details. 335 Computational Linguistics Volume 19,</context>
</contexts>
<marker>Cruse, 1986</marker>
<rawString>Cruse, D. A. (1986). Lexical Semantics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Cruse</author>
</authors>
<title>Polysemy and related phenomena from a cognitive linguistic viewpoint.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, Second Toulouse Workshop on Lexical Semantics, edited</booktitle>
<location>Toulouse, France.</location>
<marker>Cruse, 1992</marker>
<rawString>Cruse, D. A. (1992). &amp;quot;Polysemy and related phenomena from a cognitive linguistic viewpoint.&amp;quot; In Proceedings, Second Toulouse Workshop on Lexical Semantics, edited by P. Saint-Dizier and E. Viegas. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Debili</author>
<author>C Fluhr</author>
<author>P Radasoa</author>
</authors>
<title>About reformulation in full-text IRS.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings,</booktitle>
<pages>0--88</pages>
<marker>Debili, Fluhr, Radasoa, 1988</marker>
<rawString>Debili, F.; Fluhr, C.; and Radasoa, P. (1988). &amp;quot;About reformulation in full-text IRS.&amp;quot; In Proceedings, RIA0-88, 343-357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Evans</author>
<author>G Gazdar</author>
<author>editors</author>
</authors>
<title>The DATR papers: February</title>
<date>1990</date>
<booktitle>Cognitive Science Research Paper CSRP 139, School of Cognitive and Computing Sciences,</booktitle>
<institution>University of Sussex.</institution>
<marker>Evans, Gazdar, editors, 1990</marker>
<rawString>Evans, R., and Gazdar, G., editors. (1990). &amp;quot;The DATR papers: February 1990.&amp;quot; Cognitive Science Research Paper CSRP 139, School of Cognitive and Computing Sciences, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Evens</author>
</authors>
<title>Relational Models of the Lexicon.</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<marker>Evens, 1987</marker>
<rawString>Evens, M. (1987). Relational Models of the Lexicon. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fano</author>
</authors>
<title>Transmission of Information: A Statistical Theory of Communications.</title>
<date>1961</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="58501" citStr="Fano 1961" startWordPosition="9231" endWordPosition="9232">edicates form a central part of our lexicalization of concepts. Semantically motivated collocations such as these extracted from large corpora can provide presuppositional information for words that would otherwise be missing from the lexical semantics of an entry. While full automatic extraction of semantic collocations is not yet feasible, some recent research in related areas is promising. Hindle (1990) reports interesting results of this kind based on literal collocations, where he parses the corpus (Hindle 1983) into predicate—argument structures and applies a mutual information measure (Fano 1961; Magerman and Marcus 1990) to weigh the association between the predicate and each of its arguments. For example, as a list of the most frequent objects for the verb drink in his corpus, Hindle found beer, tea, Pepsi, and champagne. Based on the distributional hypothesis that the degree of shared contexts is a similarity measure for words, he develops a similarity metric for nouns based on their substitutability in certain verb contexts. Hindle thus finds sets of semantically similar nouns based on syntactic co-occurrence data. The sets he extracts are promising; for example, the ten most sim</context>
</contexts>
<marker>Fano, 1961</marker>
<rawString>Fano, R. (1961). Transmission of Information: A Statistical Theory of Communications. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fillmore</author>
</authors>
<title>The case for case.&amp;quot; In Universals in Linguistics Theory,</title>
<date>1968</date>
<note>edited by</note>
<contexts>
<context position="10949" citStr="Fillmore 1968" startWordPosition="1649" endWordPosition="1650">es appear in vastly different grammatical contexts. As with containers in general, information containers permit metonymic extensions between the container and the material contained within it. Collocations such as those in Examples 4 through 7 indicate that this metonymy is grammaticalized through specific and systematic head-PP constructions. Example 4 read a book Example 5 read a story in a book Example 6 read a tape Example 7 read the information on the tape Instruments, on the other hand, display classic agent—instrument causative alternations, such as those in Examples 8 through 11 (cf. Fillmore 1968; Lakoff 1968, 1970). Example 8 ... smash the vase with the hammer Example 9 The hammer smashed the vase. Example 10 ... kill him with a gun Example 11 The gun killed him. Finally, figure-ground nominals (Pustejovsky and Anick 1988) permit perspective shifts such as those in Examples 12 through 15. These are nouns that refer to physical objects as well as the specific enclosure or aperture associated with it. Example 12 John painted the door. Example 13 John walked through the door. 334 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis Example 14 John is scrubbing the fi</context>
</contexts>
<marker>Fillmore, 1968</marker>
<rawString>Fillmore, C. (1968). &amp;quot;The case for case.&amp;quot; In Universals in Linguistics Theory, edited by E. Bach and R. Harms. Holt, Rinehart and Winston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>L Hirschman</author>
<author>N T Nhan</author>
</authors>
<title>Discovery procedures for sublanguage selectional patterns: Initial experiments.&amp;quot;</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<pages>205--215</pages>
<marker>Grishman, Hirschman, Nhan, 1986</marker>
<rawString>Grishman, R.; Hirschman, L.; and Nhan, N. T. (1986). &amp;quot;Discovery procedures for sublanguage selectional patterns: Initial experiments.&amp;quot; Computational Linguistics, 12(3), 205-215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>J Sterling</author>
</authors>
<title>Acquisition of selectional patterns.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, 14th International Conference on Computational Linguistics (COLING-92).</booktitle>
<location>Nantes, France,</location>
<contexts>
<context position="52508" citStr="Grishman and Sterling 1992" startWordPosition="8298" endWordPosition="8301">fy the agent of the verb as its argument (cf. Nilsen (1973)). This is confirmed by our data, shown in Figure 6. Conversely, verbs governing nominals collocating with a between-phrase will not refer to the agentive since the phrase is saturated already. Indeed, the only verb occurring in this position with any frequency is the copula be, namely with the following counts: 12 be/V venture/O. Thus, weak semantic types can be induced on the basis of syntactic behavior. There is a growing literature on corpus-based acquisition and tuning (Smadja 1991a; Zernik and Jacobs 1991; Brent 1991; as well as Grishman and Sterling 1992). We share with these researchers a general dependence on well-behaved collocational patterns and distributional structures. Probably the main distinguishing feature of our approach is its reliance on a fairly well studied semantic framework to aid and guide the semantic induction process itself, whether it involves selectional restrictions or semantic types. 5. Lexical Presuppositions and Preferences In the previous section we presented algorithms for extracting collocational information from corpora, in order to supplement and fine-tune the lexical structures seeded by a machine-readable dic</context>
</contexts>
<marker>Grishman, Sterling, 1992</marker>
<rawString>Grishman, R., and Sterling, J. (1992). &amp;quot;Acquisition of selectional patterns.&amp;quot; In Proceedings, 14th International Conference on Computational Linguistics (COLING-92). Nantes, France, July 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Guthrie</author>
<author>B Slator</author>
<author>Y Wilks</author>
<author>R Bruce</author>
</authors>
<title>Is there content in Empty Heads?&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 13th International Conference of Computational Linguistics (COLING-90).</booktitle>
<location>Helsinki, Finland.</location>
<contexts>
<context position="21062" citStr="Guthrie et al. 1990" startWordPosition="3232" endWordPosition="3235">on of that information (Wang, Vandendorpe, and Evens 1985; Ahlswede and Evens 1988). Following Boguraev et al. (1989) and Wilks et al. of 1989), we believe that much of what is needed for NLP lexicons can be found either explicitly or implicitly in a dictionary, and empirical evidence suggests that this information gives rise to a sufficiently rich lexical representation for use in extracting information from texts. Techniques for identifying explicit information in machine-readable dictionaries have been developed by many researchers (Boguraev et al. 1989; Slator 1988; Slator and Wilks 1987; Guthrie et al. 1990) and are well understood. Many properties of a word sense or the semantic relationships between word senses are available in MRDs, but this information can only be identified computationally through some analysis of the definition text of an entry (Atkins 1991). Some research has already been done in this area. Alshawi (1987), Boguraev et al. (1989), Vossen, Meijs, and den Broeder (1989), and the work described in Wilks et al. (1992) have made explicit some kinds of implicit information found in MRDs. Here we propose to refine and merge some of the previous techniques to make explicit the impl</context>
</contexts>
<marker>Guthrie, Slator, Wilks, Bruce, 1990</marker>
<rawString>Guthrie, L.; Slator, B.; Wilks, Y.; and Bruce, R. (1990). &amp;quot;Is there content in Empty Heads?&amp;quot; In Proceedings, 13th International Conference of Computational Linguistics (COLING-90). Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Deterministic parsing of syntactic non-fluencies.&amp;quot;</title>
<date>1983</date>
<booktitle>In Proceedings, 21st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="58414" citStr="Hindle 1983" startWordPosition="9220" endWordPosition="9221">s argued in Pustejovsky (1991) and Miller and Fellbaum (1991), such simple oppositional predicates form a central part of our lexicalization of concepts. Semantically motivated collocations such as these extracted from large corpora can provide presuppositional information for words that would otherwise be missing from the lexical semantics of an entry. While full automatic extraction of semantic collocations is not yet feasible, some recent research in related areas is promising. Hindle (1990) reports interesting results of this kind based on literal collocations, where he parses the corpus (Hindle 1983) into predicate—argument structures and applies a mutual information measure (Fano 1961; Magerman and Marcus 1990) to weigh the association between the predicate and each of its arguments. For example, as a list of the most frequent objects for the verb drink in his corpus, Hindle found beer, tea, Pepsi, and champagne. Based on the distributional hypothesis that the degree of shared contexts is a similarity measure for words, he develops a similarity metric for nouns based on their substitutability in certain verb contexts. Hindle thus finds sets of semantically similar nouns based on syntacti</context>
</contexts>
<marker>Hindle, 1983</marker>
<rawString>Hindle, D. 1983. &amp;quot;Deterministic parsing of syntactic non-fluencies.&amp;quot; In Proceedings, 21st Annual Meeting of the Association for Computational Linguistics, Cambridge, Massachusetts, June 1983,123-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicate-argument structures.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>268--275</pages>
<location>Pittsburgh, Pennsylvania,</location>
<contexts>
<context position="33240" citStr="Hindle (1990)" startWordPosition="5114" endWordPosition="5115">drive] ], [ [database management] system]. 2. Generation of taxonomic relationships on the basis of collocational information. Technical sublanguages often express subclass relationships in noun compounds of the form &lt;instance-name&gt; &lt;class-name&gt;, as in &amp;quot;Unix operating system&amp;quot; and &amp;quot;C language.&amp;quot; Unfortunately, noun compounds are also employed to express numerous other relationships, as in &amp;quot;Unix kernel&amp;quot; and &amp;quot;C debugger.&amp;quot; We have found, however, that collocational evidence can be employed to suggest which noun compounds reflect taxonomic relationships, using a strategy similar to that employed by Hindle (1990) for detecting synonyms. Given a term T, we extract from the phrase database those nouns N, that appear as the head of any phrase in which T is the immediately preceding term. These nouns represent candidate classes of which T may be a member. We then generate the set of verbs that take T as direct object and calculate the mutual information value for each verb/T collocation (cf. Hindle 1990). We do the same for each noun N. Under the assumption that instance and class nouns are likely to co-occur with the same verbs, we compute a similarity score between T and each noun N„ by summing the prod</context>
<context position="46018" citStr="Hindle (1990)" startWordPosition="7213" endWordPosition="7214">in strike 64 begin production 59 begin meeting 59 begin term 50 begin visit 45 begin test 39 begin construction 31 begin debate 29 begin trial Figure 4 Counts for objects of begin/V. In related work being carried out with Mats Rooth of the University of Stuttgart, we are exploring what the range of coercion types is, and what environments they may appear in, as discovered in corpora. Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be. Using techniques described in Church and Hindle (1990), Church and Hanks (1990), and Hindle and Rooth (1991), Figure 4 shows some examples of the most frequent V-0 pairs from the AP corpus. Corpus studies confirm similar results for &amp;quot;weakly intensional contexts&amp;quot; such as the complement of coercive verbs such as veto. These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of &amp;quot;the proposal to,&amp;quot; thereby clothing the complement within an intensional context. The examples in Figure 5 with the verb veto indicate two things: first, that such coercions are regular and pervasive in</context>
<context position="58301" citStr="Hindle (1990)" startWordPosition="9203" endWordPosition="9204">dividual and a statement that is the negation of a proposition, /p, presupposed in the context of the utterance. As argued in Pustejovsky (1991) and Miller and Fellbaum (1991), such simple oppositional predicates form a central part of our lexicalization of concepts. Semantically motivated collocations such as these extracted from large corpora can provide presuppositional information for words that would otherwise be missing from the lexical semantics of an entry. While full automatic extraction of semantic collocations is not yet feasible, some recent research in related areas is promising. Hindle (1990) reports interesting results of this kind based on literal collocations, where he parses the corpus (Hindle 1983) into predicate—argument structures and applies a mutual information measure (Fano 1961; Magerman and Marcus 1990) to weigh the association between the predicate and each of its arguments. For example, as a list of the most frequent objects for the verb drink in his corpus, Hindle found beer, tea, Pepsi, and champagne. Based on the distributional hypothesis that the degree of shared contexts is a similarity measure for words, he develops a similarity metric for nouns based on their </context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Hindle, D. (1990). &amp;quot;Noun classification from predicate-argument structures.&amp;quot; In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics, Pittsburgh, Pennsylvania, June 1990, 268-275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
<author>M Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="46072" citStr="Hindle and Rooth (1991)" startWordPosition="7220" endWordPosition="7223">ng 59 begin term 50 begin visit 45 begin test 39 begin construction 31 begin debate 29 begin trial Figure 4 Counts for objects of begin/V. In related work being carried out with Mats Rooth of the University of Stuttgart, we are exploring what the range of coercion types is, and what environments they may appear in, as discovered in corpora. Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be. Using techniques described in Church and Hindle (1990), Church and Hanks (1990), and Hindle and Rooth (1991), Figure 4 shows some examples of the most frequent V-0 pairs from the AP corpus. Corpus studies confirm similar results for &amp;quot;weakly intensional contexts&amp;quot; such as the complement of coercive verbs such as veto. These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of &amp;quot;the proposal to,&amp;quot; thereby clothing the complement within an intensional context. The examples in Figure 5 with the verb veto indicate two things: first, that such coercions are regular and pervasive in corpora; second, that almost anything can be vetoed, </context>
</contexts>
<marker>Hindle, Rooth, 1991</marker>
<rawString>Hindle, D., and Rooth, M. (1991). &amp;quot;Structural ambiguity and lexical relations.&amp;quot; In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jacobs</author>
</authors>
<title>Making sense of lexical acquisition.&amp;quot; In Lexical Acquisition: Using On-Line Resources to Build a Lexicon, edited by U.</title>
<date>1991</date>
<location>Zernik, LEA.</location>
<contexts>
<context position="2478" citStr="Jacobs 1991" startWordPosition="356" endWordPosition="357"> content words in a document are used as keywords, is one of the most promising of recent automated approaches, yet its mediocre precision and recall characteristics indicate that there is much room for improvement (Croft 1989). The use of domain knowledge can enhance the effectiveness of a full-text system by providing related terms that can be used to broaden, narrow, or refocus a query at retrieval time (Debili, Fluhr, and Radasua 1988; Anick et al. 1989. Likewise, domain knowledge may be applied at indexing time to do word sense disambiguation (Krovetz and Croft 1989) or content analysis (Jacobs 1991). Unfortunately, for many domains, such knowledge, even in the form of a thesaurus, is either not available or is incomplete with respect to the vocabulary of the texts indexed. * Computer Science Department, Brandeis University, Waltham MA 02254. Computer Science Department, Concordia University, Montreal, Quebec H3G 1M8, Canada. Digital Equipment Corporation, 111 Locke Drive LM02-1/D12, Marlboro MA 01752. © 1993 Association for Computational Linguistics Computational Linguistics Volume 19, Number 2 In this paper we examine how linguistic phenomena such as metonymy and polysemy might be explo</context>
<context position="52456" citStr="Jacobs 1991" startWordPosition="8291" endWordPosition="8292">bject pairs with prep = with. identify the agent of the verb as its argument (cf. Nilsen (1973)). This is confirmed by our data, shown in Figure 6. Conversely, verbs governing nominals collocating with a between-phrase will not refer to the agentive since the phrase is saturated already. Indeed, the only verb occurring in this position with any frequency is the copula be, namely with the following counts: 12 be/V venture/O. Thus, weak semantic types can be induced on the basis of syntactic behavior. There is a growing literature on corpus-based acquisition and tuning (Smadja 1991a; Zernik and Jacobs 1991; Brent 1991; as well as Grishman and Sterling 1992). We share with these researchers a general dependence on well-behaved collocational patterns and distributional structures. Probably the main distinguishing feature of our approach is its reliance on a fairly well studied semantic framework to aid and guide the semantic induction process itself, whether it involves selectional restrictions or semantic types. 5. Lexical Presuppositions and Preferences In the previous section we presented algorithms for extracting collocational information from corpora, in order to supplement and fine-tune the</context>
</contexts>
<marker>Jacobs, 1991</marker>
<rawString>Jacobs, P. (1991). &amp;quot;Making sense of lexical acquisition.&amp;quot; In Lexical Acquisition: Using On-Line Resources to Build a Lexicon, edited by U. Zernik, LEA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klavans</author>
<author>M Chodorow</author>
<author>N Wacholder</author>
</authors>
<title>From dictionary to knowledge base via taxonomy.&amp;quot; In</title>
<date>1990</date>
<booktitle>Proceedings, Sixth Conference of the UW Centre for the New OED. Waterloo,</booktitle>
<pages>110--132</pages>
<marker>Klavans, Chodorow, Wacholder, 1990</marker>
<rawString>Klavans, J.; Chodorow, M.; and Wacholder, N. (1990). &amp;quot;From dictionary to knowledge base via taxonomy.&amp;quot; In Proceedings, Sixth Conference of the UW Centre for the New OED. Waterloo, 110-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Krovetz</author>
<author>W B Croft</author>
</authors>
<title>Word sense disambiguation using machine-readable dictionaries.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, SIGIR.</booktitle>
<pages>127--136</pages>
<contexts>
<context position="2444" citStr="Krovetz and Croft 1989" startWordPosition="349" endWordPosition="352">hniques. Full-text indexing, in which all the content words in a document are used as keywords, is one of the most promising of recent automated approaches, yet its mediocre precision and recall characteristics indicate that there is much room for improvement (Croft 1989). The use of domain knowledge can enhance the effectiveness of a full-text system by providing related terms that can be used to broaden, narrow, or refocus a query at retrieval time (Debili, Fluhr, and Radasua 1988; Anick et al. 1989. Likewise, domain knowledge may be applied at indexing time to do word sense disambiguation (Krovetz and Croft 1989) or content analysis (Jacobs 1991). Unfortunately, for many domains, such knowledge, even in the form of a thesaurus, is either not available or is incomplete with respect to the vocabulary of the texts indexed. * Computer Science Department, Brandeis University, Waltham MA 02254. Computer Science Department, Concordia University, Montreal, Quebec H3G 1M8, Canada. Digital Equipment Corporation, 111 Locke Drive LM02-1/D12, Marlboro MA 01752. © 1993 Association for Computational Linguistics Computational Linguistics Volume 19, Number 2 In this paper we examine how linguistic phenomena such as me</context>
</contexts>
<marker>Krovetz, Croft, 1989</marker>
<rawString>Krovetz, R., and Croft, W. B. (1989). &amp;quot;Word sense disambiguation using machine-readable dictionaries.&amp;quot; In Proceedings, SIGIR. 127-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Ladusaw</author>
</authors>
<title>Polarity Sensitivity as Inherent Scope Relations.</title>
<date>1980</date>
<institution>Indiana University Linguistics Club.</institution>
<contexts>
<context position="56638" citStr="Ladusaw (1980)" startWordPosition="8944" endWordPosition="8945">must be at least partially represented in the lexical semantics of the verb. To test whether the intuitions supported by the above data could be confirmed in corpora, Bergler (1991) derived the statistical co-occurrence of insist with discourse polarity markers in the 7 million-word corpus of Wall Street Journal articles. She derived the statistics reported in Figure 7. Let us assume, on the basis of this preliminary date presented in Bergler (1992) that these verbs in fact do behave as discourse polarity items. The question then 15 There is a rich literature on this topic. For discussion see Ladusaw (1980) and Linebarger (1980). 16 Overlap between the categories occurs in less than 35 cases. 350 Lexical Semantic Techniques for Corpus Analysis James Pustejovsky et al. Keywords Count Comments insist 586 occurrences throughout the corpus insist on 109 these have been cleaned by hand and are actually occurrences of the idiom insist on rather than accidental co-occurrences. insist &amp; but 117 occurrences of both insist and but in the same sentence insist &amp; negation 186 includes not and n&apos;t insist Sr subjunctive 159 includes would, could, should, and be Figure 7 Negative markers with insist in WSJC imm</context>
</contexts>
<marker>Ladusaw, 1980</marker>
<rawString>Ladusaw, W. (1980). Polarity Sensitivity as Inherent Scope Relations. Indiana University Linguistics Club.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lakoff</author>
</authors>
<title>Instrumental adverbs and the concept of deep structure.&amp;quot;</title>
<date>1968</date>
<journal>Foundations of Language,</journal>
<pages>4--4</pages>
<contexts>
<context position="10962" citStr="Lakoff 1968" startWordPosition="1651" endWordPosition="1652">stly different grammatical contexts. As with containers in general, information containers permit metonymic extensions between the container and the material contained within it. Collocations such as those in Examples 4 through 7 indicate that this metonymy is grammaticalized through specific and systematic head-PP constructions. Example 4 read a book Example 5 read a story in a book Example 6 read a tape Example 7 read the information on the tape Instruments, on the other hand, display classic agent—instrument causative alternations, such as those in Examples 8 through 11 (cf. Fillmore 1968; Lakoff 1968, 1970). Example 8 ... smash the vase with the hammer Example 9 The hammer smashed the vase. Example 10 ... kill him with a gun Example 11 The gun killed him. Finally, figure-ground nominals (Pustejovsky and Anick 1988) permit perspective shifts such as those in Examples 12 through 15. These are nouns that refer to physical objects as well as the specific enclosure or aperture associated with it. Example 12 John painted the door. Example 13 John walked through the door. 334 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis Example 14 John is scrubbing the fireplace. Exam</context>
</contexts>
<marker>Lakoff, 1968</marker>
<rawString>Lakoff, G. (1968). &amp;quot;Instrumental adverbs and the concept of deep structure.&amp;quot; Foundations of Language, 4,4-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lakoff</author>
</authors>
<title>Irregularity in Syntax.</title>
<date>1970</date>
<location>Holt, Rinehart, and Winston.</location>
<marker>Lakoff, 1970</marker>
<rawString>Lakoff, G. (1970). Irregularity in Syntax. Holt, Rinehart, and Winston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lakoff</author>
</authors>
<title>Women, Fire, and Dangerous Objects.</title>
<date>1987</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="31499" citStr="Lakoff 1987" startWordPosition="4863" endWordPosition="4864">se locations in which the word appears. 12 Within the current framework, a distinction is made between logical metonymy, where the metonymic extension or relation is transparent from the lexical semantics of the coerced phrase, and conventional metonymy, where the relation may not be directly calculated from information provided grammatically. For example, in the sentence &amp;quot;The Boston office called today,&amp;quot; it is not clear from logical metonymy what relation Boston bears to office other than location; i.e., it is not obvious that it is a branch office. This is well beyond lexical semantics (cf. Lakoff 1987 and Martin 1990). 340 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis The database of partially parsed sentences provides the raw material for a number of sublanguage analyses. This begins the second phase of analysis: 1. Noun compound recognition and bracketing. In technical sublanguages, noun compounds are often employed to expand the working vocabulary without the invention of new word forms. It is therefore useful in applications such as lexicon-assisted full-text information retrieval (Anick 1992) to include such noun compounds as lexical items for both querying </context>
</contexts>
<marker>Lakoff, 1987</marker>
<rawString>Lakoff, G. (1987). Women, Fire, and Dangerous Objects. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Linebarger</author>
</authors>
<title>The grammar of negative polarity.&amp;quot; Doctoral dissertation, MIT,</title>
<date>1980</date>
<location>Cambridge MA.</location>
<contexts>
<context position="56660" citStr="Linebarger (1980)" startWordPosition="8947" endWordPosition="8948">rtially represented in the lexical semantics of the verb. To test whether the intuitions supported by the above data could be confirmed in corpora, Bergler (1991) derived the statistical co-occurrence of insist with discourse polarity markers in the 7 million-word corpus of Wall Street Journal articles. She derived the statistics reported in Figure 7. Let us assume, on the basis of this preliminary date presented in Bergler (1992) that these verbs in fact do behave as discourse polarity items. The question then 15 There is a rich literature on this topic. For discussion see Ladusaw (1980) and Linebarger (1980). 16 Overlap between the categories occurs in less than 35 cases. 350 Lexical Semantic Techniques for Corpus Analysis James Pustejovsky et al. Keywords Count Comments insist 586 occurrences throughout the corpus insist on 109 these have been cleaned by hand and are actually occurrences of the idiom insist on rather than accidental co-occurrences. insist &amp; but 117 occurrences of both insist and but in the same sentence insist &amp; negation 186 includes not and n&apos;t insist Sr subjunctive 159 includes would, could, should, and be Figure 7 Negative markers with insist in WSJC immediately arises as to </context>
</contexts>
<marker>Linebarger, 1980</marker>
<rawString>Linebarger, M. (1980). &amp;quot;The grammar of negative polarity.&amp;quot; Doctoral dissertation, MIT, Cambridge MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Maarek</author>
<author>F Z Smadja</author>
</authors>
<title>Full text indexing based on lexical relations, an application: Software libraries.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, SIGIR.</booktitle>
<pages>127--136</pages>
<marker>Maarek, Smadja, 1989</marker>
<rawString>Maarek, Y. S., and Smadja, F. Z. (1989). &amp;quot;Full text indexing based on lexical relations, an application: Software libraries.&amp;quot; In Proceedings, SIGIR. 127-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Markowitz</author>
<author>T Ahlswede</author>
<author>M Evens</author>
</authors>
<title>Semantically significant patterns in dictionary definitions.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings, 24th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>112--119</pages>
<location>New York, New York,</location>
<marker>Markowitz, Ahlswede, Evens, 1986</marker>
<rawString>Markowitz, J.; Ahlswede, T.; and Evens, M. (1986). &amp;quot;Semantically significant patterns in dictionary definitions.&amp;quot; In Proceedings, 24th Annual Meeting of the Association for Computational Linguistics. New York, New York, 112-119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Magerman</author>
<author>M Marcus</author>
</authors>
<title>Parsing a natural language using mutual information statistics.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, Eighth National Conference on Artificial Intelligence (AAAI-90).</booktitle>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="58528" citStr="Magerman and Marcus 1990" startWordPosition="9233" endWordPosition="9236">rm a central part of our lexicalization of concepts. Semantically motivated collocations such as these extracted from large corpora can provide presuppositional information for words that would otherwise be missing from the lexical semantics of an entry. While full automatic extraction of semantic collocations is not yet feasible, some recent research in related areas is promising. Hindle (1990) reports interesting results of this kind based on literal collocations, where he parses the corpus (Hindle 1983) into predicate—argument structures and applies a mutual information measure (Fano 1961; Magerman and Marcus 1990) to weigh the association between the predicate and each of its arguments. For example, as a list of the most frequent objects for the verb drink in his corpus, Hindle found beer, tea, Pepsi, and champagne. Based on the distributional hypothesis that the degree of shared contexts is a similarity measure for words, he develops a similarity metric for nouns based on their substitutability in certain verb contexts. Hindle thus finds sets of semantically similar nouns based on syntactic co-occurrence data. The sets he extracts are promising; for example, the ten most similar nouns to treaty in his</context>
</contexts>
<marker>Magerman, Marcus, 1990</marker>
<rawString>Magerman, D., and Marcus, M. (1990). &amp;quot;Parsing a natural language using mutual information statistics.&amp;quot; In Proceedings, Eighth National Conference on Artificial Intelligence (AAAI-90). Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Martin</author>
</authors>
<title>A Computational Model of Metaphor Interpretation.</title>
<date>1990</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="31516" citStr="Martin 1990" startWordPosition="4866" endWordPosition="4867">which the word appears. 12 Within the current framework, a distinction is made between logical metonymy, where the metonymic extension or relation is transparent from the lexical semantics of the coerced phrase, and conventional metonymy, where the relation may not be directly calculated from information provided grammatically. For example, in the sentence &amp;quot;The Boston office called today,&amp;quot; it is not clear from logical metonymy what relation Boston bears to office other than location; i.e., it is not obvious that it is a branch office. This is well beyond lexical semantics (cf. Lakoff 1987 and Martin 1990). 340 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis The database of partially parsed sentences provides the raw material for a number of sublanguage analyses. This begins the second phase of analysis: 1. Noun compound recognition and bracketing. In technical sublanguages, noun compounds are often employed to expand the working vocabulary without the invention of new word forms. It is therefore useful in applications such as lexicon-assisted full-text information retrieval (Anick 1992) to include such noun compounds as lexical items for both querying and thesaurus bro</context>
</contexts>
<marker>Martin, 1990</marker>
<rawString>Martin, J. (1990). A Computational Model of Metaphor Interpretation. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mertuk</author>
</authors>
<title>Dependency Syntax.</title>
<date>1988</date>
<publisher>SUNY Press.</publisher>
<marker>Mertuk, 1988</marker>
<rawString>Mertuk, I. (1988). Dependency Syntax. SUNY Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>C Fellbaum</author>
</authors>
<title>Verbs in WordNet.&amp;quot; Cognition.</title>
<date>1991</date>
<contexts>
<context position="57863" citStr="Miller and Fellbaum (1991)" startWordPosition="9138" endWordPosition="9141">mediately arises as to how we represent this type of knowledge. Using the language of the qualia structure discussed above, we can make explicit reference to the polarity behavior, in the following informal but intuitive representation for the verb insist!&apos; [insist(xind,y:prop) FORMAL = REPORTING—VERB—LCP TELIC = say(x,true(y)) &amp; presupposed(0) &amp; y = This entry states that in the REPORTING—VERB sense of the word, insist is a relation between an individual and a statement that is the negation of a proposition, /p, presupposed in the context of the utterance. As argued in Pustejovsky (1991) and Miller and Fellbaum (1991), such simple oppositional predicates form a central part of our lexicalization of concepts. Semantically motivated collocations such as these extracted from large corpora can provide presuppositional information for words that would otherwise be missing from the lexical semantics of an entry. While full automatic extraction of semantic collocations is not yet feasible, some recent research in related areas is promising. Hindle (1990) reports interesting results of this kind based on literal collocations, where he parses the corpus (Hindle 1983) into predicate—argument structures and applies a</context>
</contexts>
<marker>Miller, Fellbaum, 1991</marker>
<rawString>Miller, G., and Fellbaum, C. (1991). &amp;quot;Verbs in WordNet.&amp;quot; Cognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nakamura</author>
<author>M Nagao</author>
</authors>
<title>Extraction of semantic information from an ordinary English dictionary and its evaluation.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, COLING-88.</booktitle>
<pages>459--464</pages>
<location>Budapest, Hungary,</location>
<contexts>
<context position="19845" citStr="Nakamura and Nagao (1988)" startWordPosition="3045" endWordPosition="3048"> is being done independently both from the Oxford Advanced Learners Dictionary (OALD) and from lexical entries in the Longman Dictionary of Contemporary English (Procter, Ilson, and Ayto 1978). These are then automatically adapted to the format of generative lexical structures. It is these lexical structures that are then statistically tuned against the corpus, following the methods outlined in Anick and Pustejovsky (1990) and Pustejovsky (1992). Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn (1985), Byrd et al. (1987), Markowitz, Ahlswede, and Evens (1986), and Nakamura and Nagao (1988) showed that taxonomic information and certain semantic relations can be extracted from MRDs using fairly simple techniques. Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et al. (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section. Our goal is to automate, to the extent possible, the initial construction of these structures. Extensive research has been done on the kind of information needed by natural language programs and on the representation </context>
</contexts>
<marker>Nakamura, Nagao, 1988</marker>
<rawString>Nakamura, J., and Nagao, M. (1988). &amp;quot;Extraction of semantic information from an ordinary English dictionary and its evaluation.&amp;quot; In Proceedings, COLING-88. Budapest, Hungary, 459-464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L F Nilsen</author>
</authors>
<title>The Instrumental Case in English: Syntactic and Semantic Considerations.</title>
<date>1973</date>
<publisher>Mouton.</publisher>
<contexts>
<context position="51940" citStr="Nilsen (1973)" startWordPosition="8208" endWordPosition="8209">pect that verbs governing nominals collocated with a with-phrase will be mostly those predicates referring to the agentive quale of the nominal. This is because the with-phrase is unsaturated as a predicate, and acts to 348 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis count verb object 19 form venture 3 announce venture 3 enter venture 2 discuss venture 1 be venture 1 abandon venture 1 begin venture 1 complete venture 1 negotiate venture 1 start venture 1 expect venture Figure 6 Verb-object pairs with prep = with. identify the agent of the verb as its argument (cf. Nilsen (1973)). This is confirmed by our data, shown in Figure 6. Conversely, verbs governing nominals collocating with a between-phrase will not refer to the agentive since the phrase is saturated already. Indeed, the only verb occurring in this position with any frequency is the copula be, namely with the following counts: 12 be/V venture/O. Thus, weak semantic types can be induced on the basis of syntactic behavior. There is a growing literature on corpus-based acquisition and tuning (Smadja 1991a; Zernik and Jacobs 1991; Brent 1991; as well as Grishman and Sterling 1992). We share with these researcher</context>
</contexts>
<marker>Nilsen, 1973</marker>
<rawString>Nilsen, D. L. F. (1973). The Instrumental Case in English: Syntactic and Semantic Considerations. Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nirenburg</author>
<author>I Nirenburg</author>
</authors>
<title>A framework for lexical selection in natural language generation.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, COLING-88.</booktitle>
<location>Budapest, Hungary.</location>
<marker>Nirenburg, Nirenburg, 1988</marker>
<rawString>Nirenburg, S., and Nirenburg, I. (1988). &amp;quot;A framework for lexical selection in natural language generation.&amp;quot; In Proceedings, COLING-88. Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Nunberg</author>
</authors>
<title>The Pragmatics of Reference.</title>
<date>1978</date>
<institution>Indiana University Linguistics Club.</institution>
<contexts>
<context position="13898" citStr="Nunberg (1978)" startWordPosition="2119" endWordPosition="2120">is example, the type selected for by a verb such as read refers to the &amp;quot;information&amp;quot; argument for tape, while a verb such as carry would select for the &amp;quot;physical object&amp;quot; argument. They are, however, logically related, since the noun itself denotes a relation. The representation above simply states that any semantics for tape must logically make reference to the object itself (formal), what it can contain (const), what purpose 4 This relates to Menuk&apos;s lexical functions and the syntactic structures they associate with an element. See Men&apos;uk (1988) and references therein. Cruse (1986, 1992) and Nunberg (1978) discuss the foregrounding and backgrounding of information with respect to similar examples. 5 Within the qualia structure for a term, FORMAL and CONST roles typically refer to the object domain while TELIC and AGENTIVE refer to events. Hence, the first parameter in the latter two roles refers to an event sort, i.e., a state (s), process (P), or transition (T). 6 The appropriate selection of a surface spatial preposition will follow from its formal type specification as a 2-dimen object. Cf. Pustejovsky (in press) for details. 335 Computational Linguistics Volume 19, Number 2 it serves (telic</context>
</contexts>
<marker>Nunberg, 1978</marker>
<rawString>Nunberg, G. (1978). The Pragmatics of Reference. Indiana University Linguistics Club.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Procter</author>
<author>R F Ilson</author>
<author>J Ayto</author>
</authors>
<title>Longman Dictionary of Contemporary English.</title>
<date>1978</date>
<publisher>Longman Group Limited.</publisher>
<marker>Procter, Ilson, Ayto, 1978</marker>
<rawString>Procter, P.; Ilson, R. F.; and Ayto, J. (1978). Longman Dictionary of Contemporary English. Longman Group Limited.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>Issues in computational lexical semantics.&amp;quot; In</title>
<date>1989</date>
<booktitle>Proceedings, Fourth Conference of the European Chapter of the ACL.</booktitle>
<location>Manchester, England,</location>
<marker>Pustejovsky, 1989</marker>
<rawString>Pustejovsky, J. (1989). &amp;quot;Issues in computational lexical semantics.&amp;quot; In Proceedings, Fourth Conference of the European Chapter of the ACL. Manchester, England, April 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>The generative lexicon.&amp;quot;</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>4</issue>
<pages>409--441</pages>
<contexts>
<context position="5225" citStr="Pustejovsky (1991)" startWordPosition="767" endWordPosition="768">s most queries tend to be expressed as conjunctions of nouns. From a theoretical perspective, we believe that the contribution of the lexical semantics of nominals to the overall structure of the lexicon has been somewhat neglected, relative to that of verbs. While Zernik (1989) presents ambiguity and metonymy as a potential obstacle to effective corpus analysis, we believe that the existence of motivated metonymic structures actually provides valuable clues for semantic analysis of nouns in a corpus. We will assume, for this paper, the general framework of a generative lexicon as outlined in Pustejovsky (1991). In particular, we make use of the principles of type coercion and qualia structure. This model of semantic knowledge associated with words is based on a system of generative devices that is able to recursively define new word senses for lexical items in the language. These devices and the associated dictionary make up a generative lexicon, where semantic information is distributed throughout the lexicon to all categories. The general framework assumes four basic levels of semantic description: argument structure, qualia structure, lexical inheritance structure, and event structure. Connectin</context>
<context position="15078" citStr="Pustejovsky (1991)" startWordPosition="2309" endWordPosition="2310">s Volume 19, Number 2 it serves (telic), and how it arises (agentive). This provides us with a semantic representation that can capture the multiple perspectives a single lexical item may assume in different contexts. Yet, the qualia for a lexical item such as tape are not isolated values for that one word, but are integrated into a global knowledge base indicating how these senses relate to other lexical items and their senses. This is the contribution of inheritance and the hierarchical structuring of knowledge (cf. Evans and Gazdar 1990; Copestake and Briscoe 1992; Russell et al. 1992). In Pustejovsky (1991) it is suggested that there are two types of relational structures for lexical knowledge; a fixed inheritance similar to that of an is-a hierarchy (cf. Touretzky 1986); and a dynamic structure that operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories.&apos; Reviewing briefly, the basic idea is that semantics allows for the dynamic creation of arbitrary concepts through the application of certain transformations to lexical meanings. Thus for every predicate, Q, we can generate its opposition, -Q. Similarly, these two predicates can </context>
<context position="18359" citStr="Pustejovsky (1991)" startWordPosition="2824" endWordPosition="2825"> on-line corpora. In the sections that follow, we describe several experiments indicating that the qualia structures do, in fact, correlate with well-behaved collocational patterns, thereby allowing us to perform structure-matching operations over corpora to find these relations. 7 This is similar to thesauruslike structures, within the IR community, cf. for example Sparck Jones (1981). 8 Details of the derivation are as follows. Let Q be mounted, then —Q gives ---,mounted, and &lt; applied to these two states gives Q &lt; which is lexicalized as dismount. A similar derivation exists for mount. Cf. Pustejovsky (1991) for details. 9 This is reflected in the sublanguage work of Grishman, Hirschman, and Nhan (1986), whose automated discovery procedures are aimed at clustering nouns into categories like diagnosis and symptom. 336 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis 2. Seeding Lexical Structures from MRDs In this section we discuss briefly how a lexical semantic theory can help in extracting information from machine-readable dictionaries (MRDs). We describe research on conversion of a machine-tractable dictionary (Wilks et al. 1993) into a usable lexical knowledge base (Bog</context>
<context position="44739" citStr="Pustejovsky (1991)" startWordPosition="7000" endWordPosition="7001">able from the seeding of MRDs. The identification of semantic tags is the result of type coercion on known syntactic forms, to induce a semantic feature, such as [+event] or [+object]. 4.1 Coercive Environments in Corpora A pervasive example of type coercion is seen in the complements of aspectual verbs such as begin and finish, and verbs such as enjoy. That is, in sentences such as &amp;quot;John began the book,&amp;quot; the normal complement expected is an action or event of some sort, most often expressed by a gerundive or infinitival phrase: &amp;quot;John began reading the book,&amp;quot; &amp;quot;John began to read the book.&amp;quot; In Pustejovsky (1991) it was argued that in such cases, the verb need not have multiple subcategorizations, but only one deep semantic type, in this case, an event. Thus, the verb coerces its complement (e.g. &amp;quot;the book&amp;quot;) into an event related to that object. Such information can be represented by means of a representational schema called qualia structure, which, among other things, specifies the relations associated with objects. 345 Computational Linguistics Volume 19, Number 2 count verb object 205 begin career 176 begin day 159 begin work 140 begin talk 120 begin campaign 113 begin investigation 106 begin proce</context>
<context position="57832" citStr="Pustejovsky (1991)" startWordPosition="9135" endWordPosition="9136"> with insist in WSJC immediately arises as to how we represent this type of knowledge. Using the language of the qualia structure discussed above, we can make explicit reference to the polarity behavior, in the following informal but intuitive representation for the verb insist!&apos; [insist(xind,y:prop) FORMAL = REPORTING—VERB—LCP TELIC = say(x,true(y)) &amp; presupposed(0) &amp; y = This entry states that in the REPORTING—VERB sense of the word, insist is a relation between an individual and a statement that is the negation of a proposition, /p, presupposed in the context of the utterance. As argued in Pustejovsky (1991) and Miller and Fellbaum (1991), such simple oppositional predicates form a central part of our lexicalization of concepts. Semantically motivated collocations such as these extracted from large corpora can provide presuppositional information for words that would otherwise be missing from the lexical semantics of an entry. While full automatic extraction of semantic collocations is not yet feasible, some recent research in related areas is promising. Hindle (1990) reports interesting results of this kind based on literal collocations, where he parses the corpus (Hindle 1983) into predicate—ar</context>
</contexts>
<marker>Pustejovsky, 1991</marker>
<rawString>Pustejovsky, J. (1991). &amp;quot;The generative lexicon.&amp;quot; Computational Linguistics, 17(4), 409-441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>The acquisition of lexical semantic knowledge from large corpora.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, DARPA Spoken and Written Language Workshop.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="19669" citStr="Pustejovsky (1992)" startWordPosition="3021" endWordPosition="3022">ss of converting an MRD into a lexical knowledge base, so that the process of corpus-tuning is put into the proper perspective. The initial seeding of lexical structures is being done independently both from the Oxford Advanced Learners Dictionary (OALD) and from lexical entries in the Longman Dictionary of Contemporary English (Procter, Ilson, and Ayto 1978). These are then automatically adapted to the format of generative lexical structures. It is these lexical structures that are then statistically tuned against the corpus, following the methods outlined in Anick and Pustejovsky (1990) and Pustejovsky (1992). Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn (1985), Byrd et al. (1987), Markowitz, Ahlswede, and Evens (1986), and Nakamura and Nagao (1988) showed that taxonomic information and certain semantic relations can be extracted from MRDs using fairly simple techniques. Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et al. (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section. Our goal is to automate, to the extent po</context>
<context position="25372" citStr="Pustejovsky (1992)" startWordPosition="3927" endWordPosition="3928">ation and retrieval purposes, although the result is in no way ideal or uniform over all nominal forms. (cf. Cowie, Guthrie, and Pustejovsky [1992] for details of this operation on LDOCE.):&amp;quot; cigarette(x) CONST = tobacco(y),shredded(y),paper(z) FORMAL = roll(x) TELIC = smoke(T,w,x) AGENTIVE = artifact(x) In a related set of experiments performed while constructing a large lexical database for data extraction purposes, we seeded a lexicon with 6000 verbs from LDOCE. This process and the corpus tuning for both argument typing and subcategorization acquisition are described in Cowie, Guthrie, and Pustejovsky (1992) and Pustejovsky et al. (1992). In summary, based on a theory of lexical semantics, we have discussed how an MRD can be useful as a corpus for automatically seeding lexical structures. Rather than addressing the specific problems inherent in converting MRDs into useful lexicons, we have emphasized how it provides us, in a sense, with a generic vocabulary from which to begin lexical acquisition over corpora. In the next section, we will address the problem of taking these initial, and often very incomplete lexical structures, and enriching them with information acquired from corpus analysis. As</context>
<context position="47534" citStr="Pustejovsky (1992)" startWordPosition="7453" endWordPosition="7454">omething, not the thing itself. These nouns can therefore be used with some predictive certainty for inducing the semantic type in coercive environments such as &amp;quot;veto the expedition.&amp;quot; This work is still preliminary, however, and requires further examination (Pustejovsky and Rooth [unpublished]). 4.2 Induction of Semantic Relations from Syntactic Forms In this section, we present another experiment indicating the feasibility of inducing semantic tags for lexical items from corpora.&apos; Imagine being able to take the V-0 pairs 14 This section presents an abridged version of material reported on in Pustejovsky (1992). 346 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis count verb object 303 veto bill 84 veto legislation 58 veto measure 35 veto resolution 21 veto law 14 veto item 12 veto decision 9 veto proposal 9 veto plan 7 veto package 6 veto increase 5 veto sanction 5 veto penalty 4 veto notice 4 veto idea 4 veto appropriation 4 veto mission 4 veto attempt 3 veto search 3 veto cut 3 veto deal 1 veto expedition Figure 5 Counts for objects of veto/V. such as those given in Section 4.1, and then applying semantic tags to the verbs that are appropriate to the role they play for tha</context>
</contexts>
<marker>Pustejovsky, 1992</marker>
<rawString>Pustejovsky, J. (1992). &amp;quot;The acquisition of lexical semantic knowledge from large corpora.&amp;quot; In Proceedings, DARPA Spoken and Written Language Workshop. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>Linguistic constraints on type coercion.&amp;quot;</title>
<date>1993</date>
<booktitle>In Computational Lexical Semantics, edited</booktitle>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="59646" citStr="Pustejovsky (1993)" startWordPosition="9407" endWordPosition="9408">e data. The sets he extracts are promising; for example, the ten most similar nouns to treaty in his corpus are agreement, plan, constitution, contract, proposal, accord, amendment, rule, law, and legislation. This work is very close in spirit to our own investigation here; the emphasis on syntactic co-occurrence enables Hindle to extract his similarity lists automatically; they 17 For illustration, we use an abbreviated version of the lexical entries under discussion, highlighting only certain qualia for the verbs. For the most recent representation of verbal semantics in this framework, see Pustejovsky (1993). 351 Computational Linguistics Volume 19, Number 2 are therefore easy to compile for different corpora, different sublanguages, etc. Here we are attempting to use these techniques together with a model of lexical meaning, to capture deeper lexical semantic collocations; e.g., the generalization that the list of objects occurring for the word drink contains only liquids. In the final part of this section, we turn to how the analysis of corpora can provide lexical semantic preferences for verb selection. As discussed above, there is a growing body of research on deriving collocations from corpo</context>
</contexts>
<marker>Pustejovsky, 1993</marker>
<rawString>Pustejovsky, J. (1993). &amp;quot;Linguistic constraints on type coercion.&amp;quot; In Computational Lexical Semantics, edited by P. Saint-Dizier and E. Viegas. Cambridge University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>(in press). The Generative Lexicon: A Theory of Computational Lexical Semantics.</title>
<publisher>MIT Press.</publisher>
<marker>Pustejovsky, </marker>
<rawString>Pustejovsky, J. (in press). The Generative Lexicon: A Theory of Computational Lexical Semantics. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>P Anick</author>
</authors>
<title>The semantic interpretation of nominals.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, 12th International Conference of Computational Linguistics.</booktitle>
<location>Budapest, Hungary,</location>
<contexts>
<context position="11181" citStr="Pustejovsky and Anick 1988" startWordPosition="1687" endWordPosition="1690">se in Examples 4 through 7 indicate that this metonymy is grammaticalized through specific and systematic head-PP constructions. Example 4 read a book Example 5 read a story in a book Example 6 read a tape Example 7 read the information on the tape Instruments, on the other hand, display classic agent—instrument causative alternations, such as those in Examples 8 through 11 (cf. Fillmore 1968; Lakoff 1968, 1970). Example 8 ... smash the vase with the hammer Example 9 The hammer smashed the vase. Example 10 ... kill him with a gun Example 11 The gun killed him. Finally, figure-ground nominals (Pustejovsky and Anick 1988) permit perspective shifts such as those in Examples 12 through 15. These are nouns that refer to physical objects as well as the specific enclosure or aperture associated with it. Example 12 John painted the door. Example 13 John walked through the door. 334 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis Example 14 John is scrubbing the fireplace. Example 15 The smoke filled the fireplace. That is, paint and scrub are actions on physical objects while walk through and fi// are processes in spaces. These collocational patterns, we argue, are systematically predictable</context>
</contexts>
<marker>Pustejovsky, Anick, 1988</marker>
<rawString>Pustejovsky, J., and Anick, P. (1988). &amp;quot;The semantic interpretation of nominals.&amp;quot; In Proceedings, 12th International Conference of Computational Linguistics. Budapest, Hungary, August 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>S Bergler</author>
</authors>
<title>The acquisition of conceptual structure for the lexicon.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, Sixth National Conference on Artificial Intelligence.</booktitle>
<location>Seattle, Washington.</location>
<marker>Pustejovsky, Bergler, 1987</marker>
<rawString>Pustejovsky, J., and Bergler, S. (1987). &amp;quot;The acquisition of conceptual structure for the lexicon.&amp;quot; In Proceedings, Sixth National Conference on Artificial Intelligence. Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>B Boguraev</author>
</authors>
<title>Lexical knowledge representation and natural language processing.&amp;quot;</title>
<date>1993</date>
<journal>Artificial Intelligence.</journal>
<marker>Pustejovsky, Boguraev, 1993</marker>
<rawString>Pustejovsky, J., and Boguraev, B. (1993). &amp;quot;Lexical knowledge representation and natural language processing.&amp;quot; Artificial Intelligence.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Pustejovsky</author>
<author>M Rooth</author>
</authors>
<title>Type coercive environments in corpora.&amp;quot;</title>
<marker>Pustejovsky, Rooth, </marker>
<rawString>Pustejovsky, J., and Rooth, M. (unpublished). &amp;quot;Type coercive environments in corpora.&amp;quot;</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>S Waterman</author>
<author>J Cowie</author>
<author>G Stein</author>
</authors>
<title>Overview of the DIDEROT system for the Tipster text extraction project.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, DARPA TIPSTER 12-Month Evaluation.</booktitle>
<location>San Diego, California,</location>
<contexts>
<context position="25402" citStr="Pustejovsky et al. (1992)" startWordPosition="3930" endWordPosition="3933">poses, although the result is in no way ideal or uniform over all nominal forms. (cf. Cowie, Guthrie, and Pustejovsky [1992] for details of this operation on LDOCE.):&amp;quot; cigarette(x) CONST = tobacco(y),shredded(y),paper(z) FORMAL = roll(x) TELIC = smoke(T,w,x) AGENTIVE = artifact(x) In a related set of experiments performed while constructing a large lexical database for data extraction purposes, we seeded a lexicon with 6000 verbs from LDOCE. This process and the corpus tuning for both argument typing and subcategorization acquisition are described in Cowie, Guthrie, and Pustejovsky (1992) and Pustejovsky et al. (1992). In summary, based on a theory of lexical semantics, we have discussed how an MRD can be useful as a corpus for automatically seeding lexical structures. Rather than addressing the specific problems inherent in converting MRDs into useful lexicons, we have emphasized how it provides us, in a sense, with a generic vocabulary from which to begin lexical acquisition over corpora. In the next section, we will address the problem of taking these initial, and often very incomplete lexical structures, and enriching them with information acquired from corpus analysis. As mentioned in the previous sec</context>
</contexts>
<marker>Pustejovsky, Waterman, Cowie, Stein, 1992</marker>
<rawString>Pustejovsky, J.; Waterman, S.; Cowie, J.; and Stein, G. (1992). &amp;quot;Overview of the DIDEROT system for the Tipster text extraction project.&amp;quot; In Proceedings, DARPA TIPSTER 12-Month Evaluation. San Diego, California, September 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rau</author>
<author>P Jacobs</author>
</authors>
<title>Integrating top-down and bottom-up strategies in a text processing system.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, Second Conference on Applied Natural Language Processing,</booktitle>
<location>Austin Texas,</location>
<marker>Rau, Jacobs, 1988</marker>
<rawString>Rau, L., and Jacobs, P. (1988). &amp;quot;Integrating top-down and bottom-up strategies in a text processing system.&amp;quot; In Proceedings, Second Conference on Applied Natural Language Processing, Austin Texas, February 1988,129-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Russell</author>
<author>A Ballim</author>
<author>J Carroll</author>
<author>S Warwick-Armstrong</author>
</authors>
<title>A practical approach to multiple default inheritance for unification-based lexicons.&amp;quot;</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>3</issue>
<pages>311--337</pages>
<contexts>
<context position="15055" citStr="Russell et al. 1992" startWordPosition="2304" endWordPosition="2307"> Computational Linguistics Volume 19, Number 2 it serves (telic), and how it arises (agentive). This provides us with a semantic representation that can capture the multiple perspectives a single lexical item may assume in different contexts. Yet, the qualia for a lexical item such as tape are not isolated values for that one word, but are integrated into a global knowledge base indicating how these senses relate to other lexical items and their senses. This is the contribution of inheritance and the hierarchical structuring of knowledge (cf. Evans and Gazdar 1990; Copestake and Briscoe 1992; Russell et al. 1992). In Pustejovsky (1991) it is suggested that there are two types of relational structures for lexical knowledge; a fixed inheritance similar to that of an is-a hierarchy (cf. Touretzky 1986); and a dynamic structure that operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories.&apos; Reviewing briefly, the basic idea is that semantics allows for the dynamic creation of arbitrary concepts through the application of certain transformations to lexical meanings. Thus for every predicate, Q, we can generate its opposition, -Q. Similarly, th</context>
</contexts>
<marker>Russell, Ballim, Carroll, Warwick-Armstrong, 1992</marker>
<rawString>Russell, G.; Ballim, A.; Carroll, J.; and Warwick-Armstrong, S. (1992). &amp;quot;A practical approach to multiple default inheritance for unification-based lexicons.&amp;quot; Computational Linguistics, 18(3), 311-337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B M Slator</author>
</authors>
<title>Constructing contextually organized lexical semantic knowledge-bases.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, Third Annual Rocky Mountain Conference on Artificial Intelligence.</booktitle>
<location>Denver, Colorado,</location>
<contexts>
<context position="21017" citStr="Slator 1988" startWordPosition="3226" endWordPosition="3227">age programs and on the representation of that information (Wang, Vandendorpe, and Evens 1985; Ahlswede and Evens 1988). Following Boguraev et al. (1989) and Wilks et al. of 1989), we believe that much of what is needed for NLP lexicons can be found either explicitly or implicitly in a dictionary, and empirical evidence suggests that this information gives rise to a sufficiently rich lexical representation for use in extracting information from texts. Techniques for identifying explicit information in machine-readable dictionaries have been developed by many researchers (Boguraev et al. 1989; Slator 1988; Slator and Wilks 1987; Guthrie et al. 1990) and are well understood. Many properties of a word sense or the semantic relationships between word senses are available in MRDs, but this information can only be identified computationally through some analysis of the definition text of an entry (Atkins 1991). Some research has already been done in this area. Alshawi (1987), Boguraev et al. (1989), Vossen, Meijs, and den Broeder (1989), and the work described in Wilks et al. (1992) have made explicit some kinds of implicit information found in MRDs. Here we propose to refine and merge some of the </context>
</contexts>
<marker>Slator, 1988</marker>
<rawString>Slator, B. M. (1988). &amp;quot;Constructing contextually organized lexical semantic knowledge-bases.&amp;quot; In Proceedings, Third Annual Rocky Mountain Conference on Artificial Intelligence. Denver, Colorado, June 1988,142-148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B M Slator</author>
<author>Y A Wilks</author>
</authors>
<title>Toward semantic structures from dictionary entries.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, Second Annual Rocky Mountain Conference on Artificial Intelligence.</booktitle>
<pages>85--96</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="21040" citStr="Slator and Wilks 1987" startWordPosition="3228" endWordPosition="3231">and on the representation of that information (Wang, Vandendorpe, and Evens 1985; Ahlswede and Evens 1988). Following Boguraev et al. (1989) and Wilks et al. of 1989), we believe that much of what is needed for NLP lexicons can be found either explicitly or implicitly in a dictionary, and empirical evidence suggests that this information gives rise to a sufficiently rich lexical representation for use in extracting information from texts. Techniques for identifying explicit information in machine-readable dictionaries have been developed by many researchers (Boguraev et al. 1989; Slator 1988; Slator and Wilks 1987; Guthrie et al. 1990) and are well understood. Many properties of a word sense or the semantic relationships between word senses are available in MRDs, but this information can only be identified computationally through some analysis of the definition text of an entry (Atkins 1991). Some research has already been done in this area. Alshawi (1987), Boguraev et al. (1989), Vossen, Meijs, and den Broeder (1989), and the work described in Wilks et al. (1992) have made explicit some kinds of implicit information found in MRDs. Here we propose to refine and merge some of the previous techniques to </context>
</contexts>
<marker>Slator, Wilks, 1987</marker>
<rawString>Slator, B. M., and Wilks, Y. A. (1987). &amp;quot;Toward semantic structures from dictionary entries.&amp;quot; In Proceedings, Second Annual Rocky Mountain Conference on Artificial Intelligence. Boulder, Colorado, 85-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
</authors>
<title>Macrocoding the lexicon with co-occurrence knowledge.&amp;quot; In Lexical Acquisition: Using On-Line Resources to Build a Lexicon, edited by U. Zernik. Lawrence Erlbaum Associates.</title>
<date>1991</date>
<contexts>
<context position="52431" citStr="Smadja 1991" startWordPosition="8287" endWordPosition="8288">t venture Figure 6 Verb-object pairs with prep = with. identify the agent of the verb as its argument (cf. Nilsen (1973)). This is confirmed by our data, shown in Figure 6. Conversely, verbs governing nominals collocating with a between-phrase will not refer to the agentive since the phrase is saturated already. Indeed, the only verb occurring in this position with any frequency is the copula be, namely with the following counts: 12 be/V venture/O. Thus, weak semantic types can be induced on the basis of syntactic behavior. There is a growing literature on corpus-based acquisition and tuning (Smadja 1991a; Zernik and Jacobs 1991; Brent 1991; as well as Grishman and Sterling 1992). We share with these researchers a general dependence on well-behaved collocational patterns and distributional structures. Probably the main distinguishing feature of our approach is its reliance on a fairly well studied semantic framework to aid and guide the semantic induction process itself, whether it involves selectional restrictions or semantic types. 5. Lexical Presuppositions and Preferences In the previous section we presented algorithms for extracting collocational information from corpora, in order to sup</context>
<context position="60346" citStr="Smadja 1991" startWordPosition="9516" endWordPosition="9517">ferent corpora, different sublanguages, etc. Here we are attempting to use these techniques together with a model of lexical meaning, to capture deeper lexical semantic collocations; e.g., the generalization that the list of objects occurring for the word drink contains only liquids. In the final part of this section, we turn to how the analysis of corpora can provide lexical semantic preferences for verb selection. As discussed above, there is a growing body of research on deriving collocations from corpora (cf. Church and Hanks 1990; Klavans, Chodorow, and Wacholder 1990; Wilks et al. 1993; Smadja 1991a, 1991b; Calzolari and Bindi 1990). Here we employ the tools of semantic analysis from Section 1 to examine the behavior of metonymy with reporting verbs. We will show, on the basis of corpus analysis, how verbs display marked differences in the ability to license metonymic operations over their arguments. Such information, we argue, is part of the preference semantics for a sublanguage, as automatically derived from corpus. Metonymy can be seen as a case of &amp;quot;licensed violation&amp;quot; of selectional restrictions. For example, while the verb announce selects for a human subject, sentences like The P</context>
</contexts>
<marker>Smadja, 1991</marker>
<rawString>Smadja, F. (1991a). &amp;quot;Macrocoding the lexicon with co-occurrence knowledge.&amp;quot; In Lexical Acquisition: Using On-Line Resources to Build a Lexicon, edited by U. Zernik. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
</authors>
<title>From n-grams to collocations: an evaluation of xtract.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, 29th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<location>Berkeley, California,</location>
<contexts>
<context position="52431" citStr="Smadja 1991" startWordPosition="8287" endWordPosition="8288">t venture Figure 6 Verb-object pairs with prep = with. identify the agent of the verb as its argument (cf. Nilsen (1973)). This is confirmed by our data, shown in Figure 6. Conversely, verbs governing nominals collocating with a between-phrase will not refer to the agentive since the phrase is saturated already. Indeed, the only verb occurring in this position with any frequency is the copula be, namely with the following counts: 12 be/V venture/O. Thus, weak semantic types can be induced on the basis of syntactic behavior. There is a growing literature on corpus-based acquisition and tuning (Smadja 1991a; Zernik and Jacobs 1991; Brent 1991; as well as Grishman and Sterling 1992). We share with these researchers a general dependence on well-behaved collocational patterns and distributional structures. Probably the main distinguishing feature of our approach is its reliance on a fairly well studied semantic framework to aid and guide the semantic induction process itself, whether it involves selectional restrictions or semantic types. 5. Lexical Presuppositions and Preferences In the previous section we presented algorithms for extracting collocational information from corpora, in order to sup</context>
<context position="60346" citStr="Smadja 1991" startWordPosition="9516" endWordPosition="9517">ferent corpora, different sublanguages, etc. Here we are attempting to use these techniques together with a model of lexical meaning, to capture deeper lexical semantic collocations; e.g., the generalization that the list of objects occurring for the word drink contains only liquids. In the final part of this section, we turn to how the analysis of corpora can provide lexical semantic preferences for verb selection. As discussed above, there is a growing body of research on deriving collocations from corpora (cf. Church and Hanks 1990; Klavans, Chodorow, and Wacholder 1990; Wilks et al. 1993; Smadja 1991a, 1991b; Calzolari and Bindi 1990). Here we employ the tools of semantic analysis from Section 1 to examine the behavior of metonymy with reporting verbs. We will show, on the basis of corpus analysis, how verbs display marked differences in the ability to license metonymic operations over their arguments. Such information, we argue, is part of the preference semantics for a sublanguage, as automatically derived from corpus. Metonymy can be seen as a case of &amp;quot;licensed violation&amp;quot; of selectional restrictions. For example, while the verb announce selects for a human subject, sentences like The P</context>
</contexts>
<marker>Smadja, 1991</marker>
<rawString>Smadja, F. (1991b). &amp;quot;From n-grams to collocations: an evaluation of xtract.&amp;quot; In Proceedings, 29th Annual Meeting of the Association for Computational Linguistics. Berkeley, California, June 1991,279-284.</rawString>
</citation>
<citation valid="true">
<title>Information Retrieval Experiments.</title>
<date>1981</date>
<editor>Sparck Jones, K., editor.</editor>
<location>Butterworth.</location>
<contexts>
<context position="18129" citStr="(1981)" startWordPosition="2785" endWordPosition="2785">ll nouns in a domain. Since it is our belief that such representations are generic structures across all domains, it is our long-term goal to develop methods for automatically extracting these relations and values from on-line corpora. In the sections that follow, we describe several experiments indicating that the qualia structures do, in fact, correlate with well-behaved collocational patterns, thereby allowing us to perform structure-matching operations over corpora to find these relations. 7 This is similar to thesauruslike structures, within the IR community, cf. for example Sparck Jones (1981). 8 Details of the derivation are as follows. Let Q be mounted, then —Q gives ---,mounted, and &lt; applied to these two states gives Q &lt; which is lexicalized as dismount. A similar derivation exists for mount. Cf. Pustejovsky (1991) for details. 9 This is reflected in the sublanguage work of Grishman, Hirschman, and Nhan (1986), whose automated discovery procedures are aimed at clustering nouns into categories like diagnosis and symptom. 336 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis 2. Seeding Lexical Structures from MRDs In this section we discuss briefly how a le</context>
</contexts>
<marker>1981</marker>
<rawString>Sparck Jones, K., editor. (1981). Information Retrieval Experiments. Butterworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparck Jones</author>
<author>K</author>
</authors>
<title>Synonymy and Semantic Classification. Edinburgh Information Technology Series (EDITS).</title>
<date>1986</date>
<publisher>Edinburgh University Press.</publisher>
<marker>Jones, K, 1986</marker>
<rawString>Sparck Jones, K. (1986). Synonymy and Semantic Classification. Edinburgh Information Technology Series (EDITS). Edinburgh University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Talmy</author>
</authors>
<title>Lexicalization patterns.&amp;quot;</title>
<date>1985</date>
<booktitle>In Language Typology and Syntactic Description 3: Grammatical Categories and the Computational Linguistics Volume 19, Number</booktitle>
<volume>2</volume>
<pages>57--149</pages>
<publisher>Cambridge University Press.</publisher>
<note>Lexicon, edited by</note>
<marker>Talmy, 1985</marker>
<rawString>Talmy, L. (1985). &amp;quot;Lexicalization patterns.&amp;quot; In Language Typology and Syntactic Description 3: Grammatical Categories and the Computational Linguistics Volume 19, Number 2 Lexicon, edited by T. Shopen, 57-149. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Touretzky</author>
</authors>
<title>The Mathematics of Inheritance Systems.</title>
<date>1986</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="15245" citStr="Touretzky 1986" startWordPosition="2337" endWordPosition="2338"> lexical item may assume in different contexts. Yet, the qualia for a lexical item such as tape are not isolated values for that one word, but are integrated into a global knowledge base indicating how these senses relate to other lexical items and their senses. This is the contribution of inheritance and the hierarchical structuring of knowledge (cf. Evans and Gazdar 1990; Copestake and Briscoe 1992; Russell et al. 1992). In Pustejovsky (1991) it is suggested that there are two types of relational structures for lexical knowledge; a fixed inheritance similar to that of an is-a hierarchy (cf. Touretzky 1986); and a dynamic structure that operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories.&apos; Reviewing briefly, the basic idea is that semantics allows for the dynamic creation of arbitrary concepts through the application of certain transformations to lexical meanings. Thus for every predicate, Q, we can generate its opposition, -Q. Similarly, these two predicates can be related temporally to generate the transition events defining this opposition. These operations include but may not be limited to: negation; &lt;, temporal precedence;</context>
</contexts>
<marker>Touretzky, 1986</marker>
<rawString>Touretzky, D. S. (1986). The Mathematics of Inheritance Systems. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veronis</author>
<author>N Ide</author>
</authors>
<title>An assessment of semantic information automatically extracted from machine readable dictionaries.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, Fifth Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<location>Berlin, Germany,</location>
<contexts>
<context position="20006" citStr="Veronis and Ide (1991)" startWordPosition="3069" endWordPosition="3072">octer, Ilson, and Ayto 1978). These are then automatically adapted to the format of generative lexical structures. It is these lexical structures that are then statistically tuned against the corpus, following the methods outlined in Anick and Pustejovsky (1990) and Pustejovsky (1992). Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn (1985), Byrd et al. (1987), Markowitz, Ahlswede, and Evens (1986), and Nakamura and Nagao (1988) showed that taxonomic information and certain semantic relations can be extracted from MRDs using fairly simple techniques. Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et al. (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section. Our goal is to automate, to the extent possible, the initial construction of these structures. Extensive research has been done on the kind of information needed by natural language programs and on the representation of that information (Wang, Vandendorpe, and Evens 1985; Ahlswede and Evens 1988). Following Boguraev et al. (1989) and Wilks et al. of 1989), we believe that muc</context>
</contexts>
<marker>Veronis, Ide, 1991</marker>
<rawString>Veronis, J., and Ide, N. (1991). &amp;quot;An assessment of semantic information automatically extracted from machine readable dictionaries.&amp;quot; In Proceedings, Fifth Conference of the European Chapter of the Association for Computational Linguistics. Berlin, Germany, April 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B C Vickery</author>
</authors>
<date>1975</date>
<booktitle>Classification and Indexing in Science. Butterworth and Co.</booktitle>
<contexts>
<context position="16931" citStr="Vickery 1975" startWordPosition="2602" endWordPosition="2603">discussion, with mounted as the predicate Q, successive applications of the negation and temporal precedence operators derives the transition verbs mount and dismount.&apos; We return to a discussion of this in Section 3, and to how this space relates to statistically significant collocations in text. It is our view that the approach outlined above for representing lexical knowledge can be put to use in the service of information retrieval tasks. In this respect, our proposal can be compared to attempts at object classification in information science. One approach, known as faceted classification (Vickery 1975) proceeds roughly as follows: collect all terms lying within a field; then group the terms into facets by assigning them to categories. Typical examples of this are state, property, reaction, and device. However, each subject area is likely to have its own sets of categories, which makes it difficult to re-use a set of facet classifications.9 Even if the relational information provided by the qualia structure and inheritance would improve performance in information retrieval tasks, one problem still remains, namely that it would be very time-consuming to hand-code such structures for all nouns</context>
</contexts>
<marker>Vickery, 1975</marker>
<rawString>Vickery, B. C. (1975). Classification and Indexing in Science. Butterworth and Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Walker</author>
<author>R A Amsler</author>
</authors>
<title>The use of machine-readable dictionaries in sublanguage analysis.&amp;quot;</title>
<date>1986</date>
<journal>In Analyzing Language</journal>
<note>in Restricted Domains, edited by</note>
<marker>Walker, Amsler, 1986</marker>
<rawString>Walker, D. E., and Amsler, R. A. (1986). &amp;quot;The use of machine-readable dictionaries in sublanguage analysis.&amp;quot; In Analyzing Language in Restricted Domains, edited by R. Grishman and R. Kittredge. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
<author>W Meijs</author>
<author>M den Broeder</author>
</authors>
<title>Meaning and structure in dictionary definitions.&amp;quot;</title>
<date>1989</date>
<booktitle>In Computational Lexicography for Natural Language Processing,</booktitle>
<publisher>Longman.</publisher>
<note>edited by</note>
<marker>Vossen, Meijs, den Broeder, 1989</marker>
<rawString>Vossen, P.; Meijs, W.; and den Broeder, M. (1989). &amp;quot;Meaning and structure in dictionary definitions.&amp;quot; In Computational Lexicography for Natural Language Processing, edited by B. Boguraev and T. Briscoe. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-C Wang</author>
<author>J Vandendorpe</author>
<author>M Evens</author>
</authors>
<title>Relational thesauri in information retrieval.&amp;quot;</title>
<date>1985</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>36--15</pages>
<marker>Wang, Vandendorpe, Evens, 1985</marker>
<rawString>Wang, Y.-C.; Vandendorpe, J.; and Evens, M. (1985). &amp;quot;Relational thesauri in information retrieval.&amp;quot; Journal of the American Society for Information Science, 36,15-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wilensky</author>
<author>D N Chin</author>
<author>M Luria</author>
<author>J Martin</author>
<author>J Mayfield</author>
<author>D Wu</author>
</authors>
<title>The Berkeley UNIX consultant project.&amp;quot;</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>14</volume>
<issue>4</issue>
<pages>35--84</pages>
<marker>Wilensky, Chin, Luria, Martin, Mayfield, Wu, 1988</marker>
<rawString>Wilensky, R.; Chin, D. N.; Luria, M.; Martin, J.; Mayfield, J.; and Wu, D. (1988). &amp;quot;The Berkeley UNIX consultant project.&amp;quot; Computational Linguistics, 14(4), 35-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y A Wilks</author>
</authors>
<title>Making preferences more active.&amp;quot;</title>
<date>1978</date>
<journal>Artificial Intelligence,</journal>
<volume>10</volume>
<pages>75--97</pages>
<contexts>
<context position="3706" citStr="Wilks 1978" startWordPosition="530" endWordPosition="531">ntic tagging of lexical items. Unlike purely statistical collocational analyses, employing a semantic theory allows for the automatic construction of deeper semantic relationships among words appearing in collocational systems. We illustrate the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary. In addition to conventional lexical semantic relations, we show how information concerning lexical presuppositions and preference relations (Wilks 1978) can also be acquired from corpora, when analyzed with the appropriate semantic tools. Finally, we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm linguistic hypotheses. The aim of our research is to discover what kinds of knowledge can be reliably acquired through the use of these methods, exploiting, as they do, general linguistic knowledge rather than domain knowledge. In this respect, our program is similar to Zernik&apos;s (1989) work on extracting verb semantics from corpora using lexica</context>
</contexts>
<marker>Wilks, 1978</marker>
<rawString>Wilks, Y. A. (1978). &amp;quot;Making preferences more active.&amp;quot; Artificial Intelligence, 10, 75-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y A Wilks</author>
<author>D Fass</author>
<author>C-M Guo</author>
<author>J E McDonald</author>
<author>T Plate</author>
<author>B M Slator</author>
</authors>
<title>A tractable machine dictionary as a resource for computational semantics.&amp;quot;</title>
<date>1989</date>
<booktitle>In Computational Lexicography for Natural Language Processing, edited</booktitle>
<pages>193--228</pages>
<publisher>Longman.</publisher>
<marker>Wilks, Fass, Guo, McDonald, Plate, Slator, 1989</marker>
<rawString>Wilks, Y. A.; Fass, D.; Guo, C.-M.; McDonald, J. E.; Plate, T.; and Slator, B. M. (1989). &amp;quot;A tractable machine dictionary as a resource for computational semantics.&amp;quot; In Computational Lexicography for Natural Language Processing, edited by B. Boguraev and T. Briscoe, 193-228. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
<author>D Fass</author>
<author>C-M Guo</author>
<author>J E McDonald</author>
<author>T Plate</author>
<author>B M Slator</author>
</authors>
<title>Providing machine tractable dictionary tools.&amp;quot;</title>
<date>1993</date>
<booktitle>In Semantics and the Lexicon, edited</booktitle>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="18917" citStr="Wilks et al. 1993" startWordPosition="2904" endWordPosition="2907"> similar derivation exists for mount. Cf. Pustejovsky (1991) for details. 9 This is reflected in the sublanguage work of Grishman, Hirschman, and Nhan (1986), whose automated discovery procedures are aimed at clustering nouns into categories like diagnosis and symptom. 336 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis 2. Seeding Lexical Structures from MRDs In this section we discuss briefly how a lexical semantic theory can help in extracting information from machine-readable dictionaries (MRDs). We describe research on conversion of a machine-tractable dictionary (Wilks et al. 1993) into a usable lexical knowledge base (Boguraev 1991). Although the results here are preliminary, it is important to mention the process of converting an MRD into a lexical knowledge base, so that the process of corpus-tuning is put into the proper perspective. The initial seeding of lexical structures is being done independently both from the Oxford Advanced Learners Dictionary (OALD) and from lexical entries in the Longman Dictionary of Contemporary English (Procter, Ilson, and Ayto 1978). These are then automatically adapted to the format of generative lexical structures. It is these lexica</context>
<context position="60333" citStr="Wilks et al. 1993" startWordPosition="9512" endWordPosition="9515"> to compile for different corpora, different sublanguages, etc. Here we are attempting to use these techniques together with a model of lexical meaning, to capture deeper lexical semantic collocations; e.g., the generalization that the list of objects occurring for the word drink contains only liquids. In the final part of this section, we turn to how the analysis of corpora can provide lexical semantic preferences for verb selection. As discussed above, there is a growing body of research on deriving collocations from corpora (cf. Church and Hanks 1990; Klavans, Chodorow, and Wacholder 1990; Wilks et al. 1993; Smadja 1991a, 1991b; Calzolari and Bindi 1990). Here we employ the tools of semantic analysis from Section 1 to examine the behavior of metonymy with reporting verbs. We will show, on the basis of corpus analysis, how verbs display marked differences in the ability to license metonymic operations over their arguments. Such information, we argue, is part of the preference semantics for a sublanguage, as automatically derived from corpus. Metonymy can be seen as a case of &amp;quot;licensed violation&amp;quot; of selectional restrictions. For example, while the verb announce selects for a human subject, sentenc</context>
</contexts>
<marker>Wilks, Fass, Guo, McDonald, Plate, Slator, 1993</marker>
<rawString>Wilks, Y.; Fass, D.; Guo, C.-M.; McDonald, J. E.; Plate, T.; and Slator, B. M. (1993). &amp;quot;Providing machine tractable dictionary tools.&amp;quot; In Semantics and the Lexicon, edited by J. Pustejovsky. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Zernik</author>
</authors>
<title>Lexicon acquisition: Learning from corpus by exploiting lexical categories.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, IJCAI-89.</booktitle>
<contexts>
<context position="4886" citStr="Zernik (1989)" startWordPosition="716" endWordPosition="717">mantics from corpora using lexical categories. Our research, however, differs in two respects: first, we employ a more expressive lexical semantics; second, our focus is on all major categories in the language, and not just verbs. This is important since for full-text information retrieval, information about nominals is paramount, as most queries tend to be expressed as conjunctions of nouns. From a theoretical perspective, we believe that the contribution of the lexical semantics of nominals to the overall structure of the lexicon has been somewhat neglected, relative to that of verbs. While Zernik (1989) presents ambiguity and metonymy as a potential obstacle to effective corpus analysis, we believe that the existence of motivated metonymic structures actually provides valuable clues for semantic analysis of nouns in a corpus. We will assume, for this paper, the general framework of a generative lexicon as outlined in Pustejovsky (1991). In particular, we make use of the principles of type coercion and qualia structure. This model of semantic knowledge associated with words is based on a system of generative devices that is able to recursively define new word senses for lexical items in the l</context>
</contexts>
<marker>Zernik, 1989</marker>
<rawString>Zernik, U. (1989). &amp;quot;Lexicon acquisition: Learning from corpus by exploiting lexical categories.&amp;quot; In Proceedings, IJCAI-89.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>