<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.88618">
Unsupervised Transcription of Historical Documents
</title>
<author confidence="0.997768">
Taylor Berg-Kirkpatrick Greg Durrett Dan Klein
</author>
<affiliation confidence="0.9974825">
Computer Science Division
University of California at Berkeley
</affiliation>
<email confidence="0.995996">
{tberg,gdurrett,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.994738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999425625">
We present a generative probabilistic
model, inspired by historical printing pro-
cesses, for transcribing images of docu-
ments from the printing press era. By
jointly modeling the text of the docu-
ment and the noisy (but regular) process
of rendering glyphs, our unsupervised sys-
tem is able to decipher font structure and
more accurately transcribe images into
text. Overall, our system substantially out-
performs state-of-the-art solutions for this
task, achieving a 31% relative reduction
in word error rate over the leading com-
mercial system for historical transcription,
and a 47% relative reduction over Tesser-
act, Google’s open source OCR system.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987301458333333">
Standard techniques for transcribing modern doc-
uments do not work well on historical ones. For
example, even state-of-the-art OCR systems pro-
duce word error rates of over 50% on the docu-
ments shown in Figure 1. Unsurprisingly, such er-
ror rates are too high for many research projects
(Arlitsch and Herbert, 2004; Shoemaker, 2005;
Holley, 2010). We present a new, generative
model specialized to transcribing printing-press
era documents. Our model is inspired by the un-
derlying printing processes and is designed to cap-
ture the primary sources of variation and noise.
One key challenge is that the fonts used in his-
torical documents are not standard (Shoemaker,
2005). For example, consider Figure 1a. The fonts
are not irregular like handwriting – each occur-
rence of a given character type, e.g. a, will use the
same underlying glyph. However, the exact glyphs
are unknown. Some differences between fonts are
minor, reflecting small variations in font design.
Others are more severe, like the presence of the
archaic long s character before 1804. To address
the general problem of unknown fonts, our model
(C)
</bodyText>
<figureCaption confidence="0.996071">
Figure 1: Portions of historical documents with (a) unknown
font, (b) uneven baseline, and (c) over-inking.
</figureCaption>
<bodyText confidence="0.9999194375">
learns the font in an unsupervised fashion. Font
shape and character segmentation are tightly cou-
pled, and so they are modeled jointly.
A second challenge with historical data is that
the early typesetting process was noisy. Hand-
carved blocks were somewhat uneven and often
failed to sit evenly on the mechanical baseline.
Figure 1b shows an example of the text’s baseline
moving up and down, with varying gaps between
characters. To deal with these phenomena, our
model incorporates random variables that specifi-
cally describe variations in vertical offset and hor-
izontal spacing.
A third challenge is that the actual inking was
also noisy. For example, in Figure 1c some charac-
ters are thick from over-inking while others are ob-
scured by ink bleeds. To be robust to such render-
ing irregularities, our model captures both inking
levels and pixel-level noise. Because the model
is generative, we can also treat areas that are ob-
scured by larger ink blotches as unobserved, and
let the model predict the obscured text based on
visual and linguistic context.
Our system, which we call Ocular, operates by
fitting the model to each document in an unsuper-
vised fashion. The system outperforms state-of-
the-art baselines, giving a 47% relative error re-
duction over Google’s open source Tesseract sys-
tem, and giving a 31% relative error reduction over
ABBYY’s commercial FineReader system, which
has been used in large-scale historical transcrip-
tion projects (Holley, 2010).
</bodyText>
<page confidence="0.968722">
207
</page>
<note confidence="0.9595575">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 207–217,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.990112">
Figure 2: An example image from a historical document (X)
and its transcription (E).
</figureCaption>
<sectionHeader confidence="0.999241" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999919418604651">
Relatively little prior work has built models specif-
ically for transcribing historical documents. Some
of the challenges involved have been addressed
(Ho and Nagy, 2000; Huang et al., 2006; Kae and
Learned-Miller, 2009), but not in a way targeted
to documents from the printing press era. For ex-
ample, some approaches have learned fonts in an
unsupervised fashion but require pre-segmentation
of the image into character or word regions (Ho
and Nagy, 2000; Huang et al., 2006), which is not
feasible for noisy historical documents. Kae and
Learned-Miller (2009) jointly learn the font and
image segmentation but do not outperform mod-
ern baselines.
Work that has directly addressed historical doc-
uments has done so using a pipelined approach,
and without fully integrating a strong language
model (Vamvakas et al., 2008; Kluzner et al.,
2009; Kae et al., 2010; Kluzner et al., 2011).
The most comparable work is that of Kopec and
Lomelin (1996) and Kopec et al. (2001). They
integrated typesetting models with language mod-
els, but did not model noise. In the NLP com-
munity, generative models have been developed
specifically for correcting outputs of OCR systems
(Kolak et al., 2003), but these do not deal directly
with images.
A closely related area of work is automatic de-
cipherment (Ravi and Knight, 2008; Snyder et al.,
2010; Ravi and Knight, 2011; Berg-Kirkpatrick
and Klein, 2011). The fundamental problem is
similar to our own: we are presented with a se-
quence of symbols, and we need to learn a corre-
spondence between symbols and letters. Our ap-
proach is also similar in that we use a strong lan-
guage model (in conjunction with the constraint
that the correspondence be regular) to learn the
correct mapping. However, the symbols are not
noisy in decipherment problems and in our prob-
lem we face a grid of pixels for which the segmen-
tation into symbols is unknown. In contrast, deci-
pherment typically deals only with discrete sym-
bols.
</bodyText>
<sectionHeader confidence="0.987067" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.998131555555556">
Most historical documents have unknown fonts,
noisy typesetting layouts, and inconsistent ink lev-
els, usually simultaneously. For example, the por-
tion of the document shown in Figure 2 has all
three of these problems. Our model must handle
them jointly.
We take a generative modeling approach in-
spired by the overall structure of the historical
printing process. Our model generates images of
documents line by line; we present the generative
process for the image of a single line. Our pri-
mary random variables are E (the text) and X (the
pixels in an image of the line). Additionally, we
have a random variable T that specifies the layout
of the bounding boxes of the glyphs in the image,
and a random variable R that specifies aspects of
the inking and rendering process. The joint distri-
bution is:
</bodyText>
<equation confidence="0.656286">
P(E, T, R, X) =
P(E) [Language model]
· P(T|E) [Typesetting model]
· P(R) [Inking model]
· P(X|E, T, R) [Noise model]
</equation>
<bodyText confidence="0.997521666666667">
We let capital letters denote vectors of concate-
nated random variables, and we denote the indi-
vidual random variables with lower-case letters.
For example, E represents the entire sequence of
text, while ei represents ith character in the se-
quence.
</bodyText>
<subsectionHeader confidence="0.992373">
3.1 Language Model P(E)
</subsectionHeader>
<bodyText confidence="0.994133076923077">
Our language model, P(E), is a Kneser-Ney
smoothed character n-gram model (Kneser and
Ney, 1995). We generate printed lines of text
(rather than sentences) independently, without
generating an explicit stop character. This means
that, formally, the model must separately generate
the character length of each line. We choose not to
bias the model towards longer or shorter character
sequences and let the line length m be drawn uni-
formly at random from the positive integers less
than some large constant M.1 When i &lt; 1, let ei
denote a line-initial null character. We can now
write:
</bodyText>
<equation confidence="0.980084333333333">
m
P(E) = P(m) · P(ei|ei−1, ... , ei−n)
i=1
</equation>
<footnote confidence="0.892981">
1In particular, we do not use the kind of “word bonus”
common to statistical machine translation models.
</footnote>
<figure confidence="0.868927357142857">
E It appeared that the Prisoner was very
X
Wandering baseline Historical font Over-inked
208
Inking params
Offset: ✓VERT
a
a a
Inking: ✓INK
LM params
abc••• z
a
a
a
</figure>
<equation confidence="0.88255375">
P( ·  |pe)
P ( � �
( � �h)
)
ei-1 ei ei+1
XLPAD VGLYPH VRPAD
I 112 112
li 9i ri
</equation>
<figure confidence="0.961310416666667">
Font params
Glyph weights: 0.
Bounding box probs:
Glyph width: ✓GLYPH
�
1 30
Left pad
width: ✓cLPAD
Right pad
width: ✓RPAD
�
s 1 1 s
</figure>
<figureCaption confidence="0.999648">
Figure 3: Character tokens ei are generated by the language model. For each token index i, a glyph bounding box width gi,
</figureCaption>
<bodyText confidence="0.710205">
left padding width li, and a right padding width ri, are generated. Finally, the pixels in each glyph bounding box XGLYPH are
</bodyText>
<equation confidence="0.951303">
i
generated conditioned on the corresponding character, while the pixels in left and right padding bounding boxes, XLPAD
i and
XRPAD
</equation>
<bodyText confidence="0.594832">
i , are generated from a background distribution.
</bodyText>
<subsectionHeader confidence="0.983411">
3.2 Typesetting Model P(TIE)
</subsectionHeader>
<bodyText confidence="0.988250428571429">
Generally speaking, the process of typesetting
produces a line of text by first tiling bounding
boxes of various widths and then filling in the
boxes with glyphs. Our generative model, which
is depicted in Figure 3, reflects this process. As
a first step, our model generates the dimensions
of character bounding boxes; for each character
token index i we generate three bounding box
widths: a glyph box width gi, a left padding box
width li, and a right padding box width ri, as
shown in Figure 3. We let the pixel height of all
lines be fixed to h. Let Ti = (li7 gi7 ri) so that Ti
specifies the dimensions of the character box for
token index i; T is then the concatenation of all
Ti, denoting the full layout.
Because the width of a glyph depends on its
shape, and because of effects resulting from kern-
ing and the use of ligatures, the components of
each Ti are drawn conditioned on the character
token ei. This means that, as part of our param-
eterization of the font, for each character type c
we have vectors of multinomial parameters θLPAD
c ,
θGLYPH
c , and θRPAD
c
dimensions of character boxes of type c. These
parameters are depicted on the right-hand side of
</bodyText>
<figureCaption confidence="0.8929685">
Figure 3. We can now express the typesetting lay-
out portion of the model as:
</figureCaption>
<equation confidence="0.9764286">
P(T|E) =
[P(li; θLPAD
ei ) ·P(gi; θGLYPH
ei ) ·P(ri; θRPAD
ei )]
</equation>
<bodyText confidence="0.999778833333333">
Each character type c in our font has another set
of parameters, a matrix Oc. These are weights that
specify the shape of the character type’s glyph,
and are depicted in Figure 3 as part of the font pa-
rameters. Oc will come into play when we begin
generating pixels in Section 3.3.
</bodyText>
<subsubsectionHeader confidence="0.715752">
3.2.1 Inking Model P(R)
</subsubsectionHeader>
<bodyText confidence="0.99737375">
Before we start filling the character boxes with
pixels, we need to specify some properties of
the inking and rendering process, including the
amount of ink used and vertical variation along
the text baseline. Our model does this by gener-
ating, for each character token index i, a discrete
value di that specifies the overall inking level in
the character’s bounding box, and a discrete value
vi that specifies the glyph’s vertical offset. These
variations in the inking and typesetting process are
mostly independent of character type. Thus, in
governing the distribution of the
</bodyText>
<equation confidence="0.965231166666667">
�m
i=1
P (Ti|ei)
=
�m
i=1
</equation>
<page confidence="0.992085">
209
</page>
<bodyText confidence="0.999945857142857">
our model, their distributions are not character-
specific. There is one global set of multinomial
parameters governing inking level (BINK), and an-
other governing offset (BVERT); both are depicted
on the left-hand side of Figure 3. Let Ri = (di, vi)
and let R be the concatenation of all Ri so that we
can express the inking model as:
</bodyText>
<equation confidence="0.996427333333333">
P(R) = P(Ri)
i=1
[P(di; BINK) ·P(vi; BVERT)J
</equation>
<bodyText confidence="0.9997375">
The di and vi variables are suppressed in Figure 3
to reduce clutter but are expressed in Figure 4,
which depicts the process of rendering a glyph
box.
</bodyText>
<subsectionHeader confidence="0.440684">
3.3 Noise Model P(X|E, T, R)
</subsectionHeader>
<bodyText confidence="0.995265857142857">
Now that we have generated a typesetting layout
T and an inking context R, we have to actually
generate each of the pixels in each of the charac-
ter boxes, left padding boxes, and right padding
boxes; the matrices that these groups of pixels
comprise are denoted XGLYPH
i , XLPAD
i , and XRPAD
i ,
respectively, and are depicted at the bottom of Fig-
ure 3.
We assume that pixels are binary valued and
sample their values independently from Bernoulli
distributions.2 The probability of black (the
Bernoulli parameter) depends on the type of pixel
generated. All the pixels in a padding box have
the same probability of black that depends only on
the inking level of the box, di. Since we have al-
ready generated this value and the widths li and ri
of each padding box, we have enough information
to generate left and right padding pixel matrices
</bodyText>
<equation confidence="0.551477333333333">
XLPAD
i and XRPAD
i .
</equation>
<bodyText confidence="0.992194769230769">
The Bernoulli parameter of a pixel inside a
glyph bounding box depends on the pixel’s loca-
tion inside the box (as well as on di and vi, but
for simplicity of exposition, we temporarily sup-
press this dependence) and on the model param-
eters governing glyph shape (for each character
type c, the parameter matrix Oc specifies the shape
of the character’s glyph.) The process by which
glyph pixels are generated is depicted in Figure 4.
The dependence of glyph pixels on location
complicates generation of the glyph pixel matrix
XGLYPH since the corresponding parameter matrix
i
</bodyText>
<footnote confidence="0.431607">
2We could generate real-valued pixels with a different
choice of noise distribution.
</footnote>
<figureCaption confidence="0.7390842">
Figure 4: We generate the pixels for the character token ei
by first sampling a glyph width gi, an inking level di, and
a vertical offset vi. Then we interpolate the glyph weights
φei and apply the logistic function to produce a matrix of
Bernoulli parameters of width gi, inking di, and offset vi.
</figureCaption>
<bodyText confidence="0.92293725">
BPIXEL(j, k, gi, di, vi; • 0 ) is the Bernoulli parameter at row j
a, e�
and column k. Finally, we sample from each Bernoulli distri-
bution to generate a matrix of pixel values, XGLYPH
i .
Oei has some type-level width w which may dif-
fer from the current token-level width gi. Intro-
ducing distinct parameters for each possible width
would yield a model that can learn completely dif-
ferent glyph shapes for slightly different widths of
the same character. We, instead, need a parame-
terization that ties the shapes for different widths
together, and at the same time allows mobility in
the parameter space during learning.
Our solution is to horizontally interpolate the
weights of the shape parameter matrix Oei down
to a smaller set of columns matching the token-
level choice of glyph width gi. Thus, the type-
level matrix Oei specifies the canonical shape of
the glyph for character ei when it takes its max-
imum width w. After interpolating, we apply
the logistic function to produce the individual
Bernoulli parameters. If we let [XGLYPH
i ]jk denote
the value of the pixel at the jth row and kth col-
umn of the glyph pixel matrix XGLYPH for token i,
i
and let BPIXEL(j, k, gi; Oei) denote the token-level
</bodyText>
<figure confidence="0.998212448275862">
Glyph weights a a a } Choose
0e; width
9i
} Pixel values
XGLvPH Bernoulli
� �k
I
Sample pixels
}
a
a
Bernoulli parameters
✓PIXEL(j, k, gi, di, vi; 0ei)
Interpolate, apply logistic
a
a
a
a
}
}
Vi
Choose
inking
di
Choose
offset
=
m
ri
</figure>
<page confidence="0.867333">
210
</page>
<figureCaption confidence="0.992463">
Figure 5: In order to produce Bernoulli parameter matrices
θPIXEL of variable width, we interpolate over columns of φc
with vectors µ, and apply the logistic function to each result.
</figureCaption>
<bodyText confidence="0.698944">
Bernoulli parameter for this pixel, we canD write:
</bodyText>
<equation confidence="0.566121">
[Xi&amp;quot;PH1 jk — Bernoulli(BPIXEL( \\j, k, gif 0,j)
</equation>
<bodyText confidence="0.999513571428571">
The interpolation process for a single row is de-
picted in Figure 5. We define a constant interpola-
tion vector µ(gi, k) that is specific to the glyph box
width gi and glyph box column k. Each µ(gi, k)
is shaped according to a Gaussian centered at the
relative column position in φei. The glyph pixel
Bernoulli parameters are defined as follows:
</bodyText>
<equation confidence="0.9916715">
PIXEL
θ (j, k,gi; φei) =
[ i/
µ(gi, k)k� · [φei]jk�
</equation>
<bodyText confidence="0.999959076923077">
The fact that the parameterization is log-linear will
ensure that, during the unsupervised learning pro-
cess, updating the shape parameters φc is simple
and feasible.
By varying the magnitude of µ we can change
the level of smoothing in the logistic model and
cause it to permit areas that are over-inked. This is
the effect that di controls. By offsetting the rows
of φc that we interpolate weights from, we change
the vertical offset of the glyph, which is controlled
by vi. The full pixel generation process is dia-
grammed in Figure 4, where the dependence of
θPIXEL on di and vi is also represented.
</bodyText>
<sectionHeader confidence="0.994477" genericHeader="method">
4 Learning
</sectionHeader>
<bodyText confidence="0.996056">
We use the EM algorithm (Dempster et al., 1977)
to find the maximum-likelihood font parameters:
φc, θLPAD
c , θGLYPH
c , and θRPAD
c . The image X is the
only observed random variable in our model. The
identities of the characters E the typesetting lay-
out T and the inking R will all be unobserved. We
do not learn θINK and θVERT, which are set to the
uniform distribution.
</bodyText>
<subsectionHeader confidence="0.945167">
4.1 Expectation Maximization
</subsectionHeader>
<bodyText confidence="0.99596495">
During the E-step we compute expected counts
for E and T, but maximize over R, for which
we compute hard counts. Our model is an in-
stance of a hidden semi-Markov model (HSMM),
and therefore the computation of marginals is
tractable with the semi-Markov forward-backward
algorithm (Levinson, 1986).
During the M-step, we update the parame-
ters θLPAD
c , θRPAD cusing the standard closed-form
multinomial updates and use a specialized closed-
form update for θGLYPH that enforces unimodal-
c
ity of the glyph width distribution.3 The glyph
weights, φc, do not have a closed-form update.
The noise model that φc parameterizes is a lo-
cal log-linear model, so we follow the approach
of Berg-Kirkpatrick et al. (2010) and use L-BFGS
(Liu and Nocedal, 1989) to optimize the expected
likelihood with respect to φc.
</bodyText>
<subsectionHeader confidence="0.983853">
4.2 Coarse-to-Fine Learning and Inference
</subsectionHeader>
<bodyText confidence="0.99993352">
The number of states in the dynamic programming
lattice grows exponentially with the order of the
language model (Jelinek, 1998; Koehn, 2004). As
a result, inference can become slow when the lan-
guage model order n is large. To remedy this, we
take a coarse-to-fine approach to both learning and
inference. On each iteration of EM, we perform
two passes: a coarse pass using a low-order lan-
guage model, and a fine pass using a high-order
language model (Petrov et al., 2008; Zhang and
Gildea, 2008). We use the marginals4 from the
coarse pass to prune states from the dynamic pro-
gram of the fine pass.
In the early iterations of EM, our font parame-
ters are still inaccurate, and to prune heavily based
on such parameters would rule out correct anal-
yses. Therefore, we gradually increase the ag-
gressiveness of pruning over the course of EM. To
ensure that each iteration takes approximately the
same amount of computation, we also gradually
increase the order of the fine pass, only reaching
the full order n on the last iteration. To produce a
decoding of the image into text, on the final iter-
ation we run a Viterbi pass using the pruned fine
model.
</bodyText>
<footnote confidence="0.624194">
3We compute the weighted mean and weighted variance
of the glyph width expected counts. We set θGLYPH cto be pro-
portional to a discretized Gaussian with the computed mean
and variance. This update is approximate in the sense that it
does not necessarily find the unimodal multinomial that max-
imizes expected log-likelihood, but it works well in practice.
4In practice, we use max-marginals for pruning to ensure
that there is still a valid path in the pruned lattice.
</footnote>
<figure confidence="0.9952325">
µ
Glyph weights
0c
Interpolate, apply logistic
Bernoulli params
✓PIXEL
w
E
kI—I
�logistic
</figure>
<page confidence="0.553113">
211
</page>
<figure confidence="0.9945635">
(a) Old Bailey, 1725:
(c) Trove, 1823:
</figure>
<figureCaption confidence="0.998973">
Figure 6: Portions of several documents from our test set rep-
resenting a range of difficulties are displayed. On document
(a), which exhibits noisy typesetting, our system achieves a
word error rate (WER) of 25.2. Document (b) is cleaner in
comparison, and on it we achieve a WER of 15.4. On doc-
ument (c), which is also relatively clean, we achieve a WER
of 12.5. On document (d), which is severely degraded, we
achieve a WER of 70.0.
</figureCaption>
<sectionHeader confidence="0.993519" genericHeader="method">
5 Data
</sectionHeader>
<bodyText confidence="0.9999814">
We perform experiments on two historical datasets
consisting of images of documents printed be-
tween 1700 and 1900 in England and Australia.
Examples from both datasets are displayed in Fig-
ure 6.
</bodyText>
<subsectionHeader confidence="0.990525">
5.1 Old Bailey
</subsectionHeader>
<bodyText confidence="0.9999902">
The first dataset comes from a large set of im-
ages of the proceedings of the Old Bailey, a crimi-
nal court in London, England (Shoemaker, 2005).
The Old Bailey curatorial effort, after deciding
that current OCR systems do not adequately han-
dle 18th century fonts, manually transcribed the
documents into text. We will use these manual
transcriptions to evaluate the output of our system.
From the Old Bailey proceedings, we extracted a
set of 20 images, each consisting of 30 lines of
text to use as our first test set. We picked 20 doc-
uments, printed in consecutive decades. The first
document is from 1715 and the last is from 1905.
We choose the first document in each of the corre-
sponding years, choose a random page in the doc-
ument, and extracted an image of the first 30 con-
secutive lines of text consisting of full sentences.5
The ten documents in the Old Bailey dataset that
were printed before 1810 use the long s glyph,
while the remaining ten do not.
</bodyText>
<subsectionHeader confidence="0.968944">
5.2 Trove
</subsectionHeader>
<bodyText confidence="0.999991181818182">
Our second dataset is taken from a collection of
digitized Australian newspapers that were printed
between the years of 1803 and 1954. This col-
lection is called Trove, and is maintained by the
the National Library of Australia (Holley, 2010).
We extracted ten images from this collection in the
same way that we extracted images from Old Bai-
ley, but starting from the year 1803. We manually
produced our own gold annotations for these ten
images. Only the first document of Trove uses the
long s glyph.
</bodyText>
<subsectionHeader confidence="0.998945">
5.3 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999990789473684">
Many of the images in historical collections are
bitonal (binary) as a result of how they were cap-
tured on microfilm for storage in the 1980s (Arl-
itsch and Herbert, 2004). This is part of the reason
our model is designed to work directly with bi-
narized images. For consistency, we binarized the
images in our test sets that were not already binary
by thresholding pixel values.
Our model requires that the image be pre-
segmented into lines of text. We automatically
segment lines by training an HSMM over rows of
pixels. After the lines are segmented, each line
is resampled so that its vertical resolution is 30
pixels. The line extraction process also identifies
pixels that are not located in central text regions,
and are part of large connected components of ink,
spanning multiple lines. The values of such pixels
are treated as unobserved in the model since, more
often than not, they are part of ink blotches.
</bodyText>
<footnote confidence="0.90296325">
5This ruled out portions of the document with extreme
structural abnormalities, like title pages and lists. These
might be interesting to model, but are not within the scope
of this paper.
</footnote>
<figure confidence="0.99687525">
(b)
Old Bailey, 1875:
(d)
Trove, 1883:
</figure>
<page confidence="0.99572">
212
</page>
<sectionHeader confidence="0.997662" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999915333333333">
We evaluate our system by comparing our text
recognition accuracy to that of two state-of-the-art
systems.
</bodyText>
<subsectionHeader confidence="0.998172">
6.1 Baselines
</subsectionHeader>
<bodyText confidence="0.999980461538461">
Our first baseline is Google’s open source OCR
system, Tesseract (Smith, 2007). Tesseract takes
a pipelined approach to recognition. Before rec-
ognizing the text, the document is broken into
lines, and each line is segmented into words.
Then, Tesseract uses a classifier, aided by a word-
unigram language model, to recognize whole
words.
Our second baseline, ABBYY FineReader 11
Professional Edition,6 is a state-of-the-art com-
mercial OCR system. It is the OCR system that
the National Library of Australia used to recognize
the historical documents in Trove (Holley, 2010).
</bodyText>
<subsectionHeader confidence="0.992432">
6.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999976615384616">
We evaluate the output of our system and the base-
line systems using two metrics: character error
rate (CER) and word error rate (WER). Both these
metrics are based on edit distance. CER is the edit
distance between the predicted and gold transcrip-
tions of the document, divided by the number of
characters in the gold transcription. WER is the
word-level edit distance (words, instead of char-
acters, are treated as tokens) between predicted
and gold transcriptions, divided by the number of
words in the gold transcription. When computing
WER, text is tokenized into words by splitting on
whitespace.
</bodyText>
<subsectionHeader confidence="0.997601">
6.3 Language Model
</subsectionHeader>
<bodyText confidence="0.999953">
We ran experiments using two different language
models. The first language model was trained
on the initial one million sentences of the New
York Times (NYT) portion of the Gigaword cor-
pus (Graff et al., 2007), which contains about 36
million words. This language model is out of do-
main for our experimental documents. To inves-
tigate the effects of using an in domain language
model, we created a corpus composed of the man-
ual annotations of all the documents in the Old
Bailey proceedings, excluding those used in our
test set. This corpus consists of approximately 32
million words. In all experiments we used a char-
acter n-gram order of six for the final Viterbi de-
</bodyText>
<footnote confidence="0.992446">
6http://www.abbyy.com
</footnote>
<table confidence="0.9986102">
System CER WER
Old Bailey
Google Tesseract 29.6 54.8
ABBYY FineReader 15.1 40.0
Ocular w/ NYT (this work) 12.6 28.1
Ocular w/ OB (this work) 9.7 24.1
Trove
Google Tesseract 37.5 59.3
ABBYY FineReader 22.9 49.2
Ocular w/ NYT (this work) 14.9 33.0
</table>
<tableCaption confidence="0.999775">
Table 1: We evaluate the predicted transcriptions in terms of
</tableCaption>
<bodyText confidence="0.650603111111111">
both character error rate (CER) and word error rate (WER),
and report macro-averages across documents. We compare
with two baseline systems: Google’s open source OCR sys-
tem, Tessearact, and a state-of-the-art commercial system,
ABBYY FineReader. We refer to our system as Ocular w/
NYT and Ocular w/ OB, depending on whether NYT or Old
Bailey is used to train the language model.
coding pass and an order of three for all coarse
passes.
</bodyText>
<subsectionHeader confidence="0.995569">
6.4 Initialization and Tuning
</subsectionHeader>
<bodyText confidence="0.974916705882353">
We used as a development set ten additional docu-
ments from the Old Bailey proceedings and five
additional documents from Trove that were not
part of our test set. On this data, we tuned the
model’s hyperparameters7 and the parameters of
the pruning schedule for our coarse-to-fine ap-
proach.
In experiments we initialized θRPAD
c
be uniform, and initialized θGLYPH and φc based
c
on the standard modern fonts included with the
Ubuntu Linux 12.04 distribution.8 For documents
that use the long s glyph, we introduce a special
character type for the non-word-final s, and ini-
tialize its parameters from a mixture of the modern
f and  |glyphs.9
</bodyText>
<sectionHeader confidence="0.996545" genericHeader="evaluation">
7 Results and Analysis
</sectionHeader>
<bodyText confidence="0.9954148">
The results of our experiments are summarized in
Table 1. We refer to our system as Ocular w/
NYT or Ocular w/ OB, depending on whether the
language model was trained using NYT or Old
Bailey, respectively. We compute macro-averages
</bodyText>
<footnote confidence="0.913426">
7One of the hyperparameters we tune is the exponent of
the language model. This balances the contributions of the
language model and the typesetting model to the posterior
(Och and Ney, 2004).
8http://www.ubuntu.com/
9Following Berg-Kirkpatrick et al. (2010), we use a reg-
ularization term in the optimization of the log-linear model
parameters φ. during the M-step. Instead of regularizing to-
wards zero, we regularize towards the initializer. This slightly
improves performance on our development set and can be
thought of as placing a prior on the glyph shape parameters.
and θLPAD
c to
</footnote>
<page confidence="0.97911">
213
</page>
<figure confidence="0.992763222222222">
(a) Old Bailey, 1775: Predicted text: the prisoner at the bar. Jacob Lazarus and his
Predicted typesetting:
Image:
(b) Old Bailey, 1885: Predicted text: taken ill and taken away – I remember
Predicted typesetting:
Image:
(c) Trove, 1883: Predicted text: how the murderers came to learn the nation in
Predicted typesetting:
Image:
</figure>
<figureCaption confidence="0.666049333333333">
Figure 7: For each of these portions of test documents, the first line shows the transcription predicted by our model and the
second line shows a representation of the learned typesetting layout. The grayscale glyphs show the Bernoulli pixel distributions
learned by our model, while the padding regions are depicted in blue. The third line shows the input image.
</figureCaption>
<bodyText confidence="0.99988778125">
across documents from all years. Our system, us-
ing the NYT language model, achieves an average
WER of 28.1 on Old Bailey and an average WER
of 33.0 on Trove. This represents a substantial er-
ror reduction compared to both baseline systems.
If we average over the documents in both Old
Bailey and Trove, we find that Tesseract achieved
an average WER of 56.3, ABBYY FineReader
achieved an average WER of 43.1, and our system,
using the NYT language model, achieved an aver-
age WER of 29.7. This means that while Tesseract
incorrectly predicts more than half of the words in
these documents, our system gets more than three-
quarters of them right. Overall, we achieve a rela-
tive reduction in WER of 47% compared to Tesser-
act and 31% compared to ABBYY FineReader.
The baseline systems do not have special pro-
visions for the long s glyph. In order to make
sure the comparison is fair, we separately com-
puted average WER on only the documents from
after 1810 (which do no use the long s glyph). We
found that using this evaluation our system actu-
ally acheives a larger relative reduction in WER:
50% compared to Tesseract and 35% compared to
ABBYY FineReader.
Finally, if we train the language model using
the Old Bailey corpus instead of the NYT corpus,
we see an average improvement of 4 WER on the
Old Bailey test set. This means that the domain of
the language model is important, but, the results
are not affected drastically even when using a lan-
guage model based on modern corpora (NYT).
</bodyText>
<subsectionHeader confidence="0.91464">
7.1 Learned Typesetting Layout
</subsectionHeader>
<bodyText confidence="0.6092615">
Figure 7 shows a representation of the typesetting
layout learned by our model for portions of several
</bodyText>
<figureCaption confidence="0.89797875">
Figure 8: The central glyph is a representation of the initial
model parameters for the glyph shape for g, and surrounding
this are the learned parameters for documents from various
years.
</figureCaption>
<bodyText confidence="0.999940714285714">
test documents. For each portion of a test doc-
ument, the first line shows the transcription pre-
dicted by our model, and the second line shows
padding and glyph regions predicted by the model,
where the grayscale glyphs represent the learned
Bernoulli parameters for each pixel. The third line
shows the input image.
Figure 7a demonstrates a case where our model
has effectively explained both the uneven baseline
and over-inked glyphs by using the vertical offsets
vi and inking variables di. In Figure 7b the model
has used glyph widths gi and vertical offsets to ex-
plain the thinning of glyphs and falling baseline
that occurred near the binding of the book. In sep-
arate experiments on the Old Bailey test set, using
the NYT language model, we found that remov-
ing the vertical offset variables from the model in-
creased WER by 22, and removing the inking vari-
ables increased WER by 16. This indicates that it
is very important to model both these aspects of
printing press rendering.
</bodyText>
<figure confidence="0.992894571428572">
1780
1820
1740
1860
1900
1700
Initializer
</figure>
<page confidence="0.927539">
214
</page>
<figureCaption confidence="0.998416">
Figure 9: This Old Bailey document from 1719 has severe ink bleeding from the facing page. We annotated these blotches (in
red) and treated the corresponding pixels as unobserved in the model. The layout shown is predicted by the model.
</figureCaption>
<bodyText confidence="0.9834102">
Figure 7c shows the output of our system on
a difficult document. Here, missing characters
and ink blotches confuse the model, which picks
something that is reasonable according to the lan-
guage model, but incorrect.
</bodyText>
<subsectionHeader confidence="0.986062">
7.2 Learned Fonts
</subsectionHeader>
<bodyText confidence="0.999973722222222">
It is interesting to look at the fonts learned by our
system, and track how historical fonts changed
over time. Figure 8 shows several grayscale im-
ages representing the Bernoulli pixel probabilities
for the most likely width of the glyph for g under
various conditions. At the center is the representa-
tion of the initial parameter values, and surround-
ing this are the learned parameters for documents
from various years. The learned shapes are visibly
different from the initializer, which is essentially
an average of modern fonts, and also vary across
decades.
We can ask to what extent learning the font
structure actually improved our performance. If
we turn off learning and just use the initial pa-
rameters to decode, WER increases by 8 on the
Old Bailey test set when using the NYT language
model.
</bodyText>
<subsectionHeader confidence="0.989787">
7.3 Unobserved Ink Blotches
</subsectionHeader>
<bodyText confidence="0.999977">
As noted earlier, one strength of our generative
model is that we can make the values of certain
pixels unobserved in the model, and let inference
fill them in. We conducted an additional experi-
ment on a document from the Old Bailey proceed-
ings that was printed in 1719. This document, a
fragment of which is shown in Figure 9, has se-
vere ink bleeding from the facing page. We manu-
ally annotated the ink blotches (shown in red), and
made them unobserved in the model. The result-
ing typesetting layout learned by the model is also
shown in Figure 9. The model correctly predicted
most of the obscured words. Running the model
with the manually specified unobserved pixels re-
duced the WER on this document from 58 to 19
when using the NYT language model.
</bodyText>
<subsectionHeader confidence="0.994615">
7.4 Remaining Errors
</subsectionHeader>
<bodyText confidence="0.999972208333333">
We performed error analysis on our development
set by randomly choosing 100 word errors from
the WER alignment and manually annotating them
with relevant features. Specifically, for each word
error we recorded whether or not the error con-
tained punctuation (either in the predicted word or
the gold word), whether the text in the correspond-
ing portion of the original image was italicized,
and whether the corresponding portion of the im-
age exhibited over-inking, missing ink, or signif-
icant ink blotches. These last three feature types
are subjective in nature but may still be informa-
tive. We found that 56% of errors were accompa-
nied by over-inking, 50% of errors were accom-
panied by ink blotches, 42% of errors contained
punctuation, 21% of errors showed missing ink,
and 12% of errors contained text that was itali-
cized in the original image.
Our own subjective assessment indicates that
many of these error features are in fact causal.
More often than not, italicized text is incorrectly
transcribed. In cases of extreme ink blotching,
or large areas of missing ink, the system usually
makes an error.
</bodyText>
<sectionHeader confidence="0.997497" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999854444444444">
We have demonstrated a model, based on the his-
torical typesetting process, that effectively learns
font structure in an unsupervised fashion to im-
prove transcription of historical documents into
text. The parameters of the learned fonts are inter-
pretable, as are the predicted typesetting layouts.
Our system achieves state-of-the-art results, sig-
nificantly outperforming two state-of-the-art base-
line systems.
</bodyText>
<page confidence="0.99848">
215
</page>
<sectionHeader confidence="0.983782" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999852961904762">
Kenning Arlitsch and John Herbert. 2004. Microfilm,
paper, and OCR: Issues in newspaper digitization.
the Utah digital newspapers program. Microform &amp;
Imaging Review.
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple
effective decipherment via combinatorial optimiza-
tion. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings
of the 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies:.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword third edi-
tion. Linguistic Data Consortium, Catalog Number
LDC2007T07.
Tin Kam Ho and George Nagy. 2000. OCR with no
shape training. In Proceedings of the 15th Interna-
tional Conference on Pattern Recognition.
Rose Holley. 2010. Trove: Innovation in access to
information in Australia. Ariadne.
Gary Huang, Erik G Learned-Miller, and Andrew Mc-
Callum. 2006. Cryptogram decoding for optical
character recognition. University of Massachusetts-
Amherst Technical Report.
Fred Jelinek. 1998. Statistical methods for speech
recognition. MIT press.
Andrew Kae and Erik Learned-Miller. 2009. Learn-
ing on the fly: font-free approaches to difficult OCR
problems. In Proceedings of the 2009 International
Conference on Document Analysis and Recognition.
Andrew Kae, Gary Huang, Carl Doersch, and Erik
Learned-Miller. 2010. Improving state-of-the-
art OCR through high-precision document-specific
modeling. In Proceedings of the 2010 IEEE Confer-
ence on Computer Vision and Pattern Recognition.
Vladimir Kluzner, Asaf Tzadok, Yuval Shimony, Eu-
gene Walach, and Apostolos Antonacopoulos. 2009.
Word-based adaptive OCR for historical books. In
Proceedings of the 2009 International Conference
on on Document Analysis and Recognition.
Vladimir Kluzner, Asaf Tzadok, Dan Chevion, and Eu-
gene Walach. 2011. Hybrid approach to adaptive
OCR for historical books. In Proceedings of the
2011 International Conference on Document Anal-
ysis and Recognition.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. Machine translation: From real users
to research.
Okan Kolak, William Byrne, and Philip Resnik. 2003.
A generative probabilistic OCR model for NLP ap-
plications. In Proceedings of the 2003 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Gary Kopec and Mauricio Lomelin. 1996. Document-
specific character template estimation. In Proceed-
ings of the International Society for Optics and Pho-
tonics.
Gary Kopec, Maya Said, and Kris Popat. 2001. N-
gram language models for document image decod-
ing. In Proceedings of Society of Photographic In-
strumentation Engineers.
Stephen Levinson. 1986. Continuously variable du-
ration hidden Markov models for automatic speech
recognition. Computer Speech &amp; Language.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical programming.
Franz Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing.
Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing.
Sujith Ravi and Kevin Knight. 2011. Bayesian infer-
ence for Zodiac and other homophonic ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
Robert Shoemaker. 2005. Digital London: Creating a
searchable web of interlinked sources on eighteenth
century London. Electronic Library and Informa-
tion Systems.
Ray Smith. 2007. An overview of the tesseract ocr
engine. In Proceedings of the Ninth International
Conference on Document Analysis and Recognition.
</reference>
<page confidence="0.985985">
216
</page>
<reference confidence="0.999194357142857">
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics.
Georgios Vamvakas, Basilios Gatos, Nikolaos Stam-
atopoulos, and Stavros Perantonis. 2008. A com-
plete optical character recognition methodology for
historical documents. In The Eighth IAPR Interna-
tional Workshop on Document Analysis Systems.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing.
</reference>
<page confidence="0.998382">
217
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.656701">
<title confidence="0.999525">Unsupervised Transcription of Historical Documents</title>
<author confidence="0.999716">Taylor Berg-Kirkpatrick Greg Durrett Dan</author>
<affiliation confidence="0.992152">Computer Science University of California at</affiliation>
<abstract confidence="0.979961294117647">We present a generative probabilistic model, inspired by historical printing processes, for transcribing images of documents from the printing press era. By jointly modeling the text of the document and the noisy (but regular) process of rendering glyphs, our unsupervised system is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially outperforms state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesseract, Google’s open source OCR system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenning Arlitsch</author>
<author>John Herbert</author>
</authors>
<title>Microfilm, paper, and OCR: Issues in newspaper digitization. the Utah digital newspapers program.</title>
<date>2004</date>
<journal>Microform &amp; Imaging Review.</journal>
<contexts>
<context position="1184" citStr="Arlitsch and Herbert, 2004" startWordPosition="171" endWordPosition="174">ges into text. Overall, our system substantially outperforms state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesseract, Google’s open source OCR system. 1 Introduction Standard techniques for transcribing modern documents do not work well on historical ones. For example, even state-of-the-art OCR systems produce word error rates of over 50% on the documents shown in Figure 1. Unsurprisingly, such error rates are too high for many research projects (Arlitsch and Herbert, 2004; Shoemaker, 2005; Holley, 2010). We present a new, generative model specialized to transcribing printing-press era documents. Our model is inspired by the underlying printing processes and is designed to capture the primary sources of variation and noise. One key challenge is that the fonts used in historical documents are not standard (Shoemaker, 2005). For example, consider Figure 1a. The fonts are not irregular like handwriting – each occurrence of a given character type, e.g. a, will use the same underlying glyph. However, the exact glyphs are unknown. Some differences between fonts are m</context>
<context position="21221" citStr="Arlitsch and Herbert, 2004" startWordPosition="3646" endWordPosition="3650">lian newspapers that were printed between the years of 1803 and 1954. This collection is called Trove, and is maintained by the the National Library of Australia (Holley, 2010). We extracted ten images from this collection in the same way that we extracted images from Old Bailey, but starting from the year 1803. We manually produced our own gold annotations for these ten images. Only the first document of Trove uses the long s glyph. 5.3 Pre-processing Many of the images in historical collections are bitonal (binary) as a result of how they were captured on microfilm for storage in the 1980s (Arlitsch and Herbert, 2004). This is part of the reason our model is designed to work directly with binarized images. For consistency, we binarized the images in our test sets that were not already binary by thresholding pixel values. Our model requires that the image be presegmented into lines of text. We automatically segment lines by training an HSMM over rows of pixels. After the lines are segmented, each line is resampled so that its vertical resolution is 30 pixels. The line extraction process also identifies pixels that are not located in central text regions, and are part of large connected components of ink, sp</context>
</contexts>
<marker>Arlitsch, Herbert, 2004</marker>
<rawString>Kenning Arlitsch and John Herbert. 2004. Microfilm, paper, and OCR: Issues in newspaper digitization. the Utah digital newspapers program. Microform &amp; Imaging Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Simple effective decipherment via combinatorial optimization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5237" citStr="Berg-Kirkpatrick and Klein, 2011" startWordPosition="821" endWordPosition="824"> fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typically deals only with discrete symbols. 3 Model Most historical documents have unkn</context>
</contexts>
<marker>Berg-Kirkpatrick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple effective decipherment via combinatorial optimization. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies:.</booktitle>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Proceedings of the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies:.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Dempster</author>
<author>Nan Laird</author>
<author>Donald Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society.</journal>
<contexts>
<context position="15955" citStr="Dempster et al., 1977" startWordPosition="2731" endWordPosition="2734">log-linear will ensure that, during the unsupervised learning process, updating the shape parameters φc is simple and feasible. By varying the magnitude of µ we can change the level of smoothing in the logistic model and cause it to permit areas that are over-inked. This is the effect that di controls. By offsetting the rows of φc that we interpolate weights from, we change the vertical offset of the glyph, which is controlled by vi. The full pixel generation process is diagrammed in Figure 4, where the dependence of θPIXEL on di and vi is also represented. 4 Learning We use the EM algorithm (Dempster et al., 1977) to find the maximum-likelihood font parameters: φc, θLPAD c , θGLYPH c , and θRPAD c . The image X is the only observed random variable in our model. The identities of the characters E the typesetting layout T and the inking R will all be unobserved. We do not learn θINK and θVERT, which are set to the uniform distribution. 4.1 Expectation Maximization During the E-step we compute expected counts for E and T, but maximize over R, for which we compute hard counts. Our model is an instance of a hidden semi-Markov model (HSMM), and therefore the computation of marginals is tractable with the sem</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur Dempster, Nan Laird, and Donald Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword third edition. Linguistic Data Consortium, Catalog Number LDC2007T07.</title>
<date>2007</date>
<contexts>
<context position="23752" citStr="Graff et al., 2007" startWordPosition="4063" endWordPosition="4066">ween the predicted and gold transcriptions of the document, divided by the number of characters in the gold transcription. WER is the word-level edit distance (words, instead of characters, are treated as tokens) between predicted and gold transcriptions, divided by the number of words in the gold transcription. When computing WER, text is tokenized into words by splitting on whitespace. 6.3 Language Model We ran experiments using two different language models. The first language model was trained on the initial one million sentences of the New York Times (NYT) portion of the Gigaword corpus (Graff et al., 2007), which contains about 36 million words. This language model is out of domain for our experimental documents. To investigate the effects of using an in domain language model, we created a corpus composed of the manual annotations of all the documents in the Old Bailey proceedings, excluding those used in our test set. This corpus consists of approximately 32 million words. In all experiments we used a character n-gram order of six for the final Viterbi de6http://www.abbyy.com System CER WER Old Bailey Google Tesseract 29.6 54.8 ABBYY FineReader 15.1 40.0 Ocular w/ NYT (this work) 12.6 28.1 Ocu</context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2007</marker>
<rawString>David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2007. English Gigaword third edition. Linguistic Data Consortium, Catalog Number LDC2007T07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tin Kam Ho</author>
<author>George Nagy</author>
</authors>
<title>OCR with no shape training.</title>
<date>2000</date>
<booktitle>In Proceedings of the 15th International Conference on Pattern Recognition.</booktitle>
<contexts>
<context position="4019" citStr="Ho and Nagy, 2000" startWordPosition="620" endWordPosition="623"> 31% relative error reduction over ABBYY’s commercial FineReader system, which has been used in large-scale historical transcription projects (Holley, 2010). 207 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 207–217, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 2: An example image from a historical document (X) and its transcription (E). 2 Related Work Relatively little prior work has built models specifically for transcribing historical documents. Some of the challenges involved have been addressed (Ho and Nagy, 2000; Huang et al., 2006; Kae and Learned-Miller, 2009), but not in a way targeted to documents from the printing press era. For example, some approaches have learned fonts in an unsupervised fashion but require pre-segmentation of the image into character or word regions (Ho and Nagy, 2000; Huang et al., 2006), which is not feasible for noisy historical documents. Kae and Learned-Miller (2009) jointly learn the font and image segmentation but do not outperform modern baselines. Work that has directly addressed historical documents has done so using a pipelined approach, and without fully integrat</context>
</contexts>
<marker>Ho, Nagy, 2000</marker>
<rawString>Tin Kam Ho and George Nagy. 2000. OCR with no shape training. In Proceedings of the 15th International Conference on Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rose Holley</author>
</authors>
<title>Trove: Innovation in access to information in</title>
<date>2010</date>
<journal>Australia. Ariadne.</journal>
<contexts>
<context position="1216" citStr="Holley, 2010" startWordPosition="177" endWordPosition="178">lly outperforms state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesseract, Google’s open source OCR system. 1 Introduction Standard techniques for transcribing modern documents do not work well on historical ones. For example, even state-of-the-art OCR systems produce word error rates of over 50% on the documents shown in Figure 1. Unsurprisingly, such error rates are too high for many research projects (Arlitsch and Herbert, 2004; Shoemaker, 2005; Holley, 2010). We present a new, generative model specialized to transcribing printing-press era documents. Our model is inspired by the underlying printing processes and is designed to capture the primary sources of variation and noise. One key challenge is that the fonts used in historical documents are not standard (Shoemaker, 2005). For example, consider Figure 1a. The fonts are not irregular like handwriting – each occurrence of a given character type, e.g. a, will use the same underlying glyph. However, the exact glyphs are unknown. Some differences between fonts are minor, reflecting small variation</context>
<context position="3558" citStr="Holley, 2010" startWordPosition="555" endWordPosition="556">noise. Because the model is generative, we can also treat areas that are obscured by larger ink blotches as unobserved, and let the model predict the obscured text based on visual and linguistic context. Our system, which we call Ocular, operates by fitting the model to each document in an unsupervised fashion. The system outperforms state-ofthe-art baselines, giving a 47% relative error reduction over Google’s open source Tesseract system, and giving a 31% relative error reduction over ABBYY’s commercial FineReader system, which has been used in large-scale historical transcription projects (Holley, 2010). 207 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 207–217, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 2: An example image from a historical document (X) and its transcription (E). 2 Related Work Relatively little prior work has built models specifically for transcribing historical documents. Some of the challenges involved have been addressed (Ho and Nagy, 2000; Huang et al., 2006; Kae and Learned-Miller, 2009), but not in a way targeted to documents from the printing press era. For example, some </context>
<context position="20770" citStr="Holley, 2010" startWordPosition="3569" endWordPosition="3570">om 1715 and the last is from 1905. We choose the first document in each of the corresponding years, choose a random page in the document, and extracted an image of the first 30 consecutive lines of text consisting of full sentences.5 The ten documents in the Old Bailey dataset that were printed before 1810 use the long s glyph, while the remaining ten do not. 5.2 Trove Our second dataset is taken from a collection of digitized Australian newspapers that were printed between the years of 1803 and 1954. This collection is called Trove, and is maintained by the the National Library of Australia (Holley, 2010). We extracted ten images from this collection in the same way that we extracted images from Old Bailey, but starting from the year 1803. We manually produced our own gold annotations for these ten images. Only the first document of Trove uses the long s glyph. 5.3 Pre-processing Many of the images in historical collections are bitonal (binary) as a result of how they were captured on microfilm for storage in the 1980s (Arlitsch and Herbert, 2004). This is part of the reason our model is designed to work directly with binarized images. For consistency, we binarized the images in our test sets </context>
<context position="22906" citStr="Holley, 2010" startWordPosition="3925" endWordPosition="3926">o state-of-the-art systems. 6.1 Baselines Our first baseline is Google’s open source OCR system, Tesseract (Smith, 2007). Tesseract takes a pipelined approach to recognition. Before recognizing the text, the document is broken into lines, and each line is segmented into words. Then, Tesseract uses a classifier, aided by a wordunigram language model, to recognize whole words. Our second baseline, ABBYY FineReader 11 Professional Edition,6 is a state-of-the-art commercial OCR system. It is the OCR system that the National Library of Australia used to recognize the historical documents in Trove (Holley, 2010). 6.2 Evaluation We evaluate the output of our system and the baseline systems using two metrics: character error rate (CER) and word error rate (WER). Both these metrics are based on edit distance. CER is the edit distance between the predicted and gold transcriptions of the document, divided by the number of characters in the gold transcription. WER is the word-level edit distance (words, instead of characters, are treated as tokens) between predicted and gold transcriptions, divided by the number of words in the gold transcription. When computing WER, text is tokenized into words by splitti</context>
</contexts>
<marker>Holley, 2010</marker>
<rawString>Rose Holley. 2010. Trove: Innovation in access to information in Australia. Ariadne.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary Huang</author>
<author>Erik G Learned-Miller</author>
<author>Andrew McCallum</author>
</authors>
<title>Cryptogram decoding for optical character recognition.</title>
<date>2006</date>
<tech>Technical Report.</tech>
<institution>University of MassachusettsAmherst</institution>
<contexts>
<context position="4039" citStr="Huang et al., 2006" startWordPosition="624" endWordPosition="627"> reduction over ABBYY’s commercial FineReader system, which has been used in large-scale historical transcription projects (Holley, 2010). 207 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 207–217, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 2: An example image from a historical document (X) and its transcription (E). 2 Related Work Relatively little prior work has built models specifically for transcribing historical documents. Some of the challenges involved have been addressed (Ho and Nagy, 2000; Huang et al., 2006; Kae and Learned-Miller, 2009), but not in a way targeted to documents from the printing press era. For example, some approaches have learned fonts in an unsupervised fashion but require pre-segmentation of the image into character or word regions (Ho and Nagy, 2000; Huang et al., 2006), which is not feasible for noisy historical documents. Kae and Learned-Miller (2009) jointly learn the font and image segmentation but do not outperform modern baselines. Work that has directly addressed historical documents has done so using a pipelined approach, and without fully integrating a strong languag</context>
</contexts>
<marker>Huang, Learned-Miller, McCallum, 2006</marker>
<rawString>Gary Huang, Erik G Learned-Miller, and Andrew McCallum. 2006. Cryptogram decoding for optical character recognition. University of MassachusettsAmherst Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Jelinek</author>
</authors>
<title>Statistical methods for speech recognition.</title>
<date>1998</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="17282" citStr="Jelinek, 1998" startWordPosition="2956" endWordPosition="2957">cusing the standard closed-form multinomial updates and use a specialized closedform update for θGLYPH that enforces unimodalc ity of the glyph width distribution.3 The glyph weights, φc, do not have a closed-form update. The noise model that φc parameterizes is a local log-linear model, so we follow the approach of Berg-Kirkpatrick et al. (2010) and use L-BFGS (Liu and Nocedal, 1989) to optimize the expected likelihood with respect to φc. 4.2 Coarse-to-Fine Learning and Inference The number of states in the dynamic programming lattice grows exponentially with the order of the language model (Jelinek, 1998; Koehn, 2004). As a result, inference can become slow when the language model order n is large. To remedy this, we take a coarse-to-fine approach to both learning and inference. On each iteration of EM, we perform two passes: a coarse pass using a low-order language model, and a fine pass using a high-order language model (Petrov et al., 2008; Zhang and Gildea, 2008). We use the marginals4 from the coarse pass to prune states from the dynamic program of the fine pass. In the early iterations of EM, our font parameters are still inaccurate, and to prune heavily based on such parameters would r</context>
</contexts>
<marker>Jelinek, 1998</marker>
<rawString>Fred Jelinek. 1998. Statistical methods for speech recognition. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kae</author>
<author>Erik Learned-Miller</author>
</authors>
<title>Learning on the fly: font-free approaches to difficult OCR problems.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 International Conference on Document Analysis and Recognition.</booktitle>
<contexts>
<context position="4070" citStr="Kae and Learned-Miller, 2009" startWordPosition="628" endWordPosition="631">Y’s commercial FineReader system, which has been used in large-scale historical transcription projects (Holley, 2010). 207 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 207–217, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 2: An example image from a historical document (X) and its transcription (E). 2 Related Work Relatively little prior work has built models specifically for transcribing historical documents. Some of the challenges involved have been addressed (Ho and Nagy, 2000; Huang et al., 2006; Kae and Learned-Miller, 2009), but not in a way targeted to documents from the printing press era. For example, some approaches have learned fonts in an unsupervised fashion but require pre-segmentation of the image into character or word regions (Ho and Nagy, 2000; Huang et al., 2006), which is not feasible for noisy historical documents. Kae and Learned-Miller (2009) jointly learn the font and image segmentation but do not outperform modern baselines. Work that has directly addressed historical documents has done so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008;</context>
</contexts>
<marker>Kae, Learned-Miller, 2009</marker>
<rawString>Andrew Kae and Erik Learned-Miller. 2009. Learning on the fly: font-free approaches to difficult OCR problems. In Proceedings of the 2009 International Conference on Document Analysis and Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kae</author>
<author>Gary Huang</author>
<author>Carl Doersch</author>
<author>Erik Learned-Miller</author>
</authors>
<title>Improving state-of-theart OCR through high-precision document-specific modeling.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 IEEE Conference on Computer Vision and Pattern Recognition.</booktitle>
<contexts>
<context position="4709" citStr="Kae et al., 2010" startWordPosition="733" endWordPosition="736">eted to documents from the printing press era. For example, some approaches have learned fonts in an unsupervised fashion but require pre-segmentation of the image into character or word regions (Ho and Nagy, 2000; Huang et al., 2006), which is not feasible for noisy historical documents. Kae and Learned-Miller (2009) jointly learn the font and image segmentation but do not outperform modern baselines. Work that has directly addressed historical documents has done so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a</context>
</contexts>
<marker>Kae, Huang, Doersch, Learned-Miller, 2010</marker>
<rawString>Andrew Kae, Gary Huang, Carl Doersch, and Erik Learned-Miller. 2010. Improving state-of-theart OCR through high-precision document-specific modeling. In Proceedings of the 2010 IEEE Conference on Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Kluzner</author>
</authors>
<title>Asaf Tzadok, Yuval Shimony, Eugene Walach, and Apostolos Antonacopoulos.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 International Conference on on Document Analysis and Recognition.</booktitle>
<marker>Kluzner, 2009</marker>
<rawString>Vladimir Kluzner, Asaf Tzadok, Yuval Shimony, Eugene Walach, and Apostolos Antonacopoulos. 2009. Word-based adaptive OCR for historical books. In Proceedings of the 2009 International Conference on on Document Analysis and Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Kluzner</author>
<author>Asaf Tzadok</author>
<author>Dan Chevion</author>
<author>Eugene Walach</author>
</authors>
<title>Hybrid approach to adaptive OCR for historical books.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 International Conference on Document Analysis and Recognition.</booktitle>
<contexts>
<context position="4732" citStr="Kluzner et al., 2011" startWordPosition="737" endWordPosition="740">from the printing press era. For example, some approaches have learned fonts in an unsupervised fashion but require pre-segmentation of the image into character or word regions (Ho and Nagy, 2000; Huang et al., 2006), which is not feasible for noisy historical documents. Kae and Learned-Miller (2009) jointly learn the font and image segmentation but do not outperform modern baselines. Work that has directly addressed historical documents has done so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, a</context>
</contexts>
<marker>Kluzner, Tzadok, Chevion, Walach, 2011</marker>
<rawString>Vladimir Kluzner, Asaf Tzadok, Dan Chevion, and Eugene Walach. 2011. Hybrid approach to adaptive OCR for historical books. In Proceedings of the 2011 International Conference on Document Analysis and Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="7091" citStr="Kneser and Ney, 1995" startWordPosition="1140" endWordPosition="1143">e glyphs in the image, and a random variable R that specifies aspects of the inking and rendering process. The joint distribution is: P(E, T, R, X) = P(E) [Language model] · P(T|E) [Typesetting model] · P(R) [Inking model] · P(X|E, T, R) [Noise model] We let capital letters denote vectors of concatenated random variables, and we denote the individual random variables with lower-case letters. For example, E represents the entire sequence of text, while ei represents ith character in the sequence. 3.1 Language Model P(E) Our language model, P(E), is a Kneser-Ney smoothed character n-gram model (Kneser and Ney, 1995). We generate printed lines of text (rather than sentences) independently, without generating an explicit stop character. This means that, formally, the model must separately generate the character length of each line. We choose not to bias the model towards longer or shorter character sequences and let the line length m be drawn uniformly at random from the positive integers less than some large constant M.1 When i &lt; 1, let ei denote a line-initial null character. We can now write: m P(E) = P(m) · P(ei|ei−1, ... , ei−n) i=1 1In particular, we do not use the kind of “word bonus” common to stat</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models. Machine translation: From real users to research.</title>
<date>2004</date>
<contexts>
<context position="17296" citStr="Koehn, 2004" startWordPosition="2958" endWordPosition="2959">dard closed-form multinomial updates and use a specialized closedform update for θGLYPH that enforces unimodalc ity of the glyph width distribution.3 The glyph weights, φc, do not have a closed-form update. The noise model that φc parameterizes is a local log-linear model, so we follow the approach of Berg-Kirkpatrick et al. (2010) and use L-BFGS (Liu and Nocedal, 1989) to optimize the expected likelihood with respect to φc. 4.2 Coarse-to-Fine Learning and Inference The number of states in the dynamic programming lattice grows exponentially with the order of the language model (Jelinek, 1998; Koehn, 2004). As a result, inference can become slow when the language model order n is large. To remedy this, we take a coarse-to-fine approach to both learning and inference. On each iteration of EM, we perform two passes: a coarse pass using a low-order language model, and a fine pass using a high-order language model (Petrov et al., 2008; Zhang and Gildea, 2008). We use the marginals4 from the coarse pass to prune states from the dynamic program of the fine pass. In the early iterations of EM, our font parameters are still inaccurate, and to prune heavily based on such parameters would rule out correc</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. Machine translation: From real users to research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okan Kolak</author>
<author>William Byrne</author>
<author>Philip Resnik</author>
</authors>
<title>A generative probabilistic OCR model for NLP applications.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of</booktitle>
<contexts>
<context position="5033" citStr="Kolak et al., 2003" startWordPosition="787" endWordPosition="790">ointly learn the font and image segmentation but do not outperform modern baselines. Work that has directly addressed historical documents has done so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and i</context>
</contexts>
<marker>Kolak, Byrne, Resnik, 2003</marker>
<rawString>Okan Kolak, William Byrne, and Philip Resnik. 2003. A generative probabilistic OCR model for NLP applications. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary Kopec</author>
<author>Mauricio Lomelin</author>
</authors>
<title>Documentspecific character template estimation.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Society for Optics and Photonics.</booktitle>
<contexts>
<context position="4794" citStr="Kopec and Lomelin (1996)" startWordPosition="748" endWordPosition="751">ave learned fonts in an unsupervised fashion but require pre-segmentation of the image into character or word regions (Ho and Nagy, 2000; Huang et al., 2006), which is not feasible for noisy historical documents. Kae and Learned-Miller (2009) jointly learn the font and image segmentation but do not outperform modern baselines. Work that has directly addressed historical documents has done so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and lette</context>
</contexts>
<marker>Kopec, Lomelin, 1996</marker>
<rawString>Gary Kopec and Mauricio Lomelin. 1996. Documentspecific character template estimation. In Proceedings of the International Society for Optics and Photonics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary Kopec</author>
<author>Maya Said</author>
<author>Kris Popat</author>
</authors>
<title>Ngram language models for document image decoding.</title>
<date>2001</date>
<booktitle>In Proceedings of Society of Photographic Instrumentation Engineers.</booktitle>
<contexts>
<context position="4818" citStr="Kopec et al. (2001)" startWordPosition="753" endWordPosition="756">ervised fashion but require pre-segmentation of the image into character or word regions (Ho and Nagy, 2000; Huang et al., 2006), which is not feasible for noisy historical documents. Kae and Learned-Miller (2009) jointly learn the font and image segmentation but do not outperform modern baselines. Work that has directly addressed historical documents has done so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also</context>
</contexts>
<marker>Kopec, Said, Popat, 2001</marker>
<rawString>Gary Kopec, Maya Said, and Kris Popat. 2001. Ngram language models for document image decoding. In Proceedings of Society of Photographic Instrumentation Engineers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Levinson</author>
</authors>
<title>Continuously variable duration hidden Markov models for automatic speech recognition.</title>
<date>1986</date>
<journal>Computer Speech &amp; Language.</journal>
<contexts>
<context position="16607" citStr="Levinson, 1986" startWordPosition="2846" endWordPosition="2847">parameters: φc, θLPAD c , θGLYPH c , and θRPAD c . The image X is the only observed random variable in our model. The identities of the characters E the typesetting layout T and the inking R will all be unobserved. We do not learn θINK and θVERT, which are set to the uniform distribution. 4.1 Expectation Maximization During the E-step we compute expected counts for E and T, but maximize over R, for which we compute hard counts. Our model is an instance of a hidden semi-Markov model (HSMM), and therefore the computation of marginals is tractable with the semi-Markov forward-backward algorithm (Levinson, 1986). During the M-step, we update the parameters θLPAD c , θRPAD cusing the standard closed-form multinomial updates and use a specialized closedform update for θGLYPH that enforces unimodalc ity of the glyph width distribution.3 The glyph weights, φc, do not have a closed-form update. The noise model that φc parameterizes is a local log-linear model, so we follow the approach of Berg-Kirkpatrick et al. (2010) and use L-BFGS (Liu and Nocedal, 1989) to optimize the expected likelihood with respect to φc. 4.2 Coarse-to-Fine Learning and Inference The number of states in the dynamic programming latt</context>
</contexts>
<marker>Levinson, 1986</marker>
<rawString>Stephen Levinson. 1986. Continuously variable duration hidden Markov models for automatic speech recognition. Computer Speech &amp; Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization. Mathematical programming.</title>
<date>1989</date>
<contexts>
<context position="17056" citStr="Liu and Nocedal, 1989" startWordPosition="2920" endWordPosition="2923">is an instance of a hidden semi-Markov model (HSMM), and therefore the computation of marginals is tractable with the semi-Markov forward-backward algorithm (Levinson, 1986). During the M-step, we update the parameters θLPAD c , θRPAD cusing the standard closed-form multinomial updates and use a specialized closedform update for θGLYPH that enforces unimodalc ity of the glyph width distribution.3 The glyph weights, φc, do not have a closed-form update. The noise model that φc parameterizes is a local log-linear model, so we follow the approach of Berg-Kirkpatrick et al. (2010) and use L-BFGS (Liu and Nocedal, 1989) to optimize the expected likelihood with respect to φc. 4.2 Coarse-to-Fine Learning and Inference The number of states in the dynamic programming lattice grows exponentially with the order of the language model (Jelinek, 1998; Koehn, 2004). As a result, inference can become slow when the language model order n is large. To remedy this, we take a coarse-to-fine approach to both learning and inference. On each iteration of EM, we perform two passes: a coarse pass using a low-order language model, and a fine pass using a high-order language model (Petrov et al., 2008; Zhang and Gildea, 2008). We</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation. Computational Linguistics.</title>
<date>2004</date>
<contexts>
<context position="26094" citStr="Och and Ney, 2004" startWordPosition="4459" endWordPosition="4462">ments that use the long s glyph, we introduce a special character type for the non-word-final s, and initialize its parameters from a mixture of the modern f and |glyphs.9 7 Results and Analysis The results of our experiments are summarized in Table 1. We refer to our system as Ocular w/ NYT or Ocular w/ OB, depending on whether the language model was trained using NYT or Old Bailey, respectively. We compute macro-averages 7One of the hyperparameters we tune is the exponent of the language model. This balances the contributions of the language model and the typesetting model to the posterior (Och and Ney, 2004). 8http://www.ubuntu.com/ 9Following Berg-Kirkpatrick et al. (2010), we use a regularization term in the optimization of the log-linear model parameters φ. during the M-step. Instead of regularizing towards zero, we regularize towards the initializer. This slightly improves performance on our development set and can be thought of as placing a prior on the glyph shape parameters. and θLPAD c to 213 (a) Old Bailey, 1775: Predicted text: the prisoner at the bar. Jacob Lazarus and his Predicted typesetting: Image: (b) Old Bailey, 1885: Predicted text: taken ill and taken away – I remember Predicte</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coarse-to-fine syntactic machine translation using language projections.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="17627" citStr="Petrov et al., 2008" startWordPosition="3016" endWordPosition="3019">l. (2010) and use L-BFGS (Liu and Nocedal, 1989) to optimize the expected likelihood with respect to φc. 4.2 Coarse-to-Fine Learning and Inference The number of states in the dynamic programming lattice grows exponentially with the order of the language model (Jelinek, 1998; Koehn, 2004). As a result, inference can become slow when the language model order n is large. To remedy this, we take a coarse-to-fine approach to both learning and inference. On each iteration of EM, we perform two passes: a coarse pass using a low-order language model, and a fine pass using a high-order language model (Petrov et al., 2008; Zhang and Gildea, 2008). We use the marginals4 from the coarse pass to prune states from the dynamic program of the fine pass. In the early iterations of EM, our font parameters are still inaccurate, and to prune heavily based on such parameters would rule out correct analyses. Therefore, we gradually increase the aggressiveness of pruning over the course of EM. To ensure that each iteration takes approximately the same amount of computation, we also gradually increase the order of the fine pass, only reaching the full order n on the last iteration. To produce a decoding of the image into te</context>
</contexts>
<marker>Petrov, Haghighi, Klein, 2008</marker>
<rawString>Slav Petrov, Aria Haghighi, and Dan Klein. 2008. Coarse-to-fine syntactic machine translation using language projections. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Attacking decipherment problems optimally with low-order ngram models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5158" citStr="Ravi and Knight, 2008" startWordPosition="809" endWordPosition="812">rical documents has done so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typicall</context>
</contexts>
<marker>Ravi, Knight, 2008</marker>
<rawString>Sujith Ravi and Kevin Knight. 2008. Attacking decipherment problems optimally with low-order ngram models. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Bayesian inference for Zodiac and other homophonic ciphers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<contexts>
<context position="5202" citStr="Ravi and Knight, 2011" startWordPosition="817" endWordPosition="820">d approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typically deals only with discrete symbols. 3 Model </context>
</contexts>
<marker>Ravi, Knight, 2011</marker>
<rawString>Sujith Ravi and Kevin Knight. 2011. Bayesian inference for Zodiac and other homophonic ciphers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Shoemaker</author>
</authors>
<title>Digital London: Creating a searchable web of interlinked sources on eighteenth century London. Electronic Library and Information Systems.</title>
<date>2005</date>
<contexts>
<context position="1201" citStr="Shoemaker, 2005" startWordPosition="175" endWordPosition="176">system substantially outperforms state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesseract, Google’s open source OCR system. 1 Introduction Standard techniques for transcribing modern documents do not work well on historical ones. For example, even state-of-the-art OCR systems produce word error rates of over 50% on the documents shown in Figure 1. Unsurprisingly, such error rates are too high for many research projects (Arlitsch and Herbert, 2004; Shoemaker, 2005; Holley, 2010). We present a new, generative model specialized to transcribing printing-press era documents. Our model is inspired by the underlying printing processes and is designed to capture the primary sources of variation and noise. One key challenge is that the fonts used in historical documents are not standard (Shoemaker, 2005). For example, consider Figure 1a. The fonts are not irregular like handwriting – each occurrence of a given character type, e.g. a, will use the same underlying glyph. However, the exact glyphs are unknown. Some differences between fonts are minor, reflecting </context>
<context position="19700" citStr="Shoemaker, 2005" startWordPosition="3381" endWordPosition="3382">es a word error rate (WER) of 25.2. Document (b) is cleaner in comparison, and on it we achieve a WER of 15.4. On document (c), which is also relatively clean, we achieve a WER of 12.5. On document (d), which is severely degraded, we achieve a WER of 70.0. 5 Data We perform experiments on two historical datasets consisting of images of documents printed between 1700 and 1900 in England and Australia. Examples from both datasets are displayed in Figure 6. 5.1 Old Bailey The first dataset comes from a large set of images of the proceedings of the Old Bailey, a criminal court in London, England (Shoemaker, 2005). The Old Bailey curatorial effort, after deciding that current OCR systems do not adequately handle 18th century fonts, manually transcribed the documents into text. We will use these manual transcriptions to evaluate the output of our system. From the Old Bailey proceedings, we extracted a set of 20 images, each consisting of 30 lines of text to use as our first test set. We picked 20 documents, printed in consecutive decades. The first document is from 1715 and the last is from 1905. We choose the first document in each of the corresponding years, choose a random page in the document, and e</context>
</contexts>
<marker>Shoemaker, 2005</marker>
<rawString>Robert Shoemaker. 2005. Digital London: Creating a searchable web of interlinked sources on eighteenth century London. Electronic Library and Information Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Smith</author>
</authors>
<title>An overview of the tesseract ocr engine.</title>
<date>2007</date>
<booktitle>In Proceedings of the Ninth International Conference on Document Analysis and Recognition.</booktitle>
<contexts>
<context position="22413" citStr="Smith, 2007" startWordPosition="3848" endWordPosition="3849">omponents of ink, spanning multiple lines. The values of such pixels are treated as unobserved in the model since, more often than not, they are part of ink blotches. 5This ruled out portions of the document with extreme structural abnormalities, like title pages and lists. These might be interesting to model, but are not within the scope of this paper. (b) Old Bailey, 1875: (d) Trove, 1883: 212 6 Experiments We evaluate our system by comparing our text recognition accuracy to that of two state-of-the-art systems. 6.1 Baselines Our first baseline is Google’s open source OCR system, Tesseract (Smith, 2007). Tesseract takes a pipelined approach to recognition. Before recognizing the text, the document is broken into lines, and each line is segmented into words. Then, Tesseract uses a classifier, aided by a wordunigram language model, to recognize whole words. Our second baseline, ABBYY FineReader 11 Professional Edition,6 is a state-of-the-art commercial OCR system. It is the OCR system that the National Library of Australia used to recognize the historical documents in Trove (Holley, 2010). 6.2 Evaluation We evaluate the output of our system and the baseline systems using two metrics: character</context>
</contexts>
<marker>Smith, 2007</marker>
<rawString>Ray Smith. 2007. An overview of the tesseract ocr engine. In Proceedings of the Ninth International Conference on Document Analysis and Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
<author>Kevin Knight</author>
</authors>
<title>A statistical model for lost language decipherment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5179" citStr="Snyder et al., 2010" startWordPosition="813" endWordPosition="816">e so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is similar to our own: we are presented with a sequence of symbols, and we need to learn a correspondence between symbols and letters. Our approach is also similar in that we use a strong language model (in conjunction with the constraint that the correspondence be regular) to learn the correct mapping. However, the symbols are not noisy in decipherment problems and in our problem we face a grid of pixels for which the segmentation into symbols is unknown. In contrast, decipherment typically deals only with dis</context>
</contexts>
<marker>Snyder, Barzilay, Knight, 2010</marker>
<rawString>Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010. A statistical model for lost language decipherment. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgios Vamvakas</author>
<author>Basilios Gatos</author>
<author>Nikolaos Stamatopoulos</author>
<author>Stavros Perantonis</author>
</authors>
<title>A complete optical character recognition methodology for historical documents.</title>
<date>2008</date>
<booktitle>In The Eighth IAPR International Workshop on Document Analysis Systems.</booktitle>
<contexts>
<context position="4669" citStr="Vamvakas et al., 2008" startWordPosition="725" endWordPosition="728"> Learned-Miller, 2009), but not in a way targeted to documents from the printing press era. For example, some approaches have learned fonts in an unsupervised fashion but require pre-segmentation of the image into character or word regions (Ho and Nagy, 2000; Huang et al., 2006), which is not feasible for noisy historical documents. Kae and Learned-Miller (2009) jointly learn the font and image segmentation but do not outperform modern baselines. Work that has directly addressed historical documents has done so using a pipelined approach, and without fully integrating a strong language model (Vamvakas et al., 2008; Kluzner et al., 2009; Kae et al., 2010; Kluzner et al., 2011). The most comparable work is that of Kopec and Lomelin (1996) and Kopec et al. (2001). They integrated typesetting models with language models, but did not model noise. In the NLP community, generative models have been developed specifically for correcting outputs of OCR systems (Kolak et al., 2003), but these do not deal directly with images. A closely related area of work is automatic decipherment (Ravi and Knight, 2008; Snyder et al., 2010; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). The fundamental problem is sim</context>
</contexts>
<marker>Vamvakas, Gatos, Stamatopoulos, Perantonis, 2008</marker>
<rawString>Georgios Vamvakas, Basilios Gatos, Nikolaos Stamatopoulos, and Stavros Perantonis. 2008. A complete optical character recognition methodology for historical documents. In The Eighth IAPR International Workshop on Document Analysis Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Efficient multipass decoding for synchronous context free grammars.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="17652" citStr="Zhang and Gildea, 2008" startWordPosition="3020" endWordPosition="3023">FGS (Liu and Nocedal, 1989) to optimize the expected likelihood with respect to φc. 4.2 Coarse-to-Fine Learning and Inference The number of states in the dynamic programming lattice grows exponentially with the order of the language model (Jelinek, 1998; Koehn, 2004). As a result, inference can become slow when the language model order n is large. To remedy this, we take a coarse-to-fine approach to both learning and inference. On each iteration of EM, we perform two passes: a coarse pass using a low-order language model, and a fine pass using a high-order language model (Petrov et al., 2008; Zhang and Gildea, 2008). We use the marginals4 from the coarse pass to prune states from the dynamic program of the fine pass. In the early iterations of EM, our font parameters are still inaccurate, and to prune heavily based on such parameters would rule out correct analyses. Therefore, we gradually increase the aggressiveness of pruning over the course of EM. To ensure that each iteration takes approximately the same amount of computation, we also gradually increase the order of the fine pass, only reaching the full order n on the last iteration. To produce a decoding of the image into text, on the final iteratio</context>
</contexts>
<marker>Zhang, Gildea, 2008</marker>
<rawString>Hao Zhang and Daniel Gildea. 2008. Efficient multipass decoding for synchronous context free grammars. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>