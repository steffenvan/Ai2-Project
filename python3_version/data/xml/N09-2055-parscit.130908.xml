<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015871">
<title confidence="0.9942095">
Statistical Post-Editing of a
Rule-Based Machine Translation System∗
</title>
<author confidence="0.762562">
A.-L. Lagarda, V. Alabau, F. Casacuberta R. Silva, and E. Diaz-de-Lia˜no
</author>
<affiliation confidence="0.388553">
Instituto Tecnol´ogico de Inform´atica Celer Soluciones, S.L.
Universidad Polit´ecnica de Valencia, Spain Madrid, Spain
</affiliation>
<email confidence="0.991866">
alagarda@iti.upv.es
</email>
<sectionHeader confidence="0.995501" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999048842105263">
Automatic post-editing (APE) systems aim at
correcting the output of machine translation
systems to produce better quality translations,
i.e. produce translations can be manually post-
edited with an increase in productivity. In this
work, we present an APE system that uses sta-
tistical models to enhance a commercial rule-
based machine translation (RBMT) system. In
addition, a procedure for effortless human eva-
luation has been established. We have tested
the APE system with two corpora of differ-
ent complexity. For the Parliament corpus, we
show that the APE system significantly com-
plements and improves the RBMT system. Re-
sults for the Protocols corpus, although less
conclusive, are promising as well. Finally,
several possible sources of errors have been
identified which will help develop future sys-
tem enhancements.
</bodyText>
<sectionHeader confidence="0.999128" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995321344827586">
Current machine translation systems are far from
perfect. To achieve high-quality output, the raw
translations they generate often need to be corrected,
or post-edited by human translators. One way of in-
creasing the productivity of the whole process is the
development of automatic post-editing (APE) sys-
tems (Dugast et al., 2007; Simard et al., 2007).
∗ Work supported by the EC (FEDER) and the Spanish
MEC under grant TIN2006-15694-CO2-01, by the Spanish
research programme Consolider Ingenio 2010:MIPRCV
(CSD2007-00018), and by the i3media Cenit project (CDTI
2007-1012).
Many of these works propose a combination of
rule-based machine translation (RBMT) and statisti-
cal machine translation (SMT) systems, in order to
take advantage of the particular capabilities of each
system (Chen and Chen, 1997).
A possible combination is to automatically post-
edit the output of a RBMT system employing a SMT
system. In this work, we will apply this technique
into two different corpora: Parliament and Proto-
cols. In addition, we will propose a new human eva-
luation measure that will deal with the impact of the
automatic post-editing.
This paper is structured as follows: after a brief
introduction of the RBMT, SMT, and APE systems
in Section 2, Section 3 details the carried out experi-
mentation, discussing its results. Finally, some con-
clusions and future work are presented in Section 4.
</bodyText>
<sectionHeader confidence="0.918312" genericHeader="method">
2 Systems description
</sectionHeader>
<bodyText confidence="0.9999545">
Three different systems are compared in this work,
namely the RBMT, SMT, and APE systems.
Rule-based machine translation. RBMT was the
first approach to machine translation, and thus, a
relatively mature area in this field. RBMT sys-
tems are basically constituted by two components:
the rules, that account for the syntactic knowledge,
and the lexicon, which deals with the morphologi-
cal, syntactic, and semantic information. Both rules
and lexicons are grounded on linguistic knowledge
and generated by expert linguists. As a result, the
build process is expensive and the system is difficult
to maintain (Bennett and Slocum, 1985). Further-
more, RBMT systems fail to adapt to new domains.
</bodyText>
<page confidence="0.972309">
217
</page>
<note confidence="0.35733">
Proceedings of NAACL HLT 2009: Short Papers, pages 217–220,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.99993919047619">
Although they usually provide a mechanism to cre-
ate new rules and extend and adapt the lexicon,
changes are usually very costly and the results, fre-
quently, do not pay off (Isabelle et al., 2007).
Statistical machine translation. In SMT, transla-
tions are generated on the basis of statistical models,
which are derived from the analysis of bilingual text
corpora. The translation problem can be statistically
formulated as in (Brown et al., 1993). In practice,
several models are often combined into a log-linear
fashion. Each model can represent an important fea-
ture for the translation, such as phrase-based, lan-
guage, or lexical models (Koehn et al., 2003).
Automatic post-editing. An APE system can be
viewed as a translation process between the output
from a previous MT system, and the target language.
In our case, an APE system based on statistical mod-
els will be trained to correct the translation errors
made by a RBMT system. As a result, both RBMT
and SMT technologies will be combined in order to
increase the overall translation quality.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.99983452173913">
We present some experiments carried out using the
introduced APE system, and comparing its perfor-
mance with that of the RBMT and SMT systems.
In the experimentation, two different English-to-
Spanish corpora have been chosen, Parliament and
Protocols, both of them provided by a professional
translation agency.
Corpora. The Parliament corpus consists of a se-
ries of documents from proceedings of parliamen-
tary sessions, provided by a client of the transla-
tion agency involved in this work. Most of the sen-
tences are transcriptions of parliamentary speeches,
and thus, with the peculiarities of the oral language.
Despite of the multi-topic nature of the speeches,
differences in training and test perplexities indicate
that the topics in test are well represented in the
training set (corpus statistics in Table 1).
On the other hand, the Protocols corpus is a
collection of medical protocols. This is a more
difficult task, as its statistics reflect in Table 1. There
are many factors that explain this complexity, such
as the different companies involved in training and
test sets, out-of-domain test data (see perplexity and
</bodyText>
<tableCaption confidence="0.993217">
Table 1: Corpus statistics for Parliament and Protocols.
OOV stands for out-of-vocabulary words.
</tableCaption>
<bodyText confidence="0.99857375">
out-of-vocabulary words), non-native authors, etc.
Evaluation. In order to assess the proposed sys-
tems, a series of measures have been considered. In
first place, some state-of-the-art automatic metrics
have been chosen to give a first idea of the quality of
the translations. These translations have been also
evaluated by professional translators to assess the in-
crease of productivity when using each system.
Automatic evaluation. The automatic assessment
of the translation quality has been carried out us-
ing the BiLingual Evaluation Understudy (BLEU)
(Papineni et al., 2002), and the Translation Error
Rate (TER) (Snover et al., 2006). The latter takes
into account the number of edits required to con-
vert the system output into the reference. Hence, this
measure roughly estimates the post-edition process.
Human evaluation. A new human evaluation
measure has been proposed to roughly estimate
the productivity increase when using each of the
systems in a real scenario, grounded on previous
works for human evaluation of qualitative fac-
tors (Callison-Burch et al., 2007). One of the de-
sired qualities for this measure was that it should
pose little effort to the human evaluator. Thus, a
binary measure was chosen, the suitability, where
the translations are identified as suitable or not sui-
table. A given translation is considered to be suitable
if it can be manually post-edited with effort savings,
i.e., the evaluator thinks that a manual post-editing
will increase his productivity. On the contrary, if the
evaluator prefers to ignore the proposed translation
and start it over, the sentence is deemed not suitable.
</bodyText>
<table confidence="0.99869975">
Parliament Protocols
En Sp En Sp
Sentences 90K 90K 154K 154K
Run. words 2.3M 2.5M 3.2M 3.6M
Vocabulary 29K 45K 41K 47K
Perplexity 42 37 21 19
Sentences 1K 1K 3K 3K
Run. words 33K 33K 54K 71K
OOVs 157 219 2K 1.7K
Perplexity 44 43 131 173
Training
Test
</table>
<page confidence="0.993462">
218
</page>
<bodyText confidence="0.999835444444445">
Significance tests. Significance of the results has
been assessed by the paired bootstrap resampling
method, described in (Koehn, 2004). It estimates
how confidently the conclusion that a system outper-
forms another one can be drawn from a test result.
Experimental setup. Rule-based translation was
performed by means of a commercial RBMT system.
On the other hand, statistical training and translation
in both SMT and APE systems were carried out using
the Moses toolkit (Koehn et al., 2007). It should be
noted that APE system was trained taking the RBMT
output as source, instead of the original text. In this
way, it is able to post-edit the RBMT translations.
Finally, the texts employed for the human eva-
luation were composed by 350 sentences randomly
drawn from each one of the two test corpora des-
cribed in this paper. Two professional translators
carried out the human evaluation.
</bodyText>
<subsectionHeader confidence="0.943282">
3.1 Results and discussion
</subsectionHeader>
<bodyText confidence="0.999937925925926">
Experimentation results in terms of automatic and
human evaluation are shown in this section.
Automatic evaluation. Table 2 presents Parlia-
ment and Protocols corpora translation results in
terms of automatic metrics. Note that, as there is
a single reference, this results are somehow pes-
simistic.
In the case of the Parliament corpus, SMT system
outperforms the rest of the systems. APE results are
slightly worse than SMT, but far better than RBMT.
However, when moving to the Protocols corpus, a
more difficult task (as seen in perplexity in Table 1),
the results show quite the contrary. SMT and APE
systems show how they are more sensitive to out-
of-domain documents. Nevertheless, the RBMT sys-
tem seems to be more robust under such conditions.
Despite of the degradation of the statistical models,
APE manages to achieve much better results than the
other two systems. It is able to conserve the robust-
ness of RBMT, while its statistical counterpart deals
with the particularities of the corpus.
Human evaluation. Table 3 shows the percentage
of translations deemed suitable by the human eva-
luators. Two professional evaluators analysed the
suitability of the output of each system
In the Parliament case, APE performance is found
much more suitable than the rest of the systems. In
</bodyText>
<tableCaption confidence="0.990571">
Table 2: Automatic evaluation for Parliament and Proto-
cols tests.
</tableCaption>
<table confidence="0.9984228">
Parliament Protocols
BLEU TER BLEU TER
RBMT 29.1 46.7 29.5 48.0
SMT 49.9 34.9 22.4 59.6
APE 48.4 35.9 33.6 46.2
</table>
<bodyText confidence="0.999407933333333">
fact, this difference between APE and the rest is sta-
tistically significant at a 99% level of confidence.
In addition, significance tests show that, on average,
APE improves RBMT on 59.5% of translations.
Regarding to the Protocols corpus, it must be
noted that a first review of the translations pointed
out that the SMT system performed quite poorly.
Hence, SMT was not considered for the human eva-
luation on this corpus.
Figures show that APE complements and im-
proves RBMT, although differences between them
are tighter than in the Parliament corpus. However,
significance tests still prove that these improvements
are statistically significant (68% of confidence), and
that the average improvement is 6.5%.
</bodyText>
<tableCaption confidence="0.995428666666667">
Table 3: Human evaluation for Parliament and Protocols
corpora. Percentage of suitable translated sentences for
each system.
</tableCaption>
<figure confidence="0.711397">
Parliament Protocols
58 60
60
94 67
</figure>
<bodyText confidence="0.908870944444444">
It is interesting to note how automatic measures
and human evaluation seem not to be quite corre-
lated. In terms of automatic measures, the best sys-
tem to translate the Parliament test is the SMT. This
improvement has been checked by carrying out sig-
nificance tests, resulting statistically significant with
a 99% of confidence. However, in the human eva-
luation, SMT is worse than APE (this difference is
also significant at 99%). On the other hand, when
working with the Protocols corpus, automatic me-
trics indicate that APE improves the rest (significant
improvement at 99%). Nevertheless, human evalua-
tors seem to think that the difference between APE
and RBMT is not so significant, only with a confi-
dence of 68%. Previous works confirm this apparent
RBMT
SMT
APE
</bodyText>
<page confidence="0.989276">
219
</page>
<bodyText confidence="0.999796076923077">
discrepancy between automatic and human evalua-
tions (Callison-Burch et al., 2007).
Translator’s commentaries. As a subproduct of
the human evaluation, the evaluators gave some
personal impressions regarding each system perfor-
mance. They concluded that, when working with the
Parliament corpus, there was a net improvement in
the overall performance when using APE. Changes
between RBMT and APE were minor but useful.
Thus, APE did not pose a system degradation with
respect to the RBMT. Furthermore, a rough estima-
tion indicated that over 10% of the sentences were
perfectly translated, i.e. the translation was human-
like. In addition, some frequent collocations were
found to be correctly post-edited by the APE system,
which was felt very effort saving.
With respect to the Protocols corpus, as expected,
results were found not so satisfactory. However,
human translators find themselves these documents
complex.
Finally, in both cases, APE is able to make the
translation more similar to the reference by fix-
ing some words without altering the grammatical
structure of the sentence. Finally, translators would
find very useful a system that automatically decided
when to automatically post-edit the RBMT outputs.
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999990956521739">
We have presented an automatic post-editing sys-
tem that can be added at the core of the professional
translation workflow. Furthermore, we have tested it
with two corpora of different complexity.
For the Parliament corpus, we have shown that
the APE system complements and improves the
RBMT system in terms of suitability in a real transla-
tion scenario (average improvement 59.5%). Results
for the Protocols corpus, although less conclusive,
are promising as well (average improvement 6.5%).
Moreover, 67% of Protocols translations, and 94%
of Parliament translations were considered to be sui-
table.
Finally, a procedure for effortless human eva-
luation has been established. A future improve-
ment for this would be to integrate the process in the
core of the translator’s workflow, so that on-the-fly
evaluation can be made. In addition, several pos-
sible sources of errors have been identified which
will help develop future system enhancements. For
example, as stated in the translator’s commentaries,
the automatic selection of the most suitable transla-
tion among the systems is a desirable feature.
</bodyText>
<sectionHeader confidence="0.99006" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883477272727">
W. S. Bennett and J. Slocum. 1985. The lrc machine
translation system. Comp. Linguist., 11(2-3):111–121.
P. F. Brown, S. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Comp. Linguist.,
19(2):263–312.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (meta-) evaluation of machine
translation. In Proc. of the 2nd Workshop on SMT,
pages 136–158, Prague, Czech Republic. ACL.
K. Chen and H. Chen. 1997. A hybrid approach to ma-
chine translation system design. In Comp. Linguist.
and Chinese Language Processing 23, pages 241–265.
L. Dugast, J. Senellart, and P. Koehn. 2007. Statisti-
cal post-editing on SYSTRAN’s rule-based translation
system. In Proc. of the 2nd Workshop on SMT, pages
220–223, Prague, Czech Republic. ACL.
P. Isabelle, C. Goutte, and M. Simard. 2007. Do-
main adaptation of mt systems through automatic post-
editing. In Proc. of MTSummit XI, pages 255–261,
Copenhagen, Denmark.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL-HLT,
pages 48–54, Edmonton, Canada.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL, pages
177–180, Prague, Czech Republic.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proc. of EMNLP 2004,
Barcelona, Spain.
K. Papineni, S. Roukos, T. Ward, and W.-Jing Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. ofACL, pages 311–318, Philadel-
phia, PA, USA.
M. Simard, C. Goutte, and P. Isabelle. 2007. Statis-
tical phrase-based post-editing. In Proc. of NAACL-
HLT2007, pages 508–515, Rochester, NY. ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. of AMTA, pages
223–231.
</reference>
<page confidence="0.997593">
220
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.908008">
<title confidence="0.9973715">Statistical Post-Editing of Machine Translation</title>
<author confidence="0.999577">V Alabau Lagarda</author>
<author confidence="0.999577">F Casacuberta R Silva</author>
<affiliation confidence="0.987558">Instituto Tecnol´ogico de Inform´atica Celer Soluciones, S.L. Universidad Polit´ecnica de Valencia, Spain Madrid,</affiliation>
<email confidence="0.983265">alagarda@iti.upv.es</email>
<abstract confidence="0.99747795">post-editing systems aim at correcting the output of machine translation systems to produce better quality translations, i.e. produce translations can be manually postedited with an increase in productivity. In this we present an that uses statistical models to enhance a commercial rulemachine translation system. In addition, a procedure for effortless human evaluation has been established. We have tested with two corpora of differcomplexity. For the we that the significantly comand improves the Refor the although less conclusive, are promising as well. Finally, several possible sources of errors have been identified which will help develop future system enhancements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W S Bennett</author>
<author>J Slocum</author>
</authors>
<title>The lrc machine translation system.</title>
<date>1985</date>
<journal>Comp. Linguist.,</journal>
<pages>11--2</pages>
<contexts>
<context position="3167" citStr="Bennett and Slocum, 1985" startWordPosition="480" endWordPosition="483">different systems are compared in this work, namely the RBMT, SMT, and APE systems. Rule-based machine translation. RBMT was the first approach to machine translation, and thus, a relatively mature area in this field. RBMT systems are basically constituted by two components: the rules, that account for the syntactic knowledge, and the lexicon, which deals with the morphological, syntactic, and semantic information. Both rules and lexicons are grounded on linguistic knowledge and generated by expert linguists. As a result, the build process is expensive and the system is difficult to maintain (Bennett and Slocum, 1985). Furthermore, RBMT systems fail to adapt to new domains. 217 Proceedings of NAACL HLT 2009: Short Papers, pages 217–220, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Although they usually provide a mechanism to create new rules and extend and adapt the lexicon, changes are usually very costly and the results, frequently, do not pay off (Isabelle et al., 2007). Statistical machine translation. In SMT, translations are generated on the basis of statistical models, which are derived from the analysis of bilingual text corpora. The translation problem can be stat</context>
</contexts>
<marker>Bennett, Slocum, 1985</marker>
<rawString>W. S. Bennett and J. Slocum. 1985. The lrc machine translation system. Comp. Linguist., 11(2-3):111–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Comp. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3814" citStr="Brown et al., 1993" startWordPosition="582" endWordPosition="585">fail to adapt to new domains. 217 Proceedings of NAACL HLT 2009: Short Papers, pages 217–220, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Although they usually provide a mechanism to create new rules and extend and adapt the lexicon, changes are usually very costly and the results, frequently, do not pay off (Isabelle et al., 2007). Statistical machine translation. In SMT, translations are generated on the basis of statistical models, which are derived from the analysis of bilingual text corpora. The translation problem can be statistically formulated as in (Brown et al., 1993). In practice, several models are often combined into a log-linear fashion. Each model can represent an important feature for the translation, such as phrase-based, language, or lexical models (Koehn et al., 2003). Automatic post-editing. An APE system can be viewed as a translation process between the output from a previous MT system, and the target language. In our case, an APE system based on statistical models will be trained to correct the translation errors made by a RBMT system. As a result, both RBMT and SMT technologies will be combined in order to increase the overall translation qua</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Comp. Linguist., 19(2):263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>C Fordyce</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>J Schroeder</author>
</authors>
<title>(meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the 2nd Workshop on SMT,</booktitle>
<pages>136--158</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6735" citStr="Callison-Burch et al., 2007" startWordPosition="1041" endWordPosition="1044">ssessment of the translation quality has been carried out using the BiLingual Evaluation Understudy (BLEU) (Papineni et al., 2002), and the Translation Error Rate (TER) (Snover et al., 2006). The latter takes into account the number of edits required to convert the system output into the reference. Hence, this measure roughly estimates the post-edition process. Human evaluation. A new human evaluation measure has been proposed to roughly estimate the productivity increase when using each of the systems in a real scenario, grounded on previous works for human evaluation of qualitative factors (Callison-Burch et al., 2007). One of the desired qualities for this measure was that it should pose little effort to the human evaluator. Thus, a binary measure was chosen, the suitability, where the translations are identified as suitable or not suitable. A given translation is considered to be suitable if it can be manually post-edited with effort savings, i.e., the evaluator thinks that a manual post-editing will increase his productivity. On the contrary, if the evaluator prefers to ignore the proposed translation and start it over, the sentence is deemed not suitable. Parliament Protocols En Sp En Sp Sentences 90K 9</context>
<context position="11649" citStr="Callison-Burch et al., 2007" startWordPosition="1848" endWordPosition="1851">ecked by carrying out significance tests, resulting statistically significant with a 99% of confidence. However, in the human evaluation, SMT is worse than APE (this difference is also significant at 99%). On the other hand, when working with the Protocols corpus, automatic metrics indicate that APE improves the rest (significant improvement at 99%). Nevertheless, human evaluators seem to think that the difference between APE and RBMT is not so significant, only with a confidence of 68%. Previous works confirm this apparent RBMT SMT APE 219 discrepancy between automatic and human evaluations (Callison-Burch et al., 2007). Translator’s commentaries. As a subproduct of the human evaluation, the evaluators gave some personal impressions regarding each system performance. They concluded that, when working with the Parliament corpus, there was a net improvement in the overall performance when using APE. Changes between RBMT and APE were minor but useful. Thus, APE did not pose a system degradation with respect to the RBMT. Furthermore, a rough estimation indicated that over 10% of the sentences were perfectly translated, i.e. the translation was humanlike. In addition, some frequent collocations were found to be c</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and J. Schroeder. 2007. (meta-) evaluation of machine translation. In Proc. of the 2nd Workshop on SMT, pages 136–158, Prague, Czech Republic. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Chen</author>
<author>H Chen</author>
</authors>
<title>A hybrid approach to machine translation system design.</title>
<date>1997</date>
<booktitle>In Comp. Linguist. and Chinese Language Processing 23,</booktitle>
<pages>241--265</pages>
<contexts>
<context position="1932" citStr="Chen and Chen, 1997" startWordPosition="281" endWordPosition="284">ators. One way of increasing the productivity of the whole process is the development of automatic post-editing (APE) systems (Dugast et al., 2007; Simard et al., 2007). ∗ Work supported by the EC (FEDER) and the Spanish MEC under grant TIN2006-15694-CO2-01, by the Spanish research programme Consolider Ingenio 2010:MIPRCV (CSD2007-00018), and by the i3media Cenit project (CDTI 2007-1012). Many of these works propose a combination of rule-based machine translation (RBMT) and statistical machine translation (SMT) systems, in order to take advantage of the particular capabilities of each system (Chen and Chen, 1997). A possible combination is to automatically postedit the output of a RBMT system employing a SMT system. In this work, we will apply this technique into two different corpora: Parliament and Protocols. In addition, we will propose a new human evaluation measure that will deal with the impact of the automatic post-editing. This paper is structured as follows: after a brief introduction of the RBMT, SMT, and APE systems in Section 2, Section 3 details the carried out experimentation, discussing its results. Finally, some conclusions and future work are presented in Section 4. 2 Systems descript</context>
</contexts>
<marker>Chen, Chen, 1997</marker>
<rawString>K. Chen and H. Chen. 1997. A hybrid approach to machine translation system design. In Comp. Linguist. and Chinese Language Processing 23, pages 241–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Dugast</author>
<author>J Senellart</author>
<author>P Koehn</author>
</authors>
<title>Statistical post-editing on SYSTRAN’s rule-based translation system.</title>
<date>2007</date>
<booktitle>In Proc. of the 2nd Workshop on SMT,</booktitle>
<pages>220--223</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1458" citStr="Dugast et al., 2007" startWordPosition="211" endWordPosition="214">hat the APE system significantly complements and improves the RBMT system. Results for the Protocols corpus, although less conclusive, are promising as well. Finally, several possible sources of errors have been identified which will help develop future system enhancements. 1 Introduction Current machine translation systems are far from perfect. To achieve high-quality output, the raw translations they generate often need to be corrected, or post-edited by human translators. One way of increasing the productivity of the whole process is the development of automatic post-editing (APE) systems (Dugast et al., 2007; Simard et al., 2007). ∗ Work supported by the EC (FEDER) and the Spanish MEC under grant TIN2006-15694-CO2-01, by the Spanish research programme Consolider Ingenio 2010:MIPRCV (CSD2007-00018), and by the i3media Cenit project (CDTI 2007-1012). Many of these works propose a combination of rule-based machine translation (RBMT) and statistical machine translation (SMT) systems, in order to take advantage of the particular capabilities of each system (Chen and Chen, 1997). A possible combination is to automatically postedit the output of a RBMT system employing a SMT system. In this work, we wil</context>
</contexts>
<marker>Dugast, Senellart, Koehn, 2007</marker>
<rawString>L. Dugast, J. Senellart, and P. Koehn. 2007. Statistical post-editing on SYSTRAN’s rule-based translation system. In Proc. of the 2nd Workshop on SMT, pages 220–223, Prague, Czech Republic. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Isabelle</author>
<author>C Goutte</author>
<author>M Simard</author>
</authors>
<title>Domain adaptation of mt systems through automatic postediting.</title>
<date>2007</date>
<booktitle>In Proc. of MTSummit XI,</booktitle>
<pages>255--261</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="3563" citStr="Isabelle et al., 2007" startWordPosition="544" endWordPosition="547">mantic information. Both rules and lexicons are grounded on linguistic knowledge and generated by expert linguists. As a result, the build process is expensive and the system is difficult to maintain (Bennett and Slocum, 1985). Furthermore, RBMT systems fail to adapt to new domains. 217 Proceedings of NAACL HLT 2009: Short Papers, pages 217–220, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Although they usually provide a mechanism to create new rules and extend and adapt the lexicon, changes are usually very costly and the results, frequently, do not pay off (Isabelle et al., 2007). Statistical machine translation. In SMT, translations are generated on the basis of statistical models, which are derived from the analysis of bilingual text corpora. The translation problem can be statistically formulated as in (Brown et al., 1993). In practice, several models are often combined into a log-linear fashion. Each model can represent an important feature for the translation, such as phrase-based, language, or lexical models (Koehn et al., 2003). Automatic post-editing. An APE system can be viewed as a translation process between the output from a previous MT system, and the tar</context>
</contexts>
<marker>Isabelle, Goutte, Simard, 2007</marker>
<rawString>P. Isabelle, C. Goutte, and M. Simard. 2007. Domain adaptation of mt systems through automatic postediting. In Proc. of MTSummit XI, pages 255–261, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="4027" citStr="Koehn et al., 2003" startWordPosition="616" endWordPosition="619">m to create new rules and extend and adapt the lexicon, changes are usually very costly and the results, frequently, do not pay off (Isabelle et al., 2007). Statistical machine translation. In SMT, translations are generated on the basis of statistical models, which are derived from the analysis of bilingual text corpora. The translation problem can be statistically formulated as in (Brown et al., 1993). In practice, several models are often combined into a log-linear fashion. Each model can represent an important feature for the translation, such as phrase-based, language, or lexical models (Koehn et al., 2003). Automatic post-editing. An APE system can be viewed as a translation process between the output from a previous MT system, and the target language. In our case, an APE system based on statistical models will be trained to correct the translation errors made by a RBMT system. As a result, both RBMT and SMT technologies will be combined in order to increase the overall translation quality. 3 Experiments We present some experiments carried out using the introduced APE system, and comparing its performance with that of the RBMT and SMT systems. In the experimentation, two different English-toSpa</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proc. of NAACL-HLT, pages 48–54, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="8034" citStr="Koehn et al., 2007" startWordPosition="1259" endWordPosition="1262">y 42 37 21 19 Sentences 1K 1K 3K 3K Run. words 33K 33K 54K 71K OOVs 157 219 2K 1.7K Perplexity 44 43 131 173 Training Test 218 Significance tests. Significance of the results has been assessed by the paired bootstrap resampling method, described in (Koehn, 2004). It estimates how confidently the conclusion that a system outperforms another one can be drawn from a test result. Experimental setup. Rule-based translation was performed by means of a commercial RBMT system. On the other hand, statistical training and translation in both SMT and APE systems were carried out using the Moses toolkit (Koehn et al., 2007). It should be noted that APE system was trained taking the RBMT output as source, instead of the original text. In this way, it is able to post-edit the RBMT translations. Finally, the texts employed for the human evaluation were composed by 350 sentences randomly drawn from each one of the two test corpora described in this paper. Two professional translators carried out the human evaluation. 3.1 Results and discussion Experimentation results in terms of automatic and human evaluation are shown in this section. Automatic evaluation. Table 2 presents Parliament and Protocols corpora translati</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP 2004,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="7677" citStr="Koehn, 2004" startWordPosition="1203" endWordPosition="1204">s, i.e., the evaluator thinks that a manual post-editing will increase his productivity. On the contrary, if the evaluator prefers to ignore the proposed translation and start it over, the sentence is deemed not suitable. Parliament Protocols En Sp En Sp Sentences 90K 90K 154K 154K Run. words 2.3M 2.5M 3.2M 3.6M Vocabulary 29K 45K 41K 47K Perplexity 42 37 21 19 Sentences 1K 1K 3K 3K Run. words 33K 33K 54K 71K OOVs 157 219 2K 1.7K Perplexity 44 43 131 173 Training Test 218 Significance tests. Significance of the results has been assessed by the paired bootstrap resampling method, described in (Koehn, 2004). It estimates how confidently the conclusion that a system outperforms another one can be drawn from a test result. Experimental setup. Rule-based translation was performed by means of a commercial RBMT system. On the other hand, statistical training and translation in both SMT and APE systems were carried out using the Moses toolkit (Koehn et al., 2007). It should be noted that APE system was trained taking the RBMT output as source, instead of the original text. In this way, it is able to post-edit the RBMT translations. Finally, the texts employed for the human evaluation were composed by </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP 2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="6237" citStr="Papineni et al., 2002" startWordPosition="963" endWordPosition="966">s. OOV stands for out-of-vocabulary words. out-of-vocabulary words), non-native authors, etc. Evaluation. In order to assess the proposed systems, a series of measures have been considered. In first place, some state-of-the-art automatic metrics have been chosen to give a first idea of the quality of the translations. These translations have been also evaluated by professional translators to assess the increase of productivity when using each system. Automatic evaluation. The automatic assessment of the translation quality has been carried out using the BiLingual Evaluation Understudy (BLEU) (Papineni et al., 2002), and the Translation Error Rate (TER) (Snover et al., 2006). The latter takes into account the number of edits required to convert the system output into the reference. Hence, this measure roughly estimates the post-edition process. Human evaluation. A new human evaluation measure has been proposed to roughly estimate the productivity increase when using each of the systems in a real scenario, grounded on previous works for human evaluation of qualitative factors (Callison-Burch et al., 2007). One of the desired qualities for this measure was that it should pose little effort to the human eva</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. ofACL, pages 311–318, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Simard</author>
<author>C Goutte</author>
<author>P Isabelle</author>
</authors>
<title>Statistical phrase-based post-editing.</title>
<date>2007</date>
<booktitle>In Proc. of NAACLHLT2007,</booktitle>
<pages>508--515</pages>
<publisher>ACL.</publisher>
<location>Rochester, NY.</location>
<contexts>
<context position="1480" citStr="Simard et al., 2007" startWordPosition="215" endWordPosition="218">gnificantly complements and improves the RBMT system. Results for the Protocols corpus, although less conclusive, are promising as well. Finally, several possible sources of errors have been identified which will help develop future system enhancements. 1 Introduction Current machine translation systems are far from perfect. To achieve high-quality output, the raw translations they generate often need to be corrected, or post-edited by human translators. One way of increasing the productivity of the whole process is the development of automatic post-editing (APE) systems (Dugast et al., 2007; Simard et al., 2007). ∗ Work supported by the EC (FEDER) and the Spanish MEC under grant TIN2006-15694-CO2-01, by the Spanish research programme Consolider Ingenio 2010:MIPRCV (CSD2007-00018), and by the i3media Cenit project (CDTI 2007-1012). Many of these works propose a combination of rule-based machine translation (RBMT) and statistical machine translation (SMT) systems, in order to take advantage of the particular capabilities of each system (Chen and Chen, 1997). A possible combination is to automatically postedit the output of a RBMT system employing a SMT system. In this work, we will apply this technique</context>
</contexts>
<marker>Simard, Goutte, Isabelle, 2007</marker>
<rawString>M. Simard, C. Goutte, and P. Isabelle. 2007. Statistical phrase-based post-editing. In Proc. of NAACLHLT2007, pages 508–515, Rochester, NY. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="6297" citStr="Snover et al., 2006" startWordPosition="973" endWordPosition="976">ords), non-native authors, etc. Evaluation. In order to assess the proposed systems, a series of measures have been considered. In first place, some state-of-the-art automatic metrics have been chosen to give a first idea of the quality of the translations. These translations have been also evaluated by professional translators to assess the increase of productivity when using each system. Automatic evaluation. The automatic assessment of the translation quality has been carried out using the BiLingual Evaluation Understudy (BLEU) (Papineni et al., 2002), and the Translation Error Rate (TER) (Snover et al., 2006). The latter takes into account the number of edits required to convert the system output into the reference. Hence, this measure roughly estimates the post-edition process. Human evaluation. A new human evaluation measure has been proposed to roughly estimate the productivity increase when using each of the systems in a real scenario, grounded on previous works for human evaluation of qualitative factors (Callison-Burch et al., 2007). One of the desired qualities for this measure was that it should pose little effort to the human evaluator. Thus, a binary measure was chosen, the suitability, </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. of AMTA, pages 223–231.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>