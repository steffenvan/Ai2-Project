<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.9987505">
Detecting Semantically Equivalent Questions
in Online User Forums
</title>
<author confidence="0.896265">
Dasha Bogdanova*, Cicero dos Santos†, Luciano Barbosa† and Bianca Zadrozny†
</author>
<affiliation confidence="0.6073375">
*ADAPT centre, School of Computing, Dublin City University, Dublin, Ireland
dbogdanova@computing.dcu.ie
</affiliation>
<address confidence="0.543988">
†IBM Research, 138/146 Av. Pasteur, Rio de Janeiro, Brazil
</address>
<email confidence="0.997576">
{cicerons,lucianoa,biancaz}@br.ibm.com
</email>
<sectionHeader confidence="0.997375" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999878416666667">
Two questions asking the same thing could
be too different in terms of vocabulary and
syntactic structure, which makes identify-
ing their semantic equivalence challeng-
ing. This study aims to detect semanti-
cally equivalent questions in online user
forums. We perform an extensive number
of experiments using data from two differ-
ent Stack Exchange forums. We compare
standard machine learning methods such
as Support Vector Machines (SVM) with a
convolutional neural network (CNN). The
proposed CNN generates distributed vec-
tor representations for pairs of questions
and scores them using a similarity metric.
We evaluate in-domain word embeddings
versus the ones trained with Wikipedia,
estimate the impact of the training set
size, and evaluate some aspects of do-
main adaptation. Our experimental re-
sults show that the convolutional neural
network with in-domain word embeddings
achieves high performance even with lim-
ited training data.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999506181818182">
Question-answering (Q&amp;A) community sites,
such as Yahoo! Answers,1 Quora2 and Stack Ex-
change,3 have gained a lot of attention in the recent
years. Most Q&amp;A community sites advise users to
search the forum for an answer before posting a
new question. However, this is not always an easy
task because different users could formulate the
same question in completely different ways. Some
user forums, such as those of the Stack Exchange
online community, have a duplication policy. Ex-
act duplicates, such as copy-and-paste questions,
</bodyText>
<footnote confidence="0.997276333333333">
1 https://answers.yahoo.com/
2 http://www.quora.com
3 http://stackexchange.com/
</footnote>
<bodyText confidence="0.9998997">
and nearly exact duplicates are usually quickly de-
tected, closed and removed from the forum. Nev-
ertheless, some duplicate questions are kept. The
main reason for that is that there are many ways
to ask the same question, and a user might not be
able to find the answer if they are asking it a dif-
ferent way.4
In this study we define two questions as seman-
tically equivalent if they can be adequately an-
swered by the exact same answer. Table 1 presents
an example of a pair of such questions from Ask
Ubuntu forum. Detecting semantically equivalent
questions is a very difficult task due to two main
factors: (1) the same question can be rephrased in
many different ways; and (2) two questions could
be asking different things but look for the same
solution. Therefore, traditional similarity mea-
sures based on word overlap such as shingling and
Jaccard coefficient (Broder, 1997) and its varia-
tions (Wu et al., 2011) are not able to capture many
cases of semantic equivalence.
In this paper, we propose a convolutional neural
network architecture to detect semantically equiv-
alent questions. The proposed CNN first trans-
forms words into word embeddings (Mikolov et
al., 2013), using a large collection of unlabeled
data, and then applies a convolutional network to
build distributed vector representations for pairs of
questions. Finally, it scores the questions using
a similarity metric. Pairs of questions with simi-
larity above a threshold, defined based on a held-
out set, are considered duplicates. CNN is trained
using positive and negative pairs of semantically
equivalent questions. During training, CNN is in-
duced to produce similar vector representations
for questions that are semantically equivalent.
We perform an extensive number of experi-
ments using data from two different Stack Ex-
change forums. We compare CNN performance
with a traditional classification algorithm (Support
</bodyText>
<footnote confidence="0.948358">
4 http://stackoverflow.com/help/duplicates
</footnote>
<page confidence="0.94899">
123
</page>
<note confidence="0.582128">
Proceedings of the 19th Conference on Computational Language Learning, pages 123–131,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
Title: I can’t download anything and I can’t watch videos Title: How can I install Windows software or games?
Body: Two days ago I tried to download skype and it says Body: Can .exe and .msi files (Windows software) be in-
an error occurred it says end of central directory signature stalled in Ubuntu? [sic!]
not found Either this file is not a zipfile, or it constitutes
one disk of a multi-part archive. In the latter case the cen-
tral directory and zipfile comment will be found on the last
disk(s) of this archive. zipinfo: cannot find zipfile directory
in one of /home/maria/Downloads/SkypeSetup-aoc-jd.exe or
/home/maria/Downloads/SkypeSetup-aoc-jd.exe.zip, and can-
not find /home/maria/Downloads/SkypeSetup-aoc-jd.exe.ZIP
pe... this happens whenever I try to download anything like
games and also i can’t watch videoss it’s looking for plug ins
but it doesn’t find them i hate this [sic!]
Link:http://askubuntu.com/questions/364350 http://askubuntu.com/questions/988
</note>
<tableCaption confidence="0.961297333333333">
Possible Answer (Shortened version):.exe files are not binary-compatible with Ubuntu. There are, however, compat-
ibility layers for Linux, such as Wine, that are capable of running .exe.
Table 1: An example of semantically equivalent questions from Ask Ubuntu community.
</tableCaption>
<bodyText confidence="0.999175052631579">
Vector Machines (Cortes and Vapnik, 1995)) and
a duplicate detection approach (shingling (Broder,
1997)). The results show CNN outperforms the
baselines by a large margin.
We also investigate the impact of different word
embeddings by analyzing the performance of the
network with: (1) word embeddings pre-trained on
in-domain data and all of the English Wikipedia;
(2) word vectors of different dimensionalities; (3)
training sets of different sizes; and (4) out-of-
domain training data and in-domain word embed-
dings. The numbers show that: (1) word embed-
dings pre-trained on domain-specific data achieve
very high performance; (2) bigger word embed-
dings obtain higher accuracy; (3) in-domain word
embeddings provide better performance indepen-
dent of the training set size; and (4) in-domain
word embeddings achieve relatively high accuracy
even using out-of-domain training data.
</bodyText>
<sectionHeader confidence="0.991964" genericHeader="introduction">
2 Task
</sectionHeader>
<bodyText confidence="0.9991344">
This work focuses on the task of predicting seman-
tically equivalent questions in online user forums.
Following the duplication policy of the Stack Ex-
change online community,5 we define semanti-
cally equivalent questions as follows:
</bodyText>
<construct confidence="0.647014">
Definition 1. Two questions are semantically
equivalent if they can be adequately answered by
the exact same answer.
</construct>
<bodyText confidence="0.988183666666667">
Since our definition of semantically equivalent
questions corresponds to the rules of the Stack
Exchange duplication policy, we assume that all
</bodyText>
<footnote confidence="0.88861275">
5 http://blog.stackoverflow.com/2010/11/dr-strangedupe-
or-how-i-learned-to-stop-worrying-and-love-duplication/;
http://meta.stackexchange.com/questions/32311/do-not-
delete-good-duplicates
</footnote>
<bodyText confidence="0.999635153846154">
questions of this community that were marked as
duplicates are semantically equivalent. An exam-
ple of such questions is given in Table 1. These
questions vary significantly in vocabulary, style,
length and content quality. However, both ques-
tions require the exact same answer.
The exact task that we approach in this study
consists in, given two problem definitions, predict-
ing if they are semantically equivalent. By prob-
lem definition we mean the concatenation of the
title and the body of a question. Throughout this
paper we use the term question as a synonym of
problem definition.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999957142857143">
The development of CNN architectures for tasks
that involve sentence-level and document-level
processing is currently an area of intensive re-
search in natural language processing and infor-
mation retrieval, with many recent encouraging re-
sults.
Kim (2014) proposes a simple CNN for sen-
tence classification built on top of word2vec
(2013). A multichannel variant which combines
static word vectors from word2vec and word
vectors which are fine-tuned via backpropagation
is also proposed. Experiments with different vari-
ants are performed on a number of benchmarks
for sentence classification, showing that the sim-
ple CNN performs remarkably well, with state-of-
the-art results in many of the benchmarks, high-
lighting the importance of using unsupervised pre-
training of word vectors for this task.
Hu et al.(2014) propose a CNN architecture
for hierarchical sentence modeling and, based
on that, two architectures for sentence matching.
</bodyText>
<page confidence="0.997463">
124
</page>
<bodyText confidence="0.998947082352941">
They train the latter networks using a ranking-
based loss function on three sentence match-
ing tasks of different nature: sentence comple-
tion, response matching and paraphrase identifica-
tion. The proposed architectures outperform pre-
vious work for sentence completion and response
matching, while the results are slightly worse than
the state-of-the-art in paraphrase identification.
Yih et al.(2014) focus on the task of answering
single-relation factual questions, using a novel se-
mantic similarity model based on a CNN architec-
ture. Using this architecture, they train two mod-
els: one for linking a question to an entity in the
DB and the other mapping a relation pattern to a
relation in the DB. Both models are then combined
for inferring the entity that is the answer. This ap-
proach leads to a higher precision on Q&amp;A data
from the WikiAnswers corpus than the existing
rules-based approach for this task.
Dos Santos &amp; Gatti (2014) developed a CNN
architecture for sentiment analysis of short texts
that jointly uses character-level, word-level and
sentence-level information, achieving state-of-the-
art results on well known sentiment analysis
benchmarks.
For the specific task of semantically equivalent
questions detection that we address in this pa-
per, we are not aware of any previous work using
CNNs. Muthmann and Petrova (2014) approach
the task of identifying topical near-duplicate re-
lations between questions from social media as
a classification task. They use a simple lexico-
syntactical feature set and different classifiers are
evaluated, with logistic regression reported as the
best performing one. However, it is not possible to
directly compare our results to theirs because their
experimental methodology is not clearly described
in the paper.
There are several tasks related to identifying se-
mantically equivalent questions. These tasks in-
clude near-duplicate detection, paraphrase iden-
tification and textual semantic similarity estima-
tion. In what follows, we outline the differences
between these tasks and the one addressed in this
work.
Duplicate and Near-Duplicate Detection aims
to detect exact copies or almost exact copies of
the same document in corpora. Duplicate detec-
tion is an important component of systems for Web
crawling and Web search, where it is important to
identify redundant data in large corpora. Common
techniques to detect duplicate documents include
shingling (Broder, 1997; Alonso et al., 2013) and
fingerprinting (Manku et al., 2007). State-of-the-
art work also focuses on the efficiency issues of
the task (Wu et al., 2011). It is worth noting
that even though all duplicate and near-duplicate
questions are also semantically equivalent, the re-
verse is not true. Semantically equivalent ques-
tions could have small or no word overlap (see Ta-
ble 1 for an example), and thus, are not duplicates.
Paraphrase Identification is the task of exam-
ining two sentences and determining whether they
have the same meaning (Socher et al., 2011). If
two questions are paraphrases, they are also se-
mantically equivalent. However, many seman-
tically equivalent questions are not paraphrases.
The questions shown in Table 1 significantly differ
in the details they provide, and thus, could not be
considered as having the same meaning. State-of-
the-art approaches to paraphrase identification in-
clude using Machine Translation evaluation met-
rics (Madnani et al., 2012) and Deep Learning
techniques (Socher et al., 2011).
Textual Semantic Similarity is the task of
measuring the degree of semantic similarity be-
tween two texts, usually on a graded scale from
0 to 5, with 5 being the most similar (Agirre et al.,
2013) and meaning that the texts are paraphrases.
All semantically equivalent questions are some-
what semantically similar, but semantic equiva-
lence of questions defined here does not corre-
spond to the highest value of the textual semantic
similarity for the same reasons these questions are
not always paraphrases.
</bodyText>
<sectionHeader confidence="0.999191" genericHeader="method">
4 Neural Network Architecture
</sectionHeader>
<bodyText confidence="0.999804">
In this section, we present our neural-network
strategy for detecting semantically equivalent
questions.
</bodyText>
<subsectionHeader confidence="0.99369">
4.1 Feed Forward Processing
</subsectionHeader>
<bodyText confidence="0.9999941">
As detailed in Figure 1, the input for the network
is tokenized text strings of the two questions. In
the first step, the CNN transforms words into real-
valued feature vectors, also known as word em-
beddings or word representations. Next, a convo-
lutional layer is used to construct two distributed
vector representations rqj and rq2, one for each in-
put question. Finally, the CNN computes a simi-
larity score between rqj and rq2. Pairs of questions
with similarity above a threshold, defined based on
</bodyText>
<page confidence="0.997074">
125
</page>
<figureCaption confidence="0.996969">
Figure 1: Convolutional neural network for se-
mantically equivalent questions detection.
</figureCaption>
<bodyText confidence="0.72972">
a heldout set, are considered duplicates.
</bodyText>
<subsectionHeader confidence="0.989984">
4.2 Word Representations
</subsectionHeader>
<bodyText confidence="0.998921875">
The first layer of the network transforms words
into representations that capture syntactic and
semantic information about the words. Given
a question consisting of N words q =
{w1, w2, ..., wN}, every word wn is converted into
a real-valued vector rwn. Therefore, for each ques-
tion, the input to the next NN layer is a sequence
of real-valued vectors qemb = {rw1, rw2, ..., rwN}
Word representations are encoded by column
vectors in an embedding matrix W0 ∈ Rdx|V
|,
where V is a fixed-sized vocabulary. Each column
Wi0 ∈ Rd corresponds to the word embedding of
the i-th word in the vocabulary. We transform a
word w into its word embedding rw by using the
matrix-vector product:
</bodyText>
<equation confidence="0.969625">
rw = W0vw (1)
</equation>
<bodyText confidence="0.997883">
where vw is a vector of size |V  |which has value
1 at index w and zero in all other positions. The
matrix W0 is a parameter to be learned, and the
size of the word embedding d is a hyper-parameter
to be chosen by the user.
</bodyText>
<subsectionHeader confidence="0.999876">
4.3 Question Representation and Scoring
</subsectionHeader>
<bodyText confidence="0.99861344">
The next step in the CNN consists in creating dis-
tributed vectors from word embeddings represen-
tations of the input questions. To perfom this task,
the CNN must deal with two main challenges: dif-
ferent questions can have different sizes; and im-
portant information can appear at any position in
the question. The convolutional approach (Waibel
et al., 1989) is a natural choice to tackle these
challenges. In recent work, convolutional ap-
proaches have been used to solve similar prob-
lems when creating representations for text seg-
ments of different sizes (dos Santos and Gatti,
2014) and character-level representations of words
of different sizes (dos Santos and Zadrozny, 2014).
Here, we use a convolutional layer to compute
the question-wide distributed vector representa-
tions rq1 and rq2. For each question, the convo-
lutional layer first produces local features around
each word in the question. Then, it combines these
local features using a sum operation to create a
fixed-sized feature vector (representation) for the
question.
Given a question q1, the convolutional layer
applies a matrix-vector operation to each win-
dow of size k of successive windows in qemb
</bodyText>
<equation confidence="0.6571244">
1 =
{rw1, rw2, ..., rwN}. Let us define the vector zn ∈
Rdk as the concatenation of a sequence of k word
embeddings, centralized in the n-th word:6
zn = (rwn−(k−1)/2, ..., rwn+(k−1)/2)T
</equation>
<bodyText confidence="0.993824">
The convolutional layer computes the j-th ele-
ment of the vector rq1 ∈ Rclu as follows:
</bodyText>
<equation confidence="0.976393666666667">
[rq1Ij = f �
(1n&lt;&lt;N [f (W1zn + b1)] (2)
j
</equation>
<bodyText confidence="0.9999591875">
where W1 ∈ Rcluxdk is the weight matrix of the
convolutional layer and f is the hyperbolic tangent
function. The same matrix is used to extract local
features around each word window of the given
question. The global fixed-sized feature vector for
the question is obtained by using the sum over all
word windows.7 Matrix W1 and vector b1 are pa-
rameters to be learned. The number of convolu-
tional units clu (which corresponds to the size of
the question representation), and the size of the
word context window k are hyper-parameters to
be chosen by the user.
Given rq1 and rq2, the representations for the
input pair of questions (q1, q2), the last layer of the
CNN computes a similarity score between q1 and
q2. In our experiments we use the cosine similarity
</bodyText>
<equation confidence="0.615313">
s(q1, q2) =
</equation>
<footnote confidence="0.914646166666667">
6 Words with indices outside of the sentence boundaries
use a common padding embedding.
7 Using max operation instead of sum produces very
similar results.
~re1.~re2
II~re1IIII~re2II.
</footnote>
<page confidence="0.953421">
126
</page>
<subsectionHeader confidence="0.98474">
4.4 Training Procedure
</subsectionHeader>
<bodyText confidence="0.999871555555555">
Our network is trained by minimizing the mean-
squared error over the training set D. Given a
question pair (q1, q2), the network with param-
eter set θ computes a similarity score sθ(q1,q2).
Let y(q1,q2) be the correct label of the pair, where
its possible values are 1 (equivalent questions) or
0 (not equivalent questions). We use stochastic
gradient descent (SGD) to minimize the mean-
squared error with respect to θ:
</bodyText>
<equation confidence="0.9984645">
2 (y − sθ(x))2
1 (3)
</equation>
<bodyText confidence="0.99967225">
where x = (q1, q2) corresponds to a question pair
in the training set D and y represents its respective
label y(q1,q2).
We use the backpropagation algorithm to com-
pute gradients of the network. In our experiments,
we implement the CNN architecture and the back-
propagation algorithm using Theano (Bergstra et
al., 2010).
</bodyText>
<sectionHeader confidence="0.998576" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.877787">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999912818181819">
In our experiments we use data from the
Ask Ubuntu Community Questions and Answers
(Q&amp;A) site.8 Ask Ubuntu is a community for
Ubuntu users and developers, and it is part of the
Stack Exchange9 Q&amp;A communities. The users
of these communities can ask and answer ques-
tions, and vote up and down both questions and
answers. Users with high reputation become mod-
erators and can label a new question as a dupli-
cate to an existing question.10 Usually it takes five
votes from different moderators to close a question
or to mark it as a duplicate.
We use the Ask Ubuntu data dump provided in
May 2014. We extract all question pairs linked as
duplicates. The data dump we use contains 15277
such pairs. For our experiments, we randomly se-
lect a training set of 24K pairs, a test set of 6K and
a validation set of 1K, making sure there are no
overlaps between the sets. Half of each set con-
tains pairs of semantically equivalent questions
(positive pairs) and half are pairs of questions that
are not semantically equivalent. The latter pairs
</bodyText>
<footnote confidence="0.9781748">
8http://askubuntu.com/
9http://stackexchange.com
10 More information about Stack Exchange communities
could be found here: http://stackexchange.com/
tour
</footnote>
<bodyText confidence="0.99991625">
are randomly generated from the corpus. The data
was tokenized with NLTK (Bird et al., 2009), and
all links were replaced by a unique string.
For the experiments on a different domain (see
Section 6.4) we use the Meta Stack Exchange11
data dump provided in September 2014. Meta
Stack Exchange (Meta) is used to discuss the
Stack Exchange community itself. People ask
questions about the rules, features and possible
bugs. The data dump we use contains 67746 ques-
tions, where 19456 are marked as duplicates. For
the experiments on this data set, we select random
balanced disjoint sets of 20K pairs for training, 1K
for validation and 4K for testing. We prepare the
data in exactly the same manner as the Ask Ubuntu
data.
</bodyText>
<subsectionHeader confidence="0.994519">
5.2 Baselines
</subsectionHeader>
<bodyText confidence="0.9999654">
We explore three main baselines: a method based
on the Jaccard coefficient which was reported to
provide high accuracy for the task of duplicate de-
tection (Wu et al., 2011), a Support Vector Ma-
chines (SVM) classifier (Cortes and Vapnik, 1995)
and the combination of the two.
For the first baseline, documents are first repre-
sented as sets of shingles of lengths from one to
four, and then the Jaccard coefficient for a pair of
documents is calculated as follows:
</bodyText>
<equation confidence="0.996211">
S(d1) ∩ S(d2)
J(S(d1), S(d2)) =
S(d1) U S(d2),
</equation>
<bodyText confidence="0.999613764705882">
where S(di) is the set of shingles generated from
the ith document. High values of the Jaccard co-
efficient denote high similarity between the docu-
ments. If the value exceeds a threshold T, the doc-
uments are considered semantically equivalent. In
this case, the training data is used to select the op-
timal threshold T.
For the SVM baseline, we represent documents
with n-grams of length up to four. For each pair
of questions and each n-gram we generate three
features: (1) if the n-gram is present in the first
question; (2) if the n-gram is present in the second
question; (3) the overall normalized count of the
n-gram in the two questions. We use the RBF ker-
nel and perform grid search to optimize the val-
ues of C and γ parameters. We use a frequency
threshold12 to reduce the number of features. The
</bodyText>
<page confidence="0.730212">
11 meta.stackexchange.com
</page>
<figure confidence="0.667464">
12 Several values (2, 5, 35 and 100) were tried with cross-
validation, the threshold with value 5 was selected
�θ H
(x,y)∈D
</figure>
<page confidence="0.984344">
127
</page>
<bodyText confidence="0.999260142857143">
implementation provided by LibSVM (Chang and
Lin, 2011) is used.
In order to combine the two baselines, for a pair
of questions we calculate the values of the Jaccard
coefficient with shingles size up to four, and then
add these values as additional features used by the
SVM classifier.
</bodyText>
<subsectionHeader confidence="0.998278">
5.3 Word Embeddings
</subsectionHeader>
<bodyText confidence="0.999764620689655">
The word embeddings used in our experiments are
initialized by means of unsupervised pre-training.
We perform pre-training using the skip-gram NN
architecture (Mikolov et al., 2013) available in the
word2vec13 tool. Two different corpora are used
to train word embeddings for most of the experi-
ments: the English Wikipedia and the Ask Ubuntu
community data. The experiments presented in
Section 6.4 also use word embeddings trained on
the Meta Stack Exchange community data.
In the experiments with the English Wikipedia
word embeddings, we use the embeddings pre-
viously produced by dos Santos &amp; Gatti (2014).
They have used the December 2013 snapshot of
the English Wikipedia corpus to obtain word em-
beddings with word2vec.
In the experiments with Ask Ubuntu and Meta
Stack Exchange word embeddings, we use the
Stack Exchange data dump provided in May 2014
to train word2vec. Three main steps are used to
process all questions and answers from these Stack
Exchange dumps: (1) tokenization of the text us-
ing the NLTK tokenizer; (2) image removal, URL
replacement and prefixing/removal of the code if
necessary (see Section 6.1 for more information);
(3) lowercasing of all tokens. The resulting cor-
pora contains about 121 million and 19 million to-
kens for Ask Ubuntu and Meta Stack Exchange,
respectively.
</bodyText>
<sectionHeader confidence="0.999599" genericHeader="method">
6 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.99998">
6.1 Comparison with Baselines
</subsectionHeader>
<bodyText confidence="0.998696222222222">
Ask Ubuntu community gives users an opportu-
nity to format parts of their posts as code by us-
ing code tags (an example is in italic in Table 1).
It includes not only programming code, but com-
mands, paths to directories, names of packages, er-
ror messages and links. Around 30% of all posts
in the data dump contain code tags. Since the rules
for code formatting are not well defined, it was not
clear if a learning algorithm would benefit from
</bodyText>
<footnote confidence="0.526043">
13 http://code.google.com/p/word2vec/
</footnote>
<table confidence="0.917252">
System Valid. Acc. Test Acc.
SVM + shingles 85.5 82.4
CNN + Askubuntu 93.4 92.9
</table>
<tableCaption confidence="0.889771">
Table 3: CNN and SVM accuracy on the valida-
tion and the test set using the full training set.
</tableCaption>
<bodyText confidence="0.9997026">
including it or not. Therefore, for each algorithm
we tested three different approaches to handling
code: keeping it as text; removing it; and prefix-
ing it with a special tag. The latter is done in order
to distinguish between the same term used within
text or within code or a command (e.g., a for as
a preposition and a for in a for loop). When cre-
ating the word embeddings, the same approach to
the code as for the training data was followed.
The 1K example validation set is used to tune
the hyper-parameters of the algorithms. In order
to speed up computations, we perform our initial
experiments using a 4K examples balanced subset
of the training set. The best validation accuracies
are reported in Table 2.
We test the shingling-based approach with dif-
ferent shingle sizes. As Table 2 indicates, the
accuracy decreases with the increase of the shin-
gle size. The fact that much better accuracy is
achieved when comparing questions based on sim-
ple word overlap (shingle size 1), suggests that se-
mantically equivalent questions are not duplicates
but rather have topical similarity. The SVM base-
line performs well only when combined with the
shingling approach by using the values of the Jac-
card coefficient for shingle size up to four as ad-
ditional features. A possible reason for this is that
n-gram representations do not capture enough in-
formation about semantic equivalence. The CNN
with word embeddings outperforms the baselines
by a significant margin.
The results presented in Table 2 indicate that the
algorithms do not benefit from including the code.
This is probably because the code tags are not al-
ways used appropriately and some code examples
include long error messages, which make the user
generated data even more noisy. Therefore, in the
following experiments the code is removed.
The validation accuracy and the test accuracy
using the full 24K training set is presented in Ta-
ble 3. The SVM with four additional shingling
features is found best among the baselines (see Ta-
ble 2) and is used as a baseline in this experiment.
Again, the CNN with word embeddings outper-
forms the best baseline by a significant margin.
</bodyText>
<page confidence="0.992202">
128
</page>
<table confidence="0.999620526315789">
Algorithm Features Code Best validation acc. Optimal hyper-parameters
SVM-RBF binary + freq. kept 66.2 C=8.0, -y 3.05e-05
SVM-RBF binary + freq. removed 66.53 C=2.0, -y 1.2e-04
SVM-RBF binary + freq. prefixed 66.53 C=8.0, -y 3.05e-05
Shingling (size 1) - kept 72.35 -
Shingling (size 1) - removed 72.65 -
Shingling (size 1) - prefixed 70.94 -
Shingling (size 2) - kept 69.24 -
Shingling (size 2) - removed 66.83 -
Shingling (size 2) - prefixed 67.74 -
Shingling (size 3) - kept 65.23 -
Shingling (size 3) - removed 62.93 -
Shingling (size 3) - prefixed 64.43 -
SVM-RBF binary + freq. + shingles kept 74.0 C=32.0, -y 3.05e-05
SVM-RBF binary + freq. + shingles removed 77.4 C=32.0, -y 3.05e-05
SVM-RBF binary + freq. + shingles prefixed 73.6 C=32.0, -y 3.05e-05
CNN Askubuntu word vectors kept 91.3
CNN Askubuntu word vectors removed 92.4 d=200, k=3, cl„=300, a=0.005
CNN Askubuntu word vectors prefixed 91.4
</table>
<tableCaption confidence="0.985366">
Table 2: Validation Accuracy and best parameters for the baselines and the Convolutional Neural Net-
work.
</tableCaption>
<subsectionHeader confidence="0.993512">
6.2 Impact of Domain-Specific Word
Embeddings
</subsectionHeader>
<bodyText confidence="0.999958766666667">
We perform two experiments to evaluate the im-
pact of the word embeddings on the CNN accu-
racy. In the first experiment, we gradually increase
the dimensionality of word embeddings from 50
to 400. The results are presented in Figure 2.
The vertical axis corresponds to validation accu-
racy and the horizontal axis represents the training
time in epochs. As has been shown in (Mikolov et
al., 2013), word embeddings of higher dimension-
ality trained on a large enough data set capture se-
mantic information better than those of smaller di-
mensionality. The experimental results presented
in Figure 2 correspond to these findings: we can
see improvements in the neural network perfor-
mance when increasing the word embeddings di-
mensionality from 50 to 100 and from 100 to 200.
However, the Ask Ubuntu data set containing ap-
proximately 121M tokens is not big enough for an
improvement when increasing the dimensionality
from 200 to 400.
In the second experiment, we evaluate the im-
pact of in-domain word embeddings on the net-
work’s performance. We obtain word embeddings
trained on two different corpora: Ask Ubuntu
community data and English Wikipedia (see Sec-
tion 5.3). Both word embeddings have 200 dimen-
sions. The results presented in Table 4 show that
training on in-domain data is more beneficial for
the network, even though the corpus used to cre-
ate word embeddings is much smaller.
</bodyText>
<figureCaption confidence="0.9742645">
Figure 2: CNN accuracy depending on the size of
word embeddings
</figureCaption>
<table confidence="0.999247">
Word Embeddings Num.tokens Valid.Acc.
Wikipedia ≈1.6B 85.5
AskUbuntu ≈121M 92.4
</table>
<tableCaption confidence="0.995772">
Table 4: Validation Accuracy of the CNN with
word embeddings pre-trained on different corpora.
</tableCaption>
<subsectionHeader confidence="0.999377">
6.3 Impact of Training Set Size
</subsectionHeader>
<bodyText confidence="0.9999825">
In order to measure the impact of the training set
size, we perform experiments using subsets of the
training data, starting from 100 question pairs and
gradually increasing the size to the full 24K train-
ing set.14 Figure 3 compares the learning curves
for the SVM baseline (with parameters and fea-
tures described in Section 6.1) and for the CNN
with word embeddings trained on Ask Ubuntu and
English Wikipedia. The vertical axis corresponds
to the validation accuracy, and the horizontal axis
represents the training set size. As Figure 3 indi-
cates, increasing the size of the training set pro-
</bodyText>
<footnote confidence="0.85647">
14 We use sets of 100, 1000, 4000, 12000 and 24000 ques-
tion pairs.
</footnote>
<page confidence="0.997162">
129
</page>
<bodyText confidence="0.99948125">
vides improvements. Nonetheless, the difference
in accuracy when training with the full 24K train-
ing set and 4K subset is about 9% for SVM and
only about 1% for the CNN. This difference is
small for both word embeddings pre-trained on
Ask Ubuntu and Wikipedia but, the in-domain
word embeddings provide better performance in-
dependently of the training set size.
</bodyText>
<figureCaption confidence="0.8521065">
Figure 3: Validation accuracy for the baseline and
the CNN depending on the size of training set.
</figureCaption>
<subsectionHeader confidence="0.979265">
6.4 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999982529411765">
Muthmann and Petrova (2014) report that the
Meta Stack Exchange Community15 is one of the
hardest for finding semantically equivalent ques-
tions.
We perform the same experiments described in
previous sections using the Meta data set. In Ta-
ble 5, we can see that the CNN accuracy on Meta
test data (92.68%) is similar to the one for Ask
Ubuntu community on test data (92.4%) (see Ta-
ble 3).
Also, in Table 5, we show results of a domain
adaptation experiment in which we do not use
training data from the Meta forum. In this case,
the CNN is trained using Ask Ubuntu data only.
The numbers show that even in this case using in-
domain word embeddings helps to achieve rela-
tively high accuracy: 83.35% on the test set.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="method">
7 Error Analysis
</sectionHeader>
<bodyText confidence="0.999918428571428">
As we have expected, the CNN with in-domain
word vectors outperforms the vocabulary-based
baselines in identifying semantically equivalent
questions that are too different in terms of vo-
cabulary. The CNN is also better at distinguish-
ing questions with similar vocabulary but differ-
ent meanings. For example, the question pair, (q1)
</bodyText>
<footnote confidence="0.790476">
15 http://meta.stackexchange.com/
</footnote>
<table confidence="0.999890857142857">
Train.Data Size Word Vect. Val.Acc. Test.Acc.
META 4K META 91.1 89.97
META 4K Wikipedia 86.9 86.27
META 20K META 92.8 92.68
META 20K Wikipedia 90.6 90.52
AskUbuntu 24K META 83.9 83.35
AskUbuntu 24K AskUbuntu 76.8 80.0
</table>
<tableCaption confidence="0.993829">
Table 5: Convolutional Neural Network Accuracy
tested on Meta Stack Exchange community data.
</tableCaption>
<bodyText confidence="0.989967090909091">
How can I install Ubuntu without removing Win-
dows? and (q2) How do I upgrade from x86 to x64
without losing settings?16 is erroneously classi-
fied as a positive pair by the SVM, while the CNN
classifies it correctly as a negative pair.
There are some cases where both CNN and
SVM fail to identify semantic equivalence. Some
of these cases include questions where essen-
tial information is presented as an image, e.g.,
a screenshot, which was removed during prepro-
cessing.17
</bodyText>
<sectionHeader confidence="0.997578" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999980181818182">
In this paper, we propose a method for identify-
ing semantically equivalent questions based on a
convolutional neural network. We experimentally
show that the proposed CNN achieves very high
accuracy especially when the word embeddings
are pre-trained on in-domain data. The perfor-
mance of an SVM-based approach to this task was
shown to depend highly on the size of the training
data. In contrast, the CNN with in-domain word
embeddings provides very high performance even
with limited training data. Furthermore, experi-
ments on a different domain have demonstrated
that the neural network achieves high accuracy in-
dependently of the domain.
The next step in our research is building a sys-
tem for retrieval of semantically equivalent ques-
tions. In particular, given a corpus and a question,
the task is to find all questions that are semanti-
cally equivalent to the given one in the corpus. We
believe that a CNN architecture similar to the one
proposed in this paper might be a good fit to tackle
this problem.
</bodyText>
<sectionHeader confidence="0.998838" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9608475">
The work of Dasha Bogdanova is supported by
Science Foundation Ireland through the CNGL
</bodyText>
<footnote confidence="0.62435">
16 Due to space constraints we only report the titles of the
questions
17 For instance, http://askubuntu.com/questions/450843
</footnote>
<page confidence="0.995555">
130
</page>
<bodyText confidence="0.99086825">
Programme (Grant 12/CE/I2267) in the ADAPT
Centre (www.adaptcentre.ie) at Dublin City Uni-
versity. Her contributions were made during an
internship at IBM Research.
</bodyText>
<sectionHeader confidence="0.99787" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999815650485437">
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 1, pages 32–43, Atlanta, Geor-
gia, USA, June.
Omar Alonso, Dennis Fetterly, and Mark Manasse.
2013. Duplicate news story detection revisited. In
Rafael E. Banchs, Fabrizio Silvestri, Tie-Yan Liu,
Min Zhang, Sheng Gao, and Jun Lang, editors, In-
formation Retrieval Technology, volume 8281 of
Lecture Notes in Computer Science, pages 203–214.
Springer Berlin Heidelberg.
James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy).
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python: Analyz-
ing Text with the Natural Language Toolkit.
A. Broder. 1997. On the resemblance and contain-
ment of documents. In Proceedings of the Com-
pression and Complexity of Sequences 1997, SE-
QUENCES’97, Washington, DC, USA. IEEE Com-
puter Society.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1–27:27. Software available at http://
www.csie.ntu.edu.tw/˜cjlin/libsvm.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, Volume 20(3),
pages 273–297.
C´ıcero Nogueira dos Santos and Ma´ıra Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In Proceedings of the 25th In-
ternational Conference on Computational Linguis-
tics (COLING), Dublin, Ireland.
C´ıcero Nogueira dos Santos and Bianca Zadrozny.
2014. Learning character-level representations for
part-of-speech tagging. In Proceedings of the
31st International Conference on Machine Learning
(ICML), JMLR: W&amp;CP volume 32, Beijing, China.
Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences. In
Advances in Neural Information Processing Systems
27: Annual Conference on Neural Information Pro-
cessing Systems 2014, pages 2042–2050, Montreal,
Quebec, Canada.
Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods for Natural Lan-
guage Processing, pages 1746–1751, Doha, Qatar.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 182–
190, Montr´eal, Canada.
Gurmeet Singh Manku, Arvind Jain, and Anish
Das Sarma. 2007. Detecting near-duplicates for
web crawling. In Proceedings of the 16th Interna-
tional Conference on World Wide Web, WWW ’07,
pages 141–150, New York, NY, USA.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word repre-
sentations in vector space. In Proceedings of Inter-
national Conference on Learning Representations,
ICLR 2013, Scottsdale, AZ, USA.
Klemens Muthmann and Alina Petrova. 2014. An
Automatic Approach for Identifying Topical Near-
Duplicate Relations between Questions from Social
Media Q/A . In Proceedings of Web-scale Classifi-
cation: Classifying Big Data from the Web WSCBD
2014.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In John Shawe-
Taylor, Richard S. Zemel, Peter L. Bartlett, Fer-
nando C. N. Pereira, and Kilian Q. Weinberger, ed-
itors, Advances in Neural Information Processing
Systems 24, pages 801–809.
Alexander Waibel, Toshiyuki Hanazawa, Geoffrey
Hinton, Kiyohiro Shikano, and Kevin J. Lang. 1989.
Phoneme recognition using time-delay neural net-
works. IEEE Transactions on Acoustics, Speech and
Signal Processing, Volume 37(3), pages 328–339.
Yan Wu, Qi Zhang, and Xuanjing Huang. 2011. Ef-
ficient near-duplicate detection for q&amp;a forum. In
Proceedings of 5th International Joint Conference
on Natural Language Processing, pages 1001–1009,
Chiang Mai, Thailand.
Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation ques-
tion answering. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 643–648.
</reference>
<page confidence="0.998271">
131
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.896380">
<title confidence="0.9995035">Detecting Semantically Equivalent in Online User Forums</title>
<author confidence="0.998211">Cicero dos Luciano Bianca</author>
<affiliation confidence="0.990589">centre, School of Computing, Dublin City University, Dublin,</affiliation>
<address confidence="0.919742">Research, 138/146 Av. Pasteur, Rio de Janeiro,</address>
<abstract confidence="0.99934052">Two questions asking the same thing could be too different in terms of vocabulary and syntactic structure, which makes identifying their semantic equivalence challenging. This study aims to detect semantically equivalent questions in online user forums. We perform an extensive number of experiments using data from two different Stack Exchange forums. We compare standard machine learning methods such as Support Vector Machines (SVM) with a convolutional neural network (CNN). The proposed CNN generates distributed vector representations for pairs of questions and scores them using a similarity metric. We evaluate in-domain word embeddings versus the ones trained with Wikipedia, estimate the impact of the training set size, and evaluate some aspects of domain adaptation. Our experimental results show that the convolutional neural network with in-domain word embeddings achieves high performance even with limited training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>sem 2013 shared task: Semantic textual similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<volume>1</volume>
<pages>32--43</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="12016" citStr="Agirre et al., 2013" startWordPosition="1804" endWordPosition="1807">so semantically equivalent. However, many semantically equivalent questions are not paraphrases. The questions shown in Table 1 significantly differ in the details they provide, and thus, could not be considered as having the same meaning. State-ofthe-art approaches to paraphrase identification include using Machine Translation evaluation metrics (Madnani et al., 2012) and Deep Learning techniques (Socher et al., 2011). Textual Semantic Similarity is the task of measuring the degree of semantic similarity between two texts, usually on a graded scale from 0 to 5, with 5 being the most similar (Agirre et al., 2013) and meaning that the texts are paraphrases. All semantically equivalent questions are somewhat semantically similar, but semantic equivalence of questions defined here does not correspond to the highest value of the textual semantic similarity for the same reasons these questions are not always paraphrases. 4 Neural Network Architecture In this section, we present our neural-network strategy for detecting semantically equivalent questions. 4.1 Feed Forward Processing As detailed in Figure 1, the input for the network is tokenized text strings of the two questions. In the first step, the CNN t</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *sem 2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1, pages 32–43, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar Alonso</author>
<author>Dennis Fetterly</author>
<author>Mark Manasse</author>
</authors>
<title>Duplicate news story detection revisited.</title>
<date>2013</date>
<booktitle>Information Retrieval Technology,</booktitle>
<volume>8281</volume>
<pages>203--214</pages>
<editor>In Rafael E. Banchs, Fabrizio Silvestri, Tie-Yan Liu, Min Zhang, Sheng Gao, and Jun Lang, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="10812" citStr="Alonso et al., 2013" startWordPosition="1609" endWordPosition="1612"> equivalent questions. These tasks include near-duplicate detection, paraphrase identification and textual semantic similarity estimation. In what follows, we outline the differences between these tasks and the one addressed in this work. Duplicate and Near-Duplicate Detection aims to detect exact copies or almost exact copies of the same document in corpora. Duplicate detection is an important component of systems for Web crawling and Web search, where it is important to identify redundant data in large corpora. Common techniques to detect duplicate documents include shingling (Broder, 1997; Alonso et al., 2013) and fingerprinting (Manku et al., 2007). State-of-theart work also focuses on the efficiency issues of the task (Wu et al., 2011). It is worth noting that even though all duplicate and near-duplicate questions are also semantically equivalent, the reverse is not true. Semantically equivalent questions could have small or no word overlap (see Table 1 for an example), and thus, are not duplicates. Paraphrase Identification is the task of examining two sentences and determining whether they have the same meaning (Socher et al., 2011). If two questions are paraphrases, they are also semantically </context>
</contexts>
<marker>Alonso, Fetterly, Manasse, 2013</marker>
<rawString>Omar Alonso, Dennis Fetterly, and Mark Manasse. 2013. Duplicate news story detection revisited. In Rafael E. Banchs, Fabrizio Silvestri, Tie-Yan Liu, Min Zhang, Sheng Gao, and Jun Lang, editors, Information Retrieval Technology, volume 8281 of Lecture Notes in Computer Science, pages 203–214. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Olivier Breuleux</author>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>Guillaume Desjardins</author>
<author>Joseph Turian</author>
<author>David Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: a CPU and GPU math expression compiler.</title>
<date>2010</date>
<booktitle>In Proceedings of the Python for Scientific Computing Conference (SciPy).</booktitle>
<contexts>
<context position="17327" citStr="Bergstra et al., 2010" startWordPosition="2698" endWordPosition="2701">arameter set θ computes a similarity score sθ(q1,q2). Let y(q1,q2) be the correct label of the pair, where its possible values are 1 (equivalent questions) or 0 (not equivalent questions). We use stochastic gradient descent (SGD) to minimize the meansquared error with respect to θ: 2 (y − sθ(x))2 1 (3) where x = (q1, q2) corresponds to a question pair in the training set D and y represents its respective label y(q1,q2). We use the backpropagation algorithm to compute gradients of the network. In our experiments, we implement the CNN architecture and the backpropagation algorithm using Theano (Bergstra et al., 2010). 5 Experimental Setup 5.1 Data In our experiments we use data from the Ask Ubuntu Community Questions and Answers (Q&amp;A) site.8 Ask Ubuntu is a community for Ubuntu users and developers, and it is part of the Stack Exchange9 Q&amp;A communities. The users of these communities can ask and answer questions, and vote up and down both questions and answers. Users with high reputation become moderators and can label a new question as a duplicate to an existing question.10 Usually it takes five votes from different moderators to close a question or to mark it as a duplicate. We use the Ask Ubuntu data d</context>
</contexts>
<marker>Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, Bengio, 2010</marker>
<rawString>James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<title>Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit.</title>
<date>2009</date>
<contexts>
<context position="18638" citStr="Bird et al., 2009" startWordPosition="2918" endWordPosition="2921">e use contains 15277 such pairs. For our experiments, we randomly select a training set of 24K pairs, a test set of 6K and a validation set of 1K, making sure there are no overlaps between the sets. Half of each set contains pairs of semantically equivalent questions (positive pairs) and half are pairs of questions that are not semantically equivalent. The latter pairs 8http://askubuntu.com/ 9http://stackexchange.com 10 More information about Stack Exchange communities could be found here: http://stackexchange.com/ tour are randomly generated from the corpus. The data was tokenized with NLTK (Bird et al., 2009), and all links were replaced by a unique string. For the experiments on a different domain (see Section 6.4) we use the Meta Stack Exchange11 data dump provided in September 2014. Meta Stack Exchange (Meta) is used to discuss the Stack Exchange community itself. People ask questions about the rules, features and possible bugs. The data dump we use contains 67746 questions, where 19456 are marked as duplicates. For the experiments on this data set, we select random balanced disjoint sets of 20K pairs for training, 1K for validation and 4K for testing. We prepare the data in exactly the same ma</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Broder</author>
</authors>
<title>On the resemblance and containment of documents.</title>
<date>1997</date>
<booktitle>In Proceedings of the Compression and Complexity of Sequences 1997, SEQUENCES’97,</booktitle>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="2794" citStr="Broder, 1997" startWordPosition="425" endWordPosition="426">er if they are asking it a different way.4 In this study we define two questions as semantically equivalent if they can be adequately answered by the exact same answer. Table 1 presents an example of a pair of such questions from Ask Ubuntu forum. Detecting semantically equivalent questions is a very difficult task due to two main factors: (1) the same question can be rephrased in many different ways; and (2) two questions could be asking different things but look for the same solution. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. In this paper, we propose a convolutional neural network architecture to detect semantically equivalent questions. The proposed CNN first transforms words into word embeddings (Mikolov et al., 2013), using a large collection of unlabeled data, and then applies a convolutional network to build distributed vector representations for pairs of questions. Finally, it scores the questions using a similarity metric. Pairs of questions with similarity above a threshold, defined based on a heldout set, are</context>
<context position="5366" citStr="Broder, 1997" startWordPosition="799" endWordPosition="800"> this happens whenever I try to download anything like games and also i can’t watch videoss it’s looking for plug ins but it doesn’t find them i hate this [sic!] Link:http://askubuntu.com/questions/364350 http://askubuntu.com/questions/988 Possible Answer (Shortened version):.exe files are not binary-compatible with Ubuntu. There are, however, compatibility layers for Linux, such as Wine, that are capable of running .exe. Table 1: An example of semantically equivalent questions from Ask Ubuntu community. Vector Machines (Cortes and Vapnik, 1995)) and a duplicate detection approach (shingling (Broder, 1997)). The results show CNN outperforms the baselines by a large margin. We also investigate the impact of different word embeddings by analyzing the performance of the network with: (1) word embeddings pre-trained on in-domain data and all of the English Wikipedia; (2) word vectors of different dimensionalities; (3) training sets of different sizes; and (4) out-ofdomain training data and in-domain word embeddings. The numbers show that: (1) word embeddings pre-trained on domain-specific data achieve very high performance; (2) bigger word embeddings obtain higher accuracy; (3) in-domain word embed</context>
<context position="10790" citStr="Broder, 1997" startWordPosition="1607" endWordPosition="1608">g semantically equivalent questions. These tasks include near-duplicate detection, paraphrase identification and textual semantic similarity estimation. In what follows, we outline the differences between these tasks and the one addressed in this work. Duplicate and Near-Duplicate Detection aims to detect exact copies or almost exact copies of the same document in corpora. Duplicate detection is an important component of systems for Web crawling and Web search, where it is important to identify redundant data in large corpora. Common techniques to detect duplicate documents include shingling (Broder, 1997; Alonso et al., 2013) and fingerprinting (Manku et al., 2007). State-of-theart work also focuses on the efficiency issues of the task (Wu et al., 2011). It is worth noting that even though all duplicate and near-duplicate questions are also semantically equivalent, the reverse is not true. Semantically equivalent questions could have small or no word overlap (see Table 1 for an example), and thus, are not duplicates. Paraphrase Identification is the task of examining two sentences and determining whether they have the same meaning (Socher et al., 2011). If two questions are paraphrases, they </context>
</contexts>
<marker>Broder, 1997</marker>
<rawString>A. Broder. 1997. On the resemblance and containment of documents. In Proceedings of the Compression and Complexity of Sequences 1997, SEQUENCES’97, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="20803" citStr="Chang and Lin, 2011" startWordPosition="3296" endWordPosition="3299">four. For each pair of questions and each n-gram we generate three features: (1) if the n-gram is present in the first question; (2) if the n-gram is present in the second question; (3) the overall normalized count of the n-gram in the two questions. We use the RBF kernel and perform grid search to optimize the values of C and γ parameters. We use a frequency threshold12 to reduce the number of features. The 11 meta.stackexchange.com 12 Several values (2, 5, 35 and 100) were tried with crossvalidation, the threshold with value 5 was selected �θ H (x,y)∈D 127 implementation provided by LibSVM (Chang and Lin, 2011) is used. In order to combine the two baselines, for a pair of questions we calculate the values of the Jaccard coefficient with shingles size up to four, and then add these values as additional features used by the SVM classifier. 5.3 Word Embeddings The word embeddings used in our experiments are initialized by means of unsupervised pre-training. We perform pre-training using the skip-gram NN architecture (Mikolov et al., 2013) available in the word2vec13 tool. Two different corpora are used to train word embeddings for most of the experiments: the English Wikipedia and the Ask Ubuntu commun</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27. Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Supportvector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<volume>20</volume>
<issue>3</issue>
<pages>273--297</pages>
<contexts>
<context position="5304" citStr="Cortes and Vapnik, 1995" startWordPosition="789" endWordPosition="792">ip, and cannot find /home/maria/Downloads/SkypeSetup-aoc-jd.exe.ZIP pe... this happens whenever I try to download anything like games and also i can’t watch videoss it’s looking for plug ins but it doesn’t find them i hate this [sic!] Link:http://askubuntu.com/questions/364350 http://askubuntu.com/questions/988 Possible Answer (Shortened version):.exe files are not binary-compatible with Ubuntu. There are, however, compatibility layers for Linux, such as Wine, that are capable of running .exe. Table 1: An example of semantically equivalent questions from Ask Ubuntu community. Vector Machines (Cortes and Vapnik, 1995)) and a duplicate detection approach (shingling (Broder, 1997)). The results show CNN outperforms the baselines by a large margin. We also investigate the impact of different word embeddings by analyzing the performance of the network with: (1) word embeddings pre-trained on in-domain data and all of the English Wikipedia; (2) word vectors of different dimensionalities; (3) training sets of different sizes; and (4) out-ofdomain training data and in-domain word embeddings. The numbers show that: (1) word embeddings pre-trained on domain-specific data achieve very high performance; (2) bigger wo</context>
<context position="19523" citStr="Cortes and Vapnik, 1995" startWordPosition="3069" endWordPosition="3072">f. People ask questions about the rules, features and possible bugs. The data dump we use contains 67746 questions, where 19456 are marked as duplicates. For the experiments on this data set, we select random balanced disjoint sets of 20K pairs for training, 1K for validation and 4K for testing. We prepare the data in exactly the same manner as the Ask Ubuntu data. 5.2 Baselines We explore three main baselines: a method based on the Jaccard coefficient which was reported to provide high accuracy for the task of duplicate detection (Wu et al., 2011), a Support Vector Machines (SVM) classifier (Cortes and Vapnik, 1995) and the combination of the two. For the first baseline, documents are first represented as sets of shingles of lengths from one to four, and then the Jaccard coefficient for a pair of documents is calculated as follows: S(d1) ∩ S(d2) J(S(d1), S(d2)) = S(d1) U S(d2), where S(di) is the set of shingles generated from the ith document. High values of the Jaccard coefficient denote high similarity between the documents. If the value exceeds a threshold T, the documents are considered semantically equivalent. In this case, the training data is used to select the optimal threshold T. For the SVM ba</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine Learning, Volume 20(3), pages 273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C´ıcero Nogueira dos Santos</author>
<author>Ma´ıra Gatti</author>
</authors>
<title>Deep convolutional neural networks for sentiment analysis of short texts.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics (COLING),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="14686" citStr="Santos and Gatti, 2014" startWordPosition="2249" endWordPosition="2252"> 4.3 Question Representation and Scoring The next step in the CNN consists in creating distributed vectors from word embeddings representations of the input questions. To perfom this task, the CNN must deal with two main challenges: different questions can have different sizes; and important information can appear at any position in the question. The convolutional approach (Waibel et al., 1989) is a natural choice to tackle these challenges. In recent work, convolutional approaches have been used to solve similar problems when creating representations for text segments of different sizes (dos Santos and Gatti, 2014) and character-level representations of words of different sizes (dos Santos and Zadrozny, 2014). Here, we use a convolutional layer to compute the question-wide distributed vector representations rq1 and rq2. For each question, the convolutional layer first produces local features around each word in the question. Then, it combines these local features using a sum operation to create a fixed-sized feature vector (representation) for the question. Given a question q1, the convolutional layer applies a matrix-vector operation to each window of size k of successive windows in qemb 1 = {rw1, rw2,</context>
<context position="9303" citStr="Santos &amp; Gatti (2014)" startWordPosition="1386" endWordPosition="1389">ightly worse than the state-of-the-art in paraphrase identification. Yih et al.(2014) focus on the task of answering single-relation factual questions, using a novel semantic similarity model based on a CNN architecture. Using this architecture, they train two models: one for linking a question to an entity in the DB and the other mapping a relation pattern to a relation in the DB. Both models are then combined for inferring the entity that is the answer. This approach leads to a higher precision on Q&amp;A data from the WikiAnswers corpus than the existing rules-based approach for this task. Dos Santos &amp; Gatti (2014) developed a CNN architecture for sentiment analysis of short texts that jointly uses character-level, word-level and sentence-level information, achieving state-of-theart results on well known sentiment analysis benchmarks. For the specific task of semantically equivalent questions detection that we address in this paper, we are not aware of any previous work using CNNs. Muthmann and Petrova (2014) approach the task of identifying topical near-duplicate relations between questions from social media as a classification task. They use a simple lexicosyntactical feature set and different classif</context>
<context position="21663" citStr="Santos &amp; Gatti (2014)" startWordPosition="3435" endWordPosition="3438">d Embeddings The word embeddings used in our experiments are initialized by means of unsupervised pre-training. We perform pre-training using the skip-gram NN architecture (Mikolov et al., 2013) available in the word2vec13 tool. Two different corpora are used to train word embeddings for most of the experiments: the English Wikipedia and the Ask Ubuntu community data. The experiments presented in Section 6.4 also use word embeddings trained on the Meta Stack Exchange community data. In the experiments with the English Wikipedia word embeddings, we use the embeddings previously produced by dos Santos &amp; Gatti (2014). They have used the December 2013 snapshot of the English Wikipedia corpus to obtain word embeddings with word2vec. In the experiments with Ask Ubuntu and Meta Stack Exchange word embeddings, we use the Stack Exchange data dump provided in May 2014 to train word2vec. Three main steps are used to process all questions and answers from these Stack Exchange dumps: (1) tokenization of the text using the NLTK tokenizer; (2) image removal, URL replacement and prefixing/removal of the code if necessary (see Section 6.1 for more information); (3) lowercasing of all tokens. The resulting corpora conta</context>
</contexts>
<marker>Santos, Gatti, 2014</marker>
<rawString>C´ıcero Nogueira dos Santos and Ma´ıra Gatti. 2014. Deep convolutional neural networks for sentiment analysis of short texts. In Proceedings of the 25th International Conference on Computational Linguistics (COLING), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C´ıcero Nogueira dos Santos</author>
<author>Bianca Zadrozny</author>
</authors>
<title>Learning character-level representations for part-of-speech tagging.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning (ICML), JMLR: W&amp;CP</booktitle>
<volume>32</volume>
<location>Beijing, China.</location>
<contexts>
<context position="14782" citStr="Santos and Zadrozny, 2014" startWordPosition="2262" endWordPosition="2265">buted vectors from word embeddings representations of the input questions. To perfom this task, the CNN must deal with two main challenges: different questions can have different sizes; and important information can appear at any position in the question. The convolutional approach (Waibel et al., 1989) is a natural choice to tackle these challenges. In recent work, convolutional approaches have been used to solve similar problems when creating representations for text segments of different sizes (dos Santos and Gatti, 2014) and character-level representations of words of different sizes (dos Santos and Zadrozny, 2014). Here, we use a convolutional layer to compute the question-wide distributed vector representations rq1 and rq2. For each question, the convolutional layer first produces local features around each word in the question. Then, it combines these local features using a sum operation to create a fixed-sized feature vector (representation) for the question. Given a question q1, the convolutional layer applies a matrix-vector operation to each window of size k of successive windows in qemb 1 = {rw1, rw2, ..., rwN}. Let us define the vector zn ∈ Rdk as the concatenation of a sequence of k word embed</context>
</contexts>
<marker>Santos, Zadrozny, 2014</marker>
<rawString>C´ıcero Nogueira dos Santos and Bianca Zadrozny. 2014. Learning character-level representations for part-of-speech tagging. In Proceedings of the 31st International Conference on Machine Learning (ICML), JMLR: W&amp;CP volume 32, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baotian Hu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
<author>Qingcai Chen</author>
</authors>
<title>Convolutional neural network architectures for matching natural language sentences.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014,</booktitle>
<pages>2042--2050</pages>
<location>Montreal, Quebec, Canada.</location>
<marker>Hu, Lu, Li, Chen, 2014</marker>
<rawString>Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, pages 2042–2050, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods for Natural Language Processing,</booktitle>
<pages>1746--1751</pages>
<location>Doha, Qatar.</location>
<contexts>
<context position="7686" citStr="Kim (2014)" startWordPosition="1132" endWordPosition="1133">require the exact same answer. The exact task that we approach in this study consists in, given two problem definitions, predicting if they are semantically equivalent. By problem definition we mean the concatenation of the title and the body of a question. Throughout this paper we use the term question as a synonym of problem definition. 3 Related Work The development of CNN architectures for tasks that involve sentence-level and document-level processing is currently an area of intensive research in natural language processing and information retrieval, with many recent encouraging results. Kim (2014) proposes a simple CNN for sentence classification built on top of word2vec (2013). A multichannel variant which combines static word vectors from word2vec and word vectors which are fine-tuned via backpropagation is also proposed. Experiments with different variants are performed on a number of benchmarks for sentence classification, showing that the simple CNN performs remarkably well, with state-ofthe-art results in many of the benchmarks, highlighting the importance of using unsupervised pretraining of word vectors for this task. Hu et al.(2014) propose a CNN architecture for hierarchical </context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods for Natural Language Processing, pages 1746–1751, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>182--190</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="11767" citStr="Madnani et al., 2012" startWordPosition="1760" endWordPosition="1763">erlap (see Table 1 for an example), and thus, are not duplicates. Paraphrase Identification is the task of examining two sentences and determining whether they have the same meaning (Socher et al., 2011). If two questions are paraphrases, they are also semantically equivalent. However, many semantically equivalent questions are not paraphrases. The questions shown in Table 1 significantly differ in the details they provide, and thus, could not be considered as having the same meaning. State-ofthe-art approaches to paraphrase identification include using Machine Translation evaluation metrics (Madnani et al., 2012) and Deep Learning techniques (Socher et al., 2011). Textual Semantic Similarity is the task of measuring the degree of semantic similarity between two texts, usually on a graded scale from 0 to 5, with 5 being the most similar (Agirre et al., 2013) and meaning that the texts are paraphrases. All semantically equivalent questions are somewhat semantically similar, but semantic equivalence of questions defined here does not correspond to the highest value of the textual semantic similarity for the same reasons these questions are not always paraphrases. 4 Neural Network Architecture In this sec</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182– 190, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gurmeet Singh Manku</author>
<author>Arvind Jain</author>
<author>Anish Das Sarma</author>
</authors>
<title>Detecting near-duplicates for web crawling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International Conference on World Wide Web, WWW ’07,</booktitle>
<pages>141--150</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="10852" citStr="Manku et al., 2007" startWordPosition="1615" endWordPosition="1618">e near-duplicate detection, paraphrase identification and textual semantic similarity estimation. In what follows, we outline the differences between these tasks and the one addressed in this work. Duplicate and Near-Duplicate Detection aims to detect exact copies or almost exact copies of the same document in corpora. Duplicate detection is an important component of systems for Web crawling and Web search, where it is important to identify redundant data in large corpora. Common techniques to detect duplicate documents include shingling (Broder, 1997; Alonso et al., 2013) and fingerprinting (Manku et al., 2007). State-of-theart work also focuses on the efficiency issues of the task (Wu et al., 2011). It is worth noting that even though all duplicate and near-duplicate questions are also semantically equivalent, the reverse is not true. Semantically equivalent questions could have small or no word overlap (see Table 1 for an example), and thus, are not duplicates. Paraphrase Identification is the task of examining two sentences and determining whether they have the same meaning (Socher et al., 2011). If two questions are paraphrases, they are also semantically equivalent. However, many semantically e</context>
</contexts>
<marker>Manku, Jain, Sarma, 2007</marker>
<rawString>Gurmeet Singh Manku, Arvind Jain, and Anish Das Sarma. 2007. Detecting near-duplicates for web crawling. In Proceedings of the 16th International Conference on World Wide Web, WWW ’07, pages 141–150, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of International Conference on Learning Representations, ICLR 2013,</booktitle>
<location>Scottsdale, AZ, USA.</location>
<contexts>
<context position="3090" citStr="Mikolov et al., 2013" startWordPosition="471" endWordPosition="474">ons is a very difficult task due to two main factors: (1) the same question can be rephrased in many different ways; and (2) two questions could be asking different things but look for the same solution. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. In this paper, we propose a convolutional neural network architecture to detect semantically equivalent questions. The proposed CNN first transforms words into word embeddings (Mikolov et al., 2013), using a large collection of unlabeled data, and then applies a convolutional network to build distributed vector representations for pairs of questions. Finally, it scores the questions using a similarity metric. Pairs of questions with similarity above a threshold, defined based on a heldout set, are considered duplicates. CNN is trained using positive and negative pairs of semantically equivalent questions. During training, CNN is induced to produce similar vector representations for questions that are semantically equivalent. We perform an extensive number of experiments using data from t</context>
<context position="21236" citStr="Mikolov et al., 2013" startWordPosition="3366" endWordPosition="3369">e.com 12 Several values (2, 5, 35 and 100) were tried with crossvalidation, the threshold with value 5 was selected �θ H (x,y)∈D 127 implementation provided by LibSVM (Chang and Lin, 2011) is used. In order to combine the two baselines, for a pair of questions we calculate the values of the Jaccard coefficient with shingles size up to four, and then add these values as additional features used by the SVM classifier. 5.3 Word Embeddings The word embeddings used in our experiments are initialized by means of unsupervised pre-training. We perform pre-training using the skip-gram NN architecture (Mikolov et al., 2013) available in the word2vec13 tool. Two different corpora are used to train word embeddings for most of the experiments: the English Wikipedia and the Ask Ubuntu community data. The experiments presented in Section 6.4 also use word embeddings trained on the Meta Stack Exchange community data. In the experiments with the English Wikipedia word embeddings, we use the embeddings previously produced by dos Santos &amp; Gatti (2014). They have used the December 2013 snapshot of the English Wikipedia corpus to obtain word embeddings with word2vec. In the experiments with Ask Ubuntu and Meta Stack Exchan</context>
<context position="26665" citStr="Mikolov et al., 2013" startWordPosition="4283" endWordPosition="4286"> 92.4 d=200, k=3, cl„=300, a=0.005 CNN Askubuntu word vectors prefixed 91.4 Table 2: Validation Accuracy and best parameters for the baselines and the Convolutional Neural Network. 6.2 Impact of Domain-Specific Word Embeddings We perform two experiments to evaluate the impact of the word embeddings on the CNN accuracy. In the first experiment, we gradually increase the dimensionality of word embeddings from 50 to 400. The results are presented in Figure 2. The vertical axis corresponds to validation accuracy and the horizontal axis represents the training time in epochs. As has been shown in (Mikolov et al., 2013), word embeddings of higher dimensionality trained on a large enough data set capture semantic information better than those of smaller dimensionality. The experimental results presented in Figure 2 correspond to these findings: we can see improvements in the neural network performance when increasing the word embeddings dimensionality from 50 to 100 and from 100 to 200. However, the Ask Ubuntu data set containing approximately 121M tokens is not big enough for an improvement when increasing the dimensionality from 200 to 400. In the second experiment, we evaluate the impact of in-domain word </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Proceedings of International Conference on Learning Representations, ICLR 2013, Scottsdale, AZ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klemens Muthmann</author>
<author>Alina Petrova</author>
</authors>
<title>An Automatic Approach for Identifying Topical NearDuplicate Relations between Questions from Social Media Q/A .</title>
<date>2014</date>
<booktitle>In Proceedings of Web-scale Classification: Classifying Big Data from the Web WSCBD</booktitle>
<contexts>
<context position="9705" citStr="Muthmann and Petrova (2014)" startWordPosition="1444" endWordPosition="1447"> are then combined for inferring the entity that is the answer. This approach leads to a higher precision on Q&amp;A data from the WikiAnswers corpus than the existing rules-based approach for this task. Dos Santos &amp; Gatti (2014) developed a CNN architecture for sentiment analysis of short texts that jointly uses character-level, word-level and sentence-level information, achieving state-of-theart results on well known sentiment analysis benchmarks. For the specific task of semantically equivalent questions detection that we address in this paper, we are not aware of any previous work using CNNs. Muthmann and Petrova (2014) approach the task of identifying topical near-duplicate relations between questions from social media as a classification task. They use a simple lexicosyntactical feature set and different classifiers are evaluated, with logistic regression reported as the best performing one. However, it is not possible to directly compare our results to theirs because their experimental methodology is not clearly described in the paper. There are several tasks related to identifying semantically equivalent questions. These tasks include near-duplicate detection, paraphrase identification and textual semant</context>
<context position="29097" citStr="Muthmann and Petrova (2014)" startWordPosition="4686" endWordPosition="4689">es, increasing the size of the training set pro14 We use sets of 100, 1000, 4000, 12000 and 24000 question pairs. 129 vides improvements. Nonetheless, the difference in accuracy when training with the full 24K training set and 4K subset is about 9% for SVM and only about 1% for the CNN. This difference is small for both word embeddings pre-trained on Ask Ubuntu and Wikipedia but, the in-domain word embeddings provide better performance independently of the training set size. Figure 3: Validation accuracy for the baseline and the CNN depending on the size of training set. 6.4 Domain Adaptation Muthmann and Petrova (2014) report that the Meta Stack Exchange Community15 is one of the hardest for finding semantically equivalent questions. We perform the same experiments described in previous sections using the Meta data set. In Table 5, we can see that the CNN accuracy on Meta test data (92.68%) is similar to the one for Ask Ubuntu community on test data (92.4%) (see Table 3). Also, in Table 5, we show results of a domain adaptation experiment in which we do not use training data from the Meta forum. In this case, the CNN is trained using Ask Ubuntu data only. The numbers show that even in this case using indoma</context>
</contexts>
<marker>Muthmann, Petrova, 2014</marker>
<rawString>Klemens Muthmann and Alina Petrova. 2014. An Automatic Approach for Identifying Topical NearDuplicate Relations between Questions from Social Media Q/A . In Proceedings of Web-scale Classification: Classifying Big Data from the Web WSCBD 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems 24,</booktitle>
<pages>801--809</pages>
<editor>John ShaweTaylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N. Pereira, and Kilian Q. Weinberger, editors,</editor>
<contexts>
<context position="11349" citStr="Socher et al., 2011" startWordPosition="1698" endWordPosition="1701">to detect duplicate documents include shingling (Broder, 1997; Alonso et al., 2013) and fingerprinting (Manku et al., 2007). State-of-theart work also focuses on the efficiency issues of the task (Wu et al., 2011). It is worth noting that even though all duplicate and near-duplicate questions are also semantically equivalent, the reverse is not true. Semantically equivalent questions could have small or no word overlap (see Table 1 for an example), and thus, are not duplicates. Paraphrase Identification is the task of examining two sentences and determining whether they have the same meaning (Socher et al., 2011). If two questions are paraphrases, they are also semantically equivalent. However, many semantically equivalent questions are not paraphrases. The questions shown in Table 1 significantly differ in the details they provide, and thus, could not be considered as having the same meaning. State-ofthe-art approaches to paraphrase identification include using Machine Translation evaluation metrics (Madnani et al., 2012) and Deep Learning techniques (Socher et al., 2011). Textual Semantic Similarity is the task of measuring the degree of semantic similarity between two texts, usually on a graded sca</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In John ShaweTaylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N. Pereira, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Waibel</author>
<author>Toshiyuki Hanazawa</author>
<author>Geoffrey Hinton</author>
<author>Kiyohiro Shikano</author>
<author>Kevin J Lang</author>
</authors>
<title>Phoneme recognition using time-delay neural networks.</title>
<date>1989</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing, Volume</journal>
<volume>37</volume>
<issue>3</issue>
<pages>328--339</pages>
<contexts>
<context position="14460" citStr="Waibel et al., 1989" startWordPosition="2212" endWordPosition="2215"> where vw is a vector of size |V |which has value 1 at index w and zero in all other positions. The matrix W0 is a parameter to be learned, and the size of the word embedding d is a hyper-parameter to be chosen by the user. 4.3 Question Representation and Scoring The next step in the CNN consists in creating distributed vectors from word embeddings representations of the input questions. To perfom this task, the CNN must deal with two main challenges: different questions can have different sizes; and important information can appear at any position in the question. The convolutional approach (Waibel et al., 1989) is a natural choice to tackle these challenges. In recent work, convolutional approaches have been used to solve similar problems when creating representations for text segments of different sizes (dos Santos and Gatti, 2014) and character-level representations of words of different sizes (dos Santos and Zadrozny, 2014). Here, we use a convolutional layer to compute the question-wide distributed vector representations rq1 and rq2. For each question, the convolutional layer first produces local features around each word in the question. Then, it combines these local features using a sum operat</context>
</contexts>
<marker>Waibel, Hanazawa, Hinton, Shikano, Lang, 1989</marker>
<rawString>Alexander Waibel, Toshiyuki Hanazawa, Geoffrey Hinton, Kiyohiro Shikano, and Kevin J. Lang. 1989. Phoneme recognition using time-delay neural networks. IEEE Transactions on Acoustics, Speech and Signal Processing, Volume 37(3), pages 328–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Wu</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
</authors>
<title>Efficient near-duplicate detection for q&amp;a forum.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1001--1009</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="2831" citStr="Wu et al., 2011" startWordPosition="431" endWordPosition="434">nt way.4 In this study we define two questions as semantically equivalent if they can be adequately answered by the exact same answer. Table 1 presents an example of a pair of such questions from Ask Ubuntu forum. Detecting semantically equivalent questions is a very difficult task due to two main factors: (1) the same question can be rephrased in many different ways; and (2) two questions could be asking different things but look for the same solution. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. In this paper, we propose a convolutional neural network architecture to detect semantically equivalent questions. The proposed CNN first transforms words into word embeddings (Mikolov et al., 2013), using a large collection of unlabeled data, and then applies a convolutional network to build distributed vector representations for pairs of questions. Finally, it scores the questions using a similarity metric. Pairs of questions with similarity above a threshold, defined based on a heldout set, are considered duplicates. CNN is traine</context>
<context position="10942" citStr="Wu et al., 2011" startWordPosition="1631" endWordPosition="1634">ion. In what follows, we outline the differences between these tasks and the one addressed in this work. Duplicate and Near-Duplicate Detection aims to detect exact copies or almost exact copies of the same document in corpora. Duplicate detection is an important component of systems for Web crawling and Web search, where it is important to identify redundant data in large corpora. Common techniques to detect duplicate documents include shingling (Broder, 1997; Alonso et al., 2013) and fingerprinting (Manku et al., 2007). State-of-theart work also focuses on the efficiency issues of the task (Wu et al., 2011). It is worth noting that even though all duplicate and near-duplicate questions are also semantically equivalent, the reverse is not true. Semantically equivalent questions could have small or no word overlap (see Table 1 for an example), and thus, are not duplicates. Paraphrase Identification is the task of examining two sentences and determining whether they have the same meaning (Socher et al., 2011). If two questions are paraphrases, they are also semantically equivalent. However, many semantically equivalent questions are not paraphrases. The questions shown in Table 1 significantly diff</context>
<context position="19453" citStr="Wu et al., 2011" startWordPosition="3058" endWordPosition="3061">e (Meta) is used to discuss the Stack Exchange community itself. People ask questions about the rules, features and possible bugs. The data dump we use contains 67746 questions, where 19456 are marked as duplicates. For the experiments on this data set, we select random balanced disjoint sets of 20K pairs for training, 1K for validation and 4K for testing. We prepare the data in exactly the same manner as the Ask Ubuntu data. 5.2 Baselines We explore three main baselines: a method based on the Jaccard coefficient which was reported to provide high accuracy for the task of duplicate detection (Wu et al., 2011), a Support Vector Machines (SVM) classifier (Cortes and Vapnik, 1995) and the combination of the two. For the first baseline, documents are first represented as sets of shingles of lengths from one to four, and then the Jaccard coefficient for a pair of documents is calculated as follows: S(d1) ∩ S(d2) J(S(d1), S(d2)) = S(d1) U S(d2), where S(di) is the set of shingles generated from the ith document. High values of the Jaccard coefficient denote high similarity between the documents. If the value exceeds a threshold T, the documents are considered semantically equivalent. In this case, the t</context>
</contexts>
<marker>Wu, Zhang, Huang, 2011</marker>
<rawString>Yan Wu, Qi Zhang, and Xuanjing Huang. 2011. Efficient near-duplicate detection for q&amp;a forum. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1001–1009, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Xiaodong He</author>
<author>Christopher Meek</author>
</authors>
<title>Semantic parsing for single-relation question answering.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>643--648</pages>
<marker>Yih, He, Meek, 2014</marker>
<rawString>Wen-tau Yih, Xiaodong He, and Christopher Meek. 2014. Semantic parsing for single-relation question answering. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 643–648.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>