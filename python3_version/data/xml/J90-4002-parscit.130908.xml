<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.753532">
SENTENTIAL SEMANTICS FOR PROPOSITIONAL ATTITUDES
</title>
<author confidence="0.996038">
Andrew R. Haas
</author>
<affiliation confidence="0.9932325">
Department of Computer Science
State University of New York at Albany
</affiliation>
<address confidence="0.502751">
Albany, New York, 12222
</address>
<bodyText confidence="0.9995258">
The sentential theory of propositional attitudes is very attractive to AI workers, but it is difficult to use such a
theory to assign semantics to English sentences about attitudes. The problem is that a compositional
semantics cannot easily build the logical forms that the theory requires. We present a new notation for a
sentential theory, and a unification grammar that builds logical forms in our notation. The grammar is
implemented using the standard implementation of definite clause grammars in Prolog.
</bodyText>
<sectionHeader confidence="0.9992645" genericHeader="abstract">
1 LOGICAL FORMS FOR PROPOSITIONAL
ATTITUDES
</sectionHeader>
<bodyText confidence="0.999969588235294">
The sentential theory of propositional attitudes claims that
propositions are sentences of a thought language. It has an
obvious appeal to Al workers, since their programs often
contain sentences of an artificial language, which are sup-
posed to represent the program&apos;s beliefs. These sentences
can be true or false, and the program can make inferences
from them, so they have two essential properties of beliefs.
It is tempting to conclude that they are the program&apos;s
beliefs, and that human beliefs are also sentences of a
thought language. If we extend this to all propositional
attitudes, we have a sentential theory of propositional
attitudes. In such a theory, an English sentence expresses a
proposition, and this proposition is itself a sentence—
although in a different language. Other theories of atti-
tudes hold that a proposition is a set of possible worlds, or a
situation—something very different from a sentence. Moore
and Hendrix (1979), Haas (1986), Perlis (1988), and
Konolige (1986) have argued for sentential theories and
applied them to artificial intelligence.
Kaplan (1975) proposed an analysis of quantification
into the scope of attitudes within a sentential theory, and
other authors using sentential theories have offered varia-
tions of his idea (Haas 1986; Konolige 1986). Most of these
theories present serious difficulties for formal semantics.
The problem is that they assign two very different logical
forms to a clause: one form when the clause is the object of
an attitude verb, and another when it stands alone. This
means that the logical form of the clause depends on its
context in a complicated way. It is difficult to describe this
dependence in a formal grammar. The present paper aims
to solve this problem—to present a grammar that assigns
logical forms that are correct according to Kaplan&apos;s ideas.
We also describe a parser that builds the logical forms
required by the grammar.
This grammar is a set of definite clauses written in the
notation of Pereira and Warren (1980). However, it is not a
definite clause grammar for two reasons. First, our gram-
mar cannot be parsed by the top-down left-to-right method
used for definite clause grammar (although it can be modi-
fied to allow this). Second, we do not allow any of the
nonlogical operations of Prolog, such as checking whether a
variable is bound or free, negation as failure, and the rest.
This means that our grammar is a set of ordinary first-order
sentences (in an unusual notation) and its semantics is the
ordinary semantics of first-order logic. So the grammar is
declarative, in the sense that it defines a language and
assigns logical forms without reference to any algorithm for
parsing or generation.
If we stick to the declarative semantics, a neglected
problem demands our attention. We must choose the bound
variables that appear in the logical forms generated by the
grammar. Logic grammars that include semantics nearly
always ignore this problem, using free variables of the
meta-language to represent the bound variables of the
logical form. This solution directly violates the declarative
semantics of definite clauses, and we therefore reject it. We
will see that this problem interacts with the semantics of
NP conjunction and of quantification into attitudes. To
untangle this knot and handle all three problems in one
grammar is the goal of this paper.
Section 1 of this paper will propose logical forms for
sentences about propositional attitudes and explain the
semantics of the logical forms in terms of certain relations
that we take as understood. Section 2 presents a unification
grammar for a fragment of English that includes quanti-
fiers, NP conjunction, pronouns, and relative clauses. The
grammar combines syntax and semantics and assigns one
or more logical forms to each sentence that it generates.
</bodyText>
<note confidence="0.5479745">
Computational Linguistics Volume 16, Number 4, December 1990 213
Andrew R. Haas Sentential Semantics for Propositional Attitudes
</note>
<bodyText confidence="0.999843333333333">
Section 3 extends the grammar to include verbs that de-
scribe propositional attitudes. Section 4 describes the imple-
mentation and summarizes the results.
</bodyText>
<note confidence="0.968431">
1.1 KAPLAN&apos;S ANALYSIS OF DE RE BELIEF
REPORTS
</note>
<bodyText confidence="0.999798082191781">
Noun phrases in the scope of attitude verbs commonly have
an ambiguity between de re and de dicto readings. Con-
sider the example &amp;quot;John believes that Miss America is
bald&amp;quot; (Dowty, Wall, and Peters 1981). Under the de re
reading of &amp;quot;Miss America,&amp;quot; this sentence says that John
has a belief about a woman who in fact is Miss America,
but it doesn&apos;t imply that John realizes she is Miss America.
A sentential theorist might say that the sentence tells us
that John has a belief containing some name that denotes
Miss America, but it doesn&apos;t tell us what name. The other
reading, called de dicto, says that John believes that who-
ever is Miss America is bald. The de dicto reading, unlike
the de re, does not imply that anyone actually is Miss
America—it could be true if the Miss America pageant
closed down years ago, while John falsely supposes that
someone still holds that title.
Kaplan (1975) considered examples like these. He said
that an agent may use many names that denote the same
entity, but there is a subset of those names that represent
the entity to the agent (this use of &amp;quot;represent&amp;quot; is different
from the common use in AI). If an agent has a de re belief
about an entity x, that belief must be a sentence containing,
not just any term that denotes x, but a term that represents
x to the agent. Thus if &amp;quot;person0&amp;quot; is a name that represents
Miss America to John, and the thought language sentence
&amp;quot;bald(person0)&amp;quot; is one of John&apos;s beliefs, then the sentence
&amp;quot;John thinks Miss America is bald&amp;quot; is true (under the de re
reading).
Kaplan said that a name represents an entity to an agent
if, first, it denotes that entity; second, it is sufficiently vivid;
and, finally, there is a causal connection between the entity
and the agent&apos;s use of the name. A name N is vivid to an
agent if that agent has a collection of beliefs that mention
N and give a good deal of relevant information about the
denotation of N. What is relevant may depend on the
agent&apos;s interests.
Other authors have accepted the idea of a distinguished
subset of names while offering different proposals about
how these names are distinguished. I have argued that the
distinguished names must provide information that the
agent needs to achieve his or her current goals (Haas
1986). Konolige (1986) proposed that for each agent and
each entity, the set of distinguished names has exactly one
member. In this paper, we adopt Kaplan&apos;s term &amp;quot;represent&amp;quot;
without necessarily adopting his analysis of the notion. We
assume that representation is a relation between an agent, a
name, and the entity that the name denotes. If an agent has
an attitude toward a thought-language sentence, and that
sentence contains a name that represents a certain entity to
the agent, then the agent has a de re attitude about that
entity. Our grammar will build logical forms that are
compatible with any sentential theory that includes these
assumptions.
One problem about the nature of representation should
be mentioned. This concerns the so-called de se attitude
reports. This term is attributable to Lewis (1979), but the
clearest definition is from Boer and Lycan (1986). De se
attitudes are &amp;quot;attitudes whose content would be formulated
by the subject using the equivalent in his or her language of
the first-person singular pronoun &apos;I&apos;&amp;quot; (B6er and Lycan
1986). If John thinks that he is wise, and we understand
this as a de se attitude, what name represents John to
himself? One possibility is that it is his selfname. An
agent&apos;s selfname is a thought-language constant that he
standardly uses to denote himself. It was postulated in
Haas (1986) in order to solve certain problems about
planning to acquire information. To expound and defend
this idea would take us far from the problems of composi-
tional semantics that concern us here. We simply mention
it as an example of the kinds of theories that are compatible
with the logical forms built by our grammar. See also
Rapaport (1986) for another Al approach to de se atti-
tudes.
</bodyText>
<sectionHeader confidence="0.7243665" genericHeader="introduction">
1.2 COMPOSITIONAL SEMANTICS AND LOGICAL
FORMS
</sectionHeader>
<bodyText confidence="0.99593">
Consider the logical form that Kaplan assigns for the de re
reading of &amp;quot;John believes that some man loves Mary.&amp;quot;
</bodyText>
<equation confidence="0.98841425">
(1)
3 (y,man(y) &amp;
3 (a,R(a,y,john) &amp;
believe(john,rlove(a,mary)1)))
</equation>
<bodyText confidence="0.9998635">
The notation is a slight modification of Kaplan&apos;s (Kaplan
1975), The predicate letter R denotes representation. The
symbol a is a special variable ranging over names. The
symbols randi are Quine&apos;s quasi-quotes (Quine 1947). If a
denotes a name t, then the expression &amp;quot;rlove(a,mary)1&amp;quot; will
denote the sentence &amp;quot;love(t,mary).&amp;quot;
It is hard to see how a compositional semantics can build
this representation from the English sentence &amp;quot;John be-
lieves some man loves Mary.&amp;quot; The difficult part is building
the representation for the VP &amp;quot;believes some man loves
Mary.&amp;quot; By definition, a compositional semantics must build
the representation from the representations of the constitu-
ents of the VP: the verb &amp;quot;believe&amp;quot; and the embedded clause.
Following Cooper&apos;s notion of quantifier storage (Cooper
1983), we assume that the representation of the embedded
clause has two parts: the wff &amp;quot;love(y,mary)&amp;quot; and an existen-
tial quantifier that binds the free variable y. Informally, we
can write the quantifier as &amp;quot;some(y,man(y) &amp; S),&amp;quot; where S
stands for the scope of the quantifier. Applying this
quantifier to the wff &amp;quot;love(y,mary)&amp;quot; gives the sentence
&amp;quot;some (y,man(y) &amp; love(y,mary)).&amp;quot; In the present paper,
the term &amp;quot;quantifier&amp;quot; will usually refer to this kind of
object—not to the symbols V and 3 of first-order logic, nor
to the generalized quantifiers of Barwise and Cooper (1981).
</bodyText>
<page confidence="0.820241">
214 Computational Linguistics Volume 16, Number 4, December 1990
</page>
<note confidence="0.682234">
Andrew R. Haas Sentential Semantics for Propositional Attitudes
</note>
<bodyText confidence="0.999781944444444">
In Section 2.2 we present a more precise formulation of our
representation of quantifiers.
When the clause &amp;quot;some man loves Mary&amp;quot; forms an
utterance by itself, the semantics will apply the quantifier
to the wff &amp;quot;love(y,mary)&amp;quot; to get the sentence &amp;quot;some
(y,man(y) &amp; love(y,mary)).&amp;quot; The problem is that the wff
&amp;quot;love(y,mary)&amp;quot; does not appear in Kaplan&apos;s representation.
In its place is the expression &amp;quot;love(a,mary),&amp;quot; containing a
variable that ranges over names, not men. It might be
possible to build this expression from the wff &amp;quot;love(y,mary),&amp;quot;
but this sounds like a messy operation at best. Similar
problems would arise if we chose another quotation device
(such as the one in Haas 1986) or another scoping mecha-
nism (as in Pereira and Shieber 1987).
Konolige (1986) proposed a very different notation for
quantifying in, one that would abolish the difficulty de-
scribed here. His proposal depends on an ingenious non-
standard logic. Unfortunately, Konolige&apos;s system has two
important limitations. First, he forbids a belief operator to
appear in the scope of another belief operator. Thus, he
rules out beliefs about beliefs, which are common in every-
day life. Second, he assumes that each agent assigns to
every known entity a unique &amp;quot;id constant.&amp;quot; When an agent
has a belief about an object x, that belief contains the id
constant for x. Using Kaplan&apos;s terminology, Konolige is
saying that for any entity x and agent y, there is a unique a
such that R(a,x,y). Kaplan never suggests that representa-
tion has this property, and as Moore (1988) pointed out,
the claim is hard to believe. Surely an agent can have many
names for an entity, some useful for one purpose and some
for another. Why should one of them be the unique id
constant? We will propose a notation that has the advan-
tages of Konolige&apos;s notation without its limitations. Section
1.3 will present the new notation. In Section 1.4, we return
to the problem of building logical forms for English sen-
tences.
</bodyText>
<subsectionHeader confidence="0.982967">
1.3 A NEW NOTATION FOR QUANTIFYING IN
</subsectionHeader>
<bodyText confidence="0.997732222222222">
Our logical forms are sentences in a first-order logic aug-
mented with a quotation operator. We call this language
the target language. Since the grammar is a set of definite
clauses, our notation is like Prolog&apos;s. The variables of the
target language are u, v, w, x, y, z, etc. Constants, function
letters, and atomic wffs are defined in the usual way. If p
and q are wffs, then not(p), and(p,q), and or(p,q) are wffs.
If p and q are wffs, x a variable, and t a term, the following
are wffs:
</bodyText>
<listItem confidence="0.99617">
(2) some(x,p,q)
(3) all(x,p,q)
(4) unique(x,p,q)
(5) let(x,t,p)
</listItem>
<bodyText confidence="0.999829134615385">
The first wff is true if p and q are both true for some value
of x. The second is true iff q is true for all values of x that
make p true. The third is true iff there is exactly one value
of x that makes p true, and q is true for that value of x. The
last wff is true if p is true when the value of x is set to the
value of t. This language should be extended to include the
iota operator, forming definite descriptions, since a definite
description may often represent an entity to an agent.
However, we omit definite descriptions for the time being.
For any expression e of the target language, q(e) is a
constant of the target language. Therefore we have a
countable infinity of constants. The intended models of our
language are all first-order models in which the domain of
discourse includes every expression of the language, and
each constant q(e) has the expression e as its denotation.
Models of this kind are somewhat unusual, but they are
perfectly consistent with standard definitions of first-order
logic, which allow the universe of discourse to be any
nonempty set (Enderton 1972). Our language does depart
from standard logic in one way. We allow a variable to
appear inside a constant—for example, since v is a variable,
q(v) is a constant that denotes the variable v. Enderton
explicitly forbids this: &amp;quot;no symbol is a finite sequence of
other symbols&amp;quot; (p. 68). However, allowing a variable to
appear inside a constant is harmless, as long as we are
careful about the definition of a free occurrence of a
variable. We modify Enderton&apos;s definition (p. 75) by chang-
ing his first clause, which defines free occurrences of a
variable in an atomic wff. We say instead that a variable v
appears free in variable w if v = w; no variable occurs free
in any constant; a variable v occurs free in the term
f(11 . . . tn) if it occurs free in one of t1 . . . tn; and v occurs
free in the atomic wff p(t, . . . 4) iff it occurs free in one of
tm. Under this definition x does not occur free in the
constant q(red(x)), although it does occur free in the wff
red (x).
As usual in a sentential theory of attitudes, we assume
that an agent&apos;s beliefs are sentences of thought language
stored in the head, and that knowledge consists of a subset
of those sentences. Then simple belief is a relation between
an agent and a sentence of thought language. To represent
de re belief reports, we introduce a predicate of three
arguments, and we define its extension in terms of simple
belief and the notion of representation. If [p,l,w] is a triple
in the extension of the predicate &amp;quot;believe,&amp;quot; then p is the
agent who has the belief, I is a list of entities xi . . . x„ that
the belief is about, and w is a wff of the target language.
The free variables in w will stand for unspecified terms that
represent the entities x1 . . . x„ to the agent p. These free
variables are called dummy variables. If John believes of
Mary that she is a fool, then, using Prolog&apos;s notation for
lists we write
</bodyText>
<listItem confidence="0.895222">
(6) believe(john,[mary],q(fool(x))).
</listItem>
<bodyText confidence="0.982127675">
The constant &amp;quot;mary&amp;quot; is called a de re argument of the
predicate &amp;quot;believe.&amp;quot; The free occurrence of x in fool(x)
stands for an unspecified term that represents Mary to
John. This means that there is a term t that represents
Mary to John, and John believes fool(t). x is the dummy
variable for the de re argument &amp;quot;mary.&amp;quot; This notation is
inspired by Quine (1975), but we give a semantics quite
different from Quine&apos;s. Note that the symbol &amp;quot;believe&amp;quot; is
Computational Linguistics Volume 16, Number 4, December 1990 215
Andrew R. Haas Sentential Semantics for Propositional Attitudes
an ordinary predicate letter, not a special operator. This is a
minor technical advantage of the sentential approach: the
quotation operator eliminates the need for a variety of
special propositional attitude operators.
To define this notation precisely, we must have some way
of associating dummy variables with the de re arguments.
Suppose we have the wff believe(x, [ti . . . tn],q( p)). Let v1
. . . v„ be a list of the free variables of p in order of their first
occurrence. Then v, will be the dummy variable for ti. In
other words, the dummy variable for i-th de re argument
will be the i-th free variable of p. This method of associat-
ing dummy variables with de re arguments is somewhat
arbitrary—another possibility is to include an explicit list
of dummy variables. Our choice will make the notation a
little more compact.
Then the extension of the predicate &amp;quot;believe&amp;quot; is defined
as follows. Let s be an agent, [x1 . . . xn] a list of entities
from the domain of discourse, and p a wff of the target
language. Suppose that p has exactly n free variables, and
let v1 . . . vn be the free variables of p in order of their first
occurrence. Suppose that t1. . . tn are closed terms such that
ti represents xi to s, for i from 1 to n. Suppose the simple
belief relation holds between s and the sentence formed by
substituting t1 . . 4, for free occurrences v1 . v„ in p. Then
the extension of the predicate &amp;quot;believe&amp;quot; includes the triple
containing s, [x1 . . . xn], and p.
As an example, suppose the term &amp;quot;personl&amp;quot; represents
Mary to John, and John believes &amp;quot;fool(person1).&amp;quot; Then,
since substituting &amp;quot;person 1&amp;quot; for &amp;quot;x&amp;quot; in &amp;quot;fool(x)&amp;quot; produces
the sentence &amp;quot;fool(person1),&amp;quot; it follows that
</bodyText>
<listItem confidence="0.527145">
(7) believe( john,[mary],q(fool(x)))
</listItem>
<bodyText confidence="0.9246774">
is true in every intended model where &amp;quot;believe&amp;quot; has the
extension defined above.
Consider an example with quantifiers: &amp;quot;John believed a
prisoner escaped.&amp;quot; The reading with the quantifier inside
the attitude is easy:
</bodyText>
<listItem confidence="0.956887">
(8) believe( john,[],q(some(x,prisoner(x),escaped(x)))).
</listItem>
<bodyText confidence="0.7763905">
In this case the list of de re arguments is empty. For the
&amp;quot;quantifying in&amp;quot; reading we have:
</bodyText>
<listItem confidence="0.908856">
(9) some(x,prisoner(x),believe(john,[x],q(escaped(y)))).
</listItem>
<bodyText confidence="0.999824">
This says that for some prisoner x, John believes of x that he
escaped. The dummy variable y in the wff escaped(y)
stands for an unspecified term that occurs in one of John&apos;s
beliefs and represents the prisoner x to John.
Let us consider nested beliefs, as in the sentence &amp;quot;John
believed Bill believed Mary was wise.&amp;quot; Here the de re/de
dicto ambiguity give rise to three readings. One is a straight-
forward de dicto reading:
</bodyText>
<listItem confidence="0.924745">
(10) believe( john,[],q(believe(bill,[],q(wise(mary))))).
</listItem>
<bodyText confidence="0.9996728">
To understand examples involving nested beliefs, it is help-
ful to write down the sentence that each agent believes.
Since this example does not involve quantifying in, it is easy
to write down John&apos;s belief—we just take the quotation
mark off the last argument of &amp;quot;believe&amp;quot;:
</bodyText>
<listItem confidence="0.972972">
(11) believe(bill,[],q(wise(mary))).
If this belief of John&apos;s is true, then Bill believes
(12) wise(mary).
</listItem>
<bodyText confidence="0.998253">
In the next reading, the name &amp;quot;Mary&amp;quot; is de dicto for John,
but de re for Bill:
</bodyText>
<listItem confidence="0.840025">
(13) believe( john,[1,q(believe(bill,[mary],q(wise(x))))).
</listItem>
<bodyText confidence="0.853137">
Here, John is using the constant &amp;quot;mary&amp;quot; to denote Mary,
but he does not necessarily think that Bill is using the same
constant—he only thinks that some term represents Mary
to Bill. The sentence that John believes is
</bodyText>
<listItem confidence="0.596322222222222">
(14) believe(bill, [mary] ,q(wise(x))).
If John is right, Bill&apos;s belief is formed by substituting for
the free variable x in &amp;quot;wise(x)&amp;quot; some term that represents
Mary to Bill. Suppose this term is &amp;quot;person0,&amp;quot; then Bill&apos;s
belief would be
(15) wise(person0).
Finally, there is a reading in which &amp;quot;Mary&amp;quot; is de re for both
agents:
(16) believe(john,[mary],q(believe(bill,[x],q(wise(y))))).
</listItem>
<bodyText confidence="0.992950833333333">
Here there is a name that represents Mary to John, and
John thinks that there is a name that represents Mary to
Bill. Again, John does not necessarily believe that Bill uses
the same name that John uses. Suppose &amp;quot;person3&amp;quot; is the
term that represents Mary to John, then John&apos;s belief
would be
</bodyText>
<equation confidence="0.4346875">
(17) believe(bill,[person3],q(wise(y))).
If &amp;quot;person4&amp;quot; is the term that represents Mary to Bill, then
Bill&apos;s belief would be
(18) wise(person4).
</equation>
<bodyText confidence="0.999722777777778">
One might expect a fourth reading, in which &amp;quot;Mary&amp;quot; is
de re for John and de dicto for Bill, but our formalism
cannot represent such a reading. To see why, let us try to
construct a sentence that represents this reading. In our
nota tion a nonempty list of de re arguments represents a de
re belief, while an empty list of de re arguments represents
a de dicto belief. Therefore the desired sentence should
have a nonempty list of de re arguments for John&apos;s belief,
and an empty list for Bill&apos;s belief. This would give
</bodyText>
<listItem confidence="0.589793">
(19) believe( john,[mary],q(believe(bill,[],q(wise(x)))))
</listItem>
<bodyText confidence="0.90362875">
This sentence does not assert that John believes Bill has a
de dicto belief about Mary. To see this, consider John&apos;s
belief. If he uses the constant &amp;quot;person 1&amp;quot; to denote Mary,
the belief is
</bodyText>
<page confidence="0.937818">
216 Computational Linguistics Volume 16, Number 4, December 1990
</page>
<equation confidence="0.2176655">
Andrew R. Haas Sentential Semantics for Propositional Attitudes
(20) believe(bill,[],q(wise(x))).
</equation>
<bodyText confidence="0.99994475">
In forming John&apos;s belief we do not substitute &amp;quot;person 1&amp;quot; for
the occurrence of x under the quotation operator—because
by our definitions this is not a free occurrence of x. Thus
John&apos;s belief says that Bill has a belief containing a free
variable, which our theory forbids.
It is not clear to me whether the desired reading exists in
English, so I am not certain if this property of the notation
is a bug or a feature. In either case, other notations for
describing attitudes have similar properties. For example,
in a modal logic of attitudes we use the scope of quantifiers
to represent de re/de dicto distinctions. If a quantifier
appears in the scope of an attitude operator, we have a de
dicto reading, and if it appears outside the scope (while
binding a variable inside the scope) we get a de re reading.
In a sentence like &amp;quot;John thinks Bill thinks Mary saw a
lion,&amp;quot; there are three places to put the existential quanti-
fier: in the scope of Bill&apos;s belief operator, in the scope of
John&apos;s operator but outside Bill&apos;s, or outside both. These
give the same three readings that our formalism allows. To
make &amp;quot;a lion&amp;quot; be de re for John and de dicto for Bill, we
would have to put the quantifier outside the scope of John&apos;s
belief operator, but inside the scope of Bill&apos;s belief operator.
Since Bill&apos;s belief operator is in the scope of John&apos;s, that is
impossible.
The same method applies to other attitudes—for exam-
ple, knowledge. Given a simple knowledge relation, which
expresses de dicto readings of sentences with &amp;quot;know,&amp;quot; one
can define the predicate &amp;quot;know,&amp;quot; which expresses both de
re and de dicto readings. &amp;quot;Know&amp;quot; will take three argu-
ments just as &amp;quot;believe&amp;quot; does.
Next we consider examples like &amp;quot;John knows who likes
Mary,&amp;quot; in which &amp;quot;know&amp;quot; takes a wh noun phrase and a
sentence containing a gap. The intuition behind our analy-
sis is that John knows who likes Mary if there is a person s
such that John knows that s likes Mary. This is of course a
de re belief report, and its logical form should be
</bodyText>
<listItem confidence="0.700167181818182">
(21) some(x,person(x),know( john,[x],q(like(y,mary)))).
As an example, suppose the sentence
(22) like(bill,mary)
is one of John&apos;s beliefs, and it belongs to the subset
of beliefs that constitute his knowledge. If the constant
&amp;quot;bill&amp;quot; represents Bill to John, then since substituting
&amp;quot;bill&amp;quot; for &amp;quot;y&amp;quot; in &amp;quot;likes(y,mary)&amp;quot; gives the sentence
&amp;quot;like(bill,mary),&amp;quot; we have
(23) know( john, [bill],q(like(y,mary)))
and therefore
(24) some(x,person(x),know( john,[x],q(like(y,mary)))).
</listItem>
<bodyText confidence="0.998761575757576">
This proposed analysis of &amp;quot;knowing who&amp;quot; is probably too
weak. As a counter example, suppose a night watchman
catches a glimpse of a burglar and chases him. Then the
night watchman has formed a mental description of the
burglar—a description that he might express in English as
&amp;quot;the man I just saw sneaking around the building.&amp;quot; The
burglar might say to himself, &amp;quot;He knows I&apos;m in here.&amp;quot; This
is a de re belief report, so it follows that the night watch-
man&apos;s mental description of the burglar must represent the
burglar to the watchman (by our assumption about repre-
sentation). Yet the night watchman surely would not claim
that he knows who is sneaking around the building. It
seems that even though the watchman&apos;s mental description
represents the burglar, it is not strong enough to support
the claim that he knows who the burglar is.
It would be easy to extend our notation to allow for a
difference between &amp;quot;knowing who&amp;quot; and other cases of
quantification into attitudes. It would be much harder to
analyze this difference, Boer and Lycan (1986) have ar-
gued that when we say someone knows who N is, we always
mean that someone knows who N is for some purpose. This
purpose is not explicitly mentioned, so it must be under-
stood from the context of the utterance in which the verb
&amp;quot;know&amp;quot; appears. Then the predicate that represents &amp;quot;know-
ing who&amp;quot; must have an extra argument whose value is
somehow supplied by context. These ideas look promising,
but to represent this use of context in a grammar is a hard
problem, and outside the scope of this work.
Next we consider intensional transitive verbs like &amp;quot;want,&amp;quot;
as in &amp;quot;John wants a Porsche.&amp;quot; The intuition behind the
analysis is that this sentence is roughly synonymous with
&amp;quot;John wishes that he had a Porsche&amp;quot;—under a reading in
which &amp;quot;he&amp;quot; refers to John. Then the logical form would be
</bodyText>
<listItem confidence="0.490897">
(25) wish( john, [1,q(some(x,porsche(x),have( john,x))))
for a de dicto reading, and
(26) some(x,porsche(x),wish( john,[x],q(have( john,y))))
</listItem>
<bodyText confidence="0.961690411764706">
for a de re reading. The predicate letter &amp;quot;wish&amp;quot; need not be
identical to the one that translates the English verb
&amp;quot;wish&amp;quot;—it might only be roughly synonymous. The predi-
cate letter &amp;quot;have&amp;quot; probably is the same one that translates
the verb &amp;quot;have&amp;quot;—or rather, one of many predicates that
can translate this highly ambiguous verb. For the present
purpose let us assume that the predicate &amp;quot;have&amp;quot; represents
a sense of the verb &amp;quot;have&amp;quot; that is roughly synonymous with
&amp;quot;possess,&amp;quot; as in &amp;quot;John has a Porsche.&amp;quot; Another sense of
&amp;quot;have&amp;quot; is relational, as in &amp;quot;John has a son,&amp;quot; and &amp;quot;want&amp;quot; has
a corresponding sense, as in &amp;quot;John wants a son.&amp;quot; The
present paper will not analyze this relational sense.
This grammar will express the meanings of intensional
verbs in terms of propositional attitudes. This may not work
for all intensional verbs. For example, it is not clear that
&amp;quot;the Greeks worshipped Zeus&amp;quot; is equivalent to any state-
ment about propositional attitudes. Montague (1974a) rep-
resented intensional verbs more directly, as relations be-
tween agents and the intensions of NP&apos;s. A similar analysis
is possible in our framework, provided we extend the target
language to include typed lambda calculus. Suppose the
Computational Linguistics Volume 16, Number 4, December 1990 217
Andrew R. Haas Sentential Semantics for Propositional Attitudes
variable p ranges over sets of individuals. Then we could
represent the de dicto reading of &amp;quot;John wants a Porsche&amp;quot; as
(27) want( john,q(lambda(p,some(x,porsche(x),x E
Here the predicate &amp;quot;want&amp;quot; describes a relation between a
person and an expression of thought language, but that
expression is not a wff. Instead it is a closed term denoting a
set of sets of individuals. Certainly this is a natural general-
ization of a sentential theory of attitudes. If agents can
have attitudes toward sentences of thought language, why
shouldn&apos;t they have attitudes toward other expressions of
the same thought language?
</bodyText>
<subsectionHeader confidence="0.985072">
1.4 COMPOSITIONAL SEMANTICS AGAIN
</subsectionHeader>
<bodyText confidence="0.999381">
We now return to the problem of building logical forms
with a compositional semantics. Consider the formula
</bodyText>
<listItem confidence="0.625056">
(28) some(x,prisoner(x),believe( john, [x] ,q(escaped(y)))).
</listItem>
<bodyText confidence="0.999663142857143">
Following Cooper as before, we assume that the semantic
features of the clause &amp;quot;a prisoner escaped&amp;quot; are a wff
containing a free variable and an existential quantifier that
binds the same variable. In formula (28) the existential
quantifier does not bind the variable that appears in the wff
&amp;quot;escaped(y)&amp;quot;—it binds another variable instead. Therefore
we have the same problem that arose for Kaplan&apos;s represen-
tation—it is not clear how to build a representation for the
belief sentence from the representations of its constituents.
The choice of bound variables is arbitrary, and the choice
of dummy variables is equally arbitrary. Thus, there is an
obvious solution: let the de re arguments and the dummy
variables be the same. Thus, the wide scope reading for
&amp;quot;John believes a prisoner escaped&amp;quot; is not (28), but
</bodyText>
<listItem confidence="0.484773">
(29) some(x,prisoner(x),believe(john,[x],q(escaped(x)))).
</listItem>
<bodyText confidence="0.998654379310345">
Here the variable x serves two purposes—it is a de re
argument, and also a dummy variable. When it occurs as a
de re argument, it is bound by the quantifier in the usual
way. When it occurs as a dummy variable, it is definitely
not bound by the quantifier. In fact the dummy variable is a
mention of the variable x, not a use, because it occurs under
a quotation mark.
Formula (29) may be a little confusing, since the same
variable appears twice with very different semantics. This
formula has a major advantage over formula (28), how-
ever—it contains the wff &amp;quot;escaped(x)&amp;quot; and a quantifier
that binds the free variable of that wff. Since these are
precisely the semantic features of the clause &amp;quot;a prisoner
escaped,&amp;quot; it is fairly easy to build the logical form (29)
from the sentence &amp;quot;John believed a prisoner escaped.&amp;quot;
We can describe this technique as a convention govern-
ing the logical forms that our grammar assigns to English
phrases. In any wff of the form believe(x, [ti . . . t„], q(p)),
the nth de re argument is equal to its own dummy variable.
Then the nth de re argument t„ is equal to the nth free
variable of p. In other words, the list [ti . . . 4] is just a list
of the free variables of p in order of occurrence. The same
convention holds for all predicates that represent attitudes.
Finally, note that the convention holds only for the
logical forms that the grammar assigns to sentences. Once
the grammar has built a logical form, inference procedures
can freely violate the convention. For example, consider the
logical form of the sentence &amp;quot;Every man believes that Mary
loves him&amp;quot;:
</bodyText>
<listItem confidence="0.719018666666667">
(30) all(x,man(x),believe(x,[x],q(love(mary,x)))).
From this sentence and the premise man(bill) we can infer
(31) believe(bill,[bill],q(love(mary,x)))
</listItem>
<bodyText confidence="0.891721666666667">
by substituting for a universal variable as usual. The occur-
rence of the variable under the quotation mark is naturally
unaffected, because it is not a free occurrence of x.
</bodyText>
<subsectionHeader confidence="0.999268">
1.5 SELF-REFERENCE AND PARADOX
</subsectionHeader>
<bodyText confidence="0.999722948717949">
Other writers (cited above) have already expounded and
defended sentential theories of attitudes. This paper takes a
sentential theory as a starting point, and aims to solve
certain problems about the semantics of attitude reports in
such a theory. However, one problem about sentential
theories deserves discussion. The results of Montague
(19741)) have been widely interpreted as proof that senten-
tial theories of attitudes are inconsistent and therefore
useless. Montague did indeed show that certain sentential
theories of knowledge produce self-reference paradoxes,
and are therefore inconsistent. However, he did not show
that these were the only possible sentential theories. Re-
cently des Rivieres and Levesque (1986) have constructed
sentential theories without self-reference and proved them
consistent. Thus they showed that while Montague&apos;s theo-
rem was true, its significance had been misunderstood.
Perlis (1988) has shown that if we introduce self-reference
into a modal theory, it too can become inconsistent. In
short, there is no special connection between sentential
theories and paradoxes of self-reference. A sentential the-
ory may or may not include self-reference; a modal theory
may or may not include self-reference; and in either case,
self-:reference can lead to paradoxes.
Kripke (1975) has shown that even the most common-
place utterances can create self-reference if they occur in
unusual circumstances. Therefore the problem is not to
avoid self-reference, but to understand it. The problem for
advocates of sentential theories is to find a sentential analy-
sis of the self-reference paradoxes that is, if not wholly
satisfactory, at least as good as nonsentential analyses. For
the purposes of AT, a successful analysis must avoid para-
doxical conclusions, without sacrificing axioms or rules of
inference that have proved useful in AT programs.
One idea is that ordinary human intuitions about self-
reference are inconsistent. To most people, it appears that
the sentence &amp;quot;This statement is false&amp;quot; must be both true
and false, yet it cannot be both. The only error in the formal
analyses is that having derived a contradiction, they allow
us to derive any conclusion whatever. This happens because
</bodyText>
<page confidence="0.844166">
218 Computational Linguistics Volume 16, Number 4, December 1990
</page>
<note confidence="0.303846">
Andrew R. Haas Sentential Semantics for Propositional Attitudes
</note>
<bodyText confidence="0.997152428571429">
standard logic allows no inconsistent theories except trivial
ones, containing every sentence of the language. Therefore
we need a new kind of logic to describe the inconsistent
intuitions of the ordinary speaker. Priest (1989) attempted
this—he constructed an inconsistent but nontrivial theory
of truth using a paraconsistent logic. Priest&apos;s theory in-
cludes the T-scheme, written in our notation as
</bodyText>
<equation confidence="0.618065">
(32) P 4-* true(q(P)).
</equation>
<bodyText confidence="0.999918525641026">
P is a meta-variable ranging over sentences of the lan-
guage. Tarski (1936) proposed this scheme as capturing an
essential intuition about truth. Unfortunately, the rule of
modus ponens is invalid in Priest&apos;s system, which means
that most of the standard AI reasoning methods are invalid.
Priest considers various remedies for this problem.
Another approach is to look for a consistent theory of
self-reference. Such a theory will probably disagree with
speakers&apos; intuitions for paradoxical examples like &amp;quot;This
statement is false.&amp;quot; Yet these examples are rare in practice,
so a natural language program using a consistent theory of
self-reference might agree with speakers&apos; intuitions in the
vast majority of cases. Kripke (1975) proposed such a
theory, based on a new definition of truth in a model—an
alternative to Tarski&apos;s definition. Kripke&apos;s definition allows
truth-value gaps: some sentences are neither true nor false.
Suppose P is a sentence; then the sentence true(q(P)) is
true if P is true, and false if P is false. Therefore if P is
neither true nor false, true(q(P)) also has no truth value. In
other respects, Kripke&apos;s definition of truth resembles Tar-
ski&apos;s—it assigns the same truth values to sentences that do
not contain the predicate &amp;quot;true,&amp;quot; and it never assigns two
different truth values to one sentence. Suppose that a model
of this kind contains a sentence that says &amp;quot;I am not true.&amp;quot;
Formally, suppose the constant c denotes the sentence
-Itrue(c). What truth value can such a sentence have under
Kripke&apos;s definition? Just as in standard logic, -1 true(c) is
true if true(c) is false. True(c) in turn is false if c is false.
Since c is the sentence -I true(c), we have shown that c is
true if c is false. Since no sentence has two truth values, it
follows that c has no truth value.
Once again, problems arise because the system is too
weak. If P is a sentence with no truth value, then the
sentence P V -1 P has no truth value, even though it is a
tautology of first-order logic. One remedy for this appears
in the system of Perlis (1985). Perlis considers a first-order
model M containing a predicate &amp;quot;true,&amp;quot; whose extension is
the set of sentences that are true in M by Kripke&apos;s defini-
tion. He accepts as theorems all sentences that are Tarski-
true in every model of this kind. Thus Perlis&apos;s system uses
two notions of truth: P is a theorem only if P is Tarski-true,
but true(q(P)) is a theorem only if P is Kripke-true.
Suppose we have P *-* ---1 true(q(P)); then Perlis&apos;s system
allows us to prove both P and -1 true(q(P)). This certainly
violates the intuitions of ordinary speakers, but such viola-
tions seem to be the inevitable price of a consistent theory
of self-reference. Perlis devised a proof system for such
models, using standard first-order proof and an axiom
schema GK for the predicate &amp;quot;true.&amp;quot; Penis proved that if L
is any consistent set of first-order sentences that does not
mention the predicate &amp;quot;true,&amp;quot; then the union of L and GK
has a model M in which the extension of &amp;quot;true&amp;quot; is the set of
sentences that are Kripke-true in M. Perlis&apos;s system has
one important advantage over Kripke&apos;s: since the formal-
ism is just a standard first-order theory, we can use all the
familiar first-order inference rules. In this respect, Perlis&apos;s
system is better suited to the needs of Al than either
Kripke&apos;s or Priest&apos;s. However, it still excludes some infer-
ences that are standard in everyday reasoning. For exam-
ple, we have true(q(P)) -* P for every P, but P -* true(q(P))
is not a theorem for certain sentences P—in particular,
sentences that are self-referential and paradoxical.
An adequate account of self-reference must deal not only
with the Liar, but also with paradoxes arising from proposi-
tional attitudes—for example, the Knower Paradox (Mon-
tague and Kaplan 1974), and Thomason&apos;s paradox about
belief (Thomason 1980). Perlis (1988) has considered the
treatment of attitudes within his system, and Asher and
Kamp (1986) have treated both paradoxes using ideas akin
to Kripke&apos;s (their treatment is not sentential, but they
claim that it could be extended to a sentential treatment).
Let us briefly consider the treatment of the Knower
paradox within Perlis&apos;s system. To simplify the treatment,
we will assume that knowledge is true belief. If we are
working in Perlis&apos;s system, this naturally means that knowl-
edge is Kripke-true belief. We write &amp;quot;the agent knows that
P&amp;quot; as true(q(P)) A believe(q(P)). The paradox arises from
a sentence R that says &amp;quot;The agent knows -R.&amp;quot; Formally,
</bodyText>
<listItem confidence="0.985165285714286">
(33) R 4-* (true(q(-R)) A believe(q(-R))).
Since true(q(-R)) -* -1R is a theorem of Perlis&apos;s system,
(33) implies -IR. Now suppose that the agent believes (33);
then with modest powers of inference the agent can con-
clude -R, so we have believe(q-R). Combining this with
(33) gives
(34) R 4-4 true(q(---R)),
</listItem>
<bodyText confidence="0.9988179375">
which at once implies that -R is not Kripke-true. It follows
that although --R is a theorem of the system, and the agent
believes it, the agent does not know it—because it is not
Kripke-true, and only a sentence that is Kripke-true can be
known. The Knower paradox arises if we insist that the
agent does know -R. This example brings out a counter-
intuitive property of Perlis&apos;s system: a sentence may follow
directly from Perlis&apos;s axioms, yet he refuses to call it true,
or to allow that any agent can know it. Strange though this
appears, it is a natural consequence of the use of two
definitions of truth in a single theory.
Belief is different from knowledge because it need not be
true. This makes it surprising that Thomason&apos;s paradox
involves only the notion of belief, not knowledge or truth. In
fact the paradox arises exactly because Thomason&apos;s agent
thinks that all his beliefs are true. This is stated as
</bodyText>
<listItem confidence="0.731533">
(35) a(&lt; a(&lt; So &gt;) ---■ 40 &gt;)
</listItem>
<bodyText confidence="0.883575571428571">
Computational Linguistics Volume 16, Number 4, December 1990 219
Andrew R. Haas Sentential Semantics for Propositional Attitudes
(Thomason 1980). The notation is as follows: So is a vari-
able ranging over all formulas of the language, &lt; So &gt; is a
constant denoting (the Godel number of) So, and a(&lt; (P&gt;)
means that the agent believes So. This axiom says that for
every formula &lt;P, the agent believes
</bodyText>
<equation confidence="0.581512">
(36) a(&lt; So &gt;) —• (P.
</equation>
<bodyText confidence="0.999636619047619">
This sentence says that if the agent believes &apos;P, (P must be
true. Since So ranges over all sentences of the language, the
agent is claiming that his beliefs are infallible. This leads
the agent into a paradox similar to the Knower, and his
beliefs are therefore inconsistent. Asher and Kamp showed
that one can avoid this conclusion by denying (35) in
certain cases where 4) is a self-referential sentence. Another
alternative is to dismiss (35) completely. It is doubtful that
human beings consider their own beliefs infallible, and
Perlis (1986) has argued that a rational agent may well
believe that some of his or her beliefs are false.
We have looked at three sentential analyses of the self-
reference paradoxes, and each one sacrifices some principle
that seems useful for reasoning in an AI program. The
alternative is an analysis in which propositions are not
sentences. Thomason (1986) considers such analyses and
finds that they have no clear advantage over the sentential
approaches. The unpleasant truth is that paradoxes of
self-reference create equally serious problems for all known
theories of attitudes. It follows that they provide no evi-
dence against the sentential theories.
</bodyText>
<sectionHeader confidence="0.877944" genericHeader="method">
2 THE BASIC GRAMMAR
2.1 NOTATION
</sectionHeader>
<bodyText confidence="0.999984773584906">
The rules of our grammar are definite clauses, and we use
the notation of definite clause grammar (Pereira and War-
ren 1980). This notation is now standard among computer
scientists who study natural language and is explained in a
textbook by Pereira and Shieber (1987). Its advantages
are that it is well defined and easy to learn, because it is a
notational variant of standard first-order logic. Also, it is
often straightforward to parse with grammars written in
this notation (although there can be no general parsing
method for the notation, since it has Turing machine
power). DCG notation lacks some useful devices found in
linguistic formalisms like GPSG—there are no default
feature values or general feature agreement principles
(Gazdar et al. 1985). On the other hand, the declarative
semantics of the DCG notation is quite clear—unlike the
semantics of GPSG (Fisher 1989).
The grammar is a set of meta-language sentences describ-
ing a correspondence between English words and sentences
of the target language. Therefore, we must define a nota-
tion for talking about the target language in the meta-
language. Our choice is a notation similar to that of Haas
(1986). If f is a symbol of the target language, &apos;f is a symbol
of the meta-language. Suppose f is a constant or a variable,
taking no arguments. Then &apos;f denotes f. Thus &apos;john is a
meta-language constant that denotes a target-language
constant, while &apos;x is a meta-language constant that denotes
a target-language variable. Suppose f is a functor of the
target language and takes n arguments. Then &apos;f is a meta-
language function letter, and it denotes the function that
maps n expressions of the target language el . . . en to the
target-language expression f(ei . . . en). Thus &apos;not is a meta-
language function letter, and it denotes the function that
maps a target language wff to its negation. In the same
way, &apos;or is a meta-language function letter, and it denotes
the function that maps two target-language wffs to their
disjunction.
Given these denotations, it is easy to see that if p(a,b) is
an atomic sentence in the target language, then &apos;p(&apos;a,&apos;b) is
a term in the meta-language, and it denotes the wff p(a,b)
in the target language. Suppose that Wffl and Wff2 are
meta-language variables ranging over wffs of the target
language. Then &apos;or(Wffl ,Wff2) is a meta-language term,
and since the variables Wffl and Wff2 range over all wffs of
the target language, the value of &apos;or(Wff 1 ,Wff2) ranges
over all disjunctions in the target language. These ideas
about the relation between meta-language and target lan-
guage are not new or difficult, but it is worth the time to
explain them, because some influential papers about seman-
tics in unification grammar have confused the target lan-
guage and meta-language (see Section 2.4). For the sake of
legibility, we omit the quotation marks—so when
or(WfIl ,Wff2) appears in a rule of the grammar, it is an
abbreviation for &apos;or(Wff 1 ,Wff2).
</bodyText>
<subsectionHeader confidence="0.992473">
2.2 REPRESENTING QUANTIFIERS
</subsectionHeader>
<bodyText confidence="0.9852222">
Noun phrases in the grammar contribute to logical form in
two ways, and therefore they have two semantic features.
The first feature is a variable, which becomes a logical
argument of a verb. This produces a wff, in which the
variable appears free. The second feature is a quantifier
that binds the variable. By applying the quantifier to the
wff, we eliminate free occurrences of that particular vari-
able. After applying all the quantifiers, we have a wff
without free variables—a sentence. This is the logical form
of an utterance.
In Montague&apos;s system (Montague 1974a), the logical
form of an NP is an expression denoting a quantifier. This
kind of analysis is impossible in our system, because the
target language is first-order. It contains no expressions
that denote quantifiers. Therefore the representation of an
NP cannot be an expression of the target language. Instead
of using Montague&apos;s approach, we associate with every
quantifier a function that maps wffs to wffs. For the NP
&amp;quot;every man,&amp;quot; we have a function that maps any wff Wffl to
the wff
(37) all(V,man(V),Wffl)
where V is a variable of the target language. Notice that if
we took Montague&apos;s representation for the quantified NP,
applied it to the lambda expression lambda(V,Wff1), and
then simplified, we would get an alphabetic variant of (37).
</bodyText>
<page confidence="0.727953">
220 Computational Linguistics Volume 16, Number 4, December 1990
</page>
<note confidence="0.504642">
Andrew R. Haas Sentential Semantics for Propositional Attitudes
</note>
<bodyText confidence="0.9998541">
We will call this function the application function for the
quantified NP.
To represent application functions in a unification gram-
mar, we use a device from Pereira and Warren (1980). We
assign to each NP an infinite set of readings—one for each
ordered pair in the extension of the application function.
The first and second elements of the ordered pair are
semantic features of the NP, and the bound variable of the
quantifier is a third feature. For the NP &amp;quot;every man&amp;quot; we
have
</bodyText>
<listItem confidence="0.462735">
(38) np(V,Wffl,all(V,man(V),Wff1)) --. [every man] .
</listItem>
<bodyText confidence="0.99914625">
This says that for any variable V and wff Wffl, the string
&amp;quot;every man&amp;quot; is an NP, and if it binds the variable V, then
the pair [Wffl,all(V,man(V),Wff1)] is in the extension of
its application function. It follows that the application
function maps Wffl to the wff all(V,man(V),Wff1). When
other rules fix the values of the variables V and Wffl, the
result of the mapping will be fixed as well. A more complex
example is
</bodyText>
<equation confidence="0.760449">
(39) np(V,Wffl,and(some(V,man(V),Wff1),some(V,
woman(V),Wff1))) —&gt; [a man and a woman].
</equation>
<bodyText confidence="0.999890029411765">
Here the application function&apos;s output includes two copies
of the input.
It is important to consider the declarative semantics of
these rules. Each one states that a certain NP has an
infinite set of possible readings, because there are infinitely
many wffs in the target language. Thus we might say that
the NP in isolation is infinitely ambiguous. This &amp;quot;ambiguity&amp;quot;
is purely formal, however; in any actual utterance the value
of the variable Wffl will be supplied by other rules, so that
in the context of an utterance the ambiguity is resolved. In
the same way, the VP &amp;quot;liked Mary&amp;quot; is ambiguous in person
and number—but in the context of the utterance &amp;quot;John
liked Mary,&amp;quot; its person and number are unambiguous.
In one respect the declarative semantics of these rules is
not quite right. The variable V is supposed to range over
variables of the target language, and the variable Wffl is
supposed to range over wffs of the target language. Yet we
have not defined a type system to express these range
restrictions. However, such a type system could be added,
for example, using the methods of Walther (1987). In fact,
the type hierarchy would be a tree, which allows us to use a
simplified version of Walther&apos;s methods. For brevity&apos;s sake
we will not develop a type system in this paper. Except for
this omission, the declarative semantics of the above rules is
quite clear.
Typed variables have mnemonic value even if we do not
use a typed logic. Therefore we adopt the following conven-
tions. The meta-language variables V, VO, V1 . . . range
over target language variables. Wff, Wffl, Wff2 . . . range
over target language wffs. Q, Qi, Q2 . . . range over
quantifiers. QL, QL1, QL2 . . . range over lists of quanti-
fiers. When a wff forms the range restriction of a quantifier,
we will sometimes use the variables Range, Rangel . . . for
that wff.
</bodyText>
<subsectionHeader confidence="0.999131">
2.3 SCOPING AND QUANTIFIER STORAGE
</subsectionHeader>
<bodyText confidence="0.9744186">
Given a means of describing quantifiers, we must consider
the order of application. Cooper (1983) has shown how to
allow for different orders of application by adding to NPs,
VPs, and sentences an extra semantic feature called the
quantifier store. The store is a list of quantifiers that bind
the free variables in the logical form of the phrase. The
grammar removes quantifiers from the store and applies
them nondeterministically to produce different logical forms,
corresponding to different orders of application. If a sen-
tence has a logical form p and a quantifier store /, then
every free variable in p must be bound by a quantifier in
1—otherwise the final logical form would contain free
variables.
Our treatment of quantifier storage is different from
Cooper&apos;s in two ways. First, Cooper&apos;s grammar maps
phrases to model-theoretic denotations, not logical forms.
This sounds like a bigger difference than it is. The basic
technique is to put quantifiers in a store, and use some kind
of marker to link the stored quantifiers to the argument
positions they must bind. Whether we work with the logical
forms or with their denotations, much the same problems
arise in applying this technique.
A second difference is that in Cooper&apos;s grammar, each
NP has two readings—one in which the NP&apos;s quantifier is
in the store, and one in which it is not. The first reading
leads to wide-scope readings of the sentence, while the
second leads to narrow-scope readings. In our grammar
only the first kind of reading for an NP exists—that is, the
quantifier of an NP is always in the store. We generate both
wide- and narrow-scope readings by applying the quanti-
fiers from the store in different orders.
We represent a quantifier as a pair p(Wff 1 ,Wff2), where
the application function of the quantifier maps Wffl to
Wff2. We represent a quantifier store as a list of such pairs.
The predicate apply_quants(QL1,Wff 1 ,QL2,Wff2) means
that QL1 is a list of quantifiers, Wffl is a wff, Wff2 is the
result of applying some of the quantifiers in QL1 to Wffl,
and QL2 contains the remaining quantifiers. The first
axiom for the predicate says that if we apply none of the
quantifiers, then QL2 = QL1 and Wff2 = Wffl:
(40) apply_quants(QL,Wff,QL,Wff).
The second axiom uses the predicate choose(L1,X,L2),
which means that X is a member of list Ll, and L2 is
formed by deleting one occurrence of X from Ll.
apply_quants(QL1,Wff1,QL3,Wff3)
:- choose(QL1,p(Wffl,Wff2),QL2),
apply_quants(QL2,Wff2,QL3,Wff3).
Consider the first literal on the right side of this rule. It says
that p(Wff1,Wff2) is a member of QL1, and deleting
p(Wffl ,Wff2) from QL1 leaves QL2. By definition, if the
pair p(Wff 1 ,Wff2) is in the extension of the application
function for a certain quantifier, the application function
Computational Linguistics Volume 16, Number 4, December 1990 221
Andrew R. Haas Sentential Semantics for Propositional Attitudes
maps Wffl to Wff2. The second literal says that applying a
subset of the remaining quantifiers QL2 to Wff2 gives a
new wff Wff3 and a list QL3 of remaining quantifiers. Then
applying a subset of QL1 to Wffl gives Wff3 with remain-
ing quantifiers QL3.
Suppose that QL1 is
</bodyText>
<equation confidence="0.897101416666667">
(42) [p(Wffl,all(V1,man(V1),Wff1)),p(Wff2,some(V2,
woman(V2),Wff2) )]
Then solutions for the goal
(43) :- apply.quants (QL1,1oves(V1,V2),QL3,Wff3)
include
Wff3 = all(V1,man(V1),
some(V2,woman(V2),loves(VI,V2)))
QL3 = []
and also
Wff3 = some(V2,woman(V2),
all (V1,man (V1),loves(V1,V2)))
QL3 = [].
</equation>
<bodyText confidence="0.751934">
There are also solutions in which some quantifiers remain
in the store:
</bodyText>
<equation confidence="0.9995115">
Wff3 = all(V1,man(V1),loves(V1,V2))
QL3 = [p(Wff2,some(V2,woman(V2),Wff2))].
</equation>
<bodyText confidence="0.9997165">
These solutions will be used to build wide-scope readings
for propositional attitude reports.
</bodyText>
<sectionHeader confidence="0.8872515" genericHeader="method">
2.4 THE PROBLEM OF ASSIGNING DISTINCT
VARIABLES TO QUANTIFIERS
</sectionHeader>
<bodyText confidence="0.999836">
The rules we have given so far do not tell us which target
language variables the quantifiers bind. These rules contain
meta-language variables that range over target language
variables, rather than meta-language constants that denote
particular variables of the target language. In choosing the
bound variables it is sometimes crucial to assign distinct
variables to different quantifiers. The logical form of &amp;quot;Some
man loves every woman&amp;quot; can be
</bodyText>
<listItem confidence="0.987154333333333">
(47) some(x,man(x),a11(y,woman(y),loves(x,y)))
but it cannot be
(48) some(y,man(y),a11(y,woman(y),loves(y,y))).
</listItem>
<bodyText confidence="0.998600705882353">
This reading is wrong because the inner quantifier captures
the variables that are supposed to be bound by the outer
quantifier. To be more precise: the outer quantifier binds
the variable y, but not all occurrences of y in the scope of
the outer quantifier are bound by the outer quantifier.
Some of them are bound instead by the inner quantifier. In
this situation, we say that the inner quantifier shadows the
outer one. We require that no quantifier ever shadows
another in any logical form built by the grammar. This
requirement will not prevent us from finding logical forms
for English sentences, because any first-order sentence is
logically equivalent to a sentence without shadowing.
The same problem arises in cases of quantification into
the scope of attitudes. Consider the sentence &amp;quot;John thinks
some man loves every woman,&amp;quot; and suppose that &amp;quot;some
man&amp;quot; has wide scope and &amp;quot;every woman&amp;quot; has narrow
scope. The logical form can be
</bodyText>
<equation confidence="0.8700012">
some(x,man(x),
thinks( john,[x],q(all(y,woman(y),loves(x,y)))))
but it cannot be
some(y,man(y),
thinks( john,[y],q(all(y,woman(y),loves(y,y))))).
</equation>
<bodyText confidence="0.827391285714286">
In this formula, the inner quantifier captures a variable
that is supposed to be a dummy variable. In this case also,
we say that the inner quantifier shadows the outer one.
Pereira and Warren (1980) prevented shadowing by
using Prolog variables to represent variables of the object
language. Thus, their translation for &amp;quot;Some man loves
every woman&amp;quot; is
</bodyText>
<listItem confidence="0.760971">
(51) exists(Y) : (man(Y) &amp; all(X) : (woman(X)
loves(Y,X)))
</listItem>
<bodyText confidence="0.916928">
where X and Y are Prolog variables. This works, but it
violates the declarative semantics of Prolog. According to
that semantics every variable in an answer is universally
quantified. Thus if Prolog returns (51) as a description of
the logical form of a sentence, this means that for all values
of X and Y the expression (51) denotes a possible logical
form for that sentence. This means that if v is a variable of
the object language, then
</bodyText>
<listItem confidence="0.88071">
(52) exists(v) : (man(v) &amp; all(v) : (woman(v)
loves (v,v)))
</listItem>
<bodyText confidence="0.997596368421053">
is a possible translation, which is clearly false. Thus, accord-
ing to the declarative interpretation, Pereira and Warren&apos;s
grammar does not express the requirement that no quanti-
fier can shadow another quantifier. Pereira and Shieber
(1987) pointed out this problem and said that while for-
mally incorrect the technique was &amp;quot;unlikely to cause
problems.&amp;quot; Yet on p. 101 they describe the structures built
by their grammar as &amp;quot;unintuitive&amp;quot; and even &amp;quot;bizarre.&amp;quot;
This confirms the conventional wisdom: violating the declar-
ative semantics makes logic programs hard to understand.
Therefore, let us look for a solution that is formally correct.
Warren (1983) suggested one possible solution. We can
use a global counter to keep track of all the variables used
in the logical form of a sentence, and assign a new variable
to every quantifier. Then no two quantifiers would bind the
same variable, and certainly no quantifier would shadow
another. This solution would make it easier to implement
our treatment of de re attitude reports, but it would also
create serious problems in the treatment of NP conjunction
</bodyText>
<page confidence="0.664742">
222 Computational Linguistics Volume 16, Number 4, December 1990
</page>
<note confidence="0.554134">
Andrew R. Haas Sentential Semantics for Propositional Attitudes
</note>
<bodyText confidence="0.999395388888889">
and disjunction (see Section 2.5). Therefore we consider
another possibility.
Let us rewrite the definition of &amp;quot;apply_quants,&amp;quot; adding
the requirement that each quantifier binds a variable that is
not bound in the scope of that quantifier. For each integer
N, let v(N) be a variable of the target language. If N is not
equal to M, then v(M) and v(N) are distinct variables. We
represent the integers using the constant 0 and the function
&amp;quot;s&amp;quot; for &amp;quot;successor&amp;quot; in the usual way. The predicate
highest_bound_var(Wffl ,N) means that N is the largest
number such that v(N) is bound in Wffl. To define this
predicate, we need one axiom for each quantifier, connec-
tive, and predicate letter of the target language. These
axioms are obvious and are therefore omitted.
We also need the predicate binds(Wffl,V), which means
that the outermost quantifier of Wff1 binds the variable V.
To define this predicate we need an axiom for each quanti-
fier and connective. Typical axioms are:
</bodyText>
<listItem confidence="0.8550475">
(53) binds(all(V,Wffl ,Wff2),V).
(54) binds(and(Wff 1 ,Wff2),V) binds(Wffl,V).
</listItem>
<bodyText confidence="0.9592225">
The second axiom applies to complex quantifiers arising
from conjoined NPs. In this case there are two branches,
but each branch binds the same variable (the rules for NP
conjunction ensure that this is so). Therefore, we recur-
sively check the first branch to find the bound variable.
Given these predicates, we can rewrite the second axiom
for &amp;quot;apply_quants&amp;quot;:
apply_quants(QL1,Wffl,QL3,Wff3)
choose(QL1,p(Wffl,Wff2),QL2),
highest_bound_var(Wffl,N),
binds(Wff2,v(s(N))),
apply_quants(QL2,Wff2,QL3,Wff3).
Wffl is the scope of the quantifier, and v(N) is the highest
bound variable of Wffl . The new quantifier binds the
variable v(s(N)), which is different from every bound
variable in the scope Wffl. Therefore, the new quantifier is
not shadowed by any lower quantifier.
As an example, suppose that QL1 is
(56) [p(Wff2,all(V2,woman(V2),Wff2))].
Then solutions for the goal
</bodyText>
<equation confidence="0.716318">
(57) :- apply_quants(QL1,1oves(VI,V2),QL3,Wff3)
include
Wff3 = all(v(1),woman(v(1)),loves(v(1),V2)))
QL3 = [].
</equation>
<bodyText confidence="0.801030571428571">
(We have reverted to standard notation for integers.) Sup-
pose that QL1 is
(59) [p(Wffl,some(VI,man(V1),Wff1)),p(Wff2,all
(V2,woman(V2), Wff2) )].
Then solutions for the goal
apply_quants(QL1,1oves(V1,V2),QL3,Wff3).
include
</bodyText>
<equation confidence="0.937517333333333">
Wff 3 = some(v(2),man(v(2),
all(v(1),woman(v(1),loves(v(2),v(1))))
QL3 = [1.
</equation>
<bodyText confidence="0.9997776">
The inner quantifier binds the variable v(1), and the outer
quantifier binds the variable v(2). This notation for vari-
ables is very hard to read, so in the rest of the paper we will
use the constants x, y, and z to represent variables of the
target language.
</bodyText>
<subsectionHeader confidence="0.623032">
2.5 RULES FOR NOUN PHRASES
</subsectionHeader>
<bodyText confidence="0.933177291666667">
The following grammar is very similar to the work of
Pereira and Shieber (1987, Sections 4.1 and 4.2). There are
two major differences, however, First, the treatment of
quantifiers and scoping uses a version of Cooper&apos;s quanti-
fier storage, instead of the &amp;quot;quantifier tree&amp;quot; of Pereira and
Shieber. Second, Pereira and Shieber started with a seman-
tics using lambda calculus, which they &amp;quot;encoded&amp;quot; in Pro-
log. In the present grammar, unification semantics stands
on its own—it is not a way of encoding some other formal-
ism.
Formula numbering uses the following conventions. The
rules of the grammar are numbered (R1), (R2), etc. En-
tries in the lexicon are numbered (L1), (L2), etc. Formulas
built in the course of a derivation get numbers without a
prefix. Groups of related rules are marked by lower case
letters: (L I a), (L 1 b), and so forth.
Every noun phrase has a quantifier store as one of its
semantic features. If the NP is a gap, the store is empty; if
the NP is not a gap, the first element of the store is the
quantifier generated by the NP (in the present grammar,
the quantifier store of an NP has at most one quantifier).
We represent the quantifier store as a difference list, using
the infix operator &amp;quot;-&amp;quot;. Thus if L2 is a tail of Ll, Li-L2 is
the list difference of LI and L2: the list formed by removing
L2 from the end of Li. Therefore a noun phrase has the
form np(V,QL1-QL2,Fx-Fy,VL). V is the bound variable
of the NP. QL1-QL2 is the quantifier store of the NP. We
describe wh-movement using the standard gap-threading
technique (Pereira and Shieber 1987), and Fx-Fy is the
filler list. Finally, VL is a list of target-language variables
representing NPs that are available for reference by a
pronoun, which we will call the pronoun reference list.
Consider an NP consisting of a determiner and a head
noun: &amp;quot;every man,&amp;quot; &amp;quot;no woman,&amp;quot; and so forth. The head
noun supplies the range restriction of the NP&apos;s quantifier,
and the determiner builds the quantifier given the range
restriction. The bound variable of the NP is a feature of
both the determiner and the head noun. Then the following
rule generates NPs consisting of a determiner and a head
Computational Linguistics Volume 16, Number 4, December 1990 223
Andrew R. Haas Sentential Semantics for Propositional Attitudes
noun:
(R1)
np(V,[QIQL]-QL,Fx-Fx,VL)
- det(V,Wffl,Q),n(V,Wff1).
The quantifier list [Q I QL] — QL = [Q] contains the
quantifier for the NP. We have the following rules for
common nouns:
</bodyText>
<equation confidence="0.998965">
(R2a) n(V1,pizza(V1)) [pizza]
(R2b) n(V1,man(V1)) [man]
(R2c) n(V1,woman(V1)) [woman].
</equation>
<bodyText confidence="0.659883">
Recall that p(Wffl,Wff2) is a quantifier that maps Wffl to
Wff2. Then for determiners we have
</bodyText>
<equation confidence="0.986821307692308">
(R3a) det(V2,Range,p(Wff1,some(V2,Range,Wff1)))—■
[a]
(R3b) det(V2,Range,p(Wffl,all(V2,Range,Wff1)))
[every]
(R3c) det(V2,Range,p(Wff1,unique(V2,Range, Wffl)))
- [the]
(R3d) det(V2,Range,p(Wffl,not(some(V2,Range,
Wff1)))) —■ [no].
Then we get
(62) np(V,[p(Wffl,all(V,man(V),Wff1))1 QL]-QL,Fx-
Fx,L) —■ [every man]
(63) np(V,[p(Wffl,not(some(V,woman(V),Wff1)))1
QL]-QL,Fx-Fx,L) [no woman].
</equation>
<bodyText confidence="0.9990604">
Thus &amp;quot;every man&amp;quot; is an NP that binds the variable V and
maps Wffl to all(V,man(V),Wff1).
Following Moore (1988), we interpret the proper name
&amp;quot;John&amp;quot; as equivalent to the definite description &amp;quot;the one
named &amp;quot;John.&amp;quot;&amp;quot;
</bodyText>
<equation confidence="0.677788333333333">
(R4)
np(V,[p(Wff,unique(V,name(V,C),Wff))I QL1-
QL,Fx-Fx,VL)
</equation>
<bodyText confidence="0.95570425">
- [Terminal], 1 proper_noun(Terminal,C) 1.
The wff proper_noun(X,Y) means that X is a proper noun
and Y is its logical form. Our lexicon includes the axioms
(LI a) proper_noun(johnjohn)
</bodyText>
<equation confidence="0.70699">
(Li b) proper_noun(mary,mary)
(Li c) proper_noun(bill,bill).
</equation>
<bodyText confidence="0.99986575">
These axioms use the constant &amp;quot;john&amp;quot; to denote both a
terminal symbol of the grammar and a constant of the
target language—a convenient abuse of notation. Using
(Lia) we get
</bodyText>
<listItem confidence="0.7091995">
(64) np(V,[p(Wffl,unique(V,name(V,john),Wff1))I QL]-
QL,Fx-Fx,VL) [john].
</listItem>
<bodyText confidence="0.818775923076923">
That is, &amp;quot;john&amp;quot; is an NP that binds the variable V and maps
Wffl to the wff unique(V,name(V,john),Wff1).
Pronouns use the &amp;quot;let&amp;quot; quantifier. We have
(R5)
np(V2,[p(V2,Wffl,let(V2,V,Wff1))1 QL]-QL,Fx-
Fx,VL)
[he], {member(V,VL)}.
If V is a variable chosen from the pronoun reference list
VL, then &amp;quot;he&amp;quot; is an NP that binds the variable V2 and
maps Wffl to let(V2,V,Wff1). Thus, the pronoun refers
back to a noun phrase whose bound variable is V. Later, we
wi:1 see the rules that put variables into the list VL. As an
example, we have
</bodyText>
<equation confidence="0.7691425">
(65) np(V2,[p(Wffl,let(V2,V1,Wff1))1
Fx,[V1]) [he].
</equation>
<bodyText confidence="0.999385125">
The &amp;quot;let&amp;quot; quantifier in pronouns looks redundant, but it
is useful because it makes the semantics of NPs uniform—
every NP (except gaps) has a quantifier. This is helpful in
describing conjoined NPs. Suppose that NP1 binds vari-
able V and maps Wffl to Wff2. Suppose NP2 also binds
variable V and maps the same Wffl to Wff3. Then the
conjunction of NP! and NP2 binds V and maps Wffl to
and(Wff2,Wff3):
</bodyText>
<equation confidence="0.8216744">
(R6) np(V,[p(Wffl,and(Wff2,Wff3))1 QL1]-QL3,Fx-
Fx,VL)
np(V, [p(Wff 1 ,Wff2)IQL1]-QL2,Fz-Fz,VL),
[and],
np(V,[p(Wffl,Wff3)I QL2]-QL3,Fy-Fy,VL).
</equation>
<bodyText confidence="0.774963">
As an example we have
</bodyText>
<equation confidence="0.8233555">
(66) np(V,[p(Wffl,all(V,man(V),Wff1))I QL1]-QL1,Fx-
Fx,L) [every man]
(67) np(V,[p(Wffl,all(V,woman(V),Wff1))I QL2]-
QL2,Fx-Fx,L) [every woman]
np(V,
[p(Wffl,and(all(V,man(V),Wff1),all(V,woman
(V),Wff1))) QL1]-QL1,
Fx-Fx,VL)
</equation>
<bodyText confidence="0.912896666666667">
—[every man and every woman].
That is, &amp;quot;every man and every woman&amp;quot; is an NP that binds
variable V and maps Wffl to
</bodyText>
<listItem confidence="0.9267975">
(69) and(all(V,man(V),Wff1),all(V,woman(V),Wff1)).
We also have
(70) np(V,[p(Wffl,let(V,V1,Wff1))I QL1]-QL1,Fx-
Fx,[V1]) [he]
(71) np(V,[p(Wffl,unique(V,name(V,john),Wff1))1
QL2]-QL2,Fx-Fx,VL) [john]
(72) np(V, [p(Wffl,and(let(V,V1,Wff1),unique(V,name
(V,john),Wff1)))1 QL1]-QL1,
</listItem>
<bodyText confidence="0.708144">
Fx-Fx,[V1])
[he and john].
That is, &amp;quot;he and John&amp;quot; is an NP that binds V and maps
Wffl to
</bodyText>
<page confidence="0.605889">
(73) and(let(V,V1,Wff1),unique(V,name(V,john),Wffl)
224 Computational Linguistics Volume 16, Number 4, December 1990
</page>
<note confidence="0.496181">
Andrew R. Haas Sentential Semantics for Propositional Attitudes
</note>
<bodyText confidence="0.986461423076923">
where V1 is chosen from the pronoun reference list. Thus
the conjunction rule works for pronouns and proper nouns
exactly as it does for NPs with determiners. There is a
similar rule for disjunction of NPs.
In conjoining two NPs we combine their quantifiers,
which are the first elements of their quantifier stores. We
must also collect the remaining elements of both quantifier
stores. The above rule achieves this result by concatenating
difference lists in the usual way: if QL1-QL2 is the tail of
the first NP&apos;s quantifier list, and QL2-QL3 is the tail of the
second NP&apos;s quantifier list, then QL1-QL3 is the concatena-
tion of the tails. In the present grammar both tails are
empty, because the quantifier store of an NP contains at
most one quantifier, but in a more general grammar the
tails might contain quantifiers—for example, quantifiers
from prepositional phrases modifying the NP. Thus the
Montague-style semantics for NP conjunction and disjunc-
tion requires an extension of standard Cooper storage.
When the quantifier store of an NP contains several quanti-
fiers, we must be able to identify the one that represents the
NP itself (as opposed to quantifiers that arise from at-
tached PPs, for example). We must then be able to remove
this quantifier from the store, build a new quantifier, and
put the new quantifier back into the store.
Rule (R6) requires that the two NPs being conjoined
should have quantifiers that bind the same variable. Sup-
pose we had chosen the bound variables in the logical forms
by using a global counter to ensure that no two quantifiers
ever bind the same variable (as suggested in Section 2.4).
Then (R6) could never apply. Thus our treatment of NP
conjunction forces us to choose the bound variables after
the quantifiers from conjoined NPs have been combined
into a single quantifier, as described in Section 2.4. This
choice in turn creates difficulties in implementing our
treatment of de re attitude reports, as we will see in Section
3.1.
In this grammar, a conjunction of quantified NPs pro-
duces a logical form in which the two quantifiers are in
separate wffs, and these wffs are joined by the connective
and. Thus, neither quantifier is in the scope of the other.
This gives the desired reading for a sentence such as &amp;quot;John
has no house and no car&amp;quot;:
(74) and (not(some(x,house(x),has(john,x))),not(some
(x,car(x), has( john,x)))).
However, consider the sentence &amp;quot;John met a farmer and his
wife&amp;quot; and suppose the pronoun &amp;quot;his&amp;quot; refers to &amp;quot;a farmer.&amp;quot;
Under our analysis, the quantifier from &amp;quot;a farmer&amp;quot; cannot
bind a variable in the range restriction of the other quanti-
fier—because its scope does not include the other quanti-
fier. Thus, the Montagovian analysis of NP conjunction is
certainly correct in some cases, but it cannot be the whole
story.
</bodyText>
<subsectionHeader confidence="0.968659">
2.6 VERB PHRASE AND SENTENCE RULES
</subsectionHeader>
<bodyText confidence="0.996107724137931">
Our grammar includes two kinds of transitive verbs: ordi-
nary verbs like &amp;quot;eat&amp;quot; and &amp;quot;buy,&amp;quot; and propositional attitude
verbs like &amp;quot;want&amp;quot; and &amp;quot;seek.&amp;quot; Only verbs of the second
kind have de dicto readings. There is a de dicto reading for
&amp;quot;John wants a Ferrari,&amp;quot; which does not imply that there is
any particular Ferrari he wants. There is no such reading
for &amp;quot;John bought a Ferrari.&amp;quot; To build a de dicto reading, a
verb like &amp;quot;want&amp;quot; must have access to the quantifier of its
direct object. Verbs like &amp;quot;buy&amp;quot; do not need this access. This
leads to a problem that has been well known since Mon-
tague. The two kinds of verbs, although very different in
their semantics, seem to be identical in their syntax. We
would like to avoid duplication in our syntax by writing a
single rule for VPs with transitive verbs. This rule must
allow for both kinds of semantics.
Montague&apos;s solution was to build a general semantic
representation, which handles both cases. When the verb is
&amp;quot;eat&amp;quot; or &amp;quot;buy,&amp;quot; one uses a meaning postulate to simplify
the representation. Our solution is similar: we allow every
transitive verb to have access to the quantifier of its direct
object, and then assert that some verbs don&apos;t actually use
the quantifier. However, our solution improves on Mon-
tague and Cooper by avoiding the simplification step. In-
stead, we build a simple representation in the first place.
A verb has one feature, the subcategorization frame,
which determines what arguments it will accept and what
logical form it builds. The rule for verbs says that if a
terminal symbol has a subcategorization frame Subcat,
then it is a verb:
</bodyText>
<equation confidence="0.96606">
(R7) v(Subcat) [Terminal], has_subcat(Terminal,
Subcat) 1.
</equation>
<bodyText confidence="0.851270047619048">
A subcategorization frame for a transitive verb has the
form
(75) trans(V1,V2,QL1,QL2,Wff1).
V1 is a variable representing the subject, and V2 is a
variable representing the object. QL1 is the quantifier store
of the object. QL2 is a list of quantifiers remaining after the
verb has built its logical form. For an ordinary transitive
verb, QL2 equals QL1. Wffl is the logical form of the verb.
In the case of ordinary transitive verbs, we would like to
assert once and for all that QL1 = QL2. Therefore, we
write
(L2)
has_subcat(Terminal,trans(V1,V2,QL1,QL1,Wff))
ordinary_trans(Terminal,V1,V2,Wff).
This axiom says that for an ordinary transitive verb, the
two lists of quantifiers are equal, and the values of the other
features are fixed by the predicate &amp;quot;ordinary_trans.&amp;quot; We
have
(L3a) ordinary_trans(saw,V1,V2,saw(V1,V2))
(L3b) ordinary_trans(ate,V1,V2,ate(V1,V2)).
From (R7), (L2), and (L3a) we get
</bodyText>
<listItem confidence="0.611777">
(76) v(trans(V1,V2,QL1,QL1,saw(V1,V2))) [saw].
</listItem>
<bodyText confidence="0.798767071428571">
Computational Linguistics Volume 16, Number 4, December 1990 225
Andrew R. Haas Sentential Semantics for Propositional Attitudes
The features of a verb phrase are a variable (representing
the subject), a wff (the logical form of the VP), a quantifier
store, a list of fillers, and a pronoun reference list. The rule
for a verb phrase with a transitive verb is
(R8)
vp(V1,W ff 1 ,QL2,Fx-Fy,L)
v(trans(V1,V2,QL1,QL2,Wff1)),
np(V2,QL1,Fx-Fy,L).
If the verb is an ordinary transitive verb, then QL1 = QL2,
so the quantifier store of the VP is equal to the quantifier
store of the direct object. From (R1), (R3a), and (R2b) we
have
</bodyText>
<equation confidence="0.9669956">
(77) np(V, [p(Wff 1 ,some(V,man(V),Wff1))1Q11 -QL,Fx-
Fx,L) [a man].
Resolving (76) and (77) against the right side of R8 gives
vp(VI,saw(V1,V2),[p(Wffl,some(V2,man(V2),
Wff1))1 QL1-QL,Fx-Fx,L)
</equation>
<bodyText confidence="0.941531333333333">
[saw a man].
The quantifier store contains the quantifier of the NP &amp;quot;a
man.&amp;quot;
A sentence has four features: a wff, a quantifier store, a
list of fillers, and a pronoun reference list. The rule for a
declarative sentence is
</bodyText>
<equation confidence="0.9797082">
(R9)
s(Wff2,QL4,Fx-Fz,L)
np(V,QL1-QL2,Fx-Fy,L),
vp(V,Wff 1 ,QL2-QL3,Fy-Fz,L),
1 apply_quants(QL1-QL3,Wff 1 ,QL4,Wff2) 1.
</equation>
<bodyText confidence="0.999179714285714">
The variable V represents the subject, so it becomes the
first argument of the VP. QL1-QL3 is the concatenation of
the quantifier stores from the subject and the VP.
&amp;quot;Apply_quants&amp;quot; will apply some of these quantifiers to the
logical form of the VP to produce the logical form Wff2 of
the sentence. The list QL4 of remaining quantifiers be-
comes the quantifier store of the sentence. We have
</bodyText>
<equation confidence="0.59424">
np(V1, [p(Wff0,all(V1,woman(V1),Wff0))1 QL11-
QL1,Fx-Fx,L)
</equation>
<bodyText confidence="0.981188333333333">
[every woman].
From (R9), (79), and (78), we get
s(Wff2,QL4,Fx-Fx,L)
[every woman saw a man],
lapply_quants([p(Wff0,all(V1,woman(V1),Wff0)),
p(Wffl ,some(V2,man(V2),Wff1))1QL31-
</bodyText>
<page confidence="0.976083333333333">
saw(V1,V2),
Wff2)
1.
</page>
<bodyText confidence="0.999411333333333">
The &amp;quot;apply_quants&amp;quot; subgoal has several solutions. Choos-
ing the one in which &amp;quot;every woman&amp;quot; outscopes &amp;quot;a man,&amp;quot; we
get
</bodyText>
<equation confidence="0.54101">
(81) s(all(x,woman(x),some(y,man(y),saw(x,y))),QL3-
QL3,Fx-Fx,L) [every woman saw a man].
</equation>
<bodyText confidence="0.996894060606061">
The derivation is not yet complete, because &amp;quot;s&amp;quot; is not the
start symbol of our grammar. Instead we use a special
symbol &amp;quot;start,&amp;quot; which never appears on the right side of a
rule. Thus, the start symbol derives only top-level sen-
tences—it cannot derive an embedded sentence. This is
useful because top-level sentences have a unique semantic
property: their logical forms must not contain free vari-
ables. It might seem that one can eliminate free variables
simply by applying all the quantifiers in the store. Hobbs
and Shieber (1987) pointed out that this is not so—it is
essential to apply the quantifiers in a proper order. Con-
sider the sentence &amp;quot;every man knows a woman who loves
him,&amp;quot; with &amp;quot;him&amp;quot; referring to the subject. The subject
quantifier binds a variable that occurs free in the range
restriction of the object quantifier, so one must apply the
object quantifier first in order to eliminate all free vari-
ables.
Therefore our grammar includes a filter that eliminates
readings of top-level sentences containing free variables.
Let free_vars(Wff 1 ,L) mean that L is a list of the free
variables of Wffl in order of first occurrence. We omit the
easy definition of this predicate. The rule for top-level
sentences is:
(R10) start(Wffl) s(Wffl,QL-QL,Fx-Fx,[]),
free_vars(Wffl , []).
The goal free_vars(Wffl,[]) filters out readings with free
variables. The above rule allows us to complete the deriva-
tion for &amp;quot;every woman saw a man&amp;quot;:
(82) start(all(x,woman(x),some(y,man(y),saw(x,y))))
[every woman saw a man].
Having treated sentences, we can now consider gaps and
relative clauses. The rule for gaps follows Pereira and
Schieber (1987):
</bodyText>
<equation confidence="0.500651">
(R11) np(V,QL-QL, [gap(V)1 Fx]-Fx,VL) [].
</equation>
<bodyText confidence="0.9907698">
This rule removes the marker gap(V) from the filler list,
and makes the associated variable V the variable of the
empty NP. The list difference QL-QL is the empty list, so
the quantifier store of the gap is empty.
The rule that generates NPs with relative clauses is
np(V, [Q1 QL]-QL,Fx-Fx,L)
det(V,and(Rangel,Range2),Q),
n(V,Rangel),
[that],
s(Range2,QL1-QL1,[gap(V)]-[],[]).
</bodyText>
<page confidence="0.87621">
226 Computational Linguistics Volume 16, Number 4, December 1990
</page>
<note confidence="0.624058">
Andrew R. Haas Sentential Semantics for Propositional Attitudes
</note>
<bodyText confidence="0.9998284">
The relative clause is a sentence containing a gap, and the
logical form of the gap is the variable V—the same variable
that the quantifier binds. The logical form of the relative
clause becomes part of the range restriction of the quanti-
fier. We have
</bodyText>
<listItem confidence="0.5487365">
(83) s(some(x,pizza(x),ate(V,x)),QL-QL,[gap(V)I Fx]-
Fx,[]) [ate a pizza].
</listItem>
<bodyText confidence="0.959566107142857">
The derivation of this sentence is much like the one for
&amp;quot;every woman saw a man&amp;quot; above, except that in place of
the subject &amp;quot;every woman&amp;quot; we have a gap as a subject. The
rule for gaps above ensures that the variable of the gap is V,
the only variable in the filler list, and its quantifier store is
empty. Therefore, V appears as the first argument of the
predicate &amp;quot;ate.&amp;quot; Continuing the derivation we get
(84) np( V, [ p ( W ffl ,some (V,and (man ( V),some (x,pizza
(x),ate(V,x))),Wff QL]-QL,Fx-Fx,[])
man that ate a pizza].
The string &amp;quot;a man that ate a pizza&amp;quot; is an NP that binds V
and maps Wffl to the wff
(85) some(V,and(man(V),some(x,pizza(x),ate(V,x))),
Wffl).
Notice that in the rule for NPs with relative clauses, the
quantifier store of the relative clause is empty. This means
that no quantifier can be raised out of a relative clause.
Thus there is no scope ambiguity in &amp;quot;I saw a man that loves
every woman.&amp;quot; According to Cooper (1979), this is correct.
The restriction is easy to state because in our grammar,
quantifier raising is combined with syntax and semantics in
a single set of rules. It would be harder to state the same
facts in a grammar like Pereira and Shieber&apos;s (1987),
because quantifier raising there operates on a separate
representation called a quantifier tree. This tree leaves out
syntactic information that is needed for determining
scopes—for example, the difference between a relative
clause and a prepositional phrase.
</bodyText>
<sectionHeader confidence="0.99209" genericHeader="method">
3 PROPOSITIONAL ATTITUDES IN THE
GRAMMAR
</sectionHeader>
<subsectionHeader confidence="0.997952">
3.1 ATTITUDE VERBS TAKING CLAUSES
</subsectionHeader>
<bodyText confidence="0.91161424">
The following rule introduces verbs such as &amp;quot;believe&amp;quot; and
&amp;quot;know,&amp;quot; which take clauses as their objects.
(R13)
vp(V1,Wff1,QL1,Fx,L)
v(takes_s(V1,Wff2,Wff1)),
s(Wff2,QL1,Fx,[V1 I L]).
The verb takes the logical form Wff2 of the object clause
and the subject variable V1, and builds the wff Wffl
representing the VP. This rule also adds the subject vari-
able to the pronoun reference list of the object clause. For
the verb &amp;quot;thought,&amp;quot; we have the following subcategoriza-
tion frame:
(L4)
has_subcat(thought,
takes_s(V1,Wffl,
thought(VI,Varsl,q(Wff1))))
free_vars(Wffl,Vars1).
The subject variable becomes the first argument of the
predicate &amp;quot;thought.&amp;quot; The logical form of the object clause
is Wffl, and it appears under a quotation mark as the third
argument of &amp;quot;thought.&amp;quot; The second argument of &amp;quot;thought&amp;quot;
is the de re argument list, and the predicate &amp;quot;free_vars&amp;quot;
ensures that the de re argument list is a list of the free
variables in Wffl, as required by our convention. From the
rule (R8) for verbs and (L4), we get
</bodyText>
<equation confidence="0.969572333333333">
(86)
v(takes_s(V1,Wff2,thought(V1,Varsl,q(Wff2))))
- [thought], { free_vars(Wff2,Vars1) }.
</equation>
<bodyText confidence="0.853594255319149">
The &amp;quot;free_vars&amp;quot; subgoal has not been solved—it has been
postponed. Indeed it must be postponed, because as long as
its first argument is a variable, it has an infinite number of
solutions—one for each wff of our language.
Consider the example &amp;quot;John thought Mary ate a pizza.&amp;quot;
We consider two readings. &amp;quot;A pizza&amp;quot; is understood de dicto
in both readings, but &amp;quot;Mary&amp;quot; is de re in one reading and de
dicto in the other. The ambiguity arises from the embedded
sentence, because the predicate &amp;quot;apply_quants&amp;quot; can either
apply the quantifiers or leave them in the store. If it applies
the quantifiers from &amp;quot;Mary&amp;quot; and &amp;quot;a pizza&amp;quot; in their surface
order, we get
(87)
s(unique(y,name(y,mary),some(x,pizza(x),ate
(y,x)),QL-QL, Fx-Fx,L)
- [mary ate a pizza].
From (R13), (86), and (87) we get
(88)
vp(V1,
thought(V1,Varsl,
q(unique(y,name(y,mary),some(x,pizza
(x),ate(y,x))))),
QL-QL,Fx-Fx,L)
—÷ [thought Mary ate a pizza],
{free_vars(unique(y,name(y,mary),some(x,piz-
za(x), ate(y,x))),Vars1) 1.
Since the first argument of &amp;quot;free_vars&amp;quot; is now a ground
term, we can solve the &amp;quot;free_vars&amp;quot; subgoal, getting
Varsl = []. Then we have
(89)
vp(V1,
thought(V1,[],q(unique(y,name(y,mary),some(x,
pizza(x),ate(y,x))))),
QL-QL,Fx-Fx,L)
- [thought Mary ate a pizza].
Computational Linguistics Volume 16, Number 4, December 1990 227
Andrew R. Haas Sentential Semantics for Propositional Attitudes
If we combine this VP with the subject &amp;quot;John&amp;quot; we get a
sentence whose logical form is
unique(z,name(z,john),
thought(z, [],q(unique(y,name(y,mary),some
(x,pizza(x),ate(y,x)))))).
Now for the reading in which &amp;quot;mary&amp;quot; is de re. Once again,
consider the embedded sentence &amp;quot;Mary ate a pizza.&amp;quot; Sup-
pose that the predicate &amp;quot;apply_quants&amp;quot; applies the quanti-
fier from &amp;quot;a pizza&amp;quot; and leaves the one from &amp;quot;Mary&amp;quot; in the
store. We get
</bodyText>
<equation confidence="0.8224076">
(91) s(some(x,pizza(x),ate(V1,x)),
[P(Wffl,unique(V1,name(V1,mary),Wffl ))
QL]-QL,
Fx-Fx,
L)
</equation>
<bodyText confidence="0.921829083333333">
[Mary ate a pizza].
VI is the bound variable of the NP &amp;quot;Mary.&amp;quot; The predicate
&amp;quot;apply_quants&amp;quot; will choose a value for VI when it applies
the quantifier. From (R13), (86), and (91), we get
vp(V2,
thought(V2,Varsl,q(some(x,pizza(x),ate(V1,
x))))),
[p(Wffl,unique(V1,name(V1,mary),Wff
QL]-QL,
Fx-Fx,
L)
[thought Mary ate a pizza],
</bodyText>
<equation confidence="0.6949">
free_vars(some(x,pizza(x),ate(V1,x)),Vars1) 1.
</equation>
<bodyText confidence="0.999823411764706">
In this case, the first argument of &amp;quot;free_vars&amp;quot; contains the
meta-language variable Vi. Then the &amp;quot;free_vars&amp;quot; subgoal
has an infinity of solutions—one in which VI = x and there
are no free variables, and an infinite number in which V1 —
y, for some y not equal to x, and the list of free variables is
[y]. Therefore, it is necessary to postpone the &amp;quot;free_vars&amp;quot;
subgoal once more. The standard technique for parsing
DCGs does not allow for this postponing of subgoals, and
this will create a problem for our implementation.
This problem would be greatly simplified if we had
chosen to assign a different variable to every quantifier by
using a global counter. The DCG parser would work from
left to right and assign a target-language variable to each
NP as soon as it parsed that NP. In the above example,
&amp;quot;Mary&amp;quot; and &amp;quot;a pizza&amp;quot; would both have their variables
assigned by the time we reached the right end of the VP.
Then we could handle the &amp;quot;free_vars&amp;quot; subgoals by rewriting
the grammar as follows: remove the &amp;quot;free_vars&amp;quot; subgoals
from the lexical entries for the attitude verbs, and place a
&amp;quot;free_vars&amp;quot; subgoal at the right end of each VP rule that
introduces an attitude verb ((R 13), (R15), and (R8)). This
would ensure that when the parser attempted to solve the
&amp;quot;free_vars&amp;quot; subgoal, its first argument would be a ground
term. However, this solution would make it impossible to
use the rule (R6) for NP conjunction (see Section 2.5). If
we pick one solution for the problem of choosing bound
variables, we have problems with NP conjunction; if we
pick the other solution we get problems in implementing
our analysis of de re attitude reports. This is the kind of
difficulty that we cannot even notice, let alone solve, until
we write formal grammars that cover a reasonable variety
of phenomena.
Continuing our derivation, we combine the VP with the
subject &amp;quot;John,&amp;quot; apply the quantifier from &amp;quot;Mary,&amp;quot; and get
</bodyText>
<equation confidence="0.875176142857143">
(93) s(unique(z,name(z,john),
unique(y,name(y,mary),
thought(z,Varsl,q(some(x,pizza (x),ate
(M)))))),
QL-QL,Fx-Fx,L)
[John thought Mary ate a pizza],
1 free_vars(some(x,pizza(x),ate(y,x)),Vars1) 1.
</equation>
<bodyText confidence="0.974627484848485">
Now the first argument of &amp;quot;free_vars&amp;quot; is a ground term,
because applying the quantifier that arose from &amp;quot;Mary&amp;quot;
includes choosing the target language variable that the
quantifier binds. The &amp;quot;free_vars&amp;quot; subgoal now has only one
solution, Vars 1 = [y]. Then the logical form of the sen-
tence is
(94) unique(z,name(z,john),
unique(y,name(y,mary),
thought(z,[y],q(some(x,pizza(x),
ate(y,x)))))).
This means that there is a term T that represents Mary to
John, and John believes the sentence
(95) some(x,pizza(x),ate(T,x)).
For the sentence &amp;quot;John thought he saw Mary,&amp;quot; our limited
treatment of pronouns allows only one reading, in which
&amp;quot;he&amp;quot; refers to John. Using (R9), we get the following
reading for the embedded clause:
s(let(y,V1,unique(x,name(x,mary),saw(y,x)),QL-
QL,Fx-Fx, [ V1])
[he saw Mary].
The pronoun &amp;quot;he&amp;quot; gives rise to a &amp;quot;let&amp;quot; quantifier, which
binds the variable y to VI, the first member of the pronoun
reference list. From (R13), (86), and (96) we get
vp(V2,thought(V2,Vars1,q(let(y,V2,unique(x,name
(x,mary), saw(y,x)),
QL-QL,Fx-Fx,[])
[thought he saw Mary],
{free_vars(let(y,V2,unique(x,name(x,mary),saw
(y,x))),Vars1) 1.
The VP rule (R13) unifies the subject variable V2 with the
first element of the pronoun reference list of the embedded
clause, so the &amp;quot;let&amp;quot; quantifier now binds the variable y to
the subject variable. Once again, we postpone the &amp;quot;free_vars&amp;quot;
</bodyText>
<page confidence="0.838458">
228 Computational Linguistics Volume 16, Number 4, December 1990
</page>
<note confidence="0.585095">
Andrew R. Haas Sentential Semantics for Propositional Attitudes
</note>
<bodyText confidence="0.976320181818182">
goal until its first argument is a ground term. Combining
this VP with the subject &amp;quot;John&amp;quot; gives
s(unique(z,name(z,john),
thought(z,Varsl,q(let(y,z,unique(x,name(x,mary),
saw(y,x)))))),
QL-QL,Fx-Fx,VL)
—. [John thought he saw Mary],
1free_vars(let(y,z,unique(x,name(x,mary),saw(y,x))),
Varsl) 1.
The first argument of &amp;quot;free_vars&amp;quot; is now a ground term, and
solving the &amp;quot;free_vars&amp;quot; subgoal gives Varsl = [z]. The
logical form of the sentence is
unique(z,name(z,john),
thought(z,[z],q(let(y,z,
unique(x,name(x,mary),
saw(y,x)))))).
The dummy variable z stands for a term T that represents
John to himself. Then John&apos;s belief looks like this:
let(y,T,
unique(x,name(x,mary),
saw(y,x))).
If John simplifies this belief, he will infer
</bodyText>
<listItem confidence="0.50984">
(101) unique(x,name(x,mary),saw(T,x)).
</listItem>
<subsectionHeader confidence="0.846197">
3.2 ATTITUDE VERBS TAKING A CLAUSE
WITH A GAP
</subsectionHeader>
<bodyText confidence="0.9996345">
We proposed the following logical form for &amp;quot;John knows
who Mary likes&amp;quot;:
</bodyText>
<listItem confidence="0.557508">
(102) some(x,person(x),know(john,[x],q(like(mary,x)))).
</listItem>
<bodyText confidence="0.99781325">
The grammar will generate a similar logical form, except
for the translations of the proper nouns. The existential
quantifier comes from the word &amp;quot;who.&amp;quot; The rules for
&amp;quot;who&amp;quot; and &amp;quot;what&amp;quot; are
</bodyText>
<equation confidence="0.7428705">
(R14a) wh(V1,[p(Wffl,some(V1,and(person(V1),
Wff1)))I QL]-QL) ---. [who]
(R14b) wh(V1,[p(Wffl,some(V1,and(thing(V1),
Wffl))) I QL]-QL) --. [what].
</equation>
<bodyText confidence="0.915803133333333">
The semantic features of a wh word are a variable, and a
list containing a quantifier that binds that variable.
The following rule builds VPs in which the verb takes a
wh word and a clause as its objects:
(R15)
vp(V1,Wff3,QL1,Fx-Fx,L)
--. v(takes_wh(V1,Wff 1 ,Wff2)),
wh(V,QL0),
s(Wffl,QL1,[gap(V)]-[],[V1 I Li),
1 apply_quants(QLO,Wff2,QL-QL,Wff3)}.
The embedded S contains a gap, and the variable of that
gap is the one bound by the quantifier from the wh word.
The main verb takes the subject variable and the logical
form of the embedded S and builds a wff Wff2. The rule
finally calls &amp;quot;apply_quants&amp;quot; to apply the quantifier from the
wh word to Wff2. &amp;quot;Apply_quants&amp;quot; can apply any subset of
the quantifiers in its first argument, but the rule requires
the output list of quantifiers to be empty, and this guaran-
tees that the quantifier from the wh word will actually be
applied. The resulting wff becomes the logical form of the
VP.
The rule requires a verb whose subcategorization frame
has the form takes_wh(V1,Wffl,Wff2). &amp;quot;Know&amp;quot; is such a
verb:
(L5)
has_subcat(knows,
takes_wh(VI,Wffl,know(VI,Varsl,q(Wff1))))
:- free_vars(Wffl,Vars1).
Combining this clause with the rule (R7) for verbs gives
(103)
v(takes_wh(V1,Wffl,know(V1,Varsl,q(Wff1))))
—÷ [knows], 1 free_vars(Wffl,Vars1) 1.
Consider the example &amp;quot;John knows who Mary likes,&amp;quot; and
suppose &amp;quot;Mary&amp;quot; is understood de dicto. The embedded S
has the following reading:
(104)
s(unique(x,name(x,mary),like(x,V1)),QL-QL,
[gap(V1) IFx]-Fx,L)
—÷ [Mary likes].
The object of &amp;quot;likes&amp;quot; is a gap, so the variable V1 from the
filler list becomes the second argument of the predicate
&amp;quot;like.&amp;quot; Resolving (103), (R14a), and (104) against the
right side of (R15) gives
vp(V2,Wff3,QL-QL,Fx-Fx,L)
—■ [knows who Mary likes],
</bodyText>
<equation confidence="0.811322888888889">
1 apply_quants([p(Wffl,some(VI,person(V1),
Wffl))
I QL]-QL,
know(V2,Varsl,q(unique(x,name(x,mary),
like(x,V1))),
QL-QL,
Wff3),
1free_vars(unique(x,name(x,mary),like(x,V1)),
Varsl) 1.
</equation>
<bodyText confidence="0.9019516">
Solving the &amp;quot;apply_quants&amp;quot; subgoal gives
Wff3 = some(y,person(y),
know(V2,Varsl,q(unique(x,name(x,mary),
like (x,y))))).
Solving the &amp;quot;free_vars&amp;quot; subgoal gives Varsl = [y], and we
</bodyText>
<figure confidence="0.917989076923077">
Computational Linguistics Volume 16, Number 4, December 1990 229
Andrew R. Haas Sentential Semantics for Propositional Attitudes
then have
Wff3 = some(y,person(y),
know(V2,[y],q(unique(x,name(x,mary),
like(x,y))))).
Therefore
vp(V2,
some (y,person(y),
know(V2,[y],q(unique(x,name(x,mary),like
(x,Y))))),
QL- QL,Fx-Fx,L)
[knows who Mary likes].
</figure>
<bodyText confidence="0.930702">
This VP combines with the subject &amp;quot;John&amp;quot; in the usual way
to give a sentence whose logical form is
unique(z,name(z,john),
some(y,person(y),
know(z,[y],q(unique(x,name(x,mary),
like(x,y)))))).
</bodyText>
<subsectionHeader confidence="0.996084">
3.3 ATTITUDE VERBS TAKING A NOUN PHRASE
</subsectionHeader>
<bodyText confidence="0.9845208">
Finally, we consider an example with &amp;quot;want.&amp;quot; This verb is
semantically very different from most transitive verbs, but
syntactically it is an ordinary transitive verb, introduced by
the rule already given:
(R8)
vp(VI,Wffl,QL2,Fx-Fy,L)
v(trans(V1,V2,QL1,QL2,Wff1)),
np(V2,QL1,Fx-Fy,L).
The difference between &amp;quot;want&amp;quot; and other transitive verbs is
in its subcategorization frame:
</bodyText>
<equation confidence="0.67427">
(L6)
has_subcat(wants,
trans(V1,V2,QL1,QL2,wish(V1,Varsl,q
(Wff1))))
</equation>
<bodyText confidence="0.9165805">
apply_quants(QL1,have(V1,V2),QL2,Wff1),
free_vars(Wffl,Vars1).
Resolving this rule against the verb rule (R7) gives the
following rule for the verb &amp;quot;wants&amp;quot;:
</bodyText>
<equation confidence="0.7941928">
v(trans(V1,V2,QL1,QL2,wish(V1,Varsl,q(Wff1))))
[wants],
1 apply_quants(QL1,have(V1,V2),QL2,Wff1),
free_vars(Wffl,Vars1)
1
</equation>
<bodyText confidence="0.999492545454545">
The quantifier list QL1 contains the quantifier from the
object NP. The predicate &amp;quot;apply_quants&amp;quot; may or may not
apply this quantifier to the wff have(V1,V2), and this
nondeterminism gives rise to a de re/de dicto ambiguity. If
&amp;quot;apply_quants&amp;quot; does not apply the object quantifier, then
QL2 = QL1, so the object quantifier is passed up for later
application. Otherwise, QL2 is the empty list. As usual, the
predicate &amp;quot;free_vars&amp;quot; ensures that the de re arguments obey
our convention.
Consider the VP &amp;quot;wants a Porsche.&amp;quot; The object &amp;quot;a
Porsche&amp;quot; has the following interpretation:
</bodyText>
<figure confidence="0.790998368421053">
np(V2,[p,(Wff0,some(V2,porsche(V2),Wff0))I QL1-
QL,Fx-Fx,VL)
[a porsche].
Resolving (110) and (111) against the left side of (R7)
gives
vp(V1,wish(V1,Varsl,q(Wff1)),QL2,Fx-Fx,VL)
[wants a porsche],
{apply_quants([p(Wff0,some(V2,porsche(V2),
Wff0))1 QL]-QL,
have(V1,V2),
QL2,
Wffl),
free_vars(Wffl,Vars1)
1.
One solution of the &amp;quot;apply_quants&amp;quot; subgoal is
Wffl = some(x,porsche(x),have(V1,x))
QL2 = QLO-QLO.
Given this solution, the logical form of the VP is
(114) wish(V1,Varsl,q(some(x,porsche(x),have(V1,x))))
</figure>
<bodyText confidence="0.99066575">
where V1 is the subject variable and the &amp;quot;free_vars&amp;quot; sub-
goal has been postponed. We can combine this VP with the
subject &amp;quot;John&amp;quot; to get a sentence whose logical form is
unique(y,name(y,john),
wish(y,Varsl,q(some(x,porsche(x),have
(M))))).
Solving the &amp;quot;free_vars&amp;quot; subgoal will then give Varsl = [y],
so the final logical form is
unique(y,name(y,john),
wish(y,[y],q(some(x,porsche(x),have(y,x))))).
This means that there is a term T that represents John to
himself, and the sentence that John wishes to be true is
</bodyText>
<listItem confidence="0.539263">
(117) some(x,porsche(x),have(T,x)).
</listItem>
<bodyText confidence="0.990778333333333">
This is a de dicto reading—there is not any particular
Porsche that John wants.
The other solution for the &amp;quot;apply_quants&amp;quot; subgoal is
</bodyText>
<equation confidence="0.813827666666667">
Wffl = have(V1,V2)
QL2 = [p(Wff0,some(V2,porsche(V2),Wff0))I
QL]-QL.
</equation>
<page confidence="0.812356">
230 Computational Linguistics Volume 16, Number 4, December 1990
</page>
<note confidence="0.565522">
Andrew R. Haas Sentential Semantics for Propositional Attitudes
</note>
<bodyText confidence="0.992439254901961">
In this case, the logical form of the VP is
(119) wish (V1,Vars 1 ,q(have(V1,V2)))
and its quantifier store is equal to QL2. Combining this VP
with the subject &amp;quot;John&amp;quot; and applying the quantifiers gives
a sentence whose logical form is
unique(y,name(y,john),
some(x,porsche(x),
wish (y,Varsl,q (have(y,x)))) ).
Solving the &amp;quot;free_vars&amp;quot; subgoal gives Varsl = [y,x] so the
final logical form is
unique(y,name(y,john),
some(x,porsche(x),
wish (y,[y,x],q(have(y,x))))).
This means that there exist terms T1 and T2 such that Ti
represents John to himself, T2 represents some Porsche to
John, and the sentence John wishes to be true is
(122) have(T 1 ,T2)
This is a de re reading, in which John wants some particular
Porsche.
The rules for verbs that take clauses as complements did
not need to call &amp;quot;apply_quants,&amp;quot; because the rules that
build the clauses will call &amp;quot;apply_quants&amp;quot; and so create the
desired ambiguity. In Cooper&apos;s grammar, all NPs have the
option of applying their quantifiers, and so there is no need
for verbs like &amp;quot;want&amp;quot; to apply quantifiers—they can rely on
the rule that built the verb&apos;s object, just as other intensional
verbs do. This is a minor advantage of Cooper&apos;s grammar.
standard parser becomes efficient. In particular, the rule
for top-level clauses calls a Prolog predicate that finds all de
re argument lists in the final logical form and calls
&amp;quot;free_vars&amp;quot; for each one.
There is a similar problem about the predicate
&amp;quot;apply_quants&amp;quot; in the rule for &amp;quot;want.&amp;quot; Since the parser
works left to right, the quantifier from the object of &amp;quot;want&amp;quot;
is not available when the logical form for the verb is being
constructed. This means that the first argument of
&amp;quot;apply_quants&amp;quot; is a free variable—so it has an infinite
number of solutions. Here the implementation takes advan-
tage of Prolog&apos;s &amp;quot;call&amp;quot; predicate, which allows us to delay
the solution of a subgoal. The &amp;quot;apply_quants&amp;quot; subgoal is an
extra feature of the verb &amp;quot;want&amp;quot; (in the case of an ordinary
transitive verb, this feature is set to the empty list of goals).
The rule for VPs with transitive verbs uses the &amp;quot;call&amp;quot;
predicate to solve the subgoal—after the object of the verb
has been parsed. At this point the first argument is properly
instantiated and the call produces a finite set of solutions.
The grammar given above contains the rule NP NP
[and] NP, which is left recursive and cannot be parsed by
the standard DCG parser. The implementation avoids this
problem by adding a flag that indicates whether an NP is
conjunctive. This gives the rule
</bodyText>
<listItem confidence="0.497738">
(123) NP( conj) NP( —conj) [and] NP(Conj),
</listItem>
<bodyText confidence="0.999471857142857">
which is not left recursive—it assigns a right-branching
structure to all conjunctions of NPs. These are the only
differences between the grammar presented here and the
Prolog code. The implementation was easy to write and
modify, and it supports the claim that Prolog allows us to
turn formal definitions into running programs with a mini-
mum of effort.
</bodyText>
<sectionHeader confidence="0.948452666666667" genericHeader="method">
4.2 CONCLUSIONS AND FUTURE WORK
4 IMPLEMENTATION AND CONCLUSIONS
4.1 IMPLEMENTATION
</sectionHeader>
<bodyText confidence="0.997800589552239">
The implementation uses the standard Prolog facility for
parsing definite clause grammars. This facility translates
the grammar into a top-down, left-to-right parser. This
order of parsing leads to problems with the predicates
&amp;quot;apply_quants&amp;quot; and &amp;quot;free_vars.&amp;quot; We cannot run &amp;quot;free_vars&amp;quot;
until its first argument is a ground term—otherwise we
might get an infinite number of solutions. In our exposition,
we solved this problem by delaying the execution of
&amp;quot;free_vars.&amp;quot; The standard DCG parser has no built-in
facility for such delaying. As usual in such situations, there
are two options: rewrite the predicates so that the existing
interpreter works efficiently, or define a more general inter-
preter that allows the desired order of execution. The
second approach is more desirable in the long run, because
it achieves a central goal of logic programming: to use
logical sentences that express our understanding of the
problem in the clearest way. However, defining new inter-
preters is hard. The present implementation takes the low
road—that is, the author rewrote the predicates so that the
This paper has presented a new notation for a sentential
theory of attitudes, which unlike most existing notations
makes it possible to give a compositional semantics for
attitude reports. Our notation distinguishes between the de
re arguments of an attitude operator and the dummy
variables, which stand for unspecified terms that represent
the values of the de re arguments. The choice of dummy
variables is quite arbitrary—just as the choice of bound
variables in first-order logic is arbitrary. This allows us to
impose a convention, which says that in fact the dummy
variables are equal to the de re arguments. Given this
convention, the logical form of a clause is the same whether
it stands alone or appears as the argument of an attitude
verb.
This is a simple proposal, and it would be easy to write
and implement a grammar that applies the proposal to a
few examples. The real question is whether the proposal is
robust—whether it can function in a grammar that covers a
variety of phenomena. We chose definite clauses and a
first-order object language as our semantic formalism. We
found a nonobvious interaction between our proposal for de
re attitude reports, and two other problems about quantifi-
Computational Linguistics Volume 16, Number 4, December 1990 231
Andrew R. Haas Sentential Semantics for Propositional Attitudes
cation: the choice of bound variables in a logical form, and
the conjunction and disjunction of quantified NPs. We
considered two possibilities for choosing the bound vari-
ables: assigning a different variable to every NP using a
global counter, or requiring each quantifier to bind a
variable that is not bound by any quantifier within its scope.
The first approach makes it impossible to use our rules for
NP conjunction and disjunction, while the second creates
implementation problems for the de re argument lists. We
resolved the dilemma by picking the second approach, and
then rewriting the grammar to solve the implementation
problems. Thus we have shown that the proposal for de re
attitude reports is not just a plausible notion—it can be
made to work in a grammar that is not trivial.
The grammar handles three kinds of attitude construc-
tions: an attitude verb taking a clause as its object (&amp;quot;John
thought he saw Mary&amp;quot;), an attitude verb taking a clause
with a gap (&amp;quot;John knows who Mary likes&amp;quot;), and an attitude
verb taking a noun phrase as its object (&amp;quot;John wants a
Porsche&amp;quot;). The grammar includes de re/de dicto ambigu-
ities, conjunction of NPs, and a very limited treatment of
pronouns.
Another contribution of our work is that it seems to be
the first unification grammar that builds logical forms, and
at the same time respects the declarative semantics of the
notation. We explicitly choose the bound variables of the
logical form, instead of using meta-language variables. We
also explain the semantics of our representation for quanti-
fied NPs: each NP has an infinite set of readings, one for
each ordered pair in the extension of the application func-
tion. Several authors treat unification grammars with se-
mantics as poor relations of Montague grammar. Pereira
and Shieber (1987) propose to &amp;quot;encode&amp;quot; Montague&apos;s ideas
in unification grammar, while Moore (1989) fears that
building logical forms with unification grammar is &amp;quot;unprin-
cipled feature hacking.&amp;quot; We claim that these problems
arise not from shortcomings of unification grammar, but
from failure to take unification grammar seriously—which
means respecting its declarative semantics.
The most obvious line for future work is to extend the
grammar. It would be fairly easy to include complex wh
noun phrases, such as &amp;quot;whose cat&amp;quot; or &amp;quot;how many children&amp;quot;
(Cooper&apos;s grammar handled these). A more difficult prob-
lem arises when a gap is the object of an intensional
verb—as in &amp;quot;John knows what Mary wants.&amp;quot; The grammar
can generate this sentence, but it assigns only a de re
reading:
unique(x,name(x,john),
some(y,thing(y),
know(x,[y],q(unique(z,name(z,mary),
wish(z,[z,y],q(have(z,y)))))))).
This is the only reading because the gap has an empty
quantifier store—there is no quantifier available to be
applied to the wff &amp;quot;have(z,y).&amp;quot; Yet there are examples in
which such sentences do have de dicto readings. For exam-
ple, consider &amp;quot;What John wants is a Porsche.&amp;quot; Surely this
sentence has a de dicto reading—yet the object of &amp;quot;want&amp;quot; is
a gap, not a quantified NP. Cooper discusses this problem,
but his grammars could not handle it, and neither can ours.
Hirst and Fawcett (1986) have argued that the ambigu-
ities in attitude reports are more complex than the familiar
distinction between de re and de dicto readings. They claim
that the sentence &amp;quot;Nadia wants a dog like Ross&apos;s&amp;quot; has a
reading in which Nadia doesn&apos;t want a particular dog (so
the quantifier 3 is inside the scope of the attitude operator),
but the description &amp;quot;dog like Ross&apos;s&amp;quot; is the speaker&apos;s, not
Nadia&apos;s (so the description is outside the scope of the
attitude operator). This reading is certainly different from
the usual de re and de dicto readings, in which either the
whole logical form of the NP is under the attitude, or none
of it is. To represent it, we must be able to separate the
logical form of the NP &amp;quot;a dog like Ross&apos;s&amp;quot; into two parts
(the quantifier 3 and its range restriction), and we must be
able to move the range restriction out of the scope of the
attitude without moving the quantifier. This will mean that
we cannot use the same mechanism for both quantifier
scope ambiguities and the ambiguities that arise from
attitudes. These extensions appear feasible, but they amount
to a major change in the formalism.
Another possibility for future work is to incorporate the
exist: ng implementation into a question-answering pro-
gram. This requires finding a way to reason about proposi-
tional attitudes efficiently without assuming unique stan-
dard designators—which means a substantial generalization
of the work of Konolige. It also requires us to make much
stronger claims about the properties of &apos;representation&apos;
than we have made in this paper. If these problems can be
solved, it should be possible to build a natural language
question-answering program that can describe the extent of
its own knowledge—answering questions like &amp;quot;Do you
know the salary of every employee?&amp;quot;
</bodyText>
<sectionHeader confidence="0.999287" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.991810621052631">
Asher, Nicholas M. and Kamp, John A. W. (1986). &amp;quot;The knower&apos;s
paradox and representational theories of attitudes.&amp;quot; In Theoretical
Aspects of Reasoning about Knowledge, edited by Joseph Halpern,
1.31-147. Morgan Kaufmann.
Barwise, Jon and Cooper, Robin. (1981). &amp;quot;Generalized quantifiers and
natural language.&amp;quot; Linguistics and Philosophy, 4: 159-219.
Boer, Steven E. and Lycan, William G. (1986). Knowing Who. MIT
Press.
Cooper, Robin. (1983). Quantification and Syntactic Theory. D. Reidel.
Cooper, Robin. (1979). &amp;quot;Variable binding and relative clauses.&amp;quot; In For-
mal Semantics and Pragmatics for Natural Languages, edited by F.
Gventhner and S. J. Schmidt, 131-169. D. Reidel.
des Rivieres, J., and Levesque, H. 1986. &amp;quot;The Consistency of Syntactical
Treatments of Knowledge.&amp;quot; in Joseph Y. Halpern, ed., Theoretical
Aspects of Reasoning about Knowledge. Los Altos, CA, Morgan
Kaufmann: 115-130.
End erton, Herbert. (1972). A Mathematical Introduction to Logic. Aca-
demic Press.
Fisher, Anthony J. (1989). &amp;quot;Practical parsing of GPSG&apos;s.&amp;quot; Computa-
tional Linguistics, 15: 139-148.
Ga2:dar, Gerald; Klein, Ewan; Pullum, Geoffrey; and Sag, Ivan. (1985).
Generalized Phrase Structure Grammar. Harvard University Press.
232 Computational Linguistics Volume 16, Number 4, December 1990
Andrew R. Haas Sentential Semantics for Propositional Attitudes
Haas, Andrew R. (1986). &amp;quot;A syntactic theory of belief and action.&amp;quot;
Artificial Intelligence, 28: 245-292.
Hobbs, Jerry R. and Shieber, Stuart M. (1987). &amp;quot;An algorithm for
generating quantifier scopings.&amp;quot; Computational Linguistics, 13: 47-63.
Hirst, Graeme and Fawcett, Brenda. (1986). &amp;quot;The detection and represen-
tation of ambiguities of intension and description.&amp;quot; In Proceedings of
the 24th Annual Meeting of the Association for Computational Linguis-
tics, 192-199.
Kaplan, David. (1975). &amp;quot;Quantifying in.&amp;quot; In The Logic of Grammar,
edited by Davidson, Donald and Harman, Gilbert, 112-144. Dickinson.
Konolige, Kurt. (1986). A Deduction Model of Belief. Pitman.
Kripke, Saul. &amp;quot;Outline of a Theory of Truth.&amp;quot; Journal of Philosophy, 72:
690-715.
Lewis, David. (1979). &amp;quot;Attitudes de dicto and de se.&amp;quot; Philosophical
Review, 88: 513-543.
Montague, Richard. (1974a). &amp;quot;The proper treatment of quantification in
ordinary english.&amp;quot; In Formal Philosophy: Selected Papers of Richard
Montague, edited by Thomason, Richmond H., 247-270. Yale Univer-
sity Press.
Montague, Richard. (1974b). &amp;quot;Syntactical treatments of modality, with
corollaries on reflexion principles and finite axiomatizability.&amp;quot; In For-
mal Philosophy: Selected Papers of Richard Montague, edited by
Thomason, Richmond H., 286-302. Yale University Press.
Montague, Richard and Kaplan, David. (1974). &amp;quot;A paradox regained.&amp;quot; In
Formal Philosophy: Selected Papers of Richard Montague, edited by
Thomason, Richmond H., 271-301. Yale University Press.
Moore, Robert C. (1989). &amp;quot;Unification-based semantic interpretation.&amp;quot;
In Proceedings of the 27th Annual Meeting of the Association for
Computational Linguistics, 33-41.
Moore, Robert C. (1988). &amp;quot;Propositional attitudes and russellian
propositions.&amp;quot; Report No. CSLI-88-119, Center for the Study of Lan-
guage and Information, Menlo Park, CA.
Moore, Robert C. and Hendrix, Gary. (1979). &amp;quot;Computational models of
belief and semantics of belief sentences.&amp;quot; Technical report 187, SRI
International, Menlo Park, CA.
Moran, Douglas. (1988). &amp;quot;Quantifier scoping in the SRI core language
engine.&amp;quot; In Proceedings of the 26th Annual Meeting of the Association
for Computational Linguistics, 33-40.
Pereira, Fernando C. N. and Shieber, Stuart M. (1987). Prolog and
Natural-Language Analysis. Center for the Study of Language and
Information, Stanford, CA.
Pereira, Fernando C. N. and Warren, David H. D. (1980). &amp;quot;Definite
clause grammars for language analysis-a survey of the formalism and
a comparison with augmented transition networks.&amp;quot; Artificial Intelli-
gence, 13: 231-278.
Perlis, Donald. (1988). &amp;quot;Languages with self reference II: knowledge,
belief, and modality.&amp;quot; Artificial Intelligence, 34: 179-212.
Perlis, Donald. (1986). &amp;quot;On the consistency of commonsense reasoning.&amp;quot;
Computational Intelligence, 2: 180-190.
Perlis, Donald. (1985). &amp;quot;Languages with self-reference I: foundations.&amp;quot;
Artificial Intelligence, 25: 301-322.
Rapaport, William J. (1986). &amp;quot;Logical Foundations for Belief Represen-
tation.&amp;quot; Cognitive Science, 10: 371-42.
Quine, Willard van Orman. (1975). &amp;quot;Quantifiers and propositional
attitudes.&amp;quot; In The Logic of Grammar, edited by Davidson, Donald and
Harman, Gilbert. Dickinson.
Quine, Willard van Orman. (1947). Mathematical Logic. Harvard Uni-
versity Press.
Tarski, Alfred. (1936). &amp;quot;Der Wahrheitsbegriff in den Formalisierten
Sprachen. Studia Philosophia, 1: 261-405.
Thomason, Richmond H. (1986). &amp;quot;Paradoxes and semantic represen-
tation.&amp;quot; In Theoretical Aspects of Reasoning about Knowledge, edited
by Joseph Halpern, 225-239. Morgan Kaufmann.
Thomason, Richmond H. (1980). &amp;quot;A note on syntactical treatments of
modality.&amp;quot; Synthese, 44(3): 391-395.
Walther, Christoph. (1987). A Many-Sorted Calculus for Resolution and
Paramodulation. Pitman.
Warren, David S. (1983). &amp;quot;Using lambda-calculus to represent meanings
in logic grammars.&amp;quot; In Proceedings of the 21st Annual Meeting of the
Association for Computational Linguistics, 51-56.
Computational Linguistics Volume 16, Number 4, December 1990 233
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999522">SENTENTIAL SEMANTICS FOR PROPOSITIONAL ATTITUDES</title>
<author confidence="0.999996">Andrew R Haas</author>
<affiliation confidence="0.9993785">Department of Computer Science State University of New York at</affiliation>
<address confidence="0.993041">Albany, New York, 12222</address>
<abstract confidence="0.990301016976559">The sentential theory of propositional attitudes is very attractive to AI workers, but it is difficult to use such a theory to assign semantics to English sentences about attitudes. The problem is that a compositional semantics cannot easily build the logical forms that the theory requires. We present a new notation for a sentential theory, and a unification grammar that builds logical forms in our notation. The grammar is implemented using the standard implementation of definite clause grammars in Prolog. 1 LOGICAL FORMS FOR ATTITUDES The sentential theory of propositional attitudes claims that propositions are sentences of a thought language. It has an appeal to since their programs often contain sentences of an artificial language, which are supposed to represent the program&apos;s beliefs. These sentences can be true or false, and the program can make inferences from them, so they have two essential properties of beliefs. It is tempting to conclude that they are the program&apos;s beliefs, and that human beliefs are also sentences of a thought language. If we extend this to all propositional attitudes, we have a sentential theory of propositional attitudes. In such a theory, an English sentence expresses a proposition, and this proposition is itself a sentence— although in a different language. Other theories of attitudes hold that a proposition is a set of possible worlds, or a situation—something very different from a sentence. Moore and Hendrix (1979), Haas (1986), Perlis (1988), and Konolige (1986) have argued for sentential theories and applied them to artificial intelligence. Kaplan (1975) proposed an analysis of quantification into the scope of attitudes within a sentential theory, and other authors using sentential theories have offered variations of his idea (Haas 1986; Konolige 1986). Most of these theories present serious difficulties for formal semantics. The problem is that they assign two very different logical forms to a clause: one form when the clause is the object of an attitude verb, and another when it stands alone. This means that the logical form of the clause depends on its context in a complicated way. It is difficult to describe this dependence in a formal grammar. The present paper aims to solve this problem—to present a grammar that assigns logical forms that are correct according to Kaplan&apos;s ideas. We also describe a parser that builds the logical forms required by the grammar. This grammar is a set of definite clauses written in the notation of Pereira and Warren (1980). However, it is not a definite clause grammar for two reasons. First, our grammar cannot be parsed by the top-down left-to-right method used for definite clause grammar (although it can be modified to allow this). Second, we do not allow any of the nonlogical operations of Prolog, such as checking whether a variable is bound or free, negation as failure, and the rest. This means that our grammar is a set of ordinary first-order sentences (in an unusual notation) and its semantics is the ordinary semantics of first-order logic. So the grammar is declarative, in the sense that it defines a language and assigns logical forms without reference to any algorithm for parsing or generation. If we stick to the declarative semantics, a neglected problem demands our attention. We must choose the bound variables that appear in the logical forms generated by the grammar. Logic grammars that include semantics nearly always ignore this problem, using free variables of the meta-language to represent the bound variables of the logical form. This solution directly violates the declarative semantics of definite clauses, and we therefore reject it. We will see that this problem interacts with the semantics of NP conjunction and of quantification into attitudes. To untangle this knot and handle all three problems in one grammar is the goal of this paper. Section 1 of this paper will propose logical forms for sentences about propositional attitudes and explain the semantics of the logical forms in terms of certain relations that we take as understood. Section 2 presents a unification grammar for a fragment of English that includes quantifiers, NP conjunction, pronouns, and relative clauses. The grammar combines syntax and semantics and assigns one or more logical forms to each sentence that it generates. Computational Linguistics Volume 16, Number 4, December 1990 213 Andrew R. Haas Sentential Semantics for Propositional Attitudes Section 3 extends the grammar to include verbs that describe propositional attitudes. Section 4 describes the implementation and summarizes the results. KAPLAN&apos;S ANALYSIS OF RE REPORTS Noun phrases in the scope of attitude verbs commonly have ambiguity between re dicto Consider the example &amp;quot;John believes that Miss America is (Dowty, Wall, and Peters 1981). Under the re reading of &amp;quot;Miss America,&amp;quot; this sentence says that John has a belief about a woman who in fact is Miss America, but it doesn&apos;t imply that John realizes she is Miss America. A sentential theorist might say that the sentence tells us that John has a belief containing some name that denotes Miss America, but it doesn&apos;t tell us what name. The other called dicto, that John believes that whois Miss America is bald. The dicto unlike re, not imply that anyone actually is Miss America—it could be true if the Miss America pageant closed down years ago, while John falsely supposes that someone still holds that title. Kaplan (1975) considered examples like these. He said that an agent may use many names that denote the same but there is a subset of those names that the entity to the agent (this use of &amp;quot;represent&amp;quot; is different the common use in AI). If an agent has a re about an entity x, that belief must be a sentence containing, not just any term that denotes x, but a term that represents x to the agent. Thus if &amp;quot;person0&amp;quot; is a name that represents Miss America to John, and the thought language sentence &amp;quot;bald(person0)&amp;quot; is one of John&apos;s beliefs, then the sentence thinks Miss America is bald&amp;quot; is true (under the re reading). Kaplan said that a name represents an entity to an agent first, it denotes that entity; second, it is sufficiently and, finally, there is a causal connection between the entity and the agent&apos;s use of the name. A name N is vivid to an agent if that agent has a collection of beliefs that mention N and give a good deal of relevant information about the denotation of N. What is relevant may depend on the agent&apos;s interests. Other authors have accepted the idea of a distinguished subset of names while offering different proposals about how these names are distinguished. I have argued that the distinguished names must provide information that the agent needs to achieve his or her current goals (Haas 1986). Konolige (1986) proposed that for each agent and each entity, the set of distinguished names has exactly one member. In this paper, we adopt Kaplan&apos;s term &amp;quot;represent&amp;quot; without necessarily adopting his analysis of the notion. We assume that representation is a relation between an agent, a name, and the entity that the name denotes. If an agent has an attitude toward a thought-language sentence, and that sentence contains a name that represents a certain entity to agent, then the agent has a re about that entity. Our grammar will build logical forms that are compatible with any sentential theory that includes these assumptions. One problem about the nature of representation should mentioned. This concerns the so-called se reports. This term is attributable to Lewis (1979), but the definition is from Boer and Lycan (1986). se attitudes are &amp;quot;attitudes whose content would be formulated by the subject using the equivalent in his or her language of the first-person singular pronoun &apos;I&apos;&amp;quot; (B6er and Lycan 1986). If John thinks that he is wise, and we understand as a se what name represents John to One possibility is that it is his agent&apos;s selfname is a thought-language constant that he standardly uses to denote himself. It was postulated in Haas (1986) in order to solve certain problems about planning to acquire information. To expound and defend this idea would take us far from the problems of compositional semantics that concern us here. We simply mention it as an example of the kinds of theories that are compatible with the logical forms built by our grammar. See also (1986) for another Al approach to se attitudes. 1.2 COMPOSITIONAL SEMANTICS AND LOGICAL FORMS the logical form that Kaplan assigns for the re reading of &amp;quot;John believes that some man loves Mary.&amp;quot; (1) 3 (y,man(y) &amp; 3 (a,R(a,y,john) &amp; The notation is a slight modification of Kaplan&apos;s (Kaplan 1975), The predicate letter R denotes representation. The symbol a is a special variable ranging over names. The symbols randi are Quine&apos;s quasi-quotes (Quine 1947). If a a name the expression will denote the sentence &amp;quot;love(t,mary).&amp;quot; It is hard to see how a compositional semantics can build this representation from the English sentence &amp;quot;John believes some man loves Mary.&amp;quot; The difficult part is building the representation for the VP &amp;quot;believes some man loves Mary.&amp;quot; By definition, a compositional semantics must build the representation from the representations of the constituents of the VP: the verb &amp;quot;believe&amp;quot; and the embedded clause. Following Cooper&apos;s notion of quantifier storage (Cooper 1983), we assume that the representation of the embedded clause has two parts: the wff &amp;quot;love(y,mary)&amp;quot; and an existential quantifier that binds the free variable y. Informally, we can write the quantifier as &amp;quot;some(y,man(y) &amp; S),&amp;quot; where S stands for the scope of the quantifier. Applying this quantifier to the wff &amp;quot;love(y,mary)&amp;quot; gives the sentence &amp;quot;some (y,man(y) &amp; love(y,mary)).&amp;quot; In the present paper, the term &amp;quot;quantifier&amp;quot; will usually refer to this kind of object—not to the symbols V and 3 of first-order logic, nor to the generalized quantifiers of Barwise and Cooper (1981). 214 Computational Linguistics Volume 16, Number 4, December 1990 Andrew R. Haas Sentential Semantics for Propositional Attitudes In Section 2.2 we present a more precise formulation of our representation of quantifiers. When the clause &amp;quot;some man loves Mary&amp;quot; forms an utterance by itself, the semantics will apply the quantifier to the wff &amp;quot;love(y,mary)&amp;quot; to get the sentence &amp;quot;some (y,man(y) &amp; love(y,mary)).&amp;quot; The problem is that the wff &amp;quot;love(y,mary)&amp;quot; does not appear in Kaplan&apos;s representation. In its place is the expression &amp;quot;love(a,mary),&amp;quot; containing a variable that ranges over names, not men. It might be possible to build this expression from the wff &amp;quot;love(y,mary),&amp;quot; but this sounds like a messy operation at best. Similar problems would arise if we chose another quotation device (such as the one in Haas 1986) or another scoping mechanism (as in Pereira and Shieber 1987). Konolige (1986) proposed a very different notation for quantifying in, one that would abolish the difficulty described here. His proposal depends on an ingenious nonstandard logic. Unfortunately, Konolige&apos;s system has two important limitations. First, he forbids a belief operator to appear in the scope of another belief operator. Thus, he rules out beliefs about beliefs, which are common in everyday life. Second, he assumes that each agent assigns to every known entity a unique &amp;quot;id constant.&amp;quot; When an agent has a belief about an object x, that belief contains the id constant for x. Using Kaplan&apos;s terminology, Konolige is that for any entity x and agent is a unique such that R(a,x,y). Kaplan never suggests that representation has this property, and as Moore (1988) pointed out, the claim is hard to believe. Surely an agent can have many names for an entity, some useful for one purpose and some for another. Why should one of them be the unique id constant? We will propose a notation that has the advantages of Konolige&apos;s notation without its limitations. Section 1.3 will present the new notation. In Section 1.4, we return to the problem of building logical forms for English sentences. 1.3 A NEW NOTATION FOR QUANTIFYING IN Our logical forms are sentences in a first-order logic augmented with a quotation operator. We call this language language. the grammar is a set of definite clauses, our notation is like Prolog&apos;s. The variables of the target language are u, v, w, x, y, z, etc. Constants, function and atomic wffs are defined in the usual way. If wffs, then not(p), and(p,q), and or(p,q) are wffs. wffs, variable, and term, the following are wffs: (2) some(x,p,q) (3) all(x,p,q) (4) unique(x,p,q) (5) let(x,t,p) first wff is true if both true for some value x. The second is true iff true for all values of x that The third is true iff there is exactly one value makes and true for that value of x. The wff is true if true when the value of x is set to the of language should be extended to include the iota operator, forming definite descriptions, since a definite description may often represent an entity to an agent. However, we omit definite descriptions for the time being. any expression the target language, q(e) is a constant of the target language. Therefore we have a countable infinity of constants. The intended models of our language are all first-order models in which the domain of discourse includes every expression of the language, and constant q(e) has the expression its denotation. Models of this kind are somewhat unusual, but they are perfectly consistent with standard definitions of first-order logic, which allow the universe of discourse to be any nonempty set (Enderton 1972). Our language does depart from standard logic in one way. We allow a variable to appear inside a constant—for example, since v is a variable, q(v) is a constant that denotes the variable v. Enderton explicitly forbids this: &amp;quot;no symbol is a finite sequence of other symbols&amp;quot; (p. 68). However, allowing a variable to appear inside a constant is harmless, as long as we are careful about the definition of a free occurrence of a variable. We modify Enderton&apos;s definition (p. 75) by changing his first clause, which defines free occurrences of a variable in an atomic wff. We say instead that a variable v appears free in variable w if v = w; no variable occurs free in any constant; a variable v occurs free in the term . . . it occurs free in one of . . . v occurs free in the atomic wff p(t, . . . 4) iff it occurs free in one of this definition x does not occur free in the constant q(red(x)), although it does occur free in the wff red (x). As usual in a sentential theory of attitudes, we assume that an agent&apos;s beliefs are sentences of thought language stored in the head, and that knowledge consists of a subset of those sentences. Then simple belief is a relation between an agent and a sentence of thought language. To represent re reports, we introduce a predicate of three arguments, and we define its extension in terms of simple belief and the notion of representation. If [p,l,w] is a triple the extension of the predicate &amp;quot;believe,&amp;quot; then the who has the belief, a list of entities . . . x„ that the belief is about, and w is a wff of the target language. The free variables in w will stand for unspecified terms that the entities . . . x„ to the agent free are called variables. John believes of Mary that she is a fool, then, using Prolog&apos;s notation for lists we write (6) believe(john,[mary],q(fool(x))). constant &amp;quot;mary&amp;quot; is called a re argument the predicate &amp;quot;believe.&amp;quot; The free occurrence of x in fool(x) stands for an unspecified term that represents Mary to This means that there is a term represents Mary to John, and John believes fool(t). x is the dummy for the re &amp;quot;mary.&amp;quot; This notation is inspired by Quine (1975), but we give a semantics quite different from Quine&apos;s. Note that the symbol &amp;quot;believe&amp;quot; is Computational Linguistics Volume 16, Number 4, December 1990 215 Andrew R. Haas Sentential Semantics for Propositional Attitudes an ordinary predicate letter, not a special operator. This is a minor technical advantage of the sentential approach: the quotation operator eliminates the need for a variety of special propositional attitude operators. To define this notation precisely, we must have some way associating dummy variables with the re we have the wff believe(x, . . . . . v„ be a list of the free variables of order of their first Then v, will be the dummy variable for words, the dummy variable for i-th re be the i-th free variable of method of associatdummy variables with re is somewhat arbitrary—another possibility is to include an explicit list of dummy variables. Our choice will make the notation a little more compact. Then the extension of the predicate &amp;quot;believe&amp;quot; is defined follows. Let an agent, . . . list of entities the domain of discourse, and wff of the target Suppose that exactly variables, and . . . be the free variables of order of their first Suppose that . . are closed terms such that represents to 1 to the simple relation holds between the sentence formed by . . free occurrences . v„ in the extension of the predicate &amp;quot;believe&amp;quot; includes the triple . . . and As an example, suppose the term &amp;quot;personl&amp;quot; represents Mary to John, and John believes &amp;quot;fool(person1).&amp;quot; Then, since substituting &amp;quot;person 1&amp;quot; for &amp;quot;x&amp;quot; in &amp;quot;fool(x)&amp;quot; produces the sentence &amp;quot;fool(person1),&amp;quot; it follows that (7) believe( john,[mary],q(fool(x))) is true in every intended model where &amp;quot;believe&amp;quot; has the extension defined above. Consider an example with quantifiers: &amp;quot;John believed a prisoner escaped.&amp;quot; The reading with the quantifier inside the attitude is easy: (8) believe( john,[],q(some(x,prisoner(x),escaped(x)))). this case the list of re is empty. For the &amp;quot;quantifying in&amp;quot; reading we have: (9) some(x,prisoner(x),believe(john,[x],q(escaped(y)))). This says that for some prisoner x, John believes of x that he escaped. The dummy variable y in the wff escaped(y) stands for an unspecified term that occurs in one of John&apos;s beliefs and represents the prisoner x to John. Let us consider nested beliefs, as in the sentence &amp;quot;John Bill believed Mary was wise.&amp;quot; Here the re/de give rise to three readings. One is a straightdicto (10) believe( john,[],q(believe(bill,[],q(wise(mary))))). To understand examples involving nested beliefs, it is helpful to write down the sentence that each agent believes. Since this example does not involve quantifying in, it is easy to write down John&apos;s belief—we just take the quotation mark off the last argument of &amp;quot;believe&amp;quot;: (11) believe(bill,[],q(wise(mary))). If this belief of John&apos;s is true, then Bill believes (12) wise(mary). the next reading, the name &amp;quot;Mary&amp;quot; is dicto John, re Bill: (13) believe( john,[1,q(believe(bill,[mary],q(wise(x))))). Here, John is using the constant &amp;quot;mary&amp;quot; to denote Mary, but he does not necessarily think that Bill is using the same constant—he only thinks that some term represents Mary to Bill. The sentence that John believes is (14) believe(bill, [mary] ,q(wise(x))). If John is right, Bill&apos;s belief is formed by substituting for the free variable x in &amp;quot;wise(x)&amp;quot; some term that represents Mary to Bill. Suppose this term is &amp;quot;person0,&amp;quot; then Bill&apos;s belief would be (15) wise(person0). there is a reading in which &amp;quot;Mary&amp;quot; is re both agents: (16) believe(john,[mary],q(believe(bill,[x],q(wise(y))))). Here there is a name that represents Mary to John, and John thinks that there is a name that represents Mary to Bill. Again, John does not necessarily believe that Bill uses the same name that John uses. Suppose &amp;quot;person3&amp;quot; is the term that represents Mary to John, then John&apos;s belief would be (17) believe(bill,[person3],q(wise(y))). If &amp;quot;person4&amp;quot; is the term that represents Mary to Bill, then Bill&apos;s belief would be (18) wise(person4). One might expect a fourth reading, in which &amp;quot;Mary&amp;quot; is re John and dicto Bill, but our formalism cannot represent such a reading. To see why, let us try to construct a sentence that represents this reading. In our tion a nonempty list of re represents a while an empty list of re represents dicto Therefore the desired sentence should a nonempty list of re for John&apos;s belief, list for Bill&apos;s belief. This would give (19) believe( john,[mary],q(believe(bill,[],q(wise(x))))) This sentence does not assert that John believes Bill has a dicto about Mary. To see this, consider John&apos;s belief. If he uses the constant &amp;quot;person 1&amp;quot; to denote Mary, the belief is 216 Computational Linguistics Volume 16, Number 4, December 1990 Andrew R. Haas Sentential Semantics for Propositional Attitudes (20) believe(bill,[],q(wise(x))). In forming John&apos;s belief we do not substitute &amp;quot;person 1&amp;quot; for the occurrence of x under the quotation operator—because by our definitions this is not a free occurrence of x. Thus John&apos;s belief says that Bill has a belief containing a free variable, which our theory forbids. It is not clear to me whether the desired reading exists in English, so I am not certain if this property of the notation is a bug or a feature. In either case, other notations for describing attitudes have similar properties. For example, in a modal logic of attitudes we use the scope of quantifiers represent re/de dicto If a quantifier in the scope of an attitude operator, we have a and if it appears outside the scope (while a variable inside the scope) we get a re In a sentence like &amp;quot;John thinks Bill thinks Mary saw a lion,&amp;quot; there are three places to put the existential quantifier: in the scope of Bill&apos;s belief operator, in the scope of John&apos;s operator but outside Bill&apos;s, or outside both. These give the same three readings that our formalism allows. To &amp;quot;a lion&amp;quot; be re John and dicto Bill, we would have to put the quantifier outside the scope of John&apos;s belief operator, but inside the scope of Bill&apos;s belief operator. Since Bill&apos;s belief operator is in the scope of John&apos;s, that is impossible. The same method applies to other attitudes—for example, knowledge. Given a simple knowledge relation, which dicto of sentences with &amp;quot;know,&amp;quot; one define the predicate &amp;quot;know,&amp;quot; which expresses both dicto &amp;quot;Know&amp;quot; will take three arguments just as &amp;quot;believe&amp;quot; does. consider examples like &amp;quot;John knows who likes Mary,&amp;quot; in which &amp;quot;know&amp;quot; takes a wh noun phrase and a sentence containing a gap. The intuition behind our analyis that John knows who likes Mary if there is a person that John knows that Mary. This is of course a re report, and its logical form should be (21) some(x,person(x),know( john,[x],q(like(y,mary)))). As an example, suppose the sentence (22) like(bill,mary) is one of John&apos;s beliefs, and it belongs to the subset of beliefs that constitute his knowledge. If the constant &amp;quot;bill&amp;quot; represents Bill to John, then since substituting &amp;quot;bill&amp;quot; for &amp;quot;y&amp;quot; in &amp;quot;likes(y,mary)&amp;quot; gives the sentence &amp;quot;like(bill,mary),&amp;quot; we have (23) know( john, [bill],q(like(y,mary))) and therefore (24) some(x,person(x),know( john,[x],q(like(y,mary)))). This proposed analysis of &amp;quot;knowing who&amp;quot; is probably too weak. As a counter example, suppose a night watchman catches a glimpse of a burglar and chases him. Then the night watchman has formed a mental description of the burglar—a description that he might express in English as &amp;quot;the man I just saw sneaking around the building.&amp;quot; The burglar might say to himself, &amp;quot;He knows I&apos;m in here.&amp;quot; This a re report, so it follows that the night watchman&apos;s mental description of the burglar must represent the burglar to the watchman (by our assumption about representation). Yet the night watchman surely would not claim that he knows who is sneaking around the building. It seems that even though the watchman&apos;s mental description represents the burglar, it is not strong enough to support the claim that he knows who the burglar is. It would be easy to extend our notation to allow for a difference between &amp;quot;knowing who&amp;quot; and other cases of quantification into attitudes. It would be much harder to analyze this difference, Boer and Lycan (1986) have argued that when we say someone knows who N is, we always mean that someone knows who N is for some purpose. This purpose is not explicitly mentioned, so it must be understood from the context of the utterance in which the verb &amp;quot;know&amp;quot; appears. Then the predicate that represents &amp;quot;knowing who&amp;quot; must have an extra argument whose value is somehow supplied by context. These ideas look promising, but to represent this use of context in a grammar is a hard problem, and outside the scope of this work. Next we consider intensional transitive verbs like &amp;quot;want,&amp;quot; &amp;quot;John wants a Porsche.&amp;quot; The intuition behind the analysis is that this sentence is roughly synonymous with &amp;quot;John wishes that he had a Porsche&amp;quot;—under a reading in which &amp;quot;he&amp;quot; refers to John. Then the logical form would be (25) wish( john, [1,q(some(x,porsche(x),have( john,x)))) a dicto and (26) some(x,porsche(x),wish( john,[x],q(have( john,y)))) a re The predicate letter &amp;quot;wish&amp;quot; need not be identical to the one that translates the English verb &amp;quot;wish&amp;quot;—it might only be roughly synonymous. The predicate letter &amp;quot;have&amp;quot; probably is the same one that translates the verb &amp;quot;have&amp;quot;—or rather, one of many predicates that can translate this highly ambiguous verb. For the present purpose let us assume that the predicate &amp;quot;have&amp;quot; represents a sense of the verb &amp;quot;have&amp;quot; that is roughly synonymous with &amp;quot;possess,&amp;quot; as in &amp;quot;John has a Porsche.&amp;quot; Another sense of &amp;quot;have&amp;quot; is relational, as in &amp;quot;John has a son,&amp;quot; and &amp;quot;want&amp;quot; has a corresponding sense, as in &amp;quot;John wants a son.&amp;quot; The present paper will not analyze this relational sense. This grammar will express the meanings of intensional verbs in terms of propositional attitudes. This may not work for all intensional verbs. For example, it is not clear that &amp;quot;the Greeks worshipped Zeus&amp;quot; is equivalent to any statement about propositional attitudes. Montague (1974a) represented intensional verbs more directly, as relations between agents and the intensions of NP&apos;s. A similar analysis is possible in our framework, provided we extend the target language to include typed lambda calculus. Suppose the Computational Linguistics Volume 16, Number 4, December 1990 217 Andrew R. Haas Sentential Semantics for Propositional Attitudes variable p ranges over sets of individuals. Then we could the dicto of &amp;quot;John wants a Porsche&amp;quot; as want( john,q(lambda(p,some(x,porsche(x),x Here the predicate &amp;quot;want&amp;quot; describes a relation between a person and an expression of thought language, but that expression is not a wff. Instead it is a closed term denoting a set of sets of individuals. Certainly this is a natural generalization of a sentential theory of attitudes. If agents can have attitudes toward sentences of thought language, why shouldn&apos;t they have attitudes toward other expressions of the same thought language? 1.4 COMPOSITIONAL SEMANTICS AGAIN We now return to the problem of building logical forms with a compositional semantics. Consider the formula (28) some(x,prisoner(x),believe( john, [x] ,q(escaped(y)))). Following Cooper as before, we assume that the semantic features of the clause &amp;quot;a prisoner escaped&amp;quot; are a wff containing a free variable and an existential quantifier that binds the same variable. In formula (28) the existential quantifier does not bind the variable that appears in the wff &amp;quot;escaped(y)&amp;quot;—it binds another variable instead. Therefore we have the same problem that arose for Kaplan&apos;s representation—it is not clear how to build a representation for the belief sentence from the representations of its constituents. The choice of bound variables is arbitrary, and the choice of dummy variables is equally arbitrary. Thus, there is an solution: let the re and the dummy variables be the same. Thus, the wide scope reading for &amp;quot;John believes a prisoner escaped&amp;quot; is not (28), but (29) some(x,prisoner(x),believe(john,[x],q(escaped(x)))). the variable x serves two purposes—it is a re argument, and also a dummy variable. When it occurs as a re it is bound by the quantifier in the usual way. When it occurs as a dummy variable, it is definitely not bound by the quantifier. In fact the dummy variable is a mention of the variable x, not a use, because it occurs under a quotation mark. Formula (29) may be a little confusing, since the same variable appears twice with very different semantics. This formula has a major advantage over formula (28), however—it contains the wff &amp;quot;escaped(x)&amp;quot; and a quantifier that binds the free variable of that wff. Since these are precisely the semantic features of the clause &amp;quot;a prisoner escaped,&amp;quot; it is fairly easy to build the logical form (29) from the sentence &amp;quot;John believed a prisoner escaped.&amp;quot; We can describe this technique as a convention governing the logical forms that our grammar assigns to English In any wff of the form believe(x, . . . nth re is equal to its own dummy variable. the nth re equal to the nth free of other words, the list . . . just a list the free variables of order of occurrence. The same convention holds for all predicates that represent attitudes. Finally, note that the convention holds only for the logical forms that the grammar assigns to sentences. Once the grammar has built a logical form, inference procedures can freely violate the convention. For example, consider the logical form of the sentence &amp;quot;Every man believes that Mary loves him&amp;quot;: (30) all(x,man(x),believe(x,[x],q(love(mary,x)))). From this sentence and the premise man(bill) we can infer (31) believe(bill,[bill],q(love(mary,x))) by substituting for a universal variable as usual. The occurrence of the variable under the quotation mark is naturally unaffected, because it is not a free occurrence of x. 1.5 SELF-REFERENCE AND PARADOX Other writers (cited above) have already expounded and defended sentential theories of attitudes. This paper takes a sentential theory as a starting point, and aims to solve certain problems about the semantics of attitude reports in such a theory. However, one problem about sentential theories deserves discussion. The results of Montague (19741)) have been widely interpreted as proof that sentential theories of attitudes are inconsistent and therefore useless. Montague did indeed show that certain sentential theories of knowledge produce self-reference paradoxes, and are therefore inconsistent. However, he did not show that these were the only possible sentential theories. Recently des Rivieres and Levesque (1986) have constructed sentential theories without self-reference and proved them consistent. Thus they showed that while Montague&apos;s theorem was true, its significance had been misunderstood. Perlis (1988) has shown that if we introduce self-reference into a modal theory, it too can become inconsistent. In short, there is no special connection between sentential theories and paradoxes of self-reference. A sentential theory may or may not include self-reference; a modal theory may or may not include self-reference; and in either case, self-:reference can lead to paradoxes. Kripke (1975) has shown that even the most commonplace utterances can create self-reference if they occur in unusual circumstances. Therefore the problem is not to avoid self-reference, but to understand it. The problem for advocates of sentential theories is to find a sentential analysis of the self-reference paradoxes that is, if not wholly satisfactory, at least as good as nonsentential analyses. For the purposes of AT, a successful analysis must avoid paradoxical conclusions, without sacrificing axioms or rules of inference that have proved useful in AT programs. idea is that ordinary human intuitions about selfreference are inconsistent. To most people, it appears that the sentence &amp;quot;This statement is false&amp;quot; must be both true and false, yet it cannot be both. The only error in the formal analyses is that having derived a contradiction, they allow us to derive any conclusion whatever. This happens because 218 Computational Linguistics Volume 16, Number 4, December 1990 Andrew R. Haas Sentential Semantics for Propositional Attitudes standard logic allows no inconsistent theories except trivial containing every of the language. Therefore we need a new kind of logic to describe the inconsistent intuitions of the ordinary speaker. Priest (1989) attempted this—he constructed an inconsistent but nontrivial theory of truth using a paraconsistent logic. Priest&apos;s theory includes the T-scheme, written in our notation as (32) P 4-* true(q(P)). P is a meta-variable ranging over sentences of the language. Tarski (1936) proposed this scheme as capturing an essential intuition about truth. Unfortunately, the rule of modus ponens is invalid in Priest&apos;s system, which means that most of the standard AI reasoning methods are invalid. Priest considers various remedies for this problem. Another approach is to look for a consistent theory of self-reference. Such a theory will probably disagree with speakers&apos; intuitions for paradoxical examples like &amp;quot;This statement is false.&amp;quot; Yet these examples are rare in practice, so a natural language program using a consistent theory of self-reference might agree with speakers&apos; intuitions in the vast majority of cases. Kripke (1975) proposed such a theory, based on a new definition of truth in a model—an alternative to Tarski&apos;s definition. Kripke&apos;s definition allows truth-value gaps: some sentences are neither true nor false. Suppose P is a sentence; then the sentence true(q(P)) is true if P is true, and false if P is false. Therefore if P is neither true nor false, true(q(P)) also has no truth value. In other respects, Kripke&apos;s definition of truth resembles Tarski&apos;s—it assigns the same truth values to sentences that do not contain the predicate &amp;quot;true,&amp;quot; and it never assigns two different truth values to one sentence. Suppose that a model of this kind contains a sentence that says &amp;quot;I am not true.&amp;quot; Formally, suppose the constant c denotes the sentence -Itrue(c). What truth value can such a sentence have under Kripke&apos;s definition? Just as in standard logic, -1 true(c) is true if true(c) is false. True(c) in turn is false if c is false. Since c is the sentence -I true(c), we have shown that c is true if c is false. Since no sentence has two truth values, it follows that c has no truth value. Once again, problems arise because the system is too weak. If P is a sentence with no truth value, then the sentence P V -1 P has no truth value, even though it is a tautology of first-order logic. One remedy for this appears in the system of Perlis (1985). Perlis considers a first-order model M containing a predicate &amp;quot;true,&amp;quot; whose extension is the set of sentences that are true in M by Kripke&apos;s definition. He accepts as theorems all sentences that are Tarskitrue in every model of this kind. Thus Perlis&apos;s system uses two notions of truth: P is a theorem only if P is Tarski-true, but true(q(P)) is a theorem only if P is Kripke-true. Suppose we have P *-* ---1 true(q(P)); then Perlis&apos;s system allows us to prove both P and -1 true(q(P)). This certainly violates the intuitions of ordinary speakers, but such violations seem to be the inevitable price of a consistent theory of self-reference. Perlis devised a proof system for such models, using standard first-order proof and an axiom schema GK for the predicate &amp;quot;true.&amp;quot; Penis proved that if L is any consistent set of first-order sentences that does not mention the predicate &amp;quot;true,&amp;quot; then the union of L and GK has a model M in which the extension of &amp;quot;true&amp;quot; is the set of sentences that are Kripke-true in M. Perlis&apos;s system has one important advantage over Kripke&apos;s: since the formalism is just a standard first-order theory, we can use all the familiar first-order inference rules. In this respect, Perlis&apos;s system is better suited to the needs of Al than either Kripke&apos;s or Priest&apos;s. However, it still excludes some inferences that are standard in everyday reasoning. For example, we have true(q(P)) -* P for every P, but P -* true(q(P)) is not a theorem for certain sentences P—in particular, sentences that are self-referential and paradoxical. An adequate account of self-reference must deal not only with the Liar, but also with paradoxes arising from propositional attitudes—for example, the Knower Paradox (Montague and Kaplan 1974), and Thomason&apos;s paradox about belief (Thomason 1980). Perlis (1988) has considered the treatment of attitudes within his system, and Asher and Kamp (1986) have treated both paradoxes using ideas akin to Kripke&apos;s (their treatment is not sentential, but they claim that it could be extended to a sentential treatment). Let us briefly consider the treatment of the Knower paradox within Perlis&apos;s system. To simplify the treatment, we will assume that knowledge is true belief. If we are working in Perlis&apos;s system, this naturally means that knowledge is Kripke-true belief. We write &amp;quot;the agent knows that P&amp;quot; as true(q(P)) A believe(q(P)). The paradox arises from a sentence R that says &amp;quot;The agent knows -R.&amp;quot; Formally, (33) R 4-* (true(q(-R)) A believe(q(-R))). Since true(q(-R)) -* -1R is a theorem of Perlis&apos;s system, (33) implies -IR. Now suppose that the agent believes (33); then with modest powers of inference the agent can conclude -R, so we have believe(q-R). Combining this with (33) gives (34) R 4-4 true(q(---R)), which at once implies that -R is not Kripke-true. It follows that although --R is a theorem of the system, and the agent believes it, the agent does not know it—because it is not Kripke-true, and only a sentence that is Kripke-true can be known. The Knower paradox arises if we insist that the agent does know -R. This example brings out a counterintuitive property of Perlis&apos;s system: a sentence may follow directly from Perlis&apos;s axioms, yet he refuses to call it true, or to allow that any agent can know it. Strange though this appears, it is a natural consequence of the use of two definitions of truth in a single theory. Belief is different from knowledge because it need not be true. This makes it surprising that Thomason&apos;s paradox involves only the notion of belief, not knowledge or truth. In fact the paradox arises exactly because Thomason&apos;s agent thinks that all his beliefs are true. This is stated as a(&lt; a(&lt; So &gt;) ---■ &gt;) Computational Linguistics Volume 16, Number 4, December 1990 219 Andrew R. Haas Sentential Semantics for Propositional Attitudes (Thomason 1980). The notation is as follows: So is a variable ranging over all formulas of the language, &lt; So &gt; is a denoting (the Godel number of) So, and means that the agent believes So. This axiom says that for every formula &lt;P, the agent believes a(&lt; So &gt;) —• sentence says that if the agent believes (P be true. Since So ranges over all sentences of the language, the agent is claiming that his beliefs are infallible. This leads the agent into a paradox similar to the Knower, and his beliefs are therefore inconsistent. Asher and Kamp showed that one can avoid this conclusion by denying (35) in certain cases where 4) is a self-referential sentence. Another alternative is to dismiss (35) completely. It is doubtful that human beings consider their own beliefs infallible, and Perlis (1986) has argued that a rational agent may well believe that some of his or her beliefs are false. We have looked at three sentential analyses of the selfreference paradoxes, and each one sacrifices some principle that seems useful for reasoning in an AI program. The alternative is an analysis in which propositions are not sentences. Thomason (1986) considers such analyses and finds that they have no clear advantage over the sentential approaches. The unpleasant truth is that paradoxes of self-reference create equally serious problems for all known theories of attitudes. It follows that they provide no evidence against the sentential theories. BASIC 2.1 NOTATION The rules of our grammar are definite clauses, and we use the notation of definite clause grammar (Pereira and Warren 1980). This notation is now standard among computer scientists who study natural language and is explained in a textbook by Pereira and Shieber (1987). Its advantages are that it is well defined and easy to learn, because it is a notational variant of standard first-order logic. Also, it is often straightforward to parse with grammars written in this notation (although there can be no general parsing method for the notation, since it has Turing machine power). DCG notation lacks some useful devices found in linguistic formalisms like GPSG—there are no default feature values or general feature agreement principles (Gazdar et al. 1985). On the other hand, the declarative semantics of the DCG notation is quite clear—unlike the semantics of GPSG (Fisher 1989). The grammar is a set of meta-language sentences describing a correspondence between English words and sentences of the target language. Therefore, we must define a notation for talking about the target language in the metalanguage. Our choice is a notation similar to that of Haas (1986). If f is a symbol of the target language, &apos;f is a symbol of the meta-language. Suppose f is a constant or a variable, taking no arguments. Then &apos;f denotes f. Thus &apos;john is a meta-language constant that denotes a target-language constant, while &apos;x is a meta-language constant that denotes a target-language variable. Suppose f is a functor of the language and takes Then &apos;f is a metalanguage function letter, and it denotes the function that of the target language . . . to the expression . . . &apos;not is a metalanguage function letter, and it denotes the function that maps a target language wff to its negation. In the same way, &apos;or is a meta-language function letter, and it denotes the function that maps two target-language wffs to their disjunction. Given these denotations, it is easy to see that if p(a,b) is an atomic sentence in the target language, then &apos;p(&apos;a,&apos;b) is a term in the meta-language, and it denotes the wff p(a,b) in the target language. Suppose that Wffl and Wff2 are meta-language variables ranging over wffs of the target language. Then &apos;or(Wffl ,Wff2) is a meta-language term, and since the variables Wffl and Wff2 range over all wffs of the target language, the value of &apos;or(Wff 1 ,Wff2) ranges over all disjunctions in the target language. These ideas about the relation between meta-language and target language are not new or difficult, but it is worth the time to explain them, because some influential papers about semantics in unification grammar have confused the target language and meta-language (see Section 2.4). For the sake of legibility, we omit the quotation marks—so when or(WfIl ,Wff2) appears in a rule of the grammar, it is an abbreviation for &apos;or(Wff 1 ,Wff2). 2.2 REPRESENTING QUANTIFIERS Noun phrases in the grammar contribute to logical form in two ways, and therefore they have two semantic features. The first feature is a variable, which becomes a logical argument of a verb. This produces a wff, in which the variable appears free. The second feature is a quantifier that binds the variable. By applying the quantifier to the wff, we eliminate free occurrences of that particular variable. After applying all the quantifiers, we have a wff without free variables—a sentence. This is the logical form of an utterance. In Montague&apos;s system (Montague 1974a), the logical form of an NP is an expression denoting a quantifier. This kind of analysis is impossible in our system, because the target language is first-order. It contains no expressions that denote quantifiers. Therefore the representation of an NP cannot be an expression of the target language. Instead of using Montague&apos;s approach, we associate with every quantifier a function that maps wffs to wffs. For the NP &amp;quot;every man,&amp;quot; we have a function that maps any wff Wffl to the wff (37) all(V,man(V),Wffl) where V is a variable of the target language. Notice that if we took Montague&apos;s representation for the quantified NP, applied it to the lambda expression lambda(V,Wff1), and then simplified, we would get an alphabetic variant of (37). 220 Computational Linguistics Volume 16, Number 4, December 1990 Andrew R. Haas Sentential Semantics for Propositional Attitudes We will call this function the application function for the quantified NP. To represent application functions in a unification grammar, we use a device from Pereira and Warren (1980). We assign to each NP an infinite set of readings—one for each ordered pair in the extension of the application function. The first and second elements of the ordered pair are semantic features of the NP, and the bound variable of the quantifier is a third feature. For the NP &amp;quot;every man&amp;quot; we have (38) np(V,Wffl,all(V,man(V),Wff1)) --. [every man] . This says that for any variable V and wff Wffl, the string &amp;quot;every man&amp;quot; is an NP, and if it binds the variable V, then the pair [Wffl,all(V,man(V),Wff1)] is in the extension of its application function. It follows that the application function maps Wffl to the wff all(V,man(V),Wff1). When other rules fix the values of the variables V and Wffl, the result of the mapping will be fixed as well. A more complex example is (39) woman(V),Wff1))) —&gt; [a man and a woman]. Here the application function&apos;s output includes two copies of the input. It is important to consider the declarative semantics of these rules. Each one states that a certain NP has an infinite set of possible readings, because there are infinitely many wffs in the target language. Thus we might say that the NP in isolation is infinitely ambiguous. This &amp;quot;ambiguity&amp;quot; is purely formal, however; in any actual utterance the value of the variable Wffl will be supplied by other rules, so that in the context of an utterance the ambiguity is resolved. In the same way, the VP &amp;quot;liked Mary&amp;quot; is ambiguous in person and number—but in the context of the utterance &amp;quot;John liked Mary,&amp;quot; its person and number are unambiguous. In one respect the declarative semantics of these rules is not quite right. The variable V is supposed to range over variables of the target language, and the variable Wffl is supposed to range over wffs of the target language. Yet we have not defined a type system to express these range restrictions. However, such a type system could be added, for example, using the methods of Walther (1987). In fact, the type hierarchy would be a tree, which allows us to use a simplified version of Walther&apos;s methods. For brevity&apos;s sake we will not develop a type system in this paper. Except for this omission, the declarative semantics of the above rules is quite clear. Typed variables have mnemonic value even if we do not use a typed logic. Therefore we adopt the following conventions. The meta-language variables V, VO, V1 . . . range over target language variables. Wff, Wffl, Wff2 . . . range over target language wffs. Q, Qi, Q2 . . . range over quantifiers. QL, QL1, QL2 . . . range over lists of quantifiers. When a wff forms the range restriction of a quantifier, we will sometimes use the variables Range, Rangel . . . for that wff. 2.3 SCOPING AND QUANTIFIER STORAGE Given a means of describing quantifiers, we must consider the order of application. Cooper (1983) has shown how to allow for different orders of application by adding to NPs, VPs, and sentences an extra semantic feature called the store. store is a list of quantifiers that bind the free variables in the logical form of the phrase. The grammar removes quantifiers from the store and applies them nondeterministically to produce different logical forms, corresponding to different orders of application. If a senhas a logical form a quantifier store /, then free variable in be bound by a quantifier in 1—otherwise the final logical form would contain free variables. Our treatment of quantifier storage is different from Cooper&apos;s in two ways. First, Cooper&apos;s grammar maps phrases to model-theoretic denotations, not logical forms. This sounds like a bigger difference than it is. The basic technique is to put quantifiers in a store, and use some kind of marker to link the stored quantifiers to the argument positions they must bind. Whether we work with the logical forms or with their denotations, much the same problems arise in applying this technique. A second difference is that in Cooper&apos;s grammar, each NP has two readings—one in which the NP&apos;s quantifier is in the store, and one in which it is not. The first reading leads to wide-scope readings of the sentence, while the second leads to narrow-scope readings. In our grammar only the first kind of reading for an NP exists—that is, the quantifier of an NP is always in the store. We generate both wideand narrow-scope readings by applying the quantifiers from the store in different orders. We represent a quantifier as a pair p(Wff 1 ,Wff2), where the application function of the quantifier maps Wffl to Wff2. We represent a quantifier store as a list of such pairs. The predicate apply_quants(QL1,Wff 1 ,QL2,Wff2) means that QL1 is a list of quantifiers, Wffl is a wff, Wff2 is the result of applying some of the quantifiers in QL1 to Wffl, and QL2 contains the remaining quantifiers. The first axiom for the predicate says that if we apply none of the quantifiers, then QL2 = QL1 and Wff2 = Wffl: (40) apply_quants(QL,Wff,QL,Wff). The second axiom uses the predicate choose(L1,X,L2), which means that X is a member of list Ll, and L2 is formed by deleting one occurrence of X from Ll. apply_quants(QL1,Wff1,QL3,Wff3) :choose(QL1,p(Wffl,Wff2),QL2), apply_quants(QL2,Wff2,QL3,Wff3). Consider the first literal on the right side of this rule. It says that p(Wff1,Wff2) is a member of QL1, and deleting p(Wffl ,Wff2) from QL1 leaves QL2. By definition, if the pair p(Wff 1 ,Wff2) is in the extension of the application function for a certain quantifier, the application function Computational Linguistics Volume 16, Number 4, December 1990 221 Andrew R. Haas Sentential Semantics for Propositional Attitudes maps Wffl to Wff2. The second literal says that applying a subset of the remaining quantifiers QL2 to Wff2 gives a new wff Wff3 and a list QL3 of remaining quantifiers. Then applying a subset of QL1 to Wffl gives Wff3 with remaining quantifiers QL3. Suppose that QL1 is (42) [p(Wffl,all(V1,man(V1),Wff1)),p(Wff2,some(V2, woman(V2),Wff2) )] Then solutions for the goal (43) :apply.quants (QL1,1oves(V1,V2),QL3,Wff3) include Wff3 = all(V1,man(V1), some(V2,woman(V2),loves(VI,V2))) QL3 = [] and also Wff3 = some(V2,woman(V2), all (V1,man (V1),loves(V1,V2))) QL3 = []. There are also solutions in which some quantifiers remain in the store: Wff3 = all(V1,man(V1),loves(V1,V2)) QL3 = [p(Wff2,some(V2,woman(V2),Wff2))]. These solutions will be used to build wide-scope readings for propositional attitude reports. 2.4 THE PROBLEM OF ASSIGNING VARIABLES TO QUANTIFIERS The rules we have given so far do not tell us which target language variables the quantifiers bind. These rules contain meta-language variables that range over target language variables, rather than meta-language constants that denote particular variables of the target language. In choosing the bound variables it is sometimes crucial to assign distinct variables to different quantifiers. The logical form of &amp;quot;Some man loves every woman&amp;quot; can be (47) some(x,man(x),a11(y,woman(y),loves(x,y))) but it cannot be (48) some(y,man(y),a11(y,woman(y),loves(y,y))). This reading is wrong because the inner quantifier captures the variables that are supposed to be bound by the outer quantifier. To be more precise: the outer quantifier binds the variable y, but not all occurrences of y in the scope of the outer quantifier are bound by the outer quantifier. Some of them are bound instead by the inner quantifier. In situation, we say that the inner quantifier outer one. We require that no quantifier ever shadows another in any logical form built by the grammar. This requirement will not prevent us from finding logical forms for English sentences, because any first-order sentence is logically equivalent to a sentence without shadowing. The same problem arises in cases of quantification into the scope of attitudes. Consider the sentence &amp;quot;John thinks some man loves every woman,&amp;quot; and suppose that &amp;quot;some man&amp;quot; has wide scope and &amp;quot;every woman&amp;quot; has narrow scope. The logical form can be some(x,man(x), thinks( john,[x],q(all(y,woman(y),loves(x,y))))) but it cannot be some(y,man(y), thinks( john,[y],q(all(y,woman(y),loves(y,y))))). In this formula, the inner quantifier captures a variable that is supposed to be a dummy variable. In this case also, we say that the inner quantifier shadows the outer one. Pereira and Warren (1980) prevented shadowing by using Prolog variables to represent variables of the object language. Thus, their translation for &amp;quot;Some man loves every woman&amp;quot; is (51) exists(Y) : (man(Y) &amp; all(X) : (woman(X) loves(Y,X))) where X and Y are Prolog variables. This works, but it violates the declarative semantics of Prolog. According to that semantics every variable in an answer is universally quantified. Thus if Prolog returns (51) as a description of the logical form of a sentence, this means that for all values of X and Y the expression (51) denotes a possible logical form for that sentence. This means that if v is a variable of the object language, then (52) exists(v) : (man(v) &amp; all(v) : (woman(v) loves (v,v))) is a possible translation, which is clearly false. Thus, according to the declarative interpretation, Pereira and Warren&apos;s grammar does not express the requirement that no quantifier can shadow another quantifier. Pereira and Shieber (1987) pointed out this problem and said that while formally incorrect the technique was &amp;quot;unlikely to cause problems.&amp;quot; Yet on p. 101 they describe the structures built by their grammar as &amp;quot;unintuitive&amp;quot; and even &amp;quot;bizarre.&amp;quot; This confirms the conventional wisdom: violating the declarative semantics makes logic programs hard to understand. Therefore, let us look for a solution that is formally correct. Warren (1983) suggested one possible solution. We can use a global counter to keep track of all the variables used in the logical form of a sentence, and assign a new variable to every quantifier. Then no two quantifiers would bind the same variable, and certainly no quantifier would shadow another. This solution would make it easier to implement treatment of re reports, but it would also create serious problems in the treatment of NP conjunction Linguistics Volume 16, Number 4, December 1990 Andrew R. Haas Sentential Semantics for Propositional Attitudes and disjunction (see Section 2.5). Therefore we consider another possibility. Let us rewrite the definition of &amp;quot;apply_quants,&amp;quot; adding the requirement that each quantifier binds a variable that is not bound in the scope of that quantifier. For each integer N, let v(N) be a variable of the target language. If N is not equal to M, then v(M) and v(N) are distinct variables. We represent the integers using the constant 0 and the function &amp;quot;s&amp;quot; for &amp;quot;successor&amp;quot; in the usual way. The predicate highest_bound_var(Wffl ,N) means that N is the largest number such that v(N) is bound in Wffl. To define this predicate, we need one axiom for each quantifier, connective, and predicate letter of the target language. These axioms are obvious and are therefore omitted. We also need the predicate binds(Wffl,V), which means that the outermost quantifier of Wff1 binds the variable V. To define this predicate we need an axiom for each quantifier and connective. Typical axioms are: (53) binds(all(V,Wffl ,Wff2),V). (54) binds(and(Wff 1 ,Wff2),V) binds(Wffl,V). The second axiom applies to complex quantifiers arising from conjoined NPs. In this case there are two branches, but each branch binds the same variable (the rules for NP conjunction ensure that this is so). Therefore, we recursively check the first branch to find the bound variable. Given these predicates, we can rewrite the second axiom for &amp;quot;apply_quants&amp;quot;: apply_quants(QL1,Wffl,QL3,Wff3) choose(QL1,p(Wffl,Wff2),QL2), highest_bound_var(Wffl,N), binds(Wff2,v(s(N))), apply_quants(QL2,Wff2,QL3,Wff3). Wffl is the scope of the quantifier, and v(N) is the highest bound variable of Wffl . The new quantifier binds the variable v(s(N)), which is different from every bound variable in the scope Wffl. Therefore, the new quantifier is not shadowed by any lower quantifier. As an example, suppose that QL1 is (56) [p(Wff2,all(V2,woman(V2),Wff2))]. Then solutions for the goal (57) :apply_quants(QL1,1oves(VI,V2),QL3,Wff3) include Wff3 = all(v(1),woman(v(1)),loves(v(1),V2))) QL3 = []. (We have reverted to standard notation for integers.) Suppose that QL1 is (59) [p(Wffl,some(VI,man(V1),Wff1)),p(Wff2,all (V2,woman(V2), Wff2) )]. Then solutions for the goal apply_quants(QL1,1oves(V1,V2),QL3,Wff3). include Wff 3 = some(v(2),man(v(2), all(v(1),woman(v(1),loves(v(2),v(1)))) QL3 = [1. The inner quantifier binds the variable v(1), and the outer quantifier binds the variable v(2). This notation for variables is very hard to read, so in the rest of the paper we will use the constants x, y, and z to represent variables of the target language. 2.5 RULES FOR NOUN PHRASES The following grammar is very similar to the work of Pereira and Shieber (1987, Sections 4.1 and 4.2). There are two major differences, however, First, the treatment of quantifiers and scoping uses a version of Cooper&apos;s quantifier storage, instead of the &amp;quot;quantifier tree&amp;quot; of Pereira and Shieber. Second, Pereira and Shieber started with a semantics using lambda calculus, which they &amp;quot;encoded&amp;quot; in Prolog. In the present grammar, unification semantics stands on its own—it is not a way of encoding some other formalism. Formula numbering uses the following conventions. The rules of the grammar are numbered (R1), (R2), etc. Entries in the lexicon are numbered (L1), (L2), etc. Formulas built in the course of a derivation get numbers without a prefix. Groups of related rules are marked by lower case letters: (L I a), (L 1 b), and so forth. Every noun phrase has a quantifier store as one of its semantic features. If the NP is a gap, the store is empty; if the NP is not a gap, the first element of the store is the quantifier generated by the NP (in the present grammar, the quantifier store of an NP has at most one quantifier). We represent the quantifier store as a difference list, using the infix operator &amp;quot;-&amp;quot;. Thus if L2 is a tail of Ll, Li-L2 is the list difference of LI and L2: the list formed by removing L2 from the end of Li. Therefore a noun phrase has the form np(V,QL1-QL2,Fx-Fy,VL). V is the bound variable of the NP. QL1-QL2 is the quantifier store of the NP. We describe wh-movement using the standard gap-threading technique (Pereira and Shieber 1987), and Fx-Fy is the filler list. Finally, VL is a list of target-language variables representing NPs that are available for reference by a pronoun, which we will call the pronoun reference list. Consider an NP consisting of a determiner and a head noun: &amp;quot;every man,&amp;quot; &amp;quot;no woman,&amp;quot; and so forth. The head noun supplies the range restriction of the NP&apos;s quantifier, and the determiner builds the quantifier given the range restriction. The bound variable of the NP is a feature of both the determiner and the head noun. Then the following rule generates NPs consisting of a determiner and a head Computational Linguistics Volume 16, Number 4, December 1990 223 Andrew R. Haas Sentential Semantics for Propositional Attitudes noun: (R1) np(V,[QIQL]-QL,Fx-Fx,VL) det(V,Wffl,Q),n(V,Wff1). The quantifier list [Q I QL] — QL = [Q] contains the quantifier for the NP. We have the following rules for common nouns: (R2a) n(V1,pizza(V1)) [pizza] (R2b) n(V1,man(V1)) [man] (R2c) n(V1,woman(V1)) [woman]. Recall that p(Wffl,Wff2) is a quantifier that maps Wffl to Wff2. Then for determiners we have (R3a) det(V2,Range,p(Wff1,some(V2,Range,Wff1)))—■ [a] (R3b) det(V2,Range,p(Wffl,all(V2,Range,Wff1))) [every] (R3c) det(V2,Range,p(Wff1,unique(V2,Range, Wffl))) - [the] (R3d) det(V2,Range,p(Wffl,not(some(V2,Range, Wff1)))) —■ [no]. Then we get (62) np(V,[p(Wffl,all(V,man(V),Wff1))1 QL]-QL,Fx- Fx,L) —■ [every man] (63) np(V,[p(Wffl,not(some(V,woman(V),Wff1)))1 QL]-QL,Fx-Fx,L) [no woman]. Thus &amp;quot;every man&amp;quot; is an NP that binds the variable V and maps Wffl to all(V,man(V),Wff1). Following Moore (1988), we interpret the proper name &amp;quot;John&amp;quot; as equivalent to the definite description &amp;quot;the one named &amp;quot;John.&amp;quot;&amp;quot; (R4) np(V,[p(Wff,unique(V,name(V,C),Wff))I QL1- QL,Fx-Fx,VL) - [Terminal], 1 proper_noun(Terminal,C) 1. The wff proper_noun(X,Y) means that X is a proper noun and Y is its logical form. Our lexicon includes the axioms (LI a) proper_noun(johnjohn) (Li b) proper_noun(mary,mary) (Li c) proper_noun(bill,bill). These axioms use the constant &amp;quot;john&amp;quot; to denote both a terminal symbol of the grammar and a constant of the target language—a convenient abuse of notation. Using (Lia) we get (64) np(V,[p(Wffl,unique(V,name(V,john),Wff1))I QL]- QL,Fx-Fx,VL) [john]. That is, &amp;quot;john&amp;quot; is an NP that binds the variable V and maps Wffl to the wff unique(V,name(V,john),Wff1). Pronouns use the &amp;quot;let&amp;quot; quantifier. We have (R5) np(V2,[p(V2,Wffl,let(V2,V,Wff1))1 QL]-QL,Fx- Fx,VL) [he], {member(V,VL)}. If V is a variable chosen from the pronoun reference list VL, then &amp;quot;he&amp;quot; is an NP that binds the variable V2 and maps Wffl to let(V2,V,Wff1). Thus, the pronoun refers back to a noun phrase whose bound variable is V. Later, we wi:1 see the rules that put variables into the list VL. As an example, we have (65) np(V2,[p(Wffl,let(V2,V1,Wff1))1 Fx,[V1]) [he]. The &amp;quot;let&amp;quot; quantifier in pronouns looks redundant, but it is useful because it makes the semantics of NPs uniform— every NP (except gaps) has a quantifier. This is helpful in describing conjoined NPs. Suppose that NP1 binds variable V and maps Wffl to Wff2. Suppose NP2 also binds variable V and maps the same Wffl to Wff3. Then the conjunction of NP! and NP2 binds V and maps Wffl to and(Wff2,Wff3): (R6) np(V,[p(Wffl,and(Wff2,Wff3))1 QL1]-QL3,Fx- Fx,VL) np(V, [p(Wff 1 ,Wff2)IQL1]-QL2,Fz-Fz,VL), [and], np(V,[p(Wffl,Wff3)I QL2]-QL3,Fy-Fy,VL). As an example we have np(V,[p(Wffl,all(V,man(V),Wff1))I QL1]-QL1,Fx- Fx,L) [every man] (67) np(V,[p(Wffl,all(V,woman(V),Wff1))I QL2]- QL2,Fx-Fx,L) [every woman] np(V, [p(Wffl,and(all(V,man(V),Wff1),all(V,woman (V),Wff1))) QL1]-QL1, Fx-Fx,VL) —[every man and every woman]. That is, &amp;quot;every man and every woman&amp;quot; is an NP that binds variable V and maps Wffl to (69) and(all(V,man(V),Wff1),all(V,woman(V),Wff1)). We also have (70) np(V,[p(Wffl,let(V,V1,Wff1))I QL1]-QL1,Fx- Fx,[V1]) [he] (71) np(V,[p(Wffl,unique(V,name(V,john),Wff1))1 QL2]-QL2,Fx-Fx,VL) [john]</abstract>
<note confidence="0.865486">(72) np(V, [p(Wffl,and(let(V,V1,Wff1),unique(V,name (V,john),Wff1)))1 QL1]-QL1, Fx-Fx,[V1]) [he and john]. That is, &amp;quot;he and John&amp;quot; is an NP that binds V and maps Wffl to (73) and(let(V,V1,Wff1),unique(V,name(V,john),Wffl) 224 Computational Linguistics Volume 16, Number 4, December 1990 Andrew R. Haas Sentential Semantics for Propositional Attitudes</note>
<abstract confidence="0.972387703030303">where V1 is chosen from the pronoun reference list. Thus the conjunction rule works for pronouns and proper nouns exactly as it does for NPs with determiners. There is a similar rule for disjunction of NPs. In conjoining two NPs we combine their quantifiers, which are the first elements of their quantifier stores. We must also collect the remaining elements of both quantifier stores. The above rule achieves this result by concatenating difference lists in the usual way: if QL1-QL2 is the tail of the first NP&apos;s quantifier list, and QL2-QL3 is the tail of the second NP&apos;s quantifier list, then QL1-QL3 is the concatenation of the tails. In the present grammar both tails are empty, because the quantifier store of an NP contains at most one quantifier, but in a more general grammar the tails might contain quantifiers—for example, quantifiers from prepositional phrases modifying the NP. Thus the Montague-style semantics for NP conjunction and disjunction requires an extension of standard Cooper storage. When the quantifier store of an NP contains several quantifiers, we must be able to identify the one that represents the NP itself (as opposed to quantifiers that arise from attached PPs, for example). We must then be able to remove this quantifier from the store, build a new quantifier, and put the new quantifier back into the store. Rule (R6) requires that the two NPs being conjoined should have quantifiers that bind the same variable. Suppose we had chosen the bound variables in the logical forms by using a global counter to ensure that no two quantifiers ever bind the same variable (as suggested in Section 2.4). Then (R6) could never apply. Thus our treatment of NP conjunction forces us to choose the bound variables after the quantifiers from conjoined NPs have been combined into a single quantifier, as described in Section 2.4. This choice in turn creates difficulties in implementing our of re reports, as we will see in Section 3.1. In this grammar, a conjunction of quantified NPs produces a logical form in which the two quantifiers are in separate wffs, and these wffs are joined by the connective neither quantifier is in the scope of the other. This gives the desired reading for a sentence such as &amp;quot;John has no house and no car&amp;quot;: (74) and (not(some(x,house(x),has(john,x))),not(some (x,car(x), has( john,x)))). However, consider the sentence &amp;quot;John met a farmer and his wife&amp;quot; and suppose the pronoun &amp;quot;his&amp;quot; refers to &amp;quot;a farmer.&amp;quot; Under our analysis, the quantifier from &amp;quot;a farmer&amp;quot; cannot bind a variable in the range restriction of the other quantifier—because its scope does not include the other quantifier. Thus, the Montagovian analysis of NP conjunction is certainly correct in some cases, but it cannot be the whole story. 2.6 VERB PHRASE AND SENTENCE RULES Our grammar includes two kinds of transitive verbs: ordinary verbs like &amp;quot;eat&amp;quot; and &amp;quot;buy,&amp;quot; and propositional attitude verbs like &amp;quot;want&amp;quot; and &amp;quot;seek.&amp;quot; Only verbs of the second have dicto There is a dicto for &amp;quot;John wants a Ferrari,&amp;quot; which does not imply that there is any particular Ferrari he wants. There is no such reading &amp;quot;John bought a Ferrari.&amp;quot; To build a dicto a verb like &amp;quot;want&amp;quot; must have access to the quantifier of its direct object. Verbs like &amp;quot;buy&amp;quot; do not need this access. This leads to a problem that has been well known since Montague. The two kinds of verbs, although very different in their semantics, seem to be identical in their syntax. We would like to avoid duplication in our syntax by writing a single rule for VPs with transitive verbs. This rule must allow for both kinds of semantics. Montague&apos;s solution was to build a general semantic representation, which handles both cases. When the verb is &amp;quot;eat&amp;quot; or &amp;quot;buy,&amp;quot; one uses a meaning postulate to simplify the representation. Our solution is similar: we allow every transitive verb to have access to the quantifier of its direct object, and then assert that some verbs don&apos;t actually use the quantifier. However, our solution improves on Montague and Cooper by avoiding the simplification step. Instead, we build a simple representation in the first place. A verb has one feature, the subcategorization frame, which determines what arguments it will accept and what logical form it builds. The rule for verbs says that if a terminal symbol has a subcategorization frame Subcat, then it is a verb: (R7) v(Subcat) [Terminal], has_subcat(Terminal, Subcat) 1. A subcategorization frame for a transitive verb has the form (75) trans(V1,V2,QL1,QL2,Wff1). V1 is a variable representing the subject, and V2 is a variable representing the object. QL1 is the quantifier store of the object. QL2 is a list of quantifiers remaining after the verb has built its logical form. For an ordinary transitive verb, QL2 equals QL1. Wffl is the logical form of the verb. In the case of ordinary transitive verbs, we would like to assert once and for all that QL1 = QL2. Therefore, we write (L2) has_subcat(Terminal,trans(V1,V2,QL1,QL1,Wff)) ordinary_trans(Terminal,V1,V2,Wff). This axiom says that for an ordinary transitive verb, the two lists of quantifiers are equal, and the values of the other features are fixed by the predicate &amp;quot;ordinary_trans.&amp;quot; We have (L3a) ordinary_trans(saw,V1,V2,saw(V1,V2)) (L3b) ordinary_trans(ate,V1,V2,ate(V1,V2)). From (R7), (L2), and (L3a) we get (76) v(trans(V1,V2,QL1,QL1,saw(V1,V2))) [saw]. Computational Linguistics Volume 16, Number 4, December 1990 225 Andrew R. Haas Sentential Semantics for Propositional Attitudes The features of a verb phrase are a variable (representing the subject), a wff (the logical form of the VP), a quantifier store, a list of fillers, and a pronoun reference list. The rule for a verb phrase with a transitive verb is (R8) vp(V1,W ff 1 ,QL2,Fx-Fy,L) v(trans(V1,V2,QL1,QL2,Wff1)), np(V2,QL1,Fx-Fy,L). If the verb is an ordinary transitive verb, then QL1 = QL2, so the quantifier store of the VP is equal to the quantifier store of the direct object. From (R1), (R3a), and (R2b) we have np(V, [p(Wff 1 ,some(V,man(V),Wff1))1Q11 -QL,Fx- Fx,L) [a man]. Resolving (76) and (77) against the right side of R8 gives vp(VI,saw(V1,V2),[p(Wffl,some(V2,man(V2), Wff1))1 QL1-QL,Fx-Fx,L) [saw a man]. The quantifier store contains the quantifier of the NP &amp;quot;a man.&amp;quot; A sentence has four features: a wff, a quantifier store, a list of fillers, and a pronoun reference list. The rule for a declarative sentence is (R9) s(Wff2,QL4,Fx-Fz,L) np(V,QL1-QL2,Fx-Fy,L), vp(V,Wff 1 ,QL2-QL3,Fy-Fz,L), 1 apply_quants(QL1-QL3,Wff 1 ,QL4,Wff2) 1. The variable V represents the subject, so it becomes the first argument of the VP. QL1-QL3 is the concatenation of the quantifier stores from the subject and the VP. &amp;quot;Apply_quants&amp;quot; will apply some of these quantifiers to the logical form of the VP to produce the logical form Wff2 of the sentence. The list QL4 of remaining quantifiers becomes the quantifier store of the sentence. We have np(V1, [p(Wff0,all(V1,woman(V1),Wff0))1 QL11- QL1,Fx-Fx,L) [every woman]. From (R9), (79), and (78), we get s(Wff2,QL4,Fx-Fx,L) [every woman saw a man], lapply_quants([p(Wff0,all(V1,woman(V1),Wff0)), p(Wffl ,some(V2,man(V2),Wff1))1QL31- QL3, saw(V1,V2), QL4, Wff2) 1. The &amp;quot;apply_quants&amp;quot; subgoal has several solutions. Choosing the one in which &amp;quot;every woman&amp;quot; outscopes &amp;quot;a man,&amp;quot; we get (81) s(all(x,woman(x),some(y,man(y),saw(x,y))),QL3- QL3,Fx-Fx,L) [every woman saw a man]. The derivation is not yet complete, because &amp;quot;s&amp;quot; is not the start symbol of our grammar. Instead we use a special symbol &amp;quot;start,&amp;quot; which never appears on the right side of a rule. Thus, the start symbol derives only top-level sentences—it cannot derive an embedded sentence. This is useful because top-level sentences have a unique semantic property: their logical forms must not contain free variables. It might seem that one can eliminate free variables simply by applying all the quantifiers in the store. Hobbs and Shieber (1987) pointed out that this is not so—it is essential to apply the quantifiers in a proper order. Consider the sentence &amp;quot;every man knows a woman who loves him,&amp;quot; with &amp;quot;him&amp;quot; referring to the subject. The subject quantifier binds a variable that occurs free in the range restriction of the object quantifier, so one must apply the object quantifier first in order to eliminate all free variables. Therefore our grammar includes a filter that eliminates readings of top-level sentences containing free variables. Let free_vars(Wff 1 ,L) mean that L is a list of the free variables of Wffl in order of first occurrence. We omit the easy definition of this predicate. The rule for top-level sentences is: (R10) start(Wffl) s(Wffl,QL-QL,Fx-Fx,[]), free_vars(Wffl , []). The goal free_vars(Wffl,[]) filters out readings with free variables. The above rule allows us to complete the derivation for &amp;quot;every woman saw a man&amp;quot;: (82) start(all(x,woman(x),some(y,man(y),saw(x,y)))) [every woman saw a man]. Having treated sentences, we can now consider gaps and relative clauses. The rule for gaps follows Pereira and Schieber (1987): (R11) np(V,QL-QL, [gap(V)1 Fx]-Fx,VL) []. This rule removes the marker gap(V) from the filler list, and makes the associated variable V the variable of the empty NP. The list difference QL-QL is the empty list, so the quantifier store of the gap is empty. The rule that generates NPs with relative clauses is np(V, [Q1 QL]-QL,Fx-Fx,L) det(V,and(Rangel,Range2),Q), n(V,Rangel), [that], s(Range2,QL1-QL1,[gap(V)]-[],[]). 226 Computational Linguistics Volume 16, Number 4, December 1990 Andrew R. Haas Sentential Semantics for Propositional Attitudes The relative clause is a sentence containing a gap, and the logical form of the gap is the variable V—the same variable that the quantifier binds. The logical form of the relative clause becomes part of the range restriction of the quantifier. We have s(some(x,pizza(x),ate(V,x)),QL-QL,[gap(V)I Fx]- Fx,[]) [ate a pizza]. The derivation of this sentence is much like the one for &amp;quot;every woman saw a man&amp;quot; above, except that in place of the subject &amp;quot;every woman&amp;quot; we have a gap as a subject. The rule for gaps above ensures that the variable of the gap is V, the only variable in the filler list, and its quantifier store is empty. Therefore, V appears as the first argument of the predicate &amp;quot;ate.&amp;quot; Continuing the derivation we get (84) np( V, [ p ( W ffl ,some (V,and (man ( V),some (x,pizza (x),ate(V,x))),Wff QL]-QL,Fx-Fx,[]) man that ate a pizza]. The string &amp;quot;a man that ate a pizza&amp;quot; is an NP that binds V and maps Wffl to the wff (85) some(V,and(man(V),some(x,pizza(x),ate(V,x))), Wffl). Notice that in the rule for NPs with relative clauses, the quantifier store of the relative clause is empty. This means that no quantifier can be raised out of a relative clause. there is no scope ambiguity in a man that loves every woman.&amp;quot; According to Cooper (1979), this is correct. The restriction is easy to state because in our grammar, quantifier raising is combined with syntax and semantics in a single set of rules. It would be harder to state the same facts in a grammar like Pereira and Shieber&apos;s (1987), because quantifier raising there operates on a separate called a tree. tree leaves out syntactic information that is needed for determining scopes—for example, the difference between a relative clause and a prepositional phrase. GRAMMAR 3.1 ATTITUDE VERBS TAKING CLAUSES The following rule introduces verbs such as &amp;quot;believe&amp;quot; and &amp;quot;know,&amp;quot; which take clauses as their objects. (R13) vp(V1,Wff1,QL1,Fx,L) v(takes_s(V1,Wff2,Wff1)), s(Wff2,QL1,Fx,[V1 I L]). The verb takes the logical form Wff2 of the object clause and the subject variable V1, and builds the wff Wffl representing the VP. This rule also adds the subject variable to the pronoun reference list of the object clause. For verb &amp;quot;thought,&amp;quot; we have the following subcategorization frame: (L4) has_subcat(thought, takes_s(V1,Wffl, thought(VI,Varsl,q(Wff1)))) free_vars(Wffl,Vars1). The subject variable becomes the first argument of the predicate &amp;quot;thought.&amp;quot; The logical form of the object clause is Wffl, and it appears under a quotation mark as the third argument of &amp;quot;thought.&amp;quot; The second argument of &amp;quot;thought&amp;quot; the re list, and the predicate &amp;quot;free_vars&amp;quot; that the re list is a list of the free variables in Wffl, as required by our convention. From the (R8) for verbs and get (86) v(takes_s(V1,Wff2,thought(V1,Varsl,q(Wff2)))) - [thought], { free_vars(Wff2,Vars1) }. The &amp;quot;free_vars&amp;quot; subgoal has not been solved—it has been postponed. Indeed it must be postponed, because as long as its first argument is a variable, it has an infinite number of solutions—one for each wff of our language. Consider the example &amp;quot;John thought Mary ate a pizza.&amp;quot; consider two readings. &amp;quot;A pizza&amp;quot; is understood dicto both readings, but &amp;quot;Mary&amp;quot; is re one reading and the other. The ambiguity arises from the embedded sentence, because the predicate &amp;quot;apply_quants&amp;quot; can either the quantifiers or leave them in the store. applies the quantifiers from &amp;quot;Mary&amp;quot; and &amp;quot;a pizza&amp;quot; in their surface order, we get (87) s(unique(y,name(y,mary),some(x,pizza(x),ate (y,x)),QL-QL, Fx-Fx,L) - [mary ate a pizza]. From (R13), (86), and (87) we get (88) vp(V1, thought(V1,Varsl, q(unique(y,name(y,mary),some(x,pizza (x),ate(y,x))))), QL-QL,Fx-Fx,L) —÷ [thought Mary ate a pizza], {free_vars(unique(y,name(y,mary),some(x,pizza(x), ate(y,x))),Vars1) 1. Since the first argument of &amp;quot;free_vars&amp;quot; is now a ground term, we can solve the &amp;quot;free_vars&amp;quot; subgoal, getting Varsl = []. Then we have (89) vp(V1, thought(V1,[],q(unique(y,name(y,mary),some(x, pizza(x),ate(y,x))))), QL-QL,Fx-Fx,L) - [thought Mary ate a pizza]. Computational Linguistics Volume 16, Number 4, December 1990 227 Andrew R. Haas Sentential Semantics for Propositional Attitudes If we combine this VP with the subject &amp;quot;John&amp;quot; we get a sentence whose logical form is unique(z,name(z,john), thought(z, [],q(unique(y,name(y,mary),some (x,pizza(x),ate(y,x)))))). for the reading in which &amp;quot;mary&amp;quot; is re. again, consider the embedded sentence &amp;quot;Mary ate a pizza.&amp;quot; Suppose that the predicate &amp;quot;apply_quants&amp;quot; applies the quantifier from &amp;quot;a pizza&amp;quot; and leaves the one from &amp;quot;Mary&amp;quot; in the store. We get (91) s(some(x,pizza(x),ate(V1,x)), [P(Wffl,unique(V1,name(V1,mary),Wffl )) QL]-QL, Fx-Fx, L) [Mary ate a pizza]. VI is the bound variable of the NP &amp;quot;Mary.&amp;quot; The predicate &amp;quot;apply_quants&amp;quot; will choose a value for VI when it applies the quantifier. From (R13), (86), and (91), we get vp(V2, thought(V2,Varsl,q(some(x,pizza(x),ate(V1, x))))), [p(Wffl,unique(V1,name(V1,mary),Wff QL]-QL, Fx-Fx, L) [thought Mary ate a pizza], free_vars(some(x,pizza(x),ate(V1,x)),Vars1) 1. In this case, the first argument of &amp;quot;free_vars&amp;quot; contains the meta-language variable Vi. Then the &amp;quot;free_vars&amp;quot; subgoal has an infinity of solutions—one in which VI = x and there are no free variables, and an infinite number in which V1 — y, for some y not equal to x, and the list of free variables is [y]. Therefore, it is necessary to postpone the &amp;quot;free_vars&amp;quot; subgoal once more. The standard technique for parsing DCGs does not allow for this postponing of subgoals, and this will create a problem for our implementation. This problem would be greatly simplified if we had chosen to assign a different variable to every quantifier by using a global counter. The DCG parser would work from left to right and assign a target-language variable to each NP as soon as it parsed that NP. In the above example, &amp;quot;Mary&amp;quot; and &amp;quot;a pizza&amp;quot; would both have their variables assigned by the time we reached the right end of the VP. Then we could handle the &amp;quot;free_vars&amp;quot; subgoals by rewriting the grammar as follows: remove the &amp;quot;free_vars&amp;quot; subgoals from the lexical entries for the attitude verbs, and place a &amp;quot;free_vars&amp;quot; subgoal at the right end of each VP rule that introduces an attitude verb ((R 13), (R15), and (R8)). This would ensure that when the parser attempted to solve the &amp;quot;free_vars&amp;quot; subgoal, its first argument would be a ground term. However, this solution would make it impossible to use the rule (R6) for NP conjunction (see Section 2.5). If we pick one solution for the problem of choosing bound variables, we have problems with NP conjunction; if we pick the other solution we get problems in implementing analysis of re reports. This is the kind of difficulty that we cannot even notice, let alone solve, until we write formal grammars that cover a reasonable variety of phenomena. Continuing our derivation, we combine the VP with the subject &amp;quot;John,&amp;quot; apply the quantifier from &amp;quot;Mary,&amp;quot; and get (93) s(unique(z,name(z,john), unique(y,name(y,mary), thought(z,Varsl,q(some(x,pizza (x),ate (M)))))), QL-QL,Fx-Fx,L) [John thought Mary ate a pizza], 1 free_vars(some(x,pizza(x),ate(y,x)),Vars1) 1. Now the first argument of &amp;quot;free_vars&amp;quot; is a ground term, because applying the quantifier that arose from &amp;quot;Mary&amp;quot; includes choosing the target language variable that the quantifier binds. The &amp;quot;free_vars&amp;quot; subgoal now has only one solution, Vars 1 = [y]. Then the logical form of the sentence is (94) unique(z,name(z,john), unique(y,name(y,mary), thought(z,[y],q(some(x,pizza(x), ate(y,x)))))). This means that there is a term T that represents Mary to John, and John believes the sentence (95) some(x,pizza(x),ate(T,x)). For the sentence &amp;quot;John thought he saw Mary,&amp;quot; our limited treatment of pronouns allows only one reading, in which &amp;quot;he&amp;quot; refers to John. Using (R9), we get the following reading for the embedded clause: s(let(y,V1,unique(x,name(x,mary),saw(y,x)),QL- QL,Fx-Fx, [ V1]) [he saw Mary]. The pronoun &amp;quot;he&amp;quot; gives rise to a &amp;quot;let&amp;quot; quantifier, which binds the variable y to VI, the first member of the pronoun reference list. From (R13), (86), and (96) we get vp(V2,thought(V2,Vars1,q(let(y,V2,unique(x,name (x,mary), saw(y,x)), QL-QL,Fx-Fx,[]) [thought he saw Mary], {free_vars(let(y,V2,unique(x,name(x,mary),saw (y,x))),Vars1) 1. The VP rule (R13) unifies the subject variable V2 with the first element of the pronoun reference list of the embedded clause, so the &amp;quot;let&amp;quot; quantifier now binds the variable y to the subject variable. Once again, we postpone the &amp;quot;free_vars&amp;quot; 228 Computational Linguistics Volume 16, Number 4, December 1990 Andrew R. Haas Sentential Semantics for Propositional Attitudes goal until its first argument is a ground term. Combining this VP with the subject &amp;quot;John&amp;quot; gives s(unique(z,name(z,john), thought(z,Varsl,q(let(y,z,unique(x,name(x,mary), saw(y,x)))))), QL-QL,Fx-Fx,VL) —. [John thought he saw Mary], 1free_vars(let(y,z,unique(x,name(x,mary),saw(y,x))), Varsl) 1. The first argument of &amp;quot;free_vars&amp;quot; is now a ground term, and solving the &amp;quot;free_vars&amp;quot; subgoal gives Varsl = [z]. The logical form of the sentence is unique(z,name(z,john), thought(z,[z],q(let(y,z, unique(x,name(x,mary), saw(y,x)))))). The dummy variable z stands for a term T that represents John to himself. Then John&apos;s belief looks like this: (100) let(y,T, unique(x,name(x,mary), saw(y,x))). If John simplifies this belief, he will infer (101) unique(x,name(x,mary),saw(T,x)). 3.2 ATTITUDE VERBS TAKING A CLAUSE WITH A GAP We proposed the following logical form for &amp;quot;John knows who Mary likes&amp;quot;: (102) some(x,person(x),know(john,[x],q(like(mary,x)))). The grammar will generate a similar logical form, except for the translations of the proper nouns. The existential quantifier comes from the word &amp;quot;who.&amp;quot; The rules for &amp;quot;who&amp;quot; and &amp;quot;what&amp;quot; are (R14a) wh(V1,[p(Wffl,some(V1,and(person(V1), Wff1)))I QL]-QL) ---. [who] (R14b) wh(V1,[p(Wffl,some(V1,and(thing(V1), Wffl))) I QL]-QL) --. [what]. The semantic features of a wh word are a variable, and a list containing a quantifier that binds that variable. The following rule builds VPs in which the verb takes a wh word and a clause as its objects: (R15) vp(V1,Wff3,QL1,Fx-Fx,L) --. v(takes_wh(V1,Wff 1 ,Wff2)), wh(V,QL0), s(Wffl,QL1,[gap(V)]-[],[V1 I Li), 1 apply_quants(QLO,Wff2,QL-QL,Wff3)}. The embedded S contains a gap, and the variable of that gap is the one bound by the quantifier from the wh word. The main verb takes the subject variable and the logical form of the embedded S and builds a wff Wff2. The rule finally calls &amp;quot;apply_quants&amp;quot; to apply the quantifier from the wh word to Wff2. &amp;quot;Apply_quants&amp;quot; can apply any subset of the quantifiers in its first argument, but the rule requires the output list of quantifiers to be empty, and this guarantees that the quantifier from the wh word will actually be applied. The resulting wff becomes the logical form of the VP. The rule requires a verb whose subcategorization frame has the form takes_wh(V1,Wffl,Wff2). &amp;quot;Know&amp;quot; is such a verb: (L5) has_subcat(knows, takes_wh(VI,Wffl,know(VI,Varsl,q(Wff1)))) :free_vars(Wffl,Vars1). Combining this clause with the rule (R7) for verbs gives (103) v(takes_wh(V1,Wffl,know(V1,Varsl,q(Wff1)))) —÷ [knows], 1 free_vars(Wffl,Vars1) 1. Consider the example &amp;quot;John knows who Mary likes,&amp;quot; and &amp;quot;Mary&amp;quot; is understood dicto. embedded S has the following reading: (104) s(unique(x,name(x,mary),like(x,V1)),QL-QL, [gap(V1) IFx]-Fx,L) —÷ [Mary likes]. The object of &amp;quot;likes&amp;quot; is a gap, so the variable V1 from the filler list becomes the second argument of the predicate &amp;quot;like.&amp;quot; Resolving (103), (R14a), and (104) against the right side of (R15) gives vp(V2,Wff3,QL-QL,Fx-Fx,L) —■ [knows who Mary likes],</abstract>
<note confidence="0.657090888888889">1 apply_quants([p(Wffl,some(VI,person(V1), Wffl)) I QL]-QL, know(V2,Varsl,q(unique(x,name(x,mary), like(x,V1))), QL-QL, Wff3), 1free_vars(unique(x,name(x,mary),like(x,V1)), Varsl) 1.</note>
<title confidence="0.739675">Solving the &amp;quot;apply_quants&amp;quot; subgoal gives</title>
<abstract confidence="0.949436493288591">Wff3 = some(y,person(y), know(V2,Varsl,q(unique(x,name(x,mary), like (x,y))))). Solving the &amp;quot;free_vars&amp;quot; subgoal gives Varsl = [y], and we Computational Linguistics Volume 16, Number 4, December 1990 229 Andrew R. Haas Sentential Semantics for Propositional Attitudes then have Wff3 = some(y,person(y), know(V2,[y],q(unique(x,name(x,mary), like(x,y))))). Therefore vp(V2, some (y,person(y), know(V2,[y],q(unique(x,name(x,mary),like (x,Y))))), QL- QL,Fx-Fx,L) [knows who Mary likes]. This VP combines with the subject &amp;quot;John&amp;quot; in the usual way to give a sentence whose logical form is unique(z,name(z,john), some(y,person(y), know(z,[y],q(unique(x,name(x,mary), like(x,y)))))). 3.3 ATTITUDE VERBS TAKING A NOUN PHRASE Finally, we consider an example with &amp;quot;want.&amp;quot; This verb is semantically very different from most transitive verbs, but syntactically it is an ordinary transitive verb, introduced by the rule already given: (R8) vp(VI,Wffl,QL2,Fx-Fy,L) v(trans(V1,V2,QL1,QL2,Wff1)), np(V2,QL1,Fx-Fy,L). The difference between &amp;quot;want&amp;quot; and other transitive verbs is in its subcategorization frame: (L6) has_subcat(wants, trans(V1,V2,QL1,QL2,wish(V1,Varsl,q apply_quants(QL1,have(V1,V2),QL2,Wff1), free_vars(Wffl,Vars1). Resolving this rule against the verb rule (R7) gives the following rule for the verb &amp;quot;wants&amp;quot;: v(trans(V1,V2,QL1,QL2,wish(V1,Varsl,q(Wff1)))) [wants], 1 apply_quants(QL1,have(V1,V2),QL2,Wff1), free_vars(Wffl,Vars1) 1 The quantifier list QL1 contains the quantifier from the object NP. The predicate &amp;quot;apply_quants&amp;quot; may or may not apply this quantifier to the wff have(V1,V2), and this gives rise to a re/de dicto If &amp;quot;apply_quants&amp;quot; does not apply the object quantifier, then QL2 = QL1, so the object quantifier is passed up for later application. Otherwise, QL2 is the empty list. As usual, the &amp;quot;free_vars&amp;quot; ensures that the re obey our convention. Consider the VP &amp;quot;wants a Porsche.&amp;quot; The object &amp;quot;a Porsche&amp;quot; has the following interpretation: np(V2,[p,(Wff0,some(V2,porsche(V2),Wff0))I QL1- QL,Fx-Fx,VL) [a porsche]. Resolving (110) and (111) against the left side of (R7) gives vp(V1,wish(V1,Varsl,q(Wff1)),QL2,Fx-Fx,VL) [wants a porsche], {apply_quants([p(Wff0,some(V2,porsche(V2), Wff0))1 QL]-QL, have(V1,V2), QL2, Wffl), free_vars(Wffl,Vars1) 1. One solution of the &amp;quot;apply_quants&amp;quot; subgoal is Wffl = some(x,porsche(x),have(V1,x)) QL2 = QLO-QLO. Given this solution, the logical form of the VP is (114) wish(V1,Varsl,q(some(x,porsche(x),have(V1,x)))) where V1 is the subject variable and the &amp;quot;free_vars&amp;quot; subgoal has been postponed. We can combine this VP with the subject &amp;quot;John&amp;quot; to get a sentence whose logical form is unique(y,name(y,john), wish(y,Varsl,q(some(x,porsche(x),have (M))))). Solving the &amp;quot;free_vars&amp;quot; subgoal will then give Varsl = [y], so the final logical form is unique(y,name(y,john), wish(y,[y],q(some(x,porsche(x),have(y,x))))). This means that there is a term T that represents John to himself, and the sentence that John wishes to be true is (117) some(x,porsche(x),have(T,x)). is a dicto is not any particular Porsche that John wants. The other solution for the &amp;quot;apply_quants&amp;quot; subgoal is Wffl = have(V1,V2) QL2 = [p(Wff0,some(V2,porsche(V2),Wff0))I QL]-QL. 230 Computational Linguistics Volume 16, Number 4, December 1990 Andrew R. Haas Sentential Semantics for Propositional Attitudes In this case, the logical form of the VP is (119) wish (V1,Vars 1 ,q(have(V1,V2))) and its quantifier store is equal to QL2. Combining this VP with the subject &amp;quot;John&amp;quot; and applying the quantifiers gives a sentence whose logical form is unique(y,name(y,john), some(x,porsche(x), wish (y,Varsl,q (have(y,x)))) ). Solving the &amp;quot;free_vars&amp;quot; subgoal gives Varsl = [y,x] so the final logical form is unique(y,name(y,john), some(x,porsche(x), wish (y,[y,x],q(have(y,x))))). This means that there exist terms T1 and T2 such that Ti represents John to himself, T2 represents some Porsche to John, and the sentence John wishes to be true is (122) have(T 1 ,T2) is a re in which John wants some particular Porsche. The rules for verbs that take clauses as complements did not need to call &amp;quot;apply_quants,&amp;quot; because the rules that build the clauses will call &amp;quot;apply_quants&amp;quot; and so create the desired ambiguity. In Cooper&apos;s grammar, all NPs have the option of applying their quantifiers, and so there is no need for verbs like &amp;quot;want&amp;quot; to apply quantifiers—they can rely on the rule that built the verb&apos;s object, just as other intensional verbs do. This is a minor advantage of Cooper&apos;s grammar. standard parser becomes efficient. In particular, the rule top-level clauses calls a Prolog predicate that finds all lists in the final logical form and calls &amp;quot;free_vars&amp;quot; for each one. There is a similar problem about the predicate &amp;quot;apply_quants&amp;quot; in the rule for &amp;quot;want.&amp;quot; Since the parser works left to right, the quantifier from the object of &amp;quot;want&amp;quot; is not available when the logical form for the verb is being constructed. This means that the first argument of &amp;quot;apply_quants&amp;quot; is a free variable—so it has an infinite number of solutions. Here the implementation takes advantage of Prolog&apos;s &amp;quot;call&amp;quot; predicate, which allows us to delay the solution of a subgoal. The &amp;quot;apply_quants&amp;quot; subgoal is an extra feature of the verb &amp;quot;want&amp;quot; (in the case of an ordinary transitive verb, this feature is set to the empty list of goals). The rule for VPs with transitive verbs uses the &amp;quot;call&amp;quot; predicate to solve the subgoal—after the object of the verb has been parsed. At this point the first argument is properly instantiated and the call produces a finite set of solutions. The grammar given above contains the rule NP NP [and] NP, which is left recursive and cannot be parsed by the standard DCG parser. The implementation avoids this problem by adding a flag that indicates whether an NP is conjunctive. This gives the rule (123) NP( conj) NP( —conj) [and] NP(Conj), which is not left recursive—it assigns a right-branching structure to all conjunctions of NPs. These are the only differences between the grammar presented here and the Prolog code. The implementation was easy to write and modify, and it supports the claim that Prolog allows us to turn formal definitions into running programs with a minimum of effort. 4.2 CONCLUSIONS AND FUTURE WORK AND 4.1 IMPLEMENTATION The implementation uses the standard Prolog facility for parsing definite clause grammars. This facility translates the grammar into a top-down, left-to-right parser. This order of parsing leads to problems with the predicates &amp;quot;apply_quants&amp;quot; and &amp;quot;free_vars.&amp;quot; We cannot run &amp;quot;free_vars&amp;quot; until its first argument is a ground term—otherwise we might get an infinite number of solutions. In our exposition, we solved this problem by delaying the execution of &amp;quot;free_vars.&amp;quot; The standard DCG parser has no built-in facility for such delaying. As usual in such situations, there are two options: rewrite the predicates so that the existing interpreter works efficiently, or define a more general interpreter that allows the desired order of execution. The second approach is more desirable in the long run, because it achieves a central goal of logic programming: to use logical sentences that express our understanding of the problem in the clearest way. However, defining new interpreters is hard. The present implementation takes the low road—that is, the author rewrote the predicates so that the This paper has presented a new notation for a sentential theory of attitudes, which unlike most existing notations makes it possible to give a compositional semantics for reports. Our notation distinguishes between the of an attitude operator and the dummy variables, which stand for unspecified terms that represent values of the re The choice of dummy variables is quite arbitrary—just as the choice of bound variables in first-order logic is arbitrary. This allows us to impose a convention, which says that in fact the dummy are equal to the re Given this convention, the logical form of a clause is the same whether it stands alone or appears as the argument of an attitude verb. This is a simple proposal, and it would be easy to write and implement a grammar that applies the proposal to a few examples. The real question is whether the proposal is robust—whether it can function in a grammar that covers a variety of phenomena. We chose definite clauses and a first-order object language as our semantic formalism. We a nonobvious interaction between our proposal for reports, and two other problems about quantifi- Computational Linguistics Volume 16, Number 4, December 1990 231 Andrew R. Haas Sentential Semantics for Propositional Attitudes cation: the choice of bound variables in a logical form, and the conjunction and disjunction of quantified NPs. We considered two possibilities for choosing the bound variables: assigning a different variable to every NP using a global counter, or requiring each quantifier to bind a variable that is not bound by any quantifier within its scope. The first approach makes it impossible to use our rules for NP conjunction and disjunction, while the second creates problems for the re lists. We resolved the dilemma by picking the second approach, and then rewriting the grammar to solve the implementation Thus we have shown that the proposal for re attitude reports is not just a plausible notion—it can be made to work in a grammar that is not trivial. The grammar handles three kinds of attitude constructions: an attitude verb taking a clause as its object (&amp;quot;John thought he saw Mary&amp;quot;), an attitude verb taking a clause with a gap (&amp;quot;John knows who Mary likes&amp;quot;), and an attitude verb taking a noun phrase as its object (&amp;quot;John wants a The grammar includes re/de dicto ambiguities, conjunction of NPs, and a very limited treatment of pronouns. Another contribution of our work is that it seems to be the first unification grammar that builds logical forms, and at the same time respects the declarative semantics of the notation. We explicitly choose the bound variables of the logical form, instead of using meta-language variables. We also explain the semantics of our representation for quantified NPs: each NP has an infinite set of readings, one for each ordered pair in the extension of the application function. Several authors treat unification grammars with semantics as poor relations of Montague grammar. Pereira and Shieber (1987) propose to &amp;quot;encode&amp;quot; Montague&apos;s ideas in unification grammar, while Moore (1989) fears that building logical forms with unification grammar is &amp;quot;unprincipled feature hacking.&amp;quot; We claim that these problems arise not from shortcomings of unification grammar, but from failure to take unification grammar seriously—which means respecting its declarative semantics. The most obvious line for future work is to extend the grammar. It would be fairly easy to include complex wh noun phrases, such as &amp;quot;whose cat&amp;quot; or &amp;quot;how many children&amp;quot; (Cooper&apos;s grammar handled these). A more difficult problem arises when a gap is the object of an intensional verb—as in &amp;quot;John knows what Mary wants.&amp;quot; The grammar generate this sentence, but it assigns only a re reading: unique(x,name(x,john), some(y,thing(y), know(x,[y],q(unique(z,name(z,mary), wish(z,[z,y],q(have(z,y)))))))). This is the only reading because the gap has an empty quantifier store—there is no quantifier available to be applied to the wff &amp;quot;have(z,y).&amp;quot; Yet there are examples in such sentences do have dicto For example, consider &amp;quot;What John wants is a Porsche.&amp;quot; Surely this has a dicto the object of &amp;quot;want&amp;quot; is a gap, not a quantified NP. Cooper discusses this problem, but his grammars could not handle it, and neither can ours. Hirst and Fawcett (1986) have argued that the ambiguities in attitude reports are more complex than the familiar between re dicto They claim that the sentence &amp;quot;Nadia wants a dog like Ross&apos;s&amp;quot; has a reading in which Nadia doesn&apos;t want a particular dog (so the quantifier 3 is inside the scope of the attitude operator), but the description &amp;quot;dog like Ross&apos;s&amp;quot; is the speaker&apos;s, not Nadia&apos;s (so the description is outside the scope of the attitude operator). This reading is certainly different from usual re dicto in which either the whole logical form of the NP is under the attitude, or none of it is. To represent it, we must be able to separate the logical form of the NP &amp;quot;a dog like Ross&apos;s&amp;quot; into two parts (the quantifier 3 and its range restriction), and we must be able to move the range restriction out of the scope of the attitude without moving the quantifier. This will mean that we cannot use the same mechanism for both quantifier scope ambiguities and the ambiguities that arise from attitudes. These extensions appear feasible, but they amount to a major change in the formalism. Another possibility for future work is to incorporate the exist: ng implementation into a question-answering program. This requires finding a way to reason about propositional attitudes efficiently without assuming unique standard designators—which means a substantial generalization of the work of Konolige. It also requires us to make much stronger claims about the properties of &apos;representation&apos; than we have made in this paper. If these problems can be solved, it should be possible to build a natural language question-answering program that can describe the extent of its own knowledge—answering questions like &amp;quot;Do you know the salary of every employee?&amp;quot; REFERENCES Asher, Nicholas M. and Kamp, John A. W. (1986). &amp;quot;The knower&apos;s and representational theories of attitudes.&amp;quot; In of Reasoning about Knowledge, by Joseph Halpern, 1.31-147. Morgan Kaufmann.</abstract>
<note confidence="0.918559416666667">Barwise, Jon and Cooper, Robin. (1981). &amp;quot;Generalized quantifiers and language.&amp;quot; and Philosophy, Steven E. and Lycan, William G. (1986). Who. Press. Robin. (1983). and Syntactic Theory. Reidel. Robin. (1979). &amp;quot;Variable binding and relative clauses.&amp;quot; Forand Pragmatics for Natural Languages, by F. Gventhner and S. J. Schmidt, 131-169. D. Reidel. des Rivieres, J., and Levesque, H. 1986. &amp;quot;The Consistency of Syntactical of Knowledge.&amp;quot; in Joseph Y. Halpern, ed., of Reasoning about Knowledge. Altos, CA, Morgan Kaufmann: 115-130.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas M Asher</author>
<author>John A W Kamp</author>
</authors>
<title>The knower&apos;s paradox and representational theories of attitudes.&amp;quot;</title>
<date>1986</date>
<booktitle>In Theoretical Aspects of Reasoning about Knowledge, edited by Joseph Halpern,</booktitle>
<pages>1--31</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="38027" citStr="Asher and Kamp (1986)" startWordPosition="6334" endWordPosition="6337">r Priest&apos;s. However, it still excludes some inferences that are standard in everyday reasoning. For example, we have true(q(P)) -* P for every P, but P -* true(q(P)) is not a theorem for certain sentences P—in particular, sentences that are self-referential and paradoxical. An adequate account of self-reference must deal not only with the Liar, but also with paradoxes arising from propositional attitudes—for example, the Knower Paradox (Montague and Kaplan 1974), and Thomason&apos;s paradox about belief (Thomason 1980). Perlis (1988) has considered the treatment of attitudes within his system, and Asher and Kamp (1986) have treated both paradoxes using ideas akin to Kripke&apos;s (their treatment is not sentential, but they claim that it could be extended to a sentential treatment). Let us briefly consider the treatment of the Knower paradox within Perlis&apos;s system. To simplify the treatment, we will assume that knowledge is true belief. If we are working in Perlis&apos;s system, this naturally means that knowledge is Kripke-true belief. We write &amp;quot;the agent knows that P&amp;quot; as true(q(P)) A believe(q(P)). The paradox arises from a sentence R that says &amp;quot;The agent knows -R.&amp;quot; Formally, (33) R 4-* (true(q(-R)) A believe(q(-R)</context>
</contexts>
<marker>Asher, Kamp, 1986</marker>
<rawString>Asher, Nicholas M. and Kamp, John A. W. (1986). &amp;quot;The knower&apos;s paradox and representational theories of attitudes.&amp;quot; In Theoretical Aspects of Reasoning about Knowledge, edited by Joseph Halpern, 1.31-147. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Barwise</author>
<author>Robin Cooper</author>
</authors>
<title>Generalized quantifiers and natural language.&amp;quot;</title>
<date>1981</date>
<journal>Linguistics and Philosophy,</journal>
<volume>4</volume>
<pages>159--219</pages>
<contexts>
<context position="10451" citStr="Barwise and Cooper (1981)" startWordPosition="1715" endWordPosition="1718">Cooper&apos;s notion of quantifier storage (Cooper 1983), we assume that the representation of the embedded clause has two parts: the wff &amp;quot;love(y,mary)&amp;quot; and an existential quantifier that binds the free variable y. Informally, we can write the quantifier as &amp;quot;some(y,man(y) &amp; S),&amp;quot; where S stands for the scope of the quantifier. Applying this quantifier to the wff &amp;quot;love(y,mary)&amp;quot; gives the sentence &amp;quot;some (y,man(y) &amp; love(y,mary)).&amp;quot; In the present paper, the term &amp;quot;quantifier&amp;quot; will usually refer to this kind of object—not to the symbols V and 3 of first-order logic, nor to the generalized quantifiers of Barwise and Cooper (1981). 214 Computational Linguistics Volume 16, Number 4, December 1990 Andrew R. Haas Sentential Semantics for Propositional Attitudes In Section 2.2 we present a more precise formulation of our representation of quantifiers. When the clause &amp;quot;some man loves Mary&amp;quot; forms an utterance by itself, the semantics will apply the quantifier to the wff &amp;quot;love(y,mary)&amp;quot; to get the sentence &amp;quot;some (y,man(y) &amp; love(y,mary)).&amp;quot; The problem is that the wff &amp;quot;love(y,mary)&amp;quot; does not appear in Kaplan&apos;s representation. In its place is the expression &amp;quot;love(a,mary),&amp;quot; containing a variable that ranges over names, not men. I</context>
</contexts>
<marker>Barwise, Cooper, 1981</marker>
<rawString>Barwise, Jon and Cooper, Robin. (1981). &amp;quot;Generalized quantifiers and natural language.&amp;quot; Linguistics and Philosophy, 4: 159-219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven E Boer</author>
<author>William G Lycan</author>
</authors>
<title>Knowing Who.</title>
<date>1986</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7998" citStr="Boer and Lycan (1986)" startWordPosition="1317" endWordPosition="1320">ation is a relation between an agent, a name, and the entity that the name denotes. If an agent has an attitude toward a thought-language sentence, and that sentence contains a name that represents a certain entity to the agent, then the agent has a de re attitude about that entity. Our grammar will build logical forms that are compatible with any sentential theory that includes these assumptions. One problem about the nature of representation should be mentioned. This concerns the so-called de se attitude reports. This term is attributable to Lewis (1979), but the clearest definition is from Boer and Lycan (1986). De se attitudes are &amp;quot;attitudes whose content would be formulated by the subject using the equivalent in his or her language of the first-person singular pronoun &apos;I&apos;&amp;quot; (B6er and Lycan 1986). If John thinks that he is wise, and we understand this as a de se attitude, what name represents John to himself? One possibility is that it is his selfname. An agent&apos;s selfname is a thought-language constant that he standardly uses to denote himself. It was postulated in Haas (1986) in order to solve certain problems about planning to acquire information. To expound and defend this idea would take us far </context>
<context position="25391" citStr="Boer and Lycan (1986)" startWordPosition="4276" endWordPosition="4279">lows that the night watchman&apos;s mental description of the burglar must represent the burglar to the watchman (by our assumption about representation). Yet the night watchman surely would not claim that he knows who is sneaking around the building. It seems that even though the watchman&apos;s mental description represents the burglar, it is not strong enough to support the claim that he knows who the burglar is. It would be easy to extend our notation to allow for a difference between &amp;quot;knowing who&amp;quot; and other cases of quantification into attitudes. It would be much harder to analyze this difference, Boer and Lycan (1986) have argued that when we say someone knows who N is, we always mean that someone knows who N is for some purpose. This purpose is not explicitly mentioned, so it must be understood from the context of the utterance in which the verb &amp;quot;know&amp;quot; appears. Then the predicate that represents &amp;quot;knowing who&amp;quot; must have an extra argument whose value is somehow supplied by context. These ideas look promising, but to represent this use of context in a grammar is a hard problem, and outside the scope of this work. Next we consider intensional transitive verbs like &amp;quot;want,&amp;quot; as in &amp;quot;John wants a Porsche.&amp;quot; The int</context>
</contexts>
<marker>Boer, Lycan, 1986</marker>
<rawString>Boer, Steven E. and Lycan, William G. (1986). Knowing Who. MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Robin Cooper</author>
</authors>
<title>Variable binding and relative clauses.&amp;quot;</title>
<date>1983</date>
<journal>Quantification</journal>
<booktitle>In Formal Semantics and Pragmatics for Natural Languages,</booktitle>
<pages>115--130</pages>
<editor>edited by F. Gventhner and S. J. Schmidt, 131-169. D. Reidel. des Rivieres, J., and</editor>
<publisher>Morgan Kaufmann:</publisher>
<location>Los Altos, CA,</location>
<contexts>
<context position="9877" citStr="Cooper 1983" startWordPosition="1624" endWordPosition="1625">randi are Quine&apos;s quasi-quotes (Quine 1947). If a denotes a name t, then the expression &amp;quot;rlove(a,mary)1&amp;quot; will denote the sentence &amp;quot;love(t,mary).&amp;quot; It is hard to see how a compositional semantics can build this representation from the English sentence &amp;quot;John believes some man loves Mary.&amp;quot; The difficult part is building the representation for the VP &amp;quot;believes some man loves Mary.&amp;quot; By definition, a compositional semantics must build the representation from the representations of the constituents of the VP: the verb &amp;quot;believe&amp;quot; and the embedded clause. Following Cooper&apos;s notion of quantifier storage (Cooper 1983), we assume that the representation of the embedded clause has two parts: the wff &amp;quot;love(y,mary)&amp;quot; and an existential quantifier that binds the free variable y. Informally, we can write the quantifier as &amp;quot;some(y,man(y) &amp; S),&amp;quot; where S stands for the scope of the quantifier. Applying this quantifier to the wff &amp;quot;love(y,mary)&amp;quot; gives the sentence &amp;quot;some (y,man(y) &amp; love(y,mary)).&amp;quot; In the present paper, the term &amp;quot;quantifier&amp;quot; will usually refer to this kind of object—not to the symbols V and 3 of first-order logic, nor to the generalized quantifiers of Barwise and Cooper (1981). 214 Computational Lingui</context>
<context position="48942" citStr="Cooper (1983)" startWordPosition="8182" endWordPosition="8183">clear. Typed variables have mnemonic value even if we do not use a typed logic. Therefore we adopt the following conventions. The meta-language variables V, VO, V1 . . . range over target language variables. Wff, Wffl, Wff2 . . . range over target language wffs. Q, Qi, Q2 . . . range over quantifiers. QL, QL1, QL2 . . . range over lists of quantifiers. When a wff forms the range restriction of a quantifier, we will sometimes use the variables Range, Rangel . . . for that wff. 2.3 SCOPING AND QUANTIFIER STORAGE Given a means of describing quantifiers, we must consider the order of application. Cooper (1983) has shown how to allow for different orders of application by adding to NPs, VPs, and sentences an extra semantic feature called the quantifier store. The store is a list of quantifiers that bind the free variables in the logical form of the phrase. The grammar removes quantifiers from the store and applies them nondeterministically to produce different logical forms, corresponding to different orders of application. If a sentence has a logical form p and a quantifier store /, then every free variable in p must be bound by a quantifier in 1—otherwise the final logical form would contain free </context>
</contexts>
<marker>Cooper, 1983</marker>
<rawString>Cooper, Robin. (1983). Quantification and Syntactic Theory. D. Reidel. Cooper, Robin. (1979). &amp;quot;Variable binding and relative clauses.&amp;quot; In Formal Semantics and Pragmatics for Natural Languages, edited by F. Gventhner and S. J. Schmidt, 131-169. D. Reidel. des Rivieres, J., and Levesque, H. 1986. &amp;quot;The Consistency of Syntactical Treatments of Knowledge.&amp;quot; in Joseph Y. Halpern, ed., Theoretical Aspects of Reasoning about Knowledge. Los Altos, CA, Morgan Kaufmann: 115-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>End erton</author>
<author>Herbert</author>
</authors>
<title>A Mathematical Introduction to Logic.</title>
<date>1972</date>
<publisher>Academic Press.</publisher>
<marker>erton, Herbert, 1972</marker>
<rawString>End erton, Herbert. (1972). A Mathematical Introduction to Logic. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony J Fisher</author>
</authors>
<title>Practical parsing of GPSG&apos;s.&amp;quot;</title>
<date>1989</date>
<journal>Computational Linguistics,</journal>
<volume>15</volume>
<pages>139--148</pages>
<contexts>
<context position="42373" citStr="Fisher 1989" startWordPosition="7066" endWordPosition="7067">ts advantages are that it is well defined and easy to learn, because it is a notational variant of standard first-order logic. Also, it is often straightforward to parse with grammars written in this notation (although there can be no general parsing method for the notation, since it has Turing machine power). DCG notation lacks some useful devices found in linguistic formalisms like GPSG—there are no default feature values or general feature agreement principles (Gazdar et al. 1985). On the other hand, the declarative semantics of the DCG notation is quite clear—unlike the semantics of GPSG (Fisher 1989). The grammar is a set of meta-language sentences describing a correspondence between English words and sentences of the target language. Therefore, we must define a notation for talking about the target language in the metalanguage. Our choice is a notation similar to that of Haas (1986). If f is a symbol of the target language, &apos;f is a symbol of the meta-language. Suppose f is a constant or a variable, taking no arguments. Then &apos;f denotes f. Thus &apos;john is a meta-language constant that denotes a target-language constant, while &apos;x is a meta-language constant that denotes a target-language vari</context>
</contexts>
<marker>Fisher, 1989</marker>
<rawString>Fisher, Anthony J. (1989). &amp;quot;Practical parsing of GPSG&apos;s.&amp;quot; Computational Linguistics, 15: 139-148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ga2 dar</author>
<author>Klein Gerald</author>
<author>Pullum Ewan</author>
<author>Geoffrey</author>
<author>Ivan Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<journal>Computational Linguistics</journal>
<volume>16</volume>
<pages>232</pages>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="42249" citStr="dar et al. 1985" startWordPosition="7044" endWordPosition="7047">ow standard among computer scientists who study natural language and is explained in a textbook by Pereira and Shieber (1987). Its advantages are that it is well defined and easy to learn, because it is a notational variant of standard first-order logic. Also, it is often straightforward to parse with grammars written in this notation (although there can be no general parsing method for the notation, since it has Turing machine power). DCG notation lacks some useful devices found in linguistic formalisms like GPSG—there are no default feature values or general feature agreement principles (Gazdar et al. 1985). On the other hand, the declarative semantics of the DCG notation is quite clear—unlike the semantics of GPSG (Fisher 1989). The grammar is a set of meta-language sentences describing a correspondence between English words and sentences of the target language. Therefore, we must define a notation for talking about the target language in the metalanguage. Our choice is a notation similar to that of Haas (1986). If f is a symbol of the target language, &apos;f is a symbol of the meta-language. Suppose f is a constant or a variable, taking no arguments. Then &apos;f denotes f. Thus &apos;john is a meta-languag</context>
</contexts>
<marker>dar, Gerald, Ewan, Geoffrey, Sag, 1985</marker>
<rawString>Ga2:dar, Gerald; Klein, Ewan; Pullum, Geoffrey; and Sag, Ivan. (1985). Generalized Phrase Structure Grammar. Harvard University Press. 232 Computational Linguistics Volume 16, Number 4, December 1990 Andrew R. Haas Sentential Semantics for Propositional Attitudes</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Haas</author>
</authors>
<title>A syntactic theory of belief and action.&amp;quot;</title>
<date>1986</date>
<journal>Artificial Intelligence,</journal>
<volume>28</volume>
<pages>245--292</pages>
<contexts>
<context position="1677" citStr="Haas (1986)" startWordPosition="261" endWordPosition="262">from them, so they have two essential properties of beliefs. It is tempting to conclude that they are the program&apos;s beliefs, and that human beliefs are also sentences of a thought language. If we extend this to all propositional attitudes, we have a sentential theory of propositional attitudes. In such a theory, an English sentence expresses a proposition, and this proposition is itself a sentence— although in a different language. Other theories of attitudes hold that a proposition is a set of possible worlds, or a situation—something very different from a sentence. Moore and Hendrix (1979), Haas (1986), Perlis (1988), and Konolige (1986) have argued for sentential theories and applied them to artificial intelligence. Kaplan (1975) proposed an analysis of quantification into the scope of attitudes within a sentential theory, and other authors using sentential theories have offered variations of his idea (Haas 1986; Konolige 1986). Most of these theories present serious difficulties for formal semantics. The problem is that they assign two very different logical forms to a clause: one form when the clause is the object of an attitude verb, and another when it stands alone. This means that the</context>
<context position="7127" citStr="Haas 1986" startWordPosition="1178" endWordPosition="1179">fficiently vivid; and, finally, there is a causal connection between the entity and the agent&apos;s use of the name. A name N is vivid to an agent if that agent has a collection of beliefs that mention N and give a good deal of relevant information about the denotation of N. What is relevant may depend on the agent&apos;s interests. Other authors have accepted the idea of a distinguished subset of names while offering different proposals about how these names are distinguished. I have argued that the distinguished names must provide information that the agent needs to achieve his or her current goals (Haas 1986). Konolige (1986) proposed that for each agent and each entity, the set of distinguished names has exactly one member. In this paper, we adopt Kaplan&apos;s term &amp;quot;represent&amp;quot; without necessarily adopting his analysis of the notion. We assume that representation is a relation between an agent, a name, and the entity that the name denotes. If an agent has an attitude toward a thought-language sentence, and that sentence contains a name that represents a certain entity to the agent, then the agent has a de re attitude about that entity. Our grammar will build logical forms that are compatible with any </context>
<context position="8473" citStr="Haas (1986)" startWordPosition="1400" endWordPosition="1401">e so-called de se attitude reports. This term is attributable to Lewis (1979), but the clearest definition is from Boer and Lycan (1986). De se attitudes are &amp;quot;attitudes whose content would be formulated by the subject using the equivalent in his or her language of the first-person singular pronoun &apos;I&apos;&amp;quot; (B6er and Lycan 1986). If John thinks that he is wise, and we understand this as a de se attitude, what name represents John to himself? One possibility is that it is his selfname. An agent&apos;s selfname is a thought-language constant that he standardly uses to denote himself. It was postulated in Haas (1986) in order to solve certain problems about planning to acquire information. To expound and defend this idea would take us far from the problems of compositional semantics that concern us here. We simply mention it as an example of the kinds of theories that are compatible with the logical forms built by our grammar. See also Rapaport (1986) for another Al approach to de se attitudes. 1.2 COMPOSITIONAL SEMANTICS AND LOGICAL FORMS Consider the logical form that Kaplan assigns for the de re reading of &amp;quot;John believes that some man loves Mary.&amp;quot; (1) 3 (y,man(y) &amp; 3 (a,R(a,y,john) &amp; believe(john,rlove</context>
<context position="11269" citStr="Haas 1986" startWordPosition="1845" endWordPosition="1846"> quantifiers. When the clause &amp;quot;some man loves Mary&amp;quot; forms an utterance by itself, the semantics will apply the quantifier to the wff &amp;quot;love(y,mary)&amp;quot; to get the sentence &amp;quot;some (y,man(y) &amp; love(y,mary)).&amp;quot; The problem is that the wff &amp;quot;love(y,mary)&amp;quot; does not appear in Kaplan&apos;s representation. In its place is the expression &amp;quot;love(a,mary),&amp;quot; containing a variable that ranges over names, not men. It might be possible to build this expression from the wff &amp;quot;love(y,mary),&amp;quot; but this sounds like a messy operation at best. Similar problems would arise if we chose another quotation device (such as the one in Haas 1986) or another scoping mechanism (as in Pereira and Shieber 1987). Konolige (1986) proposed a very different notation for quantifying in, one that would abolish the difficulty described here. His proposal depends on an ingenious nonstandard logic. Unfortunately, Konolige&apos;s system has two important limitations. First, he forbids a belief operator to appear in the scope of another belief operator. Thus, he rules out beliefs about beliefs, which are common in everyday life. Second, he assumes that each agent assigns to every known entity a unique &amp;quot;id constant.&amp;quot; When an agent has a belief about an ob</context>
<context position="42662" citStr="Haas (1986)" startWordPosition="7115" endWordPosition="7116">uring machine power). DCG notation lacks some useful devices found in linguistic formalisms like GPSG—there are no default feature values or general feature agreement principles (Gazdar et al. 1985). On the other hand, the declarative semantics of the DCG notation is quite clear—unlike the semantics of GPSG (Fisher 1989). The grammar is a set of meta-language sentences describing a correspondence between English words and sentences of the target language. Therefore, we must define a notation for talking about the target language in the metalanguage. Our choice is a notation similar to that of Haas (1986). If f is a symbol of the target language, &apos;f is a symbol of the meta-language. Suppose f is a constant or a variable, taking no arguments. Then &apos;f denotes f. Thus &apos;john is a meta-language constant that denotes a target-language constant, while &apos;x is a meta-language constant that denotes a target-language variable. Suppose f is a functor of the target language and takes n arguments. Then &apos;f is a metalanguage function letter, and it denotes the function that maps n expressions of the target language el . . . en to the target-language expression f(ei . . . en). Thus &apos;not is a metalanguage functi</context>
</contexts>
<marker>Haas, 1986</marker>
<rawString>Haas, Andrew R. (1986). &amp;quot;A syntactic theory of belief and action.&amp;quot; Artificial Intelligence, 28: 245-292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Stuart M Shieber</author>
</authors>
<title>An algorithm for generating quantifier scopings.&amp;quot;</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<volume>13</volume>
<pages>47--63</pages>
<contexts>
<context position="72891" citStr="Hobbs and Shieber (1987)" startWordPosition="11901" endWordPosition="11904">et (81) s(all(x,woman(x),some(y,man(y),saw(x,y))),QL3- QL3,Fx-Fx,L) [every woman saw a man]. The derivation is not yet complete, because &amp;quot;s&amp;quot; is not the start symbol of our grammar. Instead we use a special symbol &amp;quot;start,&amp;quot; which never appears on the right side of a rule. Thus, the start symbol derives only top-level sentences—it cannot derive an embedded sentence. This is useful because top-level sentences have a unique semantic property: their logical forms must not contain free variables. It might seem that one can eliminate free variables simply by applying all the quantifiers in the store. Hobbs and Shieber (1987) pointed out that this is not so—it is essential to apply the quantifiers in a proper order. Consider the sentence &amp;quot;every man knows a woman who loves him,&amp;quot; with &amp;quot;him&amp;quot; referring to the subject. The subject quantifier binds a variable that occurs free in the range restriction of the object quantifier, so one must apply the object quantifier first in order to eliminate all free variables. Therefore our grammar includes a filter that eliminates readings of top-level sentences containing free variables. Let free_vars(Wff 1 ,L) mean that L is a list of the free variables of Wffl in order of first oc</context>
</contexts>
<marker>Hobbs, Shieber, 1987</marker>
<rawString>Hobbs, Jerry R. and Shieber, Stuart M. (1987). &amp;quot;An algorithm for generating quantifier scopings.&amp;quot; Computational Linguistics, 13: 47-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>Brenda Fawcett</author>
</authors>
<title>The detection and representation of ambiguities of intension and description.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="98620" citStr="Hirst and Fawcett (1986)" startWordPosition="15773" endWordPosition="15776">assigns only a de re reading: unique(x,name(x,john), some(y,thing(y), know(x,[y],q(unique(z,name(z,mary), wish(z,[z,y],q(have(z,y)))))))). This is the only reading because the gap has an empty quantifier store—there is no quantifier available to be applied to the wff &amp;quot;have(z,y).&amp;quot; Yet there are examples in which such sentences do have de dicto readings. For example, consider &amp;quot;What John wants is a Porsche.&amp;quot; Surely this sentence has a de dicto reading—yet the object of &amp;quot;want&amp;quot; is a gap, not a quantified NP. Cooper discusses this problem, but his grammars could not handle it, and neither can ours. Hirst and Fawcett (1986) have argued that the ambiguities in attitude reports are more complex than the familiar distinction between de re and de dicto readings. They claim that the sentence &amp;quot;Nadia wants a dog like Ross&apos;s&amp;quot; has a reading in which Nadia doesn&apos;t want a particular dog (so the quantifier 3 is inside the scope of the attitude operator), but the description &amp;quot;dog like Ross&apos;s&amp;quot; is the speaker&apos;s, not Nadia&apos;s (so the description is outside the scope of the attitude operator). This reading is certainly different from the usual de re and de dicto readings, in which either the whole logical form of the NP is under </context>
</contexts>
<marker>Hirst, Fawcett, 1986</marker>
<rawString>Hirst, Graeme and Fawcett, Brenda. (1986). &amp;quot;The detection and representation of ambiguities of intension and description.&amp;quot; In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, 192-199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kaplan</author>
</authors>
<title>Quantifying in.&amp;quot; In The Logic of Grammar, edited by</title>
<date>1975</date>
<pages>112--144</pages>
<publisher>Pitman.</publisher>
<location>Dickinson. Konolige, Kurt.</location>
<contexts>
<context position="1808" citStr="Kaplan (1975)" startWordPosition="279" endWordPosition="280">that human beliefs are also sentences of a thought language. If we extend this to all propositional attitudes, we have a sentential theory of propositional attitudes. In such a theory, an English sentence expresses a proposition, and this proposition is itself a sentence— although in a different language. Other theories of attitudes hold that a proposition is a set of possible worlds, or a situation—something very different from a sentence. Moore and Hendrix (1979), Haas (1986), Perlis (1988), and Konolige (1986) have argued for sentential theories and applied them to artificial intelligence. Kaplan (1975) proposed an analysis of quantification into the scope of attitudes within a sentential theory, and other authors using sentential theories have offered variations of his idea (Haas 1986; Konolige 1986). Most of these theories present serious difficulties for formal semantics. The problem is that they assign two very different logical forms to a clause: one form when the clause is the object of an attitude verb, and another when it stands alone. This means that the logical form of the clause depends on its context in a complicated way. It is difficult to describe this dependence in a formal gr</context>
<context position="5764" citStr="Kaplan (1975)" startWordPosition="934" endWordPosition="935">a belief about a woman who in fact is Miss America, but it doesn&apos;t imply that John realizes she is Miss America. A sentential theorist might say that the sentence tells us that John has a belief containing some name that denotes Miss America, but it doesn&apos;t tell us what name. The other reading, called de dicto, says that John believes that whoever is Miss America is bald. The de dicto reading, unlike the de re, does not imply that anyone actually is Miss America—it could be true if the Miss America pageant closed down years ago, while John falsely supposes that someone still holds that title. Kaplan (1975) considered examples like these. He said that an agent may use many names that denote the same entity, but there is a subset of those names that represent the entity to the agent (this use of &amp;quot;represent&amp;quot; is different from the common use in AI). If an agent has a de re belief about an entity x, that belief must be a sentence containing, not just any term that denotes x, but a term that represents x to the agent. Thus if &amp;quot;person0&amp;quot; is a name that represents Miss America to John, and the thought language sentence &amp;quot;bald(person0)&amp;quot; is one of John&apos;s beliefs, then the sentence &amp;quot;John thinks Miss America</context>
<context position="9149" citStr="Kaplan 1975" startWordPosition="1512" endWordPosition="1513">formation. To expound and defend this idea would take us far from the problems of compositional semantics that concern us here. We simply mention it as an example of the kinds of theories that are compatible with the logical forms built by our grammar. See also Rapaport (1986) for another Al approach to de se attitudes. 1.2 COMPOSITIONAL SEMANTICS AND LOGICAL FORMS Consider the logical form that Kaplan assigns for the de re reading of &amp;quot;John believes that some man loves Mary.&amp;quot; (1) 3 (y,man(y) &amp; 3 (a,R(a,y,john) &amp; believe(john,rlove(a,mary)1))) The notation is a slight modification of Kaplan&apos;s (Kaplan 1975), The predicate letter R denotes representation. The symbol a is a special variable ranging over names. The symbols randi are Quine&apos;s quasi-quotes (Quine 1947). If a denotes a name t, then the expression &amp;quot;rlove(a,mary)1&amp;quot; will denote the sentence &amp;quot;love(t,mary).&amp;quot; It is hard to see how a compositional semantics can build this representation from the English sentence &amp;quot;John believes some man loves Mary.&amp;quot; The difficult part is building the representation for the VP &amp;quot;believes some man loves Mary.&amp;quot; By definition, a compositional semantics must build the representation from the representations of the c</context>
</contexts>
<marker>Kaplan, 1975</marker>
<rawString>Kaplan, David. (1975). &amp;quot;Quantifying in.&amp;quot; In The Logic of Grammar, edited by Davidson, Donald and Harman, Gilbert, 112-144. Dickinson. Konolige, Kurt. (1986). A Deduction Model of Belief. Pitman.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Saul Kripke</author>
</authors>
<title>Outline of a Theory of Truth.&amp;quot;</title>
<journal>Journal of Philosophy,</journal>
<volume>72</volume>
<pages>690--715</pages>
<marker>Kripke, </marker>
<rawString>Kripke, Saul. &amp;quot;Outline of a Theory of Truth.&amp;quot; Journal of Philosophy, 72: 690-715.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lewis</author>
</authors>
<title>Attitudes de dicto and de se.&amp;quot;</title>
<date>1979</date>
<journal>Philosophical Review,</journal>
<volume>88</volume>
<pages>513--543</pages>
<contexts>
<context position="7939" citStr="Lewis (1979)" startWordPosition="1309" endWordPosition="1310">s analysis of the notion. We assume that representation is a relation between an agent, a name, and the entity that the name denotes. If an agent has an attitude toward a thought-language sentence, and that sentence contains a name that represents a certain entity to the agent, then the agent has a de re attitude about that entity. Our grammar will build logical forms that are compatible with any sentential theory that includes these assumptions. One problem about the nature of representation should be mentioned. This concerns the so-called de se attitude reports. This term is attributable to Lewis (1979), but the clearest definition is from Boer and Lycan (1986). De se attitudes are &amp;quot;attitudes whose content would be formulated by the subject using the equivalent in his or her language of the first-person singular pronoun &apos;I&apos;&amp;quot; (B6er and Lycan 1986). If John thinks that he is wise, and we understand this as a de se attitude, what name represents John to himself? One possibility is that it is his selfname. An agent&apos;s selfname is a thought-language constant that he standardly uses to denote himself. It was postulated in Haas (1986) in order to solve certain problems about planning to acquire info</context>
</contexts>
<marker>Lewis, 1979</marker>
<rawString>Lewis, David. (1979). &amp;quot;Attitudes de dicto and de se.&amp;quot; Philosophical Review, 88: 513-543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>The proper treatment of quantification in ordinary english.&amp;quot; In Formal Philosophy: Selected Papers of Richard Montague,</title>
<date>1974</date>
<pages>247--270</pages>
<publisher>Yale University Press.</publisher>
<note>edited by</note>
<contexts>
<context position="27286" citStr="Montague (1974" startWordPosition="4595" endWordPosition="4596">et us assume that the predicate &amp;quot;have&amp;quot; represents a sense of the verb &amp;quot;have&amp;quot; that is roughly synonymous with &amp;quot;possess,&amp;quot; as in &amp;quot;John has a Porsche.&amp;quot; Another sense of &amp;quot;have&amp;quot; is relational, as in &amp;quot;John has a son,&amp;quot; and &amp;quot;want&amp;quot; has a corresponding sense, as in &amp;quot;John wants a son.&amp;quot; The present paper will not analyze this relational sense. This grammar will express the meanings of intensional verbs in terms of propositional attitudes. This may not work for all intensional verbs. For example, it is not clear that &amp;quot;the Greeks worshipped Zeus&amp;quot; is equivalent to any statement about propositional attitudes. Montague (1974a) represented intensional verbs more directly, as relations between agents and the intensions of NP&apos;s. A similar analysis is possible in our framework, provided we extend the target language to include typed lambda calculus. Suppose the Computational Linguistics Volume 16, Number 4, December 1990 217 Andrew R. Haas Sentential Semantics for Propositional Attitudes variable p ranges over sets of individuals. Then we could represent the de dicto reading of &amp;quot;John wants a Porsche&amp;quot; as (27) want( john,q(lambda(p,some(x,porsche(x),x E Here the predicate &amp;quot;want&amp;quot; describes a relation between a person an</context>
<context position="31636" citStr="Montague (1974" startWordPosition="5295" endWordPosition="5296">n(bill) we can infer (31) believe(bill,[bill],q(love(mary,x))) by substituting for a universal variable as usual. The occurrence of the variable under the quotation mark is naturally unaffected, because it is not a free occurrence of x. 1.5 SELF-REFERENCE AND PARADOX Other writers (cited above) have already expounded and defended sentential theories of attitudes. This paper takes a sentential theory as a starting point, and aims to solve certain problems about the semantics of attitude reports in such a theory. However, one problem about sentential theories deserves discussion. The results of Montague (19741)) have been widely interpreted as proof that sentential theories of attitudes are inconsistent and therefore useless. Montague did indeed show that certain sentential theories of knowledge produce self-reference paradoxes, and are therefore inconsistent. However, he did not show that these were the only possible sentential theories. Recently des Rivieres and Levesque (1986) have constructed sentential theories without self-reference and proved them consistent. Thus they showed that while Montague&apos;s theorem was true, its significance had been misunderstood. Perlis (1988) has shown that if we </context>
<context position="45043" citStr="Montague 1974" startWordPosition="7521" endWordPosition="7522">(Wff 1 ,Wff2). 2.2 REPRESENTING QUANTIFIERS Noun phrases in the grammar contribute to logical form in two ways, and therefore they have two semantic features. The first feature is a variable, which becomes a logical argument of a verb. This produces a wff, in which the variable appears free. The second feature is a quantifier that binds the variable. By applying the quantifier to the wff, we eliminate free occurrences of that particular variable. After applying all the quantifiers, we have a wff without free variables—a sentence. This is the logical form of an utterance. In Montague&apos;s system (Montague 1974a), the logical form of an NP is an expression denoting a quantifier. This kind of analysis is impossible in our system, because the target language is first-order. It contains no expressions that denote quantifiers. Therefore the representation of an NP cannot be an expression of the target language. Instead of using Montague&apos;s approach, we associate with every quantifier a function that maps wffs to wffs. For the NP &amp;quot;every man,&amp;quot; we have a function that maps any wff Wffl to the wff (37) all(V,man(V),Wffl) where V is a variable of the target language. Notice that if we took Montague&apos;s represen</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>Montague, Richard. (1974a). &amp;quot;The proper treatment of quantification in ordinary english.&amp;quot; In Formal Philosophy: Selected Papers of Richard Montague, edited by Thomason, Richmond H., 247-270. Yale University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>Syntactical treatments of modality, with corollaries on reflexion principles and finite axiomatizability.&amp;quot; In Formal Philosophy: Selected Papers of Richard Montague,</title>
<date>1974</date>
<pages>286--302</pages>
<publisher>Yale University Press.</publisher>
<note>edited by</note>
<contexts>
<context position="27286" citStr="Montague (1974" startWordPosition="4595" endWordPosition="4596">et us assume that the predicate &amp;quot;have&amp;quot; represents a sense of the verb &amp;quot;have&amp;quot; that is roughly synonymous with &amp;quot;possess,&amp;quot; as in &amp;quot;John has a Porsche.&amp;quot; Another sense of &amp;quot;have&amp;quot; is relational, as in &amp;quot;John has a son,&amp;quot; and &amp;quot;want&amp;quot; has a corresponding sense, as in &amp;quot;John wants a son.&amp;quot; The present paper will not analyze this relational sense. This grammar will express the meanings of intensional verbs in terms of propositional attitudes. This may not work for all intensional verbs. For example, it is not clear that &amp;quot;the Greeks worshipped Zeus&amp;quot; is equivalent to any statement about propositional attitudes. Montague (1974a) represented intensional verbs more directly, as relations between agents and the intensions of NP&apos;s. A similar analysis is possible in our framework, provided we extend the target language to include typed lambda calculus. Suppose the Computational Linguistics Volume 16, Number 4, December 1990 217 Andrew R. Haas Sentential Semantics for Propositional Attitudes variable p ranges over sets of individuals. Then we could represent the de dicto reading of &amp;quot;John wants a Porsche&amp;quot; as (27) want( john,q(lambda(p,some(x,porsche(x),x E Here the predicate &amp;quot;want&amp;quot; describes a relation between a person an</context>
<context position="31636" citStr="Montague (1974" startWordPosition="5295" endWordPosition="5296">n(bill) we can infer (31) believe(bill,[bill],q(love(mary,x))) by substituting for a universal variable as usual. The occurrence of the variable under the quotation mark is naturally unaffected, because it is not a free occurrence of x. 1.5 SELF-REFERENCE AND PARADOX Other writers (cited above) have already expounded and defended sentential theories of attitudes. This paper takes a sentential theory as a starting point, and aims to solve certain problems about the semantics of attitude reports in such a theory. However, one problem about sentential theories deserves discussion. The results of Montague (19741)) have been widely interpreted as proof that sentential theories of attitudes are inconsistent and therefore useless. Montague did indeed show that certain sentential theories of knowledge produce self-reference paradoxes, and are therefore inconsistent. However, he did not show that these were the only possible sentential theories. Recently des Rivieres and Levesque (1986) have constructed sentential theories without self-reference and proved them consistent. Thus they showed that while Montague&apos;s theorem was true, its significance had been misunderstood. Perlis (1988) has shown that if we </context>
<context position="45043" citStr="Montague 1974" startWordPosition="7521" endWordPosition="7522">(Wff 1 ,Wff2). 2.2 REPRESENTING QUANTIFIERS Noun phrases in the grammar contribute to logical form in two ways, and therefore they have two semantic features. The first feature is a variable, which becomes a logical argument of a verb. This produces a wff, in which the variable appears free. The second feature is a quantifier that binds the variable. By applying the quantifier to the wff, we eliminate free occurrences of that particular variable. After applying all the quantifiers, we have a wff without free variables—a sentence. This is the logical form of an utterance. In Montague&apos;s system (Montague 1974a), the logical form of an NP is an expression denoting a quantifier. This kind of analysis is impossible in our system, because the target language is first-order. It contains no expressions that denote quantifiers. Therefore the representation of an NP cannot be an expression of the target language. Instead of using Montague&apos;s approach, we associate with every quantifier a function that maps wffs to wffs. For the NP &amp;quot;every man,&amp;quot; we have a function that maps any wff Wffl to the wff (37) all(V,man(V),Wffl) where V is a variable of the target language. Notice that if we took Montague&apos;s represen</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>Montague, Richard. (1974b). &amp;quot;Syntactical treatments of modality, with corollaries on reflexion principles and finite axiomatizability.&amp;quot; In Formal Philosophy: Selected Papers of Richard Montague, edited by Thomason, Richmond H., 286-302. Yale University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
<author>David Kaplan</author>
</authors>
<title>A paradox regained.&amp;quot; In Formal Philosophy: Selected Papers of Richard Montague,</title>
<date>1974</date>
<pages>271--301</pages>
<publisher>Yale University Press.</publisher>
<note>edited by</note>
<contexts>
<context position="37872" citStr="Montague and Kaplan 1974" startWordPosition="6310" endWordPosition="6314">er theory, we can use all the familiar first-order inference rules. In this respect, Perlis&apos;s system is better suited to the needs of Al than either Kripke&apos;s or Priest&apos;s. However, it still excludes some inferences that are standard in everyday reasoning. For example, we have true(q(P)) -* P for every P, but P -* true(q(P)) is not a theorem for certain sentences P—in particular, sentences that are self-referential and paradoxical. An adequate account of self-reference must deal not only with the Liar, but also with paradoxes arising from propositional attitudes—for example, the Knower Paradox (Montague and Kaplan 1974), and Thomason&apos;s paradox about belief (Thomason 1980). Perlis (1988) has considered the treatment of attitudes within his system, and Asher and Kamp (1986) have treated both paradoxes using ideas akin to Kripke&apos;s (their treatment is not sentential, but they claim that it could be extended to a sentential treatment). Let us briefly consider the treatment of the Knower paradox within Perlis&apos;s system. To simplify the treatment, we will assume that knowledge is true belief. If we are working in Perlis&apos;s system, this naturally means that knowledge is Kripke-true belief. We write &amp;quot;the agent knows th</context>
</contexts>
<marker>Montague, Kaplan, 1974</marker>
<rawString>Montague, Richard and Kaplan, David. (1974). &amp;quot;A paradox regained.&amp;quot; In Formal Philosophy: Selected Papers of Richard Montague, edited by Thomason, Richmond H., 271-301. Yale University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Unification-based semantic interpretation.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--41</pages>
<contexts>
<context position="97351" citStr="Moore (1989)" startWordPosition="15577" endWordPosition="15578">unification grammar that builds logical forms, and at the same time respects the declarative semantics of the notation. We explicitly choose the bound variables of the logical form, instead of using meta-language variables. We also explain the semantics of our representation for quantified NPs: each NP has an infinite set of readings, one for each ordered pair in the extension of the application function. Several authors treat unification grammars with semantics as poor relations of Montague grammar. Pereira and Shieber (1987) propose to &amp;quot;encode&amp;quot; Montague&apos;s ideas in unification grammar, while Moore (1989) fears that building logical forms with unification grammar is &amp;quot;unprincipled feature hacking.&amp;quot; We claim that these problems arise not from shortcomings of unification grammar, but from failure to take unification grammar seriously—which means respecting its declarative semantics. The most obvious line for future work is to extend the grammar. It would be fairly easy to include complex wh noun phrases, such as &amp;quot;whose cat&amp;quot; or &amp;quot;how many children&amp;quot; (Cooper&apos;s grammar handled these). A more difficult problem arises when a gap is the object of an intensional verb—as in &amp;quot;John knows what Mary wants.&amp;quot; Th</context>
</contexts>
<marker>Moore, 1989</marker>
<rawString>Moore, Robert C. (1989). &amp;quot;Unification-based semantic interpretation.&amp;quot; In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, 33-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Propositional attitudes and russellian propositions.&amp;quot;</title>
<date>1988</date>
<tech>Report No. CSLI-88-119,</tech>
<institution>Center for</institution>
<location>Menlo Park, CA.</location>
<contexts>
<context position="12123" citStr="Moore (1988)" startWordPosition="1988" endWordPosition="1989">c. Unfortunately, Konolige&apos;s system has two important limitations. First, he forbids a belief operator to appear in the scope of another belief operator. Thus, he rules out beliefs about beliefs, which are common in everyday life. Second, he assumes that each agent assigns to every known entity a unique &amp;quot;id constant.&amp;quot; When an agent has a belief about an object x, that belief contains the id constant for x. Using Kaplan&apos;s terminology, Konolige is saying that for any entity x and agent y, there is a unique a such that R(a,x,y). Kaplan never suggests that representation has this property, and as Moore (1988) pointed out, the claim is hard to believe. Surely an agent can have many names for an entity, some useful for one purpose and some for another. Why should one of them be the unique id constant? We will propose a notation that has the advantages of Konolige&apos;s notation without its limitations. Section 1.3 will present the new notation. In Section 1.4, we return to the problem of building logical forms for English sentences. 1.3 A NEW NOTATION FOR QUANTIFYING IN Our logical forms are sentences in a first-order logic augmented with a quotation operator. We call this language the target language. </context>
<context position="62201" citStr="Moore (1988)" startWordPosition="10253" endWordPosition="10254">woman(V1)) [woman]. Recall that p(Wffl,Wff2) is a quantifier that maps Wffl to Wff2. Then for determiners we have (R3a) det(V2,Range,p(Wff1,some(V2,Range,Wff1)))—■ [a] (R3b) det(V2,Range,p(Wffl,all(V2,Range,Wff1))) [every] (R3c) det(V2,Range,p(Wff1,unique(V2,Range, Wffl))) - [the] (R3d) det(V2,Range,p(Wffl,not(some(V2,Range, Wff1)))) —■ [no]. Then we get (62) np(V,[p(Wffl,all(V,man(V),Wff1))1 QL]-QL,FxFx,L) —■ [every man] (63) np(V,[p(Wffl,not(some(V,woman(V),Wff1)))1 QL]-QL,Fx-Fx,L) [no woman]. Thus &amp;quot;every man&amp;quot; is an NP that binds the variable V and maps Wffl to all(V,man(V),Wff1). Following Moore (1988), we interpret the proper name &amp;quot;John&amp;quot; as equivalent to the definite description &amp;quot;the one named &amp;quot;John.&amp;quot;&amp;quot; (R4) np(V,[p(Wff,unique(V,name(V,C),Wff))I QL1- QL,Fx-Fx,VL) - [Terminal], 1 proper_noun(Terminal,C) 1. The wff proper_noun(X,Y) means that X is a proper noun and Y is its logical form. Our lexicon includes the axioms (LI a) proper_noun(johnjohn) (Li b) proper_noun(mary,mary) (Li c) proper_noun(bill,bill). These axioms use the constant &amp;quot;john&amp;quot; to denote both a terminal symbol of the grammar and a constant of the target language—a convenient abuse of notation. Using (Lia) we get (64) np(V,[p(W</context>
</contexts>
<marker>Moore, 1988</marker>
<rawString>Moore, Robert C. (1988). &amp;quot;Propositional attitudes and russellian propositions.&amp;quot; Report No. CSLI-88-119, Center for the Study of Language and Information, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Gary Hendrix</author>
</authors>
<title>Computational models of belief and semantics of belief sentences.&amp;quot;</title>
<date>1979</date>
<tech>Technical report 187,</tech>
<institution>SRI International,</institution>
<location>Menlo Park, CA.</location>
<contexts>
<context position="1664" citStr="Moore and Hendrix (1979)" startWordPosition="257" endWordPosition="260">ogram can make inferences from them, so they have two essential properties of beliefs. It is tempting to conclude that they are the program&apos;s beliefs, and that human beliefs are also sentences of a thought language. If we extend this to all propositional attitudes, we have a sentential theory of propositional attitudes. In such a theory, an English sentence expresses a proposition, and this proposition is itself a sentence— although in a different language. Other theories of attitudes hold that a proposition is a set of possible worlds, or a situation—something very different from a sentence. Moore and Hendrix (1979), Haas (1986), Perlis (1988), and Konolige (1986) have argued for sentential theories and applied them to artificial intelligence. Kaplan (1975) proposed an analysis of quantification into the scope of attitudes within a sentential theory, and other authors using sentential theories have offered variations of his idea (Haas 1986; Konolige 1986). Most of these theories present serious difficulties for formal semantics. The problem is that they assign two very different logical forms to a clause: one form when the clause is the object of an attitude verb, and another when it stands alone. This m</context>
</contexts>
<marker>Moore, Hendrix, 1979</marker>
<rawString>Moore, Robert C. and Hendrix, Gary. (1979). &amp;quot;Computational models of belief and semantics of belief sentences.&amp;quot; Technical report 187, SRI International, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Moran</author>
</authors>
<title>Quantifier scoping in the SRI core language engine.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<marker>Moran, 1988</marker>
<rawString>Moran, Douglas. (1988). &amp;quot;Quantifier scoping in the SRI core language engine.&amp;quot; In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Stuart M Shieber</author>
</authors>
<title>Prolog and Natural-Language Analysis. Center for the Study of Language and Information,</title>
<date>1987</date>
<location>Stanford, CA.</location>
<contexts>
<context position="11331" citStr="Pereira and Shieber 1987" startWordPosition="1854" endWordPosition="1857">ary&amp;quot; forms an utterance by itself, the semantics will apply the quantifier to the wff &amp;quot;love(y,mary)&amp;quot; to get the sentence &amp;quot;some (y,man(y) &amp; love(y,mary)).&amp;quot; The problem is that the wff &amp;quot;love(y,mary)&amp;quot; does not appear in Kaplan&apos;s representation. In its place is the expression &amp;quot;love(a,mary),&amp;quot; containing a variable that ranges over names, not men. It might be possible to build this expression from the wff &amp;quot;love(y,mary),&amp;quot; but this sounds like a messy operation at best. Similar problems would arise if we chose another quotation device (such as the one in Haas 1986) or another scoping mechanism (as in Pereira and Shieber 1987). Konolige (1986) proposed a very different notation for quantifying in, one that would abolish the difficulty described here. His proposal depends on an ingenious nonstandard logic. Unfortunately, Konolige&apos;s system has two important limitations. First, he forbids a belief operator to appear in the scope of another belief operator. Thus, he rules out beliefs about beliefs, which are common in everyday life. Second, he assumes that each agent assigns to every known entity a unique &amp;quot;id constant.&amp;quot; When an agent has a belief about an object x, that belief contains the id constant for x. Using Kapl</context>
<context position="41758" citStr="Pereira and Shieber (1987)" startWordPosition="6966" endWordPosition="6969">ntences. Thomason (1986) considers such analyses and finds that they have no clear advantage over the sentential approaches. The unpleasant truth is that paradoxes of self-reference create equally serious problems for all known theories of attitudes. It follows that they provide no evidence against the sentential theories. 2 THE BASIC GRAMMAR 2.1 NOTATION The rules of our grammar are definite clauses, and we use the notation of definite clause grammar (Pereira and Warren 1980). This notation is now standard among computer scientists who study natural language and is explained in a textbook by Pereira and Shieber (1987). Its advantages are that it is well defined and easy to learn, because it is a notational variant of standard first-order logic. Also, it is often straightforward to parse with grammars written in this notation (although there can be no general parsing method for the notation, since it has Turing machine power). DCG notation lacks some useful devices found in linguistic formalisms like GPSG—there are no default feature values or general feature agreement principles (Gazdar et al. 1985). On the other hand, the declarative semantics of the DCG notation is quite clear—unlike the semantics of GPS</context>
<context position="55432" citStr="Pereira and Shieber (1987)" startWordPosition="9201" endWordPosition="9204">hat semantics every variable in an answer is universally quantified. Thus if Prolog returns (51) as a description of the logical form of a sentence, this means that for all values of X and Y the expression (51) denotes a possible logical form for that sentence. This means that if v is a variable of the object language, then (52) exists(v) : (man(v) &amp; all(v) : (woman(v) loves (v,v))) is a possible translation, which is clearly false. Thus, according to the declarative interpretation, Pereira and Warren&apos;s grammar does not express the requirement that no quantifier can shadow another quantifier. Pereira and Shieber (1987) pointed out this problem and said that while formally incorrect the technique was &amp;quot;unlikely to cause problems.&amp;quot; Yet on p. 101 they describe the structures built by their grammar as &amp;quot;unintuitive&amp;quot; and even &amp;quot;bizarre.&amp;quot; This confirms the conventional wisdom: violating the declarative semantics makes logic programs hard to understand. Therefore, let us look for a solution that is formally correct. Warren (1983) suggested one possible solution. We can use a global counter to keep track of all the variables used in the logical form of a sentence, and assign a new variable to every quantifier. Then no</context>
<context position="59122" citStr="Pereira and Shieber (1987" startWordPosition="9763" endWordPosition="9766">for integers.) Suppose that QL1 is (59) [p(Wffl,some(VI,man(V1),Wff1)),p(Wff2,all (V2,woman(V2), Wff2) )]. Then solutions for the goal apply_quants(QL1,1oves(V1,V2),QL3,Wff3). include Wff 3 = some(v(2),man(v(2), all(v(1),woman(v(1),loves(v(2),v(1)))) QL3 = [1. The inner quantifier binds the variable v(1), and the outer quantifier binds the variable v(2). This notation for variables is very hard to read, so in the rest of the paper we will use the constants x, y, and z to represent variables of the target language. 2.5 RULES FOR NOUN PHRASES The following grammar is very similar to the work of Pereira and Shieber (1987, Sections 4.1 and 4.2). There are two major differences, however, First, the treatment of quantifiers and scoping uses a version of Cooper&apos;s quantifier storage, instead of the &amp;quot;quantifier tree&amp;quot; of Pereira and Shieber. Second, Pereira and Shieber started with a semantics using lambda calculus, which they &amp;quot;encoded&amp;quot; in Prolog. In the present grammar, unification semantics stands on its own—it is not a way of encoding some other formalism. Formula numbering uses the following conventions. The rules of the grammar are numbered (R1), (R2), etc. Entries in the lexicon are numbered (L1), (L2), etc. F</context>
<context position="60617" citStr="Pereira and Shieber 1987" startWordPosition="10029" endWordPosition="10032"> empty; if the NP is not a gap, the first element of the store is the quantifier generated by the NP (in the present grammar, the quantifier store of an NP has at most one quantifier). We represent the quantifier store as a difference list, using the infix operator &amp;quot;-&amp;quot;. Thus if L2 is a tail of Ll, Li-L2 is the list difference of LI and L2: the list formed by removing L2 from the end of Li. Therefore a noun phrase has the form np(V,QL1-QL2,Fx-Fy,VL). V is the bound variable of the NP. QL1-QL2 is the quantifier store of the NP. We describe wh-movement using the standard gap-threading technique (Pereira and Shieber 1987), and Fx-Fy is the filler list. Finally, VL is a list of target-language variables representing NPs that are available for reference by a pronoun, which we will call the pronoun reference list. Consider an NP consisting of a determiner and a head noun: &amp;quot;every man,&amp;quot; &amp;quot;no woman,&amp;quot; and so forth. The head noun supplies the range restriction of the NP&apos;s quantifier, and the determiner builds the quantifier given the range restriction. The bound variable of the NP is a feature of both the determiner and the head noun. Then the following rule generates NPs consisting of a determiner and a head Computati</context>
<context position="97271" citStr="Pereira and Shieber (1987)" startWordPosition="15564" endWordPosition="15567">ited treatment of pronouns. Another contribution of our work is that it seems to be the first unification grammar that builds logical forms, and at the same time respects the declarative semantics of the notation. We explicitly choose the bound variables of the logical form, instead of using meta-language variables. We also explain the semantics of our representation for quantified NPs: each NP has an infinite set of readings, one for each ordered pair in the extension of the application function. Several authors treat unification grammars with semantics as poor relations of Montague grammar. Pereira and Shieber (1987) propose to &amp;quot;encode&amp;quot; Montague&apos;s ideas in unification grammar, while Moore (1989) fears that building logical forms with unification grammar is &amp;quot;unprincipled feature hacking.&amp;quot; We claim that these problems arise not from shortcomings of unification grammar, but from failure to take unification grammar seriously—which means respecting its declarative semantics. The most obvious line for future work is to extend the grammar. It would be fairly easy to include complex wh noun phrases, such as &amp;quot;whose cat&amp;quot; or &amp;quot;how many children&amp;quot; (Cooper&apos;s grammar handled these). A more difficult problem arises when a</context>
</contexts>
<marker>Pereira, Shieber, 1987</marker>
<rawString>Pereira, Fernando C. N. and Shieber, Stuart M. (1987). Prolog and Natural-Language Analysis. Center for the Study of Language and Information, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>David H D Warren</author>
</authors>
<title>Definite clause grammars for language analysis-a survey of the formalism and a comparison with augmented transition networks.&amp;quot;</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<volume>13</volume>
<pages>231--278</pages>
<contexts>
<context position="2729" citStr="Pereira and Warren (1980)" startWordPosition="431" endWordPosition="434">t they assign two very different logical forms to a clause: one form when the clause is the object of an attitude verb, and another when it stands alone. This means that the logical form of the clause depends on its context in a complicated way. It is difficult to describe this dependence in a formal grammar. The present paper aims to solve this problem—to present a grammar that assigns logical forms that are correct according to Kaplan&apos;s ideas. We also describe a parser that builds the logical forms required by the grammar. This grammar is a set of definite clauses written in the notation of Pereira and Warren (1980). However, it is not a definite clause grammar for two reasons. First, our grammar cannot be parsed by the top-down left-to-right method used for definite clause grammar (although it can be modified to allow this). Second, we do not allow any of the nonlogical operations of Prolog, such as checking whether a variable is bound or free, negation as failure, and the rest. This means that our grammar is a set of ordinary first-order sentences (in an unusual notation) and its semantics is the ordinary semantics of first-order logic. So the grammar is declarative, in the sense that it defines a lang</context>
<context position="41613" citStr="Pereira and Warren 1980" startWordPosition="6942" endWordPosition="6946">one sacrifices some principle that seems useful for reasoning in an AI program. The alternative is an analysis in which propositions are not sentences. Thomason (1986) considers such analyses and finds that they have no clear advantage over the sentential approaches. The unpleasant truth is that paradoxes of self-reference create equally serious problems for all known theories of attitudes. It follows that they provide no evidence against the sentential theories. 2 THE BASIC GRAMMAR 2.1 NOTATION The rules of our grammar are definite clauses, and we use the notation of definite clause grammar (Pereira and Warren 1980). This notation is now standard among computer scientists who study natural language and is explained in a textbook by Pereira and Shieber (1987). Its advantages are that it is well defined and easy to learn, because it is a notational variant of standard first-order logic. Also, it is often straightforward to parse with grammars written in this notation (although there can be no general parsing method for the notation, since it has Turing machine power). DCG notation lacks some useful devices found in linguistic formalisms like GPSG—there are no default feature values or general feature agree</context>
<context position="46101" citStr="Pereira and Warren (1980)" startWordPosition="7688" endWordPosition="7691">&amp;quot; we have a function that maps any wff Wffl to the wff (37) all(V,man(V),Wffl) where V is a variable of the target language. Notice that if we took Montague&apos;s representation for the quantified NP, applied it to the lambda expression lambda(V,Wff1), and then simplified, we would get an alphabetic variant of (37). 220 Computational Linguistics Volume 16, Number 4, December 1990 Andrew R. Haas Sentential Semantics for Propositional Attitudes We will call this function the application function for the quantified NP. To represent application functions in a unification grammar, we use a device from Pereira and Warren (1980). We assign to each NP an infinite set of readings—one for each ordered pair in the extension of the application function. The first and second elements of the ordered pair are semantic features of the NP, and the bound variable of the quantifier is a third feature. For the NP &amp;quot;every man&amp;quot; we have (38) np(V,Wffl,all(V,man(V),Wff1)) --. [every man] . This says that for any variable V and wff Wffl, the string &amp;quot;every man&amp;quot; is an NP, and if it binds the variable V, then the pair [Wffl,all(V,man(V),Wff1)] is in the extension of its application function. It follows that the application function maps W</context>
<context position="54478" citStr="Pereira and Warren (1980)" startWordPosition="9045" endWordPosition="9048">to a sentence without shadowing. The same problem arises in cases of quantification into the scope of attitudes. Consider the sentence &amp;quot;John thinks some man loves every woman,&amp;quot; and suppose that &amp;quot;some man&amp;quot; has wide scope and &amp;quot;every woman&amp;quot; has narrow scope. The logical form can be some(x,man(x), thinks( john,[x],q(all(y,woman(y),loves(x,y))))) but it cannot be some(y,man(y), thinks( john,[y],q(all(y,woman(y),loves(y,y))))). In this formula, the inner quantifier captures a variable that is supposed to be a dummy variable. In this case also, we say that the inner quantifier shadows the outer one. Pereira and Warren (1980) prevented shadowing by using Prolog variables to represent variables of the object language. Thus, their translation for &amp;quot;Some man loves every woman&amp;quot; is (51) exists(Y) : (man(Y) &amp; all(X) : (woman(X) loves(Y,X))) where X and Y are Prolog variables. This works, but it violates the declarative semantics of Prolog. According to that semantics every variable in an answer is universally quantified. Thus if Prolog returns (51) as a description of the logical form of a sentence, this means that for all values of X and Y the expression (51) denotes a possible logical form for that sentence. This means</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, Fernando C. N. and Warren, David H. D. (1980). &amp;quot;Definite clause grammars for language analysis-a survey of the formalism and a comparison with augmented transition networks.&amp;quot; Artificial Intelligence, 13: 231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Perlis</author>
</authors>
<title>Languages with self reference II: knowledge, belief, and modality.&amp;quot;</title>
<date>1988</date>
<journal>Artificial Intelligence,</journal>
<volume>34</volume>
<pages>179--212</pages>
<contexts>
<context position="1692" citStr="Perlis (1988)" startWordPosition="263" endWordPosition="264"> they have two essential properties of beliefs. It is tempting to conclude that they are the program&apos;s beliefs, and that human beliefs are also sentences of a thought language. If we extend this to all propositional attitudes, we have a sentential theory of propositional attitudes. In such a theory, an English sentence expresses a proposition, and this proposition is itself a sentence— although in a different language. Other theories of attitudes hold that a proposition is a set of possible worlds, or a situation—something very different from a sentence. Moore and Hendrix (1979), Haas (1986), Perlis (1988), and Konolige (1986) have argued for sentential theories and applied them to artificial intelligence. Kaplan (1975) proposed an analysis of quantification into the scope of attitudes within a sentential theory, and other authors using sentential theories have offered variations of his idea (Haas 1986; Konolige 1986). Most of these theories present serious difficulties for formal semantics. The problem is that they assign two very different logical forms to a clause: one form when the clause is the object of an attitude verb, and another when it stands alone. This means that the logical form o</context>
<context position="32214" citStr="Perlis (1988)" startWordPosition="5376" endWordPosition="5377">sion. The results of Montague (19741)) have been widely interpreted as proof that sentential theories of attitudes are inconsistent and therefore useless. Montague did indeed show that certain sentential theories of knowledge produce self-reference paradoxes, and are therefore inconsistent. However, he did not show that these were the only possible sentential theories. Recently des Rivieres and Levesque (1986) have constructed sentential theories without self-reference and proved them consistent. Thus they showed that while Montague&apos;s theorem was true, its significance had been misunderstood. Perlis (1988) has shown that if we introduce self-reference into a modal theory, it too can become inconsistent. In short, there is no special connection between sentential theories and paradoxes of self-reference. A sentential theory may or may not include self-reference; a modal theory may or may not include self-reference; and in either case, self-:reference can lead to paradoxes. Kripke (1975) has shown that even the most commonplace utterances can create self-reference if they occur in unusual circumstances. Therefore the problem is not to avoid self-reference, but to understand it. The problem for ad</context>
<context position="37940" citStr="Perlis (1988)" startWordPosition="6322" endWordPosition="6323">ect, Perlis&apos;s system is better suited to the needs of Al than either Kripke&apos;s or Priest&apos;s. However, it still excludes some inferences that are standard in everyday reasoning. For example, we have true(q(P)) -* P for every P, but P -* true(q(P)) is not a theorem for certain sentences P—in particular, sentences that are self-referential and paradoxical. An adequate account of self-reference must deal not only with the Liar, but also with paradoxes arising from propositional attitudes—for example, the Knower Paradox (Montague and Kaplan 1974), and Thomason&apos;s paradox about belief (Thomason 1980). Perlis (1988) has considered the treatment of attitudes within his system, and Asher and Kamp (1986) have treated both paradoxes using ideas akin to Kripke&apos;s (their treatment is not sentential, but they claim that it could be extended to a sentential treatment). Let us briefly consider the treatment of the Knower paradox within Perlis&apos;s system. To simplify the treatment, we will assume that knowledge is true belief. If we are working in Perlis&apos;s system, this naturally means that knowledge is Kripke-true belief. We write &amp;quot;the agent knows that P&amp;quot; as true(q(P)) A believe(q(P)). The paradox arises from a sente</context>
</contexts>
<marker>Perlis, 1988</marker>
<rawString>Perlis, Donald. (1988). &amp;quot;Languages with self reference II: knowledge, belief, and modality.&amp;quot; Artificial Intelligence, 34: 179-212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Perlis</author>
</authors>
<title>On the consistency of commonsense reasoning.&amp;quot;</title>
<date>1986</date>
<journal>Computational Intelligence,</journal>
<volume>2</volume>
<pages>180--190</pages>
<contexts>
<context position="40810" citStr="Perlis (1986)" startWordPosition="6814" endWordPosition="6815">t for every formula &lt;P, the agent believes (36) a(&lt; So &gt;) —• (P. This sentence says that if the agent believes &apos;P, (P must be true. Since So ranges over all sentences of the language, the agent is claiming that his beliefs are infallible. This leads the agent into a paradox similar to the Knower, and his beliefs are therefore inconsistent. Asher and Kamp showed that one can avoid this conclusion by denying (35) in certain cases where 4) is a self-referential sentence. Another alternative is to dismiss (35) completely. It is doubtful that human beings consider their own beliefs infallible, and Perlis (1986) has argued that a rational agent may well believe that some of his or her beliefs are false. We have looked at three sentential analyses of the selfreference paradoxes, and each one sacrifices some principle that seems useful for reasoning in an AI program. The alternative is an analysis in which propositions are not sentences. Thomason (1986) considers such analyses and finds that they have no clear advantage over the sentential approaches. The unpleasant truth is that paradoxes of self-reference create equally serious problems for all known theories of attitudes. It follows that they provid</context>
</contexts>
<marker>Perlis, 1986</marker>
<rawString>Perlis, Donald. (1986). &amp;quot;On the consistency of commonsense reasoning.&amp;quot; Computational Intelligence, 2: 180-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Perlis</author>
</authors>
<title>Languages with self-reference I: foundations.&amp;quot;</title>
<date>1985</date>
<journal>Artificial Intelligence,</journal>
<volume>25</volume>
<pages>301--322</pages>
<contexts>
<context position="36128" citStr="Perlis (1985)" startWordPosition="6016" endWordPosition="6017">he sentence -Itrue(c). What truth value can such a sentence have under Kripke&apos;s definition? Just as in standard logic, -1 true(c) is true if true(c) is false. True(c) in turn is false if c is false. Since c is the sentence -I true(c), we have shown that c is true if c is false. Since no sentence has two truth values, it follows that c has no truth value. Once again, problems arise because the system is too weak. If P is a sentence with no truth value, then the sentence P V -1 P has no truth value, even though it is a tautology of first-order logic. One remedy for this appears in the system of Perlis (1985). Perlis considers a first-order model M containing a predicate &amp;quot;true,&amp;quot; whose extension is the set of sentences that are true in M by Kripke&apos;s definition. He accepts as theorems all sentences that are Tarskitrue in every model of this kind. Thus Perlis&apos;s system uses two notions of truth: P is a theorem only if P is Tarski-true, but true(q(P)) is a theorem only if P is Kripke-true. Suppose we have P *-* ---1 true(q(P)); then Perlis&apos;s system allows us to prove both P and -1 true(q(P)). This certainly violates the intuitions of ordinary speakers, but such violations seem to be the inevitable pric</context>
</contexts>
<marker>Perlis, 1985</marker>
<rawString>Perlis, Donald. (1985). &amp;quot;Languages with self-reference I: foundations.&amp;quot; Artificial Intelligence, 25: 301-322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William J Rapaport</author>
</authors>
<title>Logical Foundations for Belief Representation.&amp;quot;</title>
<date>1986</date>
<journal>Cognitive Science,</journal>
<volume>10</volume>
<pages>371--42</pages>
<contexts>
<context position="8814" citStr="Rapaport (1986)" startWordPosition="1459" endWordPosition="1460">inks that he is wise, and we understand this as a de se attitude, what name represents John to himself? One possibility is that it is his selfname. An agent&apos;s selfname is a thought-language constant that he standardly uses to denote himself. It was postulated in Haas (1986) in order to solve certain problems about planning to acquire information. To expound and defend this idea would take us far from the problems of compositional semantics that concern us here. We simply mention it as an example of the kinds of theories that are compatible with the logical forms built by our grammar. See also Rapaport (1986) for another Al approach to de se attitudes. 1.2 COMPOSITIONAL SEMANTICS AND LOGICAL FORMS Consider the logical form that Kaplan assigns for the de re reading of &amp;quot;John believes that some man loves Mary.&amp;quot; (1) 3 (y,man(y) &amp; 3 (a,R(a,y,john) &amp; believe(john,rlove(a,mary)1))) The notation is a slight modification of Kaplan&apos;s (Kaplan 1975), The predicate letter R denotes representation. The symbol a is a special variable ranging over names. The symbols randi are Quine&apos;s quasi-quotes (Quine 1947). If a denotes a name t, then the expression &amp;quot;rlove(a,mary)1&amp;quot; will denote the sentence &amp;quot;love(t,mary).&amp;quot; It </context>
</contexts>
<marker>Rapaport, 1986</marker>
<rawString>Rapaport, William J. (1986). &amp;quot;Logical Foundations for Belief Representation.&amp;quot; Cognitive Science, 10: 371-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willard van Orman Quine</author>
</authors>
<title>Quantifiers and propositional attitudes.&amp;quot;</title>
<date>1975</date>
<booktitle>In The Logic of Grammar,</booktitle>
<location>Dickinson.</location>
<note>edited by</note>
<contexts>
<context position="16524" citStr="Quine (1975)" startWordPosition="2800" endWordPosition="2801">nspecified terms that represent the entities x1 . . . x„ to the agent p. These free variables are called dummy variables. If John believes of Mary that she is a fool, then, using Prolog&apos;s notation for lists we write (6) believe(john,[mary],q(fool(x))). The constant &amp;quot;mary&amp;quot; is called a de re argument of the predicate &amp;quot;believe.&amp;quot; The free occurrence of x in fool(x) stands for an unspecified term that represents Mary to John. This means that there is a term t that represents Mary to John, and John believes fool(t). x is the dummy variable for the de re argument &amp;quot;mary.&amp;quot; This notation is inspired by Quine (1975), but we give a semantics quite different from Quine&apos;s. Note that the symbol &amp;quot;believe&amp;quot; is Computational Linguistics Volume 16, Number 4, December 1990 215 Andrew R. Haas Sentential Semantics for Propositional Attitudes an ordinary predicate letter, not a special operator. This is a minor technical advantage of the sentential approach: the quotation operator eliminates the need for a variety of special propositional attitude operators. To define this notation precisely, we must have some way of associating dummy variables with the de re arguments. Suppose we have the wff believe(x, [ti . . . tn</context>
</contexts>
<marker>Quine, 1975</marker>
<rawString>Quine, Willard van Orman. (1975). &amp;quot;Quantifiers and propositional attitudes.&amp;quot; In The Logic of Grammar, edited by Davidson, Donald and Harman, Gilbert. Dickinson.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willard van Orman Quine</author>
</authors>
<title>Mathematical Logic.</title>
<date>1947</date>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="9308" citStr="Quine 1947" startWordPosition="1536" endWordPosition="1537">le of the kinds of theories that are compatible with the logical forms built by our grammar. See also Rapaport (1986) for another Al approach to de se attitudes. 1.2 COMPOSITIONAL SEMANTICS AND LOGICAL FORMS Consider the logical form that Kaplan assigns for the de re reading of &amp;quot;John believes that some man loves Mary.&amp;quot; (1) 3 (y,man(y) &amp; 3 (a,R(a,y,john) &amp; believe(john,rlove(a,mary)1))) The notation is a slight modification of Kaplan&apos;s (Kaplan 1975), The predicate letter R denotes representation. The symbol a is a special variable ranging over names. The symbols randi are Quine&apos;s quasi-quotes (Quine 1947). If a denotes a name t, then the expression &amp;quot;rlove(a,mary)1&amp;quot; will denote the sentence &amp;quot;love(t,mary).&amp;quot; It is hard to see how a compositional semantics can build this representation from the English sentence &amp;quot;John believes some man loves Mary.&amp;quot; The difficult part is building the representation for the VP &amp;quot;believes some man loves Mary.&amp;quot; By definition, a compositional semantics must build the representation from the representations of the constituents of the VP: the verb &amp;quot;believe&amp;quot; and the embedded clause. Following Cooper&apos;s notion of quantifier storage (Cooper 1983), we assume that the representa</context>
</contexts>
<marker>Quine, 1947</marker>
<rawString>Quine, Willard van Orman. (1947). Mathematical Logic. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred Tarski</author>
</authors>
<title>Der Wahrheitsbegriff in den Formalisierten Sprachen.</title>
<date>1936</date>
<journal>Studia Philosophia,</journal>
<volume>1</volume>
<pages>261--405</pages>
<contexts>
<context position="34141" citStr="Tarski (1936)" startWordPosition="5676" endWordPosition="5677">nal Linguistics Volume 16, Number 4, December 1990 Andrew R. Haas Sentential Semantics for Propositional Attitudes standard logic allows no inconsistent theories except trivial ones, containing every sentence of the language. Therefore we need a new kind of logic to describe the inconsistent intuitions of the ordinary speaker. Priest (1989) attempted this—he constructed an inconsistent but nontrivial theory of truth using a paraconsistent logic. Priest&apos;s theory includes the T-scheme, written in our notation as (32) P 4-* true(q(P)). P is a meta-variable ranging over sentences of the language. Tarski (1936) proposed this scheme as capturing an essential intuition about truth. Unfortunately, the rule of modus ponens is invalid in Priest&apos;s system, which means that most of the standard AI reasoning methods are invalid. Priest considers various remedies for this problem. Another approach is to look for a consistent theory of self-reference. Such a theory will probably disagree with speakers&apos; intuitions for paradoxical examples like &amp;quot;This statement is false.&amp;quot; Yet these examples are rare in practice, so a natural language program using a consistent theory of self-reference might agree with speakers&apos; i</context>
</contexts>
<marker>Tarski, 1936</marker>
<rawString>Tarski, Alfred. (1936). &amp;quot;Der Wahrheitsbegriff in den Formalisierten Sprachen. Studia Philosophia, 1: 261-405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richmond H Thomason</author>
</authors>
<title>Paradoxes and semantic representation.&amp;quot;</title>
<date>1986</date>
<booktitle>In Theoretical Aspects of Reasoning about Knowledge, edited by Joseph Halpern,</booktitle>
<pages>225--239</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="41156" citStr="Thomason (1986)" startWordPosition="6872" endWordPosition="6873">her and Kamp showed that one can avoid this conclusion by denying (35) in certain cases where 4) is a self-referential sentence. Another alternative is to dismiss (35) completely. It is doubtful that human beings consider their own beliefs infallible, and Perlis (1986) has argued that a rational agent may well believe that some of his or her beliefs are false. We have looked at three sentential analyses of the selfreference paradoxes, and each one sacrifices some principle that seems useful for reasoning in an AI program. The alternative is an analysis in which propositions are not sentences. Thomason (1986) considers such analyses and finds that they have no clear advantage over the sentential approaches. The unpleasant truth is that paradoxes of self-reference create equally serious problems for all known theories of attitudes. It follows that they provide no evidence against the sentential theories. 2 THE BASIC GRAMMAR 2.1 NOTATION The rules of our grammar are definite clauses, and we use the notation of definite clause grammar (Pereira and Warren 1980). This notation is now standard among computer scientists who study natural language and is explained in a textbook by Pereira and Shieber (198</context>
</contexts>
<marker>Thomason, 1986</marker>
<rawString>Thomason, Richmond H. (1986). &amp;quot;Paradoxes and semantic representation.&amp;quot; In Theoretical Aspects of Reasoning about Knowledge, edited by Joseph Halpern, 225-239. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richmond H Thomason</author>
</authors>
<title>A note on syntactical treatments of modality.&amp;quot;</title>
<date>1980</date>
<journal>Synthese,</journal>
<volume>44</volume>
<issue>3</issue>
<pages>391--395</pages>
<contexts>
<context position="37925" citStr="Thomason 1980" startWordPosition="6320" endWordPosition="6321">es. In this respect, Perlis&apos;s system is better suited to the needs of Al than either Kripke&apos;s or Priest&apos;s. However, it still excludes some inferences that are standard in everyday reasoning. For example, we have true(q(P)) -* P for every P, but P -* true(q(P)) is not a theorem for certain sentences P—in particular, sentences that are self-referential and paradoxical. An adequate account of self-reference must deal not only with the Liar, but also with paradoxes arising from propositional attitudes—for example, the Knower Paradox (Montague and Kaplan 1974), and Thomason&apos;s paradox about belief (Thomason 1980). Perlis (1988) has considered the treatment of attitudes within his system, and Asher and Kamp (1986) have treated both paradoxes using ideas akin to Kripke&apos;s (their treatment is not sentential, but they claim that it could be extended to a sentential treatment). Let us briefly consider the treatment of the Knower paradox within Perlis&apos;s system. To simplify the treatment, we will assume that knowledge is true belief. If we are working in Perlis&apos;s system, this naturally means that knowledge is Kripke-true belief. We write &amp;quot;the agent knows that P&amp;quot; as true(q(P)) A believe(q(P)). The paradox aris</context>
<context position="39985" citStr="Thomason 1980" startWordPosition="6668" endWordPosition="6669"> allow that any agent can know it. Strange though this appears, it is a natural consequence of the use of two definitions of truth in a single theory. Belief is different from knowledge because it need not be true. This makes it surprising that Thomason&apos;s paradox involves only the notion of belief, not knowledge or truth. In fact the paradox arises exactly because Thomason&apos;s agent thinks that all his beliefs are true. This is stated as (35) a(&lt; a(&lt; So &gt;) ---■ 40 &gt;) Computational Linguistics Volume 16, Number 4, December 1990 219 Andrew R. Haas Sentential Semantics for Propositional Attitudes (Thomason 1980). The notation is as follows: So is a variable ranging over all formulas of the language, &lt; So &gt; is a constant denoting (the Godel number of) So, and a(&lt; (P&gt;) means that the agent believes So. This axiom says that for every formula &lt;P, the agent believes (36) a(&lt; So &gt;) —• (P. This sentence says that if the agent believes &apos;P, (P must be true. Since So ranges over all sentences of the language, the agent is claiming that his beliefs are infallible. This leads the agent into a paradox similar to the Knower, and his beliefs are therefore inconsistent. Asher and Kamp showed that one can avoid this </context>
</contexts>
<marker>Thomason, 1980</marker>
<rawString>Thomason, Richmond H. (1980). &amp;quot;A note on syntactical treatments of modality.&amp;quot; Synthese, 44(3): 391-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Walther</author>
</authors>
<title>A Many-Sorted Calculus for Resolution and Paramodulation.</title>
<date>1987</date>
<publisher>Pitman.</publisher>
<contexts>
<context position="48068" citStr="Walther (1987)" startWordPosition="8024" endWordPosition="8025">he context of an utterance the ambiguity is resolved. In the same way, the VP &amp;quot;liked Mary&amp;quot; is ambiguous in person and number—but in the context of the utterance &amp;quot;John liked Mary,&amp;quot; its person and number are unambiguous. In one respect the declarative semantics of these rules is not quite right. The variable V is supposed to range over variables of the target language, and the variable Wffl is supposed to range over wffs of the target language. Yet we have not defined a type system to express these range restrictions. However, such a type system could be added, for example, using the methods of Walther (1987). In fact, the type hierarchy would be a tree, which allows us to use a simplified version of Walther&apos;s methods. For brevity&apos;s sake we will not develop a type system in this paper. Except for this omission, the declarative semantics of the above rules is quite clear. Typed variables have mnemonic value even if we do not use a typed logic. Therefore we adopt the following conventions. The meta-language variables V, VO, V1 . . . range over target language variables. Wff, Wffl, Wff2 . . . range over target language wffs. Q, Qi, Q2 . . . range over quantifiers. QL, QL1, QL2 . . . range over lists </context>
</contexts>
<marker>Walther, 1987</marker>
<rawString>Walther, Christoph. (1987). A Many-Sorted Calculus for Resolution and Paramodulation. Pitman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David S Warren</author>
</authors>
<title>Using lambda-calculus to represent meanings in logic grammars.&amp;quot;</title>
<date>1983</date>
<booktitle>In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>51--56</pages>
<contexts>
<context position="55841" citStr="Warren (1983)" startWordPosition="9267" endWordPosition="9268">clearly false. Thus, according to the declarative interpretation, Pereira and Warren&apos;s grammar does not express the requirement that no quantifier can shadow another quantifier. Pereira and Shieber (1987) pointed out this problem and said that while formally incorrect the technique was &amp;quot;unlikely to cause problems.&amp;quot; Yet on p. 101 they describe the structures built by their grammar as &amp;quot;unintuitive&amp;quot; and even &amp;quot;bizarre.&amp;quot; This confirms the conventional wisdom: violating the declarative semantics makes logic programs hard to understand. Therefore, let us look for a solution that is formally correct. Warren (1983) suggested one possible solution. We can use a global counter to keep track of all the variables used in the logical form of a sentence, and assign a new variable to every quantifier. Then no two quantifiers would bind the same variable, and certainly no quantifier would shadow another. This solution would make it easier to implement our treatment of de re attitude reports, but it would also create serious problems in the treatment of NP conjunction 222 Computational Linguistics Volume 16, Number 4, December 1990 Andrew R. Haas Sentential Semantics for Propositional Attitudes and disjunction (</context>
</contexts>
<marker>Warren, 1983</marker>
<rawString>Warren, David S. (1983). &amp;quot;Using lambda-calculus to represent meanings in logic grammars.&amp;quot; In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics, 51-56.</rawString>
</citation>
<citation valid="false">
<date>1990</date>
<volume>16</volume>
<pages>233</pages>
<institution>Computational Linguistics</institution>
<marker>1990</marker>
<rawString>Computational Linguistics Volume 16, Number 4, December 1990 233</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>