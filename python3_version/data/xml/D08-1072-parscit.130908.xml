<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001387">
<title confidence="0.997586">
Online Methods for Multi-Domain Learning and Adaptation
</title>
<author confidence="0.996147">
Mark Dredze and Koby Crammer
</author>
<affiliation confidence="0.998599">
Department of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.707564">
Philadelphia, PA 19104 USA
</address>
<email confidence="0.998612">
{mdredze,crammer}@cis.upenn.edu
</email>
<sectionHeader confidence="0.995632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999033538461539">
NLP tasks are often domain specific, yet sys-
tems can learn behaviors across multiple do-
mains. We develop a new multi-domain online
learning framework based on parameter com-
bination from multiple classifiers. Our algo-
rithms draw from multi-task learning and do-
main adaptation to adapt multiple source do-
main classifiers to a new target domain, learn
across multiple similar domains, and learn
across a large number of disparate domains.
We evaluate our algorithms on two popular
NLP domain adaptation tasks: sentiment clas-
sification and spam filtering.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951318181818">
Statistical classifiers routinely process millions of
websites, emails, blogs and other text every day.
Variability across different data sources means that
training a single classifier obscures differences and
separate classifiers ignore similarities. Similarly,
adding new domains to existing systems requires
adapting existing classifiers.
We present new online algorithms for three multi-
domain learning scenarios: adapting existing classi-
fiers to new domains, learning across multiple simi-
lar domains and scaling systems to many disparate
domains. Multi-domain learning combines char-
acteristics of both multi-task learning and domain
adaptation and drawing from both areas, we de-
velop a multi-classifier parameter combination tech-
nique for confidence-weighted (CW) linear classi-
fiers (Dredze et al., 2008). We focus on online algo-
rithms that scale to large amounts of data.
Next, we describe multi-domain learning and re-
view the CW algorithm. We then consider our three
settings using multi-classifier parameter combina-
tion. We conclude with related work.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="introduction">
2 Multi-Domain Learning
</sectionHeader>
<bodyText confidence="0.999828535714286">
In online multi-domain learning, each instance x is
drawn from a domain d specific distribution x — Dd
over a vectors space RN and labeled with a domain
specific function fd with label y E {−1, +1} (for
binary classification.) On round i the classifier re-
ceives instance xi and domain identifier di and pre-
dicts label yi E {−1, +1}. It then receives the true
label yi E {−1, +1} and updates its prediction rule.
As an example, consider a multi-user spam fil-
ter, which must give high quality predictions for
new users (without new user data), learn on multi-
ple users simultaneously and scale to thousands of
accounts. While a single classifier trained on all
users would generalize across users and extend to
new users, it would fail to learn user-specific prefer-
ences. Alternatively, separate classifiers would cap-
ture user-specific behaviors but would not general-
ize across users. The approach we take to solv-
ing multi-domain problems is to combine domain-
specific classifiers. In the adaptation setting, we
combine source domain classifiers for a new tar-
get domain. For learning across domains, we com-
bine domain-specific classifiers and a shared classi-
fier learned across all domains. For learning across
disparate domains we learn which domain-specific
and shared classifiers to combine.
Multi-domain learning combines properties of
both multi-task learning and domain adaptation. As
</bodyText>
<page confidence="0.984674">
689
</page>
<note confidence="0.9621395">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 689–697,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999714533333334">
in multi-task learning, we consider domains that are
labeled with different classification functions. For
example, one user may enjoy some emails that an-
other user considers spam: differing in their classifi-
cation function. The goal of multi-task learning is to
generalize across tasks/domains (Dekel et al., 2006;
Evgeniou and Pontil, 2004). Furthermore, as in do-
main adaptation, some examples are draw from dif-
ferent distributions. For example, one user may re-
ceive emails about engineering while another about
art, differing in their distribution over features. Do-
main adaptation deals with these feature distribution
changes (Blitzer et al., 2007; Jiang and Zhai, 2007).
Our work combines these two areas by learning both
across distributions and behaviors or functions.
</bodyText>
<sectionHeader confidence="0.993222" genericHeader="method">
3 Confidence-Weighted Linear Classifiers
</sectionHeader>
<bodyText confidence="0.9991896">
Confidence-weighted (CW) linear classification
(Dredze et al., 2008), a new online algorithm, main-
tains a probabilistic measure of parameter confi-
dence, which may be useful in combining parame-
ters from different domain distributions. We sum-
marize CW learning to familiarize the reader.
Parameter confidence is formalized by a Gaussian
distribution over weight vectors with mean µ E RN
and diagonal covariance E E RN,N. The values
µj and Ej,j represent knowledge of and confidence
in the parameter for feature j. The smaller Ej,j,
the more confidence we have in the mean parameter
value µj. In this work we consider diagonal covari-
ance matrices to scale to NLP data.
A model predicts the highest probability label,
</bodyText>
<equation confidence="0.926104">
Prw-N(µ,Σ) [yi(w · xi) &gt;— 0] .
</equation>
<bodyText confidence="0.999096875">
The Gaussian distribution over parameter vectors w
induces a univariate Gaussian distribution over the
score Si = w · xi parameterized by µ, E and the
instance xi: Si — N fµi, σD, with mean µi = µ·xi
and variance σ2i= xi Exi.
The CW algorithm is inspired by the Passive Ag-
gressive (PA) update (Crammer et al., 2006) —
which ensures a positive margin while minimizing
parameter change. CW replaces the Euclidean dis-
tance used in the PA update with the Kullback-
Leibler (KL) divergence over Gaussian distribu-
tions. It also replaces the minimal margin constraint
with a minimal probability constraint: with some
given probability η E (0.5, 1] a drawn classifier will
assign the correct label. This strategy yields the fol-
lowing objective solved on each round of learning:
</bodyText>
<equation confidence="0.91478">
min DKL (N (µ, E) II N (µi, Ei))
s.t. Pr [yi (w · xi) &gt;— 0] &gt;— η
,
</equation>
<bodyText confidence="0.992258777777778">
where (µi, Ei) are the parameters on round i and
w — N (µ, E). The constraint ensures that the re-
sulting parameters (µi+1, Ei+1) will correctly clas-
sify xi with probability at least η. For convenience
we write φ = 4b-1 (η), where 4b is the cumula-
tive function of the normal distribution. The opti-
mization problem above is not convex, but a closed
form approximation of its solution has the follow-
ing additive form: µi+1 = µi + αiyiEixi and
</bodyText>
<equation confidence="0.992814166666667">
E-1 i+1= E�1
i + 2αiφxixz for,
q(1+2φµi)2��φ �µi�φσ2 �
�(1+2φµi)+ i .
4φσ2
i
</equation>
<bodyText confidence="0.999783285714286">
Each update changes the feature weights µ, and in-
creases confidence (variance E always decreases).
We employ CW classifiers since they provide con-
fidence estimates, which are useful for classifier
combination. Additionally, since we require per-
parameter confidence estimates, other confidence
based classifiers are not suitable for this setting.
</bodyText>
<sectionHeader confidence="0.974679" genericHeader="method">
4 Multi-Classifier Parameter Combination
</sectionHeader>
<bodyText confidence="0.999937941176471">
The basis of our approach to multi-domain learning
is to combine the parameters of CW classifiers from
separate domains while respecting parameter confi-
dence. A combination method takes M CW classi-
fiers each parameterized by its own mean and vari-
ance parameters {(µm, Em)}Mm=1 and produces a
single combined classifier (µc, Ec). A simple tech-
nique would be to average the parameters of classi-
fiers into a new classifier. However, this ignores the
difference in feature distributions. Consider for ex-
ample that the weight associated with some word in
a source classifier has a value of 0. This could either
mean that the word is very rare or that it is neutral
for prediction (like the work “the”). The informa-
tion captured by the variance parameter allow us to
distinguish between the two cases: an high-variance
indicates a lack of confidence in the value of the
</bodyText>
<equation confidence="0.893634666666667">
arg max
yE{f11
αi =
</equation>
<page confidence="0.952937">
690
</page>
<bodyText confidence="0.997193333333333">
weight vectors because of small number of exam-
ples (first case), and vise-versa, small-variance indi-
cates that the value of the weight is based on plenty
of evidence. We favor combinations sensitive to this
distinction.
Since CW classifiers are Gaussian distributions,
we formalize classifier parameter combination as
finding a new distribution that minimizes the
weighted-divergence to a set of given distributions:
</bodyText>
<equation confidence="0.994514666666667">
M
(µc, Ec) = arg min X D((µc, Ec)||(µm, Em) i bm)
m
</equation>
<bodyText confidence="0.586942">
where (since E is diagonal),
</bodyText>
<equation confidence="0.9044585">
D((µc, Ec)||(µ, E) i b) =
PNf bfD((µcf,Ecf,f)||(µf,Ef,f)) .
</equation>
<bodyText confidence="0.99849075">
The (classifier specific) importance-weights bm E
RN + are used to weigh certain parameters of some
domains differently in the combination. When D is
the Euclidean distance (L2), we have,
</bodyText>
<equation confidence="0.9949135">
D((µcf, Ecf�f) ||(µf, Ef,f)) =
(µcf − µf) + (Ecf,f − Ef,f)2 .
</equation>
<bodyText confidence="0.864033">
and we obtain:
</bodyText>
<equation confidence="0.896244">
1 XM
µc f = bm
PM f µm f ,
m bm f m
M
1
Ecf,f = P bm f Em f,f . (1)
m∈M bm f m
</equation>
<bodyText confidence="0.999951333333333">
Note that this is a (weighted) average of parameters.
The other case we consider is when D is a weighted
KL divergence we obtain a weighting of µ by E−1:
</bodyText>
<sectionHeader confidence="0.995483" genericHeader="method">
5 Datasets
</sectionHeader>
<bodyText confidence="0.9647803125">
each user for task A and B respectively and 300 test
emails for each user.
The sentiment data contains product reviews from
Amazon for four product types: books,
elec-
tronics and kitchen appliances and we extended this
with three additional domains: apparel, music and
videos. We follow Blitzer et. al. for feature ex-
traction. We created different datasets by modify-
ing the decision boundary using the ordinal rating
of each instance (1-5 stars) and excluding boundary
instan
,
dvds,
ces. We use four versions of this data:
d DVDs
</bodyText>
<equation confidence="0.9982546">
!−1 M
X
m
(Emf,f)−1bf (Emf,f)−1µmf bf
M
µcf = X
m
ces per domain.
M M
(Ec)−1 = m
</equation>
<bodyText confidence="0.7713605">
as
= a
&gt;_ 0, where a is the initializa-
tion value for
We call this weighting
as opposed to a uniform weighting of param-
eters
= 1). We therefore have two combination
methods (L2 and KL) and two weighting methods
(uniform and varian
</bodyText>
<equation confidence="0.9257492">
bmf
−Emf,f
Emf,f.
“vari-
ance”
(bmf
ce).
!−1XMbm (Em f )−1bf m . (2)
fm
While each parameter is weighed by its variance in
</equation>
<bodyText confidence="0.968691666666667">
the KL, we can also explicitly encode this behavior
For evaluation we selected two domain adaptation
datasets: spam (Jiang and Zhai, 2007) and sentiment
(Blitzer et al., 2007). The spam data contains two
tasks, one with three users (task A) and one with 15
(task B). The goal is to classify an email (bag-of-
words) as either spam or ham (not-spam) and each
user may have slightly different preferences and fea-
tures. We used 700 and 100 training messages for
</bodyText>
<listItem confidence="0.9032976">
• All - 7 domains, one per product type
• Books - 3 domains of books with the binary
decision boundary set to 2, 3 and 4 stars
• DVDs -Same as Books but with DVD reviews
•Books+DVDs -Combined Books an
</listItem>
<bodyText confidence="0.999924714285714">
The All dataset captures the typical domain adap-
tation scenario, where each domain has the same
decision function but different features. Books
and DVDs have the opposite problem: the same
features but different classification boundaries.
Books+DVDs combines both issues. Experiments
use 1500 training and 100 test instan
</bodyText>
<sectionHeader confidence="0.995465" genericHeader="method">
6 Multi-Domain Adaptation
</sectionHeader>
<bodyText confidence="0.992646083333333">
We begin by examining the typical domain adapta-
tion scenario, but from an online perspective since
learning systems often must adapt to new users or
domains quickly and with no training data. For ex-
ample, aspam filter with separate classifiers trained
on each user must also classify mail for a new
user. Since other
training data may have been
deleted or be pri
user’s
vate, the existing classifiers must be
combined for the new user.
</bodyText>
<page confidence="0.988187">
691
</page>
<table confidence="0.999886266666667">
Target Domain All Src Target Train Avg Src Uniform L2 Uniform KL
Best Src Variance Variance
user0 3.85 1.80 4.80 8.26 5.25 4.63 4.53 4.32
user1 3.57 3.17 4.28 6.91 4.53 3.80 4.23 3.83
user2 3.30 2.40 3.77 5.75 4.75 4.60 4.93 4.67
Spam 12.32 12.02 14.12 21.15 14.03 13.18 13.50 13.48
apparel
books 16.85 18.95 22.95 25.76 19.58 18.63 19.53 19.05
dvd 13.65 17.40 17.30 21.89 15.53 13.73 14.48 14.15
Sentiment
kitchen 13.65 14.40 15.52 22.88 16.68 15.10 14.78 16.82
electronics 15.00 14.93 15.52 23.84 18.75 17.37 17.45
14.02
music 18.20 18.30 20.75 24.19 18.38 17.83 18.10 18.22
video 17.00 19.27 19.43 25.78 17.13 16.25 16.33 16.42
</table>
<tableCaption confidence="0.9910255">
Table 1: Test error for multi-source adaptation on sentiment and spam data. Combining classifiers improves over
selecting a single classifier a priori (Avg Src).
</tableCaption>
<bodyText confidence="0.999466933333333">
We combine the existing user-specific classifiers
into a single new classifier for a new user. Since
nothing is known about the new user (their deci-
sion function), each source classifier may be useful.
However, feature similarity – possibly measured us-
ing unlabeled data – could be used to weigh source
domains. Specifically, we combine the parameters
of each classifier according to their confidence us-
ing the combination methods described above.
We evaluated the four combination strategies – L2
vs. KL, uniform vs. variance – on spam and sen-
timent data. For each evaluation, a single domain
was held out for testing while separate classifiers
were trained on each source domain, i.e. no target
training. Source classifiers are then combined and
the combined classifier is evaluated on the test data
(400 instances) of the target domain. Each classi-
fier was trained for 5 iterations over the training data
(to ensure convergence) and each experiment was
repeated using 10-fold cross validation. The CW
parameter 0 was tuned on a single randomized run
for each experiment. We include several baselines:
training on target data to obtain an upper bound
on performance (Target), training on all source do-
mains together, a useful strategy if all source data is
maintained (All Src), selecting (with omniscience)
the best performing source classifier on target data
(Best Src), and the expected real world performance
of randomly selecting a source classifier (Avg Src).
While at least one source classifier achieved high
performance on the target domain (Best Src), the
correct source classifier cannot be selected without
target data and selecting a random source classifier
yields high error. In contrast, a combined classifier
almost always improved over the best source domain
classifier (table 1). That some of our results improve
over the best training scenario is likely caused by in-
creased training data from using multiple domains.
Increases over all available training data are very in-
teresting and may be due to a regularization effect of
training separate models.
The L2 methods performed best and KL improved
7 out of 10 combinations. Classifier parameter com-
bination can clearly yield good classifiers without
prior knowledge of the target domain.
</bodyText>
<sectionHeader confidence="0.989433" genericHeader="method">
7 Learning Across Domains
</sectionHeader>
<bodyText confidence="0.99981725">
In addition to adapting to new domains, multi-
domain systems should learn common behaviors
across domains. Naively, we can assume that the
domains are either sufficiently similar to warrant
one classifier or different enough for separate clas-
sifiers. The reality is often more complex. Instead,
we maintain shared and domain-specific parameters
and combine them for learning and prediction.
Multi-task learning aims to learn common behav-
iors across related problems, a similar goal to multi-
domain learning. The primary difference is the na-
ture of the domains/tasks: in our setting each domain
is the same task but differs in the types of features in
addition to the decision function. A multi-task ap-
proach can be adapted to our setting by using our
classifier combination techniques.
</bodyText>
<page confidence="0.986214">
692
</page>
<table confidence="0.9990257">
Method Spam Task B Books DVD Sentiment All
Task A Books+DVD
Single 3.88 8.75 23.7 25.11 23.26 16.57
Separate 5.46 14.53 22.22 21.64 21.23 21.89
Feature Splitting 4.16 8.93 15.65 16.20 14.60 17.45
MDR 4.09 9.18 15.65 15.12 13.76 17.45
MDR+L2 4.27 8.61 12.70 14.95 12.73 17.16
MDR+L2-Var 3.75 7.52 12.90 14.21 12.52 17.37
MDR+KL 4.32 9.22 13.51 13.81 13.32 17.20
MDR+KL-Var 4.02 8.70 14.93 14.03 14.22 18.40
</table>
<tableCaption confidence="0.992089">
Table 2: Online training error for learning across domains.
</tableCaption>
<table confidence="0.9997896">
Method Spam Task B Books DVD Sentiment All
Task A Books+DVD
Single 2.11 5.60 18.43 18.67 19.08 14.09
Separate 2.43 8.5 18.87 15.97 16.45 17.23
Feature Splitting 1.94 5.51 9.97 9.70 9.05 14.73
MDR 1.94 5.69 9.97 8.33 8.20 14.73
MDR+L2 1.87 5.16 6.63 7.97 7.62 14.20
MDR+L2-Var 1.90 4.78 6.40 7.83 7.30 14.33
MDR+KL 1.94 5.61 8.37 7.07 8.43 14.60
MDR+KL-Var 1.97 5.46 9.40 7.50 8.05 15.50
</table>
<tableCaption confidence="0.999867">
Table 3: Test data error: learning across domains (MDR) improves over the baselines and Daum´e (2007).
</tableCaption>
<bodyText confidence="0.999386382352941">
We seek to learn domain specific parameters
guided by shared parameters. Dekel et al. (2006)
followed this approach for an online multi-task algo-
rithm, although they did not have shared parameters
and assumed that a training round comprised an ex-
ample from each task. Evgeniou and Pontil (2004)
achieved a similar goal by using shared parameters
for multi-task regularization. Specifically, they as-
sumed that the weight vector for problem d could be
represented as wc = wd+ws, where wd are task spe-
cific parameters and ws are shared across all tasks.
In this framework, all tasks are close to some under-
lying mean ws and each one deviates from this mean
by wd. Their SVM style multi-task objective mini-
mizes the loss of wc and the norm of wd and ws, with
a tradeoff parameter allowing for domain deviance
from the mean. The simple domain adaptation al-
gorithm of feature splitting used by Daum´e (2007)
is a special case of this model where the norms are
equally weighted. An analogous CW objective is:
(µd, Ed) are the parameters for domain d, (µs, Es)
for the shared classifier and (µc, Ec) for the com-
bination of the domain and shared classifiers. The
parameters are combined via (2) with only two ele-
ments summed - one for the shared parameters s and
the other for the domain parameters d . This captures
the intuition of Evgeniou and Pontil: updates en-
force the learning condition on the combined param-
eters and minimize parameter change. For conve-
nience, we rewrite A2 = 2 - 2A1, where A1 E [0, 1].
If classifiers are combined using the sum of the indi-
vidual weight vectors and A1 = 0.5, this is identical
to feature splitting (Daum´e) for CW classifiers.
The domain specific and shared classifiers can be
</bodyText>
<figure confidence="0.987071333333333">
1
min
A1
A 2
s.t. Pr.,,,-N(µc,Ec) [yi (w - xi) ? 0] ? 77 . (3)
DKL (Al (µd, Ed) II Al (µdi, Ed))
) i
1
+ DKL (Al (µs, Es) II Al (µsi , Esi))
</figure>
<page confidence="0.997275">
693
</page>
<bodyText confidence="0.979949">
updated using the closed form solution to (3) as:
</bodyText>
<equation confidence="0.9983398">
µs = µsi + A2αyiΣcxi
(Σs)−1 = (Σsi)−1 + 2A2αOxixTi
µd = µdi + A1αyiΣcixi
(Σd)−1 = (Σdi )−1 + 2A1αoxixTi
(4)
</equation>
<bodyText confidence="0.999986108108108">
We call this objective Multi-Domain Regulariza-
tion (MDR). As before, the combined parameters
are produced by one of the combination methods.
On each round, the algorithm receives instance xi
and domain di for which it creates a combined clas-
sifier (µc, Σc) using the shared (µs, Σs) and domain
specific parameters (µd, Σd). A prediction is is-
sued using the standard linear classifier prediction
rule sign(µc · x) and updates follow (4). The ef-
fect is that features similar across domains quickly
converge in the shared classifier, sharing informa-
tion across domains. The combined classifier re-
flects shared and domain specific parameter confi-
dences: weights with low variance (i.e. greater con-
fidence) will contribute more.
We evaluate MDR on a single pass over a stream
of instances from multiple domains, simulating a
real world setting. Parameters A1 and 0 are iter-
atively optimized on a single randomized run for
each dataset. All experiments use 10-fold CV. In ad-
dition to evaluating the four combination methods
with MDR, we evaluate the performance of a sin-
gle classifier trained on all domains (Single), a sep-
arate classifier trained on each domain (Separate),
Feature Splitting (Daum´e) and feature splitting with
optimized A1 (MDR). Table 3 shows results on test
data and table 2 shows online training error.
In this setting, L2 combinations prove best on 5
of 6 datasets, with the variance weighted combina-
tion doing the best. MDR (optimizing A1) slightly
improves over feature splitting, and the combination
methods improve in every case. Our best result is
statistically significant compared to Feature Split-
ting using McNemar’s test (p = .001) for Task B,
Books, DVD, Books+DVD. While a single or sepa-
rate classifiers have a different effect on each dataset,
MDR gives the best performance overall.
</bodyText>
<sectionHeader confidence="0.726846" genericHeader="method">
8 Learning in Many Domains
</sectionHeader>
<bodyText confidence="0.999991543478261">
So far we have considered settings with a small
number of similar domains. While this is typical
of multi-task problems, real world settings present
many domains which do not all share the same be-
haviors. Online algorithms scale to numerous ex-
amples and we desire the same behavior for numer-
ous domains. Consider a spam filter used by a large
email provider, which filters billions of emails for
millions of users. Suppose that spammers control
many accounts and maliciously label spam as legiti-
mate. Alternatively, subsets of users may share pref-
erences. Since behaviors are not consistent across
domains, shared parameters cannot be learned. We
seek algorithms robust to this behavior.
Since subsets of users share behaviors, these can
be learned using our MDR framework. For example,
discovering spammer and legitimate mail accounts
would enable intra-group learning. The challenge is
the online discovery of these subsets while learning
model parameters. We augment the MDR frame-
work to additionally learn this mapping.
We begin by generalizing MDR to include k
shared classifiers instead of a single set of shared pa-
rameters. Each set of shared parameters represents
a different subset of domains. If the corresponding
shared parameters are known for a domain, we could
use the same objective (3) and update (4) as before.
If there are many fewer shared parameters than do-
mains (k « D), we can benefit from multi-domain
learning. Next, we augment the learning algorithm
to learn a mapping between the domains and shared
classifiers. Intuitively, a domain should be mapped
to shared parameters that correctly classify that do-
main. A common technique for learning such ex-
perts in the Weighted Majority algorithm (Little-
stone and Warmuth, 1994), which weighs a mixture
of experts (classifiers). However, since we require a
hard assignment — pick a single shared parameter
set s — rather than a mixture, the algorithm reduces
to picking the classifier s with the fewest mistakes
in predicting domain d. This requires tracking the
number of mistakes made by each shared classifier
on each domain once a label is revealed. For learn-
ing, the shared classifier with the fewest mistakes
for a domain is selected for an MDR update. Clas-
sifier ties are broken randomly. While we experi-
</bodyText>
<page confidence="0.998068">
694
</page>
<figureCaption confidence="0.9999975">
Figure 1: Learning across many domains - spam (left) and sentiment (right) - with MDR using k shared classifiers.
Figure 2: Learning across many domains - spam (left) and sentiment (right) - with no domain specific parameters.
</figureCaption>
<bodyText confidence="0.99994221875">
mented with more complex techniques, this simple
method worked well in practice. When a new do-
main is added to the system, it takes fewer exam-
ples to learn which shared classifier to use instead of
learning a new model from scratch.
While this approach adds another free parameter
(k) that can be set using development data, we ob-
serve that k can instead be fixed to a large constant.
Since only a single shared classifier is updated each
round, the algorithm will favor selecting a previ-
ously used classifier as opposed to a new one, using
as many classifiers as needed but not scaling up to k.
This may not be optimal, but it is a simple.
To evaluate a larger number of domains, we cre-
ated many varying domains using spam and senti-
ment data. For spam, 6 email users were created by
splitting the 3 task A users into 2 users, and flipping
the label of one of these users (a malicious user),
yielding 400 train and 100 test emails per user. For
sentiment, the book domain was split into 3 groups
with binary boundaries at a rating of 2, 3 or 4. Each
of these groups was split into 8 groups of which half
had their labels flipped, creating 24 domains. The
same procedure was repeated for DVD reviews but
for a decision boundary of 3, 6 groups were created,
and for a boundary of 2 and 4, 3 groups were created
with 1 and 2 domains flipped respectively, resulting
in 12 DVD domains and 36 total domains with var-
ious decision boundaries, features, and inverted de-
cision functions. Each domain used 300 train and
100 test instances. 10-fold cross validation with one
training iteration was used to train models on these
</bodyText>
<page confidence="0.997195">
695
</page>
<bodyText confidence="0.999977925925926">
two datasets. Parameters were optimized as before.
Experiments were repeated for various settings of
k. Since L2 performed well before, we evaluated
MDR+L2 and MDR+L2-Var.
The results are shown in figure 1. For both spam
and sentiment adding additional shared parameters
beyond the single shared classifier significantly re-
duces error, with further reductions as k increases.
This yields a 45% error reduction for spam and a
38% reduction for sentiment over the best baseline.
While each task has an optimal k (about 5 for spam,
2 for sentiment), larger values still achieve low error,
indicating the flexibility of using large k values.
While adding parameters clearly helps for many
domains, it may be impractical to keep domain-
specific classifiers for thousands or millions of do-
mains. In this case, we could eliminate the domain-
specific classifiers and rely on the k shared clas-
sifiers only, learning the domain to classifier map-
ping. We compare this approach using the best result
from MDR above, again varying k. Figure 2 shows
that losing domain-specific parameters hurts perfor-
mance, but is still an improvement over baseline
methods. Additionally, we can expect better perfor-
mance as the number of similar domains increases.
This may be an attractive alternative to keeping a
very large number of parameters.
</bodyText>
<sectionHeader confidence="0.999887" genericHeader="related work">
9 Related Work
</sectionHeader>
<bodyText confidence="0.999961254901961">
Multi-domain learning intersects two areas of re-
search: domain adaptation and multi-task learning.
In domain adaptation, a classifier trained for a source
domain is transfered to a target domain using either
unlabeled or a small amount of labeled target data.
Blitzer et al. (2007) used structural correspondence
learning to train a classifier on source data with
new features induced from target unlabeled data. In
a complimentary approach, Jiang and Zhai (2007)
weighed training instances based on their similarity
to unlabeled target domain data. Several approaches
utilize source data for training on a limited number
of target labels, including feature splitting (Daum´e,
2007) and adding the source classifier’s prediction
as a feature (Chelba and Acero, 2004). Others have
considered transfer learning, in which an existing
domain is used to improve learning in a new do-
main, such as constructing priors (Raina et al., 2006;
Marx et al., 2008) and learning parameter functions
for text classification from related data (Do and Ng,
2006). These methods largely require batch learn-
ing, unlabeled target data, or available source data
at adaptation. In contrast, our algorithms operate
purely online and can be applied when no target data
is available.
Multi-task algorithms, also known as inductive
transfer, learn a set of related problems simultane-
ously (Caruana, 1997). The most relevant approach
is that of Regularized Multi-Task Learning (Evge-
niou and Pontil, 2004), which we use to motivate
our online algorithm. Dekel et al. (2006) gave a sim-
ilar online approach but did not use shared parame-
ters and assumed multiple instances for each round.
We generalize this work to both include an arbi-
trary classifier combination and many shared classi-
fiers. Some multi-task work has also considered the
grouping of tasks similar to our learning of domain
subgroups (Thrun and O’Sullivan, 1998; Bakker and
Heskes, 2003).
There are many techniques for combining the out-
put of multiple classifiers for ensemble learning or
mixture of experts. Kittler et al. (Mar 1998) provide
a theoretical framework for combining classifiers.
Some empirical work has considered adding versus
multiplying classifier output (Tax et al., 2000), using
local accuracy estimates for combination (Woods et
al., 1997), and applications to NLP tasks (Florian et
al., 2003). However, these papers consider combin-
ing classifier output for prediction. In contrast, we
consider parameter combination for both prediction
and learning.
</bodyText>
<sectionHeader confidence="0.993105" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999987833333333">
We have explored several multi-domain learning
settings using CW classifiers and a combination
method. Our approach creates a better classifier for
a new target domain than selecting a random source
classifier a prior, reduces learning error on multiple
domains compared to baseline approaches, can han-
dle many disparate domains by using many shared
classifiers, and scales to a very large number of do-
mains with a small performance reduction. These
scenarios are realistic for NLP systems in the wild.
This work also raises some questions about learning
on large numbers of disparate domains: can a hi-
</bodyText>
<page confidence="0.99672">
696
</page>
<bodyText confidence="0.9995938">
erarchical online clustering yield a better represen-
tation than just selecting between k shared parame-
ters? Additionally, how can prior knowledge about
domain similarity be included into the combination
methods? We plan to explore these questions in fu-
ture work.
Acknowledgements This material is based upon
work supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. FA8750-07-D-0185.
</bodyText>
<sectionHeader confidence="0.999154" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999733567164179">
B. Bakker and T. Heskes. 2003. Task clustering and gat-
ing for bayesian multi–task learning. Journal of Ma-
chine Learning Research, 4:83–99.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In As-
sociation for Computational Linguistics (ACL).
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28:41–75.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
max- imum entropy classifier: Little data can help a
lot. In Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551–585.
Hal Daum´e. 2007. Frustratingly easy domain adaptation.
In Association for Computational Linguistics (ACL).
Ofer Dekel, Philip M. Long, and Yoram Singer. 2006.
Online multitask learning. In Conference on Learning
Theory (COLT).
Chuong B. Do and Andrew Ng. 2006. Transfer learning
for text classification. In Advances in Neural Informa-
tion Processing Systems (NIPS).
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification.
In International Conference on Machine Learning
(ICML).
Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi-task learning. In Conference on
Knowledge Discovery and Data Mining (KDD).
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recognition through clas-
sifier combination. In Conference on Computational
Natural Language Learning (CONLL).
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Association for
Computational Linguistics (ACL).
J. Kittler, M. Hatef, R.P.W. Duin, and J. Matas. Mar
1998. On combining classifiers. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
20(3):226–239.
N. Littlestone and M. K. Warmuth. 1994. The weighted
majority algorithm. Information and Computation,
108:212–261.
Zvika Marx, Michael T. Rosenstein, Thomas G. Diet-
terich, and Leslie Pack Kaelbling. 2008. Two algo-
rithms for transfer learning. In Inductive Transfer: 10
years later.
Rajat Raina, Andrew Ng, and Daphne Koller. 2006.
Constructing informative priors using transfer learn-
ing. In International Conference on Machine Learn-
ing (ICML).
David M. J. Tax, Martijn van Breukelen, Robert P. W.
Duina, and Josef Kittler. 2000. Combining multiple
classifiers by averaging or by multiplying? Pattern
Recognition, 33(9):1475–1485, September.
S. Thrun and J. O’Sullivan. 1998. Clustering learning
tasks and the selective cross–task transfer of knowl-
edge. In S. Thrun and L.Y. Pratt, editors, Learning To
Learn. Kluwer Academic Publishers.
Kevin Woods, W. Philip Kegelmeyer Jr., and Kevin
Bowyer. 1997. Combination of multiple classifiers
using local accuracy estimates. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 19(4):405–
410.
</reference>
<page confidence="0.997706">
697
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.760591">
<title confidence="0.998578">Online Methods for Multi-Domain Learning and Adaptation</title>
<author confidence="0.805307">Dredze</author>
<affiliation confidence="0.999566">Department of Computer and Information University of</affiliation>
<address confidence="0.997703">Philadelphia, PA 19104</address>
<abstract confidence="0.996199857142857">NLP tasks are often domain specific, yet systems can learn behaviors across multiple domains. We develop a new multi-domain online learning framework based on parameter combination from multiple classifiers. Our algorithms draw from multi-task learning and domain adaptation to adapt multiple source domain classifiers to a new target domain, learn across multiple similar domains, and learn across a large number of disparate domains. We evaluate our algorithms on two popular NLP domain adaptation tasks: sentiment classification and spam filtering.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Bakker</author>
<author>T Heskes</author>
</authors>
<title>Task clustering and gating for bayesian multi–task learning.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>4--83</pages>
<contexts>
<context position="27168" citStr="Bakker and Heskes, 2003" startWordPosition="4498" endWordPosition="4501">uctive transfer, learn a set of related problems simultaneously (Caruana, 1997). The most relevant approach is that of Regularized Multi-Task Learning (Evgeniou and Pontil, 2004), which we use to motivate our online algorithm. Dekel et al. (2006) gave a similar online approach but did not use shared parameters and assumed multiple instances for each round. We generalize this work to both include an arbitrary classifier combination and many shared classifiers. Some multi-task work has also considered the grouping of tasks similar to our learning of domain subgroups (Thrun and O’Sullivan, 1998; Bakker and Heskes, 2003). There are many techniques for combining the output of multiple classifiers for ensemble learning or mixture of experts. Kittler et al. (Mar 1998) provide a theoretical framework for combining classifiers. Some empirical work has considered adding versus multiplying classifier output (Tax et al., 2000), using local accuracy estimates for combination (Woods et al., 1997), and applications to NLP tasks (Florian et al., 2003). However, these papers consider combining classifier output for prediction. In contrast, we consider parameter combination for both prediction and learning. 10 Conclusion W</context>
</contexts>
<marker>Bakker, Heskes, 2003</marker>
<rawString>B. Bakker and T. Heskes. 2003. Task clustering and gating for bayesian multi–task learning. Journal of Machine Learning Research, 4:83–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4094" citStr="Blitzer et al., 2007" startWordPosition="608" endWordPosition="611">sider domains that are labeled with different classification functions. For example, one user may enjoy some emails that another user considers spam: differing in their classification function. The goal of multi-task learning is to generalize across tasks/domains (Dekel et al., 2006; Evgeniou and Pontil, 2004). Furthermore, as in domain adaptation, some examples are draw from different distributions. For example, one user may receive emails about engineering while another about art, differing in their distribution over features. Domain adaptation deals with these feature distribution changes (Blitzer et al., 2007; Jiang and Zhai, 2007). Our work combines these two areas by learning both across distributions and behaviors or functions. 3 Confidence-Weighted Linear Classifiers Confidence-weighted (CW) linear classification (Dredze et al., 2008), a new online algorithm, maintains a probabilistic measure of parameter confidence, which may be useful in combining parameters from different domain distributions. We summarize CW learning to familiarize the reader. Parameter confidence is formalized by a Gaussian distribution over weight vectors with mean µ E RN and diagonal covariance E E RN,N. The values µj a</context>
<context position="9836" citStr="Blitzer et al., 2007" startWordPosition="1607" endWordPosition="1610"> data: d DVDs !−1 M X m (Emf,f)−1bf (Emf,f)−1µmf bf M µcf = X m ces per domain. M M (Ec)−1 = m as = a &gt;_ 0, where a is the initialization value for We call this weighting as opposed to a uniform weighting of parameters = 1). We therefore have two combination methods (L2 and KL) and two weighting methods (uniform and varian bmf −Emf,f Emf,f. “variance” (bmf ce). !−1XMbm (Em f )−1bf m . (2) fm While each parameter is weighed by its variance in the KL, we can also explicitly encode this behavior For evaluation we selected two domain adaptation datasets: spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al., 2007). The spam data contains two tasks, one with three users (task A) and one with 15 (task B). The goal is to classify an email (bag-ofwords) as either spam or ham (not-spam) and each user may have slightly different preferences and features. We used 700 and 100 training messages for • All - 7 domains, one per product type • Books - 3 domains of books with the binary decision boundary set to 2, 3 and 4 stars • DVDs -Same as Books but with DVD reviews •Books+DVDs -Combined Books an The All dataset captures the typical domain adaptation scenario, where each domain has the same decision function but</context>
<context position="25528" citStr="Blitzer et al. (2007)" startWordPosition="4242" endWordPosition="4245">R above, again varying k. Figure 2 shows that losing domain-specific parameters hurts performance, but is still an improvement over baseline methods. Additionally, we can expect better performance as the number of similar domains increases. This may be an attractive alternative to keeping a very large number of parameters. 9 Related Work Multi-domain learning intersects two areas of research: domain adaptation and multi-task learning. In domain adaptation, a classifier trained for a source domain is transfered to a target domain using either unlabeled or a small amount of labeled target data. Blitzer et al. (2007) used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data. In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data. Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daum´e, 2007) and adding the source classifier’s prediction as a feature (Chelba and Acero, 2004). Others have considered transfer learning, in which an existing domain is used to improve learning in a new domain</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich Caruana</author>
</authors>
<date>1997</date>
<booktitle>Multitask learning. Machine Learning,</booktitle>
<pages>28--41</pages>
<contexts>
<context position="26623" citStr="Caruana, 1997" startWordPosition="4411" endWordPosition="4412">004). Others have considered transfer learning, in which an existing domain is used to improve learning in a new domain, such as constructing priors (Raina et al., 2006; Marx et al., 2008) and learning parameter functions for text classification from related data (Do and Ng, 2006). These methods largely require batch learning, unlabeled target data, or available source data at adaptation. In contrast, our algorithms operate purely online and can be applied when no target data is available. Multi-task algorithms, also known as inductive transfer, learn a set of related problems simultaneously (Caruana, 1997). The most relevant approach is that of Regularized Multi-Task Learning (Evgeniou and Pontil, 2004), which we use to motivate our online algorithm. Dekel et al. (2006) gave a similar online approach but did not use shared parameters and assumed multiple instances for each round. We generalize this work to both include an arbitrary classifier combination and many shared classifiers. Some multi-task work has also considered the grouping of tasks similar to our learning of domain subgroups (Thrun and O’Sullivan, 1998; Bakker and Heskes, 2003). There are many techniques for combining the output of</context>
</contexts>
<marker>Caruana, 1997</marker>
<rawString>Rich Caruana. 1997. Multitask learning. Machine Learning, 28:41–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of max- imum entropy classifier: Little data can help a lot.</title>
<date>2004</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="26013" citStr="Chelba and Acero, 2004" startWordPosition="4313" endWordPosition="4316">or a source domain is transfered to a target domain using either unlabeled or a small amount of labeled target data. Blitzer et al. (2007) used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data. In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data. Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daum´e, 2007) and adding the source classifier’s prediction as a feature (Chelba and Acero, 2004). Others have considered transfer learning, in which an existing domain is used to improve learning in a new domain, such as constructing priors (Raina et al., 2006; Marx et al., 2008) and learning parameter functions for text classification from related data (Do and Ng, 2006). These methods largely require batch learning, unlabeled target data, or available source data at adaptation. In contrast, our algorithms operate purely online and can be applied when no target data is available. Multi-task algorithms, also known as inductive transfer, learn a set of related problems simultaneously (Caru</context>
</contexts>
<marker>Chelba, Acero, 2004</marker>
<rawString>Ciprian Chelba and Alex Acero. 2004. Adaptation of max- imum entropy classifier: Little data can help a lot. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="5320" citStr="Crammer et al., 2006" startWordPosition="812" endWordPosition="815">,j represent knowledge of and confidence in the parameter for feature j. The smaller Ej,j, the more confidence we have in the mean parameter value µj. In this work we consider diagonal covariance matrices to scale to NLP data. A model predicts the highest probability label, Prw-N(µ,Σ) [yi(w · xi) &gt;— 0] . The Gaussian distribution over parameter vectors w induces a univariate Gaussian distribution over the score Si = w · xi parameterized by µ, E and the instance xi: Si — N fµi, σD, with mean µi = µ·xi and variance σ2i= xi Exi. The CW algorithm is inspired by the Passive Aggressive (PA) update (Crammer et al., 2006) — which ensures a positive margin while minimizing parameter change. CW replaces the Euclidean distance used in the PA update with the KullbackLeibler (KL) divergence over Gaussian distributions. It also replaces the minimal margin constraint with a minimal probability constraint: with some given probability η E (0.5, 1] a drawn classifier will assign the correct label. This strategy yields the following objective solved on each round of learning: min DKL (N (µ, E) II N (µi, Ei)) s.t. Pr [yi (w · xi) &gt;— 0] &gt;— η , where (µi, Ei) are the parameters on round i and w — N (µ, E). The constraint en</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e. 2007. Frustratingly easy domain adaptation. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ofer Dekel</author>
<author>Philip M Long</author>
<author>Yoram Singer</author>
</authors>
<title>Online multitask learning.</title>
<date>2006</date>
<booktitle>In Conference on Learning Theory (COLT).</booktitle>
<contexts>
<context position="3757" citStr="Dekel et al., 2006" startWordPosition="557" endWordPosition="560">hared classifiers to combine. Multi-domain learning combines properties of both multi-task learning and domain adaptation. As 689 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 689–697, Honolulu, October 2008.c�2008 Association for Computational Linguistics in multi-task learning, we consider domains that are labeled with different classification functions. For example, one user may enjoy some emails that another user considers spam: differing in their classification function. The goal of multi-task learning is to generalize across tasks/domains (Dekel et al., 2006; Evgeniou and Pontil, 2004). Furthermore, as in domain adaptation, some examples are draw from different distributions. For example, one user may receive emails about engineering while another about art, differing in their distribution over features. Domain adaptation deals with these feature distribution changes (Blitzer et al., 2007; Jiang and Zhai, 2007). Our work combines these two areas by learning both across distributions and behaviors or functions. 3 Confidence-Weighted Linear Classifiers Confidence-weighted (CW) linear classification (Dredze et al., 2008), a new online algorithm, mai</context>
<context position="16022" citStr="Dekel et al. (2006)" startWordPosition="2621" endWordPosition="2624">g error for learning across domains. Method Spam Task B Books DVD Sentiment All Task A Books+DVD Single 2.11 5.60 18.43 18.67 19.08 14.09 Separate 2.43 8.5 18.87 15.97 16.45 17.23 Feature Splitting 1.94 5.51 9.97 9.70 9.05 14.73 MDR 1.94 5.69 9.97 8.33 8.20 14.73 MDR+L2 1.87 5.16 6.63 7.97 7.62 14.20 MDR+L2-Var 1.90 4.78 6.40 7.83 7.30 14.33 MDR+KL 1.94 5.61 8.37 7.07 8.43 14.60 MDR+KL-Var 1.97 5.46 9.40 7.50 8.05 15.50 Table 3: Test data error: learning across domains (MDR) improves over the baselines and Daum´e (2007). We seek to learn domain specific parameters guided by shared parameters. Dekel et al. (2006) followed this approach for an online multi-task algorithm, although they did not have shared parameters and assumed that a training round comprised an example from each task. Evgeniou and Pontil (2004) achieved a similar goal by using shared parameters for multi-task regularization. Specifically, they assumed that the weight vector for problem d could be represented as wc = wd+ws, where wd are task specific parameters and ws are shared across all tasks. In this framework, all tasks are close to some underlying mean ws and each one deviates from this mean by wd. Their SVM style multi-task obje</context>
<context position="26790" citStr="Dekel et al. (2006)" startWordPosition="4436" endWordPosition="4439">, 2006; Marx et al., 2008) and learning parameter functions for text classification from related data (Do and Ng, 2006). These methods largely require batch learning, unlabeled target data, or available source data at adaptation. In contrast, our algorithms operate purely online and can be applied when no target data is available. Multi-task algorithms, also known as inductive transfer, learn a set of related problems simultaneously (Caruana, 1997). The most relevant approach is that of Regularized Multi-Task Learning (Evgeniou and Pontil, 2004), which we use to motivate our online algorithm. Dekel et al. (2006) gave a similar online approach but did not use shared parameters and assumed multiple instances for each round. We generalize this work to both include an arbitrary classifier combination and many shared classifiers. Some multi-task work has also considered the grouping of tasks similar to our learning of domain subgroups (Thrun and O’Sullivan, 1998; Bakker and Heskes, 2003). There are many techniques for combining the output of multiple classifiers for ensemble learning or mixture of experts. Kittler et al. (Mar 1998) provide a theoretical framework for combining classifiers. Some empirical </context>
</contexts>
<marker>Dekel, Long, Singer, 2006</marker>
<rawString>Ofer Dekel, Philip M. Long, and Yoram Singer. 2006. Online multitask learning. In Conference on Learning Theory (COLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuong B Do</author>
<author>Andrew Ng</author>
</authors>
<title>Transfer learning for text classification.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="26290" citStr="Do and Ng, 2006" startWordPosition="4359" endWordPosition="4362">tary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data. Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daum´e, 2007) and adding the source classifier’s prediction as a feature (Chelba and Acero, 2004). Others have considered transfer learning, in which an existing domain is used to improve learning in a new domain, such as constructing priors (Raina et al., 2006; Marx et al., 2008) and learning parameter functions for text classification from related data (Do and Ng, 2006). These methods largely require batch learning, unlabeled target data, or available source data at adaptation. In contrast, our algorithms operate purely online and can be applied when no target data is available. Multi-task algorithms, also known as inductive transfer, learn a set of related problems simultaneously (Caruana, 1997). The most relevant approach is that of Regularized Multi-Task Learning (Evgeniou and Pontil, 2004), which we use to motivate our online algorithm. Dekel et al. (2006) gave a similar online approach but did not use shared parameters and assumed multiple instances for</context>
</contexts>
<marker>Do, Ng, 2006</marker>
<rawString>Chuong B. Do and Andrew Ng. 2006. Transfer learning for text classification. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Confidence-weighted linear classification.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="1603" citStr="Dredze et al., 2008" startWordPosition="221" endWordPosition="224">cures differences and separate classifiers ignore similarities. Similarly, adding new domains to existing systems requires adapting existing classifiers. We present new online algorithms for three multidomain learning scenarios: adapting existing classifiers to new domains, learning across multiple similar domains and scaling systems to many disparate domains. Multi-domain learning combines characteristics of both multi-task learning and domain adaptation and drawing from both areas, we develop a multi-classifier parameter combination technique for confidence-weighted (CW) linear classifiers (Dredze et al., 2008). We focus on online algorithms that scale to large amounts of data. Next, we describe multi-domain learning and review the CW algorithm. We then consider our three settings using multi-classifier parameter combination. We conclude with related work. 2 Multi-Domain Learning In online multi-domain learning, each instance x is drawn from a domain d specific distribution x — Dd over a vectors space RN and labeled with a domain specific function fd with label y E {−1, +1} (for binary classification.) On round i the classifier receives instance xi and domain identifier di and predicts label yi E {−</context>
<context position="4328" citStr="Dredze et al., 2008" startWordPosition="639" endWordPosition="642">neralize across tasks/domains (Dekel et al., 2006; Evgeniou and Pontil, 2004). Furthermore, as in domain adaptation, some examples are draw from different distributions. For example, one user may receive emails about engineering while another about art, differing in their distribution over features. Domain adaptation deals with these feature distribution changes (Blitzer et al., 2007; Jiang and Zhai, 2007). Our work combines these two areas by learning both across distributions and behaviors or functions. 3 Confidence-Weighted Linear Classifiers Confidence-weighted (CW) linear classification (Dredze et al., 2008), a new online algorithm, maintains a probabilistic measure of parameter confidence, which may be useful in combining parameters from different domain distributions. We summarize CW learning to familiarize the reader. Parameter confidence is formalized by a Gaussian distribution over weight vectors with mean µ E RN and diagonal covariance E E RN,N. The values µj and Ej,j represent knowledge of and confidence in the parameter for feature j. The smaller Ej,j, the more confidence we have in the mean parameter value µj. In this work we consider diagonal covariance matrices to scale to NLP data. A </context>
</contexts>
<marker>Dredze, Crammer, Pereira, 2008</marker>
<rawString>Mark Dredze, Koby Crammer, and Fernando Pereira. 2008. Confidence-weighted linear classification. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theodoros Evgeniou</author>
<author>Massimiliano Pontil</author>
</authors>
<title>Regularized multi-task learning.</title>
<date>2004</date>
<booktitle>In Conference on Knowledge Discovery and Data Mining (KDD).</booktitle>
<contexts>
<context position="3785" citStr="Evgeniou and Pontil, 2004" startWordPosition="561" endWordPosition="564"> combine. Multi-domain learning combines properties of both multi-task learning and domain adaptation. As 689 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 689–697, Honolulu, October 2008.c�2008 Association for Computational Linguistics in multi-task learning, we consider domains that are labeled with different classification functions. For example, one user may enjoy some emails that another user considers spam: differing in their classification function. The goal of multi-task learning is to generalize across tasks/domains (Dekel et al., 2006; Evgeniou and Pontil, 2004). Furthermore, as in domain adaptation, some examples are draw from different distributions. For example, one user may receive emails about engineering while another about art, differing in their distribution over features. Domain adaptation deals with these feature distribution changes (Blitzer et al., 2007; Jiang and Zhai, 2007). Our work combines these two areas by learning both across distributions and behaviors or functions. 3 Confidence-Weighted Linear Classifiers Confidence-weighted (CW) linear classification (Dredze et al., 2008), a new online algorithm, maintains a probabilistic measu</context>
<context position="16224" citStr="Evgeniou and Pontil (2004)" startWordPosition="2654" endWordPosition="2657">ng 1.94 5.51 9.97 9.70 9.05 14.73 MDR 1.94 5.69 9.97 8.33 8.20 14.73 MDR+L2 1.87 5.16 6.63 7.97 7.62 14.20 MDR+L2-Var 1.90 4.78 6.40 7.83 7.30 14.33 MDR+KL 1.94 5.61 8.37 7.07 8.43 14.60 MDR+KL-Var 1.97 5.46 9.40 7.50 8.05 15.50 Table 3: Test data error: learning across domains (MDR) improves over the baselines and Daum´e (2007). We seek to learn domain specific parameters guided by shared parameters. Dekel et al. (2006) followed this approach for an online multi-task algorithm, although they did not have shared parameters and assumed that a training round comprised an example from each task. Evgeniou and Pontil (2004) achieved a similar goal by using shared parameters for multi-task regularization. Specifically, they assumed that the weight vector for problem d could be represented as wc = wd+ws, where wd are task specific parameters and ws are shared across all tasks. In this framework, all tasks are close to some underlying mean ws and each one deviates from this mean by wd. Their SVM style multi-task objective minimizes the loss of wc and the norm of wd and ws, with a tradeoff parameter allowing for domain deviance from the mean. The simple domain adaptation algorithm of feature splitting used by Daum´e</context>
<context position="26722" citStr="Evgeniou and Pontil, 2004" startWordPosition="4423" endWordPosition="4427">improve learning in a new domain, such as constructing priors (Raina et al., 2006; Marx et al., 2008) and learning parameter functions for text classification from related data (Do and Ng, 2006). These methods largely require batch learning, unlabeled target data, or available source data at adaptation. In contrast, our algorithms operate purely online and can be applied when no target data is available. Multi-task algorithms, also known as inductive transfer, learn a set of related problems simultaneously (Caruana, 1997). The most relevant approach is that of Regularized Multi-Task Learning (Evgeniou and Pontil, 2004), which we use to motivate our online algorithm. Dekel et al. (2006) gave a similar online approach but did not use shared parameters and assumed multiple instances for each round. We generalize this work to both include an arbitrary classifier combination and many shared classifiers. Some multi-task work has also considered the grouping of tasks similar to our learning of domain subgroups (Thrun and O’Sullivan, 1998; Bakker and Heskes, 2003). There are many techniques for combining the output of multiple classifiers for ensemble learning or mixture of experts. Kittler et al. (Mar 1998) provid</context>
</contexts>
<marker>Evgeniou, Pontil, 2004</marker>
<rawString>Theodoros Evgeniou and Massimiliano Pontil. 2004. Regularized multi-task learning. In Conference on Knowledge Discovery and Data Mining (KDD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Tong Zhang</author>
</authors>
<title>Named entity recognition through classifier combination.</title>
<date>2003</date>
<booktitle>In Conference on Computational Natural Language Learning (CONLL).</booktitle>
<contexts>
<context position="27595" citStr="Florian et al., 2003" startWordPosition="4562" endWordPosition="4565">tion and many shared classifiers. Some multi-task work has also considered the grouping of tasks similar to our learning of domain subgroups (Thrun and O’Sullivan, 1998; Bakker and Heskes, 2003). There are many techniques for combining the output of multiple classifiers for ensemble learning or mixture of experts. Kittler et al. (Mar 1998) provide a theoretical framework for combining classifiers. Some empirical work has considered adding versus multiplying classifier output (Tax et al., 2000), using local accuracy estimates for combination (Woods et al., 1997), and applications to NLP tasks (Florian et al., 2003). However, these papers consider combining classifier output for prediction. In contrast, we consider parameter combination for both prediction and learning. 10 Conclusion We have explored several multi-domain learning settings using CW classifiers and a combination method. Our approach creates a better classifier for a new target domain than selecting a random source classifier a prior, reduces learning error on multiple domains compared to baseline approaches, can handle many disparate domains by using many shared classifiers, and scales to a very large number of domains with a small perform</context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong Zhang. 2003. Named entity recognition through classifier combination. In Conference on Computational Natural Language Learning (CONLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4117" citStr="Jiang and Zhai, 2007" startWordPosition="612" endWordPosition="615"> labeled with different classification functions. For example, one user may enjoy some emails that another user considers spam: differing in their classification function. The goal of multi-task learning is to generalize across tasks/domains (Dekel et al., 2006; Evgeniou and Pontil, 2004). Furthermore, as in domain adaptation, some examples are draw from different distributions. For example, one user may receive emails about engineering while another about art, differing in their distribution over features. Domain adaptation deals with these feature distribution changes (Blitzer et al., 2007; Jiang and Zhai, 2007). Our work combines these two areas by learning both across distributions and behaviors or functions. 3 Confidence-Weighted Linear Classifiers Confidence-weighted (CW) linear classification (Dredze et al., 2008), a new online algorithm, maintains a probabilistic measure of parameter confidence, which may be useful in combining parameters from different domain distributions. We summarize CW learning to familiarize the reader. Parameter confidence is formalized by a Gaussian distribution over weight vectors with mean µ E RN and diagonal covariance E E RN,N. The values µj and Ej,j represent knowl</context>
<context position="9799" citStr="Jiang and Zhai, 2007" startWordPosition="1601" endWordPosition="1604">ds, ces. We use four versions of this data: d DVDs !−1 M X m (Emf,f)−1bf (Emf,f)−1µmf bf M µcf = X m ces per domain. M M (Ec)−1 = m as = a &gt;_ 0, where a is the initialization value for We call this weighting as opposed to a uniform weighting of parameters = 1). We therefore have two combination methods (L2 and KL) and two weighting methods (uniform and varian bmf −Emf,f Emf,f. “variance” (bmf ce). !−1XMbm (Em f )−1bf m . (2) fm While each parameter is weighed by its variance in the KL, we can also explicitly encode this behavior For evaluation we selected two domain adaptation datasets: spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al., 2007). The spam data contains two tasks, one with three users (task A) and one with 15 (task B). The goal is to classify an email (bag-ofwords) as either spam or ham (not-spam) and each user may have slightly different preferences and features. We used 700 and 100 training messages for • All - 7 domains, one per product type • Books - 3 domains of books with the binary decision boundary set to 2, 3 and 4 stars • DVDs -Same as Books but with DVD reviews •Books+DVDs -Combined Books an The All dataset captures the typical domain adaptation scenario, where each doma</context>
<context position="25710" citStr="Jiang and Zhai (2007)" startWordPosition="4269" endWordPosition="4272">er performance as the number of similar domains increases. This may be an attractive alternative to keeping a very large number of parameters. 9 Related Work Multi-domain learning intersects two areas of research: domain adaptation and multi-task learning. In domain adaptation, a classifier trained for a source domain is transfered to a target domain using either unlabeled or a small amount of labeled target data. Blitzer et al. (2007) used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data. In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data. Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daum´e, 2007) and adding the source classifier’s prediction as a feature (Chelba and Acero, 2004). Others have considered transfer learning, in which an existing domain is used to improve learning in a new domain, such as constructing priors (Raina et al., 2006; Marx et al., 2008) and learning parameter functions for text classification from related data (Do and Ng, 2006). These methods larg</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kittler</author>
<author>M Hatef</author>
<author>R P W Duin</author>
<author>J Matas</author>
</authors>
<title>On combining classifiers. Pattern Analysis and Machine Intelligence,</title>
<date>1998</date>
<journal>IEEE Transactions on,</journal>
<volume>20</volume>
<issue>3</issue>
<marker>Kittler, Hatef, Duin, Matas, 1998</marker>
<rawString>J. Kittler, M. Hatef, R.P.W. Duin, and J. Matas. Mar 1998. On combining classifiers. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 20(3):226–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Littlestone</author>
<author>M K Warmuth</author>
</authors>
<title>The weighted majority algorithm.</title>
<date>1994</date>
<journal>Information and Computation,</journal>
<pages>108--212</pages>
<contexts>
<context position="21530" citStr="Littlestone and Warmuth, 1994" startWordPosition="3556" endWordPosition="3560">ed parameters. Each set of shared parameters represents a different subset of domains. If the corresponding shared parameters are known for a domain, we could use the same objective (3) and update (4) as before. If there are many fewer shared parameters than domains (k « D), we can benefit from multi-domain learning. Next, we augment the learning algorithm to learn a mapping between the domains and shared classifiers. Intuitively, a domain should be mapped to shared parameters that correctly classify that domain. A common technique for learning such experts in the Weighted Majority algorithm (Littlestone and Warmuth, 1994), which weighs a mixture of experts (classifiers). However, since we require a hard assignment — pick a single shared parameter set s — rather than a mixture, the algorithm reduces to picking the classifier s with the fewest mistakes in predicting domain d. This requires tracking the number of mistakes made by each shared classifier on each domain once a label is revealed. For learning, the shared classifier with the fewest mistakes for a domain is selected for an MDR update. Classifier ties are broken randomly. While we experi694 Figure 1: Learning across many domains - spam (left) and sentim</context>
</contexts>
<marker>Littlestone, Warmuth, 1994</marker>
<rawString>N. Littlestone and M. K. Warmuth. 1994. The weighted majority algorithm. Information and Computation, 108:212–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zvika Marx</author>
<author>Michael T Rosenstein</author>
<author>Thomas G Dietterich</author>
<author>Leslie Pack Kaelbling</author>
</authors>
<title>Two algorithms for transfer learning. In Inductive Transfer: 10 years later.</title>
<date>2008</date>
<contexts>
<context position="26197" citStr="Marx et al., 2008" startWordPosition="4345" endWordPosition="4348"> classifier on source data with new features induced from target unlabeled data. In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data. Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daum´e, 2007) and adding the source classifier’s prediction as a feature (Chelba and Acero, 2004). Others have considered transfer learning, in which an existing domain is used to improve learning in a new domain, such as constructing priors (Raina et al., 2006; Marx et al., 2008) and learning parameter functions for text classification from related data (Do and Ng, 2006). These methods largely require batch learning, unlabeled target data, or available source data at adaptation. In contrast, our algorithms operate purely online and can be applied when no target data is available. Multi-task algorithms, also known as inductive transfer, learn a set of related problems simultaneously (Caruana, 1997). The most relevant approach is that of Regularized Multi-Task Learning (Evgeniou and Pontil, 2004), which we use to motivate our online algorithm. Dekel et al. (2006) gave a</context>
</contexts>
<marker>Marx, Rosenstein, Dietterich, Kaelbling, 2008</marker>
<rawString>Zvika Marx, Michael T. Rosenstein, Thomas G. Dietterich, and Leslie Pack Kaelbling. 2008. Two algorithms for transfer learning. In Inductive Transfer: 10 years later.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Andrew Ng</author>
<author>Daphne Koller</author>
</authors>
<title>Constructing informative priors using transfer learning.</title>
<date>2006</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="26177" citStr="Raina et al., 2006" startWordPosition="4341" endWordPosition="4344"> learning to train a classifier on source data with new features induced from target unlabeled data. In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data. Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daum´e, 2007) and adding the source classifier’s prediction as a feature (Chelba and Acero, 2004). Others have considered transfer learning, in which an existing domain is used to improve learning in a new domain, such as constructing priors (Raina et al., 2006; Marx et al., 2008) and learning parameter functions for text classification from related data (Do and Ng, 2006). These methods largely require batch learning, unlabeled target data, or available source data at adaptation. In contrast, our algorithms operate purely online and can be applied when no target data is available. Multi-task algorithms, also known as inductive transfer, learn a set of related problems simultaneously (Caruana, 1997). The most relevant approach is that of Regularized Multi-Task Learning (Evgeniou and Pontil, 2004), which we use to motivate our online algorithm. Dekel </context>
</contexts>
<marker>Raina, Ng, Koller, 2006</marker>
<rawString>Rajat Raina, Andrew Ng, and Daphne Koller. 2006. Constructing informative priors using transfer learning. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M J Tax</author>
<author>Martijn van Breukelen</author>
<author>Robert P W Duina</author>
<author>Josef Kittler</author>
</authors>
<title>Combining multiple classifiers by averaging or by multiplying?</title>
<date>2000</date>
<journal>Pattern Recognition,</journal>
<volume>33</volume>
<issue>9</issue>
<marker>Tax, van Breukelen, Duina, Kittler, 2000</marker>
<rawString>David M. J. Tax, Martijn van Breukelen, Robert P. W. Duina, and Josef Kittler. 2000. Combining multiple classifiers by averaging or by multiplying? Pattern Recognition, 33(9):1475–1485, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Thrun</author>
<author>J O’Sullivan</author>
</authors>
<title>Clustering learning tasks and the selective cross–task transfer of knowledge.</title>
<date>1998</date>
<editor>In S. Thrun and L.Y. Pratt, editors, Learning To Learn.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Thrun, O’Sullivan, 1998</marker>
<rawString>S. Thrun and J. O’Sullivan. 1998. Clustering learning tasks and the selective cross–task transfer of knowledge. In S. Thrun and L.Y. Pratt, editors, Learning To Learn. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Woods</author>
<author>W Philip Kegelmeyer Jr</author>
<author>Kevin Bowyer</author>
</authors>
<title>Combination of multiple classifiers using local accuracy estimates.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<pages>410</pages>
<contexts>
<context position="27541" citStr="Woods et al., 1997" startWordPosition="4553" endWordPosition="4556">work to both include an arbitrary classifier combination and many shared classifiers. Some multi-task work has also considered the grouping of tasks similar to our learning of domain subgroups (Thrun and O’Sullivan, 1998; Bakker and Heskes, 2003). There are many techniques for combining the output of multiple classifiers for ensemble learning or mixture of experts. Kittler et al. (Mar 1998) provide a theoretical framework for combining classifiers. Some empirical work has considered adding versus multiplying classifier output (Tax et al., 2000), using local accuracy estimates for combination (Woods et al., 1997), and applications to NLP tasks (Florian et al., 2003). However, these papers consider combining classifier output for prediction. In contrast, we consider parameter combination for both prediction and learning. 10 Conclusion We have explored several multi-domain learning settings using CW classifiers and a combination method. Our approach creates a better classifier for a new target domain than selecting a random source classifier a prior, reduces learning error on multiple domains compared to baseline approaches, can handle many disparate domains by using many shared classifiers, and scales </context>
</contexts>
<marker>Woods, Jr, Bowyer, 1997</marker>
<rawString>Kevin Woods, W. Philip Kegelmeyer Jr., and Kevin Bowyer. 1997. Combination of multiple classifiers using local accuracy estimates. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):405– 410.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>