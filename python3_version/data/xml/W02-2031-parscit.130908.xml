<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000106">
<title confidence="0.750890666666667">
Learning with Multiple Stacking for Named Entity Recognition
Koji Tsukamoto and Yutaka Mitsuishi and Manabu Sassano
Fujitsu Laboratories Ltd.
</title>
<email confidence="0.969925">
tukamoto,mitsuishi-y,sassano @jp.fujitsu.com
</email>
<sectionHeader confidence="0.99948" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9992385">
In this paper, we present a learning method us-
ing multiple stacking for named entity recognition.
In order to take into account the tags of the sur-
rounding words, we propose a method which em-
ploys stacked learners using the tags predicted by
the lower level learners. We have applied this ap-
proach to the CoNLL-2002 shared task to improve
a base system.
</bodyText>
<sectionHeader confidence="0.987661" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999462961538462">
Before describing our system, let us see one aspect
of the named entity recognition, the outline of our
method, and the relation to the previous works.
The task of named entity recognition can be re-
garded as a process of assigning a named entity tag
to each given word, taking into account the patterns
of surrounding words. Suppose that a sequence of
words is given as below:
... W , W , W , W , W ...
Then, given that the current position is at word W ,
the task is to assign tag T to W .
In the named entity recognition task, an entity is
often made up of a sequence of words, rather than
a single word. For example, an entity “the United
States of America” consists of five words. In order
to allocate a tag to each word, the tags of the sur-
rounding words (we call these tags the surrounding
tags) can be a clue to predict the tag of the word
(we call this tag the current tag). For the test set,
however, these tags are unknown.
In order to take into account the surrounding tags
for the prediction of the current tag, we propose a
method which employs multiple stacked learners,
an extension of stacking method (Wolpert, 1992).
Stacking based method for named entity recognition
usually employs two or more level learners. The
higher level learner uses the current tags predicted
by its lower level learners. In our method, by con-
trast, the higher level learner uses not only the cur-
rent tag but also the surrounding tags predicted by
the lower level learner. Our aim is to leverage the
performance of the base system using the surround-
ing tags as the features.
At least two groups have previously proposed
systems which use the predicted surrounding tags.
One system, proposed by van Halteren et al. (1998),
also uses stacking method. This system uses four
completely different types of taggers as the first
level learners, because it has been assumed that first
level learners should be as different as possible. The
tags predicted by the first level learners are used as
the features of the second level learner.
The other system, proposed by (Kudo and Mat-
sumoto, 2000; Yamada et al., 2001), uses the ”dy-
namic features”. In the test phase, the predicted tags
of the preceding (or subsequent) words are used as
the features, which are called “dynamic features”.
In the training phase, the system uses the answer
tags of the preceding (or subsequent) words as the
features.
More detailed descriptions of our system are
shown below:
</bodyText>
<subsectionHeader confidence="0.99813">
2.1 Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.999819714285714">
As the learning algorithm for all the levels , we use
an extension of AdaBoost, the real AdaBoost.MH
which is extended to handle multiclass problems
(Schapire and Singer, 1999). For weak learners, we
use decision stumps (Schapire and Singer, 1999),
which select only one feature to classify an exam-
ple.
</bodyText>
<subsectionHeader confidence="0.930298">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.875717">
We use the following types of the features for the
prediction of the tag of the word.
surface form of W , W , W , W and W .
</bodyText>
<tableCaption confidence="0.988704">
Table 1: Word features and examples
</tableCaption>
<bodyText confidence="0.9996515">
One of the eight word features in Table 1.
These features are similar to those used in
(Bikel et al., 1997).
First and last two/three letters of W
Estimated tag of W based on the word uni-
gram model in the training set.
Additionally, we use the surrounding tag feature.
This feature is discussed in Section 2.3.
</bodyText>
<subsectionHeader confidence="0.999816">
2.3 Multiple Stacking
</subsectionHeader>
<bodyText confidence="0.999993681818182">
In order to take into account the tags of the sur-
rounding words, our system employs stacked learn-
ers. Figure 1 gives the outline of the learning and
applying algorithm of our system. In the learning
phase, the base system is trained at first. After that,
the higher level learners are trained using word fea-
tures (described in Section 2.2), current tag T and
surrounding tags T predicted by the lower
level learner. While these tag may not be correctly
predicted , if the accuracy of the prediction of the
lower level learner is improved, the features used in
each prediction become accurate. In the applying
phase, all of the learners are cascaded in the order.
Compared to the previous systems (van Halteren
et al., 1998; Kudo and Matsumoto, 2000; Yamada
et al., 2001), our system is: (i) employing more
than two levels stacking, (ii) using only one algo-
rithm and training only one learner at each level,
(iii) using the surrounding tag given by the lower
level learner. (iv) using both the preceding and sub-
sequent tags as the features. (v) using the predicted
tags instead of the answer tags in the training phase.
</bodyText>
<sectionHeader confidence="0.994245" genericHeader="evaluation">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999984285714286">
In this section, the experimental conditions and the
results of the proposed method are shown.
In order to improve the performance of the base
system, the tag sequence to be predicted is format-
ted according to IOB1, even though the sequence
Let L denote the th level learner and let T denote th
level output tags for W .
</bodyText>
<note confidence="0.356185">
Learning:
</note>
<tableCaption confidence="0.804005">
1. Train the base learner L using the features described in
Table 1.
2. for = 1,...,N
</tableCaption>
<bodyText confidence="0.6377556">
Get T of the training set using L , the fea-
tures described in section 2.2 (and T , T ,
T , T , T if 1).
Train L using the features described in section 2.2
and T , T , T , T , T .
</bodyText>
<listItem confidence="0.955216">
3. Output L , L ,...,L .
Applying:
1. for k = 0,...,N
</listItem>
<figure confidence="0.3352655">
Get T of test set using L , the features de-
scribed in section 2.2 (and T , T , T ,
T , T if 0).
2. Output T .
</figure>
<figureCaption confidence="0.999605">
Figure 1: Outline of multiple stacking algorithm
</figureCaption>
<bodyText confidence="0.9995442">
in the original corpus was formatted according to
IOB2 (Tjong Kim Sang and Veenstra, 1999).
To reduce the computational cost, features ap-
pearing fewer than three times are eliminated in the
training phase.
</bodyText>
<subsectionHeader confidence="0.999938">
3.1 Base System
</subsectionHeader>
<bodyText confidence="0.999966466666667">
To evaluate the effect of multiple stacking in the
next section, the performance of the base system
is shown in Figure 2. A performance peak is ob-
served after 10,000 rounds of boosting. Note that a
decision stump used in the real AdaBoost.MH takes
into account only one feature. Hence the number
of features used by real AdaBoost.MH is less than
the number of the rounds. In our experiment, be-
cause the rounds of boosting are always less than
the number of the features (about 40,000), a large
proportion of features are not used by the learners.
If the rounds of boosting in the base system are not
enough, stacking effect may be similar to increas-
ing the rounds of boosting. In Figure 2, however,
we can see that 10,000 rounds is enough.
</bodyText>
<subsectionHeader confidence="0.999314">
3.2 Multiple Stacking
</subsectionHeader>
<bodyText confidence="0.9999342">
We examine the effect of multiple stacking com-
pared to the base system.
The F score of multiple stacking for the Span-
ish test set (esp.testa) is shown in Table 2. By stack-
ing learners, the score of each named entity is im-
</bodyText>
<figure confidence="0.98707185">
Word Feature
Example Text
Digit
25
Digit+Alphabet
Symbol
Uppercase
Capitalized
Lowercase(word length characters)
Lowercase(word length characters)
Other
CDG1
.
EFE
Australia
necesidad
del
hoy,
0 5000 10000 15000 20000
rounds
</figure>
<table confidence="0.9891255">
overall LOC MISC ORG PER
60.94 64.12 33.61 61.37 70.91
65.68 67.35 40.04 66.26 74.82
66.91 68.02 41.69 67.38 75.51
67.24 67.96 41.76 67.95 75.77
67.35 67.81 42.75 67.91 75.89
67.35 67.78 42.30 67.95 75.99
67.30 67.57 42.12 68.01 76.01
</table>
<bodyText confidence="0.996758607142857">
proved. Compared to the overall F score of L ,
the score of L , stacking one learner over the base
system, is improved by 4.74 point. Further more,
compared to the score of L , the score of L is
higher by 1.67 point. Through five iterations of
stacking, the score is continuously increased. The
overall scores for the six tests are briefly shown in
Table 3. The effect of two level stacking is higher
for the Spanish tests. However, multiple staking ef-
fects greater for the Dutch test, especially for the
corpus without part of speech. As discussed in Sec-
tion 3.1, the improvement of the score is not due to
the rounds of boosting. Thus, it is due to multiple
stacking.
In Table 2, stacking effects for MISC and ORG
appear greater than those for LOC and PER. It is
reasonable to suppose that MISC and ORG entities
consist of a relatively long sequence of words, and
the surrounding tags can be good clues for the pre-
diction of the current tag. Indeed, in the Spanish
training set, the ratios of entities which consist of
more than three words are 9.7%, 22.4%, 4.4% and
3.5% for ORG, MISC, LOC and PER respectively.
Table 4 and 5 show examples of the predicted tags
through the stacked level. Let us see how multi-
ple stacking works using the examples in Table 5.
Let the word “fin” be the current position. The an-
swer tag is “I-MISC”. When we use the base system
</bodyText>
<table confidence="0.99335525">
esp.b ned.a* ned.b* ned.a ned.b
65.70 55.50 58.04 54.23 57.43
69.39 56.72 59.18 56.20 58.84
71.49 58.23 60.74 58.83 60.93
</table>
<tableCaption confidence="0.960901">
Table 3: F scores for six tests. “*” indicates use
of part of speech tags.
</tableCaption>
<table confidence="0.9998579">
Answer L L L
:
Colegio I-ORG I-ORG I-ORG I-ORG
Plico I-ORG I-ORG I-ORG I-ORG
Arias I-ORG I-LOC I-LOC I-ORG
Montano I-ORG I-LOC I-ORG I-ORG
, O O O O
de O O O O
Badajoz I-LOC I-LOC I-LOC I-LOC
:
</table>
<tableCaption confidence="0.94422">
Table 4: Example of the prediction (line 2434 to
2440 in esp.testa)
</tableCaption>
<table confidence="0.999775416666667">
Answer L L L L
:
el O O O O O
libro O O O O O
“ I-MISC I-MISC I-MISC I-MISC I-MISC
Una I-MISC O I-MISC I-MISC I-MISC
sonrisa I-MISC I-MISC I-MISC I-MISC I-MISC
sin I-MISC O I-MISC I-MISC I-MISC
fin I-MISC O O I-MISC I-MISC
“ I-MISC O O O I-MISC
, O O O O O
:
</table>
<tableCaption confidence="0.975489">
Table 5: Example of the prediction (line 16231 to
16239 in esp.testa)
</tableCaption>
<bodyText confidence="0.999865523809524">
L , the predicted tag of the word is “O”. In the next
level, L uses the surrounding tag features “I-MISC,
O, (O,) O, O” and also outputs “O”. In the third
level, however, L correctly predicts the tag using
the surrounding tag features “I-MISC, I-MISC, (O,)
O, O”. Note that no other feature changes through
the levels. The improvement in the example is
clearly caused by multiple stacking. As a result,
this MISC entity is allocated tags correctly by L .
The above effect would not be achieved by two level
stacking. This result clearly shows that multiple
stacking method has an advantage.
Next we examine the effect of the learning al-
gorithm to multiple stacking. We use the real Ad-
aBoost.MH for 300, 1,000, 3,000, 10,000, 20,000
rounds. Their F scores in each stacking level are
plotted in Figure 3. The score improves by stack-
ing for all algorithms. The highest score is achieved
by 10,000 iterations at every stacking level. The
shapes of the curves in Figure 3 are similar to each
other. This result suggests that the stacking effect
</bodyText>
<figure confidence="0.6210135">
training
test
</figure>
<figureCaption confidence="0.794388">
Figure 2: F score of the base system
</figureCaption>
<tableCaption confidence="0.446916">
Table 2: F score of stacked learner
</tableCaption>
<figure confidence="0.995797482758621">
95
90
85
80
FB1
75
✱
B✱
70
65
60
55
50
n
L
L
L
L
L
L
L
esp.a
L 60.94
L 65.68
L 67.35
is scarcely affected by the performance of the algo-
rithm.
0 2 4 6 8 10
number of stacking learners
</figure>
<figureCaption confidence="0.998308">
Figure 3: F scores of different base system
</figureCaption>
<sectionHeader confidence="0.998713" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999727125">
We have presented a new method for recognizing
named entity by multiple stacking. This method
can leverage the performance of the base system
employing multiple stacked learner and using not
only the current tag but also the surrounding tags
predicted by the lower level learner. By stacking 5
real AdaBoost.MH learners, we can obtain F of
67.35 for the Spanish named entity recognition task.
</bodyText>
<sectionHeader confidence="0.998594" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.961313041666667">
D. M. Bikel, S. Miller, R. Schwartz and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder. In Fifth Conference on Applied Natural Lan-
guage Processing.
H. van Halteren, J. Zavrel and W. Daelemans. 1998. Im-
proving data driven wordclass tagging by system com-
bination. In Proceedings of the 17th COLING and the
36th Annual Meeting ofACL.
T. Kudo and Y. Matsumoto. 2000. Japanese dependency
structure analysis based on support vector machines.
In Proceedings of the 2000 Joint SIGDAT Conference
on Empirical Methods in Natural Language Process-
ing and Very Large Corpora.
R. E. Schapire and Y. Singer. 1999. Improved boosting
algorithms using confidence-rated predictions. Ma-
chine Learning, 37(3).
E. F. Tjong Kim Sang and J. Veenstra. 1999. Represent-
ing text chunks. In Proceedings ofEACL’99.
D. H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5.
H. Yamada, T. Kudo and Y. Matsumoto. 2001. Japanese
named entity extraction using support vector ma-
chines. Information Processing Society ofJapan, SIG
Notes NL 142-17 (in Japanese).
</reference>
<table confidence="0.997014666666667">
Spanish dev. precision recall F
LOC 59.75% 78.38% 67.81
MISC 40.40% 45.39% 42.75
ORG 67.48% 68.35% 67.91
PER 78.26% 73.65% 75.89
overall 65.09% 69.76% 67.35
Spanish test precision recall F
LOC 70.96% 73.25% 72.08
MISC 41.83% 42.94% 42.38
ORG 68.21% 76.93% 72.31
PER 80.23% 84.49% 82.31
overall 69.04% 74.12% 71.49
Dutch dev. precision recall F
LOC 54.87% 65.13% 59.56
MISC 59.12% 65.15% 61.99
ORG 67.95% 51.84% 58.81
PER 47.58% 66.53% 55.48
overall 55.92% 62.05% 58.83
Dutch test precision recall F
LOC 63.79% 73.80% 68.43
MISC 56.89% 59.14% 57.99
ORG 60.16% 53.02% 56.36
PER 52.61% 74.73% 61.75
overall 57.33% 65.02% 60.93
</table>
<tableCaption confidence="0.895422">
Table 6: Overview of the precision, recall and F
</tableCaption>
<bodyText confidence="0.675027333333333">
of multiple stacking with =4 and 10,000 rounds of
boosting. Dutch data is processed without part of
speech tags.
</bodyText>
<figure confidence="0.9980845">
✱
FB1
B
✱
75
70
65
60
55
300 rounds
1000 rounds
3000 rounds
10000 rounds
20000 rounds
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.324175">
<title confidence="0.999743">Learning with Multiple Stacking for Named Entity Recognition</title>
<author confidence="0.779315">Koji Tsukamoto</author>
<author confidence="0.779315">Yutaka Mitsuishi</author>
<author confidence="0.779315">Manabu</author>
<affiliation confidence="0.679149">Fujitsu Laboratories Ltd.</affiliation>
<intro confidence="0.484351">tukamoto,mitsuishi-y,sassano @jp.fujitsu.com</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Nymble: a high-performance learning namefinder.</title>
<date>1997</date>
<booktitle>In Fifth Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="3622" citStr="Bikel et al., 1997" startWordPosition="632" endWordPosition="635">shown below: 2.1 Learning Algorithm As the learning algorithm for all the levels , we use an extension of AdaBoost, the real AdaBoost.MH which is extended to handle multiclass problems (Schapire and Singer, 1999). For weak learners, we use decision stumps (Schapire and Singer, 1999), which select only one feature to classify an example. 2.2 Features We use the following types of the features for the prediction of the tag of the word. surface form of W , W , W , W and W . Table 1: Word features and examples One of the eight word features in Table 1. These features are similar to those used in (Bikel et al., 1997). First and last two/three letters of W Estimated tag of W based on the word unigram model in the training set. Additionally, we use the surrounding tag feature. This feature is discussed in Section 2.3. 2.3 Multiple Stacking In order to take into account the tags of the surrounding words, our system employs stacked learners. Figure 1 gives the outline of the learning and applying algorithm of our system. In the learning phase, the base system is trained at first. After that, the higher level learners are trained using word features (described in Section 2.2), current tag T and surrounding tag</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>D. M. Bikel, S. Miller, R. Schwartz and R. Weischedel. 1997. Nymble: a high-performance learning namefinder. In Fifth Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Improving data driven wordclass tagging by system combination.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th COLING and the 36th Annual Meeting ofACL.</booktitle>
<marker>van Halteren, Zavrel, Daelemans, 1998</marker>
<rawString>H. van Halteren, J. Zavrel and W. Daelemans. 1998. Improving data driven wordclass tagging by system combination. In Proceedings of the 17th COLING and the 36th Annual Meeting ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Japanese dependency structure analysis based on support vector machines.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="2654" citStr="Kudo and Matsumoto, 2000" startWordPosition="459" endWordPosition="463">vel learner. Our aim is to leverage the performance of the base system using the surrounding tags as the features. At least two groups have previously proposed systems which use the predicted surrounding tags. One system, proposed by van Halteren et al. (1998), also uses stacking method. This system uses four completely different types of taggers as the first level learners, because it has been assumed that first level learners should be as different as possible. The tags predicted by the first level learners are used as the features of the second level learner. The other system, proposed by (Kudo and Matsumoto, 2000; Yamada et al., 2001), uses the ”dynamic features”. In the test phase, the predicted tags of the preceding (or subsequent) words are used as the features, which are called “dynamic features”. In the training phase, the system uses the answer tags of the preceding (or subsequent) words as the features. More detailed descriptions of our system are shown below: 2.1 Learning Algorithm As the learning algorithm for all the levels , we use an extension of AdaBoost, the real AdaBoost.MH which is extended to handle multiclass problems (Schapire and Singer, 1999). For weak learners, we use decision st</context>
<context position="4596" citStr="Kudo and Matsumoto, 2000" startWordPosition="801" endWordPosition="804">line of the learning and applying algorithm of our system. In the learning phase, the base system is trained at first. After that, the higher level learners are trained using word features (described in Section 2.2), current tag T and surrounding tags T predicted by the lower level learner. While these tag may not be correctly predicted , if the accuracy of the prediction of the lower level learner is improved, the features used in each prediction become accurate. In the applying phase, all of the learners are cascaded in the order. Compared to the previous systems (van Halteren et al., 1998; Kudo and Matsumoto, 2000; Yamada et al., 2001), our system is: (i) employing more than two levels stacking, (ii) using only one algorithm and training only one learner at each level, (iii) using the surrounding tag given by the lower level learner. (iv) using both the preceding and subsequent tags as the features. (v) using the predicted tags instead of the answer tags in the training phase. 3 Experiments and Results In this section, the experimental conditions and the results of the proposed method are shown. In order to improve the performance of the base system, the tag sequence to be predicted is formatted accord</context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>T. Kudo and Y. Matsumoto. 2000. Japanese dependency structure analysis based on support vector machines. In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Improved boosting algorithms using confidence-rated predictions.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="3215" citStr="Schapire and Singer, 1999" startWordPosition="552" endWordPosition="555"> learner. The other system, proposed by (Kudo and Matsumoto, 2000; Yamada et al., 2001), uses the ”dynamic features”. In the test phase, the predicted tags of the preceding (or subsequent) words are used as the features, which are called “dynamic features”. In the training phase, the system uses the answer tags of the preceding (or subsequent) words as the features. More detailed descriptions of our system are shown below: 2.1 Learning Algorithm As the learning algorithm for all the levels , we use an extension of AdaBoost, the real AdaBoost.MH which is extended to handle multiclass problems (Schapire and Singer, 1999). For weak learners, we use decision stumps (Schapire and Singer, 1999), which select only one feature to classify an example. 2.2 Features We use the following types of the features for the prediction of the tag of the word. surface form of W , W , W , W and W . Table 1: Word features and examples One of the eight word features in Table 1. These features are similar to those used in (Bikel et al., 1997). First and last two/three letters of W Estimated tag of W based on the word unigram model in the training set. Additionally, we use the surrounding tag feature. This feature is discussed in Se</context>
</contexts>
<marker>Schapire, Singer, 1999</marker>
<rawString>R. E. Schapire and Y. Singer. 1999. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>J Veenstra</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In Proceedings ofEACL’99.</booktitle>
<contexts>
<context position="5893" citStr="Sang and Veenstra, 1999" startWordPosition="1061" endWordPosition="1064">nd let T denote th level output tags for W . Learning: 1. Train the base learner L using the features described in Table 1. 2. for = 1,...,N Get T of the training set using L , the features described in section 2.2 (and T , T , T , T , T if 1). Train L using the features described in section 2.2 and T , T , T , T , T . 3. Output L , L ,...,L . Applying: 1. for k = 0,...,N Get T of test set using L , the features described in section 2.2 (and T , T , T , T , T if 0). 2. Output T . Figure 1: Outline of multiple stacking algorithm in the original corpus was formatted according to IOB2 (Tjong Kim Sang and Veenstra, 1999). To reduce the computational cost, features appearing fewer than three times are eliminated in the training phase. 3.1 Base System To evaluate the effect of multiple stacking in the next section, the performance of the base system is shown in Figure 2. A performance peak is observed after 10,000 rounds of boosting. Note that a decision stump used in the real AdaBoost.MH takes into account only one feature. Hence the number of features used by real AdaBoost.MH is less than the number of the rounds. In our experiment, because the rounds of boosting are always less than the number of the feature</context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>E. F. Tjong Kim Sang and J. Veenstra. 1999. Representing text chunks. In Proceedings ofEACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Wolpert</author>
</authors>
<title>Stacked generalization.</title>
<date>1992</date>
<journal>Neural Networks,</journal>
<volume>5</volume>
<contexts>
<context position="1709" citStr="Wolpert, 1992" startWordPosition="303" endWordPosition="304">tion task, an entity is often made up of a sequence of words, rather than a single word. For example, an entity “the United States of America” consists of five words. In order to allocate a tag to each word, the tags of the surrounding words (we call these tags the surrounding tags) can be a clue to predict the tag of the word (we call this tag the current tag). For the test set, however, these tags are unknown. In order to take into account the surrounding tags for the prediction of the current tag, we propose a method which employs multiple stacked learners, an extension of stacking method (Wolpert, 1992). Stacking based method for named entity recognition usually employs two or more level learners. The higher level learner uses the current tags predicted by its lower level learners. In our method, by contrast, the higher level learner uses not only the current tag but also the surrounding tags predicted by the lower level learner. Our aim is to leverage the performance of the base system using the surrounding tags as the features. At least two groups have previously proposed systems which use the predicted surrounding tags. One system, proposed by van Halteren et al. (1998), also uses stackin</context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>D. H. Wolpert. 1992. Stacked generalization. Neural Networks, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Japanese named entity extraction using support vector machines.</title>
<date>2001</date>
<booktitle>Information Processing Society ofJapan, SIG Notes NL</booktitle>
<pages>142--17</pages>
<note>(in Japanese).</note>
<contexts>
<context position="2676" citStr="Yamada et al., 2001" startWordPosition="464" endWordPosition="467"> leverage the performance of the base system using the surrounding tags as the features. At least two groups have previously proposed systems which use the predicted surrounding tags. One system, proposed by van Halteren et al. (1998), also uses stacking method. This system uses four completely different types of taggers as the first level learners, because it has been assumed that first level learners should be as different as possible. The tags predicted by the first level learners are used as the features of the second level learner. The other system, proposed by (Kudo and Matsumoto, 2000; Yamada et al., 2001), uses the ”dynamic features”. In the test phase, the predicted tags of the preceding (or subsequent) words are used as the features, which are called “dynamic features”. In the training phase, the system uses the answer tags of the preceding (or subsequent) words as the features. More detailed descriptions of our system are shown below: 2.1 Learning Algorithm As the learning algorithm for all the levels , we use an extension of AdaBoost, the real AdaBoost.MH which is extended to handle multiclass problems (Schapire and Singer, 1999). For weak learners, we use decision stumps (Schapire and Sin</context>
<context position="4618" citStr="Yamada et al., 2001" startWordPosition="805" endWordPosition="808">pplying algorithm of our system. In the learning phase, the base system is trained at first. After that, the higher level learners are trained using word features (described in Section 2.2), current tag T and surrounding tags T predicted by the lower level learner. While these tag may not be correctly predicted , if the accuracy of the prediction of the lower level learner is improved, the features used in each prediction become accurate. In the applying phase, all of the learners are cascaded in the order. Compared to the previous systems (van Halteren et al., 1998; Kudo and Matsumoto, 2000; Yamada et al., 2001), our system is: (i) employing more than two levels stacking, (ii) using only one algorithm and training only one learner at each level, (iii) using the surrounding tag given by the lower level learner. (iv) using both the preceding and subsequent tags as the features. (v) using the predicted tags instead of the answer tags in the training phase. 3 Experiments and Results In this section, the experimental conditions and the results of the proposed method are shown. In order to improve the performance of the base system, the tag sequence to be predicted is formatted according to IOB1, even thou</context>
</contexts>
<marker>Yamada, Kudo, Matsumoto, 2001</marker>
<rawString>H. Yamada, T. Kudo and Y. Matsumoto. 2001. Japanese named entity extraction using support vector machines. Information Processing Society ofJapan, SIG Notes NL 142-17 (in Japanese).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>