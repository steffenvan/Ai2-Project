<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.120023">
<title confidence="0.998552">
Extracting Opinion Expressions and Their Polarities – Exploration of
Pipelines and Joint Models
</title>
<author confidence="0.996435">
Richard Johansson and Alessandro Moschitti
</author>
<affiliation confidence="0.997655">
DISI, University of Trento
</affiliation>
<address confidence="0.954773">
Via Sommarive 14, 38123 Trento (TN), Italy
</address>
<email confidence="0.995747">
{johansson, moschitti}@disi.unitn.it
</email>
<sectionHeader confidence="0.995551" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961285714286">
We investigate systems that identify opinion
expressions and assigns polarities to the ex-
tracted expressions. In particular, we demon-
strate the benefit of integrating opinion ex-
traction and polarity classification into a joint
model using features reflecting the global po-
larity structure. The model is trained using
large-margin structured prediction methods.
The system is evaluated on the MPQA opinion
corpus, where we compare it to the only previ-
ously published end-to-end system for opinion
expression extraction and polarity classifica-
tion. The results show an improvement of be-
tween 10 and 15 absolute points in F-measure.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995237254902">
Automatic systems for the analysis of opinions ex-
pressed in text on the web have been studied exten-
sively. Initially, this was formulated as a coarse-
grained task – locating opinionated documents –
and tackled using methods derived from standard re-
trieval or categorization. However, in recent years
there has been a shift towards a more detailed task:
not only finding the text expressing the opinion, but
also analysing it: who holds the opinion and to what
is addressed; it is positive or negative (polarity);
what its intensity is. This more complex formula-
tion leads us deep into NLP territory; the methods
employed here have been inspired by information
extraction and semantic role labeling, combinatorial
optimization and structured machine learning.
A crucial step in the automatic analysis of opinion
is to mark up the opinion expressions: the pieces of
text allowing us to infer that someone has a partic-
ular feeling about some topic. Then, opinions can
be assigned a polarity describing whether the feel-
ing is positive, neutral or negative. These two tasks
have generally been tackled in isolation. Breck et al.
(2007) introduced a sequence model to extract opin-
ions and we took this one step further by adding a
reranker on top of the sequence labeler to take the
global sentence structure into account in (Johansson
and Moschitti, 2010b); later we also added holder
extraction (Johansson and Moschitti, 2010a). For
the task of classifiying the polarity of a given expres-
sion, there has been fairly extensive work on suitable
classification features (Wilson et al., 2009).
While the tasks of expression detection and polar-
ity classification have mostly been studied in isola-
tion, Choi and Cardie (2010) developed a sequence
labeler that simultaneously extracted opinion ex-
pressions and assigned polarities. This is so far
the only published result on joint opinion segmenta-
tion and polarity classification. However, their ex-
periment lacked the obvious baseline: a standard
pipeline consisting of an expression identifier fol-
lowed by a polarity classifier.
In addition, while theirs is the first end-to-end sys-
tem for expression extraction with polarities, it is
still a sequence labeler, which, by construction, is
restricted to use simple local features. In contrast, in
(Johansson and Moschitti, 2010b), we showed that
global structure matters: opinions interact to a large
extent, and we can learn about their interactions on
the opinion level by means of their interactions on
the syntactic and semantic levels. It is intuitive that
this should also be valid when polarities enter the
</bodyText>
<page confidence="0.984758">
101
</page>
<note confidence="0.5840825">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 101–106,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999969533333333">
picture – this was also noted by Choi and Cardie
(2008). Evaluative adjectives referring to the same
evaluee may cluster together in the same clause or
be dominated by a verb of categorization; opinions
with opposite polarities may be conjoined through a
contrastive discourse connective such as but.
In this paper, we first implement two strong base-
lines consisting of pipelines of opinion expression
segmentation and polarity labeling and compare
them to the joint opinion extractor and polarity clas-
sifier by Choi and Cardie (2010). Secondly, we ex-
tend the global structure approach and add features
reflecting the polarity structure of the sentence. Our
systems were superior by between 8 and 14 absolute
F-measure points.
</bodyText>
<sectionHeader confidence="0.989617" genericHeader="method">
2 The MPQA Opinion Corpus
</sectionHeader>
<bodyText confidence="0.999884866666667">
Our system was developed using version 2.0 of the
MPQA corpus (Wiebe et al., 2005). The central
building block in the MPQA annotation is the opin-
ion expression. Opinion expressions belong to two
categories: Direct subjective expressions (DSEs)
are explicit mentions of opinion whereas expressive
subjective elements (ESEs) signal the attitude of the
speaker by the choice of words. Opinions have two
features: polarity and intensity, and most expres-
sions are also associated with a holder, also called
source. In this work, we only consider polarities,
not intensities or holders. The polarity takes the val-
ues POSITIVE, NEUTRAL, NEGATIVE, and BOTH;
for compatibility with Choi and Cardie (2010), we
mapped BOTH to NEUTRAL.
</bodyText>
<sectionHeader confidence="0.983893" genericHeader="method">
3 The Baselines
</sectionHeader>
<bodyText confidence="0.999619034482759">
In order to test our hypothesis against strong base-
lines, we developed two pipeline systems. The first
part of each pipeline extracts opinion expressions,
and this is followed by a multiclass classifier assign-
ing a polarity to a given opinion expression, similar
to that described by Wilson et al. (2009).
The first of the two baselines extracts opinion ex-
pressions using a sequence labeler similar to that by
Breck et al. (2007) and Choi et al. (2006). Sequence
labeling techniques such as HMMs and CRFs are
widely used for segmentation problems such as
named entity recognition and noun chunk extraction.
We trained a first-order labeler with the discrimi-
native training method by Collins (2002) and used
common features: words, POS, lemmas in a sliding
window. In addition, we used subjectivity clues ex-
tracted from the lexicon by Wilson et al. (2005).
For the second baseline, we added our opinion ex-
pression reranker (Johansson and Moschitti, 2010b)
on top of the expression sequence labeler.
Given an expression, we use a classifier to assign
a polarity value: positive, neutral, or negative. We
trained linear support vector machines to carry out
this classification. The problem of polarity classi-
fication has been studied in detail by Wilson et al.
(2009), who used a set of carefully devised linguis-
tic features. Our classifier is simpler and is based
on fairly shallow features: words, POS, subjectivity
clues, and bigrams inside and around the expression.
</bodyText>
<sectionHeader confidence="0.995247" genericHeader="method">
4 The Joint Model
</sectionHeader>
<bodyText confidence="0.999987142857143">
We formulate the opinion extraction task as a struc-
tured prediction problem y� = arg maxy w ·-b(x, y).
where w is a weight vector and 4b a feature extractor
representing a sentence x and a set y of polarity-
labeled opinions. This is a high-level formulation –
we still need an inference procedure for the arg max
and a learner to estimate w on a training set.
</bodyText>
<subsectionHeader confidence="0.997269">
4.1 Approximate Inference
</subsectionHeader>
<bodyText confidence="0.9999787">
Since there is a combinatorial number of ways to
segment a sentence and label the segments with po-
larities, the tractability of the arg max operation will
obviously depend on whether we can factorize the
problem for a particular 4b.
Choi and Cardie (2010) used a Markov factor-
ization and could thus apply standard sequence la-
beling with a Viterbi arg max. However, in (Jo-
hansson and Moschitti, 2010b), we showed that a
large improvement can be achieved if relations be-
tween possible expressions are considered; these re-
lations can be syntactic or semantic in nature, for
instance. This representation breaks the Markov as-
sumption and the arg max becomes intractable. We
instead used a reranking approximation: a Viterbi-
based sequence tagger following Breck et al. (2007)
generated a manageable hypothesis set of complete
segmentations, from which the reranking classifier
picked one hypothesis as its final output. Since the
set is small, no particular structure assumption (such
</bodyText>
<page confidence="0.994599">
102
</page>
<bodyText confidence="0.999991">
as Markovization) needs to be made, so the reranker
can in principle use features of arbitrary complexity.
We now adapt that approach to the problem of
joint opinion expression segmentation and polarity
classification. In that case, we not only need hy-
potheses generated by a sequence labeler, but also
the polarity labelings output by a polarity classifier.
The hypothesis generation thus proceeds as follows:
</bodyText>
<listItem confidence="0.9244428">
• For a given sentence, let the base sequence la-
beler generate up to ks sequences of unlabeled
opinion expressions;
• for every sequence, apply the base polarity
classifier to generate up to kp polarity labelings.
</listItem>
<bodyText confidence="0.998731928571429">
Thus, the hypothesis set size is at most ks · kp. We
used a ks of 64 and a kp of 4 in all experiments.
To illustrate this process we give a hypothetical
example, assuming ks = kp = 2 and the sentence
The appeasement emboldened the terrorists. We
first generate the opinion expression sequence
candidates:
The [appeasement] emboldened the [terrorists]
The [appeasement] [emboldened] the [terrorists]
and in the second step we add polarity values:
The [appeasement]− emboldened the [terrorists]−
The [appeasement]− [emboldened]+ the [terrorists]−
The [appeasement]0 emboldened the [terrorists]−
The [appeasement]− [emboldened]0 the [terrorists]−
</bodyText>
<subsectionHeader confidence="0.988799">
4.2 Features of the Joint Model
</subsectionHeader>
<bodyText confidence="0.999619777777778">
The features used by the joint opinion segmenter and
polarity classifier are based on pairs of opinions: ba-
sic features extracted from each expression such as
polarities and words, and relational features describ-
ing their interaction. To extract relations we used the
parser by Johansson and Nugues (2008) to annotate
sentences with dependencies and shallow semantics
in the PropBank (Palmer et al., 2005) and NomBank
(Meyers et al., 2004) frameworks.
Figure 1 shows the sentence the appeasement em-
boldened the terrorists, where appeasement and ter-
rorists are opinions with negative polarity, with de-
pendency syntax (above the text) and a predicate–
argument structure (below). The predicate em-
boldened, an instance of the PropBank frame
embolden.01, has two semantic arguments: the
Agent (A0) and the Theme (A1), realized syntacti-
cally as a subject and a direct object, respectively.
</bodyText>
<figure confidence="0.978235">
OBJ
NMOD
[appeasement] emboldened the [terrorists
A0
embolden.01
</figure>
<figureCaption confidence="0.99999">
Figure 1: Syntactic and shallow semantic structure.
</figureCaption>
<bodyText confidence="0.996119">
The model used the following novel features that
take the polarities of the expressions into account.
The examples are given with respect to the two ex-
pressions (appeasement and terrorists) in Figure 1.
Base polarity classifier score. Sum of the scores
from the polarity classifier for every opinion.
Polarity pair. For every pair of opinions in the
sentence, we add the pair of polarities: NEG-
ATIVE+NEGATIVE.
Polarity pair and syntactic path. For a pair
of opinions, we use the polarities and a
representation of the path through the syn-
tax tree between the expressions, follow-
ing standard practice from dependency-based
SRL (Johansson and Nugues, 2008): NEGA-
TIVE+SBJTOBJJ+NEGATIVE.
Polarity pair and syntactic dominance. In addition
to the detailed syntactic path, we use a simpler
feature based on dominance, i.e. that one ex-
pression is above the other in the syntax tree. In
the example, no such feature is extracted since
neither of the expressions dominates the other.
Polarity pair and word pair. The polarity pair
concatenated with the words of the clos-
est nodes of the two expressions: NEGA-
TIVE+NEGATIVE+appeasement+terrorists.
Polarity pair and types and syntactic path. From
the opinion sequence labeler, we get the expres-
sion type as in MPQA (DSE or ESE): ESE-
NEGATIVE:+SBJTOBJJ+ESE-NEGATIVE.
Polarity pair and semantic relation. When two
opinions are directly connected through a link
in the semantic structure, we add the role label
as a feature.
</bodyText>
<figure confidence="0.84801775">
NMOD SBJ
The
A1
]
</figure>
<page confidence="0.987827">
103
</page>
<bodyText confidence="0.969465625">
Polarity pair and words along syntactic path. We
follow the path between the expressions and
add a feature for every word we pass: NEG-
ATIVE:+emboldened+NEGATIVE.
We also used the features we developed in (Jo-
hansson and Moschitti, 2010b) to represent relations
between expressions without taking polarity into ac-
count.
</bodyText>
<subsectionHeader confidence="0.995927">
4.3 Training the Model
</subsectionHeader>
<bodyText confidence="0.999907285714286">
To train the model – find w – we applied max-margin
estimation for structured outputs, a generalization of
the well-known support vector machine from binary
classification to prediction of structured objects.
Formally, for a training set T = {(xi, yi)}, where
the output space for the input xi is Yi, we state the
learning problem as a quadratic program:
</bodyText>
<equation confidence="0.993503333333333">
minimize,,, 11w112
subject to w()(xi,yi) − -1)(xi,yij)) &gt; A(yi,yij),
d(xi, yi) E T , yij E Yi
</equation>
<bodyText confidence="0.999973615384615">
Since real-world data tends to be noisy, we may
regularize to reduce overfitting and introduce a pa-
rameter C as in regular SVMs (Taskar et al., 2004).
The quadratic program is usually not solved directly
since the number of constraints precludes a direct
solution. Instead, an approximation is needed in
practice; we used SVMstruct (Tsochantaridis et al.,
2005; Joachims et al., 2009), which finds a solu-
tion by successively finding the most violated con-
straints and adding them to a working set. The
loss A was defined as 1 minus a weighted combi-
nation of polarity-labeled and unlabeled intersection
F-measure as described in Section 5.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999983111111111">
Opinion expression boundaries are hard to define
rigorously (Wiebe et al., 2005), so evaluations of
their quality typically use soft metrics. The MPQA
annotators used the overlap metric: an expression
is counted as correct if it overlaps with one in the
gold standard. This has also been used to evaluate
opinion extractors (Choi et al., 2006; Breck et al.,
2007). However, this metric has a number of prob-
lems: 1) it is possible to ”fool” the metric by creat-
ing expressions that cover the whole sentence; 2) it
does not give higher credit to output that is ”almost
perfect” rather than ”almost incorrect”. Therefore,
in (Johansson and Moschitti, 2010b), we measured
the intersection between the system output and the
gold standard: every compared segment is assigned
a score between 0 and 1, as opposed to strict or over-
lap scoring that only assigns 0 or 1. For compatibil-
ity we present results in both metrics.
</bodyText>
<subsectionHeader confidence="0.999338">
5.1 Evaluation of Segmentation with Polarity
</subsectionHeader>
<bodyText confidence="0.99975">
We first compared the two baselines to the new
integrated segmentation/polarity system. Table 1
shows the performance according to the intersec-
tion metric. Our first baseline consists of an expres-
sion segmenter and a polarity classifier (ES+PC),
while in the second baseline we also add the ex-
pression reranker (ER) as we did in (Johansson and
Moschitti, 2010b). The new reranker described in
this paper is referred to as the expression/polarity
reranker (EPR). We carried out the evaluation using
the same partition of the MPQA dataset as in our
previous work (Johansson and Moschitti, 2010b),
with 541 documents in the training set and 150 in
the test set.
</bodyText>
<table confidence="0.99947525">
System P R F
ES+PC 56.5 38.4 45.7
ES+ER+PC 53.8 44.5 48.8
ES+PC+EPR 54.7 45.6 49.7
</table>
<tableCaption confidence="0.999991">
Table 1: Results with intersection metric.
</tableCaption>
<bodyText confidence="0.999866">
The result shows that the reranking-based mod-
els give us significant boosts in recall, following
our previous results in (Johansson and Moschitti,
2010b), which also mainly improved the recall. The
precision shows a slight drop but much lower than
the recall improvement.
In addition, we see the benefit of the new reranker
with polarity interaction features. The system using
this reranker (ES+PC+EPR) outperforms the expres-
sion reranker (ES+ER+PC). The performance dif-
ferences are statistically significant according to a
permutation test: precision p &lt; 0.02, recall and F-
measure p &lt; 0.005.
</bodyText>
<subsectionHeader confidence="0.999664">
5.2 Comparison with Previous Results
</subsectionHeader>
<bodyText confidence="0.967357">
Since the results by Choi and Cardie (2010) are the
only ones that we are aware of, we carried out an
</bodyText>
<page confidence="0.997872">
104
</page>
<bodyText confidence="0.99961925">
evaluation in their setting.1 Table 2 shows our fig-
ures (for the two baselines and the new reranker)
along with theirs, referred to as C &amp; C (2010).
The table shows the scores for every polarity value.
For compatibility with their evaluation, we used the
overlap metric and carried out the evaluation us-
ing a 10-fold cross-validation procedure on a 400-
document subset of the MPQA corpus.
</bodyText>
<table confidence="0.9999112">
POSITIVE P R F
ES+PC 59.3 46.2 51.8
ES+ER+PC 53.1 50.9 52.0
ES+PC+EPR 58.2 49.3 53.4
C &amp; C (2010) 67.1 31.8 43.1
NEUTRAL P R F
ES+PC 61.0 49.3 54.3
ES+ER+PC 55.1 57.7 56.4
ES+PC+EPR 60.3 55.8 58.0
C &amp; C (2010) 66.6 31.9 43.1
NEGATIVE P R F
ES+PC 71.6 52.2 60.3
ES+ER+PC 65.4 58.2 61.6
ES+PC+EPR 67.6 59.9 63.5
C &amp; C (2010) 76.2 40.4 52.8
</table>
<tableCaption confidence="0.999951">
Table 2: Results with overlap metric.
</tableCaption>
<bodyText confidence="0.999837636363636">
The C &amp; C system shows a large precision
bias despite being optimized with respect to the
recall-promoting overlap metric. In recall and F-
measure, their system scores much lower than our
simplest baseline, which is in turn clearly outper-
formed by the stronger baseline and the polarity-
based reranker. The precision is lower than for C
&amp; C overall, but this is offset by recall boosts for
all polarities that are much larger than the precision
drops. The polarity-based reranker (ES+PC+EPR)
soundly outperforms all other systems.
</bodyText>
<sectionHeader confidence="0.99671" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.995865261904762">
We have studied the implementation of end-to-end
systems for opinion expression extraction and po-
larity labeling. We first showed that it was easy to
1In addition to polarity, their system also assigned opinion
intensity which we do not consider here.
improve over previous results simply by combining
an opinion extractor and a polarity classifier; the im-
provements were between 7.5 and 11 points in over-
lap F-measure.
However, our most interesting result is that a joint
model of expression extraction and polarity label-
ing significantly improves over the sequential ap-
proach. This model uses features describing the in-
teraction of opinions through linguistic structures.
This precludes exact inference, but we resorted to
a reranker. The model was trained using approx-
imate max-margin learning. The final system im-
proved over the baseline by 4 points in intersection
F-measure and 7 points in recall. The improvements
over Choi and Cardie (2010) ranged between 10 and
15 in overlap F-measure and between 17 and 24 in
recall.
This is not only of practical value but also con-
firms our linguistic intuitions that surface phenom-
ena such as syntax and semantic roles are used in
encoding the rhetorical organization of the sentence,
and that we can thus extract useful information from
those structures. This would also suggest that we
should leave the surface and instead process the dis-
course structure, and this has indeed been proposed
(Somasundaran et al., 2009). However, automatic
discourse structure analysis is still in its infancy
while syntactic and shallow semantic parsing are rel-
atively mature.
Interesting future work should be devoted to ad-
dress the use of structural kernels for the proposed
reranker. This would allow to better exploit syn-
tactic and shallow semantic structures, e.g. as in
(Moschitti, 2008), also applying lexical similarity
and syntactic kernels (Bloehdorn et al., 2006; Bloe-
hdorn and Moschitti, 2007a; Bloehdorn and Mos-
chitti, 2007b; Moschitti, 2009).
</bodyText>
<sectionHeader confidence="0.996525" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.995909142857143">
The research described in this paper has received
funding from the European Community’s Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant 231126: LivingKnowledge – Facts, Opin-
ions and Bias in Time, and under grant 247758:
Trustworthy Eternal Systems via Evolving Software,
Data and Knowledge (EternalS).
</bodyText>
<page confidence="0.998953">
105
</page>
<sectionHeader confidence="0.990017" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999560382352941">
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Proceedings of ECIR 2007, Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels. In
In Proceedings of CIKM ’07.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa, and
Alessandro Moschitti. 2006. Semantic kernels for text
classification based on topological measures of feature
similarity. In Proceedings of ICDM 06, Hong Kong,
2006.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In IJCAI
2007, Proceedings of the 20th International Joint Con-
ference on Artificial Intelligence, pages 2683–2688,
Hyderabad, India.
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 793–801, Honolulu, United
States.
Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their at-
tributes. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
269–274, Uppsala, Sweden.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 431–439, Sydney, Australia.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002), pages 1–8.
Thorsten Joachims, Thomas Finley, and Chun-Nam Yu.
2009. Cutting-plane training of structural SVMs. Ma-
chine Learning, 77(1):27–59.
Richard Johansson and Alessandro Moschitti. 2010a.
Reranking models in fine-grained opinion analysis. In
Proceedings of the 23rd International Conference of
Computational Linguistics (Coling 2010), pages 519–
527, Beijing, China.
Richard Johansson and Alessandro Moschitti. 2010b.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67–76, Uppsala, Sweden.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic–semantic analysis
with PropBank and NomBank. In CoNLL 2008:
Proceedings of the Twelfth Conference on Natural
Language Learning, pages 183–187, Manchester,
United Kingdom.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In HLT-NAACL 2004 Workshop: Frontiers
in Corpus Annotation, pages 24–31, Boston, United
States.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Pro-
ceeding of CIKM ’08, NY, USA.
Alessandro Moschitti. 2009. Syntactic and Seman-
tic Kernels for Short Text Pair Categorization. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 576–584,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71–105.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
EMNLP 2009: conference on Empirical Methods in
Natural Language Processing.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004.
Max-margin Markov networks. In Advances in Neu-
ral Information Processing Systems 16, Vancouver,
Canada.
Iannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin meth-
ods for structured and interdependent output variables.
Journal of Machine Learning Research, 6(Sep):1453–
1484.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165–210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of Human Lan-
guage Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational Linguistics, 35(3):399–433.
</reference>
<page confidence="0.997323">
106
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.609672">
<title confidence="0.9957435">Extracting Opinion Expressions and Their Polarities – Exploration Pipelines and Joint Models</title>
<author confidence="0.982016">Johansson</author>
<affiliation confidence="0.8603225">DISI, University of Via Sommarive 14, 38123 Trento (TN),</affiliation>
<abstract confidence="0.990217266666667">We investigate systems that identify opinion expressions and assigns polarities to the extracted expressions. In particular, we demonstrate the benefit of integrating opinion extraction and polarity classification into a joint model using features reflecting the global polarity structure. The model is trained using large-margin structured prediction methods. The system is evaluated on the MPQA opinion corpus, where we compare it to the only previously published end-to-end system for opinion expression extraction and polarity classification. The results show an improvement of between 10 and 15 absolute points in F-measure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Combined syntactic and semantic kernels for text classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ECIR 2007,</booktitle>
<location>Rome, Italy.</location>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Stephan Bloehdorn and Alessandro Moschitti. 2007a. Combined syntactic and semantic kernels for text classification. In Proceedings of ECIR 2007, Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structure and semantics for expressive text kernels. In</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM ’07.</booktitle>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Stephan Bloehdorn and Alessandro Moschitti. 2007b. Structure and semantics for expressive text kernels. In In Proceedings of CIKM ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Roberto Basili</author>
<author>Marco Cammisa</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Semantic kernels for text classification based on topological measures of feature similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of ICDM 06,</booktitle>
<location>Hong Kong,</location>
<marker>Bloehdorn, Basili, Cammisa, Moschitti, 2006</marker>
<rawString>Stephan Bloehdorn, Roberto Basili, Marco Cammisa, and Alessandro Moschitti. 2006. Semantic kernels for text classification based on topological measures of feature similarity. In Proceedings of ICDM 06, Hong Kong, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying expressions of opinion in context.</title>
<date>2007</date>
<booktitle>In IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2683--2688</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="2029" citStr="Breck et al. (2007)" startWordPosition="309" endWordPosition="312">; what its intensity is. This more complex formulation leads us deep into NLP territory; the methods employed here have been inspired by information extraction and semantic role labeling, combinatorial optimization and structured machine learning. A crucial step in the automatic analysis of opinion is to mark up the opinion expressions: the pieces of text allowing us to infer that someone has a particular feeling about some topic. Then, opinions can be assigned a polarity describing whether the feeling is positive, neutral or negative. These two tasks have generally been tackled in isolation. Breck et al. (2007) introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in (Johansson and Moschitti, 2010b); later we also added holder extraction (Johansson and Moschitti, 2010a). For the task of classifiying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al., 2009). While the tasks of expression detection and polarity classification have mostly been studied in isolation, Choi and Cardie (2010) developed a se</context>
<context position="5628" citStr="Breck et al. (2007)" startWordPosition="873" endWordPosition="876">s, not intensities or holders. The polarity takes the values POSITIVE, NEUTRAL, NEGATIVE, and BOTH; for compatibility with Choi and Cardie (2010), we mapped BOTH to NEUTRAL. 3 The Baselines In order to test our hypothesis against strong baselines, we developed two pipeline systems. The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al. (2009). The first of the two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al. (2007) and Choi et al. (2006). Sequence labeling techniques such as HMMs and CRFs are widely used for segmentation problems such as named entity recognition and noun chunk extraction. We trained a first-order labeler with the discriminative training method by Collins (2002) and used common features: words, POS, lemmas in a sliding window. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al. (2005). For the second baseline, we added our opinion expression reranker (Johansson and Moschitti, 2010b) on top of the expression sequence labeler. Given an expression, we use a c</context>
<context position="7840" citStr="Breck et al. (2007)" startWordPosition="1241" endWordPosition="1244">ill obviously depend on whether we can factorize the problem for a particular 4b. Choi and Cardie (2010) used a Markov factorization and could thus apply standard sequence labeling with a Viterbi arg max. However, in (Johansson and Moschitti, 2010b), we showed that a large improvement can be achieved if relations between possible expressions are considered; these relations can be syntactic or semantic in nature, for instance. This representation breaks the Markov assumption and the arg max becomes intractable. We instead used a reranking approximation: a Viterbibased sequence tagger following Breck et al. (2007) generated a manageable hypothesis set of complete segmentations, from which the reranking classifier picked one hypothesis as its final output. Since the set is small, no particular structure assumption (such 102 as Markovization) needs to be made, so the reranker can in principle use features of arbitrary complexity. We now adapt that approach to the problem of joint opinion expression segmentation and polarity classification. In that case, we not only need hypotheses generated by a sequence labeler, but also the polarity labelings output by a polarity classifier. The hypothesis generation t</context>
<context position="13644" citStr="Breck et al., 2007" startWordPosition="2158" endWordPosition="2161">s a solution by successively finding the most violated constraints and adding them to a working set. The loss A was defined as 1 minus a weighted combination of polarity-labeled and unlabeled intersection F-measure as described in Section 5. 5 Experiments Opinion expression boundaries are hard to define rigorously (Wiebe et al., 2005), so evaluations of their quality typically use soft metrics. The MPQA annotators used the overlap metric: an expression is counted as correct if it overlaps with one in the gold standard. This has also been used to evaluate opinion extractors (Choi et al., 2006; Breck et al., 2007). However, this metric has a number of problems: 1) it is possible to ”fool” the metric by creating expressions that cover the whole sentence; 2) it does not give higher credit to output that is ”almost perfect” rather than ”almost incorrect”. Therefore, in (Johansson and Moschitti, 2010b), we measured the intersection between the system output and the gold standard: every compared segment is assigned a score between 0 and 1, as opposed to strict or overlap scoring that only assigns 0 or 1. For compatibility we present results in both metrics. 5.1 Evaluation of Segmentation with Polarity We fi</context>
</contexts>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. In IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 2683–2688, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Learning with compositional semantics as structural inference for subsentential sentiment analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>793--801</pages>
<location>Honolulu, United States.</location>
<contexts>
<context position="3761" citStr="Choi and Cardie (2008)" startWordPosition="576" endWordPosition="579">o use simple local features. In contrast, in (Johansson and Moschitti, 2010b), we showed that global structure matters: opinions interact to a large extent, and we can learn about their interactions on the opinion level by means of their interactions on the syntactic and semantic levels. It is intuitive that this should also be valid when polarities enter the 101 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 101–106, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics picture – this was also noted by Choi and Cardie (2008). Evaluative adjectives referring to the same evaluee may cluster together in the same clause or be dominated by a verb of categorization; opinions with opposite polarities may be conjoined through a contrastive discourse connective such as but. In this paper, we first implement two strong baselines consisting of pipelines of opinion expression segmentation and polarity labeling and compare them to the joint opinion extractor and polarity classifier by Choi and Cardie (2010). Secondly, we extend the global structure approach and add features reflecting the polarity structure of the sentence. O</context>
</contexts>
<marker>Choi, Cardie, 2008</marker>
<rawString>Yejin Choi and Claire Cardie. 2008. Learning with compositional semantics as structural inference for subsentential sentiment analysis. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793–801, Honolulu, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Hierarchical sequential learning for extracting opinions and their attributes.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>269--274</pages>
<location>Uppsala,</location>
<contexts>
<context position="2614" citStr="Choi and Cardie (2010)" startWordPosition="404" endWordPosition="407">led in isolation. Breck et al. (2007) introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in (Johansson and Moschitti, 2010b); later we also added holder extraction (Johansson and Moschitti, 2010a). For the task of classifiying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al., 2009). While the tasks of expression detection and polarity classification have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned polarities. This is so far the only published result on joint opinion segmentation and polarity classification. However, their experiment lacked the obvious baseline: a standard pipeline consisting of an expression identifier followed by a polarity classifier. In addition, while theirs is the first end-to-end system for expression extraction with polarities, it is still a sequence labeler, which, by construction, is restricted to use simple local features. In contrast, in (Johansson and Moschitti, 2010</context>
<context position="4240" citStr="Choi and Cardie (2010)" startWordPosition="650" endWordPosition="653">6, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics picture – this was also noted by Choi and Cardie (2008). Evaluative adjectives referring to the same evaluee may cluster together in the same clause or be dominated by a verb of categorization; opinions with opposite polarities may be conjoined through a contrastive discourse connective such as but. In this paper, we first implement two strong baselines consisting of pipelines of opinion expression segmentation and polarity labeling and compare them to the joint opinion extractor and polarity classifier by Choi and Cardie (2010). Secondly, we extend the global structure approach and add features reflecting the polarity structure of the sentence. Our systems were superior by between 8 and 14 absolute F-measure points. 2 The MPQA Opinion Corpus Our system was developed using version 2.0 of the MPQA corpus (Wiebe et al., 2005). The central building block in the MPQA annotation is the opinion expression. Opinion expressions belong to two categories: Direct subjective expressions (DSEs) are explicit mentions of opinion whereas expressive subjective elements (ESEs) signal the attitude of the speaker by the choice of words.</context>
<context position="7325" citStr="Choi and Cardie (2010)" startWordPosition="1158" endWordPosition="1161">pinion extraction task as a structured prediction problem y� = arg maxy w ·-b(x, y). where w is a weight vector and 4b a feature extractor representing a sentence x and a set y of polaritylabeled opinions. This is a high-level formulation – we still need an inference procedure for the arg max and a learner to estimate w on a training set. 4.1 Approximate Inference Since there is a combinatorial number of ways to segment a sentence and label the segments with polarities, the tractability of the arg max operation will obviously depend on whether we can factorize the problem for a particular 4b. Choi and Cardie (2010) used a Markov factorization and could thus apply standard sequence labeling with a Viterbi arg max. However, in (Johansson and Moschitti, 2010b), we showed that a large improvement can be achieved if relations between possible expressions are considered; these relations can be syntactic or semantic in nature, for instance. This representation breaks the Markov assumption and the arg max becomes intractable. We instead used a reranking approximation: a Viterbibased sequence tagger following Breck et al. (2007) generated a manageable hypothesis set of complete segmentations, from which the rera</context>
<context position="15697" citStr="Choi and Cardie (2010)" startWordPosition="2493" endWordPosition="2496">ve us significant boosts in recall, following our previous results in (Johansson and Moschitti, 2010b), which also mainly improved the recall. The precision shows a slight drop but much lower than the recall improvement. In addition, we see the benefit of the new reranker with polarity interaction features. The system using this reranker (ES+PC+EPR) outperforms the expression reranker (ES+ER+PC). The performance differences are statistically significant according to a permutation test: precision p &lt; 0.02, recall and Fmeasure p &lt; 0.005. 5.2 Comparison with Previous Results Since the results by Choi and Cardie (2010) are the only ones that we are aware of, we carried out an 104 evaluation in their setting.1 Table 2 shows our figures (for the two baselines and the new reranker) along with theirs, referred to as C &amp; C (2010). The table shows the scores for every polarity value. For compatibility with their evaluation, we used the overlap metric and carried out the evaluation using a 10-fold cross-validation procedure on a 400- document subset of the MPQA corpus. POSITIVE P R F ES+PC 59.3 46.2 51.8 ES+ER+PC 53.1 50.9 52.0 ES+PC+EPR 58.2 49.3 53.4 C &amp; C (2010) 67.1 31.8 43.1 NEUTRAL P R F ES+PC 61.0 49.3 54.3</context>
<context position="18016" citStr="Choi and Cardie (2010)" startWordPosition="2887" endWordPosition="2890"> a polarity classifier; the improvements were between 7.5 and 11 points in overlap F-measure. However, our most interesting result is that a joint model of expression extraction and polarity labeling significantly improves over the sequential approach. This model uses features describing the interaction of opinions through linguistic structures. This precludes exact inference, but we resorted to a reranker. The model was trained using approximate max-margin learning. The final system improved over the baseline by 4 points in intersection F-measure and 7 points in recall. The improvements over Choi and Cardie (2010) ranged between 10 and 15 in overlap F-measure and between 17 and 24 in recall. This is not only of practical value but also confirms our linguistic intuitions that surface phenomena such as syntax and semantic roles are used in encoding the rhetorical organization of the sentence, and that we can thus extract useful information from those structures. This would also suggest that we should leave the surface and instead process the discourse structure, and this has indeed been proposed (Somasundaran et al., 2009). However, automatic discourse structure analysis is still in its infancy while syn</context>
</contexts>
<marker>Choi, Cardie, 2010</marker>
<rawString>Yejin Choi and Claire Cardie. 2010. Hierarchical sequential learning for extracting opinions and their attributes. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 269–274, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Eric Breck</author>
<author>Claire Cardie</author>
</authors>
<title>Joint extraction of entities and relations for opinion recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>431--439</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="5651" citStr="Choi et al. (2006)" startWordPosition="878" endWordPosition="881">lders. The polarity takes the values POSITIVE, NEUTRAL, NEGATIVE, and BOTH; for compatibility with Choi and Cardie (2010), we mapped BOTH to NEUTRAL. 3 The Baselines In order to test our hypothesis against strong baselines, we developed two pipeline systems. The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al. (2009). The first of the two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al. (2007) and Choi et al. (2006). Sequence labeling techniques such as HMMs and CRFs are widely used for segmentation problems such as named entity recognition and noun chunk extraction. We trained a first-order labeler with the discriminative training method by Collins (2002) and used common features: words, POS, lemmas in a sliding window. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al. (2005). For the second baseline, we added our opinion expression reranker (Johansson and Moschitti, 2010b) on top of the expression sequence labeler. Given an expression, we use a classifier to assign a p</context>
<context position="13623" citStr="Choi et al., 2006" startWordPosition="2154" endWordPosition="2157">, 2009), which finds a solution by successively finding the most violated constraints and adding them to a working set. The loss A was defined as 1 minus a weighted combination of polarity-labeled and unlabeled intersection F-measure as described in Section 5. 5 Experiments Opinion expression boundaries are hard to define rigorously (Wiebe et al., 2005), so evaluations of their quality typically use soft metrics. The MPQA annotators used the overlap metric: an expression is counted as correct if it overlaps with one in the gold standard. This has also been used to evaluate opinion extractors (Choi et al., 2006; Breck et al., 2007). However, this metric has a number of problems: 1) it is possible to ”fool” the metric by creating expressions that cover the whole sentence; 2) it does not give higher credit to output that is ”almost perfect” rather than ”almost incorrect”. Therefore, in (Johansson and Moschitti, 2010b), we measured the intersection between the system output and the gold standard: every compared segment is assigned a score between 0 and 1, as opposed to strict or overlap scoring that only assigns 0 or 1. For compatibility we present results in both metrics. 5.1 Evaluation of Segmentatio</context>
</contexts>
<marker>Choi, Breck, Cardie, 2006</marker>
<rawString>Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint extraction of entities and relations for opinion recognition. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 431–439, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>1--8</pages>
<contexts>
<context position="5896" citStr="Collins (2002)" startWordPosition="917" endWordPosition="918"> systems. The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al. (2009). The first of the two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al. (2007) and Choi et al. (2006). Sequence labeling techniques such as HMMs and CRFs are widely used for segmentation problems such as named entity recognition and noun chunk extraction. We trained a first-order labeler with the discriminative training method by Collins (2002) and used common features: words, POS, lemmas in a sliding window. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al. (2005). For the second baseline, we added our opinion expression reranker (Johansson and Moschitti, 2010b) on top of the expression sequence labeler. Given an expression, we use a classifier to assign a polarity value: positive, neutral, or negative. We trained linear support vector machines to carry out this classification. The problem of polarity classification has been studied in detail by Wilson et al. (2009), who used a set of carefully dev</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
<author>Thomas Finley</author>
<author>Chun-Nam Yu</author>
</authors>
<title>Cutting-plane training of structural SVMs.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<volume>77</volume>
<issue>1</issue>
<contexts>
<context position="13013" citStr="Joachims et al., 2009" startWordPosition="2053" endWordPosition="2056">rmally, for a training set T = {(xi, yi)}, where the output space for the input xi is Yi, we state the learning problem as a quadratic program: minimize,,, 11w112 subject to w()(xi,yi) − -1)(xi,yij)) &gt; A(yi,yij), d(xi, yi) E T , yij E Yi Since real-world data tends to be noisy, we may regularize to reduce overfitting and introduce a parameter C as in regular SVMs (Taskar et al., 2004). The quadratic program is usually not solved directly since the number of constraints precludes a direct solution. Instead, an approximation is needed in practice; we used SVMstruct (Tsochantaridis et al., 2005; Joachims et al., 2009), which finds a solution by successively finding the most violated constraints and adding them to a working set. The loss A was defined as 1 minus a weighted combination of polarity-labeled and unlabeled intersection F-measure as described in Section 5. 5 Experiments Opinion expression boundaries are hard to define rigorously (Wiebe et al., 2005), so evaluations of their quality typically use soft metrics. The MPQA annotators used the overlap metric: an expression is counted as correct if it overlaps with one in the gold standard. This has also been used to evaluate opinion extractors (Choi et</context>
</contexts>
<marker>Joachims, Finley, Yu, 2009</marker>
<rawString>Thorsten Joachims, Thomas Finley, and Chun-Nam Yu. 2009. Cutting-plane training of structural SVMs. Machine Learning, 77(1):27–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Reranking models in fine-grained opinion analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference of Computational Linguistics (Coling</booktitle>
<pages>519--527</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2248" citStr="Johansson and Moschitti, 2010" startWordPosition="347" endWordPosition="350">ation and structured machine learning. A crucial step in the automatic analysis of opinion is to mark up the opinion expressions: the pieces of text allowing us to infer that someone has a particular feeling about some topic. Then, opinions can be assigned a polarity describing whether the feeling is positive, neutral or negative. These two tasks have generally been tackled in isolation. Breck et al. (2007) introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in (Johansson and Moschitti, 2010b); later we also added holder extraction (Johansson and Moschitti, 2010a). For the task of classifiying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al., 2009). While the tasks of expression detection and polarity classification have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned polarities. This is so far the only published result on joint opinion segmentation and polarity classification. However, their experiment lac</context>
<context position="6151" citStr="Johansson and Moschitti, 2010" startWordPosition="957" endWordPosition="960">two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al. (2007) and Choi et al. (2006). Sequence labeling techniques such as HMMs and CRFs are widely used for segmentation problems such as named entity recognition and noun chunk extraction. We trained a first-order labeler with the discriminative training method by Collins (2002) and used common features: words, POS, lemmas in a sliding window. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al. (2005). For the second baseline, we added our opinion expression reranker (Johansson and Moschitti, 2010b) on top of the expression sequence labeler. Given an expression, we use a classifier to assign a polarity value: positive, neutral, or negative. We trained linear support vector machines to carry out this classification. The problem of polarity classification has been studied in detail by Wilson et al. (2009), who used a set of carefully devised linguistic features. Our classifier is simpler and is based on fairly shallow features: words, POS, subjectivity clues, and bigrams inside and around the expression. 4 The Joint Model We formulate the opinion extraction task as a structured predictio</context>
<context position="7468" citStr="Johansson and Moschitti, 2010" startWordPosition="1182" endWordPosition="1186">or representing a sentence x and a set y of polaritylabeled opinions. This is a high-level formulation – we still need an inference procedure for the arg max and a learner to estimate w on a training set. 4.1 Approximate Inference Since there is a combinatorial number of ways to segment a sentence and label the segments with polarities, the tractability of the arg max operation will obviously depend on whether we can factorize the problem for a particular 4b. Choi and Cardie (2010) used a Markov factorization and could thus apply standard sequence labeling with a Viterbi arg max. However, in (Johansson and Moschitti, 2010b), we showed that a large improvement can be achieved if relations between possible expressions are considered; these relations can be syntactic or semantic in nature, for instance. This representation breaks the Markov assumption and the arg max becomes intractable. We instead used a reranking approximation: a Viterbibased sequence tagger following Breck et al. (2007) generated a manageable hypothesis set of complete segmentations, from which the reranking classifier picked one hypothesis as its final output. Since the set is small, no particular structure assumption (such 102 as Markovizati</context>
<context position="12073" citStr="Johansson and Moschitti, 2010" startWordPosition="1899" endWordPosition="1903">ssions: NEGATIVE+NEGATIVE+appeasement+terrorists. Polarity pair and types and syntactic path. From the opinion sequence labeler, we get the expression type as in MPQA (DSE or ESE): ESENEGATIVE:+SBJTOBJJ+ESE-NEGATIVE. Polarity pair and semantic relation. When two opinions are directly connected through a link in the semantic structure, we add the role label as a feature. NMOD SBJ The A1 ] 103 Polarity pair and words along syntactic path. We follow the path between the expressions and add a feature for every word we pass: NEGATIVE:+emboldened+NEGATIVE. We also used the features we developed in (Johansson and Moschitti, 2010b) to represent relations between expressions without taking polarity into account. 4.3 Training the Model To train the model – find w – we applied max-margin estimation for structured outputs, a generalization of the well-known support vector machine from binary classification to prediction of structured objects. Formally, for a training set T = {(xi, yi)}, where the output space for the input xi is Yi, we state the learning problem as a quadratic program: minimize,,, 11w112 subject to w()(xi,yi) − -1)(xi,yij)) &gt; A(yi,yij), d(xi, yi) E T , yij E Yi Since real-world data tends to be noisy, we </context>
<context position="13932" citStr="Johansson and Moschitti, 2010" startWordPosition="2207" endWordPosition="2210">ndaries are hard to define rigorously (Wiebe et al., 2005), so evaluations of their quality typically use soft metrics. The MPQA annotators used the overlap metric: an expression is counted as correct if it overlaps with one in the gold standard. This has also been used to evaluate opinion extractors (Choi et al., 2006; Breck et al., 2007). However, this metric has a number of problems: 1) it is possible to ”fool” the metric by creating expressions that cover the whole sentence; 2) it does not give higher credit to output that is ”almost perfect” rather than ”almost incorrect”. Therefore, in (Johansson and Moschitti, 2010b), we measured the intersection between the system output and the gold standard: every compared segment is assigned a score between 0 and 1, as opposed to strict or overlap scoring that only assigns 0 or 1. For compatibility we present results in both metrics. 5.1 Evaluation of Segmentation with Polarity We first compared the two baselines to the new integrated segmentation/polarity system. Table 1 shows the performance according to the intersection metric. Our first baseline consists of an expression segmenter and a polarity classifier (ES+PC), while in the second baseline we also add the ex</context>
<context position="15175" citStr="Johansson and Moschitti, 2010" startWordPosition="2412" endWordPosition="2415">ker (ER) as we did in (Johansson and Moschitti, 2010b). The new reranker described in this paper is referred to as the expression/polarity reranker (EPR). We carried out the evaluation using the same partition of the MPQA dataset as in our previous work (Johansson and Moschitti, 2010b), with 541 documents in the training set and 150 in the test set. System P R F ES+PC 56.5 38.4 45.7 ES+ER+PC 53.8 44.5 48.8 ES+PC+EPR 54.7 45.6 49.7 Table 1: Results with intersection metric. The result shows that the reranking-based models give us significant boosts in recall, following our previous results in (Johansson and Moschitti, 2010b), which also mainly improved the recall. The precision shows a slight drop but much lower than the recall improvement. In addition, we see the benefit of the new reranker with polarity interaction features. The system using this reranker (ES+PC+EPR) outperforms the expression reranker (ES+ER+PC). The performance differences are statistically significant according to a permutation test: precision p &lt; 0.02, recall and Fmeasure p &lt; 0.005. 5.2 Comparison with Previous Results Since the results by Choi and Cardie (2010) are the only ones that we are aware of, we carried out an 104 evaluation in t</context>
</contexts>
<marker>Johansson, Moschitti, 2010</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2010a. Reranking models in fine-grained opinion analysis. In Proceedings of the 23rd International Conference of Computational Linguistics (Coling 2010), pages 519– 527, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Syntactic and semantic structure for opinion expression detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>67--76</pages>
<location>Uppsala,</location>
<contexts>
<context position="2248" citStr="Johansson and Moschitti, 2010" startWordPosition="347" endWordPosition="350">ation and structured machine learning. A crucial step in the automatic analysis of opinion is to mark up the opinion expressions: the pieces of text allowing us to infer that someone has a particular feeling about some topic. Then, opinions can be assigned a polarity describing whether the feeling is positive, neutral or negative. These two tasks have generally been tackled in isolation. Breck et al. (2007) introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in (Johansson and Moschitti, 2010b); later we also added holder extraction (Johansson and Moschitti, 2010a). For the task of classifiying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al., 2009). While the tasks of expression detection and polarity classification have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned polarities. This is so far the only published result on joint opinion segmentation and polarity classification. However, their experiment lac</context>
<context position="6151" citStr="Johansson and Moschitti, 2010" startWordPosition="957" endWordPosition="960">two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al. (2007) and Choi et al. (2006). Sequence labeling techniques such as HMMs and CRFs are widely used for segmentation problems such as named entity recognition and noun chunk extraction. We trained a first-order labeler with the discriminative training method by Collins (2002) and used common features: words, POS, lemmas in a sliding window. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al. (2005). For the second baseline, we added our opinion expression reranker (Johansson and Moschitti, 2010b) on top of the expression sequence labeler. Given an expression, we use a classifier to assign a polarity value: positive, neutral, or negative. We trained linear support vector machines to carry out this classification. The problem of polarity classification has been studied in detail by Wilson et al. (2009), who used a set of carefully devised linguistic features. Our classifier is simpler and is based on fairly shallow features: words, POS, subjectivity clues, and bigrams inside and around the expression. 4 The Joint Model We formulate the opinion extraction task as a structured predictio</context>
<context position="7468" citStr="Johansson and Moschitti, 2010" startWordPosition="1182" endWordPosition="1186">or representing a sentence x and a set y of polaritylabeled opinions. This is a high-level formulation – we still need an inference procedure for the arg max and a learner to estimate w on a training set. 4.1 Approximate Inference Since there is a combinatorial number of ways to segment a sentence and label the segments with polarities, the tractability of the arg max operation will obviously depend on whether we can factorize the problem for a particular 4b. Choi and Cardie (2010) used a Markov factorization and could thus apply standard sequence labeling with a Viterbi arg max. However, in (Johansson and Moschitti, 2010b), we showed that a large improvement can be achieved if relations between possible expressions are considered; these relations can be syntactic or semantic in nature, for instance. This representation breaks the Markov assumption and the arg max becomes intractable. We instead used a reranking approximation: a Viterbibased sequence tagger following Breck et al. (2007) generated a manageable hypothesis set of complete segmentations, from which the reranking classifier picked one hypothesis as its final output. Since the set is small, no particular structure assumption (such 102 as Markovizati</context>
<context position="12073" citStr="Johansson and Moschitti, 2010" startWordPosition="1899" endWordPosition="1903">ssions: NEGATIVE+NEGATIVE+appeasement+terrorists. Polarity pair and types and syntactic path. From the opinion sequence labeler, we get the expression type as in MPQA (DSE or ESE): ESENEGATIVE:+SBJTOBJJ+ESE-NEGATIVE. Polarity pair and semantic relation. When two opinions are directly connected through a link in the semantic structure, we add the role label as a feature. NMOD SBJ The A1 ] 103 Polarity pair and words along syntactic path. We follow the path between the expressions and add a feature for every word we pass: NEGATIVE:+emboldened+NEGATIVE. We also used the features we developed in (Johansson and Moschitti, 2010b) to represent relations between expressions without taking polarity into account. 4.3 Training the Model To train the model – find w – we applied max-margin estimation for structured outputs, a generalization of the well-known support vector machine from binary classification to prediction of structured objects. Formally, for a training set T = {(xi, yi)}, where the output space for the input xi is Yi, we state the learning problem as a quadratic program: minimize,,, 11w112 subject to w()(xi,yi) − -1)(xi,yij)) &gt; A(yi,yij), d(xi, yi) E T , yij E Yi Since real-world data tends to be noisy, we </context>
<context position="13932" citStr="Johansson and Moschitti, 2010" startWordPosition="2207" endWordPosition="2210">ndaries are hard to define rigorously (Wiebe et al., 2005), so evaluations of their quality typically use soft metrics. The MPQA annotators used the overlap metric: an expression is counted as correct if it overlaps with one in the gold standard. This has also been used to evaluate opinion extractors (Choi et al., 2006; Breck et al., 2007). However, this metric has a number of problems: 1) it is possible to ”fool” the metric by creating expressions that cover the whole sentence; 2) it does not give higher credit to output that is ”almost perfect” rather than ”almost incorrect”. Therefore, in (Johansson and Moschitti, 2010b), we measured the intersection between the system output and the gold standard: every compared segment is assigned a score between 0 and 1, as opposed to strict or overlap scoring that only assigns 0 or 1. For compatibility we present results in both metrics. 5.1 Evaluation of Segmentation with Polarity We first compared the two baselines to the new integrated segmentation/polarity system. Table 1 shows the performance according to the intersection metric. Our first baseline consists of an expression segmenter and a polarity classifier (ES+PC), while in the second baseline we also add the ex</context>
<context position="15175" citStr="Johansson and Moschitti, 2010" startWordPosition="2412" endWordPosition="2415">ker (ER) as we did in (Johansson and Moschitti, 2010b). The new reranker described in this paper is referred to as the expression/polarity reranker (EPR). We carried out the evaluation using the same partition of the MPQA dataset as in our previous work (Johansson and Moschitti, 2010b), with 541 documents in the training set and 150 in the test set. System P R F ES+PC 56.5 38.4 45.7 ES+ER+PC 53.8 44.5 48.8 ES+PC+EPR 54.7 45.6 49.7 Table 1: Results with intersection metric. The result shows that the reranking-based models give us significant boosts in recall, following our previous results in (Johansson and Moschitti, 2010b), which also mainly improved the recall. The precision shows a slight drop but much lower than the recall improvement. In addition, we see the benefit of the new reranker with polarity interaction features. The system using this reranker (ES+PC+EPR) outperforms the expression reranker (ES+ER+PC). The performance differences are statistically significant according to a permutation test: precision p &lt; 0.02, recall and Fmeasure p &lt; 0.005. 5.2 Comparison with Previous Results Since the results by Choi and Cardie (2010) are the only ones that we are aware of, we carried out an 104 evaluation in t</context>
</contexts>
<marker>Johansson, Moschitti, 2010</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2010b. Syntactic and semantic structure for opinion expression detection. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 67–76, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based syntactic–semantic analysis with PropBank and NomBank.</title>
<date>2008</date>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning,</booktitle>
<pages>183--187</pages>
<location>Manchester, United Kingdom.</location>
<contexts>
<context position="9660" citStr="Johansson and Nugues (2008)" startWordPosition="1521" endWordPosition="1524"> [appeasement] [emboldened] the [terrorists] and in the second step we add polarity values: The [appeasement]− emboldened the [terrorists]− The [appeasement]− [emboldened]+ the [terrorists]− The [appeasement]0 emboldened the [terrorists]− The [appeasement]− [emboldened]0 the [terrorists]− 4.2 Features of the Joint Model The features used by the joint opinion segmenter and polarity classifier are based on pairs of opinions: basic features extracted from each expression such as polarities and words, and relational features describing their interaction. To extract relations we used the parser by Johansson and Nugues (2008) to annotate sentences with dependencies and shallow semantics in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows the sentence the appeasement emboldened the terrorists, where appeasement and terrorists are opinions with negative polarity, with dependency syntax (above the text) and a predicate– argument structure (below). The predicate emboldened, an instance of the PropBank frame embolden.01, has two semantic arguments: the Agent (A0) and the Theme (A1), realized syntactically as a subject and a direct object, respectively. OBJ NMOD [appeasemen</context>
<context position="11011" citStr="Johansson and Nugues, 2008" startWordPosition="1730" endWordPosition="1733">g novel features that take the polarities of the expressions into account. The examples are given with respect to the two expressions (appeasement and terrorists) in Figure 1. Base polarity classifier score. Sum of the scores from the polarity classifier for every opinion. Polarity pair. For every pair of opinions in the sentence, we add the pair of polarities: NEGATIVE+NEGATIVE. Polarity pair and syntactic path. For a pair of opinions, we use the polarities and a representation of the path through the syntax tree between the expressions, following standard practice from dependency-based SRL (Johansson and Nugues, 2008): NEGATIVE+SBJTOBJJ+NEGATIVE. Polarity pair and syntactic dominance. In addition to the detailed syntactic path, we use a simpler feature based on dominance, i.e. that one expression is above the other in the syntax tree. In the example, no such feature is extracted since neither of the expressions dominates the other. Polarity pair and word pair. The polarity pair concatenated with the words of the closest nodes of the two expressions: NEGATIVE+NEGATIVE+appeasement+terrorists. Polarity pair and types and syntactic path. From the opinion sequence labeler, we get the expression type as in MPQA </context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic–semantic analysis with PropBank and NomBank. In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning, pages 183–187, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>The NomBank project: An interim report.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation,</booktitle>
<pages>24--31</pages>
<location>Boston, United States.</location>
<contexts>
<context position="9794" citStr="Meyers et al., 2004" startWordPosition="1542" endWordPosition="1545">he [appeasement]− [emboldened]+ the [terrorists]− The [appeasement]0 emboldened the [terrorists]− The [appeasement]− [emboldened]0 the [terrorists]− 4.2 Features of the Joint Model The features used by the joint opinion segmenter and polarity classifier are based on pairs of opinions: basic features extracted from each expression such as polarities and words, and relational features describing their interaction. To extract relations we used the parser by Johansson and Nugues (2008) to annotate sentences with dependencies and shallow semantics in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows the sentence the appeasement emboldened the terrorists, where appeasement and terrorists are opinions with negative polarity, with dependency syntax (above the text) and a predicate– argument structure (below). The predicate emboldened, an instance of the PropBank frame embolden.01, has two semantic arguments: the Agent (A0) and the Theme (A1), realized syntactically as a subject and a direct object, respectively. OBJ NMOD [appeasement] emboldened the [terrorists A0 embolden.01 Figure 1: Syntactic and shallow semantic structure. The model used the following novel fe</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The NomBank project: An interim report. In HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation, pages 24–31, Boston, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Kernel methods, syntax and semantics for relational text categorization.</title>
<date>2008</date>
<booktitle>In Proceeding of CIKM ’08,</booktitle>
<location>NY, USA.</location>
<marker>Moschitti, 2008</marker>
<rawString>Alessandro Moschitti. 2008. Kernel methods, syntax and semantics for relational text categorization. In Proceeding of CIKM ’08, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Syntactic and Semantic Kernels for Short Text Pair Categorization.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>576--584</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<marker>Moschitti, 2009</marker>
<rawString>Alessandro Moschitti. 2009. Syntactic and Semantic Kernels for Short Text Pair Categorization. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 576–584, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="9760" citStr="Palmer et al., 2005" startWordPosition="1536" endWordPosition="1539">t]− emboldened the [terrorists]− The [appeasement]− [emboldened]+ the [terrorists]− The [appeasement]0 emboldened the [terrorists]− The [appeasement]− [emboldened]0 the [terrorists]− 4.2 Features of the Joint Model The features used by the joint opinion segmenter and polarity classifier are based on pairs of opinions: basic features extracted from each expression such as polarities and words, and relational features describing their interaction. To extract relations we used the parser by Johansson and Nugues (2008) to annotate sentences with dependencies and shallow semantics in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows the sentence the appeasement emboldened the terrorists, where appeasement and terrorists are opinions with negative polarity, with dependency syntax (above the text) and a predicate– argument structure (below). The predicate emboldened, an instance of the PropBank frame embolden.01, has two semantic arguments: the Agent (A0) and the Theme (A1), realized syntactically as a subject and a direct object, respectively. OBJ NMOD [appeasement] emboldened the [terrorists A0 embolden.01 Figure 1: Syntactic and shallow semantic structure. The</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Galileo Namata</author>
<author>Janyce Wiebe</author>
<author>Lise Getoor</author>
</authors>
<title>Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP 2009: conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="18533" citStr="Somasundaran et al., 2009" startWordPosition="2974" endWordPosition="2977">ne by 4 points in intersection F-measure and 7 points in recall. The improvements over Choi and Cardie (2010) ranged between 10 and 15 in overlap F-measure and between 17 and 24 in recall. This is not only of practical value but also confirms our linguistic intuitions that surface phenomena such as syntax and semantic roles are used in encoding the rhetorical organization of the sentence, and that we can thus extract useful information from those structures. This would also suggest that we should leave the surface and instead process the discourse structure, and this has indeed been proposed (Somasundaran et al., 2009). However, automatic discourse structure analysis is still in its infancy while syntactic and shallow semantic parsing are relatively mature. Interesting future work should be devoted to address the use of structural kernels for the proposed reranker. This would allow to better exploit syntactic and shallow semantic structures, e.g. as in (Moschitti, 2008), also applying lexical similarity and syntactic kernels (Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Moschitti, 2009). Acknowledgements The research described in this paper has received funding fro</context>
</contexts>
<marker>Somasundaran, Namata, Wiebe, Getoor, 2009</marker>
<rawString>Swapna Somasundaran, Galileo Namata, Janyce Wiebe, and Lise Getoor. 2009. Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. In Proceedings of EMNLP 2009: conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Carlos Guestrin</author>
<author>Daphne Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems 16,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="12778" citStr="Taskar et al., 2004" startWordPosition="2018" endWordPosition="2021">.3 Training the Model To train the model – find w – we applied max-margin estimation for structured outputs, a generalization of the well-known support vector machine from binary classification to prediction of structured objects. Formally, for a training set T = {(xi, yi)}, where the output space for the input xi is Yi, we state the learning problem as a quadratic program: minimize,,, 11w112 subject to w()(xi,yi) − -1)(xi,yij)) &gt; A(yi,yij), d(xi, yi) E T , yij E Yi Since real-world data tends to be noisy, we may regularize to reduce overfitting and introduce a parameter C as in regular SVMs (Taskar et al., 2004). The quadratic program is usually not solved directly since the number of constraints precludes a direct solution. Instead, an approximation is needed in practice; we used SVMstruct (Tsochantaridis et al., 2005; Joachims et al., 2009), which finds a solution by successively finding the most violated constraints and adding them to a working set. The loss A was defined as 1 minus a weighted combination of polarity-labeled and unlabeled intersection F-measure as described in Section 5. 5 Experiments Opinion expression boundaries are hard to define rigorously (Wiebe et al., 2005), so evaluations </context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2004</marker>
<rawString>Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004. Max-margin Markov networks. In Advances in Neural Information Processing Systems 16, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iannis Tsochantaridis</author>
<author>Thorsten Joachims</author>
<author>Thomas Hofmann</author>
<author>Yasemin Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>6</volume>
<pages>1484</pages>
<contexts>
<context position="12989" citStr="Tsochantaridis et al., 2005" startWordPosition="2049" endWordPosition="2052">ion of structured objects. Formally, for a training set T = {(xi, yi)}, where the output space for the input xi is Yi, we state the learning problem as a quadratic program: minimize,,, 11w112 subject to w()(xi,yi) − -1)(xi,yij)) &gt; A(yi,yij), d(xi, yi) E T , yij E Yi Since real-world data tends to be noisy, we may regularize to reduce overfitting and introduce a parameter C as in regular SVMs (Taskar et al., 2004). The quadratic program is usually not solved directly since the number of constraints precludes a direct solution. Instead, an approximation is needed in practice; we used SVMstruct (Tsochantaridis et al., 2005; Joachims et al., 2009), which finds a solution by successively finding the most violated constraints and adding them to a working set. The loss A was defined as 1 minus a weighted combination of polarity-labeled and unlabeled intersection F-measure as described in Section 5. 5 Experiments Opinion expression boundaries are hard to define rigorously (Wiebe et al., 2005), so evaluations of their quality typically use soft metrics. The MPQA annotators used the overlap metric: an expression is counted as correct if it overlaps with one in the gold standard. This has also been used to evaluate opi</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>Iannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6(Sep):1453– 1484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="4541" citStr="Wiebe et al., 2005" startWordPosition="700" endWordPosition="703"> polarities may be conjoined through a contrastive discourse connective such as but. In this paper, we first implement two strong baselines consisting of pipelines of opinion expression segmentation and polarity labeling and compare them to the joint opinion extractor and polarity classifier by Choi and Cardie (2010). Secondly, we extend the global structure approach and add features reflecting the polarity structure of the sentence. Our systems were superior by between 8 and 14 absolute F-measure points. 2 The MPQA Opinion Corpus Our system was developed using version 2.0 of the MPQA corpus (Wiebe et al., 2005). The central building block in the MPQA annotation is the opinion expression. Opinion expressions belong to two categories: Direct subjective expressions (DSEs) are explicit mentions of opinion whereas expressive subjective elements (ESEs) signal the attitude of the speaker by the choice of words. Opinions have two features: polarity and intensity, and most expressions are also associated with a holder, also called source. In this work, we only consider polarities, not intensities or holders. The polarity takes the values POSITIVE, NEUTRAL, NEGATIVE, and BOTH; for compatibility with Choi and </context>
<context position="13361" citStr="Wiebe et al., 2005" startWordPosition="2110" endWordPosition="2113">n regular SVMs (Taskar et al., 2004). The quadratic program is usually not solved directly since the number of constraints precludes a direct solution. Instead, an approximation is needed in practice; we used SVMstruct (Tsochantaridis et al., 2005; Joachims et al., 2009), which finds a solution by successively finding the most violated constraints and adding them to a working set. The loss A was defined as 1 minus a weighted combination of polarity-labeled and unlabeled intersection F-measure as described in Section 5. 5 Experiments Opinion expression boundaries are hard to define rigorously (Wiebe et al., 2005), so evaluations of their quality typically use soft metrics. The MPQA annotators used the overlap metric: an expression is counted as correct if it overlaps with one in the gold standard. This has also been used to evaluate opinion extractors (Choi et al., 2006; Breck et al., 2007). However, this metric has a number of problems: 1) it is possible to ”fool” the metric by creating expressions that cover the whole sentence; 2) it does not give higher credit to output that is ”almost perfect” rather than ”almost incorrect”. Therefore, in (Johansson and Moschitti, 2010b), we measured the intersect</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6053" citStr="Wilson et al. (2005)" startWordPosition="942" endWordPosition="945"> opinion expression, similar to that described by Wilson et al. (2009). The first of the two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al. (2007) and Choi et al. (2006). Sequence labeling techniques such as HMMs and CRFs are widely used for segmentation problems such as named entity recognition and noun chunk extraction. We trained a first-order labeler with the discriminative training method by Collins (2002) and used common features: words, POS, lemmas in a sliding window. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al. (2005). For the second baseline, we added our opinion expression reranker (Johansson and Moschitti, 2010b) on top of the expression sequence labeler. Given an expression, we use a classifier to assign a polarity value: positive, neutral, or negative. We trained linear support vector machines to carry out this classification. The problem of polarity classification has been studied in detail by Wilson et al. (2009), who used a set of carefully devised linguistic features. Our classifier is simpler and is based on fairly shallow features: words, POS, subjectivity clues, and bigrams inside and around th</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="2483" citStr="Wilson et al., 2009" startWordPosition="383" endWordPosition="386"> be assigned a polarity describing whether the feeling is positive, neutral or negative. These two tasks have generally been tackled in isolation. Breck et al. (2007) introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in (Johansson and Moschitti, 2010b); later we also added holder extraction (Johansson and Moschitti, 2010a). For the task of classifiying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al., 2009). While the tasks of expression detection and polarity classification have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned polarities. This is so far the only published result on joint opinion segmentation and polarity classification. However, their experiment lacked the obvious baseline: a standard pipeline consisting of an expression identifier followed by a polarity classifier. In addition, while theirs is the first end-to-end system for expression extraction with polarities, it is still a s</context>
<context position="5503" citStr="Wilson et al. (2009)" startWordPosition="851" endWordPosition="854">ntensity, and most expressions are also associated with a holder, also called source. In this work, we only consider polarities, not intensities or holders. The polarity takes the values POSITIVE, NEUTRAL, NEGATIVE, and BOTH; for compatibility with Choi and Cardie (2010), we mapped BOTH to NEUTRAL. 3 The Baselines In order to test our hypothesis against strong baselines, we developed two pipeline systems. The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al. (2009). The first of the two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al. (2007) and Choi et al. (2006). Sequence labeling techniques such as HMMs and CRFs are widely used for segmentation problems such as named entity recognition and noun chunk extraction. We trained a first-order labeler with the discriminative training method by Collins (2002) and used common features: words, POS, lemmas in a sliding window. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al. (2005). For the second baseline, we added our opinion ex</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational Linguistics, 35(3):399–433.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>