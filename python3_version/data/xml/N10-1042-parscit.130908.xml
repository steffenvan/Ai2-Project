<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.037317">
<title confidence="0.982658">
Improving Blog Polarity Classification via Topic Analysis and Adaptive
Methods
</title>
<author confidence="0.9997">
Feifan Liu Dong Wang, Bin Li, Yang Liu
</author>
<affiliation confidence="0.9997">
University of Wisconsin, Milwaukee The University of Texas at Dallas
</affiliation>
<email confidence="0.999097">
liuf@uwm.edu dongwang,yangl@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.995851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994499">
In this paper we examine different linguistic features
for sentimental polarity classification, and perform a
comparative study on this task between blog and re-
view data. We found that results on blog are much
worse than reviews and investigated two methods
to improve the performance on blogs. First we ex-
plored information retrieval based topic analysis to
extract relevant sentences to the given topics for po-
larity classification. Second, we adopted an adaptive
method where we train classifiers from review data
and incorporate their hypothesis as features. Both
methods yielded performance gain for polarity clas-
sification on blog data.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998184375">
Sentimental analysis is a task of text categorization that
focuses on recognizing and classifying opinionated text
towards a given subject. Different levels of sentimental
analysis has been performed in prior work, from binary
classes to more fine grained categories. Pang et al. (2002)
defined this task as a binary classification task and ap-
plied it to movie reviews. More sentiment classes, such
as document objectivity and subjectivity as well as dif-
ferent rating scales on the subjectivity, have also been
taken into consideration (Pang and Lee, 2005; Boiy et
al., 2007). In terms of granularity, this task has been
investigated from building word level sentiment lexicon
(Turney, 2002; Moilanen and Pulman, 2008) to detecting
phrase-level (Wilson et al., 2005; Agarwal et al., 2009)
and sentence-level (Riloff and Wiebe, 2003; Hu and Liu,
2004) sentiment orientation. However, most previous
work has mainly focused on reviews (Pang et al., 2002;
Hu and Liu, 2004), news resources (Wilson et al., 2005),
and multi-domain adaptation (Blitzer et al., 2007; Man-
sour et al., 2008). Sentiment analysis on blogs (Chesley
et al., 2005; Kim et al., 2009) is still at its early stage.
In this paper we investigate binary polarity classifica-
tion (positive vs. negative). We evaluate the genre effect
between blogs and review data and show the difference of
feature effectiveness. We demonstrate improved polarity
classification performance in blogs using two methods:
(a) integrating topic relevance analysis to perform topic
specific polarity classification; (b) adopting an adaptive
method by incorporating multiple classifiers’ hypotheses
from different review domains as features. Our manual
analysis also points out some challenges and directions
for further study in blog domain.
</bodyText>
<sectionHeader confidence="0.997363" genericHeader="method">
2 Features for Polarity Classification
</sectionHeader>
<bodyText confidence="0.999815333333333">
For the binary polarity classification task, we use a super-
vised learning framework to determine whether a docu-
ment is positive or negative. We used a subjective lex-
icon, containing 2304 positive words and 4145 negative
words respectively, based on (Wilson et al., 2005). The
features we explored are listed below.
</bodyText>
<listItem confidence="0.959111">
(i) Lexical features (LF)
</listItem>
<bodyText confidence="0.992278363636364">
We use the bag of words for the lexical features as they
have been shown very useful in previous work.
(ii) Polarized lexical features (PL)
We tagged each sentiment word in our data set with its
polarity tag based on the sentiment lexicon (“POS” for
positive, and “NEG” for negative), along with its part-
of-speech tag. For example, in the sentence “It is good,
and I like it”, “good” is tagged as “POS/ADJ”, “like” is
tagged as “POS/VRB”. Then we encode the number of
the polarized tags in a document as features.
(iii) Polarized bigram features (PB)
Contextual information around the polarized words
can be useful for sentimental analysis. A word may
flip the polarity of its neighboring sentiment words even
though this word itself is not necessarily a negative word.
For example, in “Given her sudden celebrity with those
on the left...” (a sentence in a political blog), “sudden”
preceding “celebrity” implies the author’s negative atti-
tude towards “her”. We combine the sentiment word’s
polarized tag and its following and preceding word or
its part-of-speech to comprise different bigram features
to represent this kind of contextual information. For ex-
</bodyText>
<page confidence="0.990431">
309
</page>
<subsubsectionHeader confidence="0.579544">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 309–312,
</subsubsectionHeader>
<subsectionHeader confidence="0.27776">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999944055555556">
ample, in “I recommend this.”, “recommend” is a posi-
tive verb, denoted as “POS/VRB”, and the bigram fea-
tures including this tag and its previous word “I” are
“I POS/VRB” and “pron POS/VRB”.
(iv) Transition word features (T)
Transition words, such as “although”, “even though”,
serve as function words that may change the literal opin-
ion polarity in the current sentence. This information has
not been widely explored for sentiment analysis. In this
study, we compiled a transition word list containing 31
words. We use the co-occurring feature between a transi-
tion word and its nearby content words (noun, verb, ad-
jective and adverb) or polarized tags of sentiment words
within the same sentence, but not spanning over other
transition words. For example, in “Although it is good”,
we use features like “although is”,“although good” and
“although POS/ADJ”, where “POS/ADJ” is the PL fea-
ture for word “good”.
</bodyText>
<sectionHeader confidence="0.9951985" genericHeader="method">
3 Feature Effectiveness on Blogs and
Reviews
</sectionHeader>
<bodyText confidence="0.999980625">
The blog data we used is from the TREC Blog Track eval-
uation in 2006 and 2007. The annotation was conducted
for the 100 topics used in the evaluation (blogs are rele-
vant to a given topic and also opinionated). We use 6,896
positive and 5,300 negative blogs. For the review data,
we combined multiple review data sets from (Pang et al.,
2002; Blitzer et al., 2007) together. It contains reviews
from movies and four product domains (kitchen, elec-
tronics, books, and dvd), each of which has 1000 neg-
ative and 1000 positive samples. For the data without
sentence information (e.g., blog data, some review data),
we generated sentences using the maximum entropy sen-
tence boundary detection tool1. We used TnT tagger to
obtain the part-of-speech tags for these data sets.
For classification, we use the maximum entropy clas-
sifier2 with a Gaussian prior of 1 and 100 iterations in
model training. For all the experiments below, we use
a 10-fold cross validation setup and report the average
classification accuracy. Table 1 shows classification re-
sults using various feature sets on blogs and review data.
We keep the lexical feature (LF) as a base feature, and
investigate the effectiveness of adding more different fea-
tures. We used Wilcox signed test for statistical signifi-
cance test. Symbols “†” and “§” in the table indicate the
significant level of 0.05 and 0.1 respectively, compared to
the baseline performance using LF feature setup.
For the review domain, most of the feature sets can sig-
nificantly improve the classification performance over the
baseline of using “LF” features. “PB” features yielded
more significant improvement than “PL” or “T” feature
categories. Combining “PL” and “T” features resulted in
some slight further improvement, achieving the best ac-
</bodyText>
<footnote confidence="0.999934">
1http://stp.ling.uu.se/˜gustav/java/classes/MXTERMINATOR.html
2http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
</footnote>
<table confidence="0.999859555555556">
Feature Set Blogs Reviews
LF 72.07 81.67
LF+PL 70.93 81.93
LF+PB 72.44 83.62†
LF+T 72.17 81.76
LF+PL+PB 70.81 83.61†
LF+PL+T 72.74 82.13§
LF+PB+T 72.29 83.73†
LF+PL+PB+T 71.85 83.94†
</table>
<tableCaption confidence="0.9924855">
Table 1: Polarity classification results (accuracy in %) using
different features for blogs and reviews.
</tableCaption>
<bodyText confidence="0.999749204545454">
curacy of 83.94%. We notice that incorporating our pro-
posed transition feature (T) always achieves some gain on
different feature settings, suggesting that those transition
features are useful for sentimental analysis on reviews.
From Table 1, we can see that overall the performance
on blogs is worse than on the review data. We hypoth-
esize this may be due to the large variation in terms of
contents and styles in blogs. Regarding the feature ef-
fectiveness, we also observe some differences between
blogs and reviews. Adding the polarized bigram feature
and transition feature (PB and T) individually can yield
some improvement; however, adding both of them did
not result in any further improvement – performance de-
grades compared to LF+PB. Interestingly, although “PL”
feature alone does not seem to help, by adding “PL” and
“T” together, the performance achieved the best accuracy
of 72.74%. We also found that adding all the features
together hurts the performance, suggesting that different
features interact with each other and some do not com-
bine well (e.g., PB and T features). In addition, all the
improvements here are not statistically significant.
Note that for the blog data, we randomly split them for
the cross validation experiments regardless of the queries.
In order to better understand whether the poor results on
blog data is due to the effect of different queries, we per-
formed another experiment where for each query, we ran-
domly divided the corresponding blogs into training and
test splits. Only 66 queries were kept for this experi-
ments – we did not include those queries that have fewer
than 10 relevant blogs. The results for the query balanced
split on blogs are shown in Figure 1. We also include re-
sults for the five individual review data sets in order to see
the topic effect. We present results using four represen-
tative feature sets chosen according to Table 1. For the
review data, we notice some difference across different
data sets, suggesting their inherent difference in terms of
task difficulty. We observe slight performance increase
for some feature sets using the query balanced setup for
blog data, but overall it is still much worse than the review
data. This shows that the query unbalanced training/test
split does not explain the performance gap between blogs
and reviews. This is consistent with (Zhang et al., 2007)
that found that a query-independent classifier performs
even better than query-dependent one. We expect that the
</bodyText>
<page confidence="0.9902">
310
</page>
<bodyText confidence="0.996102">
query unbalanced setup is more realistic, therefore, in the
following experiments, we continue with this setup.
</bodyText>
<figureCaption confidence="0.9845735">
Figure 1: Polarity classification results on query balanced blog
data and five individual review data sets.
</figureCaption>
<sectionHeader confidence="0.985944" genericHeader="method">
4 Improving Blog Polarity Classification
</sectionHeader>
<bodyText confidence="0.99932525">
To improve the performance of polarity classification on
blogs, we propose two methods: (a) extract only topic-
relevant segments from blogs for sentiment analysis; (b)
apply adaptive methods to leverage review data.
</bodyText>
<subsectionHeader confidence="0.997965">
4.1 Using topic-relevant blog context
</subsectionHeader>
<bodyText confidence="0.999733185185185">
Generally a review is written towards one product or one
kind of service, but a blog may cover several topics with
possibly different opinions towards each topic. The blog
data we used is annotated based on some specific topics
in the TREC Blog Track evaluation. Take topic 870 in
the data as an example, “Find opinions on alleged use
of steroids by baseball player Barry”. There is one blog
that talks about 5 different baseball players in issues of
using steroids. Since the reference opinion tag of a blog
is determined by polarity towards the given query topic, it
might be confusing for the classifier if we use the whole
blog to derive features. Recently topic analysis has been
used for polarity classification (Zhang et al., 2007; Titov
and McDonald, 2008; Wiegand and Klakow, 2009). We
take a different approach in this study.
In order to obtain a topic-relevant context, we retrieved
the top 10 relevant sentences corresponding to the given
topic using the Lemur toolkit3. Then we used these sen-
tences and their immediate previous and following sen-
tences for feature extraction in the same way as what
we did on the whole blog. In addition to using all the
words in the relevant context, we also investigated using
only content words since those are more topic indicative
than function words. We extracted content words (nouns,
verbs, adjectives and adverbs) from each blog in their
original order and apply the same feature extraction pro-
cess as for using all the words.
</bodyText>
<footnote confidence="0.737669">
3http://www.lemurproject.org/lemur/
</footnote>
<bodyText confidence="0.999657407407407">
Table 2 shows the blog polarity classification results
using the whole blog vs. relevant context composed of
all the words or only content words. For the significance
test, the comparison was done for using relevant context
with all the words vs. using the whole blog; and us-
ing content words only vs. using all the words in rele-
vant context. Each comparison was with respect to the
same feature setup. We observe improved polarity classi-
fication performance when using sentence retrieval based
topic analysis to extract relevant context. Using all the
words in the topic relevant context, all the improvements
compared to using the original entire blog are statistically
significant at the level of 0.01. We also notice that un-
like on the entire blog document, the “PL” features con-
tribute positively when combined with “LF”. All the fea-
ture settings with “PL” perform very well. The best ac-
curacy of 75.32% is achieved using feature combination
of “LF+PL” or “LF+PL+T”. This suggests that polarized
lexical features suffered from the off-topic content when
using the entire blog and are more useful within contexts
of certain topics.
When using content words only, we observe consistent
gain across all the feature sets. Three feature settings,
“LF+PB”,“LF+T” and “LF+PL+PB+T”, achieve statisti-
cally significant further improvement (compared to using
all the words of relevant contexts). The best accuracy
(75.6%) is achieved by using the “LF+PB” features.
</bodyText>
<table confidence="0.999564727272727">
Feature Set Whole Relevant Context
Blog
All Words Content Words
LF 72.07 74.921 75.14
LF+PL 70.93 75.321 75.34
LF+PB 72.44 75.031 75.61
LF+T 72.17 75.011 75.35§
LF+PL+PB 70.81 75.271 75.35
LF+PL+T 72.74 75.321 75.41
LF+PB+T 72.29 75.171 75.42
LF+PL+PB+T 71.85 75.211 75.45§
</table>
<tableCaption confidence="0.995947333333333">
Table 2: Blog polarity classification results (accuracy in %) us-
ing topic relevant context composed of all the words or only
content words.
</tableCaption>
<subsectionHeader confidence="0.830119">
4.2 Adaptive methods using review data
</subsectionHeader>
<bodyText confidence="0.999705">
Domain adaptation has been studied in some previous
work (e.g., (Blitzer et al., 2007; Mansour et al., 2008)).
In this paper, we evaluate two adaptive approaches in or-
der to leverage review data to improve blog polarity clas-
sification. In the first approach, in each of the 10-fold
cross-validation training, we pool the blog training data
(90% of the entire blog data) together with all the review
data from 5 different domains. In the second method, we
augment features with hypotheses obtained from classi-
fiers trained using other domain data. Specifically, we
first trained 5 classifiers from 5 review domain data sets
respectively, and encoded the hypotheses from different
classifiers as features for blog training (together with the
original features of the blog data). Results of these two
approaches are shown in Table 3. We use the topic rele-
</bodyText>
<figure confidence="0.994229111111111">
Accuracy(%)
87
85
83
81
79
77
75
73
71
blog_data
blog_data_query
_balanced
review_books
review_dvd
review_kitchen
review_movie
review_elec
</figure>
<page confidence="0.997297">
311
</page>
<bodyText confidence="0.999756533333333">
vant context with content words only in this experiment,
and present results for different feature combinations (ex-
cept the baseline “LF” setting). The significance test is
conducted in comparison to the results using only blog
data for training, for the same feature setting.
We find that the first approach does not yield any gain,
even though the added data is about the same size as
the blog data. It indicates that due to the large differ-
ence between the two genres, simply combining blogs
and reviews in training is not effective. However, we
can see that using augmented features in training signifi-
cantly improved the performance across different feature
sets. The best result is achieved using “LF+T” features,
76.84% compared with the best accuracy of 75.6% when
using the blog data only (“LF+PB” features).
</bodyText>
<table confidence="0.999525375">
Feature Set Only Blog Pool Data Augment Features
LF+PL 75.34 75.05 76.121
LF+PB 75.6 74.35 76.281
LF+T 75.35 74.47 76.841
LF+PL+PB 75.35 74.94 76.71
LF+PL+T 75.41 74.85 76.321
LF+PB+T 75.42 74.46 76.31
LF+PL+PB+T 75.45 74.96 76.531
</table>
<tableCaption confidence="0.991322">
Table 3: Results (accuracy in %) of blog polarity classification
using two methods leveraging review data.
</tableCaption>
<subsectionHeader confidence="0.995227">
4.3 Error analysis
</subsectionHeader>
<bodyText confidence="0.884759666666667">
Notice that after achieving some improvements the per-
formance on blogs is still much worse than on review
data. Thus we performed a manual error analysis for a
better understanding of the difficulties of sentiment anal-
ysis on blog data, and identified the following challenges.
(a) Idiomatic expressions. Compared to reviews, blog-
gers seem to use more idioms. For example, “Of course
he has me over the barrel...” expresses negative opinion,
however, there are no superficially indicative features.
(b) Ironic writing style. Some bloggers prefer ironic
style especially when speaking against something or
somebody, whereas opinions are often expressed using
plain writing style in reviews. Simply using the surface
word level features is not able to model these properly.
(c) Background knowledge. In some political blogs,
the polarized expressions are implicit. Correctly recog-
nizing them requires background knowledge and deeper
language analysis techniques.
</bodyText>
<sectionHeader confidence="0.996671" genericHeader="evaluation">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999977125">
In this paper, we have evaluated various features and the
domain effect on sentimental polarity classification. Our
experiments on blog and review data demonstrated dif-
ferent feature effectiveness and the overall poorer perfor-
mance on blogs than reviews. We found that the polarized
features and the transition word features we introduced
are useful for polarity classification. We also show that
by extracting topic-relevant context and considering only
content words, the system can achieve significantly better
performance on blogs. Furthermore, an adaptive method
using augmented features can effectively leverage data
from other domains, and yield improvement compared
to using in-domain training or training on combined data
from different domains. For our future work, we plan
to investigate other adaption methods, and try to address
some of the problems identified in our error analysis.
</bodyText>
<sectionHeader confidence="0.998936" genericHeader="conclusions">
6 Acknowledgment
</sectionHeader>
<bodyText confidence="0.999502">
The authors thank the three anonymous reviewers for
their suggestions.
</bodyText>
<sectionHeader confidence="0.999171" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999935488888889">
Apoorv Agarwal, Fadi Biadsy, and Kathleen McKeown. 2009.
Contextual phrase-level polarity analysis using lexical affect
scoring and syntactic n-grams. In Proc. of EACL.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Bi-
ographies, bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classification. In Proc. of ACL.
Erik Boiy, Pieter Hens, Koen Deschacht, and Marie-Francine
Moens. 2007. Automatic sentiment analysis in on-line text.
In Proc. of ELPUB.
Paula Chesley, Bruce Vincent, Li Xu, and Rohini K. Srihari.
2005. Using verbs and adjectives to automatically classify
blog sentiment. In Proc. of AAAI.
Minqing Hu and Bing Liu. 2004. Mining and summarizing
customer reviews. In Proc. of ACM SIGKDD.
Jungi Kim, Jin-Ji Li, and Jong-Hyeok Lee. 2009. Discovering
the discriminative views: Measuring term weights for senti-
ment analysis. In Proc. of ACL-IJCNLP.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh.
2008. Domain adaptation with multiple sources. In Proc.
of NIPS.
Karo Moilanen and Stephen Pulman. 2008. The good, the bad,
and the unknown: Morphosyllabic sentiment tagging of un-
seen words. In Proc. of ACL.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect to rat-
ing scales. In Proc. of ACL.
Bo Pang, Lilian Lee, and Shrivakumar Vaithyanathan. 2002.
Thumbs up? sentiment classification using machine learning
techniques. In Proc. of EMNLP.
Ellen Riloff and Janyce Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. of EMNLP.
Ivan Titov and Ryan McDonald. 2008. Modeling online re-
views with multi-grain topic models. In Proc. of WWW.
Peter D. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of reviews.
In Proc. of ACL.
Michael Wiegand and Dietrich Klakow. 2009. Topic-Related
polarity classification of blog sentences. In Proc. of the 14th
Portuguese Conference on Artificial Intelligence: Progress
in Artificial Intelligence, pages 658–669.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proc. of HLT-EMNLP.
Wei Zhang, Clement Yu, and Weiyi Meng. 2007. Opinion re-
trieval from blogs. In Proc. of CIKM, pages 831–840.
</reference>
<page confidence="0.998665">
312
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.966754">
<title confidence="0.9987505">Improving Blog Polarity Classification via Topic Analysis and Adaptive Methods</title>
<author confidence="0.996446">Feifan Liu Dong Wang</author>
<author confidence="0.996446">Bin Li</author>
<author confidence="0.996446">Yang Liu</author>
<affiliation confidence="0.999938">University of Wisconsin, Milwaukee The University of Texas at Dallas</affiliation>
<email confidence="0.999689">liuf@uwm.edudongwang,yangl@hlt.utdallas.edu</email>
<abstract confidence="0.998053714285714">In this paper we examine different linguistic features for sentimental polarity classification, and perform a comparative study on this task between blog and review data. We found that results on blog are much worse than reviews and investigated two methods to improve the performance on blogs. First we explored information retrieval based topic analysis to extract relevant sentences to the given topics for polarity classification. Second, we adopted an adaptive method where we train classifiers from review data and incorporate their hypothesis as features. Both methods yielded performance gain for polarity classification on blog data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Fadi Biadsy</author>
<author>Kathleen McKeown</author>
</authors>
<title>Contextual phrase-level polarity analysis using lexical affect scoring and syntactic n-grams.</title>
<date>2009</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="1687" citStr="Agarwal et al., 2009" startWordPosition="251" endWordPosition="254">ntal analysis has been performed in prior work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre effect between blogs and review data and show the difference of feature effectiveness. We demonstrat</context>
</contexts>
<marker>Agarwal, Biadsy, McKeown, 2009</marker>
<rawString>Apoorv Agarwal, Fadi Biadsy, and Kathleen McKeown. 2009. Contextual phrase-level polarity analysis using lexical affect scoring and syntactic n-grams. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1957" citStr="Blitzer et al., 2007" startWordPosition="293" endWordPosition="296">y as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre effect between blogs and review data and show the difference of feature effectiveness. We demonstrate improved polarity classification performance in blogs using two methods: (a) integrating topic relevance analysis to perform topic specific polarity classification; (b) adopting an adaptive method by incorporating multiple classifiers’ hypotheses from different review</context>
<context position="5737" citStr="Blitzer et al., 2007" startWordPosition="899" endWordPosition="902">, but not spanning over other transition words. For example, in “Although it is good”, we use features like “although is”,“although good” and “although POS/ADJ”, where “POS/ADJ” is the PL feature for word “good”. 3 Feature Effectiveness on Blogs and Reviews The blog data we used is from the TREC Blog Track evaluation in 2006 and 2007. The annotation was conducted for the 100 topics used in the evaluation (blogs are relevant to a given topic and also opinionated). We use 6,896 positive and 5,300 negative blogs. For the review data, we combined multiple review data sets from (Pang et al., 2002; Blitzer et al., 2007) together. It contains reviews from movies and four product domains (kitchen, electronics, books, and dvd), each of which has 1000 negative and 1000 positive samples. For the data without sentence information (e.g., blog data, some review data), we generated sentences using the maximum entropy sentence boundary detection tool1. We used TnT tagger to obtain the part-of-speech tags for these data sets. For classification, we use the maximum entropy classifier2 with a Gaussian prior of 1 and 100 iterations in model training. For all the experiments below, we use a 10-fold cross validation setup a</context>
<context position="14071" citStr="Blitzer et al., 2007" startWordPosition="2220" endWordPosition="2223">t contexts). The best accuracy (75.6%) is achieved by using the “LF+PB” features. Feature Set Whole Relevant Context Blog All Words Content Words LF 72.07 74.921 75.14 LF+PL 70.93 75.321 75.34 LF+PB 72.44 75.031 75.61 LF+T 72.17 75.011 75.35§ LF+PL+PB 70.81 75.271 75.35 LF+PL+T 72.74 75.321 75.41 LF+PB+T 72.29 75.171 75.42 LF+PL+PB+T 71.85 75.211 75.45§ Table 2: Blog polarity classification results (accuracy in %) using topic relevant context composed of all the words or only content words. 4.2 Adaptive methods using review data Domain adaptation has been studied in some previous work (e.g., (Blitzer et al., 2007; Mansour et al., 2008)). In this paper, we evaluate two adaptive approaches in order to leverage review data to improve blog polarity classification. In the first approach, in each of the 10-fold cross-validation training, we pool the blog training data (90% of the entire blog data) together with all the review data from 5 different domains. In the second method, we augment features with hypotheses obtained from classifiers trained using other domain data. Specifically, we first trained 5 classifiers from 5 review domain data sets respectively, and encoded the hypotheses from different classi</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Boiy</author>
<author>Pieter Hens</author>
<author>Koen Deschacht</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Automatic sentiment analysis in on-line text.</title>
<date>2007</date>
<booktitle>In Proc. of ELPUB.</booktitle>
<contexts>
<context position="1474" citStr="Boiy et al., 2007" startWordPosition="219" endWordPosition="222">lassification on blog data. 1 Introduction Sentimental analysis is a task of text categorization that focuses on recognizing and classifying opinionated text towards a given subject. Different levels of sentimental analysis has been performed in prior work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early s</context>
</contexts>
<marker>Boiy, Hens, Deschacht, Moens, 2007</marker>
<rawString>Erik Boiy, Pieter Hens, Koen Deschacht, and Marie-Francine Moens. 2007. Automatic sentiment analysis in on-line text. In Proc. of ELPUB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paula Chesley</author>
<author>Bruce Vincent</author>
<author>Li Xu</author>
<author>Rohini K Srihari</author>
</authors>
<title>Using verbs and adjectives to automatically classify blog sentiment.</title>
<date>2005</date>
<booktitle>In Proc. of AAAI.</booktitle>
<contexts>
<context position="2031" citStr="Chesley et al., 2005" startWordPosition="306" endWordPosition="309">aken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre effect between blogs and review data and show the difference of feature effectiveness. We demonstrate improved polarity classification performance in blogs using two methods: (a) integrating topic relevance analysis to perform topic specific polarity classification; (b) adopting an adaptive method by incorporating multiple classifiers’ hypotheses from different review domains as features. Our manual analysis also points out some challenges </context>
</contexts>
<marker>Chesley, Vincent, Xu, Srihari, 2005</marker>
<rawString>Paula Chesley, Bruce Vincent, Li Xu, and Rohini K. Srihari. 2005. Using verbs and adjectives to automatically classify blog sentiment. In Proc. of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proc. of ACM SIGKDD.</booktitle>
<contexts>
<context position="1749" citStr="Hu and Liu, 2004" startWordPosition="261" endWordPosition="264">s to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre effect between blogs and review data and show the difference of feature effectiveness. We demonstrate improved polarity classification performance in blogs using </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proc. of ACM SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jungi Kim</author>
<author>Jin-Ji Li</author>
<author>Jong-Hyeok Lee</author>
</authors>
<title>Discovering the discriminative views: Measuring term weights for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<contexts>
<context position="2050" citStr="Kim et al., 2009" startWordPosition="310" endWordPosition="313">n (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre effect between blogs and review data and show the difference of feature effectiveness. We demonstrate improved polarity classification performance in blogs using two methods: (a) integrating topic relevance analysis to perform topic specific polarity classification; (b) adopting an adaptive method by incorporating multiple classifiers’ hypotheses from different review domains as features. Our manual analysis also points out some challenges and directions for </context>
</contexts>
<marker>Kim, Li, Lee, 2009</marker>
<rawString>Jungi Kim, Jin-Ji Li, and Jong-Hyeok Lee. 2009. Discovering the discriminative views: Measuring term weights for sentiment analysis. In Proc. of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yishay Mansour</author>
<author>Mehryar Mohri</author>
<author>Afshin Rostamizadeh</author>
</authors>
<title>Domain adaptation with multiple sources.</title>
<date>2008</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="1980" citStr="Mansour et al., 2008" startWordPosition="297" endWordPosition="301"> rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre effect between blogs and review data and show the difference of feature effectiveness. We demonstrate improved polarity classification performance in blogs using two methods: (a) integrating topic relevance analysis to perform topic specific polarity classification; (b) adopting an adaptive method by incorporating multiple classifiers’ hypotheses from different review domains as features. O</context>
<context position="14094" citStr="Mansour et al., 2008" startWordPosition="2224" endWordPosition="2227">accuracy (75.6%) is achieved by using the “LF+PB” features. Feature Set Whole Relevant Context Blog All Words Content Words LF 72.07 74.921 75.14 LF+PL 70.93 75.321 75.34 LF+PB 72.44 75.031 75.61 LF+T 72.17 75.011 75.35§ LF+PL+PB 70.81 75.271 75.35 LF+PL+T 72.74 75.321 75.41 LF+PB+T 72.29 75.171 75.42 LF+PL+PB+T 71.85 75.211 75.45§ Table 2: Blog polarity classification results (accuracy in %) using topic relevant context composed of all the words or only content words. 4.2 Adaptive methods using review data Domain adaptation has been studied in some previous work (e.g., (Blitzer et al., 2007; Mansour et al., 2008)). In this paper, we evaluate two adaptive approaches in order to leverage review data to improve blog polarity classification. In the first approach, in each of the 10-fold cross-validation training, we pool the blog training data (90% of the entire blog data) together with all the review data from 5 different domains. In the second method, we augment features with hypotheses obtained from classifiers trained using other domain data. Specifically, we first trained 5 classifiers from 5 review domain data sets respectively, and encoded the hypotheses from different classifiers as features for b</context>
</contexts>
<marker>Mansour, Mohri, Rostamizadeh, 2008</marker>
<rawString>Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. 2008. Domain adaptation with multiple sources. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karo Moilanen</author>
<author>Stephen Pulman</author>
</authors>
<title>The good, the bad, and the unknown: Morphosyllabic sentiment tagging of unseen words.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1617" citStr="Moilanen and Pulman, 2008" startWordPosition="240" endWordPosition="243">fying opinionated text towards a given subject. Different levels of sentimental analysis has been performed in prior work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre effect between blogs and revie</context>
</contexts>
<marker>Moilanen, Pulman, 2008</marker>
<rawString>Karo Moilanen and Stephen Pulman. 2008. The good, the bad, and the unknown: Morphosyllabic sentiment tagging of unseen words. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1454" citStr="Pang and Lee, 2005" startWordPosition="215" endWordPosition="218"> gain for polarity classification on blog data. 1 Introduction Sentimental analysis is a task of text categorization that focuses on recognizing and classifying opinionated text towards a given subject. Different levels of sentimental analysis has been performed in prior work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is </context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lilian Lee</author>
<author>Shrivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1185" citStr="Pang et al. (2002)" startWordPosition="171" endWordPosition="174">etrieval based topic analysis to extract relevant sentences to the given topics for polarity classification. Second, we adopted an adaptive method where we train classifiers from review data and incorporate their hypothesis as features. Both methods yielded performance gain for polarity classification on blog data. 1 Introduction Sentimental analysis is a task of text categorization that focuses on recognizing and classifying opinionated text towards a given subject. Different levels of sentimental analysis has been performed in prior work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, mos</context>
<context position="5714" citStr="Pang et al., 2002" startWordPosition="895" endWordPosition="898">n the same sentence, but not spanning over other transition words. For example, in “Although it is good”, we use features like “although is”,“although good” and “although POS/ADJ”, where “POS/ADJ” is the PL feature for word “good”. 3 Feature Effectiveness on Blogs and Reviews The blog data we used is from the TREC Blog Track evaluation in 2006 and 2007. The annotation was conducted for the 100 topics used in the evaluation (blogs are relevant to a given topic and also opinionated). We use 6,896 positive and 5,300 negative blogs. For the review data, we combined multiple review data sets from (Pang et al., 2002; Blitzer et al., 2007) together. It contains reviews from movies and four product domains (kitchen, electronics, books, and dvd), each of which has 1000 negative and 1000 positive samples. For the data without sentence information (e.g., blog data, some review data), we generated sentences using the maximum entropy sentence boundary detection tool1. We used TnT tagger to obtain the part-of-speech tags for these data sets. For classification, we use the maximum entropy classifier2 with a Gaussian prior of 1 and 100 iterations in model training. For all the experiments below, we use a 10-fold c</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lilian Lee, and Shrivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1730" citStr="Riloff and Wiebe, 2003" startWordPosition="257" endWordPosition="260">work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre effect between blogs and review data and show the difference of feature effectiveness. We demonstrate improved polarity classification performa</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proc. of WWW.</booktitle>
<contexts>
<context position="11319" citStr="Titov and McDonald, 2008" startWordPosition="1781" endWordPosition="1784">ons towards each topic. The blog data we used is annotated based on some specific topics in the TREC Blog Track evaluation. Take topic 870 in the data as an example, “Find opinions on alleged use of steroids by baseball player Barry”. There is one blog that talks about 5 different baseball players in issues of using steroids. Since the reference opinion tag of a blog is determined by polarity towards the given query topic, it might be confusing for the classifier if we use the whole blog to derive features. Recently topic analysis has been used for polarity classification (Zhang et al., 2007; Titov and McDonald, 2008; Wiegand and Klakow, 2009). We take a different approach in this study. In order to obtain a topic-relevant context, we retrieved the top 10 relevant sentences corresponding to the given topic using the Lemur toolkit3. Then we used these sentences and their immediate previous and following sentences for feature extraction in the same way as what we did on the whole blog. In addition to using all the words in the relevant context, we also investigated using only content words since those are more topic indicative than function words. We extracted content words (nouns, verbs, adjectives and adv</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. Modeling online reviews with multi-grain topic models. In Proc. of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1589" citStr="Turney, 2002" startWordPosition="238" endWordPosition="239">ing and classifying opinionated text towards a given subject. Different levels of sentimental analysis has been performed in prior work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre ef</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wiegand</author>
<author>Dietrich Klakow</author>
</authors>
<title>Topic-Related polarity classification of blog sentences.</title>
<date>2009</date>
<booktitle>In Proc. of the 14th Portuguese Conference on Artificial Intelligence: Progress in Artificial Intelligence,</booktitle>
<pages>658--669</pages>
<contexts>
<context position="11346" citStr="Wiegand and Klakow, 2009" startWordPosition="1785" endWordPosition="1788">e blog data we used is annotated based on some specific topics in the TREC Blog Track evaluation. Take topic 870 in the data as an example, “Find opinions on alleged use of steroids by baseball player Barry”. There is one blog that talks about 5 different baseball players in issues of using steroids. Since the reference opinion tag of a blog is determined by polarity towards the given query topic, it might be confusing for the classifier if we use the whole blog to derive features. Recently topic analysis has been used for polarity classification (Zhang et al., 2007; Titov and McDonald, 2008; Wiegand and Klakow, 2009). We take a different approach in this study. In order to obtain a topic-relevant context, we retrieved the top 10 relevant sentences corresponding to the given topic using the Lemur toolkit3. Then we used these sentences and their immediate previous and following sentences for feature extraction in the same way as what we did on the whole blog. In addition to using all the words in the relevant context, we also investigated using only content words since those are more topic indicative than function words. We extracted content words (nouns, verbs, adjectives and adverbs) from each blog in the</context>
</contexts>
<marker>Wiegand, Klakow, 2009</marker>
<rawString>Michael Wiegand and Dietrich Klakow. 2009. Topic-Related polarity classification of blog sentences. In Proc. of the 14th Portuguese Conference on Artificial Intelligence: Progress in Artificial Intelligence, pages 658–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP.</booktitle>
<contexts>
<context position="1664" citStr="Wilson et al., 2005" startWordPosition="247" endWordPosition="250">ent levels of sentimental analysis has been performed in prior work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre effect between blogs and review data and show the difference of feature effec</context>
<context position="2989" citStr="Wilson et al., 2005" startWordPosition="448" endWordPosition="451">egrating topic relevance analysis to perform topic specific polarity classification; (b) adopting an adaptive method by incorporating multiple classifiers’ hypotheses from different review domains as features. Our manual analysis also points out some challenges and directions for further study in blog domain. 2 Features for Polarity Classification For the binary polarity classification task, we use a supervised learning framework to determine whether a document is positive or negative. We used a subjective lexicon, containing 2304 positive words and 4145 negative words respectively, based on (Wilson et al., 2005). The features we explored are listed below. (i) Lexical features (LF) We use the bag of words for the lexical features as they have been shown very useful in previous work. (ii) Polarized lexical features (PL) We tagged each sentiment word in our data set with its polarity tag based on the sentiment lexicon (“POS” for positive, and “NEG” for negative), along with its partof-speech tag. For example, in the sentence “It is good, and I like it”, “good” is tagged as “POS/ADJ”, “like” is tagged as “POS/VRB”. Then we encode the number of the polarized tags in a document as features. (iii) Polarized</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proc. of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Zhang</author>
<author>Clement Yu</author>
<author>Weiyi Meng</author>
</authors>
<title>Opinion retrieval from blogs.</title>
<date>2007</date>
<booktitle>In Proc. of CIKM,</booktitle>
<pages>831--840</pages>
<contexts>
<context position="9924" citStr="Zhang et al., 2007" startWordPosition="1559" endWordPosition="1562">individual review data sets in order to see the topic effect. We present results using four representative feature sets chosen according to Table 1. For the review data, we notice some difference across different data sets, suggesting their inherent difference in terms of task difficulty. We observe slight performance increase for some feature sets using the query balanced setup for blog data, but overall it is still much worse than the review data. This shows that the query unbalanced training/test split does not explain the performance gap between blogs and reviews. This is consistent with (Zhang et al., 2007) that found that a query-independent classifier performs even better than query-dependent one. We expect that the 310 query unbalanced setup is more realistic, therefore, in the following experiments, we continue with this setup. Figure 1: Polarity classification results on query balanced blog data and five individual review data sets. 4 Improving Blog Polarity Classification To improve the performance of polarity classification on blogs, we propose two methods: (a) extract only topicrelevant segments from blogs for sentiment analysis; (b) apply adaptive methods to leverage review data. 4.1 Us</context>
<context position="11293" citStr="Zhang et al., 2007" startWordPosition="1777" endWordPosition="1780">ibly different opinions towards each topic. The blog data we used is annotated based on some specific topics in the TREC Blog Track evaluation. Take topic 870 in the data as an example, “Find opinions on alleged use of steroids by baseball player Barry”. There is one blog that talks about 5 different baseball players in issues of using steroids. Since the reference opinion tag of a blog is determined by polarity towards the given query topic, it might be confusing for the classifier if we use the whole blog to derive features. Recently topic analysis has been used for polarity classification (Zhang et al., 2007; Titov and McDonald, 2008; Wiegand and Klakow, 2009). We take a different approach in this study. In order to obtain a topic-relevant context, we retrieved the top 10 relevant sentences corresponding to the given topic using the Lemur toolkit3. Then we used these sentences and their immediate previous and following sentences for feature extraction in the same way as what we did on the whole blog. In addition to using all the words in the relevant context, we also investigated using only content words since those are more topic indicative than function words. We extracted content words (nouns,</context>
</contexts>
<marker>Zhang, Yu, Meng, 2007</marker>
<rawString>Wei Zhang, Clement Yu, and Weiyi Meng. 2007. Opinion retrieval from blogs. In Proc. of CIKM, pages 831–840.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>