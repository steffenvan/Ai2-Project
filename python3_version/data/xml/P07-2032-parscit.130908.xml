<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006196">
<title confidence="0.993599">
Automatically Assessing the Post Quality in Online Discussions on Software
</title>
<author confidence="0.942847">
Markus Weimer and Iryna Gurevych and Max M¨uhlh¨auser
</author>
<affiliation confidence="0.9563835">
Ubiquitous Knowledge Processing Group, Division of Telecooperation
Darmstadt University of Technology, Germany
</affiliation>
<email confidence="0.75183">
http://www.ukp.informatik.tu-darmstadt.de
[mweimer,gurevych,max]@tk.informatik.tu-darmstadt.de
</email>
<sectionHeader confidence="0.993502" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919">
Assessing the quality of user generated con-
tent is an important problem for many web
forums. While quality is currently assessed
manually, we propose an algorithm to as-
sess the quality of forum posts automati-
cally and test it on data provided by Nab-
ble.com. We use state-of-the-art classifi-
cation techniques and experiment with five
feature classes: Surface, Lexical, Syntactic,
Forum specific and Similarity features. We
achieve an accuracy of 89% on the task of
automatically assessing post quality in the
software domain using forum specific fea-
tures. Without forum specific features, we
achieve an accuracy of 82%.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999481785714286">
Web 2.0 leads to the proliferation of user generated
content, such as blogs, wikis and forums. Key prop-
erties of user generated content are: low publication
threshold and a lack of editorial control. Therefore,
the quality of this content may vary. The end user
has problems to navigate through large repositories
of information and find information of high qual-
ity quickly. In order to address this problem, many
forum hosting companies like Google Groups1 and
Nabble2 introduce rating mechanisms, where users
can rate the information manually on a scale from 1
(low quality) to 5 (high quality). The ratings have
been shown to be consistent with the user commu-
nity by Lampe and Resnick (2004). However, the
</bodyText>
<footnote confidence="0.9999405">
1http://groups.google.com
2http://www.nabble.com
</footnote>
<bodyText confidence="0.999434294117647">
percentage of manually rated posts is very low (0.1%
in Nabble).
Departing from this, the main idea explored in the
present paper is to investigate the feasibility of au-
tomatically assessing the perceived quality of user
generated content. We test this idea for online fo-
rum discussions in the domain of software. The per-
ceived quality is not an objective measure. Rather, it
models how the community at large perceives post
quality. We choose a machine learning approach to
automatically assess it.
Our main contributions are: (1) An algorithm for
automatic quality assessment of forum posts that
learns from human ratings. We evaluate the system
on online discussions in the software domain. (2)
An analysis of the usefulness of different classes of
features for the prediction of post quality.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9999748125">
To the best of our knowledge, this is the first work
which attempts to assess the quality of forum posts
automatically. However, on the one hand work has
been done on automatic assessment of other types of
user generated content, such as essays and product
reviews. On the other hand, student online discus-
sions have been analyzed.
Automatic text quality assessment has been stud-
ied in the area of automatic essay scoring (Valenti
et al., 2003; Chodorow and Burstein, 2004; Attali
and Burstein, 2006). While there exist guidelines
for writing and assessing essays, this is not the case
for forum posts, as different users cast their rating
with possibly different quality criteria in mind. The
same argument applies to the automatic assessment
of product review usefulness (Kim et al., 2006c):
</bodyText>
<page confidence="0.980572">
125
</page>
<note confidence="0.4948465">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 125–128,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<table confidence="0.996656166666667">
Stars Label on the website Number
? Poor Post 1251
?? Below Average Post 44
? ? ? Average Post 69
? ? ?? Above Average Post 183
? ? ? ? ? Excellent Post 421
</table>
<tableCaption confidence="0.999843">
Table 1: Categories and their usage frequency.
</tableCaption>
<bodyText confidence="0.999828653846154">
Readers of a review are asked “Was this review help-
ful to you?” with the answer choices Yes/No. This
is very well defined compared to forum posts, which
are typically rated on a five star scale that does not
advertise a specific semantics.
Forums have been in the focus of another track
of research. Kim et al. (2006b) found that the re-
lation between a student’s posting behavior and the
grade obtained by that student can be assessed au-
tomatically. The main features used are the num-
ber of posts, the average post length and the aver-
age number of replies to posts of the student. Feng
et al. (2006) and Kim et al. (2006a) describe a sys-
tem to find the most authoritative answer in a fo-
rum thread. The latter add speech act analysis as a
feature for this classification. Another feature is the
author’s trustworthiness, which could be computed
based on the automatic quality classification scheme
proposed in the present paper. Finding the most au-
thoritative post could also be defined as a special
case of the quality assessment. However, it is def-
initely different from the task studied in the present
paper. We assess the perceived quality of a given
post, based solely on its intrinsic features. Any dis-
cussion thread may contain an indefinite number of
good posts, rather than a single authoritative one.
</bodyText>
<sectionHeader confidence="0.999272" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.998582636363636">
We seek to develop a system that adapts to the qual-
ity standards existing in a certain user community
by learning the relation between a set of features
and the perceived quality of posts. We experimented
with features from five classes described in table 2:
Surface, Lexical, Syntactic, Forum specific and Sim-
ilarity features.
We use forum discussions from the Software cat-
egory of Nabble.com.5 The data consists of 1968
rated posts in 1788 threads from 497 forums. Posts
can be rated by multiple users, but that happens
</bodyText>
<footnote confidence="0.591364">
5http://www.nabble.com/Software-f94.html
</footnote>
<bodyText confidence="0.999970571428571">
rarely. 1927 posts were rated by one, 40 by two and
1 post by three users. Table 1 shows the distribu-
tion of average ratings on a five star scale. From
this statistics, it becomes evident that users at Nab-
ble prefer extreme ratings. Therefore, we decided
to treat the posts as being binary rated.: Posts with
less than three stars are rated “bad”. Posts with more
than three stars are “good”.
We removed 61 posts where all ratings are ex-
actly three stars. We removed additional 14 posts
because they had contradictory ratings on the binary
scale. Those posts were mostly spam, which was
voted high for commercial interests and voted down
for being spam. Additionally, we removed 30 posts
that did not contain any text but only attachments
like pictures. Finally, we removed 331 non English
posts using a simple heuristics: Posts that contained
a certain percentage of words above a pre-defined
threshold, which are non-English according to a dic-
tionary, were considered to be non-English.
This way, we obtained 1532 binary classified
posts: 947 good posts and 585 bad posts. For each
post, we compiled a feature vector, and feature val-
ues were normalized to the range [0.0, ... ,1.0].
We use support vector machines as a state-of-the-
art-algorithm for binary classification. For all exper-
iments, we used a C-SVM with a gaussian RBF ker-
nel as implemented by LibSVM in the YALE toolkit
(Chang and Lin, 2001; Mierswa et al., 2006). Pa-
rameters were set to C = 10 and -y = 0.1. We per-
formed stratified ten-fold cross validation6 to esti-
mate the performance of our algorithm. We repeated
several experiments according to the leave-one-out
evaluation scheme and found comparable results to
the ones reported in this paper.
</bodyText>
<sectionHeader confidence="0.999631" genericHeader="evaluation">
4 Results and Analysis
</sectionHeader>
<bodyText confidence="0.99926325">
We compared our algorithm to a majority class clas-
sifier as a baseline, which achieves an accuracy of
62%. As it is evident from table 3, most system con-
figurations outperform the baseline system. The best
performing single feature category are the Forum
specific features. As we seek to build an adaptable
system, analyzing the performance without these
features is worthwhile: Using all other features, we
</bodyText>
<footnote confidence="0.9763025">
6See (Witten and Frank, 2005), chapter 5.3 for an in-depth
description.
</footnote>
<page confidence="0.981301">
126
</page>
<table confidence="0.932579428571429">
Feature category Feature name Description
Syntactic Features The percentage of part-of-speech tags as defined in the PENN Treebank tag set
(Marcus et al., 1994). We used TreeTagger (Schmid, 1995) based on the english
parameter files supplied with it.
Forum specific IsHTML Whether or not a post contains HTML. In our data, this is encoded explicitly,
features but it can also be determined by regular expressions matching HTML tags.
Properties of a post IsMail Whether or not a post has been copied from a mailing list. This is encoded
that are only explicitly in our data.
present in forum Quote Fraction The fraction of characters that are inside quotes of other posts. These quotes are
postings marked explicitly in our data.
URL and Path Count The number of URLs and filesystem paths. Post quality in the software do-
main may be influenced by the amount of tangible information, which is partly
captured by these features.
Similarity features Forums are focussed on a topic. The relatedness of a post to the topic of the
</table>
<tableCaption confidence="0.945128333333333">
forum may influence post quality. We capture this relatedness by the cosine
between the posts unigram vector and the unigram vector of the forum.
Table 2: Features used for the automatic quality assessment of posts.
</tableCaption>
<figure confidence="0.619574384615384">
Length The number of tokens in a post.
Question Frequency The percentage of sentences ending with “?”.
Exclamation Frequency The percentage of sentences ending with “!”.
Capital Word Frequency The percentage of words in CAPITAL, which is often associated with shouting.
Spelling Error Frequency The percentage of words that are not spelled correctly.3
Swear Word Frequency The percentage of words that are on a list of swear words we compiled from
resources like WordNet and Wikipedia4, which contains more than eighty words
like “asshole”, but also common transcriptions like “f*ckin”.
Surface Features
Lexical Features
Information about
the wording of the
posts
</figure>
<figureCaption confidence="0.931379">
achieve an only slightly worse classification accu-
racy. Thus, the combination of all other features
captures the quality of a post fairly well.
</figureCaption>
<table confidence="0.9969801875">
SUF LEX SYN FOR SIM Avg. accuracy
Baseline 61.82%
√ √ √ √ √ 89.10%
√ – – – – 61.82%
– √ – – – 71.82%
– – √ – – 82.64%
– – – √ √ 85.05%
– – – – 62.01%
– √ √ √ √ 89.10%
√ – √ √ √ 89.36%
√ √ – √ √ 85.03%
√ √ √ – √ 82.90%
√ √ √ √ – 88.97%
– √ √ √ – 88.56%
√ – – √ – 85.12%
– – √ √ – 88.74%
</table>
<tableCaption confidence="0.971118333333333">
Table 3: Accuracy with different feature sets. SUF: Surface,
LEX: Lexical, SYN: Syntax, FOR: Forum specific, SIM: simi-
larity. The baseline results from a majority class classifier.
</tableCaption>
<bodyText confidence="0.999179714285714">
We performed additional experiments to identify
the most important features from the Forum specific
ones. Table 4 shows that IsMail and Quote Frac-
tion are the dominant features. This is noteworthy,
as those features are not based on the domain of dis-
cussion. Thus, we believe that these features will
perform well in future experiments on other data.
</bodyText>
<table confidence="0.999149285714286">
ISM ISH QFR URL PAC Avg. accuracy
√ √ √ √ √ 85.05%
√ – – – – 73.30%
– √ – – – 61.82%
– – √ – – 73.76%
– – – √ – 61.29%
– – – – √ 61.82%
– √ √ √ √ 74.41%
√ – √ √ √ 85.05%
√ √ – √ √ 73.30%
√ √ √ – √ 85.05%
√ √ √ √ – 85.05%
√ – √ – – 84.99%
√ √ √ – – 85.05%
</table>
<tableCaption confidence="0.977257666666667">
Table 4: Accuracy with different forum specific features.
ISM: IsMail, ISH: IsHTML, QFR: QuoteFraction, URL: URL-
Count, PAC: PathCount.
</tableCaption>
<bodyText confidence="0.999942333333333">
Error Analysis Table 5 shows the confusion ma-
trix of the system using all features. Many posts
that were misclassified as good ones show no ap-
parent reason to be classified as bad posts to us. The
understanding of their rating seems to require deep
knowledge about the specific subject of discussion.
The few remaining posts are either spam or rated
negatively to signalize dissent with the opinion ex-
pressed in the post. Posts that were misclassified as
bad ones often contain program code, digital signa-
tures or other non-textual parts in the body. We plan
to address these issues with better preprocessing in
</bodyText>
<page confidence="0.976581">
127
</page>
<table confidence="0.63210675">
true good true bad sum
pred. good 490 72 562
pred. bad 95 875 970
sum 585 947 1532
</table>
<tableCaption confidence="0.99761">
Table 5: Confusion matrix for the system using all features.
</tableCaption>
<bodyText confidence="0.9994725">
the future. However, the relatively high accuracy al-
ready achieved shows that these issues are rare.
</bodyText>
<sectionHeader confidence="0.970831" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999964785714286">
Assessing post quality is an important problem for
many forums on the web. Currently, most forums
need their users to rate the posts manually, which is
error prone, labour intensive and last but not least
may lead to the problem of premature negative con-
sent (Lampe and Resnick, 2004).
We proposed an algorithm that has shown to be
able to assess the quality of forum posts. The al-
gorithm applies state-of-the-art classification tech-
niques using features such as Surface, Lexical, Syn-
tactic, Forum specific and Similarity features to
do so. Our best performing system configuration
achieves an accuracy of 89.1%, which is signifi-
cantly higher than the baseline of 61.82%. Our ex-
periments show that forum specific features perform
best. However, slightly worse but still satisfactory
performance can be obtained even without those.
So far, we have not made use of the structural in-
formation in forum threads yet. We plan to perform
experiments investigating speech act recognition in
forums to improve the automatic quality assessment.
We also plan to apply our system to further domains
of forum discussion, such as the discussions among
active Wikipedia users.
We believe that the proposed algorithm will sup-
port important applications beyond content filtering
like automatic summarization systems and forum
specific search.
</bodyText>
<sectionHeader confidence="0.998309" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98447475">
This work was supported by the German Research Foundation
as part of the Research Training Group “Feedback-Based Qual-
ity Management in eLearning” under the grant 1223. We are
thankful to Nabble for providing their data.
</bodyText>
<sectionHeader confidence="0.98916" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998527240740741">
Yigal Attali and Jill Burstein. 2006. Automated essay scoring
with e-rater v.2. The Journal of Technology, Learning, and
Assessment, 4(3), February.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library
for support vector machines. Software available at http:
//www.csie.ntu.edu.tw/—cjlin/libsvm.
Martin Chodorow and Jill Burstein. 2004. Beyond essay
length: Evaluating e-raters performance on toefl essays.
Technical report, ETS.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy. 2006.
Learning to detect conversation focus of threaded discus-
sions. In Proceedings of the Human Language Technology
Conference of the North American Chapter of the Associa-
tion of Computational Linguistics (HLT-NNACL).
Jihie Kim, Grace Chern, Donghui Feng, Erin Shaw, and Eduard
Hovya. 2006a. Mining and assessing discussions on the web
through speech act analysis. In Proceedings of the Workshop
on Web Content Mining with Human Language Technologies
at the 5th International Semantic Web Conference.
Jihie Kim, Erin Shaw, Donghui Feng, Carole Beal, and Eduard
Hovy. 2006b. Modeling and assessing student activities in
on-line discussions. In Proceedings of the Workshop on Ed-
ucational Data Mining at the conference of the American As-
sociation of Artificial Intelligence (AAAI-06), Boston, MA.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Pen-
neacchiotti. 2006c. Automatically assessing review helpful-
ness. In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages 423 –
430, Sydney, Australia, July.
Cliff Lampe and Paul Resnick. 2004. Slash(dot) and burn:
Distributed moderation in a large online conversation space.
In Proceedings of ACM CHI 2004 Conference on Human
Factors in Computing Systems, Vienna Austria, pages 543–
550.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated Corpus
of English: The Penn Treebank. Computational Linguistics,
19(2):313–330.
Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, Martin
Scholz, and Timm Euler. 2006. YALE: Rapid prototyping
for complex data mining tasks. In KDD ’06: Proceedings of
the 12th ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 935–940, New York,
NY, USA. ACM Press.
Helmut Schmid. 1995. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference on New
Methods in Language Processing, Manchester, UK.
Salvatore Valenti, Francesca Neri, and Alessandro Cucchiarelli.
2003. An overview of current research on automated es-
say grading. Journal of Information Technology Education,
2:319–329.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kaufmann,
San Francisco, 2 edition.
</reference>
<page confidence="0.996741">
128
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.489389">
<title confidence="0.996501">Automatically Assessing the Post Quality in Online Discussions on Software</title>
<author confidence="0.985596">Weimer Gurevych M¨uhlh¨auser</author>
<affiliation confidence="0.95527">Ubiquitous Knowledge Processing Group, Division of Telecooperation Darmstadt University of Technology, Germany</affiliation>
<web confidence="0.838079">http://www.ukp.informatik.tu-darmstadt.de</web>
<email confidence="0.831828">[mweimer,gurevych,max]@tk.informatik.tu-darmstadt.de</email>
<abstract confidence="0.98471375">Assessing the quality of user generated content is an important problem for many web forums. While quality is currently assessed manually, we propose an algorithm to assess the quality of forum posts automatically and test it on data provided by Nabble.com. We use state-of-the-art classification techniques and experiment with five feature classes: Surface, Lexical, Syntactic, Forum specific and Similarity features. We an accuracy of the task of automatically assessing post quality in the software domain using forum specific features. Without forum specific features, we an accuracy of</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yigal Attali</author>
<author>Jill Burstein</author>
</authors>
<title>Automated essay scoring with e-rater v.2.</title>
<date>2006</date>
<journal>The Journal of Technology, Learning, and Assessment,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="3048" citStr="Attali and Burstein, 2006" startWordPosition="464" endWordPosition="467">domain. (2) An analysis of the usefulness of different classes of features for the prediction of post quality. 2 Related work To the best of our knowledge, this is the first work which attempts to assess the quality of forum posts automatically. However, on the one hand work has been done on automatic assessment of other types of user generated content, such as essays and product reviews. On the other hand, student online discussions have been analyzed. Automatic text quality assessment has been studied in the area of automatic essay scoring (Valenti et al., 2003; Chodorow and Burstein, 2004; Attali and Burstein, 2006). While there exist guidelines for writing and assessing essays, this is not the case for forum posts, as different users cast their rating with possibly different quality criteria in mind. The same argument applies to the automatic assessment of product review usefulness (Kim et al., 2006c): 125 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 125–128, Prague, June 2007. c�2007 Association for Computational Linguistics Stars Label on the website Number ? Poor Post 1251 ?? Below Average Post 44 ? ? ? Average Post 69 ? ? ?? Above Average Post 183 ? ? ? ? ? Excellent Post 421 Table 1:</context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Yigal Attali and Jill Burstein. 2006. Automated essay scoring with e-rater v.2. The Journal of Technology, Learning, and Assessment, 4(3), February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http: //www.csie.ntu.edu.tw/—cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="6977" citStr="Chang and Lin, 2001" startWordPosition="1134" endWordPosition="1137">glish posts using a simple heuristics: Posts that contained a certain percentage of words above a pre-defined threshold, which are non-English according to a dictionary, were considered to be non-English. This way, we obtained 1532 binary classified posts: 947 good posts and 585 bad posts. For each post, we compiled a feature vector, and feature values were normalized to the range [0.0, ... ,1.0]. We use support vector machines as a state-of-theart-algorithm for binary classification. For all experiments, we used a C-SVM with a gaussian RBF kernel as implemented by LibSVM in the YALE toolkit (Chang and Lin, 2001; Mierswa et al., 2006). Parameters were set to C = 10 and -y = 0.1. We performed stratified ten-fold cross validation6 to estimate the performance of our algorithm. We repeated several experiments according to the leave-one-out evaluation scheme and found comparable results to the ones reported in this paper. 4 Results and Analysis We compared our algorithm to a majority class classifier as a baseline, which achieves an accuracy of 62%. As it is evident from table 3, most system configurations outperform the baseline system. The best performing single feature category are the Forum specific f</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines. Software available at http: //www.csie.ntu.edu.tw/—cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Jill Burstein</author>
</authors>
<title>Beyond essay length: Evaluating e-raters performance on toefl essays.</title>
<date>2004</date>
<tech>Technical report, ETS.</tech>
<contexts>
<context position="3020" citStr="Chodorow and Burstein, 2004" startWordPosition="460" endWordPosition="463"> discussions in the software domain. (2) An analysis of the usefulness of different classes of features for the prediction of post quality. 2 Related work To the best of our knowledge, this is the first work which attempts to assess the quality of forum posts automatically. However, on the one hand work has been done on automatic assessment of other types of user generated content, such as essays and product reviews. On the other hand, student online discussions have been analyzed. Automatic text quality assessment has been studied in the area of automatic essay scoring (Valenti et al., 2003; Chodorow and Burstein, 2004; Attali and Burstein, 2006). While there exist guidelines for writing and assessing essays, this is not the case for forum posts, as different users cast their rating with possibly different quality criteria in mind. The same argument applies to the automatic assessment of product review usefulness (Kim et al., 2006c): 125 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 125–128, Prague, June 2007. c�2007 Association for Computational Linguistics Stars Label on the website Number ? Poor Post 1251 ?? Below Average Post 44 ? ? ? Average Post 69 ? ? ?? Above Average Post 183 ? ? ? ? ?</context>
</contexts>
<marker>Chodorow, Burstein, 2004</marker>
<rawString>Martin Chodorow and Jill Burstein. 2004. Beyond essay length: Evaluating e-raters performance on toefl essays. Technical report, ETS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donghui Feng</author>
<author>Erin Shaw</author>
<author>Jihie Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning to detect conversation focus of threaded discussions.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NNACL).</booktitle>
<contexts>
<context position="4286" citStr="Feng et al. (2006)" startWordPosition="683" endWordPosition="686">r usage frequency. Readers of a review are asked “Was this review helpful to you?” with the answer choices Yes/No. This is very well defined compared to forum posts, which are typically rated on a five star scale that does not advertise a specific semantics. Forums have been in the focus of another track of research. Kim et al. (2006b) found that the relation between a student’s posting behavior and the grade obtained by that student can be assessed automatically. The main features used are the number of posts, the average post length and the average number of replies to posts of the student. Feng et al. (2006) and Kim et al. (2006a) describe a system to find the most authoritative answer in a forum thread. The latter add speech act analysis as a feature for this classification. Another feature is the author’s trustworthiness, which could be computed based on the automatic quality classification scheme proposed in the present paper. Finding the most authoritative post could also be defined as a special case of the quality assessment. However, it is definitely different from the task studied in the present paper. We assess the perceived quality of a given post, based solely on its intrinsic features.</context>
</contexts>
<marker>Feng, Shaw, Kim, Hovy, 2006</marker>
<rawString>Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy. 2006. Learning to detect conversation focus of threaded discussions. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NNACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jihie Kim</author>
<author>Grace Chern</author>
<author>Donghui Feng</author>
<author>Erin Shaw</author>
<author>Eduard Hovya</author>
</authors>
<title>Mining and assessing discussions on the web through speech act analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Web Content Mining with Human Language Technologies at the 5th International Semantic Web Conference.</booktitle>
<contexts>
<context position="3338" citStr="Kim et al., 2006" startWordPosition="510" endWordPosition="513">ic assessment of other types of user generated content, such as essays and product reviews. On the other hand, student online discussions have been analyzed. Automatic text quality assessment has been studied in the area of automatic essay scoring (Valenti et al., 2003; Chodorow and Burstein, 2004; Attali and Burstein, 2006). While there exist guidelines for writing and assessing essays, this is not the case for forum posts, as different users cast their rating with possibly different quality criteria in mind. The same argument applies to the automatic assessment of product review usefulness (Kim et al., 2006c): 125 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 125–128, Prague, June 2007. c�2007 Association for Computational Linguistics Stars Label on the website Number ? Poor Post 1251 ?? Below Average Post 44 ? ? ? Average Post 69 ? ? ?? Above Average Post 183 ? ? ? ? ? Excellent Post 421 Table 1: Categories and their usage frequency. Readers of a review are asked “Was this review helpful to you?” with the answer choices Yes/No. This is very well defined compared to forum posts, which are typically rated on a five star scale that does not advertise a specific semantics. Forums have</context>
</contexts>
<marker>Kim, Chern, Feng, Shaw, Hovya, 2006</marker>
<rawString>Jihie Kim, Grace Chern, Donghui Feng, Erin Shaw, and Eduard Hovya. 2006a. Mining and assessing discussions on the web through speech act analysis. In Proceedings of the Workshop on Web Content Mining with Human Language Technologies at the 5th International Semantic Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jihie Kim</author>
<author>Erin Shaw</author>
<author>Donghui Feng</author>
<author>Carole Beal</author>
<author>Eduard Hovy</author>
</authors>
<title>Modeling and assessing student activities in on-line discussions.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Educational Data Mining at the conference of the American Association of Artificial Intelligence (AAAI-06),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="3338" citStr="Kim et al., 2006" startWordPosition="510" endWordPosition="513">ic assessment of other types of user generated content, such as essays and product reviews. On the other hand, student online discussions have been analyzed. Automatic text quality assessment has been studied in the area of automatic essay scoring (Valenti et al., 2003; Chodorow and Burstein, 2004; Attali and Burstein, 2006). While there exist guidelines for writing and assessing essays, this is not the case for forum posts, as different users cast their rating with possibly different quality criteria in mind. The same argument applies to the automatic assessment of product review usefulness (Kim et al., 2006c): 125 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 125–128, Prague, June 2007. c�2007 Association for Computational Linguistics Stars Label on the website Number ? Poor Post 1251 ?? Below Average Post 44 ? ? ? Average Post 69 ? ? ?? Above Average Post 183 ? ? ? ? ? Excellent Post 421 Table 1: Categories and their usage frequency. Readers of a review are asked “Was this review helpful to you?” with the answer choices Yes/No. This is very well defined compared to forum posts, which are typically rated on a five star scale that does not advertise a specific semantics. Forums have</context>
</contexts>
<marker>Kim, Shaw, Feng, Beal, Hovy, 2006</marker>
<rawString>Jihie Kim, Erin Shaw, Donghui Feng, Carole Beal, and Eduard Hovy. 2006b. Modeling and assessing student activities in on-line discussions. In Proceedings of the Workshop on Educational Data Mining at the conference of the American Association of Artificial Intelligence (AAAI-06), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Patrick Pantel</author>
<author>Tim Chklovski</author>
<author>Marco Penneacchiotti</author>
</authors>
<title>Automatically assessing review helpfulness.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 423 – 430,</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="3338" citStr="Kim et al., 2006" startWordPosition="510" endWordPosition="513">ic assessment of other types of user generated content, such as essays and product reviews. On the other hand, student online discussions have been analyzed. Automatic text quality assessment has been studied in the area of automatic essay scoring (Valenti et al., 2003; Chodorow and Burstein, 2004; Attali and Burstein, 2006). While there exist guidelines for writing and assessing essays, this is not the case for forum posts, as different users cast their rating with possibly different quality criteria in mind. The same argument applies to the automatic assessment of product review usefulness (Kim et al., 2006c): 125 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 125–128, Prague, June 2007. c�2007 Association for Computational Linguistics Stars Label on the website Number ? Poor Post 1251 ?? Below Average Post 44 ? ? ? Average Post 69 ? ? ?? Above Average Post 183 ? ? ? ? ? Excellent Post 421 Table 1: Categories and their usage frequency. Readers of a review are asked “Was this review helpful to you?” with the answer choices Yes/No. This is very well defined compared to forum posts, which are typically rated on a five star scale that does not advertise a specific semantics. Forums have</context>
</contexts>
<marker>Kim, Pantel, Chklovski, Penneacchiotti, 2006</marker>
<rawString>Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Penneacchiotti. 2006c. Automatically assessing review helpfulness. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 423 – 430, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cliff Lampe</author>
<author>Paul Resnick</author>
</authors>
<title>Slash(dot) and burn: Distributed moderation in a large online conversation space.</title>
<date>2004</date>
<booktitle>In Proceedings of ACM CHI 2004 Conference on Human Factors in Computing Systems,</booktitle>
<pages>543--550</pages>
<location>Vienna</location>
<contexts>
<context position="1672" citStr="Lampe and Resnick (2004)" startWordPosition="244" endWordPosition="247">blogs, wikis and forums. Key properties of user generated content are: low publication threshold and a lack of editorial control. Therefore, the quality of this content may vary. The end user has problems to navigate through large repositories of information and find information of high quality quickly. In order to address this problem, many forum hosting companies like Google Groups1 and Nabble2 introduce rating mechanisms, where users can rate the information manually on a scale from 1 (low quality) to 5 (high quality). The ratings have been shown to be consistent with the user community by Lampe and Resnick (2004). However, the 1http://groups.google.com 2http://www.nabble.com percentage of manually rated posts is very low (0.1% in Nabble). Departing from this, the main idea explored in the present paper is to investigate the feasibility of automatically assessing the perceived quality of user generated content. We test this idea for online forum discussions in the domain of software. The perceived quality is not an objective measure. Rather, it models how the community at large perceives post quality. We choose a machine learning approach to automatically assess it. Our main contributions are: (1) An a</context>
<context position="12226" citStr="Lampe and Resnick, 2004" startWordPosition="2084" endWordPosition="2087"> We plan to address these issues with better preprocessing in 127 true good true bad sum pred. good 490 72 562 pred. bad 95 875 970 sum 585 947 1532 Table 5: Confusion matrix for the system using all features. the future. However, the relatively high accuracy already achieved shows that these issues are rare. 5 Conclusion and Future Work Assessing post quality is an important problem for many forums on the web. Currently, most forums need their users to rate the posts manually, which is error prone, labour intensive and last but not least may lead to the problem of premature negative consent (Lampe and Resnick, 2004). We proposed an algorithm that has shown to be able to assess the quality of forum posts. The algorithm applies state-of-the-art classification techniques using features such as Surface, Lexical, Syntactic, Forum specific and Similarity features to do so. Our best performing system configuration achieves an accuracy of 89.1%, which is significantly higher than the baseline of 61.82%. Our experiments show that forum specific features perform best. However, slightly worse but still satisfactory performance can be obtained even without those. So far, we have not made use of the structural inform</context>
</contexts>
<marker>Lampe, Resnick, 2004</marker>
<rawString>Cliff Lampe and Paul Resnick. 2004. Slash(dot) and burn: Distributed moderation in a large online conversation space. In Proceedings of ACM CHI 2004 Conference on Human Factors in Computing Systems, Vienna Austria, pages 543– 550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="7956" citStr="Marcus et al., 1994" startWordPosition="1292" endWordPosition="1295">m to a majority class classifier as a baseline, which achieves an accuracy of 62%. As it is evident from table 3, most system configurations outperform the baseline system. The best performing single feature category are the Forum specific features. As we seek to build an adaptable system, analyzing the performance without these features is worthwhile: Using all other features, we 6See (Witten and Frank, 2005), chapter 5.3 for an in-depth description. 126 Feature category Feature name Description Syntactic Features The percentage of part-of-speech tags as defined in the PENN Treebank tag set (Marcus et al., 1994). We used TreeTagger (Schmid, 1995) based on the english parameter files supplied with it. Forum specific IsHTML Whether or not a post contains HTML. In our data, this is encoded explicitly, features but it can also be determined by regular expressions matching HTML tags. Properties of a post IsMail Whether or not a post has been copied from a mailing list. This is encoded that are only explicitly in our data. present in forum Quote Fraction The fraction of characters that are inside quotes of other posts. These quotes are postings marked explicitly in our data. URL and Path Count The number o</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingo Mierswa</author>
<author>Michael Wurst</author>
<author>Ralf Klinkenberg</author>
<author>Martin Scholz</author>
<author>Timm Euler</author>
</authors>
<title>YALE: Rapid prototyping for complex data mining tasks.</title>
<date>2006</date>
<booktitle>In KDD ’06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>935--940</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7000" citStr="Mierswa et al., 2006" startWordPosition="1138" endWordPosition="1141">imple heuristics: Posts that contained a certain percentage of words above a pre-defined threshold, which are non-English according to a dictionary, were considered to be non-English. This way, we obtained 1532 binary classified posts: 947 good posts and 585 bad posts. For each post, we compiled a feature vector, and feature values were normalized to the range [0.0, ... ,1.0]. We use support vector machines as a state-of-theart-algorithm for binary classification. For all experiments, we used a C-SVM with a gaussian RBF kernel as implemented by LibSVM in the YALE toolkit (Chang and Lin, 2001; Mierswa et al., 2006). Parameters were set to C = 10 and -y = 0.1. We performed stratified ten-fold cross validation6 to estimate the performance of our algorithm. We repeated several experiments according to the leave-one-out evaluation scheme and found comparable results to the ones reported in this paper. 4 Results and Analysis We compared our algorithm to a majority class classifier as a baseline, which achieves an accuracy of 62%. As it is evident from table 3, most system configurations outperform the baseline system. The best performing single feature category are the Forum specific features. As we seek to </context>
</contexts>
<marker>Mierswa, Wurst, Klinkenberg, Scholz, Euler, 2006</marker>
<rawString>Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, Martin Scholz, and Timm Euler. 2006. YALE: Rapid prototyping for complex data mining tasks. In KDD ’06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 935–940, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1995</date>
<booktitle>In International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="7991" citStr="Schmid, 1995" startWordPosition="1299" endWordPosition="1300">line, which achieves an accuracy of 62%. As it is evident from table 3, most system configurations outperform the baseline system. The best performing single feature category are the Forum specific features. As we seek to build an adaptable system, analyzing the performance without these features is worthwhile: Using all other features, we 6See (Witten and Frank, 2005), chapter 5.3 for an in-depth description. 126 Feature category Feature name Description Syntactic Features The percentage of part-of-speech tags as defined in the PENN Treebank tag set (Marcus et al., 1994). We used TreeTagger (Schmid, 1995) based on the english parameter files supplied with it. Forum specific IsHTML Whether or not a post contains HTML. In our data, this is encoded explicitly, features but it can also be determined by regular expressions matching HTML tags. Properties of a post IsMail Whether or not a post has been copied from a mailing list. This is encoded that are only explicitly in our data. present in forum Quote Fraction The fraction of characters that are inside quotes of other posts. These quotes are postings marked explicitly in our data. URL and Path Count The number of URLs and filesystem paths. Post q</context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>Helmut Schmid. 1995. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference on New Methods in Language Processing, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Salvatore Valenti</author>
<author>Francesca Neri</author>
<author>Alessandro Cucchiarelli</author>
</authors>
<title>An overview of current research on automated essay grading.</title>
<date>2003</date>
<journal>Journal of Information Technology Education,</journal>
<pages>2--319</pages>
<contexts>
<context position="2991" citStr="Valenti et al., 2003" startWordPosition="456" endWordPosition="459">e the system on online discussions in the software domain. (2) An analysis of the usefulness of different classes of features for the prediction of post quality. 2 Related work To the best of our knowledge, this is the first work which attempts to assess the quality of forum posts automatically. However, on the one hand work has been done on automatic assessment of other types of user generated content, such as essays and product reviews. On the other hand, student online discussions have been analyzed. Automatic text quality assessment has been studied in the area of automatic essay scoring (Valenti et al., 2003; Chodorow and Burstein, 2004; Attali and Burstein, 2006). While there exist guidelines for writing and assessing essays, this is not the case for forum posts, as different users cast their rating with possibly different quality criteria in mind. The same argument applies to the automatic assessment of product review usefulness (Kim et al., 2006c): 125 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 125–128, Prague, June 2007. c�2007 Association for Computational Linguistics Stars Label on the website Number ? Poor Post 1251 ?? Below Average Post 44 ? ? ? Average Post 69 ? ? ?? Abo</context>
</contexts>
<marker>Valenti, Neri, Cucchiarelli, 2003</marker>
<rawString>Salvatore Valenti, Francesca Neri, and Alessandro Cucchiarelli. 2003. An overview of current research on automated essay grading. Journal of Information Technology Education, 2:319–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques.</title>
<date>2005</date>
<volume>2</volume>
<pages>edition.</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco,</location>
<contexts>
<context position="7749" citStr="Witten and Frank, 2005" startWordPosition="1261" endWordPosition="1264">f our algorithm. We repeated several experiments according to the leave-one-out evaluation scheme and found comparable results to the ones reported in this paper. 4 Results and Analysis We compared our algorithm to a majority class classifier as a baseline, which achieves an accuracy of 62%. As it is evident from table 3, most system configurations outperform the baseline system. The best performing single feature category are the Forum specific features. As we seek to build an adaptable system, analyzing the performance without these features is worthwhile: Using all other features, we 6See (Witten and Frank, 2005), chapter 5.3 for an in-depth description. 126 Feature category Feature name Description Syntactic Features The percentage of part-of-speech tags as defined in the PENN Treebank tag set (Marcus et al., 1994). We used TreeTagger (Schmid, 1995) based on the english parameter files supplied with it. Forum specific IsHTML Whether or not a post contains HTML. In our data, this is encoded explicitly, features but it can also be determined by regular expressions matching HTML tags. Properties of a post IsMail Whether or not a post has been copied from a mailing list. This is encoded that are only exp</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, San Francisco, 2 edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>