<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017968">
<title confidence="0.987064">
Taxonomy Learning Using Word Sense Induction
</title>
<author confidence="0.967789">
Ioannis P. Klapaftis
</author>
<affiliation confidence="0.99831">
Department of Computer Science
The University of York
</affiliation>
<address confidence="0.944496">
York, UK, YO10 5DD
</address>
<email confidence="0.998508">
giannis@cs.york.ac.uk
</email>
<author confidence="0.990136">
Suresh Manandhar
</author>
<affiliation confidence="0.998655">
Department of Computer Science
The University of York
</affiliation>
<address confidence="0.944811">
York, UK, YO10 5DD
</address>
<email confidence="0.998969">
suresh@cs.york.ac.uk
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997865">
Taxonomies are an important resource for a
variety of Natural Language Processing (NLP)
applications. Despite this, the current state-
of-the-art methods in taxonomy learning have
disregarded word polysemy, in effect, devel-
oping taxonomies that conflate word senses.
In this paper, we present an unsupervised
method that builds a taxonomy of senses
learned automatically from an unlabelled cor-
pus. Our evaluation on two WordNet-derived
taxonomies shows that the learned taxonomies
capture a higher number of correct taxonomic
relations compared to those produced by tradi-
tional distributional similarity approaches that
merge senses by grouping the features of each
word into a single vector.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998833125">
A concept or a sense, s, can be defined as the mean-
ing of a word or a multiword expression. A con-
cept s can be linguistically realised by more than one
word while at the same time a word w can be the lin-
guistic realisation of more than one concept. Given
a set of concepts 5, taxonomy learning is the task of
hierarchically classifying the elements in 5 in an au-
tomatic manner. For example, consider a set of con-
cepts linguistically realised by the words/multiword
expressions LAN, computer network, internet, mesh-
work, gauze, snood. Taxonomy learning methods
produce taxonomies, such as the ones shown in Fig-
ures 1 (a) and 1 (b).
By observing Figure 1 (a), we can express IS-
A statements, such as Internet IS-A Computer Net-
work etc. However, the same does not apply to the
</bodyText>
<page confidence="0.994675">
82
</page>
<figureCaption confidence="0.999908">
Figure 1: A labelled and an unlabelled concept taxonomy
</figureCaption>
<bodyText confidence="0.99993685">
taxonomy in Figure 1 (b), since this taxonomy is not
fully labelled. Despite this, its hierarchical organ-
isation clearly shows that the concepts are divided
into groups, which are further subdivided into sub-
groups and so forth, until we reach a level where
each concept belongs to its own group. Unlabelled
taxonomies are typically produced by agglomera-
tive hierarchical clustering algorithms (King, 1967;
Sneath and Sokal, 1973).
The knowledge encoded in taxonomies can be
utilised in a range of NLP applications. For in-
stance, taxonomies can be used in information re-
trieval to expand a user query with semantically re-
lated words or to enhance document representation
by abstracting from plain words and adding concep-
tual information (Cimiano, 2006). WordNet’s (Fell-
baum, 1998) taxonomic relations have also been
used in Word Sense Disambiguation (WSD) (Nav-
igli and Velardi, 2004b). In named entity recog-
nition, methods relying on gazetteers could make
</bodyText>
<note confidence="0.637203">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 82–90,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999524285714286">
use of automatically acquired taxonomies (Cimiano,
2006), while question answering systems have also
benefited (Moldovan and Novischi, 2002).
Despite the wide uses of taxonomies, the majority
of methods disregard or do not deal effectively with
word polysemy, in effect, developing taxonomies
that conflate the senses of words (see Section 2).
In this work, we show that Word Sense Induction
(WSI) can be effectively employed to address this
limitation of existing methods.
We present a novel method that employs WSI to
generate the different senses of a set of target words
from an unlabelled corpus and then produces a tax-
onomy of senses using Hierarchical Agglomerative
Clustering (HAC) (King, 1967; Sneath and Sokal,
1973). We evaluate our method on two WordNet-
derived sub-taxonomies and show that our method
leads to the development of concept hierarchies that
capture a higher number of correct taxonomic rela-
tions in comparison to those generated by current
distributional similarity approaches.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999946900000001">
Initial research on taxonomy learning focused on
identifying in a given text lexico-syntactic patterns
that suggest hyponymy relations (Hearst, 1992). For
instance, the pattern NP0 such as NP1,... ,NP,,,
suggests that NP0 is a hypernym of NPi. For ex-
ample, given the phrase Fruits, such as oranges, ap-
ples,..., the above pattern would suggest that fruit
is a hypernym of orange and apple. These pattern-
based approaches operate at the word level by learn-
ing lexical relations between words rather than be-
tween senses of words.
In the same spirit, other work attempted to exploit
the regularities of dictionary entries to identify hy-
ponymy relations (Amsler, 1981). For example in
WordNet, WAN is defined as a computer network
that spans .... Hence, one can easily induce that
WAN is a hyponym of computer network by assum-
ing that the first noun phrase in the definition is a hy-
pernym of the target word. These approaches learn
lexical relations at the sense level since dictionaries
separate the senses of a word. However this would
be true if and only if the glosses of the dictionaries
were sense-annotated, which is not the case for the
majority of electronic dictionaries (Cimiano, 2006).
Another limitation is that taxonomies are built ac-
cording to the sense distinctions present in dictio-
naries and not according to the actual use of words
in the corpus.
The majority of taxonomy learning approaches
are based on the distributional hypothesis (Harris,
1968). Typically, distributional similarity methods
(Cimiano et al., 2004; Cimiano et al., 2005; Faure
and N´edellec, 1998; Reinberger and Spyns, 2004;
Caraballo, 1999) utilise syntactic dependencies such
as subject/verb, object/verb relations, conjunctive
and appositive constructions and others. These de-
pendencies are used to extract the features that serve
as the dimensions of the vector space. Each target
noun is then represented as a vector of extracted fea-
tures where the frequency of co-occurrence of the
target noun with each feature is used to calculate the
weight of that feature. The constructed vectors are
the input to hierarchical clustering or formal concept
analysis (Ganter and Wille, 1999) to produce a tax-
onomy. These approaches assume that a target noun
is monosemous creating one vector of features for
each target noun. This limitation can lead to a num-
ber of problems.
Firstly, the constructed taxonomies might be bi-
ased towards the inclusion of taxonomic relation-
ships between the most frequent senses of tar-
get nouns, ignoring interesting taxonomic relations
where less frequent senses are present. For exam-
ple, consider the word house. Current distributional
similarity methods would possibly capture the hy-
ponyms of its Most Frequent Sense (MFS1), how-
ever ignoring the hyponyms of less frequent senses
of house, e.g. casino, theater, etc. Given that word
senses typically follow a Zipf distribution, these
methods construct vectors dominated by the MFS of
words. This bias significantly degrades the useful-
ness of learned taxonomies.
Secondly, given that distributional similarity ap-
proaches rely on the computation of pairwise simi-
larities between target words, merging their senses
to a single vector might lead to unreliable similarity
estimates. For example, merging the features of the
different senses of house could provide a lower sim-
ilarity with its monosemous hyponym beach house,
since only the first sense of house is related to beach
</bodyText>
<footnote confidence="0.638133">
1WordNet: A dwelling that serves as living quarters ...
</footnote>
<page confidence="0.99874">
83
</page>
<bodyText confidence="0.999980048780488">
house. This problem might lead both to inclusion
of incorrect or loss of correct taxonomic relations.
In our work, we aim to overcome these drawbacks
by identifying the different senses with which target
words appear in text and then building a hierarchy
of the identified senses.
Soft clustering approaches (Reinberger and
Spyns, 2004; Reinberger et al., 2003) have also been
applied to taxonomy learning to deal with polysemy.
These methods associate each verb with a vector of
features, where each feature is a noun appearing as
a subject or object of that verb. That way a noun can
appear in different vectors, hence in different clus-
ters during hierarchical clustering as a result of its
polysemy. However, the underlying assumption is
that a verb is monosemous with respect to its associ-
ated vector of nouns. This assumption is not always
valid and can cause the problems mentioned above.
Other work in taxonomy learning exploits the
head/modifier relationships to create taxonomic re-
lations (Buitelaar et al., 2004; Hwang, 1999;
S´anchez and Moreno, 2005). These relations are
used to create: (1) a class (concept) for each head,
and (2) subclasses by adding nominal or adjectival
modifiers. For example, credit card IS-A card. The
corresponding hyponymy relations are learned at the
lexical level disregarding word polysemy. Some of
these approaches identified the problem of polysemy
and applied sense disambiguation with respect to
WordNet in order to capture the different senses of a
target term (Navigli and Velardi, 2004b; Navigli and
Velardi, 2004a). Specifically, the taxonomy built by
exploiting head/modifiers relations was modified ac-
cording to WordNet’s hyponymy relations between
senses of disambiguated terms. One important de-
ficiency of using sense disambiguation is that dic-
tionaries miss many domain-specific senses. Addi-
tionally, the fixed-list of senses paradigm prohibits
learning word senses according to their use in con-
text. The use of sense induction we propose in this
paper aims to overcome these limitations.
</bodyText>
<sectionHeader confidence="0.986434" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.99964575">
Given a set of words W, a WSI method is applied
to each wz E W (Section 3.1). The outcome of the
first stage is a set of senses, 5, where each sw E 5
denotes the i-th sense of word w E W. This set
</bodyText>
<figureCaption confidence="0.991505">
Figure 2: WSI for network &amp; LAN
</figureCaption>
<bodyText confidence="0.977902">
of senses is the input to hierarchical clustering that
produces a hierarchy of senses (Section 3.2).
</bodyText>
<subsectionHeader confidence="0.999371">
3.1 Word sense induction
</subsectionHeader>
<bodyText confidence="0.99988775">
WSI is the task of identifying the senses of a tar-
get word in a given text. Recent WSI methods
were evaluated under the framework of SemEval-
2007 WSI task (SWSI) (Agirre and Soroa, 2007).
The evaluation framework defines two types of as-
sessment, i.e. evaluation in: (1) a clustering and
(2) a WSD setting. Based on this evaluation, we se-
lected the method of Klapaftis &amp; Manandhar (2008)
(henceforth referred to as KM) that achieves high F-
score in both evaluation schemes as compared to the
systems participating in SWSI. We briefly describe
KM mentioning its parameters used in our evalua-
tion (Section 4). Figures 2 (a) and 2 (b) describe the
different steps for inducing the senses of the target
words network and LAN.
Corpus preprocessing: The input to KM is a
base corpus bc, in which the target word w appears
in each paragraph. In Figure 2 (a), the base cor-
pus consists of the paragraphs A, B, C and D. The
aim of this stage is to capture nouns contextually
</bodyText>
<page confidence="0.989384">
84
</page>
<bodyText confidence="0.9770806">
related to w. Initially, the target word is removed
from bc, part-of-speech tagging is applied to each
paragraph, only nouns are kept and lemmatised. In
the next step, the distribution of each noun is com-
pared to the distribution of the same noun in a ref-
erence corpus2 using the log-likelihood ratio (G2)
(Dunning, 1993). Nouns with a G2 below a pre-
specified threshold (parameter p1) are removed from
each paragraph. Figure 2 (a) shows the remaining
nouns for each paragraph of bc.
Graph creation &amp; clustering: In the setting of
KM, a collocation is a juxtaposition of two nouns
within the same paragraph. Thus, each noun is com-
bined with any other noun yielding a total of (N )
2
collocations for a paragraph with N nouns. Each
collocation, cij, is assigned a weight that measures
the relative frequency of two nouns co-occurring.
This weight is the average of the conditional prob-
abilities p(ni|nj) and p(nj|ni), where p(ni|nj) =
</bodyText>
<equation confidence="0.642353">
f(cij)
f(nj) , f(cij) is the number of paragraphs nouns ni,
</equation>
<bodyText confidence="0.9999691">
nj co-occur and f(nj) is the number of paragraphs
in which nj appears. Collocations are filtered with
respect to their frequency (parameter p2) and weight
(parameter p3). Each retained collocation is rep-
resented as a vertex. Edges between vertices are
present, if two collocations co-occur in one or more
paragraphs. Figure 2 (a) shows that this process has
generated 24 collocations for the target word net-
work. On the top right of the figure we also observe
the collocations associated with each paragraph.
In the next step, a smoothing technique is applied
to discover new edges between vertices. The weight
applied to each edge connecting vertices vi and vj
(collocations cab, cde) is the maximum of their con-
ditional probabilities (max(p(cab|cde),p(cde|cab))).
Finally, the graph is clustered using Chinese whis-
pers (Biemann, 2006). The final output is a set of
senses, each one represented by a set of contextually
related collocations. In Figure 2, we generated two
senses for network and one sense for LAN.
</bodyText>
<subsectionHeader confidence="0.999717">
3.2 Hierarchical clustering of senses
</subsectionHeader>
<bodyText confidence="0.999963666666667">
Given the set of senses 5, our task at this point is to
hierarchically classify the senses using HAC. Con-
sider for example the words network and LAN, and
</bodyText>
<footnote confidence="0.655953">
2The British National Corpus, 2001, Distributed by Oxford
University Computing Services.
</footnote>
<table confidence="0.9704424">
Senses computer meshwork LAN
network
computer network 1 0.0 0.66
meshwork 0.0 1 0.14
LAN 0.66 0.14 1
</table>
<tableCaption confidence="0.999316">
Table 1: Similarity matrix for HAC.
</tableCaption>
<figureCaption confidence="0.953997">
Figure 3: WSI &amp; HAC example
</figureCaption>
<bodyText confidence="0.991168323529412">
let us assume that the WSI process has generated
the senses in Figures 2 (a) and 2 (b). HAC oper-
ates by treating each sense as a singleton cluster and
then successively merging the most similar clusters
according to a pre-defined similarity function. This
process iterates until all clusters have been merged
into a single cluster taken to be the root.
To calculate the pairwise similarities between
senses we exploit the attributes that represent each
sense, i.e. their collocations. Let BC be the cor-
pus resulting from the union of the base corpora of
all words in W. In our example, BC would consist
of the paragraphs, in which the words network and
LAN appear, i.e. A, B, ..., G. An induced sense tags
a paragraph, if one or more of its collocations ap-
pear in that paragraph. Thus, each induced sense is
associated with a set of paragraph labels that denote
the paragraphs tagged by that sense. Figure 3 shows
the paragraph labels tagged by each sense of our ex-
ample. Finally, given two senses sai , sbi and their
corresponding sets of tagged paragraphs fa i and fbi ,
we use the Jaccard coefficient to calculate their sim-
ilarity, i.e. JC(sai , sb) = |fain|, where s� denotes
|fai ufbi
the j-th sense of word k. The resulting similarity
matrix of our example is shown in Table 1. Given
that matrix, HAC would first group computer net-
work and LAN as they have the highest similarity
(Figure 3). In the final iteration, the remaining two
clusters (Cluster 1 &amp; meshwork) would be grouped
to the root.
An important parameter of HAC is the choice
of the technique for calculating cluster similarities.
Note that as we move towards the higher levels of
</bodyText>
<page confidence="0.998836">
85
</page>
<bodyText confidence="0.9999709">
the taxonomy clusters contain more than one sets of
tagged paragraphs (Figure 3 - Cluster 1), hence the
choice of the similarity function is crucial. We ex-
periment with three techniques, i.e. single-linkage,
complete-linkage and average-linkage. The first one
defines the similarity between two clusters as the
maximum similarity among all the pairs of their cor-
responding feature sets. The second considers the
minimum similarity among all the pairs, while the
third calculates the average similarity of all the pairs.
</bodyText>
<sectionHeader confidence="0.999099" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.99998575">
We evaluate our method with respect to two
WordNet-derived sub-taxonomies (Section 4.3). For
that reason, it is necessary to map the induced senses
to WordNet before applying HAC. Note that the
mapping process might map more than one induced
senses to the same WordNet sense. In that case,
these induced senses are merged to a single one
along with their corresponding collocations.
</bodyText>
<subsectionHeader confidence="0.999027">
4.1 Mapping WSI clusters to WordNet senses
</subsectionHeader>
<bodyText confidence="0.9998602">
The process of mapping the induced senses to Word-
Net is straightforward. Let w E W be a word with
n senses in WordNet. A WordNet sense i of w is de-
noted by wswi , i = [1, n]. Let us also assume that the
WSI method has produced m senses for w, where
each sense j is denoted as swj , j = [1, m]. Each in-
duced sense swj is associated with a set of features
fwj as in the previous section. These features are the
paragraphs (paragraph labels) of BC tagged by swj .
In the next step, each WordNet sense wswi is associ-
ated with its WordNet signature giw that contains the
following semantic features: hypernyms/hyponyms,
meronyms/holonyms and synonyms of wswi . For
example, the signature of the fifth WordNet sense
of network would contain internet, cyberspace and
other semantically related words. Table 2 shows par-
tial signatures for each sense of network.
The signature giw is used to formalise the Word-
Net sense wswi as a set of features qiw. These fea-
tures are the paragraphs (paragraph labels) of BC
that contain one or more of the aforementioned se-
mantically related to wswi words that exist in gwi .
Given an induced sense swj , a similarity score is cal-
culated between sw j and each WordNet sense of w.
The maximum score determines the WordNet sense
</bodyText>
<table confidence="0.999917666666667">
WordNet sense Semantically related words/phrases
1 reticulum, RF, RAS
2 communication system/equipment
3 gauze, snood, tulle
4 reseau, reticle, reticulation
5 net, internet, cyberspace
</table>
<tableCaption confidence="0.999031">
Table 2: Semantically related words/phrases to network
</tableCaption>
<bodyText confidence="0.999961857142857">
label that will be assigned to swj , i.e. label(swj ) =
argmaxi JC(fwj , qwi ), where JC is the Jaccard sim-
ilarity coefficient. In the example of Figure 2 (a),
the computer network sense would be mapped to the
fifth WordNet sense of network, since there is a sig-
nificant overlap between the paragraphs tagged by
the induced and that WordNet sense.
</bodyText>
<subsectionHeader confidence="0.954832">
4.2 Evaluation measures
</subsectionHeader>
<bodyText confidence="0.9999455">
For the purposes of this section we present one gold
standard taxonomy (Figure 1 (a)) and a second de-
rived from our method (Figure 1 (b)). The compari-
son of these taxonomies is based on the semantic co-
topy of a node, which has also been used in (Maed-
che and Staab, 2002; Cimiano et al., 2005). In par-
ticular, the semantic cotopy of a node is defined as
the set of all its super- and subnodes excluding the
root and including that node. For example, the se-
mantic cotopy of computer network in Figure 1 (a)
is {computer network, internet, LAN}. There are
two issues, which make the evaluation difficult.
The first one is that HAC produces a taxonomy in
which all internal nodes are unlabelled, as opposed
to the gold standard taxonomy. In Figure 1 (b), we
have manually labelled internal nodes with their IDs
for clarity. For example, the semantic cotopy of the
node New Cluster 1 in Figure 1 (b) is {computer net-
work, internet, LAN, New Cluster 1, New Cluster
0}. By comparing the cotopies of nodes computer
network in Figure 1 (a) and New Cluster 1 in Fig-
ure 1 (b), we observe that the automatic method has
successfully grouped all of the hypernyms and hy-
ponyms of computer network under New Cluster 1.
However, the corresponding cotopies are not iden-
tical, because the cotopy of New Cluster 1 also in-
cludes the labels produced by HAC.
To deal with this problem, we use a version of se-
mantic cotopy for nodes in the automatically learned
taxonomy which excludes nodes that do not exist in
WordNet. That way the semantic cotopies of New
Cluster 1 in Figure 1 (b) and computer network in
</bodyText>
<page confidence="0.996746">
86
</page>
<figureCaption confidence="0.918173">
Figure 1 (a) will yield maximum similarity.
</figureCaption>
<bodyText confidence="0.999736944444445">
The second issue is that the nodes that exist in the
gold standard taxonomy are leaf nodes in the auto-
matically learned taxonomy. As a result, the seman-
tic cotopy of LAN in Figure 1 (b) is {LAN} since
all of its supernodes do not exist in WordNet. In
contrast, the semantic cotopy of LAN in Figure 1
(a) is {LAN, computer network}. We observe that
there is an overlap between the two cotopies derived
by the existence of the same concept in both tax-
onomies, i.e. LAN. In fact, all of the leaf nodes of
a learned taxonomy will have a small overlap with
the corresponding concept in the gold standard. For
this problem, we observe that in our automatically
learned taxonomies it does not make sense to cal-
culate the semantic cotopy of leaf nodes. On the
contrary, we need to evaluate the internal nodes that
group the leaf nodes. Let us assume the following
notation:
</bodyText>
<equation confidence="0.9993415">
TA = automatically learned taxonomy
ηi = node in a taxonomy
C(TA) = internal nodes + leaf nodes of TA
I(TA) = internal nodes of TA
TG = gold standard taxonomy
C(TG) = internal nodes + leaf nodes of TG
I(TG) = internal nodes of TG
hyper(ηi) = supernodes of ηi excluding the root
hypo(ηi) = subnodes of ηi including ηi
For ηi E I(TA), the semantic cotopy is defined as:
SC&apos;(ηi) = (hyper(ηi) U hypo(ηi)) n C(TG)
For ηi E C(TG), the semantic cotopy is defined as:
SC&apos;&apos;(ηi) = (hyper(ηi) U hypo(ηi))
|SC&apos;(ηi) n SC&apos;&apos;(ηj) |(1)
P(ηi, ηj) = |SC&apos;(ηi)|
R(ηi, ηj) = |SC&apos;(ηi) n SC&apos;&apos;(ηj) |(2)
|SC&apos;&apos;(ηj)|
2P(ηi, ηj)R(ηi, ηj)
F(ηi, ηj) = (3)
P(ηi, ηj) + R(ηi, ηj)
</equation>
<bodyText confidence="0.9998011875">
Precision, recall and harmonic mean of node ηi E
I(TA) with respect to node ηj E C(TG) are de-
fined in Equations 1, 2 and 3. The F-score, FS, of
node ηi E I(TA) is the maximum F attained at any
ηj E C(TG) (FS(ηi) = argmaxj F(ηi,ηj)). Fi-
nally, the similarity TS of the entire taxonomy to
the gold standard taxonomy is the average of the
F-scores of each ηi E I(TA) (Equation 4). The
TS(TA,TG) in Figure 1 is 0.9. All nodes of TA
have a perfect match, apart from New Cluster 0 and
New Cluster 2, which are matched against computer
network and meshwork respectively, having a per-
fect precision but a lower recall since the cotopies
of computer network and meshwork consist of three
concepts. The automatically learned taxonomy has
two redundant clusters that decrease its similarity.
</bodyText>
<equation confidence="0.996549666666667">
1 �
TS(TA, TG) =
|I(TA) |?7zEI(TA)
</equation>
<bodyText confidence="0.9999665">
The similarity measure TS(TA,TG) provides the
similarity of the automatically learned taxonomy to
the gold standard one, but it is not symmetric. Cal-
culating the taxonomic similarity one way might not
provide accurate results, in cases where TA misses
senses of the gold standard. This is due to the
fact that we would only evaluate the internal nodes
of TA, partially ignoring the fact that TA might
have missed some parts of the gold standard taxon-
omy. For that reason, we also calculate TS(TG, TA)
which provides the similarity of the gold standard
taxonomy to the automatically learned one. Fi-
nally, taxonomic similarities are combined to pro-
duce their harmonic mean (Equation 5).
</bodyText>
<equation confidence="0.896340666666667">
2TS(TG, TA)TS(TA, TG)
TxSm(TA, TG) = (5)
TS(TG,TA) + TS(TA,TG)
</equation>
<subsectionHeader confidence="0.988179">
4.3 Evaluation datasets &amp; setting
</subsectionHeader>
<bodyText confidence="0.999690882352941">
The first gold standard taxonomy is derived by ex-
tracting from WordNet all the hyponyms of the
senses of the word network. The extracted taxonomy
contains 29 senses linguistically realized by 24 word
sets (one sense might be expressed with more than
one words), since network has 5 senses and reseau
has 2 senses in the gold standard taxonomy. Note
that we have disregarded senses only expressed by
multiword expressions. The average polysemy of
words is around 1.7. The second taxonomy is de-
rived by extracting the concepts under the senses of
the word speaker. The speaker taxonomy contains
52 senses linguistically realized by 50 word sets,
since speaker has 3 senses included in the taxonomy.
The average polysemy of words is around 1.58.
To create our datasets3 we use the Yahoo! search
api4. For each word w in each of the datasets, we is-
</bodyText>
<footnote confidence="0.999849">
3Available in http://www.cs.york.ac.uk/aig/projects/indect/taxlearn
4http://developer.yahoo.com/search/ [Accessed:10/06/2009]
</footnote>
<equation confidence="0.915904">
FS(ηi) (4)
</equation>
<page confidence="0.987695">
87
</page>
<table confidence="0.9997285">
Parameter Range
G2 threshold (p1) 5,10
Collocation frequency (p2) 4,6,8
Collocation weight (p3) 0.1,0.2,0.3,0.4
</table>
<tableCaption confidence="0.999904">
Table 3: Chosen parameters for the KM WSI method.
</tableCaption>
<bodyText confidence="0.999983517241379">
sue a query to Yahoo! that contains w and we down-
load a maximum of 1000 pages. In cases where
a particular sense is expressed by more than one
word, the query was formulated by including all the
words and putting the keyword OR between them.
For each page we extracted fragments of text (para-
graphs) that occur in &lt;p&gt; &lt;/p&gt; html tags. We ex-
tracted 58956 and 78691 paragraphs for the network
and speaker dataset respectively. The reason we ex-
tracted on average less content for the second dataset
was that Yahoo! provided a small number of results
for rare words such as alliterator, anecdotist, etc.
Table 3 shows the parameter ranges for the WSI
method. Our method is evaluated according to these
parameters. Our first baseline is RAND, which per-
forms a random hierarchical clustering of senses to
produce a binary tree. In each iteration two clusters
are randomly chosen and form a new cluster, until
we end up with one cluster taken to be the root. The
performance of RAND is calculated by executing the
random algorithm 10 times and then averaging the
results. The second baseline is the taxonomy most
frequent sense baseline (TL MFS), in which we do
not perform WSI. Instead, given a parameter setting
and a word w, all the collocations of w are grouped
into one vector, which will possibly be dominated
by collocations related to the MFS of w. WordNet
mapping takes place and finally HAC with average-
linkage is applied to create the taxonomy.
</bodyText>
<subsectionHeader confidence="0.984341">
4.4 Results &amp; discussion
</subsectionHeader>
<bodyText confidence="0.999885166666667">
Figures 4 (a) and 4 (b) show the performance
of HAC with single-linkage (HAC SNG), average-
linkage (HAC AVG) and complete-linkage (HAC
CMP) against RAND for p1 = 5 and different com-
binations of p2 and p3. It is clear that HAC SNG and
HAC AVG outperform RAND by very large margins
under all parameter combinations. In the network
dataset, both of them achieve their highest distance
from RAND (27.84%) at p2 = 8 and p3 = 0.2. In the
speaker dataset, their highest distance from RAND
(20.97% and 19.63% respectively) is achieved at
p2 = 4 and p3 = 0.1. HAC CMP performs worse
than the other HAC versions, yet it clearly outper-
forms RAND in all but one parameter combinations
(p1 = 5, p2 = 6, p3 = 0.4) in the speaker dataset.
Generally, for collocation weight equal to 0.4 the
performance of all HAC versions drops. At this
high collocation weight the WSI method produces a
larger number of small clusters than in lower thresh-
olds. This issue negatively affects both the map-
ping process and HAC. For example in the speaker
dataset, for p1 = 5, p2 = 8 and p3 = 0.1 our tax-
onomies contained 86.54% of the gold standard tax-
onomy senses. Increasing the collocation weight to
0.2 did not have any effect, but increasing the weight
to 0.3 and then 0.4 led to 71.15% and 65.38% sense
coverage. Overall, our conclusion is that all HAC
versions exploit the WSI method and learn useful
information better than chance. The picture is the
same for p1 = 10.
Figures 4 (c) and 4 (d) show the performance of
HAC versions against the TL MFS baseline in the
same parameter setting as above. We observe that
both HAC SNG and HAC AVG perform significantly
better than TL MFS apart from p3 = 0.4, in which
case all HAC versions perform worse. In the network
dataset, the largest performance difference for HAC
SNG is 10.12% and for HAC AVG 9.9% at p2 = 6
and p3 = 0.2. In the speaker dataset, the largest per-
formance difference for HAC SNG is 10.83% and
for HAC AVG 7.83% at p2 = 8 and p3 = 0.2. HAC
CMP performs worse than TL MFS under most pa-
rameter settings in both datasets. The picture is the
same for p1 = 10.
Overall, the analysis of the WSI-based taxonomy
learning approach against TL MFS shows that HAC
SNG and HAC AVG perform better than TL MFS
under all parameter combinations for both datasets.
The main reason for their superior performance is
that their learned taxonomies contain a higher num-
ber of senses than TL MFS as a result of the sense
induction process. This greater sense coverage leads
to the discovery of a higher number of correct taxo-
nomic relations between senses than TL MFS, hence
in a better performance. To conclude, our results
verify our hypothesis and suggest that the unsuper-
vised learning of word senses contributes to produc-
ing taxonomies with a higher similarity to the gold
standard ones than traditional distributional similar-
ity methods.
</bodyText>
<page confidence="0.998402">
88
</page>
<figureCaption confidence="0.999106">
Figure 4: Performance analysis of the proposed method for p1 = 5 and different combinations of p2 and p3.
</figureCaption>
<bodyText confidence="0.999978419354839">
Despite that, our evaluation also shows that in
most cases HAC CMP is unable to exploit the in-
duced senses and performs worse than TL MFS,
HAC SNG and HAC AVG. This result was not ex-
pected, since HAC SNG employs a local criterion to
merge two clusters and does not consider the global
structure of the clusters, in effect, being biased to-
wards elongated clusters. The observation of the
gold standard taxonomies shows that they consist
both of cohyponym concepts which are expected
to be contextually related, but also of cohyponyms
which are not expected to appear in similar contexts.
For example, someone would expect a high similar-
ity between WAN, LAN, or between snood and tulle.
However, the same does not apply for snood and
cheesecloth or tulle and grillwork, because cheese-
cloth and grillwork appear in significantly different
contexts than snood and tulle. Despite that, all of
them are cohyponyms. This issue is more prevalent
in the speaker dataset, where concepts such as loud-
speaker, tannoy, woofer are expected to be contex-
tually related, while cohyponyms such as whisperer,
lecturer and interviewer are not. This means that the
gold standard taxonomies include elongated clusters
and explains the superior performance of HAC SNG.
This issue is not affecting HACAVG, but it has a sig-
nificant effect on HAC CMP. Generally, HAC CMP
employs a non-local criterion by considering the di-
ameter of a candidate cluster. This results in com-
pact clusters with small diameters, as opposed to
elongated ones.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999899">
We presented an unsupervised method for taxonomy
learning that employs WSI to identify the senses of
target words and then builds a taxonomy of these
senses using HAC. We have shown that dealing with
polysemy by means of sense induction helps to de-
velop taxonomies that capture a higher number of
correct taxonomic relations than traditional distribu-
tional similarity methods, which associate each tar-
get word with one vector of features, in effect, merg-
ing its senses.
</bodyText>
<sectionHeader confidence="0.996549" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.98525875">
This work is supported by the European Commis-
sion via the EU FP7 INDECT project, Grant No.
218086, Research area: SEC-2007-1.2-01 Intelli-
gent Urban Environment Observation System.
</bodyText>
<page confidence="0.999683">
89
</page>
<sectionHeader confidence="0.990319" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99969825">
E. Agirre and A. Soroa. 2007. SemEval-2007 Task
02: Evaluating Word Sense Induction and Discrimi-
nation Systems. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations, pages 7–12,
Prague, Czech Republic.
R. A. Amsler. 1981. A Taxonomy for English Nouns and
Verbs. In Proceedings of the 19th ACL Conference,
pages 133–138, Stanford, California.
C. Biemann. 2006. Chinese Whispers - An Efficient
Graph Clustering Algorithm and its Application to
Natural Language Processing Problems. In Proceed-
ings of TextGraphs, pages 73–80, New York,USA.
P. Buitelaar, D. Olejnik, and M. Sintek. 2004. A Ptot´eg´e
Plug-in for Ontology Extraction from Text Based on
Linguistic Analysis. In Proceedings of the 1st Euro-
pean Semantic Web Symposium, pages 31–44, Crete,
Greece. CEUR-WS.org.
S. A. Caraballo. 1999. Automatic Construction of a
Hypernym-labeled Noun Hierarchy from Text. In Pro-
ceedings of the 37th ACL Conference, pages 120–126,
College Park, Maryland.
P. Cimiano, A. Hotho, and S. Staab. 2004. Compar-
ing Conceptual, Divisive and Agglomerative Cluster-
ing for Learning Taxonomies from Text. In Proceed-
ings of the 16th ECAI Conference, pages 435–439, Va-
lencia, Spain.
P. Cimiano, A. Hotho, and S. Staab. 2005. Learning
Concept Hieararchies from Text Corpora Using For-
mal Concept Analysis. Journal of Artificial Intelli-
gence Research, 24:305–339.
P. Cimiano. 2006. Ontology Learning and Population
from Text: Algorithms, Evaluation and Applications.
Springer-Verlag New York, Inc., Secaucus, NJ, USA.
T. Dunning. 1993. Accurate Methods for the Statistics of
Surprise and Coincidence. Computational Linguistics,
19(1):61–74.
D. Faure and C. N´edellec. 1998. A Corpus-based Con-
ceptual Clustering Method for Verb Frames and On-
tology Acquisition. In LREC workshop on Adapting
lexical and corpus resources to sublanguages and ap-
plications, pages 5–12, Granada, Spain.
C. Fellbaum. 1998. Wordnet: An Electronic Lexical
Database. MIT Press, Cambridge, Massachusetts,
USA.
B. Ganter and R. Wille. 1999. Formal Concept Anal-
ysis: Mathematical Foundations. Springer-Verlag
New York, Inc., Secaucus, NJ, USA. Translator-C.
Franzke.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley, New York, USA.
M. A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings of
the 14th Coling Conference, pages 539–545, Nantes,
France.
C. H. Hwang. 1999. Incompletely and Imprecisely
Speaking: Using Dynamic Ontologies for Represent-
ing and Retrieving Information. In Proceedings of
the 6th International Workshop on Knowledge Repre-
sentation Meets Databases, pages 14–20, Linkoping,
Sweden. CEUR-WS.org.
B. King. 1967. Step-wise Clustering Procedures. Jour-
nal of the American Statistical Association, 69:86–
101.
I. P. Klapaftis and S. Manandhar. 2008. Word Sense In-
duction Using Graphs of Collocations. In Proceedings
of the 18th ECAI Conference, pages 298–302, Patras,
Greece. IOS Press.
A. Maedche and S. Staab. 2002. Measuring Similarity
between Ontologies. In Proceedings of the European
Conference on Knowledge Acquisition and Manage-
ment (EKAW), pages 251–263, London,UK. Springer-
Verlag.
D. Moldovan and A. Novischi. 2002. Lexical Chains
for Question Answering. In Proceedings of the 19th
Coling Conference, pages 1–7, Taipei, Taiwan.
R. Navigli and P. Velardi. 2004a. Learning Domain On-
tologies from Document Warehouses and Dedicated
web Sites. Computational Linguistics, 30(2):151–179.
R. Navigli and P. Velardi. 2004b. Structural Semantic In-
terconnection: a Knowledge-based Approach to Word
Sense Disambiguation. In Proceedings of Senseval-
3: Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 179–
182, Barcelona, Spain.
M.L. Reinberger and P. Spyns. 2004. Discovering
Knowledge in Texts for the Learning of Dogma-
inspired Ontologies. In Proceedings of the ECAI
Workshop on Ontology Learning and Population,
pages 19–24, Valencia, Spain.
M. L. Reinberger, P. Spyns, W. Daelemans, and R. Meers-
man. 2003. Mining for Lexons: Applying Unsuper-
vised Learning Methods to create ontology bases. In
CoopIS/DOA/ODBASE, pages 803–819.
D. S´anchez and A. Moreno. 2005. Web-scale Taxon-
omy Learning. In Proceedings of the Workshop on
Learning and Extending Ontologies by using Machine
Learning methods, pages 53–60, Bonn, Germany.
P. H. A. Sneath and R. R. Sokal. 1973. Numerical Taxon-
omy, The Principles and Practice of Numerical Clas-
sification. W. H. Freeman, San Francisco, USA.
</reference>
<page confidence="0.99862">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.590496">
<title confidence="0.999385">Taxonomy Learning Using Word Sense Induction</title>
<author confidence="0.979141">P Ioannis</author>
<affiliation confidence="0.9993515">Department of Computer The University of</affiliation>
<address confidence="0.998246">York, UK, YO10</address>
<email confidence="0.997085">giannis@cs.york.ac.uk</email>
<author confidence="0.621403">Suresh</author>
<affiliation confidence="0.999093">Department of Computer The University of</affiliation>
<address confidence="0.998122">York, UK, YO10</address>
<email confidence="0.998354">suresh@cs.york.ac.uk</email>
<abstract confidence="0.998853823529412">Taxonomies are an important resource for a variety of Natural Language Processing (NLP) applications. Despite this, the current stateof-the-art methods in taxonomy learning have disregarded word polysemy, in effect, developing taxonomies that conflate word senses. In this paper, we present an unsupervised method that builds a taxonomy of senses learned automatically from an unlabelled corpus. Our evaluation on two WordNet-derived taxonomies shows that the learned taxonomies capture a higher number of correct taxonomic relations compared to those produced by traditional distributional similarity approaches that merge senses by grouping the features of each word into a single vector.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>A Soroa</author>
</authors>
<title>SemEval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations,</booktitle>
<pages>7--12</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="10074" citStr="Agirre and Soroa, 2007" startWordPosition="1608" endWordPosition="1611">se induction we propose in this paper aims to overcome these limitations. 3 Method Given a set of words W, a WSI method is applied to each wz E W (Section 3.1). The outcome of the first stage is a set of senses, 5, where each sw E 5 denotes the i-th sense of word w E W. This set Figure 2: WSI for network &amp; LAN of senses is the input to hierarchical clustering that produces a hierarchy of senses (Section 3.2). 3.1 Word sense induction WSI is the task of identifying the senses of a target word in a given text. Recent WSI methods were evaluated under the framework of SemEval2007 WSI task (SWSI) (Agirre and Soroa, 2007). The evaluation framework defines two types of assessment, i.e. evaluation in: (1) a clustering and (2) a WSD setting. Based on this evaluation, we selected the method of Klapaftis &amp; Manandhar (2008) (henceforth referred to as KM) that achieves high Fscore in both evaluation schemes as compared to the systems participating in SWSI. We briefly describe KM mentioning its parameters used in our evaluation (Section 4). Figures 2 (a) and 2 (b) describe the different steps for inducing the senses of the target words network and LAN. Corpus preprocessing: The input to KM is a base corpus bc, in whic</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>E. Agirre and A. Soroa. 2007. SemEval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems. In Proceedings of the Fourth International Workshop on Semantic Evaluations, pages 7–12, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Amsler</author>
</authors>
<title>A Taxonomy for English Nouns and Verbs.</title>
<date>1981</date>
<booktitle>In Proceedings of the 19th ACL Conference,</booktitle>
<pages>133--138</pages>
<location>Stanford, California.</location>
<contexts>
<context position="4650" citStr="Amsler, 1981" startWordPosition="732" endWordPosition="733">identifying in a given text lexico-syntactic patterns that suggest hyponymy relations (Hearst, 1992). For instance, the pattern NP0 such as NP1,... ,NP,,, suggests that NP0 is a hypernym of NPi. For example, given the phrase Fruits, such as oranges, apples,..., the above pattern would suggest that fruit is a hypernym of orange and apple. These patternbased approaches operate at the word level by learning lexical relations between words rather than between senses of words. In the same spirit, other work attempted to exploit the regularities of dictionary entries to identify hyponymy relations (Amsler, 1981). For example in WordNet, WAN is defined as a computer network that spans .... Hence, one can easily induce that WAN is a hyponym of computer network by assuming that the first noun phrase in the definition is a hypernym of the target word. These approaches learn lexical relations at the sense level since dictionaries separate the senses of a word. However this would be true if and only if the glosses of the dictionaries were sense-annotated, which is not the case for the majority of electronic dictionaries (Cimiano, 2006). Another limitation is that taxonomies are built according to the sense</context>
</contexts>
<marker>Amsler, 1981</marker>
<rawString>R. A. Amsler. 1981. A Taxonomy for English Nouns and Verbs. In Proceedings of the 19th ACL Conference, pages 133–138, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Biemann</author>
</authors>
<title>Chinese Whispers - An Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems.</title>
<date>2006</date>
<booktitle>In Proceedings of TextGraphs,</booktitle>
<pages>73--80</pages>
<location>New York,USA.</location>
<contexts>
<context position="12683" citStr="Biemann, 2006" startWordPosition="2053" endWordPosition="2054">x. Edges between vertices are present, if two collocations co-occur in one or more paragraphs. Figure 2 (a) shows that this process has generated 24 collocations for the target word network. On the top right of the figure we also observe the collocations associated with each paragraph. In the next step, a smoothing technique is applied to discover new edges between vertices. The weight applied to each edge connecting vertices vi and vj (collocations cab, cde) is the maximum of their conditional probabilities (max(p(cab|cde),p(cde|cab))). Finally, the graph is clustered using Chinese whispers (Biemann, 2006). The final output is a set of senses, each one represented by a set of contextually related collocations. In Figure 2, we generated two senses for network and one sense for LAN. 3.2 Hierarchical clustering of senses Given the set of senses 5, our task at this point is to hierarchically classify the senses using HAC. Consider for example the words network and LAN, and 2The British National Corpus, 2001, Distributed by Oxford University Computing Services. Senses computer meshwork LAN network computer network 1 0.0 0.66 meshwork 0.0 1 0.14 LAN 0.66 0.14 1 Table 1: Similarity matrix for HAC. Fig</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>C. Biemann. 2006. Chinese Whispers - An Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems. In Proceedings of TextGraphs, pages 73–80, New York,USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
<author>D Olejnik</author>
<author>M Sintek</author>
</authors>
<title>A Ptot´eg´e Plug-in for Ontology Extraction from Text Based on Linguistic Analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the 1st European Semantic Web Symposium,</booktitle>
<pages>31--44</pages>
<location>Crete, Greece. CEUR-WS.org.</location>
<contexts>
<context position="8502" citStr="Buitelaar et al., 2004" startWordPosition="1345" endWordPosition="1348">arning to deal with polysemy. These methods associate each verb with a vector of features, where each feature is a noun appearing as a subject or object of that verb. That way a noun can appear in different vectors, hence in different clusters during hierarchical clustering as a result of its polysemy. However, the underlying assumption is that a verb is monosemous with respect to its associated vector of nouns. This assumption is not always valid and can cause the problems mentioned above. Other work in taxonomy learning exploits the head/modifier relationships to create taxonomic relations (Buitelaar et al., 2004; Hwang, 1999; S´anchez and Moreno, 2005). These relations are used to create: (1) a class (concept) for each head, and (2) subclasses by adding nominal or adjectival modifiers. For example, credit card IS-A card. The corresponding hyponymy relations are learned at the lexical level disregarding word polysemy. Some of these approaches identified the problem of polysemy and applied sense disambiguation with respect to WordNet in order to capture the different senses of a target term (Navigli and Velardi, 2004b; Navigli and Velardi, 2004a). Specifically, the taxonomy built by exploiting head/mod</context>
</contexts>
<marker>Buitelaar, Olejnik, Sintek, 2004</marker>
<rawString>P. Buitelaar, D. Olejnik, and M. Sintek. 2004. A Ptot´eg´e Plug-in for Ontology Extraction from Text Based on Linguistic Analysis. In Proceedings of the 1st European Semantic Web Symposium, pages 31–44, Crete, Greece. CEUR-WS.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Caraballo</author>
</authors>
<title>Automatic Construction of a Hypernym-labeled Noun Hierarchy from Text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th ACL Conference,</booktitle>
<pages>120--126</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="5613" citStr="Caraballo, 1999" startWordPosition="889" endWordPosition="890">rd. However this would be true if and only if the glosses of the dictionaries were sense-annotated, which is not the case for the majority of electronic dictionaries (Cimiano, 2006). Another limitation is that taxonomies are built according to the sense distinctions present in dictionaries and not according to the actual use of words in the corpus. The majority of taxonomy learning approaches are based on the distributional hypothesis (Harris, 1968). Typically, distributional similarity methods (Cimiano et al., 2004; Cimiano et al., 2005; Faure and N´edellec, 1998; Reinberger and Spyns, 2004; Caraballo, 1999) utilise syntactic dependencies such as subject/verb, object/verb relations, conjunctive and appositive constructions and others. These dependencies are used to extract the features that serve as the dimensions of the vector space. Each target noun is then represented as a vector of extracted features where the frequency of co-occurrence of the target noun with each feature is used to calculate the weight of that feature. The constructed vectors are the input to hierarchical clustering or formal concept analysis (Ganter and Wille, 1999) to produce a taxonomy. These approaches assume that a tar</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>S. A. Caraballo. 1999. Automatic Construction of a Hypernym-labeled Noun Hierarchy from Text. In Proceedings of the 37th ACL Conference, pages 120–126, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>A Hotho</author>
<author>S Staab</author>
</authors>
<title>Comparing Conceptual, Divisive and Agglomerative Clustering for Learning Taxonomies from Text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 16th ECAI Conference,</booktitle>
<pages>435--439</pages>
<location>Valencia,</location>
<contexts>
<context position="5518" citStr="Cimiano et al., 2004" startWordPosition="873" endWordPosition="876">pproaches learn lexical relations at the sense level since dictionaries separate the senses of a word. However this would be true if and only if the glosses of the dictionaries were sense-annotated, which is not the case for the majority of electronic dictionaries (Cimiano, 2006). Another limitation is that taxonomies are built according to the sense distinctions present in dictionaries and not according to the actual use of words in the corpus. The majority of taxonomy learning approaches are based on the distributional hypothesis (Harris, 1968). Typically, distributional similarity methods (Cimiano et al., 2004; Cimiano et al., 2005; Faure and N´edellec, 1998; Reinberger and Spyns, 2004; Caraballo, 1999) utilise syntactic dependencies such as subject/verb, object/verb relations, conjunctive and appositive constructions and others. These dependencies are used to extract the features that serve as the dimensions of the vector space. Each target noun is then represented as a vector of extracted features where the frequency of co-occurrence of the target noun with each feature is used to calculate the weight of that feature. The constructed vectors are the input to hierarchical clustering or formal conc</context>
</contexts>
<marker>Cimiano, Hotho, Staab, 2004</marker>
<rawString>P. Cimiano, A. Hotho, and S. Staab. 2004. Comparing Conceptual, Divisive and Agglomerative Clustering for Learning Taxonomies from Text. In Proceedings of the 16th ECAI Conference, pages 435–439, Valencia, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>A Hotho</author>
<author>S Staab</author>
</authors>
<title>Learning Concept Hieararchies from Text Corpora Using Formal Concept Analysis.</title>
<date>2005</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>24--305</pages>
<contexts>
<context position="5540" citStr="Cimiano et al., 2005" startWordPosition="877" endWordPosition="880">l relations at the sense level since dictionaries separate the senses of a word. However this would be true if and only if the glosses of the dictionaries were sense-annotated, which is not the case for the majority of electronic dictionaries (Cimiano, 2006). Another limitation is that taxonomies are built according to the sense distinctions present in dictionaries and not according to the actual use of words in the corpus. The majority of taxonomy learning approaches are based on the distributional hypothesis (Harris, 1968). Typically, distributional similarity methods (Cimiano et al., 2004; Cimiano et al., 2005; Faure and N´edellec, 1998; Reinberger and Spyns, 2004; Caraballo, 1999) utilise syntactic dependencies such as subject/verb, object/verb relations, conjunctive and appositive constructions and others. These dependencies are used to extract the features that serve as the dimensions of the vector space. Each target noun is then represented as a vector of extracted features where the frequency of co-occurrence of the target noun with each feature is used to calculate the weight of that feature. The constructed vectors are the input to hierarchical clustering or formal concept analysis (Ganter a</context>
<context position="18079" citStr="Cimiano et al., 2005" startWordPosition="2991" endWordPosition="2994">abel(swj ) = argmaxi JC(fwj , qwi ), where JC is the Jaccard similarity coefficient. In the example of Figure 2 (a), the computer network sense would be mapped to the fifth WordNet sense of network, since there is a significant overlap between the paragraphs tagged by the induced and that WordNet sense. 4.2 Evaluation measures For the purposes of this section we present one gold standard taxonomy (Figure 1 (a)) and a second derived from our method (Figure 1 (b)). The comparison of these taxonomies is based on the semantic cotopy of a node, which has also been used in (Maedche and Staab, 2002; Cimiano et al., 2005). In particular, the semantic cotopy of a node is defined as the set of all its super- and subnodes excluding the root and including that node. For example, the semantic cotopy of computer network in Figure 1 (a) is {computer network, internet, LAN}. There are two issues, which make the evaluation difficult. The first one is that HAC produces a taxonomy in which all internal nodes are unlabelled, as opposed to the gold standard taxonomy. In Figure 1 (b), we have manually labelled internal nodes with their IDs for clarity. For example, the semantic cotopy of the node New Cluster 1 in Figure 1 (</context>
</contexts>
<marker>Cimiano, Hotho, Staab, 2005</marker>
<rawString>P. Cimiano, A. Hotho, and S. Staab. 2005. Learning Concept Hieararchies from Text Corpora Using Formal Concept Analysis. Journal of Artificial Intelligence Research, 24:305–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
</authors>
<title>Ontology Learning and Population from Text: Algorithms, Evaluation and Applications.</title>
<date>2006</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc., Secaucus, NJ, USA.</location>
<contexts>
<context position="2570" citStr="Cimiano, 2006" startWordPosition="410" endWordPosition="411">oncepts are divided into groups, which are further subdivided into subgroups and so forth, until we reach a level where each concept belongs to its own group. Unlabelled taxonomies are typically produced by agglomerative hierarchical clustering algorithms (King, 1967; Sneath and Sokal, 1973). The knowledge encoded in taxonomies can be utilised in a range of NLP applications. For instance, taxonomies can be used in information retrieval to expand a user query with semantically related words or to enhance document representation by abstracting from plain words and adding conceptual information (Cimiano, 2006). WordNet’s (Fellbaum, 1998) taxonomic relations have also been used in Word Sense Disambiguation (WSD) (Navigli and Velardi, 2004b). In named entity recognition, methods relying on gazetteers could make Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 82–90, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics use of automatically acquired taxonomies (Cimiano, 2006), while question answering systems have also benefited (Moldovan and Novischi, 2002). Despite the wide uses of taxonomies, the majority of metho</context>
<context position="5178" citStr="Cimiano, 2006" startWordPosition="824" endWordPosition="825">t the regularities of dictionary entries to identify hyponymy relations (Amsler, 1981). For example in WordNet, WAN is defined as a computer network that spans .... Hence, one can easily induce that WAN is a hyponym of computer network by assuming that the first noun phrase in the definition is a hypernym of the target word. These approaches learn lexical relations at the sense level since dictionaries separate the senses of a word. However this would be true if and only if the glosses of the dictionaries were sense-annotated, which is not the case for the majority of electronic dictionaries (Cimiano, 2006). Another limitation is that taxonomies are built according to the sense distinctions present in dictionaries and not according to the actual use of words in the corpus. The majority of taxonomy learning approaches are based on the distributional hypothesis (Harris, 1968). Typically, distributional similarity methods (Cimiano et al., 2004; Cimiano et al., 2005; Faure and N´edellec, 1998; Reinberger and Spyns, 2004; Caraballo, 1999) utilise syntactic dependencies such as subject/verb, object/verb relations, conjunctive and appositive constructions and others. These dependencies are used to extr</context>
</contexts>
<marker>Cimiano, 2006</marker>
<rawString>P. Cimiano. 2006. Ontology Learning and Population from Text: Algorithms, Evaluation and Applications. Springer-Verlag New York, Inc., Secaucus, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate Methods for the Statistics of Surprise and Coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="11175" citStr="Dunning, 1993" startWordPosition="1805" endWordPosition="1806">he senses of the target words network and LAN. Corpus preprocessing: The input to KM is a base corpus bc, in which the target word w appears in each paragraph. In Figure 2 (a), the base corpus consists of the paragraphs A, B, C and D. The aim of this stage is to capture nouns contextually 84 related to w. Initially, the target word is removed from bc, part-of-speech tagging is applied to each paragraph, only nouns are kept and lemmatised. In the next step, the distribution of each noun is compared to the distribution of the same noun in a reference corpus2 using the log-likelihood ratio (G2) (Dunning, 1993). Nouns with a G2 below a prespecified threshold (parameter p1) are removed from each paragraph. Figure 2 (a) shows the remaining nouns for each paragraph of bc. Graph creation &amp; clustering: In the setting of KM, a collocation is a juxtaposition of two nouns within the same paragraph. Thus, each noun is combined with any other noun yielding a total of (N ) 2 collocations for a paragraph with N nouns. Each collocation, cij, is assigned a weight that measures the relative frequency of two nouns co-occurring. This weight is the average of the conditional probabilities p(ni|nj) and p(nj|ni), where</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Faure</author>
<author>C N´edellec</author>
</authors>
<title>A Corpus-based Conceptual Clustering Method for Verb Frames and Ontology Acquisition. In LREC workshop on Adapting lexical and corpus resources to sublanguages and applications,</title>
<date>1998</date>
<pages>5--12</pages>
<location>Granada,</location>
<marker>Faure, N´edellec, 1998</marker>
<rawString>D. Faure and C. N´edellec. 1998. A Corpus-based Conceptual Clustering Method for Verb Frames and Ontology Acquisition. In LREC workshop on Adapting lexical and corpus resources to sublanguages and applications, pages 5–12, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>Wordnet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="2598" citStr="Fellbaum, 1998" startWordPosition="413" endWordPosition="415">oups, which are further subdivided into subgroups and so forth, until we reach a level where each concept belongs to its own group. Unlabelled taxonomies are typically produced by agglomerative hierarchical clustering algorithms (King, 1967; Sneath and Sokal, 1973). The knowledge encoded in taxonomies can be utilised in a range of NLP applications. For instance, taxonomies can be used in information retrieval to expand a user query with semantically related words or to enhance document representation by abstracting from plain words and adding conceptual information (Cimiano, 2006). WordNet’s (Fellbaum, 1998) taxonomic relations have also been used in Word Sense Disambiguation (WSD) (Navigli and Velardi, 2004b). In named entity recognition, methods relying on gazetteers could make Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 82–90, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics use of automatically acquired taxonomies (Cimiano, 2006), while question answering systems have also benefited (Moldovan and Novischi, 2002). Despite the wide uses of taxonomies, the majority of methods disregard or do not deal </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. Wordnet: An Electronic Lexical Database. MIT Press, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Ganter</author>
<author>R Wille</author>
</authors>
<title>Formal Concept Analysis: Mathematical Foundations.</title>
<date>1999</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc., Secaucus, NJ, USA. Translator-C. Franzke.</location>
<contexts>
<context position="6155" citStr="Ganter and Wille, 1999" startWordPosition="970" endWordPosition="973">l., 2005; Faure and N´edellec, 1998; Reinberger and Spyns, 2004; Caraballo, 1999) utilise syntactic dependencies such as subject/verb, object/verb relations, conjunctive and appositive constructions and others. These dependencies are used to extract the features that serve as the dimensions of the vector space. Each target noun is then represented as a vector of extracted features where the frequency of co-occurrence of the target noun with each feature is used to calculate the weight of that feature. The constructed vectors are the input to hierarchical clustering or formal concept analysis (Ganter and Wille, 1999) to produce a taxonomy. These approaches assume that a target noun is monosemous creating one vector of features for each target noun. This limitation can lead to a number of problems. Firstly, the constructed taxonomies might be biased towards the inclusion of taxonomic relationships between the most frequent senses of target nouns, ignoring interesting taxonomic relations where less frequent senses are present. For example, consider the word house. Current distributional similarity methods would possibly capture the hyponyms of its Most Frequent Sense (MFS1), however ignoring the hyponyms of</context>
</contexts>
<marker>Ganter, Wille, 1999</marker>
<rawString>B. Ganter and R. Wille. 1999. Formal Concept Analysis: Mathematical Foundations. Springer-Verlag New York, Inc., Secaucus, NJ, USA. Translator-C. Franzke.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley,</publisher>
<location>New York, USA.</location>
<contexts>
<context position="5450" citStr="Harris, 1968" startWordPosition="867" endWordPosition="868">e in the definition is a hypernym of the target word. These approaches learn lexical relations at the sense level since dictionaries separate the senses of a word. However this would be true if and only if the glosses of the dictionaries were sense-annotated, which is not the case for the majority of electronic dictionaries (Cimiano, 2006). Another limitation is that taxonomies are built according to the sense distinctions present in dictionaries and not according to the actual use of words in the corpus. The majority of taxonomy learning approaches are based on the distributional hypothesis (Harris, 1968). Typically, distributional similarity methods (Cimiano et al., 2004; Cimiano et al., 2005; Faure and N´edellec, 1998; Reinberger and Spyns, 2004; Caraballo, 1999) utilise syntactic dependencies such as subject/verb, object/verb relations, conjunctive and appositive constructions and others. These dependencies are used to extract the features that serve as the dimensions of the vector space. Each target noun is then represented as a vector of extracted features where the frequency of co-occurrence of the target noun with each feature is used to calculate the weight of that feature. The constru</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Z. Harris. 1968. Mathematical Structures of Language. Wiley, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th Coling Conference,</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="4137" citStr="Hearst, 1992" startWordPosition="645" endWordPosition="646"> set of target words from an unlabelled corpus and then produces a taxonomy of senses using Hierarchical Agglomerative Clustering (HAC) (King, 1967; Sneath and Sokal, 1973). We evaluate our method on two WordNetderived sub-taxonomies and show that our method leads to the development of concept hierarchies that capture a higher number of correct taxonomic relations in comparison to those generated by current distributional similarity approaches. 2 Related work Initial research on taxonomy learning focused on identifying in a given text lexico-syntactic patterns that suggest hyponymy relations (Hearst, 1992). For instance, the pattern NP0 such as NP1,... ,NP,,, suggests that NP0 is a hypernym of NPi. For example, given the phrase Fruits, such as oranges, apples,..., the above pattern would suggest that fruit is a hypernym of orange and apple. These patternbased approaches operate at the word level by learning lexical relations between words rather than between senses of words. In the same spirit, other work attempted to exploit the regularities of dictionary entries to identify hyponymy relations (Amsler, 1981). For example in WordNet, WAN is defined as a computer network that spans .... Hence, o</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. A. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In Proceedings of the 14th Coling Conference, pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Hwang</author>
</authors>
<title>Incompletely and Imprecisely Speaking: Using Dynamic Ontologies for Representing and Retrieving Information.</title>
<date>1999</date>
<booktitle>In Proceedings of the 6th International Workshop on Knowledge Representation Meets Databases,</booktitle>
<pages>14--20</pages>
<location>Linkoping, Sweden. CEUR-WS.org.</location>
<contexts>
<context position="8515" citStr="Hwang, 1999" startWordPosition="1349" endWordPosition="1350">semy. These methods associate each verb with a vector of features, where each feature is a noun appearing as a subject or object of that verb. That way a noun can appear in different vectors, hence in different clusters during hierarchical clustering as a result of its polysemy. However, the underlying assumption is that a verb is monosemous with respect to its associated vector of nouns. This assumption is not always valid and can cause the problems mentioned above. Other work in taxonomy learning exploits the head/modifier relationships to create taxonomic relations (Buitelaar et al., 2004; Hwang, 1999; S´anchez and Moreno, 2005). These relations are used to create: (1) a class (concept) for each head, and (2) subclasses by adding nominal or adjectival modifiers. For example, credit card IS-A card. The corresponding hyponymy relations are learned at the lexical level disregarding word polysemy. Some of these approaches identified the problem of polysemy and applied sense disambiguation with respect to WordNet in order to capture the different senses of a target term (Navigli and Velardi, 2004b; Navigli and Velardi, 2004a). Specifically, the taxonomy built by exploiting head/modifiers relati</context>
</contexts>
<marker>Hwang, 1999</marker>
<rawString>C. H. Hwang. 1999. Incompletely and Imprecisely Speaking: Using Dynamic Ontologies for Representing and Retrieving Information. In Proceedings of the 6th International Workshop on Knowledge Representation Meets Databases, pages 14–20, Linkoping, Sweden. CEUR-WS.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B King</author>
</authors>
<title>Step-wise Clustering Procedures.</title>
<date>1967</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>69</volume>
<pages>101</pages>
<contexts>
<context position="2223" citStr="King, 1967" startWordPosition="354" endWordPosition="355">(b). By observing Figure 1 (a), we can express ISA statements, such as Internet IS-A Computer Network etc. However, the same does not apply to the 82 Figure 1: A labelled and an unlabelled concept taxonomy taxonomy in Figure 1 (b), since this taxonomy is not fully labelled. Despite this, its hierarchical organisation clearly shows that the concepts are divided into groups, which are further subdivided into subgroups and so forth, until we reach a level where each concept belongs to its own group. Unlabelled taxonomies are typically produced by agglomerative hierarchical clustering algorithms (King, 1967; Sneath and Sokal, 1973). The knowledge encoded in taxonomies can be utilised in a range of NLP applications. For instance, taxonomies can be used in information retrieval to expand a user query with semantically related words or to enhance document representation by abstracting from plain words and adding conceptual information (Cimiano, 2006). WordNet’s (Fellbaum, 1998) taxonomic relations have also been used in Word Sense Disambiguation (WSD) (Navigli and Velardi, 2004b). In named entity recognition, methods relying on gazetteers could make Human Language Technologies: The 2010 Annual Conf</context>
<context position="3671" citStr="King, 1967" startWordPosition="577" endWordPosition="578">have also benefited (Moldovan and Novischi, 2002). Despite the wide uses of taxonomies, the majority of methods disregard or do not deal effectively with word polysemy, in effect, developing taxonomies that conflate the senses of words (see Section 2). In this work, we show that Word Sense Induction (WSI) can be effectively employed to address this limitation of existing methods. We present a novel method that employs WSI to generate the different senses of a set of target words from an unlabelled corpus and then produces a taxonomy of senses using Hierarchical Agglomerative Clustering (HAC) (King, 1967; Sneath and Sokal, 1973). We evaluate our method on two WordNetderived sub-taxonomies and show that our method leads to the development of concept hierarchies that capture a higher number of correct taxonomic relations in comparison to those generated by current distributional similarity approaches. 2 Related work Initial research on taxonomy learning focused on identifying in a given text lexico-syntactic patterns that suggest hyponymy relations (Hearst, 1992). For instance, the pattern NP0 such as NP1,... ,NP,,, suggests that NP0 is a hypernym of NPi. For example, given the phrase Fruits, s</context>
</contexts>
<marker>King, 1967</marker>
<rawString>B. King. 1967. Step-wise Clustering Procedures. Journal of the American Statistical Association, 69:86– 101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I P Klapaftis</author>
<author>S Manandhar</author>
</authors>
<title>Word Sense Induction Using Graphs of Collocations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 18th ECAI Conference,</booktitle>
<pages>298--302</pages>
<publisher>IOS Press.</publisher>
<location>Patras, Greece.</location>
<contexts>
<context position="10274" citStr="Klapaftis &amp; Manandhar (2008)" startWordPosition="1642" endWordPosition="1645">a set of senses, 5, where each sw E 5 denotes the i-th sense of word w E W. This set Figure 2: WSI for network &amp; LAN of senses is the input to hierarchical clustering that produces a hierarchy of senses (Section 3.2). 3.1 Word sense induction WSI is the task of identifying the senses of a target word in a given text. Recent WSI methods were evaluated under the framework of SemEval2007 WSI task (SWSI) (Agirre and Soroa, 2007). The evaluation framework defines two types of assessment, i.e. evaluation in: (1) a clustering and (2) a WSD setting. Based on this evaluation, we selected the method of Klapaftis &amp; Manandhar (2008) (henceforth referred to as KM) that achieves high Fscore in both evaluation schemes as compared to the systems participating in SWSI. We briefly describe KM mentioning its parameters used in our evaluation (Section 4). Figures 2 (a) and 2 (b) describe the different steps for inducing the senses of the target words network and LAN. Corpus preprocessing: The input to KM is a base corpus bc, in which the target word w appears in each paragraph. In Figure 2 (a), the base corpus consists of the paragraphs A, B, C and D. The aim of this stage is to capture nouns contextually 84 related to w. Initia</context>
</contexts>
<marker>Klapaftis, Manandhar, 2008</marker>
<rawString>I. P. Klapaftis and S. Manandhar. 2008. Word Sense Induction Using Graphs of Collocations. In Proceedings of the 18th ECAI Conference, pages 298–302, Patras, Greece. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Maedche</author>
<author>S Staab</author>
</authors>
<title>Measuring Similarity between Ontologies.</title>
<date>2002</date>
<booktitle>In Proceedings of the European Conference on Knowledge Acquisition and Management (EKAW),</booktitle>
<pages>251--263</pages>
<publisher>SpringerVerlag.</publisher>
<contexts>
<context position="18056" citStr="Maedche and Staab, 2002" startWordPosition="2986" endWordPosition="2990"> assigned to swj , i.e. label(swj ) = argmaxi JC(fwj , qwi ), where JC is the Jaccard similarity coefficient. In the example of Figure 2 (a), the computer network sense would be mapped to the fifth WordNet sense of network, since there is a significant overlap between the paragraphs tagged by the induced and that WordNet sense. 4.2 Evaluation measures For the purposes of this section we present one gold standard taxonomy (Figure 1 (a)) and a second derived from our method (Figure 1 (b)). The comparison of these taxonomies is based on the semantic cotopy of a node, which has also been used in (Maedche and Staab, 2002; Cimiano et al., 2005). In particular, the semantic cotopy of a node is defined as the set of all its super- and subnodes excluding the root and including that node. For example, the semantic cotopy of computer network in Figure 1 (a) is {computer network, internet, LAN}. There are two issues, which make the evaluation difficult. The first one is that HAC produces a taxonomy in which all internal nodes are unlabelled, as opposed to the gold standard taxonomy. In Figure 1 (b), we have manually labelled internal nodes with their IDs for clarity. For example, the semantic cotopy of the node New </context>
</contexts>
<marker>Maedche, Staab, 2002</marker>
<rawString>A. Maedche and S. Staab. 2002. Measuring Similarity between Ontologies. In Proceedings of the European Conference on Knowledge Acquisition and Management (EKAW), pages 251–263, London,UK. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>A Novischi</author>
</authors>
<title>Lexical Chains for Question Answering.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th Coling Conference,</booktitle>
<pages>1--7</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="3110" citStr="Moldovan and Novischi, 2002" startWordPosition="484" endWordPosition="487">tation by abstracting from plain words and adding conceptual information (Cimiano, 2006). WordNet’s (Fellbaum, 1998) taxonomic relations have also been used in Word Sense Disambiguation (WSD) (Navigli and Velardi, 2004b). In named entity recognition, methods relying on gazetteers could make Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 82–90, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics use of automatically acquired taxonomies (Cimiano, 2006), while question answering systems have also benefited (Moldovan and Novischi, 2002). Despite the wide uses of taxonomies, the majority of methods disregard or do not deal effectively with word polysemy, in effect, developing taxonomies that conflate the senses of words (see Section 2). In this work, we show that Word Sense Induction (WSI) can be effectively employed to address this limitation of existing methods. We present a novel method that employs WSI to generate the different senses of a set of target words from an unlabelled corpus and then produces a taxonomy of senses using Hierarchical Agglomerative Clustering (HAC) (King, 1967; Sneath and Sokal, 1973). We evaluate </context>
</contexts>
<marker>Moldovan, Novischi, 2002</marker>
<rawString>D. Moldovan and A. Novischi. 2002. Lexical Chains for Question Answering. In Proceedings of the 19th Coling Conference, pages 1–7, Taipei, Taiwan.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<booktitle>2004a. Learning Domain Ontologies from Document Warehouses and Dedicated web Sites. Computational Linguistics,</booktitle>
<volume>30</volume>
<issue>2</issue>
<marker>Navigli, Velardi, </marker>
<rawString>R. Navigli and P. Velardi. 2004a. Learning Domain Ontologies from Document Warehouses and Dedicated web Sites. Computational Linguistics, 30(2):151–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>Structural Semantic Interconnection: a Knowledge-based Approach to Word Sense Disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of Senseval3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>179--182</pages>
<location>Barcelona,</location>
<contexts>
<context position="2700" citStr="Navigli and Velardi, 2004" startWordPosition="427" endWordPosition="431"> each concept belongs to its own group. Unlabelled taxonomies are typically produced by agglomerative hierarchical clustering algorithms (King, 1967; Sneath and Sokal, 1973). The knowledge encoded in taxonomies can be utilised in a range of NLP applications. For instance, taxonomies can be used in information retrieval to expand a user query with semantically related words or to enhance document representation by abstracting from plain words and adding conceptual information (Cimiano, 2006). WordNet’s (Fellbaum, 1998) taxonomic relations have also been used in Word Sense Disambiguation (WSD) (Navigli and Velardi, 2004b). In named entity recognition, methods relying on gazetteers could make Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 82–90, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics use of automatically acquired taxonomies (Cimiano, 2006), while question answering systems have also benefited (Moldovan and Novischi, 2002). Despite the wide uses of taxonomies, the majority of methods disregard or do not deal effectively with word polysemy, in effect, developing taxonomies that conflate the senses of words (se</context>
<context position="9015" citStr="Navigli and Velardi, 2004" startWordPosition="1424" endWordPosition="1427">taxonomy learning exploits the head/modifier relationships to create taxonomic relations (Buitelaar et al., 2004; Hwang, 1999; S´anchez and Moreno, 2005). These relations are used to create: (1) a class (concept) for each head, and (2) subclasses by adding nominal or adjectival modifiers. For example, credit card IS-A card. The corresponding hyponymy relations are learned at the lexical level disregarding word polysemy. Some of these approaches identified the problem of polysemy and applied sense disambiguation with respect to WordNet in order to capture the different senses of a target term (Navigli and Velardi, 2004b; Navigli and Velardi, 2004a). Specifically, the taxonomy built by exploiting head/modifiers relations was modified according to WordNet’s hyponymy relations between senses of disambiguated terms. One important deficiency of using sense disambiguation is that dictionaries miss many domain-specific senses. Additionally, the fixed-list of senses paradigm prohibits learning word senses according to their use in context. The use of sense induction we propose in this paper aims to overcome these limitations. 3 Method Given a set of words W, a WSI method is applied to each wz E W (Section 3.1). The</context>
</contexts>
<marker>Navigli, Velardi, 2004</marker>
<rawString>R. Navigli and P. Velardi. 2004b. Structural Semantic Interconnection: a Knowledge-based Approach to Word Sense Disambiguation. In Proceedings of Senseval3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 179– 182, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Reinberger</author>
<author>P Spyns</author>
</authors>
<title>Discovering Knowledge in Texts for the Learning of Dogmainspired Ontologies.</title>
<date>2004</date>
<booktitle>In Proceedings of the ECAI Workshop on Ontology Learning and Population,</booktitle>
<pages>pages</pages>
<location>Valencia,</location>
<contexts>
<context position="5595" citStr="Reinberger and Spyns, 2004" startWordPosition="885" endWordPosition="888"> separate the senses of a word. However this would be true if and only if the glosses of the dictionaries were sense-annotated, which is not the case for the majority of electronic dictionaries (Cimiano, 2006). Another limitation is that taxonomies are built according to the sense distinctions present in dictionaries and not according to the actual use of words in the corpus. The majority of taxonomy learning approaches are based on the distributional hypothesis (Harris, 1968). Typically, distributional similarity methods (Cimiano et al., 2004; Cimiano et al., 2005; Faure and N´edellec, 1998; Reinberger and Spyns, 2004; Caraballo, 1999) utilise syntactic dependencies such as subject/verb, object/verb relations, conjunctive and appositive constructions and others. These dependencies are used to extract the features that serve as the dimensions of the vector space. Each target noun is then represented as a vector of extracted features where the frequency of co-occurrence of the target noun with each feature is used to calculate the weight of that feature. The constructed vectors are the input to hierarchical clustering or formal concept analysis (Ganter and Wille, 1999) to produce a taxonomy. These approaches</context>
<context position="7816" citStr="Reinberger and Spyns, 2004" startWordPosition="1232" endWordPosition="1235">o unreliable similarity estimates. For example, merging the features of the different senses of house could provide a lower similarity with its monosemous hyponym beach house, since only the first sense of house is related to beach 1WordNet: A dwelling that serves as living quarters ... 83 house. This problem might lead both to inclusion of incorrect or loss of correct taxonomic relations. In our work, we aim to overcome these drawbacks by identifying the different senses with which target words appear in text and then building a hierarchy of the identified senses. Soft clustering approaches (Reinberger and Spyns, 2004; Reinberger et al., 2003) have also been applied to taxonomy learning to deal with polysemy. These methods associate each verb with a vector of features, where each feature is a noun appearing as a subject or object of that verb. That way a noun can appear in different vectors, hence in different clusters during hierarchical clustering as a result of its polysemy. However, the underlying assumption is that a verb is monosemous with respect to its associated vector of nouns. This assumption is not always valid and can cause the problems mentioned above. Other work in taxonomy learning exploits</context>
</contexts>
<marker>Reinberger, Spyns, 2004</marker>
<rawString>M.L. Reinberger and P. Spyns. 2004. Discovering Knowledge in Texts for the Learning of Dogmainspired Ontologies. In Proceedings of the ECAI Workshop on Ontology Learning and Population, pages 19–24, Valencia, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Reinberger</author>
<author>P Spyns</author>
<author>W Daelemans</author>
<author>R Meersman</author>
</authors>
<title>Mining for Lexons: Applying Unsupervised Learning Methods to create ontology bases. In CoopIS/DOA/ODBASE,</title>
<date>2003</date>
<pages>803--819</pages>
<contexts>
<context position="7842" citStr="Reinberger et al., 2003" startWordPosition="1236" endWordPosition="1239">mates. For example, merging the features of the different senses of house could provide a lower similarity with its monosemous hyponym beach house, since only the first sense of house is related to beach 1WordNet: A dwelling that serves as living quarters ... 83 house. This problem might lead both to inclusion of incorrect or loss of correct taxonomic relations. In our work, we aim to overcome these drawbacks by identifying the different senses with which target words appear in text and then building a hierarchy of the identified senses. Soft clustering approaches (Reinberger and Spyns, 2004; Reinberger et al., 2003) have also been applied to taxonomy learning to deal with polysemy. These methods associate each verb with a vector of features, where each feature is a noun appearing as a subject or object of that verb. That way a noun can appear in different vectors, hence in different clusters during hierarchical clustering as a result of its polysemy. However, the underlying assumption is that a verb is monosemous with respect to its associated vector of nouns. This assumption is not always valid and can cause the problems mentioned above. Other work in taxonomy learning exploits the head/modifier relatio</context>
</contexts>
<marker>Reinberger, Spyns, Daelemans, Meersman, 2003</marker>
<rawString>M. L. Reinberger, P. Spyns, W. Daelemans, and R. Meersman. 2003. Mining for Lexons: Applying Unsupervised Learning Methods to create ontology bases. In CoopIS/DOA/ODBASE, pages 803–819.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S´anchez</author>
<author>A Moreno</author>
</authors>
<title>Web-scale Taxonomy Learning.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Learning and Extending Ontologies by using Machine Learning methods,</booktitle>
<pages>53--60</pages>
<location>Bonn, Germany.</location>
<marker>S´anchez, Moreno, 2005</marker>
<rawString>D. S´anchez and A. Moreno. 2005. Web-scale Taxonomy Learning. In Proceedings of the Workshop on Learning and Extending Ontologies by using Machine Learning methods, pages 53–60, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P H A Sneath</author>
<author>R R Sokal</author>
</authors>
<title>Numerical Taxonomy, The Principles and Practice of Numerical Classification.</title>
<date>1973</date>
<location>San Francisco, USA.</location>
<contexts>
<context position="2248" citStr="Sneath and Sokal, 1973" startWordPosition="356" endWordPosition="359">rving Figure 1 (a), we can express ISA statements, such as Internet IS-A Computer Network etc. However, the same does not apply to the 82 Figure 1: A labelled and an unlabelled concept taxonomy taxonomy in Figure 1 (b), since this taxonomy is not fully labelled. Despite this, its hierarchical organisation clearly shows that the concepts are divided into groups, which are further subdivided into subgroups and so forth, until we reach a level where each concept belongs to its own group. Unlabelled taxonomies are typically produced by agglomerative hierarchical clustering algorithms (King, 1967; Sneath and Sokal, 1973). The knowledge encoded in taxonomies can be utilised in a range of NLP applications. For instance, taxonomies can be used in information retrieval to expand a user query with semantically related words or to enhance document representation by abstracting from plain words and adding conceptual information (Cimiano, 2006). WordNet’s (Fellbaum, 1998) taxonomic relations have also been used in Word Sense Disambiguation (WSD) (Navigli and Velardi, 2004b). In named entity recognition, methods relying on gazetteers could make Human Language Technologies: The 2010 Annual Conference of the North Ameri</context>
<context position="3696" citStr="Sneath and Sokal, 1973" startWordPosition="579" endWordPosition="582">nefited (Moldovan and Novischi, 2002). Despite the wide uses of taxonomies, the majority of methods disregard or do not deal effectively with word polysemy, in effect, developing taxonomies that conflate the senses of words (see Section 2). In this work, we show that Word Sense Induction (WSI) can be effectively employed to address this limitation of existing methods. We present a novel method that employs WSI to generate the different senses of a set of target words from an unlabelled corpus and then produces a taxonomy of senses using Hierarchical Agglomerative Clustering (HAC) (King, 1967; Sneath and Sokal, 1973). We evaluate our method on two WordNetderived sub-taxonomies and show that our method leads to the development of concept hierarchies that capture a higher number of correct taxonomic relations in comparison to those generated by current distributional similarity approaches. 2 Related work Initial research on taxonomy learning focused on identifying in a given text lexico-syntactic patterns that suggest hyponymy relations (Hearst, 1992). For instance, the pattern NP0 such as NP1,... ,NP,,, suggests that NP0 is a hypernym of NPi. For example, given the phrase Fruits, such as oranges, apples,..</context>
</contexts>
<marker>Sneath, Sokal, 1973</marker>
<rawString>P. H. A. Sneath and R. R. Sokal. 1973. Numerical Taxonomy, The Principles and Practice of Numerical Classification. W. H. Freeman, San Francisco, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>