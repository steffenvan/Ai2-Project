<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000653">
<title confidence="0.978748">
Better Automatic Treebank Conversion Using A Feature-Based Approach
</title>
<author confidence="0.849312">
Muhua Zhu Jingbo Zhu Minghan Hu
</author>
<affiliation confidence="0.8267875">
Natural Language Processing Lab.
Northeastern University, China
</affiliation>
<email confidence="0.9765955">
zhumuhua@gmail.com
zhujingbo@mail.neu.edu.cn huminghan@ise.neu.edu.cn
</email>
<sectionHeader confidence="0.993636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997365">
For the task of automatic treebank conversion,
this paper presents a feature-based approach
which encodes bracketing structures in a tree-
bank into features to guide the conversion of
this treebank to a different standard. Exper-
iments on two Chinese treebanks show that
our approach improves conversion accuracy
by 1.31% over a strong baseline.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99984593220339">
In the field of syntactic parsing, research efforts have
been put onto the task of automatic conversion of
a treebank (source treebank) to fit a different stan-
dard which is exhibited by another treebank (tar-
get treebank). Treebank conversion is desirable pri-
marily because source-style and target-style annota-
tions exist for non-overlapping text samples so that a
larger target-style treebank can be obtained through
such conversion. Hereafter, source and target tree-
banks are named as heterogenous treebanks due to
their different annotation standards. In this paper,
we focus on the scenario of conversion between
phrase-structure heterogeneous treebanks (Wang et
al., 1994; Zhu and Zhu, 2010).
Due to the availability of annotation in a source
treebank, it is natural to use such annotation to
guide treebank conversion. The motivating idea is
illustrated in Fig. 1 which depicts a sentence anno-
tated with standards of Tsinghua Chinese Treebank
(TCT) (Zhou, 1996) and Penn Chinese Treebank
(CTB) (Xue et al., 2002), respectively. Suppose
that the conversion is in the direction from the TCT-
style parse (left side) to the CTB-style parse (right
side). The constituents vp:[4/will 投*/surrender],
dj:[敌人/enemy 4/will 投*/surrender], and np:[情
报/intelligence 专家/experts] in the TCT-style parse
strongly suggest a resulting CTB-style parse also
bracket the words as constituents. Zhu and
Zhu (2010) show the effectiveness of using brack-
eting structures in a source treebank (source-side
bracketing structures in short) as parsing constraints
during the decoding phase of a target treebank-based
parser.
However, using source-side bracketing structures
as parsing constraints is problematic in some cases.
As illustrated in the shadow part of Fig. 1, the TCT-
style parse takes “认h/deems” as the right bound-
ary of a constituent while in the CTB-style parse,
“认h” is the left boundary of a constituent. Ac-
cording to the criteria used in Zhu and Zhu (2010),
any CTB-style constituents with “认h” being the
left boundary are thought to be inconsistent with the
bracketing structure of the TCT-style parse and will
be pruned. However, if we prune such “inconsistent”
constituents, the correct conversion result (right side
of Fig. 1) has no chance to be generated.
The problem comes from binary distinctions used
in the approach of Zhu and Zhu (2010). With bi-
nary distinctions, constituents generated by a target
treebank-based parser are judged to be either con-
sistent or inconsistent with source-side bracketing
structures. That approach prunes inconsistent con-
stituents which instead might be correct conversion
results1. In this paper, we insist on using source-
side bracketing structures as guiding information.
Meanwhile, we aim to avoid using binary distinc-
tions. To achieve such a goal, we propose to use a
feature-based approach to treebank conversion and
to encode source-side bracketing structures as a set
</bodyText>
<footnote confidence="0.9922005">
1To show how severe this problem might be, Section 3.1
presents statistics on inconsistence between TCT and CTB.
</footnote>
<page confidence="0.9351">
715
</page>
<note confidence="0.726738">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 715–719,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<table confidence="0.331472">
敌人 将 投*
情报 专家 认为 , 敌人 将 投*
qingbao zhuanjia renwei ,diren jiang touxiang
intelligence experts deem ,enemy will surrender
</table>
<figureCaption confidence="0.996209">
Figure 1: An example sentence with TCT-style annotation (left) and CTB-style annotation (right).
</figureCaption>
<figure confidence="0.997784971428571">
AD
VV
NN
将 投*
专家
情报
zj
IP
VP
PU
,
VP
dj
,
dj
NP
np
v
,
n
vp
NN
NN
VV
IP
n
n
认为
敌人
d
v
情报
专家
认为
NP
</figure>
<bodyText confidence="0.966017375">
of features. The advantage is that inconsistent con-
stituents can be scored with a function based on the
features rather than ruled out as impossible.
To test the efficacy of our approach, we conduct
experiments on conversion from TCT to CTB. The
results show that our approach achieves a 1.31% ab-
solute improvement in conversion accuracy over the
approach used in Zhu and Zhu (2010).
</bodyText>
<sectionHeader confidence="0.985568" genericHeader="method">
2 Our Approach
</sectionHeader>
<subsectionHeader confidence="0.996736">
2.1 Generic System Architecture
</subsectionHeader>
<bodyText confidence="0.998117806451613">
To conduct treebank conversion, our approach, over-
all speaking, proceeds in the following steps.
Step 1: Build a parser (named source parser) on a
source treebank, and use it to parse sentences
in the training data of a target treebank.
Step 2: Build a parser on pairs of golden target-
style and auto-assigned (in Step 1) source-style
parses in the training data of the target tree-
bank. Such a parser is named heterogeneous
parser since it incorporates information derived
from both source and target treebanks, which
follow different annotation standards.
Step 3: In the testing phase, the heterogeneous
parser takes golden source-style parses as input
and conducts treebank conversion. This will be
explained in detail in Section 2.2.
To instantiate the generic framework described
above, we need to decide the following three factors:
(1) a parsing model for building a source parser, (2)
a parsing model for building a heterogeneous parser,
and (3) features for building a heterogeneous parser.
In principle, any off-the-shelf parsers can be used
to build a source parser, so we focus only on the
latter two factors. To build a heterogeneous parser,
we use feature-based parsing algorithms in order to
easily incorporate features that encode source-side
bracketing structures. Theoretically, any feature-
based approaches are applicable, such as Finkel et
al. (2008) and Tsuruoka et al. (2009). In this pa-
per, we use the shift-reduce parsing algorithm for its
simplicity and competitive performance.
</bodyText>
<subsectionHeader confidence="0.99776">
2.2 Shift-Reduce-Based Heterogeneous Parser
</subsectionHeader>
<bodyText confidence="0.999685411764706">
The heterogeneous parser used in this paper is based
on the shift-reduce parsing algorithm described in
Sagae and Lavie (2006a) and Wang et al. (2006).
Shift-reduce parsing is a state transition process,
where a state is defined to be a tuple (S, Q). Here, S
is a stack containing partial parses, and Q is a queue
containing word-POS pairs to be processed. At each
state transition, a shift-reduce parser either shifts the
top item of Q onto S, or reduces the top one (or two)
items on S.
A shift-reduce-based heterogeneous parser pro-
ceeds similarly as the standard shift-reduce parsing
algorithm. In the training phase, each target-style
parse tree in the training data is transformed into
a binary tree (Charniak et al., 1998) and then de-
composed into a (golden) action-state sequence. A
classifier can be trained on the set of action-states,
</bodyText>
<page confidence="0.992023">
716
</page>
<bodyText confidence="0.999893818181818">
where each state is represented as a feature vector.
In the testing phase, the trained classifier is used
to choose actions for state transition. Moreover,
beam search strategies can be used to expand the
search space of a shift-reduce-based heterogeneous
parser (Sagae and Lavie, 2006a). To incorporate in-
formation on source-side bracketing structures, in
both training and testing phases, feature vectors rep-
resenting states (S, Q) are augmented with features
that bridge the current state and the corresponding
source-style parse.
</bodyText>
<subsectionHeader confidence="0.921754">
2.3 Features
</subsectionHeader>
<bodyText confidence="0.999957235294118">
This section describes the feature functions used to
build a heterogeneous parser on the training data
of a target treebank. The features can be divided
into two groups. The first group of features are
derived solely from target-style parse trees so they
are referred to as target side features. This group
of features are completely identical to those used in
Sagae and Lavie (2006a).
In addition, we have features extracted jointly
from target-style and source-style parse trees. These
features are generated by consulting a source-style
parse (referred to as ts) while we decompose a
target-style parse into an action-state sequence.
Here, si denote the ith item from the top of the
stack, and qi denote the ith item from the front
end of the queue. We refer to these features as
heterogeneous features.
</bodyText>
<sectionHeader confidence="0.33782" genericHeader="method">
Constituent features Fc(si, ts)
</sectionHeader>
<bodyText confidence="0.999358285714286">
This feature schema covers three feature functions:
Fc(s1, ts), Fc(s2, ts), and Fc(s1 o s2, ts), which
decide whether partial parses on stack S correspond
to a constituent in the source-style parse ts. That is,
Fc(si, ts)=+ if si has a bracketing match (ignoring
grammar labels) with any constituent in ts. s1os2
represents a concatenation of spans of s1 and s2.
</bodyText>
<sectionHeader confidence="0.641343" genericHeader="method">
Relation feature Fr(Ns(s1), Ns(s2))
</sectionHeader>
<bodyText confidence="0.939501666666667">
We first position the lowest node Ns(si) in ts,
which dominates the span of si. Then a feature
function Fr(Ns(s1), Ns(s2)) is defined to indicate
the relationship of Ns(s1) and Ns(s2). If Ns(s1)
is identical to or a sibling of Ns(s2), we say
Fr(Ns(s1),Ns(s2)) =+.
</bodyText>
<table confidence="0.995689285714286">
Features Bridging Source and Target Parses
Fc(s1, ts)=−
Fc(s2,ts)=+
Fc(s1os2,ts)=+
Fr(Ns(s1),Ns(s2))=−
Ff(RF(s1), q1)=−
Fp(RF(s1),q1)= “v T dj T zj 1,”
</table>
<tableCaption confidence="0.9987955">
Table 1: An example of new features. Suppose we are
considering the sentence depicted in Fig. 1.
</tableCaption>
<sectionHeader confidence="0.292661" genericHeader="method">
Frontier-words feature Ff(RF(s1), q1)
</sectionHeader>
<bodyText confidence="0.971556304347826">
A feature function which decides whether the right
frontier word of s1 and q1 are in the same base
phrase in ts. Here, a base phrase is defined to be
any phrase which dominates no other phrases.
Path feature Fp(RF(s1), q1)
Syntactic path features are widely used in the litera-
ture of semantic role labeling (Gildea and Jurafsky,
2002) to encode information of both structures and
grammar labels. We define a string-valued feature
function Fp(RF(s1), q1) which connects the right
frontier word of s1 to q1 in ts.
To better understand the above feature func-
tions, we re-examine the example depicted in
Fig. 1. Suppose that we use a shift-reduce-based
heterogeneous parser to convert the TCT-style parse
to the CTB-style parse and that stack S currently
contains two partial parses: s2:[NP (NN 情报) (NN
专家)] and s1: (VV 认为). In such a state, we can
see that spans of both s2 and s1 os2 correspond to
constituents in ts but that of s1 does not. Moreover,
Ns(s1) is dj and Ns(s2) is np, so Ns(s1) and
Ns(s2) are neither identical nor sisters in ts. The
values of these features are collected in Table 1.
</bodyText>
<sectionHeader confidence="0.999922" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999924">
3.1 Data Preparation and Performance Metric
</subsectionHeader>
<bodyText confidence="0.970804">
In the experiments, we use two heterogeneous tree-
banks: CTB 5.1 and the TCT corpus released by
the CIPS-SIGHAN-2010 syntactic parsing competi-
tion2. We actually only use the training data of these
two corpora, that is, articles 001-270 and 400-1151
(18,100 sentences, 493,869 words) of CTB 5.1 and
</bodyText>
<footnote confidence="0.99214">
2http://www.cipsc.org.cn/clp2010/task2 en.htm
</footnote>
<page confidence="0.993956">
717
</page>
<bodyText confidence="0.999736076923077">
the training data (17,529 sentences, 481,061 words)
of TCT.
To evaluate conversion accuracy, we use the
same test set (named Sample-TCT) as in Zhu and
Zhu (2010), which is a set of 150 sentences with
manually assigned CTB-style and TCT-style parse
trees. In Sample-TCT, 6.19% (215/3473) CTB-
style constituents are inconsistent with respect to the
TCT standard and 8.87% (231/2602) TCT-style con-
stituents are inconsistent with respect to the CTB
standard.
For all experiments, bracketing F1 is used as the
performance metric, provided by EVALB 3.
</bodyText>
<subsectionHeader confidence="0.999125">
3.2 Implementation Issues
</subsectionHeader>
<bodyText confidence="0.999990157894737">
To implement a heterogeneous parser, we first build
a Berkeley parser (Petrov et al., 2006) on the TCT
training data and then use it to assign TCT-style
parses to sentences in the CTB training data. On
the “updated” CTB training data, we build two shift-
reduce-based heterogeneous parsers by using max-
imum entropy classification model, without/with
beam search. Hereafter, the two heterogeneous
parsers are referred to as Basic-SR and Beam-SR, re-
spectively.
In the testing phase, Basic-SR and Beam-SR con-
vert TCT-style parse trees in Sample-TCT to the
CTB standard. The conversion results are evalu-
ated against corresponding CTB-style parse trees in
Sample-TCT. Before conducting treebank conver-
sion, we apply the POS adaptation method proposed
in Jiang et al. (2009) to convert TCT-style POS tags
in the input to the CTB standard. The POS conver-
sion accuracy is 96.2% on Sample-TCT.
</bodyText>
<subsectionHeader confidence="0.930136">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.9999789">
Table 2 shows the results achieved by Basic-SR and
Beam-SR with heterogeneous features being added
incrementally. Here, baseline represents the systems
which use only target side features. From the table
we can see that heterogeneous features improve con-
version accuracy significantly. Specifically, adding
the constituent (Fd features to Basic-SR (Beam-
SR) achieves a 2.79% (3%) improvement, adding
the relation (Fr) and frontier-word (Ff) features
yields a 0.79% (0.98%) improvement, and adding
</bodyText>
<footnote confidence="0.929142">
3http://nlp.cs.nyu.edu/evalb
</footnote>
<table confidence="0.946272888888889">
&lt;= 40 words Unlimited
83.34 80.33
85.89 83.12
85.47 83.91
86.01 84.05
84.40 81.27
86.30 84.27
87.00 85.25
87.27 85.38
</table>
<tableCaption confidence="0.9917495">
Table 2: Adding new features to baselines improve tree-
bank conversion accuracy significantly on Sample-TCT.
</tableCaption>
<bodyText confidence="0.999893153846154">
the path (Fp) feature achieves a 0.14% (0.13%) im-
provement. The path feature is not so effective as
expected, although it manages to achieve improve-
ments. One possible reason lies on the data sparse-
ness problem incurred by this feature.
Since we use the same training and testing data
as in Zhu and Zhu (2010), we can compare our
approach directly with the informed decoding ap-
proach used in that work. We find that Basic-SR
achieves very close conversion results (84.05% vs.
84.07%) and Beam-SR even outperforms the in-
formed decoding approach (85.38% vs. 84.07%)
with a 1.31% absolute improvement.
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.99994485">
For phrase-structure treebank conversion, Wang et
al. (1994) suggest to use source-side bracketing
structures to select conversion results from k-best
lists. The approach is quite generic in the sense that
it can be used for conversion between treebanks of
different grammar formalisms, such as from a de-
pendency treebank to a constituency treebank (Niu
et al., 2009). However, it suffers from limited
variations in k-best lists (Huang, 2008). Zhu and
Zhu (2010) propose to incorporate bracketing struc-
tures as parsing constraints in the decoding phase of
a CKY-style parser. Their approach shows signifi-
cant improvements over Wang et al. (1994). How-
ever, it suffers from binary distinctions (consistent
or inconsistent), as discussed in Section 1.
The approach in this paper is reminiscent of
co-training (Blum and Mitchell, 1998; Sagae and
Lavie, 2006b) and up-training (Petrov et al., 2010).
Moreover, it coincides with the stacking method
used for dependency parser combination (Martins
</bodyText>
<figure confidence="0.841094444444444">
System Features
Basic-SR baseline
Beam-SR +Fc
+FT, +Ff
+Fp
baseline
+Fc
+FT,+Ff
+Fp
</figure>
<page confidence="0.98397">
718
</page>
<bodyText confidence="0.989688481481482">
et al., 2008; Nivre and McDonald, 2008), the Liang Huang. 2008. Forest Reranking: Discriminative
Pred method for domain adaptation (Daum´e III and Parsing with Non-local Features. In Proceedings of
Marcu, 2006), and the method for annotation adap- ACL, pages 824-831.
tation of word segmentation and POS tagging (Jiang Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
et al., 2009). As one of the most related works, tomatic Adaptation of Annotation Standards: Chinese
Jiang and Liu (2009) present a similar approach to Word Segmentation and POS Tagging - A Case Study.
conversion between dependency treebanks. In con- In Proceedings ofACL 2009, pages 522-530.
trast to Jiang and Liu (2009), the task studied in this Wenbin Jiang and Qun Liu. 2009. Automatic Adapta-
paper, phrase-structure treebank conversion, is rel- tion of Annotation Standards for Dependency Parsing
atively complicated and more efforts should be put – Using Projected Treebank As Source Corpus. In
into feature engineering. Proceedings ofIWPT 2009, pages 25-28.
5 Conclusion Andr´e F. T. Martins, Dipanjan Das, Noah A. Smith, and
To avoid binary distinctions used in previous ap- Eric P. Xing. 2008. Stack Dependency Parsers. In
proaches to automatic treebank conversion, we pro- Proceedings of EMNLP 2008, pages 157-166.
posed in this paper a feature-based approach. Exper- Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Ex-
iments on two Chinese treebanks showed that our ploiting Heterogeneous Treebanks for Parsing. In Pro-
approach outperformed the baseline system (Zhu ceedings ofACL 2009, pages 46-54.
and Zhu, 2010) by 1.31%. Joakim Nivre and Ryan McDonald. 2008. Integrat-
Acknowledgments ing Graph-Based and Transition-Based Dependency
We thank Kenji Sagae for helpful discussions on the Parsers. In Proceedings ofACL 2008, pages 950-958.
implementation of shift-reduce parser and the three Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
anonymous reviewers for comments. This work was Klein. 2006. Learning Accurate, Compact, and In-
supported in part by the National Science Founda- terpretable Tree Annotation. In Proceedings of ACL
tion of China (60873091; 61073140), Specialized 2006, pages 433-440.
Research Fund for the Doctoral Program of Higher Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Education (20100042110031), the Fundamental Re- Hiyan Alshawi. 2010. Uptraining for Accurate Deter-
search Funds for the Central Universities and Nat- ministic Question Parsing. In Proceedings of EMNLP
</bodyText>
<reference confidence="0.986968555555556">
ural Science Foundation of Liaoning Province of 2010, pages 705-713.
China. Kenji Sagae and Alon Lavie. 2006. A Best-First Prob-
References abilistic Shift-Reduce Parser. In Proceedings ofACL-
Avrim Blum and Tom Mitchell. 1998. Combining La- COLING 2006, pages 691-698.
beled and Unlabeled Data with Co-Training. In Pro- Kenji Sagae and Alon Lavie. 2006. Parser Combination
ceedings of COLT 1998. by Reparsing. In Proceedings of NAACL 2006, pages
Eugene Charniak, Sharon Goldwater, and Mark Johnson. 129-132.
1998. Edge-Based Best-First Chart Parsing. In Pro- Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Anani-
ceedings of the Six Workshop on Very Large Corpora, adou. 2009. Fast Full Parsing by Linear-Chain Condi-
pages 127-133. tional Random Fields. In Proceedings of EACL 2009,
Hal Daum´e III and Daniel Marcu. 2006. Domain Adap- pages 790-798.
tation for Statistical Classifiers. Journal of Artifical Jong-Nae Wang, Jing-Shin Chang, and Keh-Yih Su.
Intelligence Research, 26:101-166. 1994. An Automatic Treebank Conversion Algorithm
Jenny Rose Finkel, Alex Kleeman, and Christopher D. for Corpus Sharing. In Proceedings of ACL 1994,
Manning. 2008. Efficient, Feature-Based Conditional pages 248-254.
Random Fileds Parsing. In Proceedings ofACL 2008, Mengqiu Wang, Kenji Sagae, and Teruk Mitamura. 2006.
pages 959-967. A Fast, Deterministic Parser for Chinese. In Proceed-
Daniel Gildea and Daniel Jurafsky. 2002. Automatic La- ings ofACL-COLING 2006, pages 425-432.
beling for Semantic Roles. Computational Linguis- Nianwen Xue, Fu dong Chiou, and Martha Palmer. 2002.
tics, 28(3):245-288. Building a Large-Scale Annotated Chinese Corpus. In
719 Proceedings of COLING 2002, pages 1-8.
Qiang Zhou. 1996. Phrase Bracketing and Annotation on
Chinese Language Corpus (in Chinese). Ph.D. thesis,
Peking University.
Muhua Zhu, and Jingbo Zhu. 2010. Automatic Treebank
Conversion via Informed Decoding. In Porceedings of
COLING 2010, pages 1541-1549.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.285898">
<title confidence="0.998026">Better Automatic Treebank Conversion Using A Feature-Based Approach</title>
<author confidence="0.977063">Muhua Zhu Jingbo Zhu Minghan Hu</author>
<affiliation confidence="0.969512">Natural Language Processing Northeastern University,</affiliation>
<email confidence="0.997563">zhumuhua@gmail.com</email>
<author confidence="0.310695">zhujingbomail neu edu cn huminghanise neu edu cn</author>
<abstract confidence="0.998162222222222">For the task of automatic treebank conversion, this paper presents a feature-based approach which encodes bracketing structures in a treebank into features to guide the conversion of this treebank to a different standard. Experiments on two Chinese treebanks show that our approach improves conversion accuracy a strong baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>