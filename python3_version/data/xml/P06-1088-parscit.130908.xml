<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000412">
<title confidence="0.982419">
Multi-Tagging for Lexicalized-Grammar Parsing
</title>
<author confidence="0.994183">
James R. Curran
</author>
<affiliation confidence="0.985128">
School of IT
University of Sydney
</affiliation>
<note confidence="0.683485">
NSW 2006, Australia
</note>
<email confidence="0.93237">
james@it.usyd.edu.au
</email>
<author confidence="0.970483">
Stephen Clark
</author>
<affiliation confidence="0.943958">
Computing Laboratory
Oxford University
</affiliation>
<address confidence="0.9484">
Wolfson Building
Parks Road
Oxford, OX1 3QD, UK
</address>
<email confidence="0.993386">
sclark@comlab.ox.ac.uk
</email>
<note confidence="0.7171926">
David Vadas
School of IT
University of Sydney
NSW 2006, Australia
dvadas1@it.usyd.edu.au
</note>
<sectionHeader confidence="0.9587" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999688086956522">
With performance above 97% accuracy for
newspaper text, part of speech (POS) tag-
ging might be considered a solved prob-
lem. Previous studies have shown that
allowing the parser to resolve POS tag
ambiguity does not improve performance.
However, for grammar formalisms which
use more fine-grained grammatical cate-
gories, for example TAG and CCG, tagging
accuracy is much lower. In fact, for these
formalisms, premature ambiguity resolu-
tion makes parsing infeasible.
We describe a multi-tagging approach
which maintains a suitable level of lexical
category ambiguity for accurate and effi-
cient CCG parsing. We extend this multi-
tagging approach to the POS level to over-
come errors introduced by automatically
assigned POS tags. Although POS tagging
accuracy seems high, maintaining some
POS tag ambiguity in the language pro-
cessing pipeline results in more accurate
CCG supertagging.
</bodyText>
<sectionHeader confidence="0.995139" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980081632653">
State-of-the-art part of speech (POS) tagging ac-
curacy is now above 97% for newspaper text
(Collins, 2002; Toutanova et al., 2003). One pos-
sible conclusion from the POS tagging literature
is that accuracy is approaching the limit, and any
remaining improvement is within the noise of the
Penn Treebank training data (Ratnaparkhi, 1996;
Toutanova et al., 2003).
So why should we continue to work on the POS
tagging problem? Here we give two reasons. First,
for lexicalized grammar formalisms such as TAG
and CCG, the tagging problem is much harder.
Second, any errors in POS tagger output, even at
97% acuracy, can have a significant impact on
components further down the language processing
pipeline. In previous work we have shown that us-
ing automatically assigned, rather than gold stan-
dard, POS tags reduces the accuracy of our CCG
parser by almost 2% in dependency F-score (Clark
and Curran, 2004b).
CCG supertagging is much harder than POS tag-
ging because the CCG tag set consists of fine-
grained lexical categories, resulting in a larger tag
set – over 400 CCG lexical categories compared
with 45 Penn Treebank POS tags. In fact, using
a state-of-the-art tagger as a front end to a CCG
parser makes accurate parsing infeasible because
of the high supertagging error rate.
Our solution is to use multi-tagging, in which
a CCG supertagger can potentially assign more
than one lexical category to a word. In this
paper we significantly improve our earlier ap-
proach (Clark and Curran, 2004a) by adapting the
forward-backward algorithm to a Maximum En-
tropy tagger, which is used to calculate a proba-
bility distribution over lexical categories for each
word. This distribution is used to assign one or
more categories to each word (Charniak et al.,
1996). We report large increases in accuracy over
single-tagging at only a small cost in increased
ambiguity.
A further contribution of the paper is to also
use multi-tagging for the POS tags, and to main-
tain some POS ambiguity in the language process-
ing pipeline. In particular, since POS tags are im-
portant features for the supertagger, we investigate
how supertagging accuracy can be improved by
not prematurely committing to a POS tag decision.
Our results first demonstrate that a surprising in-
</bodyText>
<note confidence="0.73972">
697
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 697–704,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.99973725">
crease in POS tagging accuracy can be achieved
with only a tiny increase in ambiguity; and second
that maintaining some POS ambiguity can signifi-
cantly improve the accuracy of the supertagger.
The parser uses the CCG lexical categories to
build syntactic structure, and the POS tags are
used by the supertagger and parser as part of their
statisical models. We show that using a multi-
tagger for supertagging results in an effective pre-
processor for CCG parsing, and that using a multi-
tagger for POS tagging results in more accurate
CCG supertagging.
</bodyText>
<sectionHeader confidence="0.879698" genericHeader="introduction">
2 Maximum Entropy Tagging
</sectionHeader>
<bodyText confidence="0.999961">
The tagger uses conditional probabilities of the
form P(ylx) where y is a tag and x is a local
context containing y. The conditional probabili-
ties have the following log-linear form:
</bodyText>
<equation confidence="0.9988985">
P(y|x) = 1 eK&apos;\ifi(x,y) (1)
Z(x)
</equation>
<bodyText confidence="0.999894181818182">
where Z(x) is a normalisation constant which en-
sures a proper probability distribution for each
context x.
The feature functions fi(x, y) are binary-
valued, returning either 0 or 1 depending on the
tag y and the value of a particular contextual pred-
icate given the context x. Contextual predicates
identify elements of the context which might be
useful for predicting the tag. For example, the fol-
lowing feature returns 1 if the current word is the
and the tag is DT; otherwise it returns 0:
</bodyText>
<equation confidence="0.943873333333333">
�
1 if word(x) = the &amp; y = DT
fi(x, y) = 0 otherwise
</equation>
<bodyText confidence="0.982975137931035">
(2)
word(x) = the is an example of a contextual
predicate. The POS tagger uses the same con-
textual predicates as Ratnaparkhi (1996); the su-
pertagger adds contextual predicates correspond-
ing to POS tags and bigram combinations of POS
tags (Curran and Clark, 2003).
Each feature fi has an associated weight Ai
which is determined during training. The training
process aims to maximise the entropy of the model
subject to the constraints that the expectation of
each feature according to the model matches the
empirical expectation from the training data. This
can be also thought of in terms of maximum like-
lihood estimation (MLE) for a log-linear model
(Della Pietra et al., 1997). We use the L-BFGS op-
timisation algorithm (Nocedal and Wright, 1999;
Malouf, 2002) to perform the estimation.
MLE has a tendency to overfit the training data.
We adopt the standard approach of Chen and
Rosenfeld (1999) by introducing a Gaussian prior
term to the objective function which penalises fea-
ture weights with large absolute values. A param-
eter defined in terms of the standard deviation of
the Gaussian determines the degree of smoothing.
The conditional probability of a sequence of
tags, y1, ... , yn, given a sentence, w1, ... , wn, is
defined as the product of the individual probabili-
ties for each tag:
</bodyText>
<equation confidence="0.996653333333333">
n
P(y1,...,ynlw1,...,wn) = P(yi|xi) (3)
i=1
</equation>
<bodyText confidence="0.999843333333333">
where xi is the context for word wi. We use the
standard approach of Viterbi decoding to find the
highest probability sequence.
</bodyText>
<subsectionHeader confidence="0.999558">
2.1 Multi-tagging
</subsectionHeader>
<bodyText confidence="0.999913833333334">
Multi-tagging — assigning one or more tags to a
word — is used here in two ways: first, to retain
ambiguity in the CCG lexical category sequence
for the purpose of building parse structure; and
second, to retain ambiguity in the POS tag se-
quence. We retain ambiguity in the lexical cate-
gory sequence since a single-tagger is not accurate
enough to serve as a front-end to a CCG parser, and
we retain some POS ambiguity since POS tags are
used as features in the statistical models of the su-
pertagger and parser.
Charniak et al. (1996) investigated multi-POS
tagging in the context of PCFG parsing. It was
found that multi-tagging provides only a minor
improvement in accuracy, with a significant loss
in efficiency; hence it was concluded that, given
the particular parser and tagger used, a single-tag
POS tagger is preferable to a multi-tagger. More
recently, Watson (2006) has revisited this question
in the context of the RASP parser (Briscoe and Car-
roll, 2002) and found that, similar to Charniak et
al. (1996), multi-tagging at the POS level results in
a small increase in parsing accuracy but at some
cost in efficiency.
For lexicalized grammars, such as CCG and
TAG, the motivation for using a multi-tagger to as-
sign the elementary structures (supertags) is more
compelling. Since the set of supertags is typ-
ically much larger than a standard POS tag set,
the tagging problem becomes much harder. In
</bodyText>
<page confidence="0.884226">
698
</page>
<bodyText confidence="0.99998946875">
fact, when using a state-of-the-art single-tagger,
the per-word accuracy for CCG supertagging is so
low (around 92%) that wide coverage, high ac-
curacy parsing becomes infeasible (Clark, 2002;
Clark and Curran, 2004a). Similar results have
been found for a highly lexicalized HPSG grammar
(Prins and van Noord, 2003), and also for TAG.
As far as we are aware, the only approach to suc-
cessfully integrate a TAG supertagger and parser is
the Lightweight Dependency Analyser of Banga-
lore (2000). Hence, in order to perform effective
full parsing with these lexicalized grammars, the
tagger front-end must be a multi-tagger (given the
current state-of-the-art).
The simplest approach to CCG supertagging is
to assign all categories to a word which the word
was seen with in the data. This leaves the parser
the task of managing the very large parse space re-
sulting from the high degree of lexical category
ambiguity (Hockenmaier and Steedman, 2002;
Hockenmaier, 2003). However, one of the orig-
inal motivations for supertagging was to signifi-
cantly reduce the syntactic ambiguity before full
parsing begins (Bangalore and Joshi, 1999). Clark
and Curran (2004a) found that performing CCG
supertagging prior to parsing can significantly in-
crease parsing efficiency with no loss in accuracy.
Our multi-tagging approach follows that of
Clark and Curran (2004a) and Charniak et al.
(1996): assign all categories to a word whose
probabilities are within a factor, β, of the proba-
bility of the most probable category for that word:
</bodyText>
<equation confidence="0.947735">
Ci = {c  |P(Ci = c|S) &gt; β P(Ci = cmax|S)}
</equation>
<bodyText confidence="0.998253">
Ci is the set of categories assigned to the ith word;
Ci is the random variable corresponding to the cat-
egory of the ith word; cmax is the category with the
highest probability of being the category of the ith
word; and S is the sentence. One advantage of this
adaptive approach is that, when the probability of
the highest scoring category is much greater than
the rest, no extra categories will be added.
Clark and Curran (2004a) propose a simple
method for calculating P(Ci = c|S): use the
word and POS features in the local context to cal-
culate the probability and ignore the previously
assigned categories (the history). However, it is
possible to incorporate the history in the calcula-
tion of the tag probabilities. A greedy approach is
to use the locally highest probability history as a
feature, which avoids any summing over alterna-
tive histories. Alternatively, there is a well-known
dynamic programming algorithm — the forward
backward algorithm — which efficiently calcu-
lates P(Ci = c|S) (Charniak et al., 1996).
The multitagger uses the following conditional
probabilities:
where xi,j = xi,... xj. Here yi is to be thought of
as a fixed category, whereas yj (j =� i) varies over
the possible categories for word j. In words, the
probability of category yi, given the sentence, is
the sum of the probabilities of all sequences con-
taining yi. This sum is calculated efficiently using
the forward-backward algorithm:
</bodyText>
<equation confidence="0.999512">
P(Ci = c|S) = αi(c)βi(c) (4)
</equation>
<bodyText confidence="0.999862678571428">
where αi(c) is the total probability of all the cate-
gory sub-sequences that end at position i with cat-
egory c; and βi(c) is the total probability of all the
category sub-sequences through to the end which
start at position i with category c.
The standard description of the forward-
backward algorithm, for example Manning and
Schutze (1999), is usually given for an HMM-style
tagger. However, it is straightforward to adapt the
algorithm to the Maximum Entropy models used
here. The forward-backward algorithm we use is
similar to that for a Maximum Entropy Markov
Model (Lafferty et al., 2001).
POS tags are very informative features for the
supertagger, which suggests that using a multi-
POS tagger may benefit the supertagger (and ulti-
mately the parser). However, it is unclear whether
multi-POS tagging will be useful in this context,
since our single-tagger POS tagger is highly accu-
rate: over 97% for WSJ text (Curran and Clark,
2003). In fact, in Clark and Curran (2004b) we re-
port that using automatically assigned, as opposed
to gold-standard, POS tags as features results in a
2% loss in parsing accuracy. This suggests that re-
taining some ambiguity in the POS sequence may
be beneficial for supertagging and parsing accu-
racy. In Section 4 we show this is the case for
supertagging.
</bodyText>
<sectionHeader confidence="0.893442" genericHeader="method">
3 CCG Supertagging and Parsing
</sectionHeader>
<bodyText confidence="0.999280333333333">
Parsing using CCG can be viewed as a two-stage
process: first assign lexical categories to the words
in the sentence, and then combine the categories
</bodyText>
<equation confidence="0.735261833333333">
� P(yi, y1,i−1, yi+1,n|w1,n)
P(yi|w1,n) =
y1,i−1,yi+1,n
699
The WSJ is a paper that I enjoy reading
NP/N N (S[dcl]\NP)/NP NP/N N (NP\NP)/(S[dcl]/NP) NP (S[dcl]\NP)/(S[ng]\NP) (S[ng]\NP)/NP
</equation>
<figureCaption confidence="0.998871">
Figure 1: Example sentence with CCG lexical categories.
</figureCaption>
<bodyText confidence="0.999971488372093">
together using CCG’s combinatory rules.1 We per-
form stage one using a supertagger.
The set of lexical categories used by the su-
pertagger is obtained from CCGbank (Hocken-
maier, 2003), a corpus of CCG normal-form
derivations derived semi-automatically from the
Penn Treebank. Following our earlier work, we
apply a frequency cutoff to the training set, only
using those categories which appear at least 10
times in sections 02-21, which results in a set of
425 categories. We have shown that the resulting
set has very high coverage on unseen data (Clark
and Curran, 2004a). Figure 1 gives an example
sentence with the CCG lexical categories.
The parser is described in Clark and Curran
(2004b). It takes POS tagged sentences as input
with each word assigned a set of lexical categories.
A packed chart is used to efficiently represent
all the possible analyses for a sentence, and the
CKY chart parsing algorithm described in Steed-
man (2000) is used to build the chart. A log-linear
model is used to score the alternative analyses.
In Clark and Curran (2004a) we described a
novel approach to integrating the supertagger and
parser: start with a very restrictive supertagger set-
ting, so that only a small number of lexical cate-
gories is assigned to each word, and only assign
more categories if the parser cannot find a span-
ning analysis. This strategy results in an efficient
and accurate parser, with speeds up to 35 sen-
tences per second. Accurate supertagging at low
levels of lexical category ambiguity is therefore
particularly important when using this strategy.
We found in Clark and Curran (2004b) that a
large drop in parsing accuracy occurs if automat-
ically assigned POS tags are used throughout the
parsing process, rather than gold standard POS
tags (almost 2% F-score over labelled dependen-
cies). This is due to the drop in accuracy of the
supertagger (see Table 3) and also the fact that
the log-linear parsing model uses POS tags as fea-
tures. The large drop in parsing accuracy demon-
strates that improving the performance of POS tag-
</bodyText>
<table confidence="0.998602916666667">
1See Steedman (2000) for an introduction to CCG, and
see Hockenmaier (2003) for an introduction to wide-coverage
parsing using CCG.
TAGS/WORD Q WORD ACC SENT ACC
1.00 1 96.7 51.8
1.01 0.8125 97.1 55.4
1.05 0.2969 98.3 70.7
1.10 0.1172 99.0 80.9
1.20 0.0293 99.5 89.3
1.30 0.0111 99.6 91.7
1.40 0.0053 99.7 93.2
4.23 0 99.8 94.8
</table>
<tableCaption confidence="0.975739">
Table 1: POS tagging accuracy on Section 00 for
different levels of ambiguity.
</tableCaption>
<bodyText confidence="0.999762833333333">
gers is still an important research problem. In this
paper we aim to reduce the performance drop of
the supertagger by maintaing some POS ambiguity
through to the supertagging phase. Future work
will investigate maintaining some POS ambiguity
through to the parsing phase also.
</bodyText>
<sectionHeader confidence="0.99608" genericHeader="method">
4 Multi-tagging Experiments
</sectionHeader>
<bodyText confidence="0.999995631578947">
We performed several sets of experiments for
POS tagging and CCG supertagging to explore the
trade-off between ambiguity and tagging accuracy.
For both POS tagging and supertagging we varied
the average number of tags assigned to each word,
to see whether it is possible to significantly in-
crease tagging accuracy with only a small increase
in ambiguity. For CCG supertagging, we also com-
pared multi-tagging approaches, with a fixed cate-
gory ambiguity of 1.4 categories per word.
All of the experiments used Section 02-21 of
CCGbank as training data, Section 00 as develop-
ment data and Section 23 as final test data. We
evaluate both per-word tag accuracy and sentence
accuracy, which is the percentage of sentences for
which every word is tagged correctly. For the
multi-tagging results we consider the word to be
tagged correctly if the correct tag appears in the
set of tags assigned to the word.
</bodyText>
<subsectionHeader confidence="0.504126">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.999480666666667">
Table 1 shows the results for multi-POS tagging
for different levels of ambiguity. The row corre-
sponding to 1.01 tags per word shows that adding
</bodyText>
<table confidence="0.976963285714286">
700
METHOD GOLD POS AUTO POS
WORD SENT WORD SENT
single 92.6 36.8 91.5 32.7
noseq 96.2 51.9 95.2 46.1
best hist 97.2 63.8 96.3 57.2
fwdbwd 97.9 72.1 96.9 64.8
</table>
<tableCaption confidence="0.956131666666667">
Table 2: Supertagging accuracy on Section 00 us-
ing different approaches with multi-tagger ambi-
guity fixed at 1.4 categories per word.
</tableCaption>
<table confidence="0.999716454545455">
TAGS/ Q GOLD POS AUTO POS
WORD WORD SENT WORD SENT
1.0 1 92.6 36.8 91.5 32.7
1.2 0.1201 96.8 63.4 95.8 56.5
1.4 0.0337 97.9 72.1 96.9 64.8
1.6 0.0142 98.3 76.4 97.5 69.3
1.8 0.0074 98.4 78.3 97.7 71.0
2.0 0.0048 98.5 79.4 97.9 72.5
2.5 0.0019 98.7 80.6 98.1 74.3
3.0 0.0009 98.7 81.4 98.3 75.6
12.5 0 98.9 82.3 98.8 80.1
</table>
<tableCaption confidence="0.971384">
Table 3: Supertagging accuracy on Section 00 for
different levels of ambiguity.
</tableCaption>
<bodyText confidence="0.999894425531915">
even a tiny amount of ambiguity (1 extra tag in ev-
ery 100 words) gives a reasonable improvement,
whilst adding 1 tag in 20 words, or approximately
one extra tag per sentence on the WSJ, gives a sig-
nificant boost of 1.6% word accuracy and almost
20% sentence accuracy.
The bottom row of Table 1 gives an upper bound
on accuracy if the maximum ambiguity is allowed.
This involves setting the Q value to 0, so all feasi-
ble tags are assigned. Note that the performance
gain is only 1.6% in sentence accuracy, compared
with the previous row, at the cost of a large in-
crease in ambiguity.
Our first set of CCG supertagging experiments
compared the performance of several approaches.
In Table 2 we give the accuracies when using gold
standard POS tags, and also POS tags automatically
assigned by our POS tagger described above. Since
POS tags are important features for the supertagger
maximum entropy model, erroneous tags have a
significant impact on supertagging accuracy.
The single method is the single-tagger supertag-
ger, which at 91.5% per-word accuracy is too inac-
curate for use with the CCG parser. The remaining
rows in the table give multi-tagger results for a cat-
egory ambiguity of 1.4 categories per word. The
noseq method, which performs significantly better
than single, does not take into account the previ-
ously assigned categories. The best hist method
gains roughly another 1% in accuracy over noseq
by taking the greedy approach of using only the
two most probable previously assigned categories.
Finally, the full forward-backward approach de-
scribed in Section 2.1 gains roughly another 0.6%
by considering all possible category histories. We
see the largest jump in accuracy just by returning
multiple categories. The other more modest gains
come from producing progressively better models
of the category sequence.
The final set of supertagging experiments in Ta-
ble 3 demonstrates the trade-off between ambigu-
ity and accuracy. Note that the ambiguity levels
need to be much higher to produce similar perfor-
mance to the POS tagger and that the upper bound
case (Q = 0) has a very high average ambiguity.
This is to be expected given the much larger CCG
tag set.
</bodyText>
<sectionHeader confidence="0.529474" genericHeader="method">
5 Tag uncertainty thoughout the pipeline
</sectionHeader>
<bodyText confidence="0.999793222222222">
Tables 2 and 3 show that supertagger accuracy
when using gold-standard POS tags is typically
1% higher than when using automatically assigned
POS tags. Clearly, correct POS tags are important
features for the supertagger.
Errors made by the supertagger can multiply
out when incorrect lexical categories are passed
to the parser, so a 1% increase in lexical category
error can become much more significant in the
parser evaluation. For example, when using the
dependency-based evaluation in Clark and Curran
(2004b), getting the lexical category wrong for a
ditransitive verb automatically leads to three de-
pendencies in the output being incorrect.
We have shown that multi-tagging can signif-
icantly increase the accuracy of the POS tagger
with only a small increase in ambiguity. What
we would like to do is maintain some degree of
POS tag ambiguity and pass multiple POS tags
through to the supertagging stage (and eventually
the parser). There are several ways to encode mul-
tiple POS tags as features. The simplest approach
is to treat all of the POS tags as binary features,
but this does not take into account the uncertainty
in each of the alternative tags. What we need is a
way of incorporating probability information into
the Maximum Entropy supertagger.
</bodyText>
<page confidence="0.863634">
701
</page>
<sectionHeader confidence="0.975286" genericHeader="method">
6 Real-values in ME models
</sectionHeader>
<bodyText confidence="0.999985391304348">
Maximum Entropy (ME) models, in the NLP lit-
erature, are typically defined with binary features,
although they do allow real-valued features. The
only constraint comes from the optimisation algo-
rithm; for example, GIS only allows non-negative
values. Real-valued features are commonly used
with other machine learning algorithms.
Binary features suffer from certain limitations
of the representation, which make them unsuitable
for modelling some properties. For example, POS
taggers have difficulty determining if capitalised,
sentence initial words are proper nouns. A useful
way to model this property is to determine the ra-
tio of capitalised and non-capitalised instances of
a particular word in a large corpus and use a real-
valued feature which encodes this ratio (Vadas and
Curran, 2005). The only way to include this fea-
ture in a binary representation is to discretize (or
bin) the feature values. For this type of feature,
choosing appropriate bins is difficult and it may be
hard to find a discretization scheme that performs
optimally.
Another problem with discretizing feature val-
ues is that it imposes artificial boundaries to define
the bins. For the example above, we may choose
the bins 0 &lt; x &lt; 1 and 1 &lt; x &lt; 2, which sepa-
rate the values 0.99 and 1.01 even though they are
close in value. At the same time, the model does
not distinguish between 0.01 and 0.99 even though
they are much further apart.
Further, if we have not seen cases for the bin
2 &lt; x &lt; 3, then the discretized model has no evi-
dence to determine the contribution of this feature.
But for the real-valued model, evidence support-
ing 1 &lt; x &lt; 2 and 3 &lt; x &lt; 4 provides evidence
for the missing bin. Thus the real-valued model
generalises more effectively.
One issue that is not addressed here is the inter-
action between the Gaussian smoothing parameter
and real-valued features. Using the same smooth-
ing parameter for real-valued features with vastly
different distributions is unlikely to be optimal.
However, for these experiments we have used the
same value for the smoothing parameter on all
real-valued features. This is the same value we
have used for the binary features.
</bodyText>
<sectionHeader confidence="0.968284" genericHeader="method">
7 Multi-POS Supertagging Experiments
</sectionHeader>
<bodyText confidence="0.999876488888889">
We have experimented with four different ap-
proaches to passing multiple POS tags as features
through to the supertagger. For the later exper-
iments, this required the existing binary-valued
framework to be extended to support real values.
The level of POS tag ambiguity was varied be-
tween 1.05 and 1.3 POS tags per word on average.
These results are shown in Table 4.
The first approach is to treat the multiple POS
tags as binary features (bin). This simply involves
adding the multiple POS tags for each word in
both the training and test data. Every assigned
POS tag is treated as a separate feature and con-
sidered equally important regardless of its uncer-
tainty. Here we see a minor increase in perfor-
mance over the original supertagger at the lower
levels of POS ambiguity. However, as the POS
ambiguity is increased, the performance of the
binary-valued features decreases and is eventually
worse than the original supertagger. This is be-
cause at the lowest levels of ambiguity the extra
POS tags can be treated as being of similar reli-
ability. However, at higher levels of ambiguity
many POS tags are added which are unreliable and
should not be trusted equally.
The second approach (split) uses real-valued
features to model some degree of uncertainty in
the POS tags, dividing the POS tag probability mass
evenly among the alternatives. This has the ef-
fect of giving smaller feature values to tags where
many alternative tags have been assigned. This
produces similar results to the binary-valued fea-
tures, again performing best at low levels of ambi-
guity.
The third approach (invrank) is to use the in-
verse rank of each POS tag as a real-valued feature.
The inverse rank is the reciprocal of the tag’s rank
ordered by decreasing probability. This method
assumes the POS tagger correctly orders the alter-
native tags, but does not rely on the probability
assigned to each tag. Overall, invrank performs
worse than split.
The final and best approach is to use the prob-
abilities assigned to each alternative tag as real-
valued features:
</bodyText>
<equation confidence="0.9910885">
_( p(POS(x) = NN) if y = NP
fZ (x, y) Sl 0 otherwise
</equation>
<bodyText confidence="0.9851555">
(5)
This model gives the best performance at 1.1 POS
tags per-word average ambiguity. Note that, even
when using the probabilities as features, only a
small amount of additional POS ambiguity is re-
quired to significantly improve performance.
</bodyText>
<page confidence="0.449321">
702
</page>
<table confidence="0.999680105263158">
METHOD POS AMB WORD SENT
orig 1.00 96.9 64.8
bin 1.05 97.3 67.7
1.10 97.3 66.3
1.20 97.0 63.5
1.30 96.8 62.1
split 1.05 97.4 68.5
1.10 97.4 67.9
1.20 97.3 67.0
1.30 97.2 65.1
prob 1.05 97.5 68.7
1.10 97.5 69.1
1.20 97.5 68.7
1.30 97.5 68.7
invrank 1.05 97.3 68.0
1.10 97.4 68.0
1.20 97.3 67.1
1.30 97.3 67.1
gold - 97.9 72.1
</table>
<tableCaption confidence="0.987187">
Table 4: Multi-POS supertagging on Section 00
</tableCaption>
<bodyText confidence="0.989400352941177">
with different levels of POS ambiguity and using
different approaches to POS feature encoding.
Table 5 shows our best performance figures for
the multi-POS supertagger, against the previously
described method using both gold standard and au-
tomatically assigned POS tags.
Table 6 uses the Section 23 test data to
demonstrate the improvement in supertagging
when moving from single-tagging (single) to sim-
ple multi-tagging (noseq); from simple multi-
tagging to the full forward-backward algorithm
(fwdbwd); and finally when using the probabilities
of multiply-assigned POS tags as features (MULTI-
POS column). All of these multi-tagging experi-
ments use an ambiguity level of 1.4 categories per
word and the last result uses POS tag ambiguity of
1.1 tags per word.
</bodyText>
<sectionHeader confidence="0.994201" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999148444444444">
The NLP community may consider POS tagging to
be a solved problem. In this paper, we have sug-
gested two reasons why this is not the case. First,
tagging for lexicalized-grammar formalisms, such
as CCG and TAG, is far from solved. Second,
even modest improvements in POS tagging accu-
racy can have a large impact on the performance of
downstream components in a language processing
pipeline.
</bodyText>
<table confidence="0.9971374">
TAGS/ AUTO POS MULTI POS GOLD POS
WORD WORD SENT WORD SENT WORD SENT
1.0 91.5 32.7 91.9 34.3 92.6 36.8
1.2 95.8 56.5 96.3 59.2 96.8 63.4
1.4 96.9 64.8 97.5 67.0 97.9 72.1
1.6 97.5 69.3 97.9 73.3 98.3 76.4
1.8 97.7 71.0 98.2 76.1 98.4 78.3
2.0 97.9 72.5 98.4 77.4 98.5 79.4
2.5 98.1 74.3 98.5 78.7 98.7 80.6
3.0 98.3 75.6 98.6 79.7 98.7 81.4
</table>
<tableCaption confidence="0.749186">
Table 5: Best multi-POS supertagging accuracy on
Section 00 using POS ambiguity of 1.1 and the
probability real-valued features.
</tableCaption>
<table confidence="0.99969925">
METHOD AUTO POS MULTI POS GOLD POS
single 92.0 - 93.3
noseq 95.4 - 96.6
fwdbwd 97.1 97.7 98.2
</table>
<tableCaption confidence="0.997602">
Table 6: Final supertagging results on Section 23.
</tableCaption>
<bodyText confidence="0.999981448275862">
We have developed a novel approach to main-
taining tag ambiguity in language processing
pipelines which avoids premature ambiguity res-
olution. The tag ambiguity is maintained by using
the forward-backward algorithm to calculate indi-
vidual tag probabilities. These probabilities can
then be used to select multiple tags and can also
be encoded as real-valued features in subsequent
statistical models.
With this new approach we have increased POS
tagging accuracy significantly with only a tiny am-
biguity penalty and also significantly improved on
previous CCG supertagging results. Finally, us-
ing POS tag probabilities as real-valued features in
the supertagging model, we demonstrated perfor-
mance close to that obtained with gold-standard
POS tags. This will significantly improve the ro-
bustness of the parser on unseen text.
In future work we will investigate maintaining
tag ambiguity further down the language process-
ing pipeline and exploiting the uncertainty from
previous stages. In particular, we will incorporate
real-valued POS tag and lexical category features
in the statistical parsing model. Another possibil-
ity is to investigate whether similar techniques can
improve other tagging tasks, such as Named Entity
Recognition.
This work can be seen as part of the larger
goal of maintaining ambiguity and exploiting un-
</bodyText>
<page confidence="0.778074">
703
</page>
<bodyText confidence="0.9999735">
certainty throughout language processing systems
(Roth and Yih, 2004), which is important for cop-
ing with the compounding of errors that is a sig-
nificant problem in language processing pipelines.
</bodyText>
<sectionHeader confidence="0.992142" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99991475">
We would like to thank the anonymous reviewers
for their helpful feedback. This work has been
supported by the Australian Research Council un-
der Discovery Project DP0453131.
</bodyText>
<sectionHeader confidence="0.995975" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999112477272727">
Srinivas Bangalore and Aravind Joshi. 1999. Supertagging:
An approach to almost parsing. Computational Linguis-
tics, 25(2):237–265.
Srinivas Bangalore. 2000. A lightweight dependency anal-
yser for partial parsing. Natural Language Engineering,
6(2):113–138.
Ted Briscoe and John Carroll. 2002. Robust accurate statis-
tical annotation of general tex. In Proceedings of the 3rd
LREC Conference, pages 1499–1504, Las Palmas, Gran
Canaria.
Eugene Charniak, Glenn Carroll, John Adcock, Anthony
Cassandra, Yoshihiko Gotoh, Jeremy Katz, Michael
Littman, and John McCann. 1996. Taggers for parsers.
Artificial Intelligence, 85:45–57.
Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian prior
for smoothing maximum entropy models. Technical re-
port, Carnegie Mellon University, Pittsburgh, PA.
Stephen Clark and James R. Curran. 2004a. The impor-
tance of supertagging for wide-coverage CCG parsing.
In Proceedings of COLING-04, pages 282–288, Geneva,
Switzerland.
Stephen Clark and James R. Curran. 2004b. Parsing the
WSJ using CCG and log-linear models. In Proceedings of
the 42nd Meeting of the ACL, pages 104–111, Barcelona,
Spain.
Stephen Clark. 2002. A supertagger for Combinatory Cate-
gorial Grammar. In Proceedings of the TAG+ Workshop,
pages 19–24, Venice, Italy.
Michael Collins. 2002. Discriminative training methods for
Hidden Markov Models: Theory and experiments with
perceptron algorithms. In Proceedings of the EMNLP
Conference, pages 1–8, Philadelphia, PA.
James R. Curran and Stephen Clark. 2003. Investigating GIS
and smoothing for maximum entropy taggers. In Proceed-
ings of the 10th Meeting of the EACL, pages 91–98, Bu-
dapest, Hungary.
Stephen Della Pietra, Vincent Della Pietra, and John Laf-
ferty. 1997. Inducing features of random fields. IEEE
Transactions Pattern Analysis and Machine Intelligence,
19(4):380–393.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with Combinatory Categorial
Grammar. In Proceedings of the 40th Meeting of the ACL,
pages 335–342, Philadelphia, PA.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceedings of
the 18th International Conference on Machine Learning,
pages 282–289, Williams College, MA.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings of
the Sixth Workshop on Natural Language Learning, pages
49–55, Taipei, Taiwan.
Christopher Manning and Hinrich Schutze. 1999. Foun-
dations of Statistical Natural Language Processing. The
MIT Press, Cambridge, Massachusetts.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical Op-
timization. Springer, New York, USA.
Robbert Prins and Gertjan van Noord. 2003. Reinforcing
parser preferences through tagging. Traitement Automa-
tique des Langues, 44(3):121–139.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the EMNLP Conference,
pages 133–142, Philadelphia, PA.
D. Roth and W. Yih. 2004. A linear programming for-
mulation for global inference in natural language tasks.
In Hwee Tou Ng and Ellen Riloff, editors, Proc. of the
Annual Conference on Computational Natural Language
Learning (CoNLL), pages 1–8. Association for Computa-
tional Linguistics.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Kristina Toutanova, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tag-
ging with a cyclic dependency network. In Proceedings
of the HLT/NAACL conference, pages 252–259, Edmon-
ton, Canada.
David Vadas and James R. Curran. 2005. Tagging un-
known words with raw text features. In Proceedings of the
Australasian Language Technology Workshop 2005, pages
32–39, Sydney, Australia.
Rebecca Watson. 2006. Part-of-speech tagging models for
parsing. In Proceedings of the Computaional Linguistics
in the UK Conference (CLUK-06), Open University, Mil-
ton Keynes, UK.
</reference>
<page confidence="0.930945">
704
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.221165">
<title confidence="0.999983">Multi-Tagging for Lexicalized-Grammar Parsing</title>
<author confidence="0.999994">James R Curran</author>
<affiliation confidence="0.9998935">School of IT University of Sydney</affiliation>
<address confidence="0.997681">NSW 2006, Australia</address>
<email confidence="0.997822">james@it.usyd.edu.au</email>
<author confidence="0.989185">Stephen Clark</author>
<affiliation confidence="0.999998">Computing Laboratory Oxford University</affiliation>
<title confidence="0.49916">Wolfson Building Parks Road</title>
<address confidence="0.997272">Oxford, OX1 3QD, UK</address>
<email confidence="0.987848">sclark@comlab.ox.ac.uk</email>
<author confidence="0.999604">David Vadas</author>
<affiliation confidence="0.999871">School of IT University of Sydney</affiliation>
<address confidence="0.9958">NSW 2006, Australia</address>
<email confidence="0.993409">dvadas1@it.usyd.edu.au</email>
<abstract confidence="0.987193260869565">With performance above 97% accuracy for text, part of speech tagging might be considered a solved problem. Previous studies have shown that the parser to resolve ambiguity does not improve performance. However, for grammar formalisms which use more fine-grained grammatical catefor example tagging accuracy is much lower. In fact, for these formalisms, premature ambiguity resolution makes parsing infeasible. We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and effi- We extend this multiapproach to the to overcome errors introduced by automatically Although accuracy seems high, maintaining some ambiguity in the language processing pipeline results in more accurate</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="9066" citStr="Bangalore and Joshi, 1999" startWordPosition="1480" endWordPosition="1483">order to perform effective full parsing with these lexicalized grammars, the tagger front-end must be a multi-tagger (given the current state-of-the-art). The simplest approach to CCG supertagging is to assign all categories to a word which the word was seen with in the data. This leaves the parser the task of managing the very large parse space resulting from the high degree of lexical category ambiguity (Hockenmaier and Steedman, 2002; Hockenmaier, 2003). However, one of the original motivations for supertagging was to significantly reduce the syntactic ambiguity before full parsing begins (Bangalore and Joshi, 1999). Clark and Curran (2004a) found that performing CCG supertagging prior to parsing can significantly increase parsing efficiency with no loss in accuracy. Our multi-tagging approach follows that of Clark and Curran (2004a) and Charniak et al. (1996): assign all categories to a word whose probabilities are within a factor, β, of the probability of the most probable category for that word: Ci = {c |P(Ci = c|S) &gt; β P(Ci = cmax|S)} Ci is the set of categories assigned to the ith word; Ci is the random variable corresponding to the category of the ith word; cmax is the category with the highest pro</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
</authors>
<title>A lightweight dependency analyser for partial parsing.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="8428" citStr="Bangalore (2000)" startWordPosition="1381" endWordPosition="1383">ce the set of supertags is typically much larger than a standard POS tag set, the tagging problem becomes much harder. In 698 fact, when using a state-of-the-art single-tagger, the per-word accuracy for CCG supertagging is so low (around 92%) that wide coverage, high accuracy parsing becomes infeasible (Clark, 2002; Clark and Curran, 2004a). Similar results have been found for a highly lexicalized HPSG grammar (Prins and van Noord, 2003), and also for TAG. As far as we are aware, the only approach to successfully integrate a TAG supertagger and parser is the Lightweight Dependency Analyser of Bangalore (2000). Hence, in order to perform effective full parsing with these lexicalized grammars, the tagger front-end must be a multi-tagger (given the current state-of-the-art). The simplest approach to CCG supertagging is to assign all categories to a word which the word was seen with in the data. This leaves the parser the task of managing the very large parse space resulting from the high degree of lexical category ambiguity (Hockenmaier and Steedman, 2002; Hockenmaier, 2003). However, one of the original motivations for supertagging was to significantly reduce the syntactic ambiguity before full pars</context>
</contexts>
<marker>Bangalore, 2000</marker>
<rawString>Srinivas Bangalore. 2000. A lightweight dependency analyser for partial parsing. Natural Language Engineering, 6(2):113–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general tex.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd LREC Conference,</booktitle>
<pages>1499--1504</pages>
<location>Las Palmas, Gran Canaria.</location>
<contexts>
<context position="7491" citStr="Briscoe and Carroll, 2002" startWordPosition="1224" endWordPosition="1228">nough to serve as a front-end to a CCG parser, and we retain some POS ambiguity since POS tags are used as features in the statistical models of the supertagger and parser. Charniak et al. (1996) investigated multi-POS tagging in the context of PCFG parsing. It was found that multi-tagging provides only a minor improvement in accuracy, with a significant loss in efficiency; hence it was concluded that, given the particular parser and tagger used, a single-tag POS tagger is preferable to a multi-tagger. More recently, Watson (2006) has revisited this question in the context of the RASP parser (Briscoe and Carroll, 2002) and found that, similar to Charniak et al. (1996), multi-tagging at the POS level results in a small increase in parsing accuracy but at some cost in efficiency. For lexicalized grammars, such as CCG and TAG, the motivation for using a multi-tagger to assign the elementary structures (supertags) is more compelling. Since the set of supertags is typically much larger than a standard POS tag set, the tagging problem becomes much harder. In 698 fact, when using a state-of-the-art single-tagger, the per-word accuracy for CCG supertagging is so low (around 92%) that wide coverage, high accuracy pa</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Ted Briscoe and John Carroll. 2002. Robust accurate statistical annotation of general tex. In Proceedings of the 3rd LREC Conference, pages 1499–1504, Las Palmas, Gran Canaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Glenn Carroll</author>
<author>John Adcock</author>
<author>Anthony Cassandra</author>
<author>Yoshihiko Gotoh</author>
<author>Jeremy Katz</author>
<author>Michael Littman</author>
<author>John McCann</author>
</authors>
<title>Taggers for parsers.</title>
<date>1996</date>
<journal>Artificial Intelligence,</journal>
<pages>85--45</pages>
<contexts>
<context position="3007" citStr="Charniak et al., 1996" startWordPosition="470" endWordPosition="473">ct, using a state-of-the-art tagger as a front end to a CCG parser makes accurate parsing infeasible because of the high supertagging error rate. Our solution is to use multi-tagging, in which a CCG supertagger can potentially assign more than one lexical category to a word. In this paper we significantly improve our earlier approach (Clark and Curran, 2004a) by adapting the forward-backward algorithm to a Maximum Entropy tagger, which is used to calculate a probability distribution over lexical categories for each word. This distribution is used to assign one or more categories to each word (Charniak et al., 1996). We report large increases in accuracy over single-tagging at only a small cost in increased ambiguity. A further contribution of the paper is to also use multi-tagging for the POS tags, and to maintain some POS ambiguity in the language processing pipeline. In particular, since POS tags are important features for the supertagger, we investigate how supertagging accuracy can be improved by not prematurely committing to a POS tag decision. Our results first demonstrate that a surprising in697 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting </context>
<context position="7060" citStr="Charniak et al. (1996)" startWordPosition="1157" endWordPosition="1160">pproach of Viterbi decoding to find the highest probability sequence. 2.1 Multi-tagging Multi-tagging — assigning one or more tags to a word — is used here in two ways: first, to retain ambiguity in the CCG lexical category sequence for the purpose of building parse structure; and second, to retain ambiguity in the POS tag sequence. We retain ambiguity in the lexical category sequence since a single-tagger is not accurate enough to serve as a front-end to a CCG parser, and we retain some POS ambiguity since POS tags are used as features in the statistical models of the supertagger and parser. Charniak et al. (1996) investigated multi-POS tagging in the context of PCFG parsing. It was found that multi-tagging provides only a minor improvement in accuracy, with a significant loss in efficiency; hence it was concluded that, given the particular parser and tagger used, a single-tag POS tagger is preferable to a multi-tagger. More recently, Watson (2006) has revisited this question in the context of the RASP parser (Briscoe and Carroll, 2002) and found that, similar to Charniak et al. (1996), multi-tagging at the POS level results in a small increase in parsing accuracy but at some cost in efficiency. For le</context>
<context position="9315" citStr="Charniak et al. (1996)" startWordPosition="1518" endWordPosition="1521"> seen with in the data. This leaves the parser the task of managing the very large parse space resulting from the high degree of lexical category ambiguity (Hockenmaier and Steedman, 2002; Hockenmaier, 2003). However, one of the original motivations for supertagging was to significantly reduce the syntactic ambiguity before full parsing begins (Bangalore and Joshi, 1999). Clark and Curran (2004a) found that performing CCG supertagging prior to parsing can significantly increase parsing efficiency with no loss in accuracy. Our multi-tagging approach follows that of Clark and Curran (2004a) and Charniak et al. (1996): assign all categories to a word whose probabilities are within a factor, β, of the probability of the most probable category for that word: Ci = {c |P(Ci = c|S) &gt; β P(Ci = cmax|S)} Ci is the set of categories assigned to the ith word; Ci is the random variable corresponding to the category of the ith word; cmax is the category with the highest probability of being the category of the ith word; and S is the sentence. One advantage of this adaptive approach is that, when the probability of the highest scoring category is much greater than the rest, no extra categories will be added. Clark and </context>
</contexts>
<marker>Charniak, Carroll, Adcock, Cassandra, Gotoh, Katz, Littman, McCann, 1996</marker>
<rawString>Eugene Charniak, Glenn Carroll, John Adcock, Anthony Cassandra, Yoshihiko Gotoh, Jeremy Katz, Michael Littman, and John McCann. 1996. Taggers for parsers. Artificial Intelligence, 85:45–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="5936" citStr="Chen and Rosenfeld (1999)" startWordPosition="961" endWordPosition="964">ure fi has an associated weight Ai which is determined during training. The training process aims to maximise the entropy of the model subject to the constraints that the expectation of each feature according to the model matches the empirical expectation from the training data. This can be also thought of in terms of maximum likelihood estimation (MLE) for a log-linear model (Della Pietra et al., 1997). We use the L-BFGS optimisation algorithm (Nocedal and Wright, 1999; Malouf, 2002) to perform the estimation. MLE has a tendency to overfit the training data. We adopt the standard approach of Chen and Rosenfeld (1999) by introducing a Gaussian prior term to the objective function which penalises feature weights with large absolute values. A parameter defined in terms of the standard deviation of the Gaussian determines the degree of smoothing. The conditional probability of a sequence of tags, y1, ... , yn, given a sentence, w1, ... , wn, is defined as the product of the individual probabilities for each tag: n P(y1,...,ynlw1,...,wn) = P(yi|xi) (3) i=1 where xi is the context for word wi. We use the standard approach of Viterbi decoding to find the highest probability sequence. 2.1 Multi-tagging Multi-tagg</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian prior for smoothing maximum entropy models. Technical report, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04,</booktitle>
<pages>282--288</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2154" citStr="Clark and Curran, 2004" startWordPosition="327" endWordPosition="330">se of the Penn Treebank training data (Ratnaparkhi, 1996; Toutanova et al., 2003). So why should we continue to work on the POS tagging problem? Here we give two reasons. First, for lexicalized grammar formalisms such as TAG and CCG, the tagging problem is much harder. Second, any errors in POS tagger output, even at 97% acuracy, can have a significant impact on components further down the language processing pipeline. In previous work we have shown that using automatically assigned, rather than gold standard, POS tags reduces the accuracy of our CCG parser by almost 2% in dependency F-score (Clark and Curran, 2004b). CCG supertagging is much harder than POS tagging because the CCG tag set consists of finegrained lexical categories, resulting in a larger tag set – over 400 CCG lexical categories compared with 45 Penn Treebank POS tags. In fact, using a state-of-the-art tagger as a front end to a CCG parser makes accurate parsing infeasible because of the high supertagging error rate. Our solution is to use multi-tagging, in which a CCG supertagger can potentially assign more than one lexical category to a word. In this paper we significantly improve our earlier approach (Clark and Curran, 2004a) by adap</context>
<context position="8152" citStr="Clark and Curran, 2004" startWordPosition="1333" endWordPosition="1336"> al. (1996), multi-tagging at the POS level results in a small increase in parsing accuracy but at some cost in efficiency. For lexicalized grammars, such as CCG and TAG, the motivation for using a multi-tagger to assign the elementary structures (supertags) is more compelling. Since the set of supertags is typically much larger than a standard POS tag set, the tagging problem becomes much harder. In 698 fact, when using a state-of-the-art single-tagger, the per-word accuracy for CCG supertagging is so low (around 92%) that wide coverage, high accuracy parsing becomes infeasible (Clark, 2002; Clark and Curran, 2004a). Similar results have been found for a highly lexicalized HPSG grammar (Prins and van Noord, 2003), and also for TAG. As far as we are aware, the only approach to successfully integrate a TAG supertagger and parser is the Lightweight Dependency Analyser of Bangalore (2000). Hence, in order to perform effective full parsing with these lexicalized grammars, the tagger front-end must be a multi-tagger (given the current state-of-the-art). The simplest approach to CCG supertagging is to assign all categories to a word which the word was seen with in the data. This leaves the parser the task of </context>
<context position="9927" citStr="Clark and Curran (2004" startWordPosition="1632" endWordPosition="1635">al. (1996): assign all categories to a word whose probabilities are within a factor, β, of the probability of the most probable category for that word: Ci = {c |P(Ci = c|S) &gt; β P(Ci = cmax|S)} Ci is the set of categories assigned to the ith word; Ci is the random variable corresponding to the category of the ith word; cmax is the category with the highest probability of being the category of the ith word; and S is the sentence. One advantage of this adaptive approach is that, when the probability of the highest scoring category is much greater than the rest, no extra categories will be added. Clark and Curran (2004a) propose a simple method for calculating P(Ci = c|S): use the word and POS features in the local context to calculate the probability and ignore the previously assigned categories (the history). However, it is possible to incorporate the history in the calculation of the tag probabilities. A greedy approach is to use the locally highest probability history as a feature, which avoids any summing over alternative histories. Alternatively, there is a well-known dynamic programming algorithm — the forward backward algorithm — which efficiently calculates P(Ci = c|S) (Charniak et al., 1996). The </context>
<context position="11928" citStr="Clark and Curran (2004" startWordPosition="1963" endWordPosition="1966">given for an HMM-style tagger. However, it is straightforward to adapt the algorithm to the Maximum Entropy models used here. The forward-backward algorithm we use is similar to that for a Maximum Entropy Markov Model (Lafferty et al., 2001). POS tags are very informative features for the supertagger, which suggests that using a multiPOS tagger may benefit the supertagger (and ultimately the parser). However, it is unclear whether multi-POS tagging will be useful in this context, since our single-tagger POS tagger is highly accurate: over 97% for WSJ text (Curran and Clark, 2003). In fact, in Clark and Curran (2004b) we report that using automatically assigned, as opposed to gold-standard, POS tags as features results in a 2% loss in parsing accuracy. This suggests that retaining some ambiguity in the POS sequence may be beneficial for supertagging and parsing accuracy. In Section 4 we show this is the case for supertagging. 3 CCG Supertagging and Parsing Parsing using CCG can be viewed as a two-stage process: first assign lexical categories to the words in the sentence, and then combine the categories � P(yi, y1,i−1, yi+1,n|w1,n) P(yi|w1,n) = y1,i−1,yi+1,n 699 The WSJ is a paper that I enjoy reading NP</context>
<context position="13240" citStr="Clark and Curran, 2004" startWordPosition="2175" endWordPosition="2178">ure 1: Example sentence with CCG lexical categories. together using CCG’s combinatory rules.1 We perform stage one using a supertagger. The set of lexical categories used by the supertagger is obtained from CCGbank (Hockenmaier, 2003), a corpus of CCG normal-form derivations derived semi-automatically from the Penn Treebank. Following our earlier work, we apply a frequency cutoff to the training set, only using those categories which appear at least 10 times in sections 02-21, which results in a set of 425 categories. We have shown that the resulting set has very high coverage on unseen data (Clark and Curran, 2004a). Figure 1 gives an example sentence with the CCG lexical categories. The parser is described in Clark and Curran (2004b). It takes POS tagged sentences as input with each word assigned a set of lexical categories. A packed chart is used to efficiently represent all the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart. A log-linear model is used to score the alternative analyses. In Clark and Curran (2004a) we described a novel approach to integrating the supertagger and parser: start with a very restrictive superta</context>
<context position="19912" citStr="Clark and Curran (2004" startWordPosition="3301" endWordPosition="3304">mbiguity. This is to be expected given the much larger CCG tag set. 5 Tag uncertainty thoughout the pipeline Tables 2 and 3 show that supertagger accuracy when using gold-standard POS tags is typically 1% higher than when using automatically assigned POS tags. Clearly, correct POS tags are important features for the supertagger. Errors made by the supertagger can multiply out when incorrect lexical categories are passed to the parser, so a 1% increase in lexical category error can become much more significant in the parser evaluation. For example, when using the dependency-based evaluation in Clark and Curran (2004b), getting the lexical category wrong for a ditransitive verb automatically leads to three dependencies in the output being incorrect. We have shown that multi-tagging can significantly increase the accuracy of the POS tagger with only a small increase in ambiguity. What we would like to do is maintain some degree of POS tag ambiguity and pass multiple POS tags through to the supertagging stage (and eventually the parser). There are several ways to encode multiple POS tags as features. The simplest approach is to treat all of the POS tags as binary features, but this does not take into accoun</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004a. The importance of supertagging for wide-coverage CCG parsing. In Proceedings of COLING-04, pages 282–288, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the ACL,</booktitle>
<pages>104--111</pages>
<location>Barcelona,</location>
<contexts>
<context position="2154" citStr="Clark and Curran, 2004" startWordPosition="327" endWordPosition="330">se of the Penn Treebank training data (Ratnaparkhi, 1996; Toutanova et al., 2003). So why should we continue to work on the POS tagging problem? Here we give two reasons. First, for lexicalized grammar formalisms such as TAG and CCG, the tagging problem is much harder. Second, any errors in POS tagger output, even at 97% acuracy, can have a significant impact on components further down the language processing pipeline. In previous work we have shown that using automatically assigned, rather than gold standard, POS tags reduces the accuracy of our CCG parser by almost 2% in dependency F-score (Clark and Curran, 2004b). CCG supertagging is much harder than POS tagging because the CCG tag set consists of finegrained lexical categories, resulting in a larger tag set – over 400 CCG lexical categories compared with 45 Penn Treebank POS tags. In fact, using a state-of-the-art tagger as a front end to a CCG parser makes accurate parsing infeasible because of the high supertagging error rate. Our solution is to use multi-tagging, in which a CCG supertagger can potentially assign more than one lexical category to a word. In this paper we significantly improve our earlier approach (Clark and Curran, 2004a) by adap</context>
<context position="8152" citStr="Clark and Curran, 2004" startWordPosition="1333" endWordPosition="1336"> al. (1996), multi-tagging at the POS level results in a small increase in parsing accuracy but at some cost in efficiency. For lexicalized grammars, such as CCG and TAG, the motivation for using a multi-tagger to assign the elementary structures (supertags) is more compelling. Since the set of supertags is typically much larger than a standard POS tag set, the tagging problem becomes much harder. In 698 fact, when using a state-of-the-art single-tagger, the per-word accuracy for CCG supertagging is so low (around 92%) that wide coverage, high accuracy parsing becomes infeasible (Clark, 2002; Clark and Curran, 2004a). Similar results have been found for a highly lexicalized HPSG grammar (Prins and van Noord, 2003), and also for TAG. As far as we are aware, the only approach to successfully integrate a TAG supertagger and parser is the Lightweight Dependency Analyser of Bangalore (2000). Hence, in order to perform effective full parsing with these lexicalized grammars, the tagger front-end must be a multi-tagger (given the current state-of-the-art). The simplest approach to CCG supertagging is to assign all categories to a word which the word was seen with in the data. This leaves the parser the task of </context>
<context position="9927" citStr="Clark and Curran (2004" startWordPosition="1632" endWordPosition="1635">al. (1996): assign all categories to a word whose probabilities are within a factor, β, of the probability of the most probable category for that word: Ci = {c |P(Ci = c|S) &gt; β P(Ci = cmax|S)} Ci is the set of categories assigned to the ith word; Ci is the random variable corresponding to the category of the ith word; cmax is the category with the highest probability of being the category of the ith word; and S is the sentence. One advantage of this adaptive approach is that, when the probability of the highest scoring category is much greater than the rest, no extra categories will be added. Clark and Curran (2004a) propose a simple method for calculating P(Ci = c|S): use the word and POS features in the local context to calculate the probability and ignore the previously assigned categories (the history). However, it is possible to incorporate the history in the calculation of the tag probabilities. A greedy approach is to use the locally highest probability history as a feature, which avoids any summing over alternative histories. Alternatively, there is a well-known dynamic programming algorithm — the forward backward algorithm — which efficiently calculates P(Ci = c|S) (Charniak et al., 1996). The </context>
<context position="11928" citStr="Clark and Curran (2004" startWordPosition="1963" endWordPosition="1966">given for an HMM-style tagger. However, it is straightforward to adapt the algorithm to the Maximum Entropy models used here. The forward-backward algorithm we use is similar to that for a Maximum Entropy Markov Model (Lafferty et al., 2001). POS tags are very informative features for the supertagger, which suggests that using a multiPOS tagger may benefit the supertagger (and ultimately the parser). However, it is unclear whether multi-POS tagging will be useful in this context, since our single-tagger POS tagger is highly accurate: over 97% for WSJ text (Curran and Clark, 2003). In fact, in Clark and Curran (2004b) we report that using automatically assigned, as opposed to gold-standard, POS tags as features results in a 2% loss in parsing accuracy. This suggests that retaining some ambiguity in the POS sequence may be beneficial for supertagging and parsing accuracy. In Section 4 we show this is the case for supertagging. 3 CCG Supertagging and Parsing Parsing using CCG can be viewed as a two-stage process: first assign lexical categories to the words in the sentence, and then combine the categories � P(yi, y1,i−1, yi+1,n|w1,n) P(yi|w1,n) = y1,i−1,yi+1,n 699 The WSJ is a paper that I enjoy reading NP</context>
<context position="13240" citStr="Clark and Curran, 2004" startWordPosition="2175" endWordPosition="2178">ure 1: Example sentence with CCG lexical categories. together using CCG’s combinatory rules.1 We perform stage one using a supertagger. The set of lexical categories used by the supertagger is obtained from CCGbank (Hockenmaier, 2003), a corpus of CCG normal-form derivations derived semi-automatically from the Penn Treebank. Following our earlier work, we apply a frequency cutoff to the training set, only using those categories which appear at least 10 times in sections 02-21, which results in a set of 425 categories. We have shown that the resulting set has very high coverage on unseen data (Clark and Curran, 2004a). Figure 1 gives an example sentence with the CCG lexical categories. The parser is described in Clark and Curran (2004b). It takes POS tagged sentences as input with each word assigned a set of lexical categories. A packed chart is used to efficiently represent all the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart. A log-linear model is used to score the alternative analyses. In Clark and Curran (2004a) we described a novel approach to integrating the supertagger and parser: start with a very restrictive superta</context>
<context position="19912" citStr="Clark and Curran (2004" startWordPosition="3301" endWordPosition="3304">mbiguity. This is to be expected given the much larger CCG tag set. 5 Tag uncertainty thoughout the pipeline Tables 2 and 3 show that supertagger accuracy when using gold-standard POS tags is typically 1% higher than when using automatically assigned POS tags. Clearly, correct POS tags are important features for the supertagger. Errors made by the supertagger can multiply out when incorrect lexical categories are passed to the parser, so a 1% increase in lexical category error can become much more significant in the parser evaluation. For example, when using the dependency-based evaluation in Clark and Curran (2004b), getting the lexical category wrong for a ditransitive verb automatically leads to three dependencies in the output being incorrect. We have shown that multi-tagging can significantly increase the accuracy of the POS tagger with only a small increase in ambiguity. What we would like to do is maintain some degree of POS tag ambiguity and pass multiple POS tags through to the supertagging stage (and eventually the parser). There are several ways to encode multiple POS tags as features. The simplest approach is to treat all of the POS tags as binary features, but this does not take into accoun</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004b. Parsing the WSJ using CCG and log-linear models. In Proceedings of the 42nd Meeting of the ACL, pages 104–111, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>A supertagger for Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the TAG+ Workshop,</booktitle>
<pages>pages</pages>
<location>Venice, Italy.</location>
<contexts>
<context position="8128" citStr="Clark, 2002" startWordPosition="1331" endWordPosition="1332">o Charniak et al. (1996), multi-tagging at the POS level results in a small increase in parsing accuracy but at some cost in efficiency. For lexicalized grammars, such as CCG and TAG, the motivation for using a multi-tagger to assign the elementary structures (supertags) is more compelling. Since the set of supertags is typically much larger than a standard POS tag set, the tagging problem becomes much harder. In 698 fact, when using a state-of-the-art single-tagger, the per-word accuracy for CCG supertagging is so low (around 92%) that wide coverage, high accuracy parsing becomes infeasible (Clark, 2002; Clark and Curran, 2004a). Similar results have been found for a highly lexicalized HPSG grammar (Prins and van Noord, 2003), and also for TAG. As far as we are aware, the only approach to successfully integrate a TAG supertagger and parser is the Lightweight Dependency Analyser of Bangalore (2000). Hence, in order to perform effective full parsing with these lexicalized grammars, the tagger front-end must be a multi-tagger (given the current state-of-the-art). The simplest approach to CCG supertagging is to assign all categories to a word which the word was seen with in the data. This leaves</context>
</contexts>
<marker>Clark, 2002</marker>
<rawString>Stephen Clark. 2002. A supertagger for Combinatory Categorial Grammar. In Proceedings of the TAG+ Workshop, pages 19–24, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1359" citStr="Collins, 2002" startWordPosition="197" endWordPosition="198">r these formalisms, premature ambiguity resolution makes parsing infeasible. We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient CCG parsing. We extend this multitagging approach to the POS level to overcome errors introduced by automatically assigned POS tags. Although POS tagging accuracy seems high, maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging. 1 Introduction State-of-the-art part of speech (POS) tagging accuracy is now above 97% for newspaper text (Collins, 2002; Toutanova et al., 2003). One possible conclusion from the POS tagging literature is that accuracy is approaching the limit, and any remaining improvement is within the noise of the Penn Treebank training data (Ratnaparkhi, 1996; Toutanova et al., 2003). So why should we continue to work on the POS tagging problem? Here we give two reasons. First, for lexicalized grammar formalisms such as TAG and CCG, the tagging problem is much harder. Second, any errors in POS tagger output, even at 97% acuracy, can have a significant impact on components further down the language processing pipeline. In p</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms. In Proceedings of the EMNLP Conference, pages 1–8, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Investigating GIS and smoothing for maximum entropy taggers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Meeting of the EACL,</booktitle>
<pages>91--98</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="5300" citStr="Curran and Clark, 2003" startWordPosition="856" endWordPosition="859">ending on the tag y and the value of a particular contextual predicate given the context x. Contextual predicates identify elements of the context which might be useful for predicting the tag. For example, the following feature returns 1 if the current word is the and the tag is DT; otherwise it returns 0: � 1 if word(x) = the &amp; y = DT fi(x, y) = 0 otherwise (2) word(x) = the is an example of a contextual predicate. The POS tagger uses the same contextual predicates as Ratnaparkhi (1996); the supertagger adds contextual predicates corresponding to POS tags and bigram combinations of POS tags (Curran and Clark, 2003). Each feature fi has an associated weight Ai which is determined during training. The training process aims to maximise the entropy of the model subject to the constraints that the expectation of each feature according to the model matches the empirical expectation from the training data. This can be also thought of in terms of maximum likelihood estimation (MLE) for a log-linear model (Della Pietra et al., 1997). We use the L-BFGS optimisation algorithm (Nocedal and Wright, 1999; Malouf, 2002) to perform the estimation. MLE has a tendency to overfit the training data. We adopt the standard a</context>
<context position="11892" citStr="Curran and Clark, 2003" startWordPosition="1956" endWordPosition="1959">nning and Schutze (1999), is usually given for an HMM-style tagger. However, it is straightforward to adapt the algorithm to the Maximum Entropy models used here. The forward-backward algorithm we use is similar to that for a Maximum Entropy Markov Model (Lafferty et al., 2001). POS tags are very informative features for the supertagger, which suggests that using a multiPOS tagger may benefit the supertagger (and ultimately the parser). However, it is unclear whether multi-POS tagging will be useful in this context, since our single-tagger POS tagger is highly accurate: over 97% for WSJ text (Curran and Clark, 2003). In fact, in Clark and Curran (2004b) we report that using automatically assigned, as opposed to gold-standard, POS tags as features results in a 2% loss in parsing accuracy. This suggests that retaining some ambiguity in the POS sequence may be beneficial for supertagging and parsing accuracy. In Section 4 we show this is the case for supertagging. 3 CCG Supertagging and Parsing Parsing using CCG can be viewed as a two-stage process: first assign lexical categories to the words in the sentence, and then combine the categories � P(yi, y1,i−1, yi+1,n|w1,n) P(yi|w1,n) = y1,i−1,yi+1,n 699 The WS</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Investigating GIS and smoothing for maximum entropy taggers. In Proceedings of the 10th Meeting of the EACL, pages 91–98, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="5717" citStr="Pietra et al., 1997" startWordPosition="925" endWordPosition="928">The POS tagger uses the same contextual predicates as Ratnaparkhi (1996); the supertagger adds contextual predicates corresponding to POS tags and bigram combinations of POS tags (Curran and Clark, 2003). Each feature fi has an associated weight Ai which is determined during training. The training process aims to maximise the entropy of the model subject to the constraints that the expectation of each feature according to the model matches the empirical expectation from the training data. This can be also thought of in terms of maximum likelihood estimation (MLE) for a log-linear model (Della Pietra et al., 1997). We use the L-BFGS optimisation algorithm (Nocedal and Wright, 1999; Malouf, 2002) to perform the estimation. MLE has a tendency to overfit the training data. We adopt the standard approach of Chen and Rosenfeld (1999) by introducing a Gaussian prior term to the objective function which penalises feature weights with large absolute values. A parameter defined in terms of the standard deviation of the Gaussian determines the degree of smoothing. The conditional probability of a sequence of tags, y1, ... , yn, given a sentence, w1, ... , wn, is defined as the product of the individual probabili</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE Transactions Pattern Analysis and Machine Intelligence, 19(4):380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>335--342</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="8880" citStr="Hockenmaier and Steedman, 2002" startWordPosition="1453" endWordPosition="1456">03), and also for TAG. As far as we are aware, the only approach to successfully integrate a TAG supertagger and parser is the Lightweight Dependency Analyser of Bangalore (2000). Hence, in order to perform effective full parsing with these lexicalized grammars, the tagger front-end must be a multi-tagger (given the current state-of-the-art). The simplest approach to CCG supertagging is to assign all categories to a word which the word was seen with in the data. This leaves the parser the task of managing the very large parse space resulting from the high degree of lexical category ambiguity (Hockenmaier and Steedman, 2002; Hockenmaier, 2003). However, one of the original motivations for supertagging was to significantly reduce the syntactic ambiguity before full parsing begins (Bangalore and Joshi, 1999). Clark and Curran (2004a) found that performing CCG supertagging prior to parsing can significantly increase parsing efficiency with no loss in accuracy. Our multi-tagging approach follows that of Clark and Curran (2004a) and Charniak et al. (1996): assign all categories to a word whose probabilities are within a factor, β, of the probability of the most probable category for that word: Ci = {c |P(Ci = c|S) &gt; </context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical parsing with Combinatory Categorial Grammar. In Proceedings of the 40th Meeting of the ACL, pages 335–342, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="8900" citStr="Hockenmaier, 2003" startWordPosition="1457" endWordPosition="1458"> we are aware, the only approach to successfully integrate a TAG supertagger and parser is the Lightweight Dependency Analyser of Bangalore (2000). Hence, in order to perform effective full parsing with these lexicalized grammars, the tagger front-end must be a multi-tagger (given the current state-of-the-art). The simplest approach to CCG supertagging is to assign all categories to a word which the word was seen with in the data. This leaves the parser the task of managing the very large parse space resulting from the high degree of lexical category ambiguity (Hockenmaier and Steedman, 2002; Hockenmaier, 2003). However, one of the original motivations for supertagging was to significantly reduce the syntactic ambiguity before full parsing begins (Bangalore and Joshi, 1999). Clark and Curran (2004a) found that performing CCG supertagging prior to parsing can significantly increase parsing efficiency with no loss in accuracy. Our multi-tagging approach follows that of Clark and Curran (2004a) and Charniak et al. (1996): assign all categories to a word whose probabilities are within a factor, β, of the probability of the most probable category for that word: Ci = {c |P(Ci = c|S) &gt; β P(Ci = cmax|S)} Ci</context>
<context position="12852" citStr="Hockenmaier, 2003" startWordPosition="2112" endWordPosition="2114">ertagging. 3 CCG Supertagging and Parsing Parsing using CCG can be viewed as a two-stage process: first assign lexical categories to the words in the sentence, and then combine the categories � P(yi, y1,i−1, yi+1,n|w1,n) P(yi|w1,n) = y1,i−1,yi+1,n 699 The WSJ is a paper that I enjoy reading NP/N N (S[dcl]\NP)/NP NP/N N (NP\NP)/(S[dcl]/NP) NP (S[dcl]\NP)/(S[ng]\NP) (S[ng]\NP)/NP Figure 1: Example sentence with CCG lexical categories. together using CCG’s combinatory rules.1 We perform stage one using a supertagger. The set of lexical categories used by the supertagger is obtained from CCGbank (Hockenmaier, 2003), a corpus of CCG normal-form derivations derived semi-automatically from the Penn Treebank. Following our earlier work, we apply a frequency cutoff to the training set, only using those categories which appear at least 10 times in sections 02-21, which results in a set of 425 categories. We have shown that the resulting set has very high coverage on unseen data (Clark and Curran, 2004a). Figure 1 gives an example sentence with the CCG lexical categories. The parser is described in Clark and Curran (2004b). It takes POS tagged sentences as input with each word assigned a set of lexical categor</context>
<context position="14792" citStr="Hockenmaier (2003)" startWordPosition="2439" endWordPosition="2440">ty is therefore particularly important when using this strategy. We found in Clark and Curran (2004b) that a large drop in parsing accuracy occurs if automatically assigned POS tags are used throughout the parsing process, rather than gold standard POS tags (almost 2% F-score over labelled dependencies). This is due to the drop in accuracy of the supertagger (see Table 3) and also the fact that the log-linear parsing model uses POS tags as features. The large drop in parsing accuracy demonstrates that improving the performance of POS tag1See Steedman (2000) for an introduction to CCG, and see Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. TAGS/WORD Q WORD ACC SENT ACC 1.00 1 96.7 51.8 1.01 0.8125 97.1 55.4 1.05 0.2969 98.3 70.7 1.10 0.1172 99.0 80.9 1.20 0.0293 99.5 89.3 1.30 0.0111 99.6 91.7 1.40 0.0053 99.7 93.2 4.23 0 99.8 94.8 Table 1: POS tagging accuracy on Section 00 for different levels of ambiguity. gers is still an important research problem. In this paper we aim to reduce the performance drop of the supertagger by maintaing some POS ambiguity through to the supertagging phase. Future work will investigate maintaining some POS ambiguity through to the parsing ph</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and Models for Statistical Parsing with Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<location>Williams College, MA.</location>
<contexts>
<context position="11547" citStr="Lafferty et al., 2001" startWordPosition="1899" endWordPosition="1902">P(Ci = c|S) = αi(c)βi(c) (4) where αi(c) is the total probability of all the category sub-sequences that end at position i with category c; and βi(c) is the total probability of all the category sub-sequences through to the end which start at position i with category c. The standard description of the forwardbackward algorithm, for example Manning and Schutze (1999), is usually given for an HMM-style tagger. However, it is straightforward to adapt the algorithm to the Maximum Entropy models used here. The forward-backward algorithm we use is similar to that for a Maximum Entropy Markov Model (Lafferty et al., 2001). POS tags are very informative features for the supertagger, which suggests that using a multiPOS tagger may benefit the supertagger (and ultimately the parser). However, it is unclear whether multi-POS tagging will be useful in this context, since our single-tagger POS tagger is highly accurate: over 97% for WSJ text (Curran and Clark, 2003). In fact, in Clark and Curran (2004b) we report that using automatically assigned, as opposed to gold-standard, POS tags as features results in a 2% loss in parsing accuracy. This suggests that retaining some ambiguity in the POS sequence may be benefici</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning, pages 282–289, Williams College, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Workshop on Natural Language Learning,</booktitle>
<pages>49--55</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="5800" citStr="Malouf, 2002" startWordPosition="940" endWordPosition="941">adds contextual predicates corresponding to POS tags and bigram combinations of POS tags (Curran and Clark, 2003). Each feature fi has an associated weight Ai which is determined during training. The training process aims to maximise the entropy of the model subject to the constraints that the expectation of each feature according to the model matches the empirical expectation from the training data. This can be also thought of in terms of maximum likelihood estimation (MLE) for a log-linear model (Della Pietra et al., 1997). We use the L-BFGS optimisation algorithm (Nocedal and Wright, 1999; Malouf, 2002) to perform the estimation. MLE has a tendency to overfit the training data. We adopt the standard approach of Chen and Rosenfeld (1999) by introducing a Gaussian prior term to the objective function which penalises feature weights with large absolute values. A parameter defined in terms of the standard deviation of the Gaussian determines the degree of smoothing. The conditional probability of a sequence of tags, y1, ... , yn, given a sentence, w1, ... , wn, is defined as the product of the individual probabilities for each tag: n P(y1,...,ynlw1,...,wn) = P(yi|xi) (3) i=1 where xi is the cont</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the Sixth Workshop on Natural Language Learning, pages 49–55, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Hinrich Schutze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="11293" citStr="Manning and Schutze (1999)" startWordPosition="1858" endWordPosition="1861">j (j =� i) varies over the possible categories for word j. In words, the probability of category yi, given the sentence, is the sum of the probabilities of all sequences containing yi. This sum is calculated efficiently using the forward-backward algorithm: P(Ci = c|S) = αi(c)βi(c) (4) where αi(c) is the total probability of all the category sub-sequences that end at position i with category c; and βi(c) is the total probability of all the category sub-sequences through to the end which start at position i with category c. The standard description of the forwardbackward algorithm, for example Manning and Schutze (1999), is usually given for an HMM-style tagger. However, it is straightforward to adapt the algorithm to the Maximum Entropy models used here. The forward-backward algorithm we use is similar to that for a Maximum Entropy Markov Model (Lafferty et al., 2001). POS tags are very informative features for the supertagger, which suggests that using a multiPOS tagger may benefit the supertagger (and ultimately the parser). However, it is unclear whether multi-POS tagging will be useful in this context, since our single-tagger POS tagger is highly accurate: over 97% for WSJ text (Curran and Clark, 2003).</context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>Christopher Manning and Hinrich Schutze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
<author>Stephen J Wright</author>
</authors>
<title>Numerical Optimization.</title>
<date>1999</date>
<publisher>Springer,</publisher>
<location>New York, USA.</location>
<contexts>
<context position="5785" citStr="Nocedal and Wright, 1999" startWordPosition="936" endWordPosition="939">i (1996); the supertagger adds contextual predicates corresponding to POS tags and bigram combinations of POS tags (Curran and Clark, 2003). Each feature fi has an associated weight Ai which is determined during training. The training process aims to maximise the entropy of the model subject to the constraints that the expectation of each feature according to the model matches the empirical expectation from the training data. This can be also thought of in terms of maximum likelihood estimation (MLE) for a log-linear model (Della Pietra et al., 1997). We use the L-BFGS optimisation algorithm (Nocedal and Wright, 1999; Malouf, 2002) to perform the estimation. MLE has a tendency to overfit the training data. We adopt the standard approach of Chen and Rosenfeld (1999) by introducing a Gaussian prior term to the objective function which penalises feature weights with large absolute values. A parameter defined in terms of the standard deviation of the Gaussian determines the degree of smoothing. The conditional probability of a sequence of tags, y1, ... , yn, given a sentence, w1, ... , wn, is defined as the product of the individual probabilities for each tag: n P(y1,...,ynlw1,...,wn) = P(yi|xi) (3) i=1 where</context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>Jorge Nocedal and Stephen J. Wright. 1999. Numerical Optimization. Springer, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robbert Prins</author>
<author>Gertjan van Noord</author>
</authors>
<title>Reinforcing parser preferences through tagging.</title>
<date>2003</date>
<booktitle>Traitement Automatique des Langues,</booktitle>
<volume>44</volume>
<issue>3</issue>
<marker>Prins, van Noord, 2003</marker>
<rawString>Robbert Prins and Gertjan van Noord. 2003. Reinforcing parser preferences through tagging. Traitement Automatique des Langues, 44(3):121–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-ofspeech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>133--142</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1588" citStr="Ratnaparkhi, 1996" startWordPosition="233" endWordPosition="234">d this multitagging approach to the POS level to overcome errors introduced by automatically assigned POS tags. Although POS tagging accuracy seems high, maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging. 1 Introduction State-of-the-art part of speech (POS) tagging accuracy is now above 97% for newspaper text (Collins, 2002; Toutanova et al., 2003). One possible conclusion from the POS tagging literature is that accuracy is approaching the limit, and any remaining improvement is within the noise of the Penn Treebank training data (Ratnaparkhi, 1996; Toutanova et al., 2003). So why should we continue to work on the POS tagging problem? Here we give two reasons. First, for lexicalized grammar formalisms such as TAG and CCG, the tagging problem is much harder. Second, any errors in POS tagger output, even at 97% acuracy, can have a significant impact on components further down the language processing pipeline. In previous work we have shown that using automatically assigned, rather than gold standard, POS tags reduces the accuracy of our CCG parser by almost 2% in dependency F-score (Clark and Curran, 2004b). CCG supertagging is much harde</context>
<context position="5169" citStr="Ratnaparkhi (1996)" startWordPosition="837" endWordPosition="838">oper probability distribution for each context x. The feature functions fi(x, y) are binaryvalued, returning either 0 or 1 depending on the tag y and the value of a particular contextual predicate given the context x. Contextual predicates identify elements of the context which might be useful for predicting the tag. For example, the following feature returns 1 if the current word is the and the tag is DT; otherwise it returns 0: � 1 if word(x) = the &amp; y = DT fi(x, y) = 0 otherwise (2) word(x) = the is an example of a contextual predicate. The POS tagger uses the same contextual predicates as Ratnaparkhi (1996); the supertagger adds contextual predicates corresponding to POS tags and bigram combinations of POS tags (Curran and Clark, 2003). Each feature fi has an associated weight Ai which is determined during training. The training process aims to maximise the entropy of the model subject to the constraints that the expectation of each feature according to the model matches the empirical expectation from the training data. This can be also thought of in terms of maximum likelihood estimation (MLE) for a log-linear model (Della Pietra et al., 1997). We use the L-BFGS optimisation algorithm (Nocedal </context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy part-ofspeech tagger. In Proceedings of the EMNLP Conference, pages 133–142, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Hwee Tou Ng and Ellen Riloff, editors, Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>1--8</pages>
<publisher>Association for Computational Linguistics.</publisher>
<marker>Roth, Yih, 2004</marker>
<rawString>D. Roth and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Hwee Tou Ng and Ellen Riloff, editors, Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL), pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="13611" citStr="Steedman (2000)" startWordPosition="2238" endWordPosition="2240">toff to the training set, only using those categories which appear at least 10 times in sections 02-21, which results in a set of 425 categories. We have shown that the resulting set has very high coverage on unseen data (Clark and Curran, 2004a). Figure 1 gives an example sentence with the CCG lexical categories. The parser is described in Clark and Curran (2004b). It takes POS tagged sentences as input with each word assigned a set of lexical categories. A packed chart is used to efficiently represent all the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart. A log-linear model is used to score the alternative analyses. In Clark and Curran (2004a) we described a novel approach to integrating the supertagger and parser: start with a very restrictive supertagger setting, so that only a small number of lexical categories is assigned to each word, and only assign more categories if the parser cannot find a spanning analysis. This strategy results in an efficient and accurate parser, with speeds up to 35 sentences per second. Accurate supertagging at low levels of lexical category ambiguity is therefore particularly importan</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT/NAACL conference,</booktitle>
<pages>252--259</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1384" citStr="Toutanova et al., 2003" startWordPosition="199" endWordPosition="202">sms, premature ambiguity resolution makes parsing infeasible. We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient CCG parsing. We extend this multitagging approach to the POS level to overcome errors introduced by automatically assigned POS tags. Although POS tagging accuracy seems high, maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging. 1 Introduction State-of-the-art part of speech (POS) tagging accuracy is now above 97% for newspaper text (Collins, 2002; Toutanova et al., 2003). One possible conclusion from the POS tagging literature is that accuracy is approaching the limit, and any remaining improvement is within the noise of the Penn Treebank training data (Ratnaparkhi, 1996; Toutanova et al., 2003). So why should we continue to work on the POS tagging problem? Here we give two reasons. First, for lexicalized grammar formalisms such as TAG and CCG, the tagging problem is much harder. Second, any errors in POS tagger output, even at 97% acuracy, can have a significant impact on components further down the language processing pipeline. In previous work we have show</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the HLT/NAACL conference, pages 252–259, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Tagging unknown words with raw text features.</title>
<date>2005</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop</booktitle>
<pages>32--39</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="21487" citStr="Vadas and Curran, 2005" startWordPosition="3552" endWordPosition="3555">om the optimisation algorithm; for example, GIS only allows non-negative values. Real-valued features are commonly used with other machine learning algorithms. Binary features suffer from certain limitations of the representation, which make them unsuitable for modelling some properties. For example, POS taggers have difficulty determining if capitalised, sentence initial words are proper nouns. A useful way to model this property is to determine the ratio of capitalised and non-capitalised instances of a particular word in a large corpus and use a realvalued feature which encodes this ratio (Vadas and Curran, 2005). The only way to include this feature in a binary representation is to discretize (or bin) the feature values. For this type of feature, choosing appropriate bins is difficult and it may be hard to find a discretization scheme that performs optimally. Another problem with discretizing feature values is that it imposes artificial boundaries to define the bins. For the example above, we may choose the bins 0 &lt; x &lt; 1 and 1 &lt; x &lt; 2, which separate the values 0.99 and 1.01 even though they are close in value. At the same time, the model does not distinguish between 0.01 and 0.99 even though they a</context>
</contexts>
<marker>Vadas, Curran, 2005</marker>
<rawString>David Vadas and James R. Curran. 2005. Tagging unknown words with raw text features. In Proceedings of the Australasian Language Technology Workshop 2005, pages 32–39, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Watson</author>
</authors>
<title>Part-of-speech tagging models for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Computaional Linguistics in the UK Conference (CLUK-06),</booktitle>
<institution>Open University,</institution>
<location>Milton Keynes, UK.</location>
<contexts>
<context position="7401" citStr="Watson (2006)" startWordPosition="1211" endWordPosition="1212">uity in the lexical category sequence since a single-tagger is not accurate enough to serve as a front-end to a CCG parser, and we retain some POS ambiguity since POS tags are used as features in the statistical models of the supertagger and parser. Charniak et al. (1996) investigated multi-POS tagging in the context of PCFG parsing. It was found that multi-tagging provides only a minor improvement in accuracy, with a significant loss in efficiency; hence it was concluded that, given the particular parser and tagger used, a single-tag POS tagger is preferable to a multi-tagger. More recently, Watson (2006) has revisited this question in the context of the RASP parser (Briscoe and Carroll, 2002) and found that, similar to Charniak et al. (1996), multi-tagging at the POS level results in a small increase in parsing accuracy but at some cost in efficiency. For lexicalized grammars, such as CCG and TAG, the motivation for using a multi-tagger to assign the elementary structures (supertags) is more compelling. Since the set of supertags is typically much larger than a standard POS tag set, the tagging problem becomes much harder. In 698 fact, when using a state-of-the-art single-tagger, the per-word</context>
</contexts>
<marker>Watson, 2006</marker>
<rawString>Rebecca Watson. 2006. Part-of-speech tagging models for parsing. In Proceedings of the Computaional Linguistics in the UK Conference (CLUK-06), Open University, Milton Keynes, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>