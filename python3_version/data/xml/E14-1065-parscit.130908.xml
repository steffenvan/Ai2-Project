<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<title confidence="0.9990715">
Augmenting Translation Models with Simulated Acoustic Confusions for
Improved Spoken Language Translation
</title>
<author confidence="0.983606">
Yulia Tsvetkov Florian Metze Chris Dyer
</author>
<affiliation confidence="0.915313">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213; U.S.A.
</affiliation>
<email confidence="0.995403">
{ytsvetko, fmetze, cdyer}@cs.cmu.edu
</email>
<sectionHeader confidence="0.99387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999907">
We propose a novel technique for adapting
text-based statistical machine translation
to deal with input from automatic speech
recognition in spoken language translation
tasks. We simulate likely misrecognition
errors using only a source language pro-
nunciation dictionary and language model
(i.e., without an acoustic model), and use
these to augment the phrase table of a stan-
dard MT system. The augmented sys-
tem can thus recover from recognition er-
rors during decoding using synthesized
phrases. Using the outputs of five differ-
ent English ASR systems as input, we find
consistent and significant improvements in
translation quality. Our proposed tech-
nique can also be used in conjunction with
lattices as ASR output, leading to further
improvements.
</bodyText>
<sectionHeader confidence="0.998957" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999324607142857">
Spoken language translation (SLT) systems gen-
erally consist of two components: (i) an auto-
matic speech recognition (ASR) system that tran-
scribes source language utterances and (ii) a ma-
chine translation (MT) system that translates the
transcriptions into the target language. These two
components are usually developed independently
and then combined and integrated (Ney, 1999;
Matusov et al., 2006; Casacuberta et al., 2008;
Zhou, 2013; He and Deng, 2013).
While this architecture is attractive since it re-
lies only on components that are independently
useful, such systems face several challenges. First,
spoken language tends to be quite different from
the highly edited parallel texts that are available to
train translation systems. For example, disfluen-
cies, such as repeated words or phrases, restarts,
and revisions of content, are frequent in spon-
taneous speech,1 while these are usually absent
in written texts. In addition, ASR outputs typi-
cally lack explicit segmentation into sentences, as
well as reliable casing and punctuation informa-
tion, which are crucial for MT and other text-based
language processing applications (Ostendorf et al.,
2008). Second, ASR systems are imperfect and
make recognition errors. Even high quality sys-
tems make recognition errors, especially in acous-
tically similar words with similar language model
scores, for example morphological substitutions
like confusing bare stem and past tense forms, and
in high-frequency short words (function words)
which often lack both disambiguating context and
are subject to reduced pronunciations (Goldwater
et al., 2010).
One would expect that training an MT system
on ASR outputs (rather than the usual written-
style texts) would improve matters. Unfortunately,
there are few corpora of speech paired with text
translations into a second language that could be
used for this purpose. This has been an incentive
to various MT adaptation approaches and devel-
opment of speech-input MT systems. MT adapta-
tion has been done via input text pre-processing,
by transformation of spoken language (ASR out-
put) into written language (MT input) (Peitz et
al., 2012; Xu et al., 2012); via decoding ASR n-
best lists (Quan et al., 2005), or confusion net-
works (Bertoldi et al., 2007; Casacuberta et al.,
2008), or lattices (Dyer et al., 2008; Onishi et al.,
2010); via additional translation features captur-
ing acoustic information (Zhang et al., 2004); and
with methods that follow a paradigm of unified de-
coding (Zhou et al., 2007; Zhou, 2013). In line
with the previous research, we too adapt a standard
MT system to a speech-input MT, but by altering
the translation model itself so it is better able to
</bodyText>
<footnote confidence="0.995156333333333">
1Disfluencies constitute about 6% of word tokens in spon-
taneous speech, not including silent pauses (Tree, 1995; Kasl
and Mahl, 1965)
</footnote>
<page confidence="0.920019">
616
</page>
<note confidence="0.9943735">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 616–625,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.990963564102564">
deal with ASR output (Callison-Burch et al., 2006;
Tsvetkov et al., 2013a).
We address speech translation in a resource-
deficient scenario, specifically, adapting MT sys-
tems to SLT when ASR is unavailable. We aug-
ment a discriminative set that translation models
rescore with synthetic translation options. These
automatically generated translation rules (hence-
forth synthetic phrases) are noisy variants of ob-
served translation rules with simulated plausible
speech recognition errors (§2). To simulate ASR
errors we generate acoustically and distribution-
ally similar phrases to a source (English) phrase
with a phonologically-motivated algorithm (§4).
Likely phonetic substitutions are learned with an
unsupervised algorithm that produces clusters of
similar phones (§3). We show that MT systems
augmented with synthetic phrases increase the
coverage of input sequences that can be translated,
and yield significant improvement in the quality of
translated speech (§6).
This work makes several contributions. Primary
is our framework to adapt MT to SLT by popu-
lating translation models with synthetic phrases.2
Second, we propose a novel method to generate
acoustic confusions that are likely to be encoun-
tered in ASR transcription hypotheses. Third, we
devise simple and effective phone clustering al-
gorithm. All aforementioned algorithms work in
a low-resource scenario, without recourse to au-
dio data, speech transcripts, or ASR outputs: our
method to predict likely recognition errors uses
phonological rather than acoustic information and
does not depend on a specific ASR system. Since
our source language is English, we operate on a
phone level and employ a pronunciation dictionary
and a language model, but the algorithm can in
principle be applied without pronunciation dictio-
nary for languages with a phonemic orthography.
</bodyText>
<sectionHeader confidence="0.993691" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999622833333333">
We adopt a standard ASR-MT cascading approach
and then augment translation models with syn-
thetic phrases. Our proposed system architecture
is depicted in Figure 1.
Synthetic phrases are generated from entries in
the original translation model–phrase translation
</bodyText>
<footnote confidence="0.9982762">
2We augment phrase tables only with synthetic phrases
that capture simulated ASR errors, the methodology that we
advocate, however, is applicable to many problems in transla-
tion (Tsvetkov et al., 2013a; Ammar et al., 2013; Chahuneau
et al., 2013).
</footnote>
<figureCaption confidence="0.982908">
Figure 1: SLT architecture: ASR and MT are
</figureCaption>
<bodyText confidence="0.990367166666667">
trained independently and then cascaded. We im-
prove SLT by populating MT translation model
with synthetic phrases. Each synthetic phrase is
a variant of an original phrase pair with simulated
ASR errors on the source side.
pairs acquired from parallel data. From a source
side of an original phrase pair we generate list of
its plausible misrecognition variants (pseudo-ASR
outputs with recognition errors) and add them as
a source side of a synthetic phrase. For k-best
simulated ASR outputs we construct k synthetic
phrases: a simulated ASR output in the source
side is coupled with its translation–an original tar-
get phrase (identical for all k phrases). Synthetic
phrases are annotated with five standard phrasal
translation features (forward and reverse phrase
and lexical translation probabilities and phrase
penalty); these were found in the original phrase
and remain unchanged. In addition, we add three
new features to all phrase pairs, both synthetic and
original. First, we add a boolean feature indi-
cating the origin of a phrase: synthetic or origi-
nal. Two other features correspond to an ASR lan-
guage model score of the source side. One is LM
score of the synthetic phrase, another is a score
of a phrase from which the source side was gener-
ated. We then append synthetic phrases to a phrase
table: k synthetic phrases for each original phrase
pair, with eight features attached to each phrase.
We show synthetic phrases example in Figure 2.
</bodyText>
<sectionHeader confidence="0.918914" genericHeader="method">
3 Acoustically confusable phones
</sectionHeader>
<bodyText confidence="0.99993625">
The phonetic context of a given phone affects its
acoustic realization, and a variability in a produc-
tion of the same phone is possible depending on
coarticulation with its neighboring phones.3 In ad-
dition, there are phonotactic constraints that can
restrict allowed sequences of phones. English has
strong constraints on sequences of consonants; the
sequence [zdr], for example, cannot be a legal En-
</bodyText>
<footnote confidence="0.944561">
3These are the reasons why in context-dependent acous-
tic modeling different HMM models are trained for different
contexts.
</footnote>
<figure confidence="0.986092428571429">
MT
TM+TM&apos;
MT LM
(target lang.)
ASR
ASR LM
(source lang.)
Acoustic
Model
cat
לותח
Translation Model
augmented with
simulated ASR errors
</figure>
<page confidence="0.98668">
617
</page>
<table confidence="0.761547583333333">
Source Target Original phrase Synthetic Synthetic Original
phrase phrase translation features indicator LM score LM score
tells the story raconte l’histoire f1, f2, f3, f4, f5 0 3.9x10−3 3.9x10−3
tell their story raconte l’histoire f1, f2, f3, f4, f5 1 5.9x10−3 3.9x10−3
tells a story raconte l’histoire f1, f2, f3, f4, f5 1 2.2x10−3 3.9x10−3
tell the story raconte l’histoire f1, f2, f3, f4, f5 1 1.7x10−3 3.9x10−3
tell a story raconte l’histoire f1, f2, f3, f4, f5 1 1.3x10−3 3.9x10−3
tell that story raconte l’histoire f1, f2, f3, f4, f5 1 1.0x10−3 3.9x10−3
tell their stories raconte l’histoire f1, f2, f3, f4, f5 1 0.9x10−3 3.9x10−3
tells the stories raconte l’histoire f1, f2, f3, f4, f5 1 0.8x10−3 3.9x10−3
tells her story raconte l’histoire f1, f2, f3, f4, f5 1 0.7x10−3 3.9x10−3
chelsea star raconte l’histoire f1, f2, f3, f4, f5 1 0.5x10−3 3.9x10−3
</table>
<figureCaption confidence="0.831932">
Figure 2: Example of acoustically confusable synthetic phrases. Phrases were synthesized from the
original phrase pair in Row 1 by generating acoustically similar phrases for the English phrase tells the
story. All phrases have the same (target) French translation me raconte l’histoire and the same five basic
phrase-based translation rule features. To these, three additional features are added: a synthetic phrase
indicator, the source language LM score of the source phrase, and the source language LM score of a
source phrase in the original phrase pair.
</figureCaption>
<figure confidence="0.813872">
Tleft Tright TWleft WIHright . . .
T P(T|WIH) ...
W P(W|T) ...
II P(IH|T) P(IH|TW) ...
ER P(ER|T) ...
... ... ... ... ... ...
</figure>
<figureCaption confidence="0.99733">
Figure 3: A fragment of the co-occurrence matrix
</figureCaption>
<bodyText confidence="0.970018086956522">
for phone sequence [T W IH T ER]. Rows corre-
spond to phones; columns correspond to left/right
context phones of lengths one and two.
glish syllable onset (Jurafsky and Martin, 2000).
Motivated by the constraining effect of context
on phonetic distribution, we cluster phones using a
distance-based measure. To do so, we build a vec-
tor space model representation of each phone by
creating a co-occurrence matrix from a corpus of
phonetic forms where each row represents a phone
and columns indicate the contextual phones. We
take into account left/right context windows of
lengths one and two. A cell rp,, in the vector space
dictionary matrix represents phone p and context c
using the empirical relative frequency f(p  |c), as
estimated from a pronunciation dictionary. Fig-
ure 3 shows a fragment of the co-occurrence ma-
trix constructed from a dictionary containing just
the pronunciation of Twitter – [T W IH T ER].
Under this representation, the similarity of
phones can be easily quantified by measuring their
distance in the vector space, the cosine of the angle
between them:
</bodyText>
<equation confidence="0.9504595">
Sim(p1,p2) =p1·p2
||p1||·||p2||
</equation>
<bodyText confidence="0.996711666666667">
Armed with this similarity function, we apply the
K-means algorithm&apos; to partition the phones into
disjoint sets.
</bodyText>
<sectionHeader confidence="0.9721" genericHeader="method">
4 Plausible misrecognition variants
</sectionHeader>
<bodyText confidence="0.987286733333333">
For an input English sequence we generate top-k
pseudo-ASR outputs, that are added as a source
side of a synthetic phrase. Every ASR output that
we simulate is a plausible misrecognition that has
two distinguishing characteristics: it is acousti-
cally and linguistically confusable with the input
sequence. Former corresponds to phonetic simi-
larity and latter to distributional similarity of these
two phrases in corpus.
Given a reference string–a word or sequence
of words w in the source language, we generate
k-best hypotheses v. This can be modeled as a
weighted finite state transducer:
{v} = G o D−1 o T o D o {w} (1)
where
</bodyText>
<listItem confidence="0.99953175">
• D maps from words to pronunciations
• T is a phone confusion transducer
• D−1 maps from pronunciations to words
• G is an ASR language model
</listItem>
<bodyText confidence="0.917344666666667">
D maps words to their phonetic representation5,
or multiple representations for words with several
&apos;Value of K=12 was determined empirically.
</bodyText>
<footnote confidence="0.9904885">
5Using the CMU pronounciation dictionary
http://www.speech.cs.cmu.edu/cgi-bin/cmudict
</footnote>
<page confidence="0.98946">
618
</page>
<bodyText confidence="0.99934025">
pronunciation variants. To create a phone con-
fusion transducer T maps source to target phone
sequences by performing a number of edit opera-
tions. Allowed edits are:
</bodyText>
<listItem confidence="0.9738117">
• Deletion of a consonant (mapping to E).
• Doubling of a vowel.
• Insertion of one or two phones in the end of a
sequence from the list of possible suffixes: S
(-s), IX NG (-ing), D (-ed).
• Substitution of a phone by an acousti-
cally similar phone. The clusters of the
similar phones are {Z, S}, {XL, L, R},
{AA, AO, EY, UH}, {AXR, AX}, {XN, XM},
{P, B, F}, {DH, CH, ZH, T, SH}, {OY, AE},
</listItem>
<bodyText confidence="0.999617392857143">
{IY, AY, OW}, {EH, AH, IH, AW, ER, UW}.
The phone clustering algorithm that pro-
duced these is detailed in the previous
section.
After a series of edit operations, D−1 trans-
ducer maps new phonetic sequences from pronun-
ciations to n-grams of words. The k-best variants
resulting from the weighted composition are the
k-best plausible misrecognitions.
One important property of this method is that it
maps words in decoding vocabulary (41,487 types
are possible inputs to transducer D) into CMU
dictionary which is substantially larger (141,304
types are possible outputs of transducer D−1).
This allows to generate out-of-vocabulary (OOV)
words and phrases, which are not only recogni-
tion errors, but also plausible variants of different
source phrases that can be translated to one tar-
get phrase, e.g., verb past tense forms or function
words.
Consider a bigram tells the from our synthetic
phrase example in Figure 2. We first obtain
its phonetic representation [T EH L Z] [DH IY],
and then a sequence of possible edit operations
is Substitute(T, CH), Substitute(Z, S), Delete(DH)
and translation of phonetic sequence [CH EH L S
IY] back to words brings us to chelsea. See Fig-
ure 4 for visualization.
</bodyText>
<sectionHeader confidence="0.995249" genericHeader="method">
5 Experimental setups
</sectionHeader>
<bodyText confidence="0.963784041666666">
To establish the effectiveness and ro-
bustness of our approach, we conducted
two sets of experiments—expASR and
expMultilingual—with transcribed and
tells the T EH L Z DH IY
chelsea CH EH L S IY
Figure 4: Pseudo-ASR output generation exam-
ple for a bigram tells the. Phonetic edits are
Substitute(T, CH), Substitute(Z, S), Delete(DH).
translated TED talks (Cettolo et al., 2012b).6 En-
glish is the source language in all the experiments.
In expASR we used tst2011–the official test
set of the SLT track of the IWSLT 2011 evalu-
ation campaign on the English-French language
pair (Federico et al., 2011).7 This test set com-
prises reference transcriptions of 8 talks (approx-
imately 1.1h of speech, segmented to 818 utter-
ances), 1-best hypotheses from five different ASR
systems, a ROVER combination of four systems
(Fiscus, 1997), and three sets of lattices produced
by the participants of the IWSLT 2011 ASR track.
In this set of experiments we compare baseline
systems performance to a performance of systems
augmented with synthetic phrases on (1) reference
transcriptions, (2) 1-best hypotheses from all re-
leased ASR systems, and (3) a set of ASR lattices
produced by FBK (Ruiz et al., 2011).8 Experi-
ments with individual systems are aimed to val-
idate that MT augmented with synthetic phrases
can better translate ASR outputs with recogni-
tion errors and sequences that were not observed
in the MT training data. Consistency in perfor-
mance across different ASRs is expected if our ap-
proach to generate plausible misrecognition vari-
ants is universal, rather than biased to a specific
system. Comparison of 1-best system with syn-
thetic phrases to lattice decoding setup without
synthetic phrases should demonstrate whether n-
best plausible misrecognition variants that we gen-
erate assemble multiple paths through a lattice.
The purpose of expMultilingual is to
show that translation improvement is consistent
across different target languages. This multilin-
gual experiment is interesting because typologi-
cally different languages pose different challenges
to translation (degree and locality of reordering,
morphological richness, etc.). By showing that
we improve results across languages (even with
</bodyText>
<footnote confidence="0.99867825">
6http://www.ted.com/
7http://iwslt2011.org/doku.php?id=06_evaluation#slt_track_
english_to_french
8Pruning threshold for lattices is 0.08.
</footnote>
<page confidence="0.996457">
619
</page>
<bodyText confidence="0.99899778125">
the same underlying ASR system), we show that
our technique is robust to the different demands
that languages place on the translation model. We
could not find any publicly available multilingual
data sets of the translated speech,9 therefore we
constructed a new test set.
We use our in-house speech recognizer and
evaluate on locally crawled and pre-processed
TED audio and text data. We build SLT systems
for five target languages: French, German, Rus-
sian, Hebrew, and Hindi. Consequently, our test
systems are diverse typologically and trained on
corpora of different sizes. We sample a test set of
seven talks, representing approximately two hours
of English speech, for which we have translations
to all five languages;10 talks are listed in Table 1.
Due to segmentation differences in the released
TED (text) corpora and then several automatic
preprocessing stages, numbers of sentences for
the same talks are not identical across languages.
Therefore, we select English-French system as an
oracle (this is the largest dataset), and first align it
with the ASR output. Then, we filter out test sets
for non-French MT systems, to retain only sen-
tence pairs that are included in the English-French
test set. Thus, our test sets for non-French MT
systems are smaller, and source-side sentences in
the English-French MT is a superset of source-side
sentences in all five languages. Training, tuning,
and test corpora sizes are listed in Table 2. Same
training and development sets were used in both
expASR and expMultilingual experiments.
</bodyText>
<table confidence="0.999765333333333">
Training Dev Test
EN–FR 140,816 2,521 843
EN–DE 130,010 2,373 501
EN–RU 117,638 2,380 735
EN–HE 135,366 2,501 540
EN–HI 126,117 2,000 300
</table>
<tableCaption confidence="0.966507">
Table 2: Number of sentences in training, dev and
expMultilingual test corpora.
</tableCaption>
<subsectionHeader confidence="0.894726">
5.1 ASR
</subsectionHeader>
<bodyText confidence="0.967462894736842">
In the expMultilingual set of experiments,
we employ the JANUS Recognition Toolkit that
features the IBIS single pass decoder (Soltau et
9After we conducted our experiments, a new multilingual
parallel corpus of translated speech was released for SLT
track of IWSLT 2013 Evaluation Campaign, however, this
data set does not include Russian, Hebrew and Hindi, which
are a subject of this research.
10Since TED translation is a voluntary effort, not all talks
are available in all languages.
al., 2001). The acoustic model is maximum
likelihood system, no speaker adaptation or dis-
criminative training applied. The acoustic model
training data is 186h of Broadcast News-style
data. 5-gram language model with modified
Kneser-Ney smoothing is trained with the SRILM
toolkit (Stolcke, 2002) on the EPPS, TED, News-
Commentary, and the Gigaword corpora. The
Broadcast News test set contains 4h of audio; we
obtain 25.6% word error rate (WER) on this test
set.
We segment the TED test audio by the times-
tamps of transcripts appearance on the screen.
Then, we manually detect and discard noisy hy-
potheses around segmentation boundaries, and
manually align the remaining hypotheses with
the references which are the source side of the
English-French MT test set. The resulting test
set of 843 hypotheses, sentence aligned with tran-
scripts, yields 30.7% WER. Higher error rates (rel-
atively to the Broadcast News baseline) can be
explained by the idiosyncratic nature of the TED
genre, and the fact that our ASR system was not
trained on the TED data.
For the expASR set of experiments the ASR
outputs and lattices in standard lattice format
(SLF) were produces by the participants of IWSLT
2011 evaluation campaign.
</bodyText>
<subsectionHeader confidence="0.978897">
5.2 MT
</subsectionHeader>
<bodyText confidence="0.999989833333333">
We train and test MT using the TED corpora in
all five languages. For French, German and Rus-
sian we use sentence-aligned training and develop-
ment sets (without our test talks) released for the
IWSLT 2012 evaluation campaign (Cettolo et al.,
2012a); we split Hebrew and Hindi to training and
development respectively.11 We split Hebrew and
Hindi to sentences with simple heuristics, and then
sentence-align with the Microsoft Bilingual Sen-
tence Aligner (Moore, 2002). Punctuation marks
were removed, corpora were lowercased, and tok-
enized using the cdec scripts (Dyer et al., 2010).
In all MT experiments, both for sentence and
lattice translation, we employ the Moses toolkit
(Koehn et al., 2007), implementing the phrase-
based statistical MT model (Koehn et al., 2003)
and optimize parameters with MERT (Och, 2003).
Target language 3-gram Kneser-Ney smoothed
</bodyText>
<footnote confidence="0.61818875">
11Since TED Hindi corpus is very small (only about 6K
sentences) we augment it with additional parallel data (Bojar
et al., 2010); however, this improved Hindi system quality
only marginally, probably owing to domain mismatch.
</footnote>
<page confidence="0.995691">
620
</page>
<note confidence="0.984172125">
TED id TED talk
1 Al Gore, 15 Ways to Avert a Climate Crisis, 2006
39 Aubrey de Grey: A roadmap to end aging, 2005
142 Alan Russell: The potential of regenerative medicine, 2006
228 Alan Kay shares a powerful idea about ideas, 2007
248 Alisa Miller: The news about the news, 2008
451 Bill Gates: Mosquitos, malaria and education, 2009
535 Al Gore warns on latest climate trends, 2009
</note>
<tableCaption confidence="0.999758">
Table 1: Test set of TED talks.
</tableCaption>
<bodyText confidence="0.9996889375">
language models are trained on the training part
of each corpus. Results are reported using case-
insensitive BLEU with a single reference and no
punctuation (Papineni et al., 2002). To verify
that our improvements are consistent and are not
just an effect of optimizer instability (Clark et al.,
2011), we train three systems for each MT setup.
Statistical significance is measured with the Mul-
tEval toolkit.12 Reported BLEU scores are aver-
aged over three systems.
In MT adaptation experiments we augment
baseline phrase tables with synthetic phrases. For
each entry in the original phrase table we add (at
most) five13 best acoustic confusions, detailed in
Section 4. Table 3 contains sizes of phrase tables,
original and augmented with synthetic phrases.
</bodyText>
<table confidence="0.998551">
Original Synthetic
EN–FR 4,118,702 24,140,004
EN–DE 2,531,556 14,807,308
EN–RU 1,835,553 10,743,818
EN–HE 2,169,397 12,692,641
EN–HI 478,281 2,674,025
</table>
<tableCaption confidence="0.989132">
Table 3: Sizes of phrase tables from the baseline
systems, and phrase tables with synthetic phrases.
</tableCaption>
<sectionHeader confidence="0.998355" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.897573">
6.1 expASR
</subsectionHeader>
<bodyText confidence="0.999885">
We first measure the phrasal coverage of recog-
nition errors that our technique is able to predict.
We compute a number of 1- and 2-gram phrases
in ASR hypotheses from the tst2011 that are
not in the references: these are ASR errors. Then,
we compare their OOV rate in the English-French
phrase tables, original vs. synthetic. The pur-
pose of synthetic phrases is to capture misrecog-
nized sequences, ergo, reduction in OOV rate of
</bodyText>
<footnote confidence="0.5240042">
12https://github.com/jhclark/multeval
13This threshold is of course rather arbitrary. In future ex-
periments we are planning to conduct an in-depth investiga-
tion of the threshold value, based on ASR LM score and pho-
netic distance from the original phrase.
</footnote>
<bodyText confidence="0.996729125">
ASR errors in synthetic phrase tables corresponds
to the portion of errors that our method was able
to predict. Table 4 shows that the OOV rate of n-
grams in phrase tables augmented with synthetic
phrases drops dramatically, up to 54%. Consis-
tent reduction of recognized errors across outputs
from five different ASR systems confirms that our
error-prediction approach is ASR-independent.
</bodyText>
<table confidence="0.972955">
tst2011 #1-grams #2-grams
system0 29 (50.9%) 230 (20.3%)
system1 27 (41.5%) 234 (21.3%)
system2 36 (36.0%) 230 (20.1%)
system3 34 (44.1%) 275 (20.1%)
system4 46 (52.9%) 182 (16.8%)
ROVER 30 (54.5%) 183 (18.7%)
</table>
<tableCaption confidence="0.935792">
Table 4: Phrasal coverage of recognition errors
</tableCaption>
<bodyText confidence="0.997027807692308">
that our technique is able to predict. These are
raw counts of 1-gram and 2-gram types that are
OOVs in the baseline system and are recovered
by our method when we augment the system with
plausible misrecognitions. Percentages in paren-
theses show OOV rate reduction due to recovered
n-grams.
Next, we explore the effect of synthetic phrases
on translation performance, across different (1-
best) ASR outputs. For references, ASR hypothe-
ses, and ROVERed hypotheses we compare trans-
lations produced by MT systems trained with and
without synthetic phrases. We detail our findings
in Table 5.
Improvements in translation are significant for
all systems with synthetic phrases. This experi-
ment corroborates the underlying assumption that
simulated ASR errors are paired with correct tar-
get phrases. Moreover, this experiment supports
the claim that incorporating noisier translations in
the translation model successfully adapts MT to
SLT scenario and has indeed a positive effect on
speech translation. Interestingly, improvement of
reference translations is also observed. We spec-
ulate that this stems from better lexical selection
due to a smoothing effect that our technique may
</bodyText>
<page confidence="0.997109">
621
</page>
<table confidence="0.999867555555556">
WER BLEU BLEU P
Baseline Synthetic
references - 30.8 31.2 0.05
system0 22.0 24.3 25.0 &lt;0.01
system1 23.3 23.8 24.3 &lt;0.01
system2 21.1 23.9 24.4 0.02
system3 32.4 20.8 21.3 &lt;0.01
system4 19.5 24.5 25.0 0.01
ROVER 17.4 25.0 25.6 0.01
</table>
<tableCaption confidence="0.992165">
Table 5: Comparison of the baseline translation
</tableCaption>
<bodyText confidence="0.964124458333333">
systems with the systems augmented with syn-
thetic phrases. We measure EN–FR MT perfor-
mance on the tst2011 test set: reference tran-
scripts and ASR outputs on from five systems
and their ROVER combination. Improvements in
translation of all ASR outputs are statistically sig-
nificant. This confirms the claim that incorporat-
ing simulated ASR errors via synthetic phrases ef-
fectively adapts MT to SLT scenario.
have.
Finally, we contrast the proposed approach of
translation models adaptation to a conventional
method of lattice translation. We decode FBK lat-
tices produced for IWSLT 2011 Evaluation Cam-
paign, and compare results to FBK 1-best transla-
tion results, which correspond to system1 in Table
5. Table 6 summarizes our main finding: 1-best
system with synthetic phrases significantly outper-
forms lattice decoding setup with baseline trans-
lation table.14 The additional small improvement
in lattice decoding with synthetic phrases suggests
that lattice decoding and phrase table adaptation
are two complementary strategies and their com-
bination is beneficial.
</bodyText>
<subsectionHeader confidence="0.995804">
6.2 expMultilingual
</subsectionHeader>
<bodyText confidence="0.748879125">
In the multilingual experiment we train ten MT se-
tups: five baseline setups and five systems with
synthetic phrases, three systems per setup. For
each system we compare translations of the refer-
ence transcripts and ASR hypotheses on the multi-
lingual test set described in Section 6. We evaluate
translations produced by MT systems trained with
and without synthetic phrases. Table 7 summa-
rizes experimental results, along with the test set
WER for each language.
14Automatic evaluation results (in terms of BLEU) pub-
lished during the IWSLT 2011 Evaluation Campaign (Fed-
erico et al., 2011) (p. 21) are 26.1 for FBK systems. Unsur-
prisingly, performance of our systems is lower, as we focus
only on translation table and do not optimize factors, such as
LMs and others.
</bodyText>
<figure confidence="0.65418225">
BLEU BLEU
Baseline Synthetic
23.8 24.3
24.0 24.4
</figure>
<tableCaption confidence="0.686476333333333">
Table 6: Comparison of the baseline EN–FR trans-
lation systems with the systems augmented with
synthetic phrases, in 1-best and lattice decoding
setups. 1-best synthetic system significantly out-
performs baseline lattice decoding setup. Addi-
tional improvement in lattice decoding with syn-
thetic phrases suggests that lattice decoding and
phrase table adaptation are two complementary
strategies.
</tableCaption>
<table confidence="0.999776142857143">
WER Baseline Synthetic
Ref ASR Ref ASR
EN–FR 30.7 23.3 17.8 23.9 18.1
EN–DE 33.6 14.0 11.1 14.2 11.4
EN–RU 30.7 12.3 10.7 12.2 10.6
EN–HE 29.7 9.2 7.0 9.5 7.2
EN–HI 32.1 5.5 4.5 5.6 4.8
</table>
<tableCaption confidence="0.996736">
Table 7: Comparison of the baseline translation
</tableCaption>
<bodyText confidence="0.990629263157895">
systems with the systems augmented with syn-
thetic phrases. We measure MT performance on
the reference transcripts and ASR outputs. Con-
sistent improvements are observed in four out of
five languages.
Modest but consistent improvements are ob-
served in four out of five setups with synthetic
phrases. Only French setup yielded statistically
significant improvement (p &lt; .01). However,
if we concatenate the outputs of all languages,
the improvement in translation of references with
BLEU score averaged over all systems becomes
statistically significant (p = .03), improving from
16.8 for the baseline system to 17.3 for the adapted
MT outputs. While more careful evaluation is re-
quired in order to estimate the effect of acous-
tic confusions, the accumulated result show that
synthetic phrases facilitate MT adaptation to SLT
across languages.
</bodyText>
<sectionHeader confidence="0.970195" genericHeader="method">
7 Analysis
</sectionHeader>
<bodyText confidence="0.998965875">
We conducted careful manual analysis of actual
usages of synthetic phrases in translation. The pur-
pose of this qualitative analysis is to verify that
predicted ASR errors are paired with phrases that
contribute to better translation to a target language.
Table 8 shows some examples. In the first sentence
from the tst2011 test set (output from system 4)
the word area was erroneously recognized as airy,
</bodyText>
<figure confidence="0.2377315">
FBK 1-best
FBK lattices
</figure>
<page confidence="0.760161">
622
</page>
<table confidence="0.9966775">
English ref so what they do is they move into an area
ASR output so what they do is they move into an airy
Baseline MT donc ce qu’ils font c’est qu’ils se déplacer dans un airy
Synthetic MT donc ce qu’ils font c’est qu’ils se déplacer dans une zone
French ref donc ce qu’ils font c’est qu’ils emménagent dans une zone
English ref so i started thinking and listing what all it was that i thought would make a perfect biennial
ASR output so on i started a thinking and listing was all it was that i thought would make a pretty by neil
Baseline MT donc j’ai commencé à une pensée et listing était tout c’était que je pensais ferait un assez par neil
Synthetic MT donc j’ai commencé à penser et une liste était tout c’était que je pensais ferait un assez par neil
French ref alors j’ai commencé à penser et à lister tout ce qui selon moi ferait une biennale parfaite
</table>
<tableCaption confidence="0.999714">
Table 8: Examples of translations improved with synthetic phrases.
</tableCaption>
<bodyText confidence="0.9998896875">
which is an OOV word for the baseline system.
Our confusion generation algorithm also produced
the word airy as a plausible misrecognition variant
for the word area and attached it to a correct tar-
get phrase zone, and this synthetic phrase was se-
lected during decoding, yielding to a correct trans-
lation for the ASR error. Second example shows a
similar behavior for an indefinite article a. Third
example is taken from the English-Russian system
in the multilingual test set. Gauge was produced
as a plausible misrecognition variant to age, and
therefore correctly translated (albeit incorrectly in-
flected) as B03pacTa(age+sg+m+acc). Synthetic
phrases were also used in translations contain-
ing misrecognized function words, segmentation-
related examples, and longer n-grams.
</bodyText>
<sectionHeader confidence="0.999482" genericHeader="method">
8 Related work
</sectionHeader>
<bodyText confidence="0.999992880952381">
Predicting ASR errors to improve speech recog-
nition quality has been explored in several previ-
ous studies. Jyothi and Fosler-Lussier (2009) de-
velop weighted finite-state transducer framework
for error prediction. They build a confusion ma-
trix FST between phones to model acoustic errors
made by the recognizer. Costs in the confusion
matrix combine acoustic variations in the HMM
representations of the phones (information from
the acoustic model) and word-based phone confu-
sions (information from the pronunciation model).
In their follow-up work, Jyothi and Fosler-Lussier
(2010) employ this error-predicting framework to
train the parameters of a global linear discrimina-
tive language model that improves ASR.
Sagae et al. (2012) examined three protocols
for ‘hallucinating’ ASR n-best lists. First ap-
proach generates confusions on the phone level,
with a phone-based finite-state transducer that em-
ploys real n-best lists produced by the ASR sys-
tem. Second is generating confusions at the word
level with a MT-based approach. Third is a phrasal
cohorts approach, in which acoustically confus-
able phrases are extracted from ASR n-best lists,
based on pivots–identical left and right contexts of
a phrase. All three methods were evaluated on the
task of ASR improvement through decoding with
discriminative language models. Discriminative
language models trained on simulated n-best lists
produced with phrasal cohorts method yielded the
largest WER reduction on the telephone speech
recognition task.
Our approach to generating plausible ASR mis-
recognitions is similar to previously explored FST-
based methods. The fundamental difference, how-
ever, is in speech-free phonetic confusion trans-
ducer that does not employ any data extracted
from acoustic models or ASR outputs. Simulated
ASR errors are typically used to improve ASR ap-
plications. To the best of our knowledge no prior
work has been done on integrating ASR errors di-
rectly in the translation models.
</bodyText>
<sectionHeader confidence="0.996767" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999962444444444">
The idea behind the novel ASR error-prediction
algorithm that we devise is to identify phonolog-
ical neighbors with similar distributional proper-
ties, i.e. similar sounding words for which lan-
guage model probabilities are insufficient for their
disambiguation. These sequences have been iden-
tified as significant contributors to ASR errors
(Goldwater et al., 2010). Additional and even
more important factors that cause recognition er-
rors are disfluencies in speech (Tsvetkov et al.,
2013b). In the task of adapting MT to SLT these
and other irregularities can effectively be incor-
porated in a useful general framework: synthetic
phrases that augment phrase tables. Our exper-
iments show that simulated acoustic confusions
capture real ASR errors and that proposed frame-
work effectively exploits them to improve transla-
tion.
</bodyText>
<page confidence="0.999">
623
</page>
<sectionHeader confidence="0.998332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996779166666667">
We are grateful to João Miranda and Alan Black for providing
us the TED audio with transcriptions, and to Zaid Sheikh for
his help with ASR decoding. This work was supported in part
by the U. S. Army Research Laboratory and the U. S. Army
Research Office under contract/grant number W911NF-10-1-
0533.
</bodyText>
<sectionHeader confidence="0.998396" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999663421052632">
Waleed Ammar, Victor Chahuneau, Michael
Denkowski, Greg Hanneman, Wang Ling, Austin
Matthews, Kenton Murray, Nicola Segall, Yulia
Tsvetkov, Alon Lavie, and Chris Dyer. 2013.
The CMU machine translation systems at WMT
2013: Syntax, synthetic translation options, and
pseudo-references.
Nicola Bertoldi, Richard Zens, and Marcello Federico.
2007. Speech translation by confusion network de-
coding. In Proc. ICASSP, pages 1297–1300. IEEE.
Ondrej Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in English-to-Hindi machine translation.
In Proceedings of LREC.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine
translation using paraphrases. In Proceedings of
HLT/NAACL, pages 17–24. Association for Compu-
tational Linguistics.
Francisco Casacuberta, Marcello Federico, Hermann
Ney, and Enrique Vidal. 2008. Recent efforts
in spoken language translation. Signal Processing
Magazine, IEEE, 25(3):80–88.
Mauro Cettolo, Marcello Federico, Luisa Bentivogli,
Michael Paul, and Sebastian Stüker. 2012a.
Overview of the IWSLT 2012 evaluation campaign.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012b. WIT3: Web inventory of transcribed
and translated talks. In Proceedings of EAMT, pages
261–268, Trento, Italy.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proceed-
ings of EMNLP.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of ACL, pages
176–181. Association for Computational Linguis-
tics.
Chris Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-08: HLT, pages 1012–1020. Asso-
ciation for Computational Linguistics.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings ofACL.
Marcello Federico, Luisa Bentivogli, Michael Paul,
and Sebastian Stüker. 2011. Overview of the
IWSLT 2011 evaluation campaign. In Proc. IWSLT,
pages 8–9.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recognizer output
voting error reduction (ROVER). In Proc. ASRU,
pages 347–352. IEEE.
Sharon Goldwater, Dan Jurafsky, and Christopher D
Manning. 2010. Which words are hard to rec-
ognize? prosodic, lexical, and disfluency factors
that increase speech recognition error rates. Speech
Communication, 52(3):181–200.
Xiaodong He and Li Deng. 2013. Speech-centric in-
formation processing: An optimization-oriented ap-
proach. IEEE, 101(5):1116–1135.
Dan Jurafsky and James H Martin. 2000. Speech &amp;
Language Processing. Pearson Education India.
Preethi Jyothi and Eric Fosler-Lussier. 2009. A com-
parison of audio-free speech recognition error pre-
diction methods. In Proc. INTERSPEECH, pages
1211–1214.
Preethi Jyothi and Eric Fosler-Lussier. 2010. Discrimi-
native language modeling using simulated asr errors.
In Proc. INTERSPEECH, pages 1049–1052.
Stanislav V Kasl and George F Mahl. 1965. The re-
lationship of disturbances and hesitations in spon-
taneous speech to anxiety. In Journal of Personal-
ity and Social Psychology, volume 1(5), pages 425–
433.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL, pages 48–54. Association
for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL, pages 177–180. Asso-
ciation for Computational Linguistics.
Evgeny Matusov, Stephan Kanthak, and Hermann Ney.
2006. Integrating speech recognition and machine
translation: Where do we stand? In Proc. ICASSP,
pages V–1217–V–1220. IEEE.
</reference>
<page confidence="0.986289">
624
</page>
<reference confidence="0.999947240963855">
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings
of AMTA, pages 135–144, London, UK. Springer-
Verlag.
Hermann Ney. 1999. Speech translation: Coupling of
recognition and translation. In Proc. ICASSP, vol-
ume 1, pages 517–520. IEEE.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160–167, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase lattice for statistical machine
translation. In Proceedings of ACL.
Mari Ostendorf, Benoît Favre, Ralph Grishman, Dilek
Hakkani-Tur, Mary Harper, Dustin Hillard, Julia
Hirschberg, Heng Ji, Jeremy G Kahn, Yang Liu,
Sameer Maskey, Evgeny Matusov, Hermann Ney,
Andrew Rosenberg, Elizabeth Shriberg, Wen Wang,
and Chuck Wooters. 2008. Speech segmentation
and spoken document processing. Signal Process-
ing Magazine, IEEE, 25(3):59–69.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311–318. Association for Computa-
tional Linguistics.
Stephan Peitz, Simon Wiesler, Markus Nußbaum-
Thom, and Hermann Ney. 2012. Spoken language
translation using automatically transcribed text in
training. In Proc. IWSLT.
Vu H Quan, Marcello Federico, and Mauro Cettolo.
2005. Integrated n-best re-ranking for spoken lan-
guage translation. In Proc. INTERSPEECH, pages
3181–3184. IEEE.
Nick Ruiz, Arianna Bisazza, Fabio Brugnara, Daniele
Falavigna, Diego Giuliani, Suhel Jaber, Roberto
Gretter, and Marcello Federico. 2011. FBK@
IWSLT 2011. In Proc. IWSLT.
Kenji Sagae, M. Lehr, E. Prud’hommeaux, P. Xu,
N. Glenn, D. Karakos, S. Khudanpur, B. Roark,
M. Saraçlar, I. Shafran, D. Bikel, C. Callison-Burch,
Y. Cao, K. Hall, E. Hasler, P. Koehn, A. Lopez,
M. Post, and D. Riley. 2012. Hallucinated n-best
lists for discriminative language modeling. In Proc.
ICASSP. IEEE.
H. Soltau, F. Metze, C. Fügen, and A. Waibel. 2001.
A one-pass decoder based on polymorphic linguistic
context assignment. In Proc. ASRU.
Andreas Stolcke. 2002. SRILM—an extensible lan-
guage modeling toolkit. In Proc. ICSLP, pages 901–
904.
Jean E Fox Tree. 1995. The effects of false starts and
repetitions on the processing of subsequent words in
spontaneous speech. Journal of memory and lan-
guage, 34(6):709–738.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Bhatia. 2013a. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of WMT. Association for
Computational Linguistics.
Yulia Tsvetkov, Zaid Sheikh, and Florian Metze.
2013b. Identification and modeling of word frag-
ments in spontaneous speech. In Proc. ICASSP.
IEEE.
Ping Xu, Pascale Fung, and Ricky Chan. 2012.
Phrase-level transduction model with reordering for
spoken to written language transformation. In Proc.
ICASSP, pages 4965–4968. IEEE.
Ruiqiang Zhang, Genichiro Kikui, Hirofumi Ya-
mamoto, Taro Watanabe, Frank Soong, and Wai Kit
Lo. 2004. A unified approach in speech-to-speech
translation: integrating features of speech recog-
nition and machine translation. In Proceedings
of COLING, page 1168. Association for Computa-
tional Linguistics.
Bowen Zhou, Laurent Besacier, and Yuqing Gao.
2007. On efficient coupling of ASR and SMT for
speech translation. In Proc. ICASSP, volume 4,
pages IV–101. IEEE.
Bowen Zhou. 2013. Statistical machine translation for
speech: A perspective on structures, learning, and
decoding. IEEE, 101(5):1180–1202.
</reference>
<page confidence="0.998799">
625
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.489525">
<title confidence="0.8217775">Augmenting Translation Models with Simulated Acoustic Confusions Improved Spoken Language Translation Yulia Tsvetkov Florian Metze Chris Language Technologies</title>
<affiliation confidence="0.975054">Carnegie Mellon</affiliation>
<address confidence="0.996459">Pittsburgh, PA 15213;</address>
<email confidence="0.998161">ytsvetko@cs.cmu.edu</email>
<email confidence="0.998161">fmetze@cs.cmu.edu</email>
<email confidence="0.998161">cdyer@cs.cmu.edu</email>
<abstract confidence="0.9973728">We propose a novel technique for adapting text-based statistical machine translation to deal with input from automatic speech recognition in spoken language translation tasks. We simulate likely misrecognition errors using only a source language pronunciation dictionary and language model (i.e., without an acoustic model), and use these to augment the phrase table of a standard MT system. The augmented system can thus recover from recognition errors during decoding using synthesized phrases. Using the outputs of five different English ASR systems as input, we find consistent and significant improvements in translation quality. Our proposed technique can also be used in conjunction with lattices as ASR output, leading to further improvements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Waleed Ammar</author>
<author>Victor Chahuneau</author>
<author>Michael Denkowski</author>
<author>Greg Hanneman</author>
<author>Wang Ling</author>
</authors>
<title>The CMU machine translation systems at WMT 2013: Syntax, synthetic translation options, and pseudo-references.</title>
<date>2013</date>
<location>Austin Matthews, Kenton Murray, Nicola Segall, Yulia</location>
<contexts>
<context position="6400" citStr="Ammar et al., 2013" startWordPosition="959" endWordPosition="962">ut the algorithm can in principle be applied without pronunciation dictionary for languages with a phonemic orthography. 2 Methodology We adopt a standard ASR-MT cascading approach and then augment translation models with synthetic phrases. Our proposed system architecture is depicted in Figure 1. Synthetic phrases are generated from entries in the original translation model–phrase translation 2We augment phrase tables only with synthetic phrases that capture simulated ASR errors, the methodology that we advocate, however, is applicable to many problems in translation (Tsvetkov et al., 2013a; Ammar et al., 2013; Chahuneau et al., 2013). Figure 1: SLT architecture: ASR and MT are trained independently and then cascaded. We improve SLT by populating MT translation model with synthetic phrases. Each synthetic phrase is a variant of an original phrase pair with simulated ASR errors on the source side. pairs acquired from parallel data. From a source side of an original phrase pair we generate list of its plausible misrecognition variants (pseudo-ASR outputs with recognition errors) and add them as a source side of a synthetic phrase. For k-best simulated ASR outputs we construct k synthetic phrases: a s</context>
</contexts>
<marker>Ammar, Chahuneau, Denkowski, Hanneman, Ling, 2013</marker>
<rawString>Waleed Ammar, Victor Chahuneau, Michael Denkowski, Greg Hanneman, Wang Ling, Austin Matthews, Kenton Murray, Nicola Segall, Yulia Tsvetkov, Alon Lavie, and Chris Dyer. 2013. The CMU machine translation systems at WMT 2013: Syntax, synthetic translation options, and pseudo-references.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Richard Zens</author>
<author>Marcello Federico</author>
</authors>
<title>Speech translation by confusion network decoding.</title>
<date>2007</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>1297--1300</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3294" citStr="Bertoldi et al., 2007" startWordPosition="492" endWordPosition="495">raining an MT system on ASR outputs (rather than the usual writtenstyle texts) would improve matters. Unfortunately, there are few corpora of speech paired with text translations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontaneous speech, not including silent pauses (Tree, 1995; Kasl and Mahl, 1965) 616 Proceedings of the 14th Conference o</context>
</contexts>
<marker>Bertoldi, Zens, Federico, 2007</marker>
<rawString>Nicola Bertoldi, Richard Zens, and Marcello Federico. 2007. Speech translation by confusion network decoding. In Proc. ICASSP, pages 1297–1300. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Pavel Stranak</author>
<author>Daniel Zeman</author>
</authors>
<title>Data issues in English-to-Hindi machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="21063" citStr="Bojar et al., 2010" startWordPosition="3318" endWordPosition="3321"> heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser-Ney smoothed 11Since TED Hindi corpus is very small (only about 6K sentences) we augment it with additional parallel data (Bojar et al., 2010); however, this improved Hindi system quality only marginally, probably owing to domain mismatch. 620 TED id TED talk 1 Al Gore, 15 Ways to Avert a Climate Crisis, 2006 39 Aubrey de Grey: A roadmap to end aging, 2005 142 Alan Russell: The potential of regenerative medicine, 2006 228 Alan Kay shares a powerful idea about ideas, 2007 248 Alisa Miller: The news about the news, 2008 451 Bill Gates: Mosquitos, malaria and education, 2009 535 Al Gore warns on latest climate trends, 2009 Table 1: Test set of TED talks. language models are trained on the training part of each corpus. Results are repor</context>
</contexts>
<marker>Bojar, Stranak, Zeman, 2010</marker>
<rawString>Ondrej Bojar, Pavel Stranak, and Daniel Zeman. 2010. Data issues in English-to-Hindi machine translation. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4118" citStr="Callison-Burch et al., 2006" startWordPosition="622" endWordPosition="625">low a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontaneous speech, not including silent pauses (Tree, 1995; Kasl and Mahl, 1965) 616 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 616–625, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics deal with ASR output (Callison-Burch et al., 2006; Tsvetkov et al., 2013a). We address speech translation in a resourcedeficient scenario, specifically, adapting MT systems to SLT when ASR is unavailable. We augment a discriminative set that translation models rescore with synthetic translation options. These automatically generated translation rules (henceforth synthetic phrases) are noisy variants of observed translation rules with simulated plausible speech recognition errors (§2). To simulate ASR errors we generate acoustically and distributionally similar phrases to a source (English) phrase with a phonologically-motivated algorithm (§4</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In Proceedings of HLT/NAACL, pages 17–24. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Casacuberta</author>
<author>Marcello Federico</author>
<author>Hermann Ney</author>
<author>Enrique Vidal</author>
</authors>
<title>Recent efforts in spoken language translation.</title>
<date>2008</date>
<booktitle>Signal Processing Magazine, IEEE,</booktitle>
<pages>25--3</pages>
<contexts>
<context position="1471" citStr="Casacuberta et al., 2008" startWordPosition="208" endWordPosition="211">ind consistent and significant improvements in translation quality. Our proposed technique can also be used in conjunction with lattices as ASR output, leading to further improvements. 1 Introduction Spoken language translation (SLT) systems generally consist of two components: (i) an automatic speech recognition (ASR) system that transcribes source language utterances and (ii) a machine translation (MT) system that translates the transcriptions into the target language. These two components are usually developed independently and then combined and integrated (Ney, 1999; Matusov et al., 2006; Casacuberta et al., 2008; Zhou, 2013; He and Deng, 2013). While this architecture is attractive since it relies only on components that are independently useful, such systems face several challenges. First, spoken language tends to be quite different from the highly edited parallel texts that are available to train translation systems. For example, disfluencies, such as repeated words or phrases, restarts, and revisions of content, are frequent in spontaneous speech,1 while these are usually absent in written texts. In addition, ASR outputs typically lack explicit segmentation into sentences, as well as reliable casi</context>
<context position="3321" citStr="Casacuberta et al., 2008" startWordPosition="496" endWordPosition="499"> ASR outputs (rather than the usual writtenstyle texts) would improve matters. Unfortunately, there are few corpora of speech paired with text translations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontaneous speech, not including silent pauses (Tree, 1995; Kasl and Mahl, 1965) 616 Proceedings of the 14th Conference of the European Chapter of t</context>
</contexts>
<marker>Casacuberta, Federico, Ney, Vidal, 2008</marker>
<rawString>Francisco Casacuberta, Marcello Federico, Hermann Ney, and Enrique Vidal. 2008. Recent efforts in spoken language translation. Signal Processing Magazine, IEEE, 25(3):80–88.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mauro Cettolo</author>
<author>Marcello Federico</author>
</authors>
<location>Luisa Bentivogli,</location>
<marker>Cettolo, Federico, </marker>
<rawString>Mauro Cettolo, Marcello Federico, Luisa Bentivogli,</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael Paul</author>
<author>Sebastian Stüker</author>
</authors>
<booktitle>2012a. Overview of the IWSLT 2012 evaluation campaign.</booktitle>
<marker>Paul, Stüker, </marker>
<rawString>Michael Paul, and Sebastian Stüker. 2012a. Overview of the IWSLT 2012 evaluation campaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>WIT3: Web inventory of transcribed and translated talks.</title>
<date>2012</date>
<booktitle>In Proceedings of EAMT,</booktitle>
<pages>261--268</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="14633" citStr="Cettolo et al., 2012" startWordPosition="2313" endWordPosition="2316">Y], and then a sequence of possible edit operations is Substitute(T, CH), Substitute(Z, S), Delete(DH) and translation of phonetic sequence [CH EH L S IY] back to words brings us to chelsea. See Figure 4 for visualization. 5 Experimental setups To establish the effectiveness and robustness of our approach, we conducted two sets of experiments—expASR and expMultilingual—with transcribed and tells the T EH L Z DH IY chelsea CH EH L S IY Figure 4: Pseudo-ASR output generation example for a bigram tells the. Phonetic edits are Substitute(T, CH), Substitute(Z, S), Delete(DH). translated TED talks (Cettolo et al., 2012b).6 English is the source language in all the experiments. In expASR we used tst2011–the official test set of the SLT track of the IWSLT 2011 evaluation campaign on the English-French language pair (Federico et al., 2011).7 This test set comprises reference transcriptions of 8 talks (approximately 1.1h of speech, segmented to 818 utterances), 1-best hypotheses from five different ASR systems, a ROVER combination of four systems (Fiscus, 1997), and three sets of lattices produced by the participants of the IWSLT 2011 ASR track. In this set of experiments we compare baseline systems performance</context>
<context position="20320" citStr="Cettolo et al., 2012" startWordPosition="3204" endWordPosition="3207">30.7% WER. Higher error rates (relatively to the Broadcast News baseline) can be explained by the idiosyncratic nature of the TED genre, and the fact that our ASR system was not trained on the TED data. For the expASR set of experiments the ASR outputs and lattices in standard lattice format (SLF) were produces by the participants of IWSLT 2011 evaluation campaign. 5.2 MT We train and test MT using the TED corpora in all five languages. For French, German and Russian we use sentence-aligned training and development sets (without our test talks) released for the IWSLT 2012 evaluation campaign (Cettolo et al., 2012a); we split Hebrew and Hindi to training and development respectively.11 We split Hebrew and Hindi to sentences with simple heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser</context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012b. WIT3: Web inventory of transcribed and translated talks. In Proceedings of EAMT, pages 261–268, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Chahuneau</author>
<author>Eva Schlinger</author>
<author>Noah A Smith</author>
<author>Chris Dyer</author>
</authors>
<title>Translating into morphologically rich languages with synthetic phrases.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6425" citStr="Chahuneau et al., 2013" startWordPosition="963" endWordPosition="966"> in principle be applied without pronunciation dictionary for languages with a phonemic orthography. 2 Methodology We adopt a standard ASR-MT cascading approach and then augment translation models with synthetic phrases. Our proposed system architecture is depicted in Figure 1. Synthetic phrases are generated from entries in the original translation model–phrase translation 2We augment phrase tables only with synthetic phrases that capture simulated ASR errors, the methodology that we advocate, however, is applicable to many problems in translation (Tsvetkov et al., 2013a; Ammar et al., 2013; Chahuneau et al., 2013). Figure 1: SLT architecture: ASR and MT are trained independently and then cascaded. We improve SLT by populating MT translation model with synthetic phrases. Each synthetic phrase is a variant of an original phrase pair with simulated ASR errors on the source side. pairs acquired from parallel data. From a source side of an original phrase pair we generate list of its plausible misrecognition variants (pseudo-ASR outputs with recognition errors) and add them as a source side of a synthetic phrase. For k-best simulated ASR outputs we construct k synthetic phrases: a simulated ASR output in th</context>
</contexts>
<marker>Chahuneau, Schlinger, Smith, Dyer, 2013</marker>
<rawString>Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013. Translating into morphologically rich languages with synthetic phrases. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>176--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21881" citStr="Clark et al., 2011" startWordPosition="3458" endWordPosition="3461">o end aging, 2005 142 Alan Russell: The potential of regenerative medicine, 2006 228 Alan Kay shares a powerful idea about ideas, 2007 248 Alisa Miller: The news about the news, 2008 451 Bill Gates: Mosquitos, malaria and education, 2009 535 Al Gore warns on latest climate trends, 2009 Table 1: Test set of TED talks. language models are trained on the training part of each corpus. Results are reported using caseinsensitive BLEU with a single reference and no punctuation (Papineni et al., 2002). To verify that our improvements are consistent and are not just an effect of optimizer instability (Clark et al., 2011), we train three systems for each MT setup. Statistical significance is measured with the MultEval toolkit.12 Reported BLEU scores are averaged over three systems. In MT adaptation experiments we augment baseline phrase tables with synthetic phrases. For each entry in the original phrase table we add (at most) five13 best acoustic confusions, detailed in Section 4. Table 3 contains sizes of phrase tables, original and augmented with synthetic phrases. Original Synthetic EN–FR 4,118,702 24,140,004 EN–DE 2,531,556 14,807,308 EN–RU 1,835,553 10,743,818 EN–HE 2,169,397 12,692,641 EN–HI 478,281 2,6</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of ACL, pages 176–181. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1012--1020</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3353" citStr="Dyer et al., 2008" startWordPosition="502" endWordPosition="505">tenstyle texts) would improve matters. Unfortunately, there are few corpora of speech paired with text translations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontaneous speech, not including silent pauses (Tree, 1995; Kasl and Mahl, 1965) 616 Proceedings of the 14th Conference of the European Chapter of the Association for Computational</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Chris Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of ACL-08: HLT, pages 1012–1020. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Johnathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="20655" citStr="Dyer et al., 2010" startWordPosition="3254" endWordPosition="3257">WSLT 2011 evaluation campaign. 5.2 MT We train and test MT using the TED corpora in all five languages. For French, German and Russian we use sentence-aligned training and development sets (without our test talks) released for the IWSLT 2012 evaluation campaign (Cettolo et al., 2012a); we split Hebrew and Hindi to training and development respectively.11 We split Hebrew and Hindi to sentences with simple heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser-Ney smoothed 11Since TED Hindi corpus is very small (only about 6K sentences) we augment it with additional parallel data (Bojar et al., 2010); however, this improved Hindi system quality only marginally, probably owing to domain mismatch. 620 TED id TED talk 1 Al Gore, 15 Ways to Avert a Climate Crisis, 2006 39 Aubrey de Grey: A ro</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Luisa Bentivogli</author>
<author>Michael Paul</author>
<author>Sebastian Stüker</author>
</authors>
<title>evaluation campaign.</title>
<date>2011</date>
<journal>Overview of the IWSLT</journal>
<booktitle>In Proc. IWSLT,</booktitle>
<pages>8--9</pages>
<contexts>
<context position="14855" citStr="Federico et al., 2011" startWordPosition="2351" endWordPosition="2354">n. 5 Experimental setups To establish the effectiveness and robustness of our approach, we conducted two sets of experiments—expASR and expMultilingual—with transcribed and tells the T EH L Z DH IY chelsea CH EH L S IY Figure 4: Pseudo-ASR output generation example for a bigram tells the. Phonetic edits are Substitute(T, CH), Substitute(Z, S), Delete(DH). translated TED talks (Cettolo et al., 2012b).6 English is the source language in all the experiments. In expASR we used tst2011–the official test set of the SLT track of the IWSLT 2011 evaluation campaign on the English-French language pair (Federico et al., 2011).7 This test set comprises reference transcriptions of 8 talks (approximately 1.1h of speech, segmented to 818 utterances), 1-best hypotheses from five different ASR systems, a ROVER combination of four systems (Fiscus, 1997), and three sets of lattices produced by the participants of the IWSLT 2011 ASR track. In this set of experiments we compare baseline systems performance to a performance of systems augmented with synthetic phrases on (1) reference transcriptions, (2) 1-best hypotheses from all released ASR systems, and (3) a set of ASR lattices produced by FBK (Ruiz et al., 2011).8 Experi</context>
<context position="27079" citStr="Federico et al., 2011" startWordPosition="4265" endWordPosition="4269">s beneficial. 6.2 expMultilingual In the multilingual experiment we train ten MT setups: five baseline setups and five systems with synthetic phrases, three systems per setup. For each system we compare translations of the reference transcripts and ASR hypotheses on the multilingual test set described in Section 6. We evaluate translations produced by MT systems trained with and without synthetic phrases. Table 7 summarizes experimental results, along with the test set WER for each language. 14Automatic evaluation results (in terms of BLEU) published during the IWSLT 2011 Evaluation Campaign (Federico et al., 2011) (p. 21) are 26.1 for FBK systems. Unsurprisingly, performance of our systems is lower, as we focus only on translation table and do not optimize factors, such as LMs and others. BLEU BLEU Baseline Synthetic 23.8 24.3 24.0 24.4 Table 6: Comparison of the baseline EN–FR translation systems with the systems augmented with synthetic phrases, in 1-best and lattice decoding setups. 1-best synthetic system significantly outperforms baseline lattice decoding setup. Additional improvement in lattice decoding with synthetic phrases suggests that lattice decoding and phrase table adaptation are two comp</context>
</contexts>
<marker>Federico, Bentivogli, Paul, Stüker, 2011</marker>
<rawString>Marcello Federico, Luisa Bentivogli, Michael Paul, and Sebastian Stüker. 2011. Overview of the IWSLT 2011 evaluation campaign. In Proc. IWSLT, pages 8–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan G Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER).</title>
<date>1997</date>
<booktitle>In Proc. ASRU,</booktitle>
<pages>347--352</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="15080" citStr="Fiscus, 1997" startWordPosition="2388" endWordPosition="2389">o-ASR output generation example for a bigram tells the. Phonetic edits are Substitute(T, CH), Substitute(Z, S), Delete(DH). translated TED talks (Cettolo et al., 2012b).6 English is the source language in all the experiments. In expASR we used tst2011–the official test set of the SLT track of the IWSLT 2011 evaluation campaign on the English-French language pair (Federico et al., 2011).7 This test set comprises reference transcriptions of 8 talks (approximately 1.1h of speech, segmented to 818 utterances), 1-best hypotheses from five different ASR systems, a ROVER combination of four systems (Fiscus, 1997), and three sets of lattices produced by the participants of the IWSLT 2011 ASR track. In this set of experiments we compare baseline systems performance to a performance of systems augmented with synthetic phrases on (1) reference transcriptions, (2) 1-best hypotheses from all released ASR systems, and (3) a set of ASR lattices produced by FBK (Ruiz et al., 2011).8 Experiments with individual systems are aimed to validate that MT augmented with synthetic phrases can better translate ASR outputs with recognition errors and sequences that were not observed in the MT training data. Consistency i</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>Jonathan G. Fiscus. 1997. A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER). In Proc. ASRU, pages 347–352. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Which words are hard to recognize? prosodic, lexical, and disfluency factors that increase speech recognition error rates.</title>
<date>2010</date>
<journal>Speech Communication,</journal>
<volume>52</volume>
<issue>3</issue>
<contexts>
<context position="2648" citStr="Goldwater et al., 2010" startWordPosition="383" endWordPosition="386">ation into sentences, as well as reliable casing and punctuation information, which are crucial for MT and other text-based language processing applications (Ostendorf et al., 2008). Second, ASR systems are imperfect and make recognition errors. Even high quality systems make recognition errors, especially in acoustically similar words with similar language model scores, for example morphological substitutions like confusing bare stem and past tense forms, and in high-frequency short words (function words) which often lack both disambiguating context and are subject to reduced pronunciations (Goldwater et al., 2010). One would expect that training an MT system on ASR outputs (rather than the usual writtenstyle texts) would improve matters. Unfortunately, there are few corpora of speech paired with text translations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005)</context>
<context position="33279" citStr="Goldwater et al., 2010" startWordPosition="5256" endWordPosition="5259">oy any data extracted from acoustic models or ASR outputs. Simulated ASR errors are typically used to improve ASR applications. To the best of our knowledge no prior work has been done on integrating ASR errors directly in the translation models. 9 Conclusion The idea behind the novel ASR error-prediction algorithm that we devise is to identify phonological neighbors with similar distributional properties, i.e. similar sounding words for which language model probabilities are insufficient for their disambiguation. These sequences have been identified as significant contributors to ASR errors (Goldwater et al., 2010). Additional and even more important factors that cause recognition errors are disfluencies in speech (Tsvetkov et al., 2013b). In the task of adapting MT to SLT these and other irregularities can effectively be incorporated in a useful general framework: synthetic phrases that augment phrase tables. Our experiments show that simulated acoustic confusions capture real ASR errors and that proposed framework effectively exploits them to improve translation. 623 Acknowledgments We are grateful to João Miranda and Alan Black for providing us the TED audio with transcriptions, and to Zaid Sheikh fo</context>
</contexts>
<marker>Goldwater, Jurafsky, Manning, 2010</marker>
<rawString>Sharon Goldwater, Dan Jurafsky, and Christopher D Manning. 2010. Which words are hard to recognize? prosodic, lexical, and disfluency factors that increase speech recognition error rates. Speech Communication, 52(3):181–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Li Deng</author>
</authors>
<title>Speech-centric information processing: An optimization-oriented approach.</title>
<date>2013</date>
<journal>IEEE,</journal>
<volume>101</volume>
<issue>5</issue>
<contexts>
<context position="1503" citStr="He and Deng, 2013" startWordPosition="214" endWordPosition="217">ments in translation quality. Our proposed technique can also be used in conjunction with lattices as ASR output, leading to further improvements. 1 Introduction Spoken language translation (SLT) systems generally consist of two components: (i) an automatic speech recognition (ASR) system that transcribes source language utterances and (ii) a machine translation (MT) system that translates the transcriptions into the target language. These two components are usually developed independently and then combined and integrated (Ney, 1999; Matusov et al., 2006; Casacuberta et al., 2008; Zhou, 2013; He and Deng, 2013). While this architecture is attractive since it relies only on components that are independently useful, such systems face several challenges. First, spoken language tends to be quite different from the highly edited parallel texts that are available to train translation systems. For example, disfluencies, such as repeated words or phrases, restarts, and revisions of content, are frequent in spontaneous speech,1 while these are usually absent in written texts. In addition, ASR outputs typically lack explicit segmentation into sentences, as well as reliable casing and punctuation information, </context>
</contexts>
<marker>He, Deng, 2013</marker>
<rawString>Xiaodong He and Li Deng. 2013. Speech-centric information processing: An optimization-oriented approach. IEEE, 101(5):1116–1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>James H Martin</author>
</authors>
<date>2000</date>
<booktitle>Speech &amp; Language Processing.</booktitle>
<publisher>Pearson Education</publisher>
<contexts>
<context position="10396" citStr="Jurafsky and Martin, 2000" startWordPosition="1614" endWordPosition="1617"> the same five basic phrase-based translation rule features. To these, three additional features are added: a synthetic phrase indicator, the source language LM score of the source phrase, and the source language LM score of a source phrase in the original phrase pair. Tleft Tright TWleft WIHright . . . T P(T|WIH) ... W P(W|T) ... II P(IH|T) P(IH|TW) ... ER P(ER|T) ... ... ... ... ... ... ... Figure 3: A fragment of the co-occurrence matrix for phone sequence [T W IH T ER]. Rows correspond to phones; columns correspond to left/right context phones of lengths one and two. glish syllable onset (Jurafsky and Martin, 2000). Motivated by the constraining effect of context on phonetic distribution, we cluster phones using a distance-based measure. To do so, we build a vector space model representation of each phone by creating a co-occurrence matrix from a corpus of phonetic forms where each row represents a phone and columns indicate the contextual phones. We take into account left/right context windows of lengths one and two. A cell rp,, in the vector space dictionary matrix represents phone p and context c using the empirical relative frequency f(p |c), as estimated from a pronunciation dictionary. Figure 3 sh</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Dan Jurafsky and James H Martin. 2000. Speech &amp; Language Processing. Pearson Education India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preethi Jyothi</author>
<author>Eric Fosler-Lussier</author>
</authors>
<title>A comparison of audio-free speech recognition error prediction methods.</title>
<date>2009</date>
<booktitle>In Proc. INTERSPEECH,</booktitle>
<pages>1211--1214</pages>
<contexts>
<context position="31079" citStr="Jyothi and Fosler-Lussier (2009)" startWordPosition="4925" endWordPosition="4928">lation for the ASR error. Second example shows a similar behavior for an indefinite article a. Third example is taken from the English-Russian system in the multilingual test set. Gauge was produced as a plausible misrecognition variant to age, and therefore correctly translated (albeit incorrectly inflected) as B03pacTa(age+sg+m+acc). Synthetic phrases were also used in translations containing misrecognized function words, segmentationrelated examples, and longer n-grams. 8 Related work Predicting ASR errors to improve speech recognition quality has been explored in several previous studies. Jyothi and Fosler-Lussier (2009) develop weighted finite-state transducer framework for error prediction. They build a confusion matrix FST between phones to model acoustic errors made by the recognizer. Costs in the confusion matrix combine acoustic variations in the HMM representations of the phones (information from the acoustic model) and word-based phone confusions (information from the pronunciation model). In their follow-up work, Jyothi and Fosler-Lussier (2010) employ this error-predicting framework to train the parameters of a global linear discriminative language model that improves ASR. Sagae et al. (2012) examin</context>
</contexts>
<marker>Jyothi, Fosler-Lussier, 2009</marker>
<rawString>Preethi Jyothi and Eric Fosler-Lussier. 2009. A comparison of audio-free speech recognition error prediction methods. In Proc. INTERSPEECH, pages 1211–1214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preethi Jyothi</author>
<author>Eric Fosler-Lussier</author>
</authors>
<title>Discriminative language modeling using simulated asr errors.</title>
<date>2010</date>
<booktitle>In Proc. INTERSPEECH,</booktitle>
<pages>1049--1052</pages>
<contexts>
<context position="31521" citStr="Jyothi and Fosler-Lussier (2010)" startWordPosition="4989" endWordPosition="4992">lated examples, and longer n-grams. 8 Related work Predicting ASR errors to improve speech recognition quality has been explored in several previous studies. Jyothi and Fosler-Lussier (2009) develop weighted finite-state transducer framework for error prediction. They build a confusion matrix FST between phones to model acoustic errors made by the recognizer. Costs in the confusion matrix combine acoustic variations in the HMM representations of the phones (information from the acoustic model) and word-based phone confusions (information from the pronunciation model). In their follow-up work, Jyothi and Fosler-Lussier (2010) employ this error-predicting framework to train the parameters of a global linear discriminative language model that improves ASR. Sagae et al. (2012) examined three protocols for ‘hallucinating’ ASR n-best lists. First approach generates confusions on the phone level, with a phone-based finite-state transducer that employs real n-best lists produced by the ASR system. Second is generating confusions at the word level with a MT-based approach. Third is a phrasal cohorts approach, in which acoustically confusable phrases are extracted from ASR n-best lists, based on pivots–identical left and r</context>
</contexts>
<marker>Jyothi, Fosler-Lussier, 2010</marker>
<rawString>Preethi Jyothi and Eric Fosler-Lussier. 2010. Discriminative language modeling using simulated asr errors. In Proc. INTERSPEECH, pages 1049–1052.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanislav V Kasl</author>
<author>George F Mahl</author>
</authors>
<title>The relationship of disturbances and hesitations in spontaneous speech to anxiety.</title>
<date>1965</date>
<journal>In Journal of Personality and Social Psychology,</journal>
<volume>1</volume>
<issue>5</issue>
<pages>425--433</pages>
<contexts>
<context position="3853" citStr="Kasl and Mahl, 1965" startWordPosition="586" endWordPosition="589">n et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontaneous speech, not including silent pauses (Tree, 1995; Kasl and Mahl, 1965) 616 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 616–625, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics deal with ASR output (Callison-Burch et al., 2006; Tsvetkov et al., 2013a). We address speech translation in a resourcedeficient scenario, specifically, adapting MT systems to SLT when ASR is unavailable. We augment a discriminative set that translation models rescore with synthetic translation options. These automatically generated translation rules (henceforth synthetic phrases) </context>
</contexts>
<marker>Kasl, Mahl, 1965</marker>
<rawString>Stanislav V Kasl and George F Mahl. 1965. The relationship of disturbances and hesitations in spontaneous speech to anxiety. In Journal of Personality and Social Psychology, volume 1(5), pages 425– 433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20843" citStr="Koehn et al., 2003" startWordPosition="3284" endWordPosition="3287">s (without our test talks) released for the IWSLT 2012 evaluation campaign (Cettolo et al., 2012a); we split Hebrew and Hindi to training and development respectively.11 We split Hebrew and Hindi to sentences with simple heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser-Ney smoothed 11Since TED Hindi corpus is very small (only about 6K sentences) we augment it with additional parallel data (Bojar et al., 2010); however, this improved Hindi system quality only marginally, probably owing to domain mismatch. 620 TED id TED talk 1 Al Gore, 15 Ways to Avert a Climate Crisis, 2006 39 Aubrey de Grey: A roadmap to end aging, 2005 142 Alan Russell: The potential of regenerative medicine, 2006 228 Alan Kay shares a powerful idea about ideas, 2007 248 Alisa Miller: The news about the news, 200</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT/NAACL, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="20771" citStr="Koehn et al., 2007" startWordPosition="3273" endWordPosition="3276"> German and Russian we use sentence-aligned training and development sets (without our test talks) released for the IWSLT 2012 evaluation campaign (Cettolo et al., 2012a); we split Hebrew and Hindi to training and development respectively.11 We split Hebrew and Hindi to sentences with simple heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser-Ney smoothed 11Since TED Hindi corpus is very small (only about 6K sentences) we augment it with additional parallel data (Bojar et al., 2010); however, this improved Hindi system quality only marginally, probably owing to domain mismatch. 620 TED id TED talk 1 Al Gore, 15 Ways to Avert a Climate Crisis, 2006 39 Aubrey de Grey: A roadmap to end aging, 2005 142 Alan Russell: The potential of regenerative medicine, 2006 228 Alan Kay shares a powerf</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Stephan Kanthak</author>
<author>Hermann Ney</author>
</authors>
<title>Integrating speech recognition and machine translation: Where do we stand?</title>
<date>2006</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>1217--1220</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1445" citStr="Matusov et al., 2006" startWordPosition="204" endWordPosition="207">systems as input, we find consistent and significant improvements in translation quality. Our proposed technique can also be used in conjunction with lattices as ASR output, leading to further improvements. 1 Introduction Spoken language translation (SLT) systems generally consist of two components: (i) an automatic speech recognition (ASR) system that transcribes source language utterances and (ii) a machine translation (MT) system that translates the transcriptions into the target language. These two components are usually developed independently and then combined and integrated (Ney, 1999; Matusov et al., 2006; Casacuberta et al., 2008; Zhou, 2013; He and Deng, 2013). While this architecture is attractive since it relies only on components that are independently useful, such systems face several challenges. First, spoken language tends to be quite different from the highly edited parallel texts that are available to train translation systems. For example, disfluencies, such as repeated words or phrases, restarts, and revisions of content, are frequent in spontaneous speech,1 while these are usually absent in written texts. In addition, ASR outputs typically lack explicit segmentation into sentences</context>
</contexts>
<marker>Matusov, Kanthak, Ney, 2006</marker>
<rawString>Evgeny Matusov, Stephan Kanthak, and Hermann Ney. 2006. Integrating speech recognition and machine translation: Where do we stand? In Proc. ICASSP, pages V–1217–V–1220. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Fast and accurate sentence alignment of bilingual corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<pages>135--144</pages>
<publisher>SpringerVerlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="20540" citStr="Moore, 2002" startWordPosition="3238" endWordPosition="3239">eriments the ASR outputs and lattices in standard lattice format (SLF) were produces by the participants of IWSLT 2011 evaluation campaign. 5.2 MT We train and test MT using the TED corpora in all five languages. For French, German and Russian we use sentence-aligned training and development sets (without our test talks) released for the IWSLT 2012 evaluation campaign (Cettolo et al., 2012a); we split Hebrew and Hindi to training and development respectively.11 We split Hebrew and Hindi to sentences with simple heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser-Ney smoothed 11Since TED Hindi corpus is very small (only about 6K sentences) we augment it with additional parallel data (Bojar et al., 2010); however, this improved Hindi system quality only marginally, probably owing</context>
</contexts>
<marker>Moore, 2002</marker>
<rawString>Robert C. Moore. 2002. Fast and accurate sentence alignment of bilingual corpora. In Proceedings of AMTA, pages 135–144, London, UK. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
</authors>
<title>Speech translation: Coupling of recognition and translation.</title>
<date>1999</date>
<booktitle>In Proc. ICASSP,</booktitle>
<volume>1</volume>
<pages>517--520</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1423" citStr="Ney, 1999" startWordPosition="202" endWordPosition="203">nglish ASR systems as input, we find consistent and significant improvements in translation quality. Our proposed technique can also be used in conjunction with lattices as ASR output, leading to further improvements. 1 Introduction Spoken language translation (SLT) systems generally consist of two components: (i) an automatic speech recognition (ASR) system that transcribes source language utterances and (ii) a machine translation (MT) system that translates the transcriptions into the target language. These two components are usually developed independently and then combined and integrated (Ney, 1999; Matusov et al., 2006; Casacuberta et al., 2008; Zhou, 2013; He and Deng, 2013). While this architecture is attractive since it relies only on components that are independently useful, such systems face several challenges. First, spoken language tends to be quite different from the highly edited parallel texts that are available to train translation systems. For example, disfluencies, such as repeated words or phrases, restarts, and revisions of content, are frequent in spontaneous speech,1 while these are usually absent in written texts. In addition, ASR outputs typically lack explicit segme</context>
</contexts>
<marker>Ney, 1999</marker>
<rawString>Hermann Ney. 1999. Speech translation: Coupling of recognition and translation. In Proc. ICASSP, volume 1, pages 517–520. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="20889" citStr="Och, 2003" startWordPosition="3293" endWordPosition="3294">evaluation campaign (Cettolo et al., 2012a); we split Hebrew and Hindi to training and development respectively.11 We split Hebrew and Hindi to sentences with simple heuristics, and then sentence-align with the Microsoft Bilingual Sentence Aligner (Moore, 2002). Punctuation marks were removed, corpora were lowercased, and tokenized using the cdec scripts (Dyer et al., 2010). In all MT experiments, both for sentence and lattice translation, we employ the Moses toolkit (Koehn et al., 2007), implementing the phrasebased statistical MT model (Koehn et al., 2003) and optimize parameters with MERT (Och, 2003). Target language 3-gram Kneser-Ney smoothed 11Since TED Hindi corpus is very small (only about 6K sentences) we augment it with additional parallel data (Bojar et al., 2010); however, this improved Hindi system quality only marginally, probably owing to domain mismatch. 620 TED id TED talk 1 Al Gore, 15 Ways to Avert a Climate Crisis, 2006 39 Aubrey de Grey: A roadmap to end aging, 2005 142 Alan Russell: The potential of regenerative medicine, 2006 228 Alan Kay shares a powerful idea about ideas, 2007 248 Alisa Miller: The news about the news, 2008 451 Bill Gates: Mosquitos, malaria and educa</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Onishi</author>
<author>Masao Utiyama</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Paraphrase lattice for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3375" citStr="Onishi et al., 2010" startWordPosition="506" endWordPosition="509">ld improve matters. Unfortunately, there are few corpora of speech paired with text translations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontaneous speech, not including silent pauses (Tree, 1995; Kasl and Mahl, 1965) 616 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 61</context>
</contexts>
<marker>Onishi, Utiyama, Sumita, 2010</marker>
<rawString>Takashi Onishi, Masao Utiyama, and Eiichiro Sumita. 2010. Paraphrase lattice for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari Ostendorf</author>
<author>Benoît Favre</author>
<author>Ralph Grishman</author>
<author>Dilek Hakkani-Tur</author>
<author>Mary Harper</author>
<author>Dustin Hillard</author>
<author>Julia Hirschberg</author>
<author>Heng Ji</author>
<author>Jeremy G Kahn</author>
</authors>
<title>Yang Liu, Sameer Maskey, Evgeny Matusov,</title>
<date>2008</date>
<pages>25--3</pages>
<publisher>IEEE,</publisher>
<location>Hermann Ney, Andrew Rosenberg, Elizabeth Shriberg, Wen</location>
<contexts>
<context position="2206" citStr="Ostendorf et al., 2008" startWordPosition="320" endWordPosition="323">are independently useful, such systems face several challenges. First, spoken language tends to be quite different from the highly edited parallel texts that are available to train translation systems. For example, disfluencies, such as repeated words or phrases, restarts, and revisions of content, are frequent in spontaneous speech,1 while these are usually absent in written texts. In addition, ASR outputs typically lack explicit segmentation into sentences, as well as reliable casing and punctuation information, which are crucial for MT and other text-based language processing applications (Ostendorf et al., 2008). Second, ASR systems are imperfect and make recognition errors. Even high quality systems make recognition errors, especially in acoustically similar words with similar language model scores, for example morphological substitutions like confusing bare stem and past tense forms, and in high-frequency short words (function words) which often lack both disambiguating context and are subject to reduced pronunciations (Goldwater et al., 2010). One would expect that training an MT system on ASR outputs (rather than the usual writtenstyle texts) would improve matters. Unfortunately, there are few co</context>
</contexts>
<marker>Ostendorf, Favre, Grishman, Hakkani-Tur, Harper, Hillard, Hirschberg, Ji, Kahn, 2008</marker>
<rawString>Mari Ostendorf, Benoît Favre, Ralph Grishman, Dilek Hakkani-Tur, Mary Harper, Dustin Hillard, Julia Hirschberg, Heng Ji, Jeremy G Kahn, Yang Liu, Sameer Maskey, Evgeny Matusov, Hermann Ney, Andrew Rosenberg, Elizabeth Shriberg, Wen Wang, and Chuck Wooters. 2008. Speech segmentation and spoken document processing. Signal Processing Magazine, IEEE, 25(3):59–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21760" citStr="Papineni et al., 2002" startWordPosition="3438" endWordPosition="3441">ng to domain mismatch. 620 TED id TED talk 1 Al Gore, 15 Ways to Avert a Climate Crisis, 2006 39 Aubrey de Grey: A roadmap to end aging, 2005 142 Alan Russell: The potential of regenerative medicine, 2006 228 Alan Kay shares a powerful idea about ideas, 2007 248 Alisa Miller: The news about the news, 2008 451 Bill Gates: Mosquitos, malaria and education, 2009 535 Al Gore warns on latest climate trends, 2009 Table 1: Test set of TED talks. language models are trained on the training part of each corpus. Results are reported using caseinsensitive BLEU with a single reference and no punctuation (Papineni et al., 2002). To verify that our improvements are consistent and are not just an effect of optimizer instability (Clark et al., 2011), we train three systems for each MT setup. Statistical significance is measured with the MultEval toolkit.12 Reported BLEU scores are averaged over three systems. In MT adaptation experiments we augment baseline phrase tables with synthetic phrases. For each entry in the original phrase table we add (at most) five13 best acoustic confusions, detailed in Section 4. Table 3 contains sizes of phrase tables, original and augmented with synthetic phrases. Original Synthetic EN–F</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Peitz</author>
<author>Simon Wiesler</author>
<author>Markus NußbaumThom</author>
<author>Hermann Ney</author>
</authors>
<title>Spoken language translation using automatically transcribed text in training.</title>
<date>2012</date>
<booktitle>In Proc. IWSLT.</booktitle>
<contexts>
<context position="3180" citStr="Peitz et al., 2012" startWordPosition="470" endWordPosition="473">ambiguating context and are subject to reduced pronunciations (Goldwater et al., 2010). One would expect that training an MT system on ASR outputs (rather than the usual writtenstyle texts) would improve matters. Unfortunately, there are few corpora of speech paired with text translations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontane</context>
</contexts>
<marker>Peitz, Wiesler, NußbaumThom, Ney, 2012</marker>
<rawString>Stephan Peitz, Simon Wiesler, Markus NußbaumThom, and Hermann Ney. 2012. Spoken language translation using automatically transcribed text in training. In Proc. IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vu H Quan</author>
<author>Marcello Federico</author>
<author>Mauro Cettolo</author>
</authors>
<title>Integrated n-best re-ranking for spoken language translation.</title>
<date>2005</date>
<booktitle>In Proc. INTERSPEECH,</booktitle>
<pages>3181--3184</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3248" citStr="Quan et al., 2005" startWordPosition="484" endWordPosition="487">ater et al., 2010). One would expect that training an MT system on ASR outputs (rather than the usual writtenstyle texts) would improve matters. Unfortunately, there are few corpora of speech paired with text translations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontaneous speech, not including silent pauses (Tree, 1995; Kasl and Mahl, </context>
</contexts>
<marker>Quan, Federico, Cettolo, 2005</marker>
<rawString>Vu H Quan, Marcello Federico, and Mauro Cettolo. 2005. Integrated n-best re-ranking for spoken language translation. In Proc. INTERSPEECH, pages 3181–3184. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Ruiz</author>
<author>Arianna Bisazza</author>
<author>Fabio Brugnara</author>
<author>Daniele Falavigna</author>
<author>Diego Giuliani</author>
<author>Suhel Jaber</author>
<author>Roberto Gretter</author>
<author>Marcello Federico</author>
</authors>
<date>2011</date>
<booktitle>FBK@ IWSLT 2011. In Proc. IWSLT.</booktitle>
<contexts>
<context position="15446" citStr="Ruiz et al., 2011" startWordPosition="2447" endWordPosition="2450">air (Federico et al., 2011).7 This test set comprises reference transcriptions of 8 talks (approximately 1.1h of speech, segmented to 818 utterances), 1-best hypotheses from five different ASR systems, a ROVER combination of four systems (Fiscus, 1997), and three sets of lattices produced by the participants of the IWSLT 2011 ASR track. In this set of experiments we compare baseline systems performance to a performance of systems augmented with synthetic phrases on (1) reference transcriptions, (2) 1-best hypotheses from all released ASR systems, and (3) a set of ASR lattices produced by FBK (Ruiz et al., 2011).8 Experiments with individual systems are aimed to validate that MT augmented with synthetic phrases can better translate ASR outputs with recognition errors and sequences that were not observed in the MT training data. Consistency in performance across different ASRs is expected if our approach to generate plausible misrecognition variants is universal, rather than biased to a specific system. Comparison of 1-best system with synthetic phrases to lattice decoding setup without synthetic phrases should demonstrate whether nbest plausible misrecognition variants that we generate assemble multi</context>
</contexts>
<marker>Ruiz, Bisazza, Brugnara, Falavigna, Giuliani, Jaber, Gretter, Federico, 2011</marker>
<rawString>Nick Ruiz, Arianna Bisazza, Fabio Brugnara, Daniele Falavigna, Diego Giuliani, Suhel Jaber, Roberto Gretter, and Marcello Federico. 2011. FBK@ IWSLT 2011. In Proc. IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>M Lehr</author>
<author>E Prud’hommeaux</author>
<author>P Xu</author>
<author>N Glenn</author>
<author>D Karakos</author>
<author>S Khudanpur</author>
<author>B Roark</author>
<author>M Saraçlar</author>
<author>I Shafran</author>
<author>D Bikel</author>
<author>C Callison-Burch</author>
<author>Y Cao</author>
<author>K Hall</author>
<author>E Hasler</author>
<author>P Koehn</author>
<author>A Lopez</author>
<author>M Post</author>
<author>D Riley</author>
</authors>
<title>Hallucinated n-best lists for discriminative language modeling.</title>
<date>2012</date>
<booktitle>In Proc. ICASSP.</booktitle>
<publisher>IEEE.</publisher>
<marker>Sagae, Lehr, Prud’hommeaux, Xu, Glenn, Karakos, Khudanpur, Roark, Saraçlar, Shafran, Bikel, Callison-Burch, Cao, Hall, Hasler, Koehn, Lopez, Post, Riley, 2012</marker>
<rawString>Kenji Sagae, M. Lehr, E. Prud’hommeaux, P. Xu, N. Glenn, D. Karakos, S. Khudanpur, B. Roark, M. Saraçlar, I. Shafran, D. Bikel, C. Callison-Burch, Y. Cao, K. Hall, E. Hasler, P. Koehn, A. Lopez, M. Post, and D. Riley. 2012. Hallucinated n-best lists for discriminative language modeling. In Proc. ICASSP. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Soltau</author>
<author>F Metze</author>
<author>C Fügen</author>
<author>A Waibel</author>
</authors>
<title>A one-pass decoder based on polymorphic linguistic context assignment.</title>
<date>2001</date>
<booktitle>In Proc. ASRU.</booktitle>
<marker>Soltau, Metze, Fügen, Waibel, 2001</marker>
<rawString>H. Soltau, F. Metze, C. Fügen, and A. Waibel. 2001. A one-pass decoder based on polymorphic linguistic context assignment. In Proc. ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="19149" citStr="Stolcke, 2002" startWordPosition="3008" endWordPosition="3009">, a new multilingual parallel corpus of translated speech was released for SLT track of IWSLT 2013 Evaluation Campaign, however, this data set does not include Russian, Hebrew and Hindi, which are a subject of this research. 10Since TED translation is a voluntary effort, not all talks are available in all languages. al., 2001). The acoustic model is maximum likelihood system, no speaker adaptation or discriminative training applied. The acoustic model training data is 186h of Broadcast News-style data. 5-gram language model with modified Kneser-Ney smoothing is trained with the SRILM toolkit (Stolcke, 2002) on the EPPS, TED, NewsCommentary, and the Gigaword corpora. The Broadcast News test set contains 4h of audio; we obtain 25.6% word error rate (WER) on this test set. We segment the TED test audio by the timestamps of transcripts appearance on the screen. Then, we manually detect and discard noisy hypotheses around segmentation boundaries, and manually align the remaining hypotheses with the references which are the source side of the English-French MT test set. The resulting test set of 843 hypotheses, sentence aligned with transcripts, yields 30.7% WER. Higher error rates (relatively to the </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM—an extensible language modeling toolkit. In Proc. ICSLP, pages 901– 904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean E Fox Tree</author>
</authors>
<title>The effects of false starts and repetitions on the processing of subsequent words in spontaneous speech.</title>
<date>1995</date>
<journal>Journal of memory and language,</journal>
<volume>34</volume>
<issue>6</issue>
<contexts>
<context position="3831" citStr="Tree, 1995" startWordPosition="584" endWordPosition="585">t lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontaneous speech, not including silent pauses (Tree, 1995; Kasl and Mahl, 1965) 616 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 616–625, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics deal with ASR output (Callison-Burch et al., 2006; Tsvetkov et al., 2013a). We address speech translation in a resourcedeficient scenario, specifically, adapting MT systems to SLT when ASR is unavailable. We augment a discriminative set that translation models rescore with synthetic translation options. These automatically generated translation rules (hencefor</context>
</contexts>
<marker>Tree, 1995</marker>
<rawString>Jean E Fox Tree. 1995. The effects of false starts and repetitions on the processing of subsequent words in spontaneous speech. Journal of memory and language, 34(6):709–738.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Chris Dyer</author>
<author>Lori Levin</author>
<author>Archna Bhatia</author>
</authors>
<title>Generating English determiners in phrase-based translation with synthetic translation options.</title>
<date>2013</date>
<booktitle>In Proceedings of WMT. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4141" citStr="Tsvetkov et al., 2013" startWordPosition="626" endWordPosition="629">oding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontaneous speech, not including silent pauses (Tree, 1995; Kasl and Mahl, 1965) 616 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 616–625, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics deal with ASR output (Callison-Burch et al., 2006; Tsvetkov et al., 2013a). We address speech translation in a resourcedeficient scenario, specifically, adapting MT systems to SLT when ASR is unavailable. We augment a discriminative set that translation models rescore with synthetic translation options. These automatically generated translation rules (henceforth synthetic phrases) are noisy variants of observed translation rules with simulated plausible speech recognition errors (§2). To simulate ASR errors we generate acoustically and distributionally similar phrases to a source (English) phrase with a phonologically-motivated algorithm (§4). Likely phonetic subs</context>
<context position="6379" citStr="Tsvetkov et al., 2013" startWordPosition="955" endWordPosition="958"> and a language model, but the algorithm can in principle be applied without pronunciation dictionary for languages with a phonemic orthography. 2 Methodology We adopt a standard ASR-MT cascading approach and then augment translation models with synthetic phrases. Our proposed system architecture is depicted in Figure 1. Synthetic phrases are generated from entries in the original translation model–phrase translation 2We augment phrase tables only with synthetic phrases that capture simulated ASR errors, the methodology that we advocate, however, is applicable to many problems in translation (Tsvetkov et al., 2013a; Ammar et al., 2013; Chahuneau et al., 2013). Figure 1: SLT architecture: ASR and MT are trained independently and then cascaded. We improve SLT by populating MT translation model with synthetic phrases. Each synthetic phrase is a variant of an original phrase pair with simulated ASR errors on the source side. pairs acquired from parallel data. From a source side of an original phrase pair we generate list of its plausible misrecognition variants (pseudo-ASR outputs with recognition errors) and add them as a source side of a synthetic phrase. For k-best simulated ASR outputs we construct k s</context>
<context position="33403" citStr="Tsvetkov et al., 2013" startWordPosition="5275" endWordPosition="5278">. To the best of our knowledge no prior work has been done on integrating ASR errors directly in the translation models. 9 Conclusion The idea behind the novel ASR error-prediction algorithm that we devise is to identify phonological neighbors with similar distributional properties, i.e. similar sounding words for which language model probabilities are insufficient for their disambiguation. These sequences have been identified as significant contributors to ASR errors (Goldwater et al., 2010). Additional and even more important factors that cause recognition errors are disfluencies in speech (Tsvetkov et al., 2013b). In the task of adapting MT to SLT these and other irregularities can effectively be incorporated in a useful general framework: synthetic phrases that augment phrase tables. Our experiments show that simulated acoustic confusions capture real ASR errors and that proposed framework effectively exploits them to improve translation. 623 Acknowledgments We are grateful to João Miranda and Alan Black for providing us the TED audio with transcriptions, and to Zaid Sheikh for his help with ASR decoding. This work was supported in part by the U. S. Army Research Laboratory and the U. S. Army Resea</context>
</contexts>
<marker>Tsvetkov, Dyer, Levin, Bhatia, 2013</marker>
<rawString>Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna Bhatia. 2013a. Generating English determiners in phrase-based translation with synthetic translation options. In Proceedings of WMT. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Zaid Sheikh</author>
<author>Florian Metze</author>
</authors>
<title>Identification and modeling of word fragments in spontaneous speech.</title>
<date>2013</date>
<booktitle>In Proc. ICASSP.</booktitle>
<publisher>IEEE.</publisher>
<contexts>
<context position="4141" citStr="Tsvetkov et al., 2013" startWordPosition="626" endWordPosition="629">oding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontaneous speech, not including silent pauses (Tree, 1995; Kasl and Mahl, 1965) 616 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 616–625, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics deal with ASR output (Callison-Burch et al., 2006; Tsvetkov et al., 2013a). We address speech translation in a resourcedeficient scenario, specifically, adapting MT systems to SLT when ASR is unavailable. We augment a discriminative set that translation models rescore with synthetic translation options. These automatically generated translation rules (henceforth synthetic phrases) are noisy variants of observed translation rules with simulated plausible speech recognition errors (§2). To simulate ASR errors we generate acoustically and distributionally similar phrases to a source (English) phrase with a phonologically-motivated algorithm (§4). Likely phonetic subs</context>
<context position="6379" citStr="Tsvetkov et al., 2013" startWordPosition="955" endWordPosition="958"> and a language model, but the algorithm can in principle be applied without pronunciation dictionary for languages with a phonemic orthography. 2 Methodology We adopt a standard ASR-MT cascading approach and then augment translation models with synthetic phrases. Our proposed system architecture is depicted in Figure 1. Synthetic phrases are generated from entries in the original translation model–phrase translation 2We augment phrase tables only with synthetic phrases that capture simulated ASR errors, the methodology that we advocate, however, is applicable to many problems in translation (Tsvetkov et al., 2013a; Ammar et al., 2013; Chahuneau et al., 2013). Figure 1: SLT architecture: ASR and MT are trained independently and then cascaded. We improve SLT by populating MT translation model with synthetic phrases. Each synthetic phrase is a variant of an original phrase pair with simulated ASR errors on the source side. pairs acquired from parallel data. From a source side of an original phrase pair we generate list of its plausible misrecognition variants (pseudo-ASR outputs with recognition errors) and add them as a source side of a synthetic phrase. For k-best simulated ASR outputs we construct k s</context>
<context position="33403" citStr="Tsvetkov et al., 2013" startWordPosition="5275" endWordPosition="5278">. To the best of our knowledge no prior work has been done on integrating ASR errors directly in the translation models. 9 Conclusion The idea behind the novel ASR error-prediction algorithm that we devise is to identify phonological neighbors with similar distributional properties, i.e. similar sounding words for which language model probabilities are insufficient for their disambiguation. These sequences have been identified as significant contributors to ASR errors (Goldwater et al., 2010). Additional and even more important factors that cause recognition errors are disfluencies in speech (Tsvetkov et al., 2013b). In the task of adapting MT to SLT these and other irregularities can effectively be incorporated in a useful general framework: synthetic phrases that augment phrase tables. Our experiments show that simulated acoustic confusions capture real ASR errors and that proposed framework effectively exploits them to improve translation. 623 Acknowledgments We are grateful to João Miranda and Alan Black for providing us the TED audio with transcriptions, and to Zaid Sheikh for his help with ASR decoding. This work was supported in part by the U. S. Army Research Laboratory and the U. S. Army Resea</context>
</contexts>
<marker>Tsvetkov, Sheikh, Metze, 2013</marker>
<rawString>Yulia Tsvetkov, Zaid Sheikh, and Florian Metze. 2013b. Identification and modeling of word fragments in spontaneous speech. In Proc. ICASSP. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Xu</author>
<author>Pascale Fung</author>
<author>Ricky Chan</author>
</authors>
<title>Phrase-level transduction model with reordering for spoken to written language transformation.</title>
<date>2012</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>4965--4968</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3198" citStr="Xu et al., 2012" startWordPosition="474" endWordPosition="477">and are subject to reduced pronunciations (Goldwater et al., 2010). One would expect that training an MT system on ASR outputs (rather than the usual writtenstyle texts) would improve matters. Unfortunately, there are few corpora of speech paired with text translations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontaneous speech, not in</context>
</contexts>
<marker>Xu, Fung, Chan, 2012</marker>
<rawString>Ping Xu, Pascale Fung, and Ricky Chan. 2012. Phrase-level transduction model with reordering for spoken to written language transformation. In Proc. ICASSP, pages 4965–4968. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Genichiro Kikui</author>
<author>Hirofumi Yamamoto</author>
<author>Taro Watanabe</author>
<author>Frank Soong</author>
<author>Wai Kit Lo</author>
</authors>
<title>A unified approach in speech-to-speech translation: integrating features of speech recognition and machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1168</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="3464" citStr="Zhang et al., 2004" startWordPosition="518" endWordPosition="521">ations into a second language that could be used for this purpose. This has been an incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontaneous speech, not including silent pauses (Tree, 1995; Kasl and Mahl, 1965) 616 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 616–625, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguis</context>
</contexts>
<marker>Zhang, Kikui, Yamamoto, Watanabe, Soong, Lo, 2004</marker>
<rawString>Ruiqiang Zhang, Genichiro Kikui, Hirofumi Yamamoto, Taro Watanabe, Frank Soong, and Wai Kit Lo. 2004. A unified approach in speech-to-speech translation: integrating features of speech recognition and machine translation. In Proceedings of COLING, page 1168. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bowen Zhou</author>
<author>Laurent Besacier</author>
<author>Yuqing Gao</author>
</authors>
<title>On efficient coupling of ASR and SMT for speech translation.</title>
<date>2007</date>
<booktitle>In Proc. ICASSP,</booktitle>
<volume>4</volume>
<pages>101</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3544" citStr="Zhou et al., 2007" startWordPosition="533" endWordPosition="536">n incentive to various MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontaneous speech, not including silent pauses (Tree, 1995; Kasl and Mahl, 1965) 616 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 616–625, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics deal with ASR output (Callison-Burch et al., 2006; Tsvetkov et al., 2013a).</context>
</contexts>
<marker>Zhou, Besacier, Gao, 2007</marker>
<rawString>Bowen Zhou, Laurent Besacier, and Yuqing Gao. 2007. On efficient coupling of ASR and SMT for speech translation. In Proc. ICASSP, volume 4, pages IV–101. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bowen Zhou</author>
</authors>
<title>Statistical machine translation for speech: A perspective on structures, learning, and decoding.</title>
<date>2013</date>
<pages>101--5</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="1483" citStr="Zhou, 2013" startWordPosition="212" endWordPosition="213">cant improvements in translation quality. Our proposed technique can also be used in conjunction with lattices as ASR output, leading to further improvements. 1 Introduction Spoken language translation (SLT) systems generally consist of two components: (i) an automatic speech recognition (ASR) system that transcribes source language utterances and (ii) a machine translation (MT) system that translates the transcriptions into the target language. These two components are usually developed independently and then combined and integrated (Ney, 1999; Matusov et al., 2006; Casacuberta et al., 2008; Zhou, 2013; He and Deng, 2013). While this architecture is attractive since it relies only on components that are independently useful, such systems face several challenges. First, spoken language tends to be quite different from the highly edited parallel texts that are available to train translation systems. For example, disfluencies, such as repeated words or phrases, restarts, and revisions of content, are frequent in spontaneous speech,1 while these are usually absent in written texts. In addition, ASR outputs typically lack explicit segmentation into sentences, as well as reliable casing and punct</context>
<context position="3557" citStr="Zhou, 2013" startWordPosition="537" endWordPosition="538">ous MT adaptation approaches and development of speech-input MT systems. MT adaptation has been done via input text pre-processing, by transformation of spoken language (ASR output) into written language (MT input) (Peitz et al., 2012; Xu et al., 2012); via decoding ASR nbest lists (Quan et al., 2005), or confusion networks (Bertoldi et al., 2007; Casacuberta et al., 2008), or lattices (Dyer et al., 2008; Onishi et al., 2010); via additional translation features capturing acoustic information (Zhang et al., 2004); and with methods that follow a paradigm of unified decoding (Zhou et al., 2007; Zhou, 2013). In line with the previous research, we too adapt a standard MT system to a speech-input MT, but by altering the translation model itself so it is better able to 1Disfluencies constitute about 6% of word tokens in spontaneous speech, not including silent pauses (Tree, 1995; Kasl and Mahl, 1965) 616 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 616–625, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics deal with ASR output (Callison-Burch et al., 2006; Tsvetkov et al., 2013a). We address s</context>
</contexts>
<marker>Zhou, 2013</marker>
<rawString>Bowen Zhou. 2013. Statistical machine translation for speech: A perspective on structures, learning, and decoding. IEEE, 101(5):1180–1202.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>