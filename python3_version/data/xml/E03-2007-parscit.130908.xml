<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.069384">
<title confidence="0.968461">
WASPBENCH: a lexicographer&apos;s workbench supporting state-of-the-art
word sense disambiguation.
</title>
<author confidence="0.994598">
Adam Kilgarriff, Roger Evans, Rob Koeling
Michael Rundell, David Tugwell
</author>
<affiliation confidence="0.998667">
ITRI, University of Brighton
</affiliation>
<email confidence="0.967447">
Firstname.Lastname@itri.brighton.ac.uk
</email>
<sectionHeader confidence="0.998571" genericHeader="abstract">
1 Background
</sectionHeader>
<bodyText confidence="0.999903555555556">
Human Language Technologies (HLT) need dic-
tionaries, to tell them what words mean and how
they behave. People making dictionaries (lexi-
cographers) need HLT, to help them identify how
words behave so they can make better dictionar-
ies. Thus a potential for synergy exists across the
range of lexical data - in the construction of head-
word lists, for spelling correction, phonetics, mor-
phology and syntax, but nowhere more than for
semantics, and in particular the vexed question of
how a word&apos;s meaning should be analysed into dis-
tinct senses. HLT needs all the help it can get from
dictionaries, because it is a very hard problem to
identify which meaning of a word applies. Lexi-
cographers need all the help they can get because
the analysis of meaning is the second hardest part
of their job (Kilgarriff, 1998), it occupies a large
share of their working hours, and it is one where,
currently, they have very little to go on beyond in-
tuition and other dictionaries.
Thus HLT system developers and corpus lexi-
cographers can both benefit from a tool for find-
ing and organizing the distinctive patterns of use
of words in texts. Such a tool would be an asset
for both language research and lexicon develop-
ment, particularly for lexicons for Machine Trans-
lation. We have developed the WAS PB EN CH, a tool
that (1) presents a &amp;quot;word sketch&amp;quot;, a summary of
the corpus evidence for a word, to the lexicogra-
pher; (2) supports the lexicographer in analysing
the word into its distinct meanings and (3) uses
the lexicographer&apos;s analysis as the input to a state-
of-the-art word sense disambiguation (WSD) al-
gorithm, the output of which is a &amp;quot;word expert&amp;quot;
which can then disambiguate new instances of the
word.
</bodyText>
<sectionHeader confidence="0.948146" genericHeader="categories and subject descriptors">
2 WAS PB ENCH
</sectionHeader>
<subsectionHeader confidence="0.997884">
2.1 Grammatical relations database
</subsectionHeader>
<bodyText confidence="0.994155041666667">
The central resource of WASPBENCH is a collec-
tion of all grammatical relations holding between
words in the corpus. WA SPBENCH is currently
based on the British National Corpus&apos; (BNC): 100
million words of contemporary British English, of
a wide range of genres. Using finite-state tech-
niques operating over part-of-speech tags, we pro-
cess the whole corpus finding quintuples of the
form:
{Rel, W1 , W2, Prep, Pos}
where Rel is a relation, W1 is the lemma of the
word for which Rel holds, W2 is the lemma of the
other open-class word involved, Prep is the prepo-
sition or particle involved and Pos is the position
of W1 in the corpus. Relations may have null val-
ues for W2 and Prep. The database contains 70
million quintuples.
The inventory of relations is shown in Table 1.
There are nine unary relations (ie. with W2 and
Prep null), seven binary relations with Prep null,
two binary relations with W2 null and one trinary
relation with no null elements. All inverse rela-
tions, ie. subject-of etc, found by taking W2 as
the head word instead of W1 are explicitly repre-
</bodyText>
<footnote confidence="0.979717">
1 http://info.ox.ac.uldbnc
</footnote>
<page confidence="0.996987">
211
</page>
<figureCaption confidence="0.98306285">
relation example
bare-noun the angle of bank&apos;
possessive my bank&apos;
plural the banks&apos;
passive was seen&apos;
reflexive see&apos; herself
ing-comp love&apos; eating fish
finite-comp know&apos; he came
inf-comp decision&apos; to eat fish
wh-comp know&apos; why he came
subject the bank2 refused&apos;
object climb&amp;quot; the bank2
adj-comp grow&amp;quot; certain2
noun-modifier merchant2 bank&apos;
modifier a big2 bank&amp;quot;
and-or banks&apos; and mounds2
predicate banks&apos; are barriers2
particle grow&amp;quot; up&amp;quot;
Prep+gerund tired&amp;quot; or eating fish
PP-comp/mod banks&apos; or the river2
</figureCaption>
<tableCaption confidence="0.976525">
Table 1: Grammatical Relations
</tableCaption>
<bodyText confidence="0.999527909090909">
sented, giving six extra binary relations2 and one
extra trinary relation, to give a total of twenty-six
distinct relations. These relations provide a flexi-
ble resource to be used as the basis of the compu-
tations of WA S PB E N CH.
The relations contain a substantial number of er-
rors, originating from POS-tagging errors in the
BNC, attachment ambiguities, or limitations of
the pattern-matching grammar. However, as the
system finds high-salience patterns, given enough
data, the noise does not present great problems.
</bodyText>
<subsectionHeader confidence="0.999268">
2.2 Word Sketches
</subsectionHeader>
<bodyText confidence="0.980304263157895">
When the lexicographer starts working on a word,
s/he enters the word (and word class) at a prompt.
Using the grammatical relations database, the sys-
tem then composes a word sketch for the word.
This is a page of data such as Table 2, which
shows, for the word in question (W1), ordered lists
of high-salience grammatical relations, relation-
W2 pairs, and relation-W2-Prep triples for the
word.
The number of patterns shown is set by the user,
but will typically be over 200. These are listed
for each relation in order of salience3, with the
2and-or is considered symmetrical so does not give rise
to a new inverse relation.
&apos;Salience is estimated as the product of Mutual Infor-
count of corpus instances. The instances can be in-
stantly retrieved and shown in a concordance win-
dow. Producing a word sketch for a medium-to-
high frequency word takes around ten seconds.4
</bodyText>
<subsectionHeader confidence="0.998066">
2.3 Matching patterns with senses
</subsectionHeader>
<bodyText confidence="0.999962407407407">
The next task is to enter a preliminary list of
senses for the word, in the form of some arbitrary
mnemonics, perhaps MONEY, CLOUD and RIVER
for three senses of bank. This inventory may be
drawn from the user&apos;s knowledge, from a perusal
of the word sketch, or from a pre-existing dictio-
nary entry.
As Table 2 shows, and in keeping with &amp;quot;one
sense per collocation&amp;quot; (Yarowsky, 1993) in most
cases, high-salience patterns or clues indicate just
one of the word&apos;s senses. The user then has the
task of associating, by selecting from a pop-up
menu, the required sense for unambiguous clues.
Reference can be made at any time to the actual
corpus instances, which demonstrate the contexts
in which the triple occurs.
The number of relations marked will depend on
the time available to the lexicographer, as well as
the complexity of the sense division to be made.
The act of assigning senses to patterns may very
well lead the lexicographer to discover fresh, un-
considered senses or subsenses of the word. If so,
extra sense mnemonics can be added.
When the user deems that sufficient patterns
have been marked with senses, the pattern-sense
pairs are submitted to the next stage: automatic
disambiguation.
</bodyText>
<subsectionHeader confidence="0.99312">
2.4 The Disambiguation Algorithm
</subsectionHeader>
<bodyText confidence="0.99330275">
WASPBENCH uses Yarowsky&apos;s decision list ap-
proach to WSD (Yarowsky, 1995). This is a boot-
strapping algorithm that, given some initial seed-
ing, iteratively divides the corpus examples into
the different senses. Given a set of classified col-
locations, or clues, and a set of corpus instances
for the word, the algorithm is as follows:
mation and log frequency. Our experience of working lexi-
cographers&apos; use of Mutual Information or log-likelihood lists
shows that, for lexicographic purposes, these over-emphasise
low frequency items, and that multiplying by log frequency
is an appropriate adjustment.
</bodyText>
<footnote confidence="0.993092">
4A set of pre-compiled word sketches can be
seen at http://www.itri.brighton.ac.uk/
adam.kilgarriff/wordsketches.html
</footnote>
<page confidence="0.980437">
212
</page>
<table confidence="0.9783951">
subj-of num sal obj-of num sal modifier num sal n-mod num sal
lend 95 21.2 burst 27 16.4 central 755 25.5 merchant 213 29.4
issue 60 11.8 rob 31 15.3 Swiss 87 18.7 clearing 127 27.0
charge 29 9.5 overflow 7 10.2 commercial 231 18.6 river 217 25.4
operate 45 8.9 line 13 8.4 grassy 42 18.5 creditor 52 22.8
modifies PP iv-PP and-or
holiday 404 32.6 of England 988 37.5 governor of 108 26.2 society 287 24.6
account 503 32.0 of Scotland 242 26.9 balance at 25 20.2 bank 107 17.7
loan 108 27.5 of river 111 22.1 borrow from 42 19.1 institution 82 16.0
lending 68 26.1 of Thames 41 20.1 account with 30 18.4 Lloyds 11 14.1
</table>
<tableCaption confidence="0.999419">
Table 2: Extract of word sketch for bank
</tableCaption>
<bodyText confidence="0.8573625">
I. assign instances containing a classified clue
to the appropriate sense
</bodyText>
<listItem confidence="0.905702117647059">
2. for each clue C (already classified or not)
• for each sense, count the instances
where C holds which are assigned to it
• identify C&apos;s &apos;preferred&apos; sense P
• calculate the ratio of C-instances as-
signed to P, to C-instances assigned to
some sense other than P
3. order clues according to the value of the ratio
to give a &apos;decision list&apos;
4. assign each instance to a sense according to
the first clue in the decision list which holds
for the instance
5. if all instances are classified (or no new
instances have been newly classified/re-
classified on this iteration, or some other
stopping condition is met) STOP;
else return to step 2
</listItem>
<bodyText confidence="0.999929653846154">
Yarowsky notes that the most effective initial
seeding option he considered was labelling salient
corpus collocates with different senses. The user&apos;s
first interaction with WASPBENCH is just that.
At the user-input stage, only clues involving
grammatical relations are used. At the WSD al-
gorithm stage, some &amp;quot;bag-of-words&amp;quot; and n-gram
clues are also considered. Any content word (lem-
matised) occurring within a k-word window of the
nodeword is a bag-of-words clue. (The user can
set the value of k. The default is currently 30.)
N-gram clues capture local context which may not
be covered by any grammatical relation. The n-
gram clues are all bigrams and trigrams including
the nodeword.
Yarowsky&apos;s algorithm was selected because it
operated with easily human-readable clues, in-
tegrated straightforwardly with the WASPBENCH
modus operandi, and was or was close to being
the highest-performing system in the SENSEVAL
evaluations (Kilgarriff and Rosenzweig, 2000; Ed-
monds and Kilgarriff, 2002). The algorithm is a
&amp;quot;winner-take-all&amp;quot; algorithm: for an instance to be
disambiguated, the first matching context in the
decision-list is identified, and this alone classifies
the data instance5.
</bodyText>
<sectionHeader confidence="0.996996" genericHeader="acknowledgments">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.943603">
Evaluation presented a number of challenges:
</bodyText>
<listItem confidence="0.9956019375">
• We straddle three communities - commer-
cial dictionary-making, HLT/WSD research,
commercial/research MT - each with very
different ideas about what makes a technol-
ogy useful.
• There are no precedents. WASPBENCH
performs a function - corpus-based
disambiguating-lexicon development with
human input - which no other technology
performs. This leaves us with no points of
comparison.
• On the lexicography front: human analysis of
meaning is decidedly &apos;craft&apos; rather than &apos;sci-
ence&apos;. WASPBENCH aims to help lexicogra-
phers do their job better and faster. But there
is no tradition for even qualitative, let alone
</listItem>
<footnote confidence="0.9022004">
5Recent work (Yarowsky and Florian, 2002) has sug-
gested that the winner-take-all strategy is not always the best
strategy if the best clue is not a very good clue. In future work
we would like to extend the WASPBENCH to take account of
this insight.
</footnote>
<page confidence="0.998921">
213
</page>
<bodyText confidence="0.9411145">
quantitative, analysis of performance at this
task, either for speed or quality of output.
</bodyText>
<listItem confidence="0.870665">
• A critical question for commercial MT would
be &amp;quot;does it take less time to produce a word
</listItem>
<bodyText confidence="0.988392054054054">
expert using WASPBENCH, than using tradi-
tional methods, for the same quality of out-
put&amp;quot;. We are constrained in pursuing this
route, being without access to MT compa-
nies&apos; lexicography budgets or strategies.
In the light of these issues, we have adopted a
&apos;divide and rule&apos; strategy, setting up different eval-
uation themes for different perspectives. We pur-
sued five approaches:
SENSEVAL — seen purely as a WSD system,
WASPBENCH performed on a par with the best in
the world (Tugwell and Kilgarriff, 2001).
Expert review — three experienced lexicogra-
phers reviewed WASPBENCH very favourably, also
providing detailed feedback for future develop-
ment.
Comparison with MT — students at Leeds Uni-
versity6 were able to produce (with minimal train-
ing) word experts for medium-complexity words
in 30 minutes which outperformed translation of
ambiguous words by commercially-available MT
systems (Koeling et al., 2003).
Consistency of results — subjects at IIIT, Hyder-
abad, India7 confirmed the Leeds result and estab-
lished that different subjects produced consistent
results from the same data (Koeling and Kilgarriff,
2002).
Word sketches — lexicographers preparing the
new Macmillan English Dictionary for Advanced
Leaners (Rundell, 2002) successfully used word
sketches as the primary source of evidence for
the behaviour of all medium and high frequency
nouns, verbs and adjectives (Kilgarriff and Run-
dell, 2002).
These evaluations demonstrate that WASP-
BENCH does support accurate, efficient, semi-
automatic, integrated meaning analysis and WSD
</bodyText>
<footnote confidence="0.99563375">
6We would like to thank Prof. Tony Hartley for his help in
setting this up.
7We would like to thank Prof. Rajeev Sangal and Mrs.
Amba Kulkani for their help in setting this up.
</footnote>
<bodyText confidence="0.998105">
lexicon development, and that word sketches are
useful for lexicography and other language re-
search.
The WASPBENCH can be trialled at
http ://w asp s .itri.brighton.ac.uk.
</bodyText>
<sectionHeader confidence="0.992131" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999610842105263">
Philip Edmonds and Adam Kilgarriff. 2002. Introduction to
the special issue on evaluating word sense disambiguation
systems. Journal of Natural Language Engineering, 8(4).
Adam Kilgarriff and Joseph Rosenzweig. 2000. Framework
and results for English SENSEVAL. Computers and the
Humanities, 34(1-2):15-48. Special Issue on SENSEVAL,
edited by Adam Kilgarriff and Martha Palmer.
Adam Kilgarriff and Michael Rundell. 2002. Lexical profil-
ing software and its lexicographical applications - a case
study. In EURALEX 02, Copenhagen, August.
Adam Kilgarriff. 1998. The hard parts of lexicography. In-
ternational Journal of Lexicography, 11(1):51-54.
Rob Koeling and Adam Kilgarriff. 2002. Evaluating
the WASPbench, a lexicography tool incorporating word
sense disambiguation. In Proc. ICON, International Con-
ference on Natural Language Processing, Mumbai, India,
December.
Rob Koeling, Adam Kilgarriff, David Tugwell, and Roger
Evans. 2003. An evaluation of a Lexicographer&apos;s Work-
bench: building lexicons for Machine Translation. In
Proc. EAMT workshop at EACL03, Budapest, Hungary,
April.
Michael Rundell, editor. 2002. Macmillan English Dictio-
nary for Advanced Learners. Macmillan, London.
David Tugwell and Adam Kilgarriff. 2001. WASPBENCH: a
lexicographic tool supporting WSD. ln Proc. SENSEVAL-
2: Second International Workshop on Evaluating WSD
Systems, pages 151-154, Toulouse, July. ACL.
David Yarowsky and Radu Florian. 2002. Evaluating
sense disambiguation performance across diverse param-
eter spaces. Journal of Natural Language Engineering,
8(4):In press. Special Issue on Evaluating Word Sense
Disambiguation Systems.
David Yarowsky. 1993. One sense per collocation. In Proc.
ARPA Human Language Technology Workshop, Princeton.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivalling supervised methods. In ACL 95, pages
189-196, MIT.
</reference>
<page confidence="0.998949">
214
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.131684">
<title confidence="0.59633">lexicographer&apos;s workbench supporting state-of-the-art word sense disambiguation.</title>
<author confidence="0.9332295">Adam Kilgarriff</author>
<author confidence="0.9332295">Roger Evans</author>
<author confidence="0.9332295">Rob Koeling Michael Rundell</author>
<author confidence="0.9332295">David Tugwell</author>
<affiliation confidence="0.999713">ITRI, University of Brighton</affiliation>
<abstract confidence="0.995544561643834">Firstname.Lastname@itri.brighton.ac.uk 1 Background Human Language Technologies (HLT) need dictionaries, to tell them what words mean and how they behave. People making dictionaries (lexicographers) need HLT, to help them identify how words behave so they can make better dictionaries. Thus a potential for synergy exists across the range of lexical data in the construction of headword lists, for spelling correction, phonetics, morphology and syntax, but nowhere more than for semantics, and in particular the vexed question of how a word&apos;s meaning should be analysed into distinct senses. HLT needs all the help it can get from dictionaries, because it is a very hard problem to identify which meaning of a word applies. Lexicographers need all the help they can get because the analysis of meaning is the second hardest part of their job (Kilgarriff, 1998), it occupies a large share of their working hours, and it is one where, currently, they have very little to go on beyond intuition and other dictionaries. Thus HLT system developers and corpus lexicographers can both benefit from a tool for finding and organizing the distinctive patterns of use of words in texts. Such a tool would be an asset for both language research and lexicon development, particularly for lexicons for Machine Trans- We have developed the PB EN CH, tool that (1) presents a &amp;quot;word sketch&amp;quot;, a summary of the corpus evidence for a word, to the lexicographer; (2) supports the lexicographer in analysing the word into its distinct meanings and (3) uses the lexicographer&apos;s analysis as the input to a stateof-the-art word sense disambiguation (WSD) algorithm, the output of which is a &amp;quot;word expert&amp;quot; which can then disambiguate new instances of the word. PB ENCH 2.1 Grammatical relations database central resource of a collection of all grammatical relations holding between in the corpus. SPBENCH currently based on the British National Corpus&apos; (BNC): 100 million words of contemporary British English, of a wide range of genres. Using finite-state techniques operating over part-of-speech tags, we process the whole corpus finding quintuples of the form: {Rel, W1 , W2, Prep, Pos} where Rel is a relation, W1 is the lemma of the word for which Rel holds, W2 is the lemma of the other open-class word involved, Prep is the preposition or particle involved and Pos is the position of W1 in the corpus. Relations may have null values for W2 and Prep. The database contains 70 million quintuples. The inventory of relations is shown in Table 1. are nine (ie. with W2 and null), seven with Prep null, with W2 null and one relation with no null elements. All inverse relaie. found by taking W2 as head word instead of W1 are explicitly repre- 1http://info.ox.ac.uldbnc 211 relation example bare-noun the angle of bank&apos; possessive my bank&apos; plural the banks&apos; passive reflexive ing-comp fish finite-comp came inf-comp eat fish wh-comp he came subject refused&apos; object adj-comp noun-modifier bank&apos; modifier bank&amp;quot; and-or predicate particle grow&amp;quot; up&amp;quot; Prep+gerund or fish PP-comp/mod or Table 1: Grammatical Relations giving six extra binary and one extra trinary relation, to give a total of twenty-six distinct relations. These relations provide a flexible resource to be used as the basis of the compuof S PB E N CH. The relations contain a substantial number of errors, originating from POS-tagging errors in the BNC, attachment ambiguities, or limitations of the pattern-matching grammar. However, as the system finds high-salience patterns, given enough data, the noise does not present great problems. 2.2 Word Sketches When the lexicographer starts working on a word, s/he enters the word (and word class) at a prompt. Using the grammatical relations database, the systhen composes a sketch the word. This is a page of data such as Table 2, which shows, for the word in question (W1), ordered lists of high-salience grammatical relations, relation- W2 pairs, and relation-W2-Prep triples for the word. The number of patterns shown is set by the user, but will typically be over 200. These are listed each relation in order of with the considered symmetrical so does not give rise to a new inverse relation. is estimated as the product of Mutual Inforcount of corpus instances. The instances can be instantly retrieved and shown in a concordance window. Producing a word sketch for a medium-tofrequency word takes around ten 2.3 Matching patterns with senses The next task is to enter a preliminary list of senses for the word, in the form of some arbitrary perhaps CLOUD three senses of inventory may be drawn from the user&apos;s knowledge, from a perusal of the word sketch, or from a pre-existing dictionary entry. As Table 2 shows, and in keeping with &amp;quot;one sense per collocation&amp;quot; (Yarowsky, 1993) in most high-salience patterns or just one of the word&apos;s senses. The user then has the task of associating, by selecting from a pop-up menu, the required sense for unambiguous clues. Reference can be made at any time to the actual corpus instances, which demonstrate the contexts in which the triple occurs. The number of relations marked will depend on the time available to the lexicographer, as well as the complexity of the sense division to be made. The act of assigning senses to patterns may very well lead the lexicographer to discover fresh, unconsidered senses or subsenses of the word. If so, extra sense mnemonics can be added. When the user deems that sufficient patterns have been marked with senses, the pattern-sense pairs are submitted to the next stage: automatic disambiguation. 2.4 The Disambiguation Algorithm Yarowsky&apos;s decision list approach to WSD (Yarowsky, 1995). This is a bootstrapping algorithm that, given some initial seeding, iteratively divides the corpus examples into the different senses. Given a set of classified color a set of corpus for the word, the algorithm is as follows: mation and log frequency. Our experience of working lexicographers&apos; use of Mutual Information or log-likelihood lists shows that, for lexicographic purposes, these over-emphasise low frequency items, and that multiplying by log frequency is an appropriate adjustment. set of pre-compiled word sketches can be adam.kilgarriff/wordsketches.html 212 subj-of num sal obj-of num sal modifier num sal n-mod num sal lend 95 21.2 burst 27 16.4 central 755 25.5 merchant 213 29.4 issue 60 11.8 rob 31 15.3 Swiss 87 18.7 clearing 127 27.0 charge 29 9.5 overflow 7 10.2 commercial 231 18.6 river 217 25.4 operate 45 8.9 line 13 8.4 grassy 42 18.5 creditor 52 22.8 modifies PP iv-PP and-or holiday 404 32.6 of England 988 37.5 governor of 108 26.2 society 287 24.6 account 503 32.0 of Scotland 242 26.9 balance at 25 20.2 bank 107 17.7 loan 108 27.5 of river 111 22.1 borrow from 42 19.1 institution 82 16.0 lending 68 26.1 of Thames 41 20.1 account with 30 18.4 Lloyds 11 14.1 2: Extract of word sketch for instances containing a classified clue to the appropriate sense 2. for each clue C (already classified or not) • for each sense, count the instances where C holds which are assigned to it • identify C&apos;s &apos;preferred&apos; sense P • calculate the ratio of C-instances assigned to P, to C-instances assigned to some sense other than P 3. order clues according to the value of the ratio to give a &apos;decision list&apos; 4. assign each instance to a sense according to the first clue in the decision list which holds for the instance 5. if all instances are classified (or no new instances have been newly classified/reclassified on this iteration, or some other stopping condition is met) STOP; else return to step 2 Yarowsky notes that the most effective initial seeding option he considered was labelling salient corpus collocates with different senses. The user&apos;s interaction with just that. At the user-input stage, only clues involving grammatical relations are used. At the WSD algorithm stage, some &amp;quot;bag-of-words&amp;quot; and n-gram clues are also considered. Any content word (lemmatised) occurring within a k-word window of the nodeword is a bag-of-words clue. (The user can the value of default is currently 30.) N-gram clues capture local context which may not covered by any grammatical relation. The ngram clues are all bigrams and trigrams including the nodeword. Yarowsky&apos;s algorithm was selected because it operated with easily human-readable clues, instraightforwardly with the operandi, was or was close to being highest-performing system in the evaluations (Kilgarriff and Rosenzweig, 2000; Edmonds and Kilgarriff, 2002). The algorithm is a &amp;quot;winner-take-all&amp;quot; algorithm: for an instance to be disambiguated, the first matching context in the decision-list is identified, and this alone classifies data 3 Evaluation Evaluation presented a number of challenges: • We straddle three communities commercial dictionary-making, HLT/WSD research, commercial/research MT each with very different ideas about what makes a technology useful. There are no precedents. WASPBENCH performs a function corpus-based disambiguating-lexicon development with human input which no other technology performs. This leaves us with no points of comparison. • On the lexicography front: human analysis of meaning is decidedly &apos;craft&apos; rather than &apos;scito help lexicographers do their job better and faster. But there is no tradition for even qualitative, let alone work (Yarowsky and Florian, 2002) has suggested that the winner-take-all strategy is not always the best strategy if the best clue is not a very good clue. In future work would like to extend the take account of this insight. 213 quantitative, analysis of performance at this task, either for speed or quality of output. • A critical question for commercial MT would be &amp;quot;does it take less time to produce a word using using traditional methods, for the same quality of output&amp;quot;. We are constrained in pursuing this route, being without access to MT companies&apos; lexicography budgets or strategies. In the light of these issues, we have adopted a &apos;divide and rule&apos; strategy, setting up different evaluation themes for different perspectives. We pursued five approaches: — purely as a WSD system, on a par with the best in the world (Tugwell and Kilgarriff, 2001). review — experienced lexicograreviewed favourably, also providing detailed feedback for future development. with MT — at Leeds Uniwere able to produce (with minimal training) word experts for medium-complexity words in 30 minutes which outperformed translation of ambiguous words by commercially-available MT systems (Koeling et al., 2003). of results — at Hyderconfirmed the Leeds result and established that different subjects produced consistent results from the same data (Koeling and Kilgarriff, 2002). sketches — preparing the new Macmillan English Dictionary for Advanced Leaners (Rundell, 2002) successfully used word sketches as the primary source of evidence for the behaviour of all medium and high frequency nouns, verbs and adjectives (Kilgarriff and Rundell, 2002). evaluations demonstrate that WASPsupport accurate, efficient, semiautomatic, integrated meaning analysis and WSD would like to thank Prof. Tony Hartley for his help in setting this up. would like to thank Prof. Rajeev Sangal and Mrs. Amba Kulkani for their help in setting this up. lexicon development, and that word sketches are useful for lexicography and other language research. be trialled at http ://w asp s .itri.brighton.ac.uk.</abstract>
<intro confidence="0.967067">References</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philip Edmonds</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Introduction to the special issue on evaluating word sense disambiguation systems.</title>
<date>2002</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="9385" citStr="Edmonds and Kilgarriff, 2002" startWordPosition="1556" endWordPosition="1560"> Any content word (lemmatised) occurring within a k-word window of the nodeword is a bag-of-words clue. (The user can set the value of k. The default is currently 30.) N-gram clues capture local context which may not be covered by any grammatical relation. The ngram clues are all bigrams and trigrams including the nodeword. Yarowsky&apos;s algorithm was selected because it operated with easily human-readable clues, integrated straightforwardly with the WASPBENCH modus operandi, and was or was close to being the highest-performing system in the SENSEVAL evaluations (Kilgarriff and Rosenzweig, 2000; Edmonds and Kilgarriff, 2002). The algorithm is a &amp;quot;winner-take-all&amp;quot; algorithm: for an instance to be disambiguated, the first matching context in the decision-list is identified, and this alone classifies the data instance5. 3 Evaluation Evaluation presented a number of challenges: • We straddle three communities - commercial dictionary-making, HLT/WSD research, commercial/research MT - each with very different ideas about what makes a technology useful. • There are no precedents. WASPBENCH performs a function - corpus-based disambiguating-lexicon development with human input - which no other technology performs. This lea</context>
</contexts>
<marker>Edmonds, Kilgarriff, 2002</marker>
<rawString>Philip Edmonds and Adam Kilgarriff. 2002. Introduction to the special issue on evaluating word sense disambiguation systems. Journal of Natural Language Engineering, 8(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>Framework and results for English SENSEVAL. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--1</pages>
<note>Special Issue on SENSEVAL, edited by Adam Kilgarriff</note>
<contexts>
<context position="9354" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="1552" endWordPosition="1555">n-gram clues are also considered. Any content word (lemmatised) occurring within a k-word window of the nodeword is a bag-of-words clue. (The user can set the value of k. The default is currently 30.) N-gram clues capture local context which may not be covered by any grammatical relation. The ngram clues are all bigrams and trigrams including the nodeword. Yarowsky&apos;s algorithm was selected because it operated with easily human-readable clues, integrated straightforwardly with the WASPBENCH modus operandi, and was or was close to being the highest-performing system in the SENSEVAL evaluations (Kilgarriff and Rosenzweig, 2000; Edmonds and Kilgarriff, 2002). The algorithm is a &amp;quot;winner-take-all&amp;quot; algorithm: for an instance to be disambiguated, the first matching context in the decision-list is identified, and this alone classifies the data instance5. 3 Evaluation Evaluation presented a number of challenges: • We straddle three communities - commercial dictionary-making, HLT/WSD research, commercial/research MT - each with very different ideas about what makes a technology useful. • There are no precedents. WASPBENCH performs a function - corpus-based disambiguating-lexicon development with human input - which no othe</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>Adam Kilgarriff and Joseph Rosenzweig. 2000. Framework and results for English SENSEVAL. Computers and the Humanities, 34(1-2):15-48. Special Issue on SENSEVAL, edited by Adam Kilgarriff and Martha Palmer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Michael Rundell</author>
</authors>
<title>Lexical profiling software and its lexicographical applications - a case study.</title>
<date>2002</date>
<booktitle>In EURALEX 02,</booktitle>
<location>Copenhagen,</location>
<marker>Kilgarriff, Rundell, 2002</marker>
<rawString>Adam Kilgarriff and Michael Rundell. 2002. Lexical profiling software and its lexicographical applications - a case study. In EURALEX 02, Copenhagen, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>The hard parts of lexicography.</title>
<date>1998</date>
<journal>International Journal of Lexicography,</journal>
<pages>11--1</pages>
<contexts>
<context position="1058" citStr="Kilgarriff, 1998" startWordPosition="163" endWordPosition="164">ve so they can make better dictionaries. Thus a potential for synergy exists across the range of lexical data - in the construction of headword lists, for spelling correction, phonetics, morphology and syntax, but nowhere more than for semantics, and in particular the vexed question of how a word&apos;s meaning should be analysed into distinct senses. HLT needs all the help it can get from dictionaries, because it is a very hard problem to identify which meaning of a word applies. Lexicographers need all the help they can get because the analysis of meaning is the second hardest part of their job (Kilgarriff, 1998), it occupies a large share of their working hours, and it is one where, currently, they have very little to go on beyond intuition and other dictionaries. Thus HLT system developers and corpus lexicographers can both benefit from a tool for finding and organizing the distinctive patterns of use of words in texts. Such a tool would be an asset for both language research and lexicon development, particularly for lexicons for Machine Translation. We have developed the WAS PB EN CH, a tool that (1) presents a &amp;quot;word sketch&amp;quot;, a summary of the corpus evidence for a word, to the lexicographer; (2) su</context>
</contexts>
<marker>Kilgarriff, 1998</marker>
<rawString>Adam Kilgarriff. 1998. The hard parts of lexicography. International Journal of Lexicography, 11(1):51-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Evaluating the WASPbench, a lexicography tool incorporating word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proc. ICON, International Conference on Natural Language Processing,</booktitle>
<location>Mumbai, India,</location>
<contexts>
<context position="11799" citStr="Koeling and Kilgarriff, 2002" startWordPosition="1938" endWordPosition="1941"> Kilgarriff, 2001). Expert review — three experienced lexicographers reviewed WASPBENCH very favourably, also providing detailed feedback for future development. Comparison with MT — students at Leeds University6 were able to produce (with minimal training) word experts for medium-complexity words in 30 minutes which outperformed translation of ambiguous words by commercially-available MT systems (Koeling et al., 2003). Consistency of results — subjects at IIIT, Hyderabad, India7 confirmed the Leeds result and established that different subjects produced consistent results from the same data (Koeling and Kilgarriff, 2002). Word sketches — lexicographers preparing the new Macmillan English Dictionary for Advanced Leaners (Rundell, 2002) successfully used word sketches as the primary source of evidence for the behaviour of all medium and high frequency nouns, verbs and adjectives (Kilgarriff and Rundell, 2002). These evaluations demonstrate that WASPBENCH does support accurate, efficient, semiautomatic, integrated meaning analysis and WSD 6We would like to thank Prof. Tony Hartley for his help in setting this up. 7We would like to thank Prof. Rajeev Sangal and Mrs. Amba Kulkani for their help in setting this up.</context>
</contexts>
<marker>Koeling, Kilgarriff, 2002</marker>
<rawString>Rob Koeling and Adam Kilgarriff. 2002. Evaluating the WASPbench, a lexicography tool incorporating word sense disambiguation. In Proc. ICON, International Conference on Natural Language Processing, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Adam Kilgarriff</author>
<author>David Tugwell</author>
<author>Roger Evans</author>
</authors>
<title>An evaluation of a Lexicographer&apos;s Workbench: building lexicons for Machine Translation. In</title>
<date>2003</date>
<booktitle>Proc. EAMT workshop at EACL03,</booktitle>
<location>Budapest, Hungary,</location>
<contexts>
<context position="11592" citStr="Koeling et al., 2003" startWordPosition="1907" endWordPosition="1910">ing up different evaluation themes for different perspectives. We pursued five approaches: SENSEVAL — seen purely as a WSD system, WASPBENCH performed on a par with the best in the world (Tugwell and Kilgarriff, 2001). Expert review — three experienced lexicographers reviewed WASPBENCH very favourably, also providing detailed feedback for future development. Comparison with MT — students at Leeds University6 were able to produce (with minimal training) word experts for medium-complexity words in 30 minutes which outperformed translation of ambiguous words by commercially-available MT systems (Koeling et al., 2003). Consistency of results — subjects at IIIT, Hyderabad, India7 confirmed the Leeds result and established that different subjects produced consistent results from the same data (Koeling and Kilgarriff, 2002). Word sketches — lexicographers preparing the new Macmillan English Dictionary for Advanced Leaners (Rundell, 2002) successfully used word sketches as the primary source of evidence for the behaviour of all medium and high frequency nouns, verbs and adjectives (Kilgarriff and Rundell, 2002). These evaluations demonstrate that WASPBENCH does support accurate, efficient, semiautomatic, integ</context>
</contexts>
<marker>Koeling, Kilgarriff, Tugwell, Evans, 2003</marker>
<rawString>Rob Koeling, Adam Kilgarriff, David Tugwell, and Roger Evans. 2003. An evaluation of a Lexicographer&apos;s Workbench: building lexicons for Machine Translation. In Proc. EAMT workshop at EACL03, Budapest, Hungary, April.</rawString>
</citation>
<citation valid="true">
<date>2002</date>
<booktitle>Macmillan English Dictionary for Advanced Learners.</booktitle>
<editor>Michael Rundell, editor.</editor>
<publisher>Macmillan,</publisher>
<location>London.</location>
<marker>2002</marker>
<rawString>Michael Rundell, editor. 2002. Macmillan English Dictionary for Advanced Learners. Macmillan, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Tugwell</author>
<author>Adam Kilgarriff</author>
</authors>
<title>WASPBENCH: a lexicographic tool supporting WSD. ln</title>
<date>2001</date>
<booktitle>Proc. SENSEVAL2: Second International Workshop on Evaluating WSD Systems,</booktitle>
<pages>151--154</pages>
<publisher>ACL.</publisher>
<location>Toulouse,</location>
<contexts>
<context position="11188" citStr="Tugwell and Kilgarriff, 2001" startWordPosition="1849" endWordPosition="1852">er for speed or quality of output. • A critical question for commercial MT would be &amp;quot;does it take less time to produce a word expert using WASPBENCH, than using traditional methods, for the same quality of output&amp;quot;. We are constrained in pursuing this route, being without access to MT companies&apos; lexicography budgets or strategies. In the light of these issues, we have adopted a &apos;divide and rule&apos; strategy, setting up different evaluation themes for different perspectives. We pursued five approaches: SENSEVAL — seen purely as a WSD system, WASPBENCH performed on a par with the best in the world (Tugwell and Kilgarriff, 2001). Expert review — three experienced lexicographers reviewed WASPBENCH very favourably, also providing detailed feedback for future development. Comparison with MT — students at Leeds University6 were able to produce (with minimal training) word experts for medium-complexity words in 30 minutes which outperformed translation of ambiguous words by commercially-available MT systems (Koeling et al., 2003). Consistency of results — subjects at IIIT, Hyderabad, India7 confirmed the Leeds result and established that different subjects produced consistent results from the same data (Koeling and Kilgar</context>
</contexts>
<marker>Tugwell, Kilgarriff, 2001</marker>
<rawString>David Tugwell and Adam Kilgarriff. 2001. WASPBENCH: a lexicographic tool supporting WSD. ln Proc. SENSEVAL2: Second International Workshop on Evaluating WSD Systems, pages 151-154, Toulouse, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Radu Florian</author>
</authors>
<title>Evaluating sense disambiguation performance across diverse parameter spaces.</title>
<date>2002</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="10290" citStr="Yarowsky and Florian, 2002" startWordPosition="1693" endWordPosition="1696">unities - commercial dictionary-making, HLT/WSD research, commercial/research MT - each with very different ideas about what makes a technology useful. • There are no precedents. WASPBENCH performs a function - corpus-based disambiguating-lexicon development with human input - which no other technology performs. This leaves us with no points of comparison. • On the lexicography front: human analysis of meaning is decidedly &apos;craft&apos; rather than &apos;science&apos;. WASPBENCH aims to help lexicographers do their job better and faster. But there is no tradition for even qualitative, let alone 5Recent work (Yarowsky and Florian, 2002) has suggested that the winner-take-all strategy is not always the best strategy if the best clue is not a very good clue. In future work we would like to extend the WASPBENCH to take account of this insight. 213 quantitative, analysis of performance at this task, either for speed or quality of output. • A critical question for commercial MT would be &amp;quot;does it take less time to produce a word expert using WASPBENCH, than using traditional methods, for the same quality of output&amp;quot;. We are constrained in pursuing this route, being without access to MT companies&apos; lexicography budgets or strategies.</context>
</contexts>
<marker>Yarowsky, Florian, 2002</marker>
<rawString>David Yarowsky and Radu Florian. 2002. Evaluating sense disambiguation performance across diverse parameter spaces. Journal of Natural Language Engineering, 8(4):In press. Special Issue on Evaluating Word Sense Disambiguation Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proc. ARPA Human Language Technology Workshop,</booktitle>
<location>Princeton.</location>
<contexts>
<context position="5450" citStr="Yarowsky, 1993" startWordPosition="908" endWordPosition="909">Mutual Inforcount of corpus instances. The instances can be instantly retrieved and shown in a concordance window. Producing a word sketch for a medium-tohigh frequency word takes around ten seconds.4 2.3 Matching patterns with senses The next task is to enter a preliminary list of senses for the word, in the form of some arbitrary mnemonics, perhaps MONEY, CLOUD and RIVER for three senses of bank. This inventory may be drawn from the user&apos;s knowledge, from a perusal of the word sketch, or from a pre-existing dictionary entry. As Table 2 shows, and in keeping with &amp;quot;one sense per collocation&amp;quot; (Yarowsky, 1993) in most cases, high-salience patterns or clues indicate just one of the word&apos;s senses. The user then has the task of associating, by selecting from a pop-up menu, the required sense for unambiguous clues. Reference can be made at any time to the actual corpus instances, which demonstrate the contexts in which the triple occurs. The number of relations marked will depend on the time available to the lexicographer, as well as the complexity of the sense division to be made. The act of assigning senses to patterns may very well lead the lexicographer to discover fresh, unconsidered senses or sub</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>David Yarowsky. 1993. One sense per collocation. In Proc. ARPA Human Language Technology Workshop, Princeton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivalling supervised methods.</title>
<date>1995</date>
<booktitle>In ACL 95,</booktitle>
<pages>189--196</pages>
<publisher>MIT.</publisher>
<contexts>
<context position="6376" citStr="Yarowsky, 1995" startWordPosition="1059" endWordPosition="1060"> the triple occurs. The number of relations marked will depend on the time available to the lexicographer, as well as the complexity of the sense division to be made. The act of assigning senses to patterns may very well lead the lexicographer to discover fresh, unconsidered senses or subsenses of the word. If so, extra sense mnemonics can be added. When the user deems that sufficient patterns have been marked with senses, the pattern-sense pairs are submitted to the next stage: automatic disambiguation. 2.4 The Disambiguation Algorithm WASPBENCH uses Yarowsky&apos;s decision list approach to WSD (Yarowsky, 1995). This is a bootstrapping algorithm that, given some initial seeding, iteratively divides the corpus examples into the different senses. Given a set of classified collocations, or clues, and a set of corpus instances for the word, the algorithm is as follows: mation and log frequency. Our experience of working lexicographers&apos; use of Mutual Information or log-likelihood lists shows that, for lexicographic purposes, these over-emphasise low frequency items, and that multiplying by log frequency is an appropriate adjustment. 4A set of pre-compiled word sketches can be seen at http://www.itri.brig</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivalling supervised methods. In ACL 95, pages 189-196, MIT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>