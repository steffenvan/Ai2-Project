<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996948">
Midge: Generating Image Descriptions From Computer Vision
Detections
</title>
<author confidence="0.8130775">
Margaret Mitchell† Jesse Dodge$$ Amit Goyal†† Kota Yamaguchi§ Karl StratosII
Xufeng Han§ Alyssa Mensch** Alex Berg§ Tamara Berg§ Hal Daum´e III††
</author>
<affiliation confidence="0.7591054">
† U. of Aberdeen and Oregon Health and Science University, m.mitchell@abdn.ac.uk
§ Stony Brook University, laberg,tlberg,xufhan,kyamagul@cs.stonybrook.edu
††U. of Maryland, fhal,amitl@umiacs.umd.edu
II Columbia University, stratos@cs.columbia.edu
$$U. of Washington, dodgejesse@gmail.com, **MIT, acmensch@mit.edu
</affiliation>
<sectionHeader confidence="0.989916" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948692307692">
This paper introduces a novel generation
system that composes humanlike descrip-
tions of images from computer vision de-
tections. By leveraging syntactically in-
formed word co-occurrence statistics, the
generator filters and constrains the noisy
detections output from a vision system to
generate syntactic trees that detail what
the computer vision system sees. Results
show that the generation system outper-
forms state-of-the-art systems, automati-
cally generating some of the most natural
image descriptions to date.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99322852173913">
It is becoming a real possibility for intelligent sys-
tems to talk about the visual world. New ways of
mapping computer vision to generated language
have emerged in the past few years, with a fo-
cus on pairing detections in an image to words
(Farhadi et al., 2010; Li et al., 2011; Kulkarni et
al., 2011; Yang et al., 2011). The goal in connect-
ing vision to language has varied: systems have
started producing language that is descriptive and
poetic (Li et al., 2011), summaries that add con-
tent where the computer vision system does not
(Yang et al., 2011), and captions copied directly
from other images that are globally (Farhadi et al.,
2010) and locally similar (Ordonez et al., 2011).
A commonality between all of these ap-
proaches is that they aim to produce natural-
sounding descriptions from computer vision de-
tections. This commonality is our starting point:
We aim to design a system capable of producing
natural-sounding descriptions from computer vi-
sion detections that are flexible enough to become
more descriptive and poetic, or include likely in-
The bus by the road with a clear blue sky
</bodyText>
<figureCaption confidence="0.982875">
Figure 1: Example image with generated description.
</figureCaption>
<bodyText confidence="0.988920692307692">
formation from a language model, or to be short
and simple, but as true to the image as possible.
Rather than using a fixed template capable of
generating one kind of utterance, our approach
therefore lies in generating syntactic trees. We
use a tree-generating process (Section 4.3) simi-
lar to a Tree Substitution Grammar, but preserv-
ing some of the idiosyncrasies of the Penn Tree-
bank syntax (Marcus et al., 1995) on which most
statistical parsers are developed. This allows us
to automatically parse and train on an unlimited
amount of text, creating data-driven models that
flesh out descriptions around detected objects in a
principled way, based on what is both likely and
syntactically well-formed.
An example generated description is given in
Figure 1, and example vision output/natural lan-
guage generation (NLG) input is given in Fig-
ure 2. The system (“Midge”) generates descrip-
tions in present-tense, declarative phrases, as a
naive viewer without prior knowledge of the pho-
tograph’s content.1
Midge is built using the following approach:
An image processed by computer vision algo-
rithms can be characterized as a triple &lt;Ai, Bi,
Ci&gt;, where:
</bodyText>
<footnote confidence="0.9995235">
1Midge is available to try online at:
http://recognition.cs.stonybrook.edu:8080/˜mitchema/midge/.
</footnote>
<page confidence="0.887309">
747
</page>
<note confidence="0.9972245">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 747–756,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
stuff: sky .999
id: 1
atts: clear:0.432, blue:0.945
grey:0.853, white:0.501...
b. box: (1,1 440,141)
stuff: road .908
id: 2
atts: wooden:0.722 clear:0.020...
b. box: (1,236 188,94)
object: bus .307
id: 3
atts: black:0.872, red:0.244 ...
b. box: (38,38 366,293)
preps: id 1, id 2: by id 1, id 3: by id 2, id 3: below
</note>
<figureCaption confidence="0.99473">
Figure 2: Example computer vision output and natu-
ral language generation input. Values correspond to
scores from the vision detections.
</figureCaption>
<listItem confidence="0.9688095">
• Ai is the set of object/stuff detections with
bounding boxes and associated “attribute”
detections within those bounding boxes.
• Bi is the set of action or pose detections as-
sociated to each ai E Ai.
• Ci is the set of spatial relationships that hold
between the bounding boxes of each pair
ai, aj E Ai.
Similarly, a description of an image can be char-
acterized as a triple &lt;Ad, Bd, Cd&gt; where:
• Ad is the set of nouns in the description with
associated modifiers.
• Bd is the set of verbs associated to each ad E
Ad.
• Cd is the set of prepositions that hold be-
tween each pair of ad, ae E Ad.
</listItem>
<bodyText confidence="0.999788269230769">
With this representation, mapping &lt;Ai, Bi, Ci&gt;
to &lt;Ad, Bd, Cd&gt; is trivial. The problem then
becomes: (1) How to filter out detections that
are wrong; (2) how to order the objects so that
they are mentioned in a natural way; (3) how to
connect these ordered objects within a syntacti-
cally/semantically well-formed tree; and (4) how
to add further descriptive information from lan-
guage modeling alone, if required.
Our solution lies in using Ai and Ad as descrip-
tion anchors. In computer vision, object detec-
tions form the basis of action/pose, attribute, and
spatial relationship detections; therefore, in our
approach to language generation, nouns for the
object detections are used as the basis for the de-
scription. Likelihood estimates of syntactic struc-
ture and word co-occurrence are conditioned on
object nouns, and this enables each noun head in
a description to select for the kinds of structures it
tends to appear in (syntactic constraints) and the
other words it tends to occur with (semantic con-
straints). This is a data-driven way to generate
likely adjectives, prepositions, determiners, etc.,
taking the intersection of what the vision system
predicts and how the object noun tends to be de-
scribed.
</bodyText>
<sectionHeader confidence="0.974478" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99969735">
Our approach to describing images starts with
a system from Kulkarni et al. (2011) that com-
poses novel captions for images in the PASCAL
sentence data set,2 introduced in Rashtchian et
al. (2010). This provides multiple object detec-
tions based on Felzenszwalb’s mixtures of multi-
scale deformable parts models (Felzenszwalb et
al., 2008), and stuff detections (roughly, mass
nouns, things like sky and grass) based on linear
SVMs for low level region features.
Appearance characteristics are predicted using
trained detectors for colors, shapes, textures, and
materials, an idea originally introduced in Farhadi
et al. (2009). Local texture, Histograms of Ori-
ented Gradients (HOG) (Dalal and Triggs, 2005),
edge, and color descriptors inside the bounding
box of a recognized object are binned into his-
tograms for a vision system to learn to recognize
when an object is rectangular, wooden, metal,
etc. Finally, simple preposition functions are used
to compute the spatial relations between objects
based on their bounding boxes.
The original Kulkarni et al. (2011) system gen-
erates descriptions with a template, filling in slots
by combining computer vision outputs with text
based statistics in a conditional random field to
predict the most likely image labeling. Template-
based generation is also used in the recent Yang et
al. (2011) system, which fills in likely verbs and
prepositions by dependency parsing the human-
written UIUC Pascal-VOC dataset (Farhadi et al.,
2010) and selecting the dependent/head relation
with the highest log likelihood ratio.
Template-based generation is useful for auto-
matically generating consistent sentences, how-
ever, if the goal is to vary or add to the text pro-
duced, it may be suboptimal (cf. Reiter and Dale
(1997)). Work that does not use template-based
generation includes Yao et al. (2010), who gener-
ate syntactic trees, similar to the approach in this
</bodyText>
<footnote confidence="0.980816">
2http://vision.cs.uiuc.edu/pascal-sentences/
</footnote>
<page confidence="0.982817">
748
</page>
<bodyText confidence="0.963394466666667">
Kulkarni et al.: This is a pic- Kulkarni et al.: This is
ture of three persons, one bot- a picture of two potted-
tle and one diningtable. The plants, one dog and one
first rusty person is beside the person. The black dog is
second person. The rusty bot- by the black person, and
tle is near the first rusty per- near the second feathered
son, and within the colorful pottedplant.
diningtable. The second per-
son is by the third rusty per-
son. The colorful diningtable
is near the first rusty person,
and near the second person,
and near the third rusty person.
Yang et al.: Three people Yang et al.: The person is
are showing the bottle on the sitting in the chair in the
</bodyText>
<figure confidence="0.9360355">
street room
Midge: people with a bottle at Midge: a person in black
the table with a black dog by potted
plants
</figure>
<figureCaption confidence="0.99510425">
Figure 3: Descriptions generated by Midge, Kulkarni
et al. (2011) and Yang et al. (2011) on the same images.
Midge uses the Kulkarni et al. (2011) front-end, and so
outputs are directly comparable.
</figureCaption>
<bodyText confidence="0.9926555">
paper. However, their system is not automatic, re-
quiring extensive hand-coded semantic and syn-
tactic details. Another approach is provided in
Li et al. (2011), who use image detections to se-
lect and combine web-scale n-grams (Brants and
Franz, 2006). This automatically generates de-
scriptions that are either poetic or strange (e.g.,
“tree snowing black train”).
A different line of work transfers captions of
similar images directly to a query image. Farhadi
et al. (2010) use &lt;object,action,scene&gt; triples
predicted from the visual characteristics of the
image to find potential captions. Ordonez et al.
(2011) use global image matching with local re-
ordering from a much larger set of captioned pho-
tographs. These transfer-based approaches result
in natural captions (they are written by humans)
that may not actually be true of the image.
This work learns and builds from these ap-
proaches. Following Kulkarni et al. and Li et al.,
the system uses large-scale text corpora to esti-
mate likely words around object detections. Fol-
lowing Yang et al., the system can hallucinate
likely words using word co-occurrence statistics
alone. And following Yao et al., the system aims
black, blue, brown, colorful, golden, gray,
green, orange, pink, red, silver, white, yel-
low, bare, clear, cute, dirty, feathered, flying,
furry, pine, plastic, rectangular, rusty, shiny,
spotted, striped, wooden
</bodyText>
<tableCaption confidence="0.998709">
Table 1: Modifiers used to extract training corpus.
</tableCaption>
<bodyText confidence="0.999953827586207">
for naturally varied but well-formed text, generat-
ing syntactic trees rather than filling in a template.
In addition to these tasks, Midge automatically
decides what the subject and objects of the de-
scription will be, leverages the collected word co-
occurrence statistics to filter possible incorrect de-
tections, and offers the flexibility to be as de-
scriptive or as terse as possible, specified by the
user at run-time. The end result is a fully au-
tomatic vision-to-language system that is begin-
ning to generate syntactically and semantically
well-formed descriptions with naturalistic varia-
tion. Example descriptions are given in Figures 4
and 5, and descriptions from other recent systems
are given in Figure 3.
The results are promising, but it is important to
note that Midge is a first-pass system through the
steps necessary to connect vision to language at
a deep syntactic/semantic level. As such, it uses
basic solutions at each stage of the process, which
may be improved: Midge serves as an illustration
of the types of issues that should be handled to
automatically generate syntactic trees from vision
detections, and offers some possible solutions. It
is evaluated against the Kulkarni et al. system, the
Yang et al. system, and human-written descrip-
tions on the same set of images in Section 5, and
is found to significantly outperform the automatic
systems.
</bodyText>
<sectionHeader confidence="0.97093" genericHeader="method">
3 Learning from Descriptive Text
</sectionHeader>
<bodyText confidence="0.999964285714286">
To train our system on how people describe im-
ages, we use 700,000 (Flickr, 2011) images with
associated descriptions from the dataset in Or-
donez et al. (2011). This is separate from our
evaluation image set, consisting of 840 PASCAL
images. The Flickr data is messier than datasets
created specifically for vision training, but pro-
vides the largest corpus of natural descriptions of
images to date.
We normalize the text by removing emoticons
and mark-up language, and parse each caption
using the Berkeley parser (Petrov, 2010). Once
parsed, we can extract syntactic information for
individual (word, tag) pairs.
</bodyText>
<page confidence="0.985896">
749
</page>
<figure confidence="0.7674645">
a cow with sheep with a gray sky people with boats a brown cow people at
green grass by the road a wooden table
</figure>
<figureCaption confidence="0.994241">
Figure 4: Example generated outputs.
</figureCaption>
<figure confidence="0.765640333333333">
Awkward Prepositions Incorrect Detections
a person boats under a black bicycle at the sky a yellow bus cows by black sheep
on the dog the sky a green potted plant with people by the road
</figure>
<figureCaption confidence="0.998546">
Figure 5: Example generated outputs: Not quite right
</figureCaption>
<bodyText confidence="0.999988575757576">
We compute the probabilities for different
prenominal modifiers (shiny, clear, glowing, ...)
and determiners (a/an, the, None, ...) given a
head noun in a noun phrase (NP), as well as the
probabilities for each head noun in larger con-
structions, listed in Section 4.3. Probabilities are
conditioned only on open-class words, specifi-
cally, nouns and verbs. This means that a closed-
class word (such as a preposition) is never used to
generate an open-class word.
In addition to co-occurrence statistics, the
parsed Flickr data adds to our understanding of
the basic characteristics of visually descriptive
text. Using WordNet (Miller, 1995) to automati-
cally determine whether a head noun is a physical
object or not, we find that 92% of the sentences
have no more than 3 physical objects. This in-
forms generation by placing a cap on how many
objects are mentioned in each descriptive sen-
tence: When more than 3 objects are detected,
the system splits the description over several sen-
tences. We also find that many of the descriptions
are not sentences as well (tagged as S, 58% of the
data), but quite commonly noun phrases (tagged
as NP, 28% of the data), and expect that the num-
ber of noun phrases that form descriptions will be
much higher with domain adaptation. This also
informs generation, and the system is capable of
generating both sentences (contains a main verb)
and noun phrases (no main verb) in the final im-
age description. We use the term ‘sentence’ in the
rest of this paper to refer to both kinds of complex
phrases.
</bodyText>
<sectionHeader confidence="0.998545" genericHeader="method">
4 Generation
</sectionHeader>
<bodyText confidence="0.999268">
Following Penn Treebank parsing guidelines
(Marcus et al., 1995), the relationship between
two head nouns in a sentence can usually be char-
acterized among the following:
</bodyText>
<listItem confidence="0.9501995">
1. prepositional (a boy on the table)
2. verbal (a boy cleans the table)
3. verb with preposition (a boy sits on the table)
4. verb with particle (a boy cleans up the table)
5. verb with S or SBAR complement (a boy
sees that the table is clean)
</listItem>
<bodyText confidence="0.99992845">
The generation system focuses on the first three
kinds of relationships, which capture a wide range
of utterances. The process of generation is ap-
proached as a problem of generating a semanti-
cally and syntactically well-formed tree based on
object nouns. These serve as head noun anchors
in a lexicalized syntactic derivation process that
we call tree growth.
Vision detections are associated to a {tag
word} pair, and the model fleshes out the tree de-
tails around head noun anchors by utilizing syn-
tactic dependencies between words learned from
the Flickr data discussed in Section 3. The anal-
ogy of growing a tree is quite appropriate here,
where nouns are bundles of constraints akin to
seeds, giving rise to the rest of the tree based on
the lexicalized subtrees in which the nouns are
likely to occur. An example generated tree struc-
ture is shown in Figure 6, with noun anchors in
bold.
</bodyText>
<page confidence="0.970345">
750
</page>
<figure confidence="0.833931">
NP
</figure>
<figureCaption confidence="0.999681">
Figure 6: Tree generated from tree growth process.
</figureCaption>
<bodyText confidence="0.999964724137931">
Midge was developed using detections run on
Flickr images, incorporating action/pose detec-
tions for verbs as well as object detections for
nouns. In testing, we generate descriptions for
the PASCAL images, which have been used in
earlier work on the vision-to-language connection
(Kulkarni et al., 2011; Yang et al., 2011), and al-
lows us to compare systems directly. Action and
pose detection for this data set still does not work
well, and so the system does not receive these de-
tections from the vision front-end. However, the
system can still generate verbs when action and
pose detectors have been run, and this framework
allows the system to “hallucinate” likely verbal
constructions between objects if specified at run-
time. A similar approach was taken in Yang et al.
(2011). Some examples are given in Figure 7.
We follow a three-tiered generation process
(Reiter and Dale, 2000), utilizing content determi-
nation to first cluster and order the object nouns,
create their local subtrees, and filter incorrect de-
tections; microplanning to construct full syntactic
trees around the noun clusters, and surface real-
ization to order selected modifiers, realize them as
postnominal or prenominal, and select final out-
puts. The system follows an overgenerate-and-
select approach (Langkilde and Knight, 1998),
which allows different final trees to be selected
with different settings.
</bodyText>
<subsectionHeader confidence="0.99345">
4.1 Knowledge Base
</subsectionHeader>
<bodyText confidence="0.999962">
Midge uses a knowledge base that stores models
for different tasks during generation. These mod-
els are primarily data-driven, but we also include
a hand-built component to handle a small set of
rules. The data-driven component provides the
syntactically informed word co-occurrence statis-
tics learned from the Flickr data, a model for or-
dering the selected nouns in a sentence, and a
model to change computer vision attributes to at-
tribute:value pairs. Below, we discuss the three
main data-driven models within the generation
</bodyText>
<subsectionHeader confidence="0.795034">
Unordered Ordered
</subsectionHeader>
<bodyText confidence="0.9791175">
bottle, table, person person, bottle, table
road, sky, cow cow, road, sky
</bodyText>
<figureCaption confidence="0.997996">
Figure 8: Example nominal orderings.
</figureCaption>
<bodyText confidence="0.999824">
pipeline. The hand-built component contains plu-
ral forms of singular nouns, the list of possible
spatial relations shown in Table 3, and a map-
ping between attribute values and modifier sur-
face forms (e.g., a green detection for person is to
be realized as the postnominal modifier in green).
</bodyText>
<subsectionHeader confidence="0.9902315">
4.2 Content Determination
4.2.1 Step 1: Group the Nouns
</subsectionHeader>
<bodyText confidence="0.999955222222222">
An initial set of object detections must first be
split into clusters that give rise to different sen-
tences. If more than 3 objects are detected in the
image, the system begins splitting these into dif-
ferent noun groups. In future work, we aim to
compare principled approaches to this task, e.g.,
using mutual information to cluster similar nouns
together. The current system randomizes which
nouns appear in the same group.
</bodyText>
<subsubsectionHeader confidence="0.523367">
4.2.2 Step 2: Order the Nouns
</subsubsectionHeader>
<bodyText confidence="0.999471772727273">
Each group of nouns are then ordered to deter-
mine when they are mentioned in a sentence. Be-
cause the system generates declarative sentences,
this automatically determines the subject and ob-
jects. This is a novel contribution for a general
problem in NLG, and initial evaluation (Section
5) suggests it works reasonably well.
To build the nominal ordering model, we use
WordNet to associate all head nouns in the Flickr
data to all of their hypernyms. A description is
represented as an ordered set [a1...a,,,] where each
ap is a noun with position p in the set of head
nouns in the sentence. For the position pi of each
hypernym ha in each sentence with n head nouns,
we estimate p(pi|n, ha).
During generation, the system greedily maxi-
mizes p(pi|n, ha) until all nouns have been or-
dered. Example orderings are shown in Figure 8.
This model automatically places animate objects
near the beginning of a sentence, which follows
psycholinguistic work in object naming (Branigan
et al., 2007).
</bodyText>
<subsectionHeader confidence="0.829391">
4.2.3 Step 3: Filter Incorrect Attributes
</subsectionHeader>
<bodyText confidence="0.999985333333333">
For the system to be able to extend coverage as
new computer vision attribute detections become
available, we develop a method to automatically
</bodyText>
<figure confidence="0.98041125">
DT
NN
bottle
a
PP
NP
DT
the
NP
DT
-
NP
NN
people
IN
with
PP
NP
IN
at
NN
table
751
A person sitting on a sofa Cows grazing Airplanes flying A person walking a dog
</figure>
<figureCaption confidence="0.987229">
Figure 7: Hallucinating: Creating likely actions. Straightforward to do, but can often be wrong.
</figureCaption>
<table confidence="0.98096275">
COLOR purple blue green red white ...
MATERIAL plastic wooden silver ...
SURFACE furry fluffy hard soft ...
QUALITY shiny rust dirty broken ...
</table>
<tableCaption confidence="0.999587">
Table 2: Example attribute classes and values.
</tableCaption>
<bodyText confidence="0.999912166666666">
group adjectives into broader attribute classes,3
and the generation system uses these classes when
deciding how to describe objects. To group adjec-
tives, we use a bootstrapping technique (Kozareva
et al., 2008) that learns which adjectives tend to
co-occur, and groups these together to form an at-
tribute class. Co-occurrence is computed using
cosine (distributional) similarity between adjec-
tives, considering adjacent nouns as context (i.e.,
JJ NN constructions). Contexts (nouns) for adjec-
tives are weighted using Pointwise Mutual Infor-
mation and only the top 1000 nouns are selected
for every adjective. Some of the learned attribute
classes are given in Table 2.
In the Flickr corpus, we find that each attribute
(COLOR, SIZE, etc.), rarely has more than a single
value in the final description, with the most com-
mon (COLOR) co-occurring less than 2% of the
time. Midge enforces this idea to select the most
likely word v for each attribute from the detec-
tions. In a noun phrase headed by an object noun,
NP{NN noun}, the prenominal adjective (JJ v) for
each attribute is selected using maximum likeli-
hood.
</bodyText>
<subsectionHeader confidence="0.839159">
4.2.4 Step 4: Group Plurals
</subsectionHeader>
<bodyText confidence="0.906855333333333">
How to generate natural-sounding spatial rela-
tions and modifiers for a set of objects, as opposed
to a single object, is still an open problem (Fu-
nakoshi et al., 2004; Gatt, 2006). In this work, we
use a simple method to group all same-type ob-
jects together, associate them to the plural form
listed in the KB, discard the modifiers, and re-
turn spatial relations based on the first recognized
3What in computer vision are called attributes are called
values in NLG. A value like red belongs to a COLOR at-
tribute, and we use this distinction in the system.
member of the group.
</bodyText>
<figure confidence="0.891336538461538">
4.2.5 Step 5: Gather Local Subtrees Around
Object Nouns
1 2
NP
DT{0,1} ↓ JJ* ↓ NN S
n NP{NN n} VP{VBZ} ↓
3 4
NP NP
NP{NN n} VP{VB(G|N)} ↓
5 6
PP
VP
VB(G|N|Z) ↓ NP{NN n}
</figure>
<figureCaption confidence="0.979580833333333">
Figure 9: Initial subtree frames for generation, present-
tense declarative phrases. 1 marks a substitution site,
* marks &gt; 0 sister nodes of this type permitted, {0,1}
marks that this node can be included of excluded.
Input: set of ordered nouns, Output: trees preserving
nominal ordering.
</figureCaption>
<bodyText confidence="0.999430315789474">
Possible actions/poses and spatial relationships
between objects nouns, represented by verbs and
prepositions, are selected using the subtree frames
listed in Figure 9. Each head noun selects for its
likely local subtrees, some of which are not fully
formed until the Microplanning stage. As an ex-
ample of how this process works, see Figure 10,
which illustrates the combination of Trees 4 and
5. For simplicity, we do not include the selection
of further subtrees. The subject noun duck se-
lects for prepositional phrases headed by different
prepositions, and the object noun grass selects
for prepositions that head the prepositional phrase
in which it is embedded. Full PP subtrees are cre-
ated during Microplanning by taking the intersec-
tion of both.
The leftmost noun in the sequence is given a
rightward directionality constraint, placing it as
the subject of the sentence, and so it will only se-
</bodyText>
<equation confidence="0.666943333333333">
IN ↓
NP{NN n}
7
NP{NN n} PP{IN} ↓
VP
VB(G|N|Z) ↓ PP{IN} ↓
</equation>
<page confidence="0.748249">
752
</page>
<figure confidence="0.9707504">
a over b a above b b below a b beneath a a by b b by a a on b b under a
b underneath a a upon b a over b
abyb a against b b against a b around a a around b a at b b at a a beside b
b beside a a by b b by a a near b b near a b with a a with b
ainb a in b b outside a a within b a by b b by a
</figure>
<tableCaption confidence="0.846539">
Table 3: Possible prepositions from bounding boxes.
</tableCaption>
<figure confidence="0.900145555555556">
Subtree frames:
NP
NP{NN n1} PP{IN} ↓
Generated subtrees:
NP PP
NP PP IN NP
NN IN on,by,over NN
duck above,on,by grass
Combined trees:
</figure>
<figureCaption confidence="0.999131">
Figure 10: Example derivation.
</figureCaption>
<bodyText confidence="0.999813166666667">
lect for trees that expand to the right. The right-
most noun is given a leftward directionality con-
straint, placing it as an object, and so it will only
select for trees that expand to its left. The noun in
the middle, if there is one, selects for all its local
subtrees, combining first with a noun to its right
or to its left. We now walk through the deriva-
tion process for each of the listed subtree frames.
Because we are following an overgenerate-and-
select approach, all combinations above a proba-
bility threshold a and an observation cutoff -y are
created.
</bodyText>
<sectionHeader confidence="0.474076" genericHeader="method">
Tree 1:
</sectionHeader>
<listItem confidence="0.985615333333333">
Collect all NP → (DT det) (JJ adj)* (NN noun)
and NP → (JJ adj)* (NN noun) subtrees, where:
• p((JJ adj)|(NN noun)) &gt; a for each adj
• p((DT det)|JJ, (NN noun)) &gt; a, and the proba-
bility of a determiner for the head noun is higher
than the probability of no determiner.
</listItem>
<bodyText confidence="0.941132526315789">
Any number of adjectives (including none) may
be generated, and we include the presence or ab-
sence of an adjective when calculating which de-
terminer to include.
The reasoning behind the generation of these
subtrees is to automatically learn whether to treat
a given noun as a mass or count noun (not taking a
determiner or taking a determiner, respectively) or
as a given or new noun (phrases like a sky sound
unnatural because sky is given knowledge, requir-
ing the definite article the). The selection of de-
terminer is not independent of the selection of ad-
jective; a sky may sound unnatural, but a blue sky
is fine. These trees take the dependency between
determiner and adjective into account.
Trees 2 and 3:
Collect beginnings of VP subtrees headed by
(VBZ verb), (VBG verb), and (VBN verb), no-
tated here as VP{VBX verb}, where:
</bodyText>
<listItem confidence="0.768572">
• p(VP{VBX verb}|NP{NN noun}=SUBJ) &gt;a
Tree 4:
Collect beginnings of PP subtrees headed by (IN
prep), where:
• p(PP{IN prep}|NP{NN noun}=SUBJ) &gt; a
</listItem>
<subsectionHeader confidence="0.520458">
Tree 5:
</subsectionHeader>
<bodyText confidence="0.75985">
Collect PP subtrees headed by (IN prep) with
NP complements (OBJ) headed by (NN noun),
where:
</bodyText>
<listItem confidence="0.735467777777778">
• p(PP{IN prep}|NP{NN noun}=OBJ) &gt; a
Tree 6:
Collect VP subtrees headed by (VBX verb) with
embedded PP complements, where:
• p(PP{IN prep}|VP{VBX verb}=SUBJ) &gt; a
Tree 7:
Collect VP subtrees headed by (VBX verb) with
embedded NP objects, where:
• p(VP{VBX verb}|NP{NN noun}=OBJ) &gt;a
</listItem>
<subsectionHeader confidence="0.989417">
4.3 Microplanning
4.3.1 Step 6: Create Full Trees
</subsectionHeader>
<bodyText confidence="0.995264833333333">
In Microplanning, full trees are created by tak-
ing the intersection of the subtrees created in Con-
tent Determination. Because the nouns are or-
dered, it is straightforward to combine the sub-
trees surrounding a noun in position 1 with sub-
trees surrounding a noun in position 2. Two
</bodyText>
<figure confidence="0.999419714285714">
NP NP
PP
NP
NN
duck
PP
NP
NN
grass
NP
NN
grass
IN
IN
by
NP
NN
duck on
PP
NP{NN n2}
IN ↓
</figure>
<page confidence="0.631395">
753
</page>
<figure confidence="0.837730333333333">
NP
VP NP 1 CC NP 1
VP* 1 and
</figure>
<figureCaption confidence="0.995929">
Figure 11: Auxiliary trees for generation.
</figureCaption>
<bodyText confidence="0.999971666666667">
further trees are necessary to allow the subtrees
gathered to combine within the Penn Treebank
syntax. These are given in Figure 11. If two
nouns in a proposed sentence cannot be combined
with prepositions or verbs, we backoff to combine
them using (CC and).
Stepping through this process, all nouns will
have a set of subtrees selected by Tree 1. Prepo-
sitional relationships between nouns are created
by substituting Tree 1 subtrees into the NP nodes
of Trees 4 and 5, as shown in Figure 10. Verbal
relationships between nouns are created by substi-
tuting Tree 1 subtrees into Trees 2, 3, and 7. Verb
with preposition relationships are created between
nouns by substituting the VBX node in Tree 6
with the corresponding node in Trees 2 and 3 to
grow the tree to the right, and the PP node in Tree
6 with the corresponding node in Tree 5 to grow
the tree to the left. Generation of a full tree stops
when all nouns in a group are dominated by the
same node, either an S or NP.
</bodyText>
<subsectionHeader confidence="0.99969">
4.4 Surface Realization
</subsectionHeader>
<bodyText confidence="0.999347090909091">
In the surface realization stage, the system se-
lects a single tree from the generated set of pos-
sible trees and removes mark-up to produce a fi-
nal string. This is also the stage where punctua-
tion may be added. Different strings may be gen-
erated depending on different specifications from
the user, as discussed at the beginning of Section
4 and shown in the online demo. To evaluate the
system against other systems, we specify that the
system should (1) not hallucinate likely verbs; and
(2) return the longest string possible.
</bodyText>
<subsectionHeader confidence="0.924873">
4.4.1 Step 7: Get Final Tree, Clear Mark-Up
</subsectionHeader>
<bodyText confidence="0.999959866666667">
We explored two methods for selecting a final
string. In one method, a trigram language model
built using the Europarl (Koehn, 2005) data with
start/end symbols returns the highest-scoring de-
scription (normalizing for length). In the second
method, we limit the generation system to select
the most likely closed-class words (determiners,
prepositions) while building the subtrees, over-
generating all possible adjective combinations.
The final string is then the one with the most
words. We find that the second method produces
descriptions that seem more natural and varied
than the n-gram ranking method for our develop-
ment set, and so use the longest string method in
evaluation.
</bodyText>
<subsectionHeader confidence="0.916321">
4.4.2 Step 8: Prenominal Modifier Ordering
</subsectionHeader>
<bodyText confidence="0.999948">
To order sets of selected adjectives, we use the
top-scoring prenominal modifier ordering model
discussed in Mitchell et al. (2011). This is an n-
gram model constructed over noun phrases that
were extracted from an automatically parsed ver-
sion of the New York Times portion of the Giga-
word corpus (Graff and Cieri, 2003). With this
in place, blue clear sky becomes clear blue sky,
wooden brown table becomes brown wooden ta-
ble, etc.
</bodyText>
<sectionHeader confidence="0.999371" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9997126875">
Each set of sentences is generated with α (likeli-
hood cutoff) set to .01 and -y (observation count
cutoff) set to 3. We compare the system against
human-written descriptions and two state-of-the-
art vision-to-language systems, the Kulkarni et al.
(2011) and Yang et al. (2011) systems.
Human judgments were collected using Ama-
zon’s Mechanical Turk (Amazon, 2011). We
follow recommended practices for evaluating an
NLG system (Reiter and Belz, 2009) and for run-
ning a study on Mechanical Turk (Callison-Burch
and Dredze, 2010), using a balanced design with
each subject rating 3 descriptions from each sys-
tem. Subjects rated their level of agreement on
a 5-point Likert scale including a neutral mid-
dle position, and since quality ratings are ordinal
(points are not necessarily equidistant), we evalu-
ate responses using a non-parametric test. Partici-
pants that took less than 3 minutes to answer all 60
questions and did not include a humanlike rating
for at least 1 of the 3 human-written descriptions
were removed and replaced. It is important to note
that this evaluation compares full generation sys-
tems; many factors are at play in each system that
may also influence participants’ perception, e.g.,
sentence length (Napoles et al., 2011) and punc-
tuation decisions.
The systems are evaluated on a set of 840
images evaluated in the original Kulkarni et al.
(2011) system. Participants were asked to judge
the statements given in Figure 12, from Strongly
Disagree to Strongly Agree.
</bodyText>
<page confidence="0.993619">
754
</page>
<figure confidence="0.790041428571429">
Grammaticality Main Aspects Correctness Order Humanlikeness
4 (3.77, 1.19) 4 (4.09, 0.97) 4 (3.81, 1.11) 4 (3.88, 1.05) 4 (3.88, 0.96)
3 (2.95, 1.42) 3 (2.86, 1.35) 3 (2.95, 1.34) 3 (2.92, 1.25) 3 (3.16, 1.17)
3 (2.83, 1.37) 3 (2.84, 1.33) 3 (2.76, 1.34) 3 (2.78, 1.23) 3 (3.13, 1.23)
3 (2.95, 1.49) 2 (2.31, 1.30) 2 (2.46, 1.36) 2 (2.53, 1.26) 3 (2.97, 1.23)
Human
Midge
</figure>
<note confidence="0.814125">
Yang et al. 2011
Kulkarni et al. 2011
</note>
<tableCaption confidence="0.9806025">
Table 4: Median scores for systems, mean and standard deviation in parentheses. Distance between points on the
rating scale cannot be assumed to be equidistant, and so we analyze results using a non-parametric test.
</tableCaption>
<figure confidence="0.381952923076923">
GRAMMATICALITY:
This description is grammatically correct.
MAIN ASPECTS:
This description describes the main aspects of this
image.
CORRECTNESS:
This description does not include extraneous or in-
correct information.
ORDER:
The objects described are mentioned in a reasonable
order.
HUMANLIKENESS:
It sounds like a person wrote this description.
</figure>
<figureCaption confidence="0.992385">
Figure 12: Mechanical Turk prompts.
</figureCaption>
<bodyText confidence="0.999963421052632">
We report the scores for the systems in Table
4. Results are analyzed using the non-parametric
Wilcoxon Signed-Rank test, which uses median
values to compare the different systems. Midge
outperforms all recent automatic approaches on
CORRECTNESS and ORDER, and Yang et al. ad-
ditionally on HUMANLIKENESS and MAIN AS-
PECTS. Differences between Midge and Kulkarni
et al. are significant at p &lt; .01; Midge and Yang et
al. at p &lt; .001. For all metrics, human-written de-
scriptions still outperform automatic approaches
(p &lt; .001).
These findings are striking, particularly be-
cause Midge uses the same input as the Kulka-
rni et al. system. Using syntactically informed
word co-occurrence statistics from a large corpus
of descriptive text improves over state-of-the-art,
allowing syntactic trees to be generated that cap-
ture the variation of natural language.
</bodyText>
<sectionHeader confidence="0.999768" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999983870967742">
Midge automatically generates language that is as
good as or better than template-based systems,
tying vision to language at a syntactic/semantic
level to produce natural language descriptions.
Results are promising, but, there is more work to
be done: Evaluators can still tell a difference be-
tween human-written descriptions and automati-
cally generated descriptions.
Improvements to the generated language are
possible at both the vision side and the language
side. On the computer vision side, incorrect ob-
jects are often detected and salient objects are of-
ten missed. Midge does not yet screen out un-
likely objects or add likely objects, and so pro-
vides no filter for this. On the language side, like-
lihood is estimated directly, and the system pri-
marily uses simple maximum likelihood estima-
tions to combine subtrees. The descriptive cor-
pus that informs the system is not parsed with
a domain-adapted parser; with this in place, the
syntactic constructions that Midge learns will bet-
ter reflect the constructions that people use.
In future work, we hope to address these issues
as well as advance the syntactic derivation pro-
cess, providing an adjunction operation (for ex-
ample, to add likely adjectives or adverbs based
on language alone). We would also like to incor-
porate meta-data – even when no vision detection
fires for an image, the system may be able to gen-
erate descriptions of the time and place where an
image was taken based on the image file alone.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999964692307692">
We have introduced a generation system that uses
a new approach to generating language, tying a
syntactic model to computer vision detections.
Midge generates a well-formed description of an
image by filtering attribute detections that are un-
likely and placing objects into an ordered syntac-
tic structure. Humans judge Midge’s output to be
the most natural descriptions of images generated
thus far. The methods described here are promis-
ing for generating natural language descriptions
of the visual world, and we hope to expand and
refine the system to capture further linguistic phe-
nomena.
</bodyText>
<sectionHeader confidence="0.99894" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.994806">
Thanks to the Johns Hopkins CLSP summer
workshop 2011 for making this system possible,
and to reviewers for helpful comments. This
work is supported in part by Michael Collins and
by NSF Faculty Early Career Development (CA-
REER) Award #1054133.
</bodyText>
<page confidence="0.997914">
755
</page>
<sectionHeader confidence="0.995869" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999898137254902">
Amazon. 2011. Amazon mechanical turk: Artificial
artificial intelligence.
Holly P. Branigan, Martin J. Pickering, and Mikihiro
Tanaka. 2007. Contributions of animacy to gram-
matical function assignment and word order during
production. Lingua, 118(2):172–189.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram version 1.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing speech and language data with Amazon’s Me-
chanical Turk. NAACL 2010 Workshop on Creat-
ing Speech and Language Data with Amazon’s Me-
chanical Turk.
Navneet Dalal and Bill Triggs. 2005. Histograms of
oriented gradients for human detections. Proceed-
ings of CVPR 2005.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. Proceedings of CVPR 2009.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences for images.
Proceedings of ECCV 2010.
Pedro Felzenszwalb, David McAllester, and Deva Ra-
maman. 2008. A discriminatively trained, mul-
tiscale, deformable part model. Proceedings of
CVPR 2008.
Flickr. 2011. http://www.flickr.com. Accessed
1.Sep.11.
Kotaro Funakoshi, Satoru Watanabe, Naoko
Kuriyama, and Takenobu Tokunaga. 2004.
Generating referring expressions using perceptual
groups. Proceedings of the 3rd INLG.
Albert Gatt. 2006. Generating collective spatial refer-
ences. Proceedings of the 28th CogSci.
David Graff and Christopher Cieri. 2003. English Gi-
gaword. Linguistic Data Consortium, Philadelphia,
PA. LDC Catalog No. LDC2003T05.
Philipp Koehn. 2005. Europarl: A parallel cor-
pus for statistical machine translation. MT Summit.
http://www.statmt.org/europarl/.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. Proceedings of
ACL-08: HLT.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C. Berg, and Tamara
Berg. 2011. Baby talk: Understanding and gener-
ating image descriptions. Proceedings of the 24th
CVPR.
Irene Langkilde and Kevin Knight. 1998. Gener-
ation that exploits corpus-based statistical knowl-
edge. Proceedings of the 36th ACL.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
Proceedings of CoNLL 2011.
Mitchell Marcus, Ann Bies, Constance Cooper, Mark
Ferguson, and Alyson Littman. 1995. Treebank II
bracketing guide.
George A. Miller. 1995. WordNet: A lexical
database for english. Communications of the ACM,
38(11):39–41.
Margaret Mitchell, Aaron Dunlop, and Brian Roark.
2011. Semi-supervised modeling for prenomi-
nal modifier ordering. Proceedings of the 49th
ACL:HLT.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. ACL-
HLT Workshop on Monolingual Text-To-Text Gen-
eration.
Vicente Ordonez, Girish Kulkarni, and Tamara L Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. Proceedings of NIPS 2011.
Slav Petrov. 2010. Berkeley parser. GNU General
Public License v.2.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image anno-
tations using amazon’s mechanical turk. Proceed-
ings of the NAACL HLT 2010 Workshop on Creat-
ing Speech and Language Data with Amazon’s Me-
chanical Turk.
Ehud Reiter and Anja Belz. 2009. An investiga-
tion into the validity of some metrics for automat-
ically evaluating natural language generation sys-
tems. Computational Linguistics, 35(4):529–558.
Ehud Reiter and Robert Dale. 1997. Building ap-
plied natural language generation systems. Journal
of Natural Language Engineering, pages 57–87.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and
Yiannis Aloimonos. 2011. Corpus-guided sen-
tence generation of natural images. Proceedings of
EMNLP 2011.
Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2T: Image pars-
ing to text description. Proceedings of IEEE 2010,
98(8):1485–1508.
</reference>
<page confidence="0.998714">
756
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.741298">
<title confidence="0.9974575">Midge: Generating Image Descriptions From Computer Vision Detections</title>
<author confidence="0.958096">Daum´e</author>
<affiliation confidence="0.9450984">of Aberdeen and Oregon Health and Science University, Brook University, of Maryland, University, of Washington,</affiliation>
<abstract confidence="0.999409857142857">This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. By leveraging syntactically informed word co-occurrence statistics, the generator filters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amazon</author>
</authors>
<title>Amazon mechanical turk: Artificial artificial intelligence.</title>
<date>2011</date>
<contexts>
<context position="29577" citStr="Amazon, 2011" startWordPosition="4968" endWordPosition="4969">re extracted from an automatically parsed version of the New York Times portion of the Gigaword corpus (Graff and Cieri, 2003). With this in place, blue clear sky becomes clear blue sky, wooden brown table becomes brown wooden table, etc. 5 Evaluation Each set of sentences is generated with α (likelihood cutoff) set to .01 and -y (observation count cutoff) set to 3. We compare the system against human-written descriptions and two state-of-theart vision-to-language systems, the Kulkarni et al. (2011) and Yang et al. (2011) systems. Human judgments were collected using Amazon’s Mechanical Turk (Amazon, 2011). We follow recommended practices for evaluating an NLG system (Reiter and Belz, 2009) and for running a study on Mechanical Turk (Callison-Burch and Dredze, 2010), using a balanced design with each subject rating 3 descriptions from each system. Subjects rated their level of agreement on a 5-point Likert scale including a neutral middle position, and since quality ratings are ordinal (points are not necessarily equidistant), we evaluate responses using a non-parametric test. Participants that took less than 3 minutes to answer all 60 questions and did not include a humanlike rating for at lea</context>
</contexts>
<marker>Amazon, 2011</marker>
<rawString>Amazon. 2011. Amazon mechanical turk: Artificial artificial intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holly P Branigan</author>
<author>Martin J Pickering</author>
<author>Mikihiro Tanaka</author>
</authors>
<title>Contributions of animacy to grammatical function assignment and word order during production.</title>
<date>2007</date>
<journal>Lingua,</journal>
<volume>118</volume>
<issue>2</issue>
<contexts>
<context position="19492" citStr="Branigan et al., 2007" startWordPosition="3174" endWordPosition="3177">se WordNet to associate all head nouns in the Flickr data to all of their hypernyms. A description is represented as an ordered set [a1...a,,,] where each ap is a noun with position p in the set of head nouns in the sentence. For the position pi of each hypernym ha in each sentence with n head nouns, we estimate p(pi|n, ha). During generation, the system greedily maximizes p(pi|n, ha) until all nouns have been ordered. Example orderings are shown in Figure 8. This model automatically places animate objects near the beginning of a sentence, which follows psycholinguistic work in object naming (Branigan et al., 2007). 4.2.3 Step 3: Filter Incorrect Attributes For the system to be able to extend coverage as new computer vision attribute detections become available, we develop a method to automatically DT NN bottle a PP NP DT the NP DT - NP NN people IN with PP NP IN at NN table 751 A person sitting on a sofa Cows grazing Airplanes flying A person walking a dog Figure 7: Hallucinating: Creating likely actions. Straightforward to do, but can often be wrong. COLOR purple blue green red white ... MATERIAL plastic wooden silver ... SURFACE furry fluffy hard soft ... QUALITY shiny rust dirty broken ... Table 2: </context>
</contexts>
<marker>Branigan, Pickering, Tanaka, 2007</marker>
<rawString>Holly P. Branigan, Martin J. Pickering, and Mikihiro Tanaka. 2007. Contributions of animacy to grammatical function assignment and word order during production. Lingua, 118(2):172–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram version 1.</title>
<date>2006</date>
<contexts>
<context position="9130" citStr="Brants and Franz, 2006" startWordPosition="1465" endWordPosition="1468">on is are showing the bottle on the sitting in the chair in the street room Midge: people with a bottle at Midge: a person in black the table with a black dog by potted plants Figure 3: Descriptions generated by Midge, Kulkarni et al. (2011) and Yang et al. (2011) on the same images. Midge uses the Kulkarni et al. (2011) front-end, and so outputs are directly comparable. paper. However, their system is not automatic, requiring extensive hand-coded semantic and syntactic details. Another approach is provided in Li et al. (2011), who use image detections to select and combine web-scale n-grams (Brants and Franz, 2006). This automatically generates descriptions that are either poetic or strange (e.g., “tree snowing black train”). A different line of work transfers captions of similar images directly to a query image. Farhadi et al. (2010) use &lt;object,action,scene&gt; triples predicted from the visual characteristics of the image to find potential captions. Ordonez et al. (2011) use global image matching with local reordering from a much larger set of captioned photographs. These transfer-based approaches result in natural captions (they are written by humans) that may not actually be true of the image. This wo</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram version 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>NAACL 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="29740" citStr="Callison-Burch and Dredze, 2010" startWordPosition="4992" endWordPosition="4995">ce, blue clear sky becomes clear blue sky, wooden brown table becomes brown wooden table, etc. 5 Evaluation Each set of sentences is generated with α (likelihood cutoff) set to .01 and -y (observation count cutoff) set to 3. We compare the system against human-written descriptions and two state-of-theart vision-to-language systems, the Kulkarni et al. (2011) and Yang et al. (2011) systems. Human judgments were collected using Amazon’s Mechanical Turk (Amazon, 2011). We follow recommended practices for evaluating an NLG system (Reiter and Belz, 2009) and for running a study on Mechanical Turk (Callison-Burch and Dredze, 2010), using a balanced design with each subject rating 3 descriptions from each system. Subjects rated their level of agreement on a 5-point Likert scale including a neutral middle position, and since quality ratings are ordinal (points are not necessarily equidistant), we evaluate responses using a non-parametric test. Participants that took less than 3 minutes to answer all 60 questions and did not include a humanlike rating for at least 1 of the 3 human-written descriptions were removed and replaced. It is important to note that this evaluation compares full generation systems; many factors are</context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with Amazon’s Mechanical Turk. NAACL 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Navneet Dalal</author>
<author>Bill Triggs</author>
</authors>
<title>Histograms of oriented gradients for human detections.</title>
<date>2005</date>
<booktitle>Proceedings of CVPR</booktitle>
<contexts>
<context position="6663" citStr="Dalal and Triggs, 2005" startWordPosition="1052" endWordPosition="1055">) that composes novel captions for images in the PASCAL sentence data set,2 introduced in Rashtchian et al. (2010). This provides multiple object detections based on Felzenszwalb’s mixtures of multiscale deformable parts models (Felzenszwalb et al., 2008), and stuff detections (roughly, mass nouns, things like sky and grass) based on linear SVMs for low level region features. Appearance characteristics are predicted using trained detectors for colors, shapes, textures, and materials, an idea originally introduced in Farhadi et al. (2009). Local texture, Histograms of Oriented Gradients (HOG) (Dalal and Triggs, 2005), edge, and color descriptors inside the bounding box of a recognized object are binned into histograms for a vision system to learn to recognize when an object is rectangular, wooden, metal, etc. Finally, simple preposition functions are used to compute the spatial relations between objects based on their bounding boxes. The original Kulkarni et al. (2011) system generates descriptions with a template, filling in slots by combining computer vision outputs with text based statistics in a conditional random field to predict the most likely image labeling. Templatebased generation is also used i</context>
</contexts>
<marker>Dalal, Triggs, 2005</marker>
<rawString>Navneet Dalal and Bill Triggs. 2005. Histograms of oriented gradients for human detections. Proceedings of CVPR 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Ian Endres</author>
<author>Derek Hoiem</author>
<author>David Forsyth</author>
</authors>
<title>Describing objects by their attributes.</title>
<date>2009</date>
<booktitle>Proceedings of CVPR</booktitle>
<contexts>
<context position="6583" citStr="Farhadi et al. (2009)" startWordPosition="1040" endWordPosition="1043"> approach to describing images starts with a system from Kulkarni et al. (2011) that composes novel captions for images in the PASCAL sentence data set,2 introduced in Rashtchian et al. (2010). This provides multiple object detections based on Felzenszwalb’s mixtures of multiscale deformable parts models (Felzenszwalb et al., 2008), and stuff detections (roughly, mass nouns, things like sky and grass) based on linear SVMs for low level region features. Appearance characteristics are predicted using trained detectors for colors, shapes, textures, and materials, an idea originally introduced in Farhadi et al. (2009). Local texture, Histograms of Oriented Gradients (HOG) (Dalal and Triggs, 2005), edge, and color descriptors inside the bounding box of a recognized object are binned into histograms for a vision system to learn to recognize when an object is rectangular, wooden, metal, etc. Finally, simple preposition functions are used to compute the spatial relations between objects based on their bounding boxes. The original Kulkarni et al. (2011) system generates descriptions with a template, filling in slots by combining computer vision outputs with text based statistics in a conditional random field to</context>
</contexts>
<marker>Farhadi, Endres, Hoiem, Forsyth, 2009</marker>
<rawString>Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. 2009. Describing objects by their attributes. Proceedings of CVPR 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: generating sentences for images.</title>
<date>2010</date>
<booktitle>Proceedings of ECCV</booktitle>
<contexts>
<context position="1329" citStr="Farhadi et al., 2010" startWordPosition="180" endWordPosition="183">-occurrence statistics, the generator filters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date. 1 Introduction It is becoming a real possibility for intelligent systems to talk about the visual world. New ways of mapping computer vision to generated language have emerged in the past few years, with a focus on pairing detections in an image to words (Farhadi et al., 2010; Li et al., 2011; Kulkarni et al., 2011; Yang et al., 2011). The goal in connecting vision to language has varied: systems have started producing language that is descriptive and poetic (Li et al., 2011), summaries that add content where the computer vision system does not (Yang et al., 2011), and captions copied directly from other images that are globally (Farhadi et al., 2010) and locally similar (Ordonez et al., 2011). A commonality between all of these approaches is that they aim to produce naturalsounding descriptions from computer vision detections. This commonality is our starting poi</context>
<context position="7433" citStr="Farhadi et al., 2010" startWordPosition="1174" endWordPosition="1177">n an object is rectangular, wooden, metal, etc. Finally, simple preposition functions are used to compute the spatial relations between objects based on their bounding boxes. The original Kulkarni et al. (2011) system generates descriptions with a template, filling in slots by combining computer vision outputs with text based statistics in a conditional random field to predict the most likely image labeling. Templatebased generation is also used in the recent Yang et al. (2011) system, which fills in likely verbs and prepositions by dependency parsing the humanwritten UIUC Pascal-VOC dataset (Farhadi et al., 2010) and selecting the dependent/head relation with the highest log likelihood ratio. Template-based generation is useful for automatically generating consistent sentences, however, if the goal is to vary or add to the text produced, it may be suboptimal (cf. Reiter and Dale (1997)). Work that does not use template-based generation includes Yao et al. (2010), who generate syntactic trees, similar to the approach in this 2http://vision.cs.uiuc.edu/pascal-sentences/ 748 Kulkarni et al.: This is a pic- Kulkarni et al.: This is ture of three persons, one bot- a picture of two pottedtle and one diningt</context>
<context position="9354" citStr="Farhadi et al. (2010)" startWordPosition="1500" endWordPosition="1503">i et al. (2011) and Yang et al. (2011) on the same images. Midge uses the Kulkarni et al. (2011) front-end, and so outputs are directly comparable. paper. However, their system is not automatic, requiring extensive hand-coded semantic and syntactic details. Another approach is provided in Li et al. (2011), who use image detections to select and combine web-scale n-grams (Brants and Franz, 2006). This automatically generates descriptions that are either poetic or strange (e.g., “tree snowing black train”). A different line of work transfers captions of similar images directly to a query image. Farhadi et al. (2010) use &lt;object,action,scene&gt; triples predicted from the visual characteristics of the image to find potential captions. Ordonez et al. (2011) use global image matching with local reordering from a much larger set of captioned photographs. These transfer-based approaches result in natural captions (they are written by humans) that may not actually be true of the image. This work learns and builds from these approaches. Following Kulkarni et al. and Li et al., the system uses large-scale text corpora to estimate likely words around object detections. Following Yang et al., the system can hallucina</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: generating sentences for images. Proceedings of ECCV 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Felzenszwalb</author>
<author>David McAllester</author>
<author>Deva Ramaman</author>
</authors>
<title>A discriminatively trained, multiscale, deformable part model.</title>
<date>2008</date>
<booktitle>Proceedings of CVPR</booktitle>
<contexts>
<context position="6295" citStr="Felzenszwalb et al., 2008" startWordPosition="998" endWordPosition="1001"> constraints) and the other words it tends to occur with (semantic constraints). This is a data-driven way to generate likely adjectives, prepositions, determiners, etc., taking the intersection of what the vision system predicts and how the object noun tends to be described. 2 Background Our approach to describing images starts with a system from Kulkarni et al. (2011) that composes novel captions for images in the PASCAL sentence data set,2 introduced in Rashtchian et al. (2010). This provides multiple object detections based on Felzenszwalb’s mixtures of multiscale deformable parts models (Felzenszwalb et al., 2008), and stuff detections (roughly, mass nouns, things like sky and grass) based on linear SVMs for low level region features. Appearance characteristics are predicted using trained detectors for colors, shapes, textures, and materials, an idea originally introduced in Farhadi et al. (2009). Local texture, Histograms of Oriented Gradients (HOG) (Dalal and Triggs, 2005), edge, and color descriptors inside the bounding box of a recognized object are binned into histograms for a vision system to learn to recognize when an object is rectangular, wooden, metal, etc. Finally, simple preposition functio</context>
</contexts>
<marker>Felzenszwalb, McAllester, Ramaman, 2008</marker>
<rawString>Pedro Felzenszwalb, David McAllester, and Deva Ramaman. 2008. A discriminatively trained, multiscale, deformable part model. Proceedings of CVPR 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Flickr</author>
</authors>
<date>2011</date>
<note>http://www.flickr.com. Accessed 1.Sep.11.</note>
<contexts>
<context position="11806" citStr="Flickr, 2011" startWordPosition="1893" endWordPosition="1894">tactic/semantic level. As such, it uses basic solutions at each stage of the process, which may be improved: Midge serves as an illustration of the types of issues that should be handled to automatically generate syntactic trees from vision detections, and offers some possible solutions. It is evaluated against the Kulkarni et al. system, the Yang et al. system, and human-written descriptions on the same set of images in Section 5, and is found to significantly outperform the automatic systems. 3 Learning from Descriptive Text To train our system on how people describe images, we use 700,000 (Flickr, 2011) images with associated descriptions from the dataset in Ordonez et al. (2011). This is separate from our evaluation image set, consisting of 840 PASCAL images. The Flickr data is messier than datasets created specifically for vision training, but provides the largest corpus of natural descriptions of images to date. We normalize the text by removing emoticons and mark-up language, and parse each caption using the Berkeley parser (Petrov, 2010). Once parsed, we can extract syntactic information for individual (word, tag) pairs. 749 a cow with sheep with a gray sky people with boats a brown cow</context>
</contexts>
<marker>Flickr, 2011</marker>
<rawString>Flickr. 2011. http://www.flickr.com. Accessed 1.Sep.11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kotaro Funakoshi</author>
<author>Satoru Watanabe</author>
<author>Naoko Kuriyama</author>
<author>Takenobu Tokunaga</author>
</authors>
<title>Generating referring expressions using perceptual groups.</title>
<date>2004</date>
<booktitle>Proceedings of the 3rd INLG.</booktitle>
<contexts>
<context position="21437" citStr="Funakoshi et al., 2004" startWordPosition="3501" endWordPosition="3505">rpus, we find that each attribute (COLOR, SIZE, etc.), rarely has more than a single value in the final description, with the most common (COLOR) co-occurring less than 2% of the time. Midge enforces this idea to select the most likely word v for each attribute from the detections. In a noun phrase headed by an object noun, NP{NN noun}, the prenominal adjective (JJ v) for each attribute is selected using maximum likelihood. 4.2.4 Step 4: Group Plurals How to generate natural-sounding spatial relations and modifiers for a set of objects, as opposed to a single object, is still an open problem (Funakoshi et al., 2004; Gatt, 2006). In this work, we use a simple method to group all same-type objects together, associate them to the plural form listed in the KB, discard the modifiers, and return spatial relations based on the first recognized 3What in computer vision are called attributes are called values in NLG. A value like red belongs to a COLOR attribute, and we use this distinction in the system. member of the group. 4.2.5 Step 5: Gather Local Subtrees Around Object Nouns 1 2 NP DT{0,1} ↓ JJ* ↓ NN S n NP{NN n} VP{VBZ} ↓ 3 4 NP NP NP{NN n} VP{VB(G|N)} ↓ 5 6 PP VP VB(G|N|Z) ↓ NP{NN n} Figure 9: Initial su</context>
</contexts>
<marker>Funakoshi, Watanabe, Kuriyama, Tokunaga, 2004</marker>
<rawString>Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama, and Takenobu Tokunaga. 2004. Generating referring expressions using perceptual groups. Proceedings of the 3rd INLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
</authors>
<title>Generating collective spatial references.</title>
<date>2006</date>
<booktitle>Proceedings of the 28th CogSci.</booktitle>
<contexts>
<context position="21450" citStr="Gatt, 2006" startWordPosition="3506" endWordPosition="3507">attribute (COLOR, SIZE, etc.), rarely has more than a single value in the final description, with the most common (COLOR) co-occurring less than 2% of the time. Midge enforces this idea to select the most likely word v for each attribute from the detections. In a noun phrase headed by an object noun, NP{NN noun}, the prenominal adjective (JJ v) for each attribute is selected using maximum likelihood. 4.2.4 Step 4: Group Plurals How to generate natural-sounding spatial relations and modifiers for a set of objects, as opposed to a single object, is still an open problem (Funakoshi et al., 2004; Gatt, 2006). In this work, we use a simple method to group all same-type objects together, associate them to the plural form listed in the KB, discard the modifiers, and return spatial relations based on the first recognized 3What in computer vision are called attributes are called values in NLG. A value like red belongs to a COLOR attribute, and we use this distinction in the system. member of the group. 4.2.5 Step 5: Gather Local Subtrees Around Object Nouns 1 2 NP DT{0,1} ↓ JJ* ↓ NN S n NP{NN n} VP{VBZ} ↓ 3 4 NP NP NP{NN n} VP{VB(G|N)} ↓ 5 6 PP VP VB(G|N|Z) ↓ NP{NN n} Figure 9: Initial subtree frames </context>
</contexts>
<marker>Gatt, 2006</marker>
<rawString>Albert Gatt. 2006. Generating collective spatial references. Proceedings of the 28th CogSci.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Christopher Cieri</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium,</title>
<date>2003</date>
<tech>No. LDC2003T05.</tech>
<publisher>LDC Catalog</publisher>
<location>Philadelphia, PA.</location>
<contexts>
<context position="29090" citStr="Graff and Cieri, 2003" startWordPosition="4887" endWordPosition="4890">binations. The final string is then the one with the most words. We find that the second method produces descriptions that seem more natural and varied than the n-gram ranking method for our development set, and so use the longest string method in evaluation. 4.4.2 Step 8: Prenominal Modifier Ordering To order sets of selected adjectives, we use the top-scoring prenominal modifier ordering model discussed in Mitchell et al. (2011). This is an ngram model constructed over noun phrases that were extracted from an automatically parsed version of the New York Times portion of the Gigaword corpus (Graff and Cieri, 2003). With this in place, blue clear sky becomes clear blue sky, wooden brown table becomes brown wooden table, etc. 5 Evaluation Each set of sentences is generated with α (likelihood cutoff) set to .01 and -y (observation count cutoff) set to 3. We compare the system against human-written descriptions and two state-of-theart vision-to-language systems, the Kulkarni et al. (2011) and Yang et al. (2011) systems. Human judgments were collected using Amazon’s Mechanical Turk (Amazon, 2011). We follow recommended practices for evaluating an NLG system (Reiter and Belz, 2009) and for running a study on</context>
</contexts>
<marker>Graff, Cieri, 2003</marker>
<rawString>David Graff and Christopher Cieri. 2003. English Gigaword. Linguistic Data Consortium, Philadelphia, PA. LDC Catalog No. LDC2003T05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<note>MT Summit. http://www.statmt.org/europarl/.</note>
<contexts>
<context position="28177" citStr="Koehn, 2005" startWordPosition="4745" endWordPosition="4746">possible trees and removes mark-up to produce a final string. This is also the stage where punctuation may be added. Different strings may be generated depending on different specifications from the user, as discussed at the beginning of Section 4 and shown in the online demo. To evaluate the system against other systems, we specify that the system should (1) not hallucinate likely verbs; and (2) return the longest string possible. 4.4.1 Step 7: Get Final Tree, Clear Mark-Up We explored two methods for selecting a final string. In one method, a trigram language model built using the Europarl (Koehn, 2005) data with start/end symbols returns the highest-scoring description (normalizing for length). In the second method, we limit the generation system to select the most likely closed-class words (determiners, prepositions) while building the subtrees, overgenerating all possible adjective combinations. The final string is then the one with the most words. We find that the second method produces descriptions that seem more natural and varied than the n-gram ranking method for our development set, and so use the longest string method in evaluation. 4.4.2 Step 8: Prenominal Modifier Ordering To ord</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. MT Summit. http://www.statmt.org/europarl/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="20341" citStr="Kozareva et al., 2008" startWordPosition="3318" endWordPosition="3321">NN people IN with PP NP IN at NN table 751 A person sitting on a sofa Cows grazing Airplanes flying A person walking a dog Figure 7: Hallucinating: Creating likely actions. Straightforward to do, but can often be wrong. COLOR purple blue green red white ... MATERIAL plastic wooden silver ... SURFACE furry fluffy hard soft ... QUALITY shiny rust dirty broken ... Table 2: Example attribute classes and values. group adjectives into broader attribute classes,3 and the generation system uses these classes when deciding how to describe objects. To group adjectives, we use a bootstrapping technique (Kozareva et al., 2008) that learns which adjectives tend to co-occur, and groups these together to form an attribute class. Co-occurrence is computed using cosine (distributional) similarity between adjectives, considering adjacent nouns as context (i.e., JJ NN constructions). Contexts (nouns) for adjectives are weighted using Pointwise Mutual Information and only the top 1000 nouns are selected for every adjective. Some of the learned attribute classes are given in Table 2. In the Flickr corpus, we find that each attribute (COLOR, SIZE, etc.), rarely has more than a single value in the final description, with the </context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara Berg</author>
</authors>
<title>Baby talk: Understanding and generating image descriptions.</title>
<date>2011</date>
<booktitle>Proceedings of the 24th CVPR.</booktitle>
<contexts>
<context position="1369" citStr="Kulkarni et al., 2011" startWordPosition="188" endWordPosition="191">ilters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date. 1 Introduction It is becoming a real possibility for intelligent systems to talk about the visual world. New ways of mapping computer vision to generated language have emerged in the past few years, with a focus on pairing detections in an image to words (Farhadi et al., 2010; Li et al., 2011; Kulkarni et al., 2011; Yang et al., 2011). The goal in connecting vision to language has varied: systems have started producing language that is descriptive and poetic (Li et al., 2011), summaries that add content where the computer vision system does not (Yang et al., 2011), and captions copied directly from other images that are globally (Farhadi et al., 2010) and locally similar (Ordonez et al., 2011). A commonality between all of these approaches is that they aim to produce naturalsounding descriptions from computer vision detections. This commonality is our starting point: We aim to design a system capable of</context>
<context position="6041" citStr="Kulkarni et al. (2011)" startWordPosition="959" endWordPosition="962">the basis for the description. Likelihood estimates of syntactic structure and word co-occurrence are conditioned on object nouns, and this enables each noun head in a description to select for the kinds of structures it tends to appear in (syntactic constraints) and the other words it tends to occur with (semantic constraints). This is a data-driven way to generate likely adjectives, prepositions, determiners, etc., taking the intersection of what the vision system predicts and how the object noun tends to be described. 2 Background Our approach to describing images starts with a system from Kulkarni et al. (2011) that composes novel captions for images in the PASCAL sentence data set,2 introduced in Rashtchian et al. (2010). This provides multiple object detections based on Felzenszwalb’s mixtures of multiscale deformable parts models (Felzenszwalb et al., 2008), and stuff detections (roughly, mass nouns, things like sky and grass) based on linear SVMs for low level region features. Appearance characteristics are predicted using trained detectors for colors, shapes, textures, and materials, an idea originally introduced in Farhadi et al. (2009). Local texture, Histograms of Oriented Gradients (HOG) (D</context>
<context position="8748" citStr="Kulkarni et al. (2011)" startWordPosition="1402" endWordPosition="1405">econd person. The rusty bot- by the black person, and tle is near the first rusty per- near the second feathered son, and within the colorful pottedplant. diningtable. The second person is by the third rusty person. The colorful diningtable is near the first rusty person, and near the second person, and near the third rusty person. Yang et al.: Three people Yang et al.: The person is are showing the bottle on the sitting in the chair in the street room Midge: people with a bottle at Midge: a person in black the table with a black dog by potted plants Figure 3: Descriptions generated by Midge, Kulkarni et al. (2011) and Yang et al. (2011) on the same images. Midge uses the Kulkarni et al. (2011) front-end, and so outputs are directly comparable. paper. However, their system is not automatic, requiring extensive hand-coded semantic and syntactic details. Another approach is provided in Li et al. (2011), who use image detections to select and combine web-scale n-grams (Brants and Franz, 2006). This automatically generates descriptions that are either poetic or strange (e.g., “tree snowing black train”). A different line of work transfers captions of similar images directly to a query image. Farhadi et al. </context>
<context position="15946" citStr="Kulkarni et al., 2011" startWordPosition="2595" endWordPosition="2598">riate here, where nouns are bundles of constraints akin to seeds, giving rise to the rest of the tree based on the lexicalized subtrees in which the nouns are likely to occur. An example generated tree structure is shown in Figure 6, with noun anchors in bold. 750 NP Figure 6: Tree generated from tree growth process. Midge was developed using detections run on Flickr images, incorporating action/pose detections for verbs as well as object detections for nouns. In testing, we generate descriptions for the PASCAL images, which have been used in earlier work on the vision-to-language connection (Kulkarni et al., 2011; Yang et al., 2011), and allows us to compare systems directly. Action and pose detection for this data set still does not work well, and so the system does not receive these detections from the vision front-end. However, the system can still generate verbs when action and pose detectors have been run, and this framework allows the system to “hallucinate” likely verbal constructions between objects if specified at runtime. A similar approach was taken in Yang et al. (2011). Some examples are given in Figure 7. We follow a three-tiered generation process (Reiter and Dale, 2000), utilizing cont</context>
<context position="29468" citStr="Kulkarni et al. (2011)" startWordPosition="4949" endWordPosition="4952">ifier ordering model discussed in Mitchell et al. (2011). This is an ngram model constructed over noun phrases that were extracted from an automatically parsed version of the New York Times portion of the Gigaword corpus (Graff and Cieri, 2003). With this in place, blue clear sky becomes clear blue sky, wooden brown table becomes brown wooden table, etc. 5 Evaluation Each set of sentences is generated with α (likelihood cutoff) set to .01 and -y (observation count cutoff) set to 3. We compare the system against human-written descriptions and two state-of-theart vision-to-language systems, the Kulkarni et al. (2011) and Yang et al. (2011) systems. Human judgments were collected using Amazon’s Mechanical Turk (Amazon, 2011). We follow recommended practices for evaluating an NLG system (Reiter and Belz, 2009) and for running a study on Mechanical Turk (Callison-Burch and Dredze, 2010), using a balanced design with each subject rating 3 descriptions from each system. Subjects rated their level of agreement on a 5-point Likert scale including a neutral middle position, and since quality ratings are ordinal (points are not necessarily equidistant), we evaluate responses using a non-parametric test. Participan</context>
<context position="31115" citStr="Kulkarni et al. 2011" startWordPosition="5226" endWordPosition="5229">ms are evaluated on a set of 840 images evaluated in the original Kulkarni et al. (2011) system. Participants were asked to judge the statements given in Figure 12, from Strongly Disagree to Strongly Agree. 754 Grammaticality Main Aspects Correctness Order Humanlikeness 4 (3.77, 1.19) 4 (4.09, 0.97) 4 (3.81, 1.11) 4 (3.88, 1.05) 4 (3.88, 0.96) 3 (2.95, 1.42) 3 (2.86, 1.35) 3 (2.95, 1.34) 3 (2.92, 1.25) 3 (3.16, 1.17) 3 (2.83, 1.37) 3 (2.84, 1.33) 3 (2.76, 1.34) 3 (2.78, 1.23) 3 (3.13, 1.23) 3 (2.95, 1.49) 2 (2.31, 1.30) 2 (2.46, 1.36) 2 (2.53, 1.26) 3 (2.97, 1.23) Human Midge Yang et al. 2011 Kulkarni et al. 2011 Table 4: Median scores for systems, mean and standard deviation in parentheses. Distance between points on the rating scale cannot be assumed to be equidistant, and so we analyze results using a non-parametric test. GRAMMATICALITY: This description is grammatically correct. MAIN ASPECTS: This description describes the main aspects of this image. CORRECTNESS: This description does not include extraneous or incorrect information. ORDER: The objects described are mentioned in a reasonable order. HUMANLIKENESS: It sounds like a person wrote this description. Figure 12: Mechanical Turk prompts. We</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara Berg. 2011. Baby talk: Understanding and generating image descriptions. Proceedings of the 24th CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>Proceedings of the 36th ACL.</booktitle>
<contexts>
<context position="16949" citStr="Langkilde and Knight, 1998" startWordPosition="2755" endWordPosition="2758">structions between objects if specified at runtime. A similar approach was taken in Yang et al. (2011). Some examples are given in Figure 7. We follow a three-tiered generation process (Reiter and Dale, 2000), utilizing content determination to first cluster and order the object nouns, create their local subtrees, and filter incorrect detections; microplanning to construct full syntactic trees around the noun clusters, and surface realization to order selected modifiers, realize them as postnominal or prenominal, and select final outputs. The system follows an overgenerate-andselect approach (Langkilde and Knight, 1998), which allows different final trees to be selected with different settings. 4.1 Knowledge Base Midge uses a knowledge base that stores models for different tasks during generation. These models are primarily data-driven, but we also include a hand-built component to handle a small set of rules. The data-driven component provides the syntactically informed word co-occurrence statistics learned from the Flickr data, a model for ordering the selected nouns in a sentence, and a model to change computer vision attributes to attribute:value pairs. Below, we discuss the three main data-driven models</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. Proceedings of the 36th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siming Li</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
<author>Alexander C Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Composing simple image descriptions using web-scale n-grams.</title>
<date>2011</date>
<booktitle>Proceedings of CoNLL</booktitle>
<contexts>
<context position="1346" citStr="Li et al., 2011" startWordPosition="184" endWordPosition="187">, the generator filters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date. 1 Introduction It is becoming a real possibility for intelligent systems to talk about the visual world. New ways of mapping computer vision to generated language have emerged in the past few years, with a focus on pairing detections in an image to words (Farhadi et al., 2010; Li et al., 2011; Kulkarni et al., 2011; Yang et al., 2011). The goal in connecting vision to language has varied: systems have started producing language that is descriptive and poetic (Li et al., 2011), summaries that add content where the computer vision system does not (Yang et al., 2011), and captions copied directly from other images that are globally (Farhadi et al., 2010) and locally similar (Ordonez et al., 2011). A commonality between all of these approaches is that they aim to produce naturalsounding descriptions from computer vision detections. This commonality is our starting point: We aim to des</context>
<context position="9039" citStr="Li et al. (2011)" startWordPosition="1450" endWordPosition="1453">on, and near the third rusty person. Yang et al.: Three people Yang et al.: The person is are showing the bottle on the sitting in the chair in the street room Midge: people with a bottle at Midge: a person in black the table with a black dog by potted plants Figure 3: Descriptions generated by Midge, Kulkarni et al. (2011) and Yang et al. (2011) on the same images. Midge uses the Kulkarni et al. (2011) front-end, and so outputs are directly comparable. paper. However, their system is not automatic, requiring extensive hand-coded semantic and syntactic details. Another approach is provided in Li et al. (2011), who use image detections to select and combine web-scale n-grams (Brants and Franz, 2006). This automatically generates descriptions that are either poetic or strange (e.g., “tree snowing black train”). A different line of work transfers captions of similar images directly to a query image. Farhadi et al. (2010) use &lt;object,action,scene&gt; triples predicted from the visual characteristics of the image to find potential captions. Ordonez et al. (2011) use global image matching with local reordering from a much larger set of captioned photographs. These transfer-based approaches result in natura</context>
</contexts>
<marker>Li, Kulkarni, Berg, Berg, Choi, 2011</marker>
<rawString>Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi. 2011. Composing simple image descriptions using web-scale n-grams. Proceedings of CoNLL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Ann Bies</author>
<author>Constance Cooper</author>
<author>Mark Ferguson</author>
<author>Alyson Littman</author>
</authors>
<title>Treebank II bracketing guide.</title>
<date>1995</date>
<contexts>
<context position="2635" citStr="Marcus et al., 1995" startWordPosition="399" endWordPosition="402">computer vision detections that are flexible enough to become more descriptive and poetic, or include likely inThe bus by the road with a clear blue sky Figure 1: Example image with generated description. formation from a language model, or to be short and simple, but as true to the image as possible. Rather than using a fixed template capable of generating one kind of utterance, our approach therefore lies in generating syntactic trees. We use a tree-generating process (Section 4.3) similar to a Tree Substitution Grammar, but preserving some of the idiosyncrasies of the Penn Treebank syntax (Marcus et al., 1995) on which most statistical parsers are developed. This allows us to automatically parse and train on an unlimited amount of text, creating data-driven models that flesh out descriptions around detected objects in a principled way, based on what is both likely and syntactically well-formed. An example generated description is given in Figure 1, and example vision output/natural language generation (NLG) input is given in Figure 2. The system (“Midge”) generates descriptions in present-tense, declarative phrases, as a naive viewer without prior knowledge of the photograph’s content.1 Midge is bu</context>
<context position="14343" citStr="Marcus et al., 1995" startWordPosition="2320" endWordPosition="2323">e also find that many of the descriptions are not sentences as well (tagged as S, 58% of the data), but quite commonly noun phrases (tagged as NP, 28% of the data), and expect that the number of noun phrases that form descriptions will be much higher with domain adaptation. This also informs generation, and the system is capable of generating both sentences (contains a main verb) and noun phrases (no main verb) in the final image description. We use the term ‘sentence’ in the rest of this paper to refer to both kinds of complex phrases. 4 Generation Following Penn Treebank parsing guidelines (Marcus et al., 1995), the relationship between two head nouns in a sentence can usually be characterized among the following: 1. prepositional (a boy on the table) 2. verbal (a boy cleans the table) 3. verb with preposition (a boy sits on the table) 4. verb with particle (a boy cleans up the table) 5. verb with S or SBAR complement (a boy sees that the table is clean) The generation system focuses on the first three kinds of relationships, which capture a wide range of utterances. The process of generation is approached as a problem of generating a semantically and syntactically well-formed tree based on object n</context>
</contexts>
<marker>Marcus, Bies, Cooper, Ferguson, Littman, 1995</marker>
<rawString>Mitchell Marcus, Ann Bies, Constance Cooper, Mark Ferguson, and Alyson Littman. 1995. Treebank II bracketing guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="13371" citStr="Miller, 1995" startWordPosition="2149" endWordPosition="2150">ies for different prenominal modifiers (shiny, clear, glowing, ...) and determiners (a/an, the, None, ...) given a head noun in a noun phrase (NP), as well as the probabilities for each head noun in larger constructions, listed in Section 4.3. Probabilities are conditioned only on open-class words, specifically, nouns and verbs. This means that a closedclass word (such as a preposition) is never used to generate an open-class word. In addition to co-occurrence statistics, the parsed Flickr data adds to our understanding of the basic characteristics of visually descriptive text. Using WordNet (Miller, 1995) to automatically determine whether a head noun is a physical object or not, we find that 92% of the sentences have no more than 3 physical objects. This informs generation by placing a cap on how many objects are mentioned in each descriptive sentence: When more than 3 objects are detected, the system splits the description over several sentences. We also find that many of the descriptions are not sentences as well (tagged as S, 58% of the data), but quite commonly noun phrases (tagged as NP, 28% of the data), and expect that the number of noun phrases that form descriptions will be much high</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Aaron Dunlop</author>
<author>Brian Roark</author>
</authors>
<title>Semi-supervised modeling for prenominal modifier ordering.</title>
<date>2011</date>
<booktitle>Proceedings of the 49th ACL:HLT.</booktitle>
<contexts>
<context position="28902" citStr="Mitchell et al. (2011)" startWordPosition="4853" endWordPosition="4856">econd method, we limit the generation system to select the most likely closed-class words (determiners, prepositions) while building the subtrees, overgenerating all possible adjective combinations. The final string is then the one with the most words. We find that the second method produces descriptions that seem more natural and varied than the n-gram ranking method for our development set, and so use the longest string method in evaluation. 4.4.2 Step 8: Prenominal Modifier Ordering To order sets of selected adjectives, we use the top-scoring prenominal modifier ordering model discussed in Mitchell et al. (2011). This is an ngram model constructed over noun phrases that were extracted from an automatically parsed version of the New York Times portion of the Gigaword corpus (Graff and Cieri, 2003). With this in place, blue clear sky becomes clear blue sky, wooden brown table becomes brown wooden table, etc. 5 Evaluation Each set of sentences is generated with α (likelihood cutoff) set to .01 and -y (observation count cutoff) set to 3. We compare the system against human-written descriptions and two state-of-theart vision-to-language systems, the Kulkarni et al. (2011) and Yang et al. (2011) systems. H</context>
</contexts>
<marker>Mitchell, Dunlop, Roark, 2011</marker>
<rawString>Margaret Mitchell, Aaron Dunlop, and Brian Roark. 2011. Semi-supervised modeling for prenominal modifier ordering. Proceedings of the 49th ACL:HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Evaluating sentence compression: Pitfalls and suggested remedies.</title>
<date>2011</date>
<booktitle>ACLHLT Workshop on Monolingual Text-To-Text Generation.</booktitle>
<marker>Napoles, Van Durme, Callison-Burch, 2011</marker>
<rawString>Courtney Napoles, Benjamin Van Durme, and Chris Callison-Burch. 2011. Evaluating sentence compression: Pitfalls and suggested remedies. ACLHLT Workshop on Monolingual Text-To-Text Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicente Ordonez</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
</authors>
<title>Im2text: Describing images using 1 million captioned photographs.</title>
<date>2011</date>
<booktitle>Proceedings of NIPS</booktitle>
<contexts>
<context position="1755" citStr="Ordonez et al., 2011" startWordPosition="253" endWordPosition="256">about the visual world. New ways of mapping computer vision to generated language have emerged in the past few years, with a focus on pairing detections in an image to words (Farhadi et al., 2010; Li et al., 2011; Kulkarni et al., 2011; Yang et al., 2011). The goal in connecting vision to language has varied: systems have started producing language that is descriptive and poetic (Li et al., 2011), summaries that add content where the computer vision system does not (Yang et al., 2011), and captions copied directly from other images that are globally (Farhadi et al., 2010) and locally similar (Ordonez et al., 2011). A commonality between all of these approaches is that they aim to produce naturalsounding descriptions from computer vision detections. This commonality is our starting point: We aim to design a system capable of producing natural-sounding descriptions from computer vision detections that are flexible enough to become more descriptive and poetic, or include likely inThe bus by the road with a clear blue sky Figure 1: Example image with generated description. formation from a language model, or to be short and simple, but as true to the image as possible. Rather than using a fixed template ca</context>
<context position="9493" citStr="Ordonez et al. (2011)" startWordPosition="1519" endWordPosition="1522">parable. paper. However, their system is not automatic, requiring extensive hand-coded semantic and syntactic details. Another approach is provided in Li et al. (2011), who use image detections to select and combine web-scale n-grams (Brants and Franz, 2006). This automatically generates descriptions that are either poetic or strange (e.g., “tree snowing black train”). A different line of work transfers captions of similar images directly to a query image. Farhadi et al. (2010) use &lt;object,action,scene&gt; triples predicted from the visual characteristics of the image to find potential captions. Ordonez et al. (2011) use global image matching with local reordering from a much larger set of captioned photographs. These transfer-based approaches result in natural captions (they are written by humans) that may not actually be true of the image. This work learns and builds from these approaches. Following Kulkarni et al. and Li et al., the system uses large-scale text corpora to estimate likely words around object detections. Following Yang et al., the system can hallucinate likely words using word co-occurrence statistics alone. And following Yao et al., the system aims black, blue, brown, colorful, golden, </context>
<context position="11884" citStr="Ordonez et al. (2011)" startWordPosition="1903" endWordPosition="1907">of the process, which may be improved: Midge serves as an illustration of the types of issues that should be handled to automatically generate syntactic trees from vision detections, and offers some possible solutions. It is evaluated against the Kulkarni et al. system, the Yang et al. system, and human-written descriptions on the same set of images in Section 5, and is found to significantly outperform the automatic systems. 3 Learning from Descriptive Text To train our system on how people describe images, we use 700,000 (Flickr, 2011) images with associated descriptions from the dataset in Ordonez et al. (2011). This is separate from our evaluation image set, consisting of 840 PASCAL images. The Flickr data is messier than datasets created specifically for vision training, but provides the largest corpus of natural descriptions of images to date. We normalize the text by removing emoticons and mark-up language, and parse each caption using the Berkeley parser (Petrov, 2010). Once parsed, we can extract syntactic information for individual (word, tag) pairs. 749 a cow with sheep with a gray sky people with boats a brown cow people at green grass by the road a wooden table Figure 4: Example generated </context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>Vicente Ordonez, Girish Kulkarni, and Tamara L Berg. 2011. Im2text: Describing images using 1 million captioned photographs. Proceedings of NIPS 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<date>2010</date>
<booktitle>Berkeley parser. GNU General Public License v.2.</booktitle>
<contexts>
<context position="12254" citStr="Petrov, 2010" startWordPosition="1964" endWordPosition="1965"> to significantly outperform the automatic systems. 3 Learning from Descriptive Text To train our system on how people describe images, we use 700,000 (Flickr, 2011) images with associated descriptions from the dataset in Ordonez et al. (2011). This is separate from our evaluation image set, consisting of 840 PASCAL images. The Flickr data is messier than datasets created specifically for vision training, but provides the largest corpus of natural descriptions of images to date. We normalize the text by removing emoticons and mark-up language, and parse each caption using the Berkeley parser (Petrov, 2010). Once parsed, we can extract syntactic information for individual (word, tag) pairs. 749 a cow with sheep with a gray sky people with boats a brown cow people at green grass by the road a wooden table Figure 4: Example generated outputs. Awkward Prepositions Incorrect Detections a person boats under a black bicycle at the sky a yellow bus cows by black sheep on the dog the sky a green potted plant with people by the road Figure 5: Example generated outputs: Not quite right We compute the probabilities for different prenominal modifiers (shiny, clear, glowing, ...) and determiners (a/an, the, </context>
</contexts>
<marker>Petrov, 2010</marker>
<rawString>Slav Petrov. 2010. Berkeley parser. GNU General Public License v.2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyrus Rashtchian</author>
<author>Peter Young</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Collecting image annotations using amazon’s mechanical turk.</title>
<date>2010</date>
<booktitle>Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="6154" citStr="Rashtchian et al. (2010)" startWordPosition="978" endWordPosition="981">ed on object nouns, and this enables each noun head in a description to select for the kinds of structures it tends to appear in (syntactic constraints) and the other words it tends to occur with (semantic constraints). This is a data-driven way to generate likely adjectives, prepositions, determiners, etc., taking the intersection of what the vision system predicts and how the object noun tends to be described. 2 Background Our approach to describing images starts with a system from Kulkarni et al. (2011) that composes novel captions for images in the PASCAL sentence data set,2 introduced in Rashtchian et al. (2010). This provides multiple object detections based on Felzenszwalb’s mixtures of multiscale deformable parts models (Felzenszwalb et al., 2008), and stuff detections (roughly, mass nouns, things like sky and grass) based on linear SVMs for low level region features. Appearance characteristics are predicted using trained detectors for colors, shapes, textures, and materials, an idea originally introduced in Farhadi et al. (2009). Local texture, Histograms of Oriented Gradients (HOG) (Dalal and Triggs, 2005), edge, and color descriptors inside the bounding box of a recognized object are binned int</context>
</contexts>
<marker>Rashtchian, Young, Hodosh, Hockenmaier, 2010</marker>
<rawString>Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. 2010. Collecting image annotations using amazon’s mechanical turk. Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Anja Belz</author>
</authors>
<title>An investigation into the validity of some metrics for automatically evaluating natural language generation systems.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="29663" citStr="Reiter and Belz, 2009" startWordPosition="4979" endWordPosition="4982">on of the Gigaword corpus (Graff and Cieri, 2003). With this in place, blue clear sky becomes clear blue sky, wooden brown table becomes brown wooden table, etc. 5 Evaluation Each set of sentences is generated with α (likelihood cutoff) set to .01 and -y (observation count cutoff) set to 3. We compare the system against human-written descriptions and two state-of-theart vision-to-language systems, the Kulkarni et al. (2011) and Yang et al. (2011) systems. Human judgments were collected using Amazon’s Mechanical Turk (Amazon, 2011). We follow recommended practices for evaluating an NLG system (Reiter and Belz, 2009) and for running a study on Mechanical Turk (Callison-Burch and Dredze, 2010), using a balanced design with each subject rating 3 descriptions from each system. Subjects rated their level of agreement on a 5-point Likert scale including a neutral middle position, and since quality ratings are ordinal (points are not necessarily equidistant), we evaluate responses using a non-parametric test. Participants that took less than 3 minutes to answer all 60 questions and did not include a humanlike rating for at least 1 of the 3 human-written descriptions were removed and replaced. It is important to</context>
</contexts>
<marker>Reiter, Belz, 2009</marker>
<rawString>Ehud Reiter and Anja Belz. 2009. An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics, 35(4):529–558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building applied natural language generation systems.</title>
<date>1997</date>
<journal>Journal of Natural Language Engineering,</journal>
<pages>57--87</pages>
<contexts>
<context position="7711" citStr="Reiter and Dale (1997)" startWordPosition="1219" endWordPosition="1222"> combining computer vision outputs with text based statistics in a conditional random field to predict the most likely image labeling. Templatebased generation is also used in the recent Yang et al. (2011) system, which fills in likely verbs and prepositions by dependency parsing the humanwritten UIUC Pascal-VOC dataset (Farhadi et al., 2010) and selecting the dependent/head relation with the highest log likelihood ratio. Template-based generation is useful for automatically generating consistent sentences, however, if the goal is to vary or add to the text produced, it may be suboptimal (cf. Reiter and Dale (1997)). Work that does not use template-based generation includes Yao et al. (2010), who generate syntactic trees, similar to the approach in this 2http://vision.cs.uiuc.edu/pascal-sentences/ 748 Kulkarni et al.: This is a pic- Kulkarni et al.: This is ture of three persons, one bot- a picture of two pottedtle and one diningtable. The plants, one dog and one first rusty person is beside the person. The black dog is second person. The rusty bot- by the black person, and tle is near the first rusty per- near the second feathered son, and within the colorful pottedplant. diningtable. The second person</context>
</contexts>
<marker>Reiter, Dale, 1997</marker>
<rawString>Ehud Reiter and Robert Dale. 1997. Building applied natural language generation systems. Journal of Natural Language Engineering, pages 57–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="16530" citStr="Reiter and Dale, 2000" startWordPosition="2694" endWordPosition="2697">uage connection (Kulkarni et al., 2011; Yang et al., 2011), and allows us to compare systems directly. Action and pose detection for this data set still does not work well, and so the system does not receive these detections from the vision front-end. However, the system can still generate verbs when action and pose detectors have been run, and this framework allows the system to “hallucinate” likely verbal constructions between objects if specified at runtime. A similar approach was taken in Yang et al. (2011). Some examples are given in Figure 7. We follow a three-tiered generation process (Reiter and Dale, 2000), utilizing content determination to first cluster and order the object nouns, create their local subtrees, and filter incorrect detections; microplanning to construct full syntactic trees around the noun clusters, and surface realization to order selected modifiers, realize them as postnominal or prenominal, and select final outputs. The system follows an overgenerate-andselect approach (Langkilde and Knight, 1998), which allows different final trees to be selected with different settings. 4.1 Knowledge Base Midge uses a knowledge base that stores models for different tasks during generation.</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
</authors>
<title>Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos.</title>
<date>2011</date>
<booktitle>Proceedings of EMNLP</booktitle>
<marker>Yang, 2011</marker>
<rawString>Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. Proceedings of EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Z Yao</author>
<author>Xiong Yang</author>
<author>Liang Lin</author>
<author>Mun Wai Lee</author>
<author>Song-Chun Zhu</author>
</authors>
<title>I2T: Image parsing to text description.</title>
<date>2010</date>
<booktitle>Proceedings of IEEE 2010,</booktitle>
<pages>98--8</pages>
<contexts>
<context position="7789" citStr="Yao et al. (2010)" startWordPosition="1231" endWordPosition="1234">om field to predict the most likely image labeling. Templatebased generation is also used in the recent Yang et al. (2011) system, which fills in likely verbs and prepositions by dependency parsing the humanwritten UIUC Pascal-VOC dataset (Farhadi et al., 2010) and selecting the dependent/head relation with the highest log likelihood ratio. Template-based generation is useful for automatically generating consistent sentences, however, if the goal is to vary or add to the text produced, it may be suboptimal (cf. Reiter and Dale (1997)). Work that does not use template-based generation includes Yao et al. (2010), who generate syntactic trees, similar to the approach in this 2http://vision.cs.uiuc.edu/pascal-sentences/ 748 Kulkarni et al.: This is a pic- Kulkarni et al.: This is ture of three persons, one bot- a picture of two pottedtle and one diningtable. The plants, one dog and one first rusty person is beside the person. The black dog is second person. The rusty bot- by the black person, and tle is near the first rusty per- near the second feathered son, and within the colorful pottedplant. diningtable. The second person is by the third rusty person. The colorful diningtable is near the first rust</context>
</contexts>
<marker>Yao, Yang, Lin, Lee, Zhu, 2010</marker>
<rawString>Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai Lee, and Song-Chun Zhu. 2010. I2T: Image parsing to text description. Proceedings of IEEE 2010, 98(8):1485–1508.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>