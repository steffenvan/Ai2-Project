<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000042">
<title confidence="0.9995905">
Discriminative Learning with Natural Annotations:
Word Segmentation as a Case Study
</title>
<author confidence="0.999618">
Wenbin Jiang 1 Meng Sun 1 Yajuan L¨u 1 Yating Yang 2 Qun Liu 3, 1
</author>
<affiliation confidence="0.99831">
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
</affiliation>
<email confidence="0.937634">
{jiangwenbin, sunmeng, lvyajuan}@ict.ac.cn
</email>
<affiliation confidence="0.824458">
2Multilingual Information Technology Research Center
The Xinjiang Technical Institute of Physics &amp; Chemistry, Chinese Academy of Sciences
</affiliation>
<email confidence="0.940933">
yangyt@ms.xjb.ac.cn
</email>
<affiliation confidence="0.954432">
3Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
</affiliation>
<email confidence="0.992155">
qliu@computing.dcu.ie
</email>
<sectionHeader confidence="0.995542" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999855944444445">
Structural information in web text pro-
vides natural annotations for NLP prob-
lems such as word segmentation and pars-
ing. In this paper we propose a discrim-
inative learning algorithm to take advan-
tage of the linguistic knowledge in large
amounts of natural annotations on the In-
ternet. It utilizes the Internet as an external
corpus with massive (although slight and
sparse) natural annotations, and enables a
classifier to evolve on the large-scaled and
real-time updated web text. With Chinese
word segmentation as a case study, exper-
iments show that the segmenter enhanced
with the Chinese wikipedia achieves sig-
nificant improvement on a series of testing
sets from different domains, even with a
single classifier and local features.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9840259375">
Problems related to information retrieval, machine
translation and social computing need fast and ac-
curate text processing, for example, word segmen-
tation and parsing. Taking Chinese word seg-
mentation for example, the state-of-the-art mod-
els (Xue and Shen, 2003; Ng and Low, 2004;
Gao et al., 2005; Nakagawa and Uchimoto, 2007;
Zhao and Kit, 2008; Jiang et al., 2009; Zhang and
Clark, 2010; Sun, 2011b; Li, 2011) are usually
trained on human-annotated corpora such as the
Penn Chinese Treebank (CTB) (Xue et al., 2005),
and perform quite well on corresponding test sets.
Since the text used for corpus annotating are usu-
ally drawn from specific fields (e.g. newswire or
finance), and the annotated corpora are limited in
think that NLP has already ...
</bodyText>
<equation confidence="0.95642275">
n ఊ น ᆑ ௶ ဴ ཝ ҉ स ྸ ࠼ n
i-1 i j j+1
(a) Natural annotation by hyperlink
n ఊ น ᆑ ௶ ဴ ཝ ҉ स ྸ ࠼ n
i-1 i j j+1
(b) Knowledge for word segmentation
n ఊ น ᆑ ௶ ဴ ཝ ҉ स ྸ ࠼ n
i-1 i j j+1
</equation>
<listItem confidence="0.764067">
(c) Knowledge for dependency parsing
</listItem>
<figureCaption confidence="0.9756715">
Figure 1: Natural annotations for word segmenta-
tion and dependency parsing.
</figureCaption>
<bodyText confidence="0.995136647058824">
size (e.g. tens of thousands), the performance of
word segmentation tends to degrade sharply when
applied to new domains.
Internet provides large amounts of raw text, and
statistics collected from it have been used to im-
prove parsing performance (Nakov and Hearst,
2005; Pitler et al., 2010; Bansal and Klein, 2011;
Zhou et al., 2011). The Internet also gives mas-
sive (although slight and sparse) natural annota-
tions in the forms of structural information includ-
ing hyperlinks, fonts, colors and layouts (Sun,
2011a). These annotations usually imply valuable
knowledge for problems such as word segmen-
tation and parsing, based on the hypothesis that
the subsequences marked by structural informa-
tion are meaningful fragments in sentences. Fig-
ure 1 shows an example. The hyperlink indicates
</bodyText>
<page confidence="0.959982">
761
</page>
<note confidence="0.914376">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761–769,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.988002653846154">
a Chinese phrase (meaning NLP), and it probably
corresponds to a connected sub-graph for depen-
dency parsing. Creators of web text give valuable
annotations during editing, the whole Internet can
be treated as a wide-coveraged and real-time up-
dated corpus.
Different from the dense and accurate annota-
tions in human-annotated corpora, natural annota-
tions in web text are sparse and slight, it makes
direct training of NLP models impracticable. In
this work we take for example a most important
problem, word segmentation, and propose a novel
discriminative learning algorithm to leverage the
knowledge in massive natural annotations of web
text. Character classification models for word seg-
mentation usually factorize the whole prediction
into atomic predictions on characters (Xue and
Shen, 2003; Ng and Low, 2004). Natural anno-
tations in web text can be used to get rid of im-
plausible predication candidates for related char-
acters, knowledge in the natural annotations is
therefore introduced in the manner of searching
space pruning. Since constraint decoding in the
pruned searching space integrates the knowledge
of the baseline model and natural annotations, it
gives predictions not worse than the normal decod-
ing does. Annotation differences between the out-
puts of constraint decoding and normal decoding
are used to train the enhanced classifier. This strat-
egy makes the usage of natural annotations simple
and universal, which facilitates the utilization of
massive web text and the extension to other NLP
problems.
Although there are lots of choices, we choose
the Chinese wikipedia as the knowledge source
due to its high quality. Structural information, in-
cluding hyperlinks, fonts and colors are used to de-
termine the boundaries of meaningful fragments.
Experimental results show that, the knowledge im-
plied in the natural annotations can significantly
improve the performance of a baseline segmenter
trained on CTB 5.0, an F-measure increment of
0.93 points on CTB test set, and an average incre-
ment of 1.53 points on 7 other domains. It is an ef-
fective and inexpensive strategy to build word seg-
menters adaptive to different domains. We hope to
extend this strategy to other NLP problems such
as named entity recognition and parsing.
In the rest of the paper, we first briefly intro-
duce the problems of Chinese word segmentation
and the character classification model in section
Type Templates Instances
</bodyText>
<equation confidence="0.987062666666667">
n-gram C−2 C−2=@
C−1 C−1=;Xj
C0 C0=0
C1 C1=,
C2 C2=*
C−2C−1 C−2C−1=@14J
C−1C0 C−1C0=3410
C0C1 C0C1=0,
C1C2 C1C2=,�
C−1C1 C−1C1=;X1,
function Pu(C0) Pu(C0)=false
T(C−2:2) T(C−2:2)= 44444
</equation>
<tableCaption confidence="0.939863">
Table 1: Feature templates and instances for
</tableCaption>
<bodyText confidence="0.888121181818182">
character classification-based word segmentation
model. Suppose we are considering the i-th char-
acter “0” in “...@J§ 0 ,������...”.
2, then describe the representation of the knowl-
edge in natural annotations of web text in section
3, and finally detail the strategy of discriminative
learning on natural annotations in section 4. Af-
ter giving the experimental results and analysis in
section 5, we briefly introduce the previous related
work and then give the conclusion and the expec-
tation of future research.
</bodyText>
<sectionHeader confidence="0.994817" genericHeader="method">
2 Character Classification Model
</sectionHeader>
<bodyText confidence="0.999796095238095">
Character classification models for word segmen-
tation factorize the whole prediction into atomic
predictions on single characters (Xue and Shen,
2003; Ng and Low, 2004). Although natural anno-
tations in web text do not directly support the dis-
criminative training of segmentation models, they
do get rid of the implausible candidates for predic-
tions of related characters.
Given a sentence as a sequence of n charac-
ters, word segmentation splits the sequence into
m(≤ n) subsequences, each of which indicates a
meaningful word. Word segmentation can be for-
malized as a character classification problem (Xue
and Shen, 2003), where each character in the sen-
tence is given a boundary tag representing its posi-
tion in a word. We adopt the boundary tags of Ng
and Low (2004), b, m, e and s, where b, m and
e mean the beginning, the middle and the end of a
word, and s indicates a single-character word. the
decoding procedure searches for the labeled char-
acter sequence y that maximizes the score func-
</bodyText>
<page confidence="0.988254">
762
</page>
<figure confidence="0.9967585">
(a) Original searching space
nఊ น ᆑ ௶ ဴ ཝ ҉ स ྸ ࠼ n
i-1 i j j+1
(b) Shrinked searching space
</figure>
<figureCaption confidence="0.981105">
Figure 2: Shrink of searching space for the charac-
ter classification-based word segmentation model.
</figureCaption>
<sectionHeader confidence="0.97361" genericHeader="method">
3 Knowledge in Natural Annotations
</sectionHeader>
<bodyText confidence="0.999694965517241">
Web text gives massive natural annotations in the
form of structural informations, including hyper-
links, fonts, colors and layouts (Sun, 2011a). Al-
though slight and sparse, these annotations imply
valuable knowledge for problems such as word
segmentation and parsing.
As shown in Figure 1, the subsequence P =
i..j of sentence S is composed of bolded charac-
ters determined by a hyperlink. Such natural anno-
tations do not clearly give each character a bound-
ary tag, or define the head-modifier relationship
between two words. However, they do help to
shrink the set of plausible predication candidates
for each character or word. For word segmenta-
tion, it implies that characters i − 1 and j are the
rightmost characters of words, while characters i
and j + 1 are the leftmost characters of words.
For i − 1 or j, the plausible predication set Ψ be-
comes {e, s}; For i and j + 1, it becomes {b, s};
For other characters c except the two at sentence
boundaries, Ψ(c) is still {b, m, e, s}. For depen-
dency parsing, the subsequence P tends to form
a connected dependency graph if it contains more
than one word. Here we use Ψ to denote the set of
plausible head of a word (modifier). There must
be a single word w E P as the root of subse-
quence P, whose plausible heads fall out of P,
that is, Ψ(w) = {xjx E S − P}. For the words
in P except the root, the plausible heads for each
</bodyText>
<equation confidence="0.833762">
= arg max � Φ(i, t, x, y) · α�
y (i,t)∈y
</equation>
<bodyText confidence="0.999708909090909">
The score of the whole sequence y is accumulated
across all its character-label pairs, (i, t) E y (s.t.
1 G i G n and t E {b, m, e, s}). The feature
function Φ maps a labeled sequence or a character-
label pair into a feature vector, α� is the parame-
ter vector and Φ(x, y) · α� is the inner product of
Φ(x, y) and a.
Analogous to other sequence labeling prob-
lems, word segmentation can be solved through a
viterbi-style decoding procedure. We omit the de-
coding algorithm in this paper due to its simplicity
and popularity.
The feature templates for the classifier is shown
in Table 1. C0 denotes the current character, while
C−k/Ck denote the kth character to the left/right
of C0. The function Pu(·) returns true for a punc-
tuation character and false for others, the function
T(·) classifies a character into four types, 1, 2, 3
and 4, representing number, date, English letter
and others, respectively.
The classifier can be trained with online learn-
ing algorithms such as perceptron, or offline learn-
ing models such as support vector machines.
We choose the perceptron algorithm (Collins,
2002) to train the classifier for the character
classification-based word segmentation model. It
learns a discriminative model mapping from the
inputs x E X to the outputs y˜ E Y , where X is the
set of sentences in the training corpus and Y is the
set of corresponding labeled results. Algorithm 1
shows the perceptron algorithm for tuning the pa-
rameter a. The “averaged parameters” technology
(Collins, 2002) is used for better performance.
</bodyText>
<figure confidence="0.808585163461539">
Algorithm 1 Perceptron training algorithm.
1: Input: Training corpus C
2: α�+— 0
3: for t +— 1 .. T do ⊲ T iterations
4: for (x, ˜y) E C do
5: y +— arg maxy Φ(x, y) · α�
6: if y =� y˜ then
7: α� +— α� + Φ(x, ˜y) − Φ(x, y)
8: Output: Parameters α�
tion:
f(x) = arg max S(yja, Φ, x)
y
= arg max Φ(x, y) · α� (1)
y
n ఊ น ᆑ ௶ ဴ ཝ ҉ स ྸ ࠼ n
i-1 i j j+1
m
m
m
m
m
m
m
m
m
m
b
b
b
b
b
b
b
b
b
b
e
e
e
e
e
e
e
e
e
e
s
s
s
s
s
s
s
s
s
s
n
n
n
n
n
n
n
n
b
b
b
b
b
b
b
m
m
m
m
m
b
m
e
e
e
e
e
e
e
e
s
s
s
s
s
s
s
s
s
s
n
n
n
n
n
n
n
n
</figure>
<page confidence="0.89902">
763
</page>
<bodyText confidence="0.788879">
Algorithm 2 Perceptron learning with natural an-
notations.
</bodyText>
<equation confidence="0.842819777777778">
1: α� TRAIN(C)
2: for x E T do
3: y DECODE(x, a)
4: y˜ CONSTRAINTDECODE(x, a, Ψ)
5: if y =� y˜ then
6: C′ C′ U {˜y}
7: α� TRAIN(C U C′)
word w are the words in P except w itself, that is,
Ψ(w) = {x|x E P − {w}}.
</equation>
<bodyText confidence="0.999751133333333">
Creators of web text give valuable structural
annotations during editing, these annotations re-
duce the predication uncertainty for atomic char-
acters or words, although not exactly defining
which predication is. Figure 2 shows an exam-
ple for word segmentation, depicting the shrink
of searching space for the character classification-
based model. Since the decrement of uncertainty
indicates the increment of knowledge, the whole
Internet can be treated as a wide-coveraged and
real-time updated corpus. We choose the Chinese
wikipedia as the external knowledge source, and
structural information including hyperlinks, fonts
and colors are used in the current work due to their
explicitness of representation.
</bodyText>
<sectionHeader confidence="0.944479" genericHeader="method">
4 Learning with Natural Annotations
</sectionHeader>
<bodyText confidence="0.998383130434783">
Different from the dense and accurate annotations
in human-annotated corpora, natural annotations
are sparse and slight, which makes direct training
of NLP models impracticable. Annotations im-
plied by structural information do not give an ex-
act predication to a character, however, they help
to get rid of the implausible predication candidates
for related characters, as described in the previous
section.
Previous work on constituency parsing or ma-
chine translation usually resort to some kinds of
heuristic tricks, such as punctuation restrictions,
to eliminate some implausible candidates during
decoding. Here the natural annotations also bring
knowledge in the manner of searching space prun-
ing. Conditioned on the completeness of the de-
coding algorithm, a model trained on an exist-
ing corpus probably gives better or at least not
worse predications, by constraint decoding in the
pruned searching space. The constraint decoding
procedure integrates the knowledge of the baseline
Algorithm 3 Online version of perceptron learn-
ing with natural annotations.
</bodyText>
<listItem confidence="0.966681571428571">
1: α� TRAIN(C)
2: for x with natural annotations do
3: y DECODE(x, a)
4: y˜ CONSTRAINTDECODE(x, a, Ψ)
5: if y =� y˜ then
6: α� α� + Φ(x, ˜y) − Φ(x, y)
7: output α� at regular time
</listItem>
<bodyText confidence="0.99989715">
model and natural annotations, the predication dif-
ferences between the outputs of constraint decod-
ing and normal decoding can be used to train the
enhanced classifier.
Restrictions of the searching space according to
natural annotations can be easily incorporated into
the decoder. If the completeness of the searching
algorithm can be guaranteed, the constraint decod-
ing in the pruned searching space will give predi-
cations not worse than those given by the normal
decoding. If a predication of constraint decoding
differs from that of normal decoding, it indicates
that the annotation precision is higher than the lat-
ter. Furthermore, the degree of difference between
the two predications represents the amount of new
knowledge introduced by the natural annotations
over the baseline.
The baseline model α� is trained on an exist-
ing human-annotated corpus. A set of sentences
T with natural annotations are extracted from the
Chinese wikipedia, and we reserve the ones for
which constraint decoding and normal decoding
give different predications. The predictions of re-
served sentences by constraint decoding are used
as additional training data for the enhanced classi-
fier. The overall training pipeline is analogous to
self-training (McClosky et al., 2006), Algorithm
2 shows the pseudo-codes. Considering the online
characteristic of the perceptron algorithm, if we
are able to leverage much more (than the Chinese
wikipedia) data with natural annotations, an online
version of learning procedure shown in Algorithm
3 would be a better choice. The technology of “av-
eraged parameters” (Collins, 2002) is easily to be
adapted here for better performance.
When constraint decoding and normal decod-
ing give different predications, we only know that
the former is probably better than the latter. Al-
though there is no explicit evidence for us to mea-
sure how much difference in accuracy between the
</bodyText>
<page confidence="0.981759">
764
</page>
<table confidence="0.998576571428571">
Partition Sections # of word
CTB
Training 1 − 270 0.47M
400 − 931
1001 − 1151
Developing 301 − 325 6.66K
Testing 271 − 300 7.82K
</table>
<tableCaption confidence="0.56004">
Table 2: Data partitioning for CTB 5.0.
</tableCaption>
<figure confidence="0.929935636363636">
Accuracy (F1%)
97.4
97.2
96.8
96.6
96.4
96.2
95.8
95.6
97
96
</figure>
<bodyText confidence="0.999874882352941">
two predications, we can approximate how much
new knowledge that a naturally annotated sentence
brings. For a sentence x, given the predications of
constraint decoding and normal decoding, y˜ and
y, the difference of their scores S = 5(y) − 5(˜y)
indicates the degree to which the current model
mistakes. This indicator helps us to select more
valuable training examples.
The strategy of learning with natural annota-
tions can be adapted to other situations. For ex-
ample, if we have a list of words or phrases (espe-
cially in a specific domain such as medicine and
chemical), we can generate annotated sentences
automatically by string matching in a large amount
of raw text. It probably provides a simple and
effective domain adaptation strategy for already
trained models.
</bodyText>
<sectionHeader confidence="0.999423" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999956476190476">
We use the Penn Chinese Treebank 5.0 (CTB)
(Xue et al., 2005) as the existing annotated cor-
pus for Chinese word segmentation. For conve-
nient of comparison with other work in word seg-
mentation, the whole corpus is split into three par-
titions as follows: chapters 271-300 for testing,
chapters 301-325 for developing, and others for
training. We choose the Chinese wikipedia 1 (ver-
sion 20120812) as the external knowledge source,
because it has high quality in contents and it is
much better than usual web text. Structural infor-
mations, including hyperlinks, fonts and colors are
used to derive the annotation information.
To further evaluate the improvement brought
by the fuzzy knowledge in Chinese wikipedia, a
series of testing sets from different domains are
adopted. The four testing sets from SIGHAN
Bakeoff 2010 (Zhao and Liu, 2010) are used, they
are drawn from the domains of literature, finance,
computer science and medicine. Although the ref-
erence sets are annotated according to a different
</bodyText>
<footnote confidence="0.697032">
1http://download.wikimedia.org/backup-index.html.
</footnote>
<figure confidence="0.8733845">
1 2 3 4 5 6 7 8 9 10
Training iterations
</figure>
<figureCaption confidence="0.9935735">
Figure 3: Learning curve of the averaged percep-
tron classifier on the CTB developing set.
</figureCaption>
<bodyText confidence="0.973875857142857">
word segmentation standard (Yu et al., 2001), the
quantity of accuracy improvement is still illustra-
tive since there are no vast diversities between the
two segmentation standards. We also annotated
another three testing sets 2, their texts are drawn
from the domains of chemistry, physics and ma-
chinery, and each contains 500 sentences.
</bodyText>
<subsectionHeader confidence="0.948438">
5.1 Baseline Classifier for Word
Segmentation
</subsectionHeader>
<bodyText confidence="0.999828391304348">
We train the baseline perceptron classifier for
word segmentation on the training set of CTB
5.0, using the developing set to determine the
best training iterations. The performance mea-
surement for word segmentation is balanced F-
measure, F = 2PR/(P + R), a function of preci-
sion P and recall R, where P is the percentage of
words in segmentation results that are segmented
correctly, and R is the percentage of correctly seg-
mented words in the gold standard words.
Figure 3 shows the learning curve of the aver-
aged perceptron on the developing set. The sec-
ond column of Table 3 lists the performance of
the baseline classifier on eight testing sets, where
newswire denotes the testing set of the CTB it-
self. The classifier performs much worse on the
domains of chemistry, physics and machinery, it
indicates the importance of domain adaptation for
word segmentation (Gao et al., 2004; Ma and
Way, 2009; Gao et al., 2010). The accuracy on the
testing sets from SIGHAN Bakeoff 2010 is even
lower due to the difference in both domains and
word segmentation standards.
</bodyText>
<footnote confidence="0.953743">
2They are available at http://nlp.ict.ac.cn/ jiangwenbin/.
</footnote>
<page confidence="0.99347">
765
</page>
<figure confidence="0.856824">
Dataset Baseline (F%) Enhanced (F%)
Newswire 97.35 98.28 +0.93
Out-of-Domain
Chemistry 93.61 95.68 +2.07
Physics 95.10 97.24 +2.14
Machinery 96.08 97.66 +1.58
Literature 92.42 93.53 +1.11
Finance 92.50 93.16 +0.66
Computer 89.46 91.19 +1.73
Medicine 91.88 93.34 +1.46
Average 93.01 94.54 +1.53
10000 20000 40000 80000 160000 320000 640000
97.8
97.7
97.6
97.5
97.4
97.3
97.2
97.1
using selected sentences
using all sentences
Count of selected sentences
Accuracy (F1%)
</figure>
<tableCaption confidence="0.982852666666667">
Table 3: Performance of the baseline classifier and
the classifier enhanced with natural annotations in
Chinese wikipedia.
</tableCaption>
<subsectionHeader confidence="0.560456">
5.2 Classifier Enhanced with Natural
Annotations
</subsectionHeader>
<bodyText confidence="0.999833">
The Chinese wikipedia contains about 0.5 million
items. From their description text, about 3.9 mil-
lions of sentences with natural annotations are ex-
tracted. With the CTB training set as the exist-
ing corpus C, about 0.8 million sentences are re-
served according to Algorithm 2, the segmenta-
tions given by constraint decoding are used as ad-
ditional training data for the enhanced classifier.
According to the previous description, the dif-
ference of the scores of constraint decoding and
normal decoding, S = 5(y) − 5(˜y), indicates
the importance of a constraint segmentation to the
improvement of the baseline classifier. The con-
straint segmentations of the reserved sentences are
sorted in descending order according to the dif-
ference of the scores of constraint decoding and
normal decoding, as described previously. From
the beginning of the sorted list, different amounts
of segmented sentences are used as the additional
training data for the enhanced character classifier.
Figure 4 shows the performance curve of the en-
hanced classifiers on the developing set of CTB.
We found that the highest accuracy was achieved
when 160, 000 sentences were used, while more
additional training data did not give continuous
improvement. A recent related work about self-
training for segmentation (Liu and Zhang, 2012)
also reported a very similar trend, that only a mod-
erate amount of raw data gave the most obvious
improvements.
The performance of the enhanced classifier is
listed in the third column of Table 3. On the
CTB testing set, training data from the Chinese
</bodyText>
<figureCaption confidence="0.9978415">
Figure 4: Performance curve of the classifier en-
hanced with selected sentences of different scales.
</figureCaption>
<table confidence="0.999501571428571">
Model Accuracy (F%)
(Jiang et al., 2008) 97.85
(Kruengkrai et al., 2009) 97.87
(Zhang and Clark, 2010) 97.79
(Wang et al., 2011) 98.11
(Sun, 2011b) 98.17
Our Work 98.28
</table>
<tableCaption confidence="0.982934">
Table 4: Comparison with state-of-the-art work in
Chinese word segmentation.
</tableCaption>
<bodyText confidence="0.999635230769231">
wikipedia brings an F-measure increment of 0.93
points. On out-of-domain testing sets, the im-
provements are much larger, an average increment
of 1.53 points is achieved on seven domains. It
is probably because the distribution of the knowl-
edge in the CTB training data is concentrated in
the domain of newswire, while the contents of
the Chinese wikipedia cover a broad range of do-
mains, it provides knowledge complementary to
that of CTB.
Table 4 shows the comparison with other
work in Chinese word segmentation. Our model
achieves an accuracy higher than that of the
state-of-the-art models trained on CTB only, al-
though using a single classifier with only local
features. From the viewpoint of resource uti-
lization, the comparison between our system and
previous work without using additional training
data is unfair. However, we believe this work
shows another interesting way to improve Chi-
nese word segmentation, it focuses on the utiliza-
tion of fuzzy and sparse knowledge on the Internet
rather than making full use of a specific human-
annotated corpus. On the other hand, since only
a single classifier and local features are used in
our method, better performance could be achieved
</bodyText>
<page confidence="0.994878">
766
</page>
<bodyText confidence="0.999970166666667">
resorting to complicated features, system com-
bination and other semi-supervised technologies.
What is more, since the text on Internet is wide-
coveraged and real-time updated, our strategy also
helps a word segmenter be more domain adaptive
and up to date.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999954849315069">
Li and Sun (2009) extracted character classifi-
cation instances from raw text for Chinese word
segmentation, resorting to the indication of punc-
tuation marks between characters. Sun and Xu
(Sun and Xu, 2011) utilized the features derived
from large-scaled unlabeled text to improve Chi-
nese word segmentation. Although the two work
also made use of large-scaled raw text, our method
is essentially different from theirs in the aspects
of both the source of knowledge and the learning
strategy.
Lots of efforts have been devoted to semi-
supervised methods in sequence labeling and word
segmentation (Xu et al., 2008; Suzuki and Isozaki,
2008; Haffari and Sarkar, 2008; Tomanek and
Hahn, 2009; Wang et al., 2011). A semi-
supervised method tries to find an optimal hyper-
plane of both annotated data and raw data, thus to
result in a model with better coverage and higher
accuracy. Researchers have also investigated un-
supervised methods in word segmentation (Zhao
and Kit, 2008; Johnson and Goldwater, 2009;
Mochihashi et al., 2009; Hewlett and Cohen,
2011). An unsupervised method mines the latent
distribution regularity in the raw text, and auto-
matically induces word segmentation knowledge
from it. Our method also needs large amounts of
external data, but it aims to leverage the knowl-
edge in the fuzzy and sparse annotations. It is
fundamentally different from semi-supervised and
unsupervised methods in that we aimed to exca-
vate a totally different kind of knowledge, the nat-
ural annotations implied by the structural informa-
tion in web text.
In recent years, much work has been devoted to
the improvement of word segmentation in a vari-
ety of ways. Typical approaches include the in-
troduction of global training or complicated fea-
tures (Zhang and Clark, 2007; Zhang and Clark,
2010), the investigation of word internal structures
(Zhao, 2009; Li, 2011), the adjustment or adapta-
tion of word segmentation standards (Wu, 2003;
Gao et al., 2004; Jiang et al., 2009), the integrated
solution of segmentation and related tasks such as
part-of-speech tagging and parsing (Zhou and Su,
2003; Zhang et al., 2003; Fung et al., 2004; Gold-
berg and Tsarfaty, 2008), and the strategies of hy-
brid or stacked modeling (Nakagawa and Uchi-
moto, 2007; Kruengkrai et al., 2009; Wang et al.,
2010; Sun, 2011b).
In parsing, Pereira and Schabes (1992) pro-
posed an extended inside-outside algorithm that
infers the parameters of a stochastic CFG from a
partially parsed treebank. It uses partial bracket-
ing information to improve parsing performance,
but it is specific to constituency parsing, and its
computational complexity makes it impractical for
massive natural annotations in web text. There
are also work making use of word co-occurrence
statistics collected in raw text or Internet n-grams
to improve parsing performance (Nakov and
Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011;
Bansal and Klein, 2011). When enriching the re-
lated work during writing, we found a work on de-
pendency parsing (Spitkovsky et al., 2010) who
utilized parsing constraints derived from hypertext
annotations to improve the unsupervised depen-
dency grammar induction. Compared with their
method, the strategy we proposed is formal and
universal, the discriminative learning strategy and
the quantitative measurement of fuzzy knowledge
enable more effective utilization of the natural an-
notation on the Internet when adapted to parsing.
</bodyText>
<sectionHeader confidence="0.987886" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99996752631579">
This work presents a novel discriminative learning
algorithm to utilize the knowledge in the massive
natural annotations on the Internet. Natural anno-
tations implied by structural information are used
to decrease the searching space of the classifier,
then the constraint decoding in the pruned search-
ing space gives predictions not worse than the nor-
mal decoding does. Annotation differences be-
tween the outputs of constraint decoding and nor-
mal decoding are used to train the enhanced classi-
fier, linguistic knowledge in the human-annotated
corpus and the natural annotations of web text
are thus integrated together. Experiments on Chi-
nese word segmentation show that, the enhanced
word segmenter achieves significant improvement
on testing sets of different domains, although us-
ing a single classifier with only local features.
Since the contents of web text cover a broad
range of domains, it provides knowledge comple-
</bodyText>
<page confidence="0.98623">
767
</page>
<bodyText confidence="0.999948875">
mentary to that of human-annotated corpora with
concentrated distribution of domains. The content
on the Internet is large-scaled and real-time up-
dated, it compensates for the drawback of expen-
sive building and updating of corpora. Our strat-
egy, therefore, enables us to build a classifier more
domain adaptive and up to date. In the future, we
will compare this method with self-training to bet-
ter illustrate the importance of boundary informa-
tion, and give error analysis on what types of er-
rors are reduced by the method to make this inves-
tigation more complete. We will also investigate
more efficient algorithms to leverage more mas-
sive web text with natural annotations, and further
extend the strategy to other NLP problems such as
named entity recognition and parsing.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999446">
The authors were supported by National
Natural Science Foundation of China (Con-
tracts 61202216), 863 State Key Project (No.
2011AA01A207), and National Key Technology
R&amp;D Program (No. 2012BAH39B03). Qun Liu’s
work was partially supported by Science Foun-
dation Ireland (Grant No.07/CE/I1142) as part
of the CNGL at Dublin City University. Sincere
thanks to the three anonymous reviewers for their
thorough reviewing and valuable suggestions!
</bodyText>
<sectionHeader confidence="0.986101" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.593087">
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings ofACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1–8, Philadelphia, USA.
</bodyText>
<reference confidence="0.995168">
Pascale Fung, Grace Ngai, Yongsheng Yang, and Ben-
feng Chen. 2004. A maximum-entropy chinese
parser augmented by transformation-based learning.
In Proceedings of TALIP.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In Proceed-
ings ofACL.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning
Huang. 2005. Chinese word segmentation and
named entity recognition: A pragmatic approach.
Computational Linguistics.
Wenjun Gao, Xipeng Qiu, and Xuanjing Huang. 2010.
Adaptive chinese word segmentation with online
passive-aggressive algorithm. In Proceedings of
CIPS-SIGHAN Workshop.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proceedings ofACL-HLT.
Gholamreza Haffari and Anoop Sarkar. 2008.
Homotopy-based semi-supervised hidden markov
models for sequence labeling. In Proceedings of
COLING.
Daniel Hewlett and Paul Cohen. 2011. Fully unsu-
pervised word segmentation with bve and mdl. In
Proceedings ofACL.
Wenbin Jiang, Liang Huang, Yajuan Lv, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings ofACL.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging–a case study. In
Proceedings of the 47th ACL.
Mark Johnson and Sharon Goldwater. 2009. Improv-
ing nonparameteric bayesian inference: experiments
on unsupervised word segmentation with adaptor
grammars. In Proceedings ofNAACL.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun&apos;ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings ofACL-IJCNLP.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for chinese word segmentation.
Computational Linguistics.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chinese word segmenta-
tion. In Proceedings ofACL.
Yang Liu and Yue Zhang. 2012. Unsupervised domain
adaptation for joint segmentation and pos-tagging.
In Proceedings of COLING.
Yanjun Ma and Andy Way. 2009. Bilingually moti-
vated domain-adapted word segmentation for statis-
tical machine translation. In Proceedings of EACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the HLT-NAACL.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested pitman-yor language modeling.
In Proceedings ofACL-IJCNLP.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and pos tag-
ging. In Proceedings ofACL.
Preslav Nakov and Marti Hearst. 2005. Using the
web as an implicit training set: Application to struc-
tural ambiguity resolution. In Proceedings of HLT-
EMNLP.
</reference>
<page confidence="0.970062">
768
</page>
<reference confidence="0.999558546511628">
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMILP.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings ofACL.
Emily Pitler, Shane Bergsma, Dekang Lin, and Ken-
neth Church. 2010. Using web-scale n-grams to
improve base np parsing performance. In Proceed-
ings of COLIIG.
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010. Profiting from mark-up: Hyper-text
annotations for guided parsing. In Proceedings of
ACL.
Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings ofEMILP.
Maosong Sun. 2011a. Natural language processing
based on naturally annotated web resources. CHI-
IESE IIFORMATIOI PROCESSIIG.
Weiwei Sun. 2011b. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings ofACL.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings ofACL.
Katrin Tomanek and Udo Hahn. 2009. Semi-
supervised active learning for sequence labeling. In
Proceedings ofACL.
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010.
A character-based joint model for chinese word seg-
mentation. In Proceedings of COLIIG.
Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proceedings ofIJC-
ILP.
Andi Wu. 2003. Customizable segmentation of mor-
phologically derived words in chinese. Computa-
tional Linguistics and Chinese Language Process-
ing.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of COLIIG.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
SIGHAI Workshop.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Iatural
Language Engineering.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming
Duan, Shiyong Kang, Honglin Sun, Hui Wang,
Qiang Zhao, and Weidong Zhan. 2001. Processing
norms of modern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In Proceedings ofACL 2007.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and pos-tagging using
a single discriminative model. In Proceedings of
EMILP.
Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer
ictclas. In Proceedings of SIGHAI Workshop.
Hai Zhao and Chunyu Kit. 2008. Unsupervised
segmentation helps supervised learning of charac-
ter tagging for word segmentation and named entity
recognition. In Proceedings of SIGHAI Workshop.
Hongmei Zhao and Qun Liu. 2010. The cips-sighan
clp 2010 chinese word segmentation bakeoff. In
Proceedings of CIPS-SIGHAI Workshop.
Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
EACL.
Guodong Zhou and Jian Su. 2003. A chinese effi-
cient analyser integrating word segmentation, part-
ofspeech tagging, partial parsing and full parsing. In
Proceedings of SIGHAI Workshop.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011. Exploiting web-derived selectional prefer-
ence to improve statistical dependency parsing. In
Proceedings ofACL.
</reference>
<page confidence="0.998307">
769
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.048989">
<title confidence="0.99902">Discriminative Learning with Natural</title>
<author confidence="0.758764">Word Segmentation as a Case Study</author>
<affiliation confidence="0.726918111111111">Jiang 1Meng Sun 1Yajuan L¨u 1Yating Yang 2Qun Liu Laboratory of Intelligent Information Institute of Computing Technology, Chinese Academy of Sciences sunmeng, Information Technology Research Center The Xinjiang Technical Institute of Physics &amp; Chemistry, Chinese Academy of Sciences yangyt@ms.xjb.ac.cn for Next Generation Localisation Faculty of Engineering and Computing, Dublin City University</affiliation>
<email confidence="0.934164">qliu@computing.dcu.ie</email>
<abstract confidence="0.999583684210526">Structural information in web text provides natural annotations for NLP problems such as word segmentation and parsing. In this paper we propose a discriminative learning algorithm to take advantage of the linguistic knowledge in large amounts of natural annotations on the Internet. It utilizes the Internet as an external corpus with massive (although slight and sparse) natural annotations, and enables a classifier to evolve on the large-scaled and real-time updated web text. With Chinese word segmentation as a case study, experiments show that the segmenter enhanced with the Chinese wikipedia achieves significant improvement on a series of testing sets from different domains, even with a single classifier and local features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Grace Ngai</author>
<author>Yongsheng Yang</author>
<author>Benfeng Chen</author>
</authors>
<title>A maximum-entropy chinese parser augmented by transformation-based learning.</title>
<date>2004</date>
<booktitle>In Proceedings of TALIP.</booktitle>
<contexts>
<context position="25283" citStr="Fung et al., 2004" startWordPosition="4232" endWordPosition="4235">e structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence</context>
</contexts>
<marker>Fung, Ngai, Yang, Chen, 2004</marker>
<rawString>Pascale Fung, Grace Ngai, Yongsheng Yang, and Benfeng Chen. 2004. A maximum-entropy chinese parser augmented by transformation-based learning. In Proceedings of TALIP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Andi Wu</author>
<author>Mu Li</author>
</authors>
<title>Chang-Ning Huang, Hongqiao Li, Xinsong Xia, and Haowei Qin.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="18904" citStr="Gao et al., 2004" startWordPosition="3210" endWordPosition="3213">f precision P and recall R, where P is the percentage of words in segmentation results that are segmented correctly, and R is the percentage of correctly segmented words in the gold standard words. Figure 3 shows the learning curve of the averaged perceptron on the developing set. The second column of Table 3 lists the performance of the baseline classifier on eight testing sets, where newswire denotes the testing set of the CTB itself. The classifier performs much worse on the domains of chemistry, physics and machinery, it indicates the importance of domain adaptation for word segmentation (Gao et al., 2004; Ma and Way, 2009; Gao et al., 2010). The accuracy on the testing sets from SIGHAN Bakeoff 2010 is even lower due to the difference in both domains and word segmentation standards. 2They are available at http://nlp.ict.ac.cn/ jiangwenbin/. 765 Dataset Baseline (F%) Enhanced (F%) Newswire 97.35 98.28 +0.93 Out-of-Domain Chemistry 93.61 95.68 +2.07 Physics 95.10 97.24 +2.14 Machinery 96.08 97.66 +1.58 Literature 92.42 93.53 +1.11 Finance 92.50 93.16 +0.66 Computer 89.46 91.19 +1.73 Medicine 91.88 93.34 +1.46 Average 93.01 94.54 +1.53 10000 20000 40000 80000 160000 320000 640000 97.8 97.7 97.6 9</context>
<context position="25102" citStr="Gao et al., 2004" startWordPosition="4202" endWordPosition="4205">t is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is spec</context>
</contexts>
<marker>Gao, Wu, Li, 2004</marker>
<rawString>Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang, Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004. Adaptive chinese word segmentation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Andi Wu</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Chinese word segmentation and named entity recognition: A pragmatic approach. Computational Linguistics.</title>
<date>2005</date>
<contexts>
<context position="1649" citStr="Gao et al., 2005" startWordPosition="240" endWordPosition="243">the large-scaled and real-time updated web text. With Chinese word segmentation as a case study, experiments show that the segmenter enhanced with the Chinese wikipedia achieves significant improvement on a series of testing sets from different domains, even with a single classifier and local features. 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in think that NLP has already ... n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Knowledge for word segmentation n</context>
</contexts>
<marker>Gao, Li, Wu, Huang, 2005</marker>
<rawString>Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang. 2005. Chinese word segmentation and named entity recognition: A pragmatic approach. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenjun Gao</author>
<author>Xipeng Qiu</author>
<author>Xuanjing Huang</author>
</authors>
<title>Adaptive chinese word segmentation with online passive-aggressive algorithm.</title>
<date>2010</date>
<booktitle>In Proceedings of CIPS-SIGHAN Workshop.</booktitle>
<contexts>
<context position="18941" citStr="Gao et al., 2010" startWordPosition="3218" endWordPosition="3221">is the percentage of words in segmentation results that are segmented correctly, and R is the percentage of correctly segmented words in the gold standard words. Figure 3 shows the learning curve of the averaged perceptron on the developing set. The second column of Table 3 lists the performance of the baseline classifier on eight testing sets, where newswire denotes the testing set of the CTB itself. The classifier performs much worse on the domains of chemistry, physics and machinery, it indicates the importance of domain adaptation for word segmentation (Gao et al., 2004; Ma and Way, 2009; Gao et al., 2010). The accuracy on the testing sets from SIGHAN Bakeoff 2010 is even lower due to the difference in both domains and word segmentation standards. 2They are available at http://nlp.ict.ac.cn/ jiangwenbin/. 765 Dataset Baseline (F%) Enhanced (F%) Newswire 97.35 98.28 +0.93 Out-of-Domain Chemistry 93.61 95.68 +2.07 Physics 95.10 97.24 +2.14 Machinery 96.08 97.66 +1.58 Literature 92.42 93.53 +1.11 Finance 92.50 93.16 +0.66 Computer 89.46 91.19 +1.73 Medicine 91.88 93.34 +1.46 Average 93.01 94.54 +1.53 10000 20000 40000 80000 160000 320000 640000 97.8 97.7 97.6 97.5 97.4 97.3 97.2 97.1 using selecte</context>
</contexts>
<marker>Gao, Qiu, Huang, 2010</marker>
<rawString>Wenjun Gao, Xipeng Qiu, and Xuanjing Huang. 2010. Adaptive chinese word segmentation with online passive-aggressive algorithm. In Proceedings of CIPS-SIGHAN Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
</authors>
<title>A single generative model for joint morphological segmentation and syntactic parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-HLT.</booktitle>
<contexts>
<context position="25313" citStr="Goldberg and Tsarfaty, 2008" startWordPosition="4236" endWordPosition="4240">ation in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence statistics collected in raw t</context>
</contexts>
<marker>Goldberg, Tsarfaty, 2008</marker>
<rawString>Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and syntactic parsing. In Proceedings ofACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Homotopy-based semi-supervised hidden markov models for sequence labeling.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="23823" citStr="Haffari and Sarkar, 2008" startWordPosition="3993" endWordPosition="3996">lassification instances from raw text for Chinese word segmentation, resorting to the indication of punctuation marks between characters. Sun and Xu (Sun and Xu, 2011) utilized the features derived from large-scaled unlabeled text to improve Chinese word segmentation. Although the two work also made use of large-scaled raw text, our method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also needs large amounts of external data, but it aims t</context>
</contexts>
<marker>Haffari, Sarkar, 2008</marker>
<rawString>Gholamreza Haffari and Anoop Sarkar. 2008. Homotopy-based semi-supervised hidden markov models for sequence labeling. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hewlett</author>
<author>Paul Cohen</author>
</authors>
<title>Fully unsupervised word segmentation with bve and mdl.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="24210" citStr="Hewlett and Cohen, 2011" startWordPosition="4057" endWordPosition="4060">spects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also needs large amounts of external data, but it aims to leverage the knowledge in the fuzzy and sparse annotations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. </context>
</contexts>
<marker>Hewlett, Cohen, 2011</marker>
<rawString>Daniel Hewlett and Paul Cohen. 2011. Fully unsupervised word segmentation with bve and mdl. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Yajuan Lv</author>
<author>Qun Liu</author>
</authors>
<title>A cascaded linear model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="21488" citStr="Jiang et al., 2008" startWordPosition="3616" endWordPosition="3619">that the highest accuracy was achieved when 160, 000 sentences were used, while more additional training data did not give continuous improvement. A recent related work about selftraining for segmentation (Liu and Zhang, 2012) also reported a very similar trend, that only a moderate amount of raw data gave the most obvious improvements. The performance of the enhanced classifier is listed in the third column of Table 3. On the CTB testing set, training data from the Chinese Figure 4: Performance curve of the classifier enhanced with selected sentences of different scales. Model Accuracy (F%) (Jiang et al., 2008) 97.85 (Kruengkrai et al., 2009) 97.87 (Zhang and Clark, 2010) 97.79 (Wang et al., 2011) 98.11 (Sun, 2011b) 98.17 Our Work 98.28 Table 4: Comparison with state-of-the-art work in Chinese word segmentation. wikipedia brings an F-measure increment of 0.93 points. On out-of-domain testing sets, the improvements are much larger, an average increment of 1.53 points is achieved on seven domains. It is probably because the distribution of the knowledge in the CTB training data is concentrated in the domain of newswire, while the contents of the Chinese wikipedia cover a broad range of domains, it pro</context>
</contexts>
<marker>Jiang, Huang, Lv, Liu, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Yajuan Lv, and Qun Liu. 2008. A cascaded linear model for joint chinese word segmentation and part-of-speech tagging. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging–a case study.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th ACL.</booktitle>
<contexts>
<context position="1718" citStr="Jiang et al., 2009" startWordPosition="252" endWordPosition="255">segmentation as a case study, experiments show that the segmenter enhanced with the Chinese wikipedia achieves significant improvement on a series of testing sets from different domains, even with a single classifier and local features. 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in think that NLP has already ... n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (c) Knowledge for dependency parsi</context>
<context position="25123" citStr="Jiang et al., 2009" startWordPosition="4206" endWordPosition="4209"> different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency </context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging–a case study. In Proceedings of the 47th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="24159" citStr="Johnson and Goldwater, 2009" startWordPosition="4049" endWordPosition="4052">r method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also needs large amounts of external data, but it aims to leverage the knowledge in the fuzzy and sparse annotations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the impr</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun&apos;ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP.</booktitle>
<contexts>
<context position="21520" citStr="Kruengkrai et al., 2009" startWordPosition="3621" endWordPosition="3624">as achieved when 160, 000 sentences were used, while more additional training data did not give continuous improvement. A recent related work about selftraining for segmentation (Liu and Zhang, 2012) also reported a very similar trend, that only a moderate amount of raw data gave the most obvious improvements. The performance of the enhanced classifier is listed in the third column of Table 3. On the CTB testing set, training data from the Chinese Figure 4: Performance curve of the classifier enhanced with selected sentences of different scales. Model Accuracy (F%) (Jiang et al., 2008) 97.85 (Kruengkrai et al., 2009) 97.87 (Zhang and Clark, 2010) 97.79 (Wang et al., 2011) 98.11 (Sun, 2011b) 98.17 Our Work 98.28 Table 4: Comparison with state-of-the-art work in Chinese word segmentation. wikipedia brings an F-measure increment of 0.93 points. On out-of-domain testing sets, the improvements are much larger, an average increment of 1.53 points is achieved on seven domains. It is probably because the distribution of the knowledge in the CTB training data is concentrated in the domain of newswire, while the contents of the Chinese wikipedia cover a broad range of domains, it provides knowledge complementary to</context>
<context position="25417" citStr="Kruengkrai et al., 2009" startWordPosition="4255" endWordPosition="4258">ariety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence statistics collected in raw text or Internet n-grams to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Zho</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun&apos;ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging. In Proceedings ofACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguo Li</author>
<author>Maosong Sun</author>
</authors>
<title>Punctuation as implicit annotations for chinese word segmentation. Computational Linguistics.</title>
<date>2009</date>
<contexts>
<context position="23177" citStr="Li and Sun (2009)" startWordPosition="3891" endWordPosition="3894">interesting way to improve Chinese word segmentation, it focuses on the utilization of fuzzy and sparse knowledge on the Internet rather than making full use of a specific humanannotated corpus. On the other hand, since only a single classifier and local features are used in our method, better performance could be achieved 766 resorting to complicated features, system combination and other semi-supervised technologies. What is more, since the text on Internet is widecoveraged and real-time updated, our strategy also helps a word segmenter be more domain adaptive and up to date. 6 Related Work Li and Sun (2009) extracted character classification instances from raw text for Chinese word segmentation, resorting to the indication of punctuation marks between characters. Sun and Xu (Sun and Xu, 2011) utilized the features derived from large-scaled unlabeled text to improve Chinese word segmentation. Although the two work also made use of large-scaled raw text, our method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzu</context>
</contexts>
<marker>Li, Sun, 2009</marker>
<rawString>Zhongguo Li and Maosong Sun. 2009. Punctuation as implicit annotations for chinese word segmentation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguo Li</author>
</authors>
<title>Parsing the internal structure of words: A new paradigm for chinese word segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1764" citStr="Li, 2011" startWordPosition="262" endWordPosition="263"> segmenter enhanced with the Chinese wikipedia achieves significant improvement on a series of testing sets from different domains, even with a single classifier and local features. 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in think that NLP has already ... n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (c) Knowledge for dependency parsing Figure 1: Natural annotations for word segm</context>
<context position="25013" citStr="Li, 2011" startWordPosition="4189" endWordPosition="4190">data, but it aims to leverage the knowledge in the fuzzy and sparse annotations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treeba</context>
</contexts>
<marker>Li, 2011</marker>
<rawString>Zhongguo Li. 2011. Parsing the internal structure of words: A new paradigm for chinese word segmentation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yue Zhang</author>
</authors>
<title>Unsupervised domain adaptation for joint segmentation and pos-tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="21095" citStr="Liu and Zhang, 2012" startWordPosition="3549" endWordPosition="3552">n descending order according to the difference of the scores of constraint decoding and normal decoding, as described previously. From the beginning of the sorted list, different amounts of segmented sentences are used as the additional training data for the enhanced character classifier. Figure 4 shows the performance curve of the enhanced classifiers on the developing set of CTB. We found that the highest accuracy was achieved when 160, 000 sentences were used, while more additional training data did not give continuous improvement. A recent related work about selftraining for segmentation (Liu and Zhang, 2012) also reported a very similar trend, that only a moderate amount of raw data gave the most obvious improvements. The performance of the enhanced classifier is listed in the third column of Table 3. On the CTB testing set, training data from the Chinese Figure 4: Performance curve of the classifier enhanced with selected sentences of different scales. Model Accuracy (F%) (Jiang et al., 2008) 97.85 (Kruengkrai et al., 2009) 97.87 (Zhang and Clark, 2010) 97.79 (Wang et al., 2011) 98.11 (Sun, 2011b) 98.17 Our Work 98.28 Table 4: Comparison with state-of-the-art work in Chinese word segmentation. w</context>
</contexts>
<marker>Liu, Zhang, 2012</marker>
<rawString>Yang Liu and Yue Zhang. 2012. Unsupervised domain adaptation for joint segmentation and pos-tagging. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Andy Way</author>
</authors>
<title>Bilingually motivated domain-adapted word segmentation for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="18922" citStr="Ma and Way, 2009" startWordPosition="3214" endWordPosition="3217">recall R, where P is the percentage of words in segmentation results that are segmented correctly, and R is the percentage of correctly segmented words in the gold standard words. Figure 3 shows the learning curve of the averaged perceptron on the developing set. The second column of Table 3 lists the performance of the baseline classifier on eight testing sets, where newswire denotes the testing set of the CTB itself. The classifier performs much worse on the domains of chemistry, physics and machinery, it indicates the importance of domain adaptation for word segmentation (Gao et al., 2004; Ma and Way, 2009; Gao et al., 2010). The accuracy on the testing sets from SIGHAN Bakeoff 2010 is even lower due to the difference in both domains and word segmentation standards. 2They are available at http://nlp.ict.ac.cn/ jiangwenbin/. 765 Dataset Baseline (F%) Enhanced (F%) Newswire 97.35 98.28 +0.93 Out-of-Domain Chemistry 93.61 95.68 +2.07 Physics 95.10 97.24 +2.14 Machinery 96.08 97.66 +1.58 Literature 92.42 93.53 +1.11 Finance 92.50 93.16 +0.66 Computer 89.46 91.19 +1.73 Medicine 91.88 93.34 +1.46 Average 93.01 94.54 +1.53 10000 20000 40000 80000 160000 320000 640000 97.8 97.7 97.6 97.5 97.4 97.3 97.2</context>
</contexts>
<marker>Ma, Way, 2009</marker>
<rawString>Yanjun Ma and Andy Way. 2009. Bilingually motivated domain-adapted word segmentation for statistical machine translation. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL.</booktitle>
<contexts>
<context position="14793" citStr="McClosky et al., 2006" startWordPosition="2531" endWordPosition="2534">he degree of difference between the two predications represents the amount of new knowledge introduced by the natural annotations over the baseline. The baseline model α� is trained on an existing human-annotated corpus. A set of sentences T with natural annotations are extracted from the Chinese wikipedia, and we reserve the ones for which constraint decoding and normal decoding give different predications. The predictions of reserved sentences by constraint decoding are used as additional training data for the enhanced classifier. The overall training pipeline is analogous to self-training (McClosky et al., 2006), Algorithm 2 shows the pseudo-codes. Considering the online characteristic of the perceptron algorithm, if we are able to leverage much more (than the Chinese wikipedia) data with natural annotations, an online version of learning procedure shown in Algorithm 3 would be a better choice. The technology of “averaged parameters” (Collins, 2002) is easily to be adapted here for better performance. When constraint decoding and normal decoding give different predications, we only know that the former is probably better than the latter. Although there is no explicit evidence for us to measure how mu</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested pitman-yor language modeling.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP.</booktitle>
<contexts>
<context position="24184" citStr="Mochihashi et al., 2009" startWordPosition="4053" endWordPosition="4056">rent from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also needs large amounts of external data, but it aims to leverage the knowledge in the fuzzy and sparse annotations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentat</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested pitman-yor language modeling. In Proceedings ofACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kiyotaka Uchimoto</author>
</authors>
<title>A hybrid approach to word segmentation and pos tagging.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1678" citStr="Nakagawa and Uchimoto, 2007" startWordPosition="244" endWordPosition="247">nd real-time updated web text. With Chinese word segmentation as a case study, experiments show that the segmenter enhanced with the Chinese wikipedia achieves significant improvement on a series of testing sets from different domains, even with a single classifier and local features. 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in think that NLP has already ... n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i </context>
<context position="25392" citStr="Nakagawa and Uchimoto, 2007" startWordPosition="4250" endWordPosition="4254">t of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence statistics collected in raw text or Internet n-grams to improve parsing performance (Nakov and Hearst, 2005;</context>
</contexts>
<marker>Nakagawa, Uchimoto, 2007</marker>
<rawString>Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A hybrid approach to word segmentation and pos tagging. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Using the web as an implicit training set: Application to structural ambiguity resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP.</booktitle>
<contexts>
<context position="2666" citStr="Nakov and Hearst, 2005" startWordPosition="433" endWordPosition="436">tated corpora are limited in think that NLP has already ... n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (c) Knowledge for dependency parsing Figure 1: Natural annotations for word segmentation and dependency parsing. size (e.g. tens of thousands), the performance of word segmentation tends to degrade sharply when applied to new domains. Internet provides large amounts of raw text, and statistics collected from it have been used to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Bansal and Klein, 2011; Zhou et al., 2011). The Internet also gives massive (although slight and sparse) natural annotations in the forms of structural information including hyperlinks, fonts, colors and layouts (Sun, 2011a). These annotations usually imply valuable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 761 Proceedings of the 51st Annual Meeting of the Association for Computational</context>
<context position="25991" citStr="Nakov and Hearst, 2005" startWordPosition="4342" endWordPosition="4345">gawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence statistics collected in raw text or Internet n-grams to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011; Bansal and Klein, 2011). When enriching the related work during writing, we found a work on dependency parsing (Spitkovsky et al., 2010) who utilized parsing constraints derived from hypertext annotations to improve the unsupervised dependency grammar induction. Compared with their method, the strategy we proposed is formal and universal, the discriminative learning strategy and the quantitative measurement of fuzzy knowledge enable more effective utilization of the natural annotation on the Internet when adapted to parsing. 7 Conclusion and Future Wor</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005. Using the web as an implicit training set: Application to structural ambiguity resolution. In Proceedings of HLTEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based?</title>
<date>2004</date>
<booktitle>In Proceedings of EMILP.</booktitle>
<contexts>
<context position="1631" citStr="Ng and Low, 2004" startWordPosition="236" endWordPosition="239">fier to evolve on the large-scaled and real-time updated web text. With Chinese word segmentation as a case study, experiments show that the segmenter enhanced with the Chinese wikipedia achieves significant improvement on a series of testing sets from different domains, even with a single classifier and local features. 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in think that NLP has already ... n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Knowledge for w</context>
<context position="4192" citStr="Ng and Low, 2004" startWordPosition="664" endWordPosition="667"> treated as a wide-coveraged and real-time updated corpus. Different from the dense and accurate annotations in human-annotated corpora, natural annotations in web text are sparse and slight, it makes direct training of NLP models impracticable. In this work we take for example a most important problem, word segmentation, and propose a novel discriminative learning algorithm to leverage the knowledge in massive natural annotations of web text. Character classification models for word segmentation usually factorize the whole prediction into atomic predictions on characters (Xue and Shen, 2003; Ng and Low, 2004). Natural annotations in web text can be used to get rid of implausible predication candidates for related characters, knowledge in the natural annotations is therefore introduced in the manner of searching space pruning. Since constraint decoding in the pruned searching space integrates the knowledge of the baseline model and natural annotations, it gives predictions not worse than the normal decoding does. Annotation differences between the outputs of constraint decoding and normal decoding are used to train the enhanced classifier. This strategy makes the usage of natural annotations simple</context>
<context position="6740" citStr="Ng and Low, 2004" startWordPosition="1064" endWordPosition="1067">racter “0” in “...@J§ 0 ,������...”. 2, then describe the representation of the knowledge in natural annotations of web text in section 3, and finally detail the strategy of discriminative learning on natural annotations in section 4. After giving the experimental results and analysis in section 5, we briefly introduce the previous related work and then give the conclusion and the expectation of future research. 2 Character Classification Model Character classification models for word segmentation factorize the whole prediction into atomic predictions on single characters (Xue and Shen, 2003; Ng and Low, 2004). Although natural annotations in web text do not directly support the discriminative training of segmentation models, they do get rid of the implausible candidates for predictions of related characters. Given a sentence as a sequence of n characters, word segmentation splits the sequence into m(≤ n) subsequences, each of which indicates a meaningful word. Word segmentation can be formalized as a character classification problem (Xue and Shen, 2003), where each character in the sentence is given a boundary tag representing its position in a word. We adopt the boundary tags of Ng and Low (2004)</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based? In Proceedings of EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Insideoutside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="25489" citStr="Pereira and Schabes (1992)" startWordPosition="4267" endWordPosition="4270"> training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence statistics collected in raw text or Internet n-grams to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011; Bansal and Klein, 2011). When enriching the related work</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. Insideoutside reestimation from partially bracketed corpora. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Kenneth Church</author>
</authors>
<title>Using web-scale n-grams to improve base np parsing performance.</title>
<date>2010</date>
<booktitle>In Proceedings of COLIIG.</booktitle>
<contexts>
<context position="2687" citStr="Pitler et al., 2010" startWordPosition="437" endWordPosition="440">d in think that NLP has already ... n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (c) Knowledge for dependency parsing Figure 1: Natural annotations for word segmentation and dependency parsing. size (e.g. tens of thousands), the performance of word segmentation tends to degrade sharply when applied to new domains. Internet provides large amounts of raw text, and statistics collected from it have been used to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Bansal and Klein, 2011; Zhou et al., 2011). The Internet also gives massive (although slight and sparse) natural annotations in the forms of structural information including hyperlinks, fonts, colors and layouts (Sun, 2011a). These annotations usually imply valuable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 761 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7</context>
<context position="26012" citStr="Pitler et al., 2010" startWordPosition="4346" endWordPosition="4349"> Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence statistics collected in raw text or Internet n-grams to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011; Bansal and Klein, 2011). When enriching the related work during writing, we found a work on dependency parsing (Spitkovsky et al., 2010) who utilized parsing constraints derived from hypertext annotations to improve the unsupervised dependency grammar induction. Compared with their method, the strategy we proposed is formal and universal, the discriminative learning strategy and the quantitative measurement of fuzzy knowledge enable more effective utilization of the natural annotation on the Internet when adapted to parsing. 7 Conclusion and Future Work This work presents </context>
</contexts>
<marker>Pitler, Bergsma, Lin, Church, 2010</marker>
<rawString>Emily Pitler, Shane Bergsma, Dekang Lin, and Kenneth Church. 2010. Using web-scale n-grams to improve base np parsing performance. In Proceedings of COLIIG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Daniel Jurafsky</author>
<author>Hiyan Alshawi</author>
</authors>
<title>Profiting from mark-up: Hyper-text annotations for guided parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="26169" citStr="Spitkovsky et al., 2010" startWordPosition="4374" endWordPosition="4377">rs the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence statistics collected in raw text or Internet n-grams to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011; Bansal and Klein, 2011). When enriching the related work during writing, we found a work on dependency parsing (Spitkovsky et al., 2010) who utilized parsing constraints derived from hypertext annotations to improve the unsupervised dependency grammar induction. Compared with their method, the strategy we proposed is formal and universal, the discriminative learning strategy and the quantitative measurement of fuzzy knowledge enable more effective utilization of the natural annotation on the Internet when adapted to parsing. 7 Conclusion and Future Work This work presents a novel discriminative learning algorithm to utilize the knowledge in the massive natural annotations on the Internet. Natural annotations implied by structu</context>
</contexts>
<marker>Spitkovsky, Jurafsky, Alshawi, 2010</marker>
<rawString>Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Alshawi. 2010. Profiting from mark-up: Hyper-text annotations for guided parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Jia Xu</author>
</authors>
<title>Enhancing chinese word segmentation using unlabeled data.</title>
<date>2011</date>
<booktitle>In Proceedings ofEMILP.</booktitle>
<contexts>
<context position="23366" citStr="Sun and Xu, 2011" startWordPosition="3920" endWordPosition="3923">rpus. On the other hand, since only a single classifier and local features are used in our method, better performance could be achieved 766 resorting to complicated features, system combination and other semi-supervised technologies. What is more, since the text on Internet is widecoveraged and real-time updated, our strategy also helps a word segmenter be more domain adaptive and up to date. 6 Related Work Li and Sun (2009) extracted character classification instances from raw text for Chinese word segmentation, resorting to the indication of punctuation marks between characters. Sun and Xu (Sun and Xu, 2011) utilized the features derived from large-scaled unlabeled text to improve Chinese word segmentation. Although the two work also made use of large-scaled raw text, our method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, </context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Weiwei Sun and Jia Xu. 2011. Enhancing chinese word segmentation using unlabeled data. In Proceedings ofEMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maosong Sun</author>
</authors>
<title>Natural language processing based on naturally annotated web resources.</title>
<date>2011</date>
<journal>CHIIESE IIFORMATIOI PROCESSIIG.</journal>
<contexts>
<context position="1752" citStr="Sun, 2011" startWordPosition="260" endWordPosition="261">how that the segmenter enhanced with the Chinese wikipedia achieves significant improvement on a series of testing sets from different domains, even with a single classifier and local features. 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in think that NLP has already ... n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (c) Knowledge for dependency parsing Figure 1: Natural annotations f</context>
<context position="7940" citStr="Sun, 2011" startWordPosition="1275" endWordPosition="1276">ow (2004), b, m, e and s, where b, m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. the decoding procedure searches for the labeled character sequence y that maximizes the score func762 (a) Original searching space nఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Shrinked searching space Figure 2: Shrink of searching space for the character classification-based word segmentation model. 3 Knowledge in Natural Annotations Web text gives massive natural annotations in the form of structural informations, including hyperlinks, fonts, colors and layouts (Sun, 2011a). Although slight and sparse, these annotations imply valuable knowledge for problems such as word segmentation and parsing. As shown in Figure 1, the subsequence P = i..j of sentence S is composed of bolded characters determined by a hyperlink. Such natural annotations do not clearly give each character a boundary tag, or define the head-modifier relationship between two words. However, they do help to shrink the set of plausible predication candidates for each character or word. For word segmentation, it implies that characters i − 1 and j are the rightmost characters of words, while chara</context>
<context position="21593" citStr="Sun, 2011" startWordPosition="3636" endWordPosition="3637">ot give continuous improvement. A recent related work about selftraining for segmentation (Liu and Zhang, 2012) also reported a very similar trend, that only a moderate amount of raw data gave the most obvious improvements. The performance of the enhanced classifier is listed in the third column of Table 3. On the CTB testing set, training data from the Chinese Figure 4: Performance curve of the classifier enhanced with selected sentences of different scales. Model Accuracy (F%) (Jiang et al., 2008) 97.85 (Kruengkrai et al., 2009) 97.87 (Zhang and Clark, 2010) 97.79 (Wang et al., 2011) 98.11 (Sun, 2011b) 98.17 Our Work 98.28 Table 4: Comparison with state-of-the-art work in Chinese word segmentation. wikipedia brings an F-measure increment of 0.93 points. On out-of-domain testing sets, the improvements are much larger, an average increment of 1.53 points is achieved on seven domains. It is probably because the distribution of the knowledge in the CTB training data is concentrated in the domain of newswire, while the contents of the Chinese wikipedia cover a broad range of domains, it provides knowledge complementary to that of CTB. Table 4 shows the comparison with other work in Chinese wor</context>
<context position="25447" citStr="Sun, 2011" startWordPosition="4263" endWordPosition="4264">he introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence statistics collected in raw text or Internet n-grams to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011; Bansal and Kle</context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Maosong Sun. 2011a. Natural language processing based on naturally annotated web resources. CHIIESE IIFORMATIOI PROCESSIIG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1752" citStr="Sun, 2011" startWordPosition="260" endWordPosition="261">how that the segmenter enhanced with the Chinese wikipedia achieves significant improvement on a series of testing sets from different domains, even with a single classifier and local features. 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in think that NLP has already ... n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (c) Knowledge for dependency parsing Figure 1: Natural annotations f</context>
<context position="7940" citStr="Sun, 2011" startWordPosition="1275" endWordPosition="1276">ow (2004), b, m, e and s, where b, m and e mean the beginning, the middle and the end of a word, and s indicates a single-character word. the decoding procedure searches for the labeled character sequence y that maximizes the score func762 (a) Original searching space nఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Shrinked searching space Figure 2: Shrink of searching space for the character classification-based word segmentation model. 3 Knowledge in Natural Annotations Web text gives massive natural annotations in the form of structural informations, including hyperlinks, fonts, colors and layouts (Sun, 2011a). Although slight and sparse, these annotations imply valuable knowledge for problems such as word segmentation and parsing. As shown in Figure 1, the subsequence P = i..j of sentence S is composed of bolded characters determined by a hyperlink. Such natural annotations do not clearly give each character a boundary tag, or define the head-modifier relationship between two words. However, they do help to shrink the set of plausible predication candidates for each character or word. For word segmentation, it implies that characters i − 1 and j are the rightmost characters of words, while chara</context>
<context position="21593" citStr="Sun, 2011" startWordPosition="3636" endWordPosition="3637">ot give continuous improvement. A recent related work about selftraining for segmentation (Liu and Zhang, 2012) also reported a very similar trend, that only a moderate amount of raw data gave the most obvious improvements. The performance of the enhanced classifier is listed in the third column of Table 3. On the CTB testing set, training data from the Chinese Figure 4: Performance curve of the classifier enhanced with selected sentences of different scales. Model Accuracy (F%) (Jiang et al., 2008) 97.85 (Kruengkrai et al., 2009) 97.87 (Zhang and Clark, 2010) 97.79 (Wang et al., 2011) 98.11 (Sun, 2011b) 98.17 Our Work 98.28 Table 4: Comparison with state-of-the-art work in Chinese word segmentation. wikipedia brings an F-measure increment of 0.93 points. On out-of-domain testing sets, the improvements are much larger, an average increment of 1.53 points is achieved on seven domains. It is probably because the distribution of the knowledge in the CTB training data is concentrated in the domain of newswire, while the contents of the Chinese wikipedia cover a broad range of domains, it provides knowledge complementary to that of CTB. Table 4 shows the comparison with other work in Chinese wor</context>
<context position="25447" citStr="Sun, 2011" startWordPosition="4263" endWordPosition="4264">he introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence statistics collected in raw text or Internet n-grams to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011; Bansal and Kle</context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Weiwei Sun. 2011b. A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="23797" citStr="Suzuki and Isozaki, 2008" startWordPosition="3989" endWordPosition="3992">009) extracted character classification instances from raw text for Chinese word segmentation, resorting to the indication of punctuation marks between characters. Sun and Xu (Sun and Xu, 2011) utilized the features derived from large-scaled unlabeled text to improve Chinese word segmentation. Although the two work also made use of large-scaled raw text, our method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also needs large amounts of ex</context>
</contexts>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Udo Hahn</author>
</authors>
<title>Semisupervised active learning for sequence labeling.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="23847" citStr="Tomanek and Hahn, 2009" startWordPosition="3997" endWordPosition="4000">om raw text for Chinese word segmentation, resorting to the indication of punctuation marks between characters. Sun and Xu (Sun and Xu, 2011) utilized the features derived from large-scaled unlabeled text to improve Chinese word segmentation. Although the two work also made use of large-scaled raw text, our method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also needs large amounts of external data, but it aims to leverage the knowledge</context>
</contexts>
<marker>Tomanek, Hahn, 2009</marker>
<rawString>Katrin Tomanek and Udo Hahn. 2009. Semisupervised active learning for sequence labeling. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Wang</author>
<author>Chengqing Zong</author>
<author>Keh-Yih Su</author>
</authors>
<title>A character-based joint model for chinese word segmentation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLIIG.</booktitle>
<contexts>
<context position="25436" citStr="Wang et al., 2010" startWordPosition="4259" endWordPosition="4262">pproaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence statistics collected in raw text or Internet n-grams to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011; Ban</context>
</contexts>
<marker>Wang, Zong, Su, 2010</marker>
<rawString>Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010. A character-based joint model for chinese word segmentation. In Proceedings of COLIIG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiou Wang</author>
<author>Jun’ichi Kazama</author>
<author>Yoshimasa Tsuruoka</author>
<author>Wenliang Chen</author>
<author>Yujie Zhang</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto-analyzed data.</title>
<date>2011</date>
<booktitle>In Proceedings ofIJCILP.</booktitle>
<contexts>
<context position="21576" citStr="Wang et al., 2011" startWordPosition="3631" endWordPosition="3634">tional training data did not give continuous improvement. A recent related work about selftraining for segmentation (Liu and Zhang, 2012) also reported a very similar trend, that only a moderate amount of raw data gave the most obvious improvements. The performance of the enhanced classifier is listed in the third column of Table 3. On the CTB testing set, training data from the Chinese Figure 4: Performance curve of the classifier enhanced with selected sentences of different scales. Model Accuracy (F%) (Jiang et al., 2008) 97.85 (Kruengkrai et al., 2009) 97.87 (Zhang and Clark, 2010) 97.79 (Wang et al., 2011) 98.11 (Sun, 2011b) 98.17 Our Work 98.28 Table 4: Comparison with state-of-the-art work in Chinese word segmentation. wikipedia brings an F-measure increment of 0.93 points. On out-of-domain testing sets, the improvements are much larger, an average increment of 1.53 points is achieved on seven domains. It is probably because the distribution of the knowledge in the CTB training data is concentrated in the domain of newswire, while the contents of the Chinese wikipedia cover a broad range of domains, it provides knowledge complementary to that of CTB. Table 4 shows the comparison with other wo</context>
<context position="23867" citStr="Wang et al., 2011" startWordPosition="4001" endWordPosition="4004">word segmentation, resorting to the indication of punctuation marks between characters. Sun and Xu (Sun and Xu, 2011) utilized the features derived from large-scaled unlabeled text to improve Chinese word segmentation. Although the two work also made use of large-scaled raw text, our method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also needs large amounts of external data, but it aims to leverage the knowledge in the fuzzy and sp</context>
</contexts>
<marker>Wang, Kazama, Tsuruoka, Chen, Zhang, Torisawa, 2011</marker>
<rawString>Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang, and Kentaro Torisawa. 2011. Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto-analyzed data. In Proceedings ofIJCILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andi Wu</author>
</authors>
<title>Customizable segmentation of morphologically derived words in chinese.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing.</booktitle>
<contexts>
<context position="25084" citStr="Wu, 2003" startWordPosition="4200" endWordPosition="4201">tations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performan</context>
</contexts>
<marker>Wu, 2003</marker>
<rawString>Andi Wu. 2003. Customizable segmentation of morphologically derived words in chinese. Computational Linguistics and Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Jianfeng Gao</author>
<author>Kristina Toutanova</author>
<author>Hermann Ney</author>
</authors>
<title>Bayesian semi-supervised chinese word segmentation for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of COLIIG.</booktitle>
<contexts>
<context position="23771" citStr="Xu et al., 2008" startWordPosition="3985" endWordPosition="3988">ork Li and Sun (2009) extracted character classification instances from raw text for Chinese word segmentation, resorting to the indication of punctuation marks between characters. Sun and Xu (Sun and Xu, 2011) utilized the features derived from large-scaled unlabeled text to improve Chinese word segmentation. Although the two work also made use of large-scaled raw text, our method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also</context>
</contexts>
<marker>Xu, Gao, Toutanova, Ney, 2008</marker>
<rawString>Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann Ney. 2008. Bayesian semi-supervised chinese word segmentation for statistical machine translation. In Proceedings of COLIIG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Libin Shen</author>
</authors>
<title>Chinese word segmentation as lmr tagging.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGHAI Workshop.</booktitle>
<contexts>
<context position="1613" citStr="Xue and Shen, 2003" startWordPosition="232" endWordPosition="235">and enables a classifier to evolve on the large-scaled and real-time updated web text. With Chinese word segmentation as a case study, experiments show that the segmenter enhanced with the Chinese wikipedia achieves significant improvement on a series of testing sets from different domains, even with a single classifier and local features. 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in think that NLP has already ... n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (</context>
<context position="4173" citStr="Xue and Shen, 2003" startWordPosition="660" endWordPosition="663">hole Internet can be treated as a wide-coveraged and real-time updated corpus. Different from the dense and accurate annotations in human-annotated corpora, natural annotations in web text are sparse and slight, it makes direct training of NLP models impracticable. In this work we take for example a most important problem, word segmentation, and propose a novel discriminative learning algorithm to leverage the knowledge in massive natural annotations of web text. Character classification models for word segmentation usually factorize the whole prediction into atomic predictions on characters (Xue and Shen, 2003; Ng and Low, 2004). Natural annotations in web text can be used to get rid of implausible predication candidates for related characters, knowledge in the natural annotations is therefore introduced in the manner of searching space pruning. Since constraint decoding in the pruned searching space integrates the knowledge of the baseline model and natural annotations, it gives predictions not worse than the normal decoding does. Annotation differences between the outputs of constraint decoding and normal decoding are used to train the enhanced classifier. This strategy makes the usage of natural</context>
<context position="6721" citStr="Xue and Shen, 2003" startWordPosition="1060" endWordPosition="1063">idering the i-th character “0” in “...@J§ 0 ,������...”. 2, then describe the representation of the knowledge in natural annotations of web text in section 3, and finally detail the strategy of discriminative learning on natural annotations in section 4. After giving the experimental results and analysis in section 5, we briefly introduce the previous related work and then give the conclusion and the expectation of future research. 2 Character Classification Model Character classification models for word segmentation factorize the whole prediction into atomic predictions on single characters (Xue and Shen, 2003; Ng and Low, 2004). Although natural annotations in web text do not directly support the discriminative training of segmentation models, they do get rid of the implausible candidates for predictions of related characters. Given a sentence as a sequence of n characters, word segmentation splits the sequence into m(≤ n) subsequences, each of which indicates a meaningful word. Word segmentation can be formalized as a character classification problem (Xue and Shen, 2003), where each character in the sentence is given a boundary tag representing its position in a word. We adopt the boundary tags o</context>
</contexts>
<marker>Xue, Shen, 2003</marker>
<rawString>Nianwen Xue and Libin Shen. 2003. Chinese word segmentation as lmr tagging. In Proceedings of SIGHAI Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus. In Iatural Language Engineering.</title>
<date>2005</date>
<contexts>
<context position="1870" citStr="Xue et al., 2005" startWordPosition="277" endWordPosition="280">ting sets from different domains, even with a single classifier and local features. 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in think that NLP has already ... n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (c) Knowledge for dependency parsing Figure 1: Natural annotations for word segmentation and dependency parsing. size (e.g. tens of thousands), the performance of word segmentation tends</context>
<context position="16513" citStr="Xue et al., 2005" startWordPosition="2821" endWordPosition="2824"> = 5(y) − 5(˜y) indicates the degree to which the current model mistakes. This indicator helps us to select more valuable training examples. The strategy of learning with natural annotations can be adapted to other situations. For example, if we have a list of words or phrases (especially in a specific domain such as medicine and chemical), we can generate annotated sentences automatically by string matching in a large amount of raw text. It probably provides a simple and effective domain adaptation strategy for already trained models. 5 Experiments We use the Penn Chinese Treebank 5.0 (CTB) (Xue et al., 2005) as the existing annotated corpus for Chinese word segmentation. For convenient of comparison with other work in word segmentation, the whole corpus is split into three partitions as follows: chapters 271-300 for testing, chapters 301-325 for developing, and others for training. We choose the Chinese wikipedia 1 (version 20120812) as the external knowledge source, because it has high quality in contents and it is much better than usual web text. Structural informations, including hyperlinks, fonts and colors are used to derive the annotation information. To further evaluate the improvement bro</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. In Iatural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiwen Yu</author>
</authors>
<title>Jianming Lu, Xuefeng Zhu, Huiming Duan, Shiyong Kang, Honglin Sun, Hui Wang, Qiang Zhao, and Weidong Zhan.</title>
<date>2001</date>
<tech>Technical report.</tech>
<marker>Yu, 2001</marker>
<rawString>Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming Duan, Shiyong Kang, Honglin Sun, Hui Wang, Qiang Zhao, and Weidong Zhan. 2001. Processing norms of modern chinese corpus. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="24919" citStr="Zhang and Clark, 2007" startWordPosition="4173" endWordPosition="4176">utomatically induces word segmentation knowledge from it. Our method also needs large amounts of external data, but it aims to leverage the knowledge in the fuzzy and sparse annotations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-o</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In Proceedings ofACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and pos-tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings of EMILP.</booktitle>
<contexts>
<context position="1741" citStr="Zhang and Clark, 2010" startWordPosition="256" endWordPosition="259">se study, experiments show that the segmenter enhanced with the Chinese wikipedia achieves significant improvement on a series of testing sets from different domains, even with a single classifier and local features. 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in think that NLP has already ... n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (c) Knowledge for dependency parsing Figure 1: Natural an</context>
<context position="21550" citStr="Zhang and Clark, 2010" startWordPosition="3626" endWordPosition="3629">ces were used, while more additional training data did not give continuous improvement. A recent related work about selftraining for segmentation (Liu and Zhang, 2012) also reported a very similar trend, that only a moderate amount of raw data gave the most obvious improvements. The performance of the enhanced classifier is listed in the third column of Table 3. On the CTB testing set, training data from the Chinese Figure 4: Performance curve of the classifier enhanced with selected sentences of different scales. Model Accuracy (F%) (Jiang et al., 2008) 97.85 (Kruengkrai et al., 2009) 97.87 (Zhang and Clark, 2010) 97.79 (Wang et al., 2011) 98.11 (Sun, 2011b) 98.17 Our Work 98.28 Table 4: Comparison with state-of-the-art work in Chinese word segmentation. wikipedia brings an F-measure increment of 0.93 points. On out-of-domain testing sets, the improvements are much larger, an average increment of 1.53 points is achieved on seven domains. It is probably because the distribution of the knowledge in the CTB training data is concentrated in the domain of newswire, while the contents of the Chinese wikipedia cover a broad range of domains, it provides knowledge complementary to that of CTB. Table 4 shows th</context>
<context position="24943" citStr="Zhang and Clark, 2010" startWordPosition="4177" endWordPosition="4180">rd segmentation knowledge from it. Our method also needs large amounts of external data, but it aims to leverage the knowledge in the fuzzy and sparse annotations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that in</context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and pos-tagging using a single discriminative model. In Proceedings of EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huaping Zhang</author>
<author>Hongkui Yu</author>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
</authors>
<title>Hhmm-based chinese lexical analyzer ictclas.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGHAI Workshop.</booktitle>
<contexts>
<context position="25264" citStr="Zhang et al., 2003" startWordPosition="4228" endWordPosition="4231">ations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun Liu. 2003. Hhmm-based chinese lexical analyzer ictclas. In Proceedings of SIGHAI Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGHAI Workshop.</booktitle>
<contexts>
<context position="1698" citStr="Zhao and Kit, 2008" startWordPosition="248" endWordPosition="251">. With Chinese word segmentation as a case study, experiments show that the segmenter enhanced with the Chinese wikipedia achieves significant improvement on a series of testing sets from different domains, even with a single classifier and local features. 1 Introduction Problems related to information retrieval, machine translation and social computing need fast and accurate text processing, for example, word segmentation and parsing. Taking Chinese word segmentation for example, the state-of-the-art models (Xue and Shen, 2003; Ng and Low, 2004; Gao et al., 2005; Nakagawa and Uchimoto, 2007; Zhao and Kit, 2008; Jiang et al., 2009; Zhang and Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al., 2005), and perform quite well on corresponding test sets. Since the text used for corpus annotating are usually drawn from specific fields (e.g. newswire or finance), and the annotated corpora are limited in think that NLP has already ... n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (c) Knowledge </context>
<context position="24130" citStr="Zhao and Kit, 2008" startWordPosition="4045" endWordPosition="4048">-scaled raw text, our method is essentially different from theirs in the aspects of both the source of knowledge and the learning strategy. Lots of efforts have been devoted to semisupervised methods in sequence labeling and word segmentation (Xu et al., 2008; Suzuki and Isozaki, 2008; Haffari and Sarkar, 2008; Tomanek and Hahn, 2009; Wang et al., 2011). A semisupervised method tries to find an optimal hyperplane of both annotated data and raw data, thus to result in a model with better coverage and higher accuracy. Researchers have also investigated unsupervised methods in word segmentation (Zhao and Kit, 2008; Johnson and Goldwater, 2009; Mochihashi et al., 2009; Hewlett and Cohen, 2011). An unsupervised method mines the latent distribution regularity in the raw text, and automatically induces word segmentation knowledge from it. Our method also needs large amounts of external data, but it aims to leverage the knowledge in the fuzzy and sparse annotations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition. In Proceedings of SIGHAI Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongmei Zhao</author>
<author>Qun Liu</author>
</authors>
<title>The cips-sighan clp 2010 chinese word segmentation bakeoff.</title>
<date>2010</date>
<booktitle>In Proceedings of CIPS-SIGHAI Workshop.</booktitle>
<contexts>
<context position="17291" citStr="Zhao and Liu, 2010" startWordPosition="2946" endWordPosition="2949">nto three partitions as follows: chapters 271-300 for testing, chapters 301-325 for developing, and others for training. We choose the Chinese wikipedia 1 (version 20120812) as the external knowledge source, because it has high quality in contents and it is much better than usual web text. Structural informations, including hyperlinks, fonts and colors are used to derive the annotation information. To further evaluate the improvement brought by the fuzzy knowledge in Chinese wikipedia, a series of testing sets from different domains are adopted. The four testing sets from SIGHAN Bakeoff 2010 (Zhao and Liu, 2010) are used, they are drawn from the domains of literature, finance, computer science and medicine. Although the reference sets are annotated according to a different 1http://download.wikimedia.org/backup-index.html. 1 2 3 4 5 6 7 8 9 10 Training iterations Figure 3: Learning curve of the averaged perceptron classifier on the CTB developing set. word segmentation standard (Yu et al., 2001), the quantity of accuracy improvement is still illustrative since there are no vast diversities between the two segmentation standards. We also annotated another three testing sets 2, their texts are drawn fro</context>
</contexts>
<marker>Zhao, Liu, 2010</marker>
<rawString>Hongmei Zhao and Qun Liu. 2010. The cips-sighan clp 2010 chinese word segmentation bakeoff. In Proceedings of CIPS-SIGHAI Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
</authors>
<title>Character-level dependencies in chinese: Usefulness and learning.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="25002" citStr="Zhao, 2009" startWordPosition="4187" endWordPosition="4188">of external data, but it aims to leverage the knowledge in the fuzzy and sparse annotations. It is fundamentally different from semi-supervised and unsupervised methods in that we aimed to excavate a totally different kind of knowledge, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially pa</context>
</contexts>
<marker>Zhao, 2009</marker>
<rawString>Hai Zhao. 2009. Character-level dependencies in chinese: Usefulness and learning. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Jian Su</author>
</authors>
<title>A chinese efficient analyser integrating word segmentation, partofspeech tagging, partial parsing and full parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGHAI Workshop.</booktitle>
<contexts>
<context position="25244" citStr="Zhou and Su, 2003" startWordPosition="4224" endWordPosition="4227">, the natural annotations implied by the structural information in web text. In recent years, much work has been devoted to the improvement of word segmentation in a variety of ways. Typical approaches include the introduction of global training or complicated features (Zhang and Clark, 2007; Zhang and Clark, 2010), the investigation of word internal structures (Zhao, 2009; Li, 2011), the adjustment or adaptation of word segmentation standards (Wu, 2003; Gao et al., 2004; Jiang et al., 2009), the integrated solution of segmentation and related tasks such as part-of-speech tagging and parsing (Zhou and Su, 2003; Zhang et al., 2003; Fung et al., 2004; Goldberg and Tsarfaty, 2008), and the strategies of hybrid or stacked modeling (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are als</context>
</contexts>
<marker>Zhou, Su, 2003</marker>
<rawString>Guodong Zhou and Jian Su. 2003. A chinese efficient analyser integrating word segmentation, partofspeech tagging, partial parsing and full parsing. In Proceedings of SIGHAI Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
<author>Kang Liu</author>
<author>Li Cai</author>
</authors>
<title>Exploiting web-derived selectional preference to improve statistical dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2731" citStr="Zhou et al., 2011" startWordPosition="445" endWordPosition="448">  ཝ ҉ स ྸ  n i-1 i j j+1 (a) Natural annotation by hyperlink n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (b) Knowledge for word segmentation n ఊ น ᆑ ௶  ཝ ҉ स ྸ  n i-1 i j j+1 (c) Knowledge for dependency parsing Figure 1: Natural annotations for word segmentation and dependency parsing. size (e.g. tens of thousands), the performance of word segmentation tends to degrade sharply when applied to new domains. Internet provides large amounts of raw text, and statistics collected from it have been used to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Bansal and Klein, 2011; Zhou et al., 2011). The Internet also gives massive (although slight and sparse) natural annotations in the forms of structural information including hyperlinks, fonts, colors and layouts (Sun, 2011a). These annotations usually imply valuable knowledge for problems such as word segmentation and parsing, based on the hypothesis that the subsequences marked by structural information are meaningful fragments in sentences. Figure 1 shows an example. The hyperlink indicates 761 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761–769, Sofia, Bulgaria, August 4-9 2013. c�</context>
<context position="26031" citStr="Zhou et al., 2011" startWordPosition="4350" endWordPosition="4353">009; Wang et al., 2010; Sun, 2011b). In parsing, Pereira and Schabes (1992) proposed an extended inside-outside algorithm that infers the parameters of a stochastic CFG from a partially parsed treebank. It uses partial bracketing information to improve parsing performance, but it is specific to constituency parsing, and its computational complexity makes it impractical for massive natural annotations in web text. There are also work making use of word co-occurrence statistics collected in raw text or Internet n-grams to improve parsing performance (Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011; Bansal and Klein, 2011). When enriching the related work during writing, we found a work on dependency parsing (Spitkovsky et al., 2010) who utilized parsing constraints derived from hypertext annotations to improve the unsupervised dependency grammar induction. Compared with their method, the strategy we proposed is formal and universal, the discriminative learning strategy and the quantitative measurement of fuzzy knowledge enable more effective utilization of the natural annotation on the Internet when adapted to parsing. 7 Conclusion and Future Work This work presents a novel discriminat</context>
</contexts>
<marker>Zhou, Zhao, Liu, Cai, 2011</marker>
<rawString>Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011. Exploiting web-derived selectional preference to improve statistical dependency parsing. In Proceedings ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>