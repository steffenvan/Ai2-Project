<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001241">
<title confidence="0.9991625">
HadoopPerceptron: a Toolkit for Distributed Perceptron Training and
Prediction with MapReduce
</title>
<author confidence="0.997305">
Andrea Gesmundo
</author>
<affiliation confidence="0.902395333333333">
Computer Science Department
University of Geneva
Geneva, Switzerland
</affiliation>
<email confidence="0.988933">
andrea.gesmundo@unige.ch
</email>
<author confidence="0.516705">
Nadi Tomeh
</author>
<affiliation confidence="0.310748666666667">
LIMSI-CNRS and
Universit´e Paris-Sud
Orsay, France
</affiliation>
<email confidence="0.99234">
nadi.tomeh@limsi.fr
</email>
<sectionHeader confidence="0.991386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.773925166666667">
We propose a set of open-source software
modules to perform structured Perceptron
Training, Prediction and Evaluation within
the Hadoop framework. Apache Hadoop
is a freely available environment for run-
ning distributed applications on a com-
</bodyText>
<figureCaption confidence="0.780025058823529">
puter cluster. The software is designed
within the Map-Reduce paradigm. Thanks
to distributed computing, the proposed soft-
ware reduces substantially execution times
while handling huge data-sets. The dis-
tributed Perceptron training algorithm pre-
serves convergence properties, thus guar-
anties same accuracy performances as the
serial Perceptron. The presented modules
can be executed as stand-alone software or
easily extended or integrated in complex
systems. The execution of the modules ap-
plied to specific NLP tasks can be demon-
strated and tested via an interactive web in-
terface that allows the user to inspect the
status and structure of the cluster and inter-
act with the MapReduce jobs.
</figureCaption>
<sectionHeader confidence="0.998012" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999915456521739">
The Perceptron training algorithm (Rosenblatt,
1958; Freund and Schapire, 1999; Collins, 2002)
is widely applied in the Natural Language Pro-
cessing community for learning complex struc-
tured models. The non-probabilistic nature of the
perceptron parameters makes it possible to incor-
porate arbitrary features without the need to cal-
culate a partition function, which is required for
its discriminative probabilistic counterparts such
as CRFs (Lafferty et al., 2001). Additionally, the
Perceptron is robust to approximate inference in
large search spaces.
Nevertheless, Perceptron training is propor-
tional to inference which is frequently non-linear
in the input sequence size. Therefore, training can
be time-consuming for complex model structures.
Furthermore, for an increasing number of tasks is
fundamental to leverage on huge sources of data
as the World Wide Web. Such difficulties render
the scalability of the Perceptron a challenge.
In order to improve scalability, Mcdonald et
al. (2010) propose a distributed training strat-
egy called iterative parameter mixing, and show
that it has similar convergence properties to the
standard perceptron algorithm; it finds a separat-
ing hyperplane if the training set is separable; it
produces models with comparable accuracies to
those trained serially on all the data; and reduces
training times significantly by exploiting comput-
ing clusters.
With this paper we present the HadoopPer-
ceptron package. It provides a freely available
open-source implementation of the iterative pa-
rameter mixing algorithm for training the struc-
tured perceptron on a generic sequence labeling
tasks. Furthermore, the package provides two ad-
ditional modules for prediction and evaluation.
The three software modules are designed within
the MapReduce programming model (Dean and
Ghemawat, 2004) and implemented using the
Apache Hadoop distributed programming Frame-
work (White, 2009; Lin and Dyer, 2010). The
presented HadoopPerceptron package reduces ex-
ecution time significantly compared to its serial
counterpart while maintaining comparable perfor-
mance.
</bodyText>
<page confidence="0.994773">
97
</page>
<bodyText confidence="0.57193">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 97–101,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</bodyText>
<equation confidence="0.981080666666667">
tronIterParamMix T = x T
P ( {( t,Yt)})
Perce t—i
</equation>
<listItem confidence="0.9871016">
1. Split T into S pieces T = {T1, ... , TS}
2. w = 0
3. for n : 1..N
4. w(i,n) = OneEpochPerceptron(Ti, w)
w = �
5. i µi,nw(i,n)
6. return w
OneEpochPerceptron(Ti, w*)
1. w(°) = w*; k = 0
2. for n : 1..T
3. Let y′ = arg maxy′ w(k).f(xt,y′t)
4. if y′ =6 yt
5. x(k+1) = x(k) + f(xt, yt) − f(xt, y′t)
6. k = k + 1
7. return w(k)
</listItem>
<figureCaption confidence="0.859914">
Figure 1: Distributed perceptron with iterative param-
</figureCaption>
<bodyText confidence="0.223048333333333">
eter mixing strategy. Each w(i,n) is computed in par-
allel. t n = {µ1,n, ... , µS,n}, ∀µi,n ∈ t n : µi,n ≥
0 and ∀n : Ei µi,n = 1.
</bodyText>
<sectionHeader confidence="0.850957" genericHeader="method">
2 Distributed Structured Perceptron
</sectionHeader>
<bodyText confidence="0.9999729375">
The structured perceptron (Collins, 2002) is an
online learning algorithm that processes train-
ing instances one at a time during each training
epoch. In sequence labeling tasks, the algorithm
predicts a sequence of labels (an element from
the structured output space) for each input se-
quence. Prediction is determined by linear opera-
tions on high-dimensional feature representations
of candidate input-output pairs and an associated
weight vector. During training, the parameters are
updated whenever the prediction that employed
them is incorrect.
Unlike many batch learning algorithms that can
easily be distributed through the gradient calcula-
tion, the perceptron online training is more subtle
to parallelize. However, Mcdonald et al. (2010)
present a simple distributed training through a pa-
rameter mixing scheme.
The Iterative Parameter Mixing is given in Fig-
ure 2 (Mcdonald et al., 2010). First the training
data is divided into disjoint splits of example pairs
(xt, yt) where xt is the observation sequence and
yt is the associated labels. The algorithm pro-
ceeds to train a single epoch of the perceptron
algorithm for each split in parallel, and mix the
local models weights w(i,n) to produce the global
weight vector w. The mixed model is then passed
to each split to reset the perceptron local weights,
and a new iteration is started. Mcdonald et al.
(2010) provide bound analysis for the algorithm
and show that it is guaranteed to converge and find
a seperation hyperplane if one exists.
</bodyText>
<sectionHeader confidence="0.989605" genericHeader="method">
3 MapReduce and Hadoop
</sectionHeader>
<bodyText confidence="0.999969594594595">
Many algorithms need to iterate over number
of records and 1) perform some calculation on
each of them and then 2) aggregate the results.
The MapReduce programming model implements
a functional abstraction of these two operations
called respectively Map and Reduce. The Map
function takes a value-key pairs and produces a
list of key-value pairs: map(k, v) → (k′, v′)*;
while the input the Reduce function is a key with
all the associated values produced by all the map-
pers: reduce(k′, (v′)*) → (k″, v″)*. The model
requires that all values with the same key are re-
duced together.
Apache Hadoop is an open-source implementa-
tion of the MapReduce model on cluster of com-
puters. A cluster is composed by a set of comput-
ers (nodes) connected into a network. One node
is designated as the Master while other nodes
are referred to as Worker Nodes. Hadoop is de-
signed to scale out to large clusters built from
commodity hardware and achieves seamless scal-
ability. To allow rapid development, Hadoop
hides system-level details from the application
developer.The MapReduce runtime automatically
schedule worker assignment to mappers and re-
ducers;handles synchronization required by the
programming model including gathering, sort-
ing and shuffling of intermediate data across the
network; and provides robustness by detecting
worker failures and managing restarts. The frame-
work is built on top of he Hadoop Distributed
File System (HDFS), which allows to distribute
the data across the cluster nodes. Network traffic
is minimized by moving the process to the node
storing the data. In Hadoop terminology an entire
MapReduce program is called ajob while individ-
ual mappers and reducers are called tasks.
</bodyText>
<sectionHeader confidence="0.997157" genericHeader="method">
4 HadoopPerceptron Implementation
</sectionHeader>
<bodyText confidence="0.999848333333333">
In this section we give details on how the train-
ing, prediction and evaluation modules are im-
plemented for the Hadoop framework using the
</bodyText>
<page confidence="0.988662">
98
</page>
<figureCaption confidence="0.999693">
Figure 2: HadoopPerceptron in MapReduce.
</figureCaption>
<bodyText confidence="0.999236029411765">
MapReduce programming model1.
Our implementation of the iterative parame-
ter mixing algorithm is sketched in Figure 2.
At the beginning of each iteration, the train-
ing data is split and distributed to the worker
nodes. The set of training examples in a
data split is streamed to map workers as pairs
(sentence-id, (xt, yt)). Each map worker per-
forms a standard perceptron training epoch and
outputs a pair (feature-id, wi�f) for each feature.
The set of such pairs emitted by a map worker rep-
resents its local weight vector. After map workers
have finished, the MapReduce framework guaran-
tees that all local weights associated with a given
feature are aggregated together as input to a dis-
tinct reduce worker. Each reduce worker produces
as output the average of the associated feature
weight. At the end of each iteration, the reduce
workers outputs are aggregated into the global av-
eraged weight vector. The algorithm iterates N
times or until convergence is achieved. At the
beginning of each iteration the weight vector of
each distinct model is initialized with the global
averaged weight vector resultant from the previ-
ous iteration. Thus, for all the iterations except
for the first, the global averaged weight vector re-
sultant from the previous iteration needs to be pro-
vided the map workers. In Hadoop it is possible
to pass this information via the Distributed Cache
System.
In addition to the training module, the Hadoop-
Perceptron package provides separate modules
for prediction and evaluation both of them are
designed as MapReduce programs. The evalu-
</bodyText>
<footnote confidence="0.991946">
1The Hadoop Perceptron toolkit is available from
https://github.com/agesmundo/HadoopPerceptron .
</footnote>
<bodyText confidence="0.999910666666667">
ation module output the accuracy measure com-
puted against provided gold standards. Prediction
and evaluation modules are independent from the
training modules, the weight vector given as input
could have been computed with any other system
using any other training algorithm as long as they
employ the same features.
The implementation is in Java, and we inter-
face with the Hadoop cluster via the native Java
API. It can be easily adapted to a wide range of
NLP tasks. Incorporating new features by mod-
ifying the extensible feature extractor is straight-
forward. The package includes the implementa-
tion of the basic feature set described in (Suzuki
and Isozaki, 2008).
</bodyText>
<sectionHeader confidence="0.973313" genericHeader="method">
5 The Web User Interface
</sectionHeader>
<bodyText confidence="0.999980111111111">
Hadoop is bundled with several web interfaces
that provide concise tracking information for jobs,
tasks, data nodes, etc. as shown in Figure 3. These
web interfaces can be used to demonstrate the
HadoopPerceptron running phases and monitor
the distributed execution of the training, predic-
tion and evaluation modules for several sequence
labeling tasks including part-of-speech tagging
and named entity recognition.
</bodyText>
<sectionHeader confidence="0.999029" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999972818181818">
We investigate HadoopPerceptron training time
and prediction accuracy on a part-of-speech
(POS) task using the PennTreeBank corpus (Mar-
cus et al., 1994). We use sections 0-18 of the Wall
Street Journal for training, and sections 22-24 for
testing.
We compare the regular percepton trained se-
rially on all the training data with the distributed
perceptron trained with iterative parameter mix-
ing with variable number of splits S E {10, 20}.
For each system, we report the prediction accu-
racy measure on the final test set to determine
if any loss is observed as a consequence of dis-
tributed training.
For each system, Figure 4 plots accuracy re-
sults computed at the end of every training epoch
against consumed wall-clock time. We observe
that iterative mixing parameter achieves compa-
rable performance to its serial counterpart while
converging orders of magnitude faster.
Furthermore, we note that the distributed al-
gorithm achieves a slightly higher final accuracy
</bodyText>
<page confidence="0.996743">
99
</page>
<figureCaption confidence="0.998956666666667">
Figure 3: Hadoop interfaces for HadoopPerceptron.
Figure 4: Accuracy vs. training time. Each point cor-
responds to a training epoch.
</figureCaption>
<bodyText confidence="0.9999702">
than serial training. Mcdonald et al. (2010) sug-
gest that this is due to the bagging effect that
the distributed training has, and due to parameter
mixing that is similar to the averaged perceptron.
We note also that increasing the number of
splits increases the number of epoch required to
attain convergence, while reducing the time re-
quired per epoch. This implies a trade-off be-
tween slower convergence and quicker epochs
when selecting a larger number of splits.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999969368421053">
The HadoopPerceptron package provides the first
freely-available open-source implementation of
iterative parameter mixing Perceptron Training,
Prediction and Evaluation for a distributed Map-
Reduce framework. It is a versatile stand alone
software or building block, that can be easily
extended, modified, adapted, and integrated in
broader systems.
HadoopPerceptron is a useful tool for the in-
creasing number of applications that need to per-
form large-scale structured learning. This is the
first freely available implementation of an ap-
proach that has already been applied with success
in private sectors (e.g. Google Inc.). Making it
possible for everybody to fully leverage on huge
data sources as the World Wide Web, and develop
structured learning solutions that can scale keep-
ing feasible execution times and cluster-network
usage to a minimum.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9762925">
This work was funded by Google and The Scot-
tish Informatics and Computer Science Alliance
(SICSA). We thank Keith Hall, Chris Dyer and
Miles Osborne for help and advice.
</bodyText>
<page confidence="0.989161">
100
</page>
<sectionHeader confidence="0.990077" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998936045454546">
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP ’02:
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, Philadel-
phia, PA, USA.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapre-
duce: simplified data processing on large clusters.
In Proceedings of the 6th Symposium on Opeart-
ing Systems Design and Implementation, San Fran-
cisco, CA, USA.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algo-
rithm. Machine Learning, 37(3):277–296.
John Lafferty, Andrew Mccallum, and Fernando
Pereira. 2001. John lafferty and andrew mc-
callum and fernando pereira. In Proceedings of
the International Conference on Machine Learning,
Williamstown, MA, USA.
Jimmy Lin and Chris Dyer. 2010. Data-Intensive Text
Processing with MapReduce. Morgan &amp; Claypool
Publishers.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313–330.
Ryan Mcdonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In NAACL ’10: Proceedings of the 11th
Conference of the North American Chapter of the
Association for Computational Linguistics, Los An-
geles, CA, USA.
Frank Rosenblatt. 1958. The Perceptron: A proba-
bilistic model for information storage and organiza-
tion in the brain. Psychological Review, 65(6):386–
408.
Jun Suzuki and Hideki Isozaki. 2008. Semi-
supervised sequential labeling and segmentation us-
ing giga-word scale unlabeled data. In ACL ’08:
Proceedings of the 46th Conference of the Associa-
tion for Computational Linguistics, Columbus, OH,
USA.
Tom White. 2009. Hadoop: The Definitive Guide.
O’Reilly Media Inc.
</reference>
<page confidence="0.998607">
101
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.234117">
<title confidence="0.999251">HadoopPerceptron: a Toolkit for Distributed Perceptron Training Prediction with MapReduce</title>
<author confidence="0.999249">Andrea</author>
<affiliation confidence="0.917052333333333">Computer Science University of Geneva,</affiliation>
<email confidence="0.77417">andrea.gesmundo@unige.ch</email>
<affiliation confidence="0.900565">Nadi LIMSI-CNRS Universit´e</affiliation>
<address confidence="0.676767">Orsay,</address>
<email confidence="0.99713">nadi.tomeh@limsi.fr</email>
<abstract confidence="0.995985291666667">We propose a set of open-source software modules to perform structured Perceptron Training, Prediction and Evaluation within the Hadoop framework. Apache Hadoop is a freely available environment for running distributed applications on a computer cluster. The software is designed within the Map-Reduce paradigm. Thanks to distributed computing, the proposed software reduces substantially execution times while handling huge data-sets. The distributed Perceptron training algorithm preserves convergence properties, thus guaranties same accuracy performances as the serial Perceptron. The presented modules can be executed as stand-alone software or easily extended or integrated in complex systems. The execution of the modules applied to specific NLP tasks can be demonstrated and tested via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the MapReduce jobs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP ’02: Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="1337" citStr="Collins, 2002" startWordPosition="185" endWordPosition="186">a-sets. The distributed Perceptron training algorithm preserves convergence properties, thus guaranties same accuracy performances as the serial Perceptron. The presented modules can be executed as stand-alone software or easily extended or integrated in complex systems. The execution of the modules applied to specific NLP tasks can be demonstrated and tested via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the MapReduce jobs. 1 Introduction The Perceptron training algorithm (Rosenblatt, 1958; Freund and Schapire, 1999; Collins, 2002) is widely applied in the Natural Language Processing community for learning complex structured models. The non-probabilistic nature of the perceptron parameters makes it possible to incorporate arbitrary features without the need to calculate a partition function, which is required for its discriminative probabilistic counterparts such as CRFs (Lafferty et al., 2001). Additionally, the Perceptron is robust to approximate inference in large search spaces. Nevertheless, Perceptron training is proportional to inference which is frequently non-linear in the input sequence size. Therefore, trainin</context>
<context position="4175" citStr="Collins, 2002" startWordPosition="643" endWordPosition="644"> T P ( {( t,Yt)}) Perce t—i 1. Split T into S pieces T = {T1, ... , TS} 2. w = 0 3. for n : 1..N 4. w(i,n) = OneEpochPerceptron(Ti, w) w = � 5. i µi,nw(i,n) 6. return w OneEpochPerceptron(Ti, w*) 1. w(°) = w*; k = 0 2. for n : 1..T 3. Let y′ = arg maxy′ w(k).f(xt,y′t) 4. if y′ =6 yt 5. x(k+1) = x(k) + f(xt, yt) − f(xt, y′t) 6. k = k + 1 7. return w(k) Figure 1: Distributed perceptron with iterative parameter mixing strategy. Each w(i,n) is computed in parallel. t n = {µ1,n, ... , µS,n}, ∀µi,n ∈ t n : µi,n ≥ 0 and ∀n : Ei µi,n = 1. 2 Distributed Structured Perceptron The structured perceptron (Collins, 2002) is an online learning algorithm that processes training instances one at a time during each training epoch. In sequence labeling tasks, the algorithm predicts a sequence of labels (an element from the structured output space) for each input sequence. Prediction is determined by linear operations on high-dimensional feature representations of candidate input-output pairs and an associated weight vector. During training, the parameters are updated whenever the prediction that employed them is incorrect. Unlike many batch learning algorithms that can easily be distributed through the gradient ca</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In EMNLP ’02: Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>Mapreduce: simplified data processing on large clusters.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th Symposium on Opearting Systems Design and Implementation,</booktitle>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="3064" citStr="Dean and Ghemawat, 2004" startWordPosition="437" endWordPosition="440"> the training set is separable; it produces models with comparable accuracies to those trained serially on all the data; and reduces training times significantly by exploiting computing clusters. With this paper we present the HadoopPerceptron package. It provides a freely available open-source implementation of the iterative parameter mixing algorithm for training the structured perceptron on a generic sequence labeling tasks. Furthermore, the package provides two additional modules for prediction and evaluation. The three software modules are designed within the MapReduce programming model (Dean and Ghemawat, 2004) and implemented using the Apache Hadoop distributed programming Framework (White, 2009; Lin and Dyer, 2010). The presented HadoopPerceptron package reduces execution time significantly compared to its serial counterpart while maintaining comparable performance. 97 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 97–101, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics tronIterParamMix T = x T P ( {( t,Yt)}) Perce t—i 1. Split T into S pieces T = {T1, ... , TS} 2. w = 0 3. for n : 1..N 4. w(i</context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce: simplified data processing on large clusters. In Proceedings of the 6th Symposium on Opearting Systems Design and Implementation, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="1321" citStr="Freund and Schapire, 1999" startWordPosition="181" endWordPosition="184">mes while handling huge data-sets. The distributed Perceptron training algorithm preserves convergence properties, thus guaranties same accuracy performances as the serial Perceptron. The presented modules can be executed as stand-alone software or easily extended or integrated in complex systems. The execution of the modules applied to specific NLP tasks can be demonstrated and tested via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the MapReduce jobs. 1 Introduction The Perceptron training algorithm (Rosenblatt, 1958; Freund and Schapire, 1999; Collins, 2002) is widely applied in the Natural Language Processing community for learning complex structured models. The non-probabilistic nature of the perceptron parameters makes it possible to incorporate arbitrary features without the need to calculate a partition function, which is required for its discriminative probabilistic counterparts such as CRFs (Lafferty et al., 2001). Additionally, the Perceptron is robust to approximate inference in large search spaces. Nevertheless, Perceptron training is proportional to inference which is frequently non-linear in the input sequence size. Th</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew Mccallum</author>
<author>Fernando Pereira</author>
</authors>
<title>John lafferty and andrew mccallum and fernando pereira.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<location>Williamstown, MA, USA.</location>
<contexts>
<context position="1707" citStr="Lafferty et al., 2001" startWordPosition="238" endWordPosition="241">via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the MapReduce jobs. 1 Introduction The Perceptron training algorithm (Rosenblatt, 1958; Freund and Schapire, 1999; Collins, 2002) is widely applied in the Natural Language Processing community for learning complex structured models. The non-probabilistic nature of the perceptron parameters makes it possible to incorporate arbitrary features without the need to calculate a partition function, which is required for its discriminative probabilistic counterparts such as CRFs (Lafferty et al., 2001). Additionally, the Perceptron is robust to approximate inference in large search spaces. Nevertheless, Perceptron training is proportional to inference which is frequently non-linear in the input sequence size. Therefore, training can be time-consuming for complex model structures. Furthermore, for an increasing number of tasks is fundamental to leverage on huge sources of data as the World Wide Web. Such difficulties render the scalability of the Perceptron a challenge. In order to improve scalability, Mcdonald et al. (2010) propose a distributed training strategy called iterative parameter </context>
</contexts>
<marker>Lafferty, Mccallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew Mccallum, and Fernando Pereira. 2001. John lafferty and andrew mccallum and fernando pereira. In Proceedings of the International Conference on Machine Learning, Williamstown, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
<author>Chris Dyer</author>
</authors>
<date>2010</date>
<booktitle>Data-Intensive Text Processing with MapReduce.</booktitle>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="3172" citStr="Lin and Dyer, 2010" startWordPosition="453" endWordPosition="456">e data; and reduces training times significantly by exploiting computing clusters. With this paper we present the HadoopPerceptron package. It provides a freely available open-source implementation of the iterative parameter mixing algorithm for training the structured perceptron on a generic sequence labeling tasks. Furthermore, the package provides two additional modules for prediction and evaluation. The three software modules are designed within the MapReduce programming model (Dean and Ghemawat, 2004) and implemented using the Apache Hadoop distributed programming Framework (White, 2009; Lin and Dyer, 2010). The presented HadoopPerceptron package reduces execution time significantly compared to its serial counterpart while maintaining comparable performance. 97 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 97–101, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics tronIterParamMix T = x T P ( {( t,Yt)}) Perce t—i 1. Split T into S pieces T = {T1, ... , TS} 2. w = 0 3. for n : 1..N 4. w(i,n) = OneEpochPerceptron(Ti, w) w = � 5. i µi,nw(i,n) 6. return w OneEpochPerceptron(Ti, w*) 1. w(°) = w*; k</context>
</contexts>
<marker>Lin, Dyer, 2010</marker>
<rawString>Jimmy Lin and Chris Dyer. 2010. Data-Intensive Text Processing with MapReduce. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="10503" citStr="Marcus et al., 1994" startWordPosition="1648" endWordPosition="1652">zaki, 2008). 5 The Web User Interface Hadoop is bundled with several web interfaces that provide concise tracking information for jobs, tasks, data nodes, etc. as shown in Figure 3. These web interfaces can be used to demonstrate the HadoopPerceptron running phases and monitor the distributed execution of the training, prediction and evaluation modules for several sequence labeling tasks including part-of-speech tagging and named entity recognition. 6 Experiments We investigate HadoopPerceptron training time and prediction accuracy on a part-of-speech (POS) task using the PennTreeBank corpus (Marcus et al., 1994). We use sections 0-18 of the Wall Street Journal for training, and sections 22-24 for testing. We compare the regular percepton trained serially on all the training data with the distributed perceptron trained with iterative parameter mixing with variable number of splits S E {10, 20}. For each system, we report the prediction accuracy measure on the final test set to determine if any loss is observed as a consequence of distributed training. For each system, Figure 4 plots accuracy results computed at the end of every training epoch against consumed wall-clock time. We observe that iterative</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Mcdonald</author>
<author>Keith Hall</author>
<author>Gideon Mann</author>
</authors>
<title>Distributed training strategies for the structured perceptron.</title>
<date>2010</date>
<booktitle>In NAACL ’10: Proceedings of the 11th Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Los Angeles, CA, USA.</location>
<contexts>
<context position="2239" citStr="Mcdonald et al. (2010)" startWordPosition="316" endWordPosition="319">red for its discriminative probabilistic counterparts such as CRFs (Lafferty et al., 2001). Additionally, the Perceptron is robust to approximate inference in large search spaces. Nevertheless, Perceptron training is proportional to inference which is frequently non-linear in the input sequence size. Therefore, training can be time-consuming for complex model structures. Furthermore, for an increasing number of tasks is fundamental to leverage on huge sources of data as the World Wide Web. Such difficulties render the scalability of the Perceptron a challenge. In order to improve scalability, Mcdonald et al. (2010) propose a distributed training strategy called iterative parameter mixing, and show that it has similar convergence properties to the standard perceptron algorithm; it finds a separating hyperplane if the training set is separable; it produces models with comparable accuracies to those trained serially on all the data; and reduces training times significantly by exploiting computing clusters. With this paper we present the HadoopPerceptron package. It provides a freely available open-source implementation of the iterative parameter mixing algorithm for training the structured perceptron on a </context>
<context position="4879" citStr="Mcdonald et al. (2010)" startWordPosition="745" endWordPosition="748">uring each training epoch. In sequence labeling tasks, the algorithm predicts a sequence of labels (an element from the structured output space) for each input sequence. Prediction is determined by linear operations on high-dimensional feature representations of candidate input-output pairs and an associated weight vector. During training, the parameters are updated whenever the prediction that employed them is incorrect. Unlike many batch learning algorithms that can easily be distributed through the gradient calculation, the perceptron online training is more subtle to parallelize. However, Mcdonald et al. (2010) present a simple distributed training through a parameter mixing scheme. The Iterative Parameter Mixing is given in Figure 2 (Mcdonald et al., 2010). First the training data is divided into disjoint splits of example pairs (xt, yt) where xt is the observation sequence and yt is the associated labels. The algorithm proceeds to train a single epoch of the perceptron algorithm for each split in parallel, and mix the local models weights w(i,n) to produce the global weight vector w. The mixed model is then passed to each split to reset the perceptron local weights, and a new iteration is started.</context>
<context position="11497" citStr="Mcdonald et al. (2010)" startWordPosition="1809" endWordPosition="1812">determine if any loss is observed as a consequence of distributed training. For each system, Figure 4 plots accuracy results computed at the end of every training epoch against consumed wall-clock time. We observe that iterative mixing parameter achieves comparable performance to its serial counterpart while converging orders of magnitude faster. Furthermore, we note that the distributed algorithm achieves a slightly higher final accuracy 99 Figure 3: Hadoop interfaces for HadoopPerceptron. Figure 4: Accuracy vs. training time. Each point corresponds to a training epoch. than serial training. Mcdonald et al. (2010) suggest that this is due to the bagging effect that the distributed training has, and due to parameter mixing that is similar to the averaged perceptron. We note also that increasing the number of splits increases the number of epoch required to attain convergence, while reducing the time required per epoch. This implies a trade-off between slower convergence and quicker epochs when selecting a larger number of splits. 7 Conclusion The HadoopPerceptron package provides the first freely-available open-source implementation of iterative parameter mixing Perceptron Training, Prediction and Evalu</context>
</contexts>
<marker>Mcdonald, Hall, Mann, 2010</marker>
<rawString>Ryan Mcdonald, Keith Hall, and Gideon Mann. 2010. Distributed training strategies for the structured perceptron. In NAACL ’10: Proceedings of the 11th Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Rosenblatt</author>
</authors>
<title>The Perceptron: A probabilistic model for information storage and organization in the brain.</title>
<date>1958</date>
<journal>Psychological Review,</journal>
<volume>65</volume>
<issue>6</issue>
<pages>408</pages>
<contexts>
<context position="1294" citStr="Rosenblatt, 1958" startWordPosition="179" endWordPosition="180">ially execution times while handling huge data-sets. The distributed Perceptron training algorithm preserves convergence properties, thus guaranties same accuracy performances as the serial Perceptron. The presented modules can be executed as stand-alone software or easily extended or integrated in complex systems. The execution of the modules applied to specific NLP tasks can be demonstrated and tested via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the MapReduce jobs. 1 Introduction The Perceptron training algorithm (Rosenblatt, 1958; Freund and Schapire, 1999; Collins, 2002) is widely applied in the Natural Language Processing community for learning complex structured models. The non-probabilistic nature of the perceptron parameters makes it possible to incorporate arbitrary features without the need to calculate a partition function, which is required for its discriminative probabilistic counterparts such as CRFs (Lafferty et al., 2001). Additionally, the Perceptron is robust to approximate inference in large search spaces. Nevertheless, Perceptron training is proportional to inference which is frequently non-linear in </context>
</contexts>
<marker>Rosenblatt, 1958</marker>
<rawString>Frank Rosenblatt. 1958. The Perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6):386– 408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Semisupervised sequential labeling and segmentation using giga-word scale unlabeled data. In</title>
<date>2008</date>
<booktitle>ACL ’08: Proceedings of the 46th Conference of the Association for Computational Linguistics,</booktitle>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="9894" citStr="Suzuki and Isozaki, 2008" startWordPosition="1560" endWordPosition="1563">computed against provided gold standards. Prediction and evaluation modules are independent from the training modules, the weight vector given as input could have been computed with any other system using any other training algorithm as long as they employ the same features. The implementation is in Java, and we interface with the Hadoop cluster via the native Java API. It can be easily adapted to a wide range of NLP tasks. Incorporating new features by modifying the extensible feature extractor is straightforward. The package includes the implementation of the basic feature set described in (Suzuki and Isozaki, 2008). 5 The Web User Interface Hadoop is bundled with several web interfaces that provide concise tracking information for jobs, tasks, data nodes, etc. as shown in Figure 3. These web interfaces can be used to demonstrate the HadoopPerceptron running phases and monitor the distributed execution of the training, prediction and evaluation modules for several sequence labeling tasks including part-of-speech tagging and named entity recognition. 6 Experiments We investigate HadoopPerceptron training time and prediction accuracy on a part-of-speech (POS) task using the PennTreeBank corpus (Marcus et a</context>
</contexts>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>Jun Suzuki and Hideki Isozaki. 2008. Semisupervised sequential labeling and segmentation using giga-word scale unlabeled data. In ACL ’08: Proceedings of the 46th Conference of the Association for Computational Linguistics, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom White</author>
</authors>
<title>Hadoop: The Definitive Guide.</title>
<date>2009</date>
<publisher>O’Reilly Media Inc.</publisher>
<contexts>
<context position="3151" citStr="White, 2009" startWordPosition="451" endWordPosition="452">lly on all the data; and reduces training times significantly by exploiting computing clusters. With this paper we present the HadoopPerceptron package. It provides a freely available open-source implementation of the iterative parameter mixing algorithm for training the structured perceptron on a generic sequence labeling tasks. Furthermore, the package provides two additional modules for prediction and evaluation. The three software modules are designed within the MapReduce programming model (Dean and Ghemawat, 2004) and implemented using the Apache Hadoop distributed programming Framework (White, 2009; Lin and Dyer, 2010). The presented HadoopPerceptron package reduces execution time significantly compared to its serial counterpart while maintaining comparable performance. 97 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 97–101, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics tronIterParamMix T = x T P ( {( t,Yt)}) Perce t—i 1. Split T into S pieces T = {T1, ... , TS} 2. w = 0 3. for n : 1..N 4. w(i,n) = OneEpochPerceptron(Ti, w) w = � 5. i µi,nw(i,n) 6. return w OneEpochPerceptron(Ti</context>
</contexts>
<marker>White, 2009</marker>
<rawString>Tom White. 2009. Hadoop: The Definitive Guide. O’Reilly Media Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>