<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.996219">
Semi-supervised Convex Training for Dependency Parsing
</title>
<author confidence="0.996379">
Qin Iris Wang
</author>
<affiliation confidence="0.867397666666667">
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
</affiliation>
<email confidence="0.991343">
wqin@cs.ualberta.ca
</email>
<author confidence="0.994255">
Dale Schuurmans
</author>
<affiliation confidence="0.867472666666667">
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
</affiliation>
<email confidence="0.990571">
dale@cs.ualberta.ca
</email>
<author confidence="0.995046">
Dekang Lin
</author>
<affiliation confidence="0.782778666666667">
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA, USA, 94043
</affiliation>
<email confidence="0.990189">
lindek@google.com
</email>
<sectionHeader confidence="0.992982" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999221714285714">
We present a novel semi-supervised training
algorithm for learning dependency parsers.
By combining a supervised large margin loss
with an unsupervised least squares loss, a dis-
criminative, convex, semi-supervised learning
algorithm can be obtained that is applicable
to large-scale problems. To demonstrate the
benefits of this approach, we apply the tech-
nique to learning dependency parsers from
combined labeled and unlabeled corpora. Us-
ing a stochastic gradient descent algorithm, a
parsing model can be efficiently learned from
semi-supervised data that significantly outper-
forms corresponding supervised methods.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999686">
Supervised learning algorithms still represent the
state of the art approach for inferring dependency
parsers from data (McDonald et al., 2005a; McDon-
ald and Pereira, 2006; Wang et al., 2007). How-
ever, a key drawback of supervised training algo-
rithms is their dependence on labeled data, which
is usually very difficult to obtain. Perceiving the
limitation of supervised learning—in particular, the
heavy dependence on annotated corpora—many re-
searchers have investigated semi-supervised learn-
ing techniques that can take both labeled and unla-
beled training data as input. Following the common
theme of “more data is better data” we also use both
a limited labeled corpora and a plentiful unlabeled
data resource. Our goal is to obtain better perfor-
mance than a purely supervised approach without
unreasonable computational effort. Unfortunately,
although significant recent progress has been made
in the area of semi-supervised learning, the perfor-
mance of semi-supervised learning algorithms still
fall far short of expectations, particularly in chal-
lenging real-world tasks such as natural language
parsing or machine translation.
A large number of distinct approaches to semi-
supervised training algorithms have been investi-
gated in the literature (Bennett and Demiriz, 1998;
Zhu et al., 2003; Altun et al., 2005; Mann and
McCallum, 2007). Among the most prominent ap-
proaches are self-training, generative models, semi-
supervised support vector machines (S3VM), graph-
based algorithms and multi-view algorithms (Zhu,
2005).
Self-training is a commonly used technique
for semi-supervised learning that has been ap-
</bodyText>
<page confidence="0.954376">
532
</page>
<note confidence="0.717931">
Proceedings of ACL-08: HLT, pages 532–540,
</note>
<page confidence="0.538311">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999942083333333">
plied to several natural language processing tasks
(Yarowsky, 1995; Charniak, 1997; Steedman et al.,
2003). The basic idea is to bootstrap a supervised
learning algorithm by alternating between inferring
the missing label information and retraining. Re-
cently, McClosky et al. (2006a) successfully applied
self-training to parsing by exploiting available un-
labeled data, and obtained remarkable results when
the same technique was applied to parser adaptation
(McClosky et al., 2006b). More recently, Haffari
and Sarkar (2007) have extended the work of Abney
(2004) and given a better mathematical understand-
ing of self-training algorithms. They also show con-
nections between these algorithms and other related
machine learning algorithms.
Another approach, generative probabilistic mod-
els, are a well-studied framework that can be ex-
tremely effective. However, generative models use
the EM algorithm for parameter estimation in the
presence of missing labels, which is notoriously
prone to getting stuck in poor local optima. More-
over, EM optimizes a marginal likelihood score that
is not discriminative. Consequently, most previous
work that has attempted semi-supervised or unsu-
pervised approaches to parsing have not produced
results beyond the state of the art supervised results
(Klein and Manning, 2002; Klein and Manning,
2004). Subsequently, alternative estimation strate-
gies for unsupervised learning have been proposed,
such as Contrastive Estimation (CE) by Smith and
Eisner (2005). Contrastive Estimation is a general-
ization of EM, by defining a notion of learner guid-
ance. It makes use of a set of examples (its neighbor-
hood) that are similar in some way to an observed
example, requiring the learner to move probability
mass to a given example, taking only from the ex-
ample’s neighborhood. Nevertheless, CE still suf-
fers from shortcomings, including local minima.
In recent years, SVMs have demonstrated state
of the art results in many supervised learning tasks.
As a result, many researchers have put effort on
developing algorithms for semi-supervised SVMs
(S3VMs) (Bennett and Demiriz, 1998; Altun et
al., 2005). However, the standard objective of an
S3VM is non-convex on the unlabeled data, thus
requiring sophisticated global optimization heuris-
tics to obtain reasonable solutions. A number of
researchers have proposed several efficient approx-
imation algorithms for S3VMs (Bennett and Demi-
riz, 1998; Chapelle and Zien, 2005; Xu and Schu-
urmans, 2005). For example, Chapelle and Zien
(2005) propose an algorithm that smoothes the ob-
jective with a Gaussian function, and then performs
a gradient descent search in the primal space to
achieve a local solution. An alternative approach is
proposed by Xu and Schuurmans (2005) who formu-
late a semi-definite programming (SDP) approach.
In particular, they present an algorithm for multi-
class unsupervised and semi-supervised SVM learn-
ing, which relaxes the original non-convex objective
into a close convex approximation, thereby allowing
a global solution to be obtained. However, the com-
putational cost of SDP is still quite expensive.
Instead of devising various techniques for cop-
ing with non-convex loss functions, we approach the
problem from a different perspective. We simply re-
place the non-convex loss on unlabeled data with an
alternative loss that is jointly convex with respect
to both the model parameters and (the encoding of)
the self-trained prediction targets. More specifically,
for the loss on the unlabeled data part, we substi-
tute the original unsupervised structured SVM loss
with a least squares loss, but keep constraints on
the inferred prediction targets, which avoids trivial-
ization. Although using a least squares loss func-
tion for classification appears misguided, there is
a precedent for just this approach in the early pat-
tern recognition literature (Duda et al., 2000). This
loss function has the advantage that the entire train-
ing objective on both the labeled and unlabeled data
now becomes convex, since it consists of a convex
structured large margin loss on labeled data and a
convex least squares loss on unlabeled data. As
we will demonstrate below, this approach admits an
efficient training procedure that can find a global
minimum, and, perhaps surprisingly, can systemat-
ically improve the accuracy of supervised training
approaches for learning dependency parsers.
Thus, in this paper, we focus on semi-supervised
language learning, where we can make use of both
labeled and unlabeled data. In particular, we in-
vestigate a semi-supervised approach for structured
large margin training, where the objective is a com-
bination of two convex functions, the structured
large margin loss on labeled data and the least
squares loss on unlabeled data. We apply the result-
</bodyText>
<page confidence="0.997451">
533
</page>
<figure confidence="0.88406">
Investors❑ continue❑ to❑ pour❑ cash❑ into❑ funds�moneyJ7
</figure>
<figureCaption confidence="0.999896">
Figure 1: A dependency tree
</figureCaption>
<bodyText confidence="0.999645352941177">
ing semi-supervised convex objective to dependency
parsing, and obtain significant improvement over
the corresponding supervised structured SVM. Note
that our approach is different from the self-training
technique proposed in (McClosky et al., 2006a),
although both methods belong to semi-supervised
training category.
In the remainder of this paper, we first review
the supervised structured large margin training tech-
nique. Then we introduce the standard semi-
supervised structured large margin objective, which
is non-convex and difficult to optimize. Next we
present a new semi-supervised training algorithm for
structured SVMs which is convex optimization. Fi-
nally, we apply this algorithm to dependency pars-
ing and show improved dependency parsing accu-
racy for both Chinese and English.
</bodyText>
<sectionHeader confidence="0.993712" genericHeader="introduction">
2 Dependency Parsing Model
</sectionHeader>
<bodyText confidence="0.999955428571429">
Given a sentence X = (x1, ..., x,,,) (xi denotes
each word in the sentence), we are interested in
computing a directed dependency tree, Y , over X.
As shown in Figure 1, in a dependency structure,
the basic units of a sentence are the syntactic re-
lationships (aka. head-child or governor-dependent
or regent-subordinate relations) between two indi-
vidual words, where the relationships are expressed
by drawing links connecting individual words (Man-
ning and Schutze, 1999). The direction of each link
points from a head word to a child word, and each
word has one and only one head, except for the head
of the sentence. Thus a dependency structure is ac-
tually a rooted, directed tree. We assume that a di-
rected dependency tree Y consists of ordered pairs
(xi —* xj) of words in X such that each word ap-
pears in at least one pair and each word has in-degree
at most one. Dependency trees are assumed to be
projective here, which means that if there is an arc
(xi —* xj), then xi is an ancestor of all the words
between xi and xj.1 Let -b(X) denote the set of all
the directed, projective trees that span on X. The
parser’s goal is then to find the most preferred parse;
that is, a projective tree, Y E 4b(X), that obtains
the highest “score”. In particular, one would assume
that the score of a complete spanning tree Y for a
given sentence, whether probabilistically motivated
or not, can be decomposed as a sum of local scores
for each link (a word pair) (Eisner, 1996; Eisner and
Satta, 1999; McDonald et al., 2005a). Given this
assumption, the parsing problem reduces to find
where the score(xi —* xj) can depend on any mea-
surable property of xi and xj within the sentence X.
This formulation is sufficiently general to capture
most dependency parsing models, including proba-
bilistic dependency models (Eisner, 1996; Wang et
al., 2005) as well as non-probabilistic models (Mc-
Donald et al., 2005a).
For standard scoring functions, particularly those
used in non-generative models, we further assume
that the score of each link in (1) can be decomposed
into a weighted linear combination of features
</bodyText>
<equation confidence="0.994103">
score(xi —* xj) = 0 · f(xi —* xj) (2)
</equation>
<bodyText confidence="0.999938">
where f(xi —* xj) is a feature vector for the link
(xi —* xj), and 0 are the weight parameters to be
estimated during training.
</bodyText>
<sectionHeader confidence="0.850932" genericHeader="method">
3 Supervised Structured Large Margin
Training
</sectionHeader>
<bodyText confidence="0.999656428571429">
Supervised structured large margin training ap-
proaches have been applied to parsing and produce
promising results (Taskar et al., 2004; McDonald et
al., 2005a; Wang et al., 2006). In particular, struc-
tured large margin training can be expressed as min-
imizing a regularized loss (Hastie et al., 2004), as
shown below:
</bodyText>
<footnote confidence="0.915214666666667">
1We assume all the dependency trees are projective in our
work (just as some other researchers do), although in the real
word, most languages are non-projective.
</footnote>
<equation confidence="0.943066">
Y ∗ = arg max score(Y |X) (1)
YEID(X)
11 = arg max score(xi —* xj)
YE4D(X) (xi→xj)EY
534
β2 θTθ + (3)
max(Δ(Li,k, Yi) − diff(θ, Yi, Li,k))
Li,k
</equation>
<bodyText confidence="0.999444583333334">
where Yi is the target tree for sentence Xi; Li,k
ranges over all possible alternative k trees in Φ(Xi);
diff(θ, Yi, Li,k) = score(θ, Yi) − score(θ, Li,k);
score(θ, Yi) = P(xm→xn)EYi θ · f(xm —* xn), as
shown in Section 2; and Δ(Li,k, Yi) is a measure of
distance between the two trees Li,k and Yi. This is
an application of the structured large margin training
approach first proposed in (Taskar et al., 2003) and
(Tsochantaridis et al., 2004).
Using the techniques of Hastie et al. (2004) one
can show that minimizing the objective (3) is equiv-
alent to solving the quadratic program
</bodyText>
<equation confidence="0.982732">
β2 θTθ + eTξ subject to
ξi,k &gt; Δ(Li,k, Yi) − diff(θ, Yi, Li,k)
ξi,k &gt; 0
for all i, Li,k E Φ(Xi) (4)
</equation>
<bodyText confidence="0.999641818181818">
where e denotes the vector of all 1’s and ξ represents
slack variables. This approach corresponds to the
training problem posed in (McDonald et al., 2005a)
and has yielded the best published results for En-
glish dependency parsing.
To compare with the new semi-supervised ap-
proach we will present in Section 5 below, we re-
implemented the supervised structured large margin
training approach in the experiments in Section 7.
More specifically, we solve the following quadratic
program, which is based on Equation (3)
</bodyText>
<equation confidence="0.996751">
Δ(Li,m,n,Yi,m,n)
− diff(θ, Yi,m,n, Li,m,n) (5)
</equation>
<bodyText confidence="0.999893111111111">
where diff(θ, Yi,m,n, Li,m,n) = score(θ, Yi,m,n) −
score(θ, Li,m,n) and k is the sentence length. We
represent a dependency tree as a k x k adjacency
matrix. In the adjacency matrix, the value of Yi,m,n
is 1 if the word m is the head of the word n, 0 oth-
erwise. Since both the distance function Δ(Li, Yi)
and the score function decompose over links, solv-
ing (5) is equivalent to solve the original constrained
quadratic program shown in (4).
</bodyText>
<sectionHeader confidence="0.851592" genericHeader="method">
4 Semi-supervised Structured Large
Margin Objective
</sectionHeader>
<bodyText confidence="0.982275">
The objective of standard semi-supervised struc-
tured SVM is a combination of structured large mar-
gin losses on both labeled and unlabeled data. It has
the following form:
structured loss (θ, Xi, Yi)
structured loss (θ, Xj, Yj) (6)
structured loss (θ, Xi, Yi)
N and U are the number of labeled and unlabeled
training sentences respectively, and Yj ranges over
guessed targets on the unsupervised data.
In the second term of the above objective shown in
(6), both θ and Yj are variables. The resulting loss
function has a hat shape (usually called hat-loss),
which is non-convex. Therefore the objective as a
whole is non-convex, making the search for global
optimal difficult. Note that the root of the optimiza-
tion difficulty for S3VMs is the non-convex property
of the second term in the objective function. We will
propose a novel approach which can deal with this
problem. We introduce an efficient approximation—
least squares loss—for the structured large margin
loss on unlabeled data below.
</bodyText>
<sectionHeader confidence="0.864497" genericHeader="method">
5 Semi-supervised Convex Training for
Structured SVM
</sectionHeader>
<bodyText confidence="0.999742777777778">
Although semi-supervised structured SVM learning
has been an active research area, semi-supervised
structured SVMs have not been used in many real
applications to date. The main reason is that most
available semi-supervised large margin learning ap-
proaches are non-convex or computationally expen-
sive (e.g. (Xu and Schuurmans, 2005)). These tech-
niques are difficult to implement and extremely hard
to scale up. We present a semi-supervised algorithm
</bodyText>
<equation confidence="0.591400724137931">
Δ(Li,m,n,Yi,m,n) (7)
−diff(θ, Yi,m,n, Li,m,n)
k
X
n=1
= max
L
k
X
m=1
min
θ
X
i
min
0,�
min α X max k k
θ 2 θTθ + L X X
i m=1 n=1
α2 θTθ +
min
θ
XN
i=1
U
X
j=1
+ min
Yj
</equation>
<bodyText confidence="0.477836">
where
</bodyText>
<page confidence="0.964957">
535
</page>
<bodyText confidence="0.999976">
for structured large margin training, whose objective
is a combination of two convex terms: the super-
vised structured large margin loss on labeled data
and the cheap least squares loss on unlabeled data.
The combined objective is still convex, easy to opti-
mize and much cheaper to implement.
</bodyText>
<subsectionHeader confidence="0.992932">
5.1 Least Squares Convex Objective
</subsectionHeader>
<bodyText confidence="0.960713481481482">
Before we introduce the new algorithm, we first in-
troduce a convex loss which we apply it to unlabeled
training data for the semi-supervised structured large
margin objective which we will introduce in Sec-
tion 5.2 below. More specifically, we use a struc-
tured least squares loss to approximate the struc-
tured large margin loss on unlabeled data. The cor-
responding objective is:
min α 2 θTθ + (8)
θ,Yj
subject to constraints on Y (explained below).
The idea behind this objective is that for each pos-
sible link (Xj,m — Xj,n), we intend to minimize the
difference between the link and the corresponding
estimated link based on the learned weight vector.
Since this is conducted on unlabeled data, we need
to estimate both θ and Yj to solve the optimization
problem. As mentioned in Section 3, a dependency
tree Yj is represented as an adjacency matrix. Thus
we need to enforce some constraints in the adjacency
matrix to make sure that each Yj satisfies the depen-
dency tree constraints. These constraints are critical
because they prevent (8) from having a trivial solu-
tion in Y. More concretely, suppose we use rows to
denote heads and columns to denote children. Then
we have the following constraints on the adjacency
matrix:
</bodyText>
<listItem confidence="0.952412888888889">
• (1) All entries in Yj are between 0 and 1
(convex relaxation of discrete directed edge in-
dicators);
• (2) The sum over all the entries on each col-
umn is equal to one (one-head rule);
• (3) All the entries on the diagonal are zeros
(no self-link rule);
• (4) Yj,m,n + Yj,n,m G_ 1 (anti-symmetric
rule), which enforces directedness.
</listItem>
<bodyText confidence="0.999939142857143">
One final constraint that is sufficient to ensure that
a directed tree is obtained, is connectedness (i.e.
acyclicity), which can be enforced with an addi-
tional semidefinite constraint. Although convex, this
constraint is more expensive to enforce, therefore we
drop it in our experiments below. (However, adding
the semidefinite connectedness constraint appears to
be feasible on a sentence by sentence level.)
Critically, the objective (8) is jointly convex in
both the weights θ and the edge indicator variables
Y. This means, for example, that there are no local
minima in (8)—any iterative improvement strategy,
if it converges at all, must converge to a global min-
imum.
</bodyText>
<subsectionHeader confidence="0.996425">
5.2 Semi-supervised Convex Objective
</subsectionHeader>
<bodyText confidence="0.9328035">
By combining the convex structured SVM loss on
labeled data (shown in Equation (5)) and the con-
vex least squares loss on unlabeled data (shown in
Equation (8)), we obtain a semi-supervised struc-
tured large margin loss
structured loss (θ, Xi, Yi) +
</bodyText>
<equation confidence="0.933383666666667">
U
X least squares loss (θ, Xj, Yj) (9)
j=1
</equation>
<bodyText confidence="0.940704">
subject to constraints on Y (explained above).
Since the summation of two convex functions is
also convex, so is (9). Replacing the two losses with
the terms shown in Equation (5) and Equation (8),
we obtain the final convex objective as follows:
</bodyText>
<equation confidence="0.9824486">
θ,Yj
min 2N θTθ +
α
2
(θT f (Xj,m — Xj,n) — Yj,m,n/ ll
</equation>
<bodyText confidence="0.866598">
subject to constraints on Y (explained above),
where diff(θ, Yi,m,n, Li,m,n) = score(θ, Yi,m,n) —
</bodyText>
<equation confidence="0.996367157894737">
(θTf(Xj,m — Xj,n) — Yj,m,n
)2
λ U
X
j=1
k
X
m=1
k
X
n=1
2
O(Li,m,n, Yi,m,n) —
diff(θ, Yi,m,n, Li,m,n) + 2UθTθ +
α (10)
XN
i=1
max
L
k
X
m=1
k
X
n=1
α 2 θTθ +
min
θ,Yj
XN
i=1
λ U
X
j=1
k
X
m=1
k
X
</equation>
<page confidence="0.861135333333333">
n=1
2
536
</page>
<bodyText confidence="0.993487875">
score(0, LZ,m,n), N and U are the number of labeled
and unlabeled training sentences respectively, as we
mentioned before. Note that in (10) we have split
the regularizer into two parts; one for the supervised
component of the objective, and the other for the
unsupervised component. Thus the semi-supervised
convex objective is regularized proportionally to the
number of labeled and unlabeled training sentences.
</bodyText>
<sectionHeader confidence="0.993551" genericHeader="method">
6 Efficient Optimization Strategy
</sectionHeader>
<bodyText confidence="0.99986075">
To solve the convex optimization problem shown in
Equation (10), we used a gradient descent approach
which simply uses stochastic gradient steps. The
procedure is as follows.
</bodyText>
<listItem confidence="0.9771228">
• Step 0, initialize the Yj variables of each
unlabeled sentence as a right-branching (left-
headed) chain model, i.e. the head of each word
is its left neighbor.
• Step 1, pass through all the labeled training sen-
tences one by one. The parameters 0 are up-
dated based on each labeled sentence.
• Step 2, based on the learned parameter weights
from the labeled data, update 0 and Yj on each
unlabeled sentence alternatively:
</listItem>
<bodyText confidence="0.869107714285714">
– treat Yj as a constant, update 0 on each
unlabeled sentence by taking a local gra-
dient step;
– treat 0 as a constant, update Yj by call-
ing the optimization software package
CPLEX to solve for an optimal local so-
lution.
</bodyText>
<listItem confidence="0.621218">
• Repeat the procedure of step 1 and step 2 until
maximum iteration number has reached.
</listItem>
<bodyText confidence="0.999913">
This procedure works efficiently on the task of
training a dependency parser. Although 0 and
Yj are updated locally on each sentence, progress
in minimizing the total objective shown in Equa-
tion (10) is made in each iteration. In our experi-
ments, the objective usually converges within 30 it-
erations.
</bodyText>
<sectionHeader confidence="0.989016" genericHeader="evaluation">
7 Experimental Results
</sectionHeader>
<bodyText confidence="0.99967875">
Given a convex approach to semi-supervised struc-
tured large margin training, and an efficient training
algorithm for achieving a global optimum, we now
investigate its effectiveness for dependency parsing.
In particular, we investigate the accuracy of the re-
sults it produces. We applied the resulting algorithm
to learn dependency parsers for both English and
Chinese.
</bodyText>
<subsectionHeader confidence="0.960698">
7.1 Experimental Design
Data Sets
</subsectionHeader>
<bodyText confidence="0.99996484">
Since we use a semi-supervised approach, both la-
beled and unlabeled training data are needed. For
experiment on English, we used the English Penn
Treebank (PTB) (Marcus et al., 1993) and the con-
stituency structures were converted to dependency
trees using the same rules as (Yamada and Mat-
sumoto, 2003). The standard training set of PTB
was spit into 2 parts: labeled training data—the
first 30k sentences in section 2-21, and unlabeled
training data—the remaining sentences in section
2-21. For Chinese, we experimented on the Penn
Chinese Treebank 4.0 (CTB4) (Palmer et al., 2004)
and we used the rules in (Bikel, 2004) for conver-
sion. We also divided the standard training set into
2 parts: sentences in section 400-931 and sentences
in section 1-270 are used as labeled and unlabeled
data respectively. For both English and Chinese,
we adopted the standard development and test sets
throughout the literature.
As listed in Table 1 with greater detail, we
experimented with sets of data with different sen-
tence length: PTB-10/CTB4-10, PTB-15/CTB4-15,
PTB-20/CTB4-20, CTB4-40 and CTB4, which
contain sentences with up to 10, 15, 20, 40 and all
words respectively.
</bodyText>
<subsectionHeader confidence="0.881222">
Features
</subsectionHeader>
<bodyText confidence="0.99996475">
For simplicity, in current work, we only used two
sets of features—word-pair and tag-pair indicator
features, which are a subset of features used by
other researchers on dependency parsing (McDon-
ald et al., 2005a; Wang et al., 2007). Although
our algorithms can take arbitrary features, by only
using these simple features, we already obtained
very promising results on dependency parsing
using both the supervised and semi-supervised
approaches. Using the full set of features described
in (McDonald et al., 2005a; Wang et al., 2007) and
comparing the corresponding dependency parsing
</bodyText>
<page confidence="0.991512">
537
</page>
<table confidence="0.999713416666667">
Training(l/ul) 3026/1016
PTB-10 Dev 163
Test 270
Training 7303/2370
English PTB-15 Dev 421
Test 603
Training 12519/4003
PTB-20 Dev 725
Test 1034
Training(l/ul) 642/347
CTB4-10 Dev 61
Test 40
Training 1262/727
CTB4-15 Dev 112
Test 83
Training 2038/1150
Chinese CTB4-20 Dev 163
Test 118
Training 4400/2452
CTB4-40 Dev 274
Test 240
Training 5314/2977
CTB4 Dev 300
Test 289
</table>
<tableCaption confidence="0.999891">
Table 1: Size of Experimental Data (# of sentences)
</tableCaption>
<bodyText confidence="0.9856415">
results with previous work remains a direction for
future work.
</bodyText>
<subsectionHeader confidence="0.947431">
Dependency Parsing Algorithms
</subsectionHeader>
<bodyText confidence="0.9998316">
For simplicity of implementation, we use a stan-
dard CKY parser in the experiments, although
Eisner’s algorithm (Eisner, 1996) and the Spanning
Tree algorithm (McDonald et al., 2005b) are also
applicable.
</bodyText>
<subsectionHeader confidence="0.846289">
7.2 Results
</subsectionHeader>
<bodyText confidence="0.99989525">
We evaluate parsing accuracy by comparing the di-
rected dependency links in the parser output against
the directed links in the treebank. The parameters
α and A which appear in Equation (10) were tuned
on the development set. Note that, during training,
we only used the raw sentences of the unlabeled
data. As shown in Table 2 and Table 3, for each
data set, the semi-supervised approach achieves a
significant improvement over the supervised one in
dependency parsing accuracy on both Chinese and
English. These positive results are somewhat sur-
prising since a very simple loss function was used on
</bodyText>
<table confidence="0.96266784">
Training Test length Supervised Semi-sup
Train-10 &lt; 10 82.98 84.50
&lt; 10 84.80 86.93
Train-15 &lt; 15 76.96 80.79
&lt; 10 84.50 86.32
Train-20 &lt; 15 78.77 80.57
&lt; 20 74.89 77.85
&lt; 10 84.19 85.71
&lt; 15 78.03 81.21
Train-40 &lt; 20 76.25 77.79
&lt; 40 68.17 70.90
&lt; 10 82.67 84.80
&lt; 15 77.92 79.30
Train-all &lt; 20 77.30 77.24
&lt; 40 70.11 71.90
all 66.30 67.35
Table 2: Supervised and Semi-supervised Dependency
Parsing Accuracy on Chinese (%)
Training Test length Supervised Semi-sup
Train-10 &lt; 10 87.77 89.17
&lt; 10 88.06 89.31
Train-15 &lt; 15 81.10 83.37
&lt; 10 88.78 90.61
Train-20 &lt; 15 83.00 83.87
&lt; 20 77.70 79.09
</table>
<tableCaption confidence="0.960465">
Table 3: Supervised and Semi-supervised Dependency
Parsing Accuracy on English (%)
</tableCaption>
<page confidence="0.99506">
538
</page>
<bodyText confidence="0.999977">
the unlabeled data. A key benefit of the approach is
that a straightforward training algorithm can be used
to obtain global solutions. Note that the results of
our model are not directly comparable with previous
parsing results shown in (McClosky et al., 2006a),
since the parsing accuracy is measured in terms of
dependency relations while their results are f-score
of the bracketings implied in the phrase structure.
</bodyText>
<sectionHeader confidence="0.981955" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999981517241379">
In this paper, we have presented a novel algorithm
for semi-supervised structured large margin training.
Unlike previous proposed approaches, we introduce
a convex objective for the semi-supervised learning
algorithm by combining a convex structured SVM
loss and a convex least square loss. This new semi-
supervised algorithm is much more computationally
efficient and can easily scale up. We have proved our
hypothesis by applying the algorithm to the signifi-
cant task of dependency parsing. The experimental
results show that the proposed semi-supervised large
margin training algorithm outperforms the super-
vised one, without much additional computational
cost.
There remain many directions for future work.
One obvious direction is to use the whole Penn Tree-
bank as labeled data and use some other unannotated
data source as unlabeled data for semi-supervised
training. Next, as we mentioned before, a much
richer feature set can be used in our model to get
better dependency parsing results. Another direc-
tion is to apply the semi-supervised algorithm to
other natural language problems, such as machine
translation, topic segmentation and chunking. In
these areas, there are only limited annotated data
available. Therefore semi-supervised approaches
are necessary to achieve better performance. The
proposed semi-supervised convex training approach
can be easily applied to these tasks.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999752666666667">
We thank the anonymous reviewers for their useful
comments. Research is supported by the Alberta In-
genuity Center for Machine Learning, NSERC, MI-
TACS, CFI and the Canada Research Chairs pro-
gram. The first author was also funded by the Queen
Elizabeth II Graduate Scholarship.
</bodyText>
<sectionHeader confidence="0.990062" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999340823529412">
S. Abney. 2004. Understanding the yarowsky algorithm.
Computational Linguistics, 30(3):365–395.
Y. Altun, D. McAllester, and M. Belkin. 2005. Max-
imum margin semi-supervised learning for structured
variables. In Proceedings of Advances in Neural In-
formation Processing Systems 18.
K. Bennett and A. Demiriz. 1998. Semi-supervised sup-
port vector machines. In Proceedings of Advances in
Neural Information Processing Systems 11.
D. Bikel. 2004. Intricacies of Collins’ parsing model.
Computational Linguistics, 30(4).
O. Chapelle and A. Zien. 2005. Semi-supervised clas-
sification by low density separation. In Proceedings
of the Tenth International Workshop on Artificial In-
teligence and Statistics.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proceedings of
the Association for the Advancement of Artificial In-
telligence, pages 598–603.
R. Duda, P. Hart, and D. Stork. 2000. Pattern Classifica-
tion. Wiley, second edition.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexi-
cal context-free grammars and head-automaton gram-
mars. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
the International Conference on Computational Lin-
guistics.
G. Haffari and A. Sarkar. 2007. Analysis of semi-
supervised learning with the yarowsky algorithm. In
Proceedings of the Conference on Uncertainty in Arti-
ficial Intelligence.
T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. 2004.
The entire regularization path for the support vector
machine. Journal of Machine Learning Research,
5:1391–1415.
D. Klein and C. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
D. Klein and C. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In Proceedingsof the Annual Meeting of the
Association for Computational Linguistics.
G. S. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation reg-
ularization. In Proceedings of International Confer-
ence on Machine Learning.
C. Manning and H. Schutze. 1999. Foundations of Sta-
tistical Natural Language Processing. MIT Press.
</reference>
<page confidence="0.984618">
539
</page>
<reference confidence="0.999905231707317">
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313–330.
D. McClosky, E. Charniak, and M. Johnson. 2006a. Ef-
fective self-training for parsing. In Proceedings of the
Human Language Technology: the Annual Conference
of the North American Chapter of the Association for
Computational Linguistics.
D. McClosky, E. Charniak, and M. Johnson. 2006b.
Reranking and self-training for parser adaptation. In
Proceedings of the International Conference on Com-
putational Linguistics and the Annual Meeting of the
Association for Computational Linguistics.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of European Chapter of the Annual Meeting
of the Association for Computational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of Human Language
Technologies and Conference on Empirical Methods
in Natural Language Processing.
M. Palmer et al. 2004. Chinese Treebank 4.0. Linguistic
Data Consortium.
N. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the European Chapter of
the Annual Meeting of the Association for Computa-
tional Linguistics, pages 331–338.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-
margin Markov networks. In Proceedings ofAdvances
in Neural Information Processing Systems 16.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proceedings of
International Conference on Machine Learning.
Q. Wang, D. Schuurmans, and D. Lin. 2005. Strictly
lexical dependency parsing. In Proceedings of the In-
ternational Workshop on Parsing Technologies, pages
152–159.
Q. Wang, C. Cherry, D. Lizotte, and D. Schuurmans.
2006. Improved large margin dependency parsing via
local constraints and Laplacian regularization. In Pro-
ceedings of The Conference on Computational Natural
Language Learning, pages 21–28.
Q. Wang, D. Lin, and D. Schuurmans. 2007. Simple
training of dependency parsers via structured boosting.
In Proceedings of the International Joint Conference
on Artificial Intelligence, pages 1756–1762.
L. Xu and D. Schuurmans. 2005. Unsupervised and
semi-supervised multi-class support vector machines.
In Proceedings the Association for the Advancement of
Artificial Intelligence.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of the International Workshop on Parsing
Technologies.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings ofthe Annual Meeting of the Association for Com-
putational Linguistics, pages 189–196, Cambridge,
Massachusetts.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using Gaussian fields and har-
monic functions. In Proceedings ofInternational Con-
ference on Machine Learning.
X. Zhu. 2005. Semi-supervised learning literature sur-
vey. Technical report, Computer Sciences, University
of Wisconsin-Madison.
</reference>
<page confidence="0.997005">
540
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.870637">
<title confidence="0.999781">Semi-supervised Convex Training for Dependency Parsing</title>
<author confidence="0.99387">Qin Iris Wang</author>
<affiliation confidence="0.9999725">Department of Computing Science University of Alberta</affiliation>
<address confidence="0.997785">Edmonton, AB, Canada, T6G 2E8</address>
<email confidence="0.995659">wqin@cs.ualberta.ca</email>
<author confidence="0.995862">Dale Schuurmans</author>
<affiliation confidence="0.9999755">Department of Computing Science University of Alberta</affiliation>
<address confidence="0.999183">Edmonton, AB, Canada, T6G 2E8</address>
<email confidence="0.997691">dale@cs.ualberta.ca</email>
<author confidence="0.931571">Dekang Lin</author>
<affiliation confidence="0.998178">Google Inc.</affiliation>
<address confidence="0.999006">1600 Amphitheatre Parkway Mountain View, CA, USA, 94043</address>
<email confidence="0.999775">lindek@google.com</email>
<abstract confidence="0.9971022">We present a novel semi-supervised training algorithm for learning dependency parsers. By combining a supervised large margin loss with an unsupervised least squares loss, a discriminative, convex, semi-supervised learning algorithm can be obtained that is applicable to large-scale problems. To demonstrate the benefits of this approach, we apply the technique to learning dependency parsers from combined labeled and unlabeled corpora. Using a stochastic gradient descent algorithm, a parsing model can be efficiently learned from semi-supervised data that significantly outperforms corresponding supervised methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Understanding the yarowsky algorithm.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="3338" citStr="Abney (2004)" startWordPosition="475" endWordPosition="476">�2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by alternating between inferring the missing label information and retraining. Recently, McClosky et al. (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al., 2006b). More recently, Haffari and Sarkar (2007) have extended the work of Abney (2004) and given a better mathematical understanding of self-training algorithms. They also show connections between these algorithms and other related machine learning algorithms. Another approach, generative probabilistic models, are a well-studied framework that can be extremely effective. However, generative models use the EM algorithm for parameter estimation in the presence of missing labels, which is notoriously prone to getting stuck in poor local optima. Moreover, EM optimizes a marginal likelihood score that is not discriminative. Consequently, most previous work that has attempted semi-su</context>
</contexts>
<marker>Abney, 2004</marker>
<rawString>S. Abney. 2004. Understanding the yarowsky algorithm. Computational Linguistics, 30(3):365–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Altun</author>
<author>D McAllester</author>
<author>M Belkin</author>
</authors>
<title>Maximum margin semi-supervised learning for structured variables.</title>
<date>2005</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems 18.</booktitle>
<contexts>
<context position="2348" citStr="Altun et al., 2005" startWordPosition="333" endWordPosition="336">d data resource. Our goal is to obtain better performance than a purely supervised approach without unreasonable computational effort. Unfortunately, although significant recent progress has been made in the area of semi-supervised learning, the performance of semi-supervised learning algorithms still fall far short of expectations, particularly in challenging real-world tasks such as natural language parsing or machine translation. A large number of distinct approaches to semisupervised training algorithms have been investigated in the literature (Bennett and Demiriz, 1998; Zhu et al., 2003; Altun et al., 2005; Mann and McCallum, 2007). Among the most prominent approaches are self-training, generative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (Zhu, 2005). Self-training is a commonly used technique for semi-supervised learning that has been ap532 Proceedings of ACL-08: HLT, pages 532–540, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by </context>
<context position="4903" citStr="Altun et al., 2005" startWordPosition="711" endWordPosition="714">mation is a generalization of EM, by defining a notion of learner guidance. It makes use of a set of examples (its neighborhood) that are similar in some way to an observed example, requiring the learner to move probability mass to a given example, taking only from the example’s neighborhood. Nevertheless, CE still suffers from shortcomings, including local minima. In recent years, SVMs have demonstrated state of the art results in many supervised learning tasks. As a result, many researchers have put effort on developing algorithms for semi-supervised SVMs (S3VMs) (Bennett and Demiriz, 1998; Altun et al., 2005). However, the standard objective of an S3VM is non-convex on the unlabeled data, thus requiring sophisticated global optimization heuristics to obtain reasonable solutions. A number of researchers have proposed several efficient approximation algorithms for S3VMs (Bennett and Demiriz, 1998; Chapelle and Zien, 2005; Xu and Schuurmans, 2005). For example, Chapelle and Zien (2005) propose an algorithm that smoothes the objective with a Gaussian function, and then performs a gradient descent search in the primal space to achieve a local solution. An alternative approach is proposed by Xu and Schu</context>
</contexts>
<marker>Altun, McAllester, Belkin, 2005</marker>
<rawString>Y. Altun, D. McAllester, and M. Belkin. 2005. Maximum margin semi-supervised learning for structured variables. In Proceedings of Advances in Neural Information Processing Systems 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bennett</author>
<author>A Demiriz</author>
</authors>
<title>Semi-supervised support vector machines.</title>
<date>1998</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems 11.</booktitle>
<contexts>
<context position="2310" citStr="Bennett and Demiriz, 1998" startWordPosition="325" endWordPosition="328">ited labeled corpora and a plentiful unlabeled data resource. Our goal is to obtain better performance than a purely supervised approach without unreasonable computational effort. Unfortunately, although significant recent progress has been made in the area of semi-supervised learning, the performance of semi-supervised learning algorithms still fall far short of expectations, particularly in challenging real-world tasks such as natural language parsing or machine translation. A large number of distinct approaches to semisupervised training algorithms have been investigated in the literature (Bennett and Demiriz, 1998; Zhu et al., 2003; Altun et al., 2005; Mann and McCallum, 2007). Among the most prominent approaches are self-training, generative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (Zhu, 2005). Self-training is a commonly used technique for semi-supervised learning that has been ap532 Proceedings of ACL-08: HLT, pages 532–540, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstr</context>
<context position="4882" citStr="Bennett and Demiriz, 1998" startWordPosition="707" endWordPosition="710">er (2005). Contrastive Estimation is a generalization of EM, by defining a notion of learner guidance. It makes use of a set of examples (its neighborhood) that are similar in some way to an observed example, requiring the learner to move probability mass to a given example, taking only from the example’s neighborhood. Nevertheless, CE still suffers from shortcomings, including local minima. In recent years, SVMs have demonstrated state of the art results in many supervised learning tasks. As a result, many researchers have put effort on developing algorithms for semi-supervised SVMs (S3VMs) (Bennett and Demiriz, 1998; Altun et al., 2005). However, the standard objective of an S3VM is non-convex on the unlabeled data, thus requiring sophisticated global optimization heuristics to obtain reasonable solutions. A number of researchers have proposed several efficient approximation algorithms for S3VMs (Bennett and Demiriz, 1998; Chapelle and Zien, 2005; Xu and Schuurmans, 2005). For example, Chapelle and Zien (2005) propose an algorithm that smoothes the objective with a Gaussian function, and then performs a gradient descent search in the primal space to achieve a local solution. An alternative approach is pr</context>
</contexts>
<marker>Bennett, Demiriz, 1998</marker>
<rawString>K. Bennett and A. Demiriz. 1998. Semi-supervised support vector machines. In Proceedings of Advances in Neural Information Processing Systems 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="21011" citStr="Bikel, 2004" startWordPosition="3424" endWordPosition="3425">use a semi-supervised approach, both labeled and unlabeled training data are needed. For experiment on English, we used the English Penn Treebank (PTB) (Marcus et al., 1993) and the constituency structures were converted to dependency trees using the same rules as (Yamada and Matsumoto, 2003). The standard training set of PTB was spit into 2 parts: labeled training data—the first 30k sentences in section 2-21, and unlabeled training data—the remaining sentences in section 2-21. For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al., 2004) and we used the rules in (Bikel, 2004) for conversion. We also divided the standard training set into 2 parts: sentences in section 400-931 and sentences in section 1-270 are used as labeled and unlabeled data respectively. For both English and Chinese, we adopted the standard development and test sets throughout the literature. As listed in Table 1 with greater detail, we experimented with sets of data with different sentence length: PTB-10/CTB4-10, PTB-15/CTB4-15, PTB-20/CTB4-20, CTB4-40 and CTB4, which contain sentences with up to 10, 15, 20, 40 and all words respectively. Features For simplicity, in current work, we only used </context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>D. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Chapelle</author>
<author>A Zien</author>
</authors>
<title>Semi-supervised classification by low density separation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth International Workshop on Artificial Inteligence and Statistics.</booktitle>
<contexts>
<context position="5219" citStr="Chapelle and Zien, 2005" startWordPosition="757" endWordPosition="760">till suffers from shortcomings, including local minima. In recent years, SVMs have demonstrated state of the art results in many supervised learning tasks. As a result, many researchers have put effort on developing algorithms for semi-supervised SVMs (S3VMs) (Bennett and Demiriz, 1998; Altun et al., 2005). However, the standard objective of an S3VM is non-convex on the unlabeled data, thus requiring sophisticated global optimization heuristics to obtain reasonable solutions. A number of researchers have proposed several efficient approximation algorithms for S3VMs (Bennett and Demiriz, 1998; Chapelle and Zien, 2005; Xu and Schuurmans, 2005). For example, Chapelle and Zien (2005) propose an algorithm that smoothes the objective with a Gaussian function, and then performs a gradient descent search in the primal space to achieve a local solution. An alternative approach is proposed by Xu and Schuurmans (2005) who formulate a semi-definite programming (SDP) approach. In particular, they present an algorithm for multiclass unsupervised and semi-supervised SVM learning, which relaxes the original non-convex objective into a close convex approximation, thereby allowing a global solution to be obtained. However</context>
</contexts>
<marker>Chapelle, Zien, 2005</marker>
<rawString>O. Chapelle and A. Zien. 2005. Semi-supervised classification by low density separation. In Proceedings of the Tenth International Workshop on Artificial Inteligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical parsing with a contextfree grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Association for the Advancement of Artificial Intelligence,</booktitle>
<pages>598--603</pages>
<contexts>
<context position="2856" citStr="Charniak, 1997" startWordPosition="404" endWordPosition="405"> have been investigated in the literature (Bennett and Demiriz, 1998; Zhu et al., 2003; Altun et al., 2005; Mann and McCallum, 2007). Among the most prominent approaches are self-training, generative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (Zhu, 2005). Self-training is a commonly used technique for semi-supervised learning that has been ap532 Proceedings of ACL-08: HLT, pages 532–540, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by alternating between inferring the missing label information and retraining. Recently, McClosky et al. (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al., 2006b). More recently, Haffari and Sarkar (2007) have extended the work of Abney (2004) and given a better mathematical understanding of self-training algorithms. They also show connections between these a</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997. Statistical parsing with a contextfree grammar and word statistics. In Proceedings of the Association for the Advancement of Artificial Intelligence, pages 598–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Duda</author>
<author>P Hart</author>
<author>D Stork</author>
</authors>
<title>Pattern Classification.</title>
<date>2000</date>
<publisher>Wiley,</publisher>
<note>second edition.</note>
<contexts>
<context position="6641" citStr="Duda et al., 2000" startWordPosition="979" endWordPosition="982">replace the non-convex loss on unlabeled data with an alternative loss that is jointly convex with respect to both the model parameters and (the encoding of) the self-trained prediction targets. More specifically, for the loss on the unlabeled data part, we substitute the original unsupervised structured SVM loss with a least squares loss, but keep constraints on the inferred prediction targets, which avoids trivialization. Although using a least squares loss function for classification appears misguided, there is a precedent for just this approach in the early pattern recognition literature (Duda et al., 2000). This loss function has the advantage that the entire training objective on both the labeled and unlabeled data now becomes convex, since it consists of a convex structured large margin loss on labeled data and a convex least squares loss on unlabeled data. As we will demonstrate below, this approach admits an efficient training procedure that can find a global minimum, and, perhaps surprisingly, can systematically improve the accuracy of supervised training approaches for learning dependency parsers. Thus, in this paper, we focus on semi-supervised language learning, where we can make use of</context>
</contexts>
<marker>Duda, Hart, Stork, 2000</marker>
<rawString>R. Duda, P. Hart, and D. Stork. 2000. Pattern Classification. Wiley, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>G Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head-automaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9946" citStr="Eisner and Satta, 1999" startWordPosition="1524" endWordPosition="1527">ependency trees are assumed to be projective here, which means that if there is an arc (xi —* xj), then xi is an ancestor of all the words between xi and xj.1 Let -b(X) denote the set of all the directed, projective trees that span on X. The parser’s goal is then to find the most preferred parse; that is, a projective tree, Y E 4b(X), that obtains the highest “score”. In particular, one would assume that the score of a complete spanning tree Y for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair) (Eisner, 1996; Eisner and Satta, 1999; McDonald et al., 2005a). Given this assumption, the parsing problem reduces to find where the score(xi —* xj) can depend on any measurable property of xi and xj within the sentence X. This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Eisner, 1996; Wang et al., 2005) as well as non-probabilistic models (McDonald et al., 2005a). For standard scoring functions, particularly those used in non-generative models, we further assume that the score of each link in (1) can be decomposed into a weighted linear combination of f</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>J. Eisner and G. Satta. 1999. Efficient parsing for bilexical context-free grammars and head-automaton grammars. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="9922" citStr="Eisner, 1996" startWordPosition="1522" endWordPosition="1523">at most one. Dependency trees are assumed to be projective here, which means that if there is an arc (xi —* xj), then xi is an ancestor of all the words between xi and xj.1 Let -b(X) denote the set of all the directed, projective trees that span on X. The parser’s goal is then to find the most preferred parse; that is, a projective tree, Y E 4b(X), that obtains the highest “score”. In particular, one would assume that the score of a complete spanning tree Y for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair) (Eisner, 1996; Eisner and Satta, 1999; McDonald et al., 2005a). Given this assumption, the parsing problem reduces to find where the score(xi —* xj) can depend on any measurable property of xi and xj within the sentence X. This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Eisner, 1996; Wang et al., 2005) as well as non-probabilistic models (McDonald et al., 2005a). For standard scoring functions, particularly those used in non-generative models, we further assume that the score of each link in (1) can be decomposed into a weighted</context>
<context position="22796" citStr="Eisner, 1996" startWordPosition="3699" endWordPosition="3700">-10 Dev 163 Test 270 Training 7303/2370 English PTB-15 Dev 421 Test 603 Training 12519/4003 PTB-20 Dev 725 Test 1034 Training(l/ul) 642/347 CTB4-10 Dev 61 Test 40 Training 1262/727 CTB4-15 Dev 112 Test 83 Training 2038/1150 Chinese CTB4-20 Dev 163 Test 118 Training 4400/2452 CTB4-40 Dev 274 Test 240 Training 5314/2977 CTB4 Dev 300 Test 289 Table 1: Size of Experimental Data (# of sentences) results with previous work remains a direction for future work. Dependency Parsing Algorithms For simplicity of implementation, we use a standard CKY parser in the experiments, although Eisner’s algorithm (Eisner, 1996) and the Spanning Tree algorithm (McDonald et al., 2005b) are also applicable. 7.2 Results We evaluate parsing accuracy by comparing the directed dependency links in the parser output against the directed links in the treebank. The parameters α and A which appear in Equation (10) were tuned on the development set. Note that, during training, we only used the raw sentences of the unlabeled data. As shown in Table 2 and Table 3, for each data set, the semi-supervised approach achieves a significant improvement over the supervised one in dependency parsing accuracy on both Chinese and English. Th</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Haffari</author>
<author>A Sarkar</author>
</authors>
<title>Analysis of semisupervised learning with the yarowsky algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="3299" citStr="Haffari and Sarkar (2007)" startWordPosition="466" endWordPosition="469">LT, pages 532–540, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by alternating between inferring the missing label information and retraining. Recently, McClosky et al. (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al., 2006b). More recently, Haffari and Sarkar (2007) have extended the work of Abney (2004) and given a better mathematical understanding of self-training algorithms. They also show connections between these algorithms and other related machine learning algorithms. Another approach, generative probabilistic models, are a well-studied framework that can be extremely effective. However, generative models use the EM algorithm for parameter estimation in the presence of missing labels, which is notoriously prone to getting stuck in poor local optima. Moreover, EM optimizes a marginal likelihood score that is not discriminative. Consequently, most p</context>
</contexts>
<marker>Haffari, Sarkar, 2007</marker>
<rawString>G. Haffari and A. Sarkar. 2007. Analysis of semisupervised learning with the yarowsky algorithm. In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hastie</author>
<author>S Rosset</author>
<author>R Tibshirani</author>
<author>J Zhu</author>
</authors>
<title>The entire regularization path for the support vector machine.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--1391</pages>
<contexts>
<context position="11065" citStr="Hastie et al., 2004" startWordPosition="1709" endWordPosition="1712">r assume that the score of each link in (1) can be decomposed into a weighted linear combination of features score(xi —* xj) = 0 · f(xi —* xj) (2) where f(xi —* xj) is a feature vector for the link (xi —* xj), and 0 are the weight parameters to be estimated during training. 3 Supervised Structured Large Margin Training Supervised structured large margin training approaches have been applied to parsing and produce promising results (Taskar et al., 2004; McDonald et al., 2005a; Wang et al., 2006). In particular, structured large margin training can be expressed as minimizing a regularized loss (Hastie et al., 2004), as shown below: 1We assume all the dependency trees are projective in our work (just as some other researchers do), although in the real word, most languages are non-projective. Y ∗ = arg max score(Y |X) (1) YEID(X) 11 = arg max score(xi —* xj) YE4D(X) (xi→xj)EY 534 β2 θTθ + (3) max(Δ(Li,k, Yi) − diff(θ, Yi, Li,k)) Li,k where Yi is the target tree for sentence Xi; Li,k ranges over all possible alternative k trees in Φ(Xi); diff(θ, Yi, Li,k) = score(θ, Yi) − score(θ, Li,k); score(θ, Yi) = P(xm→xn)EYi θ · f(xm —* xn), as shown in Section 2; and Δ(Li,k, Yi) is a measure of distance between the </context>
</contexts>
<marker>Hastie, Rosset, Tibshirani, Zhu, 2004</marker>
<rawString>T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. 2004. The entire regularization path for the support vector machine. Journal of Machine Learning Research, 5:1391–1415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>A generative constituent-context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4082" citStr="Klein and Manning, 2002" startWordPosition="582" endWordPosition="585">rithms and other related machine learning algorithms. Another approach, generative probabilistic models, are a well-studied framework that can be extremely effective. However, generative models use the EM algorithm for parameter estimation in the presence of missing labels, which is notoriously prone to getting stuck in poor local optima. Moreover, EM optimizes a marginal likelihood score that is not discriminative. Consequently, most previous work that has attempted semi-supervised or unsupervised approaches to parsing have not produced results beyond the state of the art supervised results (Klein and Manning, 2002; Klein and Manning, 2004). Subsequently, alternative estimation strategies for unsupervised learning have been proposed, such as Contrastive Estimation (CE) by Smith and Eisner (2005). Contrastive Estimation is a generalization of EM, by defining a notion of learner guidance. It makes use of a set of examples (its neighborhood) that are similar in some way to an observed example, requiring the learner to move probability mass to a given example, taking only from the example’s neighborhood. Nevertheless, CE still suffers from shortcomings, including local minima. In recent years, SVMs have dem</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>D. Klein and C. Manning. 2002. A generative constituent-context model for improved grammar induction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedingsof the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4108" citStr="Klein and Manning, 2004" startWordPosition="586" endWordPosition="589">machine learning algorithms. Another approach, generative probabilistic models, are a well-studied framework that can be extremely effective. However, generative models use the EM algorithm for parameter estimation in the presence of missing labels, which is notoriously prone to getting stuck in poor local optima. Moreover, EM optimizes a marginal likelihood score that is not discriminative. Consequently, most previous work that has attempted semi-supervised or unsupervised approaches to parsing have not produced results beyond the state of the art supervised results (Klein and Manning, 2002; Klein and Manning, 2004). Subsequently, alternative estimation strategies for unsupervised learning have been proposed, such as Contrastive Estimation (CE) by Smith and Eisner (2005). Contrastive Estimation is a generalization of EM, by defining a notion of learner guidance. It makes use of a set of examples (its neighborhood) that are similar in some way to an observed example, requiring the learner to move probability mass to a given example, taking only from the example’s neighborhood. Nevertheless, CE still suffers from shortcomings, including local minima. In recent years, SVMs have demonstrated state of the art</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proceedingsof the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>A McCallum</author>
</authors>
<title>Simple, robust, scalable semi-supervised learning via expectation regularization.</title>
<date>2007</date>
<booktitle>In Proceedings of International Conference on Machine Learning.</booktitle>
<contexts>
<context position="2374" citStr="Mann and McCallum, 2007" startWordPosition="337" endWordPosition="340"> goal is to obtain better performance than a purely supervised approach without unreasonable computational effort. Unfortunately, although significant recent progress has been made in the area of semi-supervised learning, the performance of semi-supervised learning algorithms still fall far short of expectations, particularly in challenging real-world tasks such as natural language parsing or machine translation. A large number of distinct approaches to semisupervised training algorithms have been investigated in the literature (Bennett and Demiriz, 1998; Zhu et al., 2003; Altun et al., 2005; Mann and McCallum, 2007). Among the most prominent approaches are self-training, generative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (Zhu, 2005). Self-training is a commonly used technique for semi-supervised learning that has been ap532 Proceedings of ACL-08: HLT, pages 532–540, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by alternating between inferr</context>
</contexts>
<marker>Mann, McCallum, 2007</marker>
<rawString>G. S. Mann and A. McCallum. 2007. Simple, robust, scalable semi-supervised learning via expectation regularization. In Proceedings of International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schutze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8925" citStr="Manning and Schutze, 1999" startWordPosition="1328" endWordPosition="1332">ally, we apply this algorithm to dependency parsing and show improved dependency parsing accuracy for both Chinese and English. 2 Dependency Parsing Model Given a sentence X = (x1, ..., x,,,) (xi denotes each word in the sentence), we are interested in computing a directed dependency tree, Y , over X. As shown in Figure 1, in a dependency structure, the basic units of a sentence are the syntactic relationships (aka. head-child or governor-dependent or regent-subordinate relations) between two individual words, where the relationships are expressed by drawing links connecting individual words (Manning and Schutze, 1999). The direction of each link points from a head word to a child word, and each word has one and only one head, except for the head of the sentence. Thus a dependency structure is actually a rooted, directed tree. We assume that a directed dependency tree Y consists of ordered pairs (xi —* xj) of words in X such that each word appears in at least one pair and each word has in-degree at most one. Dependency trees are assumed to be projective here, which means that if there is an arc (xi —* xj), then xi is an ancestor of all the words between xi and xj.1 Let -b(X) denote the set of all the direct</context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>C. Manning and H. Schutze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="20572" citStr="Marcus et al., 1993" startWordPosition="3350" endWordPosition="3353">terations. 7 Experimental Results Given a convex approach to semi-supervised structured large margin training, and an efficient training algorithm for achieving a global optimum, we now investigate its effectiveness for dependency parsing. In particular, we investigate the accuracy of the results it produces. We applied the resulting algorithm to learn dependency parsers for both English and Chinese. 7.1 Experimental Design Data Sets Since we use a semi-supervised approach, both labeled and unlabeled training data are needed. For experiment on English, we used the English Penn Treebank (PTB) (Marcus et al., 1993) and the constituency structures were converted to dependency trees using the same rules as (Yamada and Matsumoto, 2003). The standard training set of PTB was spit into 2 parts: labeled training data—the first 30k sentences in section 2-21, and unlabeled training data—the remaining sentences in section 2-21. For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al., 2004) and we used the rules in (Bikel, 2004) for conversion. We also divided the standard training set into 2 parts: sentences in section 400-931 and sentences in section 1-270 are used as labeled and unla</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology: the Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3055" citStr="McClosky et al. (2006" startWordPosition="432" endWordPosition="435">erative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (Zhu, 2005). Self-training is a commonly used technique for semi-supervised learning that has been ap532 Proceedings of ACL-08: HLT, pages 532–540, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by alternating between inferring the missing label information and retraining. Recently, McClosky et al. (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al., 2006b). More recently, Haffari and Sarkar (2007) have extended the work of Abney (2004) and given a better mathematical understanding of self-training algorithms. They also show connections between these algorithms and other related machine learning algorithms. Another approach, generative probabilistic models, are a well-studied framework that can be extremely effective. However, generative models us</context>
<context position="7883" citStr="McClosky et al., 2006" startWordPosition="1169" endWordPosition="1172">abeled data. In particular, we investigate a semi-supervised approach for structured large margin training, where the objective is a combination of two convex functions, the structured large margin loss on labeled data and the least squares loss on unlabeled data. We apply the result533 Investors❑ continue❑ to❑ pour❑ cash❑ into❑ funds�moneyJ7 Figure 1: A dependency tree ing semi-supervised convex objective to dependency parsing, and obtain significant improvement over the corresponding supervised structured SVM. Note that our approach is different from the self-training technique proposed in (McClosky et al., 2006a), although both methods belong to semi-supervised training category. In the remainder of this paper, we first review the supervised structured large margin training technique. Then we introduce the standard semisupervised structured large margin objective, which is non-convex and difficult to optimize. Next we present a new semi-supervised training algorithm for structured SVMs which is convex optimization. Finally, we apply this algorithm to dependency parsing and show improved dependency parsing accuracy for both Chinese and English. 2 Dependency Parsing Model Given a sentence X = (x1, ...</context>
<context position="24427" citStr="McClosky et al., 2006" startWordPosition="3978" endWordPosition="3981">71.90 all 66.30 67.35 Table 2: Supervised and Semi-supervised Dependency Parsing Accuracy on Chinese (%) Training Test length Supervised Semi-sup Train-10 &lt; 10 87.77 89.17 &lt; 10 88.06 89.31 Train-15 &lt; 15 81.10 83.37 &lt; 10 88.78 90.61 Train-20 &lt; 15 83.00 83.87 &lt; 20 77.70 79.09 Table 3: Supervised and Semi-supervised Dependency Parsing Accuracy on English (%) 538 the unlabeled data. A key benefit of the approach is that a straightforward training algorithm can be used to obtain global solutions. Note that the results of our model are not directly comparable with previous parsing results shown in (McClosky et al., 2006a), since the parsing accuracy is measured in terms of dependency relations while their results are f-score of the bracketings implied in the phrase structure. 8 Conclusion and Future Work In this paper, we have presented a novel algorithm for semi-supervised structured large margin training. Unlike previous proposed approaches, we introduce a convex objective for the semi-supervised learning algorithm by combining a convex structured SVM loss and a convex least square loss. This new semisupervised algorithm is much more computationally efficient and can easily scale up. We have proved our hyp</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006a. Effective self-training for parsing. In Proceedings of the Human Language Technology: the Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3055" citStr="McClosky et al. (2006" startWordPosition="432" endWordPosition="435">erative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (Zhu, 2005). Self-training is a commonly used technique for semi-supervised learning that has been ap532 Proceedings of ACL-08: HLT, pages 532–540, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by alternating between inferring the missing label information and retraining. Recently, McClosky et al. (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al., 2006b). More recently, Haffari and Sarkar (2007) have extended the work of Abney (2004) and given a better mathematical understanding of self-training algorithms. They also show connections between these algorithms and other related machine learning algorithms. Another approach, generative probabilistic models, are a well-studied framework that can be extremely effective. However, generative models us</context>
<context position="7883" citStr="McClosky et al., 2006" startWordPosition="1169" endWordPosition="1172">abeled data. In particular, we investigate a semi-supervised approach for structured large margin training, where the objective is a combination of two convex functions, the structured large margin loss on labeled data and the least squares loss on unlabeled data. We apply the result533 Investors❑ continue❑ to❑ pour❑ cash❑ into❑ funds�moneyJ7 Figure 1: A dependency tree ing semi-supervised convex objective to dependency parsing, and obtain significant improvement over the corresponding supervised structured SVM. Note that our approach is different from the self-training technique proposed in (McClosky et al., 2006a), although both methods belong to semi-supervised training category. In the remainder of this paper, we first review the supervised structured large margin training technique. Then we introduce the standard semisupervised structured large margin objective, which is non-convex and difficult to optimize. Next we present a new semi-supervised training algorithm for structured SVMs which is convex optimization. Finally, we apply this algorithm to dependency parsing and show improved dependency parsing accuracy for both Chinese and English. 2 Dependency Parsing Model Given a sentence X = (x1, ...</context>
<context position="24427" citStr="McClosky et al., 2006" startWordPosition="3978" endWordPosition="3981">71.90 all 66.30 67.35 Table 2: Supervised and Semi-supervised Dependency Parsing Accuracy on Chinese (%) Training Test length Supervised Semi-sup Train-10 &lt; 10 87.77 89.17 &lt; 10 88.06 89.31 Train-15 &lt; 15 81.10 83.37 &lt; 10 88.78 90.61 Train-20 &lt; 15 83.00 83.87 &lt; 20 77.70 79.09 Table 3: Supervised and Semi-supervised Dependency Parsing Accuracy on English (%) 538 the unlabeled data. A key benefit of the approach is that a straightforward training algorithm can be used to obtain global solutions. Note that the results of our model are not directly comparable with previous parsing results shown in (McClosky et al., 2006a), since the parsing accuracy is measured in terms of dependency relations while their results are f-score of the bracketings implied in the phrase structure. 8 Conclusion and Future Work In this paper, we have presented a novel algorithm for semi-supervised structured large margin training. Unlike previous proposed approaches, we introduce a convex objective for the semi-supervised learning algorithm by combining a convex structured SVM loss and a convex least square loss. This new semisupervised algorithm is much more computationally efficient and can easily scale up. We have proved our hyp</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006b. Reranking and self-training for parser adaptation. In Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of European Chapter of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1204" citStr="McDonald and Pereira, 2006" startWordPosition="160" endWordPosition="164">iminative, convex, semi-supervised learning algorithm can be obtained that is applicable to large-scale problems. To demonstrate the benefits of this approach, we apply the technique to learning dependency parsers from combined labeled and unlabeled corpora. Using a stochastic gradient descent algorithm, a parsing model can be efficiently learned from semi-supervised data that significantly outperforms corresponding supervised methods. 1 Introduction Supervised learning algorithms still represent the state of the art approach for inferring dependency parsers from data (McDonald et al., 2005a; McDonald and Pereira, 2006; Wang et al., 2007). However, a key drawback of supervised training algorithms is their dependence on labeled data, which is usually very difficult to obtain. Perceiving the limitation of supervised learning—in particular, the heavy dependence on annotated corpora—many researchers have investigated semi-supervised learning techniques that can take both labeled and unlabeled training data as input. Following the common theme of “more data is better data” we also use both a limited labeled corpora and a plentiful unlabeled data resource. Our goal is to obtain better performance than a purely su</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of European Chapter of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1175" citStr="McDonald et al., 2005" startWordPosition="156" endWordPosition="159">st squares loss, a discriminative, convex, semi-supervised learning algorithm can be obtained that is applicable to large-scale problems. To demonstrate the benefits of this approach, we apply the technique to learning dependency parsers from combined labeled and unlabeled corpora. Using a stochastic gradient descent algorithm, a parsing model can be efficiently learned from semi-supervised data that significantly outperforms corresponding supervised methods. 1 Introduction Supervised learning algorithms still represent the state of the art approach for inferring dependency parsers from data (McDonald et al., 2005a; McDonald and Pereira, 2006; Wang et al., 2007). However, a key drawback of supervised training algorithms is their dependence on labeled data, which is usually very difficult to obtain. Perceiving the limitation of supervised learning—in particular, the heavy dependence on annotated corpora—many researchers have investigated semi-supervised learning techniques that can take both labeled and unlabeled training data as input. Following the common theme of “more data is better data” we also use both a limited labeled corpora and a plentiful unlabeled data resource. Our goal is to obtain better</context>
<context position="9969" citStr="McDonald et al., 2005" startWordPosition="1528" endWordPosition="1531">med to be projective here, which means that if there is an arc (xi —* xj), then xi is an ancestor of all the words between xi and xj.1 Let -b(X) denote the set of all the directed, projective trees that span on X. The parser’s goal is then to find the most preferred parse; that is, a projective tree, Y E 4b(X), that obtains the highest “score”. In particular, one would assume that the score of a complete spanning tree Y for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair) (Eisner, 1996; Eisner and Satta, 1999; McDonald et al., 2005a). Given this assumption, the parsing problem reduces to find where the score(xi —* xj) can depend on any measurable property of xi and xj within the sentence X. This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Eisner, 1996; Wang et al., 2005) as well as non-probabilistic models (McDonald et al., 2005a). For standard scoring functions, particularly those used in non-generative models, we further assume that the score of each link in (1) can be decomposed into a weighted linear combination of features score(xi —* xj)</context>
<context position="12227" citStr="McDonald et al., 2005" startWordPosition="1922" endWordPosition="1925">on 2; and Δ(Li,k, Yi) is a measure of distance between the two trees Li,k and Yi. This is an application of the structured large margin training approach first proposed in (Taskar et al., 2003) and (Tsochantaridis et al., 2004). Using the techniques of Hastie et al. (2004) one can show that minimizing the objective (3) is equivalent to solving the quadratic program β2 θTθ + eTξ subject to ξi,k &gt; Δ(Li,k, Yi) − diff(θ, Yi, Li,k) ξi,k &gt; 0 for all i, Li,k E Φ(Xi) (4) where e denotes the vector of all 1’s and ξ represents slack variables. This approach corresponds to the training problem posed in (McDonald et al., 2005a) and has yielded the best published results for English dependency parsing. To compare with the new semi-supervised approach we will present in Section 5 below, we reimplemented the supervised structured large margin training approach in the experiments in Section 7. More specifically, we solve the following quadratic program, which is based on Equation (3) Δ(Li,m,n,Yi,m,n) − diff(θ, Yi,m,n, Li,m,n) (5) where diff(θ, Yi,m,n, Li,m,n) = score(θ, Yi,m,n) − score(θ, Li,m,n) and k is the sentence length. We represent a dependency tree as a k x k adjacency matrix. In the adjacency matrix, the valu</context>
<context position="21776" citStr="McDonald et al., 2005" startWordPosition="3541" endWordPosition="3545"> labeled and unlabeled data respectively. For both English and Chinese, we adopted the standard development and test sets throughout the literature. As listed in Table 1 with greater detail, we experimented with sets of data with different sentence length: PTB-10/CTB4-10, PTB-15/CTB4-15, PTB-20/CTB4-20, CTB4-40 and CTB4, which contain sentences with up to 10, 15, 20, 40 and all words respectively. Features For simplicity, in current work, we only used two sets of features—word-pair and tag-pair indicator features, which are a subset of features used by other researchers on dependency parsing (McDonald et al., 2005a; Wang et al., 2007). Although our algorithms can take arbitrary features, by only using these simple features, we already obtained very promising results on dependency parsing using both the supervised and semi-supervised approaches. Using the full set of features described in (McDonald et al., 2005a; Wang et al., 2007) and comparing the corresponding dependency parsing 537 Training(l/ul) 3026/1016 PTB-10 Dev 163 Test 270 Training 7303/2370 English PTB-15 Dev 421 Test 603 Training 12519/4003 PTB-20 Dev 725 Test 1034 Training(l/ul) 642/347 CTB4-10 Dev 61 Test 40 Training 1262/727 CTB4-15 Dev </context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technologies and Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1175" citStr="McDonald et al., 2005" startWordPosition="156" endWordPosition="159">st squares loss, a discriminative, convex, semi-supervised learning algorithm can be obtained that is applicable to large-scale problems. To demonstrate the benefits of this approach, we apply the technique to learning dependency parsers from combined labeled and unlabeled corpora. Using a stochastic gradient descent algorithm, a parsing model can be efficiently learned from semi-supervised data that significantly outperforms corresponding supervised methods. 1 Introduction Supervised learning algorithms still represent the state of the art approach for inferring dependency parsers from data (McDonald et al., 2005a; McDonald and Pereira, 2006; Wang et al., 2007). However, a key drawback of supervised training algorithms is their dependence on labeled data, which is usually very difficult to obtain. Perceiving the limitation of supervised learning—in particular, the heavy dependence on annotated corpora—many researchers have investigated semi-supervised learning techniques that can take both labeled and unlabeled training data as input. Following the common theme of “more data is better data” we also use both a limited labeled corpora and a plentiful unlabeled data resource. Our goal is to obtain better</context>
<context position="9969" citStr="McDonald et al., 2005" startWordPosition="1528" endWordPosition="1531">med to be projective here, which means that if there is an arc (xi —* xj), then xi is an ancestor of all the words between xi and xj.1 Let -b(X) denote the set of all the directed, projective trees that span on X. The parser’s goal is then to find the most preferred parse; that is, a projective tree, Y E 4b(X), that obtains the highest “score”. In particular, one would assume that the score of a complete spanning tree Y for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair) (Eisner, 1996; Eisner and Satta, 1999; McDonald et al., 2005a). Given this assumption, the parsing problem reduces to find where the score(xi —* xj) can depend on any measurable property of xi and xj within the sentence X. This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Eisner, 1996; Wang et al., 2005) as well as non-probabilistic models (McDonald et al., 2005a). For standard scoring functions, particularly those used in non-generative models, we further assume that the score of each link in (1) can be decomposed into a weighted linear combination of features score(xi —* xj)</context>
<context position="12227" citStr="McDonald et al., 2005" startWordPosition="1922" endWordPosition="1925">on 2; and Δ(Li,k, Yi) is a measure of distance between the two trees Li,k and Yi. This is an application of the structured large margin training approach first proposed in (Taskar et al., 2003) and (Tsochantaridis et al., 2004). Using the techniques of Hastie et al. (2004) one can show that minimizing the objective (3) is equivalent to solving the quadratic program β2 θTθ + eTξ subject to ξi,k &gt; Δ(Li,k, Yi) − diff(θ, Yi, Li,k) ξi,k &gt; 0 for all i, Li,k E Φ(Xi) (4) where e denotes the vector of all 1’s and ξ represents slack variables. This approach corresponds to the training problem posed in (McDonald et al., 2005a) and has yielded the best published results for English dependency parsing. To compare with the new semi-supervised approach we will present in Section 5 below, we reimplemented the supervised structured large margin training approach in the experiments in Section 7. More specifically, we solve the following quadratic program, which is based on Equation (3) Δ(Li,m,n,Yi,m,n) − diff(θ, Yi,m,n, Li,m,n) (5) where diff(θ, Yi,m,n, Li,m,n) = score(θ, Yi,m,n) − score(θ, Li,m,n) and k is the sentence length. We represent a dependency tree as a k x k adjacency matrix. In the adjacency matrix, the valu</context>
<context position="21776" citStr="McDonald et al., 2005" startWordPosition="3541" endWordPosition="3545"> labeled and unlabeled data respectively. For both English and Chinese, we adopted the standard development and test sets throughout the literature. As listed in Table 1 with greater detail, we experimented with sets of data with different sentence length: PTB-10/CTB4-10, PTB-15/CTB4-15, PTB-20/CTB4-20, CTB4-40 and CTB4, which contain sentences with up to 10, 15, 20, 40 and all words respectively. Features For simplicity, in current work, we only used two sets of features—word-pair and tag-pair indicator features, which are a subset of features used by other researchers on dependency parsing (McDonald et al., 2005a; Wang et al., 2007). Although our algorithms can take arbitrary features, by only using these simple features, we already obtained very promising results on dependency parsing using both the supervised and semi-supervised approaches. Using the full set of features described in (McDonald et al., 2005a; Wang et al., 2007) and comparing the corresponding dependency parsing 537 Training(l/ul) 3026/1016 PTB-10 Dev 163 Test 270 Training 7303/2370 English PTB-15 Dev 421 Test 603 Training 12519/4003 PTB-20 Dev 725 Test 1034 Training(l/ul) 642/347 CTB4-10 Dev 61 Test 40 Training 1262/727 CTB4-15 Dev </context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of Human Language Technologies and Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
</authors>
<title>Chinese Treebank 4.0. Linguistic Data Consortium.</title>
<date>2004</date>
<marker>Palmer, 2004</marker>
<rawString>M. Palmer et al. 2004. Chinese Treebank 4.0. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Smith</author>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4266" citStr="Smith and Eisner (2005)" startWordPosition="607" endWordPosition="610">ve models use the EM algorithm for parameter estimation in the presence of missing labels, which is notoriously prone to getting stuck in poor local optima. Moreover, EM optimizes a marginal likelihood score that is not discriminative. Consequently, most previous work that has attempted semi-supervised or unsupervised approaches to parsing have not produced results beyond the state of the art supervised results (Klein and Manning, 2002; Klein and Manning, 2004). Subsequently, alternative estimation strategies for unsupervised learning have been proposed, such as Contrastive Estimation (CE) by Smith and Eisner (2005). Contrastive Estimation is a generalization of EM, by defining a notion of learner guidance. It makes use of a set of examples (its neighborhood) that are similar in some way to an observed example, requiring the learner to move probability mass to a given example, taking only from the example’s neighborhood. Nevertheless, CE still suffers from shortcomings, including local minima. In recent years, SVMs have demonstrated state of the art results in many supervised learning tasks. As a result, many researchers have put effort on developing algorithms for semi-supervised SVMs (S3VMs) (Bennett a</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. Smith and J. Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
<author>M Osborne</author>
<author>A Sarkar</author>
<author>S Clark</author>
<author>R Hwa</author>
<author>J Hockenmaier</author>
<author>P Ruhlen</author>
<author>S Baker</author>
<author>J Crim</author>
</authors>
<title>Bootstrapping statistical parsers from small datasets.</title>
<date>2003</date>
<booktitle>In Proceedings of the European Chapter of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>331--338</pages>
<contexts>
<context position="2880" citStr="Steedman et al., 2003" startWordPosition="406" endWordPosition="409">tigated in the literature (Bennett and Demiriz, 1998; Zhu et al., 2003; Altun et al., 2005; Mann and McCallum, 2007). Among the most prominent approaches are self-training, generative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (Zhu, 2005). Self-training is a commonly used technique for semi-supervised learning that has been ap532 Proceedings of ACL-08: HLT, pages 532–540, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by alternating between inferring the missing label information and retraining. Recently, McClosky et al. (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al., 2006b). More recently, Haffari and Sarkar (2007) have extended the work of Abney (2004) and given a better mathematical understanding of self-training algorithms. They also show connections between these algorithms and other rela</context>
</contexts>
<marker>Steedman, Osborne, Sarkar, Clark, Hwa, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa, J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003. Bootstrapping statistical parsers from small datasets. In Proceedings of the European Chapter of the Annual Meeting of the Association for Computational Linguistics, pages 331–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Maxmargin Markov networks.</title>
<date>2003</date>
<booktitle>In Proceedings ofAdvances in Neural Information Processing Systems 16.</booktitle>
<contexts>
<context position="11799" citStr="Taskar et al., 2003" startWordPosition="1843" endWordPosition="1846">o), although in the real word, most languages are non-projective. Y ∗ = arg max score(Y |X) (1) YEID(X) 11 = arg max score(xi —* xj) YE4D(X) (xi→xj)EY 534 β2 θTθ + (3) max(Δ(Li,k, Yi) − diff(θ, Yi, Li,k)) Li,k where Yi is the target tree for sentence Xi; Li,k ranges over all possible alternative k trees in Φ(Xi); diff(θ, Yi, Li,k) = score(θ, Yi) − score(θ, Li,k); score(θ, Yi) = P(xm→xn)EYi θ · f(xm —* xn), as shown in Section 2; and Δ(Li,k, Yi) is a measure of distance between the two trees Li,k and Yi. This is an application of the structured large margin training approach first proposed in (Taskar et al., 2003) and (Tsochantaridis et al., 2004). Using the techniques of Hastie et al. (2004) one can show that minimizing the objective (3) is equivalent to solving the quadratic program β2 θTθ + eTξ subject to ξi,k &gt; Δ(Li,k, Yi) − diff(θ, Yi, Li,k) ξi,k &gt; 0 for all i, Li,k E Φ(Xi) (4) where e denotes the vector of all 1’s and ξ represents slack variables. This approach corresponds to the training problem posed in (McDonald et al., 2005a) and has yielded the best published results for English dependency parsing. To compare with the new semi-supervised approach we will present in Section 5 below, we reimpl</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Maxmargin Markov networks. In Proceedings ofAdvances in Neural Information Processing Systems 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="10900" citStr="Taskar et al., 2004" startWordPosition="1681" endWordPosition="1684">al., 2005) as well as non-probabilistic models (McDonald et al., 2005a). For standard scoring functions, particularly those used in non-generative models, we further assume that the score of each link in (1) can be decomposed into a weighted linear combination of features score(xi —* xj) = 0 · f(xi —* xj) (2) where f(xi —* xj) is a feature vector for the link (xi —* xj), and 0 are the weight parameters to be estimated during training. 3 Supervised Structured Large Margin Training Supervised structured large margin training approaches have been applied to parsing and produce promising results (Taskar et al., 2004; McDonald et al., 2005a; Wang et al., 2006). In particular, structured large margin training can be expressed as minimizing a regularized loss (Hastie et al., 2004), as shown below: 1We assume all the dependency trees are projective in our work (just as some other researchers do), although in the real word, most languages are non-projective. Y ∗ = arg max score(Y |X) (1) YEID(X) 11 = arg max score(xi —* xj) YE4D(X) (xi→xj)EY 534 β2 θTθ + (3) max(Δ(Li,k, Yi) − diff(θ, Yi, Li,k)) Li,k where Yi is the target tree for sentence Xi; Li,k ranges over all possible alternative k trees in Φ(Xi); diff(θ</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning. 2004. Max-margin parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of International Conference on Machine Learning.</booktitle>
<contexts>
<context position="11833" citStr="Tsochantaridis et al., 2004" startWordPosition="1848" endWordPosition="1851">ord, most languages are non-projective. Y ∗ = arg max score(Y |X) (1) YEID(X) 11 = arg max score(xi —* xj) YE4D(X) (xi→xj)EY 534 β2 θTθ + (3) max(Δ(Li,k, Yi) − diff(θ, Yi, Li,k)) Li,k where Yi is the target tree for sentence Xi; Li,k ranges over all possible alternative k trees in Φ(Xi); diff(θ, Yi, Li,k) = score(θ, Yi) − score(θ, Li,k); score(θ, Yi) = P(xm→xn)EYi θ · f(xm —* xn), as shown in Section 2; and Δ(Li,k, Yi) is a measure of distance between the two trees Li,k and Yi. This is an application of the structured large margin training approach first proposed in (Taskar et al., 2003) and (Tsochantaridis et al., 2004). Using the techniques of Hastie et al. (2004) one can show that minimizing the objective (3) is equivalent to solving the quadratic program β2 θTθ + eTξ subject to ξi,k &gt; Δ(Li,k, Yi) − diff(θ, Yi, Li,k) ξi,k &gt; 0 for all i, Li,k E Φ(Xi) (4) where e denotes the vector of all 1’s and ξ represents slack variables. This approach corresponds to the training problem posed in (McDonald et al., 2005a) and has yielded the best published results for English dependency parsing. To compare with the new semi-supervised approach we will present in Section 5 below, we reimplemented the supervised structured </context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Wang</author>
<author>D Schuurmans</author>
<author>D Lin</author>
</authors>
<title>Strictly lexical dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="10291" citStr="Wang et al., 2005" startWordPosition="1579" endWordPosition="1582"> the highest “score”. In particular, one would assume that the score of a complete spanning tree Y for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair) (Eisner, 1996; Eisner and Satta, 1999; McDonald et al., 2005a). Given this assumption, the parsing problem reduces to find where the score(xi —* xj) can depend on any measurable property of xi and xj within the sentence X. This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Eisner, 1996; Wang et al., 2005) as well as non-probabilistic models (McDonald et al., 2005a). For standard scoring functions, particularly those used in non-generative models, we further assume that the score of each link in (1) can be decomposed into a weighted linear combination of features score(xi —* xj) = 0 · f(xi —* xj) (2) where f(xi —* xj) is a feature vector for the link (xi —* xj), and 0 are the weight parameters to be estimated during training. 3 Supervised Structured Large Margin Training Supervised structured large margin training approaches have been applied to parsing and produce promising results (Taskar et </context>
</contexts>
<marker>Wang, Schuurmans, Lin, 2005</marker>
<rawString>Q. Wang, D. Schuurmans, and D. Lin. 2005. Strictly lexical dependency parsing. In Proceedings of the International Workshop on Parsing Technologies, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Wang</author>
<author>C Cherry</author>
<author>D Lizotte</author>
<author>D Schuurmans</author>
</authors>
<title>Improved large margin dependency parsing via local constraints and Laplacian regularization.</title>
<date>2006</date>
<booktitle>In Proceedings of The Conference on Computational Natural Language Learning,</booktitle>
<pages>21--28</pages>
<contexts>
<context position="10944" citStr="Wang et al., 2006" startWordPosition="1689" endWordPosition="1692">s (McDonald et al., 2005a). For standard scoring functions, particularly those used in non-generative models, we further assume that the score of each link in (1) can be decomposed into a weighted linear combination of features score(xi —* xj) = 0 · f(xi —* xj) (2) where f(xi —* xj) is a feature vector for the link (xi —* xj), and 0 are the weight parameters to be estimated during training. 3 Supervised Structured Large Margin Training Supervised structured large margin training approaches have been applied to parsing and produce promising results (Taskar et al., 2004; McDonald et al., 2005a; Wang et al., 2006). In particular, structured large margin training can be expressed as minimizing a regularized loss (Hastie et al., 2004), as shown below: 1We assume all the dependency trees are projective in our work (just as some other researchers do), although in the real word, most languages are non-projective. Y ∗ = arg max score(Y |X) (1) YEID(X) 11 = arg max score(xi —* xj) YE4D(X) (xi→xj)EY 534 β2 θTθ + (3) max(Δ(Li,k, Yi) − diff(θ, Yi, Li,k)) Li,k where Yi is the target tree for sentence Xi; Li,k ranges over all possible alternative k trees in Φ(Xi); diff(θ, Yi, Li,k) = score(θ, Yi) − score(θ, Li,k);</context>
</contexts>
<marker>Wang, Cherry, Lizotte, Schuurmans, 2006</marker>
<rawString>Q. Wang, C. Cherry, D. Lizotte, and D. Schuurmans. 2006. Improved large margin dependency parsing via local constraints and Laplacian regularization. In Proceedings of The Conference on Computational Natural Language Learning, pages 21–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Wang</author>
<author>D Lin</author>
<author>D Schuurmans</author>
</authors>
<title>Simple training of dependency parsers via structured boosting.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1756--1762</pages>
<contexts>
<context position="1224" citStr="Wang et al., 2007" startWordPosition="165" endWordPosition="168">rvised learning algorithm can be obtained that is applicable to large-scale problems. To demonstrate the benefits of this approach, we apply the technique to learning dependency parsers from combined labeled and unlabeled corpora. Using a stochastic gradient descent algorithm, a parsing model can be efficiently learned from semi-supervised data that significantly outperforms corresponding supervised methods. 1 Introduction Supervised learning algorithms still represent the state of the art approach for inferring dependency parsers from data (McDonald et al., 2005a; McDonald and Pereira, 2006; Wang et al., 2007). However, a key drawback of supervised training algorithms is their dependence on labeled data, which is usually very difficult to obtain. Perceiving the limitation of supervised learning—in particular, the heavy dependence on annotated corpora—many researchers have investigated semi-supervised learning techniques that can take both labeled and unlabeled training data as input. Following the common theme of “more data is better data” we also use both a limited labeled corpora and a plentiful unlabeled data resource. Our goal is to obtain better performance than a purely supervised approach wi</context>
<context position="21797" citStr="Wang et al., 2007" startWordPosition="3546" endWordPosition="3549">ata respectively. For both English and Chinese, we adopted the standard development and test sets throughout the literature. As listed in Table 1 with greater detail, we experimented with sets of data with different sentence length: PTB-10/CTB4-10, PTB-15/CTB4-15, PTB-20/CTB4-20, CTB4-40 and CTB4, which contain sentences with up to 10, 15, 20, 40 and all words respectively. Features For simplicity, in current work, we only used two sets of features—word-pair and tag-pair indicator features, which are a subset of features used by other researchers on dependency parsing (McDonald et al., 2005a; Wang et al., 2007). Although our algorithms can take arbitrary features, by only using these simple features, we already obtained very promising results on dependency parsing using both the supervised and semi-supervised approaches. Using the full set of features described in (McDonald et al., 2005a; Wang et al., 2007) and comparing the corresponding dependency parsing 537 Training(l/ul) 3026/1016 PTB-10 Dev 163 Test 270 Training 7303/2370 English PTB-15 Dev 421 Test 603 Training 12519/4003 PTB-20 Dev 725 Test 1034 Training(l/ul) 642/347 CTB4-10 Dev 61 Test 40 Training 1262/727 CTB4-15 Dev 112 Test 83 Training </context>
</contexts>
<marker>Wang, Lin, Schuurmans, 2007</marker>
<rawString>Q. Wang, D. Lin, and D. Schuurmans. 2007. Simple training of dependency parsers via structured boosting. In Proceedings of the International Joint Conference on Artificial Intelligence, pages 1756–1762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Xu</author>
<author>D Schuurmans</author>
</authors>
<title>Unsupervised and semi-supervised multi-class support vector machines.</title>
<date>2005</date>
<booktitle>In Proceedings the Association for the Advancement of Artificial Intelligence.</booktitle>
<contexts>
<context position="5245" citStr="Xu and Schuurmans, 2005" startWordPosition="761" endWordPosition="765">mings, including local minima. In recent years, SVMs have demonstrated state of the art results in many supervised learning tasks. As a result, many researchers have put effort on developing algorithms for semi-supervised SVMs (S3VMs) (Bennett and Demiriz, 1998; Altun et al., 2005). However, the standard objective of an S3VM is non-convex on the unlabeled data, thus requiring sophisticated global optimization heuristics to obtain reasonable solutions. A number of researchers have proposed several efficient approximation algorithms for S3VMs (Bennett and Demiriz, 1998; Chapelle and Zien, 2005; Xu and Schuurmans, 2005). For example, Chapelle and Zien (2005) propose an algorithm that smoothes the objective with a Gaussian function, and then performs a gradient descent search in the primal space to achieve a local solution. An alternative approach is proposed by Xu and Schuurmans (2005) who formulate a semi-definite programming (SDP) approach. In particular, they present an algorithm for multiclass unsupervised and semi-supervised SVM learning, which relaxes the original non-convex objective into a close convex approximation, thereby allowing a global solution to be obtained. However, the computational cost o</context>
<context position="14513" citStr="Xu and Schuurmans, 2005" startWordPosition="2292" endWordPosition="2295">rty of the second term in the objective function. We will propose a novel approach which can deal with this problem. We introduce an efficient approximation— least squares loss—for the structured large margin loss on unlabeled data below. 5 Semi-supervised Convex Training for Structured SVM Although semi-supervised structured SVM learning has been an active research area, semi-supervised structured SVMs have not been used in many real applications to date. The main reason is that most available semi-supervised large margin learning approaches are non-convex or computationally expensive (e.g. (Xu and Schuurmans, 2005)). These techniques are difficult to implement and extremely hard to scale up. We present a semi-supervised algorithm Δ(Li,m,n,Yi,m,n) (7) −diff(θ, Yi,m,n, Li,m,n) k X n=1 = max L k X m=1 min θ X i min 0,� min α X max k k θ 2 θTθ + L X X i m=1 n=1 α2 θTθ + min θ XN i=1 U X j=1 + min Yj where 535 for structured large margin training, whose objective is a combination of two convex terms: the supervised structured large margin loss on labeled data and the cheap least squares loss on unlabeled data. The combined objective is still convex, easy to optimize and much cheaper to implement. 5.1 Least S</context>
</contexts>
<marker>Xu, Schuurmans, 2005</marker>
<rawString>L. Xu and D. Schuurmans. 2005. Unsupervised and semi-supervised multi-class support vector machines. In Proceedings the Association for the Advancement of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="20692" citStr="Yamada and Matsumoto, 2003" startWordPosition="3369" endWordPosition="3373">d an efficient training algorithm for achieving a global optimum, we now investigate its effectiveness for dependency parsing. In particular, we investigate the accuracy of the results it produces. We applied the resulting algorithm to learn dependency parsers for both English and Chinese. 7.1 Experimental Design Data Sets Since we use a semi-supervised approach, both labeled and unlabeled training data are needed. For experiment on English, we used the English Penn Treebank (PTB) (Marcus et al., 1993) and the constituency structures were converted to dependency trees using the same rules as (Yamada and Matsumoto, 2003). The standard training set of PTB was spit into 2 parts: labeled training data—the first 30k sentences in section 2-21, and unlabeled training data—the remaining sentences in section 2-21. For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al., 2004) and we used the rules in (Bikel, 2004) for conversion. We also divided the standard training set into 2 parts: sentences in section 400-931 and sentences in section 1-270 are used as labeled and unlabeled data respectively. For both English and Chinese, we adopted the standard development and test sets throughout the </context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings ofthe Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="2840" citStr="Yarowsky, 1995" startWordPosition="402" endWordPosition="403">ining algorithms have been investigated in the literature (Bennett and Demiriz, 1998; Zhu et al., 2003; Altun et al., 2005; Mann and McCallum, 2007). Among the most prominent approaches are self-training, generative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (Zhu, 2005). Self-training is a commonly used technique for semi-supervised learning that has been ap532 Proceedings of ACL-08: HLT, pages 532–540, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by alternating between inferring the missing label information and retraining. Recently, McClosky et al. (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al., 2006b). More recently, Haffari and Sarkar (2007) have extended the work of Abney (2004) and given a better mathematical understanding of self-training algorithms. They also show connections</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings ofthe Annual Meeting of the Association for Computational Linguistics, pages 189–196, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>Semisupervised learning using Gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proceedings ofInternational Conference on Machine Learning.</booktitle>
<contexts>
<context position="2328" citStr="Zhu et al., 2003" startWordPosition="329" endWordPosition="332">plentiful unlabeled data resource. Our goal is to obtain better performance than a purely supervised approach without unreasonable computational effort. Unfortunately, although significant recent progress has been made in the area of semi-supervised learning, the performance of semi-supervised learning algorithms still fall far short of expectations, particularly in challenging real-world tasks such as natural language parsing or machine translation. A large number of distinct approaches to semisupervised training algorithms have been investigated in the literature (Bennett and Demiriz, 1998; Zhu et al., 2003; Altun et al., 2005; Mann and McCallum, 2007). Among the most prominent approaches are self-training, generative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (Zhu, 2005). Self-training is a commonly used technique for semi-supervised learning that has been ap532 Proceedings of ACL-08: HLT, pages 532–540, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised le</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semisupervised learning using Gaussian fields and harmonic functions. In Proceedings ofInternational Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
</authors>
<title>Semi-supervised learning literature survey.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Computer Sciences, University of Wisconsin-Madison.</institution>
<contexts>
<context position="2556" citStr="Zhu, 2005" startWordPosition="363" endWordPosition="364">semi-supervised learning, the performance of semi-supervised learning algorithms still fall far short of expectations, particularly in challenging real-world tasks such as natural language parsing or machine translation. A large number of distinct approaches to semisupervised training algorithms have been investigated in the literature (Bennett and Demiriz, 1998; Zhu et al., 2003; Altun et al., 2005; Mann and McCallum, 2007). Among the most prominent approaches are self-training, generative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (Zhu, 2005). Self-training is a commonly used technique for semi-supervised learning that has been ap532 Proceedings of ACL-08: HLT, pages 532–540, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by alternating between inferring the missing label information and retraining. Recently, McClosky et al. (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained</context>
</contexts>
<marker>Zhu, 2005</marker>
<rawString>X. Zhu. 2005. Semi-supervised learning literature survey. Technical report, Computer Sciences, University of Wisconsin-Madison.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>