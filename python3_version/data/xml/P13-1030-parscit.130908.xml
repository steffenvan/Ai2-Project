<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.984288">
A Context Free TAG Variant
</title>
<author confidence="0.982044">
Ben Swanson
</author>
<affiliation confidence="0.964749">
Brown University
</affiliation>
<address confidence="0.782114">
Providence, RI
</address>
<email confidence="0.984678">
chonger@cs.brown.edu
</email>
<author confidence="0.992552">
Eugene Charniak
</author>
<affiliation confidence="0.989971">
Brown University
</affiliation>
<address confidence="0.84953">
Providence, RI
</address>
<email confidence="0.99909">
ec@cs.brown.edu
</email>
<sectionHeader confidence="0.994804" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999862388888889">
We propose a new variant of Tree-
Adjoining Grammar that allows adjunc-
tion of full wrapping trees but still bears
only context-free expressivity. We provide
a transformation to context-free form, and
a further reduction in probabilistic model
size through factorization and pooling of
parameters. This collapsed context-free
form is used to implement efficient gram-
mar estimation and parsing algorithms.
We perform parsing experiments the Penn
Treebank and draw comparisons to Tree-
Substitution Grammars and between dif-
ferent variations in probabilistic model de-
sign. Examination of the most probable
derivations reveals examples of the lin-
guistically relevant structure that our vari-
ant makes possible.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999864352941176">
While it is widely accepted that natural language
is not context-free, practical limitations of ex-
isting algorithms motivate Context-Free Gram-
mars (CFGs) as a good balance between model-
ing power and asymptotic performance (Charniak,
1996). In constituent-based parsing work, the pre-
vailing technique to combat this divide between
efficient models and real world data has been to
selectively strengthen the dependencies in a CFG
by increasing the grammar size through methods
such as symbol refinement (Petrov et al., 2006).
Another approach is to employ a more power-
ful grammatical formalism and devise constraints
and transformations that allow use of essential ef-
ficient algorithms such as the Inside-Outside al-
gorithm (Lari and Young, 1990) and CYK pars-
ing. Tree-Adjoining Grammar (TAG) is a natural
</bodyText>
<note confidence="0.892146666666667">
Elif Yamangil
Harvard University
Cambridge, MA
</note>
<email confidence="0.744467">
elif@eecs.harvard.edu
</email>
<author confidence="0.892565">
Stuart Shieber
</author>
<affiliation confidence="0.885996">
Harvard University
</affiliation>
<address confidence="0.681049">
Cambridge, MA
</address>
<email confidence="0.98111">
shieber@eecs.harvard.edu
</email>
<bodyText confidence="0.999619888888889">
starting point for such methods as it is the canoni-
cal member of the mildly context-sensitive family,
falling just above CFGs in the hierarchy of for-
mal grammars. TAG has a crucial advantage over
CFGs in its ability to represent long distance in-
teractions in the face of the interposing variations
that commonly manifest in natural language (Joshi
and Schabes, 1997). Consider, for example, the
sentences
These pretzels are making me thirsty.
These pretzels are not making me thirsty.
These pretzels that I ate are making me thirsty.
Using a context-free language model with
proper phrase bracketing, the connection between
the words pretzels and thirsty must be recorded
with three separate patterns, which can lead to
poor generalizability and unreliable sparse fre-
quency estimates in probabilistic models. While
these problems can be overcome to some extent
with large amounts of data, redundant representa-
tion of patterns is particularly undesirable for sys-
tems that seek to extract coherent and concise in-
formation from text.
TAG allows a linguistically motivated treatment
of the example sentences above by generating the
last two sentences through modification of the
first, applying operations corresponding to nega-
tion and the use of a subordinate clause. Un-
fortunately, the added expressive power of TAG
comes with O(n6) time complexity for essential
algorithms on sentences of length n, as opposed to
O(n3) for the CFG (Schabes, 1990). This makes
TAG infeasible to analyze real world data in a rea-
sonable time frame.
In this paper, we define OSTAG, a new way to
constrain TAG in a conceptually simple way so
</bodyText>
<page confidence="0.970169">
302
</page>
<note confidence="0.917543">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 302–310,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.997793733333333">
S
NP
VP
NP VP NP
NN
NNS
VBP
RB
VP
NP
DT
the lack computers do not
NP NP PP VP
NP PP PRP IN PRP VB
I of them fear
</figure>
<figureCaption confidence="0.997796">
Figure 1: A simple Tree-Substitution Grammar using S as its start symbol. This grammar derives the
sentences from a quote of Isaac Asimov’s - “I do not fear computers. I fear the lack of them.”
</figureCaption>
<bodyText confidence="0.982445058823529">
that it can be reduced to a CFG, allowing the use of
traditional cubic-time algorithms. The reduction is
reversible, so that the original TAG derivation can
be recovered exactly from the CFG parse. We pro-
vide this reduction in detail below and highlight
the compression afforded by this TAG variant on
synthetic formal languages.
We evaluate OSTAG on the familiar task of
parsing the Penn Treebank. Using an automati-
cally induced Tree-Substitution Grammar (TSG),
we heuristically extract an OSTAG and estimate
its parameters from data using models with var-
ious reduced probabilistic models of adjunction.
We contrast these models and investigate the use
of adjunction in the most probable derivations of
the test corpus, demonstating the superior model-
ing performance of OSTAG over TSG.
</bodyText>
<sectionHeader confidence="0.890534" genericHeader="introduction">
2 TAG and Variants
</sectionHeader>
<bodyText confidence="0.999996571428571">
Here we provide a short history of the relevant
work in related grammar formalisms, leading up
to a definition of OSTAG. We start with context-
free grammars, the components of which are
(N, T, R, S), where N and T are the sets of non-
terminal and terminal symbols respectively, and S
is a distinguished nonterminal, the start symbol.
The rules R can be thought of as elementary trees
of depth 1, which are combined by substituting a
derived tree rooted at a nonterminal X at some leaf
node in an elementary tree with a frontier node
labeled with that same nonterminal. The derived
trees rooted at the start symbol S are taken to be
the trees generated by the grammar.
</bodyText>
<subsectionHeader confidence="0.986882">
2.1 Tree-Substitution Grammar
</subsectionHeader>
<bodyText confidence="0.999202484848485">
By generalizing CFG to allow elementary trees in
R to be of depth greater than or equal to 1, we
get the Tree-Substitution Grammar. TSG remains
in the family of context-free grammars, as can be
easily seen by the removal of the internal nodes
in all elementary trees; what is left is a CFG that
generates the same language. As a reversible al-
ternative that preserves the internal structure, an-
notation of each internal node with a unique index
creates a large number of deterministic CFG rules
that record the structure of the original elementary
trees. A more compact CFG representation can be
obtained by marking each node in each elemen-
tary tree with a signature of its subtree. This trans-
form, presented by Goodman (2003), can rein in
the grammar constant G, as the crucial CFG algo-
rithms for a sentence of length n have complexity
O(Gn3).
A simple probabilistic model for a TSG is a set
of multinomials, one for each nonterminal in N
corresponding to its possible substitutions in R. A
more flexible model allows a potentially infinite
number of substitution rules using a Dirichlet Pro-
cess (Cohn et al., 2009; Cohn and Blunsom, 2010).
This model has proven effective for grammar in-
duction via Markov Chain Monte Carlo (MCMC),
in which TSG derivations of the training set are re-
peatedly sampled to find frequently occurring el-
ementary trees. A straightforward technique for
induction of a finite TSG is to perform this non-
parametric induction and select the set of rules that
appear in at least one sampled derivation at one or
several of the final iterations.
</bodyText>
<subsectionHeader confidence="0.999563">
2.2 Tree-Adjoining Grammar
</subsectionHeader>
<bodyText confidence="0.99990825">
Tree-adjoining grammar (TAG) (Joshi, 1985;
Joshi, 1987; Joshi and Schabes, 1997) is an exten-
sion of TSG defined by a tuple (N, T, R, A, S),
and differs from TSG only in the addition of a
</bodyText>
<page confidence="0.998367">
303
</page>
<figure confidence="0.998400545454545">
VP
always VP
VP* quickly
+ S
⇒ S
NP VP
always VP
runs
NP VP
VP quickly
runs
</figure>
<figureCaption confidence="0.776112">
Figure 2: The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle)
to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is
denoted with an asterisk. The OSTAG constraint would disallow further adjunction at the bold VP node
</figureCaption>
<bodyText confidence="0.985600736842105">
only, as it is along the spine of the auxiliary tree.
set of auxiliary trees A and the adjunction oper-
ation that governs their use. An auxiliary tree α
is an elementary tree containing a single distin-
guished nonterminal leaf, the foot node, with the
same symbol as the root of α. An auxiliary tree
with root and foot node X can be adjoined into an
internal node of an elementary tree labeled with
X by splicing the auxiliary tree in at that internal
node, as pictured in Figure 2. We refer to the path
between the root and foot nodes in an auxiliary
tree as the spine of the tree.
As mentioned above, the added power afforded
by adjunction comes at a serious price in time
complexity. As such, probabilistic modeling for
TAG in its original form is uncommon. However,
a large effort in non-probabilistic grammar induc-
tion has been performed through manual annota-
tion with the XTAG project(Doran et al., 1994).
</bodyText>
<subsectionHeader confidence="0.992152">
2.3 Tree Insertion Grammar
</subsectionHeader>
<bodyText confidence="0.999675814814815">
Tree Insertion Grammars (TIGs) are a longstand-
ing compromise between the intuitive expressivity
of TAG and the algorithmic simplicity of CFGs.
Schabes and Waters (1995) showed that by re-
stricting the form of the auxiliary trees in A and
the set of auxiliary trees that may adjoin at par-
ticular nodes, a TAG generates only context-free
languages. The TIG restriction on auxiliary trees
states that the foot node must occur as either the
leftmost or rightmost leaf node. This introduces
an important distinction between left, right, and
wrapping auxiliary trees, of which only the first
two are allowed in TIG. Furthermore, TIG disal-
lows adjunction of left auxiliary trees on the spines
of right auxiliary trees, and vice versa. This is
to prevent the construction of wrapping auxiliary
trees, whose removal is essential for the simplified
complexity of TIG.
Several probabilistic models have been pro-
posed for TIG. While earlier approaches such as
Hwa (1998) and Chiang (2000) relied on hueristic
induction methods, they were nevertheless sucess-
ful at parsing. Later approaches (Shindo et al.,
2011; Yamangil and Shieber, 2012) were able to
extend the non-parametric modeling of TSGs to
TIG, providing methods for both modeling and
grammar induction.
</bodyText>
<subsectionHeader confidence="0.902849">
2.4 OSTAG
</subsectionHeader>
<bodyText confidence="0.999979277777778">
Our new TAG variant is extremely simple. We al-
low arbitrary initial and auxiliary trees, and place
only one restriction on adjunction: we disallow
adjunction at any node on the spine of an aux-
iliary tree below the root (though we discuss re-
laxing that constraint in Section 4.2). We refer to
this variant as Off Spine TAG (OSTAG) and note
that it allows the use of full wrapping rules, which
are forbidden in TIG. This targeted blocking of
recursion has similar motivations and benefits to
the approximation of CFGs with regular languages
(Mohri and jan Nederhof, 2000).
The following sections discuss in detail the
context-free nature of OSTAG and alternative
probabilistic models for its equivalent CFG form.
We propose a simple but empirically effective
heuristic for grammar induction for our experi-
ments on Penn Treebank data.
</bodyText>
<sectionHeader confidence="0.977982" genericHeader="method">
3 Transformation to CFG
</sectionHeader>
<bodyText confidence="0.9998022">
To demonstrate that OSTAG has only context-
free power, we provide a reduction to context-free
grammar. Given an OSTAG (N, T, R, A, S), we
define the set N of nodes of the corresponding
CFG to be pairs of a tree in R or A together with an
</bodyText>
<page confidence="0.99068">
304
</page>
<figure confidence="0.743578888888889">
β: T
a T* a
ry: T
b T* b
α: S
T
x
T
y
</figure>
<equation confidence="0.964829625">
S X Y S X Y
X x X x
Y y Y y
X A
X B
Y A0
Y B0
A a X0 a X aXa
A0 a Y 0 a Y aYa
X0 X
Y 0 Y
B b X00 b X b X b
B0 b Y00 b Y bYb
X00 X
Y00 Y
(a) (b) (c)
</equation>
<figureCaption confidence="0.54495825">
Figure 3: (a) OSTAG for the language wxwRvyvR where w, v E {ajb}+ and R reverses a string. (b) A
CFG for the same language, which of necessity must distinguish between nonterminals X and Y playing
the role of T in the OSTAG. (c) Simplified CFG, conflating nonterminals, but which must still distinguish
between X and Y .
</figureCaption>
<bodyText confidence="0.9997164">
address (Gorn number (Gorn, 1965)) in that tree.
We take the nonterminals of the target CFG gram-
mar to be nodes or pairs of nodes, elements of the
set N +N xN. We notate the pairs of nodes with
a kind of “applicative” notation. Given two nodes
q and q0, we notate a target nonterminal as q(q0).
Now for each tree τ and each interior node q
in τ that is not on the spine of τ, with children
q1, ... , qk, we add a context-free rule to the gram-
mar
</bodyText>
<equation confidence="0.940846">
q q1 ···qk (1)
</equation>
<bodyText confidence="0.99619625">
and if interior node q is on the spine of τ with
qs the child node also on the spine of τ (that is,
dominating the foot node of τ) and q0 is a node (in
any tree) where τ is adjoinable, we add a rule
</bodyText>
<equation confidence="0.997414">
q(q0) q1 ··· qs(q0) ···qk . (2)
</equation>
<bodyText confidence="0.997639333333333">
Rules of type (1) handle the expansion of a node
not on the spine of an auxiliary tree and rules of
type (2) a spinal node.
In addition, to initiate adjunction at any node q0
where a tree τ with root q is adjoinable, we use a
rule
</bodyText>
<equation confidence="0.986354">
q0 q(q0) (3)
</equation>
<bodyText confidence="0.993638">
and for the foot node qf of τ, we use a rule
</bodyText>
<equation confidence="0.922533">
qf(q) q (4)
</equation>
<bodyText confidence="0.999466384615385">
The OSTAG constraint follows immediately
from the structure of the rules of type (2). Any
child spine node qs manifests as a CFG nonter-
minal qs(q0). If child spine nodes themselves al-
lowed adjunction, we would need a type (3) rule
of the form qs(q0) qs(q0)(q00). This rule itself
would feed adjunction, requiring further stacking
of nodes, and an infinite set of CFG nonterminals
and rules. This echoes exactly the stacking found
in the LIG reduction of TAG.
To handle substitution, any frontier node q that
allows substitution of a tree rooted with node q0
engenders a rule
</bodyText>
<equation confidence="0.983544">
q q0 (5)
</equation>
<bodyText confidence="0.999612230769231">
This transformation is reversible, which is to
say that each parse tree derived with this CFG im-
plies exactly one OSTAG derivation, with substi-
tutions and adjunctions coded by rules of type (5)
and (3) respectively. Depending on the definition
of a TAG derivation, however, the converse is not
necessarily true. This arises from the spurious am-
biguity between adjunction at a substitution site
(before applying a type (5) rule) versus the same
adjunction at the root of the substituted initial tree
(after applying a type (5) rule). These choices
lead to different derivations in CFG form, but their
TAG derivations can be considered conceptually
</bodyText>
<page confidence="0.996358">
305
</page>
<bodyText confidence="0.9936516">
identical. To avoid double-counting derivations,
which can adversely effect probabilistic modeling,
type (3) and type (4) rules in which the side with
the unapplied symbol is a nonterminal leaf can be
omitted.
</bodyText>
<subsectionHeader confidence="0.991444">
3.1 Example
</subsectionHeader>
<bodyText confidence="0.999251375">
The grammar of Figure 3(a) can be converted to
a CFG by this method. We indicate for each CFG
rule its type as defined above the production arrow.
All types are used save type (5), as substitution
is not employed in this example. For the initial
tree α, we have the following generated rules (with
nodes notated by the tree name and a Gorn number
subscript):
</bodyText>
<equation confidence="0.991712952380952">
-+ α1 α2
1 α1
1
α1 -+ x α1
-+ 0E(α2)
3
-+ &apos;YE(α2)
3
For the auxiliary trees 0 and &apos;Y we have:
0E(α1) -+2 a 01(α1) a
0E(α2) -+2 a 01(α2) a
4
01 (α1) -+ α1
4
01(α2) -+ α2
&apos;YE(α1) -+2 b&apos;Y1(α1) b
&apos;YE(α2) -+2 b&apos;Y1(α2) b
4
&apos;Y1 (α1) -+ α1
4
&apos;Y1(α2) -+ α2
</equation>
<bodyText confidence="0.9999005">
The grammar of Figure 3(b) is simply a renaming
of this grammar.
</bodyText>
<sectionHeader confidence="0.999626" genericHeader="method">
4 Applications
</sectionHeader>
<subsectionHeader confidence="0.999668">
4.1 Compact grammars
</subsectionHeader>
<bodyText confidence="0.9990764">
The OSTAG framework provides some leverage in
expressing particular context-free languages more
compactly than a CFG or even a TSG can. As
an example, consider the language of bracketed
palindromes
</bodyText>
<equation confidence="0.9834205">
1 &lt; i &lt; k
w E {bj  |1 &lt; j &lt; m}∗
</equation>
<bodyText confidence="0.999476533333333">
containing strings like a2 b1b3 a2 b3b1 a2. Any
TSG for this language must include as substrings
some subpalindrome constituents for long enough
strings. Whatever nonterminal covers such a
string, it must be specific to the a index within
it, and must introduce at least one pair of bs as
well. Thus, there are at least m such nontermi-
nals, each introducing at least k rules, requiring at
least km rules overall. The simplest such gram-
mar, expressed as a CFG, is in Figure 4(a). The
ability to use adjunction allows expression of the
same language as an OSTAG with k + m elemen-
tary trees (Figure 4(b)). This example shows that
an OSTAG can be quadratically smaller than the
corresponding TSG or CFG.
</bodyText>
<subsectionHeader confidence="0.974915">
4.2 Extensions
</subsectionHeader>
<bodyText confidence="0.999248588235294">
The technique in OSTAG can be extended to ex-
pand its expressiveness without increasing gener-
ative capacity.
First, OSTAG allows zero adjunctions on each
node on the spine below the root of an auxiliary
tree, but any non-zero finite bound on the num-
ber of adjunctions allowed on-spine would simi-
larly limit generative capacity. The tradeoff is in
the grammar constant of the effective probabilis-
tic CFG; an extension that allows k levels of on
spine adjunction has a grammar constant that is
O(|N|(k+2)) .
Second, the OSTAG form of adjunction is con-
sistent with the TIG form. That is, we can extend
OSTAG by allowing on-spine adjunction of left- or
right-auxiliary trees in keeping with the TIG con-
straints without increasing generative capacity.
</bodyText>
<subsectionHeader confidence="0.998043">
4.3 Probabilistic OSTAG
</subsectionHeader>
<bodyText confidence="0.999991894736842">
One major motivation for adherence to a context-
free grammar formalism is the ability to employ
algorithms designed for probabilistic CFGs such
as the CYK algorithm for parsing or the Inside-
Outside algorithm for grammar estimation. In this
section we present a probabilistic model for an OS-
TAG grammar in PCFG form that can be used in
such algorithms, and show that many parameters
of this PCFG can be pooled or set equal to one and
ignored. References to rules of types (1-5) below
refer to the CFG transformation rules defined in
Section 3. While in the preceeding discussion we
used Gorn numbers for clarity, our discussion ap-
plies equally well for the Goodman transform dis-
cussed above, in which each node is labeled with a
signature of its subtree. This simply redefines q in
the CFG reduction described in Section 3 to be a
subtree indicator, and dramatically reduces redun-
dancy in the generated grammar.
</bodyText>
<equation confidence="0.871492">
αE
-+ 0E(α1)
3
-+ &apos;YE(α1)
3
1
α2 -+ y α2
α2
Pal = az w az wR az
</equation>
<page confidence="0.600554">
306
</page>
<figure confidence="0.5432551">
αi  |1 ≤ i ≤ k: S
ai T
ai
βj  |1≤ j ≤ M: T
bj T* bj
S → ai Ti ai
Ti → bj Ti bj
Ti → ai
ai
(a) (b)
</figure>
<figureCaption confidence="0.99986">
Figure 4: A CFG (a) and more compact OSTAG (b) for the language Pal
</figureCaption>
<bodyText confidence="0.999982875">
The first practical consideration is that CFG
rules of type (2) are deterministic, and as such
we need only record the rule itself and no asso-
ciated parameter. Furthermore, these rules employ
a template in which the stored symbol appears in
the left-hand side and in exactly one symbol on
the right-hand side where the spine of the auxil-
iary tree proceeds. One deterministic rule exists
for this template applied to each q, and so we may
record only the template. In order to perform CYK
or IO, it is not even necessary to record the index
in the right-hand side where the spine continues;
these algorithms fill a chart bottom up and we can
simply propagate the stored nonterminal up in the
chart.
CFG rules of type (4) are also deterministic and
do not require parameters. In these cases it is not
necessary to record the rules, as they all have ex-
actly the same form. All that is required is a check
that a given symbol is adjoinable, which is true for
all symbols except nonterminal leaves and applied
symbols. Rules of type (5) are necessary to cap-
ture the probability of substitution and so we will
require a parameter for each.
At first glance, it would seem that due to the
identical domain of the left-hand sides of rules of
types (1) and (3) a parameter is required for each
such rule. To avoid this we propose the follow-
ing factorization for the probabilistic expansion of
an off spine node. First, a decision is made as to
whether a type (1) or (3) rule will be used; this cor-
responds to deciding if adjunction will or will not
take place at the node. If adjunction is rejected,
then there is only one type (1) rule available, and
so parameterization of type (1) rules is unneces-
sary. If we decide on adjunction, one of the avail-
able type (3) rules is chosen from a multinomial.
By conditioning the probability of adjunction on
varying amounts of information about the node,
alternative models can easily be defined.
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999978684210526">
As a proof of concept, we investigate OSTAG in
the context of the classic Penn Treebank statistical
parsing setup; training on section 2-21 and testing
on section 23. For preprocessing, words that oc-
cur only once in the training data are mapped to
the unknown categories employed in the parser of
Petrov et al. (2006). We also applied the annota-
tion from Klein and Manning (2003) that appends
“-U” to each nonterminal node with a single child,
drastically reducing the presence of looping unary
chains. This allows the use of a coarse to fine
parsing strategy (Charniak et al., 2006) in which
a sentence is first parsed with the Maximum Like-
lihood PCFG and only constituents whose prob-
ability exceeds a cutoff of 10−4 are allowed in
the OSTAG chart. Designed to facilitate sister ad-
junction, we define our binarization scheme by ex-
ample in which the added nodes, indicated by @,
record both the parent and head child of the rule.
</bodyText>
<equation confidence="0.858754">
NP
@NN-NP
@NN-NP
DT @NN-NP
JJ NN
</equation>
<bodyText confidence="0.9198323">
A compact TSG can be obtained automatically
using the MCMC grammar induction technique of
Cohn and Blunsom (2010), retaining all TSG rules
that appear in at least one derivation in after 1000
iterations of sampling. We use EM to estimate the
parameters of this grammar on sections 2-21, and
use this as our baseline.
To generate a set of TAG rules, we consider
each rule in our baseline TSG and find all possi-
SBAR
</bodyText>
<page confidence="0.90493">
307
</page>
<table confidence="0.9350655">
All 40 #Adj #Wrap
TSG 85.00 86.08 – –
TSG&apos; 85.12 86.21 – –
OSTAG1 85.42 86.43 1336 52
OSTAG2 85.54 86.56 1952 44
OSTAG3 85.86 86.84 3585 41
</table>
<figureCaption confidence="0.880424">
Figure 5: Parsing F-Score for the models under
</figureCaption>
<bodyText confidence="0.983981149425288">
comparison for both the full test set and sentences
of length 40 or less. For the OSTAG models, we
list the number of adjunctions in the MPD of the
full test set, as well as the number of wrapping
adjunctions.
ble auxiliary root and foot node pairs it contains.
For each such root/foot pair, we include the TAG
rule implied by removal of the structure above the
root and below the foot. We also include the TSG
rule left behind when the adjunction of this auxil-
iary tree is removed. To be sure that experimental
gains are not due to this increased number of TSG
initial trees, we calculate parameters using EM for
this expanded TSG and use it as a second base-
line (TSG&apos;). With our full set of initial and aux-
iliary trees, we use EM and the PCFG reduction
described above to estimate the parameters of an
OSTAG.
We investigate three models for the probabil-
ity of adjunction at a node. The first uses a con-
servative number of parameters, with a Bernoulli
variable for each symbol (OSTAG1). The second
employs more parameters, conditioning on both
the node’s symbol and the symbol of its leftmost
child (OSTAG2).The third is highly parameterized
but most prone to data sparsity, with a separate
Bernoulli distribution for each Goodman index η
(OSTAG3). We report results for Most Probable
Derivation (MPD) parses of section 23 in Figure
5.
Our results show that OSTAG outperforms both
baselines. Furthermore, the various parameteri-
zations of adjunction with OSTAG indicate that,
at least in the case of the Penn Treebank, the
finer grained modeling of a full table of adjunction
probabilities for each Goodman index OSTAG3
overcomes the danger of sparse data estimates.
Not only does such a model lead to better parsing
performance, but it uses adjunction more exten-
sively than its more lightly parameterized alterna-
tives. While different representations make direct
comparison inappropriate, the OSTAG results lie
in the same range as previous work with statistical
TIG on this task, such as Chiang (2000) (86.00)
and Shindo et al. (2011) (85.03).
The OSTAG constraint can be relaxed as de-
scribed in Section 4.2 to allow any finite number of
on-spine adjunctions without sacrificing context-
free form. However, the increase to the grammar
constant quickly makes parsing with such models
an arduous task. To determine the effect of such a
relaxation, we allow a single level of on-spine ad-
junction using the adjunction model of OSTAG1,
and estimate this model with EM on the training
data. We parse sentences of length 40 or less in
section 23 and observe that on-spine adjunction is
never used in the MPD parses. This suggests that
the OSTAG constraint is reasonable, at least for
the domain of English news text.
We performed further examination of the MPD
using OSTAG for each of the sentences in the test
corpus. As an artifact of the English language, the
majority have their foot node on the left spine and
would also be usable by TIG, and so we discuss
the instances of wrapping auxiliary trees in these
derivations that are uniquely available to OSTAG.
We remove binarization for clarity and denote the
foot node with an asterisk.
A frequent use of wrapping adjunction is to co-
ordinate symbols such as quotes, parentheses, and
dashes on both sides of a noun phrase. One com-
mon wrapping auxiliary tree in our experiments is
NP
“ NP* ” PP
This is used frequently in the news text of
the Wall Street Journal for reported speech when
avoiding a full quotation. This sentence is an ex-
ample of the way the rule is employed, using what
Joshi and Schabes (1997) referred to as “factoring
recursion from linguistic constraints” with TAG.
Note that replacing the quoted noun phrase and
its following prepositional phrase with the noun
phrase itself yields a valid sentence, in line with
the linguistic theory underlying TAG.
Another frequent wrapping rule, shown below,
allows direct coordination between the contents of
an appositive with the rest of the sentence.
</bodyText>
<page confidence="0.989266">
308
</page>
<figure confidence="0.498224777777778">
NP* ,
All Wrap Right Left
Total 3585 (1374) 41 (26) 1698 (518) 1846 (830)
Sister 2851 (1180) 17 (11) 1109 (400) 1725 (769)
Lex 2244 (990) 28 (19) 894 (299) 1322 (672)
FLex 1028 (558) 7 (2) 835 (472) 186 (84)
NP
NP ,CC
or
</figure>
<bodyText confidence="0.98858775">
This is a valuable ability, as it is common to
use an appositive to provide context or explanation
for a proper noun. As our information on proper
nouns will most likely be very sparse, the apposi-
tive may be more reliably connected to the rest of
the sentence. An example of this from one of the
sentences in which this rule appears in the MPD is
the phrase “since the market fell 156.83, or 8 %,
a week after Black Monday”. The wrapping rule
allows us to coordinate the verb “fell” with the pat-
tern “X %” instead of 156.83, which is mapped to
an unknown word category.
These rules highlight the linguistic intuitions
that back TAG; if their adjunction were undone,
the remaining derivation would be a valid sen-
tence that simply lacks the modifying structure of
the auxiliary tree. However, the MPD parses re-
veal that not all useful adjunctions conform to this
paradigm, and that left-auxiliary trees that are not
used for sister adjunction are susceptible to this
behavior. The most common such tree is used to
create noun phrases such as
P&amp;G’s share of [the Japanese market]
the House’s repeal of [a law]
Apple’s family of [Macintosh Computers]
Canada’s output of [crude oil]
by adjoining the shared unbracketed syntax onto
the NP dominating the bracketed text. If adjunc-
tion is taken to model modification, this rule dras-
tically changes the semantics of the unmodified
sentence. Furthermore, in some cases removing
the adjunction can leave a grammatically incorrect
sentence, as in the third example where the noun
phrase changes plurality.
While our grammar induction method is a crude
(but effective) heuristic, we can still highlight the
qualities of the more important auxiliary trees
by examining aggregate statistics over the MPD
parses, shown in Figure 6. The use of left-
auxiliary trees for sister adjunction is a clear trend,
as is the predominant use of right-auxiliary trees
for the complementary set of “regular” adjunc-
tions, which is to be expected in a right branch-
ing language such as English. The statistics also
Figure 6: Statistics for MPD auxiliary trees us-
ing OSTAG3. The columns indicate type of aux-
iliary tree and the rows correspond respectively to
the full set found in the MPD, those that perform
sister adjunction, those that are lexicalized, and
those that are fully lexicalized. Each cell shows
the number of tokens followed by the number of
types of auxiliary tree that fit its conditions.
reflect the importance of substitution in right-
auxiliary trees, as they must capture the wide va-
riety of right branching modifiers of the English
language.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999984827586207">
The OSTAG variant of Tree-Adjoining Grammar
is a simple weakly context-free formalism that
still provides for all types of adjunction and is
a bit more concise than TSG (quadratically so).
OSTAG can be reversibly transformed into CFG
form, allowing the use of a wide range of well
studied techniques in statistical parsing.
OSTAG provides an alternative to TIG as a
context-free TAG variant that offers wrapping ad-
junction in exchange for recursive left/right spine
adjunction. It would be interesting to apply both
OSTAG and TIG to different languages to deter-
mine where the constraints of one or the other are
more or less appropriate. Another possibility is the
combination of OSTAG with TIG, which would
strictly expand the abilities of both approaches.
The most important direction of future work for
OSTAG is the development of a principled gram-
mar induction model, perhaps using the same tech-
niques that have been successfully applied to TSG
and TIG. In order to motivate this and other re-
lated research, we release our implementation of
EM and CYK parsing for OSTAG1. Our system
performs the CFG transform described above and
optionally employs coarse to fine pruning and re-
laxed (finite) limits on the number of spine adjunc-
tions. As a TSG is simply a TAG without adjunc-
tion rules, our parser can easily be used as a TSG
estimator and parser as well.
</bodyText>
<footnote confidence="0.58142">
1bllip.cs.brown.edu/download/bucketparser.tar
</footnote>
<page confidence="0.99897">
309
</page>
<sectionHeader confidence="0.99382" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999880360465116">
Eugene Charniak, Mark Johnson, Micha Elsner,
Joseph L. Austerweil, David Ellis, Isaac Hax-
ton, Catherine Hill, R. Shrivaths, Jeremy Moore,
Michael Pozar, and Theresa Vu. 2006. Multilevel
coarse-to-fine PCFG parsing. In North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Eugene Charniak. 1996. Tree-bank grammars. In As-
sociation for the Advancement of Artificial Intelli-
gence, pages 1031–1036.
David Chiang. 2000. Statistical parsing with
an automatically-extracted tree adjoining grammar.
Association for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked infer-
ence in bayesian tree substitution grammars. pages
225–230. Association for Computational Linguis-
tics.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 548–556.
Association for Computational Linguistics.
Christy Doran, Dania Egedi, Beth Ann Hockey, Banga-
lore Srinivas, and Martin Zaidel. 1994. XTAG sys-
tem: a wide coverage grammar for English. pages
922–928. Association for Computational Linguis-
tics.
J. Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. Bod et al. 2003.
Saul Gorn. 1965. Explicit definitions and linguistic
dominoes. In Systems and Computer Science, pages
77–115.
Rebecca Hwa. 1998. An empirical evaluation of prob-
abilistic lexicalized tree insertion grammars. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 557–563. Association for Computational Lin-
guistics.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, vol-
ume 3, pages 69–124. Springer.
Aravind K Joshi. 1985. Tree adjoining grammars:
How much context-sensitivity is required to provide
reasonable structural descriptions? University of
Pennsylvania.
Aravind K Joshi. 1987. An introduction to tree ad-
joining grammars. Mathematics of Language, pages
87–115.
Dan Klein and Christopher D Manning. 2003. Accu-
rate unlexicalized parsing. pages 423–430. Associ-
ation for Computational Linguistics.
K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
pages 35–56.
Mehryar Mohri and Mark jan Nederhof. 2000. Regu-
lar approximation of context-free grammars through
transformation. In Robustness in language and
speech technology.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433–
440. Association for Computational Linguistics.
Yves Schabes and Richard C. Waters. 1995. Tree
insertion grammar: a cubic-time, parsable formal-
ism that lexicalizes context-free grammar without
changing the trees produced. Computational Lin-
guistics, (4):479–513.
Yves Schabes. 1990. Mathematical and computa-
tional aspects of lexicalized grammars. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2011. Insertion operator for bayesian tree substi-
tution grammars. pages 206–211. Association for
Computational Linguistics.
Elif Yamangil and Stuart M. Shieber. 2012. Estimat-
ing compact yet rich tree insertion grammars. pages
110–114. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.998607">
310
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.259435">
<title confidence="0.999819">A Context Free TAG Variant</title>
<author confidence="0.985804">Ben</author>
<affiliation confidence="0.820032">Brown Providence,</affiliation>
<email confidence="0.999719">chonger@cs.brown.edu</email>
<author confidence="0.653265">Eugene</author>
<affiliation confidence="0.813787">Brown Providence,</affiliation>
<email confidence="0.9999">ec@cs.brown.edu</email>
<abstract confidence="0.998883684210526">We propose a new variant of Tree- Adjoining Grammar that allows adjunction of full wrapping trees but still bears only context-free expressivity. We provide a transformation to context-free form, and a further reduction in probabilistic model size through factorization and pooling of parameters. This collapsed context-free form is used to implement efficient grammar estimation and parsing algorithms. We perform parsing experiments the Penn Treebank and draw comparisons to Tree- Substitution Grammars and between different variations in probabilistic model design. Examination of the most probable derivations reveals examples of the linguistically relevant structure that our variant makes possible.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
<author>Micha Elsner</author>
<author>Joseph L Austerweil</author>
<author>David Ellis</author>
<author>Isaac Haxton</author>
<author>Catherine Hill</author>
<author>R Shrivaths</author>
<author>Jeremy Moore</author>
<author>Michael Pozar</author>
<author>Theresa Vu</author>
</authors>
<title>Multilevel coarse-to-fine PCFG parsing. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</title>
<date>2006</date>
<contexts>
<context position="19959" citStr="Charniak et al., 2006" startWordPosition="3507" endWordPosition="3510">s can easily be defined. 5 Experiments As a proof of concept, we investigate OSTAG in the context of the classic Penn Treebank statistical parsing setup; training on section 2-21 and testing on section 23. For preprocessing, words that occur only once in the training data are mapped to the unknown categories employed in the parser of Petrov et al. (2006). We also applied the annotation from Klein and Manning (2003) that appends “-U” to each nonterminal node with a single child, drastically reducing the presence of looping unary chains. This allows the use of a coarse to fine parsing strategy (Charniak et al., 2006) in which a sentence is first parsed with the Maximum Likelihood PCFG and only constituents whose probability exceeds a cutoff of 10−4 are allowed in the OSTAG chart. Designed to facilitate sister adjunction, we define our binarization scheme by example in which the added nodes, indicated by @, record both the parent and head child of the rule. NP @NN-NP @NN-NP DT @NN-NP JJ NN A compact TSG can be obtained automatically using the MCMC grammar induction technique of Cohn and Blunsom (2010), retaining all TSG rules that appear in at least one derivation in after 1000 iterations of sampling. We u</context>
</contexts>
<marker>Charniak, Johnson, Elsner, Austerweil, Ellis, Haxton, Hill, Shrivaths, Moore, Pozar, Vu, 2006</marker>
<rawString>Eugene Charniak, Mark Johnson, Micha Elsner, Joseph L. Austerweil, David Ellis, Isaac Haxton, Catherine Hill, R. Shrivaths, Jeremy Moore, Michael Pozar, and Theresa Vu. 2006. Multilevel coarse-to-fine PCFG parsing. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In Association for the Advancement of Artificial Intelligence,</booktitle>
<pages>1031--1036</pages>
<contexts>
<context position="1119" citStr="Charniak, 1996" startWordPosition="158" endWordPosition="159">implement efficient grammar estimation and parsing algorithms. We perform parsing experiments the Penn Treebank and draw comparisons to TreeSubstitution Grammars and between different variations in probabilistic model design. Examination of the most probable derivations reveals examples of the linguistically relevant structure that our variant makes possible. 1 Introduction While it is widely accepted that natural language is not context-free, practical limitations of existing algorithms motivate Context-Free Grammars (CFGs) as a good balance between modeling power and asymptotic performance (Charniak, 1996). In constituent-based parsing work, the prevailing technique to combat this divide between efficient models and real world data has been to selectively strengthen the dependencies in a CFG by increasing the grammar size through methods such as symbol refinement (Petrov et al., 2006). Another approach is to employ a more powerful grammatical formalism and devise constraints and transformations that allow use of essential efficient algorithms such as the Inside-Outside algorithm (Lari and Young, 1990) and CYK parsing. Tree-Adjoining Grammar (TAG) is a natural Elif Yamangil Harvard University Ca</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>Eugene Charniak. 1996. Tree-bank grammars. In Association for the Advancement of Artificial Intelligence, pages 1031–1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar. Association for Computational Linguistics.</title>
<date>2000</date>
<contexts>
<context position="9531" citStr="Chiang (2000)" startWordPosition="1570" endWordPosition="1571">ion on auxiliary trees states that the foot node must occur as either the leftmost or rightmost leaf node. This introduces an important distinction between left, right, and wrapping auxiliary trees, of which only the first two are allowed in TIG. Furthermore, TIG disallows adjunction of left auxiliary trees on the spines of right auxiliary trees, and vice versa. This is to prevent the construction of wrapping auxiliary trees, whose removal is essential for the simplified complexity of TIG. Several probabilistic models have been proposed for TIG. While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. Later approaches (Shindo et al., 2011; Yamangil and Shieber, 2012) were able to extend the non-parametric modeling of TSGs to TIG, providing methods for both modeling and grammar induction. 2.4 OSTAG Our new TAG variant is extremely simple. We allow arbitrary initial and auxiliary trees, and place only one restriction on adjunction: we disallow adjunction at any node on the spine of an auxiliary tree below the root (though we discuss relaxing that constraint in Section 4.2). We refer to this variant as Off Spi</context>
<context position="22947" citStr="Chiang (2000)" startWordPosition="4030" endWordPosition="4031">s. Furthermore, the various parameterizations of adjunction with OSTAG indicate that, at least in the case of the Penn Treebank, the finer grained modeling of a full table of adjunction probabilities for each Goodman index OSTAG3 overcomes the danger of sparse data estimates. Not only does such a model lead to better parsing performance, but it uses adjunction more extensively than its more lightly parameterized alternatives. While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang (2000) (86.00) and Shindo et al. (2011) (85.03). The OSTAG constraint can be relaxed as described in Section 4.2 to allow any finite number of on-spine adjunctions without sacrificing contextfree form. However, the increase to the grammar constant quickly makes parsing with such models an arduous task. To determine the effect of such a relaxation, we allow a single level of on-spine adjunction using the adjunction model of OSTAG1, and estimate this model with EM on the training data. We parse sentences of length 40 or less in section 23 and observe that on-spine adjunction is never used in the MPD p</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>Blocked inference in bayesian tree substitution grammars.</title>
<date>2010</date>
<pages>225--230</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6584" citStr="Cohn and Blunsom, 2010" startWordPosition="1061" endWordPosition="1064">ure of the original elementary trees. A more compact CFG representation can be obtained by marking each node in each elementary tree with a signature of its subtree. This transform, presented by Goodman (2003), can rein in the grammar constant G, as the crucial CFG algorithms for a sentence of length n have complexity O(Gn3). A simple probabilistic model for a TSG is a set of multinomials, one for each nonterminal in N corresponding to its possible substitutions in R. A more flexible model allows a potentially infinite number of substitution rules using a Dirichlet Process (Cohn et al., 2009; Cohn and Blunsom, 2010). This model has proven effective for grammar induction via Markov Chain Monte Carlo (MCMC), in which TSG derivations of the training set are repeatedly sampled to find frequently occurring elementary trees. A straightforward technique for induction of a finite TSG is to perform this nonparametric induction and select the set of rules that appear in at least one sampled derivation at one or several of the final iterations. 2.2 Tree-Adjoining Grammar Tree-adjoining grammar (TAG) (Joshi, 1985; Joshi, 1987; Joshi and Schabes, 1997) is an extension of TSG defined by a tuple (N, T, R, A, S), and di</context>
<context position="20452" citStr="Cohn and Blunsom (2010)" startWordPosition="3594" endWordPosition="3597">ly reducing the presence of looping unary chains. This allows the use of a coarse to fine parsing strategy (Charniak et al., 2006) in which a sentence is first parsed with the Maximum Likelihood PCFG and only constituents whose probability exceeds a cutoff of 10−4 are allowed in the OSTAG chart. Designed to facilitate sister adjunction, we define our binarization scheme by example in which the added nodes, indicated by @, record both the parent and head child of the rule. NP @NN-NP @NN-NP DT @NN-NP JJ NN A compact TSG can be obtained automatically using the MCMC grammar induction technique of Cohn and Blunsom (2010), retaining all TSG rules that appear in at least one derivation in after 1000 iterations of sampling. We use EM to estimate the parameters of this grammar on sections 2-21, and use this as our baseline. To generate a set of TAG rules, we consider each rule in our baseline TSG and find all possiSBAR 307 All 40 #Adj #Wrap TSG 85.00 86.08 – – TSG&apos; 85.12 86.21 – – OSTAG1 85.42 86.43 1336 52 OSTAG2 85.54 86.56 1952 44 OSTAG3 85.86 86.84 3585 41 Figure 5: Parsing F-Score for the models under comparison for both the full test set and sentences of length 40 or less. For the OSTAG models, we list the </context>
</contexts>
<marker>Cohn, Blunsom, 2010</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2010. Blocked inference in bayesian tree substitution grammars. pages 225–230. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Phil Blunsom</author>
</authors>
<title>Inducing compact but accurate treesubstitution grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>548--556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6559" citStr="Cohn et al., 2009" startWordPosition="1057" endWordPosition="1060">t record the structure of the original elementary trees. A more compact CFG representation can be obtained by marking each node in each elementary tree with a signature of its subtree. This transform, presented by Goodman (2003), can rein in the grammar constant G, as the crucial CFG algorithms for a sentence of length n have complexity O(Gn3). A simple probabilistic model for a TSG is a set of multinomials, one for each nonterminal in N corresponding to its possible substitutions in R. A more flexible model allows a potentially infinite number of substitution rules using a Dirichlet Process (Cohn et al., 2009; Cohn and Blunsom, 2010). This model has proven effective for grammar induction via Markov Chain Monte Carlo (MCMC), in which TSG derivations of the training set are repeatedly sampled to find frequently occurring elementary trees. A straightforward technique for induction of a finite TSG is to perform this nonparametric induction and select the set of rules that appear in at least one sampled derivation at one or several of the final iterations. 2.2 Tree-Adjoining Grammar Tree-adjoining grammar (TAG) (Joshi, 1985; Joshi, 1987; Joshi and Schabes, 1997) is an extension of TSG defined by a tupl</context>
</contexts>
<marker>Cohn, Goldwater, Blunsom, 2009</marker>
<rawString>Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate treesubstitution grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 548–556. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christy Doran</author>
<author>Dania Egedi</author>
<author>Beth Ann Hockey</author>
<author>Bangalore Srinivas</author>
<author>Martin Zaidel</author>
</authors>
<title>XTAG system: a wide coverage grammar for English. pages 922–928. Association for Computational Linguistics.</title>
<date>1994</date>
<contexts>
<context position="8527" citStr="Doran et al., 1994" startWordPosition="1406" endWordPosition="1409">ary tree with root and foot node X can be adjoined into an internal node of an elementary tree labeled with X by splicing the auxiliary tree in at that internal node, as pictured in Figure 2. We refer to the path between the root and foot nodes in an auxiliary tree as the spine of the tree. As mentioned above, the added power afforded by adjunction comes at a serious price in time complexity. As such, probabilistic modeling for TAG in its original form is uncommon. However, a large effort in non-probabilistic grammar induction has been performed through manual annotation with the XTAG project(Doran et al., 1994). 2.3 Tree Insertion Grammar Tree Insertion Grammars (TIGs) are a longstanding compromise between the intuitive expressivity of TAG and the algorithmic simplicity of CFGs. Schabes and Waters (1995) showed that by restricting the form of the auxiliary trees in A and the set of auxiliary trees that may adjoin at particular nodes, a TAG generates only context-free languages. The TIG restriction on auxiliary trees states that the foot node must occur as either the leftmost or rightmost leaf node. This introduces an important distinction between left, right, and wrapping auxiliary trees, of which o</context>
</contexts>
<marker>Doran, Egedi, Hockey, Srinivas, Zaidel, 1994</marker>
<rawString>Christy Doran, Dania Egedi, Beth Ann Hockey, Bangalore Srinivas, and Martin Zaidel. 1994. XTAG system: a wide coverage grammar for English. pages 922–928. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Efficient parsing of DOP with PCFG-reductions.</title>
<date>2003</date>
<journal>Bod</journal>
<contexts>
<context position="6170" citStr="Goodman (2003)" startWordPosition="991" endWordPosition="992">ubstitution Grammar. TSG remains in the family of context-free grammars, as can be easily seen by the removal of the internal nodes in all elementary trees; what is left is a CFG that generates the same language. As a reversible alternative that preserves the internal structure, annotation of each internal node with a unique index creates a large number of deterministic CFG rules that record the structure of the original elementary trees. A more compact CFG representation can be obtained by marking each node in each elementary tree with a signature of its subtree. This transform, presented by Goodman (2003), can rein in the grammar constant G, as the crucial CFG algorithms for a sentence of length n have complexity O(Gn3). A simple probabilistic model for a TSG is a set of multinomials, one for each nonterminal in N corresponding to its possible substitutions in R. A more flexible model allows a potentially infinite number of substitution rules using a Dirichlet Process (Cohn et al., 2009; Cohn and Blunsom, 2010). This model has proven effective for grammar induction via Markov Chain Monte Carlo (MCMC), in which TSG derivations of the training set are repeatedly sampled to find frequently occurr</context>
</contexts>
<marker>Goodman, 2003</marker>
<rawString>J. Goodman. 2003. Efficient parsing of DOP with PCFG-reductions. Bod et al. 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saul Gorn</author>
</authors>
<title>Explicit definitions and linguistic dominoes.</title>
<date>1965</date>
<booktitle>In Systems and Computer Science,</booktitle>
<pages>77--115</pages>
<contexts>
<context position="11453" citStr="Gorn, 1965" startWordPosition="1943" endWordPosition="1944">corresponding CFG to be pairs of a tree in R or A together with an 304 β: T a T* a ry: T b T* b α: S T x T y S X Y S X Y X x X x Y y Y y X A X B Y A0 Y B0 A a X0 a X aXa A0 a Y 0 a Y aYa X0 X Y 0 Y B b X00 b X b X b B0 b Y00 b Y bYb X00 X Y00 Y (a) (b) (c) Figure 3: (a) OSTAG for the language wxwRvyvR where w, v E {ajb}+ and R reverses a string. (b) A CFG for the same language, which of necessity must distinguish between nonterminals X and Y playing the role of T in the OSTAG. (c) Simplified CFG, conflating nonterminals, but which must still distinguish between X and Y . address (Gorn number (Gorn, 1965)) in that tree. We take the nonterminals of the target CFG grammar to be nodes or pairs of nodes, elements of the set N +N xN. We notate the pairs of nodes with a kind of “applicative” notation. Given two nodes q and q0, we notate a target nonterminal as q(q0). Now for each tree τ and each interior node q in τ that is not on the spine of τ, with children q1, ... , qk, we add a context-free rule to the grammar q q1 ···qk (1) and if interior node q is on the spine of τ with qs the child node also on the spine of τ (that is, dominating the foot node of τ) and q0 is a node (in any tree) where τ is</context>
</contexts>
<marker>Gorn, 1965</marker>
<rawString>Saul Gorn. 1965. Explicit definitions and linguistic dominoes. In Systems and Computer Science, pages 77–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>An empirical evaluation of probabilistic lexicalized tree insertion grammars.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>557--563</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9513" citStr="Hwa (1998)" startWordPosition="1567" endWordPosition="1568">he TIG restriction on auxiliary trees states that the foot node must occur as either the leftmost or rightmost leaf node. This introduces an important distinction between left, right, and wrapping auxiliary trees, of which only the first two are allowed in TIG. Furthermore, TIG disallows adjunction of left auxiliary trees on the spines of right auxiliary trees, and vice versa. This is to prevent the construction of wrapping auxiliary trees, whose removal is essential for the simplified complexity of TIG. Several probabilistic models have been proposed for TIG. While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. Later approaches (Shindo et al., 2011; Yamangil and Shieber, 2012) were able to extend the non-parametric modeling of TSGs to TIG, providing methods for both modeling and grammar induction. 2.4 OSTAG Our new TAG variant is extremely simple. We allow arbitrary initial and auxiliary trees, and place only one restriction on adjunction: we disallow adjunction at any node on the spine of an auxiliary tree below the root (though we discuss relaxing that constraint in Section 4.2). We refer to this </context>
</contexts>
<marker>Hwa, 1998</marker>
<rawString>Rebecca Hwa. 1998. An empirical evaluation of probabilistic lexicalized tree insertion grammars. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 557–563. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Treeadjoining grammars.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>69--124</pages>
<editor>In G. Rozenberg and A. Salomaa, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="2192" citStr="Joshi and Schabes, 1997" startWordPosition="319" endWordPosition="322">h as the Inside-Outside algorithm (Lari and Young, 1990) and CYK parsing. Tree-Adjoining Grammar (TAG) is a natural Elif Yamangil Harvard University Cambridge, MA elif@eecs.harvard.edu Stuart Shieber Harvard University Cambridge, MA shieber@eecs.harvard.edu starting point for such methods as it is the canonical member of the mildly context-sensitive family, falling just above CFGs in the hierarchy of formal grammars. TAG has a crucial advantage over CFGs in its ability to represent long distance interactions in the face of the interposing variations that commonly manifest in natural language (Joshi and Schabes, 1997). Consider, for example, the sentences These pretzels are making me thirsty. These pretzels are not making me thirsty. These pretzels that I ate are making me thirsty. Using a context-free language model with proper phrase bracketing, the connection between the words pretzels and thirsty must be recorded with three separate patterns, which can lead to poor generalizability and unreliable sparse frequency estimates in probabilistic models. While these problems can be overcome to some extent with large amounts of data, redundant representation of patterns is particularly undesirable for systems </context>
<context position="7118" citStr="Joshi and Schabes, 1997" startWordPosition="1147" endWordPosition="1150">f substitution rules using a Dirichlet Process (Cohn et al., 2009; Cohn and Blunsom, 2010). This model has proven effective for grammar induction via Markov Chain Monte Carlo (MCMC), in which TSG derivations of the training set are repeatedly sampled to find frequently occurring elementary trees. A straightforward technique for induction of a finite TSG is to perform this nonparametric induction and select the set of rules that appear in at least one sampled derivation at one or several of the final iterations. 2.2 Tree-Adjoining Grammar Tree-adjoining grammar (TAG) (Joshi, 1985; Joshi, 1987; Joshi and Schabes, 1997) is an extension of TSG defined by a tuple (N, T, R, A, S), and differs from TSG only in the addition of a 303 VP always VP VP* quickly + S ⇒ S NP VP always VP runs NP VP VP quickly runs Figure 2: The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle) to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is denoted with an asterisk. The OSTAG constraint would disallow further adjunction at the bold VP node only, as it is along the spine of the auxiliary tree. set of auxiliary trees A and the adjunction o</context>
<context position="24501" citStr="Joshi and Schabes (1997)" startWordPosition="4302" endWordPosition="4305">TIG, and so we discuss the instances of wrapping auxiliary trees in these derivations that are uniquely available to OSTAG. We remove binarization for clarity and denote the foot node with an asterisk. A frequent use of wrapping adjunction is to coordinate symbols such as quotes, parentheses, and dashes on both sides of a noun phrase. One common wrapping auxiliary tree in our experiments is NP “ NP* ” PP This is used frequently in the news text of the Wall Street Journal for reported speech when avoiding a full quotation. This sentence is an example of the way the rule is employed, using what Joshi and Schabes (1997) referred to as “factoring recursion from linguistic constraints” with TAG. Note that replacing the quoted noun phrase and its following prepositional phrase with the noun phrase itself yields a valid sentence, in line with the linguistic theory underlying TAG. Another frequent wrapping rule, shown below, allows direct coordination between the contents of an appositive with the rest of the sentence. 308 NP* , All Wrap Right Left Total 3585 (1374) 41 (26) 1698 (518) 1846 (830) Sister 2851 (1180) 17 (11) 1109 (400) 1725 (769) Lex 2244 (990) 28 (19) 894 (299) 1322 (672) FLex 1028 (558) 7 (2) 835 </context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Aravind K. Joshi and Yves Schabes. 1997. Treeadjoining grammars. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages, volume 3, pages 69–124. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Tree adjoining grammars: How much context-sensitivity is required to provide reasonable structural descriptions?</title>
<date>1985</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="7079" citStr="Joshi, 1985" startWordPosition="1143" endWordPosition="1144">entially infinite number of substitution rules using a Dirichlet Process (Cohn et al., 2009; Cohn and Blunsom, 2010). This model has proven effective for grammar induction via Markov Chain Monte Carlo (MCMC), in which TSG derivations of the training set are repeatedly sampled to find frequently occurring elementary trees. A straightforward technique for induction of a finite TSG is to perform this nonparametric induction and select the set of rules that appear in at least one sampled derivation at one or several of the final iterations. 2.2 Tree-Adjoining Grammar Tree-adjoining grammar (TAG) (Joshi, 1985; Joshi, 1987; Joshi and Schabes, 1997) is an extension of TSG defined by a tuple (N, T, R, A, S), and differs from TSG only in the addition of a 303 VP always VP VP* quickly + S ⇒ S NP VP always VP runs NP VP VP quickly runs Figure 2: The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle) to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is denoted with an asterisk. The OSTAG constraint would disallow further adjunction at the bold VP node only, as it is along the spine of the auxiliary tree. set of</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Aravind K Joshi. 1985. Tree adjoining grammars: How much context-sensitivity is required to provide reasonable structural descriptions? University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>An introduction to tree adjoining grammars. Mathematics of Language,</title>
<date>1987</date>
<pages>87--115</pages>
<contexts>
<context position="7092" citStr="Joshi, 1987" startWordPosition="1145" endWordPosition="1146">nite number of substitution rules using a Dirichlet Process (Cohn et al., 2009; Cohn and Blunsom, 2010). This model has proven effective for grammar induction via Markov Chain Monte Carlo (MCMC), in which TSG derivations of the training set are repeatedly sampled to find frequently occurring elementary trees. A straightforward technique for induction of a finite TSG is to perform this nonparametric induction and select the set of rules that appear in at least one sampled derivation at one or several of the final iterations. 2.2 Tree-Adjoining Grammar Tree-adjoining grammar (TAG) (Joshi, 1985; Joshi, 1987; Joshi and Schabes, 1997) is an extension of TSG defined by a tuple (N, T, R, A, S), and differs from TSG only in the addition of a 303 VP always VP VP* quickly + S ⇒ S NP VP always VP runs NP VP VP quickly runs Figure 2: The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle) to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is denoted with an asterisk. The OSTAG constraint would disallow further adjunction at the bold VP node only, as it is along the spine of the auxiliary tree. set of auxiliary tr</context>
</contexts>
<marker>Joshi, 1987</marker>
<rawString>Aravind K Joshi. 1987. An introduction to tree adjoining grammars. Mathematics of Language, pages 87–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19755" citStr="Klein and Manning (2003)" startWordPosition="3473" endWordPosition="3476">f we decide on adjunction, one of the available type (3) rules is chosen from a multinomial. By conditioning the probability of adjunction on varying amounts of information about the node, alternative models can easily be defined. 5 Experiments As a proof of concept, we investigate OSTAG in the context of the classic Penn Treebank statistical parsing setup; training on section 2-21 and testing on section 23. For preprocessing, words that occur only once in the training data are mapped to the unknown categories employed in the parser of Petrov et al. (2006). We also applied the annotation from Klein and Manning (2003) that appends “-U” to each nonterminal node with a single child, drastically reducing the presence of looping unary chains. This allows the use of a coarse to fine parsing strategy (Charniak et al., 2006) in which a sentence is first parsed with the Maximum Likelihood PCFG and only constituents whose probability exceeds a cutoff of 10−4 are allowed in the OSTAG chart. Designed to facilitate sister adjunction, we define our binarization scheme by example in which the added nodes, indicated by @, record both the parent and head child of the rule. NP @NN-NP @NN-NP DT @NN-NP JJ NN A compact TSG ca</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the insideoutside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>35--56</pages>
<contexts>
<context position="1624" citStr="Lari and Young, 1990" startWordPosition="234" endWordPosition="237">te Context-Free Grammars (CFGs) as a good balance between modeling power and asymptotic performance (Charniak, 1996). In constituent-based parsing work, the prevailing technique to combat this divide between efficient models and real world data has been to selectively strengthen the dependencies in a CFG by increasing the grammar size through methods such as symbol refinement (Petrov et al., 2006). Another approach is to employ a more powerful grammatical formalism and devise constraints and transformations that allow use of essential efficient algorithms such as the Inside-Outside algorithm (Lari and Young, 1990) and CYK parsing. Tree-Adjoining Grammar (TAG) is a natural Elif Yamangil Harvard University Cambridge, MA elif@eecs.harvard.edu Stuart Shieber Harvard University Cambridge, MA shieber@eecs.harvard.edu starting point for such methods as it is the canonical member of the mildly context-sensitive family, falling just above CFGs in the hierarchy of formal grammars. TAG has a crucial advantage over CFGs in its ability to represent long distance interactions in the face of the interposing variations that commonly manifest in natural language (Joshi and Schabes, 1997). Consider, for example, the sen</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S. J. Young. 1990. The estimation of stochastic context-free grammars using the insideoutside algorithm. Computer Speech and Language, pages 35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Mark jan Nederhof</author>
</authors>
<title>Regular approximation of context-free grammars through transformation.</title>
<date>2000</date>
<booktitle>In Robustness in language and speech technology.</booktitle>
<marker>Mohri, Nederhof, 2000</marker>
<rawString>Mehryar Mohri and Mark jan Nederhof. 2000. Regular approximation of context-free grammars through transformation. In Robustness in language and speech technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1403" citStr="Petrov et al., 2006" startWordPosition="200" endWordPosition="203">examples of the linguistically relevant structure that our variant makes possible. 1 Introduction While it is widely accepted that natural language is not context-free, practical limitations of existing algorithms motivate Context-Free Grammars (CFGs) as a good balance between modeling power and asymptotic performance (Charniak, 1996). In constituent-based parsing work, the prevailing technique to combat this divide between efficient models and real world data has been to selectively strengthen the dependencies in a CFG by increasing the grammar size through methods such as symbol refinement (Petrov et al., 2006). Another approach is to employ a more powerful grammatical formalism and devise constraints and transformations that allow use of essential efficient algorithms such as the Inside-Outside algorithm (Lari and Young, 1990) and CYK parsing. Tree-Adjoining Grammar (TAG) is a natural Elif Yamangil Harvard University Cambridge, MA elif@eecs.harvard.edu Stuart Shieber Harvard University Cambridge, MA shieber@eecs.harvard.edu starting point for such methods as it is the canonical member of the mildly context-sensitive family, falling just above CFGs in the hierarchy of formal grammars. TAG has a cruc</context>
<context position="19693" citStr="Petrov et al. (2006)" startWordPosition="3462" endWordPosition="3465">nd so parameterization of type (1) rules is unnecessary. If we decide on adjunction, one of the available type (3) rules is chosen from a multinomial. By conditioning the probability of adjunction on varying amounts of information about the node, alternative models can easily be defined. 5 Experiments As a proof of concept, we investigate OSTAG in the context of the classic Penn Treebank statistical parsing setup; training on section 2-21 and testing on section 23. For preprocessing, words that occur only once in the training data are mapped to the unknown categories employed in the parser of Petrov et al. (2006). We also applied the annotation from Klein and Manning (2003) that appends “-U” to each nonterminal node with a single child, drastically reducing the presence of looping unary chains. This allows the use of a coarse to fine parsing strategy (Charniak et al., 2006) in which a sentence is first parsed with the Maximum Likelihood PCFG and only constituents whose probability exceeds a cutoff of 10−4 are allowed in the OSTAG chart. Designed to facilitate sister adjunction, we define our binarization scheme by example in which the added nodes, indicated by @, record both the parent and head child </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 433– 440. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Richard C Waters</author>
</authors>
<title>Tree insertion grammar: a cubic-time, parsable formalism that lexicalizes context-free grammar without changing the trees produced.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>4--479</pages>
<contexts>
<context position="8724" citStr="Schabes and Waters (1995)" startWordPosition="1435" endWordPosition="1438"> We refer to the path between the root and foot nodes in an auxiliary tree as the spine of the tree. As mentioned above, the added power afforded by adjunction comes at a serious price in time complexity. As such, probabilistic modeling for TAG in its original form is uncommon. However, a large effort in non-probabilistic grammar induction has been performed through manual annotation with the XTAG project(Doran et al., 1994). 2.3 Tree Insertion Grammar Tree Insertion Grammars (TIGs) are a longstanding compromise between the intuitive expressivity of TAG and the algorithmic simplicity of CFGs. Schabes and Waters (1995) showed that by restricting the form of the auxiliary trees in A and the set of auxiliary trees that may adjoin at particular nodes, a TAG generates only context-free languages. The TIG restriction on auxiliary trees states that the foot node must occur as either the leftmost or rightmost leaf node. This introduces an important distinction between left, right, and wrapping auxiliary trees, of which only the first two are allowed in TIG. Furthermore, TIG disallows adjunction of left auxiliary trees on the spines of right auxiliary trees, and vice versa. This is to prevent the construction of wr</context>
</contexts>
<marker>Schabes, Waters, 1995</marker>
<rawString>Yves Schabes and Richard C. Waters. 1995. Tree insertion grammar: a cubic-time, parsable formalism that lexicalizes context-free grammar without changing the trees produced. Computational Linguistics, (4):479–513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Mathematical and computational aspects of lexicalized grammars.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="3271" citStr="Schabes, 1990" startWordPosition="488" endWordPosition="489">an be overcome to some extent with large amounts of data, redundant representation of patterns is particularly undesirable for systems that seek to extract coherent and concise information from text. TAG allows a linguistically motivated treatment of the example sentences above by generating the last two sentences through modification of the first, applying operations corresponding to negation and the use of a subordinate clause. Unfortunately, the added expressive power of TAG comes with O(n6) time complexity for essential algorithms on sentences of length n, as opposed to O(n3) for the CFG (Schabes, 1990). This makes TAG infeasible to analyze real world data in a reasonable time frame. In this paper, we define OSTAG, a new way to constrain TAG in a conceptually simple way so 302 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 302–310, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics S NP VP NP VP NP NN NNS VBP RB VP NP DT the lack computers do not NP NP PP VP NP PP PRP IN PRP VB I of them fear Figure 1: A simple Tree-Substitution Grammar using S as its start symbol. This grammar derives the sentences from a quote </context>
</contexts>
<marker>Schabes, 1990</marker>
<rawString>Yves Schabes. 1990. Mathematical and computational aspects of lexicalized grammars. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Shindo</author>
<author>Akinori Fujino</author>
<author>Masaaki Nagata</author>
</authors>
<title>Insertion operator for bayesian tree substitution grammars.</title>
<date>2011</date>
<pages>206--211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9653" citStr="Shindo et al., 2011" startWordPosition="1586" endWordPosition="1589">oduces an important distinction between left, right, and wrapping auxiliary trees, of which only the first two are allowed in TIG. Furthermore, TIG disallows adjunction of left auxiliary trees on the spines of right auxiliary trees, and vice versa. This is to prevent the construction of wrapping auxiliary trees, whose removal is essential for the simplified complexity of TIG. Several probabilistic models have been proposed for TIG. While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. Later approaches (Shindo et al., 2011; Yamangil and Shieber, 2012) were able to extend the non-parametric modeling of TSGs to TIG, providing methods for both modeling and grammar induction. 2.4 OSTAG Our new TAG variant is extremely simple. We allow arbitrary initial and auxiliary trees, and place only one restriction on adjunction: we disallow adjunction at any node on the spine of an auxiliary tree below the root (though we discuss relaxing that constraint in Section 4.2). We refer to this variant as Off Spine TAG (OSTAG) and note that it allows the use of full wrapping rules, which are forbidden in TIG. This targeted blocking </context>
<context position="22980" citStr="Shindo et al. (2011)" startWordPosition="4034" endWordPosition="4037">s parameterizations of adjunction with OSTAG indicate that, at least in the case of the Penn Treebank, the finer grained modeling of a full table of adjunction probabilities for each Goodman index OSTAG3 overcomes the danger of sparse data estimates. Not only does such a model lead to better parsing performance, but it uses adjunction more extensively than its more lightly parameterized alternatives. While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang (2000) (86.00) and Shindo et al. (2011) (85.03). The OSTAG constraint can be relaxed as described in Section 4.2 to allow any finite number of on-spine adjunctions without sacrificing contextfree form. However, the increase to the grammar constant quickly makes parsing with such models an arduous task. To determine the effect of such a relaxation, we allow a single level of on-spine adjunction using the adjunction model of OSTAG1, and estimate this model with EM on the training data. We parse sentences of length 40 or less in section 23 and observe that on-spine adjunction is never used in the MPD parses. This suggests that the OST</context>
</contexts>
<marker>Shindo, Fujino, Nagata, 2011</marker>
<rawString>Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata. 2011. Insertion operator for bayesian tree substitution grammars. pages 206–211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elif Yamangil</author>
<author>Stuart M Shieber</author>
</authors>
<title>Estimating compact yet rich tree insertion grammars.</title>
<date>2012</date>
<pages>110--114</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9682" citStr="Yamangil and Shieber, 2012" startWordPosition="1590" endWordPosition="1593">istinction between left, right, and wrapping auxiliary trees, of which only the first two are allowed in TIG. Furthermore, TIG disallows adjunction of left auxiliary trees on the spines of right auxiliary trees, and vice versa. This is to prevent the construction of wrapping auxiliary trees, whose removal is essential for the simplified complexity of TIG. Several probabilistic models have been proposed for TIG. While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. Later approaches (Shindo et al., 2011; Yamangil and Shieber, 2012) were able to extend the non-parametric modeling of TSGs to TIG, providing methods for both modeling and grammar induction. 2.4 OSTAG Our new TAG variant is extremely simple. We allow arbitrary initial and auxiliary trees, and place only one restriction on adjunction: we disallow adjunction at any node on the spine of an auxiliary tree below the root (though we discuss relaxing that constraint in Section 4.2). We refer to this variant as Off Spine TAG (OSTAG) and note that it allows the use of full wrapping rules, which are forbidden in TIG. This targeted blocking of recursion has similar moti</context>
</contexts>
<marker>Yamangil, Shieber, 2012</marker>
<rawString>Elif Yamangil and Stuart M. Shieber. 2012. Estimating compact yet rich tree insertion grammars. pages 110–114. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>