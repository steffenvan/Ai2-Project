<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000622">
<title confidence="0.995135">
Fertility Models for Statistical Natural Language Understanding
</title>
<author confidence="0.923493">
Stephen Della Pietra*, Mark Epstein, Salim Roukos, Todd Ward
</author>
<note confidence="0.956774">
IBM Thomas J. Watson Research Center
P.O. Box 218
</note>
<address confidence="0.465011">
Yorktown Heights, NY 10598, USA
(*Now With Renaissance Technologies, Stonybrook, NY, USA)
</address>
<email confidence="0.816774">
sdella@rentec.com
[meps/roukos/tward]@watson.ibm.com
</email>
<bodyText confidence="0.981719551724138">
Abstract model p,
Several recent efforts in statistical nat-
ural language understanding (NLU) have
focused on generating clumps of English
words from semantic meaning concepts
(Miller et al., 1995; Levin and Pierac-
cini, 1995; Epstein et al., 1996; Epstein,
1996). This paper extends the IBM Ma-
chine Translation Group&apos;s concept of fertil-
ity (Brown et al., 1993) to the generation
of clumps for natural language understand-
ing. The basic underlying intuition is that
a single concept may be expressed in Eng-
lish as many disjoint clump of words. We
present two fertility models which attempt
to capture this phenomenon. The first is
a Poisson model which leads to appeal-
ing computational simplicity. The second
is a general nonparametric fertility model.
The general model&apos;s parameters are boot-
strapped from the Poisson model and up-
dated by the EM algorithm. These fertility
models can be used to impose clump fertil-
ity structure on top of preexisting clump
generation models. Here, we present re-
sults for adding fertility structure to uni-
gram, bigram, and headword clump gener-
ation models on ARPA&apos;s Air Travel Infor-
mation Service (ATIS) domain.
</bodyText>
<sectionHeader confidence="0.999503" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.951503375">
The goal of a natural language understanding (NLU)
system is to interpret a user&apos;s request and respond
with an appropriate action. We view this interpre-
tation as translation from a natural language ex-
pression, E, into an equivalent expression, F, in
an unambigous formal language. Typically, this for-
mal language will be hand-crafted to enhance per-
formance on some task-specific domain. A statisti-
cal NLU system translates a request E as the most
likely formal expression P according to a probability
.fr= arg max p(FIE)--= arg max p(F, E).
over all F over all F
We have previously built a fully automatic statis-
tical NLU system (Epstein et al., 1996) based on the
source-channel factorization of the joint distribution
p(F,
</bodyText>
<equation confidence="0.61095">
p(F, E) = p(F)p(EIF).
</equation>
<bodyText confidence="0.992554727272727">
This factorization, which has proven effective in
speech recognition (Bahl, Jelinek, and Mercer,
1983), partitions the joint probability into an a pri-
ori intention model p(F), and a translation model
p(EIF) which models how a user might phrase a re-
quest F in English.
For the ATIS task, our formal language is a mi-
nor variant of the NL-Parse (Hemphill, Godfrey, and
Doddington, 1990) used by ARPA to annotate the
ATIS corpus. An example of a formal and natural
language pair is:
</bodyText>
<listItem confidence="0.99241725">
• F: List flights from New Orleans to Memphis
flying on Monday departing early_morning
• E: do you have any flights going to Memphis
leaving New Orleans early Monday morning
</listItem>
<bodyText confidence="0.997408052631579">
Here, the evidence for the formal language concept
`early_rnorning&apos; resides in the two disjoint clumps of
English &apos;early&apos; and &apos;morning&apos;. In this paper, we in-
troduce the notion of concept fertility into our trans-
lation models p(EIF) to capture this effect and the
more general linguistic phenomenon of embedded
clauses. Basically, this entails augmenting the trans-
lation model with terms of the form p(n I f), where n
is the number of clumps generated by the formal lan-
guage word f. The resulting model can be trained
automatically from a bilingual corpus of English and
formal language sentence pairs.
Other attempts at statistical NLU systems have
used various meaning representations such as con-
cepts in the AT&amp;T system (Levin and Pieraccini,
1995) or initial semantic structure in the BBN sys-
tem (Miller et al., 1995). Both of these systems re-
quire significant rule-based transformations to pro-
duce disambiguated interpretations which are then
</bodyText>
<page confidence="0.997243">
168
</page>
<bodyText confidence="0.994884132075472">
used to generate the SQL query for ATIS. More re-
cently, BBN has replaced handwritten rules with de-
cision trees (Miller et al., 1996). Moreover, both sys-
tems were trained using English annotated by hand
with segmentation and labeling, and both systems
produce a semantic representation which is forced
to preserve the time order expressed in the Eng-
lish. Interestingly, both the AT&amp;T and BBN sys-
tems generate words within a clump according to
bigram models. Other statistical approachs to NLU
include decision trees (Kuhn and Mori, 1995) and
neural nets (Gorin et al., 1991).
In earlier IBM translation systems (Brown et al.,
1993) each English word would be generated by,
or &amp;quot;aligned to&amp;quot;, exactly one formal language word.
This mapping between the English and formal lan-
guage expressions is called the &amp;quot;alignment&amp;quot;. In the
simplest case, the translation model is simply pro-
portional to the product of word-pair translation
probabilities, one per element in the alignment. In
these models, the alignment provides all of the struc-
ture in the translation model. The alignment is a
&amp;quot;hidden&amp;quot; quantity which is not annotated in the
training data and must be inferred indirectly. The
EM algorithm (Dempster, Laird, and Rubin, 1977)
used to train such &amp;quot;hidden&amp;quot; models requires us to
sum an expression over all possible alignments.
These early models were developed for French to
English translation. However, in NLU there is a fun-
damental asymmetry between the natural language
and the unambiguous formal language. Most no-
tably, one formal language word may frequently cor-
respond to whole English phrases. We added the
&amp;quot;clump&amp;quot;, an extra layer of structure, to accomodate
this phenomenon (Epstein et al., 1996). In this para-
digm, formal language words first generate a clump-
ing, or partition, of the word slots of the English
expression. Then, each clump is filled in according
to a translation model as before. The alignment is
defined between the formal language words and the
clumps. Then, both the alignment and the clumping
are hidden structures which must be summed over
to train the models.
Already, these models represent significant
progress. They learn automatically from a bilin-
gual corpus of English and formal language sen-
tences. They do not require linguistically knowl-
edgeable experts to tediously annotate a training
corpus. Rather, they rely upon a group of trans-
lators with significantly less linguistic knowledge to
produce a bilingual training corpus. The fertility
models introduced below maintain these benefits
while slightly improving performance.
</bodyText>
<sectionHeader confidence="0.9479415" genericHeader="keywords">
2 Fertility Clumping Translation
Models
</sectionHeader>
<bodyText confidence="0.97222817948718">
The rationale behind a clumping model is that
the input English can be clumped or bracketed into
phrases. Each clump is then generated from a sin-
gle formal language word using a translation model.
The notion of what constitutes a natural clumping
depends on the formal language. For example, sup-
pose the English sentence were:
I want to fly to Memphis please.
If the formal language for this sentence were:
LIST FLIGHTS TO LOCATION,
then the most plausible clumping would be:
[I want] [to fly] [to] [Memphis] [please],
for which we would expect &amp;quot;[I want]&amp;quot; and &amp;quot;[please]&amp;quot;
to be generated from &amp;quot;LIST&amp;quot;, &amp;quot;[to fly]&amp;quot; from
&amp;quot;FLIGHTS&amp;quot;, &amp;quot;[to]&amp;quot; from &amp;quot;TO, and &amp;quot;[Memphis]&amp;quot;
from LOCATION. Similarly, if the formal language
were:
LIST FLIGHTS DESTINATION_LOC
then the most natural clumping would be:
[I want] [to fly] [to Memphis] [please],
in which we would now expect &amp;quot;[to Memphis]&amp;quot; to be
generated by &amp;quot;DESTINATION_LOC&amp;quot; .
Although these clumpings are perhaps the most
natural, neither the clumping nor the alignment is
annotated in our training data. Instead, both the
alignment and the clumping are viewed as &amp;quot;hidden&amp;quot;
quantities for which all values are possible with some
probability. The EM algorithm is used to produce a
maximum likelihood estimate of the model parame-
ters, taking into account all possible alignments and
clumpings.
In the discussion of fertility models we denote an
English sentence by E, which consists of i(E) words.
Similarly, we denote the formal language by F, a
tuple of order i(F), whose individual elements are
denoted by fj. A clumping for a sentence partitions
E into a tuple of clumps C. The number of clumps
in C is denoted by 1(C), and is an integer in the
range 1 • • • 1(E). A particular clump is denoted by
</bodyText>
<listItem confidence="0.997925">
• where i E {1• • •1(C)}. The number of words in
• is denoted by t(ci). c1 begins at the first word
</listItem>
<bodyText confidence="0.999303642857143">
in the sentence, and ct(c) ends at the last word in
the sentence. The clumps form a proper partition
of E. All the words in a clump c must align to the
same f. An alignment between E and F determines
which f generates each clump of E in C. Similarly,
A denotes the alignment, with 1(A) = 1(C), and the
ai denote the formal language word to which each e
in ci align. The individual words in a clump c are
represented by el • • et().
For all fertility models, the fundamental parame-
ters are the joint probabilities p(E, C, A, F). Since
the clumping and alignment are hidden, to compute
the probability that E is generated by F, one calcu-
lates:
</bodyText>
<equation confidence="0.620048">
p(E I F) = p(E ,C, A I F)
C,A
</equation>
<page confidence="0.995624">
169
</page>
<sectionHeader confidence="0.973684" genericHeader="method">
3 General and Poisson Fertility
</sectionHeader>
<bodyText confidence="0.998891">
In the general fertility model, the translation prob-
ability with &amp;quot;revealed&amp;quot; alignment and clumping is
</bodyText>
<equation confidence="0.9975475">
p(E,C, A I F) =
t(P) 1(c)
AN I fi)ni! P( I fa,) (1)
i=i J=1
t(c)
p(c If) = P(e(c) I f)llP(ei fc) (2)
</equation>
<bodyText confidence="0.999900444444444">
where p(ni I fi) is the fertility probability of gen-
erating ni clumps by formal word f. Note that
E ni = L. The factorial terms combine to give an
inverse multinomial coefficient which is the uniform
probability distribution for the alignment A of F to
C.
It appears that the computation of the likelihood,
which is the sum of 2(F)(P(F) 1)1(E)-1 product
terms, is exponential. Although dynamic program-
ming can reduce the complexity, there remain an
exponentially large number of terms to evaluate in
each iteration of the EM algorithm. We resort to
a top-N approximation to the EM sum for the gen-
eral model, summing over candidate clumpings and
alignments proposed by the Poisson fertility model
developed below.
If one assumes that the fertility is modeled by the
Poisson distribution with mean fertility Af
</bodyText>
<equation confidence="0.986902333333333">
e-Af Aft&apos;
P(n f) = (3)
q(c I f) =
</equation>
<bodyText confidence="0.999054166666667">
where Ain has been absorbed into the effective
clump score q(c I 1). In this form, it is particu-
larly simple to explicitly sum over all alignments A
to obtain p(E, C I F) by repeated application of the
distributive law. The resulting polynomial time ex-
pressions are:
</bodyText>
<equation confidence="0.9898234">
1
t(F) t(C)
p(E,C F) H CA/. ri ii(c; 1 F) (7)
d(c F) = q(c I n (8)
fEF
</equation>
<bodyText confidence="0.999334666666666">
The i(C I F) values for all possible clumpings
can be calculated in 0(i(E)2((F)) time if the maxi-
mum clump size is unbounded, and in 0(i(E)E(F))
if bounded. The Viterbi decoding algorithm (For-
ney, 1973) is used to calculate p(E j L, F) from
these expressions. The Viterbi algorithm produces
a score which is the sum over all possible clump-
ings for a fixed L. This score must then normal-
ized by the exp(— Ef(Fi) Af, )/L! factor. The EM
count accumulation is done using an adaptation
of the Baum-Welch algorithm (Baum, 1972) which
searches through the space of all possible clumpings,
first considering 1 clump, then 2, and so forth.
Initial values for p(e I f) are bootstrapped from
Model 1 (Epstein et al., 1996) with the initial mean
fertilities Af set to 1. We also fixed the maximum
clump size at 5 words. Empirically, we found it ben-
eficial to hold the p(e I f) parameters fixed for 20
iterations to allow the other parameters to train to
reasonable values. After training, the translation
probabilities and clump lengths are smoothed using
deleted interpolation (Bahl, Jelinek, and Mercer,
1983).
Since we have been unable to find a polynomial
time algorithm to train the general fertility model,
we use the Poisson model to &amp;quot;expose&amp;quot; the hidden
alignments. The Poisson fertility model gives the
most likely 1000 clumpings and alignments, which
are then rescored according to the current general
fertility model parameters. This gives fractional
counts for each of the 1000 alignments, which are
then used to update the the general fertility model
parameters.
</bodyText>
<sectionHeader confidence="0.99781" genericHeader="method">
4 Improved Clump Modeling
</sectionHeader>
<bodyText confidence="0.998511">
In both the Poisson and general fertility models, the
computation of p(cif) in equation 2 uses a unigram
model. Each English word ei is generated with prob-
ability p(ei IL). Two more powerful modeling tech-
niques for modeling clump generation are n-gram
language models (Miller et al., 1995; Levin and Pier-
accini, 1995; Epstein, 1996), and headword language
models (Epstein, 1996). A bigram language model
uses:
</bodyText>
<equation confidence="0.991934">
P( f) =
p(4c) I f)P(ei I bdY, ic)12(bdY I et(c), fc) X
1(c)
HkeiI ei—i, fc)
i.2
</equation>
<bodyText confidence="0.999588846153846">
where bdy is a special marker to delimit the begin-
ning and end of the clump.
A headword language model uses two unigram
models, a headword model and a non-headword
model. Each clump is required to have a headword.
All other words are non-headwords. The identity of
a clump&apos;s headword is hidden, hence it is necessary
n!
then a polynomial time training algorithm exists.
The simplicity arises from the fortuitous cancella-
tion of n! between the Poisson distribution and the
uniform alignment probability. Substituting equa-
tion 3 into equation 1 yields:
</bodyText>
<equation confidence="0.960501777777778">
p(E, C, A I F)
t(F) t(C)
1
H f&apos; Af,n‘r1P(Ci fa,) (4)
t(F) t(C)
1
— L! CAI&apos; II q(ci I fa.,)
j.1
Af p(c f),
</equation>
<page confidence="0.918183">
170
</page>
<table confidence="0.999033166666667">
Word A p (n = 0) p (n = 1) p (n = 2) p (n &gt;= 3)
late 1.49 .00 .62 .28 .10
early 1.55 .00 .89 .03 .08
morning 1.40 .01 .85 .11 .03
afternoon 1.62 .00 .85 .12 .03
early_morning 2.50 .00 .16 .69 .15
</table>
<tableCaption confidence="0.996924">
Table 1: Trained Poisson and General Fertility
</tableCaption>
<table confidence="0.580573846153846">
Word Top p(elf) Score Top ,h Score Top Pnonhead(elf) Score
ead(elf)
early early .37 early .68 an .30
an .22 i .23 flight .29
i .09 day .06 would .10
morning morning .63 morning .75 the .43
in .12 leaving .06 in .37
leaving .05 flights .05 of .08
List the .21 show .49 me .45
me .19 what .17 the .19
show .18 give .07 all .12
what .17 you .06 are .05
please .04 list .05 please .05
</table>
<tableCaption confidence="0.949457">
Table 2: Trained Translation Probabilities using Poisson Fertility
</tableCaption>
<table confidence="0.999960090909091">
Model DEC93 DEC93a
1 75.00 75.22
Clump 74.78 77.01
Clump-HW 75.89 78.35
Clump-BG 76.79 78.12
Poisson 78.12 81.25
Poisson-HW 78.12 81.25
Poisson-BG 78.12 81.25
General 79.91 82.59
General-HW 79.91 79.91
General-BG 73.21 83.04
</table>
<tableCaption confidence="0.99934">
Table 3: Class A CAS on Patterns for DEC93
</tableCaption>
<page confidence="0.98787">
171
</page>
<equation confidence="0.8623834">
to sum over all possible headwords:
P(c1 =
„ t(c)
P(&apos;(c) &apos; Phead(ei I fc)11Pnonhead(ei I fc)
AC)
</equation>
<sectionHeader confidence="0.906712" genericHeader="method">
5 Example Fertilities
</sectionHeader>
<bodyText confidence="0.999987137931034">
To illustrate how well fertility captures simple cases
of embedding, trained fertilities are shown in table 1
for several formal language words denoting time in-
tervals. As expected, &amp;quot;early_rnorning&amp;quot; dominantly
produces two clumps, but can produce either one or
three clumps with reasonable probability. &amp;quot;morn-
ing&amp;quot; and &amp;quot;afternoon&amp;quot; train to comparable fertilities
and preferentially generate a single clump. Another
interesting case is the formal language token &amp;quot;List&amp;quot;
which trains to a A of 0.62 indicating that it fre-
quently generates no English text. As a further
check, the A values for &amp;quot;from&amp;quot;, &amp;quot;to&amp;quot;, and the two
special classed words &amp;quot;CITY-1&amp;quot; and &amp;quot;CITY-2&amp;quot; are
near 1, ranging between 0.96 and 1.17.
Some trained translation probabilities are shown
for the unigram and headword models in table 2.
The formal language words have captured reason-
able English words for their most likely transla-
tion or headword translation. However, &amp;quot;early&amp;quot;
and &amp;quot;morning&amp;quot; have fairly undesirable looking sec-
ond and third choices. The reason for this is that
these undesirable words are frequently adjacent to
the English words &amp;quot;early&amp;quot; and &amp;quot;morning&amp;quot;; hence
the training algorithm includes contributions with
two word clumps containing these extraneous words.
This is the price we pay for not using supervised
training data. Intriguingly, the headword model is
more strongly biased towards the likely translations
and has a smoother tail than the unigram model.
</bodyText>
<sectionHeader confidence="0.999961" genericHeader="conclusions">
6 Results
</sectionHeader>
<bodyText confidence="0.999955777777778">
The translation models were trained with 5627
context-independent ATIS sentences and smoothed
with 600 sentences. In addition, 3567 training sen-
tences were manually aligned and included in a sep-
arate training experiment. This allows comparison
between an unannotated corpus and a partially an-
notated one.
We employ a trivial decoder and language model
since our emphasis is on evaluating the performance
of different translation models. Our decoder is a sim-
ple pattern matcher. That is, we accumulate the dif-
ferent formal language patterns seen in the training
set, and score each of them on the test set. The lan-
guage model is just the unsmoothed unigram prob-
ability distribution of the patterns. This LM has a
10% chance of not including a test pattern and its
use leads to pessimistic performance estimates. A
more general language model for ATIS is presented
in (Koppelman et al., 1995). Answers are gener-
ated by an SQL program which is a deterministically
constructed from the formal language of our system.
The accuracy of these database answers is measured
using ARPA&apos;s Common Answer Specification (CAS)
metric.
The results are presented in table 3 for ARPA&apos;s
December 1993 blind test set. The column headed
DEC93 reports results on unsupervised training
data, while the column entitled DEC93a contains the
results from using models trained on the partially
annotated corpus. The rows correspond to various
translation models. Model 1 is the word-pair trans-
lation model used in simple machine translation and
understanding models (Brown et al., 1993; Epstein
et al., 1996). The models labeled &amp;quot;Clump&amp;quot; use a
basic clumped model without fertility. The mod-
els labeled &amp;quot;Poisson&amp;quot; and &amp;quot;General&amp;quot; use the Poisson
and general fertility models presented in this paper.
The &amp;quot;HW&amp;quot; and &amp;quot;BG&amp;quot; suffixes indicate the results
when p(cif) is computed with a headword or bigram
model.
The partially annotated corpus provides an in-
crease in performance of about 2-3% for most mod-
els. For General-LM, results increased by 8-10%.
The Poisson and general fertility models show a 2-
5% gain in performance over the basic clump model
when using the partially annotated corpus. This is
a reduction of the error rate by 10-20%. The unan-
notated corpus also shows a comparable gain.
Acknowledgement: This work was sponsored
in part by ARPA and monitored by Fort Huachuca
HJ1500-4309-0513. The views and conclusions con-
tained in this document should not be interpreted
as representing the official policies of the U.S. Gov-
ernment.
</bodyText>
<sectionHeader confidence="0.998857" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998562722222222">
Bahl, Lalit R., Frederick Jelinek, and Robert L.
Mercer. 1983. A maximum likelihood approach
to continuous speech recognition. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, PAMI-5(2):179-190, March.
Baum, L.E. 1972. An inequality and associated
maximization technique in statistical estimation
of probabilistic functions of a Markov process. In-
equalities, 3:1-8.
Brown, Peter F., Stephen A. DellaPietra, Vincent J.
DellaPietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation:
Parameter estimation. In Computational Linguis-
tics, pages 19(2):263-311, June.
Dempster, A.P., N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical
Society, 39(B):1-38.
</reference>
<page confidence="0.976779">
172
</page>
<reference confidence="0.9965175">
Epstein, M. 1996. Statistical Source Channel Mod-
els for Natural Language Understanding. Ph.D.
thesis, New York University, September.
Epstein, M., K. Papineni, S. Roukos, T. Ward, and
S. Della Pietra. 1996. Statistical natural lan-
guage understanding using hidden clumpings. In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, pages
176-179, Atlanta, Georgia, May.
Forney, G. David. 1973. The viterbi algorithm. Pro-
ceedings of the IEEE, 61:268-278, March.
Gorin, A., S. Levinson, A. Gertner, and E. Goldman.
1991. Adaptive acquisition of language. Com-
puter Speech and Language, 5:101-132.
Hemphill, C., J. Godfrey, and G. Doddington. 1990.
The ATIS spoken language systems pilot corpus.
In Proceedings of the DARPA Speech and Natural
Language Workshop, pages 96-101, Hidden Valley,
PA, June. Morgan Kaufmann Publishers, Inc.
Koppelman, J., S. Della Pietra, M. Epstein, and
S. Roukos. 1995. A statistical approach to lan-
guage modeling for the ATIS task. In Proceedings
of the Spoken Language Systems Workshop, pages
1785-1788, Madrid, Spain, September.
Kuhn, R. and R. De Mori. 1995. The application of
semantic classification trees to natural language
understanding. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 17(5):449-460,
May.
Levin, E. and R. Pieraccini. 1995. Chronus, the next
generation. In Proceedings of the Spoken Lan-
guage Systems Workshop, pages 269-271, Austin,
Texas, January.
Miller, S., M. Bates, R. Bobrow, R. Ingria,
J. Makhoul, and R. Schwartz. 1995. Recent
progress in hidden understanding models. In Pro-
ceedings of the Spoken Language Systems Work-
shop, pages 276-279, Austin, Texas, January.
Miller, S., D. Stallard, R. Bobrow, and R. Schwartz.
1996. A fully statistical approach to natural lan-
guage interfaces. In Proceedings of the 34th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 55-61, Santa Cruz, CA,
June. Morgan Kaufmann Publishers, Inc.
</reference>
<page confidence="0.999089">
173
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.369548">
<title confidence="0.999945">Fertility Models for Statistical Natural Language Understanding</title>
<author confidence="0.998836">Stephen Della Pietra</author>
<author confidence="0.998836">Mark Epstein</author>
<author confidence="0.998836">Salim Roukos</author>
<author confidence="0.998836">Todd Ward</author>
<affiliation confidence="0.953994">J. Watson Research Center</affiliation>
<address confidence="0.890170666666667">P.O. Box 218 Yorktown Heights, NY 10598, USA (*Now With Renaissance Technologies, Stonybrook, NY, USA)</address>
<email confidence="0.97714">sdella@rentec.com[meps/roukos/tward]@watson.ibm.com</email>
<abstract confidence="0.9838075">p, Several recent efforts in statistical natural language understanding (NLU) have focused on generating clumps of English words from semantic meaning concepts (Miller et al., 1995; Levin and Pieraccini, 1995; Epstein et al., 1996; Epstein, 1996). This paper extends the IBM Ma- Translation Group&apos;s concept of fertilet al., 1993) to the generation of clumps for natural language understanding. The basic underlying intuition is that a single concept may be expressed in English as many disjoint clump of words. We present two fertility models which attempt to capture this phenomenon. The first is a Poisson model which leads to appealing computational simplicity. The second is a general nonparametric fertility model. The general model&apos;s parameters are bootstrapped from the Poisson model and updated by the EM algorithm. These fertility models can be used to impose clump fertility structure on top of preexisting clump generation models. Here, we present results for adding fertility structure to unigram, bigram, and headword clump genermodels on ARPA&apos;s Travel Infor-</abstract>
<intro confidence="0.838199">Service (ATIS)</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>A maximum likelihood approach to continuous speech recognition.</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>5--2</pages>
<contexts>
<context position="2322" citStr="Bahl, Jelinek, and Mercer, 1983" startWordPosition="359" endWordPosition="363"> an equivalent expression, F, in an unambigous formal language. Typically, this formal language will be hand-crafted to enhance performance on some task-specific domain. A statistical NLU system translates a request E as the most likely formal expression P according to a probability .fr= arg max p(FIE)--= arg max p(F, E). over all F over all F We have previously built a fully automatic statistical NLU system (Epstein et al., 1996) based on the source-channel factorization of the joint distribution p(F, p(F, E) = p(F)p(EIF). This factorization, which has proven effective in speech recognition (Bahl, Jelinek, and Mercer, 1983), partitions the joint probability into an a priori intention model p(F), and a translation model p(EIF) which models how a user might phrase a request F in English. For the ATIS task, our formal language is a minor variant of the NL-Parse (Hemphill, Godfrey, and Doddington, 1990) used by ARPA to annotate the ATIS corpus. An example of a formal and natural language pair is: • F: List flights from New Orleans to Memphis flying on Monday departing early_morning • E: do you have any flights going to Memphis leaving New Orleans early Monday morning Here, the evidence for the formal language conce</context>
<context position="11450" citStr="Bahl, Jelinek, and Mercer, 1983" startWordPosition="1924" endWordPosition="1928">an adaptation of the Baum-Welch algorithm (Baum, 1972) which searches through the space of all possible clumpings, first considering 1 clump, then 2, and so forth. Initial values for p(e I f) are bootstrapped from Model 1 (Epstein et al., 1996) with the initial mean fertilities Af set to 1. We also fixed the maximum clump size at 5 words. Empirically, we found it beneficial to hold the p(e I f) parameters fixed for 20 iterations to allow the other parameters to train to reasonable values. After training, the translation probabilities and clump lengths are smoothed using deleted interpolation (Bahl, Jelinek, and Mercer, 1983). Since we have been unable to find a polynomial time algorithm to train the general fertility model, we use the Poisson model to &amp;quot;expose&amp;quot; the hidden alignments. The Poisson fertility model gives the most likely 1000 clumpings and alignments, which are then rescored according to the current general fertility model parameters. This gives fractional counts for each of the 1000 alignments, which are then used to update the the general fertility model parameters. 4 Improved Clump Modeling In both the Poisson and general fertility models, the computation of p(cif) in equation 2 uses a unigram mode</context>
</contexts>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Bahl, Lalit R., Frederick Jelinek, and Robert L. Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-5(2):179-190, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process.</title>
<date>1972</date>
<journal>Inequalities,</journal>
<pages>3--1</pages>
<contexts>
<context position="10873" citStr="Baum, 1972" startWordPosition="1830" endWordPosition="1831"> expressions are: 1 t(F) t(C) p(E,C F) H CA/. ri ii(c; 1 F) (7) d(c F) = q(c I n (8) fEF The i(C I F) values for all possible clumpings can be calculated in 0(i(E)2((F)) time if the maximum clump size is unbounded, and in 0(i(E)E(F)) if bounded. The Viterbi decoding algorithm (Forney, 1973) is used to calculate p(E j L, F) from these expressions. The Viterbi algorithm produces a score which is the sum over all possible clumpings for a fixed L. This score must then normalized by the exp(— Ef(Fi) Af, )/L! factor. The EM count accumulation is done using an adaptation of the Baum-Welch algorithm (Baum, 1972) which searches through the space of all possible clumpings, first considering 1 clump, then 2, and so forth. Initial values for p(e I f) are bootstrapped from Model 1 (Epstein et al., 1996) with the initial mean fertilities Af set to 1. We also fixed the maximum clump size at 5 words. Empirically, we found it beneficial to hold the p(e I f) parameters fixed for 20 iterations to allow the other parameters to train to reasonable values. After training, the translation probabilities and clump lengths are smoothed using deleted interpolation (Bahl, Jelinek, and Mercer, 1983). Since we have been u</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>Baum, L.E. 1972. An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process. Inequalities, 3:1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A DellaPietra</author>
<author>Vincent J DellaPietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="677" citStr="Brown et al., 1993" startWordPosition="92" endWordPosition="95">nding Stephen Della Pietra*, Mark Epstein, Salim Roukos, Todd Ward IBM Thomas J. Watson Research Center P.O. Box 218 Yorktown Heights, NY 10598, USA (*Now With Renaissance Technologies, Stonybrook, NY, USA) sdella@rentec.com [meps/roukos/tward]@watson.ibm.com Abstract model p, Several recent efforts in statistical natural language understanding (NLU) have focused on generating clumps of English words from semantic meaning concepts (Miller et al., 1995; Levin and Pieraccini, 1995; Epstein et al., 1996; Epstein, 1996). This paper extends the IBM Machine Translation Group&apos;s concept of fertility (Brown et al., 1993) to the generation of clumps for natural language understanding. The basic underlying intuition is that a single concept may be expressed in English as many disjoint clump of words. We present two fertility models which attempt to capture this phenomenon. The first is a Poisson model which leads to appealing computational simplicity. The second is a general nonparametric fertility model. The general model&apos;s parameters are bootstrapped from the Poisson model and updated by the EM algorithm. These fertility models can be used to impose clump fertility structure on top of preexisting clump genera</context>
<context position="4455" citStr="Brown et al., 1993" startWordPosition="715" endWordPosition="718">erate the SQL query for ATIS. More recently, BBN has replaced handwritten rules with decision trees (Miller et al., 1996). Moreover, both systems were trained using English annotated by hand with segmentation and labeling, and both systems produce a semantic representation which is forced to preserve the time order expressed in the English. Interestingly, both the AT&amp;T and BBN systems generate words within a clump according to bigram models. Other statistical approachs to NLU include decision trees (Kuhn and Mori, 1995) and neural nets (Gorin et al., 1991). In earlier IBM translation systems (Brown et al., 1993) each English word would be generated by, or &amp;quot;aligned to&amp;quot;, exactly one formal language word. This mapping between the English and formal language expressions is called the &amp;quot;alignment&amp;quot;. In the simplest case, the translation model is simply proportional to the product of word-pair translation probabilities, one per element in the alignment. In these models, the alignment provides all of the structure in the translation model. The alignment is a &amp;quot;hidden&amp;quot; quantity which is not annotated in the training data and must be inferred indirectly. The EM algorithm (Dempster, Laird, and Rubin, 1977) used t</context>
<context position="17181" citStr="Brown et al., 1993" startWordPosition="2886" endWordPosition="2889">is a deterministically constructed from the formal language of our system. The accuracy of these database answers is measured using ARPA&apos;s Common Answer Specification (CAS) metric. The results are presented in table 3 for ARPA&apos;s December 1993 blind test set. The column headed DEC93 reports results on unsupervised training data, while the column entitled DEC93a contains the results from using models trained on the partially annotated corpus. The rows correspond to various translation models. Model 1 is the word-pair translation model used in simple machine translation and understanding models (Brown et al., 1993; Epstein et al., 1996). The models labeled &amp;quot;Clump&amp;quot; use a basic clumped model without fertility. The models labeled &amp;quot;Poisson&amp;quot; and &amp;quot;General&amp;quot; use the Poisson and general fertility models presented in this paper. The &amp;quot;HW&amp;quot; and &amp;quot;BG&amp;quot; suffixes indicate the results when p(cif) is computed with a headword or bigram model. The partially annotated corpus provides an increase in performance of about 2-3% for most models. For General-LM, results increased by 8-10%. The Poisson and general fertility models show a 2- 5% gain in performance over the basic clump model when using the partially annotated corpus.</context>
</contexts>
<marker>Brown, DellaPietra, DellaPietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. DellaPietra, Vincent J. DellaPietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. In Computational Linguistics, pages 19(2):263-311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<contexts>
<context position="5047" citStr="Dempster, Laird, and Rubin, 1977" startWordPosition="809" endWordPosition="813">translation systems (Brown et al., 1993) each English word would be generated by, or &amp;quot;aligned to&amp;quot;, exactly one formal language word. This mapping between the English and formal language expressions is called the &amp;quot;alignment&amp;quot;. In the simplest case, the translation model is simply proportional to the product of word-pair translation probabilities, one per element in the alignment. In these models, the alignment provides all of the structure in the translation model. The alignment is a &amp;quot;hidden&amp;quot; quantity which is not annotated in the training data and must be inferred indirectly. The EM algorithm (Dempster, Laird, and Rubin, 1977) used to train such &amp;quot;hidden&amp;quot; models requires us to sum an expression over all possible alignments. These early models were developed for French to English translation. However, in NLU there is a fundamental asymmetry between the natural language and the unambiguous formal language. Most notably, one formal language word may frequently correspond to whole English phrases. We added the &amp;quot;clump&amp;quot;, an extra layer of structure, to accomodate this phenomenon (Epstein et al., 1996). In this paradigm, formal language words first generate a clumping, or partition, of the word slots of the English expres</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A.P., N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(B):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Epstein</author>
</authors>
<title>Statistical Source Channel Models for Natural Language Understanding.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>New York University,</institution>
<contexts>
<context position="12273" citStr="Epstein, 1996" startWordPosition="2059" endWordPosition="2060">ely 1000 clumpings and alignments, which are then rescored according to the current general fertility model parameters. This gives fractional counts for each of the 1000 alignments, which are then used to update the the general fertility model parameters. 4 Improved Clump Modeling In both the Poisson and general fertility models, the computation of p(cif) in equation 2 uses a unigram model. Each English word ei is generated with probability p(ei IL). Two more powerful modeling techniques for modeling clump generation are n-gram language models (Miller et al., 1995; Levin and Pieraccini, 1995; Epstein, 1996), and headword language models (Epstein, 1996). A bigram language model uses: P( f) = p(4c) I f)P(ei I bdY, ic)12(bdY I et(c), fc) X 1(c) HkeiI ei—i, fc) i.2 where bdy is a special marker to delimit the beginning and end of the clump. A headword language model uses two unigram models, a headword model and a non-headword model. Each clump is required to have a headword. All other words are non-headwords. The identity of a clump&apos;s headword is hidden, hence it is necessary n! then a polynomial time training algorithm exists. The simplicity arises from the fortuitous cancellation of n! between the</context>
</contexts>
<marker>Epstein, 1996</marker>
<rawString>Epstein, M. 1996. Statistical Source Channel Models for Natural Language Understanding. Ph.D. thesis, New York University, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Epstein</author>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>S Della Pietra</author>
</authors>
<title>Statistical natural language understanding using hidden clumpings.</title>
<date>1996</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>176--179</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="2125" citStr="Epstein et al., 1996" startWordPosition="332" endWordPosition="335"> understanding (NLU) system is to interpret a user&apos;s request and respond with an appropriate action. We view this interpretation as translation from a natural language expression, E, into an equivalent expression, F, in an unambigous formal language. Typically, this formal language will be hand-crafted to enhance performance on some task-specific domain. A statistical NLU system translates a request E as the most likely formal expression P according to a probability .fr= arg max p(FIE)--= arg max p(F, E). over all F over all F We have previously built a fully automatic statistical NLU system (Epstein et al., 1996) based on the source-channel factorization of the joint distribution p(F, p(F, E) = p(F)p(EIF). This factorization, which has proven effective in speech recognition (Bahl, Jelinek, and Mercer, 1983), partitions the joint probability into an a priori intention model p(F), and a translation model p(EIF) which models how a user might phrase a request F in English. For the ATIS task, our formal language is a minor variant of the NL-Parse (Hemphill, Godfrey, and Doddington, 1990) used by ARPA to annotate the ATIS corpus. An example of a formal and natural language pair is: • F: List flights from Ne</context>
<context position="5525" citStr="Epstein et al., 1996" startWordPosition="886" endWordPosition="889">idden&amp;quot; quantity which is not annotated in the training data and must be inferred indirectly. The EM algorithm (Dempster, Laird, and Rubin, 1977) used to train such &amp;quot;hidden&amp;quot; models requires us to sum an expression over all possible alignments. These early models were developed for French to English translation. However, in NLU there is a fundamental asymmetry between the natural language and the unambiguous formal language. Most notably, one formal language word may frequently correspond to whole English phrases. We added the &amp;quot;clump&amp;quot;, an extra layer of structure, to accomodate this phenomenon (Epstein et al., 1996). In this paradigm, formal language words first generate a clumping, or partition, of the word slots of the English expression. Then, each clump is filled in according to a translation model as before. The alignment is defined between the formal language words and the clumps. Then, both the alignment and the clumping are hidden structures which must be summed over to train the models. Already, these models represent significant progress. They learn automatically from a bilingual corpus of English and formal language sentences. They do not require linguistically knowledgeable experts to tedious</context>
<context position="11063" citStr="Epstein et al., 1996" startWordPosition="1861" endWordPosition="1864">e maximum clump size is unbounded, and in 0(i(E)E(F)) if bounded. The Viterbi decoding algorithm (Forney, 1973) is used to calculate p(E j L, F) from these expressions. The Viterbi algorithm produces a score which is the sum over all possible clumpings for a fixed L. This score must then normalized by the exp(— Ef(Fi) Af, )/L! factor. The EM count accumulation is done using an adaptation of the Baum-Welch algorithm (Baum, 1972) which searches through the space of all possible clumpings, first considering 1 clump, then 2, and so forth. Initial values for p(e I f) are bootstrapped from Model 1 (Epstein et al., 1996) with the initial mean fertilities Af set to 1. We also fixed the maximum clump size at 5 words. Empirically, we found it beneficial to hold the p(e I f) parameters fixed for 20 iterations to allow the other parameters to train to reasonable values. After training, the translation probabilities and clump lengths are smoothed using deleted interpolation (Bahl, Jelinek, and Mercer, 1983). Since we have been unable to find a polynomial time algorithm to train the general fertility model, we use the Poisson model to &amp;quot;expose&amp;quot; the hidden alignments. The Poisson fertility model gives the most likely </context>
<context position="17204" citStr="Epstein et al., 1996" startWordPosition="2890" endWordPosition="2893">ly constructed from the formal language of our system. The accuracy of these database answers is measured using ARPA&apos;s Common Answer Specification (CAS) metric. The results are presented in table 3 for ARPA&apos;s December 1993 blind test set. The column headed DEC93 reports results on unsupervised training data, while the column entitled DEC93a contains the results from using models trained on the partially annotated corpus. The rows correspond to various translation models. Model 1 is the word-pair translation model used in simple machine translation and understanding models (Brown et al., 1993; Epstein et al., 1996). The models labeled &amp;quot;Clump&amp;quot; use a basic clumped model without fertility. The models labeled &amp;quot;Poisson&amp;quot; and &amp;quot;General&amp;quot; use the Poisson and general fertility models presented in this paper. The &amp;quot;HW&amp;quot; and &amp;quot;BG&amp;quot; suffixes indicate the results when p(cif) is computed with a headword or bigram model. The partially annotated corpus provides an increase in performance of about 2-3% for most models. For General-LM, results increased by 8-10%. The Poisson and general fertility models show a 2- 5% gain in performance over the basic clump model when using the partially annotated corpus. This is a reduction of</context>
</contexts>
<marker>Epstein, Papineni, Roukos, Ward, Pietra, 1996</marker>
<rawString>Epstein, M., K. Papineni, S. Roukos, T. Ward, and S. Della Pietra. 1996. Statistical natural language understanding using hidden clumpings. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 176-179, Atlanta, Georgia, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G David Forney</author>
</authors>
<title>The viterbi algorithm.</title>
<date>1973</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>61--268</pages>
<contexts>
<context position="10553" citStr="Forney, 1973" startWordPosition="1771" endWordPosition="1773">on distribution with mean fertility Af e-Af Aft&apos; P(n f) = (3) q(c I f) = where Ain has been absorbed into the effective clump score q(c I 1). In this form, it is particularly simple to explicitly sum over all alignments A to obtain p(E, C I F) by repeated application of the distributive law. The resulting polynomial time expressions are: 1 t(F) t(C) p(E,C F) H CA/. ri ii(c; 1 F) (7) d(c F) = q(c I n (8) fEF The i(C I F) values for all possible clumpings can be calculated in 0(i(E)2((F)) time if the maximum clump size is unbounded, and in 0(i(E)E(F)) if bounded. The Viterbi decoding algorithm (Forney, 1973) is used to calculate p(E j L, F) from these expressions. The Viterbi algorithm produces a score which is the sum over all possible clumpings for a fixed L. This score must then normalized by the exp(— Ef(Fi) Af, )/L! factor. The EM count accumulation is done using an adaptation of the Baum-Welch algorithm (Baum, 1972) which searches through the space of all possible clumpings, first considering 1 clump, then 2, and so forth. Initial values for p(e I f) are bootstrapped from Model 1 (Epstein et al., 1996) with the initial mean fertilities Af set to 1. We also fixed the maximum clump size at 5 </context>
</contexts>
<marker>Forney, 1973</marker>
<rawString>Forney, G. David. 1973. The viterbi algorithm. Proceedings of the IEEE, 61:268-278, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gorin</author>
<author>S Levinson</author>
<author>A Gertner</author>
<author>E Goldman</author>
</authors>
<title>Adaptive acquisition of language.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--101</pages>
<contexts>
<context position="4398" citStr="Gorin et al., 1991" startWordPosition="706" endWordPosition="709">ambiguated interpretations which are then 168 used to generate the SQL query for ATIS. More recently, BBN has replaced handwritten rules with decision trees (Miller et al., 1996). Moreover, both systems were trained using English annotated by hand with segmentation and labeling, and both systems produce a semantic representation which is forced to preserve the time order expressed in the English. Interestingly, both the AT&amp;T and BBN systems generate words within a clump according to bigram models. Other statistical approachs to NLU include decision trees (Kuhn and Mori, 1995) and neural nets (Gorin et al., 1991). In earlier IBM translation systems (Brown et al., 1993) each English word would be generated by, or &amp;quot;aligned to&amp;quot;, exactly one formal language word. This mapping between the English and formal language expressions is called the &amp;quot;alignment&amp;quot;. In the simplest case, the translation model is simply proportional to the product of word-pair translation probabilities, one per element in the alignment. In these models, the alignment provides all of the structure in the translation model. The alignment is a &amp;quot;hidden&amp;quot; quantity which is not annotated in the training data and must be inferred indirectly. T</context>
</contexts>
<marker>Gorin, Levinson, Gertner, Goldman, 1991</marker>
<rawString>Gorin, A., S. Levinson, A. Gertner, and E. Goldman. 1991. Adaptive acquisition of language. Computer Speech and Language, 5:101-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hemphill</author>
<author>J Godfrey</author>
<author>G Doddington</author>
</authors>
<title>The ATIS spoken language systems pilot corpus.</title>
<date>1990</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<pages>96--101</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>Hidden Valley, PA,</location>
<contexts>
<context position="2603" citStr="Hemphill, Godfrey, and Doddington, 1990" startWordPosition="409" endWordPosition="413">robability .fr= arg max p(FIE)--= arg max p(F, E). over all F over all F We have previously built a fully automatic statistical NLU system (Epstein et al., 1996) based on the source-channel factorization of the joint distribution p(F, p(F, E) = p(F)p(EIF). This factorization, which has proven effective in speech recognition (Bahl, Jelinek, and Mercer, 1983), partitions the joint probability into an a priori intention model p(F), and a translation model p(EIF) which models how a user might phrase a request F in English. For the ATIS task, our formal language is a minor variant of the NL-Parse (Hemphill, Godfrey, and Doddington, 1990) used by ARPA to annotate the ATIS corpus. An example of a formal and natural language pair is: • F: List flights from New Orleans to Memphis flying on Monday departing early_morning • E: do you have any flights going to Memphis leaving New Orleans early Monday morning Here, the evidence for the formal language concept `early_rnorning&apos; resides in the two disjoint clumps of English &apos;early&apos; and &apos;morning&apos;. In this paper, we introduce the notion of concept fertility into our translation models p(EIF) to capture this effect and the more general linguistic phenomenon of embedded clauses. Basically,</context>
</contexts>
<marker>Hemphill, Godfrey, Doddington, 1990</marker>
<rawString>Hemphill, C., J. Godfrey, and G. Doddington. 1990. The ATIS spoken language systems pilot corpus. In Proceedings of the DARPA Speech and Natural Language Workshop, pages 96-101, Hidden Valley, PA, June. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Koppelman</author>
<author>S Della Pietra</author>
<author>M Epstein</author>
<author>S Roukos</author>
</authors>
<title>A statistical approach to language modeling for the ATIS task.</title>
<date>1995</date>
<booktitle>In Proceedings of the Spoken Language Systems Workshop,</booktitle>
<pages>1785--1788</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="16515" citStr="Koppelman et al., 1995" startWordPosition="2783" endWordPosition="2786">corpus and a partially annotated one. We employ a trivial decoder and language model since our emphasis is on evaluating the performance of different translation models. Our decoder is a simple pattern matcher. That is, we accumulate the different formal language patterns seen in the training set, and score each of them on the test set. The language model is just the unsmoothed unigram probability distribution of the patterns. This LM has a 10% chance of not including a test pattern and its use leads to pessimistic performance estimates. A more general language model for ATIS is presented in (Koppelman et al., 1995). Answers are generated by an SQL program which is a deterministically constructed from the formal language of our system. The accuracy of these database answers is measured using ARPA&apos;s Common Answer Specification (CAS) metric. The results are presented in table 3 for ARPA&apos;s December 1993 blind test set. The column headed DEC93 reports results on unsupervised training data, while the column entitled DEC93a contains the results from using models trained on the partially annotated corpus. The rows correspond to various translation models. Model 1 is the word-pair translation model used in simpl</context>
</contexts>
<marker>Koppelman, Pietra, Epstein, Roukos, 1995</marker>
<rawString>Koppelman, J., S. Della Pietra, M. Epstein, and S. Roukos. 1995. A statistical approach to language modeling for the ATIS task. In Proceedings of the Spoken Language Systems Workshop, pages 1785-1788, Madrid, Spain, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>R De Mori</author>
</authors>
<title>The application of semantic classification trees to natural language understanding.</title>
<date>1995</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>17--5</pages>
<marker>Kuhn, De Mori, 1995</marker>
<rawString>Kuhn, R. and R. De Mori. 1995. The application of semantic classification trees to natural language understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 17(5):449-460, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
</authors>
<title>Chronus, the next generation.</title>
<date>1995</date>
<booktitle>In Proceedings of the Spoken Language Systems Workshop,</booktitle>
<pages>269--271</pages>
<location>Austin, Texas,</location>
<contexts>
<context position="3624" citStr="Levin and Pieraccini, 1995" startWordPosition="580" endWordPosition="583">nd &apos;morning&apos;. In this paper, we introduce the notion of concept fertility into our translation models p(EIF) to capture this effect and the more general linguistic phenomenon of embedded clauses. Basically, this entails augmenting the translation model with terms of the form p(n I f), where n is the number of clumps generated by the formal language word f. The resulting model can be trained automatically from a bilingual corpus of English and formal language sentence pairs. Other attempts at statistical NLU systems have used various meaning representations such as concepts in the AT&amp;T system (Levin and Pieraccini, 1995) or initial semantic structure in the BBN system (Miller et al., 1995). Both of these systems require significant rule-based transformations to produce disambiguated interpretations which are then 168 used to generate the SQL query for ATIS. More recently, BBN has replaced handwritten rules with decision trees (Miller et al., 1996). Moreover, both systems were trained using English annotated by hand with segmentation and labeling, and both systems produce a semantic representation which is forced to preserve the time order expressed in the English. Interestingly, both the AT&amp;T and BBN systems </context>
<context position="12257" citStr="Levin and Pieraccini, 1995" startWordPosition="2054" endWordPosition="2058">ity model gives the most likely 1000 clumpings and alignments, which are then rescored according to the current general fertility model parameters. This gives fractional counts for each of the 1000 alignments, which are then used to update the the general fertility model parameters. 4 Improved Clump Modeling In both the Poisson and general fertility models, the computation of p(cif) in equation 2 uses a unigram model. Each English word ei is generated with probability p(ei IL). Two more powerful modeling techniques for modeling clump generation are n-gram language models (Miller et al., 1995; Levin and Pieraccini, 1995; Epstein, 1996), and headword language models (Epstein, 1996). A bigram language model uses: P( f) = p(4c) I f)P(ei I bdY, ic)12(bdY I et(c), fc) X 1(c) HkeiI ei—i, fc) i.2 where bdy is a special marker to delimit the beginning and end of the clump. A headword language model uses two unigram models, a headword model and a non-headword model. Each clump is required to have a headword. All other words are non-headwords. The identity of a clump&apos;s headword is hidden, hence it is necessary n! then a polynomial time training algorithm exists. The simplicity arises from the fortuitous cancellation o</context>
</contexts>
<marker>Levin, Pieraccini, 1995</marker>
<rawString>Levin, E. and R. Pieraccini. 1995. Chronus, the next generation. In Proceedings of the Spoken Language Systems Workshop, pages 269-271, Austin, Texas, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>M Bates</author>
<author>R Bobrow</author>
<author>R Ingria</author>
<author>J Makhoul</author>
<author>R Schwartz</author>
</authors>
<title>Recent progress in hidden understanding models.</title>
<date>1995</date>
<booktitle>In Proceedings of the Spoken Language Systems Workshop,</booktitle>
<pages>276--279</pages>
<location>Austin, Texas,</location>
<contexts>
<context position="3694" citStr="Miller et al., 1995" startWordPosition="593" endWordPosition="596">o our translation models p(EIF) to capture this effect and the more general linguistic phenomenon of embedded clauses. Basically, this entails augmenting the translation model with terms of the form p(n I f), where n is the number of clumps generated by the formal language word f. The resulting model can be trained automatically from a bilingual corpus of English and formal language sentence pairs. Other attempts at statistical NLU systems have used various meaning representations such as concepts in the AT&amp;T system (Levin and Pieraccini, 1995) or initial semantic structure in the BBN system (Miller et al., 1995). Both of these systems require significant rule-based transformations to produce disambiguated interpretations which are then 168 used to generate the SQL query for ATIS. More recently, BBN has replaced handwritten rules with decision trees (Miller et al., 1996). Moreover, both systems were trained using English annotated by hand with segmentation and labeling, and both systems produce a semantic representation which is forced to preserve the time order expressed in the English. Interestingly, both the AT&amp;T and BBN systems generate words within a clump according to bigram models. Other statis</context>
<context position="12229" citStr="Miller et al., 1995" startWordPosition="2050" endWordPosition="2053">s. The Poisson fertility model gives the most likely 1000 clumpings and alignments, which are then rescored according to the current general fertility model parameters. This gives fractional counts for each of the 1000 alignments, which are then used to update the the general fertility model parameters. 4 Improved Clump Modeling In both the Poisson and general fertility models, the computation of p(cif) in equation 2 uses a unigram model. Each English word ei is generated with probability p(ei IL). Two more powerful modeling techniques for modeling clump generation are n-gram language models (Miller et al., 1995; Levin and Pieraccini, 1995; Epstein, 1996), and headword language models (Epstein, 1996). A bigram language model uses: P( f) = p(4c) I f)P(ei I bdY, ic)12(bdY I et(c), fc) X 1(c) HkeiI ei—i, fc) i.2 where bdy is a special marker to delimit the beginning and end of the clump. A headword language model uses two unigram models, a headword model and a non-headword model. Each clump is required to have a headword. All other words are non-headwords. The identity of a clump&apos;s headword is hidden, hence it is necessary n! then a polynomial time training algorithm exists. The simplicity arises from t</context>
</contexts>
<marker>Miller, Bates, Bobrow, Ingria, Makhoul, Schwartz, 1995</marker>
<rawString>Miller, S., M. Bates, R. Bobrow, R. Ingria, J. Makhoul, and R. Schwartz. 1995. Recent progress in hidden understanding models. In Proceedings of the Spoken Language Systems Workshop, pages 276-279, Austin, Texas, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>D Stallard</author>
<author>R Bobrow</author>
<author>R Schwartz</author>
</authors>
<title>A fully statistical approach to natural language interfaces.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>55--61</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>Santa Cruz, CA,</location>
<contexts>
<context position="3957" citStr="Miller et al., 1996" startWordPosition="635" endWordPosition="638">l language word f. The resulting model can be trained automatically from a bilingual corpus of English and formal language sentence pairs. Other attempts at statistical NLU systems have used various meaning representations such as concepts in the AT&amp;T system (Levin and Pieraccini, 1995) or initial semantic structure in the BBN system (Miller et al., 1995). Both of these systems require significant rule-based transformations to produce disambiguated interpretations which are then 168 used to generate the SQL query for ATIS. More recently, BBN has replaced handwritten rules with decision trees (Miller et al., 1996). Moreover, both systems were trained using English annotated by hand with segmentation and labeling, and both systems produce a semantic representation which is forced to preserve the time order expressed in the English. Interestingly, both the AT&amp;T and BBN systems generate words within a clump according to bigram models. Other statistical approachs to NLU include decision trees (Kuhn and Mori, 1995) and neural nets (Gorin et al., 1991). In earlier IBM translation systems (Brown et al., 1993) each English word would be generated by, or &amp;quot;aligned to&amp;quot;, exactly one formal language word. This mapp</context>
</contexts>
<marker>Miller, Stallard, Bobrow, Schwartz, 1996</marker>
<rawString>Miller, S., D. Stallard, R. Bobrow, and R. Schwartz. 1996. A fully statistical approach to natural language interfaces. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 55-61, Santa Cruz, CA, June. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>