<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012485">
<title confidence="0.9950345">
A flexible distributed architecture for NLP system
development and use
</title>
<author confidence="0.99479">
Freddy Y. Y. Choi
</author>
<affiliation confidence="0.9932175">
Artificial Intelligence Group
University of Manchester
</affiliation>
<address confidence="0.788552">
Manchester, U.K.
</address>
<email confidence="0.997046">
choif@cs.man.ac.uk
</email>
<sectionHeader confidence="0.997365" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99956775">
We describe a distributed, modular architecture
for platform independent natural language sys-
tems. It features automatic interface genera-
tion and self-organization. Adaptive (and non-
adaptive) voting mechanisms are used for inte-
grating discrete modules. The architecture is
suitable for rapid prototyping and product de-
livery.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991887322580645">
This article describes TEA&apos;, a flexible architec-
ture for developing and delivering platform in-
dependent text engineering (TE) systems. TEA
provides a generalized framework for organizing
and applying reusable TE components (e.g. to-
kenizer, stemmer). Thus, developers are able
to focus on problem solving rather than imple-
mentation. For product delivery, the end user
receives an exact copy of the developer&apos;s edition.
The visibility of configurable options (different
levels of detail) is adjustable along a simple gra-
dient via the automatically generated user inter-
face (Edwards, Forthcoming).
Our target application is telegraphic text
compression (Choi (1999b); of Roelofs (Forth-
coming); Grefenstette (1998)). We aim to im-
prove the efficiency of screen readers for the
visually disabled by removing uninformative
words (e.g. determiners) in text documents.
This produces a stream of topic cues for rapid
skimming. The information value of each word
is to be estimated based on an unusually wide
range of linguistic information.
TEA was designed to be a development en-
vironment for this work. However, the target
application has led us to produce an interesting
&apos;TEA is an acronym for Text Engineering Architec-
ture.
architecture and techniques that are more gen-
erally applicable, and it is these which we will
focus on in this paper.
</bodyText>
<sectionHeader confidence="0.988546" genericHeader="introduction">
2 Architecture
</sectionHeader>
<figureCaption confidence="0.9867525">
Figure 1: An overview of the TEA system
framework.
</figureCaption>
<bodyText confidence="0.999512538461539">
The central component of TEA is a frame-
based data model (F) (see Fig.2). In this model,
a document is a list of frames (Rich and Knight,
1991) for recording the properties about each
token in the text (example in Fig.2). A typical
TE system converts a document into F with an
input plug-in. The information required at the
output determines the set of process plug-ins to
activate. These use the information in F to add
annotations to F. Their dependencies are auto-
matically resolved by TEA. System behavior is
controlled by adjusting the configurable param-
eters.
</bodyText>
<figureCaption confidence="0.999191">
Frame 1: (:token An :pos art :begin_s 1)
Frame 2: (:token example :pos n)
Frame 3: (:token sentence :pos n)
Frame 4: (:token. :pos punc :end_s 1)
Figure 2: &amp;quot;An example sentence.&amp;quot; in a frame-
based data model
</figureCaption>
<figure confidence="0.997964833333333">
Plug-ins
I I
Shared knowledge System control
structure
System input
and output
</figure>
<page confidence="0.997135">
615
</page>
<bodyText confidence="0.9999565">
This type of architecture has been imple-
mented, classically, as a &apos;blackboard&apos; system
such as Hearsay-II (Erman, 1980), where inter-
module communication takes place through a
shared knowledge structure; or as a &apos;message-
passing&apos; system where the modules communi-
cate directly. Our architecture is similar to
blackboard systems. However, the purpose of
F (the shared knowledge structure in TEA) is
to provide a single extendable data structure for
annotating text. It also defines a standard in-
terface for inter-module communication, thus,
improves system integration and ease of soft-
ware reuse.
</bodyText>
<subsectionHeader confidence="0.997322">
2.1 Voting mechanism
</subsectionHeader>
<bodyText confidence="0.999900631578947">
A feature that distinguishes TEA from similar
systems is its use of voting mechanisms for sys-
tem integration. Our approach has two distinct
but uniformly treated applications. First, for
any type of language analysis, different tech-
niques ti will return successful results P(r) on
different subsets of the problem space. Thus
combining the outputs P(riti) from several ti
should give a result more accurate than any one
in isolation. This has been demonstrated in sev-
eral systems (e.g. Choi (1999a); van Halteren
et al. (1998); Brill and Wu (1998); Veronis and
Ide (1991)). Our architecture currently offers
two types of voting mechanisms: weighted av-
erage (Eq.1) and weighted maximum (Eq.2). A
Bayesian classifier (Weiss and Kulikowski, 1991)
based weight estimation algorithm (Eq.3) is in-
cluded for constructing adaptive voting mecha-
nisms.
</bodyText>
<equation confidence="0.999264333333333">
P(r) = &gt;wiP(rltj) (1)
= max{wiP(riti), - • • ,wnP(ritn)} (2)
= P(tilP(riti), • - • ,P(ritn)) (3)
</equation>
<bodyText confidence="0.999168777777778">
Second, different types of analysis ai will pro-
vide different information about a problem,
hence, a solution is improved by combining sev-
eral ai. For telegraphic text compression, we es-
timate E(w), the information value of a word,
based on a wide range of different information
sources (Fig.2.1 shows a subset of our working
system). The output of each ai are combined by
a voting mechanism to form a single measure.
</bodyText>
<figure confidence="0.8127445">
Technique Analysis
combination combination
</figure>
<figureCaption confidence="0.9601645">
Figure 3: An example configuration of TEA for
telegraphic text compression.
</figureCaption>
<bodyText confidence="0.99994775">
Thus, for example, if our system encoun-
ters the phrase &apos;President Clinton&apos;, both lexical
lookup and automatic tagging will agree that
&apos;President&apos; is a noun. Nouns are generally infor-
mative, so should be retained in the compressed
output text. However, grammar-based syntac-
tic analysis gives a lower weighting to the first
noun of a noun-noun construction, and bigram
analysis tells us that &apos;President Clinton&apos; is a
common word pair. These two modules overrule
the simple POS value, and &apos;President Clinton&apos;
is reduced to &apos;Clinton&apos;.
</bodyText>
<sectionHeader confidence="0.999981" genericHeader="method">
3 Related work
</sectionHeader>
<bodyText confidence="0.999372933333333">
Current trends in the development of reusable
TE tools are best represented by the Edinburgh
tools (LTGT)2 (LTG, 1999) and GATE3 (Cun-
ningham et al., 1995). Like TEA, both LTGT
and GATE are frameworks for TE.
LTGT adopts the pipeline architecture for
module integration. For processing, a text doc-
ument is converted into SGML format. Pro-
cessing modules are then applied to the SGML
file sequentially. Annotations are accumulated
as mark-up tags in the text. The architecture is
simple to understand, robust and future proof.
The SGML/XML standard is well developed
and supported by the community. This im-
proves the reusability of the tools. However,
</bodyText>
<footnote confidence="0.99813825">
2LTGT is an acronym for the Edinburgh Language
lbchnology Group Tools
3GATE is an acronym for General Architecture for
Text Engineering.
</footnote>
<page confidence="0.995922">
616
</page>
<bodyText confidence="0.997345425">
the architecture encourages tool development
rather than reuse of existing TE components.
GATE is based on an object-oriented data
model (similar to the TIPSTER architecture
(Grishman, 1997)). Modules communicate by
reading and writing information to and from a
central database. Unlike LTGT, both GATE
and TEA are designed to encourage software
reuse. Existing TE tools are easily incorporated
with Tcl wrapper scripts and Java interfaces, re-
spectively.
Features that distinguish LTGT, GATE and
TEA are the configuration methods, portabil-
ity and motivation. Users of LTGT write shell
scripts to define a system (as a chain of LTGT
components). With GATE, a system is con-
structed manually by wiring TE components to-
gether using the graphical interface. TEA as-
sumes the user knows nothing but the available
input and required output. The appropriate set
of plug-ins are automatically activated. Module
selection can be manually configured by adjust-
ing the parameters of the voting mechanisms.
This ensures a TE system is accessible to com-
plete novices lid yet has sufficient control for
developers.
LTGT and GATE are both open-source C ap-
plications. They can be recompiled for many
platforms. TEA is a Java application. It can
run directly (without compilation) on any Java
supported systems. However, applications con-
structed with the current release of GATE and
TEA are less portable than those produced with
LTGT. GATE and TEA encourage reuse of ex-
isting components, not all of which are platform
independent4. We believe this is a worth while
trade off since it allows developers to construct
prototypes with components that are only avail-
able as separate applications. Native tools can
be developed incrementally.
</bodyText>
<sectionHeader confidence="0.920005" genericHeader="method">
4 An example
</sectionHeader>
<bodyText confidence="0.999948285714286">
Our application is telegraphic text compression.
The examples were generated with a subset of
our working system using a section of the book
HAL&apos;s legacy (Stork, 1997) as test data. First,
we use different compression techniques to gen-
erate the examples in Fig.4. This was done by
simply adjusting a parameter of an output plug-
</bodyText>
<footnote confidence="0.708935">
4This is not a problem for LTGT since the architec-
ture does not encourage component reuse.
</footnote>
<bodyText confidence="0.8825576">
in. It is clear that the output is inadequate for
rapid text skimming. To improve the system,
the three measures were combine with an un-
weighted voting mechanism. Fig.4 presents two
levels of compression using the new measure.
</bodyText>
<listItem confidence="0.966065">
1. With science fiction films the more science
you understand the less you admire the film or
respect its makers
2. fiction films understand less admire respect
makers
3. fiction understand less admire respect makers
4. science fiction films science film makers
Figure 4: Three measures of information value:
(1) Original sentence, (2) Token frequency, (3)
Stem frequency and (4) POS.
1. science fiction films understand less admire
film respect makers
2. fiction makers
</listItem>
<figureCaption confidence="0.9960345">
Figure 5: Improving telegraphic text compres-
sion by analysis combination.
</figureCaption>
<sectionHeader confidence="0.816143" genericHeader="conclusions">
5 Conclusions and future directions
</sectionHeader>
<bodyText confidence="0.999931625">
We have described an interesting architecture
(TEA) for developing platform independent
text engineering applications. Product delivery,
configuration and development are made sim-
ple by the self-organizing architecture and vari-
able interface. The use of voting mechanisms
for integrating discrete modules is original. Its
motivation is well supported.
The current implementation of TEA is geared
towards token analysis. We plan to extend
the data model to cater for structural annota-
tions. The tool set for TEA is constantly be-
ing extended, recent additions include a proto-
type symbolic classifier, shallow parser (Choi,
Forthcoming), sentence segmentation algorithm
(Reynar and Ratnaparkhi, 1997) and a POS
tagger (Ratnaparkhi, 1996). Other adaptive
voting mechanisms are to be investigated. Fu-
ture release of TEA will support concurrent ex-
ecution (distributed processing) over a network.
Finally, we plan to investigate means of im-
proving system integration and module orga-
nization, e.g. annotation, module and tag set
compatibility.
</bodyText>
<page confidence="0.996293">
617
</page>
<sectionHeader confidence="0.9774" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.717676935897436">
E. Brill and J. Wu. 1998. Classifier combina-
tion for improved lexical disambiguation. In
Proceedings of COLING-ACL&apos;98, pages 191-
195, Montreal, Canada, August.
F. Choi. 1999a. An adaptive voting mechanism
for improving the reliability of natural lan-
guage processing systems. Paper submitted
to EACL&apos;99, January.
F. Choi. 1999b. Speed reading for the
visually disabled. Paper submitted to
SIGART/AAAI&apos;99 Doctoral Consortium,
February.
F. Choi. Forthcoming. A probabilistic ap-
proach to learning shallow linguistic patterns.
In Proeeedings of ECAI&apos;99 (Student Session),
Greece.
H. Cunningham, R.G. Gaizauskas, and
Y. Wilks. 1995. A general architecture for
text engineering (gate) - a new approach
to language engineering research and de-
velopment. Technical Report CD-95-21,
Department of Computer Science, University
of Sheffield. http://xxx.lanl.gov/ps/cmp-
1g/9601009.
M. Edwards. Forthcoming. An approach to
automatic interface generation. Final year
project report, Department of Computer Sci-
ence, University of Manchester, Manchester,
England.
L. Erman. 1980. The hearsay-ii speech under-
standing system: Integrating knowledge to
resolve uncertainty. In ACM Computer Sur-
veys, volume 12.
G. Grefenstette. 1998. Producing intelligent
telegraphic text reduction to provide an audio
scanning service for the blind. In AAAI&apos;98
Workshop on Intelligent Text Summariza-
tion, San Francisco, March.
R. Grishman. 1997. Tipster architecture de-
sign document version 2.3. Technical report,
DARPA. http://www.tipster.org.
LTG. 1999. Edinburgh univer-
sity, hcrc, ltg software. WWW.
http: //www ac. uk/software/index.ht ml.
H. Rollfs of Roelofs. Forthcoming. Telegraph-
ese: Converting text into telegram style.
Master&apos;s thesis, Department of Computer Sci-
ence, University of Manchester, Manchester,
England.
G. M. P. O&apos;Hare and N. R. Jennings, edi-
tors. 1996. Foundations of Distributed Ar-
tificial Intelligence. Sixth generation com-
puter series. Wiley Interscience Publishers,
New York. ISBN 0-471-00675.
A . Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceed-
ings of the empirical methods in NLP confer-
ence, University of Pennsylvania.
J. Reynar and A. Ratnaparkhi. 1997. A max-
imum entropy approach to identifying sen-
tence boundaries. In Proceedings of the fifth
conference on Applied NLP, Washington D.C.
E. Rich and K. Knight. 1991. Artificial Intel-
ligence. McGraw-Hill, Inc., second edition.
ISBN 0-07-100894-2.
. Stork, editor. 1997. Hal&apos;s Legacy: 2001&apos;s
Computer in Dream and Reality. MIT Press.
http://mitpress.mit.edu/e-books/Hal/.
. van Halteren, J. Zavrel, and W. Daelemans.
1998. Improving data driven wordclass tag-
ging by system combination. In Proceedings
of COLING-ACL&apos;98, volume 1.
J. Veronis and N. Ide. 1991. An accessment of
semantic information automatically extracted
from machine readable dictionaries. In Pro-
ceedings of EACL&apos;91, pages 227-232, Berlin.
S. Weiss and C. Kulikowski. 1991. Computer
Systems That Learn. Morgan Kaufmann.
</reference>
<page confidence="0.994067">
618
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.669757">
<title confidence="0.992601">A flexible distributed architecture for NLP system development and use</title>
<author confidence="0.999995">Freddy Y Y Choi</author>
<affiliation confidence="0.999992">Artificial Intelligence Group University of Manchester</affiliation>
<address confidence="0.956099">Manchester, U.K.</address>
<email confidence="0.998556">choif@cs.man.ac.uk</email>
<abstract confidence="0.967784">We describe a distributed, modular architecture for platform independent natural language systems. It features automatic interface generation and self-organization. Adaptive (and nonadaptive) voting mechanisms are used for integrating discrete modules. The architecture is suitable for rapid prototyping and product delivery.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Wu</author>
</authors>
<title>Classifier combination for improved lexical disambiguation.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL&apos;98,</booktitle>
<pages>191--195</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="3945" citStr="Brill and Wu (1998)" startWordPosition="615" endWordPosition="618">ves system integration and ease of software reuse. 2.1 Voting mechanism A feature that distinguishes TEA from similar systems is its use of voting mechanisms for system integration. Our approach has two distinct but uniformly treated applications. First, for any type of language analysis, different techniques ti will return successful results P(r) on different subsets of the problem space. Thus combining the outputs P(riti) from several ti should give a result more accurate than any one in isolation. This has been demonstrated in several systems (e.g. Choi (1999a); van Halteren et al. (1998); Brill and Wu (1998); Veronis and Ide (1991)). Our architecture currently offers two types of voting mechanisms: weighted average (Eq.1) and weighted maximum (Eq.2). A Bayesian classifier (Weiss and Kulikowski, 1991) based weight estimation algorithm (Eq.3) is included for constructing adaptive voting mechanisms. P(r) = &gt;wiP(rltj) (1) = max{wiP(riti), - • • ,wnP(ritn)} (2) = P(tilP(riti), • - • ,P(ritn)) (3) Second, different types of analysis ai will provide different information about a problem, hence, a solution is improved by combining several ai. For telegraphic text compression, we estimate E(w), the inform</context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>E. Brill and J. Wu. 1998. Classifier combination for improved lexical disambiguation. In Proceedings of COLING-ACL&apos;98, pages 191-195, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Choi</author>
</authors>
<title>An adaptive voting mechanism for improving the reliability of natural language processing systems.</title>
<date>1999</date>
<note>Paper submitted to EACL&apos;99,</note>
<contexts>
<context position="1191" citStr="Choi (1999" startWordPosition="165" endWordPosition="166">e for developing and delivering platform independent text engineering (TE) systems. TEA provides a generalized framework for organizing and applying reusable TE components (e.g. tokenizer, stemmer). Thus, developers are able to focus on problem solving rather than implementation. For product delivery, the end user receives an exact copy of the developer&apos;s edition. The visibility of configurable options (different levels of detail) is adjustable along a simple gradient via the automatically generated user interface (Edwards, Forthcoming). Our target application is telegraphic text compression (Choi (1999b); of Roelofs (Forthcoming); Grefenstette (1998)). We aim to improve the efficiency of screen readers for the visually disabled by removing uninformative words (e.g. determiners) in text documents. This produces a stream of topic cues for rapid skimming. The information value of each word is to be estimated based on an unusually wide range of linguistic information. TEA was designed to be a development environment for this work. However, the target application has led us to produce an interesting &apos;TEA is an acronym for Text Engineering Architecture. architecture and techniques that are more g</context>
<context position="3894" citStr="Choi (1999" startWordPosition="608" endWordPosition="609">or inter-module communication, thus, improves system integration and ease of software reuse. 2.1 Voting mechanism A feature that distinguishes TEA from similar systems is its use of voting mechanisms for system integration. Our approach has two distinct but uniformly treated applications. First, for any type of language analysis, different techniques ti will return successful results P(r) on different subsets of the problem space. Thus combining the outputs P(riti) from several ti should give a result more accurate than any one in isolation. This has been demonstrated in several systems (e.g. Choi (1999a); van Halteren et al. (1998); Brill and Wu (1998); Veronis and Ide (1991)). Our architecture currently offers two types of voting mechanisms: weighted average (Eq.1) and weighted maximum (Eq.2). A Bayesian classifier (Weiss and Kulikowski, 1991) based weight estimation algorithm (Eq.3) is included for constructing adaptive voting mechanisms. P(r) = &gt;wiP(rltj) (1) = max{wiP(riti), - • • ,wnP(ritn)} (2) = P(tilP(riti), • - • ,P(ritn)) (3) Second, different types of analysis ai will provide different information about a problem, hence, a solution is improved by combining several ai. For telegra</context>
</contexts>
<marker>Choi, 1999</marker>
<rawString>F. Choi. 1999a. An adaptive voting mechanism for improving the reliability of natural language processing systems. Paper submitted to EACL&apos;99, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Choi</author>
</authors>
<title>Speed reading for the visually disabled.</title>
<date>1999</date>
<note>Paper submitted to SIGART/AAAI&apos;99 Doctoral Consortium,</note>
<contexts>
<context position="1191" citStr="Choi (1999" startWordPosition="165" endWordPosition="166">e for developing and delivering platform independent text engineering (TE) systems. TEA provides a generalized framework for organizing and applying reusable TE components (e.g. tokenizer, stemmer). Thus, developers are able to focus on problem solving rather than implementation. For product delivery, the end user receives an exact copy of the developer&apos;s edition. The visibility of configurable options (different levels of detail) is adjustable along a simple gradient via the automatically generated user interface (Edwards, Forthcoming). Our target application is telegraphic text compression (Choi (1999b); of Roelofs (Forthcoming); Grefenstette (1998)). We aim to improve the efficiency of screen readers for the visually disabled by removing uninformative words (e.g. determiners) in text documents. This produces a stream of topic cues for rapid skimming. The information value of each word is to be estimated based on an unusually wide range of linguistic information. TEA was designed to be a development environment for this work. However, the target application has led us to produce an interesting &apos;TEA is an acronym for Text Engineering Architecture. architecture and techniques that are more g</context>
<context position="3894" citStr="Choi (1999" startWordPosition="608" endWordPosition="609">or inter-module communication, thus, improves system integration and ease of software reuse. 2.1 Voting mechanism A feature that distinguishes TEA from similar systems is its use of voting mechanisms for system integration. Our approach has two distinct but uniformly treated applications. First, for any type of language analysis, different techniques ti will return successful results P(r) on different subsets of the problem space. Thus combining the outputs P(riti) from several ti should give a result more accurate than any one in isolation. This has been demonstrated in several systems (e.g. Choi (1999a); van Halteren et al. (1998); Brill and Wu (1998); Veronis and Ide (1991)). Our architecture currently offers two types of voting mechanisms: weighted average (Eq.1) and weighted maximum (Eq.2). A Bayesian classifier (Weiss and Kulikowski, 1991) based weight estimation algorithm (Eq.3) is included for constructing adaptive voting mechanisms. P(r) = &gt;wiP(rltj) (1) = max{wiP(riti), - • • ,wnP(ritn)} (2) = P(tilP(riti), • - • ,P(ritn)) (3) Second, different types of analysis ai will provide different information about a problem, hence, a solution is improved by combining several ai. For telegra</context>
</contexts>
<marker>Choi, 1999</marker>
<rawString>F. Choi. 1999b. Speed reading for the visually disabled. Paper submitted to SIGART/AAAI&apos;99 Doctoral Consortium, February.</rawString>
</citation>
<citation valid="false">
<authors>
<author>F Choi</author>
</authors>
<title>Forthcoming. A probabilistic approach to learning shallow linguistic patterns.</title>
<booktitle>In Proeeedings of ECAI&apos;99</booktitle>
<location>(Student Session), Greece.</location>
<marker>Choi, </marker>
<rawString>F. Choi. Forthcoming. A probabilistic approach to learning shallow linguistic patterns. In Proeeedings of ECAI&apos;99 (Student Session), Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>R G Gaizauskas</author>
<author>Y Wilks</author>
</authors>
<title>A general architecture for text engineering (gate) - a new approach to language engineering research and development.</title>
<date>1995</date>
<tech>Technical Report CD-95-21,</tech>
<institution>Department of Computer Science, University of Sheffield.</institution>
<note>http://xxx.lanl.gov/ps/cmp1g/9601009.</note>
<contexts>
<context position="5573" citStr="Cunningham et al., 1995" startWordPosition="873" endWordPosition="877">both lexical lookup and automatic tagging will agree that &apos;President&apos; is a noun. Nouns are generally informative, so should be retained in the compressed output text. However, grammar-based syntactic analysis gives a lower weighting to the first noun of a noun-noun construction, and bigram analysis tells us that &apos;President Clinton&apos; is a common word pair. These two modules overrule the simple POS value, and &apos;President Clinton&apos; is reduced to &apos;Clinton&apos;. 3 Related work Current trends in the development of reusable TE tools are best represented by the Edinburgh tools (LTGT)2 (LTG, 1999) and GATE3 (Cunningham et al., 1995). Like TEA, both LTGT and GATE are frameworks for TE. LTGT adopts the pipeline architecture for module integration. For processing, a text document is converted into SGML format. Processing modules are then applied to the SGML file sequentially. Annotations are accumulated as mark-up tags in the text. The architecture is simple to understand, robust and future proof. The SGML/XML standard is well developed and supported by the community. This improves the reusability of the tools. However, 2LTGT is an acronym for the Edinburgh Language lbchnology Group Tools 3GATE is an acronym for General Arc</context>
</contexts>
<marker>Cunningham, Gaizauskas, Wilks, 1995</marker>
<rawString>H. Cunningham, R.G. Gaizauskas, and Y. Wilks. 1995. A general architecture for text engineering (gate) - a new approach to language engineering research and development. Technical Report CD-95-21, Department of Computer Science, University of Sheffield. http://xxx.lanl.gov/ps/cmp1g/9601009.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Edwards</author>
</authors>
<title>Forthcoming. An approach to automatic interface generation. Final year project report,</title>
<institution>Department of Computer Science, University of Manchester,</institution>
<location>Manchester, England.</location>
<marker>Edwards, </marker>
<rawString>M. Edwards. Forthcoming. An approach to automatic interface generation. Final year project report, Department of Computer Science, University of Manchester, Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Erman</author>
</authors>
<title>The hearsay-ii speech understanding system: Integrating knowledge to resolve uncertainty.</title>
<date>1980</date>
<journal>In ACM Computer Surveys,</journal>
<volume>12</volume>
<contexts>
<context position="2903" citStr="Erman, 1980" startWordPosition="453" endWordPosition="454"> plug-ins to activate. These use the information in F to add annotations to F. Their dependencies are automatically resolved by TEA. System behavior is controlled by adjusting the configurable parameters. Frame 1: (:token An :pos art :begin_s 1) Frame 2: (:token example :pos n) Frame 3: (:token sentence :pos n) Frame 4: (:token. :pos punc :end_s 1) Figure 2: &amp;quot;An example sentence.&amp;quot; in a framebased data model Plug-ins I I Shared knowledge System control structure System input and output 615 This type of architecture has been implemented, classically, as a &apos;blackboard&apos; system such as Hearsay-II (Erman, 1980), where intermodule communication takes place through a shared knowledge structure; or as a &apos;messagepassing&apos; system where the modules communicate directly. Our architecture is similar to blackboard systems. However, the purpose of F (the shared knowledge structure in TEA) is to provide a single extendable data structure for annotating text. It also defines a standard interface for inter-module communication, thus, improves system integration and ease of software reuse. 2.1 Voting mechanism A feature that distinguishes TEA from similar systems is its use of voting mechanisms for system integrat</context>
</contexts>
<marker>Erman, 1980</marker>
<rawString>L. Erman. 1980. The hearsay-ii speech understanding system: Integrating knowledge to resolve uncertainty. In ACM Computer Surveys, volume 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Producing intelligent telegraphic text reduction to provide an audio scanning service for the blind.</title>
<date>1998</date>
<booktitle>In AAAI&apos;98 Workshop on Intelligent Text Summarization,</booktitle>
<location>San Francisco,</location>
<contexts>
<context position="1240" citStr="Grefenstette (1998)" startWordPosition="171" endWordPosition="172"> independent text engineering (TE) systems. TEA provides a generalized framework for organizing and applying reusable TE components (e.g. tokenizer, stemmer). Thus, developers are able to focus on problem solving rather than implementation. For product delivery, the end user receives an exact copy of the developer&apos;s edition. The visibility of configurable options (different levels of detail) is adjustable along a simple gradient via the automatically generated user interface (Edwards, Forthcoming). Our target application is telegraphic text compression (Choi (1999b); of Roelofs (Forthcoming); Grefenstette (1998)). We aim to improve the efficiency of screen readers for the visually disabled by removing uninformative words (e.g. determiners) in text documents. This produces a stream of topic cues for rapid skimming. The information value of each word is to be estimated based on an unusually wide range of linguistic information. TEA was designed to be a development environment for this work. However, the target application has led us to produce an interesting &apos;TEA is an acronym for Text Engineering Architecture. architecture and techniques that are more generally applicable, and it is these which we wil</context>
</contexts>
<marker>Grefenstette, 1998</marker>
<rawString>G. Grefenstette. 1998. Producing intelligent telegraphic text reduction to provide an audio scanning service for the blind. In AAAI&apos;98 Workshop on Intelligent Text Summarization, San Francisco, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
</authors>
<title>Tipster architecture design document version 2.3.</title>
<date>1997</date>
<tech>Technical report, DARPA. http://www.tipster.org.</tech>
<contexts>
<context position="6399" citStr="Grishman, 1997" startWordPosition="1004" endWordPosition="1005">ied to the SGML file sequentially. Annotations are accumulated as mark-up tags in the text. The architecture is simple to understand, robust and future proof. The SGML/XML standard is well developed and supported by the community. This improves the reusability of the tools. However, 2LTGT is an acronym for the Edinburgh Language lbchnology Group Tools 3GATE is an acronym for General Architecture for Text Engineering. 616 the architecture encourages tool development rather than reuse of existing TE components. GATE is based on an object-oriented data model (similar to the TIPSTER architecture (Grishman, 1997)). Modules communicate by reading and writing information to and from a central database. Unlike LTGT, both GATE and TEA are designed to encourage software reuse. Existing TE tools are easily incorporated with Tcl wrapper scripts and Java interfaces, respectively. Features that distinguish LTGT, GATE and TEA are the configuration methods, portability and motivation. Users of LTGT write shell scripts to define a system (as a chain of LTGT components). With GATE, a system is constructed manually by wiring TE components together using the graphical interface. TEA assumes the user knows nothing bu</context>
</contexts>
<marker>Grishman, 1997</marker>
<rawString>R. Grishman. 1997. Tipster architecture design document version 2.3. Technical report, DARPA. http://www.tipster.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LTG</author>
</authors>
<title>Edinburgh university, hcrc, ltg software. WWW. http: //www ac. uk/software/index.ht ml.</title>
<date>1999</date>
<contexts>
<context position="5537" citStr="LTG, 1999" startWordPosition="869" endWordPosition="870"> &apos;President Clinton&apos;, both lexical lookup and automatic tagging will agree that &apos;President&apos; is a noun. Nouns are generally informative, so should be retained in the compressed output text. However, grammar-based syntactic analysis gives a lower weighting to the first noun of a noun-noun construction, and bigram analysis tells us that &apos;President Clinton&apos; is a common word pair. These two modules overrule the simple POS value, and &apos;President Clinton&apos; is reduced to &apos;Clinton&apos;. 3 Related work Current trends in the development of reusable TE tools are best represented by the Edinburgh tools (LTGT)2 (LTG, 1999) and GATE3 (Cunningham et al., 1995). Like TEA, both LTGT and GATE are frameworks for TE. LTGT adopts the pipeline architecture for module integration. For processing, a text document is converted into SGML format. Processing modules are then applied to the SGML file sequentially. Annotations are accumulated as mark-up tags in the text. The architecture is simple to understand, robust and future proof. The SGML/XML standard is well developed and supported by the community. This improves the reusability of the tools. However, 2LTGT is an acronym for the Edinburgh Language lbchnology Group Tools</context>
</contexts>
<marker>LTG, 1999</marker>
<rawString>LTG. 1999. Edinburgh university, hcrc, ltg software. WWW. http: //www ac. uk/software/index.ht ml.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H</author>
</authors>
<title>Rollfs of Roelofs. Forthcoming. Telegraphese: Converting text into telegram style.</title>
<tech>Master&apos;s thesis,</tech>
<institution>Department of Computer Science, University of Manchester,</institution>
<location>Manchester, England.</location>
<marker>H, </marker>
<rawString>H. Rollfs of Roelofs. Forthcoming. Telegraphese: Converting text into telegram style. Master&apos;s thesis, Department of Computer Science, University of Manchester, Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G M P O&apos;Hare</author>
<author>N R Jennings</author>
<author>editors</author>
</authors>
<date>1996</date>
<booktitle>Foundations of Distributed Artificial Intelligence. Sixth generation computer series.</booktitle>
<pages>0--471</pages>
<publisher>Wiley Interscience Publishers,</publisher>
<location>New York.</location>
<marker>O&apos;Hare, Jennings, editors, 1996</marker>
<rawString>G. M. P. O&apos;Hare and N. R. Jennings, editors. 1996. Foundations of Distributed Artificial Intelligence. Sixth generation computer series. Wiley Interscience Publishers, New York. ISBN 0-471-00675.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the empirical methods in NLP conference,</booktitle>
<institution>University of Pennsylvania.</institution>
<marker>A, 1996</marker>
<rawString>A . Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the empirical methods in NLP conference, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reynar</author>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the fifth conference on Applied NLP,</booktitle>
<location>Washington D.C.</location>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>J. Reynar and A. Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the fifth conference on Applied NLP, Washington D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rich</author>
<author>K Knight</author>
</authors>
<date>1991</date>
<journal>Artificial Intelligence. McGraw-Hill, Inc., second edition. ISBN</journal>
<pages>0--07</pages>
<contexts>
<context position="2074" citStr="Rich and Knight, 1991" startWordPosition="313" endWordPosition="316">nformation value of each word is to be estimated based on an unusually wide range of linguistic information. TEA was designed to be a development environment for this work. However, the target application has led us to produce an interesting &apos;TEA is an acronym for Text Engineering Architecture. architecture and techniques that are more generally applicable, and it is these which we will focus on in this paper. 2 Architecture Figure 1: An overview of the TEA system framework. The central component of TEA is a framebased data model (F) (see Fig.2). In this model, a document is a list of frames (Rich and Knight, 1991) for recording the properties about each token in the text (example in Fig.2). A typical TE system converts a document into F with an input plug-in. The information required at the output determines the set of process plug-ins to activate. These use the information in F to add annotations to F. Their dependencies are automatically resolved by TEA. System behavior is controlled by adjusting the configurable parameters. Frame 1: (:token An :pos art :begin_s 1) Frame 2: (:token example :pos n) Frame 3: (:token sentence :pos n) Frame 4: (:token. :pos punc :end_s 1) Figure 2: &amp;quot;An example sentence.&amp;quot;</context>
</contexts>
<marker>Rich, Knight, 1991</marker>
<rawString>E. Rich and K. Knight. 1991. Artificial Intelligence. McGraw-Hill, Inc., second edition. ISBN 0-07-100894-2.</rawString>
</citation>
<citation valid="true">
<date>1997</date>
<booktitle>Hal&apos;s Legacy: 2001&apos;s Computer in Dream and Reality.</booktitle>
<editor>Stork, editor.</editor>
<publisher>MIT Press. http://mitpress.mit.edu/e-books/Hal/.</publisher>
<marker>1997</marker>
<rawString>. Stork, editor. 1997. Hal&apos;s Legacy: 2001&apos;s Computer in Dream and Reality. MIT Press. http://mitpress.mit.edu/e-books/Hal/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zavrel van Halteren</author>
<author>W Daelemans</author>
</authors>
<title>Improving data driven wordclass tagging by system combination.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL&apos;98,</booktitle>
<volume>1</volume>
<marker>van Halteren, Daelemans, 1998</marker>
<rawString>. van Halteren, J. Zavrel, and W. Daelemans. 1998. Improving data driven wordclass tagging by system combination. In Proceedings of COLING-ACL&apos;98, volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veronis</author>
<author>N Ide</author>
</authors>
<title>An accessment of semantic information automatically extracted from machine readable dictionaries.</title>
<date>1991</date>
<booktitle>In Proceedings of EACL&apos;91,</booktitle>
<pages>227--232</pages>
<location>Berlin.</location>
<contexts>
<context position="3969" citStr="Veronis and Ide (1991)" startWordPosition="619" endWordPosition="622">n and ease of software reuse. 2.1 Voting mechanism A feature that distinguishes TEA from similar systems is its use of voting mechanisms for system integration. Our approach has two distinct but uniformly treated applications. First, for any type of language analysis, different techniques ti will return successful results P(r) on different subsets of the problem space. Thus combining the outputs P(riti) from several ti should give a result more accurate than any one in isolation. This has been demonstrated in several systems (e.g. Choi (1999a); van Halteren et al. (1998); Brill and Wu (1998); Veronis and Ide (1991)). Our architecture currently offers two types of voting mechanisms: weighted average (Eq.1) and weighted maximum (Eq.2). A Bayesian classifier (Weiss and Kulikowski, 1991) based weight estimation algorithm (Eq.3) is included for constructing adaptive voting mechanisms. P(r) = &gt;wiP(rltj) (1) = max{wiP(riti), - • • ,wnP(ritn)} (2) = P(tilP(riti), • - • ,P(ritn)) (3) Second, different types of analysis ai will provide different information about a problem, hence, a solution is improved by combining several ai. For telegraphic text compression, we estimate E(w), the information value of a word, b</context>
</contexts>
<marker>Veronis, Ide, 1991</marker>
<rawString>J. Veronis and N. Ide. 1991. An accessment of semantic information automatically extracted from machine readable dictionaries. In Proceedings of EACL&apos;91, pages 227-232, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Weiss</author>
<author>C Kulikowski</author>
</authors>
<title>Computer Systems That Learn.</title>
<date>1991</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="4141" citStr="Weiss and Kulikowski, 1991" startWordPosition="643" endWordPosition="646">proach has two distinct but uniformly treated applications. First, for any type of language analysis, different techniques ti will return successful results P(r) on different subsets of the problem space. Thus combining the outputs P(riti) from several ti should give a result more accurate than any one in isolation. This has been demonstrated in several systems (e.g. Choi (1999a); van Halteren et al. (1998); Brill and Wu (1998); Veronis and Ide (1991)). Our architecture currently offers two types of voting mechanisms: weighted average (Eq.1) and weighted maximum (Eq.2). A Bayesian classifier (Weiss and Kulikowski, 1991) based weight estimation algorithm (Eq.3) is included for constructing adaptive voting mechanisms. P(r) = &gt;wiP(rltj) (1) = max{wiP(riti), - • • ,wnP(ritn)} (2) = P(tilP(riti), • - • ,P(ritn)) (3) Second, different types of analysis ai will provide different information about a problem, hence, a solution is improved by combining several ai. For telegraphic text compression, we estimate E(w), the information value of a word, based on a wide range of different information sources (Fig.2.1 shows a subset of our working system). The output of each ai are combined by a voting mechanism to form a sin</context>
</contexts>
<marker>Weiss, Kulikowski, 1991</marker>
<rawString>S. Weiss and C. Kulikowski. 1991. Computer Systems That Learn. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>