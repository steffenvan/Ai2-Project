<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.039043">
<title confidence="0.9951585">
Optimal and Syntactically-Informed Decoding for Monolingual
Phrase-Based Alignment
</title>
<author confidence="0.994973">
Kapil Thadani and Kathleen McKeown
</author>
<affiliation confidence="0.9960285">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.988673">
New York, NY 10027, USA
</address>
<email confidence="0.999619">
{kapil,kathy}@cs.columbia.edu
</email>
<sectionHeader confidence="0.995651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999816176470588">
The task of aligning corresponding phrases
across two related sentences is an important
component of approaches for natural language
problems such as textual inference, paraphrase
detection and text-to-text generation. In this
work, we examine a state-of-the-art struc-
tured prediction model for the alignment task
which uses a phrase-based representation and
is forced to decode alignments using an ap-
proximate search approach. We propose in-
stead a straightforward exact decoding tech-
nique based on integer linear programming
that yields order-of-magnitude improvements
in decoding speed. This ILP-based decoding
strategy permits us to consider syntactically-
informed constraints on alignments which sig-
nificantly increase the precision of the model.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999719461538462">
Natural language processing problems frequently in-
volve scenarios in which a pair or group of related
sentences need to be aligned to each other, establish-
ing links between their common words or phrases.
For instance, most approaches for natural language
inference (NLI) rely on alignment techniques to es-
tablish the overlap between the given premise and a
hypothesis before determining if the former entails
the latter. Such monolingual alignment techniques
are also frequently employed in systems for para-
phrase generation, multi-document summarization,
sentence fusion and question answering.
Previous work (MacCartney et al., 2008) has pre-
sented a phrase-based monolingual aligner for NLI
(MANLI) that has been shown to significantly out-
perform a token-based NLI aligner (Chambers et
al., 2007) as well as popular alignment techniques
borrowed from machine translation (Och and Ney,
2003; Liang et al., 2006). However, MANLI’s use
of a phrase-based alignment representation appears
to pose a challenge to the decoding task, i.e. the
task of recovering the highest-scoring alignment un-
der some parameters. Consequently, MacCartney et
al. (2008) employ a stochastic search algorithm to
decode alignments approximately while remaining
consistent with regard to phrase segmentation.
In this paper, we propose an exact decoding tech-
nique for MANLI that retrieves the globally opti-
mal alignment for a sentence pair given some pa-
rameters. Our approach is based on integer lin-
ear programming (ILP) and can leverage optimized
general-purpose LP solvers to recover exact solu-
tions. This strategy boosts decoding speed by an
order of magnitude over stochastic search in our ex-
periments. Additionally, we introduce hard syntac-
tic constraints on alignments produced by the model,
yielding better precision and a large increase in the
number of perfect alignments produced over our
evaluation corpus.
</bodyText>
<sectionHeader confidence="0.99976" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.991210428571429">
Alignment is an integral part of statistical MT (Vo-
gel et al., 1996; Och and Ney, 2003; Liang et al.,
2006) but the task is often substantively different
from monolingual alignment, which poses unique
challenges depending on the application (MacCart-
ney et al., 2008). Outside of NLI, prior research has
also explored the task of monolingual word align-
</bodyText>
<page confidence="0.976155">
254
</page>
<note confidence="0.582839">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.998060538461538">
ment using extensions of statistical MT (Quirk et al.,
2004) and multi-sequence alignment (Barzilay and
Lee, 2002).
ILP has been used extensively for applications
ranging from text-to-text generation (Clarke and La-
pata, 2008; Filippova and Strube, 2008; Wood-
send et al., 2010) to dependency parsing (Martins
et al., 2009). It has also been recently employed for
finding phrase-based MT alignments (DeNero and
Klein, 2008) in a manner similar to this work; how-
ever, we further build upon this model through syn-
tactic constraints on the words participating in align-
ments.
</bodyText>
<sectionHeader confidence="0.998132" genericHeader="method">
3 The MANLI Aligner
</sectionHeader>
<bodyText confidence="0.999898833333333">
Our alignment system is structured identically to
MANLI (MacCartney et al., 2008) and uses the same
phrase-based alignment representation. An align-
ment E between two fragments of text T1 and T2
is represented by a set of edits {e1, e2, ...1, each be-
longing to one of the following types:
</bodyText>
<listItem confidence="0.999965333333333">
• INS and DEL edits covering unaligned words in
T1 and T2 respectively
• SUB and EQ edits connecting a phrase in T1 to
</listItem>
<bodyText confidence="0.9704051">
a phrase in T2. EQ edits are a specific case of
SUB edits that denote a word/lemma match; we
refer to both types as SUB edits in this paper.
Every token in T1 and T2 participates in exactly one
edit. While alignments are one-to-one at the phrase
level, a phrase-based representation effectively per-
mits many-to-many alignments at the token level.
This enables the aligner to properly link paraphrases
such as death penalty and capital punishment by ex-
ploiting lexical resources.
</bodyText>
<subsectionHeader confidence="0.995179">
3.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999938">
MANLI was trained and evaluated on a corpus of
human-generated alignment annotations produced
by Microsoft Research (Brockett, 2007) for infer-
ence problems from the second Recognizing Tex-
tual Entailment (RTE2) challenge (Bar-Haim et al.,
2006). The corpus consists of a development set
and test set that both feature 800 inference prob-
lems, each of which consists of a premise, a hy-
pothesis and three independently-annotated human
alignments. In our experiments, we merge the an-
notations using majority rule in the same manner as
MacCartney et al. (2008).
</bodyText>
<subsectionHeader confidence="0.991605">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999979473684211">
A MANLI alignment is scored as a sum of weighted
feature values over the edits that it contains. Fea-
tures encode the type of edit, the size of the phrases
involved in SUB edits, whether the phrases are con-
stituents and their similarity (determined by lever-
aging various lexical resources). Additionally, con-
textual features note the similarity of neighboring
words and the relative positions of phrases while
a positional distortion feature accounts for the dif-
ference between the relative positions of SUB edit
phrases in their respective sentences.
Our implementation uses the same set of fea-
tures as MacCartney et al. (2008) with some mi-
nor changes: we use a shallow parser (Daum´e and
Marcu, 2005) for detecting constituents and employ
only string similarity and WordNet for determining
semantic relatedness, forgoing NomBank and the
distributional similarity resources used in the orig-
inal MANLI implementation.
</bodyText>
<subsectionHeader confidence="0.998643">
3.3 Parameter Inference
</subsectionHeader>
<bodyText confidence="0.999988888888889">
Feature weights are learned using the averaged
structured perceptron algorithm (Collins, 2002), an
intuitive structured prediction technique. We deviate
from MacCartney et al. (2008) and do not introduce
L2 normalization of weights during learning as this
could have an unpredictable effect on the averaged
parameters. For efficiency reasons, we parallelize
the training procedure using iterative parameter mix-
ing (McDonald et al., 2010) in our experiments.
</bodyText>
<subsectionHeader confidence="0.989426">
3.4 Decoding
</subsectionHeader>
<bodyText confidence="0.999980642857143">
The decoding problem is that of finding the highest-
scoring alignment under some parameter values for
the model. MANLI’s phrase-based representation
makes decoding more complex because the segmen-
tation of T1 and T2 into phrases is not known before-
hand. Every pair of phrases considered for inclusion
in an alignment must adhere to some consistent seg-
mentation so that overlapping edits and uncovered
words are avoided.
Consequently, the decoding problem cannot be
factored into a number of independent decisions
and MANLI searches for a good alignment using
a stochastic simulated annealing strategy. While
seemingly quite effective at avoiding local maxima,
</bodyText>
<page confidence="0.994541">
255
</page>
<table confidence="0.999508285714286">
System Data P% R% Fl% E%
MANLI dev 83.4 85.5 84.4 21.7
(reported 2008) test 85.4 85.3 85.3 21.3
MANLI dev 85.7 84.8 85.0 23.8
(reimplemented) test 87.2 86.3 86.7 24.5
MANLI-Exact dev 85.7 84.7 85.2 24.6
(this work) test 87.8 86.1 86.8 24.8
</table>
<tableCaption confidence="0.98832">
Table 1: Performance of aligners in terms of precision, re-
call, F-measure and number of perfect alignments (E%).
</tableCaption>
<bodyText confidence="0.999225">
this iterative search strategy is computationally ex-
pensive and moreover is not guaranteed to return the
highest-scoring alignment under the parameters.
</bodyText>
<sectionHeader confidence="0.985963" genericHeader="method">
4 Exact Decoding via ILP
</sectionHeader>
<bodyText confidence="0.9996883">
Instead of resorting to approximate solutions, we
can simply reformulate the decoding problem as the
optimization of a linear objective function with lin-
ear constraints, which can be solved by well-studied
algorithms using off-the-shelf solvers1. We first de-
fine boolean indicator variables xe for every possible
edit e between T1 and T2 that indicate whether e is
present in the alignment or not. The linear objective
that maximizes the score of edits for a given param-
eter vector w is expressed as follows:
</bodyText>
<equation confidence="0.992865">
�f(w) = max xe X score„(e)
e
�=max xe X w · 4b(e) (1)
e
</equation>
<bodyText confidence="0.9999698">
where 4b(e) is the feature vector over an edit. This
expresses the score of an alignment as the sum of
scores of edits that are present in it, i.e., edits e that
have xe = 1.
In order to address the phrase segmentation issue
discussed in §3.4, we merely need to add linear con-
straints ensuring that every token participates in ex-
actly one edit. Introducing the notation e � t to in-
dicate that edit e covers token t in one of its phrases,
this constraint can be encoded as:
</bodyText>
<equation confidence="0.9670605">
E xe = 1 bt E Ti, i = {1, 21
e: eft
</equation>
<bodyText confidence="0.880136">
On solving this integer program, the values of the
variables xe indicate which edits are present in the
</bodyText>
<footnote confidence="0.986275">
1We useLPsolve: http://lpsolve.sourceforge.net/
</footnote>
<table confidence="0.9967485">
Corpus Size Approximate Exact
Search ILP
RTE2 dev 800 2.58 0.11
test 800 1.67 0.08
McKeown et al. 297 61.96 2.45
(2010)
</table>
<tableCaption confidence="0.97074325">
Table 2: Approximate running time per decoding task in
seconds for the search-based approximate decoder and
the ILP-based exact decoder on various corpora (see text
for details).
</tableCaption>
<bodyText confidence="0.987764666666667">
highest-scoring alignment under w. A similar ap-
proach is employed by DeNero and Klein (2008) for
finding optimal phrase-based alignments for MT.
</bodyText>
<subsectionHeader confidence="0.993097">
4.1 Alignment experiments
</subsectionHeader>
<bodyText confidence="0.999987961538461">
For evaluation purposes, we compare the perfor-
mance of approximate search decoding against ex-
act ILP-based decoding on a reimplementation of
MANLI as described in §3. All models are trained
on the development section of the Microsoft Re-
search RTE2 alignment corpus (cf. §3.1) using
the training parameters specified in MacCartney
et al. (2008). Aligner performance is determined
by counting aligned token pairs per problem and
macro-averaging over all problems. The results are
shown in Table 1.
We first observe that our reimplemented version
of MANLI improves over the results reported in
MacCartney et al. (2008), gaining 2% in precision,
1% in recall and 2-3% in the fraction of alignments
that exactly matched human annotations. We at-
tribute at least some part of this gain to our modified
parameter inference (cf. §3.3) which avoids normal-
izing the structured perceptron weights and instead
adheres closely to the algorithm of Collins (2002).
Although exact decoding improves alignment per-
formance over the approximate search approach, the
gain is marginal and not significant. This seems to
indicate that the simulated annealing search strategy
is fairly effective at avoiding local maxima and find-
ing the highest-scoring alignments.
</bodyText>
<subsectionHeader confidence="0.986515">
4.2 Runtime experiments
</subsectionHeader>
<bodyText confidence="0.995921666666667">
Table 2 contains the results from timing alignment
tasks over various corpora on the same machine us-
ing the models trained as per §4.1. We observe a
</bodyText>
<page confidence="0.995311">
256
</page>
<bodyText confidence="0.999802538461538">
twenty-fold improvement in performance with ILP-
based decoding. It is important to note that the spe-
cific implementations being compared2 may be re-
sponsible for the relative speed of decoding.
The short hypotheses featured in the RTE2 cor-
pus (averaging 11 words) dampen the effect of the
quadratic growth in number of edits with sentence
length. For this reason, we also run the aligners on
a corpus of 297 related sentence pairs which don’t
have a particular disparity in sentence lengths (McK-
eown et al., 2010). The large difference in decoding
time illustrates the scaling limitations of the search-
based decoder.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="method">
5 Syntactically-Informed Constraints
</sectionHeader>
<bodyText confidence="0.999881181818182">
The use of an integer program for decoding pro-
vides us with a convenient mechanism to prevent
common alignment errors by introducting additional
constraints on edits. For example, function words
such as determiners and prepositions are often mis-
aligned just because they occur frequently in many
different contexts. Although MANLI makes use
of contextual features which consider the similar-
ity of neighboring words around phrase pairs, out-
of-context alignments of function words often ap-
pear in the output. We address this issue by adding
constraints to the integer program from §4 that look
at the syntactic structure of T1 and T2 and prevent
matching function words from appearing in an align-
ment unless they are syntactically linked with other
words that are aligned.
To enforce token-based constraints, we define
boolean indicator variables yt for each token t in
text snippets T1 and T2 that indicate whether t is in-
volved in a SUB edit or not. The following constraint
ensures that yt = 1 if and only if it is covered by a
SUB edit that is present in the alignment.
</bodyText>
<equation confidence="0.907394333333333">
�yt − xe = 0 bt E Ti, i = {1,2}
e: eft,
e is SUB
</equation>
<bodyText confidence="0.996399">
We refer to tokens t with yt = 1 as being active in
the alignment. Constraints can now be applied over
any token with specific part-of-speech (POS) tag in
</bodyText>
<footnote confidence="0.6853855">
2Our Python reimplementation closely follows the original
Java implementation of MANLI and was optimized for perfor-
mance. MacCartney et al. (2008) report a decoding time of
about 2 seconds per problem.
</footnote>
<table confidence="0.999500857142857">
System Data P% R% Fl% E%
MANLI-Exact with dev 86.8 84.5 85.6 25.3
M constraints test 88.8 85.7 87.2 29.9
MANLI-Exact with dev 86.1 84.6 85.3 24.5
L constraints test 88.2 86.4 87.3 27.6
MANLI-Exact with dev 87.1 84.4 85.8 25.4
M + L constraints test 89.5 86.2 87.8 33.0
</table>
<tableCaption confidence="0.9935415">
Table 3: Performance of MANLI-Exact featuring addi-
tional modifier (M) and lineage (L) constraints. Figures
in boldface are statistically significant over the uncon-
strained MANLI reimplementation (p &lt; 0.05).
</tableCaption>
<bodyText confidence="0.9487222">
order to ensure that it can only be active if a differ-
ent token related to it in a dependency parse of the
sentence is also active. We consider the following
classes of constraints:
Modifier constraints: Tokens t that represent con-
junctions, determiners, modals and cardinals can
only be active if their parent tokens 7r(t) are active.
yt − y,,(t) &lt;= 0
if POS(t) E {CC, CD, MD, DT, PDT, WDT}
Lineage constraints: Tokens t that represent prepo-
sitions and particles (which are often confused by
parsers) can only be active if one of their ancestors
α(t) or descendants S(t) is active. These constraints
are less restrictive than the modifier constraints in
order to account for attachment errors.
</bodyText>
<equation confidence="0.852025">
�yt − �ya − yd &lt;= 0
aEca(t) dES(t)
</equation>
<bodyText confidence="0.760069">
if POS(t) E {IN, TO, RP}
</bodyText>
<subsectionHeader confidence="0.998972">
5.1 Alignment experiments
</subsectionHeader>
<bodyText confidence="0.9997975">
A TAG-based probabilistic dependency parser (Ban-
galore et al., 2009) is used to formulate the above
constraints in our experiments. The results are
shown in Table 3 and indicate a notable increase in
alignment precision, which is to be expected as the
constraints specifically seek to exclude poor edits.
Despite the simple and overly general restrictions
being applied, recall is almost unaffected. Most
compellingly, the number of perfect alignments pro-
duced by the system increases significantly when
</bodyText>
<page confidence="0.990265">
257
</page>
<bodyText confidence="0.9943005">
compared to the unconstrained models from Table 1
(a relative increase of 35% on the test corpus).
</bodyText>
<sectionHeader confidence="0.997472" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999980348837209">
The results of our evaluation indicate that exact de-
coding via ILP is a robust and efficient technique for
solving alignment problems. Furthermore, the in-
corporation of simple constraints over a dependency
parse can help to shape more accurate alignments.
An examination of the alignments produced by our
system reveals that many remaining errors can be
tackled by the use of named-entity recognition and
better paraphrase corpora; this was also noted by
MacCartney et al. (2008) with regard to the original
MANLI system. In addition, stricter constraints that
enforce the alignment of syntactically-related tokens
(rather than just their inclusion in the solution) may
also yield performance gains.
Although MANLI’s structured prediction ap-
proach to the alignment problem allows us to encode
preferences as features and learn their weights via
the structured perceptron, the decoding constraints
used here can be used to establish dynamic links be-
tween alignment edits which cannot be determined
a priori. The interaction between the selection of
soft features for structured prediction and hard con-
straints for decoding is an interesting avenue for fur-
ther research on this task. Initial experiments with
a feature that considers the similarity of dependency
heads of tokens in an edit (similar to MANLI’s con-
textual features that look at preceding and following
words) yielded some improvement over the base-
line models; however, this did not perform as well
as the simple constraints described above. Specific
features that approximate soft variants of these con-
straints could also be devised but this was not ex-
plored here.
In addition to the NLI applications considered in
this work, we have also employed the MANLI align-
ment technique to tackle alignment problems that
are not inherently asymmetric such as the sentence
fusion problems from McKeown et al. (2010). Al-
though the absence of asymmetric alignment fea-
tures affects performance marginally over the RTE2
dataset, all the performance gains exhibited by exact
decoding with constraints appear to be preserved in
symmetric settings.
</bodyText>
<sectionHeader confidence="0.996742" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999988928571429">
We present a simple exact decoding technique as an
alternative to approximate search-based decoding in
MANLI that exhibits a twenty-fold improvement in
runtime performance in our experiments. In addi-
tion, we propose novel syntactically-informed con-
straints to increase precision. Our final system im-
proves over the results reported in MacCartney et al.
(2008) by about 4.5% in precision and 1% in recall,
with a large gain in the number of perfect alignments
over the test corpus. Finally, we analyze the align-
ments produced and suggest that further improve-
ments are possible through careful feature/constraint
design, as well as the use of named-entity recogni-
tion and additional resources.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999983444444444">
The authors are grateful to Bill MacCartney for pro-
viding a reference MANLI implementation and the
anonymous reviewers for their useful feedback. This
material is based on research supported in part by
the U.S. National Science Foundation (NSF) under
IIS-05-34871. Any opinions, findings and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the NSF.
</bodyText>
<sectionHeader confidence="0.998552" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9994058">
Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen
Rambow, and Benoit Sagot. 2009. MICA: a prob-
abilistic dependency parser based on tree insertion
grammars. In Proceedings of HLT-NAACL, pages
185–188.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second PASCAL Recognising Textual En-
tailment challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment.
Regina Barzilay and Lilian Lee. 2002. Bootstrapping
lexical choice via multiple-sequence alignment. In
Proceedings of EMNLP.
Chris Brockett. 2007. Aligning the 2006 RTE cor-
pus. Technical Report MSR-TR-2007-77, Microsoft
Research.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh, and
</reference>
<page confidence="0.963451">
258
</page>
<reference confidence="0.999885264150943">
Christopher D. Manning. 2007. Learning alignments
and leveraging natural logic. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 165–170.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: an integer linear pro-
gramming approach. Journal of Artifical Intelligence
Research, 31:399–429, March.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models. In Proceedings of
EMNLP, pages 1–8.
Hal Daum´e, III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin meth-
ods for structured prediction. In Proceedings of ICML,
pages 169–176.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
HLT, pages 25–28.
Katja Filippova and Michael Strube. 2008. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of EMNLP, pages 177–185.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL,
pages 104–111.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP, pages 802–811.
Andr´e F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of ACL-
IJCNLP, pages 342–350.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Proceedings of HLT-NAACL, pages 456–
464.
Kathleen McKeown, Sara Rosenthal, Kapil Thadani, and
Coleman Moore. 2010. Time-efficient creation of an
accurate sentence fusion corpus. In Proceedings of
HLT-NAACL, pages 317–320.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19–51, March.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In Proceedings of EMNLP, pages 142–149,
July.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proceedings of COLING, pages 836–841.
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Title generation with quasi-synchronous gram-
mar. In Proceedings of EMNLP, pages 513–523.
</reference>
<page confidence="0.998534">
259
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.746205">
<title confidence="0.996083">Optimal and Syntactically-Informed Decoding for Phrase-Based Alignment</title>
<author confidence="0.9669">Thadani</author>
<affiliation confidence="0.999922">Department of Computer</affiliation>
<address confidence="0.9002765">Columbia New York, NY 10027,</address>
<abstract confidence="0.998211333333333">The task of aligning corresponding phrases across two related sentences is an important component of approaches for natural language problems such as textual inference, paraphrase detection and text-to-text generation. In this work, we examine a state-of-the-art structured prediction model for the alignment task which uses a phrase-based representation and is forced to decode alignments using an approximate search approach. We propose instead a straightforward exact decoding technique based on integer linear programming that yields order-of-magnitude improvements in decoding speed. This ILP-based decoding strategy permits us to consider syntacticallyinformed constraints on alignments which significantly increase the precision of the model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Pierre Boullier</author>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
<author>Benoit Sagot</author>
</authors>
<title>MICA: a probabilistic dependency parser based on tree insertion grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>185--188</pages>
<contexts>
<context position="14690" citStr="Bangalore et al., 2009" startWordPosition="2347" endWordPosition="2351">t conjunctions, determiners, modals and cardinals can only be active if their parent tokens 7r(t) are active. yt − y,,(t) &lt;= 0 if POS(t) E {CC, CD, MD, DT, PDT, WDT} Lineage constraints: Tokens t that represent prepositions and particles (which are often confused by parsers) can only be active if one of their ancestors α(t) or descendants S(t) is active. These constraints are less restrictive than the modifier constraints in order to account for attachment errors. �yt − �ya − yd &lt;= 0 aEca(t) dES(t) if POS(t) E {IN, TO, RP} 5.1 Alignment experiments A TAG-based probabilistic dependency parser (Bangalore et al., 2009) is used to formulate the above constraints in our experiments. The results are shown in Table 3 and indicate a notable increase in alignment precision, which is to be expected as the constraints specifically seek to exclude poor edits. Despite the simple and overly general restrictions being applied, recall is almost unaffected. Most compellingly, the number of perfect alignments produced by the system increases significantly when 257 compared to the unconstrained models from Table 1 (a relative increase of 35% on the test corpus). 6 Discussion The results of our evaluation indicate that exac</context>
</contexts>
<marker>Bangalore, Boullier, Nasr, Rambow, Sagot, 2009</marker>
<rawString>Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen Rambow, and Benoit Sagot. 2009. MICA: a probabilistic dependency parser based on tree insertion grammars. In Proceedings of HLT-NAACL, pages 185–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The second PASCAL Recognising Textual Entailment challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="5190" citStr="Bar-Haim et al., 2006" startWordPosition="785" endWordPosition="788"> as SUB edits in this paper. Every token in T1 and T2 participates in exactly one edit. While alignments are one-to-one at the phrase level, a phrase-based representation effectively permits many-to-many alignments at the token level. This enables the aligner to properly link paraphrases such as death penalty and capital punishment by exploiting lexical resources. 3.1 Dataset MANLI was trained and evaluated on a corpus of human-generated alignment annotations produced by Microsoft Research (Brockett, 2007) for inference problems from the second Recognizing Textual Entailment (RTE2) challenge (Bar-Haim et al., 2006). The corpus consists of a development set and test set that both feature 800 inference problems, each of which consists of a premise, a hypothesis and three independently-annotated human alignments. In our experiments, we merge the annotations using majority rule in the same manner as MacCartney et al. (2008). 3.2 Features A MANLI alignment is scored as a sum of weighted feature values over the edits that it contains. Features encode the type of edit, the size of the phrases involved in SUB edits, whether the phrases are constituents and their similarity (determined by leveraging various lexi</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second PASCAL Recognising Textual Entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lilian Lee</author>
</authors>
<title>Bootstrapping lexical choice via multiple-sequence alignment.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3573" citStr="Barzilay and Lee, 2002" startWordPosition="517" endWordPosition="520"> (Vogel et al., 1996; Och and Ney, 2003; Liang et al., 2006) but the task is often substantively different from monolingual alignment, which poses unique challenges depending on the application (MacCartney et al., 2008). Outside of NLI, prior research has also explored the task of monolingual word align254 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002). ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. 3 The MANLI Aligner Our alignment system is structured identically to MANLI (MacCartney et al., 2008) and uses the same phrase-based alignment r</context>
</contexts>
<marker>Barzilay, Lee, 2002</marker>
<rawString>Regina Barzilay and Lilian Lee. 2002. Bootstrapping lexical choice via multiple-sequence alignment. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
</authors>
<title>Aligning the 2006 RTE corpus.</title>
<date>2007</date>
<tech>Technical Report MSR-TR-2007-77, Microsoft Research.</tech>
<contexts>
<context position="5079" citStr="Brockett, 2007" startWordPosition="770" endWordPosition="771"> in T2. EQ edits are a specific case of SUB edits that denote a word/lemma match; we refer to both types as SUB edits in this paper. Every token in T1 and T2 participates in exactly one edit. While alignments are one-to-one at the phrase level, a phrase-based representation effectively permits many-to-many alignments at the token level. This enables the aligner to properly link paraphrases such as death penalty and capital punishment by exploiting lexical resources. 3.1 Dataset MANLI was trained and evaluated on a corpus of human-generated alignment annotations produced by Microsoft Research (Brockett, 2007) for inference problems from the second Recognizing Textual Entailment (RTE2) challenge (Bar-Haim et al., 2006). The corpus consists of a development set and test set that both feature 800 inference problems, each of which consists of a premise, a hypothesis and three independently-annotated human alignments. In our experiments, we merge the annotations using majority rule in the same manner as MacCartney et al. (2008). 3.2 Features A MANLI alignment is scored as a sum of weighted feature values over the edits that it contains. Features encode the type of edit, the size of the phrases involved</context>
</contexts>
<marker>Brockett, 2007</marker>
<rawString>Chris Brockett. 2007. Aligning the 2006 RTE corpus. Technical Report MSR-TR-2007-77, Microsoft Research.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nathanael Chambers</author>
<author>Daniel Cer</author>
<author>Trond Grenager</author>
<author>David Hall</author>
<author>Chloe Kiddon</author>
<author>Bill MacCartney</author>
<author>MarieCatherine de Marneffe</author>
<author>Daniel Ramage</author>
</authors>
<title>Eric Yeh,</title>
<location>and</location>
<marker>Chambers, Cer, Grenager, Hall, Kiddon, MacCartney, de Marneffe, Ramage, </marker>
<rawString>Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon, Bill MacCartney, MarieCatherine de Marneffe, Daniel Ramage, Eric Yeh, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Learning alignments and leveraging natural logic.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>165--170</pages>
<marker>Manning, 2007</marker>
<rawString>Christopher D. Manning. 2007. Learning alignments and leveraging natural logic. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 165–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression: an integer linear programming approach.</title>
<date>2008</date>
<journal>Journal of Artifical Intelligence Research,</journal>
<pages>31--399</pages>
<contexts>
<context position="3683" citStr="Clarke and Lapata, 2008" startWordPosition="532" endWordPosition="536">m monolingual alignment, which poses unique challenges depending on the application (MacCartney et al., 2008). Outside of NLI, prior research has also explored the task of monolingual word align254 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002). ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. 3 The MANLI Aligner Our alignment system is structured identically to MANLI (MacCartney et al., 2008) and uses the same phrase-based alignment representation. An alignment E between two fragments of text T1 and T2 is represented by a set of edits {e1, e2</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: an integer linear programming approach. Journal of Artifical Intelligence Research, 31:399–429, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="6551" citStr="Collins, 2002" startWordPosition="1000" endWordPosition="1001"> distortion feature accounts for the difference between the relative positions of SUB edit phrases in their respective sentences. Our implementation uses the same set of features as MacCartney et al. (2008) with some minor changes: we use a shallow parser (Daum´e and Marcu, 2005) for detecting constituents and employ only string similarity and WordNet for determining semantic relatedness, forgoing NomBank and the distributional similarity resources used in the original MANLI implementation. 3.3 Parameter Inference Feature weights are learned using the averaged structured perceptron algorithm (Collins, 2002), an intuitive structured prediction technique. We deviate from MacCartney et al. (2008) and do not introduce L2 normalization of weights during learning as this could have an unpredictable effect on the averaged parameters. For efficiency reasons, we parallelize the training procedure using iterative parameter mixing (McDonald et al., 2010) in our experiments. 3.4 Decoding The decoding problem is that of finding the highestscoring alignment under some parameter values for the model. MANLI’s phrase-based representation makes decoding more complex because the segmentation of T1 and T2 into phra</context>
<context position="10764" citStr="Collins (2002)" startWordPosition="1691" endWordPosition="1692">al. (2008). Aligner performance is determined by counting aligned token pairs per problem and macro-averaging over all problems. The results are shown in Table 1. We first observe that our reimplemented version of MANLI improves over the results reported in MacCartney et al. (2008), gaining 2% in precision, 1% in recall and 2-3% in the fraction of alignments that exactly matched human annotations. We attribute at least some part of this gain to our modified parameter inference (cf. §3.3) which avoids normalizing the structured perceptron weights and instead adheres closely to the algorithm of Collins (2002). Although exact decoding improves alignment performance over the approximate search approach, the gain is marginal and not significant. This seems to indicate that the simulated annealing search strategy is fairly effective at avoiding local maxima and finding the highest-scoring alignments. 4.2 Runtime experiments Table 2 contains the results from timing alignment tasks over various corpora on the same machine using the models trained as per §4.1. We observe a 256 twenty-fold improvement in performance with ILPbased decoding. It is important to note that the specific implementations being co</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models. In Proceedings of EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Learning as search optimization: approximate large margin methods for structured prediction.</title>
<date>2005</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>169--176</pages>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e, III and Daniel Marcu. 2005. Learning as search optimization: approximate large margin methods for structured prediction. In Proceedings of ICML, pages 169–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In Proceedings of ACLHLT,</booktitle>
<pages>25--28</pages>
<contexts>
<context position="3880" citStr="DeNero and Klein, 2008" startWordPosition="564" endWordPosition="567"> Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002). ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. 3 The MANLI Aligner Our alignment system is structured identically to MANLI (MacCartney et al., 2008) and uses the same phrase-based alignment representation. An alignment E between two fragments of text T1 and T2 is represented by a set of edits {e1, e2, ...1, each belonging to one of the following types: • INS and DEL edits covering unaligned words in T1 and T2 respectively • SUB and EQ edits connecting a phrase in T1 to a phrase in T2. EQ edits</context>
<context position="9738" citStr="DeNero and Klein (2008)" startWordPosition="1529" endWordPosition="1532">phrases, this constraint can be encoded as: E xe = 1 bt E Ti, i = {1, 21 e: eft On solving this integer program, the values of the variables xe indicate which edits are present in the 1We useLPsolve: http://lpsolve.sourceforge.net/ Corpus Size Approximate Exact Search ILP RTE2 dev 800 2.58 0.11 test 800 1.67 0.08 McKeown et al. 297 61.96 2.45 (2010) Table 2: Approximate running time per decoding task in seconds for the search-based approximate decoder and the ILP-based exact decoder on various corpora (see text for details). highest-scoring alignment under w. A similar approach is employed by DeNero and Klein (2008) for finding optimal phrase-based alignments for MT. 4.1 Alignment experiments For evaluation purposes, we compare the performance of approximate search decoding against exact ILP-based decoding on a reimplementation of MANLI as described in §3. All models are trained on the development section of the Microsoft Research RTE2 alignment corpus (cf. §3.1) using the training parameters specified in MacCartney et al. (2008). Aligner performance is determined by counting aligned token pairs per problem and macro-averaging over all problems. The results are shown in Table 1. We first observe that our</context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>John DeNero and Dan Klein. 2008. The complexity of phrase alignment problems. In Proceedings of ACLHLT, pages 25–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Sentence fusion via dependency graph compression.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>177--185</pages>
<contexts>
<context position="3711" citStr="Filippova and Strube, 2008" startWordPosition="537" endWordPosition="540">which poses unique challenges depending on the application (MacCartney et al., 2008). Outside of NLI, prior research has also explored the task of monolingual word align254 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002). ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. 3 The MANLI Aligner Our alignment system is structured identically to MANLI (MacCartney et al., 2008) and uses the same phrase-based alignment representation. An alignment E between two fragments of text T1 and T2 is represented by a set of edits {e1, e2, ...1, each belonging to on</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Katja Filippova and Michael Strube. 2008. Sentence fusion via dependency graph compression. In Proceedings of EMNLP, pages 177–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="1909" citStr="Liang et al., 2006" startWordPosition="266" endWordPosition="269">niques to establish the overlap between the given premise and a hypothesis before determining if the former entails the latter. Such monolingual alignment techniques are also frequently employed in systems for paraphrase generation, multi-document summarization, sentence fusion and question answering. Previous work (MacCartney et al., 2008) has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006). However, MANLI’s use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, MacCartney et al. (2008) employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation. In this paper, we propose an exact decoding technique for MANLI that retrieves the globally optimal alignment for a sentence pair given some parameters. Our approach is based on integer linear programming (ILP) and can leverage o</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A phrase-based alignment model for natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>802--811</pages>
<contexts>
<context position="1632" citStr="MacCartney et al., 2008" startWordPosition="222" endWordPosition="225">guage processing problems frequently involve scenarios in which a pair or group of related sentences need to be aligned to each other, establishing links between their common words or phrases. For instance, most approaches for natural language inference (NLI) rely on alignment techniques to establish the overlap between the given premise and a hypothesis before determining if the former entails the latter. Such monolingual alignment techniques are also frequently employed in systems for paraphrase generation, multi-document summarization, sentence fusion and question answering. Previous work (MacCartney et al., 2008) has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006). However, MANLI’s use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, MacCartney et al. (2008) employ a stochastic search algorithm to decode alignments approximately while remaining</context>
<context position="3169" citStr="MacCartney et al., 2008" startWordPosition="460" endWordPosition="464">to recover exact solutions. This strategy boosts decoding speed by an order of magnitude over stochastic search in our experiments. Additionally, we introduce hard syntactic constraints on alignments produced by the model, yielding better precision and a large increase in the number of perfect alignments produced over our evaluation corpus. 2 Related Work Alignment is an integral part of statistical MT (Vogel et al., 1996; Och and Ney, 2003; Liang et al., 2006) but the task is often substantively different from monolingual alignment, which poses unique challenges depending on the application (MacCartney et al., 2008). Outside of NLI, prior research has also explored the task of monolingual word align254 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002). ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et</context>
<context position="5501" citStr="MacCartney et al. (2008)" startWordPosition="837" endWordPosition="840">y and capital punishment by exploiting lexical resources. 3.1 Dataset MANLI was trained and evaluated on a corpus of human-generated alignment annotations produced by Microsoft Research (Brockett, 2007) for inference problems from the second Recognizing Textual Entailment (RTE2) challenge (Bar-Haim et al., 2006). The corpus consists of a development set and test set that both feature 800 inference problems, each of which consists of a premise, a hypothesis and three independently-annotated human alignments. In our experiments, we merge the annotations using majority rule in the same manner as MacCartney et al. (2008). 3.2 Features A MANLI alignment is scored as a sum of weighted feature values over the edits that it contains. Features encode the type of edit, the size of the phrases involved in SUB edits, whether the phrases are constituents and their similarity (determined by leveraging various lexical resources). Additionally, contextual features note the similarity of neighboring words and the relative positions of phrases while a positional distortion feature accounts for the difference between the relative positions of SUB edit phrases in their respective sentences. Our implementation uses the same s</context>
<context position="10160" citStr="MacCartney et al. (2008)" startWordPosition="1593" endWordPosition="1596">e search-based approximate decoder and the ILP-based exact decoder on various corpora (see text for details). highest-scoring alignment under w. A similar approach is employed by DeNero and Klein (2008) for finding optimal phrase-based alignments for MT. 4.1 Alignment experiments For evaluation purposes, we compare the performance of approximate search decoding against exact ILP-based decoding on a reimplementation of MANLI as described in §3. All models are trained on the development section of the Microsoft Research RTE2 alignment corpus (cf. §3.1) using the training parameters specified in MacCartney et al. (2008). Aligner performance is determined by counting aligned token pairs per problem and macro-averaging over all problems. The results are shown in Table 1. We first observe that our reimplemented version of MANLI improves over the results reported in MacCartney et al. (2008), gaining 2% in precision, 1% in recall and 2-3% in the fraction of alignments that exactly matched human annotations. We attribute at least some part of this gain to our modified parameter inference (cf. §3.3) which avoids normalizing the structured perceptron weights and instead adheres closely to the algorithm of Collins (2</context>
<context position="13309" citStr="MacCartney et al. (2008)" startWordPosition="2113" endWordPosition="2116">traints, we define boolean indicator variables yt for each token t in text snippets T1 and T2 that indicate whether t is involved in a SUB edit or not. The following constraint ensures that yt = 1 if and only if it is covered by a SUB edit that is present in the alignment. �yt − xe = 0 bt E Ti, i = {1,2} e: eft, e is SUB We refer to tokens t with yt = 1 as being active in the alignment. Constraints can now be applied over any token with specific part-of-speech (POS) tag in 2Our Python reimplementation closely follows the original Java implementation of MANLI and was optimized for performance. MacCartney et al. (2008) report a decoding time of about 2 seconds per problem. System Data P% R% Fl% E% MANLI-Exact with dev 86.8 84.5 85.6 25.3 M constraints test 88.8 85.7 87.2 29.9 MANLI-Exact with dev 86.1 84.6 85.3 24.5 L constraints test 88.2 86.4 87.3 27.6 MANLI-Exact with dev 87.1 84.4 85.8 25.4 M + L constraints test 89.5 86.2 87.8 33.0 Table 3: Performance of MANLI-Exact featuring additional modifier (M) and lineage (L) constraints. Figures in boldface are statistically significant over the unconstrained MANLI reimplementation (p &lt; 0.05). order to ensure that it can only be active if a different token rela</context>
<context position="15721" citStr="MacCartney et al. (2008)" startWordPosition="2511" endWordPosition="2514">increases significantly when 257 compared to the unconstrained models from Table 1 (a relative increase of 35% on the test corpus). 6 Discussion The results of our evaluation indicate that exact decoding via ILP is a robust and efficient technique for solving alignment problems. Furthermore, the incorporation of simple constraints over a dependency parse can help to shape more accurate alignments. An examination of the alignments produced by our system reveals that many remaining errors can be tackled by the use of named-entity recognition and better paraphrase corpora; this was also noted by MacCartney et al. (2008) with regard to the original MANLI system. In addition, stricter constraints that enforce the alignment of syntactically-related tokens (rather than just their inclusion in the solution) may also yield performance gains. Although MANLI’s structured prediction approach to the alignment problem allows us to encode preferences as features and learn their weights via the structured perceptron, the decoding constraints used here can be used to establish dynamic links between alignment edits which cannot be determined a priori. The interaction between the selection of soft features for structured pr</context>
<context position="17709" citStr="MacCartney et al. (2008)" startWordPosition="2816" endWordPosition="2819">rom McKeown et al. (2010). Although the absence of asymmetric alignment features affects performance marginally over the RTE2 dataset, all the performance gains exhibited by exact decoding with constraints appear to be preserved in symmetric settings. 7 Conclusion We present a simple exact decoding technique as an alternative to approximate search-based decoding in MANLI that exhibits a twenty-fold improvement in runtime performance in our experiments. In addition, we propose novel syntactically-informed constraints to increase precision. Our final system improves over the results reported in MacCartney et al. (2008) by about 4.5% in precision and 1% in recall, with a large gain in the number of perfect alignments over the test corpus. Finally, we analyze the alignments produced and suggest that further improvements are possible through careful feature/constraint design, as well as the use of named-entity recognition and additional resources. Acknowledgments The authors are grateful to Bill MacCartney for providing a reference MANLI implementation and the anonymous reviewers for their useful feedback. This material is based on research supported in part by the U.S. National Science Foundation (NSF) under </context>
</contexts>
<marker>MacCartney, Galley, Manning, 2008</marker>
<rawString>Bill MacCartney, Michel Galley, and Christopher D. Manning. 2008. A phrase-based alignment model for natural language inference. In Proceedings of EMNLP, pages 802–811.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of ACLIJCNLP,</booktitle>
<pages>342--350</pages>
<contexts>
<context position="3780" citStr="Martins et al., 2009" startWordPosition="549" endWordPosition="552">al., 2008). Outside of NLI, prior research has also explored the task of monolingual word align254 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002). ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. 3 The MANLI Aligner Our alignment system is structured identically to MANLI (MacCartney et al., 2008) and uses the same phrase-based alignment representation. An alignment E between two fragments of text T1 and T2 is represented by a set of edits {e1, e2, ...1, each belonging to one of the following types: • INS and DEL edits covering unaligned word</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Andr´e F. T. Martins, Noah A. Smith, and Eric P. Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proceedings of ACLIJCNLP, pages 342–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Keith Hall</author>
<author>Gideon Mann</author>
</authors>
<title>Distributed training strategies for the structured perceptron.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>456--464</pages>
<contexts>
<context position="6894" citStr="McDonald et al., 2010" startWordPosition="1048" endWordPosition="1051">larity and WordNet for determining semantic relatedness, forgoing NomBank and the distributional similarity resources used in the original MANLI implementation. 3.3 Parameter Inference Feature weights are learned using the averaged structured perceptron algorithm (Collins, 2002), an intuitive structured prediction technique. We deviate from MacCartney et al. (2008) and do not introduce L2 normalization of weights during learning as this could have an unpredictable effect on the averaged parameters. For efficiency reasons, we parallelize the training procedure using iterative parameter mixing (McDonald et al., 2010) in our experiments. 3.4 Decoding The decoding problem is that of finding the highestscoring alignment under some parameter values for the model. MANLI’s phrase-based representation makes decoding more complex because the segmentation of T1 and T2 into phrases is not known beforehand. Every pair of phrases considered for inclusion in an alignment must adhere to some consistent segmentation so that overlapping edits and uncovered words are avoided. Consequently, the decoding problem cannot be factored into a number of independent decisions and MANLI searches for a good alignment using a stochas</context>
</contexts>
<marker>McDonald, Hall, Mann, 2010</marker>
<rawString>Ryan McDonald, Keith Hall, and Gideon Mann. 2010. Distributed training strategies for the structured perceptron. In Proceedings of HLT-NAACL, pages 456– 464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>Sara Rosenthal</author>
<author>Kapil Thadani</author>
<author>Coleman Moore</author>
</authors>
<title>Time-efficient creation of an accurate sentence fusion corpus.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>317--320</pages>
<contexts>
<context position="11746" citStr="McKeown et al., 2010" startWordPosition="1847" endWordPosition="1851">ignment tasks over various corpora on the same machine using the models trained as per §4.1. We observe a 256 twenty-fold improvement in performance with ILPbased decoding. It is important to note that the specific implementations being compared2 may be responsible for the relative speed of decoding. The short hypotheses featured in the RTE2 corpus (averaging 11 words) dampen the effect of the quadratic growth in number of edits with sentence length. For this reason, we also run the aligners on a corpus of 297 related sentence pairs which don’t have a particular disparity in sentence lengths (McKeown et al., 2010). The large difference in decoding time illustrates the scaling limitations of the searchbased decoder. 5 Syntactically-Informed Constraints The use of an integer program for decoding provides us with a convenient mechanism to prevent common alignment errors by introducting additional constraints on edits. For example, function words such as determiners and prepositions are often misaligned just because they occur frequently in many different contexts. Although MANLI makes use of contextual features which consider the similarity of neighboring words around phrase pairs, outof-context alignment</context>
<context position="17110" citStr="McKeown et al. (2010)" startWordPosition="2728" endWordPosition="2731">of dependency heads of tokens in an edit (similar to MANLI’s contextual features that look at preceding and following words) yielded some improvement over the baseline models; however, this did not perform as well as the simple constraints described above. Specific features that approximate soft variants of these constraints could also be devised but this was not explored here. In addition to the NLI applications considered in this work, we have also employed the MANLI alignment technique to tackle alignment problems that are not inherently asymmetric such as the sentence fusion problems from McKeown et al. (2010). Although the absence of asymmetric alignment features affects performance marginally over the RTE2 dataset, all the performance gains exhibited by exact decoding with constraints appear to be preserved in symmetric settings. 7 Conclusion We present a simple exact decoding technique as an alternative to approximate search-based decoding in MANLI that exhibits a twenty-fold improvement in runtime performance in our experiments. In addition, we propose novel syntactically-informed constraints to increase precision. Our final system improves over the results reported in MacCartney et al. (2008) </context>
</contexts>
<marker>McKeown, Rosenthal, Thadani, Moore, 2010</marker>
<rawString>Kathleen McKeown, Sara Rosenthal, Kapil Thadani, and Coleman Moore. 2010. Time-efficient creation of an accurate sentence fusion corpus. In Proceedings of HLT-NAACL, pages 317–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="1888" citStr="Och and Ney, 2003" startWordPosition="262" endWordPosition="265">y on alignment techniques to establish the overlap between the given premise and a hypothesis before determining if the former entails the latter. Such monolingual alignment techniques are also frequently employed in systems for paraphrase generation, multi-document summarization, sentence fusion and question answering. Previous work (MacCartney et al., 2008) has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006). However, MANLI’s use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, MacCartney et al. (2008) employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation. In this paper, we propose an exact decoding technique for MANLI that retrieves the globally optimal alignment for a sentence pair given some parameters. Our approach is based on integer linear programming (IL</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation. In</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>142--149</pages>
<contexts>
<context position="3519" citStr="Quirk et al., 2004" startWordPosition="510" endWordPosition="513">rk Alignment is an integral part of statistical MT (Vogel et al., 1996; Och and Ney, 2003; Liang et al., 2006) but the task is often substantively different from monolingual alignment, which poses unique challenges depending on the application (MacCartney et al., 2008). Outside of NLI, prior research has also explored the task of monolingual word align254 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002). ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. 3 The MANLI Aligner Our alignment system is structured identically to MANLI (MacCartney et</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William Dolan. 2004. Monolingual machine translation for paraphrase generation. In In Proceedings of EMNLP, pages 142–149, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="2970" citStr="Vogel et al., 1996" startWordPosition="429" endWordPosition="433">rieves the globally optimal alignment for a sentence pair given some parameters. Our approach is based on integer linear programming (ILP) and can leverage optimized general-purpose LP solvers to recover exact solutions. This strategy boosts decoding speed by an order of magnitude over stochastic search in our experiments. Additionally, we introduce hard syntactic constraints on alignments produced by the model, yielding better precision and a large increase in the number of perfect alignments produced over our evaluation corpus. 2 Related Work Alignment is an integral part of statistical MT (Vogel et al., 1996; Och and Ney, 2003; Liang et al., 2006) but the task is often substantively different from monolingual alignment, which poses unique challenges depending on the application (MacCartney et al., 2008). Outside of NLI, prior research has also explored the task of monolingual word align254 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 20</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignment in statistical translation. In Proceedings of COLING, pages 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Title generation with quasi-synchronous grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>513--523</pages>
<contexts>
<context position="3735" citStr="Woodsend et al., 2010" startWordPosition="541" endWordPosition="545">s depending on the application (MacCartney et al., 2008). Outside of NLI, prior research has also explored the task of monolingual word align254 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002). ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. 3 The MANLI Aligner Our alignment system is structured identically to MANLI (MacCartney et al., 2008) and uses the same phrase-based alignment representation. An alignment E between two fragments of text T1 and T2 is represented by a set of edits {e1, e2, ...1, each belonging to one of the following types</context>
</contexts>
<marker>Woodsend, Feng, Lapata, 2010</marker>
<rawString>Kristian Woodsend, Yansong Feng, and Mirella Lapata. 2010. Title generation with quasi-synchronous grammar. In Proceedings of EMNLP, pages 513–523.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>