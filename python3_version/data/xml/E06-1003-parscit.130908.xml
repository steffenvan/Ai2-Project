<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.625644">
Weakly Supervised Approaches for Ontology Population
Hristo Tanev Tanev Bernardo Magnini
ITC-irst ITC-irst
</title>
<address confidence="0.65105">
38050, Povo 38050, Povo
Trento, Italy Trento, Italy
</address>
<email confidence="0.997777">
htanev@yahoo.co.uk magnini@itc.it
</email>
<sectionHeader confidence="0.998591" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999910727272728">
We present a weakly supervised approach
to automatic Ontology Population from
text and compare it with other two unsu-
pervised approaches. In our experiments
we populate a part of our ontology of
Named Entities. We considered two high
level categories - geographical locations
and person names and ten sub-classes for
each category. For each sub-class, from
a list of training examples and a syntac-
tically parsed corpus, we automatically
learn a syntactic model - a set of weighted
syntactic features, i.e. words which typ-
ically co-occur in certain syntactic posi-
tions with the members of that class. The
model is then used to classify the unknown
Named Entities in the test set. The method
is weakly supervised, since no annotated
corpus is used in the learning process. We
achieved promising results, i.e. 65% accu-
racy, outperforming significantly previous
unsupervised approaches.
</bodyText>
<sectionHeader confidence="0.999453" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999159941176471">
Automatic Ontology Population (OP) from texts
has recently emerged as a new field of application
for knowledge acquisition techniques (see, among
others, (Buitelaar et al., 2005)). Although there
is no a univocally accepted definition for the OP
task, a useful approximation has been suggested
(Bontcheva and Cunningham, 2003) as Ontology
Driven Information Extraction, where, in place of
a template to be filled, the goal of the task is the ex-
traction and classification of instances of concepts
and relations defined in a Ontology. The task has
been approached in a variety of similar perspec-
tives, including term clustering (e.g. (Lin, 1998a)
and (Almuhareb and Poesio, 2004)) and term cat-
egorization (e.g. (Avancini et al., 2003)).
A rather different task is Ontology Learning
(OL), where new concepts and relations are sup-
posed to be acquired, with the consequence of
changing the definition of the Ontology itself (see,
for instance, (Velardi et al., 2005)).
In this paper OP is defined in the following sce-
nario. Given a set of terms T = t1, t2, ..., tn, a
document collection D, where terms in T are sup-
posed to appear, and a set of predefined classes
C = c1, c2,..., cm denoting concepts in an Ontol-
ogy, each term ti has to be assigned to the proper
class in C. For the purposes of the experiments
presented in this paper we assume that (i) classes
in C are mutually disjoint and (ii) each term is as-
signed to just one class.
As we have defined it, OP shows a strong sim-
ilarity with Named Entity Recognition and Clas-
sification (NERC). However, a major difference is
that in NERC each occurrences of a recognized
term has to be classified separately, while in OP it
is the term, independently of the context in which
it appears, that has to be classified.
While Information Extraction, and NERC in
particular, have been addressed prevalently by
means of supervised approaches, Ontology Popu-
lation is typically attacked in an unsupervised way.
As many authors have pointed out (e.g. (Cimiano
and V¨olker, 2005)), the main motivation is the fact
that in OP the set of classes is usually larger and
more fine grained than in NERC (where the typ-
ical set includes Person, Location, Organization,
GPE, and a Miscellanea class for all other kind
of entities). In addition, by definition, the set of
classes in C changes as a new ontology is consid-
ered, making the creation of annotated data almost
impossible practically.
</bodyText>
<page confidence="0.998303">
17
</page>
<bodyText confidence="0.999394958333334">
According with the demand for weakly super-
vised approaches to OP, we propose a method,
called Class − Example, which learns a classi-
fication model from a set of classified terms, ex-
ploiting lexico-syntactic features. Unlike most of
the approaches which consider pair wise similarity
between terms ((Cimiano and V¨olker, 2005); (Lin,
1998a)), the Class-Example method considers the
similarity between a term tz and a set of training
examples which represent a certain class. This re-
sults in a great number of class features and opens
the possibility to exploit more statistical data, such
as the frequency of appearance of a class feature in
different training terms.
In order to show the effectiveness of the Class-
Example approach, it has been compared against
two different approaches: (i) a Class-Pattern unsu-
pervised approach, in the style of (Hearst, 1998);
(ii) an unsupervised approach that considers the
word of the class as a pivot word for acquiring
relevant contexts for the class (we refer to this
method as Class−Word). Results of the compar-
ison show that the Class-Example method outper-
forms significantly the other two methods, making
it appealing even considering the need of supervi-
sion.
Although the Class-Example method we pro-
pose is applicable in general, in this paper we
show its usefulness when applied to terms denot-
ing Named Entities. The motivation behind this
choice is the practical value of Named Entity clas-
sifications, as, for instance, in applications such as
Questions Answering and Information Extraction.
Moreover, some Named Entity classes, including
names of writers, athletes and organizations, dy-
namically change over the time, which makes it
impossible to capture them in a static Ontology.
The rest of the paper is structured as follows.
Section 2 describes the state-of-the-art methods in
Ontology Population. Section 3 presents the three
approaches to the task we have compared. Section
4 introduces Syntactic Network, a formalism used
for the representation of syntactic information and
exploited in both the Class-Word and the Class-
Example approaches. Section 5 reports on the
experimental settings, results obtained, and dis-
cusses the three approaches. Section 6 concludes
the paper and suggests directions for future work.
</bodyText>
<sectionHeader confidence="0.999346" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999949133333334">
There are two main paradigms distinguishing On-
tology Population approaches. In the first one
Ontology Population is performed using patterns
(Hearst, 1998) or relying on the structure of terms
(Velardi et al., 2005). In the second paradigm the
task is addressed using contextual features (Cimi-
ano and V¨olker, 2005).
Pattern-based approaches search for phrases
which explicitly show that there is an “is-a” re-
lation between two words, e.g. “the ant is an in-
sect” or “ants and other insects”. However, such
phrases do not appear frequently in a text cor-
pus. For this reason, some approaches use the Web
(Schlobach et al., 2004). (Velardi et al., 2005) ex-
perimented several head-matching heuristics ac-
cording to which if a term1 is in the head of
term2, then there is an “is-a” relation between
them: For example “Christmas tree” is a kind of
“tree”.
Context feature approaches use a corpus to ex-
tract features from the context in which a se-
mantic class tends to appear. Contextual features
may be superficial (Fleischman and Hovy, 2002)
or syntactic (Lin, 1998a), (Almuhareb and Poe-
sio, 2004). Comparative evaluation in (Cimiano
and V¨olker, 2005) shows that syntactic features
lead to better performance. Feature weights can
be calculated either by Machine Learning algo-
rithms (Fleischman and Hovy, 2002) or by statisti-
cal measures, like Point Wise Mutual Information
or the Jaccard coefficient (Lin, 1998a).
A hybrid approach using both pattern-based,
term structure, and contextual feature methods is
presented in (Cimiano et al., 2005).
State-of-the-art approaches may be divided in
two classes, according to different use of train-
ing data: Unsupervised approaches (see (Cimi-
ano et al., 2005) for details) and supervised ap-
proaches which use manually tagged training data,
e.g. (Fleischman and Hovy, 2002). While state-
of-the-art unsupervised methods have low perfor-
mance, supervised approaches reach higher ac-
curacy, but require the manual construction of a
training set, which impedes them from large scale
applications.
</bodyText>
<sectionHeader confidence="0.7613435" genericHeader="method">
3 Weakly supervised approaches for
Ontology Population
</sectionHeader>
<bodyText confidence="0.993504">
In this Section we present three Ontology Popula-
tion approaches. Two of them are unsupervised:
</bodyText>
<page confidence="0.998488">
18
</page>
<bodyText confidence="0.999161333333333">
(i) a pattern-based approach described in (Hearst,
1998), which we refer to as Class-Pattern and (ii)
a feature similarity method reported in (Cimiano
and V¨olker, 2005) to which we will refer as Class-
Word. Finally, we describe a new weakly super-
vised approach for ontology population which ac-
cepts as a training data lists of instances for each
class under consideration. This method we call
Class-Example.
</bodyText>
<subsectionHeader confidence="0.998708">
3.1 Class-Pattern approach
</subsectionHeader>
<bodyText confidence="0.999794428571429">
This approach was described first in (Hearst,
1998). The main idea is that if a term t belongs
to a class c, then in a text corpus we may expect
the occurrence of phrases like such c as t,.... In our
experiments for ontology population we used the
patterns described in the Hearst’s paper plus the
pattern t is (a  |the) c:
</bodyText>
<listItem confidence="0.9981432">
1. t is (a  |the) c
2. such c as t
3. such c as (NP,)*, (and  |or) t
4. t (,NP)* (and  |or) other c
5. c, (especially  |including) (NP, )* t
</listItem>
<bodyText confidence="0.999751">
For each instance from the test set t and for each
concept c we instantiated the patterns and searched
with them in the corpus. If a pattern which is in-
stantiated with a concept c and a term t appears
in the corpus, then we assume the t belongs to c.
For example, if the term to be classified is “Etna”
and the concept is “mountain”, one of the instan-
tiated patterns will be “mountains such as Etna”;
if this pattern is found in the text, then “Etna” is
considered to be a “mountain”. If the algorithm
assigns a term to several categories, we choose the
one which co-occurs most often with the term.
</bodyText>
<subsectionHeader confidence="0.999529">
3.2 Class-Word approach
</subsectionHeader>
<bodyText confidence="0.999961090909091">
(Cimiano and V¨olker, 2005) describes an unsu-
pervised approach for ontology population based
on vector-feature similarity between each concept
c and a term to be classified t. For example,
in order to conclude how much “Etna” is an ap-
propriate instance of the class “mountain”, this
method finds the feature-vector similarity between
the word “Etna” and the word “mountain”. Each
instance from the test set T is assigned to one of
the classes in the set C. Features are collected
from Corpus and the classification algorithm on
</bodyText>
<equation confidence="0.734915777777778">
classify(T, C, Corpus)
foreach(t in T) do{
vt = getContextVector(t, Corpus);}
foreach(c in C) do{
v, = getContextVector(c, Corpus);}
foreach(t in T) do{
classes[t] = argmax,∈Csim(vt,v,);}
return classes[];
end classify
</equation>
<figureCaption confidence="0.9834575">
Figure 1: Unsupervised algorithm for Ontology
Population.
</figureCaption>
<bodyText confidence="0.981419921052632">
figure 1 is applied. The problem with this ap-
proach is that the context distribution of a name
(e.g. “Etna”) is sometimes different than the con-
text distribution of the class word (e.g. “moun-
tain”). Moreover, a single word provides a limited
quantity of contextual data.
In this algorithm the context vectors vt and
v, are feature vectors whose elements represent
weighted context features from Corpus of the
term t (e.g. “Etna”) or the concept word c (e.g.
“mountain”). Cimiano and V¨olker evaluate differ-
ent context features and prove that syntactic fea-
tures work best. Therefore, in our experimen-
tal settings we considered only such features ex-
tracted from a corpus parsed with a dependency
parser. Unlike the original approach which relies
on pseudo-syntactic features, we used features ex-
tracted from dependency parse trees. Moreover,
we used virtually all the words connected syntacti-
cally to a term, not only the modifiers. A syntactic
feature is a pair: (word, syntactic relation) (Lin,
1998a). We use two feature types: First order fea-
tures, which are directly connected to the training
or test examples in the dependency parse trees of
Corpus; second order features, which are con-
nected to the training or test instances indirectly
by skipping one word (the verb) in the dependency
tree. As an example, let’s consider two sentences:
“Edison invented the phonograph” and “Edison
created the phonograph”. If “Edison” is a name
to be classified, then two first order features of this
name exist - (“invent”, subject-of) and (“create”,
subject-of). One second order feature can be ex-
tracted - (“phonograph”, object-of+subject); it co-
occurs two times with the word “Edison”. In our
experiments second order features are considered
only those words which are governed by the same
verb whose subject is the name which is a training
</bodyText>
<page confidence="0.99547">
19
</page>
<bodyText confidence="0.911857">
or test instance (in this example “Edison”).
</bodyText>
<subsectionHeader confidence="0.736374">
3.3 Weakly Supervised Class-Example
Approach
</subsectionHeader>
<bodyText confidence="0.999967818181818">
The approach we put forward here uses the same
processing stages as the one presented in Fig-
ure 1 and relies on syntactic features extracted
from a corpus. However, the Class-Example al-
gorithm receives as an additional input parame-
ter the sets of training examples for each class
c E C. These training sets are simple lists of
instances (i.e. terms denoting Named Entities),
without context, and can be acquired automati-
cally or semi-automatically from an existing on-
tology or gazetteer. To facilitate their acquisition,
the Class-Example approach imposes no restric-
tions to the training examples - they can be am-
biguous and have different frequencies. However,
they have to appear in Corpus (in our experimen-
tal settings - at least twice). For example, for the
class “mountain” training examples are: “Ever-
est”, “Mauna Loa”, etc.
The algorithm learns from each training set
Train(c) a single feature vector vc, called the syn-
tactic model of the class. Therefore, in our algo-
rithm, the statement
</bodyText>
<equation confidence="0.755818">
vc = getContextVector(c, Corpus)
</equation>
<bodyText confidence="0.929497142857143">
in Figure 1 is substituted with
vc = getSyntacticModel(Train(c), Corpus).
For each class c, a set of syntactic features F(c)
are collected by finding the union of the features
extracted from each occurrence in the corpus of
each training instance in Train(c). Next, the fea-
ture vector vc is constructed: If a feature is not
present in F(c), then its corresponding coordinate
in vc has value 0; otherwise, it has a value equal to
the feature weight.
The weight of a feature fc E F(c) is calculated
in three steps:
1. First, the co-occurrence of fc with the train-
ing set is calculated:
</bodyText>
<equation confidence="0.987160666666667">
� α.log( P(fc, t) )
weights(fc) = P(fc).P(t)
t∈Train(c)
</equation>
<bodyText confidence="0.915114181818182">
where P(fc, t) is the probability that feature
fc co-occurs with t, P(fc) and P(t) are the
probabilities that fc and t appear in the cor-
pus, α = 14 for syntactic features with lexi-
cal element noun and α = 1 for all the other
3. To obtain the final weight of fc, we divide
weightN(fc) by the number of classes in
which this feature appears. This is motivated
by the intuition that a feature which appears
in the syntactic models of many classes is not
a good class predictor.
</bodyText>
<equation confidence="0.99849">
weightN(fc)
weight(fc) = |Classes(fc)|
</equation>
<bodyText confidence="0.999965916666667">
where |Classes(fc) |is the number of classes
for which fc is present in the syntactic model.
As shown in Figure 1, the classification uses a
similarity function sim(vt, vc) whose arguments
are the feature vector of a term vt and the feature
vector vc for a class c. We defined the similarity
function as the dot product of the two feature vec-
tors: sim(vt, vc) = vc.vt. Vectors vt are binary
(i.e. the feature value is 1 if the feature is present
and, 0-otherwise), while the features in the syntac-
tic model vectors vc receive weights according to
the approach described in this Section.
</bodyText>
<sectionHeader confidence="0.982768" genericHeader="method">
4 Representing Syntactic Information
</sectionHeader>
<bodyText confidence="0.9953143">
Since both the Class-Word and the Class-Example
methods work with syntactic features, the main
source of information is a syntactically parsed cor-
pus. We parsed about half a gigabyte of a news
corpus with MiniPar (Lin, 1998b). It is a statis-
tically based dependency parser which is reported
to reach 89% precision and 82% recall on press re-
portage texts. MiniPar generates syntactic depen-
dency structures - directed labeled graphs whose
syntactic features. The α parameter reflects
the linguistic intuition that nouns are more in-
formative than verbs and adjectives which in
most cases represent generic predicates. The
values of α were automatically learned from
the training data.
2. We normalize the feature weights, since we
observed that they vary a lot between dif-
ferent classes: for each class c we find the
feature with maximal weight and denote its
weight with mxW (c),
</bodyText>
<equation confidence="0.974984">
mxW (c) = maxfc∈F(c)weights(fc)
</equation>
<bodyText confidence="0.6570445">
Next, the weight of each feature fc E F(c) is
normalized by dividing it with mxW (c):
</bodyText>
<equation confidence="0.820916333333333">
weights(fc)
weightN(fc) = mxW (c)
20
g1 g2 SyntNet(g1, g2)
lovesJ1
lovesJ4 o /�
</equation>
<figure confidence="0.883611666666667">
JJT
JaneJ6 lovesJ1,4
JaneJ6
JTTo
JTT
JT
soJs
JohnJ2
12 MaryJ3 Jo
</figure>
<figureCaption confidence="0.683523">
Figure 2: Two syntactic graphs and their Syntactic Network.
</figureCaption>
<figure confidence="0.979981962962963">
(4,6)
/�
(1,2)(4,5)
1 2 2
one aggregate representation
Vertices labeled with equal words in
and
are merged into one generalizing vertex in
g2). For example, the vertices with
label John in
and
are merged into one vertex
John in
g2).
Edges are merged in a similar way:
(loves, John) E
and (loves, John) E
are represented through one edge (loves, John)
in
g2).
Each vertex in
and
is labeled addition-
ally with a numerical index which is unique
for the graph set. Numbers on vertices in
g2) show which vertices from
and
</figure>
<figureCaption confidence="0.979235842105263">
are merged in the corresponding Synt-
Net vertices. For example, vertex loves E
g2) has a set 11,41 which means
that vertices 1 and 4 are merged in it. In a similar
way the edge (loves, John) E
g2)
is labeled with two pairs of indices (4, 5) and
(1, 2), which shows that it represents two edges:
the edge between vertices 4 and 5 and the edge
between 1 and 2.
Two properties of SyntNet are important: first
isomorphic sub-structures from all the graphs rep-
resented via a SyntNet are mapped into one struc-
ture. This allows us to easily find all the oc-
currences of multiword terms and named enti-
ties. Second, using the numerical indices on ver-
tices and edges, we can efficiently calculate which
structures are connected syntactically to the train-
ing and test terms. As an example,
</figureCaption>
<bodyText confidence="0.901797">
try to cal-
culate in which constructions the word
ap-
pears considering SyntNet in Figure 2. First, in
SyntNet we can directly observe that there is the
relation loves —* Mary labeled with the pair 1 —* 3
- therefore this relation appears once in the corpus.
Next, tracing the numerical indices on the ver-
tices and edges we can find a path from
to
through
The path passes through
the following numerical indices: 3 +— 1 —* 2: this
means that there is one appearance of the str
</bodyText>
<equation confidence="0.975766647058823">
SyntNet(g1,g2).
g1
g2
SyntNet(g1,
g1
g2
SyntNet(g1,
g1
g2
SyntNet(g1,
g1
g2
SyntNet(g1,
g1
g2
SyntNet(g1,
SyntNet(g1,
</equation>
<bodyText confidence="0.969375396226415">
let’s
“Mary”
“Mary”
“John”
“loves”.
ucture
vertices represent words and the edges between
them represent syntactic relations like subject, ob-
ject, modifier, etc. Examples for two dependency
structures -
and g2, are shown in Figure 2:
They represent the sentences
loves
and
loves
labels s and o on their
edges stand for subject and object respectively.
The syntactic structures generated by MiniPar are
dendroid (tree-like), but still cycles appear in some
cases.
In order to extract information from the parsed
corpus, we had to choose a model for represent-
ing dependency trees which allows to search ef-
ficiently for syntactic structures and to calculate
their frequencies. Building a classic index at word
level was not an option, since we have to search for
syntactic structures, not words. On the other hand,
indexing syntactic relations (i.e. word pair and the
relation between the words) would be useful, but
still does not resolve the problem, since in many
cases we search for more complex structures than
a relation between two words: for example, when
we have to find which words are syntactically re-
lated to a Named Entity composed by two words,
we have to search for syntactic structures which
consists of three vertices and two edges.
In order to trace efficiently more complex struc-
tures in the corpus, we put forward a model for
representation of a set of labeled graphs, called
Syntactic Network (SyntNet for short). The model
is inspired by a model presented earlier in (Szpek-
tor et al., 2004), however our model allows more
efficient construction of the representation. The
scope of SyntNet is to represent a set of labeled
graphs through one aggregate structure in which
the isomorphic sub-structures overlap. When
SyntNet represents a syntactically parsed text cor-
pus, its vertices are labeled with words from the
text while edges represent syntactic relations from
the corpus and are labeled accordingly.
An example is shown in Figure 2, where two
syntactic graphs,
and
</bodyText>
<figure confidence="0.4375316">
g1
“John
Mary”
“John
Jane”;
</figure>
<page confidence="0.640882">
g1
</page>
<note confidence="0.331993">
g2, are merged into
</note>
<page confidence="0.9974">
21
</page>
<bodyText confidence="0.999760740740741">
“John loves Mary” in the corpus, spanning through
vertices 1, 2, and 3. Such a path through the nu-
merical indices cannot be found between “Mary”
and “Jane” which means that they do not appear in
the same syntactic construction in the corpus.
SyntNet is built incrementally in a straightfor-
ward manner: Each new vertex or edge added to
the network is merged with the identical vertex or
edge, if such already exists in SyntNet. Otherwise,
a new vertex or edge is added to the network. The
time necessary for building a SyntNet is propor-
tional to the number of the vertices and the edges
in the represented graphs (and does not otherwise
depend on their complexity).
The efficiency of the SyntNet model when
representing and searching for labeled structures
makes it very appropriate for the representation of
a syntactically parsed corpus. We used the prop-
erties of SyntNet in order to trace efficiently the
occurrences of Named Entities in the parsed cor-
pus, to calculate their frequencies, to find the syn-
tactic features which co-occur with these Named
Entities, as well as the frequencies of these co-
occurrences. Moreover, the SyntNet model al-
lowed us to extract more complex, second order
syntactic features which are connected indirectly
to the terms in the training and the test set.
</bodyText>
<sectionHeader confidence="0.903088" genericHeader="method">
5 Experimental settings and results
</sectionHeader>
<bodyText confidence="0.999556954545455">
We have evaluated all the three approaches de-
scribed in Section 3. The same evaluation settings
were used for the three experiments. The source
of features was a news corpus of about half a gi-
gabyte. The corpus was parsed with MiniPar and
a Syntactic Network representation was built from
the dependency parse trees produced by the parser.
Syntactic features were extracted from this Synt-
Net.
We considered two high-level Named Entity
categories: Locations and Persons. For each of
them five fine-grained sub-classes were taken into
consideration. For locations: mountain, lake,
river, city, and country; for persons: statesman,
writer, athlete, actor, and inventor.
For each class under consideration we created
a test set of Named Entities using WordNet 2.0
and Internet sites like Wikipedia. For the Class-
Example approach we also provided training data
using the same resources. WordNet was the pri-
mary data source for training and test data. The ex-
amples from it were extracted automatically. We
</bodyText>
<table confidence="0.999808789473684">
P (%) R (%) F (%)
mountain 58 78 67
lake 75 50 60
river 69 55 61
city 56 76 65
country 86 93 89
locations macro 69 70 68
locations micro 78 78 78
statesman 42 72 53
writer 93 55 69
athlete 90 47 62
actor 90 73 80
inventor 12 33 18
persons macro 65 56 57
persons micro 57 57 57
total macro 67 63 62
total micro 65 65 65
category location 83 91 87
category person 95 89 92
</table>
<tableCaption confidence="0.8438915">
Table 1: Performance of the Class-Example ap-
proach.
</tableCaption>
<bodyText confidence="0.995791703703704">
used Internet to get additional examples for some
classes. To do this, we created automatic text ex-
traction scripts for Web pages and manually fil-
tered their output when it was necessary.
Totally, the test data comprised 280 Named En-
tities which were not ambiguous and appeared at
least twice in the corpus.
For the Class-Example approach we provided
a training set of 1194 names. The only require-
ment to the names in the training set was that
they appear at least twice in the parsed corpus.
They were allowed to be ambiguous and no man-
ual post-processing or filtering was carried out on
this data.
For both context feature approaches (i.e. Class-
Word and Class-Example), we used the same type
of syntactic features and the same classification
function, namely the one described in Section 3.3.
This was done in order to compare better the ap-
proaches.
Results from the comparative evaluation are
shown in Table 2. For each approach we mea-
sured macro average precision, macro average re-
call, macro average F-measure and micro average
F; for Class-Word and Class-Example micro F is
equal to the overall accuracy, i.e. the percent of the
instances classified correctly. The first row shows
</bodyText>
<page confidence="0.993262">
22
</page>
<table confidence="0.9993332">
macro P (%) macro R (%) macro F (%) micro F(%)
Class-Patterns 18 6 9 10
Class-Word 32 41 33 42
Class-Example 67 63 62 65
Class-Example (sec. ord.) 65 61 62 68
</table>
<tableCaption confidence="0.999887">
Table 2: Comparison of different approaches.
</tableCaption>
<bodyText confidence="0.999968206896552">
the results obtained with superficial patterns. The
second row presents the results from the Class-
Word approach. The third row shows the results
of our Class-Example method. The fourth line
presents the results for the same approach but us-
ing second-order features for the person category.
The Class-Pattern approach showed low perfor-
mance, similar to the random classification, for
which macro and micro F=10%. Patterns suc-
ceeded to classify correctly only instances of the
classes “river” and “city”. For the class “city”
the patterns reached precision of 100% and recall
65%; for the class “river” precision was high (i.e.
75%), but recall was 15%.
The Class-Word approach showed significantly
better performance (macro F=33%, micro F=42%)
than the Class-Pattern approach.
The performance of the Class-Example (62%
macro F and 65%-68% micro F) is much higher
than the performance of Class-Word (29% in-
crease in macro F and 23% in micro F). The last
row of the table shows that second-order syntactic
features augment further the performance of the
Class-Example method in terms of micro average
F (68% vs. 65%).
A more detailed evaluation of the Class-
Example approach is shown in Table 1. In this
table we show the performance of the approach
without the second-order features. Results vary
between different classes: The highest F is mea-
sured for the class “country” - 89% and the low-
est is for the class “inventor” - 18%. However,
the class “inventor” is an exception - for all the
other classes the F measure is over 50%. Another
difference may be observed between the Location
and Person classes: Our approach performs sig-
nificantly better for the locations (68% vs. 57%
macro F and 78% vs. 57% micro F). Although
different classes had different number of training
examples, we observed that the performance for
a class does not depend on the size of its training
set. We think, that the variation in performance be-
tween categories is due to the different specificity
of their textual contexts. As a consequence, some
classes tend to co-occur with more specific syn-
tactic features, while for other classes this is not
true.
Additionally, we measured the performance
of our approach considering only the macro-
categories “Location” and “Person”. For this pur-
pose we did not run another experiment, we rather
used the results from the fine-grained classifica-
tion and grouped the already obtained classes. Re-
sults are shown in the last two rows of table 1: It
turns out that the Class-Example method makes
very well the difference between “location” and
“person” - 90% of the test instances were classi-
fied correctly between these categories.
</bodyText>
<sectionHeader confidence="0.996314" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.9970995">
In this paper we presented a new weakly super-
vised approach for Ontology Population, called
Class-Example, and confronted it with two other
methods. Experimental results show that the
Class-Example approach has best performance. In
particular, it reached 65% of accuracy, outper-
forming in our experimental framework the state-
of-the-art Class-Word method by 42%. Moreover,
for location names the method reached accuracy
of 78%. Although the experiments are not com-
parable, we would like to state that some super-
vised approaches for fine-grained Named Entity
classification, e.g. (Fleischman, 2001), have sim-
ilar accuracy. On the other hand, the presented
weakly supervised Class-Example approach re-
quires as a training data only a list of terms for
each class under consideration. Training exam-
ples can be automatically acquired from existing
ontologies or other sources, since the approach
imposes virtually no restrictions on them. This
makes our weakly supervised methodology appli-
cable on larger scale than supervised approaches,
still having significantly better performance than
the unsupervised ones.
In our experimental framework we used syntac-
tic features extracted from dependency parse trees
</bodyText>
<page confidence="0.991403">
23
</page>
<bodyText confidence="0.99997455">
and we put forward a novel model for the repre-
sentation of a syntactically parsed corpus. This
model allows for performing a comprehensive ex-
traction of syntactic features from a corpus includ-
ing more complex second-order ones, which re-
sulted in an improvement of performance. This
and other empirical observations not described in
this paper lead us to the conclusion that the per-
formance of an Ontology Population system im-
proves with the increase of the types of syntactic
features under consideration.
In our future work we consider applying our
Ontology Population methodology to more se-
mantic categories and to experiment with other
types of syntactic features, as well as other types
of feature-weighting formulae and learning algo-
rithms. We consider also the integration of the
approach in a Question Answering or Information
Extraction system, where it can be used to perform
fine-grained type checking.
</bodyText>
<sectionHeader confidence="0.99909" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999823222222222">
A. Almuhareb and M. Poesio. 2004. Attribute-
based and value-based clustering: An evaluation.
In Proceedings of EMNLP 2004, pages 158–165,
Barcelona, Spain.
H. Avancini, A. Lavelli, B. Magnini, F. Sebastiani, and
R. Zanoli. 2003. Expanding Domain-Specific Lex-
icons by Term Categorization. In Proceedings of
SAC 2003, pages 793–79.
K. Bontcheva and H. Cunningham. 2003. The Se-
mantic Web: A New Opportunity and Challenge for
HLT. In Proceedings of the Workshop HLT for the
Semantic Web and Web Services at ISWC 2003.
P. Buitelaar, P. Cimiano, and B. Magnini, editors.
2005. Ontology Learning from Text: Methods, Eval-
uation and Applications. IOS Press, Amsterdam,
The Netherlands.
P. Cimiano and J. V¨olker. 2005. Towards large-scale,
open-domain and ontology-based named entity clas-
sification. In Proceedings of RANLP’05, pages 166–
172, Borovets, Bulgaria.
P. Cimiano, A. Pivk, L.S. Thieme, and S. Staab. 2005.
Learning Taxonomic Relations from Heterogeneous
Sources of Evidence. In Ontology Learning from
Text: Methods, Evaluation and Applications. IOS
Press.
M. Fleischman and E. Hovy. 2002. Fine Grained
Classification of Named Entities. In Proceedings of
COLING 2002, Taipei, Taiwan, August.
M. Fleischman. 2001. Automated Subcategorization
of Named Entities. In 39th Annual Meeting of the
ACL, Student Research Workshop, Toulouse, France,
July.
M. Hearst. 1998. Automated Discovery of Word-
Net Relations. In WordNet: An Electronic Lexical
Database. MIT Press.
D. Lin. 1998a. Automatic Retrieval and Clustering of
Similar Words. In Proceedings of COLING-ACL98,
Montreal, Canada, August.
D. Lin. 1998b. Dependency-based Evaluation of Mini-
Par. In Proceedings of Workshop on the Evaluation
of Parsing Systems, Granada, Spain.
S. Schlobach, M. Olsthoorn, and M. de Rijke. 2004.
Type Checking in Open-Domain Question Answer-
ing. In Proceedings of ECAI 2004.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling Web-based Acquisition of Entailment Rela-
tions. In Proceedings of EMNLP 2004, Barcelona,
Spain.
P. Velardi, R.Navigli, A. Cuchiarelli, and F.Neri. 2005.
Evaluation of Ontolearn, a Methodology for Auto-
matic Population of Domain Ontologies. In P. Buite-
laar, P. Cimiano, and B. Magnini, editors, Ontology
Learning from Text: Methods, Evaluation and Ap-
plications. IOS Press.
</reference>
<page confidence="0.999177">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.866881">
<title confidence="0.999751">Weakly Supervised Approaches for Ontology Population</title>
<author confidence="0.998973">Hristo Tanev Tanev Bernardo Magnini</author>
<affiliation confidence="0.930759">ITC-irst ITC-irst</affiliation>
<address confidence="0.986265">38050, Povo 38050, Povo Trento, Italy Trento, Italy</address>
<email confidence="0.966461">htanev@yahoo.co.ukmagnini@itc.it</email>
<abstract confidence="0.998978347826087">We present a weakly supervised approach to automatic Ontology Population from text and compare it with other two unsupervised approaches. In our experiments we populate a part of our ontology of Named Entities. We considered two high categories locations names ten sub-classes for each category. For each sub-class, from a list of training examples and a syntactically parsed corpus, we automatically a model a set of weighted i.e. words which typically co-occur in certain syntactic positions with the members of that class. The model is then used to classify the unknown Named Entities in the test set. The method is weakly supervised, since no annotated corpus is used in the learning process. We achieved promising results, i.e. 65% accuracy, outperforming significantly previous unsupervised approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Almuhareb</author>
<author>M Poesio</author>
</authors>
<title>Attributebased and value-based clustering: An evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004,</booktitle>
<pages>158--165</pages>
<location>Barcelona,</location>
<contexts>
<context position="1778" citStr="Almuhareb and Poesio, 2004" startWordPosition="268" endWordPosition="271">ecently emerged as a new field of application for knowledge acquisition techniques (see, among others, (Buitelaar et al., 2005)). Although there is no a univocally accepted definition for the OP task, a useful approximation has been suggested (Bontcheva and Cunningham, 2003) as Ontology Driven Information Extraction, where, in place of a template to be filled, the goal of the task is the extraction and classification of instances of concepts and relations defined in a Ontology. The task has been approached in a variety of similar perspectives, including term clustering (e.g. (Lin, 1998a) and (Almuhareb and Poesio, 2004)) and term categorization (e.g. (Avancini et al., 2003)). A rather different task is Ontology Learning (OL), where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (see, for instance, (Velardi et al., 2005)). In this paper OP is defined in the following scenario. Given a set of terms T = t1, t2, ..., tn, a document collection D, where terms in T are supposed to appear, and a set of predefined classes C = c1, c2,..., cm denoting concepts in an Ontology, each term ti has to be assigned to the proper class in C. For the</context>
<context position="6901" citStr="Almuhareb and Poesio, 2004" startWordPosition="1116" endWordPosition="1120">r “ants and other insects”. However, such phrases do not appear frequently in a text corpus. For this reason, some approaches use the Web (Schlobach et al., 2004). (Velardi et al., 2005) experimented several head-matching heuristics according to which if a term1 is in the head of term2, then there is an “is-a” relation between them: For example “Christmas tree” is a kind of “tree”. Context feature approaches use a corpus to extract features from the context in which a semantic class tends to appear. Contextual features may be superficial (Fleischman and Hovy, 2002) or syntactic (Lin, 1998a), (Almuhareb and Poesio, 2004). Comparative evaluation in (Cimiano and V¨olker, 2005) shows that syntactic features lead to better performance. Feature weights can be calculated either by Machine Learning algorithms (Fleischman and Hovy, 2002) or by statistical measures, like Point Wise Mutual Information or the Jaccard coefficient (Lin, 1998a). A hybrid approach using both pattern-based, term structure, and contextual feature methods is presented in (Cimiano et al., 2005). State-of-the-art approaches may be divided in two classes, according to different use of training data: Unsupervised approaches (see (Cimiano et al., 2</context>
</contexts>
<marker>Almuhareb, Poesio, 2004</marker>
<rawString>A. Almuhareb and M. Poesio. 2004. Attributebased and value-based clustering: An evaluation. In Proceedings of EMNLP 2004, pages 158–165, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Avancini</author>
<author>A Lavelli</author>
<author>B Magnini</author>
<author>F Sebastiani</author>
<author>R Zanoli</author>
</authors>
<title>Expanding Domain-Specific Lexicons by Term Categorization.</title>
<date>2003</date>
<booktitle>In Proceedings of SAC</booktitle>
<pages>793--79</pages>
<contexts>
<context position="1833" citStr="Avancini et al., 2003" startWordPosition="277" endWordPosition="280">acquisition techniques (see, among others, (Buitelaar et al., 2005)). Although there is no a univocally accepted definition for the OP task, a useful approximation has been suggested (Bontcheva and Cunningham, 2003) as Ontology Driven Information Extraction, where, in place of a template to be filled, the goal of the task is the extraction and classification of instances of concepts and relations defined in a Ontology. The task has been approached in a variety of similar perspectives, including term clustering (e.g. (Lin, 1998a) and (Almuhareb and Poesio, 2004)) and term categorization (e.g. (Avancini et al., 2003)). A rather different task is Ontology Learning (OL), where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (see, for instance, (Velardi et al., 2005)). In this paper OP is defined in the following scenario. Given a set of terms T = t1, t2, ..., tn, a document collection D, where terms in T are supposed to appear, and a set of predefined classes C = c1, c2,..., cm denoting concepts in an Ontology, each term ti has to be assigned to the proper class in C. For the purposes of the experiments presented in this paper we</context>
</contexts>
<marker>Avancini, Lavelli, Magnini, Sebastiani, Zanoli, 2003</marker>
<rawString>H. Avancini, A. Lavelli, B. Magnini, F. Sebastiani, and R. Zanoli. 2003. Expanding Domain-Specific Lexicons by Term Categorization. In Proceedings of SAC 2003, pages 793–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bontcheva</author>
<author>H Cunningham</author>
</authors>
<title>The Semantic Web: A New Opportunity and Challenge for HLT.</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop HLT for the Semantic Web and Web Services at ISWC</booktitle>
<contexts>
<context position="1426" citStr="Bontcheva and Cunningham, 2003" startWordPosition="210" endWordPosition="213">class. The model is then used to classify the unknown Named Entities in the test set. The method is weakly supervised, since no annotated corpus is used in the learning process. We achieved promising results, i.e. 65% accuracy, outperforming significantly previous unsupervised approaches. 1 Introduction Automatic Ontology Population (OP) from texts has recently emerged as a new field of application for knowledge acquisition techniques (see, among others, (Buitelaar et al., 2005)). Although there is no a univocally accepted definition for the OP task, a useful approximation has been suggested (Bontcheva and Cunningham, 2003) as Ontology Driven Information Extraction, where, in place of a template to be filled, the goal of the task is the extraction and classification of instances of concepts and relations defined in a Ontology. The task has been approached in a variety of similar perspectives, including term clustering (e.g. (Lin, 1998a) and (Almuhareb and Poesio, 2004)) and term categorization (e.g. (Avancini et al., 2003)). A rather different task is Ontology Learning (OL), where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (see, </context>
</contexts>
<marker>Bontcheva, Cunningham, 2003</marker>
<rawString>K. Bontcheva and H. Cunningham. 2003. The Semantic Web: A New Opportunity and Challenge for HLT. In Proceedings of the Workshop HLT for the Semantic Web and Web Services at ISWC 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
<author>P Cimiano</author>
<author>B Magnini</author>
<author>editors</author>
</authors>
<date>2005</date>
<booktitle>Ontology Learning from Text: Methods, Evaluation and Applications.</booktitle>
<publisher>IOS Press,</publisher>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="1278" citStr="Buitelaar et al., 2005" startWordPosition="188" endWordPosition="191">c model - a set of weighted syntactic features, i.e. words which typically co-occur in certain syntactic positions with the members of that class. The model is then used to classify the unknown Named Entities in the test set. The method is weakly supervised, since no annotated corpus is used in the learning process. We achieved promising results, i.e. 65% accuracy, outperforming significantly previous unsupervised approaches. 1 Introduction Automatic Ontology Population (OP) from texts has recently emerged as a new field of application for knowledge acquisition techniques (see, among others, (Buitelaar et al., 2005)). Although there is no a univocally accepted definition for the OP task, a useful approximation has been suggested (Bontcheva and Cunningham, 2003) as Ontology Driven Information Extraction, where, in place of a template to be filled, the goal of the task is the extraction and classification of instances of concepts and relations defined in a Ontology. The task has been approached in a variety of similar perspectives, including term clustering (e.g. (Lin, 1998a) and (Almuhareb and Poesio, 2004)) and term categorization (e.g. (Avancini et al., 2003)). A rather different task is Ontology Learni</context>
</contexts>
<marker>Buitelaar, Cimiano, Magnini, editors, 2005</marker>
<rawString>P. Buitelaar, P. Cimiano, and B. Magnini, editors. 2005. Ontology Learning from Text: Methods, Evaluation and Applications. IOS Press, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>J V¨olker</author>
</authors>
<title>Towards large-scale, open-domain and ontology-based named entity classification.</title>
<date>2005</date>
<booktitle>In Proceedings of RANLP’05,</booktitle>
<pages>166--172</pages>
<location>Borovets, Bulgaria.</location>
<marker>Cimiano, V¨olker, 2005</marker>
<rawString>P. Cimiano and J. V¨olker. 2005. Towards large-scale, open-domain and ontology-based named entity classification. In Proceedings of RANLP’05, pages 166– 172, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>A Pivk</author>
<author>L S Thieme</author>
<author>S Staab</author>
</authors>
<title>Learning Taxonomic Relations from Heterogeneous Sources of Evidence. In Ontology Learning from Text: Methods, Evaluation and Applications.</title>
<date>2005</date>
<publisher>IOS Press.</publisher>
<contexts>
<context position="7348" citStr="Cimiano et al., 2005" startWordPosition="1182" endWordPosition="1185"> context in which a semantic class tends to appear. Contextual features may be superficial (Fleischman and Hovy, 2002) or syntactic (Lin, 1998a), (Almuhareb and Poesio, 2004). Comparative evaluation in (Cimiano and V¨olker, 2005) shows that syntactic features lead to better performance. Feature weights can be calculated either by Machine Learning algorithms (Fleischman and Hovy, 2002) or by statistical measures, like Point Wise Mutual Information or the Jaccard coefficient (Lin, 1998a). A hybrid approach using both pattern-based, term structure, and contextual feature methods is presented in (Cimiano et al., 2005). State-of-the-art approaches may be divided in two classes, according to different use of training data: Unsupervised approaches (see (Cimiano et al., 2005) for details) and supervised approaches which use manually tagged training data, e.g. (Fleischman and Hovy, 2002). While stateof-the-art unsupervised methods have low performance, supervised approaches reach higher accuracy, but require the manual construction of a training set, which impedes them from large scale applications. 3 Weakly supervised approaches for Ontology Population In this Section we present three Ontology Population appro</context>
</contexts>
<marker>Cimiano, Pivk, Thieme, Staab, 2005</marker>
<rawString>P. Cimiano, A. Pivk, L.S. Thieme, and S. Staab. 2005. Learning Taxonomic Relations from Heterogeneous Sources of Evidence. In Ontology Learning from Text: Methods, Evaluation and Applications. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>E Hovy</author>
</authors>
<title>Fine Grained Classification of Named Entities.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING 2002,</booktitle>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="6845" citStr="Fleischman and Hovy, 2002" startWordPosition="1108" endWordPosition="1111">lation between two words, e.g. “the ant is an insect” or “ants and other insects”. However, such phrases do not appear frequently in a text corpus. For this reason, some approaches use the Web (Schlobach et al., 2004). (Velardi et al., 2005) experimented several head-matching heuristics according to which if a term1 is in the head of term2, then there is an “is-a” relation between them: For example “Christmas tree” is a kind of “tree”. Context feature approaches use a corpus to extract features from the context in which a semantic class tends to appear. Contextual features may be superficial (Fleischman and Hovy, 2002) or syntactic (Lin, 1998a), (Almuhareb and Poesio, 2004). Comparative evaluation in (Cimiano and V¨olker, 2005) shows that syntactic features lead to better performance. Feature weights can be calculated either by Machine Learning algorithms (Fleischman and Hovy, 2002) or by statistical measures, like Point Wise Mutual Information or the Jaccard coefficient (Lin, 1998a). A hybrid approach using both pattern-based, term structure, and contextual feature methods is presented in (Cimiano et al., 2005). State-of-the-art approaches may be divided in two classes, according to different use of traini</context>
</contexts>
<marker>Fleischman, Hovy, 2002</marker>
<rawString>M. Fleischman and E. Hovy. 2002. Fine Grained Classification of Named Entities. In Proceedings of COLING 2002, Taipei, Taiwan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
</authors>
<title>Automated Subcategorization of Named Entities.</title>
<date>2001</date>
<booktitle>In 39th Annual Meeting of the ACL, Student Research Workshop,</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="27591" citStr="Fleischman, 2001" startWordPosition="4601" endWordPosition="4602">ions and future work In this paper we presented a new weakly supervised approach for Ontology Population, called Class-Example, and confronted it with two other methods. Experimental results show that the Class-Example approach has best performance. In particular, it reached 65% of accuracy, outperforming in our experimental framework the stateof-the-art Class-Word method by 42%. Moreover, for location names the method reached accuracy of 78%. Although the experiments are not comparable, we would like to state that some supervised approaches for fine-grained Named Entity classification, e.g. (Fleischman, 2001), have similar accuracy. On the other hand, the presented weakly supervised Class-Example approach requires as a training data only a list of terms for each class under consideration. Training examples can be automatically acquired from existing ontologies or other sources, since the approach imposes virtually no restrictions on them. This makes our weakly supervised methodology applicable on larger scale than supervised approaches, still having significantly better performance than the unsupervised ones. In our experimental framework we used syntactic features extracted from dependency parse </context>
</contexts>
<marker>Fleischman, 2001</marker>
<rawString>M. Fleischman. 2001. Automated Subcategorization of Named Entities. In 39th Annual Meeting of the ACL, Student Research Workshop, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automated Discovery of WordNet Relations. In WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4386" citStr="Hearst, 1998" startWordPosition="716" endWordPosition="717">which consider pair wise similarity between terms ((Cimiano and V¨olker, 2005); (Lin, 1998a)), the Class-Example method considers the similarity between a term tz and a set of training examples which represent a certain class. This results in a great number of class features and opens the possibility to exploit more statistical data, such as the frequency of appearance of a class feature in different training terms. In order to show the effectiveness of the ClassExample approach, it has been compared against two different approaches: (i) a Class-Pattern unsupervised approach, in the style of (Hearst, 1998); (ii) an unsupervised approach that considers the word of the class as a pivot word for acquiring relevant contexts for the class (we refer to this method as Class−Word). Results of the comparison show that the Class-Example method outperforms significantly the other two methods, making it appealing even considering the need of supervision. Although the Class-Example method we propose is applicable in general, in this paper we show its usefulness when applied to terms denoting Named Entities. The motivation behind this choice is the practical value of Named Entity classifications, as, for ins</context>
<context position="5965" citStr="Hearst, 1998" startWordPosition="959" endWordPosition="960"> in Ontology Population. Section 3 presents the three approaches to the task we have compared. Section 4 introduces Syntactic Network, a formalism used for the representation of syntactic information and exploited in both the Class-Word and the ClassExample approaches. Section 5 reports on the experimental settings, results obtained, and discusses the three approaches. Section 6 concludes the paper and suggests directions for future work. 2 Related Work There are two main paradigms distinguishing Ontology Population approaches. In the first one Ontology Population is performed using patterns (Hearst, 1998) or relying on the structure of terms (Velardi et al., 2005). In the second paradigm the task is addressed using contextual features (Cimiano and V¨olker, 2005). Pattern-based approaches search for phrases which explicitly show that there is an “is-a” relation between two words, e.g. “the ant is an insect” or “ants and other insects”. However, such phrases do not appear frequently in a text corpus. For this reason, some approaches use the Web (Schlobach et al., 2004). (Velardi et al., 2005) experimented several head-matching heuristics according to which if a term1 is in the head of term2, the</context>
<context position="8044" citStr="Hearst, 1998" startWordPosition="1287" endWordPosition="1288"> use of training data: Unsupervised approaches (see (Cimiano et al., 2005) for details) and supervised approaches which use manually tagged training data, e.g. (Fleischman and Hovy, 2002). While stateof-the-art unsupervised methods have low performance, supervised approaches reach higher accuracy, but require the manual construction of a training set, which impedes them from large scale applications. 3 Weakly supervised approaches for Ontology Population In this Section we present three Ontology Population approaches. Two of them are unsupervised: 18 (i) a pattern-based approach described in (Hearst, 1998), which we refer to as Class-Pattern and (ii) a feature similarity method reported in (Cimiano and V¨olker, 2005) to which we will refer as ClassWord. Finally, we describe a new weakly supervised approach for ontology population which accepts as a training data lists of instances for each class under consideration. This method we call Class-Example. 3.1 Class-Pattern approach This approach was described first in (Hearst, 1998). The main idea is that if a term t belongs to a class c, then in a text corpus we may expect the occurrence of phrases like such c as t,.... In our experiments for ontol</context>
</contexts>
<marker>Hearst, 1998</marker>
<rawString>M. Hearst. 1998. Automated Discovery of WordNet Relations. In WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL98,</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="1743" citStr="Lin, 1998" startWordPosition="265" endWordPosition="266"> from texts has recently emerged as a new field of application for knowledge acquisition techniques (see, among others, (Buitelaar et al., 2005)). Although there is no a univocally accepted definition for the OP task, a useful approximation has been suggested (Bontcheva and Cunningham, 2003) as Ontology Driven Information Extraction, where, in place of a template to be filled, the goal of the task is the extraction and classification of instances of concepts and relations defined in a Ontology. The task has been approached in a variety of similar perspectives, including term clustering (e.g. (Lin, 1998a) and (Almuhareb and Poesio, 2004)) and term categorization (e.g. (Avancini et al., 2003)). A rather different task is Ontology Learning (OL), where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (see, for instance, (Velardi et al., 2005)). In this paper OP is defined in the following scenario. Given a set of terms T = t1, t2, ..., tn, a document collection D, where terms in T are supposed to appear, and a set of predefined classes C = c1, c2,..., cm denoting concepts in an Ontology, each term ti has to be assigne</context>
<context position="3863" citStr="Lin, 1998" startWordPosition="631" endWordPosition="632">set includes Person, Location, Organization, GPE, and a Miscellanea class for all other kind of entities). In addition, by definition, the set of classes in C changes as a new ontology is considered, making the creation of annotated data almost impossible practically. 17 According with the demand for weakly supervised approaches to OP, we propose a method, called Class − Example, which learns a classification model from a set of classified terms, exploiting lexico-syntactic features. Unlike most of the approaches which consider pair wise similarity between terms ((Cimiano and V¨olker, 2005); (Lin, 1998a)), the Class-Example method considers the similarity between a term tz and a set of training examples which represent a certain class. This results in a great number of class features and opens the possibility to exploit more statistical data, such as the frequency of appearance of a class feature in different training terms. In order to show the effectiveness of the ClassExample approach, it has been compared against two different approaches: (i) a Class-Pattern unsupervised approach, in the style of (Hearst, 1998); (ii) an unsupervised approach that considers the word of the class as a piv</context>
<context position="6869" citStr="Lin, 1998" startWordPosition="1114" endWordPosition="1115">s an insect” or “ants and other insects”. However, such phrases do not appear frequently in a text corpus. For this reason, some approaches use the Web (Schlobach et al., 2004). (Velardi et al., 2005) experimented several head-matching heuristics according to which if a term1 is in the head of term2, then there is an “is-a” relation between them: For example “Christmas tree” is a kind of “tree”. Context feature approaches use a corpus to extract features from the context in which a semantic class tends to appear. Contextual features may be superficial (Fleischman and Hovy, 2002) or syntactic (Lin, 1998a), (Almuhareb and Poesio, 2004). Comparative evaluation in (Cimiano and V¨olker, 2005) shows that syntactic features lead to better performance. Feature weights can be calculated either by Machine Learning algorithms (Fleischman and Hovy, 2002) or by statistical measures, like Point Wise Mutual Information or the Jaccard coefficient (Lin, 1998a). A hybrid approach using both pattern-based, term structure, and contextual feature methods is presented in (Cimiano et al., 2005). State-of-the-art approaches may be divided in two classes, according to different use of training data: Unsupervised ap</context>
<context position="11311" citStr="Lin, 1998" startWordPosition="1846" endWordPosition="1847">from Corpus of the term t (e.g. “Etna”) or the concept word c (e.g. “mountain”). Cimiano and V¨olker evaluate different context features and prove that syntactic features work best. Therefore, in our experimental settings we considered only such features extracted from a corpus parsed with a dependency parser. Unlike the original approach which relies on pseudo-syntactic features, we used features extracted from dependency parse trees. Moreover, we used virtually all the words connected syntactically to a term, not only the modifiers. A syntactic feature is a pair: (word, syntactic relation) (Lin, 1998a). We use two feature types: First order features, which are directly connected to the training or test examples in the dependency parse trees of Corpus; second order features, which are connected to the training or test instances indirectly by skipping one word (the verb) in the dependency tree. As an example, let’s consider two sentences: “Edison invented the phonograph” and “Edison created the phonograph”. If “Edison” is a name to be classified, then two first order features of this name exist - (“invent”, subject-of) and (“create”, subject-of). One second order feature can be extracted - </context>
<context position="15270" citStr="Lin, 1998" startWordPosition="2515" endWordPosition="2516">re vector vc for a class c. We defined the similarity function as the dot product of the two feature vectors: sim(vt, vc) = vc.vt. Vectors vt are binary (i.e. the feature value is 1 if the feature is present and, 0-otherwise), while the features in the syntactic model vectors vc receive weights according to the approach described in this Section. 4 Representing Syntactic Information Since both the Class-Word and the Class-Example methods work with syntactic features, the main source of information is a syntactically parsed corpus. We parsed about half a gigabyte of a news corpus with MiniPar (Lin, 1998b). It is a statistically based dependency parser which is reported to reach 89% precision and 82% recall on press reportage texts. MiniPar generates syntactic dependency structures - directed labeled graphs whose syntactic features. The α parameter reflects the linguistic intuition that nouns are more informative than verbs and adjectives which in most cases represent generic predicates. The values of α were automatically learned from the training data. 2. We normalize the feature weights, since we observed that they vary a lot between different classes: for each class c we find the feature w</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998a. Automatic Retrieval and Clustering of Similar Words. In Proceedings of COLING-ACL98, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based Evaluation of MiniPar.</title>
<date>1998</date>
<booktitle>In Proceedings of Workshop on the Evaluation of Parsing Systems,</booktitle>
<location>Granada,</location>
<contexts>
<context position="1743" citStr="Lin, 1998" startWordPosition="265" endWordPosition="266"> from texts has recently emerged as a new field of application for knowledge acquisition techniques (see, among others, (Buitelaar et al., 2005)). Although there is no a univocally accepted definition for the OP task, a useful approximation has been suggested (Bontcheva and Cunningham, 2003) as Ontology Driven Information Extraction, where, in place of a template to be filled, the goal of the task is the extraction and classification of instances of concepts and relations defined in a Ontology. The task has been approached in a variety of similar perspectives, including term clustering (e.g. (Lin, 1998a) and (Almuhareb and Poesio, 2004)) and term categorization (e.g. (Avancini et al., 2003)). A rather different task is Ontology Learning (OL), where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (see, for instance, (Velardi et al., 2005)). In this paper OP is defined in the following scenario. Given a set of terms T = t1, t2, ..., tn, a document collection D, where terms in T are supposed to appear, and a set of predefined classes C = c1, c2,..., cm denoting concepts in an Ontology, each term ti has to be assigne</context>
<context position="3863" citStr="Lin, 1998" startWordPosition="631" endWordPosition="632">set includes Person, Location, Organization, GPE, and a Miscellanea class for all other kind of entities). In addition, by definition, the set of classes in C changes as a new ontology is considered, making the creation of annotated data almost impossible practically. 17 According with the demand for weakly supervised approaches to OP, we propose a method, called Class − Example, which learns a classification model from a set of classified terms, exploiting lexico-syntactic features. Unlike most of the approaches which consider pair wise similarity between terms ((Cimiano and V¨olker, 2005); (Lin, 1998a)), the Class-Example method considers the similarity between a term tz and a set of training examples which represent a certain class. This results in a great number of class features and opens the possibility to exploit more statistical data, such as the frequency of appearance of a class feature in different training terms. In order to show the effectiveness of the ClassExample approach, it has been compared against two different approaches: (i) a Class-Pattern unsupervised approach, in the style of (Hearst, 1998); (ii) an unsupervised approach that considers the word of the class as a piv</context>
<context position="6869" citStr="Lin, 1998" startWordPosition="1114" endWordPosition="1115">s an insect” or “ants and other insects”. However, such phrases do not appear frequently in a text corpus. For this reason, some approaches use the Web (Schlobach et al., 2004). (Velardi et al., 2005) experimented several head-matching heuristics according to which if a term1 is in the head of term2, then there is an “is-a” relation between them: For example “Christmas tree” is a kind of “tree”. Context feature approaches use a corpus to extract features from the context in which a semantic class tends to appear. Contextual features may be superficial (Fleischman and Hovy, 2002) or syntactic (Lin, 1998a), (Almuhareb and Poesio, 2004). Comparative evaluation in (Cimiano and V¨olker, 2005) shows that syntactic features lead to better performance. Feature weights can be calculated either by Machine Learning algorithms (Fleischman and Hovy, 2002) or by statistical measures, like Point Wise Mutual Information or the Jaccard coefficient (Lin, 1998a). A hybrid approach using both pattern-based, term structure, and contextual feature methods is presented in (Cimiano et al., 2005). State-of-the-art approaches may be divided in two classes, according to different use of training data: Unsupervised ap</context>
<context position="11311" citStr="Lin, 1998" startWordPosition="1846" endWordPosition="1847">from Corpus of the term t (e.g. “Etna”) or the concept word c (e.g. “mountain”). Cimiano and V¨olker evaluate different context features and prove that syntactic features work best. Therefore, in our experimental settings we considered only such features extracted from a corpus parsed with a dependency parser. Unlike the original approach which relies on pseudo-syntactic features, we used features extracted from dependency parse trees. Moreover, we used virtually all the words connected syntactically to a term, not only the modifiers. A syntactic feature is a pair: (word, syntactic relation) (Lin, 1998a). We use two feature types: First order features, which are directly connected to the training or test examples in the dependency parse trees of Corpus; second order features, which are connected to the training or test instances indirectly by skipping one word (the verb) in the dependency tree. As an example, let’s consider two sentences: “Edison invented the phonograph” and “Edison created the phonograph”. If “Edison” is a name to be classified, then two first order features of this name exist - (“invent”, subject-of) and (“create”, subject-of). One second order feature can be extracted - </context>
<context position="15270" citStr="Lin, 1998" startWordPosition="2515" endWordPosition="2516">re vector vc for a class c. We defined the similarity function as the dot product of the two feature vectors: sim(vt, vc) = vc.vt. Vectors vt are binary (i.e. the feature value is 1 if the feature is present and, 0-otherwise), while the features in the syntactic model vectors vc receive weights according to the approach described in this Section. 4 Representing Syntactic Information Since both the Class-Word and the Class-Example methods work with syntactic features, the main source of information is a syntactically parsed corpus. We parsed about half a gigabyte of a news corpus with MiniPar (Lin, 1998b). It is a statistically based dependency parser which is reported to reach 89% precision and 82% recall on press reportage texts. MiniPar generates syntactic dependency structures - directed labeled graphs whose syntactic features. The α parameter reflects the linguistic intuition that nouns are more informative than verbs and adjectives which in most cases represent generic predicates. The values of α were automatically learned from the training data. 2. We normalize the feature weights, since we observed that they vary a lot between different classes: for each class c we find the feature w</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998b. Dependency-based Evaluation of MiniPar. In Proceedings of Workshop on the Evaluation of Parsing Systems, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schlobach</author>
<author>M Olsthoorn</author>
<author>M de Rijke</author>
</authors>
<title>Type Checking in Open-Domain Question Answering.</title>
<date>2004</date>
<booktitle>In Proceedings of ECAI</booktitle>
<marker>Schlobach, Olsthoorn, de Rijke, 2004</marker>
<rawString>S. Schlobach, M. Olsthoorn, and M. de Rijke. 2004. Type Checking in Open-Domain Question Answering. In Proceedings of ECAI 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>H Tanev</author>
<author>I Dagan</author>
<author>B Coppola</author>
</authors>
<title>Scaling Web-based Acquisition of Entailment Relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="19642" citStr="Szpektor et al., 2004" startWordPosition="3267" endWordPosition="3271">ords) would be useful, but still does not resolve the problem, since in many cases we search for more complex structures than a relation between two words: for example, when we have to find which words are syntactically related to a Named Entity composed by two words, we have to search for syntactic structures which consists of three vertices and two edges. In order to trace efficiently more complex structures in the corpus, we put forward a model for representation of a set of labeled graphs, called Syntactic Network (SyntNet for short). The model is inspired by a model presented earlier in (Szpektor et al., 2004), however our model allows more efficient construction of the representation. The scope of SyntNet is to represent a set of labeled graphs through one aggregate structure in which the isomorphic sub-structures overlap. When SyntNet represents a syntactically parsed text corpus, its vertices are labeled with words from the text while edges represent syntactic relations from the corpus and are labeled accordingly. An example is shown in Figure 2, where two syntactic graphs, and g1 “John Mary” “John Jane”; g1 g2, are merged into 21 “John loves Mary” in the corpus, spanning through vertices 1, 2, </context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004. Scaling Web-based Acquisition of Entailment Relations. In Proceedings of EMNLP 2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Velardi</author>
<author>A Cuchiarelli R Navigli</author>
<author>F Neri</author>
</authors>
<title>Evaluation of Ontolearn, a Methodology for Automatic Population of Domain Ontologies.</title>
<date>2005</date>
<booktitle>Ontology Learning from Text: Methods, Evaluation and Applications.</booktitle>
<editor>In P. Buitelaar, P. Cimiano, and B. Magnini, editors,</editor>
<publisher>IOS Press.</publisher>
<contexts>
<context position="2062" citStr="Velardi et al., 2005" startWordPosition="314" endWordPosition="317">iven Information Extraction, where, in place of a template to be filled, the goal of the task is the extraction and classification of instances of concepts and relations defined in a Ontology. The task has been approached in a variety of similar perspectives, including term clustering (e.g. (Lin, 1998a) and (Almuhareb and Poesio, 2004)) and term categorization (e.g. (Avancini et al., 2003)). A rather different task is Ontology Learning (OL), where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (see, for instance, (Velardi et al., 2005)). In this paper OP is defined in the following scenario. Given a set of terms T = t1, t2, ..., tn, a document collection D, where terms in T are supposed to appear, and a set of predefined classes C = c1, c2,..., cm denoting concepts in an Ontology, each term ti has to be assigned to the proper class in C. For the purposes of the experiments presented in this paper we assume that (i) classes in C are mutually disjoint and (ii) each term is assigned to just one class. As we have defined it, OP shows a strong similarity with Named Entity Recognition and Classification (NERC). However, a major d</context>
<context position="6025" citStr="Velardi et al., 2005" startWordPosition="968" endWordPosition="971">e approaches to the task we have compared. Section 4 introduces Syntactic Network, a formalism used for the representation of syntactic information and exploited in both the Class-Word and the ClassExample approaches. Section 5 reports on the experimental settings, results obtained, and discusses the three approaches. Section 6 concludes the paper and suggests directions for future work. 2 Related Work There are two main paradigms distinguishing Ontology Population approaches. In the first one Ontology Population is performed using patterns (Hearst, 1998) or relying on the structure of terms (Velardi et al., 2005). In the second paradigm the task is addressed using contextual features (Cimiano and V¨olker, 2005). Pattern-based approaches search for phrases which explicitly show that there is an “is-a” relation between two words, e.g. “the ant is an insect” or “ants and other insects”. However, such phrases do not appear frequently in a text corpus. For this reason, some approaches use the Web (Schlobach et al., 2004). (Velardi et al., 2005) experimented several head-matching heuristics according to which if a term1 is in the head of term2, then there is an “is-a” relation between them: For example “Chr</context>
</contexts>
<marker>Velardi, Navigli, Neri, 2005</marker>
<rawString>P. Velardi, R.Navigli, A. Cuchiarelli, and F.Neri. 2005. Evaluation of Ontolearn, a Methodology for Automatic Population of Domain Ontologies. In P. Buitelaar, P. Cimiano, and B. Magnini, editors, Ontology Learning from Text: Methods, Evaluation and Applications. IOS Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>