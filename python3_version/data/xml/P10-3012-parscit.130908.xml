<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001434">
<title confidence="0.998077">
A Framework for Figurative Language Detection Based on Sense
Differentiation
</title>
<author confidence="0.997065">
Daria Bogdanova
</author>
<affiliation confidence="0.999615">
University of Saint Petersburg
</affiliation>
<address confidence="0.547239">
Saint Petersburg
</address>
<email confidence="0.994854">
dasha.bogdanova@gmail.com
</email>
<sectionHeader confidence="0.997343" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999687928571429">
Various text mining algorithms require the
process of feature selection. High-level se-
mantically rich features, such as figurative
language uses, speech errors etc., are very
promising for such problems as e.g. writ-
ing style detection, but automatic extrac-
tion of such features is a big challenge.
In this paper, we propose a framework for
figurative language use detection. This
framework is based on the idea of sense
differentiation. We describe two algo-
rithms illustrating the mentioned idea. We
show then how these algorithms work by
applying them to Russian language data.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999811463414634">
Various text mining algorithms require the pro-
cess of feature selection. For example, author-
ship attribution algorithms need to determine fea-
tures to quantify the writing style. Previous work
on authorship attribution among computer scien-
tists is mostly based on low-level features such as
word frequencies, sentence length counts, n-grams
etc. A significant advantage of such features is
that they can be easily extracted from any corpus.
But the study by Batov and Sorokin (1975) shows
that such features do not always provide accurate
measures for authorship attribution. The linguistic
approach to the problem involves such high-level
characteristics as the use of figurative language,
irony, sound devices and so on. Such character-
istics are very promising for the mentioned above
tasks, but the extraction of these features is ex-
tremely hard to automate. As a result, very few
attempts have been made to exploit high-level fea-
tures for stylometric purposes (Stamatatos, 2009).
Therefore, our long-term objective is the extrac-
tion of high-level semantically rich features.
Since the mentioned topic is very broad, we fo-
cus our attention only on some particular prob-
lems and approaches. In this paper, we examine
one of such problems, the problem of automatic
figurative language use detection. We propose a
framework for figurative language detection based
on the idea of sense differentiation. Then, we de-
scribe two algorithms illustrating the mentioned
idea. One of them is intended to decide whether
a usage is literal by comparing the texts related to
the target expression and the set of texts related
to the context itself. The other is aimed at group-
ing instances into literal and non-literal uses and is
based on DBSCAN clustering (Ester et al, 1996).
We illustrate then how these algorithms work by
applying them to Russian language data. Finally,
we propose some ideas on modifications which
can significantly improve the accuracy of the al-
gorithms.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999956772727273">
Sporleder and Li (April 2009) proposed an unsu-
pervised method for recognition of literal and non-
literal use of idiomatic expressions. Given an id-
iom the method detects the presence or absence of
cohesive links between the words the idiom con-
sists of and the surrounding text. When such links
exist, the occurence is considered as a literal us-
age and as a non-literal when there are no such
links. For most idioms the experiments showed an
accuracy above 50% (it varies between 11% and
98% for different idioms). The authors then pro-
posed an improvement of the algorithm (Li and
Sporleder, August 2009) by adding the Support
Vector Machine classifier as a second stage. They
use the mentioned above unsupervised algorithm
to label the training data for the supervised classi-
fier. The average accuracy of the improved algo-
rithm is about 90%. Our approach is also based
on the idea of the relatedness between the expres-
sion and the surrounding context. Unlike the men-
tioned study, we do not focus our attention only
on idioms. So far we have mostly dealt with ex-
</bodyText>
<page confidence="0.994089">
67
</page>
<note confidence="0.6035055">
Proceedings of the ACL 2010 Student Research Workshop, pages 67–72,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999955825">
pressions, which are not necessarily idiomatic by
themselves, but become metaphors in a particular
context (e.g. ”she is the sunshine”, ”life is a jour-
ney”) and expressions that are invented by an au-
thor (e.g. ”my heart’s in the Highlands”). More-
over, the improved algorithm (Li and Sporleder,
August 2009) is supervised, and our approach is
unsupervised.
The study by Katz and Giesbrecht (2006) is also
supervised, unlike ours. It also considers multi-
word expressions that have idiomatic meanings.
They propose an algorithm, which computes the
vectors for literal and non-literal usages and then
use the nearest neighbor classifier to label an un-
seen occurence of the given idiom.
The approach proposed by Birke and
Sarkar (2006) is nearly unsupervised. They
constructed two seed sets: one consists of literal
usages of different expressions and the other
consists of non-literal usages. They calculate
the distance between an occurence in question
and these two sets and assign to the occurence
the label of the closest set. This work, as well
as ours, refers to the ideas from Word Sense
Disambiguation area. Unlike our approach, the
authors focus their attention only on the detection
of figuratevely used verbs and, whereas we only
refer to the concepts and ideas of WSD, they adapt
a particular existing one-word disambiguation
method.
As we have already said, we deal with dif-
ferent types of figurative language (metaphors,
metonymies etc.). However, there are some works
aimed at extracting particular types of figura-
tive language. For example, Nissim and Mark-
ert (2003) proposed a machine learning algorithm
for metonymy resolution. They state the problem
of metonymy resolution as a classification task be-
tween literal use of a word and a number of prede-
fined metonymy types.
</bodyText>
<sectionHeader confidence="0.987991" genericHeader="method">
3 Sense Differentiation
</sectionHeader>
<bodyText confidence="0.998968176470588">
We could treat a figurative meaning of a word as
an additional, not common meaning of this word.
Actually, some metaphors are quite common (e.g.
eye of a needle, leg of a table, etc.) and are called
catachretic metaphors. They appear in a language
to remedy the gap in vocabulary (Black, 1954).
These metaphors do not indicate an author’s writ-
ing style: an author uses such metaphor for an ob-
ject because the language has no other name for
that object. Therefore the algorithms we are de-
veloping do not work with this type of metaphors.
Our approach to figurative language detection
is based on the following idea: the fact that the
sense of a word significantly differs from the sense
of the surrounding text usually indicates that the
word is used figuratively. Two questions arise im-
mediately:
</bodyText>
<listItem confidence="0.99512575">
1. How do we represent the sense of both the
word and the surrounding context?
2. How do we find out that these senses differ
significantly?
</listItem>
<bodyText confidence="0.999980675675676">
To answer the first question, we refer to the
ideas popular in the Word Sense Disambiguation
community: sense is a group of contextually simi-
lar occurrences of a word (Sch¨utze, 1996). Hence,
we represent the senses of both a word and its con-
text as sets of documents related to the word and
the context respectively. These sets can be ob-
tained e.g. by searching Wikipedia, Google or an-
other web search engine. For a word the query can
be the word itself. As for a text, this query can be
formulated as the whole text or as a set of some
words contained in this text. It seems to us that
querying the lexical chains (Halliday and Hasan,
1976) extracted from the text should provide bet-
ter results than querying the whole text.
As soon as we have a sense representation for
such objects as a word and a text, we should find
a way to measure the difference between these
sense representations and find out what difference
is strong enough for the considered occurence to
be classified as a non-literal usage. One way to
do this is representing sets of documents as sets
of vectors and measuring the distance between the
centers of the obtained vector sets. Another way
is to apply clustering techniques to the sets and to
measure the accuracy of the produced clustering.
The higher the accuracy is, the more different the
sets are.
Besides, this can be done by calculating text-
to-text semantic similarity using for example the
measure proposed by Mihalcea et al (2006). This
is rather difficult in case of the Russian language
because at the moment there is no WordNet-like
taxonomies for Russian.
In the next section, we propose two algorithms
based on the mentioned above idea. We state the
algorithms generally and try to find out experi-
</bodyText>
<page confidence="0.996703">
68
</page>
<bodyText confidence="0.999197">
mentally what combination of the described tech-
niques provides the best results.
</bodyText>
<sectionHeader confidence="0.86762" genericHeader="method">
4 Finding the Distance to the Typical
</sectionHeader>
<subsectionHeader confidence="0.602555">
Context Set
</subsectionHeader>
<bodyText confidence="0.999838941176471">
The algorithm is intended to determine whether a
word (or an expression) in a given context is used
literaly or not.
As it was mentioned above, we decided to rep-
resent senses of both an expression and a context
as sets of documents. Our hypothesis is that these
document sets differ significantly if and only if
an expression is used figuratevely. Thus, the al-
gorithm decides whether the occurence is literal
by comparing two sets of documents: the typical
context set, which represents a sense of the expres-
sion, and the related context set, which represents
a sense of the context. A naive way to construct
the typical context set is searching some searching
engine (e.g. Google) for the expression. Given a
context with a target expression, the related con-
text set can be constructed as follows:
</bodyText>
<listItem confidence="0.9989676">
1. Remove the target expression from the con-
text;
2. Extract the longest lexical chains from the re-
sulting context;
3. For every chain put to the set the first N arti-
</listItem>
<bodyText confidence="0.989618206896552">
cles retrieved by searching a searching engine
for the chain;
After constructing the sets the algorithm should
estimate the similarity between these two sets.
This, for example, can be done by applying any
clustering algorithm to the data and measuring the
accuracy. Evidently, the higher the accuracy of the
obtained clustering is, the more separated the sets
are. It means that, when the usage is literal, the
accuracy should be lower because we try to make
two clusters out of data that should appear as the
only cluster.
We hypothesize that in case of non-literal us-
ages these two sets should be significantly sepa-
rated.
Our experiments include two stages. During the
first one we test our idea and estimate the param-
eters of the algorithms. During the second stage
we test the more precise algorithm obtained dur-
ing the first stage.
For the first stage, we found literal and non-
literal occurences of the following Russian words
and expressions:
вьюга (snowstorm), дыхание (breath),
кинжальный (dagger), плясать (dance),
стебель гибкий (flexible (flower) stalk),
утонуть (be drowned), хрустальный (crystal),
шотландская волынка (bagpipes), мед
(honey), лекарство (medicine).
For every expression, the typical context set con-
sists of the first 10 articles retrieved by searching
Google for the expression. In order to construct
the second set we removed the target expression
from the context and manually extracted lexical
chains from the texts, although, the process of lex-
ical chains extraction can be done automatically.
However the algorithms on lexical chains extrac-
tion usually use WordNet to calculate the related-
ness, but as it was already mentioned WordNet
for the Russian language does not exist yet. An-
other way to calculate semantic relatedness is us-
ing Wikipedia (Mihalcea, 2007; Turdakov and Ve-
likhov, 2008), but it takes much effort. The sec-
ond set for each occurence consists of the first 10
articles retrieved by searching Google for the ex-
tracted chains. Then we applied k-means cluster-
ing algorithm (k = 2) to these sets. To evaluate the
clustering we used measures from the clustering
literature. We denote our sets by G = g1, g2 and
the clusters obtained by k-means as C = c1, c2.
We define a mapping f from the elements of G to
the elements of C, such that each set gi is mapped
to a cluster cj = f(gi) that has the highest per-
centage of common elements with gi. Precision
and recall for a cluster gi, i = 1, 2 are defined as
follows:
Precision, Pr, and recall, Re, of the clustering
are defined as the weighted averages of the preci-
</bodyText>
<equation confidence="0.805759666666667">
1
1
2(Pr1 + Pr2) and Re = 2(Re1 + Re2)
</equation>
<bodyText confidence="0.8422565">
F1-measure is defined as the harmonic mean of
precision and recall, i.e.,
</bodyText>
<equation confidence="0.807278">
2xPrxRe
.
Pr + Re
</equation>
<bodyText confidence="0.9445826">
Table 1 shows the results of the clustering. For
9 expressions out of 10, the clustering accuracy
is higher in case of a metaphorical usage than in
case of a literal one. Moreover, for 9 out of 10
and Rei =  |f (gi) n gi |
</bodyText>
<equation confidence="0.9376382">
 |f (gi)   ||gi |
Pri =  |f(gi) n gi |
sion and recall values over the sets:
Pr =
F1 =
</equation>
<page confidence="0.996496">
69
</page>
<table confidence="0.999911071428571">
Figurative usage Literal usage
Pr Re F Pr Re F
вьюга 0,85 0,85 0,85 0,50 0,50 0,50
дыхание 0,83 0,75 0,79 0,65 0,60 0,63
кинжальный 0,85 0,85 0,85 0,70 0,65 0,67
плясать 0,95 0,95 0,95 0,66 0,65 0,66
стебель 0,85 0,85 0,85 0,88 0,85 0,86
гибкий
утонул 0,85 0,85 0,85 0,81 0,70 0,75
хрустальный 0,95 0,95 0,95 0,83 0,75 0,78
шотландская 0,88 0,85 0,86 0,70 0,70 0,70
волынка
мед 0,90 0,90 0,90 0,88 0,85 0,87
лекарство 0,90 0,90 0,90 0,81 0,70 0,75
</table>
<tableCaption confidence="0.999981">
Table 1: Results provided by k-means clustering
</tableCaption>
<bodyText confidence="0.999590615384615">
metaphorical usages, F-measure is 0,85 or higher.
And for 7 out of 10 literal usages, F-measure is
0,75 or less.
The first stage of the experiments illustrates the
idea of sense differentiation. Based on the ob-
tained results, we have concluded, that F-measure
value equal to 0,85 or higher indicates a figurative
usage, and the value equal to 0,75 or less indicates
a literal usage.
At the second stage, we applied the algorithm
to several Russian language expressions used lit-
erally or figuratively. The accuracy of the k-means
clustering is shown in Table 2.
</bodyText>
<table confidence="0.999244692307692">
Figurative usages
живой костер из снега и вина 0,76 0,55 0,64
лев 1,00 1,00 1,00
иней 0,90 0,90 0,90
ключ 0,95 0,93 0,94
лютый зверь 0,88 0,85 0,87
рогатый 0,92 0,90 0,91
терлась о локоть 0,88 0,85 0,86
иглою снежного огня 0,95 0,95 0,95
клавишей стая 0,76 0,55 0,64
горели глаза 0,95 0,95 0,95
цветок 0,80 0,80 0,80
загорелся 0,91 0,90 0,90
Literal usages
ловил рыбу 0,71 0,70 0,70
играл в футбол 0,74 0,70 0,71
детство 0,66 0,65 0,66
кухня 0,88 0,85 0,87
снег 0,95 0,95 0,95
весна 0,50 0,50 0,50
пить кофе 0,85 0,85 0,85
танцы 0,90 0,90 0,90
платье 0,65 0,65 0,65
человек 0,81 0,70 0,75
ветер 0,85 0,85 0,85
дождь 0,91 0,90 0,90
</table>
<tableCaption confidence="0.973303">
Table 2: Testing the algorithm. Accuracy of the
k-means clustering
</tableCaption>
<bodyText confidence="0.967593">
For 75% of metaphorical usages F-measure is
0,85 or more as was expected and for 50% of lit-
eral usages F-measure is 0,75 or less.
</bodyText>
<sectionHeader confidence="0.943617" genericHeader="method">
5 Figurative Language Uses as Outliers
</sectionHeader>
<bodyText confidence="0.999989517241379">
The described above approach is to decide
whether a word in a context is used literally or not.
Unlike the first one, the second approach we pro-
pose, deals with a set of occurences of a word as to
label every occurence as ’literal’ or ’non-literal’.
We formulate this task as a clustering problem and
apply DBSCAN (Ester et al, 1996) clustering al-
gorithm to the data. Miller and Charles (1991) hy-
pothesized that words with similar meanings are
often used in similar contexts. As it was men-
tioned, we can treat a meaning of a metaphoric
usage of an expression as an additional, not com-
mon for the expression. That’s why we expect
metaphorical usages to be ouliers, while clustering
together with common (i.e. literal) usages. Theo-
retically, the algorithm should also distinguish be-
tween all literal senses so that the contexts of the
same meaning appear in the same cluster and the
contexts of different meanings - in different clus-
ters. Therefore, ideally, the algorithm should solve
word sense discrimination and non-literal usages
detection tasks simultaneously.
For each Russian word shown in Table 3,
we extracted from the Russian National Cor-
pora (http://ruscorpora.ru/) several lit-
eral and non-literal occurences. Some of these
words have more than one meaning in Russian,
e.g. KnFo-i can be translated as a key or water
spring and the word Koca as a plait, scythe or spit.
</bodyText>
<table confidence="0.999533666666667">
word literal non-literal
6а6оYKа (butterfly, bow-tie) 12 2
иней (frost) 14 2
KJioY (key, spring(water)) 14 2
Kоса (plait, scythe, spit) 21 2
Jев (lion, Bulgarian lev) 17 5
JуK (onion, bow) 17 1
MуKа (flour, pain) 21 2
ribiJь (dust) 14 4
</table>
<tableCaption confidence="0.999896">
Table 3: Data used in the first experiment
</tableCaption>
<bodyText confidence="0.9998227">
All the documents are stemmed and all stop-
words are removed with the SnowBall Stem-
mer(http://snowball.tartarus.org/)
for the Russian language.
As it was mentioned above, this algorithm is
aimed at providing word sense discrimination and
non-literal usages detection simultaneously. So
far we have paid attention only to the non-literal
usages detection aspects. DBSCAN algorithm is
a density-based clustering algorithm designed to
</bodyText>
<page confidence="0.995116">
70
</page>
<bodyText confidence="0.992236352941176">
discover clusters of arbitrary shape. This algo-
rithm requires two parameters: ε (eps) and the
minimum number of points in a cluster (minPts).
We set minPts to 3 and run the algorithm for
different eps between 1.45 and 1.55.
As was mentioned, so far we have considered
only figurative language detection issues: The al-
gorithm marks an instance as a figurative usage iff
the instance is labeled as an outlier. Thus, we mea-
sure the accuracy of the algorithm as follows:
|precision =
figurative uses  |n  |outliers
,
 |outliers |
Table 4 shows ”the best” eps for each word and
the corresponding accuracies of metaphor detec-
tion
</bodyText>
<table confidence="0.999779666666667">
word eps precision recall
6a6oYKa 1.520 0.66 1.00
xaeift 1.520 0.50 0.50
K.nioY 1.500 0.33 1.00
Koca 1.510 0.40 1.00
.nes 1.490 1.00 0.83
.nyK 1.505 0.17 1.00
MyKa 1.525 0.67 0.50
rib.nb 1.505 0.50 0.60
</table>
<tableCaption confidence="0.9809915">
Table 4: The best eps parameters and correspond-
ing accuracies of the algorithm
</tableCaption>
<bodyText confidence="0.670173">
recall = |figurative uses  |n  |outliers |
 |figurative uses  |.
Figures 1 and 2 show the dependency between
the eps parameter and the algorithm’s accuracy for
different words.
</bodyText>
<sectionHeader confidence="0.999856" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.960268666666667">
So far we have worked only with tf-idf and word
frequency model for both algorithms. The next
step in our study is utilizing different text repre-
sentation models, e.g. second order context vec-
tors. We are also going to develop an efficient
parameter estimation procedure for the algorithm
based on DBSCAN clustering.
As for the other algorithm, we are going to dis-
tinguish between different figurative language ex-
pressions:
• one word expressions
– monosemous word
– polysemous word
• multiword expressions
We expect the basic algorithm to provide dif-
ferent accuracy in case of different types of ex-
pressions. Dealing with multiword expressions
and monosemous words should be easier than with
polysemous words: i.e., for monosemous word
we expect the second set to appear as one cluster,
whereas this set for a polysemous word is expected
to have the number of clusters equal to the number
of senses it has.
Another direction of the future work is develop-
ing an algorithm for figurative language uses ex-
traction. The algorithm has to find figuratively
used expressions in a text.
</bodyText>
<sectionHeader confidence="0.999284" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998222666666667">
In this paper, we have proposed a framework for
figurative language detection based on the idea of
sense differentiation. We have illustrated how this
</bodyText>
<figureCaption confidence="0.999913">
Figure 1: Dependency between eps and F-measure
Figure 2: Dependency between eps and F-measure
</figureCaption>
<page confidence="0.991482">
71
</page>
<bodyText confidence="0.999985307692308">
idea works by presenting two clustering-based al-
gorithms. The first algorithm deals with only one
context. It is based on comparing two context sets:
one is related to the expression and the other is se-
mantically related to the given context. The sec-
ond algorithm groups the given contexts in literal
and non-literal usages. This algorithm should also
distinguish between different senses of a word, but
we have not yet paid enough attention to this as-
pect. By applying these algorithms to small data
sets we have illustrated how the idea of sense dif-
ferentiation works. These algorithms show quite
good results and are worth further work.
</bodyText>
<sectionHeader confidence="0.999507" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997865">
This work was partially supported by Russian
Foundation for Basic Research RFBR, grant 10-
07-00156.
</bodyText>
<sectionHeader confidence="0.999558" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999528724137931">
Vitaly I. Batov and Yury A. Sorokin. 1975. Text at-
tribution based on objective characteristics. Seriya
yazyka i literatury, 34, 1.
Julia Birke and Anoop Sarkar. 2006. A Clustering Ap-
proach for the Nearly Unsupervised Recognition of
Nonliteral Language. Proceedings of EACL-06
Max Black. 1954. Metaphor. Proceedings of the Aris-
totelian Society, 55, pp. 273-294.
Martin Ester, Hans-Peter Kriegel, J¨org Sander and
Xiaowei Xu. 1996. A density-based algorithm for
discovering clusters in large spatial databases with
noise. Proceedings of the Second International Con-
ference on Knowledge Discovery and Data Mining
(KDD-96), AAAI Press, pp. 226231
Michael Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman, London
Graham Katz and Eugenie Giesbrecht. 2006. Au-
tomatic identification of non-compositional multi-
word expressions using latent semantic analysis.
Proceedings of the ACL/COLING-06 Workshop on
Multiword Expressions: Identifying and Exploiting
Underlying Properties
Linlin Li and Caroline Sporleder. August 2009. Clas-
sifier combination for contextual idiom detection
without labeled data. Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pp. 315-323.
Rada Mihalcea, Courtney Corley and Carlo Strappa-
rava. 2006. Corpus-based and Knowledge-based
Measures of Text Semantic Similarity. Proceedings
of AAAI-06
Rada Mihalcea. 2007. Using Wikipedia for Auto-
matic Word Sense Disambiguation. Proceedings of
the North American Chapter of the Association for
Computational Linguistics (NAACL 2007)
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):128.
Malvina Nissim and Katja Markert. 2003. Syn-
tactic features and word similarity for supervised
metonymy resolution. Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics (ACL-03) (Sapporo, Japan, 2003). 56-
63.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1), pp.
97-123
Caroline Sporleder and Linlin Li. April 2009. Unsu-
pervised recognition of literal and non-literal use of
idiomatic expressions. Proceedings of EACL-09
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for information Science and Technology,
60(3): 538-556.
Denis Turdakov and Pavel Velikhov. 2008. Semantic
Relatedness Metric for Wikipedia Concepts Based
on Link Analysis and its Application to Word Sense
Disambiguation SYRCoDIS 2008
</reference>
<page confidence="0.998709">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951052">
<title confidence="0.999285">A Framework for Figurative Language Detection Based on Sense Differentiation</title>
<author confidence="0.999862">Daria Bogdanova</author>
<affiliation confidence="0.998176">University of Saint Petersburg</affiliation>
<address confidence="0.968357">Saint Petersburg</address>
<email confidence="0.999896">dasha.bogdanova@gmail.com</email>
<abstract confidence="0.998974266666667">Various text mining algorithms require the process of feature selection. High-level semantically rich features, such as figurative language uses, speech errors etc., are very promising for such problems as e.g. writing style detection, but automatic extraction of such features is a big challenge. In this paper, we propose a framework for figurative language use detection. This framework is based on the idea of sense differentiation. We describe two algorithms illustrating the mentioned idea. We show then how these algorithms work by applying them to Russian language data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Vitaly I Batov</author>
<author>Yury A Sorokin</author>
</authors>
<title>Text attribution based on objective characteristics. Seriya yazyka i literatury,</title>
<date>1975</date>
<volume>34</volume>
<pages>1</pages>
<contexts>
<context position="1251" citStr="Batov and Sorokin (1975)" startWordPosition="184" endWordPosition="187">o algorithms illustrating the mentioned idea. We show then how these algorithms work by applying them to Russian language data. 1 Introduction Various text mining algorithms require the process of feature selection. For example, authorship attribution algorithms need to determine features to quantify the writing style. Previous work on authorship attribution among computer scientists is mostly based on low-level features such as word frequencies, sentence length counts, n-grams etc. A significant advantage of such features is that they can be easily extracted from any corpus. But the study by Batov and Sorokin (1975) shows that such features do not always provide accurate measures for authorship attribution. The linguistic approach to the problem involves such high-level characteristics as the use of figurative language, irony, sound devices and so on. Such characteristics are very promising for the mentioned above tasks, but the extraction of these features is extremely hard to automate. As a result, very few attempts have been made to exploit high-level features for stylometric purposes (Stamatatos, 2009). Therefore, our long-term objective is the extraction of high-level semantically rich features. Sin</context>
</contexts>
<marker>Batov, Sorokin, 1975</marker>
<rawString>Vitaly I. Batov and Yury A. Sorokin. 1975. Text attribution based on objective characteristics. Seriya yazyka i literatury, 34, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Birke</author>
<author>Anoop Sarkar</author>
</authors>
<title>A Clustering Approach for the Nearly Unsupervised Recognition of Nonliteral Language.</title>
<date>2006</date>
<booktitle>Proceedings of EACL-06</booktitle>
<contexts>
<context position="4690" citStr="Birke and Sarkar (2006)" startWordPosition="746" endWordPosition="749">(e.g. ”she is the sunshine”, ”life is a journey”) and expressions that are invented by an author (e.g. ”my heart’s in the Highlands”). Moreover, the improved algorithm (Li and Sporleder, August 2009) is supervised, and our approach is unsupervised. The study by Katz and Giesbrecht (2006) is also supervised, unlike ours. It also considers multiword expressions that have idiomatic meanings. They propose an algorithm, which computes the vectors for literal and non-literal usages and then use the nearest neighbor classifier to label an unseen occurence of the given idiom. The approach proposed by Birke and Sarkar (2006) is nearly unsupervised. They constructed two seed sets: one consists of literal usages of different expressions and the other consists of non-literal usages. They calculate the distance between an occurence in question and these two sets and assign to the occurence the label of the closest set. This work, as well as ours, refers to the ideas from Word Sense Disambiguation area. Unlike our approach, the authors focus their attention only on the detection of figuratevely used verbs and, whereas we only refer to the concepts and ideas of WSD, they adapt a particular existing one-word disambiguat</context>
</contexts>
<marker>Birke, Sarkar, 2006</marker>
<rawString>Julia Birke and Anoop Sarkar. 2006. A Clustering Approach for the Nearly Unsupervised Recognition of Nonliteral Language. Proceedings of EACL-06</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Black</author>
</authors>
<date>1954</date>
<journal>Metaphor. Proceedings of the Aristotelian Society,</journal>
<volume>55</volume>
<pages>273--294</pages>
<contexts>
<context position="6063" citStr="Black, 1954" startWordPosition="973" endWordPosition="974">racting particular types of figurative language. For example, Nissim and Markert (2003) proposed a machine learning algorithm for metonymy resolution. They state the problem of metonymy resolution as a classification task between literal use of a word and a number of predefined metonymy types. 3 Sense Differentiation We could treat a figurative meaning of a word as an additional, not common meaning of this word. Actually, some metaphors are quite common (e.g. eye of a needle, leg of a table, etc.) and are called catachretic metaphors. They appear in a language to remedy the gap in vocabulary (Black, 1954). These metaphors do not indicate an author’s writing style: an author uses such metaphor for an object because the language has no other name for that object. Therefore the algorithms we are developing do not work with this type of metaphors. Our approach to figurative language detection is based on the following idea: the fact that the sense of a word significantly differs from the sense of the surrounding text usually indicates that the word is used figuratively. Two questions arise immediately: 1. How do we represent the sense of both the word and the surrounding context? 2. How do we find</context>
</contexts>
<marker>Black, 1954</marker>
<rawString>Max Black. 1954. Metaphor. Proceedings of the Aristotelian Society, 55, pp. 273-294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Ester</author>
<author>Hans-Peter Kriegel</author>
<author>J¨org Sander</author>
<author>Xiaowei Xu</author>
</authors>
<title>A density-based algorithm for discovering clusters in large spatial databases with noise.</title>
<date>1996</date>
<booktitle>Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96),</booktitle>
<pages>226231</pages>
<publisher>AAAI Press,</publisher>
<contexts>
<context position="2533" citStr="Ester et al, 1996" startWordPosition="390" endWordPosition="393">y on some particular problems and approaches. In this paper, we examine one of such problems, the problem of automatic figurative language use detection. We propose a framework for figurative language detection based on the idea of sense differentiation. Then, we describe two algorithms illustrating the mentioned idea. One of them is intended to decide whether a usage is literal by comparing the texts related to the target expression and the set of texts related to the context itself. The other is aimed at grouping instances into literal and non-literal uses and is based on DBSCAN clustering (Ester et al, 1996). We illustrate then how these algorithms work by applying them to Russian language data. Finally, we propose some ideas on modifications which can significantly improve the accuracy of the algorithms. 2 Related Work Sporleder and Li (April 2009) proposed an unsupervised method for recognition of literal and nonliteral use of idiomatic expressions. Given an idiom the method detects the presence or absence of cohesive links between the words the idiom consists of and the surrounding text. When such links exist, the occurence is considered as a literal usage and as a non-literal when there are n</context>
<context position="14800" citStr="Ester et al, 1996" startWordPosition="2510" endWordPosition="2513">0 0,75 ветер 0,85 0,85 0,85 дождь 0,91 0,90 0,90 Table 2: Testing the algorithm. Accuracy of the k-means clustering For 75% of metaphorical usages F-measure is 0,85 or more as was expected and for 50% of literal usages F-measure is 0,75 or less. 5 Figurative Language Uses as Outliers The described above approach is to decide whether a word in a context is used literally or not. Unlike the first one, the second approach we propose, deals with a set of occurences of a word as to label every occurence as ’literal’ or ’non-literal’. We formulate this task as a clustering problem and apply DBSCAN (Ester et al, 1996) clustering algorithm to the data. Miller and Charles (1991) hypothesized that words with similar meanings are often used in similar contexts. As it was mentioned, we can treat a meaning of a metaphoric usage of an expression as an additional, not common for the expression. That’s why we expect metaphorical usages to be ouliers, while clustering together with common (i.e. literal) usages. Theoretically, the algorithm should also distinguish between all literal senses so that the contexts of the same meaning appear in the same cluster and the contexts of different meanings - in different cluste</context>
</contexts>
<marker>Ester, Kriegel, Sander, Xu, 1996</marker>
<rawString>Martin Ester, Hans-Peter Kriegel, J¨org Sander and Xiaowei Xu. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96), AAAI Press, pp. 226231</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English.</booktitle>
<location>Longman, London</location>
<contexts>
<context position="7350" citStr="Halliday and Hasan, 1976" startWordPosition="1203" endWordPosition="1206">st question, we refer to the ideas popular in the Word Sense Disambiguation community: sense is a group of contextually similar occurrences of a word (Sch¨utze, 1996). Hence, we represent the senses of both a word and its context as sets of documents related to the word and the context respectively. These sets can be obtained e.g. by searching Wikipedia, Google or another web search engine. For a word the query can be the word itself. As for a text, this query can be formulated as the whole text or as a set of some words contained in this text. It seems to us that querying the lexical chains (Halliday and Hasan, 1976) extracted from the text should provide better results than querying the whole text. As soon as we have a sense representation for such objects as a word and a text, we should find a way to measure the difference between these sense representations and find out what difference is strong enough for the considered occurence to be classified as a non-literal usage. One way to do this is representing sets of documents as sets of vectors and measuring the distance between the centers of the obtained vector sets. Another way is to apply clustering techniques to the sets and to measure the accuracy o</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Michael Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman, London</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Katz</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Automatic identification of non-compositional multiword expressions using latent semantic analysis.</title>
<date>2006</date>
<booktitle>Proceedings of the ACL/COLING-06 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties</booktitle>
<contexts>
<context position="4355" citStr="Katz and Giesbrecht (2006)" startWordPosition="693" endWordPosition="696">e do not focus our attention only on idioms. So far we have mostly dealt with ex67 Proceedings of the ACL 2010 Student Research Workshop, pages 67–72, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics pressions, which are not necessarily idiomatic by themselves, but become metaphors in a particular context (e.g. ”she is the sunshine”, ”life is a journey”) and expressions that are invented by an author (e.g. ”my heart’s in the Highlands”). Moreover, the improved algorithm (Li and Sporleder, August 2009) is supervised, and our approach is unsupervised. The study by Katz and Giesbrecht (2006) is also supervised, unlike ours. It also considers multiword expressions that have idiomatic meanings. They propose an algorithm, which computes the vectors for literal and non-literal usages and then use the nearest neighbor classifier to label an unseen occurence of the given idiom. The approach proposed by Birke and Sarkar (2006) is nearly unsupervised. They constructed two seed sets: one consists of literal usages of different expressions and the other consists of non-literal usages. They calculate the distance between an occurence in question and these two sets and assign to the occurenc</context>
</contexts>
<marker>Katz, Giesbrecht, 2006</marker>
<rawString>Graham Katz and Eugenie Giesbrecht. 2006. Automatic identification of non-compositional multiword expressions using latent semantic analysis. Proceedings of the ACL/COLING-06 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linlin Li</author>
<author>Caroline Sporleder</author>
</authors>
<title>Classifier combination for contextual idiom detection without labeled data.</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>315--323</pages>
<marker>Li, Sporleder, 2009</marker>
<rawString>Linlin Li and Caroline Sporleder. August 2009. Classifier combination for contextual idiom detection without labeled data. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 315-323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and Knowledge-based Measures of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>Proceedings of AAAI-06</booktitle>
<contexts>
<context position="8174" citStr="Mihalcea et al (2006)" startWordPosition="1344" endWordPosition="1347">ifference between these sense representations and find out what difference is strong enough for the considered occurence to be classified as a non-literal usage. One way to do this is representing sets of documents as sets of vectors and measuring the distance between the centers of the obtained vector sets. Another way is to apply clustering techniques to the sets and to measure the accuracy of the produced clustering. The higher the accuracy is, the more different the sets are. Besides, this can be done by calculating textto-text semantic similarity using for example the measure proposed by Mihalcea et al (2006). This is rather difficult in case of the Russian language because at the moment there is no WordNet-like taxonomies for Russian. In the next section, we propose two algorithms based on the mentioned above idea. We state the algorithms generally and try to find out experi68 mentally what combination of the described techniques provides the best results. 4 Finding the Distance to the Typical Context Set The algorithm is intended to determine whether a word (or an expression) in a given context is used literaly or not. As it was mentioned above, we decided to represent senses of both an expressi</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley and Carlo Strapparava. 2006. Corpus-based and Knowledge-based Measures of Text Semantic Similarity. Proceedings of AAAI-06</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Using Wikipedia for Automatic Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL</booktitle>
<contexts>
<context position="11340" citStr="Mihalcea, 2007" startWordPosition="1868" endWordPosition="1869">r every expression, the typical context set consists of the first 10 articles retrieved by searching Google for the expression. In order to construct the second set we removed the target expression from the context and manually extracted lexical chains from the texts, although, the process of lexical chains extraction can be done automatically. However the algorithms on lexical chains extraction usually use WordNet to calculate the relatedness, but as it was already mentioned WordNet for the Russian language does not exist yet. Another way to calculate semantic relatedness is using Wikipedia (Mihalcea, 2007; Turdakov and Velikhov, 2008), but it takes much effort. The second set for each occurence consists of the first 10 articles retrieved by searching Google for the extracted chains. Then we applied k-means clustering algorithm (k = 2) to these sets. To evaluate the clustering we used measures from the clustering literature. We denote our sets by G = g1, g2 and the clusters obtained by k-means as C = c1, c2. We define a mapping f from the elements of G to the elements of C, such that each set gi is mapped to a cluster cj = f(gi) that has the highest percentage of common elements with gi. Precis</context>
</contexts>
<marker>Mihalcea, 2007</marker>
<rawString>Rada Mihalcea. 2007. Using Wikipedia for Automatic Word Sense Disambiguation. Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL 2007)</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="14860" citStr="Miller and Charles (1991)" startWordPosition="2520" endWordPosition="2523">e 2: Testing the algorithm. Accuracy of the k-means clustering For 75% of metaphorical usages F-measure is 0,85 or more as was expected and for 50% of literal usages F-measure is 0,75 or less. 5 Figurative Language Uses as Outliers The described above approach is to decide whether a word in a context is used literally or not. Unlike the first one, the second approach we propose, deals with a set of occurences of a word as to label every occurence as ’literal’ or ’non-literal’. We formulate this task as a clustering problem and apply DBSCAN (Ester et al, 1996) clustering algorithm to the data. Miller and Charles (1991) hypothesized that words with similar meanings are often used in similar contexts. As it was mentioned, we can treat a meaning of a metaphoric usage of an expression as an additional, not common for the expression. That’s why we expect metaphorical usages to be ouliers, while clustering together with common (i.e. literal) usages. Theoretically, the algorithm should also distinguish between all literal senses so that the contexts of the same meaning appear in the same cluster and the contexts of different meanings - in different clusters. Therefore, ideally, the algorithm should solve word sens</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malvina Nissim</author>
<author>Katja Markert</author>
</authors>
<title>Syntactic features and word similarity for supervised metonymy resolution.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03)</booktitle>
<pages>56--63</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="5538" citStr="Nissim and Markert (2003)" startWordPosition="881" endWordPosition="885"> these two sets and assign to the occurence the label of the closest set. This work, as well as ours, refers to the ideas from Word Sense Disambiguation area. Unlike our approach, the authors focus their attention only on the detection of figuratevely used verbs and, whereas we only refer to the concepts and ideas of WSD, they adapt a particular existing one-word disambiguation method. As we have already said, we deal with different types of figurative language (metaphors, metonymies etc.). However, there are some works aimed at extracting particular types of figurative language. For example, Nissim and Markert (2003) proposed a machine learning algorithm for metonymy resolution. They state the problem of metonymy resolution as a classification task between literal use of a word and a number of predefined metonymy types. 3 Sense Differentiation We could treat a figurative meaning of a word as an additional, not common meaning of this word. Actually, some metaphors are quite common (e.g. eye of a needle, leg of a table, etc.) and are called catachretic metaphors. They appear in a language to remedy the gap in vocabulary (Black, 1954). These metaphors do not indicate an author’s writing style: an author uses</context>
</contexts>
<marker>Nissim, Markert, 2003</marker>
<rawString>Malvina Nissim and Katja Markert. 2003. Syntactic features and word similarity for supervised metonymy resolution. Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03) (Sapporo, Japan, 2003). 56-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>97--123</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1), pp. 97-123</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Linlin Li</author>
</authors>
<title>Unsupervised recognition of literal and non-literal use of idiomatic expressions.</title>
<date>2009</date>
<booktitle>Proceedings of EACL-09</booktitle>
<marker>Sporleder, Li, 2009</marker>
<rawString>Caroline Sporleder and Linlin Li. April 2009. Unsupervised recognition of literal and non-literal use of idiomatic expressions. Proceedings of EACL-09</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efstathios Stamatatos</author>
</authors>
<title>A survey of modern authorship attribution methods.</title>
<date>2009</date>
<journal>Journal of the American Society for information Science and Technology,</journal>
<volume>60</volume>
<issue>3</issue>
<pages>538--556</pages>
<contexts>
<context position="1751" citStr="Stamatatos, 2009" startWordPosition="263" endWordPosition="264">tage of such features is that they can be easily extracted from any corpus. But the study by Batov and Sorokin (1975) shows that such features do not always provide accurate measures for authorship attribution. The linguistic approach to the problem involves such high-level characteristics as the use of figurative language, irony, sound devices and so on. Such characteristics are very promising for the mentioned above tasks, but the extraction of these features is extremely hard to automate. As a result, very few attempts have been made to exploit high-level features for stylometric purposes (Stamatatos, 2009). Therefore, our long-term objective is the extraction of high-level semantically rich features. Since the mentioned topic is very broad, we focus our attention only on some particular problems and approaches. In this paper, we examine one of such problems, the problem of automatic figurative language use detection. We propose a framework for figurative language detection based on the idea of sense differentiation. Then, we describe two algorithms illustrating the mentioned idea. One of them is intended to decide whether a usage is literal by comparing the texts related to the target expressio</context>
</contexts>
<marker>Stamatatos, 2009</marker>
<rawString>Efstathios Stamatatos. 2009. A survey of modern authorship attribution methods. Journal of the American Society for information Science and Technology, 60(3): 538-556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Turdakov</author>
<author>Pavel Velikhov</author>
</authors>
<title>Semantic Relatedness Metric for Wikipedia Concepts Based on Link Analysis and its Application to Word Sense Disambiguation SYRCoDIS</title>
<date>2008</date>
<contexts>
<context position="11370" citStr="Turdakov and Velikhov, 2008" startWordPosition="1870" endWordPosition="1874">on, the typical context set consists of the first 10 articles retrieved by searching Google for the expression. In order to construct the second set we removed the target expression from the context and manually extracted lexical chains from the texts, although, the process of lexical chains extraction can be done automatically. However the algorithms on lexical chains extraction usually use WordNet to calculate the relatedness, but as it was already mentioned WordNet for the Russian language does not exist yet. Another way to calculate semantic relatedness is using Wikipedia (Mihalcea, 2007; Turdakov and Velikhov, 2008), but it takes much effort. The second set for each occurence consists of the first 10 articles retrieved by searching Google for the extracted chains. Then we applied k-means clustering algorithm (k = 2) to these sets. To evaluate the clustering we used measures from the clustering literature. We denote our sets by G = g1, g2 and the clusters obtained by k-means as C = c1, c2. We define a mapping f from the elements of G to the elements of C, such that each set gi is mapped to a cluster cj = f(gi) that has the highest percentage of common elements with gi. Precision and recall for a cluster g</context>
</contexts>
<marker>Turdakov, Velikhov, 2008</marker>
<rawString>Denis Turdakov and Pavel Velikhov. 2008. Semantic Relatedness Metric for Wikipedia Concepts Based on Link Analysis and its Application to Word Sense Disambiguation SYRCoDIS 2008</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>