<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000290">
<title confidence="0.984261">
Acquiring Knowledge From Encyclopedic Texts*
</title>
<author confidence="0.998721">
Fernando Gomez
</author>
<affiliation confidence="0.999099">
Dept. of Computer Science
Univ. of Central Florida
</affiliation>
<address confidence="0.556538">
Orlando, FL 32816, USA
</address>
<email confidence="0.998794">
gomez@cs.ucf.edu
</email>
<author confidence="0.995995">
Richard Hull
</author>
<affiliation confidence="0.999096">
Dept. of Computer Science
Univ. of Central Florida
</affiliation>
<address confidence="0.5565">
Orlando, FL 32816, USA
</address>
<email confidence="0.998688">
hull@cs.ucf.edu
</email>
<author confidence="0.942771">
Carlos Segami
</author>
<affiliation confidence="0.932613">
Math. and Comp. Sci. Dept.
Barry University
</affiliation>
<address confidence="0.897278">
Miami Shores, FL 33161, USA
</address>
<email confidence="0.999434">
segami@buvax.barry.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999820157894737">
A computational model for the acquisition
of knowledge from encyclopedic texts is de-
scribed. The model has been implemented
in a program, called SNOWY, that reads
unedited texts from The World Book En-
cyclopedia, and acquires new concepts and
conceptual relations about topics dealing
with the dietary habits of animals, their
classifications and habitats. The program
is also able to answer an ample set of ques-
tions about the knowledge that it has ac-
quired. This paper describes the essential
components of this model, namely seman-
tic interpretation, inferences and represen-
tation, and ends with an evaluation of the
performance of the program, a sample of
the questions that it is able to answer, and
its relation to other programs of similar na-
ture.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933388888889">
We present an approach to the acquisition of knowl-
edge from encyclopedic texts. The goal of this re-
search is to build a knowledge base about a given
topic by reading an encyclopedic article. Expert sys-
tems could use this database to tap in for pieces
of knowledge, or a user could directly query the
database for specific answers. Then, two possible
applications could be derived from our research: (a)
the automatic construction of databases from ency-
clopedic texts for problem-solvers and (b) querying
an encyclopedia in natural language. The idea is to
build a database from an encyclopedic text on the
fly. Then, if a user asks the question, say, Which
bears eat seals? the system would reply by saying
something like &amp;quot;I don&apos;t know. But, wait a minute, I
am going to read this article and let you know.&amp;quot; In
the process of reading the article, the system builds
a small knowledge base about bears and calls the
</bodyText>
<footnote confidence="0.965702">
*This research is being funded by NASA-KSC Con-
tract NAG-10-0120
</footnote>
<bodyText confidence="0.998418648648649">
question-answering system to answer any question
posed by an expert system or a human. The long-
term goal of our research is to read an entire ar-
ticle on, say bears, and to build a knowledge base
about bears. Since this goal requires dealing with
an extraordinary number of research issues, we have
concentrated on a series of topics about animals, in-
cluding diet, habitat, and classification. A skimmer
scans the article for sentences relevant to a given
topic and passes these sentences to the understand-
ing system, called SNOWY, for complete parsing, in-
terpretation, concept formation, concept recognition
and integration in long-term memory (LTM). But,
if, in the process of learning about the dietary habits
of, say beetles, the program is told that they culti-
vate fungi, and the program is able to interpret that
sentence, that knowledge will also be integrated in
LTM. Consequently, our approach to understanding
expository texts is a bottom-up approach in which
final knowledge representation structures are built
from the logical form of the sentences, without inter-
vening scriptal or frame knowledge about the topic.
Hence, our system does not start with a frame con-
taining the main slots to be filled for a topic, say
&amp;quot;diet,&amp;quot; as in recent MUG projects (Sundheim, 1992),
but rather it will build everything relevant to diet
from the output of the interpretation phase. Then,
when we are talking about the topic, it will not make
any difference if the sentences refer to what the an-
imals eat, or what eats them. Every aspect dealing
with the general idea of ingest can be analyzed and
properly integrated into memory. Our corpora for
testing our ideas has been The World Book Ency-
clopedia (World Book, 1987), which is one or two
levels less complex than the Collier&apos;s Encyclopedia,
which, in turn, is less complex than the Encyclopae-
dia Britannica.
</bodyText>
<sectionHeader confidence="0.99664" genericHeader="introduction">
2 Interpretation
</sectionHeader>
<bodyText confidence="0.99975725">
In order for the integration component to integrate
a concept in LTM, a successful parse and interpreta-
tion needs to be produced for a sentence or at least
for one of its clauses. The input to the interpre-
</bodyText>
<page confidence="0.994001">
84
</page>
<figure confidence="0.977985785714286">
(ACTION
(is-a (relation))
(subj (thing (actor))) (obj (thing (theme)))
(adverb
(time (at-time))
(negation (negation)) (Itogmm.f
(PMP
(in (Itm-ctgy (physical-thing (at-loc (weak)))
(time-unit (at-time (strong)))))
(at (him-ctgy (physical-thing (at-loc (strong)))
(time-unit (at-time (stmng)))))
(during (ltm-ctgy (physical-thing (at-time (strong)))))
(with (Itm-ctgy (animate-body-part (instrument (strong))))
(state (state (strong)))))
</figure>
<figureCaption confidence="0.999985">
Figure 1: Organization of the Verbal Concepts
</figureCaption>
<bodyText confidence="0.955051605263158">
tation phase is built by a top-down, lexical-driven
parser(Gomez, 1989), which parses the sentences di-
rectly into syntactic cases. Prepositional phrases
are left unattached inside the structure built by the
parser. It is up to the interpreter to attach them,
identify their meaning and the thematic roles that
they may stand for. The parser is a deterministic
machine, containing mechanisms for minimizing the
need for backing up. The average amount of time
in parsing a sentence from the encyclopedia is about
one second. The parser has presently a lexicon of
about 35,000 words. The rate of success of produc-
ing a full and correct parse of a sentence is, as of
this writing, 76.8% for this Encyclopedia (see below
for a detailed discussion of test results and machine
used). The parser begins the parsing of a sentence
on a syntactic basis until the meaning of the verb
is recognized. The rules that determine the mean-
ing of the verbs, called VM rules, are classified as
subj-rules, verb-rules, obj-rules, io-rules, pred-rules,
prep-rules, and end-of-clause-rules. These rules are
activated when the verb, a syntactic case or a prepo-
sitional phrase has been parsed, or when the end of
the clause has been reached, respectively. In most
cases, the antecedents of these rules contain selec-
tional restrictions which determine whether the in-
terpretation of the syntactic constituent is a sub-
class of some concept in SNOWY&apos;s LTM ontology.
If during an examination of LTM the selectional re-
striction is passed, the consequent(s) of the VM rule
establish the proper meaning or verbal concept for
the verb. If no rules fire, the parser inserts the syn-
tactic case or prepositional phrase in the structure
being built and continues parsing. These rules, of
which there are just over 300, incorporate a con-
siderable degree of ambiguity procrastination (Rich,
1987). For instance, rather than writing an obj-rule
for determining the meaning of &amp;quot;take&amp;quot; saying: If
</bodyText>
<footnote confidence="0.492705571428571">
LTM(obj) is-a medicament then meaning-of take is
ingest, which will immediately jump to determine
the meaning of &amp;quot;take&amp;quot; in Peter took an aspirin when
the obj is parsed, it is better to write that rule as an
end-of-clause rule. This will avoid making the wrong
assumption about the meaning of the verb in Peter
took an aspirin to Mary.
</footnote>
<table confidence="0.959653722222222">
(REPS ((VRS (0233)))
SUB) ((PARSER ((DFART THE) (VERB CROWNED) (NOUN EAGLE)))
(REF (DEFINITE)) (PLURAL NIL)
(INTERP (CROWNED-EAGLE (Q (ALL)))) (SEMANTIC-ROLE (ACTOR)))
PREP ((PARSE (OF ((PN AFRICA)))) (INTERP (AFRICA (Q (CONSTANT))))
(ATTACH-TO (CROWNED-EAGLE (LOCATION-R (AFRICA)))))
VERB ((MAIN-VERB EAT EATS) (TENSE PRES)(NUM SING) (PRIM (INGEST)))
OBI ((PARSE ((NOUN MONKEYS))) (PLURAL T)) (INTERP (MONKEY (Q (?))))
(SEMANTIC-ROLE (THEME))))
Output for Structure G233:
(SUB) ((PARSE ((DFART THE) (VERB CROWNED) (NOUN EAGLE)))
(REF (DEFINITE)) (PLURAL NIL)
(INTERP (CROWNED-EAGLE (Q (ALL)))) (SEMANTIC-ROLE (ACTOR)))
VERB ((MAIN-VERB LIVE LIVES) (TEMSE FEES) (NUM SING) (PRIM (INHABIT)))
PREP ((PARSE (IN ((DFART THE) (NOUN RAIN) (NOUN FOREST))))
(REF (DEFINITE)) (PLURAL NIL)
(INTERP (RAIN-FOREST)) (SEMANTIC-ROLE (AT-LOC))
(ATTACH-TO (VERB (STRONGLY))))
</table>
<figureCaption confidence="0.986086333333333">
Figure 2: Output of the parser and interpreter for
the sentence The crowned eagle of Africa lives in the
rain forests and eats monkeys.
</figureCaption>
<bodyText confidence="0.99584825">
When the verbal concept has been identified, the
syntactic cases and prepositional phrases already
identified by the parser and any subsequent con-
stituents are interpreted by matching them against
the representation of the verbal concept. Verbal con-
cepts are not reduced to a small set of primitives; on
the contrary they are organized into a classification
hierarchy containing the most general actions near
the root node and the most specific at the bottom.
Figure 1 contains simplified examples of the root
node action, the subconcept ingest, and one sub-
concept of ingest, drink. The entry subj in the node
ingest means that the actor of ingest is an animate,
and that this is indicated syntactically by the case
subj. The entries for the preposition &amp;quot;with&amp;quot; mean
that if the object of the PP is a utensil, then the
case expressed by the preposition is the instrument
case; if the object of the preposition is a human,
then the case is the accompany case, etc. The en-
tries strong and weak indicate whether the verbal
concept claims that preposition strongly or weakly,
respectively. This information is used by the inter-
preter to attach PPs. The drink node has only one
entry for the case theme. The others are inherited
from the nodes, ingest and action. If the context
to be understood requires a more detailed under-
standing of how animals drink, distinct from how hu-
mans drink, then the concept drink may be split into
the concepts human-drink and animal-drink. Con-
sequently, the specification of the hierarchy depends
on the domain knowledge to be acquired. The algo-
rithm that matches syntactic cases or prepositional
phrases to the representation of the verbal concept
searches the hierarchy in a bottom-up fashion. The
search ends with success or failure, if the LTM entry
in the verbal concept is true or false, respectively.
Hence, entries in subconcepts override the same en-
tries in superconcepts. The node action provides an
excellent way to handle situations not contemplated
in subconcepts by defaulting them to those in the
</bodyText>
<figure confidence="0.980208230769231">
(INGEST
(is-a (action))
(subj (animate (actor)))
(Kg,
(with (Ilm-ctgy
(utensil (instrument (strong)))
(hurnan (accompany (strong)))
(physical-thing
(co-theme (weak))))))
(obj (physical-thing (theme)))
(DRINK
(is-a (ingest))
(obj (liquid (theme))))
</figure>
<page confidence="0.994778">
85
</page>
<bodyText confidence="0.999439294117647">
action root node.
Figure 2 depicts the output of the interpreter.
The slot subject contains the output of the parser
marked by the slot PARSE, the interpretation of
the noun phrase marked by INTERP, and the the-
matic role marked by SEMANTIC-ROLE. The entry
Q within the INTERP slot indicates the quantifier
for that case. The entries for the slot PREP, contain,
in addition to the PARSE slot, the slot ATTACH-
TO indicating which concept in the parse structure
to attach that constituent to, and the meaning of
the preposition indicated, in this case, by the sub-
slot LOCATION-R. The quantifier for &amp;quot;monkey&amp;quot; is
a question mark, because its value is not indicated
in the text. In (Gomez et al., 1992), the reader may
find a detailed discussion of the interpretation issues
presented in this session.
</bodyText>
<sectionHeader confidence="0.999638" genericHeader="method">
3 Inferences
</sectionHeader>
<bodyText confidence="0.8868537">
If the knowledge about dietary habits of animals
were indicated in the texts by using &amp;quot;eat&amp;quot; and its
cognates, the task of acquiring this knowledge would
be rather simple. But, the fact is that an encyclo-
pedia article may refer to the dietary habits in a
variety of manners. Figure 3 contains a hierarchy
about the diet topic. The verbs that trigger these
verbal concepts are indicated by writing [verb]. The
verb &amp;quot;dig out&amp;quot; triggers the action dig-r. Those ver-
bal concepts from which an ingest relation is inferred
are indicated by writing an asterisk by its side. The
representation of dig-r is:
(dig-r (is-a (action))
(subj (animal (actor)) (machine (actor)))
(obj (physical-thing (theme)))
(addition-mien (fire-one
((and (if% is-a actor animal)
(if% is-a theme animate))
(add (((actor (actot)) (pr (ingest))
(theme (theme)))))))))
The representation of dig-r is similar to the previ-
ous verbal concepts, except for the entry that says
addition-rule. This is an inference rule saying that if
the actor of this action is an animal and the theme
is an animate, then add to LTM the relation say-
ing that the actor ingests the theme. This rule per-
mits the acquisition of the fact that bears eat squir-
rels and mice from the sentence A grizzly has long,
curved claws that it uses chiefly to dig out ground
squirrels and mice. Similar inference rules are stored
in the other actions marked with an asterisk. Then,
if the system reads the sentence Bears are fond of
honey, it will infer that they eat honey. The infer-
ence rules are also inherited from the nodes repre-
senting the actions in the hierarchy. Hence, if the
verbal concept animal-fish, shown in Figure 3, was
suggested for the sentence Owls have been known
to fish in shallow creeks, we would inherit an addi-
tion rule from the verbal concept animal-hunt which
would then infer an ingest relation.
</bodyText>
<figure confidence="0.451171">
action
</figure>
<figureCaption confidence="0.998626">
Figure 3: Conceptual Verbs Organizing the Topic
</figureCaption>
<bodyText confidence="0.999996055555556">
The hierarchy is also helpful in avoiding incorrect
inferences. Sentences discussing humans hunting an-
imals do not automatically imply ingest relations,
especially when an explicit purpose is given which is
not ingest-related. For example in People hunt some
kinds of seals for their soft fur, it is unlikely that
the people mentioned will eat those seals. There-
fore, we have separate verbal concepts for hunt re-
lations where humans are the actors, whose infer-
ence rules do not suggest ingest relations, except in
cases like Eskimos hunt polar bears for food. Fur-
thermore, addition rules typically have constraints
within their antecedents to prevent inappropriate in-
ferences, i.e., we would not want to infer an ingest
relation when processing the sentence Tigers search
for warm places to sleep during the day, and a con-
straint on the theme of &amp;quot;search for&amp;quot; to be at least
animate rejects the inference.
</bodyText>
<sectionHeader confidence="0.9519055" genericHeader="method">
4 Interpreting Noun Phrases and
Restrictive Modifiers
</sectionHeader>
<bodyText confidence="0.999740166666667">
Detecting classification relations in the text becomes
a must, not only if questions of the type Which owls
eat fish?, or Which eagles eat hyraxes? are to be
answered, but also for the acquisition of the knowl-
edge in sentences like The prey of polar bears con-
sists of seals, or The diet of bears consists of nuts,
berries and small rodents. In order to achieve this,
complex noun groups and restrictive modifiers are
represented by the noun group interpreter as clas-
sification hierarchies. One of the senses of &amp;quot;diet&amp;quot; is
represented as: X1 (cf (is-a (food) R1)), where R1 is
the relation ingest with actor = animal, and theme
= food. Then, &amp;quot;the diet of bears&amp;quot; is represented as
the concept X2 (cf (is-a (food) R2)), where R2 is
the relation ingest with actor = bear, and theme =
food. The slot cf contains the necessary and suffi-
cient conditions that define the concept X2. Then,
the meaning of X2 is:
</bodyText>
<figure confidence="0.982289255813953">
V(x)(X2(x) Food(x) A R2(x))
animal-hunt human-hunt harm dig-r ingest
[stalk] [shoot]
[dig-out] [swallow]
[chase]
[hunt] /7\\&apos;&apos;&apos;itt.&amp; *destroy *damage ldig-up] [consume]
[kill] [nibble]
[devour]
[search]
*transport
[Inch-uP1
animal-capture-animal
[trap] I [catch]
[capture]
[take]
animal-fish
[fish[
*experience-affection-for
[are fond-of]
[like]
[love]
[prefer]
[bite]
[chew]
[gulp-down]
[gobble-up]
[gorge]
[feed-on]
Dive-on]
[eat]
[ingest]
Rake-in]
[keep-down]
hunt-for-sport hunt-animal-for-
body-part
[skin]
animal-steal
[steal]
[take-away]
[snatch]
[take]
Solid Lines - IS-A links
86
</figure>
<figureCaption confidence="0.681483568965517">
The same interpretation is given to restrictive rel- 87 Segami, 1989), the reader may find a detailed dis-
ative clauses. The phrase &amp;quot;eagles that live in the rain cussion of the recognizer algorithm. Note that the
forests&amp;quot; is represented as X3 (cf (is-a (eagle) R3)), algorithm will produce the concept X.4 (cf(is-a(lion)
where R3 is the relation &amp;quot;live in the rain forests.&amp;quot; live-in(sea))) if the noun group &amp;quot;sea lion&amp;quot; is not in
Once these structures are built by the interpreter, a quotation marks or capitalized.
classifier that is a component of the integration al- 5 Final Knowledge Representation
gorithm integrates these concepts in the proper po- Structures
sition in LTM. The interpretation of complex nouns The input of the interpreter is passed to the for-
proceeds by attempting to determine the meaning mation phase that builds the final knowledge rep-
of pairs of items in the complex noun, utilizing a resentation structures. These are in turn integrated
scheme that combines the items in the complex noun into LTM upon activating a recognizer algorithm and
from left to right. For example, in the interpretation an integration algorithm both of which make exten-
of &amp;quot;big red wine bottle&amp;quot; an attempt is made to find sive use of a classifier similar to the one reported
a meaning for the terms &amp;quot;big red,&amp;quot; &amp;quot;big wine,&amp;quot; &amp;quot;big in (Schmolze and Lipkis, 1983). The construction of
bottle,&amp;quot; &amp;quot;red wine,&amp;quot; &amp;quot;red bottle&amp;quot; and &amp;quot;wine bottle.&amp;quot; the final knowledge representation structures is done
If one item in the complex noun can be paired (i.e., as follows. The interpretation phase, if successful,
a meaning can be found) with more than one other has built a relation, and a set of thematic roles for
item in the complex noun, then the algorithm re- each sentence. Let us call the thematic roles of the
turns more than one interpretation for the complex relation the entities for that relation. All the n en-
noun, and disambiguation routines are activated. In tities of a n-ary relation are represented as objects
our example, a meaning is found for &amp;quot;big bottle,&amp;quot; in our language, and links are created pointing to
&amp;quot;red wine,&amp;quot; &amp;quot;red bottle,&amp;quot; and &amp;quot;wine bottle,&amp;quot; from the representation of the relation, which is repre-
which the algorithm returns the two possible inter- sented as a separate structure, called an a-structure.
pretations: Figure 4 depicts the representation produced from
(bottle (size (big)) (color (red)) (pertain-to (wine))) the interpretation of the sentence The crowned ea-
(bottle (size (big)) (pertain-to (wine (color (red))))) gle of Africa lives in the rain forests and eats mon-
Finding the meaning of terms of the form &amp;quot;iteml keys. Five objects have been created; CROWNED-
item2&amp;quot; reduces to finding a relation that connects EAGLE, AFRICA, RAIN-FOREST, MONKEY and
the concepts corresponding to &amp;quot;item 1&amp;quot; and &amp;quot;item2&amp;quot; @X235, which stands for the concept &amp;quot;crowned ea-
in LTM. Consequently, this algorithm as well as the gle of Africa.&amp;quot; The relation @A237 represents the
algorithm that finds the meaning of PPs and syn- ingest relation between the object @X235 and the
tactic cases depends on a a priori set of concepts object MONKEY. The object MONKEY points to
that constitutes the basic ontology of SNOWY. As this structure by the entry under MONKEY that
SNOWY reads, it adds new concepts to this ontol- says ingest%by (@r235 ($more (@a237)))), and the
ogy as explained below. Its initial ontology consists object @X235 also points to this structure by the
of 1243 concepts. slot (ingest (monkey ($more (a237)))). The scope
In order to find a semantic relation between two of the quantifiers is from left to right. Then, the
pair of items in a NP, the items or any of their su- meaning of structure @A239 (assuming that the
perconcepts must belong to the a priori ontology. If question Mark in the quantifier slot of MONKEY
the noun group interpreter does not find a seman- stands for &amp;quot;some&amp;quot; as the question-answering as-
tic relation between two items, the algorithm will sumes) is V(x)(@X235(x) 3y(MONK EY (y) A
hyphenate them. This has been the case for &amp;quot;rain IN GE ST(x , y))). An a-structure can be linked to
forest,&amp;quot; and &amp;quot;crowned eagle&amp;quot; (see Figure 2). &amp;quot;Rain- other a-structures by slots expressing causality, time,
forest&amp;quot; is constructed in LTM as a subconcept of etc., as becomes necessary in the representation of
&amp;quot;forest.&amp;quot; But, no semantic relation will be built Birds migrate south when it freezes.
between &amp;quot;rain&amp;quot; and &amp;quot;forest.&amp;quot; However, if the pair 6 Results
of items is &amp;quot;sea mammal,&amp;quot; the algorithm builds X3 Table 1 below provides statistics revealing how well
(cf(is-a(mammal) live-in(sea))), because &amp;quot;mammal&amp;quot; the system performed during testing. The sys-
and &amp;quot;sea&amp;quot; are categorized in LTM as subconcepts of tem was initially trained on ten articles: bears,
&amp;quot;animate&amp;quot; and &amp;quot;habitat,&amp;quot; respectively. The algo- beavers, beetles, elephants, frogs, penguins, rac-
rithm will produce the same representation for &amp;quot;sea coons, seals, snakes, and tigers. Then, two more
mammal&amp;quot; and &amp;quot;mammals that live in the sea,&amp;quot; ex- articles about sharks and eagles were analyzed to as-
cept for the names of the concepts, which are dummy sess our progress. Test texts were chosen randomly
names with no meaning. The recognizer algorithm by a student selecting a letter of the alphabet and
is able to tell that the two concepts are the same then finding texts about animals within those vol-
concept by examining the content of the cf slot, and
activating a classifier that analyzes the subsumption
relations between a pair of concepts. In (Gomez and
</figureCaption>
<figure confidence="0.895575470588235">
CROWNED-EAGLE AFRICA
(is-a (eagle)) (location-of (03035 (Smore (tga236))))
RAIN-FOREST MONKEY
(is-a (forest)) (ingest%by (t0x235 (Smote ((7a237))))
(related-to ((6a239))
@A237
OX235 (0a235) (monkey))
(cf (is-a (crowned-eagle)) (Oa235)) (pr (ingest))
(location-r (erica (Smote (0a237)))) (actor (45,s235 (q (all))))
(ingest (monkey (Smote (ea237)))) (theme (monkey (q (?))))
(inhabit (Snail (Smote ((5 a239)))) (instance-of (action))
0A235 0A239
(instance-of (description)) bugs (4s235) (rain-forest))
Wes (Ox235) (erica)) (r. (inhabit))
(pr (location-r)) (actor (@o235 (q (WO)))
(descr-subj (Or.235 (q (all)))) (at-lc (rain-forest (q (?))))
(descr-obj (africa (constant)))) (instance-of (action))
</figure>
<figureCaption confidence="0.999625">
Figure 4: Formation Structures
</figureCaption>
<bodyText confidence="0.9980715">
umes of the encyclopedia. The letter &amp;quot;B&amp;quot; and the
letter &amp;quot;M&amp;quot; were chosen. The texts were then se-
lected from those volumes. In December of 1993, an
article about birds was chosen. No component of the
system (lexicon, parser, interpreter, etc.) was pre-
prepared with information about this article. The
lexicon of the parser consisted of 10,000 words. This
text was the largest article that the system had ana-
lyzed, containing approximately 1330 sentences and
16,000 words. None of the designers of the system
read this article prior to the test. And even if they
had read it, it would have been of very little use be-
cause the system has reached such complexity that
it is not easy to assess how it is going to perform in
an article of 1330 sentences.
The first row of Table 1 indicates that 145
sentences were selected from the bird text by a
keyword/pattern-based skimmer. Of these 145 sen-
tences, 91 were relevant to the dietary habits do-
main. A total of 23 relevant sentences were missed,
primarily due to keywords or patterns not contem-
plated. An example of sentence that is relevant
but was not selected is Robins and sparrows, for
example, are highly effective against cabbageworms,
tomato worms, and leaf beetles. The parser was able
to produce a correct parse for 58 of the 91 relevant
sentences (64%), even in cases where the sentence
contained unknown words. Among the sentences
successfully parsed, the parser encountered some 40
unknown words of which approximately 70% were
names of birds, such as &amp;quot;grosbeaks&amp;quot;, &amp;quot;flycatchers&amp;quot; ,
&amp;quot;titmice&amp;quot;, &amp;quot;thrashers,&amp;quot; etc., and 30% were common
words. Of those 58 parsed sentences, 32 (55%) were
interpreted correctly. The output for these 32 sen-
tences was then passed to the formation, recogni-
tion, and integration phases to be inserted into LTM.
Interpretation failures can be attributed to missing
VM rules, comparatives, and problems of anaphoric
reference.
Later, in April of 1994, three more texts were ran-
domly chosen for testing. A summary of the results
for those three tests is also given in Table 1. In
</bodyText>
<tableCaption confidence="0.8860765">
Table 1: Statistics for the Sentences of the Bird, Bat,
Monkey, and Mouse Texts
</tableCaption>
<table confidence="0.998897">
Bird Bat Monkey Mouse Combined
Selected Sentences 145 27 22 29 223
Average Words/Sentence 17.1 11.6 16.8 13.7 16.0
Longest Sentence 35 21 27 26 35
</table>
<bodyText confidence="0.992640581395349">
this test, the parser ran with a lexicon consisting of
about 35,000 words. The improvement of the inter-
preter was mainly due to new interpretation rules,
additions to the hierarchy of verbal concepts, and
to the hierarchy of concepts that organize the infer-
ences about the topic.
All of the tests were run on a SPARC Classic Ma-
chine executing Allegro Common Lisp. The average
time to completely process a selected sentence on
this platform was 3.1 seconds. This is a conserva-
tive figure because it includes the processing time of
the skimmer, i.e., some amount of overhead is nec-
essary for file handling and for determining when a
sentence is irrelevant. Therefore, the average time
for processing a sentence is actually less than 43-:1
seconds.
The following is a list of natural language ques-
tions posed to the system after reading the bird,
bat, and monkey articles and the contents of the
system-generated answers. Note that the system
output has been altered and condensed for the sake
of brevity. In answering the questions, the sys-
tem uses classification-based reasoning, not theorem
proving. Many complex chains of inferences can be
obtained by keeping memory organized in a princi-
pled manner. In those cases in which a question asks
about a concept that does not exist in LTM, the clas-
sifier is activated to place that concept in LTM and
obtain an answer. See (Gomez and Segami, 1991)
for a detailed discussion of all these issues and the
theorems proving the soundness of the inference al-
gorithms.
What do birds eat? sapsucker ingest tree-sap; hum-
mingbird ingest nectar; duck ingest plant-matter,
grass, seaweed; louisiana-water-thrush ingest water-
insect; young-bird ingest earthworm, insect, small
animal;
Which birds eat nectar? hummingbird ingest nectar
What kinds of insect eaters are there? chickadee,
creeper, flycatcher, kinglet, swallow, swift, thrasher,
titmice, vireo, warbler, woodpecker, owl
What is gravel? I don&apos;t know, but I know that: bird
ingest gravel &lt; related-to &gt; bird *assist* grinding-
</bodyText>
<table confidence="0.980711142857143">
Sel Rel Irrel Miss Parse Interp Concepts Time. Date
Bird 145 91 54 23 64% 55% 325 441 secs 12/93
Bat 27 26 1 9 93% 60% 118 79 secs 4/94
Monkey 22 8 14 4 73% 75% 72 79 secs 4/94
Mouse 29 21 8 3 79% 50% 123 93 secs 4/94
* this time includes skirruning, parsing, interpreting, forming, and integrating
on a SPARC Classic machine (microSPARC 50MH. CPU).
</table>
<page confidence="0.996004">
88
</page>
<bodyText confidence="0.964998052631579">
process
Do most cactus dwellers eat insects? yes
What kills birds? eagle is-a bird, and hunters and
trappers kill eagles; osprey is-a bird, and hunters
and trappers kill osprey
When do most birds search for food? at-time day
Do birds help people? yes, bird help farmer
How do birds help farmers? bird ingest &lt;insect
which ingest crop&gt;; bird ingest weed-seed
Do bats eat blood? yes, some bat eat blood because
vampire-bat is-a bat and vampire-bat ingest blood
How much blood do vampire bats eat? vampire-bat
ingest blood quantity 1 tablespoon *frequency* day
Do vampire bats attack human beings? yes, vampire
bat harm human *frequency* sometimes
Do monkeys have enemies? yes, some monkey has-
enemy cheetah hyena jackal leopard lion because
&lt;monkeys inhabit at-lc ground&gt; has-enemy chee-
tah hyena jackal leopard lion
</bodyText>
<sectionHeader confidence="0.999272" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.9999743">
In (Sondheimer et al., 1984), frame-like structures,
KL-ONE structures in fact, are also used to guide
semantic interpretation in an application domain.
However, the overall approach to the interpreta-
tion task presented here differs from that work. In
approaching the problem of unrestricted texts, we
agree with those researchers (Hobbs, 1991; Grish-
man et al., 1991) who think that it is possible to
build correct parses and interpretations for real-
world texts. In fact, it is hard for us to see how
statistical methods (de Marcken, 1990; Church et
al., 1991) could be used for building knowledge-bases
with sufficient expressive power to correctly answer
questions posed by expert systems or human users.
We think that the same critique applies to skim-
mers (Lehnert et al., 1991), but for very different
reasons. In order to guarantee the correctness of
the knowledge-base built, every element in the sen-
tence needs to be interpreted. For instance, if the
adverb &amp;quot;mostly&amp;quot; is not interpreted in the sentence
These owls eat mostly rodents, the integrity of the
knowledge-base built is not going to suffer greatly.
But, if we are talking about the adverb &amp;quot;rarely&amp;quot; in
the sentence These owls rarely eat rodents, the situ-
ation becomes much more serious, as we found out.
This work has advanced a new approach to se-
mantic interpretation that occupies a middle ground
between those approaches that rely heavily on the
parser for building structures and attaching PPs,
subordinate clauses, etc. (Grishman et al., 1991;
Hobbs, 1978; Tomita, 1985) and semantic-centered
approaches (Riesbeck and Martin, 1986; Cardie and
Lehnert, 1991; Slator and Wilks, 1991). Our parser
delegates all the burden of dealing with structural
ambiguity (attachment of PPs, relative clauses, sub-
ordinate clauses, etc.) to the interpreter. That is one
of the reasons why it is so fast. The interpreter has a
very sophisticated algorithm that uses the informa-
tion built in the verbal concepts in order to attach
PPs. Yet, if the parser does not build a parse, albeit
a shallow one, the interpreter will not know what
to do. Moreover, the interpreter does not question
the parser when it says this constituent is an obj, or
subj, or a time-np, etc. This is a situation that we
are not happy about, because the parser identifies
some constituents incorrectly, especially the time-
np. We are studying mechanisms under which the
interpreter will override the parser and will get it
out of trouble in processing very complex sentences
(Krupka et al., 1991; Jacobs et al., 1991).
</bodyText>
<sectionHeader confidence="0.998425" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999912209302325">
We have presented a method for the acquisition of
knowledge from encyclopedic texts. The method de-
pends on understanding what is being read, which
in turn depends on: (1) providing a successful parse
and interpretation for a sentence, (2) building final
knowledge representation structures from the log-
ical form of the sentence, which involves creating
new concepts and relations as the system is reading,
and (3) integrating in LTM those concepts and rela-
tions that the recognizer algorithm fails to recognize,
which in many cases involves the reorganization of
concepts in LTM.
The results that are reported in this paper are very
encouraging, because a high percentage of the fail-
ures are due to some incomplete implementations of
some aspects of the system. For instance, in dealing
with anaphora we have incorporated in our system
some of the ideas reported in (Grosz, 1983; Hobbs,
1978), but our work is clearly insufficient in that
regard. A major hole in the interpreter, as of this
writing, is that it does not interpret comparatives,
except very simple ones like quantifiers, e.g., &amp;quot;more
than 2.&amp;quot; The interpreter needs to have mechanisms
in place to recover the elliptical elements in compara-
tives, which in many cases require solving extrasen-
tential reference, e.g., The golden eagle defends a
territory of about 20 to 60 square miles. The bald
eagle holds a smaller territory. The skimmer uses
very rudimentary techniques, and there is a lot of
room for improvement here. In any case, this has
not been a major concern of this research. Moreover,
because the system is so incredibly fast, if the skim-
mer overgenerates, it is not much of a problem. An
aspect related to the skimmer that we have no space
to discuss is that it became necessary to build an
algorithm to recognize subclasses of the class of ani-
mals being searched for every question. For instance,
if the question is Do sharks eat plankton, this algo-
rithm analyzes every sentence in the encyclopedic ar-
ticle, before being passed to the skimmer, searching
for NPs denoting subclasses of sharks. This is neces-
sary because the author may introduce the concept
&amp;quot;Mako sharks&amp;quot; in a context unrelated to the relation
</bodyText>
<page confidence="0.999297">
89
</page>
<bodyText confidence="0.999957823529412">
ingest, and consequently, the skimmer is not going
to select this sentence. Then, if the author later
on says Ma/cos feed on other fish, including herring,
mackerel, and swordfish, the system has no way to
relate &amp;quot;Mako&amp;quot; to sharks, missing the fact that sharks
feed on herring, mackerel, and swordfish. In June,
we tested the parser on 2900 sentences from 25 ar-
ticles, including non-animal articles such as black-
holes, cancer, computer chips, napoleon, greenhouse
effect, etc., and the rate of success was 76.8%. How-
ever, some sentences are still not parsed because
some subcategorizations of verbs are wrong or in-
complete, or the phrasal lexicon is incomplete. We
are confident that the parser may reach a plateau at
85% or 90% for this Encyclopedia. The remaining
10% or 15% may require considerable help from the
interpreter to be parsed.
</bodyText>
<sectionHeader confidence="0.998479" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999727943820225">
Cardie, C. and Lehnert, W. (1991). A cognitively
plausible approach to understanding complex syn-
tax. In Proc. AAAI-91, pp. 117-124, Anaheim,
CA, July 1991.
Church, K., Gale, W., Hanks, P., and Hindle, D.
(1991). Parsing, Word Associations, and Typi-
cal Predicate-Argument Relations. In Current Is-
sues in Parsing Technology, Tomita, Masaru, ed.,
Kluwer Academic Publishers, Boston, MA, pp.
103-112.
Gomez, F. (1989). WUP: A parser Based on Word
Usage. UCF-Tech-89-2, Dept. of Computer Sci-
ence, University of Central Florida, Orlando, Fl
32816, (Under Revision).
Gomez, F., and Segami, C. (1989). The Recogni-
tion and Integration of Concepts in Understand-
ing Scientific Texts. Journal of Experimental and
Theoretical Artificial Intelligence. 1, pp. 51-77.
Gomez, F., Segami, C., and Hull, R. (1993). Prepo-
sitional Attachment, Prepositional Meaning and
Determination of Primitives and Thematic Roles.
UCF Technical Report.
Gomez, F., and Segami, C. (1991) Classification-
Based Reasoning. IEEE Trans. on Systems, Man
and Cybernetics, 21, no. 3, pp. 644-659.
Grishman, R., Sterling, J., and Macleod, C. (1991).
New York University: Description of the PRO-
TEUS System as Used for MUC-3. Proc. 3rd Mes-
sage Understanding Conference (MUC-3), May,
1991, Morgan Kaufmann, pp. 3-16.
Grosz, B. J., Providing a Unified Account of Definite
Noun Phrase in Discourse, Proc. ACL, 1983, pp.
44-50.
Hobbs, J. R. (1991). SRI International: Descrip-
tion of the TACITUS System as Used for MUC-3.
Proc. 3rd Message Understanding Conf. (MUG-
3), May, May, 1991, Morgan Kaufmann, pp. 3-16.
Hobbs, J. R. (1978). Resolving pronoun refer-
ences. In Readings in Natural Language Process-
ing, Grosz, B., Jones, K. S., Webber, B., eds., pp
339-352, Morgan Kaufmann Publishers, Los Al-
tos, CA.
Jacobs, P. S., Krupka, G. R., Rau, L. F. (1991).
Lexico-semantic pattern matching as a companion
to parsing in text understanding. In Proc. DARPA
Speech and Natural Language Workshop, pp. 337-
341, Asilomar, CA.
Krupka, G., Jacobs, P., Rau, L., and Iwanska, L.
(1991). GE: Description of the NLToolset Sys-
tem as Used for MUC-3. Proc. 3rd Message Un-
derstanding Conf. (MUC-3), May, 1991, Morgan
Kaufmann, pp. 3-16.
Lehnert, W., Cardie, C., Fisher, D., Riloff, E., and
Williams, R. (1991). University of Massachusetts:
Description of the CIRCUS System as Used for
MUC-3. Proc. 3rd Message Understanding Conf.
(MUC-3), May, 1991, Morgan Kaufmann, pp. 3-
16.
de Marcken, C. G. (1990). Parsing the LOB cor-
pus. In Proc. ACL, pp. 243-251, Helsinki, Finland,
Aug. 1990.
Rich, E., Barnett, J., Wittenburg, K., and Wrob-
lewski, D. (1987). Ambiguity Procrastination.
AAAI-87, pp. 571-574.
Riesbeck, C. K., and Martin, C. E. (1986). Direct
Memory Access Parsing. Experience, Memory and
Reasoning, ed. Kolodner, J. and Riesbeck, C. K.,
Lawrence Erlbaum Associates, Hillsdale, N.J.
Slator, Brian M., and Wilks, Yorick. (1991).
PREMO: Parsing by conspicuous lexical con-
sumption. In Current Issues in Parsing Technol-
ogy, Tomita, Masaru, ed., Kluwer Academic Pub-
lishers, Boston, MA, pp. 85-102.
Schmolze, J., and Lipkis, T. (1983). Classification in
the KL-ONE Knowledge Representation System,
in Proc. IJCAI-83, Karlsruhe: IJCAI, pp. 330-
332, 1983.
Sondheimer, N. K., Weischedel, R. M., and Bobrow,
R. J. (1984). Semantic Interpretation Using KL-
ONE. COLING-84, pp. 101-107.
Sundheim, Beth M. (1992). Overview of the Fourth
Message Understanding Evaluation and Confer-
ence. Proc. 4th Message Understanding Conf.
(MUC-4), June, 1992, Morgan Kaufmann, pp. 3-
21.
Tomita, M. (1985). Efficient parsing for natural lan-
guage. Kluwer Academic Publishers, Boston, MA.
The World Book Encyclopedia. (1987). World Book,
Inc., Chicago.
</reference>
<page confidence="0.998637">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.631247">
<title confidence="0.999835">Acquiring Knowledge From Encyclopedic Texts*</title>
<author confidence="0.999977">Fernando Gomez</author>
<affiliation confidence="0.9997495">Dept. of Computer Science Univ. of Central Florida</affiliation>
<address confidence="0.998813">Orlando, FL 32816, USA</address>
<email confidence="0.999853">gomez@cs.ucf.edu</email>
<author confidence="0.999465">Richard Hull</author>
<affiliation confidence="0.9997475">Dept. of Computer Science Univ. of Central Florida</affiliation>
<address confidence="0.9986">Orlando, FL 32816, USA</address>
<email confidence="0.999603">hull@cs.ucf.edu</email>
<author confidence="0.996597">Carlos Segami</author>
<affiliation confidence="0.900821">Math. and Comp. Sci. Dept. Barry University</affiliation>
<address confidence="0.999217">Miami Shores, FL 33161, USA</address>
<email confidence="0.999854">segami@buvax.barry.edu</email>
<abstract confidence="0.98963895">A computational model for the acquisition of knowledge from encyclopedic texts is described. The model has been implemented in a program, called SNOWY, that reads texts from World Book Enacquires new concepts and conceptual relations about topics dealing with the dietary habits of animals, their classifications and habitats. The program is also able to answer an ample set of questions about the knowledge that it has acquired. This paper describes the essential components of this model, namely semantic interpretation, inferences and representation, and ends with an evaluation of the performance of the program, a sample of the questions that it is able to answer, and its relation to other programs of similar nature.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>W Lehnert</author>
</authors>
<title>A cognitively plausible approach to understanding complex syntax.</title>
<date>1991</date>
<booktitle>In Proc. AAAI-91,</booktitle>
<pages>117--124</pages>
<location>Anaheim, CA,</location>
<contexts>
<context position="28779" citStr="Cardie and Lehnert, 1991" startWordPosition="4652" endWordPosition="4655">entence These owls eat mostly rodents, the integrity of the knowledge-base built is not going to suffer greatly. But, if we are talking about the adverb &amp;quot;rarely&amp;quot; in the sentence These owls rarely eat rodents, the situation becomes much more serious, as we found out. This work has advanced a new approach to semantic interpretation that occupies a middle ground between those approaches that rely heavily on the parser for building structures and attaching PPs, subordinate clauses, etc. (Grishman et al., 1991; Hobbs, 1978; Tomita, 1985) and semantic-centered approaches (Riesbeck and Martin, 1986; Cardie and Lehnert, 1991; Slator and Wilks, 1991). Our parser delegates all the burden of dealing with structural ambiguity (attachment of PPs, relative clauses, subordinate clauses, etc.) to the interpreter. That is one of the reasons why it is so fast. The interpreter has a very sophisticated algorithm that uses the information built in the verbal concepts in order to attach PPs. Yet, if the parser does not build a parse, albeit a shallow one, the interpreter will not know what to do. Moreover, the interpreter does not question the parser when it says this constituent is an obj, or subj, or a time-np, etc. This is </context>
</contexts>
<marker>Cardie, Lehnert, 1991</marker>
<rawString>Cardie, C. and Lehnert, W. (1991). A cognitively plausible approach to understanding complex syntax. In Proc. AAAI-91, pp. 117-124, Anaheim, CA, July 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>W Gale</author>
<author>P Hanks</author>
<author>D Hindle</author>
</authors>
<title>Parsing, Word Associations, and Typical Predicate-Argument Relations.</title>
<date>1991</date>
<booktitle>In Current Issues in Parsing Technology,</booktitle>
<pages>103--112</pages>
<editor>Tomita, Masaru, ed.,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston, MA,</location>
<contexts>
<context position="27714" citStr="Church et al., 1991" startWordPosition="4482" endWordPosition="4485"> has-enemy cheetah hyena jackal leopard lion 7 Related Work In (Sondheimer et al., 1984), frame-like structures, KL-ONE structures in fact, are also used to guide semantic interpretation in an application domain. However, the overall approach to the interpretation task presented here differs from that work. In approaching the problem of unrestricted texts, we agree with those researchers (Hobbs, 1991; Grishman et al., 1991) who think that it is possible to build correct parses and interpretations for realworld texts. In fact, it is hard for us to see how statistical methods (de Marcken, 1990; Church et al., 1991) could be used for building knowledge-bases with sufficient expressive power to correctly answer questions posed by expert systems or human users. We think that the same critique applies to skimmers (Lehnert et al., 1991), but for very different reasons. In order to guarantee the correctness of the knowledge-base built, every element in the sentence needs to be interpreted. For instance, if the adverb &amp;quot;mostly&amp;quot; is not interpreted in the sentence These owls eat mostly rodents, the integrity of the knowledge-base built is not going to suffer greatly. But, if we are talking about the adverb &amp;quot;rarel</context>
</contexts>
<marker>Church, Gale, Hanks, Hindle, 1991</marker>
<rawString>Church, K., Gale, W., Hanks, P., and Hindle, D. (1991). Parsing, Word Associations, and Typical Predicate-Argument Relations. In Current Issues in Parsing Technology, Tomita, Masaru, ed., Kluwer Academic Publishers, Boston, MA, pp. 103-112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Gomez</author>
</authors>
<title>WUP: A parser Based on Word Usage.</title>
<date>1989</date>
<tech>UCF-Tech-89-2,</tech>
<institution>Dept. of Computer Science, University of Central Florida, Orlando, Fl 32816, (Under Revision).</institution>
<contexts>
<context position="4753" citStr="Gomez, 1989" startWordPosition="758" endWordPosition="759"> least for one of its clauses. The input to the interpre84 (ACTION (is-a (relation)) (subj (thing (actor))) (obj (thing (theme))) (adverb (time (at-time)) (negation (negation)) (Itogmm.f (PMP (in (Itm-ctgy (physical-thing (at-loc (weak))) (time-unit (at-time (strong))))) (at (him-ctgy (physical-thing (at-loc (strong))) (time-unit (at-time (stmng))))) (during (ltm-ctgy (physical-thing (at-time (strong))))) (with (Itm-ctgy (animate-body-part (instrument (strong)))) (state (state (strong))))) Figure 1: Organization of the Verbal Concepts tation phase is built by a top-down, lexical-driven parser(Gomez, 1989), which parses the sentences directly into syntactic cases. Prepositional phrases are left unattached inside the structure built by the parser. It is up to the interpreter to attach them, identify their meaning and the thematic roles that they may stand for. The parser is a deterministic machine, containing mechanisms for minimizing the need for backing up. The average amount of time in parsing a sentence from the encyclopedia is about one second. The parser has presently a lexicon of about 35,000 words. The rate of success of producing a full and correct parse of a sentence is, as of this wri</context>
</contexts>
<marker>Gomez, 1989</marker>
<rawString>Gomez, F. (1989). WUP: A parser Based on Word Usage. UCF-Tech-89-2, Dept. of Computer Science, University of Central Florida, Orlando, Fl 32816, (Under Revision).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Gomez</author>
<author>C Segami</author>
</authors>
<title>The Recognition and Integration of Concepts in Understanding Scientific Texts.</title>
<date>1989</date>
<journal>Journal of Experimental and Theoretical Artificial Intelligence.</journal>
<volume>1</volume>
<pages>51--77</pages>
<marker>Gomez, Segami, 1989</marker>
<rawString>Gomez, F., and Segami, C. (1989). The Recognition and Integration of Concepts in Understanding Scientific Texts. Journal of Experimental and Theoretical Artificial Intelligence. 1, pp. 51-77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Gomez</author>
<author>C Segami</author>
<author>R Hull</author>
</authors>
<title>Prepositional Attachment, Prepositional Meaning and Determination of Primitives and Thematic Roles.</title>
<date>1993</date>
<tech>UCF Technical Report.</tech>
<marker>Gomez, Segami, Hull, 1993</marker>
<rawString>Gomez, F., Segami, C., and Hull, R. (1993). Prepositional Attachment, Prepositional Meaning and Determination of Primitives and Thematic Roles. UCF Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Gomez</author>
<author>C Segami</author>
</authors>
<title>ClassificationBased Reasoning.</title>
<date>1991</date>
<journal>IEEE Trans. on Systems, Man and Cybernetics,</journal>
<volume>21</volume>
<pages>644--659</pages>
<contexts>
<context position="25349" citStr="Gomez and Segami, 1991" startWordPosition="4095" endWordPosition="4098">of natural language questions posed to the system after reading the bird, bat, and monkey articles and the contents of the system-generated answers. Note that the system output has been altered and condensed for the sake of brevity. In answering the questions, the system uses classification-based reasoning, not theorem proving. Many complex chains of inferences can be obtained by keeping memory organized in a principled manner. In those cases in which a question asks about a concept that does not exist in LTM, the classifier is activated to place that concept in LTM and obtain an answer. See (Gomez and Segami, 1991) for a detailed discussion of all these issues and the theorems proving the soundness of the inference algorithms. What do birds eat? sapsucker ingest tree-sap; hummingbird ingest nectar; duck ingest plant-matter, grass, seaweed; louisiana-water-thrush ingest waterinsect; young-bird ingest earthworm, insect, small animal; Which birds eat nectar? hummingbird ingest nectar What kinds of insect eaters are there? chickadee, creeper, flycatcher, kinglet, swallow, swift, thrasher, titmice, vireo, warbler, woodpecker, owl What is gravel? I don&apos;t know, but I know that: bird ingest gravel &lt; related-to </context>
</contexts>
<marker>Gomez, Segami, 1991</marker>
<rawString>Gomez, F., and Segami, C. (1991) ClassificationBased Reasoning. IEEE Trans. on Systems, Man and Cybernetics, 21, no. 3, pp. 644-659.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>J Sterling</author>
<author>C Macleod</author>
</authors>
<date>1991</date>
<booktitle>Description of the PROTEUS System as Used for MUC-3. Proc. 3rd Message Understanding Conference (MUC-3),</booktitle>
<pages>3--16</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>New York University:</location>
<contexts>
<context position="27521" citStr="Grishman et al., 1991" startWordPosition="4446" endWordPosition="4450"> attack human beings? yes, vampire bat harm human *frequency* sometimes Do monkeys have enemies? yes, some monkey hasenemy cheetah hyena jackal leopard lion because &lt;monkeys inhabit at-lc ground&gt; has-enemy cheetah hyena jackal leopard lion 7 Related Work In (Sondheimer et al., 1984), frame-like structures, KL-ONE structures in fact, are also used to guide semantic interpretation in an application domain. However, the overall approach to the interpretation task presented here differs from that work. In approaching the problem of unrestricted texts, we agree with those researchers (Hobbs, 1991; Grishman et al., 1991) who think that it is possible to build correct parses and interpretations for realworld texts. In fact, it is hard for us to see how statistical methods (de Marcken, 1990; Church et al., 1991) could be used for building knowledge-bases with sufficient expressive power to correctly answer questions posed by expert systems or human users. We think that the same critique applies to skimmers (Lehnert et al., 1991), but for very different reasons. In order to guarantee the correctness of the knowledge-base built, every element in the sentence needs to be interpreted. For instance, if the adverb &amp;quot;m</context>
</contexts>
<marker>Grishman, Sterling, Macleod, 1991</marker>
<rawString>Grishman, R., Sterling, J., and Macleod, C. (1991). New York University: Description of the PROTEUS System as Used for MUC-3. Proc. 3rd Message Understanding Conference (MUC-3), May, 1991, Morgan Kaufmann, pp. 3-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
</authors>
<title>Providing a Unified Account of Definite Noun Phrase in Discourse,</title>
<date>1983</date>
<booktitle>Proc. ACL,</booktitle>
<pages>44--50</pages>
<contexts>
<context position="30588" citStr="Grosz, 1983" startWordPosition="4955" endWordPosition="4956">resentation structures from the logical form of the sentence, which involves creating new concepts and relations as the system is reading, and (3) integrating in LTM those concepts and relations that the recognizer algorithm fails to recognize, which in many cases involves the reorganization of concepts in LTM. The results that are reported in this paper are very encouraging, because a high percentage of the failures are due to some incomplete implementations of some aspects of the system. For instance, in dealing with anaphora we have incorporated in our system some of the ideas reported in (Grosz, 1983; Hobbs, 1978), but our work is clearly insufficient in that regard. A major hole in the interpreter, as of this writing, is that it does not interpret comparatives, except very simple ones like quantifiers, e.g., &amp;quot;more than 2.&amp;quot; The interpreter needs to have mechanisms in place to recover the elliptical elements in comparatives, which in many cases require solving extrasentential reference, e.g., The golden eagle defends a territory of about 20 to 60 square miles. The bald eagle holds a smaller territory. The skimmer uses very rudimentary techniques, and there is a lot of room for improvement </context>
</contexts>
<marker>Grosz, 1983</marker>
<rawString>Grosz, B. J., Providing a Unified Account of Definite Noun Phrase in Discourse, Proc. ACL, 1983, pp. 44-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>SRI International: Description of the TACITUS System as Used for MUC-3.</title>
<date>1991</date>
<booktitle>Proc. 3rd Message Understanding Conf. (MUG3),</booktitle>
<pages>3--16</pages>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="27497" citStr="Hobbs, 1991" startWordPosition="4444" endWordPosition="4445"> vampire bats attack human beings? yes, vampire bat harm human *frequency* sometimes Do monkeys have enemies? yes, some monkey hasenemy cheetah hyena jackal leopard lion because &lt;monkeys inhabit at-lc ground&gt; has-enemy cheetah hyena jackal leopard lion 7 Related Work In (Sondheimer et al., 1984), frame-like structures, KL-ONE structures in fact, are also used to guide semantic interpretation in an application domain. However, the overall approach to the interpretation task presented here differs from that work. In approaching the problem of unrestricted texts, we agree with those researchers (Hobbs, 1991; Grishman et al., 1991) who think that it is possible to build correct parses and interpretations for realworld texts. In fact, it is hard for us to see how statistical methods (de Marcken, 1990; Church et al., 1991) could be used for building knowledge-bases with sufficient expressive power to correctly answer questions posed by expert systems or human users. We think that the same critique applies to skimmers (Lehnert et al., 1991), but for very different reasons. In order to guarantee the correctness of the knowledge-base built, every element in the sentence needs to be interpreted. For in</context>
</contexts>
<marker>Hobbs, 1991</marker>
<rawString>Hobbs, J. R. (1991). SRI International: Description of the TACITUS System as Used for MUC-3. Proc. 3rd Message Understanding Conf. (MUG3), May, May, 1991, Morgan Kaufmann, pp. 3-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>Resolving pronoun references.</title>
<date>1978</date>
<booktitle>In Readings in Natural Language Processing,</booktitle>
<pages>339--352</pages>
<editor>Grosz, B., Jones, K. S., Webber, B., eds.,</editor>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>Los Altos, CA.</location>
<contexts>
<context position="28678" citStr="Hobbs, 1978" startWordPosition="4641" endWordPosition="4642">eeds to be interpreted. For instance, if the adverb &amp;quot;mostly&amp;quot; is not interpreted in the sentence These owls eat mostly rodents, the integrity of the knowledge-base built is not going to suffer greatly. But, if we are talking about the adverb &amp;quot;rarely&amp;quot; in the sentence These owls rarely eat rodents, the situation becomes much more serious, as we found out. This work has advanced a new approach to semantic interpretation that occupies a middle ground between those approaches that rely heavily on the parser for building structures and attaching PPs, subordinate clauses, etc. (Grishman et al., 1991; Hobbs, 1978; Tomita, 1985) and semantic-centered approaches (Riesbeck and Martin, 1986; Cardie and Lehnert, 1991; Slator and Wilks, 1991). Our parser delegates all the burden of dealing with structural ambiguity (attachment of PPs, relative clauses, subordinate clauses, etc.) to the interpreter. That is one of the reasons why it is so fast. The interpreter has a very sophisticated algorithm that uses the information built in the verbal concepts in order to attach PPs. Yet, if the parser does not build a parse, albeit a shallow one, the interpreter will not know what to do. Moreover, the interpreter does </context>
<context position="30602" citStr="Hobbs, 1978" startWordPosition="4957" endWordPosition="4958">tructures from the logical form of the sentence, which involves creating new concepts and relations as the system is reading, and (3) integrating in LTM those concepts and relations that the recognizer algorithm fails to recognize, which in many cases involves the reorganization of concepts in LTM. The results that are reported in this paper are very encouraging, because a high percentage of the failures are due to some incomplete implementations of some aspects of the system. For instance, in dealing with anaphora we have incorporated in our system some of the ideas reported in (Grosz, 1983; Hobbs, 1978), but our work is clearly insufficient in that regard. A major hole in the interpreter, as of this writing, is that it does not interpret comparatives, except very simple ones like quantifiers, e.g., &amp;quot;more than 2.&amp;quot; The interpreter needs to have mechanisms in place to recover the elliptical elements in comparatives, which in many cases require solving extrasentential reference, e.g., The golden eagle defends a territory of about 20 to 60 square miles. The bald eagle holds a smaller territory. The skimmer uses very rudimentary techniques, and there is a lot of room for improvement here. In any c</context>
</contexts>
<marker>Hobbs, 1978</marker>
<rawString>Hobbs, J. R. (1978). Resolving pronoun references. In Readings in Natural Language Processing, Grosz, B., Jones, K. S., Webber, B., eds., pp 339-352, Morgan Kaufmann Publishers, Los Altos, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Jacobs</author>
<author>G R Krupka</author>
<author>L F Rau</author>
</authors>
<title>Lexico-semantic pattern matching as a companion to parsing in text understanding.</title>
<date>1991</date>
<booktitle>In Proc. DARPA Speech and Natural Language Workshop,</booktitle>
<pages>337--341</pages>
<location>Asilomar, CA.</location>
<contexts>
<context position="29694" citStr="Jacobs et al., 1991" startWordPosition="4808" endWordPosition="4811"> information built in the verbal concepts in order to attach PPs. Yet, if the parser does not build a parse, albeit a shallow one, the interpreter will not know what to do. Moreover, the interpreter does not question the parser when it says this constituent is an obj, or subj, or a time-np, etc. This is a situation that we are not happy about, because the parser identifies some constituents incorrectly, especially the timenp. We are studying mechanisms under which the interpreter will override the parser and will get it out of trouble in processing very complex sentences (Krupka et al., 1991; Jacobs et al., 1991). 8 Conclusions We have presented a method for the acquisition of knowledge from encyclopedic texts. The method depends on understanding what is being read, which in turn depends on: (1) providing a successful parse and interpretation for a sentence, (2) building final knowledge representation structures from the logical form of the sentence, which involves creating new concepts and relations as the system is reading, and (3) integrating in LTM those concepts and relations that the recognizer algorithm fails to recognize, which in many cases involves the reorganization of concepts in LTM. The </context>
</contexts>
<marker>Jacobs, Krupka, Rau, 1991</marker>
<rawString>Jacobs, P. S., Krupka, G. R., Rau, L. F. (1991). Lexico-semantic pattern matching as a companion to parsing in text understanding. In Proc. DARPA Speech and Natural Language Workshop, pp. 337-341, Asilomar, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Krupka</author>
<author>P Jacobs</author>
<author>L Rau</author>
<author>L Iwanska</author>
</authors>
<title>GE: Description of the NLToolset System as Used for MUC-3.</title>
<date>1991</date>
<booktitle>Proc. 3rd Message Understanding Conf. (MUC-3),</booktitle>
<pages>3--16</pages>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="29672" citStr="Krupka et al., 1991" startWordPosition="4804" endWordPosition="4807">gorithm that uses the information built in the verbal concepts in order to attach PPs. Yet, if the parser does not build a parse, albeit a shallow one, the interpreter will not know what to do. Moreover, the interpreter does not question the parser when it says this constituent is an obj, or subj, or a time-np, etc. This is a situation that we are not happy about, because the parser identifies some constituents incorrectly, especially the timenp. We are studying mechanisms under which the interpreter will override the parser and will get it out of trouble in processing very complex sentences (Krupka et al., 1991; Jacobs et al., 1991). 8 Conclusions We have presented a method for the acquisition of knowledge from encyclopedic texts. The method depends on understanding what is being read, which in turn depends on: (1) providing a successful parse and interpretation for a sentence, (2) building final knowledge representation structures from the logical form of the sentence, which involves creating new concepts and relations as the system is reading, and (3) integrating in LTM those concepts and relations that the recognizer algorithm fails to recognize, which in many cases involves the reorganization of</context>
</contexts>
<marker>Krupka, Jacobs, Rau, Iwanska, 1991</marker>
<rawString>Krupka, G., Jacobs, P., Rau, L., and Iwanska, L. (1991). GE: Description of the NLToolset System as Used for MUC-3. Proc. 3rd Message Understanding Conf. (MUC-3), May, 1991, Morgan Kaufmann, pp. 3-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
<author>C Cardie</author>
<author>D Fisher</author>
<author>E Riloff</author>
<author>R Williams</author>
</authors>
<date>1991</date>
<booktitle>University of Massachusetts: Description of the CIRCUS System as Used for MUC-3. Proc. 3rd Message Understanding Conf. (MUC-3),</booktitle>
<pages>3--16</pages>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="27935" citStr="Lehnert et al., 1991" startWordPosition="4517" endWordPosition="4520">, the overall approach to the interpretation task presented here differs from that work. In approaching the problem of unrestricted texts, we agree with those researchers (Hobbs, 1991; Grishman et al., 1991) who think that it is possible to build correct parses and interpretations for realworld texts. In fact, it is hard for us to see how statistical methods (de Marcken, 1990; Church et al., 1991) could be used for building knowledge-bases with sufficient expressive power to correctly answer questions posed by expert systems or human users. We think that the same critique applies to skimmers (Lehnert et al., 1991), but for very different reasons. In order to guarantee the correctness of the knowledge-base built, every element in the sentence needs to be interpreted. For instance, if the adverb &amp;quot;mostly&amp;quot; is not interpreted in the sentence These owls eat mostly rodents, the integrity of the knowledge-base built is not going to suffer greatly. But, if we are talking about the adverb &amp;quot;rarely&amp;quot; in the sentence These owls rarely eat rodents, the situation becomes much more serious, as we found out. This work has advanced a new approach to semantic interpretation that occupies a middle ground between those appr</context>
</contexts>
<marker>Lehnert, Cardie, Fisher, Riloff, Williams, 1991</marker>
<rawString>Lehnert, W., Cardie, C., Fisher, D., Riloff, E., and Williams, R. (1991). University of Massachusetts: Description of the CIRCUS System as Used for MUC-3. Proc. 3rd Message Understanding Conf. (MUC-3), May, 1991, Morgan Kaufmann, pp. 3-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C G de Marcken</author>
</authors>
<title>Parsing the LOB corpus.</title>
<date>1990</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>243--251</pages>
<location>Helsinki, Finland,</location>
<marker>de Marcken, 1990</marker>
<rawString>de Marcken, C. G. (1990). Parsing the LOB corpus. In Proc. ACL, pp. 243-251, Helsinki, Finland, Aug. 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rich</author>
<author>J Barnett</author>
<author>K Wittenburg</author>
<author>D Wroblewski</author>
</authors>
<date>1987</date>
<booktitle>Ambiguity Procrastination. AAAI-87,</booktitle>
<pages>571--574</pages>
<marker>Rich, Barnett, Wittenburg, Wroblewski, 1987</marker>
<rawString>Rich, E., Barnett, J., Wittenburg, K., and Wroblewski, D. (1987). Ambiguity Procrastination. AAAI-87, pp. 571-574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Riesbeck</author>
<author>C E Martin</author>
</authors>
<title>Direct Memory Access Parsing. Experience, Memory and Reasoning,</title>
<date>1986</date>
<editor>ed. Kolodner, J. and Riesbeck, C. K., Lawrence</editor>
<location>Hillsdale, N.J.</location>
<contexts>
<context position="28753" citStr="Riesbeck and Martin, 1986" startWordPosition="4648" endWordPosition="4651">is not interpreted in the sentence These owls eat mostly rodents, the integrity of the knowledge-base built is not going to suffer greatly. But, if we are talking about the adverb &amp;quot;rarely&amp;quot; in the sentence These owls rarely eat rodents, the situation becomes much more serious, as we found out. This work has advanced a new approach to semantic interpretation that occupies a middle ground between those approaches that rely heavily on the parser for building structures and attaching PPs, subordinate clauses, etc. (Grishman et al., 1991; Hobbs, 1978; Tomita, 1985) and semantic-centered approaches (Riesbeck and Martin, 1986; Cardie and Lehnert, 1991; Slator and Wilks, 1991). Our parser delegates all the burden of dealing with structural ambiguity (attachment of PPs, relative clauses, subordinate clauses, etc.) to the interpreter. That is one of the reasons why it is so fast. The interpreter has a very sophisticated algorithm that uses the information built in the verbal concepts in order to attach PPs. Yet, if the parser does not build a parse, albeit a shallow one, the interpreter will not know what to do. Moreover, the interpreter does not question the parser when it says this constituent is an obj, or subj, o</context>
</contexts>
<marker>Riesbeck, Martin, 1986</marker>
<rawString>Riesbeck, C. K., and Martin, C. E. (1986). Direct Memory Access Parsing. Experience, Memory and Reasoning, ed. Kolodner, J. and Riesbeck, C. K., Lawrence Erlbaum Associates, Hillsdale, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian M Slator</author>
<author>Yorick Wilks</author>
</authors>
<title>PREMO: Parsing by conspicuous lexical consumption.</title>
<date>1991</date>
<booktitle>In Current Issues in Parsing Technology,</booktitle>
<pages>85--102</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Tomita, Masaru, ed.,</location>
<contexts>
<context position="28804" citStr="Slator and Wilks, 1991" startWordPosition="4656" endWordPosition="4659">tly rodents, the integrity of the knowledge-base built is not going to suffer greatly. But, if we are talking about the adverb &amp;quot;rarely&amp;quot; in the sentence These owls rarely eat rodents, the situation becomes much more serious, as we found out. This work has advanced a new approach to semantic interpretation that occupies a middle ground between those approaches that rely heavily on the parser for building structures and attaching PPs, subordinate clauses, etc. (Grishman et al., 1991; Hobbs, 1978; Tomita, 1985) and semantic-centered approaches (Riesbeck and Martin, 1986; Cardie and Lehnert, 1991; Slator and Wilks, 1991). Our parser delegates all the burden of dealing with structural ambiguity (attachment of PPs, relative clauses, subordinate clauses, etc.) to the interpreter. That is one of the reasons why it is so fast. The interpreter has a very sophisticated algorithm that uses the information built in the verbal concepts in order to attach PPs. Yet, if the parser does not build a parse, albeit a shallow one, the interpreter will not know what to do. Moreover, the interpreter does not question the parser when it says this constituent is an obj, or subj, or a time-np, etc. This is a situation that we are n</context>
</contexts>
<marker>Slator, Wilks, 1991</marker>
<rawString>Slator, Brian M., and Wilks, Yorick. (1991). PREMO: Parsing by conspicuous lexical consumption. In Current Issues in Parsing Technology, Tomita, Masaru, ed., Kluwer Academic Publishers, Boston, MA, pp. 85-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schmolze</author>
<author>T Lipkis</author>
</authors>
<title>Classification in the KL-ONE Knowledge Representation System, in</title>
<date>1983</date>
<booktitle>Proc. IJCAI-83, Karlsruhe: IJCAI,</booktitle>
<pages>330--332</pages>
<contexts>
<context position="16771" citStr="Schmolze and Lipkis, 1983" startWordPosition="2694" endWordPosition="2697">interpreter is passed to the forproceeds by attempting to determine the meaning mation phase that builds the final knowledge repof pairs of items in the complex noun, utilizing a resentation structures. These are in turn integrated scheme that combines the items in the complex noun into LTM upon activating a recognizer algorithm and from left to right. For example, in the interpretation an integration algorithm both of which make extenof &amp;quot;big red wine bottle&amp;quot; an attempt is made to find sive use of a classifier similar to the one reported a meaning for the terms &amp;quot;big red,&amp;quot; &amp;quot;big wine,&amp;quot; &amp;quot;big in (Schmolze and Lipkis, 1983). The construction of bottle,&amp;quot; &amp;quot;red wine,&amp;quot; &amp;quot;red bottle&amp;quot; and &amp;quot;wine bottle.&amp;quot; the final knowledge representation structures is done If one item in the complex noun can be paired (i.e., as follows. The interpretation phase, if successful, a meaning can be found) with more than one other has built a relation, and a set of thematic roles for item in the complex noun, then the algorithm re- each sentence. Let us call the thematic roles of the turns more than one interpretation for the complex relation the entities for that relation. All the n ennoun, and disambiguation routines are activated. In titi</context>
</contexts>
<marker>Schmolze, Lipkis, 1983</marker>
<rawString>Schmolze, J., and Lipkis, T. (1983). Classification in the KL-ONE Knowledge Representation System, in Proc. IJCAI-83, Karlsruhe: IJCAI, pp. 330-332, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N K Sondheimer</author>
<author>R M Weischedel</author>
<author>R J Bobrow</author>
</authors>
<date>1984</date>
<booktitle>Semantic Interpretation Using KLONE. COLING-84,</booktitle>
<pages>101--107</pages>
<contexts>
<context position="27182" citStr="Sondheimer et al., 1984" startWordPosition="4396" endWordPosition="4399">s help people? yes, bird help farmer How do birds help farmers? bird ingest &lt;insect which ingest crop&gt;; bird ingest weed-seed Do bats eat blood? yes, some bat eat blood because vampire-bat is-a bat and vampire-bat ingest blood How much blood do vampire bats eat? vampire-bat ingest blood quantity 1 tablespoon *frequency* day Do vampire bats attack human beings? yes, vampire bat harm human *frequency* sometimes Do monkeys have enemies? yes, some monkey hasenemy cheetah hyena jackal leopard lion because &lt;monkeys inhabit at-lc ground&gt; has-enemy cheetah hyena jackal leopard lion 7 Related Work In (Sondheimer et al., 1984), frame-like structures, KL-ONE structures in fact, are also used to guide semantic interpretation in an application domain. However, the overall approach to the interpretation task presented here differs from that work. In approaching the problem of unrestricted texts, we agree with those researchers (Hobbs, 1991; Grishman et al., 1991) who think that it is possible to build correct parses and interpretations for realworld texts. In fact, it is hard for us to see how statistical methods (de Marcken, 1990; Church et al., 1991) could be used for building knowledge-bases with sufficient expressi</context>
</contexts>
<marker>Sondheimer, Weischedel, Bobrow, 1984</marker>
<rawString>Sondheimer, N. K., Weischedel, R. M., and Bobrow, R. J. (1984). Semantic Interpretation Using KLONE. COLING-84, pp. 101-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth M Sundheim</author>
</authors>
<title>Overview of the Fourth Message Understanding Evaluation and Conference.</title>
<date>1992</date>
<booktitle>Proc. 4th Message Understanding Conf. (MUC-4),</booktitle>
<pages>3--21</pages>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="3396" citStr="Sundheim, 1992" startWordPosition="555" endWordPosition="556">the process of learning about the dietary habits of, say beetles, the program is told that they cultivate fungi, and the program is able to interpret that sentence, that knowledge will also be integrated in LTM. Consequently, our approach to understanding expository texts is a bottom-up approach in which final knowledge representation structures are built from the logical form of the sentences, without intervening scriptal or frame knowledge about the topic. Hence, our system does not start with a frame containing the main slots to be filled for a topic, say &amp;quot;diet,&amp;quot; as in recent MUG projects (Sundheim, 1992), but rather it will build everything relevant to diet from the output of the interpretation phase. Then, when we are talking about the topic, it will not make any difference if the sentences refer to what the animals eat, or what eats them. Every aspect dealing with the general idea of ingest can be analyzed and properly integrated into memory. Our corpora for testing our ideas has been The World Book Encyclopedia (World Book, 1987), which is one or two levels less complex than the Collier&apos;s Encyclopedia, which, in turn, is less complex than the Encyclopaedia Britannica. 2 Interpretation In o</context>
</contexts>
<marker>Sundheim, 1992</marker>
<rawString>Sundheim, Beth M. (1992). Overview of the Fourth Message Understanding Evaluation and Conference. Proc. 4th Message Understanding Conf. (MUC-4), June, 1992, Morgan Kaufmann, pp. 3-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>Efficient parsing for natural language.</title>
<date>1985</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="28693" citStr="Tomita, 1985" startWordPosition="4643" endWordPosition="4644">terpreted. For instance, if the adverb &amp;quot;mostly&amp;quot; is not interpreted in the sentence These owls eat mostly rodents, the integrity of the knowledge-base built is not going to suffer greatly. But, if we are talking about the adverb &amp;quot;rarely&amp;quot; in the sentence These owls rarely eat rodents, the situation becomes much more serious, as we found out. This work has advanced a new approach to semantic interpretation that occupies a middle ground between those approaches that rely heavily on the parser for building structures and attaching PPs, subordinate clauses, etc. (Grishman et al., 1991; Hobbs, 1978; Tomita, 1985) and semantic-centered approaches (Riesbeck and Martin, 1986; Cardie and Lehnert, 1991; Slator and Wilks, 1991). Our parser delegates all the burden of dealing with structural ambiguity (attachment of PPs, relative clauses, subordinate clauses, etc.) to the interpreter. That is one of the reasons why it is so fast. The interpreter has a very sophisticated algorithm that uses the information built in the verbal concepts in order to attach PPs. Yet, if the parser does not build a parse, albeit a shallow one, the interpreter will not know what to do. Moreover, the interpreter does not question th</context>
</contexts>
<marker>Tomita, 1985</marker>
<rawString>Tomita, M. (1985). Efficient parsing for natural language. Kluwer Academic Publishers, Boston, MA.</rawString>
</citation>
<citation valid="true">
<title>The World Book Encyclopedia.</title>
<date>1987</date>
<publisher>World Book, Inc.,</publisher>
<location>Chicago.</location>
<marker>1987</marker>
<rawString>The World Book Encyclopedia. (1987). World Book, Inc., Chicago.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>