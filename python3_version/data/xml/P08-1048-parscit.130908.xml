<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000553">
<title confidence="0.97379">
Evaluating Roget’s Thesauri
</title>
<author confidence="0.994666">
Alistair Kennedy
</author>
<affiliation confidence="0.96363825">
School of Information Technology
and Engineering
University of Ottawa
Ottawa, Ontario, Canada
</affiliation>
<email confidence="0.994269">
akennedy@site.uottawa.ca
</email>
<sectionHeader confidence="0.995574" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999015">
Roget’s Thesaurus has gone through many re-
visions since it was first published 150 years
ago. But how do these revisions affect Ro-
get’s usefulness for NLP? We examine the
differences in content between the 1911 and
1987 versions of Roget’s, and we test both ver-
sions with each other and WordNet on prob-
lems such as synonym identification and word
relatedness. We also present a novel method
for measuring sentence relatedness that can be
implemented in either version of Roget’s or in
WordNet. Although the 1987 version of the
Thesaurus is better, we show that the 1911 ver-
sion performs surprisingly well and that often
the differences between the versions of Ro-
get’s and WordNet are not statistically signif-
icant. We hope that this work will encourage
others to use the 1911 Roget’s Thesaurus in
NLP tasks.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998629076923077">
Roget’s Thesaurus, first introduced over 150 years
ago, has gone through many revisions to reach its
current state. We compare two versions, the 1987
and 1911 editions of the Thesaurus with each other
and with WordNet 3.0. Roget’s Thesaurus has a
unique structure, quite different from WordNet, of
which the NLP community has yet to take full ad-
vantage. In this paper we demonstrate that although
the 1911 version of the Thesaurus is very old, it can
give results comparable to systems that use WordNet
or newer versions of Roget’s Thesaurus.
The main motivation for working with the 1911
Thesaurus instead of newer versions is that it is in
</bodyText>
<author confidence="0.801419">
Stan Szpakowicz
</author>
<affiliation confidence="0.81610575">
School of Information Technology
and Engineering
University of Ottawa
Ottawa, Ontario, Canada
and
Institute of Computer Science
Polish Academy of Sciences
Warsaw, Poland
</affiliation>
<email confidence="0.976063">
szpak@site.uottawa.ca
</email>
<bodyText confidence="0.996308885714286">
the public domain, along with related NLP-oriented
software packages. For applications that call for an
NLP-friendly thesaurus, WordNet has become the
de-facto standard. Although WordNet is a fine re-
sources, we believe that ignoring other thesauri is
a serious oversight. We show on three applications
how useful the 1911 Thesaurus is. We ran the well-
established tasks of determining semantic related-
ness of pairs of terms and identifying synonyms (Jar-
masz and Szpakowicz, 2004). We also proposed
a new method of representing the meaning of sen-
tences or other short texts using either WordNet or
Roget’s Thesaurus, and tested it on the data set pro-
vided by Li et al. (2006). We hope that this work
will encourage others to use Roget’s Thesaurus in
their own NLP tasks.
Previous research on the 1987 version of Roget’s
Thesaurus includes work of Jarmasz and Szpakow-
icz (2004). They propose a method of determin-
ing semantic relatedness between pairs of terms.
Terms that appear closer together in the Thesaurus
get higher weights than those farther apart. The
experiments aimed at identifying synonyms using
a modified version of the proposed semantic sim-
ilarity function. Similar experiments were carried
out using WordNet in combination with a variety of
semantic relatedness functions. Roget’s Thesaurus
was found generally to outperform WordNet on these
problems. We have run similar experiments using
the 1911Thesaurus.
Lexical chains have also been developed using the
1987 Roget’s Thesaurus (Jarmasz and Szpakowicz,
2003). The procedure maps words in a text to the
Head (a Roget’s concept) from which they are most
likely to come. Although we did not experiment
</bodyText>
<page confidence="0.984023">
416
</page>
<note confidence="0.714264">
Proceedings of ACL-08: HLT, pages 416–424,
</note>
<page confidence="0.538077">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.99994024137931">
with lexical chains here, they were an inspiration for
our sentence relatedness function.
Roget’s Thesaurus does not explicitly label the
relations between its terms, as WordNet does. In-
stead, it groups terms together with implied rela-
tions. Kennedy and Szpakowicz (2007) show how
disambiguating one of these relations, hypernymy,
can help improve the semantic similarity functions
in (Jarmasz and Szpakowicz, 2004). These hyper-
nym relations were also put towards solving analogy
questions.
This is not the first time the 1911 version of Ro-
get’s Thesaurus has been used in NLP research. Cas-
sidy (2000) used it to build the semantic network
FACTOTUM. This required significant (manual) re-
structuring, so FACTOTUM cannot really be con-
sidered a true version of Roget’s Thesaurus.
The 1987 data come from Penguin’s Roget’s The-
saurus (Kirkpatrick, 1987). The 1911 version is
available from Project Gutenberg1. We use WordNet
3.0, the latest version (Fellbaum, 1998). In the ex-
periments we present here, we worked with an inter-
face to Roget’s Thesaurus implemented in Java 5.02.
It is built around a large index which stores the lo-
cation in the thesaurus of each word or phrase; the
system individually indexes all words within each
phrase, as well as the phrase itself. This was shown
to improve results in a few applications, which we
will discuss later in the paper.
</bodyText>
<sectionHeader confidence="0.9750755" genericHeader="method">
2 Content comparison of the 1911 and
1987 Thesauri
</sectionHeader>
<bodyText confidence="0.999901285714286">
Although the 1987 and 1911 Thesauri are very sim-
ilar in structure, there are a few differences, among
them, the number of levels and the number of parts-
of-speech represented. For example, the 1911 ver-
sion contains some pronouns as well as more sec-
tions dedicated to phrases.
There are nine levels in Roget’s Thesaurus hierar-
chy, from Class down to Word. We show them in
Table 1 along with the counts of instances of each
level. An example of a Class in the 1911 Thesaurus
is “Words Expressing Abstract Relations”, a Section
in that Class is “Quantity” with a Subsection “Com-
parative Quantity”. Heads can be thought of as the
heart of the Thesaurus because it is at this level that
</bodyText>
<footnote confidence="0.9998795">
1http://www.gutenberg.org/ebooks/22
2http://rogets.site.uottawa.ca/
</footnote>
<table confidence="0.999387272727273">
Hierarchy 1911 1987
Class 8 8
Section 39 39
Subsection 97 95
Head Group 625 596
Head 1044 990
Part-of-speech 3934 3220
Paragraph 10244 6443
Semicolon Group 43196 59915
Total Words 98924 225124
Unique Words 59768 100470
</table>
<tableCaption confidence="0.995672">
Table 1: Frequencies of each level of the hierarchy in the
1911 and 1987 Thesauri.
</tableCaption>
<bodyText confidence="0.99991496969697">
the lexical material, organized into approximately a
thousand concepts, resides. Head Groups often pair
up opposites, for example Head #1 “Existence” and
Head #2 “Nonexistence” are found in the same Head
Group in both versions of the Thesaurus. Terms in
the Thesaurus may be labelled with cross-references
to other words in different Heads. We did not use
these references in our experiments.
The part-of-speech level is a little confusing, since
clearly no such grouping contains an exhaustive list
of all nouns, all verbs etc. We will write “POS” to in-
dicate a structure in Roget’s and “part-of-speech” to
indicate the word category in general. The four main
parts-of-speech represented in a POS are nouns,
verbs, adjectives and adverbs. Interjections are also
included in both the 1911 and 1987 thesauri; they are
usually phrases followed by an exclamation mark,
such as “for God’s sake!” and “pshaw!”. The Para-
graph and Semicolon Group are not given names,
but can often be represented by the first word.
The 1911 version also contains phrases (mostly
quotations), prefixes and pronouns. There are only
three prefixes – “tri-”, “tris-”, “laevo-” – and six pro-
nouns – “he”, “him”, “his”, “she”, “her”, “hers”.
Table 2 shows the frequency of paragraphs, semi-
colon groups and both total and unique words in a
given type of POS. Many terms occur both in the
1911 and 1987 Thesauri, but many more are unique
to either. Surprisingly, quite a few 1911 terms do not
appear in the 1987 data, as shown in Table 3; many
of them may have been considered obsolete and thus
dropped from the 1987 version. For example “in-
grafted” appears in the same semicolon group as
</bodyText>
<page confidence="0.997334">
417
</page>
<table confidence="0.9999554375">
POS Paragraph Semicolon Grp
1911 1987 1911 1987
Noun 4495 2884 19215 31174
Verb 2402 1499 10838 13958
Adjective 2080 1501 9097 12893
Adverb 594 499 2028 1825
Interjection 108 60 149 65
Phrase 561 0 1865 0
Total Word Unique Words
1911 1987 1911 1987
Noun 46308 114473 29793 56187
Verb 25295 55724 15150 24616
Adjective 20447 48802 12739 21614
Adverb 4039 5720 3016 4144
Interjection 598 405 484 383
Phrase 2228 0 2038 0
</table>
<tableCaption confidence="0.999196">
Table 2: Frequencies of paragraphs, semicolon groups,
total words and unique words by their part of speech; we
omitted prefixes and pronouns.
</tableCaption>
<table confidence="0.999740625">
POS Both Only 1911 Only 1987
All 35343 24425 65127
N. 18685 11108 37502
Vb. 8618 6532 15998
Adj. 8584 4155 13030
Adv. 1684 1332 2460
Int. 68 416 315
Phr. 0 2038 0
</table>
<tableCaption confidence="0.970240666666667">
Table 3: Frequencies of terms in either the 1911 or 1987
Thesaurus, and in both; we omitted prefixes and pro-
nouns.
</tableCaption>
<bodyText confidence="0.999916666666666">
“implanted” in the older but not the newer version.
Some mismatches may be due to small changes in
spelling, for example, “Nirvana” is capitalized in the
1911 version, but not in the 1987 version.
The lexical data in Project Gutenberg’s 1911 Ro-
get’s appear to have been somewhat added to. For
example, the citation “Go ahead, make my day!”
from the 1971 movie Dirty Harry appears twice (in
Heads #715-Defiance and #761-Prohibition) within
the Phrase POS. It is not clear to what extent new
terms have been added to the original 1911 Roget’s
Thesaurus, or what the criteria for adding such new
elements could have been.
In the end, there are many differences between the
1987 and 1911 Roget’s Thesauri, primarily in con-
tent rather than in structure. The 1987 Thesaurus is
largely an expansion of the 1911 version, with three
POSs (phrases, pronouns and prefixes) removed.
</bodyText>
<sectionHeader confidence="0.889931" genericHeader="method">
3 Comparison on applications
</sectionHeader>
<bodyText confidence="0.999881">
In this section we consider how the two versions of
Roget’s Thesaurus and WordNet perform in three ap-
plications – measuring word relatedness, synonym
identification, and sentence relatedness.
</bodyText>
<subsectionHeader confidence="0.999507">
3.1 Word relatedness
</subsectionHeader>
<bodyText confidence="0.999424454545454">
Relatedness can be measured by the closeness of the
words or phrases – henceforth referred to as terms –
in the structure of the thesaurus. Two terms in the
same semicolon group score 16, in the same para-
graph – 14, and so on (Jarmasz and Szpakowicz,
2004). The score is 0 if the terms appear in differ-
ent classes, or if either is missing. Pairs of terms get
higher scores for being closer together. When there
are multiple senses of two terms A and B, we want
to select senses a E A and b E B that maximize the
relatedness score. We define a distance function:
</bodyText>
<equation confidence="0.6605595">
semDist(A, B) = max 2 * (depth(lca(a, b)))
aEA,bEB
</equation>
<bodyText confidence="0.9998024">
lca is the lowest common ancestor and depth is the
depth in the Roget’s hierarchy; a Class has depth 0,
Section 1, ..., Semicolon Group 8. If we think of the
function as counting edges between concepts in the
Roget’s hierarchy, then it could also be written as:
</bodyText>
<equation confidence="0.99261">
semDist(A, B) = max
aEA,bEB
</equation>
<bodyText confidence="0.999277153846154">
We do not count links between words in the same
semicolon group, so in effect these methods find
distances between semicolon groups, that is to say,
these two functions will give the same results.
The 1911 and 1987 Thesauri were compared
with WordNet 3.0 on the three data sets contain-
ing pairs of words with manually assigned similarity
scores: 30 pairs (Miller and Charles, 1991), 65 pairs
(Rubenstein and Goodenough, 1965) and 353 pairs3
(Finkelstein et al., 2001). We assume that all terms
are nouns, so that we can have a fair comparison
of the two Thesauri with WordNet. We measure the
correlation with Pearson’s Correlation Coefficient.
</bodyText>
<footnote confidence="0.889317666666667">
3http://www.cs.technion.ac.il/˜gabr/resources/data/
wordsim353/wordsim353.html
16−edgesBetween(a, b)
</footnote>
<page confidence="0.974962">
418
</page>
<table confidence="0.999023875">
Year Miller &amp; Rubenstein &amp; Finkelstein
Charles Goodenough et. al
Index words and phrase
1911 0.7846 0.7313 0.3449
1987 0.7984 0.7865 0.4214
Index phrase only
1911 0.7090 0.7168 0.3373
1987 0.7471 0.7777 0.3924
</table>
<tableCaption confidence="0.9955595">
Table 4: Pearson’s coefficient values when not breaking /
breaking phrases up.
</tableCaption>
<bodyText confidence="0.999954371428571">
A preliminary experiment set out to determine
whether there is any advantage to indexing the words
in a phrase separately, for example, whether the
phrase “change of direction” should be indexed only
as a whole, or as all of “change”, “of”, “direction”
and “change of direction”. The outcome of this ex-
periment appears in Table 4. There is a clear im-
provement: breaking phrases up gives superior re-
sults on all three data sets, for both versions of Ro-
get’s. In the remaining experiments, we have each
word in a phrase indexed.
We compare the results for the 1911 and 1987
Roget’s Thesauri with a variety of WordNet-based
semantic relatedness measures – see Table 5. We
consider 10 measures, noted in the table as J&amp;C
(Jiang and Conrath, 1997), Resnik (Resnik, 1995),
Lin (Lin, 1998), W&amp;P (Wu and Palmer, 1994),
L&amp;C (Leacock and Chodorow, 1998), H&amp;SO (Hirst
and St-Onge, 1998), Path (counts edges between
synsets), Lesk (Banerjee and Pedersen, 2002), and
finally Vector and Vector Pair (Patwardhan, 2003).
The latter two work with large vectors of co-
occurring terms from a corpus, so WordNet is only
part of the system. We used Pedersen’s Semantic
Distance software package (Pedersen et al., 2004).
The results suggest that neither version of Ro-
get’s is best for these data sets. In fact, the Vector
method is superior on all three sets, and the Lesk
algorithm performs very closely to Roget’s 1987.
Even on the largest set (Finkelstein et al., 2001),
however, the differences between Roget’s Thesaurus
and the Vector method are not statistically signifi-
cant at the p &lt; 0.05 level for either thesaurus on
a two-tailed test4. The difference between the 1911
Thesaurus and Vector would be statistically signifi-
</bodyText>
<footnote confidence="0.941069">
4http://faculty.vassar.edu/lowry/rdiff.html
</footnote>
<table confidence="0.999896714285714">
Method Miller &amp; Rubenstein &amp; Finkelstein
Charles Goodenough et. al
1911 0.7846 0.7313 0.3449
1987 0.7984 0.7865 0.4214
J&amp;C 0.4735 0.5755 0.2273
Resnik 0.8060 0.8224 0.3531
Lin 0.7388 0.7264 0.2932
W&amp;P 0.7641 0.7973 0.2676
L&amp;C 0.7792 0.8387 0.3094
H&amp;SO 0.6668 0.7258 0.3548
Path 0.7550 0.7842 0.3744
Lesk 0.7954 0.7780 0.4220
Vector 0.8645 0.7929 0.4621
Vct Pair 0.5101 0.5810 0.3722
</table>
<tableCaption confidence="0.982606">
Table 5: Pearson’s coefficient values for three data sets
on a variety of relatedness functions.
</tableCaption>
<bodyText confidence="0.969975875">
cant at p &lt; 0.07.
On the (Miller and Charles, 1991) and (Ruben-
stein and Goodenough, 1965) data sets the best sys-
tem did not show a statistically significant improve-
ment over the 1911 or 1987 Roget’s Thesauri, even
at p &lt; 0.1 for a two-tailed test. These data sets are
too small for a meaningful comparison of systems
with close correlation scores.
</bodyText>
<subsectionHeader confidence="0.997577">
3.2 Synonym identification
</subsectionHeader>
<bodyText confidence="0.9998125">
In this problem we take a term q and we seek the
correct synonym s from a set C. There are two steps.
We used the system from (Jarmasz and Szpakowicz,
2004) for identifying synonyms with Roget’s. First
we find a set of terms B ⊆ C with the maximum
relatedness between q and each term x ∈ C:
</bodyText>
<equation confidence="0.9833515">
B = {x  |argmax semDist(x, q)}
XEC
</equation>
<bodyText confidence="0.984154666666667">
Next, we take the set of terms A ⊆ B where each
a ∈ A has the maximum number of shortest paths
between a and q.
</bodyText>
<equation confidence="0.886585">
A = {x  |argmax number5hortestPaths(x, q)}
XEB
</equation>
<bodyText confidence="0.5811408">
If s ∈ A and |A |= 1, the correct synonym has been
selected. Often the sets A and B will contain just
one item. If s ∈ A and |A |&gt; 1, there is a tie. If
s ∈� A then the selected synonyms are incorrect. If
a multi-word phrase c ∈ C of length n is not found,
</bodyText>
<page confidence="0.983438">
419
</page>
<table confidence="0.99990865">
ESL
Method Yes Tie No QNF ANF ONF
1911 27 3 20 0 3 3
1987 36 6 8 0 0 1
J&amp;C 30 4 16 4 4 10
Resnik 26 6 18 4 4 10
Lin 31 5 14 4 4 10
W&amp;P 31 6 13 4 4 10
L&amp;C 29 11 10 4 4 10
H&amp;SO 34 4 12 0 0 0
Path 30 11 9 4 4 10
Lesk 38 0 12 0 0 0
Vector 39 0 11 0 0 0
VctPair 40 0 10 0 0 0
TOEFL
1911 52 3 25 10 5 25
1987 59 7 14 4 4 17
J&amp;C 34 37 9 33 31 90
Resnik 37 37 6 33 31 90
Lin 33 41 6 33 31 90
W&amp;P 39 36 5 33 31 90
L&amp;C 38 36 6 33 31 90
H&amp;SO 60 16 4 1 0 1
Path 38 36 6 33 31 90
Lesk 70 1 9 1 0 1
Vector 69 1 10 1 0 1
VctPair 65 2 13 1 0 1
RDWP
1911 157 13 130 57 13 76
1987 198 17 85 22 5 17
J&amp;C 100 146 54 62 58 150
Resnik 114 114 72 62 58 150
Lin 94 160 46 62 58 150
W&amp;P 147 87 66 62 58 150
L&amp;C 149 93 58 62 58 150
H&amp;SO 170 82 48 4 6 5
Path 148 96 56 62 58 150
Lesk 220 7 73 4 6 5
Vector 216 7 73 4 6 5
VctPair 187 10 103 4 6 5
</table>
<tableCaption confidence="0.996085">
Table 6: Synonym selection experiments.
</tableCaption>
<bodyText confidence="0.999964757575758">
it is replaced by each of its words c1, c2..., c,,,, and
each of these words is considered in turn. The cz
that is closest to q is chosen to represent c. When
searching for a word in Roget’s or WordNet, we look
for all forms of the word.
The results of these experiments appear in Ta-
ble 6. “Yes” indicates correct answers, “No” – in-
correct answers, and “Tie” is for ties. QNF stands
for “Question word Not Found”, ANF for “Answer
word Not Found” and ONF for “Other word Not
Found”. We used three data sets for this applica-
tion: 80 questions taken from the Test of English as a
Foreign Language (TOEFL) (Landauer and Dumais,
1997), 50 questions – from the English as a Second
Language test (ESL) (Turney, 2001) and 300 ques-
tions – from the Reader’s Digest Word Power Game
(RDWP) (Lewis, 2000 and 2001).
Lesk and the Vector-based systems perform bet-
ter than all others, including Roget’s 1911 and 1987.
Even so, both versions of Roget’s Thesaurus per-
formed well, and were never worse than the worst
WordNet systems. In fact, six of the ten Word-
Net-based methods are consistently worse than the
1911 Thesaurus. Since the two Vector-based sys-
tems make use of additional data beyond WordNet,
Lesk is the only completely WordNet-based system
to outperform Roget’s 1987. One advantage of Ro-
get’s Thesaurus is that both versions generally have
fewer missing terms than WordNet, though Lesk,
Hirst &amp; St-Onge and the two vector based methods
had fewer missing terms than Roget’s. This may be
because the other WordNet methods will only work
for nouns and verbs.
</bodyText>
<subsectionHeader confidence="0.999329">
3.3 Sentence relatedness
</subsectionHeader>
<bodyText confidence="0.996376818181818">
Our final experiment concerns sentence relatedness.
We worked with a data set from (Li et al., 2006)5.
They took a subset of the term pairs from (Ruben-
stein and Goodenough, 1965) and chose sentences
to represent these terms; the sentences are defini-
tions from the Collins Cobuild dictionary (Sinclair,
2001). Thirty people were then asked to assign re-
latedness scores to these sentences, and the average
of these similarities was taken for each sentence.
Other methods of determining sentence seman-
tic relatedness expand term relatedness functions to
</bodyText>
<footnote confidence="0.9834235">
5http://www.docm.mmu.ac.uk/STAFF/D.McLean/
SentenceResults.htm
</footnote>
<page confidence="0.998094">
420
</page>
<bodyText confidence="0.99994475">
create a sentence relatedness function (Islam and
Inkpen, 2007; Mihalcea et al., 2006). We propose
to approach the task by exploiting in other ways the
commonalities in the structure of Roget’s Thesaurus
and of WordNet. We use the OpenNLP toolkit6 for
segmentation and part-of-speech tagging.
We use a method of sentence representation that
involves mapping the sentence into weighted con-
cepts in either Roget’s or WordNet. We mean a
concept in Roget’s to be either a Class, Section, ...,
Semicolon Group, while a concept in WordNet is any
synset. Essentially a concept is a grouping of words
from either resource. Concepts are weighted by two
criteria. The first is how frequently words from the
sentence appear in these concepts. The second is the
depth (or specificity) of the concept itself.
</bodyText>
<subsectionHeader confidence="0.922153">
3.3.1 Weighting based on word frequency
</subsectionHeader>
<bodyText confidence="0.999979090909091">
Each word and punctuation mark w in a sentence
is given a score of 1. (Naturally, only open-category
words will be found in the thesaurus.) If w has n
word senses w1, ..., wn, each sense gets a score of
1/n, so that 1/n is added to each concept in the
Roget’s hierarchy (semicolon group, paragraph, ...,
class) or WordNet hierarchy that contains wi. We
weight concepts in this way simply because, unable
to determine which sense is correct, we assume that
all senses are equally probable. Each concept in Ro-
get’s Thesaurus and WordNet gets the sum of the
scores of the concepts below it in its hierarchy.
We will define the scores recursively for a concept
c in a sentence s and sub-concepts ci. For example,
in Roget’s if the concept c were a Class, then each ci
would be a Section. Likewise, in WordNet if c were
a synset, then each ci would be a hyponym synset of
c. Obviously if c is a word sense wi (a word in either
a synset or a Semicolon Group), then there can be no
sub-concepts ci. When c = wi, the score for c is the
sum of all occurrences of the word w in sentence s
divided by the number of senses of the word w.
</bodyText>
<equation confidence="0.839887333333333">
� instancesOf(w,s)
score(c, s) =
sensesO f (w) if c = wi
</equation>
<bodyText confidence="0.9568452">
EczEc score(ci, s) otherwise
See Table 7 for an example of how this sentence
representation works. The sentence “A gem is a
jewel or stone that is used in jewellery.” is repre-
sented using the 1911 Roget’s. A concept is identi-
</bodyText>
<footnote confidence="0.738514">
6http://opennlp.sourceforge.net
</footnote>
<bodyText confidence="0.99991075">
fied by a name and a series of up to 9 numbers that
indicate where in the thesaurus it appears. The first
number represents the Class, the second the Sec-
tion, ..., the ninth the word. We only show con-
cepts with weights greater than 1.0. Words not in
the thesaurus keep a weight of 1.0, but this weight
will not increase the weight of any concepts in Ro-
get’s or WordNet. Apart from the function words
“or”, “in”, “that” and “a” and the period, only the
word “jewellery” had a weight above 1.0. The cat-
egories labelled 6, 6.2 and 6.2.2 are the only an-
cestors of the word “use” that ended up with the
weights above 1.0. The words “gem”, “is”, “jewel”,
“stone” and “used” all contributed weight to the cat-
egories shown in Table 7, and to some categories
with weights lower than 1.0, but no sense of the
words themselves had a weight greater than 1.0.
It is worth noting that this method only relies on
the hierarchies in Roget’s and WordNet. We do not
take advantage of other WordNet relations such as
hyponymy, nor do we use any cross-reference links
that exist in Roget’s Thesaurus. Including such re-
lations might improve our sentence relatedness sys-
tem, but that has been left for future work.
</bodyText>
<subsectionHeader confidence="0.989764">
3.3.2 Weighting based on specificity
</subsectionHeader>
<bodyText confidence="0.99998347826087">
To determine sentence relatedness, one could, for
example, flatten the structures like those in Table 7
into vectors and measure their closeness by some
vector distance function such as cosine similarity.
There is a problem with this, though. A concept in-
herits the weights of all its sub-concepts, so the con-
cepts that appear closer to the root of the tree will far
outweigh others. Some sort of weighting function
should be used to re-adjust the weights of particular
concepts. Were this an Information Retrieval task,
weighting schemes such as tf.idf for each concept
could apply, but for sentence relatedness we propose
an ad hoc weighting scheme based on assumptions
about which concepts are most important to sentence
representation. This weighting scheme is the second
element of our sentence relatedness function.
We weight a concept in Roget’s and in WordNet
by how many words in a sentence give weight to it.
We need to re-weight it based on how specific it is.
Clearly, concepts near the leaves of the hierarchy are
more specific than those close to the root of the hier-
archy. We define specificity as the distance in levels
between a given word and each concept found above
</bodyText>
<page confidence="0.995863">
421
</page>
<table confidence="0.995828444444445">
Identifier Concept Weight
6 Words Relating to the Voluntary Powers - Individual Volition 2.125169028274
6.2 Prospective Volition 1.504066255252
6.2.2 Subservience to Ends 1.128154077172
8 Words Relating to the Sentiment and Moral Powers 3.13220884041
8.2 Personal Affections 1.861744448402
8.2.2 Discriminative Affections 1.636503978149
8.2.2.2 Ornament/Jewelry/Blemish [Head Group] 1.452380952380
8.2.2.2.886 Jewelry [Head] 1.452380952380
8.2.2.2.886.1 Jewelry [Noun] 1.452380952380
8.2.2.2.886.1.1 jewel [Paragraph] 1.452380952380
8.2.2.2.886.1.1.1 jewel [Semicolon Group] 1.166666666666
8.2.2.2.886.1.1.1.3 jewellery [Word Sense] 1.0
or - 1.0
in - 1.0
that - 1.0
a - 2.0
. - 1.0
</table>
<tableCaption confidence="0.999773">
Table 7: “A gem is a jewel or stone that is used in jewellery.” as represented using Roget’s 1911.
</tableCaption>
<bodyText confidence="0.999957454545454">
it in the hierarchy. In Roget’s Thesaurus there are ex-
actly 9 levels from the term to the class. In WordNet
there will be as many levels as a word has ances-
tors up the hypernymy chain. In Roget’s, a term has
specificity 1, a Semicolon Group 2, a Paragraph 3,
..., a Class 9. In WordNet, the specificity of a word
is 1, its synset – 2, the synset’s hypernym – 3, its
hypernym – 4, and so on. Words not found in the
Thesaurus or in WordNet get specificity 1.
We seek a function that, given s, assigns to
all concepts of specificity s a weight progressively
larger than to their neighbours. The weights in this
function should be assigned based on specificity, so
that all concepts of the same specificity receive the
same score. Weights will differ depending on a com-
bination of specificity and how frequently words that
signal the concepts appear in a sentence. The weight
of concepts with specificity s should be the highest,
of those with specificity s f 1 – lower, of those with
specificity s f 2 lower still, and so on. In order to
achieve this effect, we weight the concepts using a
normal distribution, where the mean is s:
</bodyText>
<equation confidence="0.940548333333333">
( C
(x−8)2 1
f lx) = Q.\/27r e 202 /I
</equation>
<bodyText confidence="0.999407727272727">
Since the Head is often considered the main cat-
egory in Roget’s, we expect a specificity of 5 to be
best, but we decided to test the values 1 through 9
as a possible setting for specificity. We do not claim
that this weighting scheme is optimal; other weight-
ing schemes might do better. For the purpose of
comparing the 1911 and 1987 Thesauri and Word-
Net, however, this method appears sufficient.
With this weighting scheme, we determine the
distance between two sentences using cosine simi-
larity:
</bodyText>
<equation confidence="0.9686695">
� az * bz
cos�im(A, B) =
</equation>
<bodyText confidence="0.997243">
For this problem we used the MIT Java WordNet In-
terface version 1.1.17.
</bodyText>
<subsectionHeader confidence="0.987806">
3.3.3 Sentence similarity results
</subsectionHeader>
<bodyText confidence="0.9998665">
We used this method of representation for Roget’s
of 1911 and of 1987, as well as for WordNet 3.0 –
see Figure 1. For comparison, we also implemented
a baseline method that we refer to as Simple: we
built vectors out of words and their count.
It can be seen in Figure 1 that each system is su-
perior for at least one of the nine specificities. The
Simple method is best at a specificity of 1, 8 and 9,
Roget’s Thesaurus 1911 is best at 6, Roget’s The-
saurus 1987 is best at 4, 5 and 7, and WordNet is
best at 2 and 3. The systems based on Roget’s and
WordNet more or less followed a bell-shaped curve,
with the curves of the 1911 and 1987 Thesauri fol-
lowing each other fairly closely and peaking close
together. WordNet clearly peaked first and then fell
the farthest.
</bodyText>
<footnote confidence="0.57228">
7http://www.mit.edu/˜markaf/projects/wordnet/
</footnote>
<equation confidence="0.944056">
�� a� �� b�
z * z
</equation>
<page confidence="0.994808">
422
</page>
<bodyText confidence="0.999908">
The best correlation result for the 1987 Roget’s
Thesaurus is 0.8725 when the mean is 4, the POS.
The maximum correlation for the 1911 Thesaurus is
0.8367, where the mean is 5, the Head. The max-
imum for WordNet is 0.8506, where the mean is 3,
or the first hypernym synset. This suggests that the
POS and Head are most important for representing
text in Roget’s Thesaurus, while the first hypernym
is most important for representing text using Word-
Net. For the Simple method, we found a more mod-
est correlation of 0.6969.
</bodyText>
<figureCaption confidence="0.999338">
Figure 1: Correlation data for all four systems.
</figureCaption>
<bodyText confidence="0.999299190476191">
Several other methods have given very good
scores on this data set. For the system in (Li et
al., 2006), where this data set was first introduced, a
correlation of 0.816 with the human annotators was
achieved. The mean of all human annotators had a
score of 0.825, with a standard deviation of 0.072.
In (Islam and Inkpen, 2007), an even better system
was proposed, with a correlation of 0.853.
Selecting the mean that gives the best correlation
could be considered as training on test data. How-
ever, were we simply to have selected a value some-
where in the middle of the graph, as was our original
intuition, it would have given an unfair advantage
to either version of Roget’s Thesaurus over Word-
Net. Our system shows good results for both ver-
sions of Roget’s Thesauri and WordNet. The 1987
Thesaurus once again performs better than the 1911
version and than WordNet. Much like (Miller and
Charles, 1991), the data set used here is not large
enough to determine if any system’s improvement is
statistically significant.
</bodyText>
<sectionHeader confidence="0.961334" genericHeader="conclusions">
4 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999975815789474">
The 1987 version of Roget’s Thesaurus performed
better than the 1911 version on all our tests, but we
did not find the differences to be statistically signifi-
cant. It is particularly interesting that the 1911 The-
saurus performed as well as it did, given that it is al-
most 100 years old. On problems such as semantic
word relatedness, the 1911 Thesaurus performance
was fairly close to that of the 1987 Thesaurus, and
was comparable to many WordNet-based measures.
For problems of identifying synonyms both versions
of Roget’s Thesaurus performed relatively well com-
pared to most WordNet-based methods.
We have presented a new method of sentence
representation that attempts to leverage the struc-
ture found in Roget’s Thesaurus and similar lexi-
cal ontologies (among them WordNet). We have
shown that given this style of text representation
both versions of Roget’s Thesaurus work compara-
bly to WordNet. All three perform fairly well com-
pared to the baseline Simple method. Once again,
the 1987 version is superior to the 1911 version, but
the 1911 version still works quite well.
We hope to investigate further the representation
of sentences and other short texts using Roget’s
Thesaurus. These kinds of measurements can help
with problems such as identifying relevant sentences
for extractive text summarization, or possibly para-
phrase identification (Dolan et al., 2004). Another
– longer-term – direction of future work could be
merging Roget’s Thesaurus with WordNet.
We also plan to study methods of automatically
updating the 1911 Roget’s Thesaurus with modern
words. Some work has been done on adding new
terms and relations to WordNet (Snow et al., 2006)
and FACTOTUM (O’Hara and Wiebe, 2003). Sim-
ilar methods could be used for identifying related
terms and assigning them to a correct semicolon
group or paragraph.
</bodyText>
<sectionHeader confidence="0.998166" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.7908036">
Our research is supported by the Natural Sciences
and Engineering Research Council of Canada and
the University of Ottawa. We thank Dr. Di-
ana Inkpen, Anna Kazantseva and Oana Frunza for
many useful comments on the paper.
</bodyText>
<page confidence="0.998988">
423
</page>
<sectionHeader confidence="0.989811" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999311413461538">
S. Banerjee and T. Pedersen. 2002. An adapted lesk al-
gorithm for word sense disambiguation using wordnet.
In Proc. CICLing 2002, pages 136–145.
P. Cassidy. 2000. An investigation of the semantic rela-
tions in the roget’s thesaurus: Preliminary results. In
Proc. CICLing 2000, pages 181–204.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: ex-
ploiting massively parallel news sources. In Proc.
COLING 2004, pages 350–356, Morristown, NJ.
C. Fellbaum. 1998. A semantic network of english verbs.
In C. Fellbaum, editor, WordNet: An Electronic Lexi-
cal Database, pages 69–104. MIT Press, Cambridge,
MA.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2001. Plac-
ing search in context: the concept revisited. In Proc.
10th International Conf. on World Wide Web, pages
406–414, New York, NY, USA. ACM Press.
G. Hirst and D. St-Onge. 1998. Lexical chains as rep-
resentation of context for the detection and correc-
tion malapropisms. In C. Fellbaum, editor, WordNet:
An Electronic Lexical Database, pages 305–322. MIT
Press, Cambridge, MA.
A. Islam and D. Inkpen. 2007. Semantic similarity of
short texts. In Proc. RANLP 2007, pages 291–297,
September.
M. Jarmasz and S. Szpakowicz. 2003. Not as easy as it
seems: Automating the construction of lexical chains
using roget’s thesaurus. In Proc. 16th Canadian Conf.
on Artificial Intelligence, pages 544–549.
M. Jarmasz and S. Szpakowicz. 2004. Roget’s thesaurus
and semantic similarity. In N. Nicolov, K. Bontcheva,
G. Angelova, and R. Mitkov, editors, Recent Advances
in Natural Language Processing III: Selected Papers
from RANLP 2003, Current Issues in Linguistic The-
ory, volume 260, pages 111–120. John Benjamins.
J. Jiang and D. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. 10th International Conf. on Research on Com-
putational Linguistics, pages 19–33.
A. Kennedy and S. Szpakowicz. 2007. Disambiguating
hypernym relations for roget’s thesaurus. In Proc. TSD
2007, pages 66–75.
B. Kirkpatrick, editor. 1987. Roget’s Thesaurus of En-
glish Words and Phrases. Penguin, Harmondsworth,
Middlesex, England.
T. Landauer and S. Dumais. 1997. A solution to Plato’s
problem: The latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological Review, 104:211–240.
C. Leacock and M. Chodorow. 1998. Combining local
context and wordnet sense similiarity for word sense
disambiguation. In C. Fellbaum, editor, WordNet:
An Electronic Lexical Database, pages 265–284. MIT
Press, Cambridge, MA.
M. Lewis, editor. 2000 and 2001. Readers Digest,
158(932, 934, 935, 936, 937, 938, 939, 940), 159(944,
948). Readers Digest Magazines Canada Limited.
Y. Li, D. McLean, Z. A. Bandar, J. D. O’Shea, and
K. Crockett. 2006. Sentence similarity based on se-
mantic nets and corpus statistics. IEEE Transactions
on Knowledge and Data Engineering, 18(8):1138–
1150.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proc. 15th International Conf. on Ma-
chine Learning, pages 296–304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proc. 21st National Conf. on
Artificial Intelligence, pages 775–780. AAAI Press.
G. A. Miller and W. G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Process, 6(1):1–28.
T. P. O’Hara and J. Wiebe. 2003. Classifying functional
relations in factotum via wordnet hypernym associa-
tions. In Proc. CICLing 2003), pages 347–359.
S. Patwardhan. 2003. Incorporating dictionary and cor-
pus information into a vector measure of semantic re-
latedness. Master’s thesis, University of Minnesota,
Duluth, August.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity - measuring the relatedness of
concepts. In Proc. of the 19th National Conference
on Artificial Intelligence., pages 1024–1025.
P. Resnik. 1995. Using information content to evaluate
semantic similarity. In Proc. 14th International Joint
Conf. on Artificial Intelligence, pages 448–453.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communication of the ACM,
8(10):627–633.
J. Sinclair. 2001. Collins Cobuild English Dictionary for
Advanced Learners. Harper Collins Pub.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
Proc COLING/ACL 2006, pages 801–808.
P. Turney. 2001. Mining the web for synonyms: Pmi-ir
versus lsa on toefl. In Proc. 12th European Conf. on
Machine Learning, pages 491–502.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In Proc. 32nd Annual Meeting of the
ACL, pages 133–138, New Mexico State University,
Las Cruces, New Mexico.
</reference>
<page confidence="0.999085">
424
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.882144">
<author confidence="0.998074">Alistair Kennedy</author>
<affiliation confidence="0.998078">School of Information Technology and Engineering University of Ottawa</affiliation>
<address confidence="0.99751">Ottawa, Ontario, Canada</address>
<email confidence="0.997262">akennedy@site.uottawa.ca</email>
<abstract confidence="0.99429375">has gone through many revisions since it was first published 150 years But how do these revisions affect Rofor NLP? We examine the differences in content between the 1911 and versions of and we test both verwith each other and problems such as synonym identification and word relatedness. We also present a novel method for measuring sentence relatedness that can be in either version of in Although the 1987 version of the Thesaurus is better, we show that the 1911 version performs surprisingly well and that often differences between the versions of Ronot statistically significant. We hope that this work will encourage to use the 1911 in NLP tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>An adapted lesk algorithm for word sense disambiguation using wordnet.</title>
<date>2002</date>
<booktitle>In Proc. CICLing</booktitle>
<pages>136--145</pages>
<contexts>
<context position="12610" citStr="Banerjee and Pedersen, 2002" startWordPosition="2082" endWordPosition="2085">n Table 4. There is a clear improvement: breaking phrases up gives superior results on all three data sets, for both versions of Roget’s. In the remaining experiments, we have each word in a phrase indexed. We compare the results for the 1911 and 1987 Roget’s Thesauri with a variety of WordNet-based semantic relatedness measures – see Table 5. We consider 10 measures, noted in the table as J&amp;C (Jiang and Conrath, 1997), Resnik (Resnik, 1995), Lin (Lin, 1998), W&amp;P (Wu and Palmer, 1994), L&amp;C (Leacock and Chodorow, 1998), H&amp;SO (Hirst and St-Onge, 1998), Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002), and finally Vector and Vector Pair (Patwardhan, 2003). The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen’s Semantic Distance software package (Pedersen et al., 2004). The results suggest that neither version of Roget’s is best for these data sets. In fact, the Vector method is superior on all three sets, and the Lesk algorithm performs very closely to Roget’s 1987. Even on the largest set (Finkelstein et al., 2001), however, the differences between Roget’s Thesaurus and the Vector method are not statistically si</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>S. Banerjee and T. Pedersen. 2002. An adapted lesk algorithm for word sense disambiguation using wordnet. In Proc. CICLing 2002, pages 136–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cassidy</author>
</authors>
<title>An investigation of the semantic relations in the roget’s thesaurus: Preliminary results.</title>
<date>2000</date>
<booktitle>In Proc. CICLing</booktitle>
<pages>181--204</pages>
<contexts>
<context position="4242" citStr="Cassidy (2000)" startWordPosition="668" endWordPosition="670"> Linguistics with lexical chains here, they were an inspiration for our sentence relatedness function. Roget’s Thesaurus does not explicitly label the relations between its terms, as WordNet does. Instead, it groups terms together with implied relations. Kennedy and Szpakowicz (2007) show how disambiguating one of these relations, hypernymy, can help improve the semantic similarity functions in (Jarmasz and Szpakowicz, 2004). These hypernym relations were also put towards solving analogy questions. This is not the first time the 1911 version of Roget’s Thesaurus has been used in NLP research. Cassidy (2000) used it to build the semantic network FACTOTUM. This required significant (manual) restructuring, so FACTOTUM cannot really be considered a true version of Roget’s Thesaurus. The 1987 data come from Penguin’s Roget’s Thesaurus (Kirkpatrick, 1987). The 1911 version is available from Project Gutenberg1. We use WordNet 3.0, the latest version (Fellbaum, 1998). In the experiments we present here, we worked with an interface to Roget’s Thesaurus implemented in Java 5.02. It is built around a large index which stores the location in the thesaurus of each word or phrase; the system individually inde</context>
</contexts>
<marker>Cassidy, 2000</marker>
<rawString>P. Cassidy. 2000. An investigation of the semantic relations in the roget’s thesaurus: Preliminary results. In Proc. CICLing 2000, pages 181–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dolan</author>
<author>C Quirk</author>
<author>C Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proc. COLING 2004,</booktitle>
<pages>350--356</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="29167" citStr="Dolan et al., 2004" startWordPosition="5069" endWordPosition="5072">gies (among them WordNet). We have shown that given this style of text representation both versions of Roget’s Thesaurus work comparably to WordNet. All three perform fairly well compared to the baseline Simple method. Once again, the 1987 version is superior to the 1911 version, but the 1911 version still works quite well. We hope to investigate further the representation of sentences and other short texts using Roget’s Thesaurus. These kinds of measurements can help with problems such as identifying relevant sentences for extractive text summarization, or possibly paraphrase identification (Dolan et al., 2004). Another – longer-term – direction of future work could be merging Roget’s Thesaurus with WordNet. We also plan to study methods of automatically updating the 1911 Roget’s Thesaurus with modern words. Some work has been done on adding new terms and relations to WordNet (Snow et al., 2006) and FACTOTUM (O’Hara and Wiebe, 2003). Similar methods could be used for identifying related terms and assigning them to a correct semicolon group or paragraph. Acknowledgments Our research is supported by the Natural Sciences and Engineering Research Council of Canada and the University of Ottawa. We thank </context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>B. Dolan, C. Quirk, and C. Brockett. 2004. Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In Proc. COLING 2004, pages 350–356, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>A semantic network of english verbs.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database,</booktitle>
<pages>69--104</pages>
<editor>In C. Fellbaum, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4601" citStr="Fellbaum, 1998" startWordPosition="724" endWordPosition="725">ve the semantic similarity functions in (Jarmasz and Szpakowicz, 2004). These hypernym relations were also put towards solving analogy questions. This is not the first time the 1911 version of Roget’s Thesaurus has been used in NLP research. Cassidy (2000) used it to build the semantic network FACTOTUM. This required significant (manual) restructuring, so FACTOTUM cannot really be considered a true version of Roget’s Thesaurus. The 1987 data come from Penguin’s Roget’s Thesaurus (Kirkpatrick, 1987). The 1911 version is available from Project Gutenberg1. We use WordNet 3.0, the latest version (Fellbaum, 1998). In the experiments we present here, we worked with an interface to Roget’s Thesaurus implemented in Java 5.02. It is built around a large index which stores the location in the thesaurus of each word or phrase; the system individually indexes all words within each phrase, as well as the phrase itself. This was shown to improve results in a few applications, which we will discuss later in the paper. 2 Content comparison of the 1911 and 1987 Thesauri Although the 1987 and 1911 Thesauri are very similar in structure, there are a few differences, among them, the number of levels and the number o</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. A semantic network of english verbs. In C. Fellbaum, editor, WordNet: An Electronic Lexical Database, pages 69–104. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing search in context: the concept revisited.</title>
<date>2001</date>
<booktitle>In Proc. 10th International Conf. on World Wide Web,</booktitle>
<pages>406--414</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11091" citStr="Finkelstein et al., 2001" startWordPosition="1843" endWordPosition="1846">oup 8. If we think of the function as counting edges between concepts in the Roget’s hierarchy, then it could also be written as: semDist(A, B) = max aEA,bEB We do not count links between words in the same semicolon group, so in effect these methods find distances between semicolon groups, that is to say, these two functions will give the same results. The 1911 and 1987 Thesauri were compared with WordNet 3.0 on the three data sets containing pairs of words with manually assigned similarity scores: 30 pairs (Miller and Charles, 1991), 65 pairs (Rubenstein and Goodenough, 1965) and 353 pairs3 (Finkelstein et al., 2001). We assume that all terms are nouns, so that we can have a fair comparison of the two Thesauri with WordNet. We measure the correlation with Pearson’s Correlation Coefficient. 3http://www.cs.technion.ac.il/˜gabr/resources/data/ wordsim353/wordsim353.html 16−edgesBetween(a, b) 418 Year Miller &amp; Rubenstein &amp; Finkelstein Charles Goodenough et. al Index words and phrase 1911 0.7846 0.7313 0.3449 1987 0.7984 0.7865 0.4214 Index phrase only 1911 0.7090 0.7168 0.3373 1987 0.7471 0.7777 0.3924 Table 4: Pearson’s coefficient values when not breaking / breaking phrases up. A preliminary experiment set </context>
<context position="13111" citStr="Finkelstein et al., 2001" startWordPosition="2168" endWordPosition="2171">and Chodorow, 1998), H&amp;SO (Hirst and St-Onge, 1998), Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002), and finally Vector and Vector Pair (Patwardhan, 2003). The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen’s Semantic Distance software package (Pedersen et al., 2004). The results suggest that neither version of Roget’s is best for these data sets. In fact, the Vector method is superior on all three sets, and the Lesk algorithm performs very closely to Roget’s 1987. Even on the largest set (Finkelstein et al., 2001), however, the differences between Roget’s Thesaurus and the Vector method are not statistically significant at the p &lt; 0.05 level for either thesaurus on a two-tailed test4. The difference between the 1911 Thesaurus and Vector would be statistically signifi4http://faculty.vassar.edu/lowry/rdiff.html Method Miller &amp; Rubenstein &amp; Finkelstein Charles Goodenough et. al 1911 0.7846 0.7313 0.3449 1987 0.7984 0.7865 0.4214 J&amp;C 0.4735 0.5755 0.2273 Resnik 0.8060 0.8224 0.3531 Lin 0.7388 0.7264 0.2932 W&amp;P 0.7641 0.7973 0.2676 L&amp;C 0.7792 0.8387 0.3094 H&amp;SO 0.6668 0.7258 0.3548 Path 0.7550 0.7842 0.3744</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2001. Placing search in context: the concept revisited. In Proc. 10th International Conf. on World Wide Web, pages 406–414, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical chains as representation of context for the detection and correction malapropisms.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database,</booktitle>
<pages>305--322</pages>
<editor>In C. Fellbaum, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="12537" citStr="Hirst and St-Onge, 1998" startWordPosition="2072" endWordPosition="2075">” and “change of direction”. The outcome of this experiment appears in Table 4. There is a clear improvement: breaking phrases up gives superior results on all three data sets, for both versions of Roget’s. In the remaining experiments, we have each word in a phrase indexed. We compare the results for the 1911 and 1987 Roget’s Thesauri with a variety of WordNet-based semantic relatedness measures – see Table 5. We consider 10 measures, noted in the table as J&amp;C (Jiang and Conrath, 1997), Resnik (Resnik, 1995), Lin (Lin, 1998), W&amp;P (Wu and Palmer, 1994), L&amp;C (Leacock and Chodorow, 1998), H&amp;SO (Hirst and St-Onge, 1998), Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002), and finally Vector and Vector Pair (Patwardhan, 2003). The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen’s Semantic Distance software package (Pedersen et al., 2004). The results suggest that neither version of Roget’s is best for these data sets. In fact, the Vector method is superior on all three sets, and the Lesk algorithm performs very closely to Roget’s 1987. Even on the largest set (Finkelstein et al., 2001), however, the differences</context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>G. Hirst and D. St-Onge. 1998. Lexical chains as representation of context for the detection and correction malapropisms. In C. Fellbaum, editor, WordNet: An Electronic Lexical Database, pages 305–322. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Islam</author>
<author>D Inkpen</author>
</authors>
<title>Semantic similarity of short texts.</title>
<date>2007</date>
<booktitle>In Proc. RANLP 2007,</booktitle>
<pages>291--297</pages>
<contexts>
<context position="18128" citStr="Islam and Inkpen, 2007" startWordPosition="3152" endWordPosition="3155">ked with a data set from (Li et al., 2006)5. They took a subset of the term pairs from (Rubenstein and Goodenough, 1965) and chose sentences to represent these terms; the sentences are definitions from the Collins Cobuild dictionary (Sinclair, 2001). Thirty people were then asked to assign relatedness scores to these sentences, and the average of these similarities was taken for each sentence. Other methods of determining sentence semantic relatedness expand term relatedness functions to 5http://www.docm.mmu.ac.uk/STAFF/D.McLean/ SentenceResults.htm 420 create a sentence relatedness function (Islam and Inkpen, 2007; Mihalcea et al., 2006). We propose to approach the task by exploiting in other ways the commonalities in the structure of Roget’s Thesaurus and of WordNet. We use the OpenNLP toolkit6 for segmentation and part-of-speech tagging. We use a method of sentence representation that involves mapping the sentence into weighted concepts in either Roget’s or WordNet. We mean a concept in Roget’s to be either a Class, Section, ..., Semicolon Group, while a concept in WordNet is any synset. Essentially a concept is a grouping of words from either resource. Concepts are weighted by two criteria. The firs</context>
<context position="27072" citStr="Islam and Inkpen, 2007" startWordPosition="4727" endWordPosition="4730">is suggests that the POS and Head are most important for representing text in Roget’s Thesaurus, while the first hypernym is most important for representing text using WordNet. For the Simple method, we found a more modest correlation of 0.6969. Figure 1: Correlation data for all four systems. Several other methods have given very good scores on this data set. For the system in (Li et al., 2006), where this data set was first introduced, a correlation of 0.816 with the human annotators was achieved. The mean of all human annotators had a score of 0.825, with a standard deviation of 0.072. In (Islam and Inkpen, 2007), an even better system was proposed, with a correlation of 0.853. Selecting the mean that gives the best correlation could be considered as training on test data. However, were we simply to have selected a value somewhere in the middle of the graph, as was our original intuition, it would have given an unfair advantage to either version of Roget’s Thesaurus over WordNet. Our system shows good results for both versions of Roget’s Thesauri and WordNet. The 1987 Thesaurus once again performs better than the 1911 version and than WordNet. Much like (Miller and Charles, 1991), the data set used he</context>
</contexts>
<marker>Islam, Inkpen, 2007</marker>
<rawString>A. Islam and D. Inkpen. 2007. Semantic similarity of short texts. In Proc. RANLP 2007, pages 291–297, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jarmasz</author>
<author>S Szpakowicz</author>
</authors>
<title>Not as easy as it seems: Automating the construction of lexical chains using roget’s thesaurus.</title>
<date>2003</date>
<booktitle>In Proc. 16th Canadian Conf. on Artificial Intelligence,</booktitle>
<pages>544--549</pages>
<contexts>
<context position="3372" citStr="Jarmasz and Szpakowicz, 2003" startWordPosition="530" endWordPosition="533"> of determining semantic relatedness between pairs of terms. Terms that appear closer together in the Thesaurus get higher weights than those farther apart. The experiments aimed at identifying synonyms using a modified version of the proposed semantic similarity function. Similar experiments were carried out using WordNet in combination with a variety of semantic relatedness functions. Roget’s Thesaurus was found generally to outperform WordNet on these problems. We have run similar experiments using the 1911Thesaurus. Lexical chains have also been developed using the 1987 Roget’s Thesaurus (Jarmasz and Szpakowicz, 2003). The procedure maps words in a text to the Head (a Roget’s concept) from which they are most likely to come. Although we did not experiment 416 Proceedings of ACL-08: HLT, pages 416–424, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics with lexical chains here, they were an inspiration for our sentence relatedness function. Roget’s Thesaurus does not explicitly label the relations between its terms, as WordNet does. Instead, it groups terms together with implied relations. Kennedy and Szpakowicz (2007) show how disambiguating one of these relations, hypernymy, </context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>M. Jarmasz and S. Szpakowicz. 2003. Not as easy as it seems: Automating the construction of lexical chains using roget’s thesaurus. In Proc. 16th Canadian Conf. on Artificial Intelligence, pages 544–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jarmasz</author>
<author>S Szpakowicz</author>
</authors>
<title>Roget’s thesaurus and semantic similarity.</title>
<date>2004</date>
<booktitle>Recent Advances in Natural Language Processing III: Selected Papers from RANLP 2003, Current Issues in Linguistic Theory,</booktitle>
<volume>260</volume>
<pages>111--120</pages>
<editor>In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors,</editor>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="2324" citStr="Jarmasz and Szpakowicz, 2004" startWordPosition="363" endWordPosition="367">neering University of Ottawa Ottawa, Ontario, Canada and Institute of Computer Science Polish Academy of Sciences Warsaw, Poland szpak@site.uottawa.ca the public domain, along with related NLP-oriented software packages. For applications that call for an NLP-friendly thesaurus, WordNet has become the de-facto standard. Although WordNet is a fine resources, we believe that ignoring other thesauri is a serious oversight. We show on three applications how useful the 1911 Thesaurus is. We ran the wellestablished tasks of determining semantic relatedness of pairs of terms and identifying synonyms (Jarmasz and Szpakowicz, 2004). We also proposed a new method of representing the meaning of sentences or other short texts using either WordNet or Roget’s Thesaurus, and tested it on the data set provided by Li et al. (2006). We hope that this work will encourage others to use Roget’s Thesaurus in their own NLP tasks. Previous research on the 1987 version of Roget’s Thesaurus includes work of Jarmasz and Szpakowicz (2004). They propose a method of determining semantic relatedness between pairs of terms. Terms that appear closer together in the Thesaurus get higher weights than those farther apart. The experiments aimed at</context>
<context position="4056" citStr="Jarmasz and Szpakowicz, 2004" startWordPosition="634" endWordPosition="637">et’s concept) from which they are most likely to come. Although we did not experiment 416 Proceedings of ACL-08: HLT, pages 416–424, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics with lexical chains here, they were an inspiration for our sentence relatedness function. Roget’s Thesaurus does not explicitly label the relations between its terms, as WordNet does. Instead, it groups terms together with implied relations. Kennedy and Szpakowicz (2007) show how disambiguating one of these relations, hypernymy, can help improve the semantic similarity functions in (Jarmasz and Szpakowicz, 2004). These hypernym relations were also put towards solving analogy questions. This is not the first time the 1911 version of Roget’s Thesaurus has been used in NLP research. Cassidy (2000) used it to build the semantic network FACTOTUM. This required significant (manual) restructuring, so FACTOTUM cannot really be considered a true version of Roget’s Thesaurus. The 1987 data come from Penguin’s Roget’s Thesaurus (Kirkpatrick, 1987). The 1911 version is available from Project Gutenberg1. We use WordNet 3.0, the latest version (Fellbaum, 1998). In the experiments we present here, we worked with an</context>
<context position="9977" citStr="Jarmasz and Szpakowicz, 2004" startWordPosition="1643" endWordPosition="1646">ructure. The 1987 Thesaurus is largely an expansion of the 1911 version, with three POSs (phrases, pronouns and prefixes) removed. 3 Comparison on applications In this section we consider how the two versions of Roget’s Thesaurus and WordNet perform in three applications – measuring word relatedness, synonym identification, and sentence relatedness. 3.1 Word relatedness Relatedness can be measured by the closeness of the words or phrases – henceforth referred to as terms – in the structure of the thesaurus. Two terms in the same semicolon group score 16, in the same paragraph – 14, and so on (Jarmasz and Szpakowicz, 2004). The score is 0 if the terms appear in different classes, or if either is missing. Pairs of terms get higher scores for being closer together. When there are multiple senses of two terms A and B, we want to select senses a E A and b E B that maximize the relatedness score. We define a distance function: semDist(A, B) = max 2 * (depth(lca(a, b))) aEA,bEB lca is the lowest common ancestor and depth is the depth in the Roget’s hierarchy; a Class has depth 0, Section 1, ..., Semicolon Group 8. If we think of the function as counting edges between concepts in the Roget’s hierarchy, then it could a</context>
<context position="14424" citStr="Jarmasz and Szpakowicz, 2004" startWordPosition="2381" endWordPosition="2384"> Table 5: Pearson’s coefficient values for three data sets on a variety of relatedness functions. cant at p &lt; 0.07. On the (Miller and Charles, 1991) and (Rubenstein and Goodenough, 1965) data sets the best system did not show a statistically significant improvement over the 1911 or 1987 Roget’s Thesauri, even at p &lt; 0.1 for a two-tailed test. These data sets are too small for a meaningful comparison of systems with close correlation scores. 3.2 Synonym identification In this problem we take a term q and we seek the correct synonym s from a set C. There are two steps. We used the system from (Jarmasz and Szpakowicz, 2004) for identifying synonyms with Roget’s. First we find a set of terms B ⊆ C with the maximum relatedness between q and each term x ∈ C: B = {x |argmax semDist(x, q)} XEC Next, we take the set of terms A ⊆ B where each a ∈ A has the maximum number of shortest paths between a and q. A = {x |argmax number5hortestPaths(x, q)} XEB If s ∈ A and |A |= 1, the correct synonym has been selected. Often the sets A and B will contain just one item. If s ∈ A and |A |&gt; 1, there is a tie. If s ∈� A then the selected synonyms are incorrect. If a multi-word phrase c ∈ C of length n is not found, 419 ESL Method Y</context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2004</marker>
<rawString>M. Jarmasz and S. Szpakowicz. 2004. Roget’s thesaurus and semantic similarity. In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing III: Selected Papers from RANLP 2003, Current Issues in Linguistic Theory, volume 260, pages 111–120. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>D Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proc. 10th International Conf. on Research on Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="12404" citStr="Jiang and Conrath, 1997" startWordPosition="2051" endWordPosition="2054">ely, for example, whether the phrase “change of direction” should be indexed only as a whole, or as all of “change”, “of”, “direction” and “change of direction”. The outcome of this experiment appears in Table 4. There is a clear improvement: breaking phrases up gives superior results on all three data sets, for both versions of Roget’s. In the remaining experiments, we have each word in a phrase indexed. We compare the results for the 1911 and 1987 Roget’s Thesauri with a variety of WordNet-based semantic relatedness measures – see Table 5. We consider 10 measures, noted in the table as J&amp;C (Jiang and Conrath, 1997), Resnik (Resnik, 1995), Lin (Lin, 1998), W&amp;P (Wu and Palmer, 1994), L&amp;C (Leacock and Chodorow, 1998), H&amp;SO (Hirst and St-Onge, 1998), Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002), and finally Vector and Vector Pair (Patwardhan, 2003). The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen’s Semantic Distance software package (Pedersen et al., 2004). The results suggest that neither version of Roget’s is best for these data sets. In fact, the Vector method is superior on all three sets, and t</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J. Jiang and D. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proc. 10th International Conf. on Research on Computational Linguistics, pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kennedy</author>
<author>S Szpakowicz</author>
</authors>
<title>Disambiguating hypernym relations for roget’s thesaurus.</title>
<date>2007</date>
<booktitle>In Proc. TSD</booktitle>
<pages>66--75</pages>
<contexts>
<context position="3912" citStr="Kennedy and Szpakowicz (2007)" startWordPosition="614" endWordPosition="617"> have also been developed using the 1987 Roget’s Thesaurus (Jarmasz and Szpakowicz, 2003). The procedure maps words in a text to the Head (a Roget’s concept) from which they are most likely to come. Although we did not experiment 416 Proceedings of ACL-08: HLT, pages 416–424, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics with lexical chains here, they were an inspiration for our sentence relatedness function. Roget’s Thesaurus does not explicitly label the relations between its terms, as WordNet does. Instead, it groups terms together with implied relations. Kennedy and Szpakowicz (2007) show how disambiguating one of these relations, hypernymy, can help improve the semantic similarity functions in (Jarmasz and Szpakowicz, 2004). These hypernym relations were also put towards solving analogy questions. This is not the first time the 1911 version of Roget’s Thesaurus has been used in NLP research. Cassidy (2000) used it to build the semantic network FACTOTUM. This required significant (manual) restructuring, so FACTOTUM cannot really be considered a true version of Roget’s Thesaurus. The 1987 data come from Penguin’s Roget’s Thesaurus (Kirkpatrick, 1987). The 1911 version is a</context>
</contexts>
<marker>Kennedy, Szpakowicz, 2007</marker>
<rawString>A. Kennedy and S. Szpakowicz. 2007. Disambiguating hypernym relations for roget’s thesaurus. In Proc. TSD 2007, pages 66–75.</rawString>
</citation>
<citation valid="true">
<date>1987</date>
<booktitle>Roget’s Thesaurus of English Words and Phrases. Penguin,</booktitle>
<editor>B. Kirkpatrick, editor.</editor>
<location>Harmondsworth, Middlesex, England.</location>
<marker>1987</marker>
<rawString>B. Kirkpatrick, editor. 1987. Roget’s Thesaurus of English Words and Phrases. Penguin, Harmondsworth, Middlesex, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>S Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<pages>104--211</pages>
<contexts>
<context position="16500" citStr="Landauer and Dumais, 1997" startWordPosition="2895" endWordPosition="2898">is replaced by each of its words c1, c2..., c,,,, and each of these words is considered in turn. The cz that is closest to q is chosen to represent c. When searching for a word in Roget’s or WordNet, we look for all forms of the word. The results of these experiments appear in Table 6. “Yes” indicates correct answers, “No” – incorrect answers, and “Tie” is for ties. QNF stands for “Question word Not Found”, ANF for “Answer word Not Found” and ONF for “Other word Not Found”. We used three data sets for this application: 80 questions taken from the Test of English as a Foreign Language (TOEFL) (Landauer and Dumais, 1997), 50 questions – from the English as a Second Language test (ESL) (Turney, 2001) and 300 questions – from the Reader’s Digest Word Power Game (RDWP) (Lewis, 2000 and 2001). Lesk and the Vector-based systems perform better than all others, including Roget’s 1911 and 1987. Even so, both versions of Roget’s Thesaurus performed well, and were never worse than the worst WordNet systems. In fact, six of the ten WordNet-based methods are consistently worse than the 1911 Thesaurus. Since the two Vector-based systems make use of additional data beyond WordNet, Lesk is the only completely WordNet-based </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. Landauer and S. Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104:211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and wordnet sense similiarity for word sense disambiguation.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database,</booktitle>
<pages>265--284</pages>
<editor>In C. Fellbaum, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="12505" citStr="Leacock and Chodorow, 1998" startWordPosition="2067" endWordPosition="2070">s all of “change”, “of”, “direction” and “change of direction”. The outcome of this experiment appears in Table 4. There is a clear improvement: breaking phrases up gives superior results on all three data sets, for both versions of Roget’s. In the remaining experiments, we have each word in a phrase indexed. We compare the results for the 1911 and 1987 Roget’s Thesauri with a variety of WordNet-based semantic relatedness measures – see Table 5. We consider 10 measures, noted in the table as J&amp;C (Jiang and Conrath, 1997), Resnik (Resnik, 1995), Lin (Lin, 1998), W&amp;P (Wu and Palmer, 1994), L&amp;C (Leacock and Chodorow, 1998), H&amp;SO (Hirst and St-Onge, 1998), Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002), and finally Vector and Vector Pair (Patwardhan, 2003). The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen’s Semantic Distance software package (Pedersen et al., 2004). The results suggest that neither version of Roget’s is best for these data sets. In fact, the Vector method is superior on all three sets, and the Lesk algorithm performs very closely to Roget’s 1987. Even on the largest set (Finkelstein et al.,</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>C. Leacock and M. Chodorow. 1998. Combining local context and wordnet sense similiarity for word sense disambiguation. In C. Fellbaum, editor, WordNet: An Electronic Lexical Database, pages 265–284. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<title>and</title>
<date>2000</date>
<booktitle>Readers Digest, 158(932, 934, 935, 936, 937, 938, 939, 940), 159(944, 948). Readers Digest Magazines</booktitle>
<editor>M. Lewis, editor.</editor>
<publisher>Canada Limited.</publisher>
<contexts>
<context position="4242" citStr="(2000)" startWordPosition="670" endWordPosition="670">tics with lexical chains here, they were an inspiration for our sentence relatedness function. Roget’s Thesaurus does not explicitly label the relations between its terms, as WordNet does. Instead, it groups terms together with implied relations. Kennedy and Szpakowicz (2007) show how disambiguating one of these relations, hypernymy, can help improve the semantic similarity functions in (Jarmasz and Szpakowicz, 2004). These hypernym relations were also put towards solving analogy questions. This is not the first time the 1911 version of Roget’s Thesaurus has been used in NLP research. Cassidy (2000) used it to build the semantic network FACTOTUM. This required significant (manual) restructuring, so FACTOTUM cannot really be considered a true version of Roget’s Thesaurus. The 1987 data come from Penguin’s Roget’s Thesaurus (Kirkpatrick, 1987). The 1911 version is available from Project Gutenberg1. We use WordNet 3.0, the latest version (Fellbaum, 1998). In the experiments we present here, we worked with an interface to Roget’s Thesaurus implemented in Java 5.02. It is built around a large index which stores the location in the thesaurus of each word or phrase; the system individually inde</context>
</contexts>
<marker>2000</marker>
<rawString>M. Lewis, editor. 2000 and 2001. Readers Digest, 158(932, 934, 935, 936, 937, 938, 939, 940), 159(944, 948). Readers Digest Magazines Canada Limited.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
<author>D McLean</author>
<author>Z A Bandar</author>
<author>J D O’Shea</author>
<author>K Crockett</author>
</authors>
<title>Sentence similarity based on semantic nets and corpus statistics.</title>
<date>2006</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<issue>8</issue>
<pages>1150</pages>
<marker>Li, McLean, Bandar, O’Shea, Crockett, 2006</marker>
<rawString>Y. Li, D. McLean, Z. A. Bandar, J. D. O’Shea, and K. Crockett. 2006. Sentence similarity based on semantic nets and corpus statistics. IEEE Transactions on Knowledge and Data Engineering, 18(8):1138– 1150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity. In</title>
<date>1998</date>
<booktitle>Proc. 15th International Conf. on Machine Learning,</booktitle>
<pages>296--304</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="12444" citStr="Lin, 1998" startWordPosition="2059" endWordPosition="2060">ion” should be indexed only as a whole, or as all of “change”, “of”, “direction” and “change of direction”. The outcome of this experiment appears in Table 4. There is a clear improvement: breaking phrases up gives superior results on all three data sets, for both versions of Roget’s. In the remaining experiments, we have each word in a phrase indexed. We compare the results for the 1911 and 1987 Roget’s Thesauri with a variety of WordNet-based semantic relatedness measures – see Table 5. We consider 10 measures, noted in the table as J&amp;C (Jiang and Conrath, 1997), Resnik (Resnik, 1995), Lin (Lin, 1998), W&amp;P (Wu and Palmer, 1994), L&amp;C (Leacock and Chodorow, 1998), H&amp;SO (Hirst and St-Onge, 1998), Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002), and finally Vector and Vector Pair (Patwardhan, 2003). The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen’s Semantic Distance software package (Pedersen et al., 2004). The results suggest that neither version of Roget’s is best for these data sets. In fact, the Vector method is superior on all three sets, and the Lesk algorithm performs very closely </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. An information-theoretic definition of similarity. In Proc. 15th International Conf. on Machine Learning, pages 296–304, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Corley</author>
<author>C Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proc. 21st National Conf. on Artificial Intelligence,</booktitle>
<pages>775--780</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="18152" citStr="Mihalcea et al., 2006" startWordPosition="3156" endWordPosition="3159"> (Li et al., 2006)5. They took a subset of the term pairs from (Rubenstein and Goodenough, 1965) and chose sentences to represent these terms; the sentences are definitions from the Collins Cobuild dictionary (Sinclair, 2001). Thirty people were then asked to assign relatedness scores to these sentences, and the average of these similarities was taken for each sentence. Other methods of determining sentence semantic relatedness expand term relatedness functions to 5http://www.docm.mmu.ac.uk/STAFF/D.McLean/ SentenceResults.htm 420 create a sentence relatedness function (Islam and Inkpen, 2007; Mihalcea et al., 2006). We propose to approach the task by exploiting in other ways the commonalities in the structure of Roget’s Thesaurus and of WordNet. We use the OpenNLP toolkit6 for segmentation and part-of-speech tagging. We use a method of sentence representation that involves mapping the sentence into weighted concepts in either Roget’s or WordNet. We mean a concept in Roget’s to be either a Class, Section, ..., Semicolon Group, while a concept in WordNet is any synset. Essentially a concept is a grouping of words from either resource. Concepts are weighted by two criteria. The first is how frequently word</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>R. Mihalcea, C. Corley, and C. Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proc. 21st National Conf. on Artificial Intelligence, pages 775–780. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>W G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<journal>Language and Cognitive Process,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="11005" citStr="Miller and Charles, 1991" startWordPosition="1830" endWordPosition="1833"> the depth in the Roget’s hierarchy; a Class has depth 0, Section 1, ..., Semicolon Group 8. If we think of the function as counting edges between concepts in the Roget’s hierarchy, then it could also be written as: semDist(A, B) = max aEA,bEB We do not count links between words in the same semicolon group, so in effect these methods find distances between semicolon groups, that is to say, these two functions will give the same results. The 1911 and 1987 Thesauri were compared with WordNet 3.0 on the three data sets containing pairs of words with manually assigned similarity scores: 30 pairs (Miller and Charles, 1991), 65 pairs (Rubenstein and Goodenough, 1965) and 353 pairs3 (Finkelstein et al., 2001). We assume that all terms are nouns, so that we can have a fair comparison of the two Thesauri with WordNet. We measure the correlation with Pearson’s Correlation Coefficient. 3http://www.cs.technion.ac.il/˜gabr/resources/data/ wordsim353/wordsim353.html 16−edgesBetween(a, b) 418 Year Miller &amp; Rubenstein &amp; Finkelstein Charles Goodenough et. al Index words and phrase 1911 0.7846 0.7313 0.3449 1987 0.7984 0.7865 0.4214 Index phrase only 1911 0.7090 0.7168 0.3373 1987 0.7471 0.7777 0.3924 Table 4: Pearson’s coe</context>
<context position="13944" citStr="Miller and Charles, 1991" startWordPosition="2294" endWordPosition="2297">hesaurus and Vector would be statistically signifi4http://faculty.vassar.edu/lowry/rdiff.html Method Miller &amp; Rubenstein &amp; Finkelstein Charles Goodenough et. al 1911 0.7846 0.7313 0.3449 1987 0.7984 0.7865 0.4214 J&amp;C 0.4735 0.5755 0.2273 Resnik 0.8060 0.8224 0.3531 Lin 0.7388 0.7264 0.2932 W&amp;P 0.7641 0.7973 0.2676 L&amp;C 0.7792 0.8387 0.3094 H&amp;SO 0.6668 0.7258 0.3548 Path 0.7550 0.7842 0.3744 Lesk 0.7954 0.7780 0.4220 Vector 0.8645 0.7929 0.4621 Vct Pair 0.5101 0.5810 0.3722 Table 5: Pearson’s coefficient values for three data sets on a variety of relatedness functions. cant at p &lt; 0.07. On the (Miller and Charles, 1991) and (Rubenstein and Goodenough, 1965) data sets the best system did not show a statistically significant improvement over the 1911 or 1987 Roget’s Thesauri, even at p &lt; 0.1 for a two-tailed test. These data sets are too small for a meaningful comparison of systems with close correlation scores. 3.2 Synonym identification In this problem we take a term q and we seek the correct synonym s from a set C. There are two steps. We used the system from (Jarmasz and Szpakowicz, 2004) for identifying synonyms with Roget’s. First we find a set of terms B ⊆ C with the maximum relatedness between q and ea</context>
<context position="27650" citStr="Miller and Charles, 1991" startWordPosition="4827" endWordPosition="4830">deviation of 0.072. In (Islam and Inkpen, 2007), an even better system was proposed, with a correlation of 0.853. Selecting the mean that gives the best correlation could be considered as training on test data. However, were we simply to have selected a value somewhere in the middle of the graph, as was our original intuition, it would have given an unfair advantage to either version of Roget’s Thesaurus over WordNet. Our system shows good results for both versions of Roget’s Thesauri and WordNet. The 1987 Thesaurus once again performs better than the 1911 version and than WordNet. Much like (Miller and Charles, 1991), the data set used here is not large enough to determine if any system’s improvement is statistically significant. 4 Conclusion and future work The 1987 version of Roget’s Thesaurus performed better than the 1911 version on all our tests, but we did not find the differences to be statistically significant. It is particularly interesting that the 1911 Thesaurus performed as well as it did, given that it is almost 100 years old. On problems such as semantic word relatedness, the 1911 Thesaurus performance was fairly close to that of the 1987 Thesaurus, and was comparable to many WordNet-based m</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>G. A. Miller and W. G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Process, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T P O’Hara</author>
<author>J Wiebe</author>
</authors>
<title>Classifying functional relations in factotum via wordnet hypernym associations.</title>
<date>2003</date>
<booktitle>In Proc. CICLing</booktitle>
<pages>347--359</pages>
<marker>O’Hara, Wiebe, 2003</marker>
<rawString>T. P. O’Hara and J. Wiebe. 2003. Classifying functional relations in factotum via wordnet hypernym associations. In Proc. CICLing 2003), pages 347–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
</authors>
<title>Incorporating dictionary and corpus information into a vector measure of semantic relatedness. Master’s thesis,</title>
<date>2003</date>
<institution>University of Minnesota,</institution>
<location>Duluth,</location>
<contexts>
<context position="12665" citStr="Patwardhan, 2003" startWordPosition="2092" endWordPosition="2093"> superior results on all three data sets, for both versions of Roget’s. In the remaining experiments, we have each word in a phrase indexed. We compare the results for the 1911 and 1987 Roget’s Thesauri with a variety of WordNet-based semantic relatedness measures – see Table 5. We consider 10 measures, noted in the table as J&amp;C (Jiang and Conrath, 1997), Resnik (Resnik, 1995), Lin (Lin, 1998), W&amp;P (Wu and Palmer, 1994), L&amp;C (Leacock and Chodorow, 1998), H&amp;SO (Hirst and St-Onge, 1998), Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002), and finally Vector and Vector Pair (Patwardhan, 2003). The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen’s Semantic Distance software package (Pedersen et al., 2004). The results suggest that neither version of Roget’s is best for these data sets. In fact, the Vector method is superior on all three sets, and the Lesk algorithm performs very closely to Roget’s 1987. Even on the largest set (Finkelstein et al., 2001), however, the differences between Roget’s Thesaurus and the Vector method are not statistically significant at the p &lt; 0.05 level for either thesaurus on</context>
</contexts>
<marker>Patwardhan, 2003</marker>
<rawString>S. Patwardhan. 2003. Incorporating dictionary and corpus information into a vector measure of semantic relatedness. Master’s thesis, University of Minnesota, Duluth, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>Wordnet::similarity - measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proc. of the 19th National Conference on Artificial Intelligence.,</booktitle>
<pages>1024--1025</pages>
<contexts>
<context position="12858" citStr="Pedersen et al., 2004" startWordPosition="2123" endWordPosition="2126">get’s Thesauri with a variety of WordNet-based semantic relatedness measures – see Table 5. We consider 10 measures, noted in the table as J&amp;C (Jiang and Conrath, 1997), Resnik (Resnik, 1995), Lin (Lin, 1998), W&amp;P (Wu and Palmer, 1994), L&amp;C (Leacock and Chodorow, 1998), H&amp;SO (Hirst and St-Onge, 1998), Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002), and finally Vector and Vector Pair (Patwardhan, 2003). The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen’s Semantic Distance software package (Pedersen et al., 2004). The results suggest that neither version of Roget’s is best for these data sets. In fact, the Vector method is superior on all three sets, and the Lesk algorithm performs very closely to Roget’s 1987. Even on the largest set (Finkelstein et al., 2001), however, the differences between Roget’s Thesaurus and the Vector method are not statistically significant at the p &lt; 0.05 level for either thesaurus on a two-tailed test4. The difference between the 1911 Thesaurus and Vector would be statistically signifi4http://faculty.vassar.edu/lowry/rdiff.html Method Miller &amp; Rubenstein &amp; Finkelstein Char</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. Wordnet::similarity - measuring the relatedness of concepts. In Proc. of the 19th National Conference on Artificial Intelligence., pages 1024–1025.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity.</title>
<date>1995</date>
<booktitle>In Proc. 14th International Joint Conf. on Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="12427" citStr="Resnik, 1995" startWordPosition="2056" endWordPosition="2057">se “change of direction” should be indexed only as a whole, or as all of “change”, “of”, “direction” and “change of direction”. The outcome of this experiment appears in Table 4. There is a clear improvement: breaking phrases up gives superior results on all three data sets, for both versions of Roget’s. In the remaining experiments, we have each word in a phrase indexed. We compare the results for the 1911 and 1987 Roget’s Thesauri with a variety of WordNet-based semantic relatedness measures – see Table 5. We consider 10 measures, noted in the table as J&amp;C (Jiang and Conrath, 1997), Resnik (Resnik, 1995), Lin (Lin, 1998), W&amp;P (Wu and Palmer, 1994), L&amp;C (Leacock and Chodorow, 1998), H&amp;SO (Hirst and St-Onge, 1998), Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002), and finally Vector and Vector Pair (Patwardhan, 2003). The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen’s Semantic Distance software package (Pedersen et al., 2004). The results suggest that neither version of Roget’s is best for these data sets. In fact, the Vector method is superior on all three sets, and the Lesk algorithm perfo</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity. In Proc. 14th International Joint Conf. on Artificial Intelligence, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communication of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="11049" citStr="Rubenstein and Goodenough, 1965" startWordPosition="1836" endWordPosition="1839">a Class has depth 0, Section 1, ..., Semicolon Group 8. If we think of the function as counting edges between concepts in the Roget’s hierarchy, then it could also be written as: semDist(A, B) = max aEA,bEB We do not count links between words in the same semicolon group, so in effect these methods find distances between semicolon groups, that is to say, these two functions will give the same results. The 1911 and 1987 Thesauri were compared with WordNet 3.0 on the three data sets containing pairs of words with manually assigned similarity scores: 30 pairs (Miller and Charles, 1991), 65 pairs (Rubenstein and Goodenough, 1965) and 353 pairs3 (Finkelstein et al., 2001). We assume that all terms are nouns, so that we can have a fair comparison of the two Thesauri with WordNet. We measure the correlation with Pearson’s Correlation Coefficient. 3http://www.cs.technion.ac.il/˜gabr/resources/data/ wordsim353/wordsim353.html 16−edgesBetween(a, b) 418 Year Miller &amp; Rubenstein &amp; Finkelstein Charles Goodenough et. al Index words and phrase 1911 0.7846 0.7313 0.3449 1987 0.7984 0.7865 0.4214 Index phrase only 1911 0.7090 0.7168 0.3373 1987 0.7471 0.7777 0.3924 Table 4: Pearson’s coefficient values when not breaking / breaking</context>
<context position="13982" citStr="Rubenstein and Goodenough, 1965" startWordPosition="2299" endWordPosition="2303">atistically signifi4http://faculty.vassar.edu/lowry/rdiff.html Method Miller &amp; Rubenstein &amp; Finkelstein Charles Goodenough et. al 1911 0.7846 0.7313 0.3449 1987 0.7984 0.7865 0.4214 J&amp;C 0.4735 0.5755 0.2273 Resnik 0.8060 0.8224 0.3531 Lin 0.7388 0.7264 0.2932 W&amp;P 0.7641 0.7973 0.2676 L&amp;C 0.7792 0.8387 0.3094 H&amp;SO 0.6668 0.7258 0.3548 Path 0.7550 0.7842 0.3744 Lesk 0.7954 0.7780 0.4220 Vector 0.8645 0.7929 0.4621 Vct Pair 0.5101 0.5810 0.3722 Table 5: Pearson’s coefficient values for three data sets on a variety of relatedness functions. cant at p &lt; 0.07. On the (Miller and Charles, 1991) and (Rubenstein and Goodenough, 1965) data sets the best system did not show a statistically significant improvement over the 1911 or 1987 Roget’s Thesauri, even at p &lt; 0.1 for a two-tailed test. These data sets are too small for a meaningful comparison of systems with close correlation scores. 3.2 Synonym identification In this problem we take a term q and we seek the correct synonym s from a set C. There are two steps. We used the system from (Jarmasz and Szpakowicz, 2004) for identifying synonyms with Roget’s. First we find a set of terms B ⊆ C with the maximum relatedness between q and each term x ∈ C: B = {x |argmax semDist(</context>
<context position="17626" citStr="Rubenstein and Goodenough, 1965" startWordPosition="3083" endWordPosition="3087">ector-based systems make use of additional data beyond WordNet, Lesk is the only completely WordNet-based system to outperform Roget’s 1987. One advantage of Roget’s Thesaurus is that both versions generally have fewer missing terms than WordNet, though Lesk, Hirst &amp; St-Onge and the two vector based methods had fewer missing terms than Roget’s. This may be because the other WordNet methods will only work for nouns and verbs. 3.3 Sentence relatedness Our final experiment concerns sentence relatedness. We worked with a data set from (Li et al., 2006)5. They took a subset of the term pairs from (Rubenstein and Goodenough, 1965) and chose sentences to represent these terms; the sentences are definitions from the Collins Cobuild dictionary (Sinclair, 2001). Thirty people were then asked to assign relatedness scores to these sentences, and the average of these similarities was taken for each sentence. Other methods of determining sentence semantic relatedness expand term relatedness functions to 5http://www.docm.mmu.ac.uk/STAFF/D.McLean/ SentenceResults.htm 420 create a sentence relatedness function (Islam and Inkpen, 2007; Mihalcea et al., 2006). We propose to approach the task by exploiting in other ways the commonal</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H. Rubenstein and J. B. Goodenough. 1965. Contextual correlates of synonymy. Communication of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sinclair</author>
</authors>
<title>Collins Cobuild English Dictionary for Advanced Learners. Harper Collins Pub.</title>
<date>2001</date>
<contexts>
<context position="17755" citStr="Sinclair, 2001" startWordPosition="3105" endWordPosition="3106">dvantage of Roget’s Thesaurus is that both versions generally have fewer missing terms than WordNet, though Lesk, Hirst &amp; St-Onge and the two vector based methods had fewer missing terms than Roget’s. This may be because the other WordNet methods will only work for nouns and verbs. 3.3 Sentence relatedness Our final experiment concerns sentence relatedness. We worked with a data set from (Li et al., 2006)5. They took a subset of the term pairs from (Rubenstein and Goodenough, 1965) and chose sentences to represent these terms; the sentences are definitions from the Collins Cobuild dictionary (Sinclair, 2001). Thirty people were then asked to assign relatedness scores to these sentences, and the average of these similarities was taken for each sentence. Other methods of determining sentence semantic relatedness expand term relatedness functions to 5http://www.docm.mmu.ac.uk/STAFF/D.McLean/ SentenceResults.htm 420 create a sentence relatedness function (Islam and Inkpen, 2007; Mihalcea et al., 2006). We propose to approach the task by exploiting in other ways the commonalities in the structure of Roget’s Thesaurus and of WordNet. We use the OpenNLP toolkit6 for segmentation and part-of-speech taggi</context>
</contexts>
<marker>Sinclair, 2001</marker>
<rawString>J. Sinclair. 2001. Collins Cobuild English Dictionary for Advanced Learners. Harper Collins Pub.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proc COLING/ACL</booktitle>
<pages>801--808</pages>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proc COLING/ACL 2006, pages 801–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Mining the web for synonyms: Pmi-ir versus lsa on toefl.</title>
<date>2001</date>
<booktitle>In Proc. 12th European Conf. on Machine Learning,</booktitle>
<pages>491--502</pages>
<contexts>
<context position="16580" citStr="Turney, 2001" startWordPosition="2911" endWordPosition="2912">rn. The cz that is closest to q is chosen to represent c. When searching for a word in Roget’s or WordNet, we look for all forms of the word. The results of these experiments appear in Table 6. “Yes” indicates correct answers, “No” – incorrect answers, and “Tie” is for ties. QNF stands for “Question word Not Found”, ANF for “Answer word Not Found” and ONF for “Other word Not Found”. We used three data sets for this application: 80 questions taken from the Test of English as a Foreign Language (TOEFL) (Landauer and Dumais, 1997), 50 questions – from the English as a Second Language test (ESL) (Turney, 2001) and 300 questions – from the Reader’s Digest Word Power Game (RDWP) (Lewis, 2000 and 2001). Lesk and the Vector-based systems perform better than all others, including Roget’s 1911 and 1987. Even so, both versions of Roget’s Thesaurus performed well, and were never worse than the worst WordNet systems. In fact, six of the ten WordNet-based methods are consistently worse than the 1911 Thesaurus. Since the two Vector-based systems make use of additional data beyond WordNet, Lesk is the only completely WordNet-based system to outperform Roget’s 1987. One advantage of Roget’s Thesaurus is that bo</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>P. Turney. 2001. Mining the web for synonyms: Pmi-ir versus lsa on toefl. In Proc. 12th European Conf. on Machine Learning, pages 491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Wu</author>
<author>M Palmer</author>
</authors>
<title>Verb semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proc. 32nd Annual Meeting of the ACL,</booktitle>
<pages>133--138</pages>
<institution>Mexico State University, Las Cruces,</institution>
<location>New</location>
<contexts>
<context position="12471" citStr="Wu and Palmer, 1994" startWordPosition="2062" endWordPosition="2065">dexed only as a whole, or as all of “change”, “of”, “direction” and “change of direction”. The outcome of this experiment appears in Table 4. There is a clear improvement: breaking phrases up gives superior results on all three data sets, for both versions of Roget’s. In the remaining experiments, we have each word in a phrase indexed. We compare the results for the 1911 and 1987 Roget’s Thesauri with a variety of WordNet-based semantic relatedness measures – see Table 5. We consider 10 measures, noted in the table as J&amp;C (Jiang and Conrath, 1997), Resnik (Resnik, 1995), Lin (Lin, 1998), W&amp;P (Wu and Palmer, 1994), L&amp;C (Leacock and Chodorow, 1998), H&amp;SO (Hirst and St-Onge, 1998), Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002), and finally Vector and Vector Pair (Patwardhan, 2003). The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen’s Semantic Distance software package (Pedersen et al., 2004). The results suggest that neither version of Roget’s is best for these data sets. In fact, the Vector method is superior on all three sets, and the Lesk algorithm performs very closely to Roget’s 1987. Even on th</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Z. Wu and M. Palmer. 1994. Verb semantics and lexical selection. In Proc. 32nd Annual Meeting of the ACL, pages 133–138, New Mexico State University, Las Cruces, New Mexico.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>