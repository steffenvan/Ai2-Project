<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.104046">
<title confidence="0.845175">
Generation Challenges 2009
</title>
<note confidence="0.9148225">
Proceedings of the 12th European Workshop on Natural Language Generation, pages 162–164,
Athens, Greece, 30 – 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.983373">
162
</page>
<note confidence="0.459188">
Preface
</note>
<bodyText confidence="0.999861847826087">
Generation Challenges 2009 was the third round of shared-task evaluation
competitions (STECs) that involve the generation of natural language, and
followed the Pilot Attribute Selection for Generating Referring Expressions
Challenge in 2007 (ASGRE’07) and Referring Expression Generation Chal-
lenges in 2008 (REG’08). More information about all these NLG STEC ac-
tivities can be found via the links on the Generation Challenges homepage:
http://www.nltg.brighton.ac.uk/research/genchal09
Generation Challenges 2009 brought together four STECs: the TUNA Refer-
ring Expression Generation Task (TUNA-REG) organised by Albert Gatt, Anja Belz
and Eric Kow; the two GREC Challenges, GREC Main Subject Reference Genera-
tion (GREC-MSR) and GREC Named Entity Generation (GREC-NEG), organised by
Anja Belz, Eric Kow, Jette Viethen and Albert Gatt; and the Giving Instructions in
Virtual Environments Challenge (GIVE) organised by Donna Byron, Justine Cas-
sell, Robert Dale, Alexander Koller, Johanna Moore, Jon Oberlander, and Kristina
Striegnitz.
In the GIVE Challenge, participating teams developed systems which generate
natural-language instructions to users navigating a virtual 3D environment and per-
forming computer-game-like tasks. The four participating systems were evaluated
by measuring how quickly, accurately and efficiently users were able to perform
tasks with a given system’s instructions. The evaluation report for the GIVE Chal-
lenge can be found in this volume; the participants’ reports will be made publicly
available at a later stage.
The TUNA-REG Task was the end-to-end referring expression generation task
(combining the attribute selection and realisation subtasks) which was first intro-
duced in REG’08, and which used the TUNA corpus of paired descriptions and
pictures of entities. This year’s TUNA-REG Task had an open call for participation,
but it was also organised in the spirit of a progress check which would give partic-
ipants from TUNA-REG’08 an opportunity to submit improved systems, the results
for which could be compared to last year’s results. Of five registered teams from
five countries, four teams submitted a total of 6 systems to TUNA-REG. These,
along with two sets of human outputs, were evaluated by automatic intrinsic and
human-based intrinsic and extrinsic evaluations. The results report and the partici-
pants’ reports can be found in this volume.
The GREC-MSR Task was the same as in REG’08 and used a corpus of intro-
ductory sections from Wikipedia articles on geographic entities and people. The
task was to generate referring expressions for mentions of the main subject of the
article in the context of the full text of the article. The new GREC-NEG Task used
a separate corpus of introductory sections from Wikipedia articles on people, and
the task was to generate referring expressions for all mentions of all people in an
article.
Eight teams from seven countries registered for each of the GREC-MSR and
GREC-NEG tasks. As the system submission deadline approached, it became clear
that just two teams were certain that they were going to complete their systems in
time. For this reason, and also because of a moving camera-ready deadline, we de-
cided, after careful consideration and consultation with participants, to extend the
system development period for the GREC Tasks and to hold the GREC’09 results
</bodyText>
<page confidence="0.998007">
163
</page>
<bodyText confidence="0.999871911764706">
meeting at the ACL-IJCNLP’09 Workshop on Language Generation and Summari-
sation in Singapore on 6 August 2009, and to publish all GREC’09 reports in the
proceedings of that workshop.
In addition to the four shared tasks, Generation Challenges 2009 offered (i) an
open submission track in which participants could submit any work involving the
data from any of the shared tasks, while opting out of the competetive element, (ii)
an evaluation track, in which proposals for new evaluation methods for the shared
task could be submitted, and (iii) a task proposal track in which proposals for new
shared tasks could be submitted. We believe that these types of open-access tracks
are important because they allow the wider research community to shape the focus
and methodologies of STECs directly. We received one submission in the open
submission track, involving the TUNA data, and none in the other tracks.
We successfully applied (with the help of support letters from many of last
year’s participants and other HLT colleagues) for funding from the Engineering
and Physical Sciences Research Council (EPSRC), the main funding body for HLT
in the UK. This support helped with all aspects of organising Generation Chal-
lenges 2009, and enabled us to create the new GREC-People corpus and to carry
out extensive human evaluations, as well as to employ a dedicated research fellow
(Eric Kow) to help with all aspects of Generation Challenges 2009.
Preparations are already underway for a fourth NLG shared-task evaluation
event next year, Generation Challenges 2010, which is likely to include a further
run of the GREC-NEG Task with an extended training/development corpus, a new
task which links GREC-NEG to a named-entity recognition preprocessing stage, and
a second run of the GIVE Challenge. We are hoping that results will be presented
at INLG’10.
Like our previous STECs, Generation Challenges 2009 would not have been
possible without the contributions of many different people. We would like to thank
the faculty and staff of Brighton University, and the students of UCL, Brighton
and Sussex Universities who participated in the evaluation experiments as well
as all other participants in our online data elicitation and evaluation exercises; the
ENLG’09 organisers, Mariet Theune and Emiel Krahmer; the research support team
at Brighton University and the EPSRC for help with obtaining funding; and last but
not least, the participants in the shared tasks for making the most of the short
available time to build some very successful systems.
</bodyText>
<subsubsectionHeader confidence="0.30954">
February 2009 Anja Belz and Albert Gatt
</subsubsectionHeader>
<page confidence="0.996889">
164
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.011209">
<note confidence="0.96514575">Generation Challenges 2009 of the 12th European Workshop on Natural Language pages Greece, 30 – 31 March 2009. Association for Computational Linguistics 162</note>
<title confidence="0.5077275">Preface Generation Challenges 2009 was the third round of shared-task evaluation</title>
<abstract confidence="0.7350104">that involve the generation of natural language, and followed the Pilot Attribute Selection for Generating Referring Expressions in 2007 and Referring Expression Generation Chalin 2008 More information about all these STEC activities can be found via the links on the Generation Challenges homepage:</abstract>
<web confidence="0.738036">http://www.nltg.brighton.ac.uk/research/genchal09</web>
<title confidence="0.8485595">Challenges 2009 brought together four the Refer- Expression Generation Task organised by Albert Gatt, Anja Belz</title>
<author confidence="0.802969">Eric Kow</author>
<author confidence="0.802969">the two Subject Reference Generaand Entity Generation organised by Anja Belz</author>
<author confidence="0.802969">Eric Kow</author>
<author confidence="0.802969">Jette Viethen</author>
<author confidence="0.802969">Albert Gatt</author>
<author confidence="0.802969">the Giving Instructions in</author>
<affiliation confidence="0.874555">Environments Challenge organised by Donna Byron, Justine Cas-</affiliation>
<address confidence="0.715188">sell, Robert Dale, Alexander Koller, Johanna Moore, Jon Oberlander, and Kristina</address>
<abstract confidence="0.999598121212121">Striegnitz. the participating teams developed systems which generate natural-language instructions to users navigating a virtual 3D environment and performing computer-game-like tasks. The four participating systems were evaluated by measuring how quickly, accurately and efficiently users were able to perform with a given system’s instructions. The evaluation report for the Challenge can be found in this volume; the participants’ reports will be made publicly available at a later stage. was the end-to-end referring expression generation task (combining the attribute selection and realisation subtasks) which was first introin and which used the of paired descriptions and of entities. This year’s had an open call for participation, but it was also organised in the spirit of a progress check which would give particfrom an opportunity to submit improved systems, the results for which could be compared to last year’s results. Of five registered teams from countries, four teams submitted a total of 6 systems to These, along with two sets of human outputs, were evaluated by automatic intrinsic and human-based intrinsic and extrinsic evaluations. The results report and the participants’ reports can be found in this volume. was the same as in and used a corpus of introductory sections from Wikipedia articles on geographic entities and people. The task was to generate referring expressions for mentions of the main subject of the in the context of the full text of the article. The new used a separate corpus of introductory sections from Wikipedia articles on people, and the task was to generate referring expressions for all mentions of all people in an article. teams from seven countries registered for each of the As the system submission deadline approached, it became clear that just two teams were certain that they were going to complete their systems in time. For this reason, and also because of a moving camera-ready deadline, we decided, after careful consideration and consultation with participants, to extend the development period for the and to hold the results 163 at the Workshop on Language Generation and Summariin Singapore on 6 August 2009, and to publish all reports in the proceedings of that workshop. In addition to the four shared tasks, Generation Challenges 2009 offered (i) an open submission track in which participants could submit any work involving the data from any of the shared tasks, while opting out of the competetive element, (ii) an evaluation track, in which proposals for new evaluation methods for the shared task could be submitted, and (iii) a task proposal track in which proposals for new shared tasks could be submitted. We believe that these types of open-access tracks are important because they allow the wider research community to shape the focus methodologies of directly. We received one submission in the open track, involving the and none in the other tracks. We successfully applied (with the help of support letters from many of last participants and other for funding from the Engineering Physical Sciences Research Council the main funding body for in the UK. This support helped with all aspects of organising Generation Chal- 2009, and enabled us to create the new corpus and to carry out extensive human evaluations, as well as to employ a dedicated research fellow (Eric Kow) to help with all aspects of Generation Challenges 2009. are already underway for a fourth evaluation event next year, Generation Challenges 2010, which is likely to include a further of the with an extended training/development corpus, a new which links a named-entity recognition preprocessing stage, and second run of the We are hoping that results will be presented our previous Generation Challenges 2009 would not have been possible without the contributions of many different people. We would like to thank faculty and staff of Brighton University, and the students of Brighton and Sussex Universities who participated in the evaluation experiments as well as all other participants in our online data elicitation and evaluation exercises; the organisers, Mariet Theune and Emiel Krahmer; the research support team Brighton University and the help with obtaining funding; and last but not least, the participants in the shared tasks for making the most of the short available time to build some very successful systems.</abstract>
<note confidence="0.2413695">February 2009 Anja Belz and Albert Gatt 164</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>