<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002703">
<title confidence="0.999489">
An Empirical Investigation of Discounting
in Cross-Domain Language Models
</title>
<author confidence="0.999206">
Greg Durrett and Dan Klein
</author>
<affiliation confidence="0.9985935">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.99939">
{gdurrett,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.998603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999036076923077">
We investigate the empirical behavior of n-
gram discounts within and across domains.
When a language model is trained and evalu-
ated on two corpora from exactly the same do-
main, discounts are roughly constant, match-
ing the assumptions of modified Kneser-Ney
LMs. However, when training and test corpora
diverge, the empirical discount grows essen-
tially as a linear function of the n-gram count.
We adapt a Kneser-Ney language model to
incorporate such growing discounts, result-
ing in perplexity improvements over modified
Kneser-Ney and Jelinek-Mercer baselines.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999311222222222">
Discounting, or subtracting from the count of each
n-gram, is one of the core aspects of Kneser-Ney
language modeling (Kneser and Ney, 1995). For all
but the smallest n-gram counts, Kneser-Ney uses a
single discount, one that does not grow with the n-
gram count, because such constant-discounting was
seen in early experiments on held-out data (Church
and Gale, 1991). However, due to increasing com-
putational power and corpus sizes, language model-
ing today presents a different set of challenges than
it did 20 years ago. In particular, modeling cross-
domain effects has become increasingly more im-
portant (Klakow, 2000; Moore and Lewis, 2010),
and deployed systems must frequently process data
that is out-of-domain from the standpoint of the lan-
guage model.
In this work, we perform experiments on held-
out data to evaluate how discounting behaves in the
</bodyText>
<page confidence="0.980109">
24
</page>
<bodyText confidence="0.999862">
cross-domain setting. We find that, when training
and testing on corpora that are as similar as possi-
ble, empirical discounts indeed do not grow with n-
gram count, which validates the parametric assump-
tion of Kneser-Ney smoothing. However, when the
train and evaluation corpora differ, even slightly, dis-
counts generally exhibit linear growth in the count of
the n-gram, with the amount of growth being closely
correlated with the corpus divergence. Finally, we
build a language model exploiting a parametric form
of the growing discount and show perplexity gains of
up to 5.4% over modified Kneser-Ney.
</bodyText>
<sectionHeader confidence="0.993203" genericHeader="introduction">
2 Discount Analysis
</sectionHeader>
<bodyText confidence="0.999297411764706">
Underlying discounting is the idea that n-grams will
occur fewer times in test data than they do in training
data. We investigate this quantitatively by conduct-
ing experiments similar in spirit to those of Church
and Gale (1991). Suppose that we have collected
counts on two corpora of the same size, which we
will call our train and test corpora. For an n-gram
w = (wi, ..., wn), let ktrain(w) denote the number of
occurrences of w in the training corpus, and ktest(w)
denote the number of occurrences of w in the test
corpus. We define the empirical discount of w to be
d(w) = ktrain(w) − ktest(w); this will be negative
when the n-gram occurs more in the test data than
in the training data. Let Wi = {w : ktrain(w) = i}
be the set of n-grams with count i in the training
corpus. We define the average empirical discount
function as
</bodyText>
<equation confidence="0.71674">
1 d(i) =  |Wi  |w�z d(w)
</equation>
<bodyText confidence="0.946353944444444">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 24–29,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
Kneser-Ney implicitly makes two assumptions:
first, that discounts do not depend on n-gram count,
i.e. that d(i) is constant in i. Modified Kneser-Ney
relaxes this assumption slightly by having indepen-
dent parameters for 1-count, 2-count, and many-
count n-grams, but still assumes that d(i) is constant
for i greater than two. Second, by using the same
discount for all n-grams with a given count, Kneser-
Ney assumes that the distribution of d(w) for w in a
particular Wi is well-approximated by its mean. In
this section, we analyze whether or not the behavior
of the average empirical discount function supports
these two assumptions. We perform experiments on
various subsets of the documents in the English Gi-
gaword corpus, chiefly drawn from New York Times
(NYT) and Agence France Presse (AFP).1
</bodyText>
<subsectionHeader confidence="0.985705">
2.1 Are Discounts Constant?
</subsectionHeader>
<bodyText confidence="0.99268852">
Similar corpora To begin, we consider the NYT
documents from Gigaword for the year 1995. In
order to create two corpora that are maximally
domain-similar, we randomly assign half of these
documents to train and half of them to test, yielding
train and test corpora of approximately 50M words
each, which we denote by NYT95 and NYT95&apos;. Fig-
ure 1 shows the average empirical discounts d(i)
for trigrams on this pair of corpora. In this setting,
we recover the results of Church and Gale (1991)
in that discounts are approximately constant for n-
gram counts of two or greater.
Divergent corpora In addition to these two cor-
pora, which were produced from a single contigu-
ous batch of documents, we consider testing on cor-
pus pairs with varying degrees of domain difference.
We construct additional corpora NYT96, NYT06,
AFP95, AFP96, and AFP06, by taking 50M words
from documents in the indicated years of NYT
and AFP data. We then collect training counts on
NYT95 and alternately take each of our five new cor-
pora as the test data. Figure 1 also shows the average
empirical discount curves for these train/test pairs.
Even within NYT newswire data, we see growing
discounts when the train and test corpora are drawn
</bodyText>
<footnote confidence="0.9774358">
1Gigaword is drawn from six newswire sources and contains
both miscellaneous text and complete, contiguous documents,
sorted chronologically. Our experiments deal exclusively with
the document text, which constitutes the majority of Gigaword
and is of higher quality than the miscellaneous text.
</footnote>
<figure confidence="0.896497">
0 5 10 15 20
Trigram count in train
</figure>
<figureCaption confidence="0.711725142857143">
Figure 1: Average empirical trigram discounts d(i) for
six configurations, training on NYT95 and testing on the
indicated corpora. For each n-gram count k, we compute
the average number of occurrences in test for all n-grams
occurring k times in training data, then report k minus
this quantity as the discount. Bigrams and bigram types
exhibit similar discount relationships.
</figureCaption>
<bodyText confidence="0.9986114">
-
from different years, and between the NYT and AFP
newswire, discounts grow even more quickly. We
observed these trends continuing steadily up into n-
gram counts in the hundreds, beyond which point it
becomes difficult to robustly estimate discounts due
to fewer n-gram types in this count range.
This result is surprising in light of the constant
discounts observed for the NYT95/NYT95&apos; pair.
Goodman (2001) proposes that discounts arise from
document-level “burstiness” in a corpus, because
language often repeats itself locally within a doc-
ument, and Moore and Quirk (2009) suggest that
discounting also corrects for quantization error due
to estimating a continuous distribution using a dis-
crete maximum likelihood estimator (MLE). Both
of these factors are at play in the NYT95/NYT95&apos;
experiment, and yet only a small, constant discount
is observed. Our growing discounts must therefore
be caused by other, larger-scale phenomena, such as
shifts in the subjects of news articles over time or in
the style of the writing between newswire sources.
The increasing rate of discount growth as the source
changes and temporal divergence increases lends
credence to this hypothesis.
</bodyText>
<subsectionHeader confidence="0.999343">
2.2 Nonuniformity of Discounts
</subsectionHeader>
<bodyText confidence="0.966584">
Figure 1 considers discounting in terms of averaged
discounts for each count, which tests one assump-
tion of modified Kneser-Ney, that discounts are a
</bodyText>
<figure confidence="0.988903421052632">
6
5
4
3
2
1
0
AFP06
AFP96
AFP95
NYT06
NYT96
NYT95’
n
gne K.
A
25
0 5 10 15 20
Trigram count in test
</figure>
<figureCaption confidence="0.937602285714286">
Figure 2: Empirical probability mass functions of occur-
rences in the test data for trigrams that appeared 10 times
in training data. Discounting by a single value is plau-
sible in the case of similar train and test corpora, where
the mean of the distribution (8.50) is close to the median
(8.0), but not in the case of divergent corpora, where the
mean (6.04) and median (1.0) are very different.
</figureCaption>
<bodyText confidence="0.989587705882353">
igraip
constant function of n-gram counts. In Figure 2, we
investigate the second assumption, namely that the
distribution over discounts for a givennn-gram count
is well-approximated by its mean. For similar cor-
pora, this seems to be true, with a histogram of test
counts for trigrams of count 10 that is nearly sym-
metric. For divergent corpora, the data exhibit high
skew: almost 40% of the trigrams simply never ap-
pear in the test data, and the distribution has very
high standard deviation (17.0) due to a heavy tail
(not shown). Using a discount that depends only on
the n-gram count is less appropriate in this case.
In combination with the growing discounts of sec-
tion 2. 1, these results point to the fact that modified
Kneser-Ney does not faithfully models the discount-
ing in even a mildly cross-domain setting.
</bodyText>
<equation confidence="0.2665915">
o
3tn- fr Dscont
</equation>
<subsectionHeader confidence="0.999744">
2.3 Correlation of Divergence and Discounts
</subsectionHeader>
<bodyText confidence="0.999993909090909">
Intuitively, corpora that are more temporally distant
within a particular newswire source should perhaps
be slightly more distinct, and still a higher degree of
divergence should exist between corpora from dif-
ferent newswire sources. From Figure 1, we see that
this notion agrees with the relative sizes of the ob-
served discounts. We now ask whether growth in
discounts is correlated with train/test dissimilarity in
a more quantitative way. For a given pair of cor-
pora, we canonicalize the degree of discounting by
selecting the point d(30), the average empirical dis-
</bodyText>
<figure confidence="0.850173">
-500 -400 -300
Log likelihood difference (in millions)
</figure>
<figureCaption confidence="0.8360256">
Figure 3: Log likelihood difference versus average empir-
ical discount of trigrams with training count 30 (d(30))
for the train/test pairs. More negative values of the log
likelihood indicate more dissimilar corpora, as the trained
model is doing less well relative to the jackknife model.
</figureCaption>
<bodyText confidence="0.959323548387097">
count for n-grams occurring 30 times in training.2
To measure divergence between the corpus pair, we
compute the difference between the log likelihood
of the test corpus under the train corpus language
model (using basic Kneser-Ney) and the likelihood
of the test corpus under a jackknife language model
from the test itself, which holds out and scores each
test n-gram in turn. This dissimilarity metric resem-
bles the cross-entropy difference used by Moore and
Lewis (2010) to subsample for domain adaptation.
We compute this canonicalization for each of
twenty pairs of corpora, with each corpus contain-
ing 240M trigram tokens between train and test. The
corpus pairs were chosen to span varying numbers
of newswire sources and lengths of time in order to
capture a wide range of corpus divergences. Our re-
sults are plotted in Figure 3. The log likelihood dif-
ference and d(30) are negatively correlated with a
correlation coefficient value of r = −0.88, which
strongly supports our hypothesis that higher diver-
gence yields higher discounting. One explanation
for the remaining variance is that the trigram dis-
count curve depends on the difference between the
number of bigram types in the train and test corpora,
which can be as large as 10%: observing more bi-
gram contexts in training fragments the token counts
2One could also imagine instead canonicalizing the curves
by using either the exponent or slope parameters from a fitted
power law as in section 3. However, there was sufficient non-
linearity in the average empirical discount curves that neither of
these parameters was an accurate proxy for d(i).
</bodyText>
<figure confidence="0.997590153846154">
trigrams
F
15
0
10
0.2
0.4
0.3
5
0.1
0
NYT95/NYT95’
NYT95/AFP95
</figure>
<page confidence="0.983456">
26
</page>
<bodyText confidence="0.954109">
and leads to smaller observed discounts.
</bodyText>
<sectionHeader confidence="0.68864" genericHeader="related work">
2.4 Related Work
</sectionHeader>
<bodyText confidence="0.999990955555556">
The results of section 2.1 point to a remarkably per-
vasive phenomenon of growing empirical discounts,
except in the case of extremely similar corpora.
Growing discounts of this sort were previously sug-
gested by the model of Teh (2006). However, we
claim that the discounting phenomenon in our data is
fundamentally different from his model’s prediction.
In the held-out experiments of section 2.1, growing
discounts only emerge when one evaluates against a
dissimilar held-out corpus, whereas his model would
predict discount growth even in NYT95/NYT95&apos;,
where we do not observe it.
Adaptation across corpora has also been ad-
dressed before. Bellegarda (2004) describes a range
of techniques, from interpolation at either the count
level or the model level (Bacchiani and Roark, 2003;
Bacchiani et al., 2006) to using explicit models of
syntax or semantics. Hsu and Glass (2008) employ
a log-linear model for multiplicatively discounting
n-grams in Kneser-Ney; when they include the log-
count of an n-gram as the only feature, they achieve
75% of their overall word error rate reduction, sug-
gesting that predicting discounts based on n-gram
count can substantially improve the model. Their
work also improves on the second assumption of
Kneser-Ney, that of the inadequacy of the average
empirical discount as a discount constant, by em-
ploying various other features in order to provide
other criteria on which to discount n-grams.
Taking a different approach, both Klakow (2000)
and Moore and Lewis (2010) use subsampling to
select the domain-relevant portion of a large, gen-
eral corpus given a small in-domain corpus. This
can be interpreted as a form of hard discounting,
and implicitly models both growing discounts, since
frequent n-grams will appear in more of the re-
jected sentences, and nonuniform discounting over
n-grams of each count, since the sentences are cho-
sen according to a likelihood criterion. Although
we do not consider this second point in constructing
our language model, an advantage of our approach
over subsampling is that we use our entire training
corpus, and in so doing compromise between min-
imizing errors from data sparsity and accommodat-
ing domain shifts to the extent possible.
</bodyText>
<sectionHeader confidence="0.98454" genericHeader="conclusions">
3 A Growing Discount Language Model
</sectionHeader>
<bodyText confidence="0.9996275">
We now implement and evaluate a language model
that incorporates growing discounts.
</bodyText>
<subsectionHeader confidence="0.998714">
3.1 Methods
</subsectionHeader>
<bodyText confidence="0.999929428571428">
Instead of using a fixed discount for most n-gram
counts, as prescribed by modified Kneser-Ney, we
discount by an increasing parametric function of the
n-gram count. We use a tune set to compute an av-
erage empirical discount curve d(i), and fit a func-
tion of the form f(x) = a + bx&apos; to this curve using
weighted least-Li-loss regression, with the weight
for each point proportional to i|Wi|, the total to-
ken counts of n-grams occurring that many times
in training. To improve the fit of the model, we
use dedicated parameters for count-1 and count-2 n-
grams as in modified Kneser-Ney, yielding a model
with five parameters per n-gram order. We call this
model GDLM. We also instantiate this model with
c fixed to one, so that the model is strictly linear
(GDLM-LIN).
As baselines for comparison, we use basic inter-
polated Kneser-Ney (KNLM), with one discount pa-
rameter per n-gram order, and modified interpolated
Kneser-Ney (MKNLM), with three parameters per
n-gram order, as described in (Chen and Goodman,
1998). We also compare against Jelinek-Mercer
smoothing (JMLM), which interpolates the undis-
counted MLEs from every order. According to Chen
and Goodman (1998), it is common to use different
interpolation weights depending on the history count
of an n-gram, since MLEs based on many samples
are presumed to be more accurate than those with
few samples. We used five history count buckets so
that JMLM would have the same number of param-
eters as GDLM.
All five models are trigram models with type
counts at the lower orders and independent discount
or interpolation parameters for each order. Param-
eters for GDLM, MKNLM, and KNLM are initial-
ized based on estimates from d(i): the regression
thereof for GDLM, and raw discounts for MKNLM
and KNLM. The parameters of JMLM are initialized
to constants independent of the data. These initial-
izations are all heuristic and not guaranteed to be
optimal, so we then iterate through the parameters
of each model several times and perform line search
</bodyText>
<page confidence="0.994432">
27
</page>
<table confidence="0.999814285714286">
Train NYT00+01 Train AFP02+05+06
Voc. 157K 50K 157K 50K
GDLM(*) 151 131 258 209
GDLM-LIN(*) 151 132 259 210
JMLM 165 143 274 221
MKNLM 152 132 273 221
KNLM 159 138 300 241
</table>
<tableCaption confidence="0.809930625">
Table 1: Perplexities of the growing discounts language
model (GDLM) and its purely linear variant (GDLM-
LIN), which are contributions of this work, versus
the modified Kneser-Ney (MKNLM), basic Kneser-Ney
(KNLM), and Jelinek-Mercer (JMLM) baselines. We
report results for in-domain (NYT00+01) and out-of-
domain (AFP02+05+06) training corpora, for two meth-
ods of closing the vocabulary.
</tableCaption>
<bodyText confidence="0.999432925925926">
in each to optimize tune-set perplexity.
For evaluation, we train, tune, and test on three
disjoint corpora. We consider two different train-
ing sets: one of 110M words of NYT from 2000
and 2001 (NYT00+01), and one of 110M words of
AFP from 2002, 2005, and 2006 (AFP02+05+06).
In both cases, we compute d(i) and tune parameters
on 110M words of NYT from 2002 and 2003, and
do our final perplexity evaluation on 4M words of
NYT from 2004. This gives us both in-domain and
out-of-domain results for our new language model.
Our tune set is chosen to be large so that we can
initialize parameters based on the average empirical
discount curve; in practice, one could compute em-
pirical discounts based on a smaller tune set with the
counts scaled up proportionately, or simply initialize
to constant values.
We use two different methods to handle out-of-
vocabulary (OOV) words: one scheme replaces any
unigram token occurring fewer than five times in
training with an UNK token, yielding a vocabulary
of approximately 157K words, and the other scheme
only keeps the top 50K words in the vocabulary.
The count truncation method has OOV rates of 0.9%
and 1.9% in the NYT/NYT and NYT/AFP settings,
respectively, and the constant-size vocabulary has
OOV rates of 2% and 3.6%.
</bodyText>
<subsectionHeader confidence="0.807201">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999852352941176">
Perplexity results are given in Table 1. As expected,
for in-domain data, GDLM performs comparably to
MKNLM, since the discounts do not grow and so
there is little to be gained by choosing a param-
eterization that permits this. Out-of-domain, our
model outperforms MKNLM and JMLM by approx-
imately 5% for both vocabulary sizes. The out-
of-domain perplexity values are competitive with
those of Rosenfeld (1996), who trained on New York
Times data and tested on AP News data under simi-
lar conditions, and even more aggressive closing of
the vocabulary. Moore and Lewis (2010) achieve
lower perplexities, but they use in-domain training
data that we do not include in our setting.
We briefly highlight some interesting features of
these results. In the small vocabulary cross-domain
setting, for GDLM-LIN, we find
</bodyText>
<equation confidence="0.976905">
dtri(i) = 1.31 + 0.27i, dbi(i) = 1.34 + 0.05i
</equation>
<bodyText confidence="0.999742">
as the trigram and bigram discount functions that
minimize tune set perplexity. For GDLM,
</bodyText>
<equation confidence="0.975376">
dtri(i) = 1.19 + 0.32i0.45, dbi(i) = 0.86 + 0.56i0.86
</equation>
<bodyText confidence="0.999878909090909">
In both cases, a growing discount is indeed learned
from the tuning procedure, demonstrating the im-
portance of this in our model. Modeling nonlin-
ear discount growth in GDLM yields only a small
marginal improvement over the linear discounting
model GDLM-LIN, so we prefer GDLM-LIN for its
simplicity.
A somewhat surprising result is the strong per-
formance of JMLM relative to MKNLM on the di-
vergent corpus pair. We conjecture that this is be-
cause the bucketed parameterization of JMLM gives
it the freedom to change interpolation weights with
n-gram count, whereas MKNLM has essentially a
fixed discount. This suggests that modified Kneser-
Ney as it is usually parameterized may be a particu-
larly poor choice in cross-domain settings.
Overall, these results show that the growing dis-
count phenomenon detailed in section 2, beyond
simply being present in out-of-domain held-out data,
provides the basis for a new discounting scheme that
allows us to improve perplexity relative to modified
Kneser-Ney and Jelinek-Mercer baselines.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999464">
The authors gratefully acknowledge partial support
from the GALE program via BBN under DARPA
contract HR0011-06-C-0022, and from an NSF fel-
lowship for the first author. Thanks to the anony-
mous reviewers for their insightful comments.
</bodyText>
<page confidence="0.99782">
28
</page>
<sectionHeader confidence="0.998344" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999894734693878">
Michiel Bacchiani and Brian Roark. 2003. Unsupervised
Langauge Model Adaptation. In Proceedings of Inter-
national Conference on Acoustics, Speech, and Signal
Processing.
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochastic
grammars. Computer Speech &amp; Language, 20(1):41 –
68.
Jerome R. Bellegarda. 2004. Statistical language model
adaptation: review and perspectives. Speech Commu-
nication, 42:93–108.
Stanley Chen and Joshua Goodman. 1998. An Empirical
Study of Smoothing Techniques for Language Model-
ing. Technical report, Harvard University, August.
Kenneth Church and William Gale. 1991. A Compari-
son of the Enhanced Good-Turing and Deleted Estima-
tion Methods for Estimating Probabilities of English
Bigrams. Computer Speech &amp; Language, 5(1):19–54.
Joshua Goodman. 2001. A Bit of Progress in Language
Modeling. Computer Speech &amp; Language, 15(4):403–
434.
Bo-June (Paul) Hsu and James Glass. 2008. N-
gram Weighting: Reducing Training Data Mismatch in
Cross-Domain Language Model Estimation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 829–838.
Dietrich Klakow. 2000. Selecting articles from the lan-
guage model training corpus. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing, volume 3, pages 1695–1698.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-off for M-Gram Language Modeling. In Pro-
ceedings of International Conference on Acoustics,
Speech, and Signal Processing.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proceed-
ings of the ACL 2010 Conference Short Papers, pages
220–224, July.
Robert C. Moore and Chris Quirk. 2009. Improved
Smoothing for N-gram Language Models Based on
Ordinary Counts. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 349–352.
Ronald Rosenfeld. 1996. A Maximum Entropy Ap-
proach to Adaptive Statistical Language Modeling.
Computer, Speech &amp; Language, 10:187–228.
Yee Whye Teh. 2006. A Hierarchical Bayesian Lan-
guage Model Based On Pitman-Yor Processes. In Pro-
ceedings of ACL, pages 985–992, Sydney, Australia,
July. Association for Computational Linguistics.
</reference>
<page confidence="0.999117">
29
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.743880">
<title confidence="0.9985605">An Empirical Investigation of in Cross-Domain Language Models</title>
<author confidence="0.969215">Durrett</author>
<affiliation confidence="0.9999115">Computer Science University of California,</affiliation>
<abstract confidence="0.982298571428571">investigate the empirical behavior of gram discounts within and across domains. When a language model is trained and evaluated on two corpora from exactly the same domain, discounts are roughly constant, matching the assumptions of modified Kneser-Ney LMs. However, when training and test corpora diverge, the empirical discount grows essenas a linear function of the count. We adapt a Kneser-Ney language model to incorporate such growing discounts, resulting in perplexity improvements over modified Kneser-Ney and Jelinek-Mercer baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Brian Roark</author>
</authors>
<title>Unsupervised Langauge Model Adaptation. In</title>
<date>2003</date>
<booktitle>Proceedings of International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="12237" citStr="Bacchiani and Roark, 2003" startWordPosition="1993" endWordPosition="1996">g discounts of this sort were previously suggested by the model of Teh (2006). However, we claim that the discounting phenomenon in our data is fundamentally different from his model’s prediction. In the held-out experiments of section 2.1, growing discounts only emerge when one evaluates against a dissimilar held-out corpus, whereas his model would predict discount growth even in NYT95/NYT95&apos;, where we do not observe it. Adaptation across corpora has also been addressed before. Bellegarda (2004) describes a range of techniques, from interpolation at either the count level or the model level (Bacchiani and Roark, 2003; Bacchiani et al., 2006) to using explicit models of syntax or semantics. Hsu and Glass (2008) employ a log-linear model for multiplicatively discounting n-grams in Kneser-Ney; when they include the logcount of an n-gram as the only feature, they achieve 75% of their overall word error rate reduction, suggesting that predicting discounts based on n-gram count can substantially improve the model. Their work also improves on the second assumption of Kneser-Ney, that of the inadequacy of the average empirical discount as a discount constant, by employing various other features in order to provid</context>
</contexts>
<marker>Bacchiani, Roark, 2003</marker>
<rawString>Michiel Bacchiani and Brian Roark. 2003. Unsupervised Langauge Model Adaptation. In Proceedings of International Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Michael Riley</author>
<author>Brian Roark</author>
<author>Richard Sproat</author>
</authors>
<title>MAP adaptation of stochastic grammars.</title>
<date>2006</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="12262" citStr="Bacchiani et al., 2006" startWordPosition="1997" endWordPosition="2000">re previously suggested by the model of Teh (2006). However, we claim that the discounting phenomenon in our data is fundamentally different from his model’s prediction. In the held-out experiments of section 2.1, growing discounts only emerge when one evaluates against a dissimilar held-out corpus, whereas his model would predict discount growth even in NYT95/NYT95&apos;, where we do not observe it. Adaptation across corpora has also been addressed before. Bellegarda (2004) describes a range of techniques, from interpolation at either the count level or the model level (Bacchiani and Roark, 2003; Bacchiani et al., 2006) to using explicit models of syntax or semantics. Hsu and Glass (2008) employ a log-linear model for multiplicatively discounting n-grams in Kneser-Ney; when they include the logcount of an n-gram as the only feature, they achieve 75% of their overall word error rate reduction, suggesting that predicting discounts based on n-gram count can substantially improve the model. Their work also improves on the second assumption of Kneser-Ney, that of the inadequacy of the average empirical discount as a discount constant, by employing various other features in order to provide other criteria on which</context>
</contexts>
<marker>Bacchiani, Riley, Roark, Sproat, 2006</marker>
<rawString>Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat. 2006. MAP adaptation of stochastic grammars. Computer Speech &amp; Language, 20(1):41 – 68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Statistical language model adaptation: review and perspectives.</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<pages>42--93</pages>
<contexts>
<context position="12113" citStr="Bellegarda (2004)" startWordPosition="1975" endWordPosition="1976">arkably pervasive phenomenon of growing empirical discounts, except in the case of extremely similar corpora. Growing discounts of this sort were previously suggested by the model of Teh (2006). However, we claim that the discounting phenomenon in our data is fundamentally different from his model’s prediction. In the held-out experiments of section 2.1, growing discounts only emerge when one evaluates against a dissimilar held-out corpus, whereas his model would predict discount growth even in NYT95/NYT95&apos;, where we do not observe it. Adaptation across corpora has also been addressed before. Bellegarda (2004) describes a range of techniques, from interpolation at either the count level or the model level (Bacchiani and Roark, 2003; Bacchiani et al., 2006) to using explicit models of syntax or semantics. Hsu and Glass (2008) employ a log-linear model for multiplicatively discounting n-grams in Kneser-Ney; when they include the logcount of an n-gram as the only feature, they achieve 75% of their overall word error rate reduction, suggesting that predicting discounts based on n-gram count can substantially improve the model. Their work also improves on the second assumption of Kneser-Ney, that of the</context>
</contexts>
<marker>Bellegarda, 2004</marker>
<rawString>Jerome R. Bellegarda. 2004. Statistical language model adaptation: review and perspectives. Speech Communication, 42:93–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Harvard University,</institution>
<contexts>
<context position="14809" citStr="Chen and Goodman, 1998" startWordPosition="2414" endWordPosition="2417">|, the total token counts of n-grams occurring that many times in training. To improve the fit of the model, we use dedicated parameters for count-1 and count-2 ngrams as in modified Kneser-Ney, yielding a model with five parameters per n-gram order. We call this model GDLM. We also instantiate this model with c fixed to one, so that the model is strictly linear (GDLM-LIN). As baselines for comparison, we use basic interpolated Kneser-Ney (KNLM), with one discount parameter per n-gram order, and modified interpolated Kneser-Ney (MKNLM), with three parameters per n-gram order, as described in (Chen and Goodman, 1998). We also compare against Jelinek-Mercer smoothing (JMLM), which interpolates the undiscounted MLEs from every order. According to Chen and Goodman (1998), it is common to use different interpolation weights depending on the history count of an n-gram, since MLEs based on many samples are presumed to be more accurate than those with few samples. We used five history count buckets so that JMLM would have the same number of parameters as GDLM. All five models are trigram models with type counts at the lower orders and independent discount or interpolation parameters for each order. Parameters fo</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley Chen and Joshua Goodman. 1998. An Empirical Study of Smoothing Techniques for Language Modeling. Technical report, Harvard University, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>William Gale</author>
</authors>
<title>A Comparison of the Enhanced Good-Turing and Deleted Estimation Methods for Estimating Probabilities of English Bigrams.</title>
<date>1991</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1146" citStr="Church and Gale, 1991" startWordPosition="167" endWordPosition="170">cal discount grows essentially as a linear function of the n-gram count. We adapt a Kneser-Ney language model to incorporate such growing discounts, resulting in perplexity improvements over modified Kneser-Ney and Jelinek-Mercer baselines. 1 Introduction Discounting, or subtracting from the count of each n-gram, is one of the core aspects of Kneser-Ney language modeling (Kneser and Ney, 1995). For all but the smallest n-gram counts, Kneser-Ney uses a single discount, one that does not grow with the ngram count, because such constant-discounting was seen in early experiments on held-out data (Church and Gale, 1991). However, due to increasing computational power and corpus sizes, language modeling today presents a different set of challenges than it did 20 years ago. In particular, modeling crossdomain effects has become increasingly more important (Klakow, 2000; Moore and Lewis, 2010), and deployed systems must frequently process data that is out-of-domain from the standpoint of the language model. In this work, we perform experiments on heldout data to evaluate how discounting behaves in the 24 cross-domain setting. We find that, when training and testing on corpora that are as similar as possible, em</context>
<context position="2489" citStr="Church and Gale (1991)" startWordPosition="384" endWordPosition="387"> However, when the train and evaluation corpora differ, even slightly, discounts generally exhibit linear growth in the count of the n-gram, with the amount of growth being closely correlated with the corpus divergence. Finally, we build a language model exploiting a parametric form of the growing discount and show perplexity gains of up to 5.4% over modified Kneser-Ney. 2 Discount Analysis Underlying discounting is the idea that n-grams will occur fewer times in test data than they do in training data. We investigate this quantitatively by conducting experiments similar in spirit to those of Church and Gale (1991). Suppose that we have collected counts on two corpora of the same size, which we will call our train and test corpora. For an n-gram w = (wi, ..., wn), let ktrain(w) denote the number of occurrences of w in the training corpus, and ktest(w) denote the number of occurrences of w in the test corpus. We define the empirical discount of w to be d(w) = ktrain(w) − ktest(w); this will be negative when the n-gram occurs more in the test data than in the training data. Let Wi = {w : ktrain(w) = i} be the set of n-grams with count i in the training corpus. We define the average empirical discount func</context>
<context position="4636" citStr="Church and Gale (1991)" startWordPosition="749" endWordPosition="752">n the English Gigaword corpus, chiefly drawn from New York Times (NYT) and Agence France Presse (AFP).1 2.1 Are Discounts Constant? Similar corpora To begin, we consider the NYT documents from Gigaword for the year 1995. In order to create two corpora that are maximally domain-similar, we randomly assign half of these documents to train and half of them to test, yielding train and test corpora of approximately 50M words each, which we denote by NYT95 and NYT95&apos;. Figure 1 shows the average empirical discounts d(i) for trigrams on this pair of corpora. In this setting, we recover the results of Church and Gale (1991) in that discounts are approximately constant for ngram counts of two or greater. Divergent corpora In addition to these two corpora, which were produced from a single contiguous batch of documents, we consider testing on corpus pairs with varying degrees of domain difference. We construct additional corpora NYT96, NYT06, AFP95, AFP96, and AFP06, by taking 50M words from documents in the indicated years of NYT and AFP data. We then collect training counts on NYT95 and alternately take each of our five new corpora as the test data. Figure 1 also shows the average empirical discount curves for t</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Kenneth Church and William Gale. 1991. A Comparison of the Enhanced Good-Turing and Deleted Estimation Methods for Estimating Probabilities of English Bigrams. Computer Speech &amp; Language, 5(1):19–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>A Bit of Progress in Language Modeling.</title>
<date>2001</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>15</volume>
<issue>4</issue>
<pages>434</pages>
<contexts>
<context position="6474" citStr="Goodman (2001)" startWordPosition="1049" endWordPosition="1050">average number of occurrences in test for all n-grams occurring k times in training data, then report k minus this quantity as the discount. Bigrams and bigram types exhibit similar discount relationships. - from different years, and between the NYT and AFP newswire, discounts grow even more quickly. We observed these trends continuing steadily up into ngram counts in the hundreds, beyond which point it becomes difficult to robustly estimate discounts due to fewer n-gram types in this count range. This result is surprising in light of the constant discounts observed for the NYT95/NYT95&apos; pair. Goodman (2001) proposes that discounts arise from document-level “burstiness” in a corpus, because language often repeats itself locally within a document, and Moore and Quirk (2009) suggest that discounting also corrects for quantization error due to estimating a continuous distribution using a discrete maximum likelihood estimator (MLE). Both of these factors are at play in the NYT95/NYT95&apos; experiment, and yet only a small, constant discount is observed. Our growing discounts must therefore be caused by other, larger-scale phenomena, such as shifts in the subjects of news articles over time or in the styl</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua Goodman. 2001. A Bit of Progress in Language Modeling. Computer Speech &amp; Language, 15(4):403– 434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo-June Hsu</author>
<author>James Glass</author>
</authors>
<title>Ngram Weighting: Reducing Training Data Mismatch in Cross-Domain Language Model Estimation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>829--838</pages>
<contexts>
<context position="12332" citStr="Hsu and Glass (2008)" startWordPosition="2009" endWordPosition="2012">t the discounting phenomenon in our data is fundamentally different from his model’s prediction. In the held-out experiments of section 2.1, growing discounts only emerge when one evaluates against a dissimilar held-out corpus, whereas his model would predict discount growth even in NYT95/NYT95&apos;, where we do not observe it. Adaptation across corpora has also been addressed before. Bellegarda (2004) describes a range of techniques, from interpolation at either the count level or the model level (Bacchiani and Roark, 2003; Bacchiani et al., 2006) to using explicit models of syntax or semantics. Hsu and Glass (2008) employ a log-linear model for multiplicatively discounting n-grams in Kneser-Ney; when they include the logcount of an n-gram as the only feature, they achieve 75% of their overall word error rate reduction, suggesting that predicting discounts based on n-gram count can substantially improve the model. Their work also improves on the second assumption of Kneser-Ney, that of the inadequacy of the average empirical discount as a discount constant, by employing various other features in order to provide other criteria on which to discount n-grams. Taking a different approach, both Klakow (2000) </context>
</contexts>
<marker>Hsu, Glass, 2008</marker>
<rawString>Bo-June (Paul) Hsu and James Glass. 2008. Ngram Weighting: Reducing Training Data Mismatch in Cross-Domain Language Model Estimation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 829–838.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dietrich Klakow</author>
</authors>
<title>Selecting articles from the language model training corpus.</title>
<date>2000</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>3</volume>
<pages>1695--1698</pages>
<contexts>
<context position="1398" citStr="Klakow, 2000" startWordPosition="209" endWordPosition="210">nting, or subtracting from the count of each n-gram, is one of the core aspects of Kneser-Ney language modeling (Kneser and Ney, 1995). For all but the smallest n-gram counts, Kneser-Ney uses a single discount, one that does not grow with the ngram count, because such constant-discounting was seen in early experiments on held-out data (Church and Gale, 1991). However, due to increasing computational power and corpus sizes, language modeling today presents a different set of challenges than it did 20 years ago. In particular, modeling crossdomain effects has become increasingly more important (Klakow, 2000; Moore and Lewis, 2010), and deployed systems must frequently process data that is out-of-domain from the standpoint of the language model. In this work, we perform experiments on heldout data to evaluate how discounting behaves in the 24 cross-domain setting. We find that, when training and testing on corpora that are as similar as possible, empirical discounts indeed do not grow with ngram count, which validates the parametric assumption of Kneser-Ney smoothing. However, when the train and evaluation corpora differ, even slightly, discounts generally exhibit linear growth in the count of th</context>
<context position="12931" citStr="Klakow (2000)" startWordPosition="2105" endWordPosition="2106">d Glass (2008) employ a log-linear model for multiplicatively discounting n-grams in Kneser-Ney; when they include the logcount of an n-gram as the only feature, they achieve 75% of their overall word error rate reduction, suggesting that predicting discounts based on n-gram count can substantially improve the model. Their work also improves on the second assumption of Kneser-Ney, that of the inadequacy of the average empirical discount as a discount constant, by employing various other features in order to provide other criteria on which to discount n-grams. Taking a different approach, both Klakow (2000) and Moore and Lewis (2010) use subsampling to select the domain-relevant portion of a large, general corpus given a small in-domain corpus. This can be interpreted as a form of hard discounting, and implicitly models both growing discounts, since frequent n-grams will appear in more of the rejected sentences, and nonuniform discounting over n-grams of each count, since the sentences are chosen according to a likelihood criterion. Although we do not consider this second point in constructing our language model, an advantage of our approach over subsampling is that we use our entire training co</context>
</contexts>
<marker>Klakow, 2000</marker>
<rawString>Dietrich Klakow. 2000. Selecting articles from the language model training corpus. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 3, pages 1695–1698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Backing-off for M-Gram Language Modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="920" citStr="Kneser and Ney, 1995" startWordPosition="130" endWordPosition="133">uage model is trained and evaluated on two corpora from exactly the same domain, discounts are roughly constant, matching the assumptions of modified Kneser-Ney LMs. However, when training and test corpora diverge, the empirical discount grows essentially as a linear function of the n-gram count. We adapt a Kneser-Ney language model to incorporate such growing discounts, resulting in perplexity improvements over modified Kneser-Ney and Jelinek-Mercer baselines. 1 Introduction Discounting, or subtracting from the count of each n-gram, is one of the core aspects of Kneser-Ney language modeling (Kneser and Ney, 1995). For all but the smallest n-gram counts, Kneser-Ney uses a single discount, one that does not grow with the ngram count, because such constant-discounting was seen in early experiments on held-out data (Church and Gale, 1991). However, due to increasing computational power and corpus sizes, language modeling today presents a different set of challenges than it did 20 years ago. In particular, modeling crossdomain effects has become increasingly more important (Klakow, 2000; Moore and Lewis, 2010), and deployed systems must frequently process data that is out-of-domain from the standpoint of t</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved Backing-off for M-Gram Language Modeling. In Proceedings of International Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>220--224</pages>
<contexts>
<context position="1422" citStr="Moore and Lewis, 2010" startWordPosition="211" endWordPosition="214">racting from the count of each n-gram, is one of the core aspects of Kneser-Ney language modeling (Kneser and Ney, 1995). For all but the smallest n-gram counts, Kneser-Ney uses a single discount, one that does not grow with the ngram count, because such constant-discounting was seen in early experiments on held-out data (Church and Gale, 1991). However, due to increasing computational power and corpus sizes, language modeling today presents a different set of challenges than it did 20 years ago. In particular, modeling crossdomain effects has become increasingly more important (Klakow, 2000; Moore and Lewis, 2010), and deployed systems must frequently process data that is out-of-domain from the standpoint of the language model. In this work, we perform experiments on heldout data to evaluate how discounting behaves in the 24 cross-domain setting. We find that, when training and testing on corpora that are as similar as possible, empirical discounts indeed do not grow with ngram count, which validates the parametric assumption of Kneser-Ney smoothing. However, when the train and evaluation corpora differ, even slightly, discounts generally exhibit linear growth in the count of the n-gram, with the amoun</context>
<context position="10192" citStr="Moore and Lewis (2010)" startWordPosition="1660" endWordPosition="1663">re negative values of the log likelihood indicate more dissimilar corpora, as the trained model is doing less well relative to the jackknife model. count for n-grams occurring 30 times in training.2 To measure divergence between the corpus pair, we compute the difference between the log likelihood of the test corpus under the train corpus language model (using basic Kneser-Ney) and the likelihood of the test corpus under a jackknife language model from the test itself, which holds out and scores each test n-gram in turn. This dissimilarity metric resembles the cross-entropy difference used by Moore and Lewis (2010) to subsample for domain adaptation. We compute this canonicalization for each of twenty pairs of corpora, with each corpus containing 240M trigram tokens between train and test. The corpus pairs were chosen to span varying numbers of newswire sources and lengths of time in order to capture a wide range of corpus divergences. Our results are plotted in Figure 3. The log likelihood difference and d(30) are negatively correlated with a correlation coefficient value of r = −0.88, which strongly supports our hypothesis that higher divergence yields higher discounting. One explanation for the remai</context>
<context position="12958" citStr="Moore and Lewis (2010)" startWordPosition="2108" endWordPosition="2111">loy a log-linear model for multiplicatively discounting n-grams in Kneser-Ney; when they include the logcount of an n-gram as the only feature, they achieve 75% of their overall word error rate reduction, suggesting that predicting discounts based on n-gram count can substantially improve the model. Their work also improves on the second assumption of Kneser-Ney, that of the inadequacy of the average empirical discount as a discount constant, by employing various other features in order to provide other criteria on which to discount n-grams. Taking a different approach, both Klakow (2000) and Moore and Lewis (2010) use subsampling to select the domain-relevant portion of a large, general corpus given a small in-domain corpus. This can be interpreted as a form of hard discounting, and implicitly models both growing discounts, since frequent n-grams will appear in more of the rejected sentences, and nonuniform discounting over n-grams of each count, since the sentences are chosen according to a likelihood criterion. Although we do not consider this second point in constructing our language model, an advantage of our approach over subsampling is that we use our entire training corpus, and in so doing compr</context>
<context position="18203" citStr="Moore and Lewis (2010)" startWordPosition="2980" endWordPosition="2983">cabulary has OOV rates of 2% and 3.6%. 3.2 Results Perplexity results are given in Table 1. As expected, for in-domain data, GDLM performs comparably to MKNLM, since the discounts do not grow and so there is little to be gained by choosing a parameterization that permits this. Out-of-domain, our model outperforms MKNLM and JMLM by approximately 5% for both vocabulary sizes. The outof-domain perplexity values are competitive with those of Rosenfeld (1996), who trained on New York Times data and tested on AP News data under similar conditions, and even more aggressive closing of the vocabulary. Moore and Lewis (2010) achieve lower perplexities, but they use in-domain training data that we do not include in our setting. We briefly highlight some interesting features of these results. In the small vocabulary cross-domain setting, for GDLM-LIN, we find dtri(i) = 1.31 + 0.27i, dbi(i) = 1.34 + 0.05i as the trigram and bigram discount functions that minimize tune set perplexity. For GDLM, dtri(i) = 1.19 + 0.32i0.45, dbi(i) = 0.86 + 0.56i0.86 In both cases, a growing discount is indeed learned from the tuning procedure, demonstrating the importance of this in our model. Modeling nonlinear discount growth in GDLM</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, pages 220–224, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>Improved Smoothing for N-gram Language Models Based on Ordinary Counts.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>349--352</pages>
<contexts>
<context position="6642" citStr="Moore and Quirk (2009)" startWordPosition="1072" endWordPosition="1075">types exhibit similar discount relationships. - from different years, and between the NYT and AFP newswire, discounts grow even more quickly. We observed these trends continuing steadily up into ngram counts in the hundreds, beyond which point it becomes difficult to robustly estimate discounts due to fewer n-gram types in this count range. This result is surprising in light of the constant discounts observed for the NYT95/NYT95&apos; pair. Goodman (2001) proposes that discounts arise from document-level “burstiness” in a corpus, because language often repeats itself locally within a document, and Moore and Quirk (2009) suggest that discounting also corrects for quantization error due to estimating a continuous distribution using a discrete maximum likelihood estimator (MLE). Both of these factors are at play in the NYT95/NYT95&apos; experiment, and yet only a small, constant discount is observed. Our growing discounts must therefore be caused by other, larger-scale phenomena, such as shifts in the subjects of news articles over time or in the style of the writing between newswire sources. The increasing rate of discount growth as the source changes and temporal divergence increases lends credence to this hypothe</context>
</contexts>
<marker>Moore, Quirk, 2009</marker>
<rawString>Robert C. Moore and Chris Quirk. 2009. Improved Smoothing for N-gram Language Models Based on Ordinary Counts. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 349–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A Maximum Entropy Approach to Adaptive Statistical Language Modeling.</title>
<date>1996</date>
<journal>Computer, Speech &amp; Language,</journal>
<pages>10--187</pages>
<contexts>
<context position="18039" citStr="Rosenfeld (1996)" startWordPosition="2953" endWordPosition="2954">ords in the vocabulary. The count truncation method has OOV rates of 0.9% and 1.9% in the NYT/NYT and NYT/AFP settings, respectively, and the constant-size vocabulary has OOV rates of 2% and 3.6%. 3.2 Results Perplexity results are given in Table 1. As expected, for in-domain data, GDLM performs comparably to MKNLM, since the discounts do not grow and so there is little to be gained by choosing a parameterization that permits this. Out-of-domain, our model outperforms MKNLM and JMLM by approximately 5% for both vocabulary sizes. The outof-domain perplexity values are competitive with those of Rosenfeld (1996), who trained on New York Times data and tested on AP News data under similar conditions, and even more aggressive closing of the vocabulary. Moore and Lewis (2010) achieve lower perplexities, but they use in-domain training data that we do not include in our setting. We briefly highlight some interesting features of these results. In the small vocabulary cross-domain setting, for GDLM-LIN, we find dtri(i) = 1.31 + 0.27i, dbi(i) = 1.34 + 0.05i as the trigram and bigram discount functions that minimize tune set perplexity. For GDLM, dtri(i) = 1.19 + 0.32i0.45, dbi(i) = 0.86 + 0.56i0.86 In both </context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Ronald Rosenfeld. 1996. A Maximum Entropy Approach to Adaptive Statistical Language Modeling. Computer, Speech &amp; Language, 10:187–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>985--992</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="11689" citStr="Teh (2006)" startWordPosition="1912" endWordPosition="1913"> the curves by using either the exponent or slope parameters from a fitted power law as in section 3. However, there was sufficient nonlinearity in the average empirical discount curves that neither of these parameters was an accurate proxy for d(i). trigrams F 15 0 10 0.2 0.4 0.3 5 0.1 0 NYT95/NYT95’ NYT95/AFP95 26 and leads to smaller observed discounts. 2.4 Related Work The results of section 2.1 point to a remarkably pervasive phenomenon of growing empirical discounts, except in the case of extremely similar corpora. Growing discounts of this sort were previously suggested by the model of Teh (2006). However, we claim that the discounting phenomenon in our data is fundamentally different from his model’s prediction. In the held-out experiments of section 2.1, growing discounts only emerge when one evaluates against a dissimilar held-out corpus, whereas his model would predict discount growth even in NYT95/NYT95&apos;, where we do not observe it. Adaptation across corpora has also been addressed before. Bellegarda (2004) describes a range of techniques, from interpolation at either the count level or the model level (Bacchiani and Roark, 2003; Bacchiani et al., 2006) to using explicit models o</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes. In Proceedings of ACL, pages 985–992, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>