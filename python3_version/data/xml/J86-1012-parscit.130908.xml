<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.526238">
ABSTRACTS OF CURRENT LITERATURE
</title>
<figure confidence="0.910926869565217">
Copies of the technical reports abstracted below are available from
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, CANADA M5S 1A4
A Computational Model for the Analysis
of Argiunents
Robin Cohen
Ph.D. Thesis
Technical Report CSRG-151,
October 1983, 225 pages
Understanding Adjectives
Yawar Au
M.Sc. Thesis
Technical Report CSRI-167,
January 1985, 85 pages
Rule-Based Processing in a
Connectionist System for Natural
Language Understanding
Bart Selman
M.Sc. Thesis
Technical Report CSRI-168,
February 1985, 57 pages
</figure>
<bodyText confidence="0.999963760869565">
This thesis proposes a model for an argument understanding system — a
natural language understanding system which processes arguments. The
form of input considered is one-way communication in a conversational
setting, where the speaker tries to convince the hearer of a particular point
of view. The main contributions are: (i) a theory of expected coherent
structure which limits analysis to the reconstruction of particular
transmission forms; (ii) a theory of linguistic clues which assigns a func-
tional interpretation to special words and phrases used by the speaker to
indicate structure; (iii) a theory of evidence relationships which includes
the demand for pragmatic analysis to accommodate beliefs not currently
held. A system designed to incorporate these theories could be used to
analyze the structure of arguments — the necessary first step for a hearer,
before judging credibility and responding.
This thesis deals with the task of understanding noun phrases containing
sequences of prenominal adjectives.
The first problem is to determine exactly what each adjective modifies.
In general, this can only be done by taking account of the semantic
properties of the adjective in question, as well as those of other adjectives
to its right, and of the noun itself. &amp;quot;Real-world&amp;quot; knowledge and contextual
factors also play a role in this process. This is addressed by developing a
classification scheme for adjectives which allows us to substantially reduce
the number of candidate interpretations, in some cases to a single one. A
system is presented which takes account of the disparate semantic behavi-
our of different classes of adjectives, word order, punctuation in the noun
phrase, and a frame-based store of real-world knowledge, in order to
determine the scope of adjectives within a noun phrase.
The second problem is to construct a representation of the description
embodied in such a noun phrase. Here, it is desirable that the structure of
the representation correspond to the structure of modification within the
phrase. Particular adjectives are taken to indicate restrictions on the values
that objects may take on for associated properties. These properties may
be featural, dimensional, or functional in nature. Frame-like structures are
used to represent the generic concepts that are taken to be associated with
noun phrases.
We present a connectionist model for natural language processing. In
contrast with previously proposed schemes, this scheme handles
traditionally sequential rule-based processing in a general manner in the
network. Another difference is the use of a computational scheme similar
to the one used in the Boltzmann machine. This allows us to formulate
general rules for the setting of weights and thresholds.
We give a detailed description of a parsing system based on context-free
grammar rules. Using simulated annealing, we show that at low temper-
atures the time average of the visited states at thermal equilibrium repres-
ents the correct parse of the input system.
The system is built from a small set of connectionist primitives that repre-
sent the grammar rules. These primitives are linked together using pairs of
</bodyText>
<page confidence="0.946238">
58 Computational Linguistics, Volume 12, Number 1, January-March 1986
</page>
<note confidence="0.932535">
The FINITE STRING Abstracts of Current Literature
</note>
<bodyText confidence="0.9941724">
computing units that behave like discrete switches. These units are used as
binders between concepts. They can be linked in such a way that individ-
ual rules can be selected from a collection of rules, and are very useful in
the construction of connectionist schemes for any form of rule-based proc-
essing.
</bodyText>
<figure confidence="0.9524266875">
Theory and Parsing of the Coordinate
Conjunction &amp;quot;and&amp;quot;
Victoria L. Snarr
M.Sc. Thesis
Technical Report CSRI-171,
September 1985, 71 pages
Toward a Computational Interpretation
of Situation Semantics
Yves Lesperance
To appear, November 1985
The Representation of Ambiguity in
Opaque Constructs
Brenda Fawcett
M.Sc. Thesis
October 1985
Technical Report, December 1985
</figure>
<bodyText confidence="0.995143591836735">
Although the conjunction and appears to have a simple function in the
English language, it has proved to be a stumbling block for both theoretical
and computational linguists.
One of the theoretical problems of conjunction is to determine what
governs the acceptability of a structure in which two elements are
connected by and. The corresponding computational problem is, given this
knowledge, to incorporate it into an efficient parser for English.
This thesis proposes a solution to the theoretical problem which is in the
form of two general constraints — a syntactic constraint and a semantic
one; and then incorporates these constraints into a &amp;quot;strictly deterministic&amp;quot;
parser for English.
Situation Semantics proposes novel and attractive treatments for several
problem areas of natural language semantics, such as efficiency (context
sensitivity) and propositional attitude reports. Its focus on the information
carried by utterances makes the approach very promising for accounting
for pragmatic phenomena. However, Situation Semantics seems to oppose
several basic assumptions underlying current approaches to natural
language processing and the design of intelligent systems in general. It
claims that efficiency undermines the standard notions of logical form,
entailment, and proof theory, and objects to the view that mental processes
necessarily involve internal representations.
The paper attempts to clarify these issues and discusses the impact of
Situation Semantics&apos; criticisms for natural language processing, knowledge
representation, and reasoning. I claim that the representational approach
is the only currently practical one for the design of large intelligent
systems, but argue that the representations used should be efficient in
order to account for the system&apos;s embedding in its environment. The paper
concludes by stating some constraints that a computational interpretation
of Situation Semantics should obey and discussing remaining problems.
A knowledge of intensions, which are used to designate concepts of
objects, is important for natural language processing systems. Certain
linguistic phrases can refer either to the concept of an entity or to the
entity itself. To properly understand a phrase and to prevent invalid
inferences from being drawn, the system must determine the type of
reference being asserted. We identify a set of &amp;quot;opaque&amp;quot; constructions
and suggest that a common mechanism be developed to handle them.
To account for the ambiguities of opaque contexts, noun phrases are
translated into descriptors. It must be made explicit to whom the descriptor
is ascribed and whether its referent is non-specific or specific. Similarly,
sentential constituents should be treated as propositions and evaluated rela-
tive to conjectured states of affairs. As a test bed for these ideas we define
a Montague-style meaning representation and implement the syntactic and
semantic components of a moderate-size NLP system in a logic program-
ming environment.
One must also consider how to disambiguate and interpret such a repre-
sentation with respect to a knowledge base. Much contextual and world
knowledge is required. We characterize what facilities are necessary for an
accurate semantic interpretation, considering what is and is not available in
current knowledge representation systems.
</bodyText>
<note confidence="0.779641">
Computational Linguistics, Volume 12, Number 1, January-March 1986 59
The FINITE STRING Abstracts of Current Literature
</note>
<bodyText confidence="0.51358">
The following abstracts are from Computational intelligence (a.k.a. Intelligence informatique), Volume 1, Number 2
(May 1985).
</bodyText>
<figure confidence="0.992639538461538">
What is a Heuristic?
Marc H.J. Romanycia
Information Services, Engineering and
Planning
Gulf Canada
Calgary, Alberta, Canada 72P 2H7
Francis Jeffry Pelletier
Departments of Philosophy and
Computing Science
University of Alberta
Edmonton, Alberta, Canada T6G 2E5
1(2): 47-58
Possible Events, Actual Events, and Robots
Andrew Haas
BBN Laboratories
10 Moulton Street
Cambridge, MA 02238
1(2): 59-70
A Functional Approach to Non-Monotonic
Logic
Erik Sandewall
Department of Computer and Information
Science
Linkoping University
Linkiiing, Sweden
1(2): 80-87
</figure>
<bodyText confidence="0.999745135135135">
From the mid-1950s to the present the notion of a heuristic has played a
crucial role in the Al researchers&apos; descriptions of their work. What has not
been generally noticed is that different researchers have often applied the
term to rather different aspects of their programs. Things that would be
called a heuristic by one researcher would not be so called by others. This is
because many heuristics embody a variety of different features, and the
various researchers have emphasized different ones of these features as
being essential to being a heuristic. This paper steps back from any partic-
ular research program and investigates the question of what things, histor-
ically, have been thought to be central to the notion of a heuristic and
which ones conflict with others. After analyzing the previous definitions
and examining current usage of the term, a synthesizing definition is
provided. The hope is that with this broader account of &amp;quot;heuristic&amp;quot; in
hand, researchers can benefit more fully from the insights of others, even if
those insights are couched in a somewhat alien vocabulary.
To plan means reasoning about possible actions, but a robot must also
reason about actual events. This paper proposes a formal theory about
actual and possible events. It presents a new modal logic as a notation for
this theory and a technique for planning in the modal logic using a first-
order theorem prover augmented with simple modal reasoning. This avoids
the need for a general modal-logic theorem prover. Adding beliefs to this
theory raises an interesting problem for which the paper offers a tentative
solution.
Axiom sets and their extensions are viewed as functions from the set of
formulas in the language to a set of four truth values: t, f, u for undefined,
and k for contradiction. Such functions form a lattice with &amp;quot;contains less
information&amp;quot; as the partial order E, and &amp;quot;combination of several sources
of knowledge&amp;quot; as the least-upper-bound operation u. Inference rules are
expressed as binary relations between such functions. We show that the
usual criterion on fixpoints, namely, to be minimal, does not apply correct-
ly in the case of non-monotonic inference rules. A stronger concept,
approachable fixpoints, is introduced and proven to be sufficient for the
existence of a derivation of the fixpoint. In addition, the usefulness of our
approach is demonstrated by concise proofs for some previously known
results about normal default rules.
The following abstracts are from the Proceedings of the 23rd Annual Meeting of the Association for Computational
Linguistics. See the form at the end of this issue for ordering information, or contact
</bodyText>
<note confidence="0.411003">
Donald E. Walker, ACL
</note>
<footnote confidence="0.324921">
Bell Communications Research
445 South Street, MRE 2A379
Morristown, NJ 07960 USA
</footnote>
<subsectionHeader confidence="0.9387435">
Semantics of Temporal Queries and
Temporal Data
</subsectionHeader>
<author confidence="0.746585">
Carole D. Hafner
</author>
<affiliation confidence="0.835116">
College of Computer Science
Northeastern University
Boston, MA 02115
</affiliation>
<note confidence="0.682357">
Proc. ACL, pp. 1-8
</note>
<title confidence="0.527377">
Temporal Inferences in Medical Texts
</title>
<author confidence="0.303589">
Klaus K. Obermeier
</author>
<bodyText confidence="0.998964857142857">
This paper analyzes the requirements for adding a temporal reasoning
component to a natural language database query system, and proposes a
computational model that satisfies those requirements. A preliminary
implementation in Prolog is used to generate examples of the model&apos;s
capabilities.
The objectives of this paper are twofold, whereby the computer program
is meant to be a particular implementation of a general natural language
</bodyText>
<page confidence="0.93434">
60 Computational Linguistics, Volume 12, Number 1, January-March 1986
</page>
<note confidence="0.8594044">
The FINITE STRING Abstracts of Current Literature
Battelle&apos;s Columbus Laboratories
505 King Avenue
Columbus, OH 43201-2693 USA
Proc. ACL, pp. 9-17
</note>
<table confidence="0.413458533333333">
Tense, Aspect and the Cognitive
Representation of Time
Kenneth Man-kam Yip
Artificial Intelligence Laboratory, M.I.T.,
545 Technology Square
Cambridge, MA 02139
Proc. ACL, pp. 18-26
Classification of Modality Function and its
Application to Japanese Language Analysis
Show Naito, Akira Shimazu, Hirosato Nomura
Musashino Electrical Communication
Laboratories, N.T.T.
3-9-11, Midori-cho, Musashino-shi
Tokyo, 180, Japan
Proc. ACL, pp. 27-34
</table>
<subsectionHeader confidence="0.73814525">
Universality and Individuality: The Inter-
action of Noun Phrase Determiners in
Copular Clauses
John C. Mallery
</subsectionHeader>
<footnote confidence="0.724898">
Political Science Department &amp; Artificial
Intelligence Laboratory, M.I.T.
545 Technology Square, NE43-797
Cambridge, MA 02139
Proc. ACL, pp. 35-42
</footnote>
<bodyText confidence="0.991657970149254">
processing system which could be used for different domains. The first
objective is to provide a theory for processing temporal information
contained in a well-structured, technical text. The second objective is to
argue for a knowledge-based approach to natural language processing in
which the parsing procedure is driven by extralinguistic knowledge.
The resulting computer program incorporates enough domain-specific
and general knowledge so that the parsing procedure can be driven by the
knowledge base of the program, while at the same time employing a
descriptively adequate theory of syntactic processing, i.e., X-bar syntax.
My parsing algorithm not only supports the prevalent theories of know-
ledge-based parsing put forth in Al, but also uses a sound linguistic theory
for the necessary syntactic information processing.
This paper explores the relationships between a computational theory of
temporal representation (as developed by James Allen) and a formal
linguistic theory of tense (as developed by Norbert Hornstein) and aspect.
It aims to provide explicit answers to four fundamental questions: (1) what
is the computational justification for the primitives of a linguistic theory;
(2) what is the computational explanation of the formal grammatical
constraints; (3) what are the processing constraints imposed on the learna-
bility and markedness of these theoretical constructs; and (4) what are the
constraints that a linguistic theory imposes on representations. We show
that one can effectively exploit the interface between the language faculty
and the cognitive faculties by using linguistic constraints to determine
restrictions on the cognitive representations and vice versa.
Three main results are obtained: (1) We derive an explanation of an
observed grammatical constraint on tense — the Linear Order Constraint —
from the information monotonicity property of the constraint propagation
algorithm of Allen&apos;s temporal system; (2) We formulate a principle of
markedness for the basic tense structures based on the computational effi-
ciency of the temporal representations; and (3) We show Allen&apos;s interval-
based temporal system is not arbitrary, but it can be used to explain
independently motivated linguistic constraints on tense and aspect inter-
pretations.
We also claim that the methodology of research developed in this study
— &amp;quot;cross-level&amp;quot; investigation of independently motivated formal grammat-
ical theory and computational models — is a powerful paradigm with which
to attack representational problems in basic cognitive domains, e.g., space,
time, causality, etc.
This paper proposes an analysis method for Japanese modality. In this
purpose, meaning of Japanese modality is classified into four semantic
categories and the role of it is formalized into five modality functions.
Based on these formalizations, information and constraints to be applied to
the modality analysis procedure are specified. Then by combining these
investigations with case analysis, the analysis method is proposed. This
analysis method has been applied to Japanese analysis for machine trans-
lation.
This paper presents an implemented theory for quantifying noun phrases in
clauses containing copular verbs (e.g., &apos;be&apos; and `become&apos;). Proceeding
from recent theoretical work by Jackendoff (1983), this computational
theory recognizes the dependence of the quantification decision on the
definiteness, indefiniteness, or classness of both the subject and object of
copular verbs in English. Jackendoff&apos;s intuition about the quantificational
interdependence of subject and object has been imported from his broader
cognitive theory and reformulated within a constraint propagation frame-
work. Extensions reported here include the addition of more active deter-
Computational Linguistics, Volume 12, Number 1, January-March 1986 61
The FINITE STRING Abstracts of Current Literature
miners, the expansion of determiner categories, and the treatment of
displaced objects. A further finding is that quantificational constraints
may propagate across some clausal boundaries. The algorithm is used by
the RELATUS Natural Language Understanding System during a phase of
analysis that posts constraints to produce a &apos;constraint tree&apos;. This phase
comes after creation of syntactic deep structure and before sentential
reference in a semantic-network model. Incorporation of the quantifica-
tion algorithm in a larger system that parses sentences and builds semantic
models from them makes RELATUS able to acquire taxonomic and identity
information from text.
</bodyText>
<subsectionHeader confidence="0.896207">
Meinongian Semantics for Propositional
Semantic Networks
</subsectionHeader>
<author confidence="0.884004">
William J. Rapaport
</author>
<affiliation confidence="0.9957675">
Department of Computer Science,
State University of New York
</affiliation>
<figure confidence="0.211305266666667">
Buffalo, NY 14260
Proc. ACL, pp. 43-48
Speech Acts and Rationality
Philip R. Cohen
Artificial Intelligence Center, SRI Interna-
tional &amp; CSLI, Stanford University
Hector J. Levesque
Department of Computer Science
University of Toronto
Proc. ACL, pp. 49-60
Ontological Promiscuity
Jerry R. Hobbs
Artificial Intelligence Center, SRI Interna-
tional &amp; CSLI, Stanford University
Proc. ACL, pp. 61-69
</figure>
<subsectionHeader confidence="0.691623333333333">
Reversible Automata and Induction of the
English Auxiliary System
Samuel F. Pilate, Robert C. Berwick
Artificial Intelligence Laboratory, M.I.T.
545 Technology Square
Cambridge, MA 02139
</subsectionHeader>
<bodyText confidence="0.667244">
Proc. ACL, pp. 70-75
</bodyText>
<subsectionHeader confidence="0.844124666666667">
The Computational Difficulty of ID/LP
Parsing
G. Edward Barton, Jr.
</subsectionHeader>
<bodyText confidence="0.999437139534884">
This paper surveys several approaches to semantic-network semantics that
have not previously been treated in the AI or computational linguistics
literature, though there is a large philosophical literature investigating them
in some detail. In particular, propositional semantic networks (exemplifed
by SNePS) are discussed, it is argued that only a fully intensional
(&amp;quot;Meinongian&amp;quot;) semantics is appropriate for them, and several Meinongi-
an systems are presented.
This paper derives the basis of a theory of communication from a formal
theory of rational interaction. The major result is a demonstration that
illocutionary acts need not be primitive, and need not be recognized. As a
test case, we derive Searle&apos;s conditions on requesting from principles of
rationality coupled with a Gricean theory of imperatives. The theory is
shown to distinguish insincere or nonserious imperatives from true
requests. Extensions to indirect speech acts, and ramifications for natural
language systems are also briefly discussed.
To facilitate work in discourse interpretation, the logical form of English
sentences should be both close to English and syntactically simple. In this
paper I propose a logical notation which is first-order and nonintensional,
and for which semantic translation can be naively compositional. The key
move is to expand what kinds of entities one allows in one&apos;s ontology,
rather than complicating the logical notation, the logical form of sentences,
or the semantic translation process. Three classical problems — opaque
adverbials, the distinction between de re and de dicto belief reports, and the
problem of identity in intensional contexts — are examined for the difficul-
ties they pose for this logical notation, and it is shown that the difficulties
can be overcome. The paper closes with a statement about the view of
semantics that is presupposed by this approach.
In this paper we apply some recent work of Angluin (1982) to the in-
duction of the English auxiliary verb system. In general, the induction of
finite automata is computationally intractable. However, Angluin shows
that restricted finite automata, the k-reversible automata, can be learned by
efficient (polynomial time) algorithms. We present an explicit computer
model demonstrating that the English auxiliary verb system can in fact be
learned as a 1-reversible automaton, and hence in a computationally feasi-
ble amount of time. The entire system can be acquired by looking at only
half the possible auxiliary verb sequences, and the pattern of generalization
seems compatible with what is known about human acquisition of auxilia-
ries. We conclude that certain linguistic subsystems may well be feasible
by inductive inference methods of this kind, and suggest an extension to
context-free language.
Modern linguistic theory attributes surface complexity to interacting
subsystems of constraints. For instance, the ID/LP grammar formalism
separates constraints on immediate dominance from those on linear order.
</bodyText>
<page confidence="0.970123">
62 Computational Linguistics, Volume 12, Number 1, January-March 1986
</page>
<table confidence="0.980193454545455">
The FINITE STRING Abstracts of Current Literature
Artificial Intelligence Laboratory, M.I.T.
545 Technology Square
Cambridge, MA 02139
Proc. ACL, pp. 76-81
Modular Logic Grammars
Michael C. McCord
IBM Thomas J. Watson Research Center
P.O. Box 218
Yorktown Heights, NY 10598
Proc. ACL, pp. 104-117
</table>
<bodyText confidence="0.999402075471698">
Shieber&apos;s (1983) ID/LP parsing algorithm shows how to use ID and LP
constraints directly in language processing, without expanding them into an
intermediate &amp;quot;object grammar&amp;quot;. However, Shieber&apos;s purported
0(1 G 12- n2) runtime bound underestimates the difficulty of ID/LP parsing.
ID/LP parsing is actually NP-complete, and the worst-case runtime of
Shieber&apos;s algorithm is actually exponential in grammar size. The growth of
parser data structures causes the difficulty. Some computational and
linguistic implications follow; in particular, it is important to note that
despite its potential for combinatorial explosion, Shieber&apos;s algorithm re-
mains better than the alternative of parsing an expanded object grammar.
Tree Adjoining Grammar (TAG) is a formalism for natural language gram-
mars. Some of the basic notions of TAGs were introduced in Joshi, Levy,
and Takahashi (1975) and by Joshi (1983). A detailed investigation of the
linguistic relevance of TAGs has been carried out in Kroch and Joshi
(1985). In this paper, we will describe some new results for TAGs, espe-
cially in the following areas: (1) parsing complexity of TAGs, (2) some
closure results for TAGs, and (3) the relationship to Head grammars.
Tree Adjoining Grammars, or &amp;quot;TAGs&amp;quot;, (Joshi, Levy &amp; Takahashi 1975;
Joshi 1983; Broch &amp; Joshi 1985) were developed as an alternative to the
standard syntactic formalisms that are used in theoretical analyses of
language. They are attractive because they may provide just the aspects of
context sensitive expressive power that actually appear in human languages
while otherwise remaining context free.
This paper describes how we have applied the theory of Tree Adjoining
Grammars to natural language generation. We have been attracted to
TAGs because their central operation — the extension of an &amp;quot;initial&amp;quot; phrase
structure tree through the inclusion, at very specifically constrained
locations, of one or more &amp;quot;auxiliary&amp;quot; trees — corresponds directly to certain
central operations of our own, performance-oriented theory.
We begin by briefly describing TAGs as a formalism for phrase structure
in a competence theory, and summarize the points in the theory of TAGs
that are germane to our own theory. We then consider generally the posi-
tion of a grammar within the generation process, introducing our use of
TAGs through a contrast with how others have used systemic grammars.
This takes us to the core results of our paper: using examples from our
research with well-written texts from newspapers, we walk through our
TAG inspired treatments of raising and wh-movement, and show the corre-
spondence of the TAG &amp;quot;adjunction&amp;quot; operation and our &amp;quot;attachment&amp;quot;
process.
In the final section we discuss extensions to the theory, motivated by the
way we use the operation corresponding to TAGs&apos; adjunction in perform-
ance. This suggests that the competence theory of TAGs can be profitably
projected to structures at the morphological level as well as the present
syntactic level.
This report describes a logic grammar formalism, Modular Logic Gram-
mars, exhibiting a high degree of modularity between syntax and seman-
tics. There is a syntax rule compiler (compiling into Prolog) which takes
care of the building of analysis structures and the interface to a clearly
separated semantic interpretation component dealing with scoping and the
construction of logical forms. The whole system can work in either a one-
pass mode or a two-pass mode. In the one-pass mode, logical forms are
built directly during parsing through interleaved calls to semantics, added
automatically by the rule compiler. In the two-pass mode, syntactic analy-
</bodyText>
<figure confidence="0.890173222222222">
Some Computational Properties of Tree
Adjoining Grammars
K. Vijay-Shankar, Aravind K. Joshi
Department of Computer and Information
Science
Room 268, Moore School/D2
University of Pennsylvania
Philadelphia, PA 19104
Proc. ACL, pp. 82-93
TAGs as a Grammatical Formalism for
Generation
David D. McDonald, James D. Pustejovsky
Department of Computer and Information
Science
University of Massachusetts at Amherst
Proc. ACL, pp. 94-103
Computational Linguistics, Volume 12, Number 1, January-March 1986 63
The FINITE STRING
New Approaches to Parsing Conjunctions
using Prolog
Sandimy Fong, Robert C. Berwick
Artificial Intelligence Laboratory, M.I.T.
545 Technology Square
Cambridge, MA 02139
Proc. ACL, pp. 118-126
Parsing with Discontinuous Constituents
Mark Johnson
CSLI &amp; Department of Linguistics,
Stanford University,
Proc. ACL, pp. 127-132
Structure Sharing with Binary Trees
Lauri Karttunen
SRI International, CSLI Stanford
Martin Kay
Xerox PARC, CSLI Stanford
Proc. ACL, pp. 133-136
</figure>
<subsectionHeader confidence="0.867261">
Abstracts of Current Literature
</subsectionHeader>
<bodyText confidence="0.99973045">
sis trees are built automatically in the first pass, and then given to the
(one-pass) semantic component. The grammar formalism includes two
devices which cause the automatically built syntactic structures to differ
from derivation trees in two ways: (1) there is a shift operator, for dealing
with left-embedding constructions such as English possessive noun phrases
while using right-recursive rules (which are appropriate for Prolog pars-
ing.) (2) There is a distinction in the syntactic formalism between strong
non-terminals and weak non-terminals, which is important for distinguish-
ing major levels of grammar.
Conjunctions are particularly difficult to parse in traditional, phrase-based
grammars. This paper shows how a different representation, not based on tree
structures, markedly improves the parsing problem for conjunctions.
It modifies the union of phrase marker model proposed by Goodal (1984),
where conjunction is considered as the linearization of a three-dimensional
union of a non-tree based phrased marker representation. A PROLOG
grammar for conjunctions using this new approach is given. It is far
simpler and more transparent than a recent phrase-based extraposition
parser for conjunctions by Dahl and McCord (1984). Unlike the Dahl and
McCord or ATN SYSCONJ approach, no special trail machinery is needed
for conjunction, beyond that required for analyzing simple sentences.
While of comparable efficiency, the new approach unifies under a single
analysis a host of related constructions: respectively sentences, right node
raising, or gapping. Another advantage is that it is also completely revers-
ible (without cuts), and therefore can be used to generate sentences.
By generalizing the notion of location of a constituent to allow discontin-
uous locations, one can describe the discontinuous constituents of non-
configurational languages. These discontinuous constituents can be
described by a variant of definite clause grammars, and these grammars
can be used in conjunction with a proof procedure to create a parser for
non-configurational languages.
Many current interfaces for natural language represent syntactic and se-
mantic information in the form of directed graphs where attributes corre-
spond to vectors and values to nodes. There is a simple correspondence
between such graphs and the matrix notation linguists traditionally use
for feature sets. The standard operation for working with such graphs is uni-
fication. The unification operation succeeds only on a pair of compatible
graphs, and its result is a graph containing the information in both contrib-
utors. When a parser applies a syntactic rule, it unifies selected features of
input constituents to check constraints and to build a representation for the
output constituent.
</bodyText>
<figure confidence="0.8699289">
cat agr
np
a.
number person
3rd
b. cat: n p
[
agr:
number: sg
[person: 3rd]
</figure>
<title confidence="0.365397">
A Structure-Sharing Representation for
Unification-Based Grammar Formalisms
</title>
<subsubsectionHeader confidence="0.529706">
Fernando C.N. Pereira
</subsubsectionHeader>
<bodyText confidence="0.947056714285714">
Artificial Intelligence Center, SRI Interna-
tional &amp; CSLI, Stanford University
This paper describes a structure-sharing method for the representation of
complex phrase types in a parser for PATR-II, a unification-based grammar
formalism.
In parsers for unification-based grammar formalisms, complex phrase
types are derived by incremental refinement of the phrase types defined in
</bodyText>
<page confidence="0.745678">
Computational Linguistics, Volume 12, Number 1, January-March 1986
64
</page>
<figure confidence="0.71478928">
The FINITE STRING Abstracts of Current Literature
Proc. ACL, pp. 137-144
Using Restriction to Extend Parsing
Algorithms for Complex-Feature-Based
Formalisms
Stuart M. Shieber
Artificial Intelligence Center, SRI Interna-
tional &amp; CSLI, Stanford University
Proc. ACL, pp. 145-152
Semantic Caseframe Parsing and Syntactic
Generality
Philip J. Hayes, Peggy M. Anderson,
Scott Sailer
Carnegie Group Incorporated
Commerce Court at Station Square
Pittsburgh, PA 15219
Proc. ACL, pp. 153-160
Movement in Active Production Networks
Mark A. Jones, Alan S. Driscoll
AT&amp;T Bell Laboratories
Murray Hill, NJ 07974
Proc. ACL, pp. 161-166
Parsing Head-driven Phrase Structure
Grammar
Derek Proudian, Carl Pollard
</figure>
<footnote confidence="0.475434333333333">
Hewlett-Packard Laboratories
1501 Page Mill Road
Palo Alto, CA 94303
</footnote>
<bodyText confidence="0.987654079365079">
Proc. ACL, pp. 167-171
grammar rules and lexical entries. In a naive implementation, a new phrase
type is built by copying older ones and then combining the copies accord-
ing to the constraints stated in the grammar rule. The structure-sharing
method was designed to eliminate most such copying; indeed, practical
tests suggest that the use of this technique reduces parsing time by as much
as 60%.
The present work is inspired by the structure-sharing method for theo-
rem proving introduced by Boyer and Moore and on the variant of it that is
used in some Prolog implementations.
Grammar formalisms based on the encoding of grammatical information in
complex-valued feature systems enjoy some currency both in linguistics
and natural-language-processing research. Such formalisms can be thought
of by analogy to context-free grammars as generalizing the notion of non-
terminal symbol from a finite domain of atomic elements to a possibly infi-
nite domain of directed graph structures of a certain sort. Unfortunately,
in moving to an infinite non-terminal domain, standard methods of parsing
may no longer be applicable to the formalism. Typically, the problem
manifests itself as gross inefficiency or even non-termination of the algo-
rithms. In this paper, we discuss a solution to the problem of extending
parsing algorithms to formalisms with possibly infinite non-terminal
domains, a solution based on a general technique we call restriction. As a
particular example of such an extension, we present a complete, correct,
terminating extension of Earley&apos;s algorithm that uses restriction to perform
top-down filtering. Our implementation of this algorithm demonstrates the
drastic elimination of chart edges that can be achieved by this technique.
Finally, we describe further uses for the technique — including parsing
other grammar formalisms, including definite-clause grammars; extending
other parsing algorithms, including LR methods and syntactic preference
modeling algorithms; and efficient indexing.
We have implemented a restricted domain parser called Plume (trademark
of Carnegie Group Incorporated). Building on previous work at Carne-
gie-Mellon University, Plume&apos;s approach to parsing is based on semantic
caseframe instantiation. This has the advantages of efficiency on gram-
matical input, and robustness in the face of ungrammatical input. While
Plume is well adapted to simple declarative and imperative utterances, it
handles passives and relative clauses and interrogatives in an ad hoc
manner, leading to patchy semantic coverage. This paper outlines Plume
as it currently exists and describes our detailed design for extending Plume
to handle passives, relative clauses, and interrogatives in a general manner.
We describe how movement is handled in a class of computational devices
called active production networks (APNs). The APN model is a parallel, acti-
vation-based framework that has been applied to other aspects of natural
language processing. The model is briefly defined, the notation and mech-
anism for movement is explained, and then several examples are given
which illustrate how various conditions on movement can naturally be
explained in terms of limitations of the APN device.
The Head-driven Phrase Structure Grammar project (HPSG) is an English
language database query system under development at Hewlett-Packard
Laboratories. Unlike other product-oriented efforts in the natural
language understanding field, the HPSG system was designed and imple-
mented by linguists on the basis of recent theoretical developments. But,
unlike other implementations of linguistic theories, this system is not a toy,
as it deals with a variety of practical problems not covered in the theore-
Computational Linguistics, Volume 12, Number 1, January-March 1986 65
The FINITE STRING Abstracts of Current Literature
tical literature. We believe that this makes the HPSG system unique in its
combination of linguistic theory and practical application.
The HPSG system differs from its predecessor GPSG, reported on at the
1982 ACL meeting (Gawron et al. 1982), in four significant respects:
syntax, lexical representation, parsing, and semantics. The paper focuses
on parsing issues, but also gives a synopsis of the underlying syntactic
formalism.
</bodyText>
<figure confidence="0.7960732">
A Computational Semantics for Natural
Language
Lewis G. Creary, Carl J. Pollard
Hewlett-Packard Laboratories
1501 Page Mill Road
Palo Alto, CA 94303
Proc. ACL, pp. 172-1 79
Analysis of Conjunctions in a Rule-Based
Parser
Leonard Lesmo, Pietro Totusso
Dipartimento di Informatica
Universita di Torino
Via Valperga Caluso 37
10125 Torino, Italy
Proc. ACL, pp. 180-18 7
</figure>
<bodyText confidence="0.999930347826087">
In the new Head-Driven Phrase Structure Grammar (HPSG) language
processing system that is currently under development at Hewlett-Packard
Laboratories, the Montagovian semantics of the earlier GPSG system (see
Gawron et al. 1982) is replaced by a radically different approach with a
number of distinct advantages. In place of the lambda calculus and stand-
ard first-order logic, our medium of conceptual representation is a new
logical formalism called NFLT (Neo-Fregean Language of Thought);
compositional semantics is effected, not by schematic lambda expressions,
but by LISP procedures that operate on NFLT expressions to produce new
expressions. NFLT has a number of features that make it well-suited for
natural language translations, including predicates of variable arity in
which explicitly marked situational roles supersede order-coded argument
positions, sortally restricted quantification, a compositional (but nonexten-
sional) semantics that handles causal contexts, and a principled conceptual
raising mechanism that we expect to lead to a computationally tractable
account of propositional attitudes. The use of semantically compositional
LISP procedures in place of lambda-schemas allows us to produce fully
reduced translations on the fly, with no need for post-processing. This
approach should simplify the task of using semantic information (such as
sortal incompatibilities) to eliminate bad parse paths.
The aim of the present paper is to show how a rule-based parser for the
Italian language has been extended to analyze sentences involving conjunc-
tions. The most noticeable fact is the ease with which the required modifi-
cations fit in the previous parser structure. In particular, the rules written
for analyzing simple sentences (without conjunctions) needed only small
changes. On the contrary, more substantial changes were made to the
exception-handling rules (called &amp;quot;natural changes&amp;quot;) that are used to
restructure the tree in case of failure of a syntactic hypothesis. The parser
described in the present work constitutes the syntactic component of the
FIDO system (a Flexible Interface for Database Operations), an interface
allowing an end-user to access a relational database in natural language
(Italian).
Intersentential elliptical utterances occur frequently in information-seeking
dialogues. This paper presents a pragmatics-based framework for inter-
preting such utterances, including identification of the speaker&apos;s discourse
goal in employing the fragment. We claim that the advantage of this
approach is its reliance upon pragmatic information, including discourse
content and conversational goals, rather than upon precise representations
of the preceding utterance alone.
In this paper we examine the pragmatic knowledge an utterance-planning
system must have in order to produce certain kinds of definite and indefi-
nite noun phrases. An utterance-planning system, like other planning
systems, plans actions to satisfy an agent&apos;s goals, but allows some of the
actions to consist of the utterance of sentences. This approach to language
generation emphasizes the view of language as action, and hence assigns a
critical role to pragmatics.
</bodyText>
<table confidence="0.961314222222222">
A Pragmatics-Based Approach to Under-
standing Intersentential Ellipsis
Sandra Carberry
Department of Computer and Information
Science, University of Delaware
Newark, DE 19715
Proc. ACL, pp. 188-197
Some Pragmatic Issues in the Planning of
Definite and Indefinite Noun Phrases
Douglas E. Appelt
Artificial Intelligence Center, SRI Interna-
tional &amp; CSLI, Stanford University
Proc. ACL, pp. 198-203
66 Computational Linguistics, Volume 12, Number 1, January-March 1986
The FINITE STRING Abstracts of Current Literature
Repairing Reference Identification Failures
by Relaxation
Bradley A. Goodman
BBN Laboratories
10 Moulton Street
Cambridge, MA 02238
Proc. ACL, pp. 204-217
Anaphora Resolution: Short-Term
Memory and Focusing
Raymonde Guindon
Microelectronics and Computer Technology
Corporation (MCC)
9430 Research Blvd.
Austin, TX 78759
Proc. ACL, pp. 218-227
Explanation Structures in XSEL
Karen Kukich
Computer Science Department
Carnegie-Mellon University
Pittsburgh, PA 15213
Proc. ACL, pp. 228-237
</table>
<bodyText confidence="0.999898436363637">
The goal of this work is the enrichment of human-machine interactions in
a natural language environment. We want to provide a framework less
restrictive than earlier ones by allowing a speaker leeway in forming an
utterance about a task and in determining the conversational vehicle to
deliver it. A speaker and listener cannot be assumed to have the same
beliefs, contexts, backgrounds, or goals at each point in a conversation. As
a result, difficulties and mistakes arise when a listener interprets a speak-
er&apos;s utterance. These mistakes can lead to various kinds of misunder-
standings between speaker and listener, including reference failures or fail-
ure to understand the speaker&apos;s intention. We call these misunderstandings
miscommunication. Such mistakes constitute a kind of &amp;quot;ill-formed&amp;quot; input
that can slow down and possibly break down communication. Our goal is
to recognize and isolate such miscommunications and circumvent them.
This paper will highlight a particular class of miscommunication — refer-
ence problems — by describing a case study, including techniques for avoid-
ing failures of reference.
Anaphora resolution is the process of determining the referent of
anaphors, such as definite noun phrases and pronouns, in a discourse.
Computational linguists, in modeling the process of anaphora resolution,
have proposed the notion of focusing. Focusing is the process, engaged in
by a reader, of selecting a subset of the discourse items and making them
highly available for further computations. This paper provides a cognitive
basis for anaphora resolution and focusing. Human memory is divided into
a short-term, an operating, and a long-term memory. Short-term memory
can only contain a small number of meaning units and its retrieval time is
fast. Short-term memory is divided into a cache and a buffer. The cache
contains a subset of meaning units expressed in the previous sentences and
the buffer holds a representation of the incoming sentence. Focusing is
realized in the cache that contains a subset of the most topical units and a
subset of the most recent units in the text. The information stored in the
cache is used to integrate the incoming sentence with the preceding
discourse. Pronouns should be used to refer to units in focus. Operating
memory contains a very large number of units but its retrieval time is slow.
It contains the previous text units that are not in the cache. It comprises
the text units not in focus. Definite noun phrases should be used to refer
to units not in focus. Two empirical studies are described that demonstrate
the cognitive basis for focusing, the use of definite noun phrases to refer to
antecedents not in focus, and the use of pronouns to refer to antecedents
in focus.
Expert systems provide a rich testbed from which to develop and test tech-
niques for natural language processing. These systems capture the know-
ledge needed to solve real-world problems in their respective domains, and
that knowledge can and should be exploited for testing computational
procedures for natural language processing. Parsing, semantic interpreta-
tion, dialog monitoring, discourse organization, and text generation are just
a few of the language processing problems that might take advantage of
the pre-structured semantic knowledge of an expert system. In particular,
the need for explanation generation facilities for expert systems provides
an opportunity to explore the relationships between the underlying know-
ledge structures needed for automated reasoning and those needed for
natural language processing. One such exploration was the development of
an explanation generator for XSEL, which is an expert system that helps a
salesperson in producing a purchase order for a computer system. This
paper describes a technique called &amp;quot;link-dependent message generation&amp;quot;
that forms the basis for explanation generation in XSEL.
</bodyText>
<table confidence="0.9796226875">
Computational Linguistics, Volume 12, Number 1, January-March 1986 67
The FINITE STRING Abstracts of Current Literature
Description Strategies for Naive and
Expert Users
Cecile L. Paris
Department of Computer Science
Columbia University
New York, NY 10027
Proc. ACL, pp. 238-245
Stress Assignment in Letter to Sound
Rules for Speech Synthesis
Kenneth Church
AT&amp;T Bell Laboratories
Proc. ACL, pp. 246-253
An Eclectic Approach to Building Natural
Language Interfaces
Brian Phillips, Michael J. Freiling,
James H. Alexander, Steven E Messick,
Steve Rehfuss, Sheldon Nicholl
Tektronix, Inc.
P.O. Box 500, M/S 50-662
Beaverton, OR 97077
Proc. ACL, pp. 254-261
Structure-Sharing in Lexical Represen-
tations
Daniel Flickinger, Carl Pollard, Thomas Wasow
Hewlett-Packard Laboratories
1501 Page Mill Road
Palo Alto, CA 94303
Proc. ACL, pp. 262-267
A Tool Kit for Lexicon Building
Thomas E. Ahlswede
</table>
<affiliation confidence="0.643556">
Computer Science Department
Illinois Institute of Technology
Chicago, IL 60616
</affiliation>
<subsubsectionHeader confidence="0.383423">
Proc. ACL, pp. 268-276
</subsubsectionHeader>
<bodyText confidence="0.999398396226415">
It is widely recognized that a question-answering system should be able to
tailor its answers to the user. One of the dimensions along which this
tailoring can occur is with respect to the level of knowledge of a user about
a domain. In particular, responses should be different depending on
whether they are addressed to naive or expert users. To understand what
these differences should be, we analyzed texts from adult and junior ency-
clopedias. We found that two different strategies were used in describing
complex physical objects to juniors and adults. We show how these strate-
gies have been implemented on a test database.
This paper will discuss how to determine word stress from spelling. Stress
assignment is a well-established weak point for many speech synthesizers
because stress dependencies cannot be determined locally. It is impossible
to determine the stress of a word by looking through a five or six character
window, as many speech synthesizers do. Well-known examples such as
degrade / degradation and telegraph / telegraphy demonstrate that stress
dependencies can span over two and three syllables. This paper will pres-
ent a principled framework for dealing with these long distance dependen-
cies. Stress assignment will be formulated in terms of Waltz&apos; style
constraint propagation with four sources of constraints: (1) syllable
weight, (2) part of speech, (3) morphology and (4) etymology. Syllable
weight is perhaps the most interesting, and will be the main focus of this
paper. Most of what follows&apos; has been implemented.
INKA is a natural language interface to facilitate knowledge acquisition
during expert system development for electronic instrument trouble-shoot-
ing. The expert system design methodology develops a domain definition,
called GLIB, in the form of a semantic grammar. This grammar format
enables GLIB to be used with the INGLISH interface, which constrains users
to create statements within a subset of English. Incremental parsing in
INGLISH allows immediate remedial information to be generated if a user
deviates from the sublanguage. Sentences are translated into production
rules using the methodology of lexical-functional grammar. The system is
written in Smalltalk and, in INKA, produces rules for a Prolog inference
engine.
The lexicon now plays a central role in our implementation of a Head-
driven Phrase Structure Grammar (HPSG), given the massive relocation
into the lexicon of linguistic information that was carried by the phrase
structure rules in the old GPSG system. HPSG&apos;s grammar contains fewer
than twenty (very general) rules; its predecessor required over 350 to
achieve roughly the same coverage. This simplification of the grammar
is made possible by an enrichment of the structure and content of lexical
entries, using both inheritance mechanisms and lexical rules to represent
the linguistic information in a general and efficient form. We will argue
that our mechanisms for structure-sharing not only provide the ability to
express important linguistic generalizations about the lexicon, but also
make possible an efficient, readily modifiable implementation that we find
quite adequate for continuing development of a large natural language
system.
This paper describes a set of interactive routines that can be used to create,
maintain, and update a computer lexicon. The routines are available to the
user as a set of commands resembling a simple operating system. The lexi-
con produced by this system is based on lexical-semantic relations, but is
compatible with a variety of other models of lexicon structure. The lexicon
builder is suitable for the generation of moderate-sized vocabularies and
</bodyText>
<page confidence="0.961419">
68 Computational Linguistics, Volume 12, Number 1, January-March 1986
</page>
<note confidence="0.772266">
The FINITE STRING Abstracts of Current Literature
</note>
<bodyText confidence="0.992435">
has been used to construct a lexicon for a small medical expert system. A
future version of the lexicon builder will create a much larger lexicon by
parsing definitions from machine-readable dictionaries.
</bodyText>
<table confidence="0.960282933333333">
Using an On-line Dictionary to Find
Rhyming Words and Pronunciations for
Unknown Words
Roy J. Byrd
IBM Thomas J. Watson Research Center
Yorktown Heights, NY 10598
Martin S. Chodorow
Department of Psychology,
Hunter College of CUNY
&amp; IBM Thomas J. Watson Research Center
Proc. ACL, pp. 277-283
Towards a Self-Extending Lexicon
Uri Zernick, Michael G. Dyer
Artificial Intelligence Laboratory
Computer Science Department
3531 Boelter Hall
University of California
Proc. ACL, pp. 284-292
Granunatical Analysis by Computer of the
Lancaster.Oslo/Bergen (LOB) Corpus of
British English Texts
Andrew David Beale
Unit for Computer Research on the English
Language
Bowland College, University of Lancaster
Bailrigg, Lancaster, England LA! 4YT
Proc. ACL, pp. 293-298
Extracting Semantic Hierarchies from a
Large On-Line Dictionary
Martin S. Chodorow
</table>
<affiliation confidence="0.565007666666667">
Department of Psychology
Hunter College of CUNY
&amp; IBM Thomas J. Watson Research Center
</affiliation>
<bodyText confidence="0.999908882352942">
Humans know a great deal about relationships among words. This paper
discusses relationships among word pronunciations. We describe a
computer system which models human judgement of rhyme by assigning
specific roles to the location of primary stress, the similarity of phonetic
segments, and other factors. By using the model as an experimental tool,
we expect to improve our understanding of rhyme. A related computer
model will attempt to generate pronunciations for unknown words by anal-
ogy with those for known words. The analogical processes involve tech-
niques for segmenting and matching word spellings, and for mapping
spelling to sound in known words. As in the case of rhyme, the computer
model will be an important tool for improving our understanding of these
processes. Both models serve as the basis for functions in the WordSmith
automated dictionary system.
The problem of manually modifying the lexicon appears with any natural
language processing program. Ideally, a program should be able to acquire
new lexical entries from context, the way People learn. We address the
problem of acquiring entire phrases, specifically figurative phrases, through
augmenting a phrasal lexicon. Facilitating such a self-extending lexicon
involves (a) disambiguation — selection of the intended phrase from a set of
matching phrases, (b) robust parsing — comprehension of partially-matching
phrases, and (c) error analysis — use of errors in forming hypotheses about
new phrases. We have designed and implemented a program called RINA
which uses demons to implement functional-grammar principles. RINA
receives new figurative phrases in context and through the application of a
sequence of failure-driven rules, creates and refines both the patterns and
the concepts which hold syntactic and semantic information about phrases.
Research has been under way at the Unit for Computer Research on the
English Language at the University of Lancaster, England, to develop a
suite of computer programs which provide a detailed grammatical analysis
of the LOB corpus, a collection of about 1 million words of British English
texts available in machine readable form.
The first phase of the project, completed in September 1983, produced
a grammatically annotated version of the corpus giving a tag showing the
word class of each word token. Over 93 per cent of the word tags were
correctly selected by using a matrix of tag pair probabilities and this figure
was upgraded by a further 3 per cent by retagging problematic strings of
words prior to disambiguation and by altering the probability weightings
for sequences of three tags. The remaining 3 to 4 per cent were corrected
by a human post-editor.
The system was originally designed to run in batch mode over the
corpus but we have recently modified procedures to run interactively for
sample sentences typed in by a user at a terminal. We are currently
extending the word tag set and improving the word tagging procedures to
further reduce manual intervention. A similar probabilistic system-is being
developed for phrase and clause tagging.
Dictionaries are rich sources of detailed semantic information, but in order
to use the information for natural language processing, it must be organ-
ized systematically. This paper describes automatic and semi-automatic
procedures for extracting and organizing semantic feature information
implicit in dictionary definitions. Two head-finding heuristics are
described for locating the genus terms in noun and verb definitions. The
</bodyText>
<table confidence="0.45928016">
Computational Linguistics, Volume 12, Number 1, January-March 1986 69
The FINITE STRING Abstracts of Current Literature
Yorktown Heights, NY 10598
Roy J. Byrd, George E. Heidorn
IBM Thomas J. Watson Research Center
Proc. ACL, pp. 299-304
Dictionaries of the Mind
George A. Miller
Department of Psychology
Princeton University
Princeton, NJ 08544
Proc. ACL, pp. 305-314
The Use of Syntactic Clues in Discourse
Processing
Nan Decker
1834 Chase Avenue
Cincinnati, OH 45223
Proc. ACL, pp. 315-323
Grammar Viewed as a Functioning Part of
a Cognitive System
Helen M. Gigley
Department of Computer Science
University of New Hampshire
Durham, NH 03824
Proc. ACL, pp. 324-332
</table>
<bodyText confidence="0.998932295454546">
assumption is that the genus term represents inherent features of the word
it defines. The two heuristics have been used to process definitions of
40,000 nouns and 8,000 verbs, producing indexes in which each genus
term is associated with the words it defined. The Sprout program interac-
tively grows a taxonomic &amp;quot;tree&amp;quot; from any specified root feature by
consulting the genus index. Its output is a tree in which all of the nodes
have the root feature for at least one of their senses. The Filter program
uses an inverted form of the genus index. Filtering begins with an initial
filter file consisting of words that have a given feature (e.g., [+human]) in
all of their senses. The program then locates, in the index, words whose
genus terms all appear in the filter file. The output is a list of new words
that have the given feature in all of their senses.
How lexical information should be formulated, and how it is organized in
computer memory for rapid retrieval, are central questions for computa-
tional linguists who want to create systems for language understanding.
How lexical knowledge is acquired, and how it is organized in human
memory for rapid retrieval during language use, are also central questions
for cognitive psychologists. Some examples of psycholinguistic research on
the lexical component of language are reviewed with special attention to
their implications for the computational problem.
The desirability of a syntactic parsing component in natural language
understanding systems has been the subject of debate for the past several
years. This paper describes an approach to automatic text processing
which is entirely based on syntactic form. A program is described which
processes one genre of discourse, that of newspaper reports. The program
creates summaries of reports by relying on an expanded concept of text
grounding: certain syntactic structures and tense/aspect pairs indicate the
most important events in a news story. Supportive, background material is
also highly coded syntactically. Certain types of information are routinely
expressed with distinct syntactic forms. Where more than one episode
occurs in a single report, a change of episode will also be marked syntac-
tically in a reliable way.
How can grammar be viewed as a functional part of a cognitive system?
Given a neural basis for the processing control paradigm of language
performance, what roles does &amp;quot;grammar&amp;quot; play? Is there evidence to
suggest that grammatical processing can be independent from other
aspects of language processing?
This paper will focus on these issues and suggest answers within the
context of one computational solution. The example mode of sentence
comprehension, HOPE, is intended to demonstrate both representational
considerations for a grammar within such a system as well as to illustrate
that by interpreting a grammar as a feedback control mechanism of a
&amp;quot;neural-like&amp;quot; process, additional insights into language processing can be
obtained.
</bodyText>
<subsectionHeader confidence="0.979694">
Selected Dissertation Abstracts
</subsectionHeader>
<bodyText confidence="0.945681666666667">
Compiled by
Susanne M. Humphrey, National Library of Medicine, Bethesda, MD 20894
Bob Krovetz, University of Massachusetts, Amherst, MA 01002
The following are citations selected by title and abstract as being related to computational linguistics or knowledge
representation, resulting from a computer search, using the BRS Information Technologies retrieval service, of the
Dissertation Abstracts International DAI) database produced by University Microfilms International.
</bodyText>
<page confidence="0.961328">
70 Computational Linguistics, Volume 12, Number 1, January-March 1986
</page>
<note confidence="0.75837">
The FINITE STRING Abstracts of Current Literature
</note>
<bodyText confidence="0.94993975">
Included are the title; author; university, degree, and, if available, number of pages; DAI subject category chosen by
the author of the dissertation; and UM order number and year-month of DAI. References are sorted first by DAI
subject category and second by author.
Unless otherwise specified, paper or microform copies of dissertations may be ordered from
</bodyText>
<table confidence="0.857108166666667">
University Microfilms International
Dissertation Copies
Post Office Box 1764
Ann Arbor, MI 48106
telephone for U.S. (except Michigan, Hawaii, Alaska): 1-800-521-3042,
for Canada: 1-800-268-6090.
</table>
<bodyText confidence="0.9815724">
Price lists and other ordering and shipping information are in the introduction to the published DAI. An alternate
source for copies is sometimes provided at the end of the abstract.
NOTICE The dissertation titles and abstracts contained here are published with the permission of University Micro-
films International, publishers of Dissertation Abstracts International (copyright 1985) by University Microfilms Inter-
national), and may not be reproduced without their prior permission.
</bodyText>
<table confidence="0.849424076923077">
Decision-Making in Manufacturing: An
Information Processing Approach
Suranjan De
Purdue University Ph.D. 1984, 211 pages
Business Administration, Management
University Microfilms Order Number
ADG85-00360. 8504.
Communication and Miscommunication
Bradley Alan Goodman
University of Illinois at Urbana-Champaign,
Ph.D. 1984, 264 pages
Computer Science
University Microfilms Order Number
</table>
<page confidence="0.298352">
ADG85-02154. 8505.
</page>
<bodyText confidence="0.98915024590164">
The broad objective of this research is to employ concepts from optimiza-
tion theory, artificial intelligence, and computer architecture to the analysis
of organizational problem-solving. Abstract organizations can be charac-
terized as consisting of information processors or resources that are inter-
connected through a network. A limited commonality of interest exists
between the processors. The primary task is to use the resources to carry
out some tasks or achieve some goals.
The purpose of this research is to model the manufacturing component
of an organization and to explore various scheduling problems associated
with this approach. The computer-based information system that models
such a problem-solving system consists of three components: a knowledge
base in which both general and domain-dependent knowledge will be
represented, a natural language interface through which the user can
communicate with the system, and a problem processor that carries out a
combination of data retrieval, computation and deduction in order to
respond to queries posed by the user.
An axiomatic framework based on equational logic is proposed to model
the computer-based information system. The use of this framework to
represent environmental and linguistic knowledge as well as perform the
language processing and problem-solving functions in a uniform manner is
demonstrated.
As an illustration of the functioning of the problem-solving component,
a framework for the on-line scheduling of jobs as they enter the system is
introduced. A heuristic solution based on artificial intelligence techniques
is proposed for the off-line or static scheduling problem. The solution is
then extended to a more dynamic environment where jobs continually
arrive over time.
Finally, a model for the decentralized control of a general multilevel
manufacturing system is proposed. Future research directions are
suggested.
This thesis discusses one aspect of enabling people to communicate in
natural language with computers. The central focus of this work is a study
on how one could build robust natural language processing systems that
can detect and recover from miscommunication. The study of miscommu-
nication is a necessary task within such a context since any computer capa-
ble of communicating with humans in natural language must be tolerant of
the imprecise, ill-devised or complex utterances that people often use. This
goal first requires an inquiry into how people communicate and how they
recover from problems in communication. That investigation centers on
the kinds of miscommunication that occur in human communication with a
Computational Linguistics, Volume 12, Number 1, January-March 1986 71
The FINITE STRING Abstracts of Current Literature
special emphasis on reference problems, i.e., problems a listener has deter-
mining whom or what a speaker is talking about. A collection of protocols
of a speaker explaining to a listener how to assemble a toy water pump was
studied and the common errors seen in speakers&apos; descriptions were cate-
gorized. This study led to the development of techniques for avoiding fail-
ures of reference that were employed in the reference identification
component of a natural language understanding program.
The traditional approaches to reference identification in previous
natural language systems were found to be less elaborate than people&apos;s real
behavior. In particular, listener&apos;s often find the correct referent even when
the speaker&apos;s description does not describe any object in the world. To
model a listener&apos;s behavior, a new component was added to the traditional
reference identification mechanism to resolve difficulties in a speaker&apos;s
description. This new component uses knowledge about linguistic and
physical context in a negotiation process that determines the most likely
places for error in the speaker&apos;s utterance. The actual repair of the speak-
er&apos;s description is achieved by using the knowledge sources to apply relaxa-
tion techniques that delete or replace portions of the description. The
algorithm developed more closely approximates people&apos;s behavior.
</bodyText>
<figure confidence="0.95637575">
A Syntactic Approach to Three-Dimen-
sional Object Representation and Recog-
nition
Wei-Chung Lin
Purdue University Ph.D. 1984, 218 pages
Computer Science
University Microfilms Order Number
ADG85-00404. 8504.
Information Modeling and Sharing in
Highly Autonomous Database Systems
Peter Lyngbaek
University of Southern California Ph.D. 1984
Computer Science
This item is not available from University
Microfilms International
ADG05-55439. 8505.
</figure>
<bodyText confidence="0.999901594594595">
The syntactic approach to pattern representation and scene analysis has
received increasing attention due to its unique capability in handling
pattern structures and their relationships. However, it has been applied
mostly on one and two-dimensional pattern recognition problems. The
difficulties of syntactic approach in dealing with three-dimensional objects
or scenes are caused by (1) model primitives are described in an observer-
centered coordinate system; (2) lack of the mechanisms for relating two-
dimensional image to three-dimensional objects, and (3) projections of
some primitives in the image are invisible or partially occluded.
In this thesis, we propose a syntactic approach to three-dimensional
object recognition from a single view. The system consists of two major
parts: analysis and recognition. The analysis part consists of primitive
surface patches selection and modeling grammar construction. The recog-
nition part consists of preprocessing, image segmentation, visible primitive
surface identification, camera model estimation, and structural analysis. In
the modeling phase, a three-dimensional object model is represented by
using surface patches as primitives and 3D-plex grammar rules as structural
relationship descriptors. Several algorithms to extract useful information
for recognition from a given 3D-plex grammar are presented. The recogni-
tion task starts with preprocessing and image segmentation. Then, the
transformation from three-dimensional object space to two-dimensional
image space is determined by a camera model estimation procedure. The
final phase of the recognition is carried out by a semantic-directed top-
down backtrack recognizer.
The past several years have seen a dramatic proliferation of personal
computers, and anticipated future technological advances will continue to
increase their power and reduce their cost. Perhaps one of the most excit-
ing and far-reaching potential uses of a personal computer is as an infor-
mation manager and a tool for information sharing and communication.
This dissertation addresses the problems of information modeling and shar-
ing in a collection of personal databases. Each database contains a number
of information objects that may be related to other objects in other data-
bases. Objects correspond to concepts or things with an associated mean-
ing.
Two related aspects of object-oriented information modeling are
described. First, a simple distributed database model is defined. The
model provides a basic set of kernel operations for object definition,
</bodyText>
<page confidence="0.950477">
72 Computational Linguistics, Volume 12, Number 1, January-March 1986
</page>
<note confidence="0.831726">
The FINITE STRING Abstracts of Current Literature
</note>
<bodyText confidence="0.986280444444444">
manipulation, and retrieval. Second, a high-level distributed database
model is defined as a specific approach to personal information manage-
ment in a network of personal workstations. Critical research issues
include information modeling, object and database naming, object equiv-
alence, object scoping, supporting database autonomy, and supporting the
dynamics of the network structure, the information structure, and the
information itself.
(Copies available exclusively from Micrographics Department, Doheny
Library, USC, Los Angeles, CA 90089.)
</bodyText>
<table confidence="0.969281625">
A Resource Oriented Formalism for Plan
Generation
Paul Henry Morris
University of California, Irvine, Ph.D.
1984, 165 pages
Computer Science
University Microfilms Order Number
ADG85-02995. 8506.
Conjunctive Conceptual Clustering: A
Methodology and Experimentation
Robert Earl Stepp HI
University of Illinois at Urbana-Champaign
Ph.D. 1984, 208 pages
Computer Science
University Microfilms Order Number
ADG85-02306. 8506.
</table>
<bodyText confidence="0.999779130434783">
A formulation of planning based on multisets is introduced. Actions are
represented by a simple well-behaved formalism which is related both to
Petri nets and to predicate logic. It is inherently resource and quantity
oriented and more accurately models constraints imposed by resource limi-
tations.
The consequences of this for plan generation are explored. The formal-
ism is simple enough to allow theoretical issues to be investigated. Parallel
plans are represented by a concise graphical structure with the property
that a syntactically complete plan is necessarily correct. Abstract algo-
rithms for plan generation with provable completeness properties are
exhibited. Techniques for recognizing and escaping from vicious circles
are studied. These turn out to also have the effect of excluding certain
correct but nonsensical plans, and allow the automatic derivation of plan-
ning-time checks on the appropriateness of specific operators, but cause
incompleteness in some domains.
Novel methods for efficient planning that arise naturally from the
formalism are described. Cut and splice operations on plan graphs permit
the order of goal selection to be based on efficiency rather than correct-
ness considerations. The multiset based formalism is particularly suited to
developing the notion of an invariant, or conservation law. Invariants are
exploited in a technique called goal targeting which provides an early
outline of the final plan. This helps to eliminate misdirected search and
facilitates certain intelligent goal selection and variable binding mech-
anisms.
This thesis describes a machine learning methodology called conjunctive
conceptual clustering. The methodology can find conceptual patterns in
data as illustrated by three sample problems. In one problem, the method
is used to rediscover categories of soybean disease when given a collection
of 47 descriptions of diseased soybeans having one of four diseases. In a
second problem, the method is used to find categories underlying a
collection of blocks-world structures. In a third problem, categories of
objects having a more complex structure are determined and contrasted
with categories generated by people.
The described method of conjunctive conceptual clustering forms clus-
ters of objects (or situations) not on the basis of a numerical similarity
measure but on the basis of the &amp;quot;conceptual cohesiveness&amp;quot; of one object to
another. The conceptual cohesiveness between two objects depends on
the descriptions of the two objects as well as the descriptions of other
nearby objects in the given collection and concepts which are available to
describe object groups or object configurations as a whole. From a
collection of objects, some background domain knowledge, and a goal or
purpose for clustering, conceptual clustering generates a hierarchical classi-
fication composed of clusters of objects and corresponding conjunctive-
form cluster descriptions (concepts). Conceptual clustering is one
paradigm of &amp;quot;learning from observation&amp;quot; in which no teacher guides the
learning process.
</bodyText>
<table confidence="0.969365888888889">
Computational Linguistics, Volume 12, Number 1, January-March 1986 73
The FINITE STRING Abstracts of Current Literature
Question Understanding: Effects on
Children&apos;s Comprehension of Stories
LiIli Kotmos
McGill University (Canada) Ph.D. 1984
Education, Psychology
This item is not available from University
Microfilms International.
ADG05-55319. 8504.
Speechkit: A Development Environment
for Speech Systems Research
Charles Jackson Cotton
University of Delaware Ph.D. 1984,
126 pages
Engineering, Electronics and Electrical
University Microfilms Order Number
ADG85-00862. 8505.
</table>
<bodyText confidence="0.999210563636364">
This study investigated the effects of different types of questions on
discourse comprehension. In addition, it examined performance on ques-
tions and its influence on comprehension. Within a theoretical framework
of discourse processing, the research focused on question type, passage
structure, and individual differences in comprehension. Comprehension
was measured by analyzing propositions recalled and inferred during free
recall. Performance on questions was measured by analyzing answers for
the presence of particular types of inferences.
Third grade children read selected fictional passages, answered ques-
tions about them, and recalled them. Results revealed that the effects of
questions on comprehension are complex, interacting with passage struc-
ture, reading level, and response type. Findings also indicated that ques-
tions influence the selective processing of propositional information in text.
Question-answering performance was found to reflect an interaction
between question type and passage structure. Furthermore, ability to
generate the appropriate inferences in responding to questions facilitated
text comprehension.
This dissertation describes the implementation and usage of a worksta-
tion-like environment called Speechkit. Speechkit is a tool that eases the
development and testing of speech and signal processing systems.
Speechkit is composed of three components: Nessie, Wed, and Spools.
Nessie, a local area network based signal server, provides quick and
convenient access to analog signals from a host computer. Wed, a screen-
based signal editor, is a powerful, interactive tool for examining signal data
files. Spools, a short name for speech tools, is a collection of utilities for
manipulating and processing speech signals.
The application of Speechkit to the developments of several isolated
word recognition systems is additionally described. The developed recog-
nition systems form test beds for the evaluation of experimental speech
recognition system components.
The purpose of an information retrieval system is to meet information
needs. People who are expert at meeting information needs go about satis-
fying them much differently and, in general, more successfully than auto-
mated systems. The model that forms the basis for this dissertation is a
descriptive model of how these experts satisfy information needs. This
model can be used prescriptively in the design of an information retrieval
system whose performance is similar to that of a human expert.
One of the most pressing issues in perfecting document retrieval methods is
the establishment of rationale criteria for deciding how to index stored
documents for purposes of later retrieval. Approaches to indexing have
tended to focus on the word or sentences level rather than on the structure
of the whole document. Linguists and cognitive psychologists suggest that
texts have an inherent structure termed a schema which is used by individ-
uals not only to understand texts but also to produce texts. Further, text
schemata of different groups of individuals are thought to vary.
The purpose of this study is to test schema theory in the analysis and
representation of text, namely published reports of clinical trials. The
hypotheses are: (1) A clinical trial schema constructed from the expert
opinions of what constitutes a clinical trial is replicated in the published
reports of clinical trials, (2) The gross schema structure of trial reports is
similar, however, representation of specific schematic elements varies
significantly between diseases.
A trial schema consisting of essential trial elements was designed based
upon a review of the literature. A random sample of trial reports was
selected by a search of the Medline database and stratified by disease cate-
</bodyText>
<table confidence="0.994160866666667">
The Retrieval Expert Model of Informa-
tion Retrieval
Scott Craig Deemester
Purdue University Ph.D. 1984, 102 pages
Information Science
University Microfilms Order Number
ADG85-00361. 8504.
Schema Theory in the Representation and
Analysis of Text
Sherrilynne Shirley Fuller
University of Southern California Ph.D.
1984, 189 pages
Information Science
University Microfilms Order Number
ADG85-00206. 8504.
</table>
<page confidence="0.625648">
74 Computational Linguistics, Volume 12, Number 1, January-March 1986
</page>
<note confidence="0.539275">
The FINITE STRING Abstracts of Current Literature
</note>
<bodyText confidence="0.97485975">
gory. Trial reports in eleven disease categories were studied: Bacterial and
Fungal, Virus, Parasitic, Oncologic, Musculoskeletal, Mouth and Tooth,
Respiratory, Nervous System, Eye, Digestive System and Otorhinolaryngo-
logic Diseases.
Results of the schema analysis revealed that trial reports, in general, are
reflective of the clinical trial as envisioned by the experts. However, based
on chi-square tests, major differences can be demonstrated between
disease categories in the representation of specific trial elements. Further,
tests of the inter-rater reliability of schema application to trial reports
revealed a concurrence of between 74% and 95% for five indexers.
The findings suggest that schema theory may offer a paradigm for
indexing which provides a means of capturing both the intra-document,
i.e., the holistic structure of the document as well as the inter-document
relationships, i.e., data on groups of documents. Further study is indicated.
(Copies available exclusively from Micrographics Department, Doheny
Library, USC, Los Angeles, CA 90089.)
</bodyText>
<subsectionHeader confidence="0.797491666666667">
A Functional Approach to English
Sentence Stress
Kathleen Bardovi-Harlig
</subsectionHeader>
<bodyText confidence="0.208053">
The University of Chicago Ph.D. 1983
Language, Linguistics
This item is not available from University
Microfilms International
ADG03-69962. 8504.
</bodyText>
<subsectionHeader confidence="0.957837">
Some Constraints Consideration on
Conversational Interactions of Politeness
and Relevance with Grice&apos;s Second
Maxim of Quantity
Susan Kay Donaldson
</subsectionHeader>
<bodyText confidence="0.8331522">
University of Illinois at Urbana-Champaign
Ph. D. 1984, 624 pages
Language, Linguistics
University Microfilms Order Number
ADG85-02128. 8505.
</bodyText>
<subsectionHeader confidence="0.779874666666667">
The Functional Role of the Closed Class
Vocabulary in Children&apos;s Language
Processing
</subsectionHeader>
<footnote confidence="0.522187333333333">
Carmen Egido
Massachusetts Institute of Technology
Ph.D. 1983
Language, Linguistics
This item is not available from University
Microfilms International
</footnote>
<bodyText confidence="0.976141582089552">
(No abstract.)
In his by now well-known paper &amp;quot;Logic and conversation&amp;quot;, philosopher of
language Paul Grice establishes four maxims speakers follow in conversing:
maxims of quantity, quality, relation, and manner. The maxim of quantity
he divides into two parts, saying that conversational participants must give
enough information to each other, but must not give too much. However,
after once establishing this maxim, Grice immediately casts doubt on its
validity, saying that its second part is adequately covered by the maxim of
relation, which states that what one says should be relevant—that is, that
any remark that would be considered overinformative would be discounted
by its being irrelevant, anyway, thus eliminating the need for the second
half of the maxim of quantity.
This dissertation, employing examples from both tape-recorded &apos;real&apos;
conversations and conversations from short stories and novels, argues that
Grice&apos;s first intuitions were correct, namely, that the second half of the
maxim of quantity is both valid and necessary. Speakers refrain, at times,
from conveying to one another information that could be highly relevant to
the material at hand, the thesis maintains, for reasons that stem in large
part from consideration for one another. A lengthy review of the literature
is included, as well as a chapter distinguishing conversation from other
sorts of verbal interaction, and one on the nature of consideration and
precedents from the literature on human interaction for consideration as a
valid form of motivation. Transcripts of four &apos;real&apos; conversations follow
the text.
(No abstract)
Computational Linguistics, Volume 12, Number 1, January-March 1986 75
The FINITE STRING Abstracts of Current Literature
ADG03-69666. 8504.
(No abstract.)
(No abstract.)
This thesis sets forth the elements of head grammar (HG), a linguistic
framework which takes as one of its chief primitives the notion of a gram-
matical head. The purpose of the study is to shed light upon certain central
questions of linguistic theory relating to computational complexity and
grammatical organization.
HG&apos;s are a class of systems which slightly exceed the power of context-
free grammars (CFGs) by manipulating strings containing a distinguished
element called the head; in addition to concatenation, HG&apos;s permit head-
wrapping operations which insert one string into another at a point adja-
cent to the latter&apos;s head. HGs share most of the formal and computational
properties of CFGs; unlike CFGs, however, their expressive power suffices
to provide a linguistically motivated account of discontinuous constituents.
Like GPSG and categorial grammar, HG posits only one level of gram-
matical structure; thus no recourse is made to the D-structures and Logical
Form of Government-Binding Theory or the F-structures of LFG. Unlike
GPSG, however, HG shares with LFG the lexical encoding of syntactic-se-
mantic subcategorization information and the expression of paradigmatic
regularities by lexical rules. The resulting system handles the kinds of
dependencies that arise in natural language without high-powered mech-
anisms such as transformations, metarules, or semantic filtering.
In HG, as in GPSG, linguistic information is encoded in grammatical
categories and propagated by a small set of local constraints (Head Feature
Principle, Binding Inheritance Principle, Control Agreement Principle,
etc.). Unlike GPSG, however, HG features may take category sequences as
values; in addition, disjunctive specifications are permitted. Consequently
both subcategorization and multiple unbounded dependencies can be
handled by sequence-valued features; at the same time, feature
&amp;quot;instantiation&amp;quot; is eliminated in favor of unification with concomitant
reduction in the number and complexity of rules.
The theoretical discussions are illustrated with accounts of key English
grammatical phenomena, including the following: agreement; subcategori-
zation for subject, objects, and controlled complements; constituent order;
subject-auxiliary &amp;quot;inversion&amp;quot;; equi and raising; natural quantifier scope;
transparent/opaque ambiguity; existential there; passivization; topicaliza-
tion; the tough-construction; multiple &amp;quot;extractions&amp;quot;; parasitic gaps; posses-
sives; and reflexivization. In addition, a detailed account is provided of the
cross-serial construction in Dutch subordinate clauses.
</bodyText>
<figure confidence="0.956569911764706">
Syntactic Affixation.
Nigel Alexander Fabb
Massachusetts Institute of Technology
Ph.D. 1984
Language, Linguistics
This item is not available from University
Microfilms International
ADG03-69503. 8504.
Grammatical Configurations and Gram-
matical Relations
Yehuda Nahum Falk
Massachusetts Institute of Technology
Ph.D. 1984
Language, Linguistics
This item is not available from University
Microfilms International.
ADG03-69504. 8504.
Generalized Phrase Structure Grammars,
Head Grammars, and Natural Language
Carl Jesse Pollard
Stanford University Ph.D. 1984, 255 pages
Language, Linguistics
University Microfilms Order Number
ADG84-29549. 8504.
76 Computational Linguistics, Volume 12, Number 1, January-March 1986
The FINITE STRING Abstracts of Current Literature
Prosodic Constraints and Lexical Parsing
Strategies
Lori Alice Taft
University of Massachusetts Ph.D. 1984,
309 pages
Language, Linguistics
University Microfilms Order Number
ADG85-00140. 8504.
</figure>
<bodyText confidence="0.99313325">
(No abstract)
This work is intended as an attempt to bring the phenomenon of resump-
tive pronouns under the scrutiny of analytical techniques current in genera-
tive grammar, particularly the syntactic theories of Government and
Binding and Generalized Phrase Structure Grammar. It is claimed that the
notion &apos;resumptive pronoun&apos; finds a definition within generative grammar
that is both interesting typologically and theoretically. It is argued that not
all apparent instances of resumptive pronouns are truly so, and that
languages may differ typologically in whether their grammars countenance
&apos;true&apos; resumptive pronouns or not.
Data from Swedish, Hebrew, Irish and Welsh is considered in some
detail and it is claimed that there is no universal uniformity in the gram-
matical devices a language may employ in its system of resumptive
pronouns. The consequences of the proposed account of the data from the
four mentioned languages for Government-Binding theory and General-
ized Phrase Structure Grammar are discussed and integrated into the
particular (sub-) theories that relate to them.
Semantically, it is claimed that resumptive pronouns show the character-
istics expected of pronouns rather than gaps (empty categories); data from
Hebrew are presented to show that there are systematic differences in
interpretation for constructions depending on whether the construction
contains an empty category or a resumptive pronoun. These semantic
issues are presented within the framework of Discourse Representation
Theory as developed by Hans Kamp.
Finally, data from English are presented to elaborate on the semantic
nature of resumptive pronouns; it is claimed that English lacks resumptive
pronouns and has instead what are dubbed &apos;intrusive&apos; pronouns. The prop-
erties of the interpretation of intrusive pronouns are shown to follow from
general and independent principles, providing support for the analysis in
terms of Discourse Representation Structures.
Among the concerns of current psycholinguistic research has been an
attempt to specify the constraints available and used in the processing of
linguistic material. Much of this work has focussed on the syntactic
constraints which the processor uses in imposing structure on the incoming
lexical material. In this thesis I argue that the case for phonological proc-
essing is not so different from the case for syntactic processing. I make the
working assumption that grammatical information of various types is avail-
able to listeners to impose an interpretation on the incoming signal, not
just at the level of syntactic parsing, but prior to that stage as well.
I hypothesize that the listener is using knowledge of language-specific
prosodic constraints to impose structure on the incoming material and to
identify likely places for word onsets, independent of lexical access itself.
Two hypotheses are advanced concerning how constraints are used by the
listener. First, I hypothesize a &amp;quot;Salience-to-Onset Strategy&amp;quot; which helps
the listener locate word onsets on the basis of salient portions of the signal.
Second, I hypothesize a &amp;quot;Prosodic Domain Strategy&amp;quot; with which the listen-
er parses the incoming material into prosodic units for comparison with
stored lexical representations.
</bodyText>
<table confidence="0.821711888888889">
The Syntactic Forms of Predication.
Susan Deborah Rothstein
Massachusetts Institute of Technology
Ph.D. 1983
Language, Linguistics
This item is not available from University
Microfilms International
ADG03-69668. 8504.
Syntax and Semantics of Resumptive
Pronouns
Peter Sells
University of Massachusetts Ph.D. 1984,
508 pages
Language, Linguistics
University Microfilms Order Number
ADG85-00135. 8504.
Computational Linguistics, Volume 12, Number 1, January-March 1986 77
The FINITE STRING Abstracts of Current Literature
</table>
<bodyText confidence="0.997241142857143">
Supporting evidence is presented in a series of four psycholinguistic
experiments testing the predictions of the SOS and PDS. Experiments 1-3
test the claims of the SOS and PDS for English. Experiment 4 is a pilot
study testing the claims of the SOS and PDS for Japanese.
I conclude by considering the implications of this work for psycholin-
guistic models of lexical access, and for explanations of certain phonologi-
cal phenomena found in natural languages.
</bodyText>
<table confidence="0.529056">
Particle Ellipses in Japanese
Michio Tsutsui
University of Illinois at Urbana-Champaign
Ph.D. 1984, 180 pages
Language, Linguistics
University Microfilms Order Number
ADG85-02325. 8505.
</table>
<subsectionHeader confidence="0.634644333333333">
Can Our Quantifiers Range over All
Collections?
Thomas Charles Antognini
</subsectionHeader>
<bodyText confidence="0.340213666666667">
Massachusetts Institute of Technology
Ph.D. 1984
Philosophy
This item is not available from University
Microfilms International.
ADG03-69725. 8504.
</bodyText>
<subsectionHeader confidence="0.748426333333333">
Proper Names, Beliefs, and Definite
Descriptions
Thomas Charles Ryckman
</subsectionHeader>
<footnote confidence="0.4485148">
University of Massachusetts Ph.D. 1984,
197 pages
Philosophy
University Microfilms Order Number
ADG85-00131. 8504.
</footnote>
<bodyText confidence="0.99990935">
In this study I consider two kinds of particle ellipsis, syntactic particle ellip-
sis and conversational particle ellipsis. Syntactic particle ellipsis (discussed
in Chapter 1) is the type of particle ellipsis caused by certain syntactic
conditions. Only case particles are elliptic under these conditions. When
one of these conditions is satisfied, the naturalness of case particle ellipsis
(or retention) varies from &amp;quot;oto natural&amp;quot; to &amp;quot;oto unnatural&amp;quot; depend-
ing on the particle and the function it performs. To describe this phenome-
non, I hypothesize that there is a hierarchical relation among case particles
which perform different functions regarding their ellipsis and retention.
Statistical data supports this hypothesis.
Conversational particle ellipsis (discussed in Chapters 2, 3 and 4) is
another type of particle ellipsis which takes place in conversation even if
none of the conditions for syntactic particle ellipsis are satisfied.
Chapter 2 discusses the ellipsis of the topic marker wa in conversation.
Here, I show that the ellipsis of wa marking X is natural if the speaker and
the hearer maintain close contact with the referent of X at the moment of
speech.
Chapter 3 presents some general rules of case particle ellipsis in conver-
sation. In one of these rules I claim that the ellipsis of the case particle
(CP) of an NP-CP is unnatural if the NP-CP conveys the idea of exclusivi-
ty, i.e., the idea &amp;quot;not others but X&amp;quot; or &amp;quot;X and only X&amp;quot;.
Chapter 4 discusses the ellipsis of the case particles ga and o in conver-
sation. Here, I demonstrate that the ellipsis of ga in an utterance is natural
if the speaker believes the utterance carries a certain kind of information. I
also show that the position of an NP-ga or NP-o in a sentence affects the
naturalness of the ellipsis of ga or o.
This study reveals that statements like &amp;quot;recoverable particles can be
elliptic&amp;quot; cannot explain the ellipsis of particles, that the ellipsis of particles
is a matter of degree of naturalness rather than naturalness versus unnatu-
ralness, and that various aspects of language, including phonology, syntax,
information and situation, have bearing on the ellipsis of particles.
(No abstract.)
This dissertation investigates issues raised by these two questions: (i) what
kinds of propositions are ordinarily expressed by uses of sentences that
contain proper names; and (ii) what kinds of beliefs are ordinarily on the
minds of speakers when they use sentences that contain proper names? It
develops a new view about the connections between beliefs, linguistic
behavior, and propositional content, one that explicitly denies that the
kinds of propositions typically expressed by uses of such sentences are the
objects of the beliefs typically on the minds of the speakers who use them.
</bodyText>
<page confidence="0.892158">
78 Computational Linguistics, Volume 12, Number 1, January-March 1986
</page>
<figure confidence="0.916086875">
The FINITE STRING Abstracts of Current Literature
Unearthing Grounds: Some Studies of
Metaphor Comprehension
Laurel J. End
Kent State University Ph.D. 1984, 112 pages
Psychology, Experimental
University Microfilms Order Number
ADG84-29789. 8504.
</figure>
<bodyText confidence="0.996993410714286">
Chapter I presents both the Millian and the description theories of prop-
er names, and reviews the advantages and disadvantages of each.
Chapter II critically evaluates Dummett&apos;s defense of the description
theory against the Modal Objection.
Chapter III introduces Kripke&apos;s puzzle about beliefs and proper names.
It shows that Kripke&apos;s puzzle is not solved by the theory of proper names
recently presented by Devitt. It critically evaluates the &amp;quot;consistency
solutions&amp;quot; proposed by Chisholm, Harrison, Noonan, and Over.
Chapter IV continues the discussion of Kripke&apos;s puzzle. It critically
evaluates the &amp;quot;inconsistency solution&amp;quot; proposed by Marcus. It examines a
commentary on the puzzle by Lewis. Finally, it presents an &amp;quot;inconsistency
solution&amp;quot; based on views suggested by the Lewis commentary.
Chapter V compares my view about the connections between beliefs,
linguistic behavior, and propositional content to the &amp;quot;naive view&amp;quot; and the
&amp;quot;Russellean view&amp;quot;. It applies my view to solve two major problems for the
Millian theory of proper names.
Utterances which convey a meaning different from their literal meaning
pose special problems for models of language comprehension. Primarily
for this reason psychologists have recently begun to examine the proc-
esses involved in metaphor comprehension. Metaphors consist of a topic
or subject, a vehicle or term used metaphorically, and a ground, the
relationship between the topic and vehicle which gives the metaphor its
meaning. For example, in Some Jobs are jails, the topic is job, the vehicle
is jails, and the ground is the notion that some jobs may be restrictive,
confining, or punishing.
Two experiments investigated the nature of the ground. Previous
research indicated that metaphors based on a common ground can prime
one another effectively. The strength of the priming effect was assessed in
Experiment 1 by inserting either 0, 1, 3, or 7 literal sentences between
pairs of metaphors related by a common ground. Subjects read each
sentence presented individually on a CRT screen and the response times
were recorded. Memory for the sentences was assessed by having subjects
recall the topics associated with each vehicle. Priming was effective if
related metaphors were presented consecutively, but the effect disappeared
with 1, 3, or 7 fillers. Related metaphors, filler metaphors, and literal
fillers were recalled equally well, but the second metaphor topic in each
related metaphor pair was recalled more often than the first.
In Experiment 2, the nature of the ground was assessed with an interfer-
ence task. The procedure was identical to Experiment 1 except that only
one filler was presented between the prime and its target. That filler was
either a literal or metaphorical sentence and was either high or low in
imageability. No priming effect was found regardless of the imageability
or type of filler sentence.
The results of Experiments 1 and 2 indicated that metaphors can share a
common ground, but representation of the ground was quite fragile, but
unavailable after only one intervening sentence. The results of Experiment
2 ruled out two possible explanations for the rapid dissipation of a priming
effect in Experiment 1. The interference of priming could not be attributed
to interference of an imaginal component of the ground nor was it due to
differing processing strategies for literal and metaphorical sentences.
However, since the interference paradigm may have been relatively insen-
sitive to subtle alteration in the representation of the ground, it is not
possible to conclude that the filler sentence type and imagery have no
differential influence on the primed ground.
The Nature of the Search for Referents in Kintsch and van Dijk assume that when a reader encounters a reference to
Discourse Processing a concept no longer available in short-term memory that a search through
</bodyText>
<figure confidence="0.940392058823529">
Computational Linguistics, Volume 12, Number 1, January-March 1986 79
The FINITE STRING Abstracts of Current Literature
Edward Joseph O&apos;Brien
University of Massachusetts Ph.D. 1984,
150 pages
Psychology, Experimental
University Microfilms Order Number
ADG85-00111. 8504.
A Neuropsychological Framework for the
Assessment of Competing theories of
Rhetoric as Epistemic
Kathy Lynn Harbert
The Pennsylvania State University Ph.D.
1984, 366 pages
Speech Communication
University Microfilms Order Number
ADG84-29087. 8504.
</figure>
<bodyText confidence="0.999962153846154">
long-term memory for the original concept is necessary. A series of four
experiments are reported that address the nature of this search process. In
the first two experiments, subjects read passages that contained two possi-
ble referents; one referent appeared early in the passages and the other
referent appeared relatively late. Read time differences for the first two
experiments demonstrated that late referents are reinstated more
quickly than early referents. Several viable search models within the
Kintsch and van Dijk framework were considered. However, none of
these models was capable of predicting faster access to the late referent.
Following Experiment 2, it was proposed that text is represented as an
integrated network and that a backward parallel search model provided the
best account of the reinstatement time differences. Experiments 3 and 4
provided further support for these assumptions. The results of these
experiments showed that concepts that appeared between a referent and
the end of a passage are often considered during the search for a referent.
Intervening concepts that are considered are tagged as &amp;quot;not appropriate&amp;quot;.
This tag produces response competition that slows verification times for
statements containing these concepts. The results of all four experiments
are discussed in terms of the Kintsch and van Dijk framework.
This investigation examines the symbols of both thought and language for
the purpose of evaluating competing theories of rhetoric as epistemic.
Emerging from the literature on rhetoric and knowing are assumptions
about the nature and function of the human brain that are no longer tena-
ble in light of the evidence from neuropsychology since the advent of
microtechnology. To update the literature and correct the assumptive base
of the past, this investigation presents a meta-theoretical framework for
understanding the symbols of both thought and language in terms that are
compatible with the state of the art in cognitive theory.
From the history of epistemology, three perspectives toward the
symbols of thought were isolated and traced to their correlates in the
cognitive sciences. By isolating from this survey those characteristics that
were found to be most neuropsychologically adequate, this investigation
advanced a definition of &amp;quot;knowing&amp;quot;, and a description of the cognitive
symbol that were consonant with the most recent evidence from neuropsy-
chology. Three perspectives toward the symbols of language were then
examined and compared with the symbols of thought. By articulating the
characteristics that were common to both thought and language, this inves-
tigation advanced a perspective from which the symbol itself could be
viewed as inherently epistemic. With this perspective as a framework, the
investigation assessed twelve competing theories of rhetoric as epistemic.
Of the theories examined, most were found to advance claims about the
nature of symbolic interaction that were at variance with the characteristics
they imputed to the symbol itself. Only one theory developed a position
that was based on an interactional perspective toward the symbol, and only
that one made a viable claim to knowing.
This investigation concluded that neither a theory of language, nor a
theory of meaning has as yet been developed that can adequately address
the interactional characteristics of the symbol itself. The metatheoretical
framework advanced by this investigation presents guidelines for the devel-
opment of a theory of meaning that would be compatible with the goals of
George Herbert Mead, but not subject to the theoretical difficulties that
were found to be inherent to his position.
</bodyText>
<page confidence="0.884404">
80 Computational Linguistics, Volume 12, Number 1, January-March 1986
</page>
<note confidence="0.500782">
The FINITE STRING Abstracts of Current Literature
</note>
<bodyText confidence="0.678612">
Requests for the following papers should be addressed to
ISSCO working papers
54 route des Acacias
1227 Geneva Switzerland
The price per paper, including air mail postage, is SFr 10 (or equivalent). Checks should be made payable to &amp;quot;Institut
DaIle Molle&amp;quot;.
</bodyText>
<figure confidence="0.994086705882353">
Three Strategic Goals in Conversational
Openings
M. Rosner
No. 46 (1981)
A Poor Man&apos;s Flavor System
F. di Primio, Th. Christaller
No. 47 (1983)
A Government-Binding Parser for French
Eric Wehrli
No. 48 (1984)
AI Approaches to Machine Translation
Patrick Shann
No. 49 (1985)
Machine Translation: Pre-ALPAC History,
Post-ALPAC Overview
Beat Bachmann, Susan Warwick
No. 50 (1985)
</figure>
<bodyText confidence="0.999942886363636">
This paper tries to explain a short transcript of a conversational opening as
completely as possible within the framework which takes conversational
behaviour as defined by the operation of a sophisticated planning mech-
anism. It is argued that a critical role is played by the satif action, for each
participant, of three strategic goals relating to attention, identification, and
greeting. Additional tactics for gaining information are also described as
necessary to account for this transcript.
This paper is the result of an attempt to understand &amp;quot;flavors&amp;quot;, the object
oriented programming system in Lispmachine Lisp. The authors argue that
the basic principles of such systems are not easily accessible to the
programming public, because papers on the subject rarely discuss concrete
details. Accordingly, the authors&apos; approach is pedagogical, and takes the
form of a description of the evolution of their own flavor system. An
appendix contains programming examples that are sufficiently detailed to
enable an average Lisp programmer to build a flavor system, and exper-
iment with the essential concepts of object-oriented programming.
This paper describes a parser for French based on an adaptation of
Chomsky&apos;s Government and Binding theory. Reflecting the modular
conception of GB-grammars, the parser consists of several modules corre-
sponding to some of the subtheories of the grammar, such as X bar, bind-
ing, etc. Making an extensive use of lexical information and following stra-
tegies which attempt to take advantage of the basic properties of natural
languages, this parser is powerful enough to produce all of the grammatical
structures of sentences for a fairly substantial subset of French. At the
same time, it is restricted enough to avoid a proliferation of alternative
analyses, even with highly complex constructions. Particular attention has
been paid to the problem of the grammatical interpretation of wh-phrases,
to clitic constructions, as well as to the organisation and management of
the lexicon.
This paper examines some experimental Al systems that were specifically
developed for machine translation (Wilks&apos; Preference Semantics, the Yale
projects, Salat and CONTRA). It concentrates on the different types of
meaning representation used, and the nature of the knowledge used for the
solution of difficult problems in MT. To explore particular Al approaches,
the resolution of several types of ambiguity is discussed from the point of
view of different systems.
This paper gives a historical overview of the field of Machine Translation
(MT). The ALPAC report, the now well-known landmark in the history of
MT, serves to delimit the two sections of this paper. The first section,
Pre-ALPAC history, looks in some detail at the hopeful beginnings, the
first euphoric developments, and the onsetting disillusionment in MT. The
second section, Post-ALPAC overview, describes more recent develop-
ments on the basis of current prototype and commercial systems. It also
reviews some of the basic theoretical and practical issues in the field.
</bodyText>
<figure confidence="0.685988333333333">
Computational Linguistics, Volume 12, Number 1, January-March 1986 81
The FINITE STRING Abstracts of Current Literature
Software Engineering for Machine
Translation
Rod Johnson, Mike Rosner
No. 51 (1985)
</figure>
<bodyText confidence="0.9990006">
In this paper we discuss the desirable properties of a software environment
for MT development, starting from the position that succesful MT depends
on a coherent theory of translation. We maintain that such an environment
should not just provide for the construction of instances of MT systems
within some preconceived (and probably weak) theoretical framework,
but should also offer tools for rapid implementation and evaluation of a
variety of experimental theories. A discussion of some potentially interest-
ing properties of theories of language and translation is followed by a
description of a prototype software system which is designed to facilitate
practical experimentation with such theories.
</bodyText>
<page confidence="0.968699">
82 Computational Linguistics, Volume 12, Number 1, January-March 1986
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9923935">ABSTRACTS OF CURRENT LITERATURE Copies of the technical reports abstracted below are available from</title>
<author confidence="0.999814">Graeme Hirst</author>
<affiliation confidence="0.999983">Department of Computer Science University of Toronto</affiliation>
<address confidence="0.998882">Toronto, CANADA M5S 1A4</address>
<title confidence="0.9310595">A Computational Model for the Analysis of Argiunents</title>
<author confidence="0.999657">Robin Cohen</author>
<pubnum confidence="0.999162">Technical Report CSRG-151,</pubnum>
<date confidence="0.459954">October 1983, 225 pages</date>
<title confidence="0.951847">Understanding Adjectives</title>
<pubnum confidence="0.997278">Technical Report CSRI-167,</pubnum>
<title confidence="0.72520725">January 1985, 85 pages Rule-Based Processing in a Connectionist System for Natural Language Understanding</title>
<author confidence="0.999919">Bart Selman</author>
<pubnum confidence="0.999317">Technical Report CSRI-168,</pubnum>
<abstract confidence="0.997935333333333">February 1985, 57 pages This thesis proposes a model for an argument understanding system — a natural language understanding system which processes arguments. The form of input considered is one-way communication in a conversational setting, where the speaker tries to convince the hearer of a particular point of view. The main contributions are: (i) a theory of expected coherent structure which limits analysis to the reconstruction of particular transmission forms; (ii) a theory of linguistic clues which assigns a functional interpretation to special words and phrases used by the speaker to indicate structure; (iii) a theory of evidence relationships which includes the demand for pragmatic analysis to accommodate beliefs not currently held. A system designed to incorporate these theories could be used to analyze the structure of arguments — the necessary first step for a hearer, before judging credibility and responding. This thesis deals with the task of understanding noun phrases containing sequences of prenominal adjectives. The first problem is to determine exactly what each adjective modifies. In general, this can only be done by taking account of the semantic properties of the adjective in question, as well as those of other adjectives to its right, and of the noun itself. &amp;quot;Real-world&amp;quot; knowledge and contextual factors also play a role in this process. This is addressed by developing a classification scheme for adjectives which allows us to substantially reduce the number of candidate interpretations, in some cases to a single one. A system is presented which takes account of the disparate semantic behaviour of different classes of adjectives, word order, punctuation in the noun phrase, and a frame-based store of real-world knowledge, in order to determine the scope of adjectives within a noun phrase. The second problem is to construct a representation of the description embodied in such a noun phrase. Here, it is desirable that the structure of the representation correspond to the structure of modification within the phrase. Particular adjectives are taken to indicate restrictions on the values that objects may take on for associated properties. These properties may be featural, dimensional, or functional in nature. Frame-like structures are used to represent the generic concepts that are taken to be associated with noun phrases. We present a connectionist model for natural language processing. In contrast with previously proposed schemes, this scheme handles traditionally sequential rule-based processing in a general manner in the network. Another difference is the use of a computational scheme similar to the one used in the Boltzmann machine. This allows us to formulate general rules for the setting of weights and thresholds. We give a detailed description of a parsing system based on context-free grammar rules. Using simulated annealing, we show that at low temperatures the time average of the visited states at thermal equilibrium represents the correct parse of the input system. system is built from a small set of primitives represent the grammar rules. These primitives are linked together using pairs of 58 Computational Linguistics, Volume 12, Number 1, January-March 1986 FINITE STRING of Current Literature computing units that behave like discrete switches. These units are used as binders between concepts. They can be linked in such a way that individual rules can be selected from a collection of rules, and are very useful in the construction of connectionist schemes for any form of rule-based processing.</abstract>
<title confidence="0.9028995">Theory and Parsing of the Coordinate Conjunction &amp;quot;and&amp;quot;</title>
<author confidence="0.999614">Victoria L Snarr</author>
<pubnum confidence="0.999772">Technical Report CSRI-171,</pubnum>
<title confidence="0.513789333333333">September 1985, 71 pages Toward a Computational Interpretation of Situation Semantics</title>
<note confidence="0.99916">To appear, November 1985</note>
<title confidence="0.996158">The Representation of Ambiguity in Opaque Constructs</title>
<author confidence="0.99942">Brenda Fawcett</author>
<date confidence="0.999481">October 1985</date>
<pubnum confidence="0.703433">Technical Report, December 1985</pubnum>
<abstract confidence="0.998367591836735">the conjunction to have a simple function in the English language, it has proved to be a stumbling block for both theoretical and computational linguists. One of the theoretical problems of conjunction is to determine what governs the acceptability of a structure in which two elements are by corresponding computational problem is, given this knowledge, to incorporate it into an efficient parser for English. This thesis proposes a solution to the theoretical problem which is in the form of two general constraints — a syntactic constraint and a semantic one; and then incorporates these constraints into a &amp;quot;strictly deterministic&amp;quot; parser for English. Situation Semantics proposes novel and attractive treatments for several problem areas of natural language semantics, such as efficiency (context sensitivity) and propositional attitude reports. Its focus on the information carried by utterances makes the approach very promising for accounting for pragmatic phenomena. However, Situation Semantics seems to oppose several basic assumptions underlying current approaches to natural language processing and the design of intelligent systems in general. It claims that efficiency undermines the standard notions of logical form, entailment, and proof theory, and objects to the view that mental processes necessarily involve internal representations. The paper attempts to clarify these issues and discusses the impact of Situation Semantics&apos; criticisms for natural language processing, knowledge representation, and reasoning. I claim that the representational approach is the only currently practical one for the design of large intelligent systems, but argue that the representations used should be efficient in order to account for the system&apos;s embedding in its environment. The paper concludes by stating some constraints that a computational interpretation of Situation Semantics should obey and discussing remaining problems. A knowledge of intensions, which are used to designate concepts of objects, is important for natural language processing systems. Certain linguistic phrases can refer either to the concept of an entity or to the entity itself. To properly understand a phrase and to prevent invalid inferences from being drawn, the system must determine the type of reference being asserted. We identify a set of &amp;quot;opaque&amp;quot; constructions and suggest that a common mechanism be developed to handle them. To account for the ambiguities of opaque contexts, noun phrases are into be made explicit to whom the descriptor is ascribed and whether its referent is non-specific or specific. Similarly, constituents should be treated as evaluated relato conjectured of affairs. a test bed for these ideas we define a Montague-style meaning representation and implement the syntactic and components of a moderate-size in a logic programming environment. One must also consider how to disambiguate and interpret such a representation with respect to a knowledge base. Much contextual and world knowledge is required. We characterize what facilities are necessary for an accurate semantic interpretation, considering what is and is not available in current knowledge representation systems.</abstract>
<note confidence="0.68928075">Computational Linguistics, Volume 12, Number 1, January-March 1986 59 The FINITE STRING Abstracts of Current Literature following abstracts are from intelligence informatique), 1, Number 2 (May 1985).</note>
<title confidence="0.924272">What is a Heuristic?</title>
<author confidence="0.997111">Marc H J Romanycia</author>
<affiliation confidence="0.578801333333333">Information Services, Engineering and Planning Gulf Canada</affiliation>
<address confidence="0.994265">Calgary, Alberta, Canada 72P 2H7</address>
<author confidence="0.997079">Francis Jeffry Pelletier</author>
<affiliation confidence="0.999806">Departments of Philosophy and Computing Science University of Alberta</affiliation>
<address confidence="0.997034">Edmonton, Alberta, Canada T6G 2E5</address>
<phone confidence="0.940848">1(2): 47-58</phone>
<title confidence="0.990241">Possible Events, Actual Events, and Robots</title>
<author confidence="0.999911">Andrew Haas</author>
<affiliation confidence="0.998873">BBN Laboratories</affiliation>
<address confidence="0.9993215">10 Moulton Street Cambridge, MA 02238</address>
<phone confidence="0.988057">1(2): 59-70</phone>
<title confidence="0.984656">A Functional Approach to Non-Monotonic Logic</title>
<author confidence="0.999772">Erik Sandewall</author>
<affiliation confidence="0.998237333333333">Department of Computer and Information Science Linkoping University</affiliation>
<address confidence="0.981076">Linkiiing, Sweden</address>
<phone confidence="0.921815">1(2): 80-87</phone>
<abstract confidence="0.998082057142857">From the mid-1950s to the present the notion of a heuristic has played a crucial role in the Al researchers&apos; descriptions of their work. What has not been generally noticed is that different researchers have often applied the term to rather different aspects of their programs. Things that would be called a heuristic by one researcher would not be so called by others. This is because many heuristics embody a variety of different features, and the various researchers have emphasized different ones of these features as being essential to being a heuristic. This paper steps back from any particular research program and investigates the question of what things, historically, have been thought to be central to the notion of a heuristic and which ones conflict with others. After analyzing the previous definitions and examining current usage of the term, a synthesizing definition is provided. The hope is that with this broader account of &amp;quot;heuristic&amp;quot; in hand, researchers can benefit more fully from the insights of others, even if those insights are couched in a somewhat alien vocabulary. To plan means reasoning about possible actions, but a robot must also reason about actual events. This paper proposes a formal theory about actual and possible events. It presents a new modal logic as a notation for this theory and a technique for planning in the modal logic using a firstorder theorem prover augmented with simple modal reasoning. This avoids the need for a general modal-logic theorem prover. Adding beliefs to this theory raises an interesting problem for which the paper offers a tentative solution. Axiom sets and their extensions are viewed as functions from the set of in the language to a set of four truth values: f, u undefined, contradiction. Such functions form a lattice with &amp;quot;contains less information&amp;quot; as the partial order E, and &amp;quot;combination of several sources knowledge&amp;quot; as the least-upper-bound operation rules are expressed as binary relations between such functions. We show that the usual criterion on fixpoints, namely, to be minimal, does not apply correctly in the case of non-monotonic inference rules. A stronger concept, approachable fixpoints, is introduced and proven to be sufficient for the existence of a derivation of the fixpoint. In addition, the usefulness of our approach is demonstrated by concise proofs for some previously known results about normal default rules.</abstract>
<title confidence="0.625321">The following abstracts are from the Proceedings of the 23rd Annual Meeting of the Association for Computational</title>
<author confidence="0.7413305">See the form at the end of this issue for ordering information</author>
<author confidence="0.7413305">or contact Donald E Walker</author>
<author confidence="0.7413305">ACL</author>
<affiliation confidence="0.998449">Bell Communications Research</affiliation>
<address confidence="0.99662">445 South Street, MRE 2A379 Morristown, NJ 07960 USA</address>
<title confidence="0.8411595">Semantics of Temporal Queries and Temporal Data</title>
<author confidence="0.999053">Carole D Hafner</author>
<affiliation confidence="0.999866">College of Computer Science Northeastern University</affiliation>
<address confidence="0.999851">Boston, MA 02115</address>
<note confidence="0.706091">Proc. ACL, pp. 1-8</note>
<title confidence="0.99377">Temporal Inferences in Medical Texts</title>
<author confidence="0.994531">Klaus K Obermeier</author>
<abstract confidence="0.924045125">This paper analyzes the requirements for adding a temporal reasoning component to a natural language database query system, and proposes a computational model that satisfies those requirements. A preliminary implementation in Prolog is used to generate examples of the model&apos;s capabilities. The objectives of this paper are twofold, whereby the computer program is meant to be a particular implementation of a general natural language Linguistics, Volume Number 1, 1986</abstract>
<title confidence="0.835619">The FINITE STRING Abstracts of Current Literature</title>
<affiliation confidence="0.578664">Battelle&apos;s Columbus Laboratories</affiliation>
<address confidence="0.993611">505 King Avenue Columbus, OH 43201-2693 USA</address>
<note confidence="0.729108">Proc. ACL, pp. 9-17</note>
<title confidence="0.970005">Tense, Aspect and the Cognitive Representation of Time</title>
<author confidence="0.973981">Yip</author>
<affiliation confidence="0.99952">Artificial Intelligence Laboratory, M.I.T.,</affiliation>
<address confidence="0.999421">545 Technology Square Cambridge, MA 02139</address>
<note confidence="0.826404">Proc. ACL, pp. 18-26</note>
<title confidence="0.980627">Classification of Modality Function and its Application to Japanese Language Analysis</title>
<author confidence="0.990088">Show Naito</author>
<author confidence="0.990088">Akira Shimazu</author>
<author confidence="0.990088">Hirosato Nomura</author>
<affiliation confidence="0.976667">Musashino Electrical Communication</affiliation>
<note confidence="0.6255575">Laboratories, N.T.T. 3-9-11, Midori-cho, Musashino-shi Tokyo, 180, Japan Proc. ACL, pp. 27-34</note>
<title confidence="0.918837666666667">Universality and Individuality: The Interaction of Noun Phrase Determiners in Copular Clauses</title>
<author confidence="0.999796">John C Mallery</author>
<affiliation confidence="0.990224">Political Science Department &amp; Artificial Intelligence Laboratory, M.I.T.</affiliation>
<address confidence="0.999692">545 Technology Square, NE43-797 Cambridge, MA 02139</address>
<phone confidence="0.412041">Proc. ACL, pp. 35-42</phone>
<abstract confidence="0.997934582089552">processing system which could be used for different domains. The first objective is to provide a theory for processing temporal information contained in a well-structured, technical text. The second objective is to argue for a knowledge-based approach to natural language processing in which the parsing procedure is driven by extralinguistic knowledge. The resulting computer program incorporates enough domain-specific and general knowledge so that the parsing procedure can be driven by the knowledge base of the program, while at the same time employing a descriptively adequate theory of syntactic processing, i.e., X-bar syntax. My parsing algorithm not only supports the prevalent theories of knowparsing put forth in also uses a sound linguistic theory for the necessary syntactic information processing. This paper explores the relationships between a computational theory of temporal representation (as developed by James Allen) and a formal linguistic theory of tense (as developed by Norbert Hornstein) and aspect. It aims to provide explicit answers to four fundamental questions: (1) what is the computational justification for the primitives of a linguistic theory; (2) what is the computational explanation of the formal grammatical constraints; (3) what are the processing constraints imposed on the learnability and markedness of these theoretical constructs; and (4) what are the constraints that a linguistic theory imposes on representations. We show that one can effectively exploit the interface between the language faculty and the cognitive faculties by using linguistic constraints to determine restrictions on the cognitive representations and vice versa. Three main results are obtained: (1) We derive an explanation of an observed grammatical constraint on tense — the Linear Order Constraint — from the information monotonicity property of the constraint propagation algorithm of Allen&apos;s temporal system; (2) We formulate a principle of markedness for the basic tense structures based on the computational efficiency of the temporal representations; and (3) We show Allen&apos;s intervalbased temporal system is not arbitrary, but it can be used to explain independently motivated linguistic constraints on tense and aspect interpretations. We also claim that the methodology of research developed in this study — &amp;quot;cross-level&amp;quot; investigation of independently motivated formal grammatical theory and computational models — is a powerful paradigm with which to attack representational problems in basic cognitive domains, e.g., space, time, causality, etc. This paper proposes an analysis method for Japanese modality. In this purpose, meaning of Japanese modality is classified into four semantic categories and the role of it is formalized into five modality functions. Based on these formalizations, information and constraints to be applied to the modality analysis procedure are specified. Then by combining these investigations with case analysis, the analysis method is proposed. This analysis method has been applied to Japanese analysis for machine translation. This paper presents an implemented theory for quantifying noun phrases in clauses containing copular verbs (e.g., &apos;be&apos; and `become&apos;). Proceeding from recent theoretical work by Jackendoff (1983), this computational theory recognizes the dependence of the quantification decision on the definiteness, indefiniteness, or classness of both the subject and object of copular verbs in English. Jackendoff&apos;s intuition about the quantificational interdependence of subject and object has been imported from his broader cognitive theory and reformulated within a constraint propagation frame- Extensions reported here include the addition of more active deter- Computational Linguistics, Volume 12, Number 1, January-March 1986 61 The FINITE STRING Abstracts of Current Literature miners, the expansion of determiner categories, and the treatment of displaced objects. A further finding is that quantificational constraints may propagate across some clausal boundaries. The algorithm is used by Language Understanding System during a phase of analysis that posts constraints to produce a &apos;constraint tree&apos;. This phase comes after creation of syntactic deep structure and before sentential reference in a semantic-network model. Incorporation of the quantification algorithm in a larger system that parses sentences and builds semantic from them makes to acquire taxonomic and identity information from text.</abstract>
<title confidence="0.9850395">Meinongian Semantics for Propositional Semantic Networks</title>
<author confidence="0.999998">William J Rapaport</author>
<affiliation confidence="0.920065">Department of Computer Science, State University of New York</affiliation>
<address confidence="0.996024">Buffalo, NY 14260</address>
<note confidence="0.94667">Proc. ACL, pp. 43-48</note>
<title confidence="0.992062">Speech Acts and Rationality</title>
<author confidence="0.999995">Philip R Cohen</author>
<affiliation confidence="0.9518425">Artificial Intelligence Center, SRI Interna- CSLI, Stanford University</affiliation>
<author confidence="0.999947">Hector J Levesque</author>
<affiliation confidence="0.999364">Department of Computer Science University of Toronto</affiliation>
<note confidence="0.591031">Proc. ACL, pp. 49-60</note>
<title confidence="0.924672">Ontological Promiscuity</title>
<author confidence="0.999872">Jerry R Hobbs</author>
<affiliation confidence="0.983197">Intelligence Center, SRI International &amp; CSLI, Stanford University</affiliation>
<note confidence="0.609538">Proc. ACL, pp. 61-69</note>
<title confidence="0.831435">Reversible Automata and Induction of the English Auxiliary System</title>
<author confidence="0.999786">Samuel F Pilate</author>
<author confidence="0.999786">Robert C Berwick</author>
<affiliation confidence="0.997303">Artificial Intelligence Laboratory, M.I.T.</affiliation>
<address confidence="0.9978985">545 Technology Square Cambridge, MA 02139</address>
<note confidence="0.772522">Proc. ACL, pp. 70-75</note>
<title confidence="0.9790915">The Computational Difficulty of ID/LP Parsing</title>
<abstract confidence="0.993805204545455">G. Edward Barton, Jr. This paper surveys several approaches to semantic-network semantics that have not previously been treated in the AI or computational linguistics literature, though there is a large philosophical literature investigating them in some detail. In particular, propositional semantic networks (exemplifed by SNePS) are discussed, it is argued that only a fully intensional (&amp;quot;Meinongian&amp;quot;) semantics is appropriate for them, and several Meinongian systems are presented. This paper derives the basis of a theory of communication from a formal theory of rational interaction. The major result is a demonstration that illocutionary acts need not be primitive, and need not be recognized. As a test case, we derive Searle&apos;s conditions on requesting from principles of rationality coupled with a Gricean theory of imperatives. The theory is shown to distinguish insincere or nonserious imperatives from true requests. Extensions to indirect speech acts, and ramifications for natural language systems are also briefly discussed. To facilitate work in discourse interpretation, the logical form of English sentences should be both close to English and syntactically simple. In this paper I propose a logical notation which is first-order and nonintensional, and for which semantic translation can be naively compositional. The key move is to expand what kinds of entities one allows in one&apos;s ontology, rather than complicating the logical notation, the logical form of sentences, or the semantic translation process. Three classical problems — opaque the distinction between re dicto reports, and the problem of identity in intensional contexts — are examined for the difficulties they pose for this logical notation, and it is shown that the difficulties can be overcome. The paper closes with a statement about the view of semantics that is presupposed by this approach. In this paper we apply some recent work of Angluin (1982) to the induction of the English auxiliary verb system. In general, the induction of finite automata is computationally intractable. However, Angluin shows restricted finite automata, the can be learned by efficient (polynomial time) algorithms. We present an explicit computer model demonstrating that the English auxiliary verb system can in fact be learned as a 1-reversible automaton, and hence in a computationally feasible amount of time. The entire system can be acquired by looking at only half the possible auxiliary verb sequences, and the pattern of generalization seems compatible with what is known about human acquisition of auxiliaries. We conclude that certain linguistic subsystems may well be feasible by inductive inference methods of this kind, and suggest an extension to context-free language. Modern linguistic theory attributes surface complexity to interacting of constraints. For instance, the formalism separates constraints on immediate dominance from those on linear order.</abstract>
<note confidence="0.613912">Linguistics, Volume 12, Number 1, January-March 1986</note>
<title confidence="0.675206">The FINITE STRING Abstracts of Current Literature</title>
<affiliation confidence="0.953695">Artificial Intelligence Laboratory, M.I.T.</affiliation>
<address confidence="0.995633">545 Technology Square Cambridge, MA 02139</address>
<note confidence="0.749861">Proc. ACL, pp. 76-81</note>
<title confidence="0.9994">Modular Logic Grammars</title>
<author confidence="0.999964">Michael C McCord</author>
<affiliation confidence="0.97013">J. Watson Research Center</affiliation>
<address confidence="0.9853955">P.O. Box 218 Yorktown Heights, NY 10598</address>
<abstract confidence="0.982643074074074">Proc. ACL, pp. 104-117 Shieber&apos;s (1983) ID/LP parsing algorithm shows how to use ID and LP constraints directly in language processing, without expanding them into an intermediate &amp;quot;object grammar&amp;quot;. However, Shieber&apos;s purported bound underestimates the difficulty of ID/LP parsing. ID/LP parsing is actually NP-complete, and the worst-case runtime of Shieber&apos;s algorithm is actually exponential in grammar size. The growth of parser data structures causes the difficulty. Some computational and linguistic implications follow; in particular, it is important to note that despite its potential for combinatorial explosion, Shieber&apos;s algorithm remains better than the alternative of parsing an expanded object grammar. Tree Adjoining Grammar (TAG) is a formalism for natural language grammars. Some of the basic notions of TAGs were introduced in Joshi, Levy, and Takahashi (1975) and by Joshi (1983). A detailed investigation of the linguistic relevance of TAGs has been carried out in Kroch and Joshi (1985). In this paper, we will describe some new results for TAGs, espein the following areas: complexity of TAGs, (2) some closure results for TAGs, and (3) the relationship to Head grammars. Tree Adjoining Grammars, or &amp;quot;TAGs&amp;quot;, (Joshi, Levy &amp; Takahashi 1975; Joshi 1983; Broch &amp; Joshi 1985) were developed as an alternative to the standard syntactic formalisms that are used in theoretical analyses of language. They are attractive because they may provide just the aspects of context sensitive expressive power that actually appear in human languages while otherwise remaining context free. This paper describes how we have applied the theory of Tree Adjoining Grammars to natural language generation. We have been attracted to TAGs because their central operation — the extension of an &amp;quot;initial&amp;quot; phrase structure tree through the inclusion, at very specifically constrained locations, of one or more &amp;quot;auxiliary&amp;quot; trees — corresponds directly to certain central operations of our own, performance-oriented theory. We begin by briefly describing TAGs as a formalism for phrase structure in a competence theory, and summarize the points in the theory of TAGs that are germane to our own theory. We then consider generally the position of a grammar within the generation process, introducing our use of TAGs through a contrast with how others have used systemic grammars. This takes us to the core results of our paper: using examples from our research with well-written texts from newspapers, we walk through our TAG inspired treatments of raising and wh-movement, and show the correspondence of the TAG &amp;quot;adjunction&amp;quot; operation and our &amp;quot;attachment&amp;quot; process. In the final section we discuss extensions to the theory, motivated by the way we use the operation corresponding to TAGs&apos; adjunction in performance. This suggests that the competence theory of TAGs can be profitably projected to structures at the morphological level as well as the present syntactic level. This report describes a logic grammar formalism, Modular Logic Grammars, exhibiting a high degree of modularity between syntax and semantics. There is a syntax rule compiler (compiling into Prolog) which takes care of the building of analysis structures and the interface to a clearly separated semantic interpretation component dealing with scoping and the construction of logical forms. The whole system can work in either a onepass mode or a two-pass mode. In the one-pass mode, logical forms are built directly during parsing through interleaved calls to semantics, added by the rule compiler. In the two-pass mode, syntactic analy-</abstract>
<title confidence="0.976135">Some Computational Properties of Tree Adjoining Grammars</title>
<author confidence="0.999869">K Vijay-Shankar</author>
<author confidence="0.999869">Aravind K Joshi</author>
<affiliation confidence="0.990052">Department of Computer and Information Science</affiliation>
<address confidence="0.926066">Room 268, Moore School/D2</address>
<affiliation confidence="0.998863">University of Pennsylvania</affiliation>
<address confidence="0.999448">Philadelphia, PA 19104</address>
<note confidence="0.842064">Proc. ACL, pp. 82-93</note>
<title confidence="0.938574">TAGs as a Grammatical Formalism for Generation</title>
<author confidence="0.99997">David D McDonald</author>
<author confidence="0.99997">James D Pustejovsky</author>
<affiliation confidence="0.993981666666667">Department of Computer and Information Science University of Massachusetts at Amherst</affiliation>
<note confidence="0.8126335">Proc. ACL, pp. 94-103 Computational Linguistics, Volume 12, Number 1, January-March 1986 63</note>
<title confidence="0.993815">The FINITE STRING New Approaches to Parsing Conjunctions using Prolog</title>
<author confidence="0.999007">Sandimy Fong</author>
<author confidence="0.999007">Robert C Berwick</author>
<affiliation confidence="0.997289">Artificial Intelligence Laboratory, M.I.T.</affiliation>
<address confidence="0.997906">545 Technology Square Cambridge, MA 02139</address>
<note confidence="0.658314">Proc. ACL, pp. 118-126</note>
<title confidence="0.991508">Parsing with Discontinuous Constituents</title>
<author confidence="0.999999">Mark Johnson</author>
<affiliation confidence="0.9996525">CSLI &amp; Department of Linguistics, Stanford University,</affiliation>
<note confidence="0.439394">Proc. ACL, pp. 127-132</note>
<title confidence="0.992255">Structure Sharing with Binary Trees</title>
<author confidence="0.999964">Lauri Karttunen</author>
<affiliation confidence="0.986792">SRI International, CSLI Stanford</affiliation>
<author confidence="0.943026">Martin Kay</author>
<note confidence="0.745797">Xerox PARC, CSLI Stanford Proc. ACL, pp. 133-136</note>
<abstract confidence="0.992971980392157">Abstracts of Current Literature sis trees are built automatically in the first pass, and then given to the (one-pass) semantic component. The grammar formalism includes two devices which cause the automatically built syntactic structures to differ derivation trees in two ways: (1) there is a for dealing with left-embedding constructions such as English possessive noun phrases while using right-recursive rules (which are appropriate for Prolog pars- (2) There is a distinction in the syntactic formalism between and weak non-terminals, which is important for distinguishing major levels of grammar. Conjunctions are particularly difficult to parse in traditional, phrase-based This paper shows how a not based on tree structures, markedly improves the parsing problem for conjunctions. It modifies the union of phrase marker model proposed by Goodal (1984), where conjunction is considered as the linearization of a three-dimensional union of a non-tree based phrased marker representation. A PROLOG grammar for conjunctions using this new approach is given. It is far simpler and more transparent than a recent phrase-based extraposition parser for conjunctions by Dahl and McCord (1984). Unlike the Dahl and ATN SYSCONJ no special trail machinery is needed for conjunction, beyond that required for analyzing simple sentences. While of comparable efficiency, the new approach unifies under a single a host of related constructions: right node raising, or gapping. Another advantage is that it is also completely reversible (without cuts), and therefore can be used to generate sentences. generalizing the notion of a constituent to allow discontinuous locations, one can describe the discontinuous constituents of nonconfigurational languages. These discontinuous constituents can be described by a variant of definite clause grammars, and these grammars can be used in conjunction with a proof procedure to create a parser for non-configurational languages. Many current interfaces for natural language represent syntactic and semantic information in the form of directed graphs where attributes correspond to vectors and values to nodes. There is a simple correspondence between such graphs and the matrix notation linguists traditionally use for feature sets. The standard operation for working with such graphs is unification. The unification operation succeeds only on a pair of compatible graphs, and its result is a graph containing the information in both contributors. When a parser applies a syntactic rule, it unifies selected features of input constituents to check constraints and to build a representation for the output constituent. cat agr np a. number person 3rd cat: p [ agr: number: sg [person: 3rd]</abstract>
<title confidence="0.985868">A Structure-Sharing Representation for Unification-Based Grammar Formalisms</title>
<author confidence="0.999707">Fernando C N Pereira</author>
<affiliation confidence="0.9934445">Artificial Intelligence Center, SRI International &amp; CSLI, Stanford University</affiliation>
<abstract confidence="0.8731928">This paper describes a structure-sharing method for the representation of complex phrase types in a parser for PATR-II, a unification-based grammar formalism. In parsers for unification-based grammar formalisms, complex are derived by incremental refinement of the phrase types in</abstract>
<note confidence="0.86521475">Computational Linguistics, Volume 12, Number 1, January-March 1986 64 The FINITE STRING Abstracts of Current Literature Proc. ACL, pp. 137-144</note>
<title confidence="0.999447">Using Restriction to Extend Parsing Algorithms for Complex-Feature-Based Formalisms</title>
<author confidence="0.999993">Stuart M Shieber</author>
<affiliation confidence="0.9905205">Artificial Intelligence Center, SRI International &amp; CSLI, Stanford University</affiliation>
<address confidence="0.43236">Proc. ACL, pp. 145-152</address>
<title confidence="0.989529">Semantic Caseframe Parsing and Syntactic Generality</title>
<author confidence="0.999754">Philip J Hayes</author>
<author confidence="0.999754">Peggy M Anderson</author>
<author confidence="0.999754">Scott Sailer</author>
<affiliation confidence="0.795422">Carnegie Group Incorporated Commerce Court at Station Square</affiliation>
<address confidence="0.998081">Pittsburgh, PA 15219</address>
<note confidence="0.946342">Proc. ACL, pp. 153-160</note>
<title confidence="0.981534">Movement in Active Production Networks</title>
<author confidence="0.999994">Mark A Jones</author>
<author confidence="0.999994">Alan S Driscoll</author>
<affiliation confidence="0.999969">AT&amp;T Bell Laboratories</affiliation>
<address confidence="0.999925">Murray Hill, NJ 07974</address>
<note confidence="0.877697">Proc. ACL, pp. 161-166</note>
<title confidence="0.945603">Parsing Head-driven Phrase Structure Grammar</title>
<author confidence="0.990359">Derek Proudian</author>
<author confidence="0.990359">Carl Pollard</author>
<affiliation confidence="0.997891">Hewlett-Packard Laboratories</affiliation>
<address confidence="0.9994055">1501 Page Mill Road Palo Alto, CA 94303</address>
<phone confidence="0.348578">Proc. ACL, pp. 167-171</phone>
<abstract confidence="0.996779870967742">grammar rules and lexical entries. In a naive implementation, a new phrase type is built by copying older ones and then combining the copies according to the constraints stated in the grammar rule. The structure-sharing method was designed to eliminate most such copying; indeed, practical tests suggest that the use of this technique reduces parsing time by as much as 60%. The present work is inspired by the structure-sharing method for theorem proving introduced by Boyer and Moore and on the variant of it that is used in some Prolog implementations. Grammar formalisms based on the encoding of grammatical information in complex-valued feature systems enjoy some currency both in linguistics and natural-language-processing research. Such formalisms can be thought of by analogy to context-free grammars as generalizing the notion of nonterminal symbol from a finite domain of atomic elements to a possibly infinite domain of directed graph structures of a certain sort. Unfortunately, in moving to an infinite non-terminal domain, standard methods of parsing may no longer be applicable to the formalism. Typically, the problem manifests itself as gross inefficiency or even non-termination of the algorithms. In this paper, we discuss a solution to the problem of extending parsing algorithms to formalisms with possibly infinite non-terminal domains, a solution based on a general technique we call restriction. As a particular example of such an extension, we present a complete, correct, terminating extension of Earley&apos;s algorithm that uses restriction to perform top-down filtering. Our implementation of this algorithm demonstrates the drastic elimination of chart edges that can be achieved by this technique. Finally, we describe further uses for the technique — including parsing other grammar formalisms, including definite-clause grammars; extending other parsing algorithms, including LR methods and syntactic preference modeling algorithms; and efficient indexing. We have implemented a restricted domain parser called Plume (trademark of Carnegie Group Incorporated). Building on previous work at Carnegie-Mellon University, Plume&apos;s approach to parsing is based on semantic caseframe instantiation. This has the advantages of efficiency on grammatical input, and robustness in the face of ungrammatical input. While Plume is well adapted to simple declarative and imperative utterances, it handles passives and relative clauses and interrogatives in an ad hoc manner, leading to patchy semantic coverage. This paper outlines Plume as it currently exists and describes our detailed design for extending Plume to handle passives, relative clauses, and interrogatives in a general manner. We describe how movement is handled in a class of computational devices production networks The a parallel, activation-based framework that has been applied to other aspects of natural language processing. The model is briefly defined, the notation and mechanism for movement is explained, and then several examples are given which illustrate how various conditions on movement can naturally be in terms of limitations of the Head-driven Phrase Structure Grammar project an English language database query system under development at Hewlett-Packard Laboratories. Unlike other product-oriented efforts in the natural understanding field, the was designed and implemented by linguists on the basis of recent theoretical developments. But, unlike other implementations of linguistic theories, this system is not a toy, it deals with a variety of practical problems not covered in the theore- Computational Linguistics, Volume 12, Number 1, January-March 1986 65 The FINITE STRING Abstracts of Current Literature tical literature. We believe that this makes the HPSG system unique in its combination of linguistic theory and practical application. The HPSG system differs from its predecessor GPSG, reported on at the 1982 ACL meeting (Gawron et al. 1982), in four significant respects: syntax, lexical representation, parsing, and semantics. The paper focuses on parsing issues, but also gives a synopsis of the underlying syntactic formalism.</abstract>
<title confidence="0.991694">A Computational Semantics for Natural Language</title>
<author confidence="0.999568">Lewis G Creary</author>
<author confidence="0.999568">Carl J Pollard</author>
<affiliation confidence="0.998796">Hewlett-Packard Laboratories</affiliation>
<address confidence="0.9989605">1501 Page Mill Road Palo Alto, CA 94303</address>
<phone confidence="0.433093">Proc. ACL, pp. 172-1 79</phone>
<title confidence="0.8797955">Analysis of Conjunctions in a Rule-Based Parser</title>
<author confidence="0.995572">Leonard Lesmo</author>
<author confidence="0.995572">Pietro Totusso</author>
<affiliation confidence="0.9990705">Dipartimento di Informatica Universita di Torino</affiliation>
<address confidence="0.976256">Via Valperga Caluso 37 10125 Torino, Italy</address>
<phone confidence="0.607499">Proc. ACL, pp. 180-18 7</phone>
<abstract confidence="0.999226108695652">In the new Head-Driven Phrase Structure Grammar (HPSG) language processing system that is currently under development at Hewlett-Packard Laboratories, the Montagovian semantics of the earlier GPSG system (see Gawron et al. 1982) is replaced by a radically different approach with a number of distinct advantages. In place of the lambda calculus and standard first-order logic, our medium of conceptual representation is a new logical formalism called NFLT (Neo-Fregean Language of Thought); compositional semantics is effected, not by schematic lambda expressions, but by LISP procedures that operate on NFLT expressions to produce new expressions. NFLT has a number of features that make it well-suited for natural language translations, including predicates of variable arity in which explicitly marked situational roles supersede order-coded argument positions, sortally restricted quantification, a compositional (but nonextensional) semantics that handles causal contexts, and a principled conceptual raising mechanism that we expect to lead to a computationally tractable account of propositional attitudes. The use of semantically compositional LISP procedures in place of lambda-schemas allows us to produce fully reduced translations on the fly, with no need for post-processing. This approach should simplify the task of using semantic information (such as sortal incompatibilities) to eliminate bad parse paths. The aim of the present paper is to show how a rule-based parser for the Italian language has been extended to analyze sentences involving conjunctions. The most noticeable fact is the ease with which the required modifications fit in the previous parser structure. In particular, the rules written for analyzing simple sentences (without conjunctions) needed only small changes. On the contrary, more substantial changes were made to the exception-handling rules (called &amp;quot;natural changes&amp;quot;) that are used to restructure the tree in case of failure of a syntactic hypothesis. The parser described in the present work constitutes the syntactic component of the FIDO system (a Flexible Interface for Database Operations), an interface allowing an end-user to access a relational database in natural language (Italian). Intersentential elliptical utterances occur frequently in information-seeking dialogues. This paper presents a pragmatics-based framework for interpreting such utterances, including identification of the speaker&apos;s discourse goal in employing the fragment. We claim that the advantage of this approach is its reliance upon pragmatic information, including discourse content and conversational goals, rather than upon precise representations of the preceding utterance alone. In this paper we examine the pragmatic knowledge an utterance-planning system must have in order to produce certain kinds of definite and indefinoun phrases. An system, other planning systems, plans actions to satisfy an agent&apos;s goals, but allows some of the actions to consist of the utterance of sentences. This approach to language generation emphasizes the view of language as action, and hence assigns a critical role to pragmatics.</abstract>
<title confidence="0.761393">A Pragmatics-Based Approach to Understanding Intersentential Ellipsis</title>
<author confidence="0.999927">Sandra Carberry</author>
<affiliation confidence="0.9991795">Department of Computer and Information Science, University of Delaware</affiliation>
<address confidence="0.970407">Newark, DE 19715</address>
<note confidence="0.835317">Proc. ACL, pp. 188-197</note>
<title confidence="0.974183">Some Pragmatic Issues in the Planning of Definite and Indefinite Noun Phrases</title>
<author confidence="0.999992">Douglas E Appelt</author>
<affiliation confidence="0.984376">Artificial Intelligence Center, SRI International &amp; CSLI, Stanford University</affiliation>
<note confidence="0.803569">Proc. ACL, pp. 198-203 Linguistics, Volume 12, Number 1, January-March 1986</note>
<title confidence="0.987288">The FINITE STRING Abstracts of Current Literature Repairing Reference Identification Failures by Relaxation</title>
<author confidence="0.999881">Bradley A Goodman</author>
<address confidence="0.9943585">10 Moulton Street Cambridge, MA 02238</address>
<note confidence="0.791351">Proc. ACL, pp. 204-217</note>
<title confidence="0.864259">Anaphora Resolution: Short-Term Memory and Focusing Raymonde Guindon</title>
<author confidence="0.491348">Microelectronics</author>
<author confidence="0.491348">Computer Technology</author>
<affiliation confidence="0.665599">Corporation (MCC)</affiliation>
<address confidence="0.762429">9430 Research Blvd. Austin, TX 78759</address>
<email confidence="0.498411">ACL,pp.</email>
<title confidence="0.968624">Explanation Structures in XSEL</title>
<author confidence="0.999781">Karen Kukich</author>
<affiliation confidence="0.9999465">Computer Science Department Carnegie-Mellon University</affiliation>
<address confidence="0.999851">Pittsburgh, PA 15213</address>
<note confidence="0.727129">Proc. ACL, pp. 228-237</note>
<abstract confidence="0.993928090909091">The goal of this work is the enrichment of human-machine interactions in a natural language environment. We want to provide a framework less restrictive than earlier ones by allowing a speaker leeway in forming an utterance about a task and in determining the conversational vehicle to deliver it. A speaker and listener cannot be assumed to have the same beliefs, contexts, backgrounds, or goals at each point in a conversation. As a result, difficulties and mistakes arise when a listener interprets a speaker&apos;s utterance. These mistakes can lead to various kinds of misunderstandings between speaker and listener, including reference failures or failure to understand the speaker&apos;s intention. We call these misunderstandings miscommunication. Such mistakes constitute a kind of &amp;quot;ill-formed&amp;quot; input that can slow down and possibly break down communication. Our goal is to recognize and isolate such miscommunications and circumvent them. This paper will highlight a particular class of miscommunication — reference problems — by describing a case study, including techniques for avoiding failures of reference. Anaphora resolution is the process of determining the referent of anaphors, such as definite noun phrases and pronouns, in a discourse. Computational linguists, in modeling the process of anaphora resolution, have proposed the notion of focusing. Focusing is the process, engaged in by a reader, of selecting a subset of the discourse items and making them highly available for further computations. This paper provides a cognitive basis for anaphora resolution and focusing. Human memory is divided into a short-term, an operating, and a long-term memory. Short-term memory can only contain a small number of meaning units and its retrieval time is fast. Short-term memory is divided into a cache and a buffer. The cache contains a subset of meaning units expressed in the previous sentences and the buffer holds a representation of the incoming sentence. Focusing is realized in the cache that contains a subset of the most topical units and a subset of the most recent units in the text. The information stored in the cache is used to integrate the incoming sentence with the preceding discourse. Pronouns should be used to refer to units in focus. Operating memory contains a very large number of units but its retrieval time is slow. It contains the previous text units that are not in the cache. It comprises the text units not in focus. Definite noun phrases should be used to refer to units not in focus. Two empirical studies are described that demonstrate the cognitive basis for focusing, the use of definite noun phrases to refer to antecedents not in focus, and the use of pronouns to refer to antecedents in focus. Expert systems provide a rich testbed from which to develop and test techniques for natural language processing. These systems capture the knowledge needed to solve real-world problems in their respective domains, and that knowledge can and should be exploited for testing computational procedures for natural language processing. Parsing, semantic interpretation, dialog monitoring, discourse organization, and text generation are just a few of the language processing problems that might take advantage of the pre-structured semantic knowledge of an expert system. In particular, the need for explanation generation facilities for expert systems provides an opportunity to explore the relationships between the underlying knowledge structures needed for automated reasoning and those needed for natural language processing. One such exploration was the development of an explanation generator for XSEL, which is an expert system that helps a salesperson in producing a purchase order for a computer system. This paper describes a technique called &amp;quot;link-dependent message generation&amp;quot; forms the basis for explanation generation in</abstract>
<note confidence="0.49263">Computational Linguistics, Volume 12, Number 1, January-March 1986 67</note>
<title confidence="0.995224">The FINITE STRING Abstracts of Current Literature Description Strategies for Naive and Expert Users</title>
<author confidence="0.999438">Cecile L Paris</author>
<affiliation confidence="0.999871">Department of Computer Science Columbia University</affiliation>
<address confidence="0.997603">New York, NY 10027</address>
<note confidence="0.827619">Proc. ACL, pp. 238-245</note>
<title confidence="0.99083">Stress Assignment in Letter to Sound Rules for Speech Synthesis</title>
<author confidence="0.999848">Kenneth Church</author>
<affiliation confidence="0.998579">AT&amp;T Bell Laboratories</affiliation>
<note confidence="0.557161">Proc. ACL, pp. 246-253</note>
<title confidence="0.996412">An Eclectic Approach to Building Natural Language Interfaces</title>
<author confidence="0.965572333333333">Brian Phillips</author>
<author confidence="0.965572333333333">Michael J Freiling</author>
<author confidence="0.965572333333333">James H Alexander</author>
<author confidence="0.965572333333333">Steven E Messick</author>
<author confidence="0.965572333333333">Steve Rehfuss</author>
<author confidence="0.965572333333333">Sheldon Nicholl</author>
<affiliation confidence="0.948163">Tektronix, Inc.</affiliation>
<address confidence="0.995832">P.O. Box 500, M/S 50-662 Beaverton, OR 97077</address>
<note confidence="0.849906">Proc. ACL, pp. 254-261</note>
<title confidence="0.95515">Structure-Sharing in Lexical Representations</title>
<author confidence="0.999738">Daniel Flickinger</author>
<author confidence="0.999738">Carl Pollard</author>
<author confidence="0.999738">Thomas Wasow</author>
<affiliation confidence="0.996958">Hewlett-Packard Laboratories</affiliation>
<address confidence="0.998867">1501 Page Mill Road Palo Alto, CA 94303</address>
<note confidence="0.556007">Proc. ACL, pp. 262-267</note>
<title confidence="0.997931">A Tool Kit for Lexicon Building</title>
<author confidence="0.999982">Thomas E Ahlswede</author>
<affiliation confidence="0.9999535">Computer Science Department Illinois Institute of Technology</affiliation>
<address confidence="0.998944">Chicago, IL 60616</address>
<abstract confidence="0.99663906779661">ACL, pp. It is widely recognized that a question-answering system should be able to tailor its answers to the user. One of the dimensions along which this tailoring can occur is with respect to the level of knowledge of a user about a domain. In particular, responses should be different depending on whether they are addressed to naive or expert users. To understand what these differences should be, we analyzed texts from adult and junior encyclopedias. We found that two different strategies were used in describing complex physical objects to juniors and adults. We show how these strategies have been implemented on a test database. This paper will discuss how to determine word stress from spelling. Stress assignment is a well-established weak point for many speech synthesizers because stress dependencies cannot be determined locally. It is impossible to determine the stress of a word by looking through a five or six character window, as many speech synthesizers do. Well-known examples such as / degradation / telegraphy that stress dependencies can span over two and three syllables. This paper will present a principled framework for dealing with these long distance dependencies. Stress assignment will be formulated in terms of Waltz&apos; style constraint propagation with four sources of constraints: (1) syllable weight, (2) part of speech, (3) morphology and (4) etymology. Syllable weight is perhaps the most interesting, and will be the main focus of this paper. Most of what follows&apos; has been implemented. INKA is a natural language interface to facilitate knowledge acquisition during expert system development for electronic instrument trouble-shooting. The expert system design methodology develops a domain definition, called GLIB, in the form of a semantic grammar. This grammar format GLIB to be used with the which constrains users to create statements within a subset of English. Incremental parsing in INGLISH allows immediate remedial information to be generated if a user deviates from the sublanguage. Sentences are translated into production rules using the methodology of lexical-functional grammar. The system is written in Smalltalk and, in INKA, produces rules for a Prolog inference engine. The lexicon now plays a central role in our implementation of a Headdriven Phrase Structure Grammar (HPSG), given the massive relocation into the lexicon of linguistic information that was carried by the phrase structure rules in the old GPSG system. HPSG&apos;s grammar contains fewer than twenty (very general) rules; its predecessor required over 350 to achieve roughly the same coverage. This simplification of the grammar is made possible by an enrichment of the structure and content of lexical entries, using both inheritance mechanisms and lexical rules to represent the linguistic information in a general and efficient form. We will argue that our mechanisms for structure-sharing not only provide the ability to express important linguistic generalizations about the lexicon, but also make possible an efficient, readily modifiable implementation that we find quite adequate for continuing development of a large natural language system. This paper describes a set of interactive routines that can be used to create, maintain, and update a computer lexicon. The routines are available to the user as a set of commands resembling a simple operating system. The lexicon produced by this system is based on lexical-semantic relations, but is compatible with a variety of other models of lexicon structure. The lexicon builder is suitable for the generation of moderate-sized vocabularies and 68 Computational Linguistics, Volume 12, Number 1, January-March 1986 The FINITE STRING Abstracts of Current Literature has been used to construct a lexicon for a small medical expert system. A future version of the lexicon builder will create a much larger lexicon by parsing definitions from machine-readable dictionaries.</abstract>
<title confidence="0.987004666666667">Using an On-line Dictionary to Find Rhyming Words and Pronunciations for Unknown Words</title>
<author confidence="0.999999">Roy J Byrd</author>
<affiliation confidence="0.999967">IBM Thomas J. Watson Research Center</affiliation>
<address confidence="0.988398">Yorktown Heights, NY 10598</address>
<author confidence="0.998976">Martin S Chodorow</author>
<affiliation confidence="0.887418333333333">Department of Psychology, Hunter College of CUNY Thomas J. Watson Research Center</affiliation>
<note confidence="0.613389">Proc. ACL, pp. 277-283</note>
<title confidence="0.9934">Towards a Self-Extending Lexicon</title>
<author confidence="0.998465">Uri Zernick</author>
<author confidence="0.998465">Michael G Dyer</author>
<affiliation confidence="0.999978">Artificial Intelligence Laboratory Computer Science Department</affiliation>
<address confidence="0.996195">3531 Boelter Hall</address>
<affiliation confidence="0.947926">University of California</affiliation>
<note confidence="0.805478">Proc. ACL, pp. 284-292 Granunatical Analysis by Computer of the Lancaster.Oslo/Bergen (LOB) Corpus of</note>
<title confidence="0.978926">British English Texts</title>
<author confidence="0.999713">Andrew David Beale</author>
<affiliation confidence="0.983699">Unit for Computer Research on the English Language Bowland College, University of Lancaster</affiliation>
<address confidence="0.987799">Bailrigg, Lancaster, England LA! 4YT</address>
<note confidence="0.824576">Proc. ACL, pp. 293-298</note>
<title confidence="0.9880195">Extracting Semantic Hierarchies from a Large On-Line Dictionary</title>
<author confidence="0.99996">Martin S Chodorow</author>
<affiliation confidence="0.994541666666667">Department of Psychology Hunter College of CUNY &amp; IBM Thomas J. Watson Research Center</affiliation>
<abstract confidence="0.993615470588236">Humans know a great deal about relationships among words. This paper discusses relationships among word pronunciations. We describe a computer system which models human judgement of rhyme by assigning specific roles to the location of primary stress, the similarity of phonetic segments, and other factors. By using the model as an experimental tool, we expect to improve our understanding of rhyme. A related computer model will attempt to generate pronunciations for unknown words by analogy with those for known words. The analogical processes involve techniques for segmenting and matching word spellings, and for mapping spelling to sound in known words. As in the case of rhyme, the computer model will be an important tool for improving our understanding of these processes. Both models serve as the basis for functions in the WordSmith automated dictionary system. The problem of manually modifying the lexicon appears with any natural language processing program. Ideally, a program should be able to acquire new lexical entries from context, the way People learn. We address the of acquiring entire phrases, specifically phrases, a lexicon. such a self-extending lexicon (a) — of the intended phrase from a set of phrases, (b) parsing — of partially-matching and (c) analysis — of errors in forming hypotheses about phrases. We have designed and implemented a program called uses implement RINA receives new figurative phrases in context and through the application of a of creates and refines both the patterns and the concepts which hold syntactic and semantic information about phrases. Research has been under way at the Unit for Computer Research on the English Language at the University of Lancaster, England, to develop a suite of computer programs which provide a detailed grammatical analysis the a collection of about 1 million words of British English texts available in machine readable form. The first phase of the project, completed in September 1983, produced a grammatically annotated version of the corpus giving a tag showing the word class of each word token. Over 93 per cent of the word tags were correctly selected by using a matrix of tag pair probabilities and this figure was upgraded by a further 3 per cent by retagging problematic strings of words prior to disambiguation and by altering the probability weightings for sequences of three tags. The remaining 3 to 4 per cent were corrected by a human post-editor. The system was originally designed to run in batch mode over the corpus but we have recently modified procedures to run interactively for sample sentences typed in by a user at a terminal. We are currently extending the word tag set and improving the word tagging procedures to reduce manual intervention. A similar probabilistic being developed for phrase and clause tagging. Dictionaries are rich sources of detailed semantic information, but in order to use the information for natural language processing, it must be organized systematically. This paper describes automatic and semi-automatic procedures for extracting and organizing semantic feature information implicit in dictionary definitions. Two head-finding heuristics are described for locating the genus terms in noun and verb definitions. The</abstract>
<note confidence="0.355613">Computational Linguistics, Volume 12, Number 1, January-March 1986 69</note>
<title confidence="0.628307">The FINITE STRING Abstracts of Current Literature</title>
<address confidence="0.828849">Yorktown Heights, NY 10598</address>
<author confidence="0.999867">Roy J Byrd</author>
<author confidence="0.999867">George E Heidorn</author>
<affiliation confidence="0.904933">J. Watson Research Center</affiliation>
<address confidence="0.320344">Proc. ACL, pp. 299-304</address>
<title confidence="0.705506">Dictionaries of the Mind</title>
<author confidence="0.998259">George A Miller</author>
<affiliation confidence="0.999934">Department of Psychology Princeton University</affiliation>
<address confidence="0.999885">Princeton, NJ 08544</address>
<note confidence="0.797777">Proc. ACL, pp. 305-314</note>
<title confidence="0.995149">The Use of Syntactic Clues in Discourse Processing</title>
<author confidence="0.994313">Nan Decker</author>
<address confidence="0.9935535">1834 Chase Avenue Cincinnati, OH 45223</address>
<note confidence="0.684656">Proc. ACL, pp. 315-323</note>
<title confidence="0.9784955">Grammar Viewed as a Functioning Part of a Cognitive System</title>
<author confidence="0.999994">Helen M Gigley</author>
<affiliation confidence="0.998066">Department of Computer Science University of New Hampshire</affiliation>
<address confidence="0.986942">Durham, NH 03824</address>
<abstract confidence="0.9887836">Proc. ACL, pp. 324-332 assumption is that the genus term represents inherent features of the word it defines. The two heuristics have been used to process definitions of 40,000 nouns and 8,000 verbs, producing indexes in which each genus term is associated with the words it defined. The Sprout program interactively grows a taxonomic &amp;quot;tree&amp;quot; from any specified root feature by consulting the genus index. Its output is a tree in which all of the nodes have the root feature for at least one of their senses. The Filter program uses an inverted form of the genus index. Filtering begins with an initial filter file consisting of words that have a given feature (e.g., [+human]) in all of their senses. The program then locates, in the index, words whose genus terms all appear in the filter file. The output is a list of new words that have the given feature in all of their senses. How lexical information should be formulated, and how it is organized in computer memory for rapid retrieval, are central questions for computational linguists who want to create systems for language understanding. How lexical knowledge is acquired, and how it is organized in human memory for rapid retrieval during language use, are also central questions for cognitive psychologists. Some examples of psycholinguistic research on the lexical component of language are reviewed with special attention to their implications for the computational problem. The desirability of a syntactic parsing component in natural language understanding systems has been the subject of debate for the past several years. This paper describes an approach to automatic text processing which is entirely based on syntactic form. A program is described which processes one genre of discourse, that of newspaper reports. The program creates summaries of reports by relying on an expanded concept of text grounding: certain syntactic structures and tense/aspect pairs indicate the most important events in a news story. Supportive, background material is also highly coded syntactically. Certain types of information are routinely expressed with distinct syntactic forms. Where more than one episode occurs in a single report, a change of episode will also be marked syntactically in a reliable way. How can grammar be viewed as a functional part of a cognitive system? Given a neural basis for the processing control paradigm of language performance, what roles does &amp;quot;grammar&amp;quot; play? Is there evidence to suggest that grammatical processing can be independent from other aspects of language processing? This paper will focus on these issues and suggest answers within the context of one computational solution. The example mode of sentence comprehension, HOPE, is intended to demonstrate both representational considerations for a grammar within such a system as well as to illustrate that by interpreting a grammar as a feedback control mechanism of a &amp;quot;neural-like&amp;quot; process, additional insights into language processing can be obtained.</abstract>
<note confidence="0.804178263157895">Selected Dissertation Abstracts Compiled by Susanne M. Humphrey, National Library of Medicine, Bethesda, MD 20894 Bob Krovetz, University of Massachusetts, Amherst, MA 01002 The following are citations selected by title and abstract as being related to computational linguistics or knowledge resulting from a computer search, using the Information retrieval service, of the Abstracts International produced by University Microfilms International. Computational Linguistics, Volume Number 1, 1986 The FINITE STRING Abstracts of Current Literature Included are the title; author; university, degree, and, if available, number of pages; DAI subject category chosen by the author of the dissertation; and UM order number and year-month of DAI. References are sorted first by DAI subject category and second by author. Unless otherwise specified, paper or microform copies of dissertations may be ordered from University Microfilms International Dissertation Copies Post Office Box 1764 Ann Arbor, MI 48106 telephone for U.S. (except Michigan, Hawaii, Alaska): 1-800-521-3042, for Canada: 1-800-268-6090.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>