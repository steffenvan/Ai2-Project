<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000171">
<title confidence="0.999653">
A Word-Order Database for Testing
Computational Models of Language Acquisition
</title>
<author confidence="0.982164">
William Gregory Sakas
</author>
<affiliation confidence="0.95697725">
Department of Computer Science
PhD Programs in Linguistics and Computer Science
Hunter College and The Graduate Center
City University of New York
</affiliation>
<email confidence="0.998411">
sakas@hunter.cuny.edu
</email>
<sectionHeader confidence="0.997384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999236">
An investment of effort over the last two
years has begun to produce a wealth of
data concerning computational psycholin-
guistic models of syntax acquisition. The
data is generated by running simulations
on a recently completed database of word
order patterns from over 3,000 abstract
languages. This article presents the design
of the database which contains sentence
patterns, grammars and derivations that
can be used to test acquisition models from
widely divergent paradigms. The domain
is generated from grammars that are lin-
guistically motivated by current syntactic
theory and the sentence patterns have been
validated as psychologically/developmen-
tally plausible by checking their frequency
of occurrence in corpora of child-directed
speech. A small case-study simulation is
also presented.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998751684210526">
The exact process by which a child acquires the
grammar of his or her native language is one of the
most beguiling open problems of cognitive
science. There has been recent interest in computer
simulation of the acquisition process and the
interrelationship between such models and linguis-
tic and psycholinguistic theory. The hope is that
through computational study, certain bounds can
be established which may be brought to bear on
pivotal issues in developmental psycholinguistics.
Simulation research is a significant departure
from standard learnability models that provide
results through formal proof (e.g., Bertolo, 2001;
Gold, 1967; Jain et al., 1999; Niyogi, 1998; Niyogi
&amp; Berwick, 1996; Pinker, 1979; Wexler &amp; Culi-
cover, 1980, among many others). Although
research in learnability theory is valuable and
ongoing, there are several disadvantages to formal
modeling of language acquisition:
</bodyText>
<listItem confidence="0.961863692307692">
• Certain proofs may involve impractically many
steps for large language domains (e.g. those
involving Markov methods).
• Certain paradigms are too complex to readily
lend themselves to deductive study (e.g. con-
nectionist models).1
• Simulations provide data on intermediate stages
whereas formal proofs typically prove whether
a domain is (or more often is not) learnable a
priori to specific trials.
• Proofs generally require simplifying assump-
tions which are often distant from natural lan-
guage.
</listItem>
<bodyText confidence="0.999151944444444">
However, simulation studies are not without
disadvantages and limitations. Most notable
perhaps, is that out of practicality, simulations are
typically carried out on small, severely circum-
scribed domains – usually just large enough to
allow the researcher to hone in on how a particular
model (e.g. a connectionist network or a principles
&amp; parameters learner) handles a few grammatical
features (e.g. long-distance agreement and/or
topicalization) often, though not always, in a single
language. So although there have been many
successful studies that demonstrate how one
algorithm or another is able to acquire some aspect
of grammatical structure, there is little doubt that
the question of what mechanism children actually
employ during the acquisition process is still open.
This paper reports the development of a large,
multilingual database of sentence patterns, gram-
</bodyText>
<note confidence="0.425515">
1 Although see Niyogi, 1998 for some insight.
</note>
<bodyText confidence="0.999931444444444">
mars and derivations that may be used to test
computational models of syntax acquisition from
widely divergent paradigms. The domain is
generated from grammars that are linguistically
motivated by current syntactic theory and the
sentence patterns have been validated as psycho-
logically/developmentally plausible by checking
their frequency of occurrence in corpora of child-
directed speech. We report here the structure of the
domain, its interface and a case-study that demon-
strates how the domain has been used to test the
feasibility of several different acquisition strate-
gies.
The domain is currently publicly available on
the web via http://146.95.2.133 and it is our hope
that it will prove to be a valuable resource for
investigators interested in computational models of
natural language acquisition.
</bodyText>
<sectionHeader confidence="0.956153" genericHeader="method">
2 The Language Domain Database
</sectionHeader>
<bodyText confidence="0.999959270833333">
The focus of the language domain database,
(hereafter LDD), is to make readily available the
different word order patterns that children are
typically exposed to, together with all possible
syntactic derivations of each pattern. The patterns
and their derivations are generated from a large
battery of grammars that incorporate many features
from the domain of natural language.
At this point the multilingual language domain
contains sentence patterns and their derivations
generated from 3,072 abstract grammars. The
patterns encode sentences in terms of tokens
denoting the grammatical roles of words and
complex phrases, e.g., subject (S), direct object
(O1), indirect object (O2), main verb (V), auxiliary
verb (Aux), adverb (Adv), preposition (P), etc. An
example pattern is S Aux V O1 which corresponds
to the English sentence: The little girl can make a
paper airplane. There are also tokens for topic and
question markers for use when a grammar specifies
overt topicalization or question marking.
Declarative sentences, imperative sentences,
negations and questions are represented within the
LDD, as is prepositional movement/stranding
(pied-piping), null subjects, null topics, topicaliza-
tion and several types of movement.
Although more work needs to be done, a first
round study of actual child-directed sentences from
the CHILDES corpus (MacWhinney, 1995)
indicates that our patterns capture many sentential
word orders that children typically encounter in the
period from 1-1/2 to 2-1/2 years; the period
generally accepted by psycholinguists to be when
children establish the correct word order of their
native language. For example, although the LDD is
currently limited to degree-0 (i.e. no embedding)
and does not contain DP-internal structure, after
examining by hand, several thousand sentences
from corpora in the CHILDES database in five
languages (English, German, Italian, Japanese and
Russian), we found that approximately 85% are
degree-0 and an approximate 10 out of 11 have no
internal DP structure.
Adopting the principles and parameters (P&amp;P)
hypothesis (Chomsky, 1981) as the underlying
framework, we implemented an application that
generated patterns and derivations given the
following points of variation between languages:
</bodyText>
<listItem confidence="0.986554571428571">
1. Affix Hopping 2. Comp Initial/Final
3. I to C Movement 4. Null Subject
5. Null Topic 6. Obligatory Topic
7. Object Final/Initial 8. Pied Piping
9. Question Inversion 10. Subject Initial/Final
11. Topic Marking 12. V to I Movement
13. Obligatory Wh movement
</listItem>
<bodyText confidence="0.965497011627907">
The patterns have fully specified X-bar struc-
ture, and movement is implemented as HPSG local
dependencies. Pattern production is generated top-
down via rules applied at each subtree level.
Subtree levels include: CP, C&apos;, IP, I&apos;, NegP, Neg&apos;,
VP, V&apos; and PP. After the rules are applied, the
subtrees are fully specified in terms of node
categories, syntactic feature values and constituent
order. The subtrees are then combined by a simple
unification process and syntactic features are
percolated down. In particular, movement chains
are represented as traditional “slash” features
which are passed (locally) from parent to daughter;
when unification is complete, there is a trace at the
bottom of each slash-feature path. Other features
include +/-NULL for non-audible tokens (e.g.
S[+NULL] represents a null subject pro), +TOPIC
to represent a topicalized token, +WH to represent
“who”, “what”, etc. (or “qui”, “que” if one pre-
fers), +/-FIN to mark if a verb is tensed or not and
the illocutionary (ILLOC) features Q, DEC, IMP
for questions, declaratives and imperatives respec-
tively.
Although further detail is beyond the scope of
this paper, those interested may refer to Fodor et
al. (2003) which resides on the LDD website.
It is important to note that the domain is suit-
able for many paradigms beyond the P&amp;P frame-
work. For example the context-free rules (with
local dependencies) could be easily extracted and
used to test probabilistic CFG learning in a
multilingual domain. Likewise the patterns,
without their derivations, could be used as input to
statistical/connectionist models which eschew
traditional (generative) structure altogether and
search for regularity in the left-to-right strings of
tokens that makeup the learner&apos;s input stream. Or,
the patterns could help bootstrap the creation of a
domain that might be used to test particular types
of lexical learning by using the patterns as tem-
plates where tokens may be instantiated with actual
words from a lexicon of interest to the investigator.
The point is that although a particular grammar
formalism was used to generate the patterns, the
patterns are valid independently of the formalism
that was in play during generation.2
To be sure, similar domains have been con-
structed. The relationship between the LDD and
other artificial domains is summarized in Table 1.
In designing the LDD, we chose to include
syntactic phenomena which:
i) occur in a relatively high proportion of the
known natural languages;
2 If this is the case, one might ask: Why bother with a
grammar formalism at all; why not use actual child-directed
speech as input instead of artificially generated patterns?
Although this approach has proved workable for several types
of non-generative acquisition models, a generative (or hybrid)
learner is faced with the task of selecting the rules or
parameter values that generate the linguistic environment
being encountered by the learner. In order to simulate this,
there must be some grammatical structure incorporated into
the experimental design that serves as the target the learner
must acquire. Constructing a viable grammar and a parser with
coverage over a multilingual domain of real child-directed
speech is a daunting proposition. Even building a parser to
parse a single language of child-directed speech turns out to be
extremely difficult. See, for example, Sagae, Lavie, &amp;
MacWhinney (2001), which discusses an impressive number
of practical difficulties encountered while attempting to build
a parser that could cope with the EVE corpus; one the cleanest
transcriptions in the CHILDES database. By abstracting away
from actual child-directed speech, we were able to build a
pattern generator and include the pattern derivations in the
database for retrieval during simulation runs, effectively
sidestepping the need to build an online multilingual parser.
ii) are frequently exemplified in speech di-
rected to 2-year-olds;
iii) pose potential learning problems (e.g. cross-
language ambiguity) for which theoretical
solutions are needed;
iv) have been a focus of linguistic and/or psy-
cholinguistic research;
v) have a syntactic analysis that is broadly
agreed on.
As a result the following have been included:
</bodyText>
<listItem confidence="0.999598125">
• By criteria (i) and (ii): negation, non-
declarative sentences (questions, impera-
tives).
• By criterion (iv): null subject parameter
(Hyams 1986 and since).
• By criterion (iv): affix-hopping (though not
widespread in natural languages).
• By criterion (v): no scrambling yet.
</listItem>
<bodyText confidence="0.972438">
There are several phenomena that the LDD
does not yet include:
</bodyText>
<listItem confidence="0.997622090909091">
• No verb subcategorization.
• No interface with LF (cf. Briscoe 2000;
Villavicencio 2000).
• No discourse contexts to license sentence
fragments (e.g., DP or PP fragments).
• No XP-internal structure yet (except PP = P
+ O3, with piping or stranding).
• No Linear Correspondence Axiom (Kayne
1994).
• No feature checking as implementation of
movement parameters (Chomsky 1995).
</listItem>
<table confidence="0.933633235294118">
# # Tree Language
parame lan- struc- properties
ters guages ture?
Gibson &amp; 3 8 Not fully Word order, V2
Wexler specified
(1994)
Bertolo et. 7 64 Yes G&amp;W + V-raising to
al (1997b) distinct Agr, T; deg-2
Kohl (1999) 12 2,304 Partial Bertolo et al.
based on (1997b) +
Bertolo scrambling
Sakas &amp; 4 16 Yes G&amp;W + null
Nishimoto subject/topic
(2002)
LDD 13 3,072 Yes S&amp;N + wh-movt +
imperatives +aux
inversion, etc.
</table>
<tableCaption confidence="0.993073">
Table 1: A history of abstract domains for word-
</tableCaption>
<bodyText confidence="0.982478233333333">
order acquisition modeling.
The LDD on the web: The two primary purposes
of the web-interface are to allow the user to
interactively peruse the patterns and the derivations
that the LDD contains and to download raw data
for the user to work with locally.
Users are asked to register before using the
LDD online. The user ID is typically an email
address, although no validity checking is carried
out. The benefit of entering a valid email address is
simply to have the ability to recover a forgotten
password, otherwise a user can have full access
anonymously.
The interface has three primary areas: Gram-
mar Selection, Sentence Selection and Data
Download. First a user has to specify, on the
Grammar Selection page, which settings of the 13
parameters are of interest and save those settings as
an available grammar. A user may specify multiple
grammars. Then in the sentence selection page a
user may peruse sentences and their derivations.
On this page a user may annotate the patterns and
derivations however he or she wishes. All grammar
settings and annotations are saved and available
the next time the user logs on. Finally on the Data
Download page, users may download data so that
they can use the patterns and derivations offline.
The derivations are stored as bracketed strings
representing tree structure. These are practically
indecipherable by human users. E.g.:
</bodyText>
<table confidence="0.978590777777778">
(CP[ILLOC Q][+FIN][+WH] &amp;quot;Adv[+TOPIC]&amp;quot; (Cbar[ILLOC
Q] [+FIN][+WH][SLASH Adv](C[ILLOC Q][+FIN] &amp;quot;KA&amp;quot; )
(IP[ILLOC Q][+FIN][+WH][SLASH Adv]&amp;quot;S&amp;quot; (Ibar[ILLOC
Q][+FIN][+WH][SLASH Adv](I[ILLOC
Q][+FIN]&amp;quot;Aux[+FIN]&amp;quot;)(NegP[+WH] [SLASH
Adv](NegBar[+WH][SLASH Adv](Neg &amp;quot;NOT&amp;quot;)
(VP[+WH][SLASH Adv](Vbar[+WH][SLASH
Adv](V&amp;quot;Verb&amp;quot;)&amp;quot;O1&amp;quot; &amp;quot;O2&amp;quot; (PP[+WH] &amp;quot;P&amp;quot; &amp;quot;O3[+WH]&amp;quot;
)&amp;quot;Adv[+NULL][SLASH Adv]&amp;quot;))))))))
</table>
<bodyText confidence="0.999876695652174">
To be readable, the derivations are displayed
graphically as tree structures. Towards this end we
have utilized a set of publicly available LaTex
macros: QTree (Siskind &amp; Dimitriadis, [online]). A
server-side script parses the bracketed structures
into the proper QTree/LaTex format from which a
pdf file is generated and subsequently sent to the
user&apos;s client application.
Even with the graphical display, a simple sen-
tence-by-sentence presentation is untenable given
the large amount of linguistic data contained in the
database. The Sentence Selection area allows users
to access the data filtered by sentence type and/or
by grammar features (e.g. all sentences that have
obligatory-wh movement and contain a preposi-
tional phrase), as well as by the user’s defined
grammar(s) (all sentences that are &amp;quot;Italian-like&amp;quot;).
On the Data Download page, users may filter
sentences as on the Sentence Selection page and
download sentences in a tab-delimited format. The
entire LDD may also be downloaded – approxi-
mately 17 MB compressed, 600 MB as a raw ascii
file.
</bodyText>
<sectionHeader confidence="0.976393" genericHeader="method">
3 A Case Study: Evaluating the efficiency
</sectionHeader>
<subsectionHeader confidence="0.494984">
of parameter-setting acquisition models.
</subsectionHeader>
<bodyText confidence="0.999990333333333">
We have recently run experiments of seven
parameter-setting (P&amp;P) models of acquisition on
the domain. What follows is a brief discussion of
the algorithms and the results of the experiments.
We note in particular where results stemming from
work with the LDD lead to conclusions that differ
from those previously reported. We stress that this
is not intended as a comprehensive study of
parameter-setting algorithms or acquisition
algorithms in general. There is a large number of
models that are omitted; some of which are targets
of current investigation. Rather, we present the
study as an example of how the LDD could be
effectively utilized.
In the discussion that follows we will use the
terms “pattern”, “sentence” and “input” inter-
changeably to mean a left-to-right string of tokens
drawn from the LDD without its derivation.
</bodyText>
<subsectionHeader confidence="0.996373">
3.1 A Measure of Feasibility
</subsectionHeader>
<bodyText confidence="0.999910083333333">
As a simple example of a learning strategy and
of our simulation approach, consider a domain of 4
binary parameters and a memoryless learner 3
which blindly guesses how all 4 parameters should
be set upon encountering an input sentence. Since
there are 4 parameters, there are 16 possible
combinations of parameter settings. i.e., 16
different grammars. Assuming that each of the 16
grammars is equally likely to be guessed, the
learner will consume, on average, 16 sentences
before achieving the target grammar. This is one
measure of a model’s efficiency or feasibility.
</bodyText>
<footnote confidence="0.911702333333333">
3 By “memoryless” we mean that the learner processes inputs
one at a time without keeping a history of encountered inputs
or past learning events.
</footnote>
<bodyText confidence="0.9999803">
However, when modeling natural language
acquisition, since practically all human learners
attain the target grammar, the average number of
expected inputs is a less informative statistic than
the expected number of inputs required for, say,
99% of all simulation trials to succeed. For our
blind-guess learner, this number is 72.4 We will
use this 99-percentile feasibility measure for most
discussion that follows, but also include the
average number of inputs for completeness.
</bodyText>
<subsectionHeader confidence="0.999088">
3.2 The Simulations
</subsectionHeader>
<bodyText confidence="0.984035">
In all experiments:
</bodyText>
<listItem confidence="0.974661888888889">
• The learners are memoryless.
• The language input sample presented to the
learner consists of only grammatical sentences
generated by the target grammar.
• For each learner, 1000 trials were run for each
of the 3,072 target languages in the LDD.
• At any point during the acquisition process,
each sentence of the target grammar is equally
likely to be presented to the learner.
</listItem>
<bodyText confidence="0.98124075">
Subset Avoidance and Other Local Maxima:
Depending on the algorithm, it may be the case
that a learner will never be motivated to change its
current hypothesis (Gcurr), and hence be unable to
ultimately achieve the target grammar (Gtarg). For
example, most error-driven learners will be trapped
if Gcurr generates a language that is a superset of
the language generated by Gtarg. There is a wealth
of learnability literature that addresses local
maxima and their ramifications.5 However, since
our study’s focus is on feasibility (rather than on
whether a domain is learnable given a particular
algorithm), we posit a built-in avoidance mecha-
nism, such as the subset principle and/or default
values that preclude local maxima; hence, we set
aside trials where a local maximum ensues.
</bodyText>
<footnote confidence="0.8917279">
4 The average and 99-percentile figures (16 and 72) in this
section are easily derived from the fact that input consumption
follows a hypergeometric distribution.
5 Discussion of the problem of subset relationships among
languages starts with Gold’s (1967) seminal paper and is
discussed in Berwick (1985) and Wexler &amp; Manzini (1987).
Detailed accounts of the types of local maxima that the learner
might encounter in a domain similar to the one we employ are
given in Frank &amp; Kapur (1996), Gibson &amp; Wexler (1994), and
Niyogi &amp; Berwick (1996).
</footnote>
<subsectionHeader confidence="0.996473">
3.3 The Learners&apos; strategies
</subsectionHeader>
<bodyText confidence="0.998693">
In all cases the learner is error-driven: if Gcurr can
parse the current input pattern, retain it.6
The following refers to what the learner does
when Gcurr fails on the current input.
</bodyText>
<listItem confidence="0.9978086">
• Error-driven, blind-guess (EDBG): adopt any
grammar from the domain chosen at random –
not psychologically plausible, it serves as our
baseline.
• TLA (Gibson &amp; Wexler, 1994): change any one
parameter value of those that make up Gcurr.
Call this new grammar Gnew. If Gnew can parse
the current input, adopt it. Otherwise, retain
Gcurr.
• Non-Greedy TLA (Niyogi &amp; Berwick, 1996):
change any one parameter value of those that
make up Gcurr. Adopt it. (I.e. there is no testing
of the new grammar against the current input).
• Non-SVC TLA (Niyogi &amp; Berwick, 1996): try
any grammar in the domain. Adopt it only in the
event that it can parse the current input.
• Guessing STL (Fodor, 1998a): Perform a
structural parse of the current input. If a choice
point is encountered, chose an alternative based
on one of the following and then set parameter
values based on the final parse tree:
• STL Random Choice (RC) – randomly pick a
parsing alternative.
• Minimal Chain (MC) – pick the choice that
obeys the Minimal Chain Principle (De Vin-
cenzi, 1991), i.e., avoid positing movement
transformations if possible.
• Local Attachment/Late Closure (LAC) –pick
the choice that attaches the new word to the
current constituent (Frazier, 1978).
</listItem>
<bodyText confidence="0.9999025">
The EDBG learner is our first learner of inter-
est. It is easy to show that the average and 99%
scores increase exponentially in the number of
parameters and syntactic research has proposed
more than 100 (e.g. Cinque, 1999). Clearly, human
learners do not employ a strategy that performs as
poorly as this. Results will serve as a baseline to
compare against other models.
</bodyText>
<footnote confidence="0.973885">
6 We intend for a “can-parse/can’t-parse outcome” to be
equivalent to the result from a language membership test. If
the current input sentence is one of the set of sentences
generated by Gcurr, can-parse is engendered; if not, can’t-
parse.
</footnote>
<table confidence="0.1900865">
99% Average
EDBG 16,663 3,589
</table>
<tableCaption confidence="0.974515">
Table 2: EDBG, # of sentences consumed
</tableCaption>
<bodyText confidence="0.99955355">
The TLA: The TLA incorporates two search
heuristics: the Single Value Constraint (SVC) and
Greediness. In the event that Gcurr cannot parse the
current input sentence s, the TLA attempts a
second parse with a randomly chosen new gram-
mar, Gnew, that differs from Gcurr by exactly one
parameter value (SVC). If Gnew can parse s, Gnew
becomes the new Gcurr otherwise Gnew is rejected as
a hypothesis (Greediness). Following Berwick and
Niyogi (1996), we also ran simulations on two
variants of the TLA – one with the Greediness
heuristic but without the SVC (TLA minus SVC,
TLA–SVC) and one with the SVC but without
Greediness (TLA minus Greediness, TLA–Greed).
The TLA has become a seminal model and has
been extensively studied (cf. Bertolo, 2001 and
references therein; Berwick &amp; Niyogi, 1996; Frank
&amp; Kapur, 1996; Sakas, 2000; among others). The
results from the TLA variants operating in the
LDD are presented in Table 3.
</bodyText>
<table confidence="0.9902865">
99% Average
TLA-SVC 67,896 11,273
TLA-Greed 19,181 4,110
TLA 16,990 961
</table>
<tableCaption confidence="0.999892">
Table 3: TLA variants, # of sentences consumed
</tableCaption>
<bodyText confidence="0.99935825">
Particularly interesting is that contrary to results
reported by Niyogi &amp; Berwick (1996) and Sakas &amp;
Nishimoto (2002), the SVC and Greediness
constraints do help the learner achieve the target in
the LDD. The previous research was based on
simulations run on much smaller 9 and 16 lan-
guage domains (see Table 1). It would seem that
the local hill-climbing search strategies employed
by the TLA do improve learning efficiency in the
LDD. However, even at best, the TLA performs
less well than the blind guess learner. We conjec-
ture that this fact probably rules out the TLA as a
viable model of human language acquisition.
The STL: Fodor’s Structural Triggers Learner
(STL) makes greater use of the parser than the
TLA. A key feature of the model is that parameter
values are not simply the standardly presumed 0 or
1, but rather bits of tree structure or treelets. Thus,
a grammar, in the STL sense, is a collection of
treelets rather than a collection of 1&apos;s and 0&apos;s. The
STL is error-driven. If Gcurr cannot license s, new
treelets will be utilized to achieve a successful
parse.7 Treelets are applied in the same way as any
“normal” grammar rule, so no unusual parsing
activity is necessary. The STL hypothesizes
grammars by adding parameter value treelets to
Gcurr when they contribute to a successful parse.
The basic algorithm for all STL variants is:
</bodyText>
<listItem confidence="0.992936428571428">
1. If Gcurr can parse the current input sentence,
retain the treelets that make up Gcurr.
2. Otherwise, parse the sentence making use of
any or all parametric treelets available and
adopt those treelets that contribute to a suc-
cessful parse. We call this parametric de-
coding.
</listItem>
<bodyText confidence="0.999510653846154">
Because the STL can decode inputs into their
parametric signatures, it stands apart from other
acquisition models in that it can detect when an
input sentence is parametrically ambiguous.
During a parse of s, if more than one treelet could
be used by the parser (i.e., a choice point is
encountered), then s is parametrically ambiguous.
The TLA variants do not have this capacity
because they rely only on a can-parse/can’t-parse
outcome and do not have access to the on-line
operations of the parser. Originally, the ability to
detect ambiguity was employed in two variations
of the STL: the strong STL (SSTL) and the weak
STL.
The SSTL executes a full parallel parse of each
input sentence and adopts only those treelets
(parameter values) that are present in all the
generated parse trees. This would seem to make
the SSTL an extremely powerful, albeit psycho-
logically implausible, learner.8 However, this is not
necessarily the case. The SSTL needs some
unambiguity to be present in the structures derived
from the sentences of the target language. For
example, there may not be a single input generated
by Gtarg that when parsed yields an unambiguous
treelet for a particular parameter.
</bodyText>
<footnote confidence="0.9858185">
7 In addition to the treelets, UG principles are also available
for parsing, as they are in the other models discussed above.
8 It is important to note that Fodor (1998a) does not put forth
the strong STL as a psychologically plausible model. Rather, it
is intended to demonstrate the potential effectiveness of
parametric decoding.
</footnote>
<bodyText confidence="0.99986152">
Unlike the SSTL, the weak STL executes a
psychologically plausible left-to-right serial
(deterministic) parse. One variant of the weak
STL, the waiting STL (WSTL), deals with ambigu-
ous inputs abiding by the heuristic: Don’t learn
from sentences that contain a choice point. These
sentences are simply discarded for the purposes of
learning. This is not to imply that children do not
parse ambiguous sentences they hear, but only that
they set no parameters if the current evidence is
ambiguous.
As with the TLA, these STL variants have been
studied from a mathematical perspective (Bertolo
et al., 1997a; Sakas, 2000). Mathematical analyses
point to the fact that the strong and weak STL are
extremely efficient learners in conducive domains
with some unambiguous inputs but may become
paralyzed in domains with high degrees of ambigu-
ity. These mathematical analyses among other
considerations spurred a new class of weak STL
variants which we informally call the guessing STL
family.
The basic idea behind the guessing STL models
is that there is some information available even in
sentences that are ambiguous, and some strategy
that can exploit that information. We incorporate
three different heuristics into the original STL
paradigm, the RC, MC and LAC heuristics
described above.
Although the MC and LAC heuristics are not
stochastic, we regard them as “guessing” heuristics
because, unlike the WSTL, a learner cannot be
certain that the parametric treelets obtained from a
parse guided by MC and LAC are correct for the
target. These heuristics are based on well-
established human parsing strategies. Interestingly,
the difference in performance between the three
variants is slight. Although we have just begun to
look at this data in detail, one reason may be that
the typical types of problems these parsing
strategies address are not included in the LDD (e.g.
relative clause attachment ambiguity). Still, the
STL variants perform the most efficiently of the
strategies presented in this small study (approxi-
mately a 100-fold improvement over the TLA).
Certainly this is due to the STL&apos;s ability to perform
parametric decoding. See Fodor (1998b) and Sakas
&amp; Fodor (2001) for detailed discussion about the
power of decoding when applied to the acquisition
process.
</bodyText>
<table confidence="0.7122362">
Guessing 99% Average
STL
RC 1,486 166
MC 1,412 160
LAC 1,923 197
</table>
<tableCaption confidence="0.991763">
Table 4: guessing STL family, # of sen-
tences consumed
</tableCaption>
<sectionHeader confidence="0.987905" genericHeader="conclusions">
4 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999985194444445">
The thrust of our current research is directed at
collecting data for a comprehensive, comparative
study of psycho-computational models of syntax
acquisition. To support this endeavor, we have
developed the Language Domain Database – a
publicly available test-bed for studying acquisition
models from diverse paradigms.
Mathematical analysis has shown that learners
are extremely sensitive to various distributions in
the input stream (Niyogi &amp; Berwick, 1996; Sakas,
2000, 2003). Approaches that thrive in one domain
may dramatically flounder in others. So, whether a
particular computational model is successful as a
model of natural language acquisition is ultimately
an empirical issue and depends on the exact
conditions under which the model performs well
and the extent to which those favorable conditions
are in line with the facts of human language. The
LDD is a useful tool that can be used within such
an empirical research program.
Future work: Though the LDD has been vali-
dated against CHILDES data in certain respects,
we intend to extend this work by adding distribu-
tions to the LDD that correspond to actual distribu-
tions of child-directed speech. For example, what
percentage of utterances, in child-directed Japa-
nese, contain pro-drop? object-drop? How often in
English does the pattern: S[+WH] aux Verb O1
occur and at what periods of a child&apos;s develop-
ment? We believe that these distributions will shed
some light on many of the complex subtleties
involved in ambiguity disambiguation and the role
of nondeterminism and statistics in the language
acquisition process. This is proving to be a
formidable, yet surmountable task; one that we are
just beginning to tackle.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.48321425">
This paper reports work done in part with other
members of CUNY-CoLAG (CUNY&apos;s Computa-
tional Language Acquisition Group) including
Janet Dean Fodor, Virginia Teller, Eiji Nishimoto,
Aaron Harnley, Yana Melnikova, Erika Troseth,
Carrie Crowther, Atsu Inoue, Yukiko Koizumi,
Lisa Resig-Ferrazzano, and Tanya Viger. Also
thanks to Charles Yang for much useful discussion,
and valuable comments from the anonymous
reviewers. This research was funded by PSC-
CUNY Grant #63387-00-32 and CUNY Collabora-
tive Grant #92902-00-07.
</bodyText>
<sectionHeader confidence="0.974081" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999857825581396">
Bertolo, S. (Ed.) (2001). Language Acquisition and
Learnability. Cambridge, UK: Cambridge University
Press.
Bertolo, S., Broihier, K., Gibson, E., &amp; Wexler, K.
(1997a). Characterizing learnability conditions for
cue-based learners in parametric language systems.
Proceedings of the Fifth Meeting on Mathematics of
Language.
Bertolo, S., Broihier, K., Gibson, E., and Wexler, K.
(1997b) Cue-based learners in parametric language
systems: Application of general results to a recently
proposed learning algorithm based on unambiguous
&apos;superparsing&apos;. In M. G. Shafto and P. Langley (eds.)
the Cognitive Science Society, Mahwah NJ: Law-
rence Erlbaum Associates.
Berwick, R. C., &amp; Niyogi, P. (1996). Learning from
triggers. Linguistic Inquiry, 27 (4), 605-622.
Briscoe, T. (2000). Grammatical acquisition: Inductive
bias and coevolution of language and the language
acquisition device. Language, 76 (2), 245-296.
Chomsky, N. (1981) Lectures on Government and
Binding, Dordrecht: Foris Publications.
Chomsky, N. (1995) The Minimalist Program. Cam-
bridge MA: MIT Press.
Cinque, G. (1999) Adverbs and Functional Heads.
Oxford Oxford, UK:University Press, Oxford, UK.
Fodor, J. D. (1998a) Unambiguous triggers, Linguistic
Inquiry 29.1, 1-36.
Fodor, J. D. (1998b) Parsing to learn. Journal of
Psycholinguistic Research 27.3, 339-374.
Fodor, J.D., Melnikova, Y. &amp; Troseth, E. (2002) A
structurally defined language domain for testing
syntax acquisition models. Technical Report. CUNY
Graduate Center.
Gibson, E. and Wexler, K. (1994) Triggers. Linguistic
Inquiry 25, 407-454.
Gold, E. M. (1967) Language identification in the limit.
Information and Control 10, 447-474.
Hyams, N. (1986) Language Acquisition and the Theory
of Parameters. Dordrecht: Reidel.
Jain, S., E. Martin, D. Osherson, J. Royer, and A.
Sharma. (1991) Systems That Learn. 2nd ed. Cam-
bridge, MA: MIT Press.
Kayne, R. S. (1994) The Antisymmetry of Syntax.
Cambridge MA: MIT Press.
Kohl, K.T. (1999) An Analysis of Finite Parameter
Learning in Linguistic Spaces. Master’s Thesis, MIT.
MacWhinney, B. (1995) The CHILDES Project: Tools
for Analyzing Talk. (2nd ed.) Hillsdale, NJ: Lawrence
Erlbaum Associates.
Niyogi, P (1998) The Informational Complexity of
Learning: Perspectives on Neural Networks and
Generative Grammar Dordrecht: Kluwer Academic.
Pinker, S. (1979) Formal models of language learning,
Cognition 7, 217-283.
Sagae, K., Lavie, A., MacWhinney, B. (2001) Parsing
the CHILDES database: Methodology and lessons
learned. In Proceedings of the Seventh International
Workshop in Parsing Technologies. Beijing, China.
Sakas, W.G. (in prep) Grammar/Language smoothness
and the need (or not) of syntactic parameters. Hunter
College and The Graduate Center, City University of
New York.
Sakas, W.G. (2000) Ambiguity and the Computational
Feasibility of Syntax Acquisition, Doctoral Disserta-
tion, City University of New York.
Sakas, W.G. and Fodor, J.D. (2001). The Structural
Triggers Learner. In S. Bertolo (ed.) Language Ac-
quisition and Learnability. Cambridge, UK: Cam-
bridge University Press.
Sakas, W.G. and Nishimoto, E. (2002) Search, Structure
or Statistics? A Comparative Study of Memoryless
Heuristics for Syntax Acquisition, Proceedings of the
24th Annual Conference of the Cognitive Science
Society. Hillsdale NJ: Lawrence Erlbaum Associ-
ates,
Siskind, J.M &amp; Dimitriadis, A., [Online 5/20/2003]
Documentation for qtree, a LaTex tree package
http://www.ling.upenn.edu/advice/latex/qtree/
Villavicencio, A. (2000) The use of default unification
in a system of lexical types. Paper presented at the
Workshop on Linguistic Theory and Grammar Im-
plementation, Birmingham,UK.
Wexler, K. and Culicover, P. (1980) Formal Principles
of Language Acquisition. Cambridge MA: MIT
Press.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.517374">
<title confidence="0.999169">A Word-Order Database for Testing Computational Models of Language Acquisition</title>
<author confidence="0.998879">William Gregory Sakas</author>
<affiliation confidence="0.879228">Department of Computer Science PhD Programs in Linguistics and Computer Science Hunter College and The Graduate Center City University of New York</affiliation>
<email confidence="0.999881">sakas@hunter.cuny.edu</email>
<abstract confidence="0.999619285714286">An investment of effort over the last two years has begun to produce a wealth of data concerning computational psycholinguistic models of syntax acquisition. The data is generated by running simulations on a recently completed database of word order patterns from over 3,000 abstract languages. This article presents the design of the database which contains sentence patterns, grammars and derivations that can be used to test acquisition models from widely divergent paradigms. The domain is generated from grammars that are linguistically motivated by current syntactic theory and the sentence patterns have been validated as psychologically/developmentally plausible by checking their frequency of occurrence in corpora of child-directed speech. A small case-study simulation is also presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bertolo</author>
</authors>
<title>Language Acquisition and Learnability.</title>
<date>2001</date>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge, UK:</location>
<contexts>
<context position="1721" citStr="Bertolo, 2001" startWordPosition="248" endWordPosition="249">ss by which a child acquires the grammar of his or her native language is one of the most beguiling open problems of cognitive science. There has been recent interest in computer simulation of the acquisition process and the interrelationship between such models and linguistic and psycholinguistic theory. The hope is that through computational study, certain bounds can be established which may be brought to bear on pivotal issues in developmental psycholinguistics. Simulation research is a significant departure from standard learnability models that provide results through formal proof (e.g., Bertolo, 2001; Gold, 1967; Jain et al., 1999; Niyogi, 1998; Niyogi &amp; Berwick, 1996; Pinker, 1979; Wexler &amp; Culicover, 1980, among many others). Although research in learnability theory is valuable and ongoing, there are several disadvantages to formal modeling of language acquisition: • Certain proofs may involve impractically many steps for large language domains (e.g. those involving Markov methods). • Certain paradigms are too complex to readily lend themselves to deductive study (e.g. connectionist models).1 • Simulations provide data on intermediate stages whereas formal proofs typically prove whether</context>
<context position="21746" citStr="Bertolo, 2001" startWordPosition="3397" endWordPosition="3398">t parse the current input sentence s, the TLA attempts a second parse with a randomly chosen new grammar, Gnew, that differs from Gcurr by exactly one parameter value (SVC). If Gnew can parse s, Gnew becomes the new Gcurr otherwise Gnew is rejected as a hypothesis (Greediness). Following Berwick and Niyogi (1996), we also ran simulations on two variants of the TLA – one with the Greediness heuristic but without the SVC (TLA minus SVC, TLA–SVC) and one with the SVC but without Greediness (TLA minus Greediness, TLA–Greed). The TLA has become a seminal model and has been extensively studied (cf. Bertolo, 2001 and references therein; Berwick &amp; Niyogi, 1996; Frank &amp; Kapur, 1996; Sakas, 2000; among others). The results from the TLA variants operating in the LDD are presented in Table 3. 99% Average TLA-SVC 67,896 11,273 TLA-Greed 19,181 4,110 TLA 16,990 961 Table 3: TLA variants, # of sentences consumed Particularly interesting is that contrary to results reported by Niyogi &amp; Berwick (1996) and Sakas &amp; Nishimoto (2002), the SVC and Greediness constraints do help the learner achieve the target in the LDD. The previous research was based on simulations run on much smaller 9 and 16 language domains (see</context>
</contexts>
<marker>Bertolo, 2001</marker>
<rawString>Bertolo, S. (Ed.) (2001). Language Acquisition and Learnability. Cambridge, UK: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bertolo</author>
<author>K Broihier</author>
<author>E Gibson</author>
<author>K Wexler</author>
</authors>
<title>Characterizing learnability conditions for cue-based learners in parametric language systems.</title>
<date>1997</date>
<booktitle>Proceedings of the Fifth Meeting on Mathematics of Language.</booktitle>
<contexts>
<context position="25805" citStr="Bertolo et al., 1997" startWordPosition="4073" endWordPosition="4076">ametric decoding. Unlike the SSTL, the weak STL executes a psychologically plausible left-to-right serial (deterministic) parse. One variant of the weak STL, the waiting STL (WSTL), deals with ambiguous inputs abiding by the heuristic: Don’t learn from sentences that contain a choice point. These sentences are simply discarded for the purposes of learning. This is not to imply that children do not parse ambiguous sentences they hear, but only that they set no parameters if the current evidence is ambiguous. As with the TLA, these STL variants have been studied from a mathematical perspective (Bertolo et al., 1997a; Sakas, 2000). Mathematical analyses point to the fact that the strong and weak STL are extremely efficient learners in conducive domains with some unambiguous inputs but may become paralyzed in domains with high degrees of ambiguity. These mathematical analyses among other considerations spurred a new class of weak STL variants which we informally call the guessing STL family. The basic idea behind the guessing STL models is that there is some information available even in sentences that are ambiguous, and some strategy that can exploit that information. We incorporate three different heuri</context>
</contexts>
<marker>Bertolo, Broihier, Gibson, Wexler, 1997</marker>
<rawString>Bertolo, S., Broihier, K., Gibson, E., &amp; Wexler, K. (1997a). Characterizing learnability conditions for cue-based learners in parametric language systems. Proceedings of the Fifth Meeting on Mathematics of Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bertolo</author>
<author>K Broihier</author>
<author>E Gibson</author>
<author>K Wexler</author>
</authors>
<title>Cue-based learners in parametric language systems: Application of general results to a recently proposed learning algorithm based on unambiguous &apos;superparsing&apos;.</title>
<date>1997</date>
<editor>In M. G. Shafto and P. Langley (eds.)</editor>
<contexts>
<context position="25805" citStr="Bertolo et al., 1997" startWordPosition="4073" endWordPosition="4076">ametric decoding. Unlike the SSTL, the weak STL executes a psychologically plausible left-to-right serial (deterministic) parse. One variant of the weak STL, the waiting STL (WSTL), deals with ambiguous inputs abiding by the heuristic: Don’t learn from sentences that contain a choice point. These sentences are simply discarded for the purposes of learning. This is not to imply that children do not parse ambiguous sentences they hear, but only that they set no parameters if the current evidence is ambiguous. As with the TLA, these STL variants have been studied from a mathematical perspective (Bertolo et al., 1997a; Sakas, 2000). Mathematical analyses point to the fact that the strong and weak STL are extremely efficient learners in conducive domains with some unambiguous inputs but may become paralyzed in domains with high degrees of ambiguity. These mathematical analyses among other considerations spurred a new class of weak STL variants which we informally call the guessing STL family. The basic idea behind the guessing STL models is that there is some information available even in sentences that are ambiguous, and some strategy that can exploit that information. We incorporate three different heuri</context>
</contexts>
<marker>Bertolo, Broihier, Gibson, Wexler, 1997</marker>
<rawString>Bertolo, S., Broihier, K., Gibson, E., and Wexler, K. (1997b) Cue-based learners in parametric language systems: Application of general results to a recently proposed learning algorithm based on unambiguous &apos;superparsing&apos;. In M. G. Shafto and P. Langley (eds.) the Cognitive Science Society, Mahwah NJ: Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Berwick</author>
<author>P Niyogi</author>
</authors>
<title>Learning from triggers.</title>
<date>1996</date>
<journal>Linguistic Inquiry,</journal>
<volume>27</volume>
<issue>4</issue>
<pages>605--622</pages>
<contexts>
<context position="21447" citStr="Berwick and Niyogi (1996)" startWordPosition="3344" endWordPosition="3347">nput sentence is one of the set of sentences generated by Gcurr, can-parse is engendered; if not, can’tparse. 99% Average EDBG 16,663 3,589 Table 2: EDBG, # of sentences consumed The TLA: The TLA incorporates two search heuristics: the Single Value Constraint (SVC) and Greediness. In the event that Gcurr cannot parse the current input sentence s, the TLA attempts a second parse with a randomly chosen new grammar, Gnew, that differs from Gcurr by exactly one parameter value (SVC). If Gnew can parse s, Gnew becomes the new Gcurr otherwise Gnew is rejected as a hypothesis (Greediness). Following Berwick and Niyogi (1996), we also ran simulations on two variants of the TLA – one with the Greediness heuristic but without the SVC (TLA minus SVC, TLA–SVC) and one with the SVC but without Greediness (TLA minus Greediness, TLA–Greed). The TLA has become a seminal model and has been extensively studied (cf. Bertolo, 2001 and references therein; Berwick &amp; Niyogi, 1996; Frank &amp; Kapur, 1996; Sakas, 2000; among others). The results from the TLA variants operating in the LDD are presented in Table 3. 99% Average TLA-SVC 67,896 11,273 TLA-Greed 19,181 4,110 TLA 16,990 961 Table 3: TLA variants, # of sentences consumed Par</context>
<context position="21793" citStr="Berwick &amp; Niyogi, 1996" startWordPosition="3402" endWordPosition="3405">the TLA attempts a second parse with a randomly chosen new grammar, Gnew, that differs from Gcurr by exactly one parameter value (SVC). If Gnew can parse s, Gnew becomes the new Gcurr otherwise Gnew is rejected as a hypothesis (Greediness). Following Berwick and Niyogi (1996), we also ran simulations on two variants of the TLA – one with the Greediness heuristic but without the SVC (TLA minus SVC, TLA–SVC) and one with the SVC but without Greediness (TLA minus Greediness, TLA–Greed). The TLA has become a seminal model and has been extensively studied (cf. Bertolo, 2001 and references therein; Berwick &amp; Niyogi, 1996; Frank &amp; Kapur, 1996; Sakas, 2000; among others). The results from the TLA variants operating in the LDD are presented in Table 3. 99% Average TLA-SVC 67,896 11,273 TLA-Greed 19,181 4,110 TLA 16,990 961 Table 3: TLA variants, # of sentences consumed Particularly interesting is that contrary to results reported by Niyogi &amp; Berwick (1996) and Sakas &amp; Nishimoto (2002), the SVC and Greediness constraints do help the learner achieve the target in the LDD. The previous research was based on simulations run on much smaller 9 and 16 language domains (see Table 1). It would seem that the local hill-cl</context>
</contexts>
<marker>Berwick, Niyogi, 1996</marker>
<rawString>Berwick, R. C., &amp; Niyogi, P. (1996). Learning from triggers. Linguistic Inquiry, 27 (4), 605-622.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
</authors>
<title>Grammatical acquisition: Inductive bias and coevolution of language and the language acquisition device.</title>
<date>2000</date>
<journal>Language,</journal>
<volume>76</volume>
<issue>2</issue>
<pages>245--296</pages>
<contexts>
<context position="11400" citStr="Briscoe 2000" startWordPosition="1730" endWordPosition="1731">oretical solutions are needed; iv) have been a focus of linguistic and/or psycholinguistic research; v) have a syntactic analysis that is broadly agreed on. As a result the following have been included: • By criteria (i) and (ii): negation, nondeclarative sentences (questions, imperatives). • By criterion (iv): null subject parameter (Hyams 1986 and since). • By criterion (iv): affix-hopping (though not widespread in natural languages). • By criterion (v): no scrambling yet. There are several phenomena that the LDD does not yet include: • No verb subcategorization. • No interface with LF (cf. Briscoe 2000; Villavicencio 2000). • No discourse contexts to license sentence fragments (e.g., DP or PP fragments). • No XP-internal structure yet (except PP = P + O3, with piping or stranding). • No Linear Correspondence Axiom (Kayne 1994). • No feature checking as implementation of movement parameters (Chomsky 1995). # # Tree Language parame lan- struc- properties ters guages ture? Gibson &amp; 3 8 Not fully Word order, V2 Wexler specified (1994) Bertolo et. 7 64 Yes G&amp;W + V-raising to al (1997b) distinct Agr, T; deg-2 Kohl (1999) 12 2,304 Partial Bertolo et al. based on (1997b) + Bertolo scrambling Sakas </context>
</contexts>
<marker>Briscoe, 2000</marker>
<rawString>Briscoe, T. (2000). Grammatical acquisition: Inductive bias and coevolution of language and the language acquisition device. Language, 76 (2), 245-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding,</booktitle>
<publisher>Foris Publications.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="6358" citStr="Chomsky, 1981" startWordPosition="943" endWordPosition="944"> 1-1/2 to 2-1/2 years; the period generally accepted by psycholinguists to be when children establish the correct word order of their native language. For example, although the LDD is currently limited to degree-0 (i.e. no embedding) and does not contain DP-internal structure, after examining by hand, several thousand sentences from corpora in the CHILDES database in five languages (English, German, Italian, Japanese and Russian), we found that approximately 85% are degree-0 and an approximate 10 out of 11 have no internal DP structure. Adopting the principles and parameters (P&amp;P) hypothesis (Chomsky, 1981) as the underlying framework, we implemented an application that generated patterns and derivations given the following points of variation between languages: 1. Affix Hopping 2. Comp Initial/Final 3. I to C Movement 4. Null Subject 5. Null Topic 6. Obligatory Topic 7. Object Final/Initial 8. Pied Piping 9. Question Inversion 10. Subject Initial/Final 11. Topic Marking 12. V to I Movement 13. Obligatory Wh movement The patterns have fully specified X-bar structure, and movement is implemented as HPSG local dependencies. Pattern production is generated topdown via rules applied at each subtree </context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, N. (1981) Lectures on Government and Binding, Dordrecht: Foris Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>The Minimalist Program.</title>
<date>1995</date>
<publisher>MIT Press.</publisher>
<location>Cambridge MA:</location>
<contexts>
<context position="11708" citStr="Chomsky 1995" startWordPosition="1778" endWordPosition="1779">(iv): null subject parameter (Hyams 1986 and since). • By criterion (iv): affix-hopping (though not widespread in natural languages). • By criterion (v): no scrambling yet. There are several phenomena that the LDD does not yet include: • No verb subcategorization. • No interface with LF (cf. Briscoe 2000; Villavicencio 2000). • No discourse contexts to license sentence fragments (e.g., DP or PP fragments). • No XP-internal structure yet (except PP = P + O3, with piping or stranding). • No Linear Correspondence Axiom (Kayne 1994). • No feature checking as implementation of movement parameters (Chomsky 1995). # # Tree Language parame lan- struc- properties ters guages ture? Gibson &amp; 3 8 Not fully Word order, V2 Wexler specified (1994) Bertolo et. 7 64 Yes G&amp;W + V-raising to al (1997b) distinct Agr, T; deg-2 Kohl (1999) 12 2,304 Partial Bertolo et al. based on (1997b) + Bertolo scrambling Sakas &amp; 4 16 Yes G&amp;W + null Nishimoto subject/topic (2002) LDD 13 3,072 Yes S&amp;N + wh-movt + imperatives +aux inversion, etc. Table 1: A history of abstract domains for wordorder acquisition modeling. The LDD on the web: The two primary purposes of the web-interface are to allow the user to interactively peruse th</context>
</contexts>
<marker>Chomsky, 1995</marker>
<rawString>Chomsky, N. (1995) The Minimalist Program. Cambridge MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Cinque</author>
</authors>
<title>Adverbs and Functional Heads.</title>
<date>1999</date>
<publisher>UK:University Press,</publisher>
<location>Oxford Oxford,</location>
<contexts>
<context position="20542" citStr="Cinque, 1999" startWordPosition="3195" endWordPosition="3196">eter values based on the final parse tree: • STL Random Choice (RC) – randomly pick a parsing alternative. • Minimal Chain (MC) – pick the choice that obeys the Minimal Chain Principle (De Vincenzi, 1991), i.e., avoid positing movement transformations if possible. • Local Attachment/Late Closure (LAC) –pick the choice that attaches the new word to the current constituent (Frazier, 1978). The EDBG learner is our first learner of interest. It is easy to show that the average and 99% scores increase exponentially in the number of parameters and syntactic research has proposed more than 100 (e.g. Cinque, 1999). Clearly, human learners do not employ a strategy that performs as poorly as this. Results will serve as a baseline to compare against other models. 6 We intend for a “can-parse/can’t-parse outcome” to be equivalent to the result from a language membership test. If the current input sentence is one of the set of sentences generated by Gcurr, can-parse is engendered; if not, can’tparse. 99% Average EDBG 16,663 3,589 Table 2: EDBG, # of sentences consumed The TLA: The TLA incorporates two search heuristics: the Single Value Constraint (SVC) and Greediness. In the event that Gcurr cannot parse t</context>
</contexts>
<marker>Cinque, 1999</marker>
<rawString>Cinque, G. (1999) Adverbs and Functional Heads. Oxford Oxford, UK:University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Fodor</author>
</authors>
<title>Unambiguous triggers,</title>
<date>1998</date>
<journal>Linguistic Inquiry</journal>
<volume>29</volume>
<pages>1--36</pages>
<contexts>
<context position="19773" citStr="Fodor, 1998" startWordPosition="3067" endWordPosition="3068">n at random – not psychologically plausible, it serves as our baseline. • TLA (Gibson &amp; Wexler, 1994): change any one parameter value of those that make up Gcurr. Call this new grammar Gnew. If Gnew can parse the current input, adopt it. Otherwise, retain Gcurr. • Non-Greedy TLA (Niyogi &amp; Berwick, 1996): change any one parameter value of those that make up Gcurr. Adopt it. (I.e. there is no testing of the new grammar against the current input). • Non-SVC TLA (Niyogi &amp; Berwick, 1996): try any grammar in the domain. Adopt it only in the event that it can parse the current input. • Guessing STL (Fodor, 1998a): Perform a structural parse of the current input. If a choice point is encountered, chose an alternative based on one of the following and then set parameter values based on the final parse tree: • STL Random Choice (RC) – randomly pick a parsing alternative. • Minimal Chain (MC) – pick the choice that obeys the Minimal Chain Principle (De Vincenzi, 1991), i.e., avoid positing movement transformations if possible. • Local Attachment/Late Closure (LAC) –pick the choice that attaches the new word to the current constituent (Frazier, 1978). The EDBG learner is our first learner of interest. It</context>
<context position="25038" citStr="Fodor (1998" startWordPosition="3954" endWordPosition="3955">e present in all the generated parse trees. This would seem to make the SSTL an extremely powerful, albeit psychologically implausible, learner.8 However, this is not necessarily the case. The SSTL needs some unambiguity to be present in the structures derived from the sentences of the target language. For example, there may not be a single input generated by Gtarg that when parsed yields an unambiguous treelet for a particular parameter. 7 In addition to the treelets, UG principles are also available for parsing, as they are in the other models discussed above. 8 It is important to note that Fodor (1998a) does not put forth the strong STL as a psychologically plausible model. Rather, it is intended to demonstrate the potential effectiveness of parametric decoding. Unlike the SSTL, the weak STL executes a psychologically plausible left-to-right serial (deterministic) parse. One variant of the weak STL, the waiting STL (WSTL), deals with ambiguous inputs abiding by the heuristic: Don’t learn from sentences that contain a choice point. These sentences are simply discarded for the purposes of learning. This is not to imply that children do not parse ambiguous sentences they hear, but only that t</context>
<context position="27357" citStr="Fodor (1998" startWordPosition="4320" endWordPosition="4321">euristics are based on wellestablished human parsing strategies. Interestingly, the difference in performance between the three variants is slight. Although we have just begun to look at this data in detail, one reason may be that the typical types of problems these parsing strategies address are not included in the LDD (e.g. relative clause attachment ambiguity). Still, the STL variants perform the most efficiently of the strategies presented in this small study (approximately a 100-fold improvement over the TLA). Certainly this is due to the STL&apos;s ability to perform parametric decoding. See Fodor (1998b) and Sakas &amp; Fodor (2001) for detailed discussion about the power of decoding when applied to the acquisition process. Guessing 99% Average STL RC 1,486 166 MC 1,412 160 LAC 1,923 197 Table 4: guessing STL family, # of sentences consumed 4 Conclusion and future work The thrust of our current research is directed at collecting data for a comprehensive, comparative study of psycho-computational models of syntax acquisition. To support this endeavor, we have developed the Language Domain Database – a publicly available test-bed for studying acquisition models from diverse paradigms. Mathematica</context>
</contexts>
<marker>Fodor, 1998</marker>
<rawString>Fodor, J. D. (1998a) Unambiguous triggers, Linguistic Inquiry 29.1, 1-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Fodor</author>
</authors>
<title>Parsing to learn.</title>
<date>1998</date>
<journal>Journal of Psycholinguistic Research</journal>
<volume>27</volume>
<pages>339--374</pages>
<contexts>
<context position="19773" citStr="Fodor, 1998" startWordPosition="3067" endWordPosition="3068">n at random – not psychologically plausible, it serves as our baseline. • TLA (Gibson &amp; Wexler, 1994): change any one parameter value of those that make up Gcurr. Call this new grammar Gnew. If Gnew can parse the current input, adopt it. Otherwise, retain Gcurr. • Non-Greedy TLA (Niyogi &amp; Berwick, 1996): change any one parameter value of those that make up Gcurr. Adopt it. (I.e. there is no testing of the new grammar against the current input). • Non-SVC TLA (Niyogi &amp; Berwick, 1996): try any grammar in the domain. Adopt it only in the event that it can parse the current input. • Guessing STL (Fodor, 1998a): Perform a structural parse of the current input. If a choice point is encountered, chose an alternative based on one of the following and then set parameter values based on the final parse tree: • STL Random Choice (RC) – randomly pick a parsing alternative. • Minimal Chain (MC) – pick the choice that obeys the Minimal Chain Principle (De Vincenzi, 1991), i.e., avoid positing movement transformations if possible. • Local Attachment/Late Closure (LAC) –pick the choice that attaches the new word to the current constituent (Frazier, 1978). The EDBG learner is our first learner of interest. It</context>
<context position="25038" citStr="Fodor (1998" startWordPosition="3954" endWordPosition="3955">e present in all the generated parse trees. This would seem to make the SSTL an extremely powerful, albeit psychologically implausible, learner.8 However, this is not necessarily the case. The SSTL needs some unambiguity to be present in the structures derived from the sentences of the target language. For example, there may not be a single input generated by Gtarg that when parsed yields an unambiguous treelet for a particular parameter. 7 In addition to the treelets, UG principles are also available for parsing, as they are in the other models discussed above. 8 It is important to note that Fodor (1998a) does not put forth the strong STL as a psychologically plausible model. Rather, it is intended to demonstrate the potential effectiveness of parametric decoding. Unlike the SSTL, the weak STL executes a psychologically plausible left-to-right serial (deterministic) parse. One variant of the weak STL, the waiting STL (WSTL), deals with ambiguous inputs abiding by the heuristic: Don’t learn from sentences that contain a choice point. These sentences are simply discarded for the purposes of learning. This is not to imply that children do not parse ambiguous sentences they hear, but only that t</context>
<context position="27357" citStr="Fodor (1998" startWordPosition="4320" endWordPosition="4321">euristics are based on wellestablished human parsing strategies. Interestingly, the difference in performance between the three variants is slight. Although we have just begun to look at this data in detail, one reason may be that the typical types of problems these parsing strategies address are not included in the LDD (e.g. relative clause attachment ambiguity). Still, the STL variants perform the most efficiently of the strategies presented in this small study (approximately a 100-fold improvement over the TLA). Certainly this is due to the STL&apos;s ability to perform parametric decoding. See Fodor (1998b) and Sakas &amp; Fodor (2001) for detailed discussion about the power of decoding when applied to the acquisition process. Guessing 99% Average STL RC 1,486 166 MC 1,412 160 LAC 1,923 197 Table 4: guessing STL family, # of sentences consumed 4 Conclusion and future work The thrust of our current research is directed at collecting data for a comprehensive, comparative study of psycho-computational models of syntax acquisition. To support this endeavor, we have developed the Language Domain Database – a publicly available test-bed for studying acquisition models from diverse paradigms. Mathematica</context>
</contexts>
<marker>Fodor, 1998</marker>
<rawString>Fodor, J. D. (1998b) Parsing to learn. Journal of Psycholinguistic Research 27.3, 339-374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Fodor</author>
<author>Y Melnikova</author>
<author>E Troseth</author>
</authors>
<title>A structurally defined language domain for testing syntax acquisition models.</title>
<date>2002</date>
<tech>Technical Report. CUNY</tech>
<institution>Graduate Center.</institution>
<marker>Fodor, Melnikova, Troseth, 2002</marker>
<rawString>Fodor, J.D., Melnikova, Y. &amp; Troseth, E. (2002) A structurally defined language domain for testing syntax acquisition models. Technical Report. CUNY Graduate Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
<author>K Wexler</author>
</authors>
<date>1994</date>
<journal>Triggers. Linguistic Inquiry</journal>
<volume>25</volume>
<pages>407--454</pages>
<contexts>
<context position="18842" citStr="Gibson &amp; Wexler (1994)" startWordPosition="2904" endWordPosition="2907">e and/or default values that preclude local maxima; hence, we set aside trials where a local maximum ensues. 4 The average and 99-percentile figures (16 and 72) in this section are easily derived from the fact that input consumption follows a hypergeometric distribution. 5 Discussion of the problem of subset relationships among languages starts with Gold’s (1967) seminal paper and is discussed in Berwick (1985) and Wexler &amp; Manzini (1987). Detailed accounts of the types of local maxima that the learner might encounter in a domain similar to the one we employ are given in Frank &amp; Kapur (1996), Gibson &amp; Wexler (1994), and Niyogi &amp; Berwick (1996). 3.3 The Learners&apos; strategies In all cases the learner is error-driven: if Gcurr can parse the current input pattern, retain it.6 The following refers to what the learner does when Gcurr fails on the current input. • Error-driven, blind-guess (EDBG): adopt any grammar from the domain chosen at random – not psychologically plausible, it serves as our baseline. • TLA (Gibson &amp; Wexler, 1994): change any one parameter value of those that make up Gcurr. Call this new grammar Gnew. If Gnew can parse the current input, adopt it. Otherwise, retain Gcurr. • Non-Greedy TLA </context>
</contexts>
<marker>Gibson, Wexler, 1994</marker>
<rawString>Gibson, E. and Wexler, K. (1994) Triggers. Linguistic Inquiry 25, 407-454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Gold</author>
</authors>
<title>Language identification in the limit.</title>
<date>1967</date>
<journal>Information and Control</journal>
<volume>10</volume>
<pages>447--474</pages>
<contexts>
<context position="1733" citStr="Gold, 1967" startWordPosition="250" endWordPosition="251">hild acquires the grammar of his or her native language is one of the most beguiling open problems of cognitive science. There has been recent interest in computer simulation of the acquisition process and the interrelationship between such models and linguistic and psycholinguistic theory. The hope is that through computational study, certain bounds can be established which may be brought to bear on pivotal issues in developmental psycholinguistics. Simulation research is a significant departure from standard learnability models that provide results through formal proof (e.g., Bertolo, 2001; Gold, 1967; Jain et al., 1999; Niyogi, 1998; Niyogi &amp; Berwick, 1996; Pinker, 1979; Wexler &amp; Culicover, 1980, among many others). Although research in learnability theory is valuable and ongoing, there are several disadvantages to formal modeling of language acquisition: • Certain proofs may involve impractically many steps for large language domains (e.g. those involving Markov methods). • Certain paradigms are too complex to readily lend themselves to deductive study (e.g. connectionist models).1 • Simulations provide data on intermediate stages whereas formal proofs typically prove whether a domain is</context>
</contexts>
<marker>Gold, 1967</marker>
<rawString>Gold, E. M. (1967) Language identification in the limit. Information and Control 10, 447-474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Hyams</author>
</authors>
<title>Language Acquisition and the Theory of Parameters.</title>
<date>1986</date>
<location>Dordrecht: Reidel.</location>
<contexts>
<context position="11135" citStr="Hyams 1986" startWordPosition="1687" endWordPosition="1688">se for retrieval during simulation runs, effectively sidestepping the need to build an online multilingual parser. ii) are frequently exemplified in speech directed to 2-year-olds; iii) pose potential learning problems (e.g. crosslanguage ambiguity) for which theoretical solutions are needed; iv) have been a focus of linguistic and/or psycholinguistic research; v) have a syntactic analysis that is broadly agreed on. As a result the following have been included: • By criteria (i) and (ii): negation, nondeclarative sentences (questions, imperatives). • By criterion (iv): null subject parameter (Hyams 1986 and since). • By criterion (iv): affix-hopping (though not widespread in natural languages). • By criterion (v): no scrambling yet. There are several phenomena that the LDD does not yet include: • No verb subcategorization. • No interface with LF (cf. Briscoe 2000; Villavicencio 2000). • No discourse contexts to license sentence fragments (e.g., DP or PP fragments). • No XP-internal structure yet (except PP = P + O3, with piping or stranding). • No Linear Correspondence Axiom (Kayne 1994). • No feature checking as implementation of movement parameters (Chomsky 1995). # # Tree Language parame </context>
</contexts>
<marker>Hyams, 1986</marker>
<rawString>Hyams, N. (1986) Language Acquisition and the Theory of Parameters. Dordrecht: Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jain</author>
<author>E Martin</author>
<author>D Osherson</author>
<author>J Royer</author>
<author>A Sharma</author>
</authors>
<date>1991</date>
<booktitle>Systems That Learn. 2nd ed.</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>Jain, Martin, Osherson, Royer, Sharma, 1991</marker>
<rawString>Jain, S., E. Martin, D. Osherson, J. Royer, and A. Sharma. (1991) Systems That Learn. 2nd ed. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Kayne</author>
</authors>
<title>The Antisymmetry of Syntax.</title>
<date>1994</date>
<publisher>MIT Press.</publisher>
<location>Cambridge MA:</location>
<contexts>
<context position="11629" citStr="Kayne 1994" startWordPosition="1767" endWordPosition="1768"> negation, nondeclarative sentences (questions, imperatives). • By criterion (iv): null subject parameter (Hyams 1986 and since). • By criterion (iv): affix-hopping (though not widespread in natural languages). • By criterion (v): no scrambling yet. There are several phenomena that the LDD does not yet include: • No verb subcategorization. • No interface with LF (cf. Briscoe 2000; Villavicencio 2000). • No discourse contexts to license sentence fragments (e.g., DP or PP fragments). • No XP-internal structure yet (except PP = P + O3, with piping or stranding). • No Linear Correspondence Axiom (Kayne 1994). • No feature checking as implementation of movement parameters (Chomsky 1995). # # Tree Language parame lan- struc- properties ters guages ture? Gibson &amp; 3 8 Not fully Word order, V2 Wexler specified (1994) Bertolo et. 7 64 Yes G&amp;W + V-raising to al (1997b) distinct Agr, T; deg-2 Kohl (1999) 12 2,304 Partial Bertolo et al. based on (1997b) + Bertolo scrambling Sakas &amp; 4 16 Yes G&amp;W + null Nishimoto subject/topic (2002) LDD 13 3,072 Yes S&amp;N + wh-movt + imperatives +aux inversion, etc. Table 1: A history of abstract domains for wordorder acquisition modeling. The LDD on the web: The two primary</context>
</contexts>
<marker>Kayne, 1994</marker>
<rawString>Kayne, R. S. (1994) The Antisymmetry of Syntax. Cambridge MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K T Kohl</author>
</authors>
<title>An Analysis of Finite Parameter Learning in Linguistic Spaces.</title>
<date>1999</date>
<tech>Master’s Thesis, MIT.</tech>
<contexts>
<context position="11923" citStr="Kohl (1999)" startWordPosition="1818" endWordPosition="1819">s not yet include: • No verb subcategorization. • No interface with LF (cf. Briscoe 2000; Villavicencio 2000). • No discourse contexts to license sentence fragments (e.g., DP or PP fragments). • No XP-internal structure yet (except PP = P + O3, with piping or stranding). • No Linear Correspondence Axiom (Kayne 1994). • No feature checking as implementation of movement parameters (Chomsky 1995). # # Tree Language parame lan- struc- properties ters guages ture? Gibson &amp; 3 8 Not fully Word order, V2 Wexler specified (1994) Bertolo et. 7 64 Yes G&amp;W + V-raising to al (1997b) distinct Agr, T; deg-2 Kohl (1999) 12 2,304 Partial Bertolo et al. based on (1997b) + Bertolo scrambling Sakas &amp; 4 16 Yes G&amp;W + null Nishimoto subject/topic (2002) LDD 13 3,072 Yes S&amp;N + wh-movt + imperatives +aux inversion, etc. Table 1: A history of abstract domains for wordorder acquisition modeling. The LDD on the web: The two primary purposes of the web-interface are to allow the user to interactively peruse the patterns and the derivations that the LDD contains and to download raw data for the user to work with locally. Users are asked to register before using the LDD online. The user ID is typically an email address, al</context>
</contexts>
<marker>Kohl, 1999</marker>
<rawString>Kohl, K.T. (1999) An Analysis of Finite Parameter Learning in Linguistic Spaces. Master’s Thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for Analyzing Talk. (2nd ed.) Hillsdale, NJ: Lawrence Erlbaum Associates.</title>
<date>1995</date>
<contexts>
<context position="5627" citStr="MacWhinney, 1995" startWordPosition="834" endWordPosition="835">, etc. An example pattern is S Aux V O1 which corresponds to the English sentence: The little girl can make a paper airplane. There are also tokens for topic and question markers for use when a grammar specifies overt topicalization or question marking. Declarative sentences, imperative sentences, negations and questions are represented within the LDD, as is prepositional movement/stranding (pied-piping), null subjects, null topics, topicalization and several types of movement. Although more work needs to be done, a first round study of actual child-directed sentences from the CHILDES corpus (MacWhinney, 1995) indicates that our patterns capture many sentential word orders that children typically encounter in the period from 1-1/2 to 2-1/2 years; the period generally accepted by psycholinguists to be when children establish the correct word order of their native language. For example, although the LDD is currently limited to degree-0 (i.e. no embedding) and does not contain DP-internal structure, after examining by hand, several thousand sentences from corpora in the CHILDES database in five languages (English, German, Italian, Japanese and Russian), we found that approximately 85% are degree-0 and</context>
</contexts>
<marker>MacWhinney, 1995</marker>
<rawString>MacWhinney, B. (1995) The CHILDES Project: Tools for Analyzing Talk. (2nd ed.) Hillsdale, NJ: Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Niyogi</author>
</authors>
<date>1998</date>
<booktitle>The Informational Complexity of Learning: Perspectives on Neural Networks and Generative Grammar</booktitle>
<publisher>Kluwer Academic.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="1766" citStr="Niyogi, 1998" startWordPosition="256" endWordPosition="257">s or her native language is one of the most beguiling open problems of cognitive science. There has been recent interest in computer simulation of the acquisition process and the interrelationship between such models and linguistic and psycholinguistic theory. The hope is that through computational study, certain bounds can be established which may be brought to bear on pivotal issues in developmental psycholinguistics. Simulation research is a significant departure from standard learnability models that provide results through formal proof (e.g., Bertolo, 2001; Gold, 1967; Jain et al., 1999; Niyogi, 1998; Niyogi &amp; Berwick, 1996; Pinker, 1979; Wexler &amp; Culicover, 1980, among many others). Although research in learnability theory is valuable and ongoing, there are several disadvantages to formal modeling of language acquisition: • Certain proofs may involve impractically many steps for large language domains (e.g. those involving Markov methods). • Certain paradigms are too complex to readily lend themselves to deductive study (e.g. connectionist models).1 • Simulations provide data on intermediate stages whereas formal proofs typically prove whether a domain is (or more often is not) learnable</context>
<context position="3398" citStr="Niyogi, 1998" startWordPosition="501" endWordPosition="502">l (e.g. a connectionist network or a principles &amp; parameters learner) handles a few grammatical features (e.g. long-distance agreement and/or topicalization) often, though not always, in a single language. So although there have been many successful studies that demonstrate how one algorithm or another is able to acquire some aspect of grammatical structure, there is little doubt that the question of what mechanism children actually employ during the acquisition process is still open. This paper reports the development of a large, multilingual database of sentence patterns, gram1 Although see Niyogi, 1998 for some insight. mars and derivations that may be used to test computational models of syntax acquisition from widely divergent paradigms. The domain is generated from grammars that are linguistically motivated by current syntactic theory and the sentence patterns have been validated as psychologically/developmentally plausible by checking their frequency of occurrence in corpora of childdirected speech. We report here the structure of the domain, its interface and a case-study that demonstrates how the domain has been used to test the feasibility of several different acquisition strategies.</context>
</contexts>
<marker>Niyogi, 1998</marker>
<rawString>Niyogi, P (1998) The Informational Complexity of Learning: Perspectives on Neural Networks and Generative Grammar Dordrecht: Kluwer Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pinker</author>
</authors>
<title>Formal models of language learning,</title>
<date>1979</date>
<journal>Cognition</journal>
<volume>7</volume>
<pages>217--283</pages>
<contexts>
<context position="1804" citStr="Pinker, 1979" startWordPosition="262" endWordPosition="263"> most beguiling open problems of cognitive science. There has been recent interest in computer simulation of the acquisition process and the interrelationship between such models and linguistic and psycholinguistic theory. The hope is that through computational study, certain bounds can be established which may be brought to bear on pivotal issues in developmental psycholinguistics. Simulation research is a significant departure from standard learnability models that provide results through formal proof (e.g., Bertolo, 2001; Gold, 1967; Jain et al., 1999; Niyogi, 1998; Niyogi &amp; Berwick, 1996; Pinker, 1979; Wexler &amp; Culicover, 1980, among many others). Although research in learnability theory is valuable and ongoing, there are several disadvantages to formal modeling of language acquisition: • Certain proofs may involve impractically many steps for large language domains (e.g. those involving Markov methods). • Certain paradigms are too complex to readily lend themselves to deductive study (e.g. connectionist models).1 • Simulations provide data on intermediate stages whereas formal proofs typically prove whether a domain is (or more often is not) learnable a priori to specific trials. • Proofs</context>
</contexts>
<marker>Pinker, 1979</marker>
<rawString>Pinker, S. (1979) Formal models of language learning, Cognition 7, 217-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>A Lavie</author>
<author>B MacWhinney</author>
</authors>
<title>Parsing the CHILDES database: Methodology and lessons learned.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventh International Workshop in Parsing Technologies.</booktitle>
<location>Beijing, China.</location>
<marker>Sagae, Lavie, MacWhinney, 2001</marker>
<rawString>Sagae, K., Lavie, A., MacWhinney, B. (2001) Parsing the CHILDES database: Methodology and lessons learned. In Proceedings of the Seventh International Workshop in Parsing Technologies. Beijing, China.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W G Sakas</author>
</authors>
<title>(in prep) Grammar/Language smoothness and the need (or not) of syntactic parameters. Hunter College and The Graduate Center,</title>
<institution>City University of New York.</institution>
<marker>Sakas, </marker>
<rawString>Sakas, W.G. (in prep) Grammar/Language smoothness and the need (or not) of syntactic parameters. Hunter College and The Graduate Center, City University of New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W G Sakas</author>
</authors>
<title>Ambiguity and the Computational Feasibility of Syntax Acquisition,</title>
<date>2000</date>
<institution>Doctoral Dissertation, City University of New York.</institution>
<contexts>
<context position="21827" citStr="Sakas, 2000" startWordPosition="3410" endWordPosition="3411">ly chosen new grammar, Gnew, that differs from Gcurr by exactly one parameter value (SVC). If Gnew can parse s, Gnew becomes the new Gcurr otherwise Gnew is rejected as a hypothesis (Greediness). Following Berwick and Niyogi (1996), we also ran simulations on two variants of the TLA – one with the Greediness heuristic but without the SVC (TLA minus SVC, TLA–SVC) and one with the SVC but without Greediness (TLA minus Greediness, TLA–Greed). The TLA has become a seminal model and has been extensively studied (cf. Bertolo, 2001 and references therein; Berwick &amp; Niyogi, 1996; Frank &amp; Kapur, 1996; Sakas, 2000; among others). The results from the TLA variants operating in the LDD are presented in Table 3. 99% Average TLA-SVC 67,896 11,273 TLA-Greed 19,181 4,110 TLA 16,990 961 Table 3: TLA variants, # of sentences consumed Particularly interesting is that contrary to results reported by Niyogi &amp; Berwick (1996) and Sakas &amp; Nishimoto (2002), the SVC and Greediness constraints do help the learner achieve the target in the LDD. The previous research was based on simulations run on much smaller 9 and 16 language domains (see Table 1). It would seem that the local hill-climbing search strategies employed </context>
<context position="25820" citStr="Sakas, 2000" startWordPosition="4077" endWordPosition="4078">e the SSTL, the weak STL executes a psychologically plausible left-to-right serial (deterministic) parse. One variant of the weak STL, the waiting STL (WSTL), deals with ambiguous inputs abiding by the heuristic: Don’t learn from sentences that contain a choice point. These sentences are simply discarded for the purposes of learning. This is not to imply that children do not parse ambiguous sentences they hear, but only that they set no parameters if the current evidence is ambiguous. As with the TLA, these STL variants have been studied from a mathematical perspective (Bertolo et al., 1997a; Sakas, 2000). Mathematical analyses point to the fact that the strong and weak STL are extremely efficient learners in conducive domains with some unambiguous inputs but may become paralyzed in domains with high degrees of ambiguity. These mathematical analyses among other considerations spurred a new class of weak STL variants which we informally call the guessing STL family. The basic idea behind the guessing STL models is that there is some information available even in sentences that are ambiguous, and some strategy that can exploit that information. We incorporate three different heuristics into the </context>
<context position="28097" citStr="Sakas, 2000" startWordPosition="4434" endWordPosition="4435"> 99% Average STL RC 1,486 166 MC 1,412 160 LAC 1,923 197 Table 4: guessing STL family, # of sentences consumed 4 Conclusion and future work The thrust of our current research is directed at collecting data for a comprehensive, comparative study of psycho-computational models of syntax acquisition. To support this endeavor, we have developed the Language Domain Database – a publicly available test-bed for studying acquisition models from diverse paradigms. Mathematical analysis has shown that learners are extremely sensitive to various distributions in the input stream (Niyogi &amp; Berwick, 1996; Sakas, 2000, 2003). Approaches that thrive in one domain may dramatically flounder in others. So, whether a particular computational model is successful as a model of natural language acquisition is ultimately an empirical issue and depends on the exact conditions under which the model performs well and the extent to which those favorable conditions are in line with the facts of human language. The LDD is a useful tool that can be used within such an empirical research program. Future work: Though the LDD has been validated against CHILDES data in certain respects, we intend to extend this work by adding</context>
</contexts>
<marker>Sakas, 2000</marker>
<rawString>Sakas, W.G. (2000) Ambiguity and the Computational Feasibility of Syntax Acquisition, Doctoral Dissertation, City University of New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W G Sakas</author>
<author>J D Fodor</author>
</authors>
<title>The Structural Triggers Learner.</title>
<date>2001</date>
<booktitle>Language Acquisition and Learnability.</booktitle>
<editor>In S. Bertolo (ed.)</editor>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge, UK:</location>
<contexts>
<context position="27384" citStr="Sakas &amp; Fodor (2001)" startWordPosition="4323" endWordPosition="4326">d on wellestablished human parsing strategies. Interestingly, the difference in performance between the three variants is slight. Although we have just begun to look at this data in detail, one reason may be that the typical types of problems these parsing strategies address are not included in the LDD (e.g. relative clause attachment ambiguity). Still, the STL variants perform the most efficiently of the strategies presented in this small study (approximately a 100-fold improvement over the TLA). Certainly this is due to the STL&apos;s ability to perform parametric decoding. See Fodor (1998b) and Sakas &amp; Fodor (2001) for detailed discussion about the power of decoding when applied to the acquisition process. Guessing 99% Average STL RC 1,486 166 MC 1,412 160 LAC 1,923 197 Table 4: guessing STL family, # of sentences consumed 4 Conclusion and future work The thrust of our current research is directed at collecting data for a comprehensive, comparative study of psycho-computational models of syntax acquisition. To support this endeavor, we have developed the Language Domain Database – a publicly available test-bed for studying acquisition models from diverse paradigms. Mathematical analysis has shown that l</context>
</contexts>
<marker>Sakas, Fodor, 2001</marker>
<rawString>Sakas, W.G. and Fodor, J.D. (2001). The Structural Triggers Learner. In S. Bertolo (ed.) Language Acquisition and Learnability. Cambridge, UK: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W G Sakas</author>
<author>E Nishimoto</author>
</authors>
<title>Search, Structure or Statistics? A Comparative Study of Memoryless Heuristics for Syntax Acquisition,</title>
<date>2002</date>
<booktitle>Proceedings of the 24th Annual Conference of the Cognitive Science Society. Hillsdale NJ: Lawrence Erlbaum Associates,</booktitle>
<contexts>
<context position="22161" citStr="Sakas &amp; Nishimoto (2002)" startWordPosition="3462" endWordPosition="3465">ut without the SVC (TLA minus SVC, TLA–SVC) and one with the SVC but without Greediness (TLA minus Greediness, TLA–Greed). The TLA has become a seminal model and has been extensively studied (cf. Bertolo, 2001 and references therein; Berwick &amp; Niyogi, 1996; Frank &amp; Kapur, 1996; Sakas, 2000; among others). The results from the TLA variants operating in the LDD are presented in Table 3. 99% Average TLA-SVC 67,896 11,273 TLA-Greed 19,181 4,110 TLA 16,990 961 Table 3: TLA variants, # of sentences consumed Particularly interesting is that contrary to results reported by Niyogi &amp; Berwick (1996) and Sakas &amp; Nishimoto (2002), the SVC and Greediness constraints do help the learner achieve the target in the LDD. The previous research was based on simulations run on much smaller 9 and 16 language domains (see Table 1). It would seem that the local hill-climbing search strategies employed by the TLA do improve learning efficiency in the LDD. However, even at best, the TLA performs less well than the blind guess learner. We conjecture that this fact probably rules out the TLA as a viable model of human language acquisition. The STL: Fodor’s Structural Triggers Learner (STL) makes greater use of the parser than the TLA</context>
</contexts>
<marker>Sakas, Nishimoto, 2002</marker>
<rawString>Sakas, W.G. and Nishimoto, E. (2002) Search, Structure or Statistics? A Comparative Study of Memoryless Heuristics for Syntax Acquisition, Proceedings of the 24th Annual Conference of the Cognitive Science Society. Hillsdale NJ: Lawrence Erlbaum Associates,</rawString>
</citation>
<citation valid="false">
<authors>
<author>J M Siskind</author>
<author>A Dimitriadis</author>
</authors>
<title>[Online 5/20/2003] Documentation for qtree, a LaTex tree package http://www.ling.upenn.edu/advice/latex/qtree/</title>
<marker>Siskind, Dimitriadis, </marker>
<rawString>Siskind, J.M &amp; Dimitriadis, A., [Online 5/20/2003] Documentation for qtree, a LaTex tree package http://www.ling.upenn.edu/advice/latex/qtree/</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Villavicencio</author>
</authors>
<title>The use of default unification in a system of lexical types.</title>
<date>2000</date>
<booktitle>Paper presented at the Workshop on Linguistic Theory and Grammar Implementation,</booktitle>
<location>Birmingham,UK.</location>
<contexts>
<context position="11421" citStr="Villavicencio 2000" startWordPosition="1732" endWordPosition="1733">ions are needed; iv) have been a focus of linguistic and/or psycholinguistic research; v) have a syntactic analysis that is broadly agreed on. As a result the following have been included: • By criteria (i) and (ii): negation, nondeclarative sentences (questions, imperatives). • By criterion (iv): null subject parameter (Hyams 1986 and since). • By criterion (iv): affix-hopping (though not widespread in natural languages). • By criterion (v): no scrambling yet. There are several phenomena that the LDD does not yet include: • No verb subcategorization. • No interface with LF (cf. Briscoe 2000; Villavicencio 2000). • No discourse contexts to license sentence fragments (e.g., DP or PP fragments). • No XP-internal structure yet (except PP = P + O3, with piping or stranding). • No Linear Correspondence Axiom (Kayne 1994). • No feature checking as implementation of movement parameters (Chomsky 1995). # # Tree Language parame lan- struc- properties ters guages ture? Gibson &amp; 3 8 Not fully Word order, V2 Wexler specified (1994) Bertolo et. 7 64 Yes G&amp;W + V-raising to al (1997b) distinct Agr, T; deg-2 Kohl (1999) 12 2,304 Partial Bertolo et al. based on (1997b) + Bertolo scrambling Sakas &amp; 4 16 Yes G&amp;W + null</context>
</contexts>
<marker>Villavicencio, 2000</marker>
<rawString>Villavicencio, A. (2000) The use of default unification in a system of lexical types. Paper presented at the Workshop on Linguistic Theory and Grammar Implementation, Birmingham,UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wexler</author>
<author>P Culicover</author>
</authors>
<title>Formal Principles of Language Acquisition.</title>
<date>1980</date>
<publisher>MIT Press.</publisher>
<location>Cambridge MA:</location>
<contexts>
<context position="1830" citStr="Wexler &amp; Culicover, 1980" startWordPosition="264" endWordPosition="268">g open problems of cognitive science. There has been recent interest in computer simulation of the acquisition process and the interrelationship between such models and linguistic and psycholinguistic theory. The hope is that through computational study, certain bounds can be established which may be brought to bear on pivotal issues in developmental psycholinguistics. Simulation research is a significant departure from standard learnability models that provide results through formal proof (e.g., Bertolo, 2001; Gold, 1967; Jain et al., 1999; Niyogi, 1998; Niyogi &amp; Berwick, 1996; Pinker, 1979; Wexler &amp; Culicover, 1980, among many others). Although research in learnability theory is valuable and ongoing, there are several disadvantages to formal modeling of language acquisition: • Certain proofs may involve impractically many steps for large language domains (e.g. those involving Markov methods). • Certain paradigms are too complex to readily lend themselves to deductive study (e.g. connectionist models).1 • Simulations provide data on intermediate stages whereas formal proofs typically prove whether a domain is (or more often is not) learnable a priori to specific trials. • Proofs generally require simplif</context>
</contexts>
<marker>Wexler, Culicover, 1980</marker>
<rawString>Wexler, K. and Culicover, P. (1980) Formal Principles of Language Acquisition. Cambridge MA: MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>