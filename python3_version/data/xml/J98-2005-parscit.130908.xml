<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.400404">
Squibs and Discussions
Estimation of Probabilistic Context-Free
Grammars1
</note>
<author confidence="0.997426">
Zhiyi Chi* Stuart Geman*
</author>
<affiliation confidence="0.996221">
Brown University Brown University
</affiliation>
<bodyText confidence="0.997927">
The assignment of probabilities to the productions of a context-free grammar may generate an
improper distribution: the probability of all finite parse trees is less than one. The condition
for proper assignment is rather subtle. Production probabilities can be estimated from parsed
or unparsed sentences, and the question arises as to whether or not an estimated system is
automatically proper. We show here that estimated production probabilities always yield proper
distributions.
</bodyText>
<sectionHeader confidence="0.990225" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9999452">
Context-free grammars (CFG&apos;s) are useful because of their relatively broad coverage
and because of the availability of efficient parsing algorithms. Furthermore, CFG&apos;s are
readily fit with a probability distribution (to make probabilistic CFG&apos;s—or PCFG&apos;s),
rendering them suitable for ambiguous languages through the maximum a posteriori
rule of choosing the most probable parse.
For each nonterminal symbol, a (normalized) probability is placed on the set of all
productions from that symbol. Unfortunately, this simple procedure runs into an un-
expected complication: the language generated by the grammar may have probability
less than one. The reason is that the derivation tree may have probability greater than
zero of never terminating—some mass can be lost to infinity. This phenomenon is well
known and well understood, and there are tests for &amp;quot;tightness&amp;quot; (by which we mean
total probability mass equal to one) involving a matrix derived from the expected
growth in numbers of symbols generated by the probabilistic rules (see for example
Booth and Thompson [1973], Grenander [1976], and Harris [1963]).
What if the production probabilities are estimated from data? Suppose, for ex-
ample, that we have a parsed corpus that we treat as a collection of (independent)
samples from a grammar. It is reasonable to hope that if the trees in the sample are fi-
nite, then an estimate of production probabilities based upon the sample will produce
a system that assigns probability zero to the set of infinite trees. For example, there is
a simple maximum-likelihood prescription for estimating the production probabilities
from a corpus of trees (see Section 2), resulting in a PCFG. Is it tight? If the corpus is
unparsed then there is an iterative approach to maximum-likelihood estimation (the
EM or Baum-Welsh algorithm—again, see Section 2) and the same question arises: do
we get actual probabilities or do the estimated PCFG&apos;s assign some mass to infinite
trees? We will show that in both cases the estimated probability is tight.&apos;
</bodyText>
<note confidence="0.935972">
* Division of Applied Mathematics, Brown University, Providence, RI 02912 USA
</note>
<footnote confidence="0.99305075">
1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recently
appeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (Sanchez and Bened( [1997]).
2 When estimating from art unparsed corpus, we shall assume a model without null or unit productions;
see Section 2.
</footnote>
<note confidence="0.365875">
Computational Linguistics Volume 24, Number 2
</note>
<bodyText confidence="0.996369857142857">
Wetherell (1980) has asked a similar question: a scheme (different from maximum
likelihood) is introduced for estimating production probabilities from an unparsed
corpus, and it is conjectured that the resulting system is tight. (Wetherell and others
use the designation &amp;quot;consistent&amp;quot; instead of &amp;quot;tight,&amp;quot; but in statistics, consistency refers
to the asymptotic correctness of an estimator.)
A trivial example is the CFG with one nonterminal and one terminal symbol, in
Chomsky normal form:
</bodyText>
<equation confidence="0.3728515">
A —&gt; AA
A a
</equation>
<bodyText confidence="0.954230166666667">
where a is the only terminal symbol. Assign probability p to the first production
(A AA) and q = 1 — p to the second (A a). Let Sh be the total probability of
all trees with depth less than or equal to h. For example, S2 = q corresponding to
A —&gt; a, and S3 = q + pq2 corresponding to {A —&gt; al U {A —&gt; AA, A a, A a}. In
general, Sh+1 = q (Condition on the first production: with probability q the tree
terminates and with probability p it produces two nonterminal symbols, each of which
must now terminate with depth less than or equal to h.) It is not hard to show that
Sh is nondecreasirtg and converges to min(1, pi), meaning that a proper probability is
obtained if and only if p &lt;
What if p is estimated from data? Given a set of finite parse trees wi, (.02, ,w,, the
maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the &amp;quot;relative
frequency&amp;quot; estimator
</bodyText>
<equation confidence="0.896132">
13 = Ein=i V (A —› AA; co,) + f (A ---+ a; w)]
</equation>
<bodyText confidence="0.706033">
where f(.; co) is the number of occurrences of the production &apos;1.&amp;quot; in the tree Le. The
sentence am, although ambiguous (there are multiple parses when m &gt; 2), always
involves m — 1 of the A —&gt; AA productions and m of the A a productions. Hence
f (A AA; co,) &lt; f (A —4 a; w) for each wi. Consequently:
</bodyText>
<equation confidence="0.9867615">
1
f (A AA; wi) &lt; V (A —&gt; AA; w1) + f (A a; co i)]
</equation>
<bodyText confidence="0.966158666666667">
for each (.0,, and fo &lt; 1. The maximum-likelihood probability is tight.
If only the yields (left-to-right sequence of terminals) Y(wi), Y(w2), , Y(con) are
available, the EM algorithm can be used to iteratively &amp;quot;climb&amp;quot; the likelihood surface
(see Section 2). In the simple example here, the estimator converges in one step and
is the same as if we had observed the entire parse tree for each w,. Thus, fr is again
less than and the distribution is again tight.
</bodyText>
<sectionHeader confidence="0.947617" genericHeader="categories and subject descriptors">
2. Maximum-Likelihood Estimation
</sectionHeader>
<bodyText confidence="0.999192">
More generally, let G = (V , T , R, S) denote a context-free grammar with finite variable
set V, start symbol S E V, finite terminal set T, and finite production (or rule) set R.
(We use R in place of the more typical P to avoid confusion with probabilities.) Each
production in R has the form A —&gt; a, where A E V and a E (Vu T)*. In the usual way,
probabilities are introduced through the productions: P : R --+ [0,1] such that VA E V:
</bodyText>
<equation confidence="0.9924985">
E p(A a) = 1. (1)
etE(VUT)•
s.t. (A—,t)ER
E7 if (A —&gt; AA; wi)
</equation>
<page confidence="0.969528">
300
</page>
<note confidence="0.726226">
Chi and Geman Probabilistic Context-Free Grammars
</note>
<bodyText confidence="0.99917175">
Given a set of finite parse trees wi, (.02, • • • ,w, drawn independently according to
the distribution imposed by p, we wish to estimate p.
In terms of the frequency function f , introduced in Section 1, the likelihood of the
data is
</bodyText>
<equation confidence="0.993118130434783">
L = L(p; w2, • • • , 4.4•41 )
= H H p(A a)f (11--&amp;quot;-1;w&apos;).
i=1 (A—n)ER
Recall the derivation of the maximum-likelihood estimator of p: The log of the likeli-
hood is:
E E Ef (A —+ a; wi) log p(A a).
AEV i=1
(A—.)ER
The function p: R [0, 1] subject to (1) that maximizes (2) satisfies:
6
6p(B )3) E E
AEV s
(A— a)ER
V (B —4 13) E R where {AA IAE v are Lagrange multipliers. Denote the maximum-likelihood
estimator by p:
0 V(B —0) E R
p(B --+
(
Since &gt; pp- &gt; 0) = 1
f (B (3)
i3(13 0) =
E E7=
(B—ta)ER if(B —&gt;
</equation>
<bodyText confidence="0.9501525">
The maximum-likelihood estimator is the natural, &amp;quot;relative frequency,&amp;quot; estimator.
Suppose B E V is unobserved among the parse trees cvl, w2, , wn. Then we can
assign p(B 0) arbitrarily, requiring only that (1) be respected. Evidently the likeli-
hood is unaffected by the particular assignment of p (B ,(3). Furthermore, it is not
hard to see that any such B has probability zero of arising in any derivation that is
based upon the maximum-likelihood probabilities3—hence the issue of tightness is
independent of this assignment.
We will show that if C2 is the set of all (finite) parse trees generated by G, and if p(w)
is the probability of cd E St under the maximum-likelihood production probabilities,
then P(12) 1.
</bodyText>
<footnote confidence="0.9874318">
3 Consider any sequence of productions that leads from S to B. If the parent (antecedent) of B arose in
the sample, then the last production has jj probability zero and hence the sequence has probability
zero. Otherwise, move &amp;quot;up&amp;quot; through the ancestors of B until finding the first variable in the S-to-B
sequence represented in the sample (certainly S is represented). Apply the same reasoning to the
production from that variable, and conclude that the given sequence has /3 probability zero.
</footnote>
<figure confidence="0.855410166666667">
(2)
0
/3 s.t.
(13 —.13)ER
301
Computational Linguistics Volume 24, Number 2
</figure>
<subsectionHeader confidence="0.954731">
2.1 The EM Algorithm
</subsectionHeader>
<bodyText confidence="0.99987375">
Usually the derivation trees are unobserved—the sample, or corpus, contains only
the yields Y(wi), Y(w2), , Y(w) (Y(w,) E T* for each 1 &lt; i &lt; n). The likelihood is
substantially more complex, since p(Y (o.))) is now a marginal probability; we need to
sum over the set of w E St that yield Y (w):
</bodyText>
<equation confidence="0.996240666666667">
P(Y(w)) = p(YG,0).
Lo&apos; ES)
Y(0/ )=Y(ca)
</equation>
<bodyText confidence="0.987504818181818">
In the case where only yields are observed, the treatment is complicated consider-
ably by the possibility of null productions (A —&gt; 0) and unit productions (A B E V).
If, however, the language of the grammar does not include the null string, then there is
an equivalent grammar (one with the same language) that has no null productions and
no unit productions (cf. Hoperoft &amp; Ullman [1979], Theorem 4.4). It is, then, perhaps
best to simplify the treatment by assuming that there are no null or unit productions.
Therefore, when the corpus consists of yields only, we shall assume a priori a model
free of null and unit productions, and study tightness for probabilities estimated under
such a model. Based upon the results of Stokke [1995] it is likely that this restriction
can be relaxed, but we have not pursued this.
Letting S/ y denote { w E ft Y (w) Y}, the likelihood of the corpus becomes
</bodyText>
<equation confidence="0.957422785714286">
H E H p (A —&gt;
i=1 wES-2Y(.,) a)ER
And the maximum-likelihood equation becomes
1 `n EwEllY0,,)f(B —4 13; W)11(A-)ER
AB +
p (B 0)
i1 E,„Esiy() no,,ER fro_
=z
=0
E7_1 Ei,[f (B —&gt; 0; w)lw E
0) =
s E =1 Egf (B ct; LOP E9Y(w,)1
(B) ER
(4)
</equation>
<bodyText confidence="0.855003333333333">
where Efr is expectation under fr and where &amp;quot;w E _Sly(,)&amp;quot; means &amp;quot;conditioned on
E
There is no hope for a closed form solution, but (4) does suggest an iteration
scheme, which, as it turns out, &amp;quot;climbs&amp;quot; the likelihood surface (though there are no
guarantees about approaching a global maximum): Let 1)0 be an arbitrary assignment
respecting (1). Define a sequence of probabilities, pn, by the iteration
</bodyText>
<equation confidence="0.6334195">
) =ELI Ep. [f (B 0; w) QY(Lei)1 (5)
E EL1 Efr„ [f(B —&gt; LOP E ClY(0,01
</equation>
<bodyText confidence="0.999835333333333">
The right-hand side is manageable, as long as we can manageably compute all possible
parses of a sentence (yield) Y(w). (More efficient approaches exist; see Baker [1979].)
This iteration procedure is an instance of the EM Algorithm. Baum [1972] first intro-
duced it for hidden Markov models (regular grammars) and Baker [1979] extended
it to the problem addressed here (estimation for context-free grammars). Dempster,
Laird, and Rubin [1977] put the idea into a much more general setting and coined the
</bodyText>
<page confidence="0.995437">
302
</page>
<note confidence="0.887387">
Chi and Geman Probabilistic Context-Free Grammars
</note>
<bodyText confidence="0.991618">
term EM for Expectation-Maximization. The right-hand side of (5) is computed us-
ing the expected frequencies under Pn; 15n1-1 is then the maximum-likelihood estimator,
treating the expected frequencies as though they were observed frequencies.
The issue of tightness comes up again. We will show that fr (Il) -= 1 for each n &gt; 0.
</bodyText>
<listItem confidence="0.424443">
3. Tightness of the Maximum-Likelihood Estimator
</listItem>
<bodyText confidence="0.999754142857143">
Given a context-free grammar G = (V ,T, R, S), let ft be the set of finite parse trees, let
p: R --&gt; [0,1] be a system of production probabilities satisfying (1), and let wi,w2, • • • , con
be a set (sample) of finite parse trees wk E ft For now, null and unit productions are
permitted. Finally, let fr be the maximum-likelihood estimator of p, as defined by (3).
(See also the remarks following [3] concerning variables unobserved M WI, w2, wn.)
More generally, fr will refer to the probability distribution on (possibly infinite) parse
trees induced by the maximum-likelihood estimator.
</bodyText>
<equation confidence="0.703051">
Theorem
fr(1l) = 1
Proof
</equation>
<bodyText confidence="0.999598666666667">
Let qA = P (derivation tree rooted with A fails to terminate). We will show that qs = 0
(i.e., derivation trees rooted with S always terminate).
For each A E V, let F (A; w) be the number of instances of A in w and let PA; w)
be the number of nonroot instances of A in w. Given a E (V U T)* , let nA(a) be the
number of instances of A in the string a, and, finally, let ai be the ith component of
the string a. For any A E V:
</bodyText>
<equation confidence="0.9546423">
qA = fr(uBEv u.t2)EER. ui st. «,=B{ai fails to terminate})
Ep(u s.t. Beck U s.t. cr,=B {a1 fails to terminate})
(A—ta)ER
BEV
E E p(A --&gt; ct)(ui 5.t. ai=B
{ai fails to terminate} IA —&gt; a)
BEV.(A!:1. B
,:t)E7
&lt;
i fr(A, a)nB(a)qB
</equation>
<table confidence="0.9407325">
BEV a s.t. BEa
(A—ta)ER
= EqB 1 E. s , BE. nB(a)E,Ptif (A --&gt;
BEV (A—.a)ER
= E qB E (A&apos;__,sat)ER al f (A --4 a; w)
BEV E&apos; (A—. L„ E a s.t•
Bea
1 (a)f (A --
)ER nB
EL, E —&gt; a; WO
(A—ta)ER
qA E F(A; wi) qB E7=1 E. s.t. BEa nB(a)f (A a; w)
=
(A---)ER
E
BEV ErLi F (A; wi)
E qB E E nB(a)f (A a; w)
REV i=i
</table>
<page confidence="0.966071">
303
</page>
<note confidence="0.2309935">
Computational Linguistics Volume 24, Number 2
Sum over A E V:
</note>
<equation confidence="0.516257428571429">
E qA EF (A; wi) EqBEE E nB (A —&gt; a; wi)
AEv i=1 BEV i=1 AEI/ BE
(A--.a)ER
= E qB E (B; wi)
BEV i=1
E q A ((A; w) _ F (A; wi)) 0
AEI/ i=1
</equation>
<bodyText confidence="0.599282875">
Clearly, for every i = 1,2, ..., n k (A; w,) = F (A; co,) whenever A 0 S and t(S; co,) &lt;
F(S; coi). Hence qs = 0, completing the proof of the theorem. 0
Now let pti be the system of probabilities produced by the nth iteration of the EM
Algorithm (5):
Corollary
If R contains no null productions and no unit productions, then frn (S2) = 1 VII &gt; 1.
Proof
Almost identical, except that we use (5) in place of (3) and end up with:
</bodyText>
<equation confidence="0.861734">
E (IA EEfr, 1[F(A;w) - F (A; (4 E ,? 0.- (6)
AEv t=1
</equation>
<bodyText confidence="0.99997175">
In the absence of unit productions and null productions, F (A; w) &lt; 2lco I (twice the
length of the string co). Hence the expectations in (6) are finite. Furthermore, F (A; w)
and F (A; co) satisfy the same conditions as before: F (A; w) = F(A; w) except when A = S.
in which case F.- (A; w) &lt; F (A; co). Again, we conclude that qs = 0.
</bodyText>
<sectionHeader confidence="0.937647" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993656875">
We are indebted to Mark Johnson for
encouraging us to look at this problem in
the first place, and for much good advice
along the way. This work was supported by
the Army Research Office
(DAAL03-92-G-0115), the National Science
Foundation (DMS-9217655), and the Office
of Naval Research (N00014-96-1-0647).
</bodyText>
<sectionHeader confidence="0.945859" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99287264">
Baker, J. K. 1979. Trainable grammars for
speech recognition. In Speech
Communications Papers of the 97th Meeting of
the Acoustical Society of America,
pages 547-550, Cambridge, MA.
Baum, L. E. 1972. An inequality and
associated maximization techniques in
statistical estimation of probabilistic
functions of Markov processes.
Inequalities, 3:1-8.
Booth, T. L. and R. A. Thompson. 1973.
Applying probability measures to abstract
languages. IEEE Trans. on Computers,
C-22:442-450.
Dempster, A., N. Laird, and D. Rubin. 1977.
Maximum likelihood from incomplete
data via the EM algorithm. Journal of the
Royal Statistical Society, Series B, 39:1-38.
Grenander, U. 1976. Lectures in Pattern Theory
Volume 1, Pattern Synthesis.
Springer-Verlag, New York.
Harris, T. E. 1963, The Theory of Branching
Processes. Springer-Verlag, Berlin.
Hoperoft, J. E. and J. D. Ullman. 1979.
Introduction to Automata Theory, Languages,
</reference>
<page confidence="0.992794">
304
</page>
<note confidence="0.772185">
Chi and Geman Probabilistic Context-Free Grammars
</note>
<reference confidence="0.997069428571428">
and Computation. Addison Wesley.
Sanchez, J. A. and J. M. Benedc. 1997.
Consistency of stochastic context-free
grammars from probabilistic estimation
based on growth transformations. IEEE
Transactions on Pattern Analysis and Machine
Intelligence, 19:1052-1055.
Stolcke, A. 1995. An efficient probabilistic
context-free parsing algorithm that
computes prefix probabilities.
Computational Linguistics, 21:165-201.
Wetherell, C. S. 1980. Probabilistic
languages: A review and some open
questions. Computing Surveys, 12:361-379.
</reference>
<page confidence="0.999239">
305
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.835942">
<title confidence="0.9985605">Squibs and Discussions Estimation of Probabilistic Context-Free</title>
<author confidence="0.995781">Zhiyi Chi Stuart Geman</author>
<affiliation confidence="0.999779">Brown University Brown University</affiliation>
<abstract confidence="0.967229666666667">The assignment of probabilities to the productions of a context-free grammar may generate an improper distribution: the probability of all finite parse trees is less than one. The condition for proper assignment is rather subtle. Production probabilities can be estimated from parsed or unparsed sentences, and the question arises as to whether or not an estimated system is automatically proper. We show here that estimated production probabilities always yield proper distributions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Speech Communications Papers of the 97th Meeting of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<location>Cambridge, MA.</location>
<marker>Baker, 1979</marker>
<rawString>Baker, J. K. 1979. Trainable grammars for speech recognition. In Speech Communications Papers of the 97th Meeting of the Acoustical Society of America, pages 547-550, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
</authors>
<title>An inequality and associated maximization techniques in statistical estimation of probabilistic functions of Markov processes.</title>
<date>1972</date>
<journal>Inequalities,</journal>
<pages>3--1</pages>
<marker>Baum, 1972</marker>
<rawString>Baum, L. E. 1972. An inequality and associated maximization techniques in statistical estimation of probabilistic functions of Markov processes. Inequalities, 3:1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Booth</author>
<author>R A Thompson</author>
</authors>
<title>Applying probability measures to abstract languages.</title>
<date>1973</date>
<journal>IEEE Trans. on Computers,</journal>
<pages>22--442</pages>
<marker>Booth, Thompson, 1973</marker>
<rawString>Booth, T. L. and R. A. Thompson. 1973. Applying probability measures to abstract languages. IEEE Trans. on Computers, C-22:442-450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<pages>39--1</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A., N. Laird, and D. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39:1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Grenander</author>
</authors>
<date>1976</date>
<booktitle>Lectures in Pattern Theory Volume 1, Pattern Synthesis.</booktitle>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<marker>Grenander, 1976</marker>
<rawString>Grenander, U. 1976. Lectures in Pattern Theory Volume 1, Pattern Synthesis. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T E Harris</author>
</authors>
<title>The Theory of Branching Processes.</title>
<date>1963</date>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<marker>Harris, 1963</marker>
<rawString>Harris, T. E. 1963, The Theory of Branching Processes. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hoperoft</author>
<author>J D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation.</title>
<date>1979</date>
<publisher>Addison Wesley.</publisher>
<marker>Hoperoft, Ullman, 1979</marker>
<rawString>Hoperoft, J. E. and J. D. Ullman. 1979. Introduction to Automata Theory, Languages, and Computation. Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Sanchez</author>
<author>J M Benedc</author>
</authors>
<title>Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--1052</pages>
<marker>Sanchez, Benedc, 1997</marker>
<rawString>Sanchez, J. A. and J. M. Benedc. 1997. Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19:1052-1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--165</pages>
<marker>Stolcke, 1995</marker>
<rawString>Stolcke, A. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21:165-201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Wetherell</author>
</authors>
<title>Probabilistic languages: A review and some open questions. Computing Surveys,</title>
<date>1980</date>
<pages>12--361</pages>
<contexts>
<context position="3143" citStr="Wetherell (1980)" startWordPosition="480" endWordPosition="481">obabilities or do the estimated PCFG&apos;s assign some mass to infinite trees? We will show that in both cases the estimated probability is tight.&apos; * Division of Applied Mathematics, Brown University, Providence, RI 02912 USA 1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recently appeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (Sanchez and Bened( [1997]). 2 When estimating from art unparsed corpus, we shall assume a model without null or unit productions; see Section 2. Computational Linguistics Volume 24, Number 2 Wetherell (1980) has asked a similar question: a scheme (different from maximum likelihood) is introduced for estimating production probabilities from an unparsed corpus, and it is conjectured that the resulting system is tight. (Wetherell and others use the designation &amp;quot;consistent&amp;quot; instead of &amp;quot;tight,&amp;quot; but in statistics, consistency refers to the asymptotic correctness of an estimator.) A trivial example is the CFG with one nonterminal and one terminal symbol, in Chomsky normal form: A —&gt; AA A a where a is the only terminal symbol. Assign probability p to the first production (A AA) and q = 1 — p to the secon</context>
</contexts>
<marker>Wetherell, 1980</marker>
<rawString>Wetherell, C. S. 1980. Probabilistic languages: A review and some open questions. Computing Surveys, 12:361-379.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>