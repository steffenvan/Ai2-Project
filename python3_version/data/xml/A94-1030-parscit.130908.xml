<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017527">
<title confidence="0.9463845">
IMPROVING CHINESE TOKENIZATION WITH LINGUISTIC
FILTERS ON STATISTICAL LEXICAL ACQUISITION
</title>
<author confidence="0.997767">
Dekai Wu
</author>
<affiliation confidence="0.862196333333333">
Department of Computer Science
University of Science &amp; Technology (HKUST)
Clear Water Bay, Hong Kong
</affiliation>
<email confidence="0.994448">
dekai@cs.ust.hk
</email>
<sectionHeader confidence="0.993819" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999226">
The first step in Chinese NLP is to tokenize or segment char-
acter sequences into words, since the text contains no word
delimiters. Recent heavy activity in this area has shown
the biggest stumbling block to be words that are absent
from the lexicon, since successful tokenizers to date have
been based on dictionary lookup (e.g., Chang &amp; Chen 1993;
Chiang et al. 1992; Lin et al. 1993; Wu &amp; Tseng 1993;
Sproat et al. 1994).
We present empirical evidence for four points concern-
ing tokenization of Chinese text: (1) More rigorous &amp;quot;blind&amp;quot;
evaluation methodology is needed to avoid inflated accuracy
measurements; we introduce the nk-blind method. (2) The
extent of the unknown-word problem is far more serious than
generally thought, when tokenizing unrestricted texts in re-
alistic domains. (3) Statistical lexical acquisition is a prac-
tical means to greatly improve tokenization accuracy with
unknown words, reducing error rates as much as 32.0%. (4)
When augmenting the lexicon, linguistic constraints can pro-
vide simple inexpensive filters yielding significantly better
precision, reducing error rates as much as 49.4%.
</bodyText>
<sectionHeader confidence="0.497472" genericHeader="keywords">
HOW TO HANDLE DOUBLE STANDARDS
</sectionHeader>
<bodyText confidence="0.999503185185185">
Current evaluation practice favors overly optimistic accuracy
estimates. Because partially-tokenized words are usually
evaluated as being correctly tokenized, failures to tokenize
unknown words can be overlooked. For example, what makes
(yuan zhü jib, a charity) a single word when 1§11t and
are both legitimate words? One answer is that translat-
ing the partially-tokenized segments individually can yield
&amp;quot;assistance gold&amp;quot; or &amp;quot;aid currency&amp;quot;, instead of the unques-
tionably correct &amp;quot;charity&amp;quot; or &amp;quot;charity fund&amp;quot;. Another answer
is that a speech synthesizer should never pause between the
two segments; otherwise Int is taken as a verb and as a
surname, changing the meaning to &amp;quot;help Gold&amp;quot;. A blind eval-
uation paradigm is needed that accommodates disagreement
between human judges, yet does not bias the judges to accept
the computer&apos;s output too generously.
We have devised a procedure called nk-blind that uses n
blind judges&apos; standards. The n judges each hand-segment
the test sentences independently, before the algorithm is run.
Then, the algorithm&apos;s output is compared against the judges&apos;;
for each segment produced by the algorithm, the segment is
considered to be a correct token if at least k of the n judges
agree. Thus, more than one segmentation may be considered
correct if we set k such that k &lt; [J. If k is set to 1, it is
sufficient for any judge to sanction a segment. If k = n, all
the judges must agree. Under the nk-blind method a precision
rate can be given under any chosen (n, k) setting.
The experiments below were conducted with 100 pairs of
</bodyText>
<subsectionHeader confidence="0.814023">
Pascale Fung
</subsectionHeader>
<affiliation confidence="0.534117">
Computer Science Department
Columbia University
</affiliation>
<address confidence="0.504729">
New York, NY 10027
</address>
<email confidence="0.554013">
pascale@cs.columbia.edu
</email>
<bodyText confidence="0.999921714285714">
sentences from the corpus containing between 2,000 and 2,600
words, sampled randomly with replacement. All results re-
ported in Figure 1 give the precision rates for n = 8 judges
with all values of k between 1 and n. Note the tendency of
higher values of k to reduce precision estimates. The wide
variance with different k (between 30% and 90%) underscores
the importance of more rigorous evaluation methodology.
</bodyText>
<sectionHeader confidence="0.994868" genericHeader="introduction">
EXPERIMENT I
</sectionHeader>
<bodyText confidence="0.99772447826087">
Tokenizing independently derived test data. The unknown
word problem is now widely recognized, but we believe its
severity is still greatly underestimated. As an &amp;quot;acid test&amp;quot;, we
tokenized a corpus that was derived completely independently
of the dictionary that our tokenizer is based on. We used a
statistical dictionary-based tokenizer designed to be represen-
tative of current tokenizing approaches, which chooses the
segmentation that maximizes the product of the individual
words&apos; probabilities. The baseline dictionary used by the tok-
enizer is the BDC dictionary (BDC 1992), containing 89,346
unique orthographic forms. The text, drawn from the HKUST
English-Chinese Parallel Bilingual Corpus (Wu 1994), con-
sists of transcripts from the parliamentary proceedings of the
Hong Kong Legislative Council. Thus, the text can be ex-
pected to contain many references to subjects outside the do-
mains under consideration by our dictionary&apos;s lexicographers
in Taiwan. Regional usage differences are also to be expected.
The results (see Figure 1) show accuracy rates far below the
90-99% range which is typically reported. Visual inspection
of tokenized output showed that an overwhelming majority of
the errors arose from missing dictionary entries. Tokeniza-
tion performance on realistic unrestricted text is still seriously
compromised.
</bodyText>
<sectionHeader confidence="0.990696" genericHeader="method">
EXPERIMENT II
</sectionHeader>
<bodyText confidence="0.978423466666667">
Tokenization with statistical lexicon augmentation. To al-
leviate the unknown word problem, we next experimented
with augmenting the tokenizer&apos;s dictionary using CXtract,
a statistical tool that finds morpheme sequences likely to be
Chinese words (Fung &amp; Wu 1994). In the earlier work we
found CXtract to be a good generator of previously unknown
lexical entries, so overall token recall was expected to im-
prove. However, it was not clear whether the gain would
outweigh errors introduced by the illegitimate lexical entries
that CXtract also produces.
The training corpus consisted of approximately 2 million
Chinese characters drawn from the Chinese half of our bilin-
gual corpus. The unsupervised training procedure is described
in detail in Fung &amp; Wu (1994). The training suggested 6,650
candidate lexical entries. Of these, 2,040 were already present
</bodyText>
<page confidence="0.986479">
180
</page>
<bodyText confidence="0.999380375">
in the dictionary, leaving 4,610 previously unknown new en-
tries.
The same tokenization experiment was then run, using the
augmented dictionary instead. The results shown in Fig-
ure 1 bear out our hypothesis that augmenting the lexicon
with CXtract9s statistically generated lexical entries would
improve the overall precision, reducing error rates as much as
32.0% fork = 2.
</bodyText>
<sectionHeader confidence="0.984296" genericHeader="method">
EXPERIMENT III
</sectionHeader>
<bodyText confidence="0.895616086206897">
Morphosyntactic filters for lexicon candidates. CXtract
produces excellent recall but we wished to improve precision
further. Ideally, the false candidates should be rejected by
some automatic means, without eliminating valid lexical en-
tries. To this end, we investigated a set of 34 simple filters
based on linguistic principles. Space precludes a full listing;
selected filters are discussed below.
Our filters can be extremely inexpensive because CXtraces
statistical criteria are already tuned for high precision. The
filtering process first segments the candidate using the orig-
inal dictionary, to identify the component words. It then
applies morphological and syntactic constraints to eliminate
(a) sequences that should remain multiple segments and (b) ill-
formed sequences.
Morphological constraints. The morphologically-based fil-
ters reject a hypothesized lexical entry if it matches any fil-
tering pattern. The particular characters in these filters are
usually classified either as morphological affixes, or as indi-
vidual words. We reject any sequence with the affix on the
wrong end (the special case of the genitive n (de) is consid-
ered below). Because morphemes such as the plural marker
IN (men) or the instance marker t (ci) are suffixes, we can
eliminate candidate sequences that begin with them. Simi-
larly, we can reject sequences that end with the ordinal prefix
M (di) or the preverbial durative (zai).
Filtering characters cannot be used if they are polysemous
or homonymous and can participate in legitimate words in
other uses. For example, the durative 4 (zhe) is not a good
filter because the same character (with varying pronuncia-
tions) can be used to mean &amp;quot;apply&amp;quot;, &amp;quot;trick&amp;quot;, or &amp;quot;touch&amp;quot;, among
others.
Any candidate lexical entry is filtered if it contains the gen-
itive/associative (Si (de). This includes, for example, both ill-
formed boundary-crossing patterns like Ji (de (de wei xian,
danger of), and phrases like 4Crillijit (xiang gang de (pan
Hong Kong&apos;s future) which should properly be segmented
Mt fit] Mit. In addition, because the compounding process
does not involve two double-character words as frequently as
other patterns, such sequences were rejected.
Closed-class syntactic constraints. The closed-class filters
operate on two distinct principles. Sequences ending with
strongly prenominal or preverbial words are rejected, as are
sequences beginning with postnominals and postverbials. A
majority of the filtering patterns match correct syntactic units,
including prepositional, conjunctive, modal, adverbial, and
verb phrases. The rationale for rejecting such sequences is
that these closed-class words do not satisfy the criteria for
being bound into compounds, and just co-occur with some
sequences by chance because of their high frequency.
Results. The same tokenization experiment was run us-
ing the filtered augmented dictionary. The filters left 5,506
candidate lexical entries out of the original 6,650, of which
3,467 were previously unknown. Figure 1 shows significantly
improved precision in every measurement except for a very
slight drop with k = 8, with an error rate reduction of 49.4%
at k = 2. Thus any loss in token recall due to the filters is
outweighed by the gain in precision. This may be taken as
indirect evidence that the loss in recall is not large.
</bodyText>
<sectionHeader confidence="0.997843" genericHeader="conclusions">
CONCLUSION
</sectionHeader>
<bodyText confidence="0.999747307692308">
We have introduced a blind evaluation method that accom-
modates multiple standards and gives some indication of how
well algorithms&apos; outputs match human preferences.
We have demonstrated that pure statistically-based lexical
acquisition on the same corpus being tokenized can signif-
icantly reduce error rates due to unknown words. We also
demonstrated empirically the effectiveness of simple mor-
phosyntactic filters in improving the precision of a hybrid
statistical/linguistic method for generating new lexical en-
tries. Using linguistic knowledge to construct filters rather
than generators has the advantage that applicability conditions
do not need to be closely checked, since the training corpus
presumably already adheres to any applicability conditions.
</bodyText>
<figure confidence="0.9983597">
100
90
80
70
60
50
40
30
20
1 2 3 4 5 &apos; 6 7 8
</figure>
<figureCaption confidence="0.999969">
Figure 1. Comparison of rek-Blind Precision Percentages
</figureCaption>
<sectionHeader confidence="0.99894" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.998451083333333">
BDC. 1992. The BDC Chinese-English electronic dictionary (version 2.0). Behavior
Design Corporation.
CHANG, CHAO-HUANG &amp; CHENG-DER CHEN. 1993. HMM-based part-of-speech tagging
for Chinese corpora. In Proceedings of the Workshop on Very Large Corpora,
40-47, Columbus, Ohio.
CHIANG, TUNG-HUI, JING-SHIN CHANG, MING-YU LIN, &amp; KEH-Y1H Su. 1992. Statis-
tical models for word segmentation and unknown resolution. In Proceedings of
ROCLING-92, 121-146. •
FUNG, PASCALE &amp; DEKA Wu. 1994. Statistical augmentation of a Chinese machine-
readable dictionary. In Proceedings of the Second Annual Workshop on Very
Large Corpora, 69-85, Kyoto.
LIN, MING-YU, TUNG-HUI CHANG, &amp; KEH-YIH SU. 1993. A preliminary study on
unknown word problem in Chinese word segmentation. In Proceedings of
ROCLJNG -93 , 119-141.
SPROAT, RICHARD, CHIL1N SHIN, WILLIAM GALE, &amp; NANCY CHANG. 1994. A stochastic
word segmentation algorithm for a Mandarin text-to-speech system. In Pro-
ceedings of the 32nd Annual Conference of the Association for Computational
Linguistics, 66-72, Las Cruces, New Mexico.
Wu, DEKA!. 1994. Aligning a parallel English-Chinese corpus statistically with lexical
criteria. In Proceedings of the 32nd Annual Conference of the Association for
Computational Linguistics, 80-87, Las Cruces, New Mexico.
Wu, Z1MIN &amp; GWYNETH TSENG. 1993. Chinese text segmentation for text retrieval:
Achievements and problems. Journal of The American Society for Information
Science, 44(9):532-542.
</reference>
<figure confidence="0.925382666666667">
&amp;quot;I-baseline&amp;quot; -e—
ll-augmented&amp;quot;
&amp;quot;VI-filtered&amp;quot;
</figure>
<page confidence="0.949302">
181
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.024228">
<title confidence="0.9957875">IMPROVING CHINESE TOKENIZATION WITH LINGUISTIC FILTERS ON STATISTICAL LEXICAL ACQUISITION</title>
<author confidence="0.999497">Dekai Wu</author>
<affiliation confidence="0.996434">Department of Computer Science of Science &amp; Technology (HKUST)</affiliation>
<address confidence="0.996246">Clear Water Bay, Hong Kong</address>
<email confidence="0.996101">dekai@cs.ust.hk</email>
<abstract confidence="0.984760428571428">The first step in Chinese NLP is to tokenize or segment character sequences into words, since the text contains no word delimiters. Recent heavy activity in this area has shown the biggest stumbling block to be words that are absent from the lexicon, since successful tokenizers to date have been based on dictionary lookup (e.g., Chang &amp; Chen 1993; al. Lin al. Wu &amp; Tseng 1993; al. We present empirical evidence for four points concerning tokenization of Chinese text: (1) More rigorous &amp;quot;blind&amp;quot; evaluation methodology is needed to avoid inflated accuracy we introduce the (2) The extent of the unknown-word problem is far more serious than generally thought, when tokenizing unrestricted texts in realistic domains. (3) Statistical lexical acquisition is a practical means to greatly improve tokenization accuracy with unknown words, reducing error rates as much as 32.0%. (4) When augmenting the lexicon, linguistic constraints can prosimple inexpensive filters yielding significantly precision, reducing error rates as much as 49.4%. HOW TO HANDLE DOUBLE STANDARDS Current evaluation practice favors overly optimistic accuracy estimates. Because partially-tokenized words are usually evaluated as being correctly tokenized, failures to tokenize unknown words can be overlooked. For example, what makes (yuan zhü jib, a charity) a single word when 1§11t and are both legitimate words? One answer is that translating the partially-tokenized segments individually can yield &amp;quot;assistance gold&amp;quot; or &amp;quot;aid currency&amp;quot;, instead of the unquestionably correct &amp;quot;charity&amp;quot; or &amp;quot;charity fund&amp;quot;. Another answer is that a speech synthesizer should never pause between the two segments; otherwise Int is taken as a verb and as a surname, changing the meaning to &amp;quot;help Gold&amp;quot;. A blind evaluation paradigm is needed that accommodates disagreement between human judges, yet does not bias the judges to accept the computer&apos;s output too generously. have devised a procedure called uses n blind judges&apos; standards. The n judges each hand-segment the test sentences independently, before the algorithm is run. Then, the algorithm&apos;s output is compared against the judges&apos;; for each segment produced by the algorithm, the segment is to be a correct token if at least the n judges agree. Thus, more than one segmentation may be considered if we set that &lt; If set to 1, it is for any judge to sanction a segment. If = n, the judges must agree. Under the nk-blind method a precision can be given under any chosen (n, The experiments below were conducted with 100 pairs of</abstract>
<author confidence="0.992463">Pascale Fung</author>
<affiliation confidence="0.9999085">Computer Science Department Columbia University</affiliation>
<address confidence="0.999361">New York, NY 10027</address>
<email confidence="0.970305">pascale@cs.columbia.edu</email>
<abstract confidence="0.998864100775194">sentences from the corpus containing between 2,000 and 2,600 words, sampled randomly with replacement. All results rein Figure 1 give the precision rates for n = all values of 1 and n. Note the tendency of values of reduce precision estimates. The wide with different 30% and 90%) underscores the importance of more rigorous evaluation methodology. EXPERIMENT I independently derived test data. unknown word problem is now widely recognized, but we believe its severity is still greatly underestimated. As an &amp;quot;acid test&amp;quot;, we tokenized a corpus that was derived completely independently of the dictionary that our tokenizer is based on. We used a statistical dictionary-based tokenizer designed to be representative of current tokenizing approaches, which chooses the segmentation that maximizes the product of the individual words&apos; probabilities. The baseline dictionary used by the tokenizer is the BDC dictionary (BDC 1992), containing 89,346 unique orthographic forms. The text, drawn from the HKUST English-Chinese Parallel Bilingual Corpus (Wu 1994), consists of transcripts from the parliamentary proceedings of the Hong Kong Legislative Council. Thus, the text can be expected to contain many references to subjects outside the domains under consideration by our dictionary&apos;s lexicographers in Taiwan. Regional usage differences are also to be expected. The results (see Figure 1) show accuracy rates far below the 90-99% range which is typically reported. Visual inspection of tokenized output showed that an overwhelming majority of the errors arose from missing dictionary entries. Tokenization performance on realistic unrestricted text is still seriously compromised. EXPERIMENT II with statistical lexicon augmentation. alleviate the unknown word problem, we next experimented augmenting the tokenizer&apos;s dictionary using a statistical tool that finds morpheme sequences likely to be Chinese words (Fung &amp; Wu 1994). In the earlier work we found CXtract to be a good generator of previously unknown lexical entries, so overall token recall was expected to improve. However, it was not clear whether the gain would outweigh errors introduced by the illegitimate lexical entries that CXtract also produces. The training corpus consisted of approximately 2 million Chinese characters drawn from the Chinese half of our bilingual corpus. The unsupervised training procedure is described in detail in Fung &amp; Wu (1994). The training suggested 6,650 candidate lexical entries. Of these, 2,040 were already present 180 in the dictionary, leaving 4,610 previously unknown new entries. The same tokenization experiment was then run, using the augmented dictionary instead. The results shown in Figure 1 bear out our hypothesis that augmenting the lexicon statistically generated lexical entries would improve the overall precision, reducing error rates as much as 2. EXPERIMENT III filters for lexicon candidates. produces excellent recall but we wished to improve precision further. Ideally, the false candidates should be rejected by some automatic means, without eliminating valid lexical entries. To this end, we investigated a set of 34 simple filters based on linguistic principles. Space precludes a full listing; selected filters are discussed below. Our filters can be extremely inexpensive because CXtraces statistical criteria are already tuned for high precision. The filtering process first segments the candidate using the original dictionary, to identify the component words. It then applies morphological and syntactic constraints to eliminate (a) sequences that should remain multiple segments and (b) illformed sequences. constraints. morphologically-based filters reject a hypothesized lexical entry if it matches any filtering pattern. The particular characters in these filters are usually classified either as morphological affixes, or as individual words. We reject any sequence with the affix on the end (the special case of the genitive considered below). Because morphemes such as the plural marker the instance marker t (ci) are suffixes, we can eliminate candidate sequences that begin with them. Similarly, we can reject sequences that end with the ordinal prefix the preverbial durative Filtering characters cannot be used if they are polysemous or homonymous and can participate in legitimate words in uses. For example, the durative not a good filter because the same character (with varying pronunciations) can be used to mean &amp;quot;apply&amp;quot;, &amp;quot;trick&amp;quot;, or &amp;quot;touch&amp;quot;, among others. Any candidate lexical entry is filtered if it contains the gen- (Si includes, for example, both illboundary-crossing patterns like (de wei xian, of), and phrases like 4Crillijit gang de (pan Hong Kong&apos;s future) which should properly be segmented fit] In addition, because the compounding process does not involve two double-character words as frequently as other patterns, such sequences were rejected. syntactic constraints. closed-class filters operate on two distinct principles. Sequences ending with strongly prenominal or preverbial words are rejected, as are sequences beginning with postnominals and postverbials. A of the filtering patterns match units, including prepositional, conjunctive, modal, adverbial, and verb phrases. The rationale for rejecting such sequences is that these closed-class words do not satisfy the criteria for being bound into compounds, and just co-occur with some sequences by chance because of their high frequency. same tokenization experiment was run using the filtered augmented dictionary. The filters left 5,506 candidate lexical entries out of the original 6,650, of which 3,467 were previously unknown. Figure 1 shows significantly improved precision in every measurement except for a very drop with = with an error rate reduction of 49.4% = Thus any loss in token recall due to the filters is outweighed by the gain in precision. This may be taken as indirect evidence that the loss in recall is not large. CONCLUSION We have introduced a blind evaluation method that accommodates multiple standards and gives some indication of how well algorithms&apos; outputs match human preferences. We have demonstrated that pure statistically-based lexical acquisition on the same corpus being tokenized can significantly reduce error rates due to unknown words. We also demonstrated empirically the effectiveness of simple morphosyntactic filters in improving the precision of a hybrid statistical/linguistic method for generating new lexical en- Using linguistic knowledge to construct than generators has the advantage that applicability conditions do not need to be closely checked, since the training corpus presumably already adheres to any applicability conditions.</abstract>
<note confidence="0.859648641025641">100 90 80 70 60 50 40 30 20 1 2 3 4 5 &apos; 6 7 8 1. Comparison of Precision Percentages REFERENCES BDC Chinese-English electronic dictionary (version 2.0). Design Corporation. CHANG, CHAO-HUANG &amp; CHENG-DER CHEN. 1993. HMM-based part-of-speech tagging Chinese corpora. In of the Workshop on Very Large Corpora, 40-47, Columbus, Ohio. CHIANG, TUNG-HUI, JING-SHIN CHANG, MING-YU LIN, &amp; KEH-Y1H Su. 1992. Statismodels for word segmentation and unknown resolution. In of • FUNG, PASCALE &amp; DEKA Wu. 1994. Statistical augmentation of a Chinese machinedictionary. In of the Second Annual Workshop on Very Corpora, Kyoto. LIN, MING-YU, TUNG-HUI CHANG, &amp; KEH-YIH SU. 1993. A preliminary study on word problem in Chinese word segmentation. In of ROCLJNG -93 , 119-141. SPROAT, RICHARD, CHIL1N SHIN, WILLIAM GALE, &amp; NANCY CHANG. 1994. A stochastic segmentation algorithm for a Mandarin text-to-speech system. In Proceedings of the 32nd Annual Conference of the Association for Computational Las Cruces, New Mexico. Wu, DEKA!. 1994. Aligning a parallel English-Chinese corpus statistically with lexical In of the 32nd Annual Conference of the Association for Linguistics, Las Cruces, New Mexico. Wu, Z1MIN &amp; GWYNETH TSENG. 1993. Chinese text segmentation for text retrieval: and problems. of The American Society for Information &amp;quot;I-baseline&amp;quot; -e— ll-augmented&amp;quot; &amp;quot;VI-filtered&amp;quot; 181</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>BDC</author>
</authors>
<title>The BDC Chinese-English electronic dictionary (version 2.0). Behavior Design Corporation.</title>
<date>1992</date>
<contexts>
<context position="4048" citStr="BDC 1992" startWordPosition="636" endWordPosition="637">tion methodology. EXPERIMENT I Tokenizing independently derived test data. The unknown word problem is now widely recognized, but we believe its severity is still greatly underestimated. As an &amp;quot;acid test&amp;quot;, we tokenized a corpus that was derived completely independently of the dictionary that our tokenizer is based on. We used a statistical dictionary-based tokenizer designed to be representative of current tokenizing approaches, which chooses the segmentation that maximizes the product of the individual words&apos; probabilities. The baseline dictionary used by the tokenizer is the BDC dictionary (BDC 1992), containing 89,346 unique orthographic forms. The text, drawn from the HKUST English-Chinese Parallel Bilingual Corpus (Wu 1994), consists of transcripts from the parliamentary proceedings of the Hong Kong Legislative Council. Thus, the text can be expected to contain many references to subjects outside the domains under consideration by our dictionary&apos;s lexicographers in Taiwan. Regional usage differences are also to be expected. The results (see Figure 1) show accuracy rates far below the 90-99% range which is typically reported. Visual inspection of tokenized output showed that an overwhel</context>
</contexts>
<marker>BDC, 1992</marker>
<rawString>BDC. 1992. The BDC Chinese-English electronic dictionary (version 2.0). Behavior Design Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CHAO-HUANG CHANG</author>
<author>CHENG-DER CHEN</author>
</authors>
<title>HMM-based part-of-speech tagging for Chinese corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora,</booktitle>
<pages>40--47</pages>
<location>Columbus, Ohio.</location>
<marker>CHANG, CHENG-DER CHEN, 1993</marker>
<rawString>CHANG, CHAO-HUANG &amp; CHENG-DER CHEN. 1993. HMM-based part-of-speech tagging for Chinese corpora. In Proceedings of the Workshop on Very Large Corpora, 40-47, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TUNG-HUI CHIANG</author>
<author>JING-SHIN CHANG</author>
<author>MING-YU LIN</author>
<author>KEH-Y1H Su</author>
</authors>
<title>Statistical models for word segmentation and unknown resolution.</title>
<date>1992</date>
<booktitle>In Proceedings of ROCLING-92,</booktitle>
<pages>121--146</pages>
<publisher></publisher>
<marker>CHIANG, CHANG, LIN, Su, 1992</marker>
<rawString>CHIANG, TUNG-HUI, JING-SHIN CHANG, MING-YU LIN, &amp; KEH-Y1H Su. 1992. Statistical models for word segmentation and unknown resolution. In Proceedings of ROCLING-92, 121-146. •</rawString>
</citation>
<citation valid="true">
<authors>
<author>PASCALE FUNG</author>
<author>DEKA Wu</author>
</authors>
<title>Statistical augmentation of a Chinese machinereadable dictionary.</title>
<date>1994</date>
<booktitle>In Proceedings of the Second Annual Workshop on Very Large Corpora,</booktitle>
<pages>69--85</pages>
<location>Kyoto.</location>
<marker>FUNG, Wu, 1994</marker>
<rawString>FUNG, PASCALE &amp; DEKA Wu. 1994. Statistical augmentation of a Chinese machinereadable dictionary. In Proceedings of the Second Annual Workshop on Very Large Corpora, 69-85, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MING-YU LIN</author>
<author>TUNG-HUI CHANG</author>
<author>KEH-YIH SU</author>
</authors>
<title>A preliminary study on unknown word problem in Chinese word segmentation.</title>
<date>1993</date>
<booktitle>In Proceedings of ROCLJNG -93 ,</booktitle>
<pages>119--141</pages>
<marker>LIN, CHANG, SU, 1993</marker>
<rawString>LIN, MING-YU, TUNG-HUI CHANG, &amp; KEH-YIH SU. 1993. A preliminary study on unknown word problem in Chinese word segmentation. In Proceedings of ROCLJNG -93 , 119-141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RICHARD SPROAT</author>
<author>CHIL1N SHIN</author>
<author>WILLIAM GALE</author>
<author>NANCY CHANG</author>
</authors>
<title>A stochastic word segmentation algorithm for a Mandarin text-to-speech system.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>66--72</pages>
<location>Las Cruces, New Mexico.</location>
<marker>SPROAT, SHIN, GALE, CHANG, 1994</marker>
<rawString>SPROAT, RICHARD, CHIL1N SHIN, WILLIAM GALE, &amp; NANCY CHANG. 1994. A stochastic word segmentation algorithm for a Mandarin text-to-speech system. In Proceedings of the 32nd Annual Conference of the Association for Computational Linguistics, 66-72, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DEKA Wu</author>
</authors>
<title>Aligning a parallel English-Chinese corpus statistically with lexical criteria.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>80--87</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="4177" citStr="Wu 1994" startWordPosition="653" endWordPosition="654">we believe its severity is still greatly underestimated. As an &amp;quot;acid test&amp;quot;, we tokenized a corpus that was derived completely independently of the dictionary that our tokenizer is based on. We used a statistical dictionary-based tokenizer designed to be representative of current tokenizing approaches, which chooses the segmentation that maximizes the product of the individual words&apos; probabilities. The baseline dictionary used by the tokenizer is the BDC dictionary (BDC 1992), containing 89,346 unique orthographic forms. The text, drawn from the HKUST English-Chinese Parallel Bilingual Corpus (Wu 1994), consists of transcripts from the parliamentary proceedings of the Hong Kong Legislative Council. Thus, the text can be expected to contain many references to subjects outside the domains under consideration by our dictionary&apos;s lexicographers in Taiwan. Regional usage differences are also to be expected. The results (see Figure 1) show accuracy rates far below the 90-99% range which is typically reported. Visual inspection of tokenized output showed that an overwhelming majority of the errors arose from missing dictionary entries. Tokenization performance on realistic unrestricted text is sti</context>
<context position="5576" citStr="Wu (1994)" startWordPosition="866" endWordPosition="867">nary using CXtract, a statistical tool that finds morpheme sequences likely to be Chinese words (Fung &amp; Wu 1994). In the earlier work we found CXtract to be a good generator of previously unknown lexical entries, so overall token recall was expected to improve. However, it was not clear whether the gain would outweigh errors introduced by the illegitimate lexical entries that CXtract also produces. The training corpus consisted of approximately 2 million Chinese characters drawn from the Chinese half of our bilingual corpus. The unsupervised training procedure is described in detail in Fung &amp; Wu (1994). The training suggested 6,650 candidate lexical entries. Of these, 2,040 were already present 180 in the dictionary, leaving 4,610 previously unknown new entries. The same tokenization experiment was then run, using the augmented dictionary instead. The results shown in Figure 1 bear out our hypothesis that augmenting the lexicon with CXtract9s statistically generated lexical entries would improve the overall precision, reducing error rates as much as 32.0% fork = 2. EXPERIMENT III Morphosyntactic filters for lexicon candidates. CXtract produces excellent recall but we wished to improve preci</context>
</contexts>
<marker>Wu, 1994</marker>
<rawString>Wu, DEKA!. 1994. Aligning a parallel English-Chinese corpus statistically with lexical criteria. In Proceedings of the 32nd Annual Conference of the Association for Computational Linguistics, 80-87, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z1MIN Wu</author>
<author>GWYNETH TSENG</author>
</authors>
<title>Chinese text segmentation for text retrieval: Achievements and problems.</title>
<date>1993</date>
<journal>Journal of The American Society for Information Science,</journal>
<pages>44--9</pages>
<marker>Wu, TSENG, 1993</marker>
<rawString>Wu, Z1MIN &amp; GWYNETH TSENG. 1993. Chinese text segmentation for text retrieval: Achievements and problems. Journal of The American Society for Information Science, 44(9):532-542.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>