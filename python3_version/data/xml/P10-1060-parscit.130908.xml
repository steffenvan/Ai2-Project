<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001661">
<title confidence="0.992504">
Generating Focused Topic-specific Sentiment Lexicons
</title>
<author confidence="0.993811">
Valentin Jijkoun Maarten de Rijke Wouter Weerkamp
</author>
<affiliation confidence="0.998258">
ISLA, University of Amsterdam, The Netherlands
</affiliation>
<email confidence="0.987525">
jijkoun,derijke,w.weerkamp@uva.nl
</email>
<sectionHeader confidence="0.993677" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99986952631579">
We present a method for automatically
generating focused and accurate topic-
specific subjectivity lexicons from a gen-
eral purpose polarity lexicon that allow
users to pin-point subjective on-topic in-
formation in a set of relevant documents.
We motivate the need for such lexicons
in the field of media analysis, describe
a bootstrapping method for generating a
topic-specific lexicon from a general pur-
pose polarity lexicon, and evaluate the
quality of the generated lexicons both
manually and using a TREC Blog track
test set for opinionated blog post retrieval.
Although the generated lexicons can be an
order of magnitude more selective than the
general purpose lexicon, they maintain, or
even improve, the performance of an opin-
ion retrieval system.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933652173913">
In the area of media analysis, one of the key
tasks is collecting detailed information about opin-
ions and attitudes toward specific topics from var-
ious sources, both offline (traditional newspapers,
archives) and online (news sites, blogs, forums).
Specifically, media analysis concerns the follow-
ing system task: given a topic and list of docu-
ments (discussing the topic), find all instances of
attitudes toward the topic (e.g., positive/negative
sentiments, or, if the topic is an organization or
person, support/criticism of this entity). For every
such instance, one should identify the source of
the sentiment, the polarity and, possibly, subtopics
that this attitude relates to (e.g., specific targets
of criticism or support). Subsequently, a (hu-
man) media analyst must be able to aggregate
the extracted information by source, polarity or
subtopics, allowing him to build support/criticism
networks etc. (Altheide, 1996). Recent advances
in language technology, especially in sentiment
analysis, promise to (partially) automate this task.
Sentiment analysis is often considered in the
context of the following two tasks:
</bodyText>
<listItem confidence="0.963369333333333">
• sentiment extraction: given a set of textual
documents, identify phrases, clauses, sen-
tences or entire documents that express atti-
tudes, and determine the polarity of these at-
titudes (Kim and Hovy, 2004); and
• sentiment retrieval: given a topic (and possi-
bly, a list of documents relevant to the topic),
identify documents that express attitudes to-
ward this topic (Ounis et al., 2007).
</listItem>
<bodyText confidence="0.9990132">
How can technology developed for sentiment
analysis be applied to media analysis? In order
to use a sentiment extraction system for a media
analysis problem, a system would have to be able
to determine which of the extracted sentiments are
actually relevant, i.e., it would not only have to
identify specific targets of all extracted sentiments,
but also decide which of the targets are relevant
for the topic at hand. This is a difficult task, as
the relation between a topic (e.g., a movie) and
specific targets of sentiments (e.g., acting or spe-
cial effects in the movie) is not always straight-
forward, in the face of ubiquitous complex lin-
guistic phenomena such as referential expressions
(“... this beautifully shot documentary”) or bridg-
ing anaphora (“the director did an excellent jobs”).
In sentiment retrieval, on the other hand, the
topic is initially present in the task definition, but
it is left to the user to identify sources and targets
of sentiments, as systems typically return a list
of documents ranked by relevance and opinion-
atedness. To use a traditional sentiment retrieval
system in media analysis, one would still have to
manually go through ranked lists of documents re-
turned by the system.
</bodyText>
<page confidence="0.980049">
585
</page>
<note confidence="0.9434755">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 585–594,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999318">
To be able to support media analysis, we need to
combine the specificity of (phrase- or word-level)
sentiment analysis with the topicality provided by
sentiment retrieval. Moreover, we should be able
to identify sources and specific targets of opinions.
Another important issue in the media analysis
context is evidence for a system’s decision. If the
output of a system is to be used to inform actions,
the system should present evidence, e.g., high-
lighting words or phrases that indicate a specific
attitude. Most modern approaches to sentiment
analysis, however, use various flavors of classifi-
cation, where decisions (typically) come with con-
fidence scores, but without explicit support.
In order to move towards the requirements of
media analysis, in this paper we focus on two of
the problems identified above: (1) pinpointing ev-
idence for a system’s decisions about the presence
of sentiment in text, and (2) identifying specific
targets of sentiment.
We address these problems by introducing a
special type of lexical resource: a topic-specific
subjectivity lexicon that indicates specific relevant
targets for which sentiments may be expressed; for
a given topic, such a lexicon consists of pairs (syn-
tactic clue, target). We present a method for au-
tomatically generating a topic-specific lexicon for
a given topic and query-biased set of documents.
We evaluate the quality of the lexicon both manu-
ally and in the setting of an opinionated blog post
retrieval task. We demonstrate that such a lexi-
con is highly focused, allowing one to effectively
pinpoint evidence for sentiment, while being com-
petetive with traditional subjectivity lexicons con-
sisting of (a large number of) clue words.
Unlike other methods for topic-specific senti-
ment analysis, we do not expand a seed lexicon.
Instead, we make an existing lexicon more fo-
cused, so that it can be used to actually pin-point
subjectivity in documents relevant to a given topic.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998438">
Much work has been done in sentiment analy-
sis. We discuss related work in four parts: sen-
timent analysis in general, domain- and target-
specific sentiment analysis, product review mining
and sentiment retrieval.
</bodyText>
<subsectionHeader confidence="0.999756">
2.1 Sentiment analysis
</subsectionHeader>
<bodyText confidence="0.999963666666667">
Sentiment analysis is often seen as two separate
steps for determining subjectivity and polarity.
Most approaches first try to identify subjective
units (documents, sentences), and for each of these
determine whether it is positive or negative. Kim
and Hovy (2004) select candidate sentiment sen-
tences and use word-based sentiment classifiers
to classify unseen words into a negative or posi-
tive class. First, the lexicon is constructed from
WordNet: from several seed words, the structure
of WordNet is used to expand this seed to a full
lexicon. Next, this lexicon is used to measure the
distance between unseen words and words in the
positive and negative classes. Based on word sen-
timents, a decision is made at the sentence level.
A similar approach is taken by Wilson et al.
(2005): a classifier is learnt that distinguishes be-
tween polar and neutral sentences, based on a prior
polarity lexicon and an annotated corpus. Among
the features used are syntactic features. After this
initial step, the sentiment sentences are classified
as negative or positive; again, a prior polarity lexi-
con and syntactic features are used. The authors
later explored the difference between prior and
contextual polarity (Wilson et al., 2009): words
that lose polarity in context, or whose polarity is
reversed because of context.
Riloff and Wiebe (2003) describe a bootstrap-
ping method to learn subjective extraction pat-
terns that match specific syntactic templates, using
a high-precision sentence-level subjectivity clas-
sifier and a large unannotated corpus. In our
method, we bootstrap from a subjectivity lexi-
cion rather than a classifier, and perform a topic-
specific analysis, learning indicators of subjectiv-
ity toward a specific topic.
</bodyText>
<subsectionHeader confidence="0.998797">
2.2 Domain- and target-specific sentiment
</subsectionHeader>
<bodyText confidence="0.999941666666667">
The way authors express their attitudes varies
with the domain: An unpredictable movie can be
positive, but unpredictable politicians are usually
something negative. Since it is unrealistic to con-
struct sentiment lexicons, or manually annotate
text for learning, for every imaginable domain or
topic, automatic methods have been developed.
Godbole et al. (2007) aim at measuring over-
all subjectivity or polarity towards a certain entity;
they identify sentiments using domain-specific
lexicons. The lexicons are generated from man-
ually selected seeds for a broad domain such as
Health or Business, following an approach simi-
lar to (Kim and Hovy, 2004). All named entites
in a sentence containing a clue from a lexicon are
</bodyText>
<page confidence="0.99737">
586
</page>
<bodyText confidence="0.999290612903226">
considered targets of sentiment for counting. Be-
cause of the data volume, no expensive linguistic
processing is performed.
Choi et al. (2009) advocate a joint topic-
sentiment analysis. They identify “sentiment top-
ics,” noun phrases assumed to be linked to a sen-
timent clue in the same expression. They address
two tasks: identifying sentiment clues, and clas-
sifying sentences into positive, negative, or neu-
tral. They start by selecting initial clues from Sen-
tiWordNet, based on sentences with known polar-
ity. Next, the sentiment topics are identified, and
based on these sentiment topics and the current list
of clues, new potential clues are extracted. The
clues can be used to classifiy sentences.
Fahrni and Klenner (2008) identify potential
targets in a given domain, and create a target-
specific polarity adjective lexicon. To this end,
they find targets using Wikipedia, and associated
adjectives. Next, the target-specific polarity of ad-
jectives is detemined using Hearst-like patterns.
Kanayama and Nasukawa (2006) introduce po-
lar atoms: minimal human-understandable syn-
tactic structures that specify polarity of clauses.
The goal is to learn new domain-specific polar
atoms, but these are not target-specific. They
use manually-created syntactic patterns to identify
atoms and coherency to determine polarity.
In contrast to much of the work in the literature,
we need to specialize subjectivity lexicons not for
a domain and target, but for “topics.”
</bodyText>
<subsectionHeader confidence="0.999912">
2.3 Product features and opinions
</subsectionHeader>
<bodyText confidence="0.999895958333333">
Much work has been carried out for the task of
mining product reviews, where the goal is to iden-
tify features of specific products (such as picture,
zoom, size, weight for digital cameras) and opin-
ions about these specific features in user reviews.
Liu et al. (2005) describe a system that identifies
such features via rules learned from a manually
annotated corpus of reviews; opinions on features
are extracted from the structure of reviews (which
explicitly separate positive and negative opinions).
Popescu and Etzioni (2005) present a method
that identifies product features for using corpus
statistics, WordNet relations and morphological
cues. Opinions about the features are extracted us-
ing a hand-crafted set of syntactic rules.
Targets extracted in our method for a topic are
similar to features extracted in review mining for
products. However, topics in our setting go be-
yond concrete products, and the diversity and gen-
erality of possible topics makes it difficult to ap-
ply such supervised or thesaurus-based methods to
identify opinion targets. Moreover, in our method
we directly use associations between targets and
opinions to extract both.
</bodyText>
<subsectionHeader confidence="0.999761">
2.4 Sentiment retrieval
</subsectionHeader>
<bodyText confidence="0.999925631578948">
At TREC, the Text REtrieval Conference, there
has been interest in a specific type of sentiment
analysis: opinion retrieval. This interest materi-
alized in 2006 (Ounis et al., 2007), with the opin-
ionated blog post retrieval task. Finding blog posts
that are not just about a topic, but also contain an
opinion on the topic, proves to be a difficult task.
Performance on the opinion-finding task is domi-
nated by performance on the underlying document
retrieval task (the topical baseline).
Opinion finding is often approached as a two-
stage problem: (1) identify documents relevant to
the query, (2) identify opinions. In stage (2) one
commonly uses either a binary classifier to distin-
guish between opinionated and non-opinionated
documents or applies reranking of the initial result
list using some opinion score. Opinion add-ons
show only slight improvements over relevance-
only baselines.
The best performing opinion finding system at
TREC 2008 is a two-stage approach using rerank-
ing in stage (2) (Lee et al., 2008). The authors
use SentiWordNet and a corpus-derived lexicon
to construct an opinion score for each post in an
initial ranking of blog posts. This opinion score
is combined with the relevance score, and posts
are reranked according to this new score. We de-
tail this approach in Section 6. Later, the authors
use domain-specific opinion indicators (Na et al.,
2009), like “interesting story” (movie review), and
“light” (notebook review). This domain-specific
lexicon is constructed using feedback-style learn-
ing: retrieve an initial list of documents and use
the top documents as training data to learn an opin-
ion lexicon. Opinion scores per document are then
computed as an average of opinion scores over
all its words. Results show slight improvements
(+3%) on mean average precision.
</bodyText>
<sectionHeader confidence="0.947314" genericHeader="method">
3 Generating Topic-Specific Lexicons
</sectionHeader>
<bodyText confidence="0.998831333333333">
In this section we describe how we generate a lex-
icon of subjectivity clues and targets for a given
topic and a list of relevant documents (e.g., re-
</bodyText>
<page confidence="0.992953">
587
</page>
<figureCaption confidence="0.89475175">
Topic-specific lexicon of tuples:
(syntactic clue, target)
Figure 1: Our method for learning a topic-
dependent subjectivity lexicon.
</figureCaption>
<bodyText confidence="0.998517933333333">
trieved by a search engine for the topic). As an ad-
ditional resource, we use a large background cor-
pus of text documents of a similar style but with
diverse subjects; we assume that the relevant doc-
uments are part of this corpus as well. As the back-
ground corpus, we used the set of documents from
the assessment pools of TREC 2006–2008 opin-
ion retrieval tasks (described in detail in section 4).
We use the Stanford lexicalized parser1 to extract
labeled dependency triples (head, label, modifier).
In the extracted triples, all words indicate their cat-
egory (noun, adjective, verb, adverb, etc.) and are
normalized to lemmas.
Figure 1 provides an overview of our method;
below we describe it in more detail.
</bodyText>
<subsectionHeader confidence="0.999016">
3.1 Step 1: Extracting syntactic contexts
</subsectionHeader>
<bodyText confidence="0.99983575">
We start with a general domain-independent prior
polarity lexicon of 8,821 clue words (Wilson et al.,
2005). First, we identify syntactic contexts in
which specific clue words can be used to express
</bodyText>
<footnote confidence="0.804136">
1http://nlp.stanford.edu/software/
lex-parser.shtml
</footnote>
<bodyText confidence="0.9998959">
attitude: we try to find how a clue word can be syn-
tactically linked to targets of sentiments. We take a
simple definition of the syntactic context: a single
labeled directed dependency relation. For every
clue word, we extract all syntactic contexts, i.e.,
all dependencies, in which the word is involved
(as head or as modifier) in the background corpus,
along with their endpoints. Table 1 shows exam-
ples of clue words and contexts that indicate sen-
timents. For every clue, we only select those con-
texts that exhibit a high entropy among the lemmas
at the other endpoint of the dependencies. E.g.,
in our background corpus, the verb to like occurs
97,179 times with a nominal subject and 52,904
times with a direct object; however, the entropy of
lemmas of the subjects is 4.33, compared to 9.56
for the direct objects. In other words, subjects of
like are more “predictable.” Indeed, the pronoun
I accounts for 50% of subjects, followed by you
(14%), they (4%), we (4%) and people (2%). The
most frequent objects of like are it (12%), what
(4%), idea (2%), they (2%). Thus, objects of to
like will be preferred by the method.
Our entropy-driven selection of syntactic con-
texts of a clue word is based on the following as-
sumption:
Assumption 1: In text, targets of sentiments
are more diverse than sources of sentiments
or other accompanying attributes such as lo-
cation, time, manner, etc. Therefore targets
exhibit higher entropy than other attributes.
For every clue word, we select the top D syntac-
tic contexts whose entropy is at least half of the
maximum entropy for this clue.
To summarize, at the end of Step 1 of our
method, we have extracted a list of pairs (clue
word, syntactic context) such that for occurrences
of the clue word, the words at the endpoint of the
syntactic dependency are likely to be targets of
sentiments. We call such a pair a syntactic clue.
</bodyText>
<subsectionHeader confidence="0.999587">
3.2 Step 2: Selecting potential targets
</subsectionHeader>
<bodyText confidence="0.999821875">
Here, we use the extracted syntantic clues to iden-
tify words that are likely to serve as specific tar-
gets for opinions about the topic in the relevant
documents. In this work we only consider individ-
ual words as potential targets and leave exploring
other options (e.g., NPs and VPs as targets) for fu-
ture work. In extracting targets, we rely on the
following assumption:
</bodyText>
<figure confidence="0.998306324324324">
Background
corpus
Topic-independent
subjectivity lexicon
Extract all
syntactic contexts
of clue words
For each clue
word, select D
contexts with
highest entropy
Step 1
Step 2
Step 3
Potential targets in
background corpus
Extract all
occurrences
endpoints of
syntactic clues
List of syntactic clues:
(clue word, syn. context)
Compare frequencies
using chi-square;
select top T targets
For each target,
find syn. clues it
co-occurs with
List of T targets
Potential targets in
relevant doc. list
Extract all
occurrences
endpoints of
syntactic clues
Relevant docs
Topic
</figure>
<page confidence="0.957325">
588
</page>
<bodyText confidence="0.948503714285714">
Clue word Syntactic context Target Example
has direct object
has clausal complement
has about-modifier
is adjectival modifier of
has nominal subject
has clausal complement
</bodyText>
<figure confidence="0.994193">
I do still like U2 very much
I don’t like to criticize our intelligence services
That’s what I like about Winter Olympics
it’s a terrible idea to recall judges for...
And Neil, that shirt is terrible!
It is terrible that a small group of extremists can ...
to like
to like
to like
terrible
terrible
terrible
u2
criticize
olympics
idea
shirt
can
</figure>
<tableCaption confidence="0.984473">
Table 1: Examples of subjective syntactic contexts of clue words (based on Stanford dependencies).
</tableCaption>
<bodyText confidence="0.986654137931034">
Assumption 2: The list of relevant documents
contains a substantial number of documents
on the topic which, moreover, contain senti-
ments about the topic.
We extract all endpoints of all occurrences of the
syntactic clues in the relevant documents, as well
as in the background corpus. To identify potential
attitude targets in the relevant documents, we com-
pare their frequency in the relevant documents to
the frequency in the background corpus using the
standard x2 statistics. This technique is based on
the following assumption:
Assumption 3: Sentiment targets related to
the topic occur more often in subjective con-
text in the set of relevant documents, than
in the background corpus. In other words,
while the background corpus contains senti-
ments towards very diverse subjects, the rel-
evant documents tend to express attitudes re-
lated to the topic.
For every potential target, we compute the x2-
score and select the top T highest scoring targets.
As the result of Steps 1 and 2, as candidate tar-
gets for a given topic, we only select words that oc-
cur in subjective contexts, and that do so more of-
ten than we would normally expect. Table 2 shows
examples of extracted targets for three TREC top-
ics (see below for a description of our experimen-
tal data).
</bodyText>
<subsectionHeader confidence="0.994284">
3.3 Step 3: Generating topic-specific lexicons
</subsectionHeader>
<bodyText confidence="0.999660777777778">
In the last step of the method, we combine clues
and targets. For each target identified in Step 2,
we take all syntactic clues extracted in Step 1 that
co-occur with the target in the relevant documents.
The resulting list of triples (clue word, syntactic
context, target) constitute the lexicon. We conjec-
ture that an occurrence of a lexicon entry in a text
indicates, with reasonable confidence, a subjective
attitude towards the target.
</bodyText>
<figure confidence="0.742869285714286">
Topic “Relationship between Abramoff and Bush”
abramoff lobbyist scandal fundraiser bush fund-raiser re-
publican prosecutor tribe swirl corrupt corruption norquist
democrat lobbying investigation scanlon reid lawmaker
dealings president
Topic “MacBook Pro”
macbook laptop powerbook connector mac processor note-
book fw800 spec firewire imac pro machine apple power-
books ibook ghz g4 ata binary keynote drive modem
Topic: “Super Bowl ads”
ad bowl commercial fridge caveman xl endorsement adver-
tising spot advertiser game super essential celebrity payoff
marketing publicity brand advertise watch viewer tv football
venue
</figure>
<tableCaption confidence="0.974927">
Table 2: Examples of targets extracted at Step 2.
</tableCaption>
<sectionHeader confidence="0.952963" genericHeader="method">
4 Data and Experimental Setup
</sectionHeader>
<bodyText confidence="0.999949041666667">
We consider two types of evaluation. In the next
section, we examine the quality of the lexicons
we generate. In the section after that we evaluate
lexicons quantitatively using the TREC Blog track
benchmark.
For extrinsic evaluation we apply our lexi-
con generation method to a collection of doc-
uments containing opinionated utterances: blog
posts. The Blogs06 collection (Macdonald and
Ounis, 2006) is a crawl of blog posts from 100,649
blogs over a period of 11 weeks (06/12/2005–
21/02/2006), with 3,215,171 posts in total. Be-
fore indexing the collection, we perform two pre-
processing steps: (i) when extracting plain text
from HTML, we only keep block-level elements
longer than 15 words (to remove boilerplate mate-
rial), and (ii) we remove non-English posts using
TextCat2 for language detection. This leaves us
with 2,574,356 posts with 506 words per post on
average. We index the collection using Indri,3 ver-
sion 2.10.
TREC 2006–2008 came with the task of opin-
ionated blog post retrieval (Ounis et al., 2007).
For each year a set of 50 topics was created, giv-
</bodyText>
<footnote confidence="0.999544666666667">
2http://odur.let.rug.nl/—vannoord/
TextCat/
3http://www.lemurproject.org/indri/
</footnote>
<page confidence="0.998657">
589
</page>
<bodyText confidence="0.999941764705882">
ing us 150 topics in total. Every topic comes with
a set of relevance judgments: Given a topic, a blog
post can be either (i) nonrelevant, (ii) relevant, but
not opinionated, or (iii) relevant and opinionated.
TREC topics consist of three fields (title, descrip-
tion, and narrative), of which we only use the title
field: a query of 1–3 keywords.
We use standard TREC evaluation measures for
opinion retrieval: MAP (mean average precision),
R-precision (precision within the top R retrieved
documents, where R is the number of known rel-
evant documents in the collection), MRR (mean
reciprocal rank), P@10 and P@100 (precision
within the top 10 and 100 retrieved documents).
In the context of media analysis, recall-oriented
measures such as MAP and R-precision are more
meaningful than the other, early precision-oriented
measures. Note that for the opinion retrieval task
a document is considered relevant if it is on topic
and contains opinions or sentiments towards the
topic.
Throughout Section 6 below, we test for signif-
icant differences using a two-tailed paired t-test,
and report on significant differences for α = 0.01
(•
and &apos;), and α = 0.05 (° and °).
For the quantative experiments in Section 6 we
need a topical baseline: a set of blog posts po-
tentially relevant to each topic. For this, we use
the Indri retrieval engine, and apply the Markov
Random Fields to model term dependencies in the
query (Metzler and Croft, 2005) to improve topi-
cal retrieval. We retrieve the top 1,000 posts for
each query.
</bodyText>
<sectionHeader confidence="0.99009" genericHeader="method">
5 Qualitative Analysis of Lexicons
</sectionHeader>
<bodyText confidence="0.9877653125">
Lexicon size (the number of entries) and selectiv-
ity (how often entries match in text) of the gen-
erated lexicons vary depending on the parame-
ters D and T introduced above. The two right-
most columns of Table 4 show the lexicon size
and the average number of matches per topic. Be-
cause our topic-specific lexicons consist of triples
(clue word, syntactic context, target), they actu-
ally contain more words than topic-independent
lexicons of the same size, but topic-specific en-
tries are more selective, which makes the lexicon
more focused. Table 3 compares the application
of topic-independent and topic-specific lexicons to
on-topic blog text.
We manually performed an explorative error
analysis on a small number of documents, anno-
There are some tragic mo- There are some tragic mo-
ments like eggs freezing , ments l ike eggs freezing ,
and predators snatching the and predators snatching the
females and little ones-you females and little ones-you
know the whole NATURE know the whole NATURE
thing ... but this movie is thing ... but this movie is
awesome awesome
Saturday was more errands, Saturday was more errands,
then spent the evening with then spent the evening with
Dad and Stepmum, and fi- Dad and Stepmum, and fi-
nally was able to see March nally was able to see March
of the Penguins, which of the Penguins, which
was wonderful. Christmas was wonderful. Christmas
Day was lovely, surrounded Day was lovely, surrounded
by family, good food and by family, good food and
drink, and little L to play drink, and little L to play
</bodyText>
<tableCaption confidence="0.707527666666667">
with. with.
Table 3: Posts with highlighted targets (bold) and
subjectivity clues (blue) using topic-independent
</tableCaption>
<listItem confidence="0.911654846153846">
(left) and topic-specific (right) lexicons.
tated using the smallest lexicon in Table 4 for the
topic “March of the Pinguins.” We assigned 186
matches of lexicon entries in 30 documents into
four classes:
• REL: sentiment towards a relevant target;
• CONTEXT: sentiment towards a target that
is irrelevant to the topic due to context (e.g.,
opinion about a target “film”, but refering to
a film different from the topic);
• IRREL: sentiment towards irrelevant target
(e.g., “game” for a topic about a movie);
• NOSENT: no sentiment at all
</listItem>
<bodyText confidence="0.9998504">
In total only 8% of matches were manually clas-
sified as REL, with 62% classified as NOSENT,
23% as CONTEXT, and 6% as IRREL. On the
other hand, among documents assessed as opio-
nionated by TREC assessors, only 13% did not
contain matches of the lexicon entries, compared
to 27% of non-opinionated documents, which
does indicate that our lexicon does attempt to sep-
arate non-opinionated documents from opinion-
ated.
</bodyText>
<sectionHeader confidence="0.983149" genericHeader="method">
6 Quantitative Evaluation of Lexicons
</sectionHeader>
<bodyText confidence="0.9999295">
In this section we assess the quality of the gen-
erated topic-specific lexicons numerically and ex-
trinsically. To this end we deploy our lexicons to
the task of opinionated blog post retrieval (Ounis
et al., 2007). A commonly used approach to this
task works in two stages: (1) identify topically rel-
evant blog posts, and (2) classify these posts as
being opinionated or not. In stage 2 the standard
</bodyText>
<page confidence="0.990681">
590
</page>
<bodyText confidence="0.999958866666667">
approach is to rerank the results from stage 1, in-
stead of doing actual binary classification. We take
this approach, as it has shown good performance
in the past TREC editions (Ounis et al., 2007) and
is fairly straightforward to implement. We also ex-
plore another way of using the lexicon: as a source
for query expansion (i.e., adding new terms to the
original query) in Section 6.2. For all experiments
we use the collection described in Section 4.
Our experiments have two goals: to compare
the use of topic-independent and topic-specific
lexicons for the opinionated post retrieval task,
and to examine how different settings for the pa-
rameters of the lexicon generation affect the em-
pirical quality.
</bodyText>
<subsectionHeader confidence="0.999866">
6.1 Reranking using a lexicon
</subsectionHeader>
<bodyText confidence="0.999931181818182">
To rerank a list of posts retrieved for a given topic,
we opt to use the method that showed best per-
formance at TREC 2008. The approach taken
by Lee et al. (2008) linearly combines a (top-
ical) relevance score with an opinion score for
each post. For the opinion score, terms from a
(topic-independent) lexicon are matched against
the post content, and weighted with the probability
of term’s subjectivity. Finally, the sum is normal-
ized using the Okapi BM25 framework. The final
opinion score Sop is computed as in Eq. 1:
</bodyText>
<equation confidence="0.994245">
Sop(D) =
Opinion(D) · (k1 + 1) ,(1)
Opinion(D) + k1 · (1 − b + b�|D|
avgdl)
</equation>
<bodyText confidence="0.997676">
where k1, and b are Okapi parameters (set to their
default values k1 = 2.0, and b = 0.75), |D |is the
length of document D, and avgdl is the average
document length in the collection. The opinion
score Opinion(D) is calculated using Eq. 2:
</bodyText>
<equation confidence="0.997878">
Opinion(D) = � P(sub|w) · n(w, D), (2)
wEO
</equation>
<bodyText confidence="0.9826961">
where O is the set of terms in the sentiment lex-
icon, P(sub|w) indicates the probability of term
w being subjective, and n(w, D) is the number of
times term w occurs in document D. The opinion
scoring can weigh lexicon terms differently, using
P(sub|w); it normalizes scores to cancel out the
effect of varying document sizes.
In our experiments we use the method de-
scribed above, and plug in the MPQA polarity
lexicon.4 We compare the results of using this
</bodyText>
<footnote confidence="0.874858">
4http://www.cs.pitt.edu/mpqa/
</footnote>
<bodyText confidence="0.9993121">
topic-independent lexicon to the topic-dependent
lexicons our method generates, which are also
plugged into the reranking of Lee et al. (2008).
In addition to using Okapi BM25 for opinion
scoring, we also consider a simpler method. As
we observed in Section 5, our topic-specific lexi-
cons are more selective than the topic-independent
lexicon, and a simple number of lexicon matches
can give a good indication of opinionatedness of a
document:
</bodyText>
<equation confidence="0.98611">
Sop(D) = min(n(O, D), 10)/10, (3)
</equation>
<bodyText confidence="0.999927">
where n(O, D) is the number of matches of the
term of sentiment lexicon O in document D.
</bodyText>
<sectionHeader confidence="0.463084" genericHeader="evaluation">
6.1.1 Results and observations
</sectionHeader>
<bodyText confidence="0.999545714285714">
There are several parameters that we can vary
when generating a topic-specific lexicon and when
using it for reranking:
D: the number of syntactic contexts per clue
T: the number of extracted targets
Sop(D): the opinion scoring function.
a: the weight of the opinion score in the linear
combination with the relevance score.
Note that a does not affect the lexicon creation,
but only how the lexicon is used in reranking.
Since we want to assess the quality of lexicons,
not in the opinionated retrieval performance as
such, we factor out a by selecting the best setting
for each lexicon (including the topic-independent)
and each evaluation measure.
In Table 4 we present the results of evaluation
of several lexicons in the context of opinionated
blog post retrieval.
First, we note that reranking using all lexi-
cons in Table 4 significantly improves over the
relevance-only baseline for all evaluation mea-
sures. When comparing topic-specific lexicons to
the topic-independent one, most of the differences
are not statistically significant, which is surpris-
ing given the fact that most topic-specific lexicons
we evaluated are substantially smaller (see the two
rightmost columns in the table). The smallest lex-
icon in Table 4 is seven times more selective than
the general one, in terms of the number of lexicon
matches per document.
The only evaluation measure where the topic-
independent lexicon consistently outperforms
topic-specific ones, is Mean Reciprocal Rank that
depends on a single relevant opinionated docu-
ment high in a ranking. A possible explanation
</bodyText>
<page confidence="0.99252">
591
</page>
<table confidence="0.999628">
Lexicon MAP R-prec MRR P@10 P@100 Ilexiconj hits per doc
no reranking 0.2966 0.3556 0.6750 0.4820 0.3666 — —
topic-independent 0.3182 0.3776 0.7714 0.5607 0.3980 8,221 36.17
D T S-V
3 50 count 0.3191 0.3769 0.7276° 0.5547 0.3963 2,327 5.02
3 100 count 0.3191 0.3777 0.7416 0.5573 0.3971 3,977 8.58
5 50 count 0.3178 0.3775 0.7246° 0.5560 0.3931 2,784 5.73
5 100 count 0.3178 0.3784 0.7316° 0.5513 0.3961 4,910 10.06
all 50 count 0.3167 0.3753 0.7264° 0.5520 0.3957 4,505 9.34
all 100 count 0.3146 0.3761 0.7283° 0.5347° 0.3955 8,217 16.72
all 50 okapi 0.3129 0.3713 0.7247• 0.5333° 0.3833° 4,505 9.34
all 100 okapi 0.3189 0.3755 0.7162• 0.5473 0.3921 8,217 16.72
all 200 okapi 0.3229&apos; 0.3803 0.7389 0.5547 0.3987 14,581 29.14
</table>
<tableCaption confidence="0.99552">
Table 4: Evaluation of topic-specific lexicons applied to the opinion retrieval task, compared to the topic-
</tableCaption>
<bodyText confidence="0.99462536">
independent lexicon. The two rightmost columns show the number of lexicon entries (average per topic)
and the number of matches of lexicon entries in blog posts (average for top 1,000 posts).
is that the large general lexicon easily finds a few
“obviously subjective” posts (those with heavily
used subjective words), but is not better at detect-
ing less obvious ones, as indicated by the recall-
oriented MAP and R-precision.
Interestingly, increasing the number of syntac-
tic contexts considered for a clue word (parame-
ter D) and the number of selected targets (param-
eter T) leads to substantially larger lexicons, but
only gives marginal improvements when lexicons
are used for opinion retrieval. This shows that our
bootstrapping method is effective at filtering out
non-relevant sentiment targets and syntactic clues.
The evaluation results also show that the choice
of opinion scoring function (Okapi or raw counts)
depends on the lexicon size: for smaller, more fo-
cused lexicons unnormalized counts are more ef-
fective. This also confirms our intuition that for
small, focused lexicons simple presence of a sen-
timent clue in text is a good indication of subjec-
tivity, while for larger lexicons an overall subjec-
tivity scoring of texts has to be used, which can be
hard to interpret for (media analysis) users.
</bodyText>
<subsectionHeader confidence="0.993086">
6.2 Query expansion with lexicons
</subsectionHeader>
<bodyText confidence="0.999539454545455">
In this section we evaluate the quality of targets
extracted as part of the lexicons by using them for
query expansion. Query expansion is a commonly
used technique in information retrieval, aimed at
getting a better representation of the user’s in-
formation need by adding terms to the original
retrieval query; for user-generated content, se-
lective query expansion has proved very benefi-
cial (Weerkamp et al., 2009). We hypothesize that
if our method manages to identify targets that cor-
respond to issues, subtopics or features associated
</bodyText>
<table confidence="0.990415888888889">
Run MAP P@10 MRR
Topical blog post retrieval
Baseline 0.4086 0.7053 0.7984
Rel. models 0.4017° 0.6867 0.7383•
Subj. targets 0.4190° 0.7373° 0.8470°
Opinion retrieval
Baseline 0.2966 0.4820 0.6750
Rel. models 0.2841• 0.4467• 0.5479•
Subj. targets 0.3075 0.5227&apos; 0.7196
</table>
<tableCaption confidence="0.979114">
Table 5: Query expansion using relevance mod-
</tableCaption>
<bodyText confidence="0.988313541666667">
els and topic-specific subjectivity targets. Signifi-
cance tested against the baseline.
with the topic, the extracted targets should be good
candidates for query expansion. The experiments
described below test this hypothesis.
For every test topic, we select the 20 top-scoring
targets as expansion terms, and use Indri to re-
turn 1,000 most relevant documents for the ex-
panded query. We evaluate the resulting ranking
using both topical retrieval and opinionated re-
trieval measures. For the sake of comparison, we
also implemented a well-known query expansion
method based on Relevance Models (Lavrenko
and Croft, 2001): this method has been shown to
work well in many settings. Table 5 shows evalu-
ation results for these two query expansion meth-
ods, compared to the baseline retrieval run.
The results show that on topical retrieval query
expansion using targets significantly improves re-
trieval performance, while using relevance mod-
els actually hurts all evaluation measures. The
failure of the latter expansion method can be at-
tributed to the relatively large amount of noise
in user-generated content, such as boilerplate
</bodyText>
<page confidence="0.991894">
592
</page>
<bodyText confidence="0.9999115625">
material, timestamps of blog posts, comments
etc. (Weerkamp and de Rijke, 2008). Our method
uses full syntactic parsing of the retrieved doc-
uments, which might substantially reduce the
amount of noise since only (relatively) well-
formed English sentences are used in lexicon gen-
eration.
For opinionated retrieval, target-based expan-
sion also improves over the baseline, although the
differences are only significant for P@10. The
consistent improvement for topical retrieval sug-
gests that a topic-specific lexicon can be used both
for query expansion (as described in this section)
and for opinion reranking (as described in Sec-
tion 6.1). We leave this combination for future
work.
</bodyText>
<sectionHeader confidence="0.998051" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99999072">
We have described a bootstrapping method for de-
riving a topic-specific lexicon from a general pur-
pose polarity lexicon. We have evaluated the qual-
ity of generated lexicons both manually and using
a TREC Blog track test set for opinionated blog
post retrieval. Although the generated lexicons
can be an order of magnitude more selective, they
maintain, or even improve, the performance of an
opinion retrieval system.
As to future work, we intend to combine our
method with known methods for topic-specific
lexicon expansion (our method is rather concerned
with lexicon “restriction”). Existing sentence-
or phrase-level (trained) sentiment classifiers can
also be used easily: when collecting/counting tar-
gets we can weigh them by “prior” score provided
by such classifiers. We also want to look at more
complex syntactic patterns: Choi et al. (2009) re-
port that many errors are due to exclusive use of
unigrams. We would also like to extend poten-
tial opinion targets to include multi-word phrases
(NPs and VPs), in addition to individual words.
Finally, we do not identify polarity yet: this can
be partially inherited from the initial lexicon and
refined automatically via bootstrapping.
</bodyText>
<sectionHeader confidence="0.997514" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999877636363636">
This research was supported by the European
Union’s ICT Policy Support Programme as part
of the Competitiveness and Innovation Framework
Programme, CIP ICT-PSP under grant agreement
nr 250430, by the DuOMAn project carried out
within the STEVIN programme which is funded
by the Dutch and Flemish Governments under
project nr STE-09-12, and by the Netherlands Or-
ganisation for Scientific Research (NWO) under
project nrs 612.066.512, 612.061.814, 612.061.-
815, 640.004.802.
</bodyText>
<sectionHeader confidence="0.996331" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999197966666667">
Altheide, D. (1996). Qualitative Media Analysis. Sage.
Choi, Y., Kim, Y., and Myaeng, S.-H. (2009). Domain-
specific sentiment analysis using contextual feature gen-
eration. In TSA ’09: Proceeding of the 1st international
CIKM workshop on Topic-sentiment analysis for mass
opinion, pages 37–44, New York, NY, USA. ACM.
Fahrni, A. and Klenner, M. (2008). Old Wine or Warm
Beer: Target-Specific Sentiment Analysis of Adjectives.
In Proc.of the Symposium on Affective Language in Hu-
man and Machine, AISB 2008 Convention, 1st-2nd April
2008. University of Aberdeen, Aberdeen, Scotland, pages
60 – 63.
Godbole, N., Srinivasaiah, M., and Skiena, S. (2007). Large-
scale sentiment analysis for news and blogs. In Proceed-
ings of the International Conference on Weblogs and So-
cial Media (ICWSM).
Kanayama, H. and Nasukawa, T. (2006). Fully automatic lex-
icon expansion for domain-oriented sentiment analysis. In
EMNLP ’06: Proceedings of the 2006 Conference on Em-
pirical Methods in Natural Language Processing, pages
355–363, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kim, S. and Hovy, E. (2004). Determining the sentiment of
opinions. In Proceedings of COLING 2004.
Lavrenko, V. and Croft, B. (2001). Relevance-based language
models. In SIGIR ’01: Proceedings of the 24th annual
international ACM SIGIR conference on research and de-
velopment in information retrieval.
Lee, Y., Na, S.-H., Kim, J., Nam, S.-H., Jung, H.-Y., and Lee,
J.-H. (2008). KLE at TREC 2008 Blog Track: Blog Post
and Feed Retrieval. In Proceedings of TREC 2008.
Liu, B., Hu, M., and Cheng, J. (2005). Opinion observer: an-
alyzing and comparing opinions on the web. In Proceed-
ings of the 14th international conference on World Wide
Web.
Macdonald, C. and Ounis, I. (2006). The TREC Blogs06
collection: Creating and analysing a blog test collection.
Technical Report TR-2006-224, Department of Computer
Science, University of Glasgow.
Metzler, D. and Croft, W. B. (2005). A markov random feld
model for term dependencies. In SIGIR ’05: Proceed-
ings of the 28th annual international ACM SIGIR con-
ference on research and development in information re-
trieval, pages 472–479, New York, NY, USA. ACM Press.
Na, S.-H., Lee, Y., Nam, S.-H., and Lee, J.-H. (2009). Im-
proving opinion retrieval based on query-specific senti-
ment lexicon. In ECIR ’09: Proceedings of the 31th Eu-
ropean Conference on IR Research on Advances in In-
formation Retrieval, pages 734–738, Berlin, Heidelberg.
Springer-Verlag.
Ounis, I., Macdonald, C., de Rijke, M., Mishne, G., and
Soboroff, I. (2007). Overview of the TREC 2006 blog
track. In The Fifteenth Text REtrieval Conference (TREC
2006). NIST.
Popescu, A.-M. and Etzioni, O. (2005). Extracting prod-
uct features and opinions from reviews. In Proceedings
of Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP).
Riloff, E. and Wiebe, J. (2003). Learning extraction patterns
</reference>
<page confidence="0.988748">
593
</page>
<reference confidence="0.999620730769231">
for subjective expressions. In Proceedings of the 2003
Conference on Empirical methods in Natural Language
Processing (EMNLP).
Weerkamp, W., Balog, K., and de Rijke, M. (2009). A gener-
ative blog post retrieval model that uses query expansion
based on external collections. In Joint conference of the
47th Annual Meeting of the Association for Computational
Linguistics and the 4th International Joint Conference on
Natural Language Processing of the Asian Federation of
Natural Language Processing (ACL-ICNLP 2009), Singa-
pore.
Weerkamp, W. and de Rijke, M. (2008). Credibility im-
proves topical blog post retrieval. In Proceedings of ACL-
08: HLT, page 923931, Columbus, Ohio. Association
for Computational Linguistics, Association for Computa-
tional Linguistics.
Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Recognizing
contextual polarity in phrase-level sentiment analysis. In
HLT ’05: Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natural Lan-
guage Processing, pages 347–354, Morristown, NJ, USA.
Association for Computational Linguistics.
Wilson, T., Wiebe, J., and Hoffmann, P. (2009). Recog-
nizing contextual polarity: an exploration of features for
phrase-level sentiment analysis. Computational Linguis-
tics, 35(3):399–433.
</reference>
<page confidence="0.998468">
594
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.610650">
<title confidence="0.999981">Generating Focused Topic-specific Sentiment Lexicons</title>
<author confidence="0.991148">Valentin Jijkoun Maarten de_Rijke Wouter Weerkamp</author>
<affiliation confidence="0.639205">ISLA, University of Amsterdam, The Netherlands</affiliation>
<email confidence="0.996144">jijkoun,derijke,w.weerkamp@uva.nl</email>
<abstract confidence="0.9980861">We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opinion retrieval system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Altheide</author>
</authors>
<title>Qualitative Media Analysis.</title>
<date>1996</date>
<publisher>Sage.</publisher>
<contexts>
<context position="1889" citStr="Altheide, 1996" startWordPosition="277" endWordPosition="278"> given a topic and list of documents (discussing the topic), find all instances of attitudes toward the topic (e.g., positive/negative sentiments, or, if the topic is an organization or person, support/criticism of this entity). For every such instance, one should identify the source of the sentiment, the polarity and, possibly, subtopics that this attitude relates to (e.g., specific targets of criticism or support). Subsequently, a (human) media analyst must be able to aggregate the extracted information by source, polarity or subtopics, allowing him to build support/criticism networks etc. (Altheide, 1996). Recent advances in language technology, especially in sentiment analysis, promise to (partially) automate this task. Sentiment analysis is often considered in the context of the following two tasks: • sentiment extraction: given a set of textual documents, identify phrases, clauses, sentences or entire documents that express attitudes, and determine the polarity of these attitudes (Kim and Hovy, 2004); and • sentiment retrieval: given a topic (and possibly, a list of documents relevant to the topic), identify documents that express attitudes toward this topic (Ounis et al., 2007). How can te</context>
</contexts>
<marker>Altheide, 1996</marker>
<rawString>Altheide, D. (1996). Qualitative Media Analysis. Sage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Choi</author>
<author>Y Kim</author>
<author>S-H Myaeng</author>
</authors>
<title>Domainspecific sentiment analysis using contextual feature generation.</title>
<date>2009</date>
<booktitle>In TSA ’09: Proceeding of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion,</booktitle>
<pages>37--44</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8719" citStr="Choi et al. (2009)" startWordPosition="1354" endWordPosition="1357">for learning, for every imaginable domain or topic, automatic methods have been developed. Godbole et al. (2007) aim at measuring overall subjectivity or polarity towards a certain entity; they identify sentiments using domain-specific lexicons. The lexicons are generated from manually selected seeds for a broad domain such as Health or Business, following an approach similar to (Kim and Hovy, 2004). All named entites in a sentence containing a clue from a lexicon are 586 considered targets of sentiment for counting. Because of the data volume, no expensive linguistic processing is performed. Choi et al. (2009) advocate a joint topicsentiment analysis. They identify “sentiment topics,” noun phrases assumed to be linked to a sentiment clue in the same expression. They address two tasks: identifying sentiment clues, and classifying sentences into positive, negative, or neutral. They start by selecting initial clues from SentiWordNet, based on sentences with known polarity. Next, the sentiment topics are identified, and based on these sentiment topics and the current list of clues, new potential clues are extracted. The clues can be used to classifiy sentences. Fahrni and Klenner (2008) identify potent</context>
<context position="36039" citStr="Choi et al. (2009)" startWordPosition="5788" endWordPosition="5791">nionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective, they maintain, or even improve, the performance of an opinion retrieval system. As to future work, we intend to combine our method with known methods for topic-specific lexicon expansion (our method is rather concerned with lexicon “restriction”). Existing sentenceor phrase-level (trained) sentiment classifiers can also be used easily: when collecting/counting targets we can weigh them by “prior” score provided by such classifiers. We also want to look at more complex syntactic patterns: Choi et al. (2009) report that many errors are due to exclusive use of unigrams. We would also like to extend potential opinion targets to include multi-word phrases (NPs and VPs), in addition to individual words. Finally, we do not identify polarity yet: this can be partially inherited from the initial lexicon and refined automatically via bootstrapping. Acknowledgements This research was supported by the European Union’s ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme, CIP ICT-PSP under grant agreement nr 250430, by the DuOMAn project carried out within the STEVI</context>
</contexts>
<marker>Choi, Kim, Myaeng, 2009</marker>
<rawString>Choi, Y., Kim, Y., and Myaeng, S.-H. (2009). Domainspecific sentiment analysis using contextual feature generation. In TSA ’09: Proceeding of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion, pages 37–44, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fahrni</author>
<author>M Klenner</author>
</authors>
<title>Old Wine or Warm Beer: Target-Specific Sentiment Analysis of Adjectives.</title>
<date>2008</date>
<booktitle>In Proc.of the Symposium on Affective Language in Human and Machine, AISB 2008 Convention, 1st-2nd</booktitle>
<pages>60--63</pages>
<institution>University of Aberdeen,</institution>
<location>Aberdeen, Scotland,</location>
<contexts>
<context position="9303" citStr="Fahrni and Klenner (2008)" startWordPosition="1448" endWordPosition="1451">ocessing is performed. Choi et al. (2009) advocate a joint topicsentiment analysis. They identify “sentiment topics,” noun phrases assumed to be linked to a sentiment clue in the same expression. They address two tasks: identifying sentiment clues, and classifying sentences into positive, negative, or neutral. They start by selecting initial clues from SentiWordNet, based on sentences with known polarity. Next, the sentiment topics are identified, and based on these sentiment topics and the current list of clues, new potential clues are extracted. The clues can be used to classifiy sentences. Fahrni and Klenner (2008) identify potential targets in a given domain, and create a targetspecific polarity adjective lexicon. To this end, they find targets using Wikipedia, and associated adjectives. Next, the target-specific polarity of adjectives is detemined using Hearst-like patterns. Kanayama and Nasukawa (2006) introduce polar atoms: minimal human-understandable syntactic structures that specify polarity of clauses. The goal is to learn new domain-specific polar atoms, but these are not target-specific. They use manually-created syntactic patterns to identify atoms and coherency to determine polarity. In cont</context>
</contexts>
<marker>Fahrni, Klenner, 2008</marker>
<rawString>Fahrni, A. and Klenner, M. (2008). Old Wine or Warm Beer: Target-Specific Sentiment Analysis of Adjectives. In Proc.of the Symposium on Affective Language in Human and Machine, AISB 2008 Convention, 1st-2nd April 2008. University of Aberdeen, Aberdeen, Scotland, pages 60 – 63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Godbole</author>
<author>M Srinivasaiah</author>
<author>S Skiena</author>
</authors>
<title>Largescale sentiment analysis for news and blogs.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="8213" citStr="Godbole et al. (2007)" startWordPosition="1272" endWordPosition="1275">ier and a large unannotated corpus. In our method, we bootstrap from a subjectivity lexicion rather than a classifier, and perform a topicspecific analysis, learning indicators of subjectivity toward a specific topic. 2.2 Domain- and target-specific sentiment The way authors express their attitudes varies with the domain: An unpredictable movie can be positive, but unpredictable politicians are usually something negative. Since it is unrealistic to construct sentiment lexicons, or manually annotate text for learning, for every imaginable domain or topic, automatic methods have been developed. Godbole et al. (2007) aim at measuring overall subjectivity or polarity towards a certain entity; they identify sentiments using domain-specific lexicons. The lexicons are generated from manually selected seeds for a broad domain such as Health or Business, following an approach similar to (Kim and Hovy, 2004). All named entites in a sentence containing a clue from a lexicon are 586 considered targets of sentiment for counting. Because of the data volume, no expensive linguistic processing is performed. Choi et al. (2009) advocate a joint topicsentiment analysis. They identify “sentiment topics,” noun phrases assu</context>
</contexts>
<marker>Godbole, Srinivasaiah, Skiena, 2007</marker>
<rawString>Godbole, N., Srinivasaiah, M., and Skiena, S. (2007). Largescale sentiment analysis for news and blogs. In Proceedings of the International Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kanayama</author>
<author>T Nasukawa</author>
</authors>
<title>Fully automatic lexicon expansion for domain-oriented sentiment analysis.</title>
<date>2006</date>
<booktitle>In EMNLP ’06: Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>355--363</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9599" citStr="Kanayama and Nasukawa (2006)" startWordPosition="1490" endWordPosition="1493">ative, or neutral. They start by selecting initial clues from SentiWordNet, based on sentences with known polarity. Next, the sentiment topics are identified, and based on these sentiment topics and the current list of clues, new potential clues are extracted. The clues can be used to classifiy sentences. Fahrni and Klenner (2008) identify potential targets in a given domain, and create a targetspecific polarity adjective lexicon. To this end, they find targets using Wikipedia, and associated adjectives. Next, the target-specific polarity of adjectives is detemined using Hearst-like patterns. Kanayama and Nasukawa (2006) introduce polar atoms: minimal human-understandable syntactic structures that specify polarity of clauses. The goal is to learn new domain-specific polar atoms, but these are not target-specific. They use manually-created syntactic patterns to identify atoms and coherency to determine polarity. In contrast to much of the work in the literature, we need to specialize subjectivity lexicons not for a domain and target, but for “topics.” 2.3 Product features and opinions Much work has been carried out for the task of mining product reviews, where the goal is to identify features of specific produ</context>
</contexts>
<marker>Kanayama, Nasukawa, 2006</marker>
<rawString>Kanayama, H. and Nasukawa, T. (2006). Fully automatic lexicon expansion for domain-oriented sentiment analysis. In EMNLP ’06: Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 355–363, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>E Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="2295" citStr="Kim and Hovy, 2004" startWordPosition="336" endWordPosition="339">icism or support). Subsequently, a (human) media analyst must be able to aggregate the extracted information by source, polarity or subtopics, allowing him to build support/criticism networks etc. (Altheide, 1996). Recent advances in language technology, especially in sentiment analysis, promise to (partially) automate this task. Sentiment analysis is often considered in the context of the following two tasks: • sentiment extraction: given a set of textual documents, identify phrases, clauses, sentences or entire documents that express attitudes, and determine the polarity of these attitudes (Kim and Hovy, 2004); and • sentiment retrieval: given a topic (and possibly, a list of documents relevant to the topic), identify documents that express attitudes toward this topic (Ounis et al., 2007). How can technology developed for sentiment analysis be applied to media analysis? In order to use a sentiment extraction system for a media analysis problem, a system would have to be able to determine which of the extracted sentiments are actually relevant, i.e., it would not only have to identify specific targets of all extracted sentiments, but also decide which of the targets are relevant for the topic at han</context>
<context position="6342" citStr="Kim and Hovy (2004)" startWordPosition="982" endWordPosition="985">ocused, so that it can be used to actually pin-point subjectivity in documents relevant to a given topic. 2 Related Work Much work has been done in sentiment analysis. We discuss related work in four parts: sentiment analysis in general, domain- and targetspecific sentiment analysis, product review mining and sentiment retrieval. 2.1 Sentiment analysis Sentiment analysis is often seen as two separate steps for determining subjectivity and polarity. Most approaches first try to identify subjective units (documents, sentences), and for each of these determine whether it is positive or negative. Kim and Hovy (2004) select candidate sentiment sentences and use word-based sentiment classifiers to classify unseen words into a negative or positive class. First, the lexicon is constructed from WordNet: from several seed words, the structure of WordNet is used to expand this seed to a full lexicon. Next, this lexicon is used to measure the distance between unseen words and words in the positive and negative classes. Based on word sentiments, a decision is made at the sentence level. A similar approach is taken by Wilson et al. (2005): a classifier is learnt that distinguishes between polar and neutral sentenc</context>
<context position="8503" citStr="Kim and Hovy, 2004" startWordPosition="1318" endWordPosition="1321">titudes varies with the domain: An unpredictable movie can be positive, but unpredictable politicians are usually something negative. Since it is unrealistic to construct sentiment lexicons, or manually annotate text for learning, for every imaginable domain or topic, automatic methods have been developed. Godbole et al. (2007) aim at measuring overall subjectivity or polarity towards a certain entity; they identify sentiments using domain-specific lexicons. The lexicons are generated from manually selected seeds for a broad domain such as Health or Business, following an approach similar to (Kim and Hovy, 2004). All named entites in a sentence containing a clue from a lexicon are 586 considered targets of sentiment for counting. Because of the data volume, no expensive linguistic processing is performed. Choi et al. (2009) advocate a joint topicsentiment analysis. They identify “sentiment topics,” noun phrases assumed to be linked to a sentiment clue in the same expression. They address two tasks: identifying sentiment clues, and classifying sentences into positive, negative, or neutral. They start by selecting initial clues from SentiWordNet, based on sentences with known polarity. Next, the sentim</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Kim, S. and Hovy, E. (2004). Determining the sentiment of opinions. In Proceedings of COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lavrenko</author>
<author>B Croft</author>
</authors>
<title>Relevance-based language models.</title>
<date>2001</date>
<booktitle>In SIGIR ’01: Proceedings of the 24th annual international ACM SIGIR conference on research and development in information retrieval.</booktitle>
<contexts>
<context position="33968" citStr="Lavrenko and Croft, 2001" startWordPosition="5465" endWordPosition="5468">nce models and topic-specific subjectivity targets. Significance tested against the baseline. with the topic, the extracted targets should be good candidates for query expansion. The experiments described below test this hypothesis. For every test topic, we select the 20 top-scoring targets as expansion terms, and use Indri to return 1,000 most relevant documents for the expanded query. We evaluate the resulting ranking using both topical retrieval and opinionated retrieval measures. For the sake of comparison, we also implemented a well-known query expansion method based on Relevance Models (Lavrenko and Croft, 2001): this method has been shown to work well in many settings. Table 5 shows evaluation results for these two query expansion methods, compared to the baseline retrieval run. The results show that on topical retrieval query expansion using targets significantly improves retrieval performance, while using relevance models actually hurts all evaluation measures. The failure of the latter expansion method can be attributed to the relatively large amount of noise in user-generated content, such as boilerplate 592 material, timestamps of blog posts, comments etc. (Weerkamp and de Rijke, 2008). Our met</context>
</contexts>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>Lavrenko, V. and Croft, B. (2001). Relevance-based language models. In SIGIR ’01: Proceedings of the 24th annual international ACM SIGIR conference on research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lee</author>
<author>S-H Na</author>
<author>J Kim</author>
<author>S-H Nam</author>
<author>H-Y Jung</author>
<author>J-H Lee</author>
</authors>
<title>Blog Track: Blog Post and Feed Retrieval.</title>
<date>2008</date>
<journal>KLE at TREC</journal>
<booktitle>In Proceedings of TREC</booktitle>
<contexts>
<context position="12270" citStr="Lee et al., 2008" startWordPosition="1909" endWordPosition="1912">minated by performance on the underlying document retrieval task (the topical baseline). Opinion finding is often approached as a twostage problem: (1) identify documents relevant to the query, (2) identify opinions. In stage (2) one commonly uses either a binary classifier to distinguish between opinionated and non-opinionated documents or applies reranking of the initial result list using some opinion score. Opinion add-ons show only slight improvements over relevanceonly baselines. The best performing opinion finding system at TREC 2008 is a two-stage approach using reranking in stage (2) (Lee et al., 2008). The authors use SentiWordNet and a corpus-derived lexicon to construct an opinion score for each post in an initial ranking of blog posts. This opinion score is combined with the relevance score, and posts are reranked according to this new score. We detail this approach in Section 6. Later, the authors use domain-specific opinion indicators (Na et al., 2009), like “interesting story” (movie review), and “light” (notebook review). This domain-specific lexicon is constructed using feedback-style learning: retrieve an initial list of documents and use the top documents as training data to lear</context>
<context position="26943" citStr="Lee et al. (2008)" startWordPosition="4326" endWordPosition="4329"> lexicon: as a source for query expansion (i.e., adding new terms to the original query) in Section 6.2. For all experiments we use the collection described in Section 4. Our experiments have two goals: to compare the use of topic-independent and topic-specific lexicons for the opinionated post retrieval task, and to examine how different settings for the parameters of the lexicon generation affect the empirical quality. 6.1 Reranking using a lexicon To rerank a list of posts retrieved for a given topic, we opt to use the method that showed best performance at TREC 2008. The approach taken by Lee et al. (2008) linearly combines a (topical) relevance score with an opinion score for each post. For the opinion score, terms from a (topic-independent) lexicon are matched against the post content, and weighted with the probability of term’s subjectivity. Finally, the sum is normalized using the Okapi BM25 framework. The final opinion score Sop is computed as in Eq. 1: Sop(D) = Opinion(D) · (k1 + 1) ,(1) Opinion(D) + k1 · (1 − b + b�|D| avgdl) where k1, and b are Okapi parameters (set to their default values k1 = 2.0, and b = 0.75), |D |is the length of document D, and avgdl is the average document length</context>
<context position="28292" citStr="Lee et al. (2008)" startWordPosition="4560" endWordPosition="4563">e set of terms in the sentiment lexicon, P(sub|w) indicates the probability of term w being subjective, and n(w, D) is the number of times term w occurs in document D. The opinion scoring can weigh lexicon terms differently, using P(sub|w); it normalizes scores to cancel out the effect of varying document sizes. In our experiments we use the method described above, and plug in the MPQA polarity lexicon.4 We compare the results of using this 4http://www.cs.pitt.edu/mpqa/ topic-independent lexicon to the topic-dependent lexicons our method generates, which are also plugged into the reranking of Lee et al. (2008). In addition to using Okapi BM25 for opinion scoring, we also consider a simpler method. As we observed in Section 5, our topic-specific lexicons are more selective than the topic-independent lexicon, and a simple number of lexicon matches can give a good indication of opinionatedness of a document: Sop(D) = min(n(O, D), 10)/10, (3) where n(O, D) is the number of matches of the term of sentiment lexicon O in document D. 6.1.1 Results and observations There are several parameters that we can vary when generating a topic-specific lexicon and when using it for reranking: D: the number of syntact</context>
</contexts>
<marker>Lee, Na, Kim, Nam, Jung, Lee, 2008</marker>
<rawString>Lee, Y., Na, S.-H., Kim, J., Nam, S.-H., Jung, H.-Y., and Lee, J.-H. (2008). KLE at TREC 2008 Blog Track: Blog Post and Feed Retrieval. In Proceedings of TREC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>M Hu</author>
<author>J Cheng</author>
</authors>
<title>Opinion observer: analyzing and comparing opinions on the web.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th international conference on World Wide Web.</booktitle>
<contexts>
<context position="10338" citStr="Liu et al. (2005)" startWordPosition="1608" endWordPosition="1611"> learn new domain-specific polar atoms, but these are not target-specific. They use manually-created syntactic patterns to identify atoms and coherency to determine polarity. In contrast to much of the work in the literature, we need to specialize subjectivity lexicons not for a domain and target, but for “topics.” 2.3 Product features and opinions Much work has been carried out for the task of mining product reviews, where the goal is to identify features of specific products (such as picture, zoom, size, weight for digital cameras) and opinions about these specific features in user reviews. Liu et al. (2005) describe a system that identifies such features via rules learned from a manually annotated corpus of reviews; opinions on features are extracted from the structure of reviews (which explicitly separate positive and negative opinions). Popescu and Etzioni (2005) present a method that identifies product features for using corpus statistics, WordNet relations and morphological cues. Opinions about the features are extracted using a hand-crafted set of syntactic rules. Targets extracted in our method for a topic are similar to features extracted in review mining for products. However, topics in </context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>Liu, B., Hu, M., and Cheng, J. (2005). Opinion observer: analyzing and comparing opinions on the web. In Proceedings of the 14th international conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Macdonald</author>
<author>I Ounis</author>
</authors>
<title>The TREC Blogs06 collection: Creating and analysing a blog test collection.</title>
<date>2006</date>
<tech>Technical Report TR-2006-224,</tech>
<institution>Department of Computer Science, University of Glasgow.</institution>
<contexts>
<context position="20702" citStr="Macdonald and Ounis, 2006" startWordPosition="3284" endWordPosition="3287">l endorsement advertising spot advertiser game super essential celebrity payoff marketing publicity brand advertise watch viewer tv football venue Table 2: Examples of targets extracted at Step 2. 4 Data and Experimental Setup We consider two types of evaluation. In the next section, we examine the quality of the lexicons we generate. In the section after that we evaluate lexicons quantitatively using the TREC Blog track benchmark. For extrinsic evaluation we apply our lexicon generation method to a collection of documents containing opinionated utterances: blog posts. The Blogs06 collection (Macdonald and Ounis, 2006) is a crawl of blog posts from 100,649 blogs over a period of 11 weeks (06/12/2005– 21/02/2006), with 3,215,171 posts in total. Before indexing the collection, we perform two preprocessing steps: (i) when extracting plain text from HTML, we only keep block-level elements longer than 15 words (to remove boilerplate material), and (ii) we remove non-English posts using TextCat2 for language detection. This leaves us with 2,574,356 posts with 506 words per post on average. We index the collection using Indri,3 version 2.10. TREC 2006–2008 came with the task of opinionated blog post retrieval (Oun</context>
</contexts>
<marker>Macdonald, Ounis, 2006</marker>
<rawString>Macdonald, C. and Ounis, I. (2006). The TREC Blogs06 collection: Creating and analysing a blog test collection. Technical Report TR-2006-224, Department of Computer Science, University of Glasgow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Metzler</author>
<author>W B Croft</author>
</authors>
<title>A markov random feld model for term dependencies.</title>
<date>2005</date>
<booktitle>In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval,</booktitle>
<pages>472--479</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="22889" citStr="Metzler and Croft, 2005" startWordPosition="3641" endWordPosition="3644">oriented measures. Note that for the opinion retrieval task a document is considered relevant if it is on topic and contains opinions or sentiments towards the topic. Throughout Section 6 below, we test for significant differences using a two-tailed paired t-test, and report on significant differences for α = 0.01 (• and &apos;), and α = 0.05 (° and °). For the quantative experiments in Section 6 we need a topical baseline: a set of blog posts potentially relevant to each topic. For this, we use the Indri retrieval engine, and apply the Markov Random Fields to model term dependencies in the query (Metzler and Croft, 2005) to improve topical retrieval. We retrieve the top 1,000 posts for each query. 5 Qualitative Analysis of Lexicons Lexicon size (the number of entries) and selectivity (how often entries match in text) of the generated lexicons vary depending on the parameters D and T introduced above. The two rightmost columns of Table 4 show the lexicon size and the average number of matches per topic. Because our topic-specific lexicons consist of triples (clue word, syntactic context, target), they actually contain more words than topic-independent lexicons of the same size, but topic-specific entries are m</context>
</contexts>
<marker>Metzler, Croft, 2005</marker>
<rawString>Metzler, D. and Croft, W. B. (2005). A markov random feld model for term dependencies. In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval, pages 472–479, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-H Na</author>
<author>Y Lee</author>
<author>S-H Nam</author>
<author>J-H Lee</author>
</authors>
<title>Improving opinion retrieval based on query-specific sentiment lexicon.</title>
<date>2009</date>
<booktitle>In ECIR ’09: Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval,</booktitle>
<pages>734--738</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="12633" citStr="Na et al., 2009" startWordPosition="1969" endWordPosition="1972">the initial result list using some opinion score. Opinion add-ons show only slight improvements over relevanceonly baselines. The best performing opinion finding system at TREC 2008 is a two-stage approach using reranking in stage (2) (Lee et al., 2008). The authors use SentiWordNet and a corpus-derived lexicon to construct an opinion score for each post in an initial ranking of blog posts. This opinion score is combined with the relevance score, and posts are reranked according to this new score. We detail this approach in Section 6. Later, the authors use domain-specific opinion indicators (Na et al., 2009), like “interesting story” (movie review), and “light” (notebook review). This domain-specific lexicon is constructed using feedback-style learning: retrieve an initial list of documents and use the top documents as training data to learn an opinion lexicon. Opinion scores per document are then computed as an average of opinion scores over all its words. Results show slight improvements (+3%) on mean average precision. 3 Generating Topic-Specific Lexicons In this section we describe how we generate a lexicon of subjectivity clues and targets for a given topic and a list of relevant documents (</context>
</contexts>
<marker>Na, Lee, Nam, Lee, 2009</marker>
<rawString>Na, S.-H., Lee, Y., Nam, S.-H., and Lee, J.-H. (2009). Improving opinion retrieval based on query-specific sentiment lexicon. In ECIR ’09: Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval, pages 734–738, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Ounis</author>
<author>C Macdonald</author>
<author>M de Rijke</author>
<author>G Mishne</author>
<author>I Soboroff</author>
</authors>
<title>blog track.</title>
<date>2007</date>
<journal>Overview of the TREC</journal>
<booktitle>In The Fifteenth Text REtrieval Conference (TREC</booktitle>
<publisher>NIST.</publisher>
<marker>Ounis, Macdonald, de Rijke, Mishne, Soboroff, 2007</marker>
<rawString>Ounis, I., Macdonald, C., de Rijke, M., Mishne, G., and Soboroff, I. (2007). Overview of the TREC 2006 blog track. In The Fifteenth Text REtrieval Conference (TREC 2006). NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A-M Popescu</author>
<author>O Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP).</booktitle>
<contexts>
<context position="10601" citStr="Popescu and Etzioni (2005)" startWordPosition="1646" endWordPosition="1649">tivity lexicons not for a domain and target, but for “topics.” 2.3 Product features and opinions Much work has been carried out for the task of mining product reviews, where the goal is to identify features of specific products (such as picture, zoom, size, weight for digital cameras) and opinions about these specific features in user reviews. Liu et al. (2005) describe a system that identifies such features via rules learned from a manually annotated corpus of reviews; opinions on features are extracted from the structure of reviews (which explicitly separate positive and negative opinions). Popescu and Etzioni (2005) present a method that identifies product features for using corpus statistics, WordNet relations and morphological cues. Opinions about the features are extracted using a hand-crafted set of syntactic rules. Targets extracted in our method for a topic are similar to features extracted in review mining for products. However, topics in our setting go beyond concrete products, and the diversity and generality of possible topics makes it difficult to apply such supervised or thesaurus-based methods to identify opinion targets. Moreover, in our method we directly use associations between targets a</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Popescu, A.-M. and Etzioni, O. (2005). Extracting product features and opinions from reviews. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="7420" citStr="Riloff and Wiebe (2003)" startWordPosition="1157" endWordPosition="1160"> sentence level. A similar approach is taken by Wilson et al. (2005): a classifier is learnt that distinguishes between polar and neutral sentences, based on a prior polarity lexicon and an annotated corpus. Among the features used are syntactic features. After this initial step, the sentiment sentences are classified as negative or positive; again, a prior polarity lexicon and syntactic features are used. The authors later explored the difference between prior and contextual polarity (Wilson et al., 2009): words that lose polarity in context, or whose polarity is reversed because of context. Riloff and Wiebe (2003) describe a bootstrapping method to learn subjective extraction patterns that match specific syntactic templates, using a high-precision sentence-level subjectivity classifier and a large unannotated corpus. In our method, we bootstrap from a subjectivity lexicion rather than a classifier, and perform a topicspecific analysis, learning indicators of subjectivity toward a specific topic. 2.2 Domain- and target-specific sentiment The way authors express their attitudes varies with the domain: An unpredictable movie can be positive, but unpredictable politicians are usually something negative. Si</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Riloff, E. and Wiebe, J. (2003). Learning extraction patterns for subjective expressions. In Proceedings of the 2003 Conference on Empirical methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Weerkamp</author>
<author>K Balog</author>
<author>M de Rijke</author>
</authors>
<title>A generative blog post retrieval model that uses query expansion based on external collections.</title>
<date>2009</date>
<booktitle>In Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-ICNLP</booktitle>
<marker>Weerkamp, Balog, de Rijke, 2009</marker>
<rawString>Weerkamp, W., Balog, K., and de Rijke, M. (2009). A generative blog post retrieval model that uses query expansion based on external collections. In Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-ICNLP 2009), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Weerkamp</author>
<author>M de Rijke</author>
</authors>
<title>Credibility improves topical blog post retrieval.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT,</booktitle>
<pages>923931</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics, Association for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<marker>Weerkamp, de Rijke, 2008</marker>
<rawString>Weerkamp, W. and de Rijke, M. (2008). Credibility improves topical blog post retrieval. In Proceedings of ACL08: HLT, page 923931, Columbus, Ohio. Association for Computational Linguistics, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6865" citStr="Wilson et al. (2005)" startWordPosition="1071" endWordPosition="1074">ntences), and for each of these determine whether it is positive or negative. Kim and Hovy (2004) select candidate sentiment sentences and use word-based sentiment classifiers to classify unseen words into a negative or positive class. First, the lexicon is constructed from WordNet: from several seed words, the structure of WordNet is used to expand this seed to a full lexicon. Next, this lexicon is used to measure the distance between unseen words and words in the positive and negative classes. Based on word sentiments, a decision is made at the sentence level. A similar approach is taken by Wilson et al. (2005): a classifier is learnt that distinguishes between polar and neutral sentences, based on a prior polarity lexicon and an annotated corpus. Among the features used are syntactic features. After this initial step, the sentiment sentences are classified as negative or positive; again, a prior polarity lexicon and syntactic features are used. The authors later explored the difference between prior and contextual polarity (Wilson et al., 2009): words that lose polarity in context, or whose polarity is reversed because of context. Riloff and Wiebe (2003) describe a bootstrapping method to learn sub</context>
<context position="14236" citStr="Wilson et al., 2005" startWordPosition="2229" endWordPosition="2232">s the background corpus, we used the set of documents from the assessment pools of TREC 2006–2008 opinion retrieval tasks (described in detail in section 4). We use the Stanford lexicalized parser1 to extract labeled dependency triples (head, label, modifier). In the extracted triples, all words indicate their category (noun, adjective, verb, adverb, etc.) and are normalized to lemmas. Figure 1 provides an overview of our method; below we describe it in more detail. 3.1 Step 1: Extracting syntactic contexts We start with a general domain-independent prior polarity lexicon of 8,821 clue words (Wilson et al., 2005). First, we identify syntactic contexts in which specific clue words can be used to express 1http://nlp.stanford.edu/software/ lex-parser.shtml attitude: we try to find how a clue word can be syntactically linked to targets of sentiments. We take a simple definition of the syntactic context: a single labeled directed dependency relation. For every clue word, we extract all syntactic contexts, i.e., all dependencies, in which the word is involved (as head or as modifier) in the background corpus, along with their endpoints. Table 1 shows examples of clue words and contexts that indicate sentime</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Recognizing contextual polarity in phrase-level sentiment analysis. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: an exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="7308" citStr="Wilson et al., 2009" startWordPosition="1139" endWordPosition="1142">een words and words in the positive and negative classes. Based on word sentiments, a decision is made at the sentence level. A similar approach is taken by Wilson et al. (2005): a classifier is learnt that distinguishes between polar and neutral sentences, based on a prior polarity lexicon and an annotated corpus. Among the features used are syntactic features. After this initial step, the sentiment sentences are classified as negative or positive; again, a prior polarity lexicon and syntactic features are used. The authors later explored the difference between prior and contextual polarity (Wilson et al., 2009): words that lose polarity in context, or whose polarity is reversed because of context. Riloff and Wiebe (2003) describe a bootstrapping method to learn subjective extraction patterns that match specific syntactic templates, using a high-precision sentence-level subjectivity classifier and a large unannotated corpus. In our method, we bootstrap from a subjectivity lexicion rather than a classifier, and perform a topicspecific analysis, learning indicators of subjectivity toward a specific topic. 2.2 Domain- and target-specific sentiment The way authors express their attitudes varies with the </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Wilson, T., Wiebe, J., and Hoffmann, P. (2009). Recognizing contextual polarity: an exploration of features for phrase-level sentiment analysis. Computational Linguistics, 35(3):399–433.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>