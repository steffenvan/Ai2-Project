<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000633">
<title confidence="0.9908425">
On the Evaluation and Comparison of Taggers: the Effect of
Noise in Testing Corpora.
</title>
<note confidence="0.644407">
Lluis Padre, and Lluis Marquez
Dep. LSI. Technical University of Catalonia
c/ Jordi Girona 1-3. 08034 Barcelona
fpadro,lluism101si.upc.es
</note>
<sectionHeader confidence="0.980009" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909923076923">
This paper addresses the issue of POS tagger
evaluation. Such evaluation is usually per-
formed by comparing the tagger output with
a reference test corpus, which is assumed to be
error-free. Currently used corpora contain noise
which causes the obtained performance to be a
distortion of the real value. We analyze to what
extent this distortion may invalidate the com-
parison between taggers or the measure of the
improvement given by a new system. The main
conclusion is that a more rigorous testing exper-
imentation setting/designing is needed to reli-
ably evaluate and compare tagger accuracies.
</bodyText>
<sectionHeader confidence="0.983237" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.99841034920635">
Part of Speech (Pos) Tagging is a quite well
defined NLP problem, which consists of assign-
ing to each word in a text the proper mor-
phosyntactic tag for the given context. Al-
though many words are ambiguous regarding
their POS, in most cases they can be completely
disambiguated taking into account an adequate
context. Successful taggers have been built us-
ing several approaches, such as statistical tech-
niques, symbolic machine learning techniques,
neural networks, etc. The accuracy reported by
most current taggers ranges from 96-97% to al-
most 100% in the linguistically—motivated Con-
straint Grammar environment.
Unfortunately, there have been very few di-
rect comparisons of alternative taggers1 on iden-
tical test data. However, in most current papers
it is argued that the performance of some tag-
gers is better than others as a result of some
kind of indirect comparisons between them. We
One of the exceptions is the work by (Samuelsson
and Voutilainen, 1997), in which a very strict comparison
between taggers is performed.
think that there are a number of not enough
controlled/considered factors that make these
conclusions dubious in most cases.
In this direction, the present paper aims to
point out some of the difficulties arising when
evaluating and comparing tagger performances
against a reference test corpus, and to make
some criticism about common practices followed
by the NLP researchers in this issue.
The above mentioned factors can affect ei-
ther the evaluation or the comparison process.
Factors affecting the evaluation process are: (1)
Training and test experiments are usually per-
formed over noisy corpora which distorts the ob-
tained results, (2) performance figures are too
often calculated from only a single or very small
number of trials, though average results from
multiple trials are crucial to obtain reliable esti-
mations of accuracy (Mooney, 1996), (3) testing
experiments are usually done on corpora with
the same characteristics as the training data
—usually a small fresh portion of the training
corpus— but no serious attempts have been done
in order to determine the reliability of the re-
sults when moving from one domain to another
(Krovetz, 1997), and (4) no figures about com-
putational effort —space/time complexity— are
usually reported, even from an empirical per-
spective. A factors affecting the comparison
process is that comparisons between taggers are
often indirect, while they should be compared
under the same conditions in a multiple—trial
experiment with statistical tests of significance.
For these reasons, this paper calls for a dis-
cussion on POS taggers evaluation, aiming to
establish a more rigorous test experimentation
setting/designing, indispensable to extract reli-
able conclusions. As a starting point, we will
focus only on how the noise in the test corpus
can affect the obtained results.
</bodyText>
<page confidence="0.99616">
997
</page>
<sectionHeader confidence="0.67462" genericHeader="method">
2 Noise in the testing corpus
</sectionHeader>
<bodyText confidence="0.996682454545455">
From a machine learning perspective, the rele-
vant noise in the corpus is that of non system-
atically mistagged words (i.e. different annota-
tions for words appearing in the same syntac-
tic/semantic contexts).
Commonly used annotated corpora have
noise. See, for instance, the following examples
from the Wall Street Journal (wsJ) corpus:
Verb participle forms are sometimes tagged as
such (vBN) and also as adjectives (n) in other
sentences with no structural differences:
</bodyText>
<equation confidence="0.999925833333333">
la) ... failing_VBG to_TO voluntarily_RB
submit_VB the_DT requested_VBN
information_NN ...
lb) ... a_DT large_JJ sample_NN of_IN
married_JJ women_NNS with_IN at_IN
least_JJS one_CD child_NN ...
</equation>
<bodyText confidence="0.998268333333333">
Another structure not coherently tagged are
noun chains when the nouns (NN) are ambigu-
ous and can be also adjectives (n):
</bodyText>
<equation confidence="0.998258375">
2a) ... Mr._NNP Hahn_NNP ,_, the_DT
62-year-old_JJ chairman_NN and_CC
chief_NN executive_JJ officer_NN of _IN
Georgia-Pacif ic_NNP Corp. _NNP . . .
2b) ... Burger_NNP King_NNP
&apos;s_POS chief_JJ executive_NN officer_NN
Barry_NNP Gibbons_NNP ,_, stars_VBZ
in_IN ads_NNS saying_VBG ...
</equation>
<bodyText confidence="0.999402">
The noise in the test set produces a wrong
estimation of accuracy, since correct answers are
computed as wrong and vice-versa. In following
sections we will show how this uncertainty in the
evaluation may be, in some cases, larger than
the reported improvements from one system to
another, so invalidating the conclusions of the
comparison.
</bodyText>
<sectionHeader confidence="0.947315" genericHeader="method">
3 Model Setting
</sectionHeader>
<bodyText confidence="0.988376333333333">
To study the appropriateness of the choices
made by a POS tagger, a reference tagging must
be selected and assumed to be correct in or-
der to compare it with the tagger output. This
is usually done by assuming that the disam-
biguated test corpora being used contains the
right POS disambiguation. This approach is
quite right when the tagger error rate is larger
enough than the test corpus error rate, never-
theless, the current POS taggers have reached a
performance level that invalidates this choice,
since the tagger error rate is getting too close
to the error rate of the test corpus.
Since we want to study the relationship be-
tween the tagger error rate and the test corpus
error rate, we have to establish an absolute ref-
erence point. Although (Church, 1992) ques-
tions the concept of correct analysis, (Samuels-
son and Voutilainen, 1997) establish that there
exists a —statistically significant— absolute cor-
rect disambiguation, respect to which the error
rates of either the tagger or the test corpus can
be computed. What we will focus on is how
distorted is the tagger error rate by the use of
a noisy test corpus as a reference.
The cases we can find when evaluating the
performance of a certain tagger are presented
in table 1. ox/—iox stand for a right/wrong
tag (respect to the absolute correct disambigua-
tion). When both the tagger and the test cor-
pus have the correct tag, the tag is correctly
evaluated as right. When the test corpus has
the correct tag and the tagger gets it wrong,
the occurrence is correctly evaluated as wrong.
But problems arise when the test corpus has
a wrong tag: If the tagger gets it correctly, it
is evaluated as wrong when it should be right
(false negative). If the tagger gets it wrong, it
will be rightly evaluated as wrong if the error
commited by the tagger is other than the er-
ror in the test corpus, but wrongly evaluated
as right (false positive) if the error is the same.
Table 1 shows the computation of the percent-
corpus tagger eval: right eval: wrong
OK, OKt (1-C)t -
OK c -&apos;OKt - (1-C)(1--i)
-&apos;OK c OKt - Cu
&apos;OK OKt -10Kt C(1-u)P C(1-u)(1-73)
</bodyText>
<tableCaption confidence="0.984618">
Table 1: Possible cases when evaluating a tagger.
</tableCaption>
<bodyText confidence="0.939189">
ages of each case. The meanings of the used
variables are:
C: Test corpus error rate. Usually an estima-
tion is supplied with the corpus.
t: Tagger performance rate on words rightly
tagged in the test corpus. It can be seen as
P(oKtIoKc).
u: Tagger performance rate on words wrongly
tagged in the test corpus. It can be seen as
P(oKti—i0Kc).
</bodyText>
<page confidence="0.991218">
998
</page>
<bodyText confidence="0.999748909090909">
p: Probability that the tagger makes the same
error as the test corpus, given that both get
a wrong tag.
x: Real performance of the tagger, i.e. what
would be obtained on an error—free test set.
K: Observed performance of the tagger, com-
puted on the noisy test corpus.
For simplicity, we will consider only perfor-
mance on ambiguous words. Considering unam-
biguous words will make the analysis more com-
plex, since it should be taken into account that
neither the behaviour of the tagger (given by u,
t, p) nor the errors in the test corpus (given by
c) are the same on ambiguous and unambiguous
words. Nevertheless, this is an issue that must
be further addressed.
If we knew each one of the above proportions,
we would be able to compute the real perfor-
mance of our tagger (x) by adding up the OKt
rows from table 1, i.e. the cases in which the
tagger got the right disambiguation indepen-
dently from the tagging of the test set:
</bodyText>
<equation confidence="0.968521">
x=(1—C)t+Cu (1)
</equation>
<bodyText confidence="0.999298333333333">
The equation of the observed performance
can also be extracted from table 1, adding up
what is evaluated as right:
</bodyText>
<equation confidence="0.999824">
K = (1—C)t+C(1—u)p (2)
</equation>
<bodyText confidence="0.959750923076923">
The relationship between the real and the ob-
served performance is derived from 1 and 2:
x=if —C(1—u)p+Cu
Since only K and C are known (or approxi-
mately estimated) we can not compute the real
performance of the tagger. All we can do is to
establish some reasonable bounds for t, u and
p, and see in which range is x.
Since all variables are probabilities, they are
bounded in [0, 1]. We also can assume2 that
K &gt; C. We can use this constraints and the
above equations to bound the values of all vari-
ables. From 2, we obtain:
</bodyText>
<equation confidence="0.998374333333333">
u = 1-K -t(1-C) ; p = K-41-C) ; K -C(1 -u)p
t
Cp C(1-u) 1-C
</equation>
<bodyText confidence="0.9967958">
Thus, u will be maximum when p and t are
maximum (i.e. 1). This gives an upper bound
&apos;In the cases we are interested in -that is, current
systems- the tagger observed performance, K, is over
90%, while the corpus error rate, C, is below 10%.
for u of (1— K)/C. When t = 0, u will range
in [—oo, 1 —KIC] depending on the value of p.
Since we are assuming K&gt;C, the most informa-
tive lower bound for u keeps being zero. Simi-
larly, p is minimum when t=1 and. u = O. When
t= 0 the value for p will range in [KIC,-Foo]
depending on u. Since K &gt;C, the most infor-
mative upper bound for p is still 1. Finally, t
will be maximum when u = 1 and p = 0, and
minimum when u=0 and p=1. Summarizing:
</bodyText>
<equation confidence="0.999023">
0 &lt; u &lt;
K +C-1}
max {0
C &lt; p &lt; 1
K -C K
&lt; t &lt; rain {11-C
1-C -
</equation>
<bodyText confidence="0.977955214285714">
Since the values of the variables are mutually
constrained, it is not possible that, for instance,
u and t have simultaneously their upper bound
values (if (1—K)IC &lt;1 then K/(1 —C) &gt;1 and
viceversa). Any bound which is out of [0, 1] is
not informative and the appropriate boundary,
0 or 1, is then used. Note that the lower bound
for t will never be negative under the assump-
tion K &gt; C.
Once we have established these bounds, we
can use equation 1 to compute the range for the
real performance value of our tagger: x will be
minimum when u and t are minimum, which
produces the following bounds:
</bodyText>
<equation confidence="0.998051333333333">
X min = K -Cp
K +C if K &lt;1-C
X max if K &gt; 1-C
</equation>
<bodyText confidence="0.9999178">
As an example, let&apos;s suppose we evaluate a tag-
ger on a test corpus which is known to contain
about 3% of errors (C=0.03), and obtain a re-
ported performance of 93%3 (K=0.93). In this
case, equations 6 and 7 yield a range for the
real performance x that varies from [0.93, 0.96]
when p=0 to [0.90, 0.96] when p=1.
This results suggest that although we observe
a performance of K, we can not be sure of how
well is our tagger performing without taking
into account the values of t, u and p.
It is also obvious that the intervals in the
above example are too wide, since they con-
sider all the possible parameter values, even
when they correspond to very unlikely param-
</bodyText>
<footnote confidence="0.53732475">
3This is a realistic case obtained by (Marquez and
Pada!, , 1997) tagger. Note that 93% is the accuracy on
ambiguous words (the equivalent overall accuracy was
about 97%).
</footnote>
<figure confidence="0.944974333333333">
min f 1 1-K1
1 C I
(3)
</figure>
<page confidence="0.988589">
999
</page>
<table confidence="0.6766922">
eter combinations4. In section 4 we will try to
narrow those intervals, limiting the possibilities
to reasonable cases.
4 Reasonable Bounds for the Basic
Parameters
</table>
<bodyText confidence="0.997653475">
In real cases, not all parameter combinations
will be equally likely. In addition, the bounds
for the values of t, it and p are closely related
to the similarities between the training and test
corpora. That is, if the training and test sets are
extracted from the same corpus, they will prob-
ably contain the same kind of errors in the same
kind of situations. This may cause the training
procedure to learn the errors —especially if they
are systematic— and thus the resulting tagger
will tend to make the same errors that appear
in the test set. On the contrary, if the train-
ing and test sets come from different sources
—sharing only the tag set— the behaviour of the
resulting tagger will not depend on the right or
wrong tagging of the test set.
We can try to establish narrower bounds for
the parameters than those obtained in section 3.
First of all, the value of t is already con-
strained enough, due to its high contribution
(1 — C) to the value of K, which forces t to
take a value close to K. For instance, apply-
ing the boundaries in equation 5 to the case
C=0.03 and K=0.93, we obtain that t belongs
to [0.928,0.959].
The range for u can be slightly narrowed con-
sidering the following: In the case of indepen-
dent test and training corpora, u will tend to
be equal to t. Otherwise, the more biased to-
wards the corpus errors is the language model,
the lower it will be. Note than it &gt; t would mean
that the tagger disambiguates better the noisy
cases than the correct ones. Concerning to the
lower bound, only in the case that all the errors
in the training and test corpus were systematic
(and thus can be learned) could it reach zero.
However, not only this is not a likely situation,
but also requires a perfect—learning tagger. It
seems more reasonable that, in normal cases, er-
rors will be random, and the tagger will behave
</bodyText>
<footnote confidence="0.9961134">
4For instance, it is not reasonable that u = 0, which
would mean that the tagger never disambiguates cor-
rectly a wrong word in the corpus, or p= 1, which would
mean that it always makes the same error when both
are wrong.
</footnote>
<bodyText confidence="0.875139">
randomly on the noisy occurrences. This yields
a lower bound for it of 1/a, being a the average
ambiguity ratio for ambiguous words.
The reasonable bounds for u are thus
</bodyText>
<equation confidence="0.7844665">
&lt; u &lt; min tt 1-K
a — C
</equation>
<bodyText confidence="0.9520817">
Finally, the value of p has similar constraints
to those of u. If the test and training corpora
are independent, the probability of making the
same error, given that both are wrong, will be
the random 1/(a — 1). If the corpora are not
independent, the errors that can be learned by
the tagger will cause p to rise up to (potentially)
1. Again, only in the case that all errors where
systematic, could p reach 1.
Then, the reasonable bounds for p are:
</bodyText>
<equation confidence="0.851716">
{ 1 K +C —1}
max 1&apos; &lt; p &lt; 1
a—
</equation>
<sectionHeader confidence="0.666943" genericHeader="method">
5 On&apos;Comparing Tagger
Performances
</sectionHeader>
<bodyText confidence="0.999297888888889">
As stated above, knowing which are the reason-
able limits for the it, p and t parameters enables
us to compute the range in which the real per-
formance of the tagger can vary.
So, given two different taggers T1 and T2, and
provided we know the values for the test corpus
error rate and the observed performance of both
cases (CI, C2, K1, K2), we can compare them
by matching the reasonable intervals for the re-
spective real performances x1 and x2.
From a conservative position, we cannot
strongly state than one of the taggers performs
better than the other when the two intervals
overlap, since this implies a chance that the real
performances of both taggers are the same.
The following real example has been ex-
tracted from (Marquez and Padro , 1997): The
tagger T1 uses only bigram information and has
an observed performance on ambiguous words
K1 = 0.9135 (96.86% overall). The tagger T2
uses trigrams and automatically acquired con-
text constraints and has an accuracy of K2 =
0.9282 (97.39% overall). Both taggers have been
evaluated on a corpus ( \vs.]) with an estimated
error rate&apos; C1 =C2=0.03. The average ambigu-
ity ratio of the ambiguous words in the corpus
is a=2.5 tags/word.
</bodyText>
<footnote confidence="0.615066666666667">
&apos;The (wsJ) corpus error rate is estimated over all
words. We are assuming that the errors distribute
uniformly among all words, although ambiguous words
</footnote>
<page confidence="0.970804">
1000
</page>
<bodyText confidence="0.889920272727273">
These data yield the following range of rea-
sonable intervals for the real performance of the
taggers.
for pi= (1/a) =0.4 for pi=1
x1 E [91.35, 94.05] xi E [90.75, 93.99]
x2 E [92.82, 95.60] X2 E [92.22, 95.55]
The same information is included in figure 1
which presents the reasonable accuracy intervals
for both taggers, for p ranging from 1/a=0.4 to
1 (the shadowed part corresponds to the over-
lapping region between intervals).
</bodyText>
<figureCaption confidence="0.998793">
Figure 1: Reasonable intervals for both taggers
</figureCaption>
<bodyText confidence="0.9998908">
The obtained intervals have a large overlap
region which implies that there are reasonable
parameter combinations that could cause the
taggers to produce different observed perfor-
mances though their real accuracies were very
similar. From this conservative approach, we
would not be able to conclude that the tagger
T2 is better than T1, even though the 95% con-
fidence intervals for the observed performances
did allow us to do so.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999456090909091">
The presented analysis of the effects of noise in
the test corpus on the evaluation of POS taggers
leads us to conclude that when a tagger is eval-
uated as better than another using noisy test
corpus, there are reasonable chances that they
are in fact very similar but one of them is just
adapting better than the other to the noise in
the corpus.
probably have a higher error rate. Nevertheless, a higher
value for C would cause the intervals to be wider and to
overlap even more.
We believe that the widespread practice of
evaluating taggers against a noisy test corpus
has reached its limit, since the performance of
current taggers is getting too close to the error
rate usually found in test corpora.
An obvious solution —and maybe not as costly
as one might think, since small test sets properly
used may yield enough statistical evidence— is
using only error—free test corpora. Another pos-
sibility is to further study the influence of noise
in order to establish a criterion —e.g. a thresh-
old depending on the amount of overlapping be-
tween intervals— to decide whether a given tag-
ger can be considered better than another.
There is still much to be done in this direc-
tion. This paper does not intend to establish
a new evaluation method for P OS tagging, but
to point out that there are some issues —such as
the noise in test corpus— that have been paid lit-
tle attention and are more important than what
they seem to be.
Some of the issues that should be further con-
sidered are: The effect of noise on unambigu-
ous words; the reasonable intervals for overall
real performance; the —probably— different val-
ues of C, p, u and t for ambiguous/unambiguous
words; how to estimate the parameter values of
the evaluated tagger in order to constrain as
much as possible the intervals; the statistical
significance of the interval overlappings; a more
informed (and less conservative) criterion to re-
ject/accept the hypothesis that both taggers are
different, etc.
</bodyText>
<sectionHeader confidence="0.999549" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.999352388888889">
Church, K.W. 1992. Current Practice in Part of
Speech Tagging and Suggestions for the Future.
In Simmons (ed.), Sbornik praci: In Honor of
Henry Kueera. Michigan Slavic Studies.
Krovetz, R. 1997. Homonymy and Polysemy in
Information Retrieval. In Proceedings of joint
E/ACL meeting.
Marquez, L. and Padre), L. 1997. A Flexible POS
Tagger Using an Automatically Acquired Lan-
guage Model. In Proceedings of joint E/ACL
meeting.
Mooney, R.J. 1996. Comparative Experiments on
Disambiguating Word Senses: An Illustration of
the Role of Bias in Machine Learning. In Proceed-
ings of EMNLP&apos;96 conference.
Samuelsson, C. and Voutilainen, A. 1997. Compar-
ing a Linguistic and a Stochastic Tagger. In Pro-
ceedings of joint E/ACL meeting.
</reference>
<page confidence="0.995912">
1001
</page>
<sectionHeader confidence="0.948684" genericHeader="conclusions">
Resum
</sectionHeader>
<bodyText confidence="0.99995825">
Aquest article versa sobre l&apos;avaluaci6 de desam-
biguadors morfosintactics. Normalment, l&apos;ava-
luacio es fa comparant la sortida del desam-
biguador amb un corpus de referencia, que se
suposa lliure d&apos;errors. De tota manera, els cor-
pus que s&apos;usen habitualment contenen soroll que
causa que el rendiment que s&apos;obte dels desam-
biguadors sigui una distorsi6 del valor real. En
aquest article analitzem fins a quin punt agues-
ta distorsio pot invalidar la comparaci6 entre
desambiguadors o la mesura de la millora apor-
tada per un nou sistema. La conclusi6 princi-
pal es que cal establir procediments alternatius
d&apos;experimentacio mes rigorosos, per poder ava-
luar i comparar fiablement les precisions dels
desambiguadors morfosintactics.
</bodyText>
<sectionHeader confidence="0.543323" genericHeader="references">
Laburtena
</sectionHeader>
<bodyText confidence="0.999558352941176">
Artikulu hau desanbiguatzaile morfosintak-
tikoen ebaluazioaren inguruan datza. Nor-
malean, ebaluazioa, desanbiguatzailearen irte-
era eta ustez errorerik gabeko erreferentziako
corpus bat konparatuz egiten da. Hala ere, maiz
corpusetan erroreak egoten dira eta horrek de-
sanbiguatzailearen emaitzaren benetako balioan
eragina izaten du. Artikulu honetan, hain
zuzed ere, horixe aztertuko dugu, alegia, zer
neurritan distortsio horrek jar dezakeen auzitan
desanbiguatzaileen arteko konparazioa edo sis-
tema berri batek ekar dezakeen hobekuntza-
maila. Konklusiorik nagusiena hauxe da: de-
sanbiguatzaile morfosintaktikoak aztertzeko eta
modu ziurrago batez konparatu ahal izateko,
azterketa-bideak sakonagoak eta zehatzagoak
izan beharko liratekeela.
</bodyText>
<page confidence="0.996034">
1002
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.378871">
<title confidence="0.9252055">On the Evaluation and Comparison of Taggers: the Effect of Noise in Testing Corpora.</title>
<author confidence="0.480983">Marquez Padre</author>
<affiliation confidence="0.539054">Dep. LSI. Technical University of Catalonia</affiliation>
<address confidence="0.530858">c/ Jordi Girona 1-3. 08034 Barcelona</address>
<email confidence="0.8635">fpadro,lluism101si.upc.es</email>
<abstract confidence="0.999317928571429">addresses the issue of evaluation. Such evaluation is usually performed by comparing the tagger output with a reference test corpus, which is assumed to be error-free. Currently used corpora contain noise which causes the obtained performance to be a distortion of the real value. We analyze to what extent this distortion may invalidate the comparison between taggers or the measure of the improvement given by a new system. The main conclusion is that a more rigorous testing experimentation setting/designing is needed to reliably evaluate and compare tagger accuracies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>Current Practice in Part of Speech Tagging and Suggestions for the Future.</title>
<date>1992</date>
<booktitle>Sbornik praci: In Honor of Henry Kueera. Michigan Slavic Studies.</booktitle>
<editor>In Simmons (ed.),</editor>
<contexts>
<context position="5935" citStr="Church, 1992" startWordPosition="937" endWordPosition="938"> with the tagger output. This is usually done by assuming that the disambiguated test corpora being used contains the right POS disambiguation. This approach is quite right when the tagger error rate is larger enough than the test corpus error rate, nevertheless, the current POS taggers have reached a performance level that invalidates this choice, since the tagger error rate is getting too close to the error rate of the test corpus. Since we want to study the relationship between the tagger error rate and the test corpus error rate, we have to establish an absolute reference point. Although (Church, 1992) questions the concept of correct analysis, (Samuelsson and Voutilainen, 1997) establish that there exists a —statistically significant— absolute correct disambiguation, respect to which the error rates of either the tagger or the test corpus can be computed. What we will focus on is how distorted is the tagger error rate by the use of a noisy test corpus as a reference. The cases we can find when evaluating the performance of a certain tagger are presented in table 1. ox/—iox stand for a right/wrong tag (respect to the absolute correct disambiguation). When both the tagger and the test corpus</context>
</contexts>
<marker>Church, 1992</marker>
<rawString>Church, K.W. 1992. Current Practice in Part of Speech Tagging and Suggestions for the Future. In Simmons (ed.), Sbornik praci: In Honor of Henry Kueera. Michigan Slavic Studies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Krovetz</author>
</authors>
<title>Homonymy and Polysemy in Information Retrieval.</title>
<date>1997</date>
<booktitle>In Proceedings of joint E/ACL meeting.</booktitle>
<contexts>
<context position="3045" citStr="Krovetz, 1997" startWordPosition="482" endWordPosition="483">test experiments are usually performed over noisy corpora which distorts the obtained results, (2) performance figures are too often calculated from only a single or very small number of trials, though average results from multiple trials are crucial to obtain reliable estimations of accuracy (Mooney, 1996), (3) testing experiments are usually done on corpora with the same characteristics as the training data —usually a small fresh portion of the training corpus— but no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another (Krovetz, 1997), and (4) no figures about computational effort —space/time complexity— are usually reported, even from an empirical perspective. A factors affecting the comparison process is that comparisons between taggers are often indirect, while they should be compared under the same conditions in a multiple—trial experiment with statistical tests of significance. For these reasons, this paper calls for a discussion on POS taggers evaluation, aiming to establish a more rigorous test experimentation setting/designing, indispensable to extract reliable conclusions. As a starting point, we will focus only o</context>
</contexts>
<marker>Krovetz, 1997</marker>
<rawString>Krovetz, R. 1997. Homonymy and Polysemy in Information Retrieval. In Proceedings of joint E/ACL meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L</author>
</authors>
<title>A Flexible POS Tagger Using an Automatically Acquired Language Model.</title>
<date>1997</date>
<booktitle>In Proceedings of joint E/ACL meeting.</booktitle>
<marker>L, 1997</marker>
<rawString>Marquez, L. and Padre), L. 1997. A Flexible POS Tagger Using an Automatically Acquired Language Model. In Proceedings of joint E/ACL meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Mooney</author>
</authors>
<title>Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP&apos;96 conference.</booktitle>
<contexts>
<context position="2739" citStr="Mooney, 1996" startWordPosition="431" endWordPosition="432">ing tagger performances against a reference test corpus, and to make some criticism about common practices followed by the NLP researchers in this issue. The above mentioned factors can affect either the evaluation or the comparison process. Factors affecting the evaluation process are: (1) Training and test experiments are usually performed over noisy corpora which distorts the obtained results, (2) performance figures are too often calculated from only a single or very small number of trials, though average results from multiple trials are crucial to obtain reliable estimations of accuracy (Mooney, 1996), (3) testing experiments are usually done on corpora with the same characteristics as the training data —usually a small fresh portion of the training corpus— but no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another (Krovetz, 1997), and (4) no figures about computational effort —space/time complexity— are usually reported, even from an empirical perspective. A factors affecting the comparison process is that comparisons between taggers are often indirect, while they should be compared under the same conditions in a mult</context>
</contexts>
<marker>Mooney, 1996</marker>
<rawString>Mooney, R.J. 1996. Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning. In Proceedings of EMNLP&apos;96 conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelsson</author>
<author>A Voutilainen</author>
</authors>
<title>Comparing a Linguistic and a Stochastic Tagger.</title>
<date>1997</date>
<booktitle>In Proceedings of joint E/ACL meeting.</booktitle>
<contexts>
<context position="1821" citStr="Samuelsson and Voutilainen, 1997" startWordPosition="286" endWordPosition="289">aggers have been built using several approaches, such as statistical techniques, symbolic machine learning techniques, neural networks, etc. The accuracy reported by most current taggers ranges from 96-97% to almost 100% in the linguistically—motivated Constraint Grammar environment. Unfortunately, there have been very few direct comparisons of alternative taggers1 on identical test data. However, in most current papers it is argued that the performance of some taggers is better than others as a result of some kind of indirect comparisons between them. We One of the exceptions is the work by (Samuelsson and Voutilainen, 1997), in which a very strict comparison between taggers is performed. think that there are a number of not enough controlled/considered factors that make these conclusions dubious in most cases. In this direction, the present paper aims to point out some of the difficulties arising when evaluating and comparing tagger performances against a reference test corpus, and to make some criticism about common practices followed by the NLP researchers in this issue. The above mentioned factors can affect either the evaluation or the comparison process. Factors affecting the evaluation process are: (1) Tra</context>
<context position="6013" citStr="Samuelsson and Voutilainen, 1997" startWordPosition="946" endWordPosition="950">that the disambiguated test corpora being used contains the right POS disambiguation. This approach is quite right when the tagger error rate is larger enough than the test corpus error rate, nevertheless, the current POS taggers have reached a performance level that invalidates this choice, since the tagger error rate is getting too close to the error rate of the test corpus. Since we want to study the relationship between the tagger error rate and the test corpus error rate, we have to establish an absolute reference point. Although (Church, 1992) questions the concept of correct analysis, (Samuelsson and Voutilainen, 1997) establish that there exists a —statistically significant— absolute correct disambiguation, respect to which the error rates of either the tagger or the test corpus can be computed. What we will focus on is how distorted is the tagger error rate by the use of a noisy test corpus as a reference. The cases we can find when evaluating the performance of a certain tagger are presented in table 1. ox/—iox stand for a right/wrong tag (respect to the absolute correct disambiguation). When both the tagger and the test corpus have the correct tag, the tag is correctly evaluated as right. When the test </context>
</contexts>
<marker>Samuelsson, Voutilainen, 1997</marker>
<rawString>Samuelsson, C. and Voutilainen, A. 1997. Comparing a Linguistic and a Stochastic Tagger. In Proceedings of joint E/ACL meeting.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>