<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001380">
<title confidence="0.985883">
Robust Systems for Preposition Error Correction Using Wikipedia Revisions
</title>
<author confidence="0.719838">
Aoife Cahill*, Nitin Madnani*, Joel Tetreault† and Diane Napolitano*
</author>
<affiliation confidence="0.506124">
* Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA
</affiliation>
<email confidence="0.909449">
{acahill, nmadnani, dnapolitano}@ets.org
</email>
<affiliation confidence="0.577418">
† Nuance Communications, Inc., 1198 E. Arques Ave, Sunnyvale, CA 94085, USA
</affiliation>
<email confidence="0.989276">
Joel.Tetreault@nuance.com
</email>
<sectionHeader confidence="0.995476" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998215583333333">
We show that existing methods for training
preposition error correction systems, whether
using well-edited text or error-annotated cor-
pora, do not generalize across very differ-
ent test sets. We present a new, large error-
annotated corpus and use it to train systems
that generalize across three different test sets,
each from a different domain and with differ-
ent error characteristics. This new corpus is
automatically extracted from Wikipedia revi-
sions and contains over one million instances
of preposition corrections.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999762764705882">
One of the main themes that has defined the field of
automatic grammatical error correction has been the
availability of error-annotated learner data to train
and test a system. Some errors, such as determiner-
noun number agreement, are easily corrected us-
ing rules and regular expressions (Leacock et al.,
2010). On the other hand, errors involving the usage
of prepositions and articles are influenced by sev-
eral factors including the local context, the prior dis-
course and semantics. These errors are better han-
dled by statistical models which potentially require
millions of training examples.
Most statistical approaches to grammatical error
correction have used one of the following training
paradigms: 1) training solely on examples of cor-
rect usage (Han et al., 2006); 2) training on exam-
ples of correct usage and artificially generated er-
rors (Rozovskaya and Roth, 2010); and 3) training
on examples of correct usage and real learner er-
rors (Dahlmeier and Ng, 2011; Dale et al., 2012).
The latter two methods require annotated corpora of
errors, and while they have shown great promise,
manually annotating grammatical errors in a large
enough corpus of learner writing is often a costly
and time-consuming endeavor.
In order to efficiently and automatically acquire a
very large corpus of annotated learner errors, we in-
vestigate the use of error corrections extracted from
Wikipedia revision history. While Wikipedia re-
vision history has shown promise for other NLP
tasks including paraphrase generation (Max and
Wisniewski, 2010; Nelken and Yamangil, 2008) and
spelling correction (Zesch, 2012), this resource has
not been used for the task of grammatical error cor-
rection.
To evaluate the usefulness of Wikipedia revision
history for grammatical error correction, we address
the task of correcting errors in preposition selection
(i.e., where the context licenses the use of a prepo-
sition, but the writer selects the wrong one). We
first train a model directly on instances of correct
and incorrect preposition usage extracted from the
Wikipedia revision data. We also generate artificial
errors using the confusion distributions derived from
this data. We compare both of these approaches to
models trained on well-edited text and evaluate each
on three test sets with a range of different character-
istics. Each training paradigm is applied to multiple
data sources for comparison. With these multiple
evaluations, we address the following research ques-
tions:
</bodyText>
<listItem confidence="0.730878">
1. Across multiple test sets, which data source
</listItem>
<page confidence="0.938632">
507
</page>
<note confidence="0.473288">
Proceedings of NAACL-HLT 2013, pages 507–517,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9983265">
is more useful for correcting preposition er-
rors: a large amount of well-edited text, a large
amount of potentially noisy error-annotated
data (either artificially generated or automati-
cally extracted) or a smaller amount of higher
quality error-annotated data?
</bodyText>
<listItem confidence="0.985253">
2. Given error-annotated data, is it better to train
on the corrections directly or to use the con-
fusion distributions derived from these correc-
tions for generating artificial errors in well-
edited text?
3. What is the impact of having a mismatch in the
error distributions of the training and test sets?
</listItem>
<sectionHeader confidence="0.999337" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9996015">
In this section, we only review work in preposi-
tion error correction in terms of the three training
paradigms and refer the reader to Leacock et al.
(2010) for a more comprehensive review of the field.
</bodyText>
<subsectionHeader confidence="0.986261">
2.1 Training on Well-Edited Text
</subsectionHeader>
<bodyText confidence="0.999987384615385">
Early approaches to error detection and correction
did not have access to large amounts of error-
annotated data to train statistical models and thus,
systems were trained on millions of well-edited ex-
amples from news text instead (Gamon et al., 2008;
Tetreault and Chodorow, 2008; De Felice and Pul-
man, 2009). Feature sets usually consisted of n-
grams around the preposition, POS sequences, syn-
tactic features and semantic information. Since the
model only had knowledge of correct usage, an error
was flagged if the system’s prediction for a particu-
lar preposition context differed from the preposition
the writer used.
</bodyText>
<subsectionHeader confidence="0.995546">
2.2 Artificial Errors
</subsectionHeader>
<bodyText confidence="0.999966636363636">
The issue with training solely on correct usage was
that the systems had no knowledge of typical learner
errors. Ideally, a system would be trained on ex-
amples of correct and incorrect usage, however, for
many years, such error-annotated corpora were not
available. Instead, several researchers generated ar-
tificial errors based on the error distributions derived
from the error-annotated learner corpora available at
the time. Izumi et al. (2003) was the first to evaluate
a model trained on incorrect usage as well as artifi-
cial errors for the task of correcting several different
error types, including prepositions. However, with
limited training data, system performance was quite
poor. Rozovskaya and Roth (2010) evaluated dif-
ferent ways of generating artificial errors and found
that a system trained on artificial errors could outper-
form the more traditional training paradigm of using
only well-edited texts. Most recently, Imamura et al.
(2012) showed that performance could be improved
by training a model on artificial errors and address-
ing domain adaptation for the task of Japanese par-
ticle correction.
</bodyText>
<subsectionHeader confidence="0.999075">
2.3 Error-Annotated Learner Corpora
</subsectionHeader>
<bodyText confidence="0.999983142857143">
Recently, error-annotated learner data has become
more readily and publicly available allowing models
to be trained on both examples of correct usage as
well typical learner errors. Han et al. (2010) showed
that a preposition error detection and correction sys-
tem trained on 100,000 annotated preposition errors
from the Chungdahm Corpus of Korean Learner En-
glish (in addition to 1 million examples of correct
usage) outperformed a model trained only on 5 mil-
lion examples of correct usage. Gamon (2010) and
Dahlmeier and Ng (2011) showed that combining
models trained separately on examples of correct
and incorrect usage could also improve the perfor-
mance of a preposition error correction system.
</bodyText>
<sectionHeader confidence="0.999316" genericHeader="method">
3 Mining Wikipedia Revisions for
Grammatical Error Corrections
</sectionHeader>
<subsectionHeader confidence="0.939088">
3.1 Related Work
</subsectionHeader>
<bodyText confidence="0.99800975">
Many NLP researchers have taken advantage of the
wealth of information available in Wikipedia revi-
sions. Dutrey et al. (2011) define a typology of mod-
ifications found in the French Wikipedia (WiCo-
PaCo). They show that the kinds of edits made range
from specific lexical changes to more general rewrite
edits. Similar types of edits are found in the En-
glish Wikipedia. The data extracted from Wikipedia
revisions has been used for a wide variety of tasks
including spelling correction (Max and Wisniewski,
2010; Zesch, 2012), lexical error detection (Nelken
and Yamangil, 2008), sentence compression (Ya-
mangil and Nelken, 2008), paraphrase generation
(Max and Wisniewski, 2010; Nelken and Yamangil,
2008), lexical simplification (Yatskar et al., 2010)
and entailment (Zanzotto and Pennacchiotti, 2010;
</bodyText>
<page confidence="0.990336">
508
</page>
<listItem confidence="0.9986678">
(1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at —* in) the refrigerator.
(2) [Wiki clean] Also none of the witnesses present (of —* on) those dates supports Ranneft’s claims.
(3) [Wiki dirty] ... cirque has a permanent production (to —* at) the Mirage, love.
(4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his
performances for French tourists (in —* to) Petersburg.
</listItem>
<figureCaption confidence="0.9990365">
Figure 1: Example sentences with preposition errors extracted from Wikipedia revisions. The second preposition is
assumed to be the correction.
</figureCaption>
<bodyText confidence="0.985882">
Cabrio et al., 2012). To our knowledge, no one has
previously extracted data for training a grammatical
error detection system from Wikipedia revisions.
</bodyText>
<subsectionHeader confidence="0.977256">
3.2 Extracting Preposition Correction Data
from Wikipedia Revisions
</subsectionHeader>
<bodyText confidence="0.997960296296296">
As the source of our Wikipedia revisions, we used an
XML snapshot of Wikipedia generated in July 2011
containing 8,735,890 articles and 288,583,063 revi-
sions.1 We then used the following process to ex-
tract preposition errors and their corresponding cor-
rections from this snapshot:
Step 1: Extract the plain text versions of all revi-
sions of all articles using the Java Wikipedia
Library (Ferschke et al., 2011).
Step 2: For each Wikipedia article, compare each
revision with the revision immediately preced-
ing it using an efficient diff algorithm.2
Step 3: Compute all 1-word edit chains for the arti-
cle, i.e., sequences of related edits derived from
all revisions of the same article. For example,
say revision 10 of an article inserts the preposi-
tion of into a sentence and revision 12 changes
that preposition to on. Assuming that no other
revisions change this sentence, the correspond-
ing edit chain would contain the following 3 el-
ements: E—*of—*on. The extracted chains con-
tain the full context on either side of the 1-word
edit, up to the automatically detected sentence
boundaries.
Step 4: (a) Ignore any circular chains, i.e., where
the first element in the edit chain is the same as
the last element. (b) Collapse all non-circular
</bodyText>
<footnote confidence="0.998951">
1http://dumps.wikimedia.org/enwiki/
2http://code.google.com/p/google-diff-match-patch/
</footnote>
<bodyText confidence="0.999484">
chains, i.e., only retain the first and the last ele-
ments in a chain. Both these decisions are mo-
tivated by the assumption that the intermediate
links in the chain are unreliable for training an
error correction system since a Wikipedia con-
tributor modified them.
Step 5 : From all remaining 2-element chains, find
those where a preposition is replaced with an-
other preposition. If the preposition edit is the
only edit in the sentence, we convert the chain
into a sentence pair and label it clean. If there
are other 1-word edits but not within 5 words of
the preposition edit on either side, we label the
sentence somewhat clean. Otherwise, we label
it dirty. The motivation is that the presence of
other nearby edits make the preposition correc-
tion less reliable when used in isolation, due to
the possible dependencies between corrections.
All extracted sentences were part-of-speech tagged
using the Stanford Tagger (Toutanova et al., 2003).
Using the above process, we are able to extract ap-
proximately 2 million sentences containing preposi-
tions errors and their corrections. Some examples
of the sentences we extracted are given in Figure 1.
Example (4) shows an example of a bad correction.
</bodyText>
<sectionHeader confidence="0.996552" genericHeader="method">
4 Corpora
</sectionHeader>
<bodyText confidence="0.999992">
We use several corpora for training and testing our
preposition error correction system. The proper-
ties of each are outlined in Table 1, organized by
paradigm. For each corpus we report the total num-
ber of prepositions used for training, as well as the
number and percentage of preposition corrections.
</bodyText>
<subsectionHeader confidence="0.969288">
4.1 Well-edited Text
</subsectionHeader>
<bodyText confidence="0.9999435">
We train our system on two well-edited corpora.
The first is the same corpus used by Tetreault and
</bodyText>
<page confidence="0.994051">
509
</page>
<table confidence="0.999909769230769">
Corpus Total # Preps # Corrected Preps
Well-edited Text Wikipedia Snapshot (10m sents) 26,069,860 0 (0%)
Lexile/SJM 6,719,077 0 (0%)
Artificially Generated Wikipedia Snapshot 26,127,464 2,844,227 (10.9%)
Errors Lexile/SJM 6,723,206 792,195 (11.8%)
Wikipedia Revisions All 7,125,317 1,027,643 (20.6%)
Naturally Occurring Wikipedia Revisions Clean 3,001,900 381,644 (12.7%)
Errors Wikipedia Revisions Clean 1,978,802 266,275 (14.4%)
Lang-8 129,987 53,493 (41.2%)
NUCLE Train 72,741 922 (1.3%)
Test Corpora NUCLE Test 9,366 125 (1.3%)
FCE 33,243 2,900 (8.7%)
HOO 2011 Test 1,703 81 (4.8%)
</table>
<tableCaption confidence="0.999876">
Table 1: Corpora characteristics
</tableCaption>
<bodyText confidence="0.999282714285714">
Chodorow (2008), comprising roughly 1.8 million
sentences from the San Jose Mercury News Corpus3
and roughly 1.8 million sentences from grades 11
and 12 of the MetaMetrics Lexile Corpus. Our sec-
ond corpus is a random sample of 10 million sen-
tences containing at least one preposition from the
June 2012 snapshot of English Wikipedia Articles.4
</bodyText>
<subsectionHeader confidence="0.998527">
4.2 Artificially Generated Errors
</subsectionHeader>
<bodyText confidence="0.999925222222222">
Similar to Foster and Andersen (2009) and Ro-
zovskaya and Roth (2010), we artificially introduce
preposition errors into well-edited corpora (the two
described above). We do this based on a distribu-
tion of possible confusions and train a model that
is aware of the corrections. The two sets of con-
fusion distributions we used were derived based on
the errors extracted from Wikipedia revisions and
Lang-8 respectively (discussed in Section 4.3). For
each corrected preposition pi in the revision data,
we calculated P(pi|pj), where pj is each of the pos-
sible original prepositions that were confused with
pi. Then, for each sentence in the well-edited text,
all prepositions are extracted. A preposition is ran-
domly selected (without replacement) and changed
based on the distribution of possible confusions
(note that the original preposition is also included
in the distribution, usually with a high probabil-
</bodyText>
<footnote confidence="0.956379166666667">
3The San Jose Mercury News is available from the Linguis-
tic Data Consortium (catalog number LDC93T3A).
4We used a newer version of the Wikipedia text for the well-
edited text, since we assume that more recent versions of the
text will be most grammatical, and therefore closer to well-
edited.
</footnote>
<bodyText confidence="0.9924924">
ity, meaning that there is a strong preference not to
change the preposition). If a preposition is changed
to something other than the original preposition, all
remaining prepositions in the sentence are left un-
changed.
</bodyText>
<subsectionHeader confidence="0.998163">
4.3 Naturally Occurring Errors
</subsectionHeader>
<bodyText confidence="0.999981047619048">
We have a number of corpora that contain annotated
preposition errors. Note that we are only considering
incorrectly selected prepositions, we do not consider
missing or extraneous.
NUCLE The NUS Corpus of Learner English (NU-
CLE)5 contains one million words of learner
essay text, manually annotated with error tags
and corrections. We use the same training, dev
and test splits as Dahlmeier and Ng (2011).
FCE The CLC FCE Dataset6 is a collection of
1,244 exam scripts written by learners of En-
glish as part of the Cambridge ESOL First Cer-
tificate in English (Yannakoudakis et al., 2011).
It includes demographic metadata about the
candidate, a grade for each essay and manually-
annotated error corrections.
Wikipedia We use three versions of the preposi-
tion errors extracted from the Wikipedia revi-
sions as described in Section 3.2. The first in-
cludes corrections where the preposition was
the only word corrected in the entire sentence
</bodyText>
<footnote confidence="0.999258">
5http://bit.ly/nuclecorpus
6http://ilexir.co.uk/applications/clc-fce-dataset/
</footnote>
<page confidence="0.993532">
510
</page>
<bodyText confidence="0.999931071428572">
(clean). The second contains all clean cor-
rections, as well as all corrections where there
were no other edits within a five-word span on
either side of the preposition (�clean). The
third contains all corrections regardless of any
other changes in the surrounding context (all).
Lang-8 The Lang-8 website contains journals writ-
ten by language learners, where native speakers
highlight and correct errors on a sentence-by-
sentence basis. As a result, it contains typical
grammatical mistakes made by language learn-
ers, which can be easily downloaded. We auto-
matically extract 75,622 sentences with prepo-
sition errors and corrections from the first mil-
lion journal entries.7
HOO 2011 We take the test set from the HOO 2011
shared task (Dale and Kilgarriff, 2011) and ex-
tract all examples of preposition selection er-
rors. The texts are fragments of ACL papers
that have been manually annotated for gram-
matical errors.8
It is important to note that the three test sets we use
are from entirely different domains: exam scripts
from non-native English speakers (FCE), essays by
highly proficient college students in Singapore (NU-
CLE) and ACL papers (HOO). In addition, they have
a different number of total prepositions as well as er-
roneous prepositions.
</bodyText>
<sectionHeader confidence="0.9888735" genericHeader="method">
5 Preposition Error Correction
Experiments
</sectionHeader>
<bodyText confidence="0.999962428571428">
We use the preposition error correction model de-
scribed in Tetreault and Chodorow (2008)9 to eval-
uate the many ways of using Wikipedia error cor-
rections as described in the Section 4. We use this
system since it has been recreated for other work
(Dahlmeier and Ng, 2011; Tetreault et al., 2010) and
is similar in methodology to Gamon et al. (2008)
</bodyText>
<footnote confidence="0.978830666666667">
7Tajiri et al. (2012) extract a corpus of English verb phrases
corrected for tense/aspect errors from Lang-8. They kindly pro-
vided us with their scripts to carry out the scraping of Lang-8.
8The results of the HOO 2011 shared task were not reported
at level of preposition selection error, therefore it is not possible
to compare the results presented in this paper with those results.
9Note that in that work, the model was evaluated in terms of
preposition error detection rather than correction, however the
model itself does not change.
</footnote>
<bodyText confidence="0.996503384615385">
and De Felice and Pulman (2009). In short, the
method models the problem of preposition error cor-
rection (for replacement errors) as a 36-way classifi-
cation problem using a multinomial logistic regres-
sion model.10 The system uses 25 lexical, syntac-
tic and n-gram features derived from the contexts of
each preposition training instance.
We modified the training paradigm of Tetreault
and Chodorow (2008) so that a model could be
trained on examples of correct usage as well as ac-
tual errors. We did this by adding a new feature
specifying the writer’s original preposition (as in
Han et al. (2010) and Dahlmeier and Ng (2011)).
</bodyText>
<sectionHeader confidence="0.795278" genericHeader="evaluation">
5.1 Results
</sectionHeader>
<bodyText confidence="0.96865903030303">
We train a preposition correction system using each
of the three data paradigms and test on the FCE,
NUCLE and HOO 2011 test corpora. For each
preposition in the test corpus, we record whether
the system predicted that it should be changed,
and if so, what it should be changed to. We then
compare the prediction to the annotation in the test
corpus. We report results in terms of f-score, where
precision and recall are calculated as follows:11
Precision = Number of correct preposition corrections
Total number of corrections suggested
Recall = Number of correct preposition corrections
Total number of corrections in test set
Note that due to the high volume of unchanged
prepositions in the test corpus, we obtain very high
accuracies, which are not indicative of true perfor-
mance, and are not included in our results.
The results of our experiments are presented in
Table 2.12 The first part of the table shows the f-
scores of preposition error correction systems that
10We use liblinear (Fan et al., 2008) with the L1-regularized
logistic regression solver and default parameters.
11As Chodorow et al. (2012) note, it is not clear how to han-
dle cases where the system predicts a preposition that is neither
the same as the writer preposition nor the correct preposition.
We count these cases as false positives.
12No thresholds were used in the systems that were trained
on well-edited text. Traditionally, thresholds are applied so as
to only predict a correction when the system is highly confident.
This has the effect of increasing precision at the cost of recall,
and sometimes leads to an overall improved f-score. Here we
take the prediction of the system, regardless of the confidence,
reflecting a lower-bound of this method.
</bodyText>
<page confidence="0.979123">
511
</page>
<table confidence="0.999975846153846">
Data Source Paradigm CLC-FCE NUCLE HOO2011
N=33,243 N=9,366 N=1,703
Without Wikipedia Snapshot Well-edited Text 24.43* 5.02* 12.36*
Wikipedia Lexile/SJM Well-edited Text 24.73* 4.29* 9.73*
Revisions Wikipedia Snapshot Artificial Errors (Lang-8) 42.15* 19.91* 28.75
(nonWikiRev) Lexile/SJM Artificial Errors (Lang-8) 45.36 18.00* 25.15
Lang-8 Error-annotated Text 38.22* 8.18* 24.00
NUCLE train Error-annotated Text 5.38* 20.14 4.82*
With Wikipedia Snapshot Artificial Errors (Wiki) 31.17* 24.52 28.30
Wikipedia Lexile/SJM Artificial Errors (Wiki) 34.35* 23.38 32.76
Revisions Wikipedia Revisions All Error-annotated Text 33.59* 26.39 36.84
(WikiRev) Wikipedia Revisions �Clean Error-annotated Text 29.68* 22.13 36.04
Wikipedia Revisions Clean Error-annotated Text 28.09* 21.74 28.30
</table>
<tableCaption confidence="0.944633">
Table 2: Preposition selection error correction results (f-score). The systems with scores in bold are statistically
significantly better than all systems marked with an asterisk (p &lt; 0.01). Confidence intervals were obtained using
bootstrap resampling with 50,000 replicates.
</tableCaption>
<bodyText confidence="0.999987666666666">
one might be able to train with publicly available
data excluding the Wikipedia revisions that we have
extracted. We refer to these systems as nonWikiRev
systems. The second part of the table shows the f-
scores of systems trained on the Wikipedia revisions
data – either directly on the annotated errors or on
the artificial errors produced using the confusion dis-
tributions derived from these annotated errors. We
refer to this second set of systems as WikiRev sys-
tems. The nonWikiRev systems perform inconsis-
tently, heavily dependent on the characteristics of
the test set in question. On the other hand, it is
obvious that the WikiRev systems — while not al-
ways outperforming the best nonWikiRev systems
— generalize much better across the three test sets.
In fact, for the NUCLE test set, the best WikiRev
system performs as well as the nonWikiRev system
trained on data from the same domain and with iden-
tical error characteristics as the test set. The distri-
butions of errors in the three test sets are not sim-
ilar, and therefore, the stability in performance of
the WikiRev systems cannot be attributed to the hy-
pothesis that the WikiRev training data error distri-
butions are more similar to the test data than any of
the other training corpora. Therefore, we claim that
if a preposition error correction system is to be de-
ployed on data for which the error characteristics are
not known in advance, i.e. most real-world scenar-
ios, training the system using Wikipedia revisions is
likely to be the most robust option.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="evaluation">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9999925">
We examine the results of our experiments in light
of the research questions we posed in Section 1.
</bodyText>
<subsectionHeader confidence="0.99873">
6.1 Which Data Source is More Useful?
</subsectionHeader>
<bodyText confidence="0.97612224">
We wanted to know whether it was better to have
a smaller corpus of carefully annotated corrections,
or a much larger (but automatically generated, and
therefore noisier) error-annotated corpus. We also
wanted to compare this scenario to training on large
amounts of well-edited text. From our experiments,
it is clear that the composition of the test set plays
a major role in answering this question. On a test
set with few corrections (NUCLE), training on well-
edited text (and without using thresholds) performs
particularly poorly. On the other hand, when eval-
uating on the FCE test set which contains far more
errors, training on well-edited text performs reason-
ably well (though statistically significantly worse
than training on all of the Wikipedia errors). Sim-
ilarly, training on the smaller, high-quality NU-
CLE corpus and evaluating on the NUCLE test set
achieves good results, however training on NUCLE
and testing on FCE achieves the lowest f-score of all
our systems on that test set.
Figure 2 shows the learning curves obtained by
increasing the size of the training data for two
of the test sets.13 Although one might assume
13For space reasons, the graph for HOO2011 is omitted. Also
note that the results in Table 2 may not appear in the graph,
</bodyText>
<page confidence="0.988933">
512
</page>
<figure confidence="0.986818705882353">
25
20
15
10
5
0
(a) NUCLE
Wiki (All)
Wiki (Clean)
Lang-8
NUCLE
Lexile (artificial via Wiki)
Lexile (artificial via Lang-8)
F-score F-score
1 2 3 4
1 2 3 4
log(training data size in thousands of instances)
</figure>
<figureCaption confidence="0.994434">
Figure 2: The effect of varying the size of the training corpus
</figureCaption>
<figure confidence="0.999481692307692">
(b) FCE
Wiki (All)
Wiki (Clean)
Lang-8
NUCLE
Lexile (artificial via Wiki)
Lexile (artificial via Lang-8)
50
40
30
20
10
0
</figure>
<bodyText confidence="0.997930176470588">
that Wikipedia-clean would be more reliable than
Wikipedia-all, the cleanness of the Wikipedia data
seems to make very little difference, probably be-
cause the data extracted in the dirty contexts is not
as noisy as we expected. Interestingly, it also seems
that additional data would lead to further improve-
ments for models trained on artificial errors in Lexile
data and for those trained on all of the automatically
extracted Wikipedia errors.
Another interesting aspect of Figure 2 is that
since we were sampling at specific data points which did not
correspond exactly to the total sizes of the training corpora.
training on the Lang-8 data shows a very steep rising
trend. This suggests that automatically-scraped data
that is highly targeted towards language learners is
very useful in correcting preposition errors in texts
where they are reasonably frequent.
</bodyText>
<subsectionHeader confidence="0.998264">
6.2 Natural or Artificially Generated Errors?
</subsectionHeader>
<bodyText confidence="0.9983186">
Table 2 shows that training on artificially generated
errors via Wikipedia revisions performs fairly con-
sistently across test corpora. While using Lang-8
for artificial error generation is also quite promis-
ing for FCE, it does not generalize across test sets.
</bodyText>
<page confidence="0.995302">
513
</page>
<figure confidence="0.9811715">
F-score F-score
0 5 10 15 20 25 30 35 40 45 50 55
0 5 10 15 20 25 30 35 40 45 50 55
Percentage of Errors in Training Data
</figure>
<figureCaption confidence="0.998267">
Figure 3: The effect of varying the percentage of errors in the training corpus
</figureCaption>
<figure confidence="0.996998791666667">
(a) NUCLE
Wiki (All)
Wiki (Clean)
Lang-8
Lexile (artificial via Wiki)
Lexile (artificial via Lang-8)
40
30
50
20
10
0
(b) FCE
Wiki (All)
Wiki (Clean)
Lang-8
Lexile (artificial via Wiki)
Lexile (artificial via Lang-8)
30
25
20
15
10
5
</figure>
<bodyText confidence="0.993342846153846">
On FCE it achieves the highest results, on NUCLE
it performs statistically significantly worse than the
best system, and on HOO 2011 it achieves a lower
(though not statistically significant) result than the
best system. This highlights that extracting errors
from Wikipedia is useful in two ways: (1) training a
system on the errors alone works well and (2) gener-
ating artificial errors in well-edited corpora of differ-
ent domains and training a system on that also works
well. It also indicates that if the system were to be
applied to a specific domain, applying the confusion
distributions to a domain specific corpus – if avail-
able – would likely yield the best results.
</bodyText>
<subsectionHeader confidence="0.999524">
6.3 Mismatching Distributions
</subsectionHeader>
<bodyText confidence="0.996797444444444">
The proportion of errors in the training and test data
plays an important role in the performance of any
preposition error correction system. This is clearly
evident by comparing system performances across
the three test sets which have fairly different compo-
sitions. FCE contains a much higher proportion of
errors than NUCLE, and HOO falls somewhere in
between. Interestingly, the system trained on Lang-
8 data (which contains the highest proportion of er-
</bodyText>
<page confidence="0.994633">
514
</page>
<bodyText confidence="0.999965153846154">
rors among all training corpora) performs best on
the FCE data. On the other hand, the same sys-
tem performs poorly on NUCLE test which contains
far fewer errors. In this instance, the system learns
to predict an incorrect preposition too often. We
see a similar pattern with the system trained on the
NUCLE training data. It performs poorly on FCE
which contains many errors, but well on NUCLE
test which contains a similar proportion of errors.
In order to better understand the relationship be-
tween the percentage of errors in the training data
and system performance, we vary the percentage of
errors in each training corpus from 1-50% and test
on the unchanged FCE and NUCLE test corpora.
For each training corpus, we reduce the size to be
twice the size of the total number of errors.14 Keep-
ing this size constant, we then artificially change the
percentage of errors. Note that because the total size
of the corpus has changed, the results in Table 2 may
not appear in the graph. Figure 3 shows the effect on
f-score when the data composition is changed. For
both test sets, there is a peak after which increas-
ing the proportion of errors in the training corpus is
detrimental. For NUCLE test with its low number
of preposition errors, this peak is very pronounced.
For FCE, it is more of a gentle degradation in per-
formance, but the pattern is clear. Also noteworthy
is the fact that the degradation for models trained on
artificial errors is less steep suggesting that they may
be more stable across test sets.
In general, these results indicate that when
building a preposition error detection using error-
annotated data, the characteristics of the data to
which the system will be applied should play a vital
role in how the system is to be trained. Our results
show that the WikiRev systems are robust across
test sets, however if the exact distribution of errors
in the data is known in advance, other models may
perform better.
</bodyText>
<sectionHeader confidence="0.993768" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.980795315789474">
Although previous approaches to preposition er-
ror correction using either well-edited text or small
hand-annotated corrections performed well on some
specific test set, they did not generalize well across
14We omit the NUCLE train corpus from this comparison,
because it contains too few errors to obtain a meaningful result.
very different test sets. In this paper, we present
work that automatically extracts preposition error
corrections from Wikipedia Revisions and uses it
to build robust error correction systems. We show
that this data is useful for two purposes. Firstly, a
model trained directly on the corrections performs
well across test sets. Secondly, models trained on ar-
tificial errors generated from the distribution of con-
fusions in the Wikipedia data perform equally well.
The distribution of confusions can also be applied to
other well-edited corpora in different domains, pro-
viding a very powerful method of automatically gen-
erating error corpora. The results of our experiments
also highlight the importance of the distribution of
expected errors in the test set. Models that perform
well on one kind of distribution may not necessar-
ily work on a completely different one, as evident
in the performances of the systems trained on either
Lang-8 or NUCLE. In general, the WikiRev mod-
els perform well across distributions. We also con-
ducted some preliminary system combination exper-
iments and found that while they yielded promising
results, further investigation is necessary. We have
also made the Wikipedia preposition correction cor-
pus available for download.15
In future work, we will examine whether the
results we obtain for English generalize to other
Wikipedia languages. We also plan to extract multi-
word corrections for other types of errors and to ex-
amine the usefulness of including error contexts in
our confusion distributions (e.g., preposition confu-
sions following verbs versus those following nouns).
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999931571428571">
The authors would like to thank Daniel Dahlmeier,
Torsten Zesch, Mamoru Komachi, Tajiri Toshikazu,
Tomoya Mizumoto and Yuji Matsumoto for provid-
ing scripts and data that enabled us to carry out
this research. We would also like to thank Martin
Chodorow and the anonymous reviewers for their
helpful suggestions and comments.
</bodyText>
<sectionHeader confidence="0.996915" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.8911175">
Elena Cabrio, Bernardo Magnini, and Angelina Ivanova.
2012. Extracting Context-Rich Entailment Rules from
</reference>
<footnote confidence="0.701119">
15http://bit.ly/etsprepdata
</footnote>
<page confidence="0.993356">
515
</page>
<reference confidence="0.999249287037037">
Wikipedia Revision History. In Proceedings of the 3rd
Workshop on the People’s Web Meets NLP: Collabora-
tively Constructed Semantic Resources and their Ap-
plications to NLP, pages 34–43, Jeju, Republic of Ko-
rea, July. Association for Computational Linguistics.
Martin Chodorow, Markus Dickinson, Ross Israel, and
Joel Tetreault. 2012. Problems in Evaluating Gram-
matical Error Detection Systems. In Proceedings of
COLING 2012, pages 611–628, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical Error Correction with Alternating Structure Op-
timization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 915–923, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. In Pro-
ceedings of the Generation Challenges Session at the
13th European Workshop on Natural Language Gener-
ation, pages 242–249, Nancy, France, September. As-
sociation for Computational Linguistics.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition
and Determiner Error Correction Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54–62,
Montr´eal, Canada, June. Association for Computa-
tional Linguistics.
Rachele De Felice and Stephen G. Pulman. 2009. Auto-
matic detection of preposition errors in learner writing.
CALICO Journal, 26(3):512–528.
Camille Dutrey, Houda Bouamor, Delphine Bernhard,
and Aur´elien Max. 2011. Local modifications and
paraphrases in Wikipedias revision history. SEPLN
journal(Revista de Procesamiento del Lenguaje Nat-
ural), 46:51–58.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.
2011. Wikipedia Revision Toolkit: Efficiently Access-
ing Wikipedia’s Edit History. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies.
System Demonstrations.
Jennifer Foster and Oistein Andersen. 2009. Gen-
ERRate: Generating Errors for Use in Grammatical
Error Detection. In Proceedings of the Fourth Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 82–90, Boulder, Colorado,
June. Association for Computational Linguistics.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex
Klementiev, William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using Contextual Speller
Techniques and Language Modeling for ESL Error
Correction. In Proceedings of the International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 449–456, Hyderabad, India.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners’ Writing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 163–171, Los An-
geles, California, June. Association for Computational
Linguistics.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115–129.
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using Error-Annotated ESL Data
to Develop an ESL Error Correction System. In Pro-
ceedings of the Seventh International Conference on
Language Resources and Evaluation (LREC), Malta.
Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and
Hitoshi Nishikawa. 2012. Grammar Error Correc-
tion Using Pseudo-Error Sentences and Domain Adap-
tation. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 388–392, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai
Supnithi, and Hitoshi Isahara. 2003. Automatic Error
Detection in the Japanese Learners’ English Spoken
Data. In The Companion Volume to the Proceedings
of 41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 145–148, Sapporo, Japan,
July. Association for Computational Linguistics.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
Claypool.
Aur´elien Max and Guillaume Wisniewski. 2010. Mining
Naturally-occurring Corrections and Paraphrases from
Wikipedia’s Revision History. In Nicoletta Calzo-
lari (Conference Chair), Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the Seventh conference on International Lan-
guage Resources and Evaluation (LREC’10), Valletta,
Malta, may. European Language Resources Associa-
tion (ELRA).
Rami Nelken and Elif Yamangil. 2008. Mining
Wikipedias Article Revision History for Training
</reference>
<page confidence="0.981961">
516
</page>
<reference confidence="0.99984953968254">
Computational Linguistics Algorithms. In Proceed-
ings of the 1st AAAI Workshop on Wikipedia and Arti-
ficial Intelligence, pages 31–36, Chicago, IL.
Alla Rozovskaya and Dan Roth. 2010. Generating Con-
fusion Sets for Context-Sensitive Error Correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 961–
970, Cambridge, MA, October. Association for Com-
putational Linguistics.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and Aspect Error Correction
for ESL Learners Using Global Context. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), Short Papers, pages
198–202, Jeju Island, Korea.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 865–872, Manchester, UK.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using Parse Features for Preposition Selection
and Error Detection. In Proceedings of the ACL 2010
Conference Short Papers, pages 353–358, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich Part-of-speech
Tagging with a Cyclic Dependency Network. In Pro-
ceedings of NAACL, pages 173–180.
Elif Yamangil and Rani Nelken. 2008. Mining
Wikipedia Revision Histories for Improving Sentence
Compression. In Proceedings of ACL-08: HLT, Short
Papers, pages 137–140, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automati-
cally Grading ESOL Texts. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 180–189, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: Unsupervised extraction of lexical simplifications
from Wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 365–368, Los Angeles, California, June.
Association for Computational Linguistics.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
Wikipedia using co-training. In Proceedings of the
2nd Workshop on The People’s Web Meets NLP: Col-
laboratively Constructed Semantic Resources, pages
28–36, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Torsten Zesch. 2012. Measuring Contextual Fitness Us-
ing Error Contexts Extracted from the Wikipedia Revi-
sion History. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 529–538, Avignon, France,
April. Association for Computational Linguistics.
</reference>
<page confidence="0.996869">
517
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.425448">
<title confidence="0.999172">Robust Systems for Preposition Error Correction Using Wikipedia Revisions</title>
<author confidence="0.977623">Nitin Joel Diane</author>
<address confidence="0.490717">Testing Service, 660 Rosedale Road, Princeton, NJ 08541,</address>
<email confidence="0.89509">nmadnani,</email>
<address confidence="0.960819">Communications, Inc., 1198 E. Arques Ave, Sunnyvale, CA 94085,</address>
<email confidence="0.994842">Joel.Tetreault@nuance.com</email>
<abstract confidence="0.999518615384616">We show that existing methods for training preposition error correction systems, whether using well-edited text or error-annotated corpora, do not generalize across very different test sets. We present a new, large errorannotated corpus and use it to train systems that generalize across three different test sets, each from a different domain and with different error characteristics. This new corpus is automatically extracted from Wikipedia revisions and contains over one million instances of preposition corrections.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Elena Cabrio</author>
<author>Bernardo Magnini</author>
<author>Angelina Ivanova</author>
</authors>
<title>Extracting Context-Rich Entailment Rules from Wikipedia Revision History.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop on the People’s Web Meets NLP: Collaboratively Constructed Semantic Resources and their Applications to NLP,</booktitle>
<pages>34--43</pages>
<institution>of Korea, July. Association for Computational Linguistics.</institution>
<location>Jeju, Republic</location>
<contexts>
<context position="8372" citStr="Cabrio et al., 2012" startWordPosition="1292" endWordPosition="1295">508 (1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at —* in) the refrigerator. (2) [Wiki clean] Also none of the witnesses present (of —* on) those dates supports Ranneft’s claims. (3) [Wiki dirty] ... cirque has a permanent production (to —* at) the Mirage, love. (4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his performances for French tourists (in —* to) Petersburg. Figure 1: Example sentences with preposition errors extracted from Wikipedia revisions. The second preposition is assumed to be the correction. Cabrio et al., 2012). To our knowledge, no one has previously extracted data for training a grammatical error detection system from Wikipedia revisions. 3.2 Extracting Preposition Correction Data from Wikipedia Revisions As the source of our Wikipedia revisions, we used an XML snapshot of Wikipedia generated in July 2011 containing 8,735,890 articles and 288,583,063 revisions.1 We then used the following process to extract preposition errors and their corresponding corrections from this snapshot: Step 1: Extract the plain text versions of all revisions of all articles using the Java Wikipedia Library (Ferschke et</context>
</contexts>
<marker>Cabrio, Magnini, Ivanova, 2012</marker>
<rawString>Elena Cabrio, Bernardo Magnini, and Angelina Ivanova. 2012. Extracting Context-Rich Entailment Rules from Wikipedia Revision History. In Proceedings of the 3rd Workshop on the People’s Web Meets NLP: Collaboratively Constructed Semantic Resources and their Applications to NLP, pages 34–43, Jeju, Republic of Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Markus Dickinson</author>
<author>Ross Israel</author>
<author>Joel Tetreault</author>
</authors>
<title>Problems in Evaluating Grammatical Error Detection Systems.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>611--628</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="18940" citStr="Chodorow et al. (2012)" startWordPosition="2983" endWordPosition="2986">rections Total number of corrections suggested Recall = Number of correct preposition corrections Total number of corrections in test set Note that due to the high volume of unchanged prepositions in the test corpus, we obtain very high accuracies, which are not indicative of true performance, and are not included in our results. The results of our experiments are presented in Table 2.12 The first part of the table shows the fscores of preposition error correction systems that 10We use liblinear (Fan et al., 2008) with the L1-regularized logistic regression solver and default parameters. 11As Chodorow et al. (2012) note, it is not clear how to handle cases where the system predicts a preposition that is neither the same as the writer preposition nor the correct preposition. We count these cases as false positives. 12No thresholds were used in the systems that were trained on well-edited text. Traditionally, thresholds are applied so as to only predict a correction when the system is highly confident. This has the effect of increasing precision at the cost of recall, and sometimes leads to an overall improved f-score. Here we take the prediction of the system, regardless of the confidence, reflecting a l</context>
</contexts>
<marker>Chodorow, Dickinson, Israel, Tetreault, 2012</marker>
<rawString>Martin Chodorow, Markus Dickinson, Ross Israel, and Joel Tetreault. 2012. Problems in Evaluating Grammatical Error Detection Systems. In Proceedings of COLING 2012, pages 611–628, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Grammatical Error Correction with Alternating Structure Optimization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>915--923</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1879" citStr="Dahlmeier and Ng, 2011" startWordPosition="279" endWordPosition="282">he usage of prepositions and articles are influenced by several factors including the local context, the prior discourse and semantics. These errors are better handled by statistical models which potentially require millions of training examples. Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage (Han et al., 2006); 2) training on examples of correct usage and artificially generated errors (Rozovskaya and Roth, 2010); and 3) training on examples of correct usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and </context>
<context position="6702" citStr="Dahlmeier and Ng (2011)" startWordPosition="1032" endWordPosition="1035">daptation for the task of Japanese particle correction. 2.3 Error-Annotated Learner Corpora Recently, error-annotated learner data has become more readily and publicly available allowing models to be trained on both examples of correct usage as well typical learner errors. Han et al. (2010) showed that a preposition error detection and correction system trained on 100,000 annotated preposition errors from the Chungdahm Corpus of Korean Learner English (in addition to 1 million examples of correct usage) outperformed a model trained only on 5 million examples of correct usage. Gamon (2010) and Dahlmeier and Ng (2011) showed that combining models trained separately on examples of correct and incorrect usage could also improve the performance of a preposition error correction system. 3 Mining Wikipedia Revisions for Grammatical Error Corrections 3.1 Related Work Many NLP researchers have taken advantage of the wealth of information available in Wikipedia revisions. Dutrey et al. (2011) define a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types of edits are found in the En</context>
<context position="14391" citStr="Dahlmeier and Ng (2011)" startWordPosition="2241" endWordPosition="2244">ng preference not to change the preposition). If a preposition is changed to something other than the original preposition, all remaining prepositions in the sentence are left unchanged. 4.3 Naturally Occurring Errors We have a number of corpora that contain annotated preposition errors. Note that we are only considering incorrectly selected prepositions, we do not consider missing or extraneous. NUCLE The NUS Corpus of Learner English (NUCLE)5 contains one million words of learner essay text, manually annotated with error tags and corrections. We use the same training, dev and test splits as Dahlmeier and Ng (2011). FCE The CLC FCE Dataset6 is a collection of 1,244 exam scripts written by learners of English as part of the Cambridge ESOL First Certificate in English (Yannakoudakis et al., 2011). It includes demographic metadata about the candidate, a grade for each essay and manuallyannotated error corrections. Wikipedia We use three versions of the preposition errors extracted from the Wikipedia revisions as described in Section 3.2. The first includes corrections where the preposition was the only word corrected in the entire sentence 5http://bit.ly/nuclecorpus 6http://ilexir.co.uk/applications/clc-fc</context>
<context position="16567" citStr="Dahlmeier and Ng, 2011" startWordPosition="2587" endWordPosition="2590"> that the three test sets we use are from entirely different domains: exam scripts from non-native English speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault and Chodorow (2008)9 to evaluate the many ways of using Wikipedia error corrections as described in the Section 4. We use this system since it has been recreated for other work (Dahlmeier and Ng, 2011; Tetreault et al., 2010) and is similar in methodology to Gamon et al. (2008) 7Tajiri et al. (2012) extract a corpus of English verb phrases corrected for tense/aspect errors from Lang-8. They kindly provided us with their scripts to carry out the scraping of Lang-8. 8The results of the HOO 2011 shared task were not reported at level of preposition selection error, therefore it is not possible to compare the results presented in this paper with those results. 9Note that in that work, the model was evaluated in terms of preposition error detection rather than correction, however the model itse</context>
<context position="17812" citStr="Dahlmeier and Ng (2011)" startWordPosition="2796" endWordPosition="2799">d De Felice and Pulman (2009). In short, the method models the problem of preposition error correction (for replacement errors) as a 36-way classification problem using a multinomial logistic regression model.10 The system uses 25 lexical, syntactic and n-gram features derived from the contexts of each preposition training instance. We modified the training paradigm of Tetreault and Chodorow (2008) so that a model could be trained on examples of correct usage as well as actual errors. We did this by adding a new feature specifying the writer’s original preposition (as in Han et al. (2010) and Dahlmeier and Ng (2011)). 5.1 Results We train a preposition correction system using each of the three data paradigms and test on the FCE, NUCLE and HOO 2011 test corpora. For each preposition in the test corpus, we record whether the system predicted that it should be changed, and if so, what it should be changed to. We then compare the prediction to the annotation in the test corpus. We report results in terms of f-score, where precision and recall are calculated as follows:11 Precision = Number of correct preposition corrections Total number of corrections suggested Recall = Number of correct preposition correcti</context>
</contexts>
<marker>Dahlmeier, Ng, 2011</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammatical Error Correction with Alternating Structure Optimization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 915–923, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Helping Our Own: The HOO 2011 Pilot Shared Task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>242--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Nancy, France,</location>
<contexts>
<context position="15766" citStr="Dale and Kilgarriff, 2011" startWordPosition="2454" endWordPosition="2457"> on either side of the preposition (�clean). The third contains all corrections regardless of any other changes in the surrounding context (all). Lang-8 The Lang-8 website contains journals written by language learners, where native speakers highlight and correct errors on a sentence-bysentence basis. As a result, it contains typical grammatical mistakes made by language learners, which can be easily downloaded. We automatically extract 75,622 sentences with preposition errors and corrections from the first million journal entries.7 HOO 2011 We take the test set from the HOO 2011 shared task (Dale and Kilgarriff, 2011) and extract all examples of preposition selection errors. The texts are fragments of ACL papers that have been manually annotated for grammatical errors.8 It is important to note that the three test sets we use are from entirely different domains: exam scripts from non-native English speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault</context>
</contexts>
<marker>Dale, Kilgarriff, 2011</marker>
<rawString>Robert Dale and Adam Kilgarriff. 2011. Helping Our Own: The HOO 2011 Pilot Shared Task. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 242–249, Nancy, France, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ilya Anisimoff</author>
<author>George Narroway</author>
</authors>
<title>HOO 2012: A Report on the Preposition and Determiner Error Correction Shared Task.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>54--62</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1899" citStr="Dale et al., 2012" startWordPosition="283" endWordPosition="286"> and articles are influenced by several factors including the local context, the prior discourse and semantics. These errors are better handled by statistical models which potentially require millions of training examples. Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage (Han et al., 2006); 2) training on examples of correct usage and artificially generated errors (Rozovskaya and Roth, 2010); and 3) training on examples of correct usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and spelling correction </context>
</contexts>
<marker>Dale, Anisimoff, Narroway, 2012</marker>
<rawString>Robert Dale, Ilya Anisimoff, and George Narroway. 2012. HOO 2012: A Report on the Preposition and Determiner Error Correction Shared Task. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 54–62, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachele De Felice</author>
<author>Stephen G Pulman</author>
</authors>
<title>Automatic detection of preposition errors in learner writing.</title>
<date>2009</date>
<journal>CALICO Journal,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>De Felice, Pulman, 2009</marker>
<rawString>Rachele De Felice and Stephen G. Pulman. 2009. Automatic detection of preposition errors in learner writing. CALICO Journal, 26(3):512–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Camille Dutrey</author>
<author>Houda Bouamor</author>
<author>Delphine Bernhard</author>
<author>Aur´elien Max</author>
</authors>
<title>Local modifications and paraphrases in Wikipedias revision history.</title>
<date>2011</date>
<booktitle>SEPLN journal(Revista de Procesamiento del Lenguaje Natural),</booktitle>
<pages>46--51</pages>
<contexts>
<context position="7076" citStr="Dutrey et al. (2011)" startWordPosition="1088" endWordPosition="1091">notated preposition errors from the Chungdahm Corpus of Korean Learner English (in addition to 1 million examples of correct usage) outperformed a model trained only on 5 million examples of correct usage. Gamon (2010) and Dahlmeier and Ng (2011) showed that combining models trained separately on examples of correct and incorrect usage could also improve the performance of a preposition error correction system. 3 Mining Wikipedia Revisions for Grammatical Error Corrections 3.1 Related Work Many NLP researchers have taken advantage of the wealth of information available in Wikipedia revisions. Dutrey et al. (2011) define a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types of edits are found in the English Wikipedia. The data extracted from Wikipedia revisions has been used for a wide variety of tasks including spelling correction (Max and Wisniewski, 2010; Zesch, 2012), lexical error detection (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008), lexical simplificati</context>
</contexts>
<marker>Dutrey, Bouamor, Bernhard, Max, 2011</marker>
<rawString>Camille Dutrey, Houda Bouamor, Delphine Bernhard, and Aur´elien Max. 2011. Local modifications and paraphrases in Wikipedias revision history. SEPLN journal(Revista de Procesamiento del Lenguaje Natural), 46:51–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="18837" citStr="Fan et al., 2008" startWordPosition="2969" endWordPosition="2972">re precision and recall are calculated as follows:11 Precision = Number of correct preposition corrections Total number of corrections suggested Recall = Number of correct preposition corrections Total number of corrections in test set Note that due to the high volume of unchanged prepositions in the test corpus, we obtain very high accuracies, which are not indicative of true performance, and are not included in our results. The results of our experiments are presented in Table 2.12 The first part of the table shows the fscores of preposition error correction systems that 10We use liblinear (Fan et al., 2008) with the L1-regularized logistic regression solver and default parameters. 11As Chodorow et al. (2012) note, it is not clear how to handle cases where the system predicts a preposition that is neither the same as the writer preposition nor the correct preposition. We count these cases as false positives. 12No thresholds were used in the systems that were trained on well-edited text. Traditionally, thresholds are applied so as to only predict a correction when the system is highly confident. This has the effect of increasing precision at the cost of recall, and sometimes leads to an overall im</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Ferschke</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Wikipedia Revision Toolkit: Efficiently Accessing Wikipedia’s Edit History.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<contexts>
<context position="8983" citStr="Ferschke et al., 2011" startWordPosition="1386" endWordPosition="1389"> al., 2012). To our knowledge, no one has previously extracted data for training a grammatical error detection system from Wikipedia revisions. 3.2 Extracting Preposition Correction Data from Wikipedia Revisions As the source of our Wikipedia revisions, we used an XML snapshot of Wikipedia generated in July 2011 containing 8,735,890 articles and 288,583,063 revisions.1 We then used the following process to extract preposition errors and their corresponding corrections from this snapshot: Step 1: Extract the plain text versions of all revisions of all articles using the Java Wikipedia Library (Ferschke et al., 2011). Step 2: For each Wikipedia article, compare each revision with the revision immediately preceding it using an efficient diff algorithm.2 Step 3: Compute all 1-word edit chains for the article, i.e., sequences of related edits derived from all revisions of the same article. For example, say revision 10 of an article inserts the preposition of into a sentence and revision 12 changes that preposition to on. Assuming that no other revisions change this sentence, the corresponding edit chain would contain the following 3 elements: E—*of—*on. The extracted chains contain the full context on either</context>
</contexts>
<marker>Ferschke, Zesch, Gurevych, 2011</marker>
<rawString>Oliver Ferschke, Torsten Zesch, and Iryna Gurevych. 2011. Wikipedia Revision Toolkit: Efficiently Accessing Wikipedia’s Edit History. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Oistein Andersen</author>
</authors>
<title>GenERRate: Generating Errors for Use in Grammatical Error Detection.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>82--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="12572" citStr="Foster and Andersen (2009)" startWordPosition="1950" endWordPosition="1953">78,802 266,275 (14.4%) Lang-8 129,987 53,493 (41.2%) NUCLE Train 72,741 922 (1.3%) Test Corpora NUCLE Test 9,366 125 (1.3%) FCE 33,243 2,900 (8.7%) HOO 2011 Test 1,703 81 (4.8%) Table 1: Corpora characteristics Chodorow (2008), comprising roughly 1.8 million sentences from the San Jose Mercury News Corpus3 and roughly 1.8 million sentences from grades 11 and 12 of the MetaMetrics Lexile Corpus. Our second corpus is a random sample of 10 million sentences containing at least one preposition from the June 2012 snapshot of English Wikipedia Articles.4 4.2 Artificially Generated Errors Similar to Foster and Andersen (2009) and Rozovskaya and Roth (2010), we artificially introduce preposition errors into well-edited corpora (the two described above). We do this based on a distribution of possible confusions and train a model that is aware of the corrections. The two sets of confusion distributions we used were derived based on the errors extracted from Wikipedia revisions and Lang-8 respectively (discussed in Section 4.3). For each corrected preposition pi in the revision data, we calculated P(pi|pj), where pj is each of the possible original prepositions that were confused with pi. Then, for each sentence in th</context>
</contexts>
<marker>Foster, Andersen, 2009</marker>
<rawString>Jennifer Foster and Oistein Andersen. 2009. GenERRate: Generating Errors for Use in Grammatical Error Detection. In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications, pages 82–90, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Jianfeng Gao</author>
<author>Chris Brockett</author>
<author>Alex Klementiev</author>
<author>William B Dolan</author>
<author>Dmitriy Belenko</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Using Contextual Speller Techniques and Language Modeling for ESL Error Correction.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>449--456</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="4625" citStr="Gamon et al., 2008" startWordPosition="709" endWordPosition="712">n welledited text? 3. What is the impact of having a mismatch in the error distributions of the training and test sets? 2 Related Work In this section, we only review work in preposition error correction in terms of the three training paradigms and refer the reader to Leacock et al. (2010) for a more comprehensive review of the field. 2.1 Training on Well-Edited Text Early approaches to error detection and correction did not have access to large amounts of errorannotated data to train statistical models and thus, systems were trained on millions of well-edited examples from news text instead (Gamon et al., 2008; Tetreault and Chodorow, 2008; De Felice and Pulman, 2009). Feature sets usually consisted of ngrams around the preposition, POS sequences, syntactic features and semantic information. Since the model only had knowledge of correct usage, an error was flagged if the system’s prediction for a particular preposition context differed from the preposition the writer used. 2.2 Artificial Errors The issue with training solely on correct usage was that the systems had no knowledge of typical learner errors. Ideally, a system would be trained on examples of correct and incorrect usage, however, for ma</context>
<context position="16645" citStr="Gamon et al. (2008)" startWordPosition="2601" endWordPosition="2604">s from non-native English speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault and Chodorow (2008)9 to evaluate the many ways of using Wikipedia error corrections as described in the Section 4. We use this system since it has been recreated for other work (Dahlmeier and Ng, 2011; Tetreault et al., 2010) and is similar in methodology to Gamon et al. (2008) 7Tajiri et al. (2012) extract a corpus of English verb phrases corrected for tense/aspect errors from Lang-8. They kindly provided us with their scripts to carry out the scraping of Lang-8. 8The results of the HOO 2011 shared task were not reported at level of preposition selection error, therefore it is not possible to compare the results presented in this paper with those results. 9Note that in that work, the model was evaluated in terms of preposition error detection rather than correction, however the model itself does not change. and De Felice and Pulman (2009). In short, the method mode</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>Michael Gamon, Jianfeng Gao, Chris Brockett, Alex Klementiev, William B. Dolan, Dmitriy Belenko, and Lucy Vanderwende. 2008. Using Contextual Speller Techniques and Language Modeling for ESL Error Correction. In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP), pages 449–456, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>Using Mostly Native Data to Correct Errors in Learners’ Writing. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>163--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="6674" citStr="Gamon (2010)" startWordPosition="1029" endWordPosition="1030">dressing domain adaptation for the task of Japanese particle correction. 2.3 Error-Annotated Learner Corpora Recently, error-annotated learner data has become more readily and publicly available allowing models to be trained on both examples of correct usage as well typical learner errors. Han et al. (2010) showed that a preposition error detection and correction system trained on 100,000 annotated preposition errors from the Chungdahm Corpus of Korean Learner English (in addition to 1 million examples of correct usage) outperformed a model trained only on 5 million examples of correct usage. Gamon (2010) and Dahlmeier and Ng (2011) showed that combining models trained separately on examples of correct and incorrect usage could also improve the performance of a preposition error correction system. 3 Mining Wikipedia Revisions for Grammatical Error Corrections 3.1 Related Work Many NLP researchers have taken advantage of the wealth of information available in Wikipedia revisions. Dutrey et al. (2011) define a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types </context>
</contexts>
<marker>Gamon, 2010</marker>
<rawString>Michael Gamon. 2010. Using Mostly Native Data to Correct Errors in Learners’ Writing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 163–171, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Na-Rae Han</author>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="1681" citStr="Han et al., 2006" startWordPosition="245" endWordPosition="248">n and test a system. Some errors, such as determinernoun number agreement, are easily corrected using rules and regular expressions (Leacock et al., 2010). On the other hand, errors involving the usage of prepositions and articles are influenced by several factors including the local context, the prior discourse and semantics. These errors are better handled by statistical models which potentially require millions of training examples. Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage (Han et al., 2006); 2) training on examples of correct usage and artificially generated errors (Rozovskaya and Roth, 2010); and 3) training on examples of correct usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extrac</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in English article usage by non-native speakers. Natural Language Engineering, 12(2):115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Na-Rae Han</author>
<author>Joel Tetreault</author>
<author>Soo-Hwa Lee</author>
<author>JinYoung Ha</author>
</authors>
<title>Using Error-Annotated ESL Data to Develop an ESL Error Correction System.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC),</booktitle>
<contexts>
<context position="6370" citStr="Han et al. (2010)" startWordPosition="978" endWordPosition="981">ent ways of generating artificial errors and found that a system trained on artificial errors could outperform the more traditional training paradigm of using only well-edited texts. Most recently, Imamura et al. (2012) showed that performance could be improved by training a model on artificial errors and addressing domain adaptation for the task of Japanese particle correction. 2.3 Error-Annotated Learner Corpora Recently, error-annotated learner data has become more readily and publicly available allowing models to be trained on both examples of correct usage as well typical learner errors. Han et al. (2010) showed that a preposition error detection and correction system trained on 100,000 annotated preposition errors from the Chungdahm Corpus of Korean Learner English (in addition to 1 million examples of correct usage) outperformed a model trained only on 5 million examples of correct usage. Gamon (2010) and Dahlmeier and Ng (2011) showed that combining models trained separately on examples of correct and incorrect usage could also improve the performance of a preposition error correction system. 3 Mining Wikipedia Revisions for Grammatical Error Corrections 3.1 Related Work Many NLP researcher</context>
<context position="17784" citStr="Han et al. (2010)" startWordPosition="2791" endWordPosition="2794">lf does not change. and De Felice and Pulman (2009). In short, the method models the problem of preposition error correction (for replacement errors) as a 36-way classification problem using a multinomial logistic regression model.10 The system uses 25 lexical, syntactic and n-gram features derived from the contexts of each preposition training instance. We modified the training paradigm of Tetreault and Chodorow (2008) so that a model could be trained on examples of correct usage as well as actual errors. We did this by adding a new feature specifying the writer’s original preposition (as in Han et al. (2010) and Dahlmeier and Ng (2011)). 5.1 Results We train a preposition correction system using each of the three data paradigms and test on the FCE, NUCLE and HOO 2011 test corpora. For each preposition in the test corpus, we record whether the system predicted that it should be changed, and if so, what it should be changed to. We then compare the prediction to the annotation in the test corpus. We report results in terms of f-score, where precision and recall are calculated as follows:11 Precision = Number of correct preposition corrections Total number of corrections suggested Recall = Number of </context>
</contexts>
<marker>Han, Tetreault, Lee, Ha, 2010</marker>
<rawString>Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and JinYoung Ha. 2010. Using Error-Annotated ESL Data to Develop an ESL Error Correction System. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC), Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Imamura</author>
<author>Kuniko Saito</author>
<author>Kugatsu Sadamitsu</author>
<author>Hitoshi Nishikawa</author>
</authors>
<title>Grammar Error Correction Using Pseudo-Error Sentences and Domain Adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>388--392</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="5972" citStr="Imamura et al. (2012)" startWordPosition="917" endWordPosition="920">he error distributions derived from the error-annotated learner corpora available at the time. Izumi et al. (2003) was the first to evaluate a model trained on incorrect usage as well as artificial errors for the task of correcting several different error types, including prepositions. However, with limited training data, system performance was quite poor. Rozovskaya and Roth (2010) evaluated different ways of generating artificial errors and found that a system trained on artificial errors could outperform the more traditional training paradigm of using only well-edited texts. Most recently, Imamura et al. (2012) showed that performance could be improved by training a model on artificial errors and addressing domain adaptation for the task of Japanese particle correction. 2.3 Error-Annotated Learner Corpora Recently, error-annotated learner data has become more readily and publicly available allowing models to be trained on both examples of correct usage as well typical learner errors. Han et al. (2010) showed that a preposition error detection and correction system trained on 100,000 annotated preposition errors from the Chungdahm Corpus of Korean Learner English (in addition to 1 million examples of</context>
</contexts>
<marker>Imamura, Saito, Sadamitsu, Nishikawa, 2012</marker>
<rawString>Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and Hitoshi Nishikawa. 2012. Grammar Error Correction Using Pseudo-Error Sentences and Domain Adaptation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 388–392, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emi Izumi</author>
<author>Kiyotaka Uchimoto</author>
<author>Toyomi Saiga</author>
<author>Thepchai Supnithi</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Automatic Error Detection in the Japanese Learners’ English Spoken Data.</title>
<date>2003</date>
<booktitle>In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>145--148</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="5465" citStr="Izumi et al. (2003)" startWordPosition="839" endWordPosition="842">of correct usage, an error was flagged if the system’s prediction for a particular preposition context differed from the preposition the writer used. 2.2 Artificial Errors The issue with training solely on correct usage was that the systems had no knowledge of typical learner errors. Ideally, a system would be trained on examples of correct and incorrect usage, however, for many years, such error-annotated corpora were not available. Instead, several researchers generated artificial errors based on the error distributions derived from the error-annotated learner corpora available at the time. Izumi et al. (2003) was the first to evaluate a model trained on incorrect usage as well as artificial errors for the task of correcting several different error types, including prepositions. However, with limited training data, system performance was quite poor. Rozovskaya and Roth (2010) evaluated different ways of generating artificial errors and found that a system trained on artificial errors could outperform the more traditional training paradigm of using only well-edited texts. Most recently, Imamura et al. (2012) showed that performance could be improved by training a model on artificial errors and addre</context>
</contexts>
<marker>Izumi, Uchimoto, Saiga, Supnithi, Isahara, 2003</marker>
<rawString>Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Supnithi, and Hitoshi Isahara. 2003. Automatic Error Detection in the Japanese Learners’ English Spoken Data. In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics, pages 145–148, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
<author>Michael Gamon</author>
<author>Joel Tetreault</author>
</authors>
<title>Automated Grammatical Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies.</title>
<date>2010</date>
<publisher>Morgan Claypool.</publisher>
<contexts>
<context position="1218" citStr="Leacock et al., 2010" startWordPosition="173" endWordPosition="176">ed corpus and use it to train systems that generalize across three different test sets, each from a different domain and with different error characteristics. This new corpus is automatically extracted from Wikipedia revisions and contains over one million instances of preposition corrections. 1 Introduction One of the main themes that has defined the field of automatic grammatical error correction has been the availability of error-annotated learner data to train and test a system. Some errors, such as determinernoun number agreement, are easily corrected using rules and regular expressions (Leacock et al., 2010). On the other hand, errors involving the usage of prepositions and articles are influenced by several factors including the local context, the prior discourse and semantics. These errors are better handled by statistical models which potentially require millions of training examples. Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage (Han et al., 2006); 2) training on examples of correct usage and artificially generated errors (Rozovskaya and Roth, 2010); and 3) training on examples of </context>
<context position="4297" citStr="Leacock et al. (2010)" startWordPosition="655" endWordPosition="658"> noisy error-annotated data (either artificially generated or automatically extracted) or a smaller amount of higher quality error-annotated data? 2. Given error-annotated data, is it better to train on the corrections directly or to use the confusion distributions derived from these corrections for generating artificial errors in welledited text? 3. What is the impact of having a mismatch in the error distributions of the training and test sets? 2 Related Work In this section, we only review work in preposition error correction in terms of the three training paradigms and refer the reader to Leacock et al. (2010) for a more comprehensive review of the field. 2.1 Training on Well-Edited Text Early approaches to error detection and correction did not have access to large amounts of errorannotated data to train statistical models and thus, systems were trained on millions of well-edited examples from news text instead (Gamon et al., 2008; Tetreault and Chodorow, 2008; De Felice and Pulman, 2009). Feature sets usually consisted of ngrams around the preposition, POS sequences, syntactic features and semantic information. Since the model only had knowledge of correct usage, an error was flagged if the syste</context>
</contexts>
<marker>Leacock, Chodorow, Gamon, Tetreault, 2010</marker>
<rawString>Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2010. Automated Grammatical Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies. Morgan Claypool.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Aur´elien Max</author>
<author>Guillaume Wisniewski</author>
</authors>
<title>Mining Naturally-occurring Corrections and Paraphrases from Wikipedia’s Revision History.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="2446" citStr="Max and Wisniewski, 2010" startWordPosition="365" endWordPosition="368">ect usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and spelling correction (Zesch, 2012), this resource has not been used for the task of grammatical error correction. To evaluate the usefulness of Wikipedia revision history for grammatical error correction, we address the task of correcting errors in preposition selection (i.e., where the context licenses the use of a preposition, but the writer selects the wrong one). We first train a model directly on instances of correct and incorrect preposition usage extracted from the Wikipedia revision data. We also generate artificial errors using the confusion distributio</context>
<context position="7460" citStr="Max and Wisniewski, 2010" startWordPosition="1152" endWordPosition="1155">eposition error correction system. 3 Mining Wikipedia Revisions for Grammatical Error Corrections 3.1 Related Work Many NLP researchers have taken advantage of the wealth of information available in Wikipedia revisions. Dutrey et al. (2011) define a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types of edits are found in the English Wikipedia. The data extracted from Wikipedia revisions has been used for a wide variety of tasks including spelling correction (Max and Wisniewski, 2010; Zesch, 2012), lexical error detection (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008), lexical simplification (Yatskar et al., 2010) and entailment (Zanzotto and Pennacchiotti, 2010; 508 (1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at —* in) the refrigerator. (2) [Wiki clean] Also none of the witnesses present (of —* on) those dates supports Ranneft’s claims. (3) [Wiki dirty] ... cirque has a permanent production (to —* at) the Mirage, love. (4) [Wiki dirt</context>
</contexts>
<marker>Max, Wisniewski, 2010</marker>
<rawString>Aur´elien Max and Guillaume Wisniewski. 2010. Mining Naturally-occurring Corrections and Paraphrases from Wikipedia’s Revision History. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rami Nelken</author>
<author>Elif Yamangil</author>
</authors>
<title>Mining Wikipedias Article Revision History for Training Computational Linguistics Algorithms.</title>
<date>2008</date>
<booktitle>In Proceedings of the 1st AAAI Workshop on Wikipedia and Artificial Intelligence,</booktitle>
<pages>31--36</pages>
<location>Chicago, IL.</location>
<contexts>
<context position="2474" citStr="Nelken and Yamangil, 2008" startWordPosition="369" endWordPosition="372"> errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and spelling correction (Zesch, 2012), this resource has not been used for the task of grammatical error correction. To evaluate the usefulness of Wikipedia revision history for grammatical error correction, we address the task of correcting errors in preposition selection (i.e., where the context licenses the use of a preposition, but the writer selects the wrong one). We first train a model directly on instances of correct and incorrect preposition usage extracted from the Wikipedia revision data. We also generate artificial errors using the confusion distributions derived from this data. W</context>
<context position="7527" citStr="Nelken and Yamangil, 2008" startWordPosition="1161" endWordPosition="1164">or Grammatical Error Corrections 3.1 Related Work Many NLP researchers have taken advantage of the wealth of information available in Wikipedia revisions. Dutrey et al. (2011) define a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types of edits are found in the English Wikipedia. The data extracted from Wikipedia revisions has been used for a wide variety of tasks including spelling correction (Max and Wisniewski, 2010; Zesch, 2012), lexical error detection (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008), lexical simplification (Yatskar et al., 2010) and entailment (Zanzotto and Pennacchiotti, 2010; 508 (1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at —* in) the refrigerator. (2) [Wiki clean] Also none of the witnesses present (of —* on) those dates supports Ranneft’s claims. (3) [Wiki dirty] ... cirque has a permanent production (to —* at) the Mirage, love. (4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took</context>
</contexts>
<marker>Nelken, Yamangil, 2008</marker>
<rawString>Rami Nelken and Elif Yamangil. 2008. Mining Wikipedias Article Revision History for Training Computational Linguistics Algorithms. In Proceedings of the 1st AAAI Workshop on Wikipedia and Artificial Intelligence, pages 31–36, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Dan Roth</author>
</authors>
<title>Generating Confusion Sets for Context-Sensitive Error Correction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>961--970</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="1785" citStr="Rozovskaya and Roth, 2010" startWordPosition="262" endWordPosition="265"> using rules and regular expressions (Leacock et al., 2010). On the other hand, errors involving the usage of prepositions and articles are influenced by several factors including the local context, the prior discourse and semantics. These errors are better handled by statistical models which potentially require millions of training examples. Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage (Han et al., 2006); 2) training on examples of correct usage and artificially generated errors (Rozovskaya and Roth, 2010); and 3) training on examples of correct usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP ta</context>
<context position="5736" citStr="Rozovskaya and Roth (2010)" startWordPosition="881" endWordPosition="884">ical learner errors. Ideally, a system would be trained on examples of correct and incorrect usage, however, for many years, such error-annotated corpora were not available. Instead, several researchers generated artificial errors based on the error distributions derived from the error-annotated learner corpora available at the time. Izumi et al. (2003) was the first to evaluate a model trained on incorrect usage as well as artificial errors for the task of correcting several different error types, including prepositions. However, with limited training data, system performance was quite poor. Rozovskaya and Roth (2010) evaluated different ways of generating artificial errors and found that a system trained on artificial errors could outperform the more traditional training paradigm of using only well-edited texts. Most recently, Imamura et al. (2012) showed that performance could be improved by training a model on artificial errors and addressing domain adaptation for the task of Japanese particle correction. 2.3 Error-Annotated Learner Corpora Recently, error-annotated learner data has become more readily and publicly available allowing models to be trained on both examples of correct usage as well typical</context>
<context position="12603" citStr="Rozovskaya and Roth (2010)" startWordPosition="1955" endWordPosition="1959">29,987 53,493 (41.2%) NUCLE Train 72,741 922 (1.3%) Test Corpora NUCLE Test 9,366 125 (1.3%) FCE 33,243 2,900 (8.7%) HOO 2011 Test 1,703 81 (4.8%) Table 1: Corpora characteristics Chodorow (2008), comprising roughly 1.8 million sentences from the San Jose Mercury News Corpus3 and roughly 1.8 million sentences from grades 11 and 12 of the MetaMetrics Lexile Corpus. Our second corpus is a random sample of 10 million sentences containing at least one preposition from the June 2012 snapshot of English Wikipedia Articles.4 4.2 Artificially Generated Errors Similar to Foster and Andersen (2009) and Rozovskaya and Roth (2010), we artificially introduce preposition errors into well-edited corpora (the two described above). We do this based on a distribution of possible confusions and train a model that is aware of the corrections. The two sets of confusion distributions we used were derived based on the errors extracted from Wikipedia revisions and Lang-8 respectively (discussed in Section 4.3). For each corrected preposition pi in the revision data, we calculated P(pi|pj), where pj is each of the possible original prepositions that were confused with pi. Then, for each sentence in the well-edited text, all preposi</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>Alla Rozovskaya and Dan Roth. 2010. Generating Confusion Sets for Context-Sensitive Error Correction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961– 970, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshikazu Tajiri</author>
<author>Mamoru Komachi</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Tense and Aspect Error Correction for ESL Learners Using Global Context.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), Short Papers,</booktitle>
<pages>198--202</pages>
<location>Jeju Island,</location>
<contexts>
<context position="16667" citStr="Tajiri et al. (2012)" startWordPosition="2605" endWordPosition="2608">lish speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault and Chodorow (2008)9 to evaluate the many ways of using Wikipedia error corrections as described in the Section 4. We use this system since it has been recreated for other work (Dahlmeier and Ng, 2011; Tetreault et al., 2010) and is similar in methodology to Gamon et al. (2008) 7Tajiri et al. (2012) extract a corpus of English verb phrases corrected for tense/aspect errors from Lang-8. They kindly provided us with their scripts to carry out the scraping of Lang-8. 8The results of the HOO 2011 shared task were not reported at level of preposition selection error, therefore it is not possible to compare the results presented in this paper with those results. 9Note that in that work, the model was evaluated in terms of preposition error detection rather than correction, however the model itself does not change. and De Felice and Pulman (2009). In short, the method models the problem of prep</context>
</contexts>
<marker>Tajiri, Komachi, Matsumoto, 2012</marker>
<rawString>Toshikazu Tajiri, Mamoru Komachi, and Yuji Matsumoto. 2012. Tense and Aspect Error Correction for ESL Learners Using Global Context. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), Short Papers, pages 198–202, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel R Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>The Ups and Downs of Preposition Error Detection in ESL Writing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>865--872</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="4655" citStr="Tetreault and Chodorow, 2008" startWordPosition="713" endWordPosition="716">. What is the impact of having a mismatch in the error distributions of the training and test sets? 2 Related Work In this section, we only review work in preposition error correction in terms of the three training paradigms and refer the reader to Leacock et al. (2010) for a more comprehensive review of the field. 2.1 Training on Well-Edited Text Early approaches to error detection and correction did not have access to large amounts of errorannotated data to train statistical models and thus, systems were trained on millions of well-edited examples from news text instead (Gamon et al., 2008; Tetreault and Chodorow, 2008; De Felice and Pulman, 2009). Feature sets usually consisted of ngrams around the preposition, POS sequences, syntactic features and semantic information. Since the model only had knowledge of correct usage, an error was flagged if the system’s prediction for a particular preposition context differed from the preposition the writer used. 2.2 Artificial Errors The issue with training solely on correct usage was that the systems had no knowledge of typical learner errors. Ideally, a system would be trained on examples of correct and incorrect usage, however, for many years, such error-annotated</context>
<context position="16386" citStr="Tetreault and Chodorow (2008)" startWordPosition="2553" endWordPosition="2556">ff, 2011) and extract all examples of preposition selection errors. The texts are fragments of ACL papers that have been manually annotated for grammatical errors.8 It is important to note that the three test sets we use are from entirely different domains: exam scripts from non-native English speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault and Chodorow (2008)9 to evaluate the many ways of using Wikipedia error corrections as described in the Section 4. We use this system since it has been recreated for other work (Dahlmeier and Ng, 2011; Tetreault et al., 2010) and is similar in methodology to Gamon et al. (2008) 7Tajiri et al. (2012) extract a corpus of English verb phrases corrected for tense/aspect errors from Lang-8. They kindly provided us with their scripts to carry out the scraping of Lang-8. 8The results of the HOO 2011 shared task were not reported at level of preposition selection error, therefore it is not possible to compare the result</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Joel R. Tetreault and Martin Chodorow. 2008. The Ups and Downs of Preposition Error Detection in ESL Writing. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 865–872, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Jennifer Foster</author>
<author>Martin Chodorow</author>
</authors>
<title>Using Parse Features for Preposition Selection and Error Detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>353--358</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="16592" citStr="Tetreault et al., 2010" startWordPosition="2591" endWordPosition="2594">s we use are from entirely different domains: exam scripts from non-native English speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault and Chodorow (2008)9 to evaluate the many ways of using Wikipedia error corrections as described in the Section 4. We use this system since it has been recreated for other work (Dahlmeier and Ng, 2011; Tetreault et al., 2010) and is similar in methodology to Gamon et al. (2008) 7Tajiri et al. (2012) extract a corpus of English verb phrases corrected for tense/aspect errors from Lang-8. They kindly provided us with their scripts to carry out the scraping of Lang-8. 8The results of the HOO 2011 shared task were not reported at level of preposition selection error, therefore it is not possible to compare the results presented in this paper with those results. 9Note that in that work, the model was evaluated in terms of preposition error detection rather than correction, however the model itself does not change. and D</context>
</contexts>
<marker>Tetreault, Foster, Chodorow, 2010</marker>
<rawString>Joel Tetreault, Jennifer Foster, and Martin Chodorow. 2010. Using Parse Features for Preposition Selection and Error Detection. In Proceedings of the ACL 2010 Conference Short Papers, pages 353–358, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich Part-of-speech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="10846" citStr="Toutanova et al., 2003" startWordPosition="1685" endWordPosition="1688">here a preposition is replaced with another preposition. If the preposition edit is the only edit in the sentence, we convert the chain into a sentence pair and label it clean. If there are other 1-word edits but not within 5 words of the preposition edit on either side, we label the sentence somewhat clean. Otherwise, we label it dirty. The motivation is that the presence of other nearby edits make the preposition correction less reliable when used in isolation, due to the possible dependencies between corrections. All extracted sentences were part-of-speech tagged using the Stanford Tagger (Toutanova et al., 2003). Using the above process, we are able to extract approximately 2 million sentences containing prepositions errors and their corrections. Some examples of the sentences we extracted are given in Figure 1. Example (4) shows an example of a bad correction. 4 Corpora We use several corpora for training and testing our preposition error correction system. The properties of each are outlined in Table 1, organized by paradigm. For each corpus we report the total number of prepositions used for training, as well as the number and percentage of preposition corrections. 4.1 Well-edited Text We train ou</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich Part-of-speech Tagging with a Cyclic Dependency Network. In Proceedings of NAACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elif Yamangil</author>
<author>Rani Nelken</author>
</authors>
<title>Mining Wikipedia Revision Histories for Improving Sentence Compression.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>137--140</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="7577" citStr="Yamangil and Nelken, 2008" startWordPosition="1167" endWordPosition="1171">Many NLP researchers have taken advantage of the wealth of information available in Wikipedia revisions. Dutrey et al. (2011) define a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types of edits are found in the English Wikipedia. The data extracted from Wikipedia revisions has been used for a wide variety of tasks including spelling correction (Max and Wisniewski, 2010; Zesch, 2012), lexical error detection (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008), lexical simplification (Yatskar et al., 2010) and entailment (Zanzotto and Pennacchiotti, 2010; 508 (1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at —* in) the refrigerator. (2) [Wiki clean] Also none of the witnesses present (of —* on) those dates supports Ranneft’s claims. (3) [Wiki dirty] ... cirque has a permanent production (to —* at) the Mirage, love. (4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his performances for French t</context>
</contexts>
<marker>Yamangil, Nelken, 2008</marker>
<rawString>Elif Yamangil and Rani Nelken. 2008. Mining Wikipedia Revision Histories for Improving Sentence Compression. In Proceedings of ACL-08: HLT, Short Papers, pages 137–140, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Yannakoudakis</author>
<author>Ted Briscoe</author>
<author>Ben Medlock</author>
</authors>
<title>A New Dataset and Method for Automatically Grading ESOL Texts.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>180--189</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="14574" citStr="Yannakoudakis et al., 2011" startWordPosition="2274" endWordPosition="2277">hanged. 4.3 Naturally Occurring Errors We have a number of corpora that contain annotated preposition errors. Note that we are only considering incorrectly selected prepositions, we do not consider missing or extraneous. NUCLE The NUS Corpus of Learner English (NUCLE)5 contains one million words of learner essay text, manually annotated with error tags and corrections. We use the same training, dev and test splits as Dahlmeier and Ng (2011). FCE The CLC FCE Dataset6 is a collection of 1,244 exam scripts written by learners of English as part of the Cambridge ESOL First Certificate in English (Yannakoudakis et al., 2011). It includes demographic metadata about the candidate, a grade for each essay and manuallyannotated error corrections. Wikipedia We use three versions of the preposition errors extracted from the Wikipedia revisions as described in Section 3.2. The first includes corrections where the preposition was the only word corrected in the entire sentence 5http://bit.ly/nuclecorpus 6http://ilexir.co.uk/applications/clc-fce-dataset/ 510 (clean). The second contains all clean corrections, as well as all corrections where there were no other edits within a five-word span on either side of the preposition</context>
</contexts>
<marker>Yannakoudakis, Briscoe, Medlock, 2011</marker>
<rawString>Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A New Dataset and Method for Automatically Grading ESOL Texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 180–189, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mark Yatskar</author>
<author>Bo Pang</author>
<author>Cristian Danescu-NiculescuMizil</author>
<author>Lillian Lee</author>
</authors>
<title>For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>365--368</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="7701" citStr="Yatskar et al., 2010" startWordPosition="1184" endWordPosition="1187">ine a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types of edits are found in the English Wikipedia. The data extracted from Wikipedia revisions has been used for a wide variety of tasks including spelling correction (Max and Wisniewski, 2010; Zesch, 2012), lexical error detection (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008), lexical simplification (Yatskar et al., 2010) and entailment (Zanzotto and Pennacchiotti, 2010; 508 (1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at —* in) the refrigerator. (2) [Wiki clean] Also none of the witnesses present (of —* on) those dates supports Ranneft’s claims. (3) [Wiki dirty] ... cirque has a permanent production (to —* at) the Mirage, love. (4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his performances for French tourists (in —* to) Petersburg. Figure 1: Example sentences with preposition errors extracted from Wikipedia revisions. The s</context>
</contexts>
<marker>Yatskar, Pang, Danescu-NiculescuMizil, Lee, 2010</marker>
<rawString>Mark Yatskar, Bo Pang, Cristian Danescu-NiculescuMizil, and Lillian Lee. 2010. For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 365–368, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Expanding textual entailment corpora from Wikipedia using co-training.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 2nd Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources,</booktitle>
<pages>28--36</pages>
<location>Beijing, China,</location>
<contexts>
<context position="7750" citStr="Zanzotto and Pennacchiotti, 2010" startWordPosition="1190" endWordPosition="1193">in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types of edits are found in the English Wikipedia. The data extracted from Wikipedia revisions has been used for a wide variety of tasks including spelling correction (Max and Wisniewski, 2010; Zesch, 2012), lexical error detection (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008), lexical simplification (Yatskar et al., 2010) and entailment (Zanzotto and Pennacchiotti, 2010; 508 (1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at —* in) the refrigerator. (2) [Wiki clean] Also none of the witnesses present (of —* on) those dates supports Ranneft’s claims. (3) [Wiki dirty] ... cirque has a permanent production (to —* at) the Mirage, love. (4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his performances for French tourists (in —* to) Petersburg. Figure 1: Example sentences with preposition errors extracted from Wikipedia revisions. The second preposition is assumed to be the correction</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, 2010</marker>
<rawString>Fabio Massimo Zanzotto and Marco Pennacchiotti. 2010. Expanding textual entailment corpora from Wikipedia using co-training. In Proceedings of the 2nd Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources, pages 28–36, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
</authors>
<title>Measuring Contextual Fitness Using Error Contexts Extracted from the Wikipedia Revision History.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>529--538</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<contexts>
<context position="2512" citStr="Zesch, 2012" startWordPosition="376" endWordPosition="377"> The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and spelling correction (Zesch, 2012), this resource has not been used for the task of grammatical error correction. To evaluate the usefulness of Wikipedia revision history for grammatical error correction, we address the task of correcting errors in preposition selection (i.e., where the context licenses the use of a preposition, but the writer selects the wrong one). We first train a model directly on instances of correct and incorrect preposition usage extracted from the Wikipedia revision data. We also generate artificial errors using the confusion distributions derived from this data. We compare both of these approaches to </context>
<context position="7474" citStr="Zesch, 2012" startWordPosition="1156" endWordPosition="1157"> system. 3 Mining Wikipedia Revisions for Grammatical Error Corrections 3.1 Related Work Many NLP researchers have taken advantage of the wealth of information available in Wikipedia revisions. Dutrey et al. (2011) define a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types of edits are found in the English Wikipedia. The data extracted from Wikipedia revisions has been used for a wide variety of tasks including spelling correction (Max and Wisniewski, 2010; Zesch, 2012), lexical error detection (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008), lexical simplification (Yatskar et al., 2010) and entailment (Zanzotto and Pennacchiotti, 2010; 508 (1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at —* in) the refrigerator. (2) [Wiki clean] Also none of the witnesses present (of —* on) those dates supports Ranneft’s claims. (3) [Wiki dirty] ... cirque has a permanent production (to —* at) the Mirage, love. (4) [Wiki dirty] In the late</context>
</contexts>
<marker>Zesch, 2012</marker>
<rawString>Torsten Zesch. 2012. Measuring Contextual Fitness Using Error Contexts Extracted from the Wikipedia Revision History. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 529–538, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>