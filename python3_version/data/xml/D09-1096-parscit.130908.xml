<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000179">
<title confidence="0.860359">
Segmenting Email Message Text into Zones
</title>
<note confidence="0.819428">
Andrew Lampert †‡†CSIRO ICT Centre Robert Dale ‡ C´ecile Paris †‡Centre for Language Technology
PO Box 76 rdale@ics.mq.edu.au Macquarie University
Epping 1710, Australia North Ryde 2109, Australia
</note>
<email confidence="0.833742">
andrew.lampert@csiro.au cecile.paris@csiro.au
</email>
<sectionHeader confidence="0.990987" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99969565">
In the early days of email, widely-used
conventions for indicating quoted reply
content and email signatures made it easy
to segment email messages into their func-
tional parts. Today, the explosion of dif-
ferent email formats and styles, coupled
with the ad hoc ways in which people vary
the structure and layout of their messages,
means that simple techniques for identify-
ing quoted replies that used to yield 95%
accuracy now find less than 10% of such
content. In this paper, we describe Zebra,
an SVM-based system for segmenting the
body text of email messages into nine zone
types based on graphic, orthographic and
lexical cues. Zebra performs this task with
an accuracy of 87.01%; when the num-
ber of zones is abstracted to two or three
zone classes, this increases to 93.60% and
91.53% respectively.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980982758621">
Email message bodies consist of different func-
tional parts such as email signatures, quoted re-
ply content and advertising content. We refer to
these as email zones. Many language process-
ing tools stand to benefit from better knowledge
of this message structure, facilitating focus on rel-
evant content in specific parts of a message. In
particular, access to zone information would al-
low email classification, summarisation and anal-
ysis tools to separate or filter out ‘noise’ and focus
on the content in specific zones of a message that
are relevant to the application at hand. Email con-
tact mining tools such as that developed by Culotta
et al. (2004), for example, might access the email
signature zone, while tools that attempt to iden-
tify tasks or action items in email (e.g., (Bellotti
et al., 2003; Corston-Oliver et al., 2004; Bennett
and Carbonell, 2007; Lampert et al., 2007)) might
restrict themselves to the sender-authored and for-
warded content. Despite previous work on this
problem, there are no available tools that can re-
liably extract or identify the different functional
zones of an email message.
While there is no agreed standard set of email
zones, there are clearly different functional parts
within the body text of email messages. For ex-
ample, the content of an email disclaimer is func-
tionally different from the sender-authored content
and from the quoted reply content automatically
included from previous messages in the thread of
conversation. Of course, there are different dis-
tinctions that can be drawn between zones; in
this paper we explore several different categorisa-
tions based on our proposed set of nine underlying
email zones.
Although we focus on content in the body of
email messages, we recognise the presence of use-
ful information in the semi-structured headers, and
indeed make use of header information such as
sender and recipient names in segmenting the un-
structured body text.
Segmenting email messages into zones is a
challenging task. Accurate segmentation is ham-
pered by the lack of standard syntax used by dif-
ferent email clients to indicate different message
parts, and by the ad hoc ways in which people vary
the structure and layout of their messages. When
replying to a message, for example, it is often use-
ful to include all or part of the original message
that is being replied to. Different email clients in-
dicate quoted material in different ways. By de-
fault, some prefix every line of the quoted message
with a character such as ‘&gt;’ or ‘|’, while others in-
dent the quoted content or insert the quoted mes-
sage unmodified, prefixed by a message header.
Sometimes the new content is above the quoted
content (a style known as ‘top-posting’); in other
cases, the new content may appear after the quoted
</bodyText>
<page confidence="0.976696">
919
</page>
<note confidence="0.9966095">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 919–928,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999924578947369">
content (bottom-posting) or interleaved with the
quoted content (inline replying). Confounding the
issue further is that users are able to configure their
email client to suit their individual tastes, and can
change both the syntax of quoting and their quot-
ing style (top, bottom or inline replying) on a per-
message basis.
To address these challenges, in this paper we
describe Zebra, our email zone classification sys-
tem. First we describe how Zebra builds and im-
proves on previous work in Section 2. Section 3
then presents our set of email zones, along with
details of the email data we use for system train-
ing and experiments. In Section 4 we describe two
approaches to zone classification, one that is line-
based and one that is fragment-based. The perfor-
mance of Zebra across two, three and nine email
zone classification tasks is presented and analysed
in Section 5.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998913448275862">
Segmenting email messages into zones requires
both text segmentation and text classification. The
main focus of most work on text segmentation
is topic-based segmentation of news text (e.g.,
(Hearst, 1997; Beeferman et al., 1997)), but there
have been some previous attempts at identifying
functional zones in email messages.
Chen et al. (1999) looked at both linguistic and
two-dimensional layout cues for extracting struc-
tured content from email signature zones in email
messages. The focus of their work was on extract-
ing information from already identified signature
blocks using a combination of two-dimensional
structural analysis and one-dimensional grammat-
ical constraints; the intended application domain
was as a component in a system for email text-
to-speech rendering. The authors claim that their
system can be modified to also identify signature
blocks within email messages, but their system
performs this task with a recall of only 53%. No
attempt is made to identify functional zones other
than email signatures.
Carvalho and Cohen’s (2004) Jangada system
attempted to identify email signatures within plain
text email messages and to extract email signa-
tures and reply lines. Unfortunately, the 20 News-
groups corpus1 they worked with contains 15-
year-old Usenet messages which are much more
homogeneous in their syntax than contemporary
</bodyText>
<footnote confidence="0.940554">
1http://people.csail.mit.edu/jrennie/20Newsgroups/
</footnote>
<bodyText confidence="0.999130239130435">
email, particularly in terms of how quoted text
from previous messages is indicated. As a result,
using a very simple metric (a line-initial ‘&gt;’ char-
acter) to identify reply lines achieves more than
95% accuracy. In contrast, this same simple met-
ric applied to the Enron email data we annotated
detects less than 10% of actual reply or forward
lines.
Usenet messages are also markedly different
from contemporary email when it comes to email
signatures. Most Usenet clients produced mes-
sages which conformed to RFC3676 (Gellens,
2004), a standard that formalised a “long-standing
convention in Usenet news ... of using two hy-
phens -- as the separator line between the body
and the signature of a message.” Unfortunately,
this convention has long since ceased to be ob-
served in email messages. Carvalho and Cohen’s
email signature detection approach also benefits
greatly from a simplifying assumption that signa-
tures are found in the last 10 lines of an email mes-
sage. While this holds true for their Usenet mes-
sage data, it is no longer the case for contemporary
email.
In attempting to use Carvalho and Cohen’s sys-
tem to identify signature blocks and reply lines
in our own work, we identified similar shortcom-
ings to those noted by Estival et al. (2007). In
particular, Jangada did not accurately identify for-
warded or reply content in email data from the
Enron email corpus. We believe that the use of
older Usenet-style messages to train Jangada is a
significant factor in the systematic errors the sys-
tem makes in failing to identify quoted reply, for-
warded and signature content in messages format-
ted in the range of message formats and styles pop-
ularised by Microsoft Outlook. These errors are
a fundamental problem with Jangada, especially
since Outlook is the most common client used to
compose messages in our annotated email collec-
tion drawn from the Enron corpus. More gen-
erally, we note that Outlook is the most popular
email client in current use, with an estimated 350–
400 million users worldwide,2 representing any-
where up to 40% of all email users.3
More recently, as part of their work on profiling
</bodyText>
<footnote confidence="0.999788333333333">
2Xobni Co-founder Adam Smith and former Engi-
neering VP Gabor Cselle have both published Outlook
user statistics. See http://www.xobni.com/asmith/archives/66
and http://gaborcselle.com/blog/2008/05/xobnis-journey-to-
right-product.html.
3http://www.campaignmonitor.com/stats/email-clients/
</footnote>
<page confidence="0.996514">
920
</page>
<bodyText confidence="0.99934625">
authors of email messages, Estival et al. (2007)
classified email bodies into five email zones. Their
paper does not provide results for five-zone classi-
fication, but they report accuracy of 88.16% using
a CRF classifier to distinguish three zones: reply,
author and signature. We use their classification
scheme as the starting point for our own set of
email zones.
</bodyText>
<sectionHeader confidence="0.99347" genericHeader="method">
3 Email Zones
</sectionHeader>
<bodyText confidence="0.999962621621622">
As noted earlier, we refer to the different func-
tional components of email messages as email
zones. The zones we propose refine and extend
the five categories — Author Text, Signature, Ad-
vertisement (automatically appended advertising),
Quoted Text (extended quotations such as song
lyrics or poems), and Reply Lines (including for-
warded and reply text) — identified by Estival et
al. (2007).
We consider that each line of text in the body
of an email message belongs to one of nine more
fine-grained email zones. We intend our nine
email zones to be abstracted and adapted to suit
different tasks. To illustrate, we present the
zones below abstracted into three classes: sender-
authored content, boilerplate content, and content
quoted from other conversations. This is the zone
partition we use to generate the three-zone results
reported in Section 5. This categorisation is use-
ful for problems such as finding action items in
email messages: such detection tools would look
in text from the sender-authored message zones
for new action item information, and could also
look in quoted conversation content to link new
action item information (such as reported comple-
tions) to previous action item content.
Our nine email zones can also be reduced to a
binary scheme to distinguish text authored by the
sender from text authored by others. This distinc-
tion is useful for problems such as author attribu-
tion or profiling tasks. In this two-class case, the
sender-authored zones would be Author, Greeting,
Signoff and Signature, while the other-authored
zones would be Reply, Forward, Disclaimer, Ad-
vertising and Attachment. This is the partition
of zones we use in our two-zone experiments re-
ported in Section 5.
</bodyText>
<subsectionHeader confidence="0.997859">
3.1 Sender Zones
</subsectionHeader>
<bodyText confidence="0.9997908">
Sender zones contain text written by the current
email sender. The Greeting and Signoff zones are
sub-zones of the Author zone, usually appearing
as the first and last items respectively in the Author
zone. Thus, our proposed sender zones are:
</bodyText>
<listItem confidence="0.999016555555556">
1. Author: New content from the current email
sender. This specifically excludes any text
authored by the sender that is included from
previous messages.
2. Greeting: Terms of address and recipient
names at the beginning of a message (e.g.,
Dear/Hi/Hey Noam).
3. Signoff: The message closing (e.g.,
Thanks/Cheers/Regards, John).
</listItem>
<subsectionHeader confidence="0.995513">
3.2 Quoted Conversation Zones
</subsectionHeader>
<bodyText confidence="0.9948068">
Quoted conversation zones include both content
quoted in reply to previous messages in the same
conversation thread and forwarded content from
other conversations.4 Our quoted conversation
zones are:
</bodyText>
<listItem confidence="0.845190923076923">
4. Reply: Content quoted from a previous mes-
sage in the same conversation thread, includ-
ing any embedded signatures, attachments,
advertising, disclaimers, author content and
forwarded content. Content in a reply content
zone may include previously sent content au-
thored by the current sender.
5. Forward: Content from an email message
outside the current conversation thread that
has been forwarded by the current email
sender, including any embedded signatures,
attachments, advertising, disclaimers, author
content and reply content.
</listItem>
<subsectionHeader confidence="0.998966">
3.3 Boilerplate Zones
</subsectionHeader>
<bodyText confidence="0.99499">
Boilerplate zones contain content that is reused
without modification across multiple email mes-
sages. Our proposed boilerplate zones are:
</bodyText>
<listItem confidence="0.996747142857143">
6. Signature: Content containing contact or
other information that is automatically in-
serted in a message. In contrast to disclaimer
or advertising content, signature content is
usually templated content written once by
the email author, and automatically or semi-
automatically included in email messages. A
</listItem>
<footnote confidence="0.604736">
4Although we recognise the need for the Quoted Text zone
proposed by Estival et al. (2007), no such data occurs in our
collection of annotated email messages. We therefore omit
this zone from our current set.
</footnote>
<page confidence="0.989681">
921
</page>
<bodyText confidence="0.666355666666667">
user may also use a Signature in place of a
Signoff; in such cases, we still mark the text
as a Signature.
</bodyText>
<listItem confidence="0.960748466666667">
7. Advertising: Advertising material in an
email message. Such material often appears
at the end of a message (e.g., Do you Ya-
hoo!?), but may also appear prefixed or in-
line with the content of the message, (e.g., in
sponsored mailing lists).
8. Disclaimer: Legal disclaimers and privacy
statements, often automatically appended.
9. Attachment: Automated text indicating or
referring to attached documents, such as that
shown in line 16 of Figure 1. Note that this
zone does not apply to manually authored ref-
erence to attachments, nor to the actual con-
tent of attachments (which we do not clas-
sify).
</listItem>
<subsectionHeader confidence="0.997623">
3.4 Email Data and Annotation
</subsectionHeader>
<bodyText confidence="0.999832192307692">
The training data for our zone classifier consists of
11881 annotated lines from almost 400 email mes-
sages drawn at random from the Enron email cor-
pus (Klimt and Yang, 2004).5 We use the database
dump of the corpus released by Andrew Fiore and
Jeff Heer.6 This version of the corpus has been
processed to remove duplicate messages and to
normalise sender and recipient names, resulting in
just over 250,000 email messages. No attachments
are included. Following Estival et al. (2007), we
used only a single annotator since the task revealed
itself to be relatively uncontroversial. Each line in
the body text of selected messages was marked by
the annotator (one of the authors) as belonging to
one of the nine zones. After removing blank lines,
which we do not attempt to classify, we are left
with 7922 annotated lines as training data for Ze-
bra. The frequency of each zone within this anno-
tated dataset is shown in Table 3.
Figure 1 shows an example of an email mes-
sage with each line annotated with the appropriate
email zone. Two zone annotations are shown for
each line (in separate columns), one using the nine
fine-grained zones and the second using the ab-
stracted three-zone scheme described in Section 3.
Note, however, that not all of the nine fine-grained
</bodyText>
<footnote confidence="0.991312333333333">
5This annotated dataset is available from
http://zebra.thoughtlets.org/.
6http://bailando.sims.berkeley.edu/enron/enron.sql.gz
</footnote>
<bodyText confidence="0.9294305">
zones, nor all of the three abstracted zones, are ac-
tually present in this particular message.
</bodyText>
<sectionHeader confidence="0.852519" genericHeader="method">
4 Zone Segmentation and Classification
</sectionHeader>
<bodyText confidence="0.999905">
Our email zone classification system is based
around an SVM classifier using features that cap-
ture graphic, orthographic and lexical information
about the content of an email message.
To classify the zones in an email message, we
experimented with two approaches. The first em-
ploys a two-stage approach that segments a mes-
sage into zone fragments and then classifies those
fragments. Our second method simply classifies
lines independently, returning a classification for
each non-blank line in an email message. Our hy-
pothesis was that classifying larger text fragments
would lead to better performance due to the text
fragments containing more cues about the zone
type.
</bodyText>
<subsectionHeader confidence="0.969908">
4.1 Zone Fragment Classification
</subsectionHeader>
<bodyText confidence="0.999837555555556">
Zone fragment classification is a two-step process.
First it predicts the zone boundaries using a simple
heuristic, then it classifies the resulting zone frag-
ments, the sets of content lines that lie between
these hypothesised boundaries.
In order to determine how well we can detect
zone boundaries, we first need to establish the cor-
rect zone boundaries in our collection of zone-
annotated email messages.
</bodyText>
<subsubsectionHeader confidence="0.459052">
4.1.1 Zone Boundaries
</subsubsectionHeader>
<bodyText confidence="0.998903894736842">
A zone boundary is defined as a continuous collec-
tion of one or more lines that separate two differ-
ent email zones. Lines that separate two zones and
are blank, contain only whitespace or contain only
punctuation characters are called buffer lines.
Since classification of blank lines between
zones is often ambiguous, empty or whitespace-
only buffer lines are not included as content in any
zone, and thus are not classified. Instead, they are
treated as strictly part of the zone boundary. In
Figure 1, these lines are shown without any zone
annotation. Zone boundary lines that are included
as content in a zone have their zone annotation
styled in bold and underlined. The important point
here is that zone boundaries are specific to a zone
classification scheme. For nine-zone classifica-
tion of the message in Figure 1, there are six zone
boundaries: line 2, lines 10–11, line 12, line 15,
lines 17–20, and lines 30–33. For three-zone clas-
</bodyText>
<page confidence="0.985977">
922
</page>
<figureCaption confidence="0.999625">
Figure 1: An example email message marked with both nine- and three-zone annotations.
</figureCaption>
<bodyText confidence="0.9998435">
sification, the only zone boundary consists of line
12, separating the sender and boilerplate zones.
Based on these definitions, there are three dif-
ferent types of zone boundaries:
</bodyText>
<listItem confidence="0.986698222222222">
1. Blank boundaries contain only empty or
whitespace-only buffer lines. Lines in these
zone boundaries are strictly separate from the
zone content. An example is Line 12 in Fig-
ure 1, for both the three- and nine-zone clas-
sification.
2. Separator boundaries contain only
buffer lines, but must contain at least
one punctuation-character buffer line that is
</listItem>
<bodyText confidence="0.986534333333333">
retained as content in one or both zones. In
Figure 1, an example is the zone boundary
containing lines 17–20 that separates the
Attachment and Disclaimer zones for nine-
zone classification, since line 20 is retained
as part of the Disclaimer zone content.
</bodyText>
<listItem confidence="0.672145142857143">
3. Adjoining boundaries consist of the last
content line of the earlier zone and the first
content line of the following zone. These
boundaries occur where no buffer lines ex-
ist between the two zones. An example is
the zone boundary containing lines 10 and 11
that separates the Author and Signoffzones in
</listItem>
<figureCaption confidence="0.421772">
Figure 1 for nine-zone classification.
</figureCaption>
<page confidence="0.991832">
923
</page>
<subsectionHeader confidence="0.891446">
4.1.2 Hypothesising Zone Boundaries
</subsectionHeader>
<bodyText confidence="0.999864">
To identify zone boundaries in unannotated email
data, we employ a very simple heuristic approach.
Specifically, we consider every line in the body of
an email message that matches any of the follow-
ing criteria to be a zone boundary:
</bodyText>
<listItem confidence="0.8874556">
1. A blank line;
2. A line containing only whitespace; or
3. A line beginning with four or more repeated
punctuation characters, optionally prefixed
by whitespace.
</listItem>
<bodyText confidence="0.999931794117647">
Our efforts to apply more sophisticated
machine-learning techniques to identifying zone
boundaries could not match the 90.15% recall
achieved by this simple heuristic. The boundaries
missed by the simple heuristic are all adjoining
boundaries, where two zones are not separated
by any buffer lines. An example of a boundary
that is not detected by our heuristic is the zone
boundary between the Author and Signoff zones
in Figure 1 formed bylines 10 and 11.
Obviously, our simple boundary heuristic de-
tects actual boundaries as well as spurious
boundaries that do not actually separate differ-
ent email zones. Unsurprisingly, the number of
spurious boundaries is large. The precision of
our simple heuristic across our annotated set of
email messages is 22.5%, meaning that less than
1 in 4 hypothesised zone boundaries is an actual
boundary. The underlying email zones average
more than 12 lines in length, including just over
8 lines of non-blank content. Due to the num-
ber of spurious boundaries, fragments contain less
than half this amount — approximately 3 lines of
non-blank content on average. One of the most
common types of spurious boundaries detected are
the blank lines that frequently separate paragraphs
within a single zone.
For three-zone classification, the set of pre-
dicted boundaries remains the same, but there are
less actual boundaries to find, so recall increases to
96.3%. However, because many boundaries from
the nine-zone classification are not boundaries for
the three-zone classification, precision decreases
to 14.7%.
</bodyText>
<subsectionHeader confidence="0.913233">
4.1.3 Classifying Zone Fragments
</subsectionHeader>
<bodyText confidence="0.999385055555556">
Having segmented the email message into candi-
date zone fragments, we classify these fragments
using the SMO implementation provided by Weka
(Witten and Frank, 2005) with the features de-
scribed in Section 4.3.
Although our boundary detection heuristic has
better than 90% recall, the small number of ac-
tual boundaries that are not detected result in some
zone fragments containing lines from more than
one underlying email zone. In these cases, we con-
sider the mode of all annotation values for lines
in the fragment (i.e., the most frequent zone an-
notation) to be the gold-standard zone type for
the fragment. This, of course, may mean that we
somewhat unfairly penalise the accuracy of our au-
tomated classification when Zebra detects a zone
that is indeed present in the fragment, but is not
the most frequent zone.
</bodyText>
<subsectionHeader confidence="0.943478">
4.2 Line Classification
</subsectionHeader>
<bodyText confidence="0.999860857142857">
Our line-based classification approach simply ex-
tracts all non-blank lines from an email message
and classifies lines one-by-one, using the same
features as for fragment-based classification. This
approach is the same as the signature and reply
line classification approach used by Carvalho and
Cohen (2004).
</bodyText>
<subsectionHeader confidence="0.998601">
4.3 Classification Features
</subsectionHeader>
<bodyText confidence="0.9999685">
We use a variety of graphic, orthographic and lex-
ical features for classification in Zebra. The same
features are applied in both the line-based and the
fragment-based zone classification (to either indi-
vidual lines or zone fragments). In the description
of our features, we refer to both single lines and
zone fragments (collections of contiguous lines) as
text fragments.
</bodyText>
<subsubsectionHeader confidence="0.472204">
4.3.1 Graphic Features
</subsubsectionHeader>
<bodyText confidence="0.996039">
Our graphic features capture information about the
presentation and layout of text in an email mes-
sage, independent of the actual words used. This
information is a crucial source of information for
identifying zones. Such information includes how
the text is organised and ordered, as well as the
‘shape’ of the text. The specific features we em-
ploy are:
</bodyText>
<listItem confidence="0.995131833333333">
• the number of words in the text fragment;
• the number of Unicode code points (i.e.,
characters) in the text fragment;
• the start position of the text fragment (equal
to one for the first line in the message, two for
the second line and increasing monotonically
</listItem>
<page confidence="0.943313">
924
</page>
<listItem confidence="0.971081625">
through the message; we also normalise the
result for message length);
• the end position of the text fragment (calcu-
lated as above and again normalised for mes-
sage length);
• the average line length (in characters) within
the text fragment (equal to the line length for
line-based text fragments);
• the length of the text fragment (in characters)
relative to the previous fragment;
• the length of the text fragment (in characters)
relative to the following fragment;
• the number of blank lines preceding the text
fragment; and
• the number of blank lines following the text
fragment.
</listItem>
<subsubsectionHeader confidence="0.587453">
4.3.2 Orthographic Features
</subsubsectionHeader>
<bodyText confidence="0.999470142857143">
Our orthographic features capture information
about the use of distinctive characters or charac-
ter sequences including punctuation, capital let-
ters and numbers. Like our graphic features, or-
thographic features tend to be independent of the
words used in an email message. The specific or-
thographic features we employ include:
</bodyText>
<listItem confidence="0.999309888888889">
• whether all lines start with the same character
(e.g., ‘&gt;’);
• whether a prior text fragment in the message
contains a quoted header;
• whether a prior text fragment in the message
contains repeated punctuation characters;
• whether the text fragment contains a URL;
• whether the text fragment contains an email
address;
• whether the text fragment contains a se-
quence of four or more digits;
• the number of capitalised words in the text
fragment;
• the percentage of capitalised words in the text
fragment;
• the number of non-alpha-numeric characters
in the text fragment;
• the percentage of non-alpha-numeric charac-
ters in the text fragment;
• the number of numeric characters in the text
fragment;
• the percentage of numeric characters in the
text fragment;
• whether the message subject line contains a
reply syntax marker such as Re: ; and
• whether the message subject line contains a
forward syntax marker such as Fw:.
</listItem>
<subsectionHeader confidence="0.775883">
4.3.3 Lexical Features
</subsectionHeader>
<bodyText confidence="0.996105428571428">
Finally, our lexical features capture information
about the words used in the email text. We use
unigrams to capture information about the vocab-
ulary and word bigram features to capture short
range word order information. More specifically,
the lexical features we apply to each text fragment
include:
</bodyText>
<listItem confidence="0.994802857142857">
• each word unigram, calculated with a mini-
mum frequency threshold cutoff of three, rep-
resented as a separate binary feature;
• each word bigram, calculated with a mini-
mum frequency threshold cutoff of three, rep-
resented as a separate binary feature;
• whether the text fragment contains the
sender’s name;
• whether a prior text fragment in the message
contains the sender’s name;
• whether the text fragment contains the
sender’s initials; and
• whether the text fragment contains a recipi-
ent’s name.
</listItem>
<bodyText confidence="0.999874071428571">
Features that look for instances of sender or recip-
ient names are less likely to be specific to a par-
ticular business or email domain. These features
use regular expressions to find name occurrences,
based on semi-structured information in the email
message headers. First, we extract and normalise
the names from the email headers to identify the
relevant person’s given name and surname. Our
features then capture whether one or both of the
given name or surname are present in the current
text fragment. Features which detect user initials
make use of the same name normalisation code to
retrieve a canonical form of the user’s name, from
which their initials are derived.
</bodyText>
<sectionHeader confidence="0.998839" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.997599666666667">
Table 1 shows Zebra’s accuracy in classifying
email zones. The results are calculated using 10-
fold cross-validation. Accuracy is shown for three
tasks — nine-, three- and two-zone classification
— using both line and zone-fragment classifica-
tion. Performance is compared against a majority
class baseline in each case.
Zebra’s performance compares favourably with
previously published results. While it is difficult to
</bodyText>
<page confidence="0.995658">
925
</page>
<table confidence="0.99726075">
2 Zones 3 Zones 9 Zones
Zebra Baseline Zebra Baseline Zebra Baseline
Lines 93.60% 61.14% 91.53% 58.55% 87.01% 30.94%
Fragments 92.09% 62.18% 91.37% 59.44% 86.45% 30.36%
</table>
<tableCaption confidence="0.995424">
Table 1: Classification accuracy compared against a majority baseline
</tableCaption>
<table confidence="0.99811675">
2 Zones 3 Zones 9 Zones
Zebra Baseline Zebra Baseline Zebra Baseline
Lines 90.62% 61.14% 86.56% 58.55% 81.05% 30.94%
Fragments 91.14% 62.18% 89.44% 59.44% 82.55% 30.36%
</table>
<tableCaption confidence="0.999824">
Table 2: Classification accuracy, without word n-gram features, compared against a majority baseline
</tableCaption>
<bodyText confidence="0.999053814285714">
directly compare, since not all systems are freely
available and they are not trained or tested over the
same data, our three-zone classification (identify-
ing sender, boilerplate and quoted reply content) is
very similar to the three-zone task for which (Es-
tival et al., 2007) report 88.16% accuracy for their
system and 64.22% accuracy using Carvalho and
Cohen’s Jangada system. Zebra outperforms both,
achieving 91.53% accuracy using a line-based ap-
proach. In the two-zone task, where we attempt
to identify sender-authored lines, Zebra achieves
93.60% accuracy and an F-measure of 0.918, ex-
ceeding the 0.907 F-measure reported for Estival
et al.’s system tuned for exactly this task.
Interestingly, the line-based approach provides
slightly better performance than the fragment-
based approach for each of the two-zone, three-
zone and nine-zone classification tasks. As noted
earlier, our original hypothesis was that zone frag-
ments would contain more information about the
sequence and text shape of the original message,
and that this would lead to better performance for
fragment-based classification.
When we restrict our feature set to those that
look only at the text of the line or zone fragment,
the fragment-based approach does perform better
than the line-based one. Using only word uni-
gram features, for example, our fragment classi-
fier achieves 78.7% accuracy. Using the same fea-
tures, the line-based classifier achieves only 57.5%
accuracy. When we add further features that cap-
ture sequence and shape information from outside
the text fragment being classified (e.g., the length
of a text segment compared to the text segment
before and after, and whether a segment occurs
after another segment containing repeated punc-
tuation or the sender’s name), the line-based ap-
proach achieves a greater increase in accuracy than
the fragment-based approach. This presumably is
because individual lines intrinsically have less in-
formation about the message context, and so ben-
efit more from the information added by the new
features.
We also experimented with removing all word
unigram and bigram features to explore the classi-
fier’s portability across different domains. This re-
moved all vocabulary and word order information
from our feature set. In doing so, our feature set
was reduced to less than thirty features, consist-
ing of mostly graphic and orthographic informa-
tion. The few remaining lexical features captured
only the presence of sender and recipient names,
which are independent of any particular email do-
main. As expected, performance did drop, but not
dramatically. Table 2 shows that average perfor-
mance without n-grams (across two-, three- and
nine-zone tasks) for line-based classification drops
by 4.67%. In contrast, fragment-based classifica-
tion accuracy drops by less than half this amount
— an average of 2.26%. This suggests that, as we
originally hypothesised, there are additional non-
lexical cues in zone fragments that give informa-
tion about the zone type. This makes the zone
fragment approach potentially more portable for
use across email data from different enterprise do-
mains.
Of course, classification accuracy gives only a
limited picture of Zebra’s performance. Table 4
shows precision and recall results for each zone in
the nine-zone line-based classification task. Per-
</bodyText>
<page confidence="0.993776">
926
</page>
<table confidence="0.9930259">
Total Author Signature Disclaim Advert Greet Signoff Reply Fwd Attach
Author 2415 2197 56 9 4 14 31 43 53 8
Signature 383 93 203 4 0 0 20 28 31 4
Disclaim 97 30 4 52 0 0 0 2 9 0
Advert 83 47 1 1 20 0 0 7 7 0
Greet 85 8 0 0 0 74 2 0 1 0
Signoff 195 30 5 0 0 0 147 11 2 0
Reply 2451 49 10 3 2 1 10 2222 154 0
Fwd 2187 72 13 7 8 1 3 125 1958 0
Attach 26 4 0 0 0 0 0 1 1 20
</table>
<tableCaption confidence="0.999904">
Table 3: Confusion Matrix for 9 Zone Line Classification
</tableCaption>
<bodyText confidence="0.999947714285714">
formance clearly varies significantly across the
different zones. For Author, Greeting, Reply and
Forward zones, performance is good, with F-
measure &gt; 0.8. This is encouraging, given that
many email tools, such as action-item detection
and email summarisation would benefit from an
ability to separate author content from reply con-
tent and forwarded content. The Advertising, Sig-
nature and Disclaimer zones show the poorest per-
formance, particularly in terms of Recall. The
Advertising and Disclaimer zones are almost cer-
tainly hindered by a lack of training data; they are
two of the smallest zones in terms of number of
lines of training data. The relatively poor Signa-
ture class performance is more interesting. Given
the potential confusion between Signoff content
and Signatures that function as Signoffs, one might
expect confusion between Signoff and Signature
zones, but Table 3 shows this is not the case.
Instead, there is significant confusion between
Signature and Author content, with almost 25%
of Signature lines misclassified as Author lines.
When word n-grams are removed from the fea-
ture set, the number of these misclassifications in-
creases to almost 50%. These results reinforce our
observation that the task of email signature extrac-
tion is much more difficult that it was in the days
of Usenet messages.
</bodyText>
<sectionHeader confidence="0.99968" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999765285714285">
Identifying functional zones in email messages is
a challenging task, due in large part to the diver-
sity in syntax used by different email software, and
the dynamic manner in which people employ dif-
ferent styles in authoring email messages. Zebra,
our system for segmenting and classifying email
message text into functional zones, achieves per-
</bodyText>
<table confidence="0.9928709">
Zone Precision Recall F-Measure
Author 0.868 0.910 0.889
Signature 0.695 0.530 0.601
Disclaimer 0.684 0.536 0.601
Advertising 0.588 0.241 0.342
Greeting 0.822 0.871 0.846
Signoff 0.690 0.754 0.721
Reply 0.911 0.907 0.909
Forward 0.884 0.895 0.889
Attachment 0.625 0.769 0.690
</table>
<tableCaption confidence="0.7691955">
Table 4: Precision and recall for nine-zone line
classification
</tableCaption>
<bodyText confidence="0.9999512">
formance that exceeds comparable systems, and
that is at a level to be practically useful to email
researchers and system builders. In addition to re-
leasing our annotated email dataset, the Zebra sys-
tem will also be available for others to use7.
Because we employ a non-sequential learn-
ing algorithm, we encode sequence information
into the feature set. In future work, we plan
to determine the effectiveness of using a sequen-
tial learning algorithm like Conditional Random
Fields (CRF). We note, however, that Carvalho
and Cohen (2004) demonstrate that using a non-
sequential learning algorithm with sequential fea-
tures, as we do, has the potential to meet or exceed
the performance of sequential learning algorithms.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999323">
The authors are grateful to the anonymous review-
ers for their insightful comments and suggestions.
</bodyText>
<footnote confidence="0.9989215">
7See http://zebra.thoughtlets.org for access to the anno-
tated data and Zebra system
</footnote>
<page confidence="0.994607">
927
</page>
<bodyText confidence="0.976448333333333">
Ian Witten and Eiba Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Mor-
gan Kaufmann, San Francisco, 2nd edition.
</bodyText>
<sectionHeader confidence="0.935848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998583211538462">
Douglas Beeferman, Adam Berger, and John Lafferty.
1997. Text segmentation using exponential models.
In Proceedings of the 2nd Conference on Empiri-
cal Methods in Natural Language Processing, pages
35–46, Providence, RI.
Victoria Bellotti, Nicolas Ducheneaut, Mark Howard,
and Ian Smith. 2003. Taking email to task: The
design and evaluation of a task management centred
email tool. In Computer Human Interaction Confer-
ence, CHI, pages 345–352, Ft Lauderdale, Florida,
USA, April 5-10.
Paul N Bennett and Jaime G Carbonell. 2007. Com-
bining probability-based rankers for action-item de-
tection. In Proceedings ofNAACL HLT 2007, pages
324–331, Rochester, NY, April.
Vitor R Carvalho and William W Cohen. 2004. Learn-
ing to extract signature reply lines from email. In
Proceedings ofFirst Conference on Email and Anti-
Spam (CEAS), Mountain View, CA, July 30-31.
Hao Chen, Jianying Hu, and Richard W Sproat. 1999.
Integrating geometrical and linguistic analysis for
email signature block parsing. ACM Transactions
on Information Systems, 17(4):343–366, October.
ISSN: 1046-8188.
Simon H. Corston-Oliver, Eric Ringger, Michael Ga-
mon, and Richard Campbell. 2004. Task-focused
summarization of email. In ACL-04 Workshop: Text
Summarization Branches Out, pages 43–50, July.
Aron Culotta, Ron Bekkerman, and Andrew McCal-
lum. 2004. Extracting social networks and contact
information from email and the web. In Proceedings
of the Conference on Email and Anti-Spam (CEAS).
Dominique Estival, Tanja Gaustad, Son Bao Pham,
Will Radford, and Ben Hutchinson. 2007. Author
profiling for English emails. In Proceedings of the
10th Conference of the Pacific Association for Com-
putational Linguistics, pages 263–272, Melbourne,
Australia, Sept 19-21.
R. Gellens. 2004. RFC3676: The text/plain format and
delsp parameters, February.
Marti A. Hearst. 1997. Texttiling: Segmenting text
into multi-paragraph subtopic passages. Computa-
tional Linguistics, 23(1):33–64.
Bryan Klimt and Yiming Yang. 2004. Introducing the
Enron corpus. In Proceedings of the Conference on
Email and Anti-Spam (CEAS).
Andrew Lampert, C´ecile Paris, and Robert Dale. 2007.
Can requests-for-action and commitments-to-act be
reliably identified in email messages? In Proceed-
ings of the 12th Australasian Document Comput-
ing Symposium, pages 48–55, Melbourne, Australia,
December 10.
</reference>
<page confidence="0.996906">
928
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.376254">
<title confidence="0.9872195">Segmenting Email Message Text into Zones Lampert ICT Dale Paris for Language</title>
<author confidence="0.946744">PO Box rdaleics mq edu au Macquarie</author>
<address confidence="0.472996">Epping 1710, North Ryde 2109,</address>
<email confidence="0.944421">andrew.lampert@csiro.aucecile.paris@csiro.au</email>
<abstract confidence="0.993586285714286">In the early days of email, widely-used conventions for indicating quoted reply content and email signatures made it easy to segment email messages into their functional parts. Today, the explosion of different email formats and styles, coupled with the ad hoc ways in which people vary the structure and layout of their messages, means that simple techniques for identifying quoted replies that used to yield 95% accuracy now find less than 10% of such content. In this paper, we describe Zebra, an SVM-based system for segmenting the body text of email messages into nine zone types based on graphic, orthographic and lexical cues. Zebra performs this task with an accuracy of 87.01%; when the number of zones is abstracted to two or three zone classes, this increases to 93.60% and 91.53% respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Text segmentation using exponential models.</title>
<date>1997</date>
<booktitle>In Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>35--46</pages>
<location>Providence, RI.</location>
<contexts>
<context position="5148" citStr="Beeferman et al., 1997" startWordPosition="839" endWordPosition="842">tion 2. Section 3 then presents our set of email zones, along with details of the email data we use for system training and experiments. In Section 4 we describe two approaches to zone classification, one that is linebased and one that is fragment-based. The performance of Zebra across two, three and nine email zone classification tasks is presented and analysed in Section 5. 2 Related Work Segmenting email messages into zones requires both text segmentation and text classification. The main focus of most work on text segmentation is topic-based segmentation of news text (e.g., (Hearst, 1997; Beeferman et al., 1997)), but there have been some previous attempts at identifying functional zones in email messages. Chen et al. (1999) looked at both linguistic and two-dimensional layout cues for extracting structured content from email signature zones in email messages. The focus of their work was on extracting information from already identified signature blocks using a combination of two-dimensional structural analysis and one-dimensional grammatical constraints; the intended application domain was as a component in a system for email textto-speech rendering. The authors claim that their system can be modifi</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1997</marker>
<rawString>Douglas Beeferman, Adam Berger, and John Lafferty. 1997. Text segmentation using exponential models. In Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing, pages 35–46, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Bellotti</author>
<author>Nicolas Ducheneaut</author>
<author>Mark Howard</author>
<author>Ian Smith</author>
</authors>
<title>Taking email to task: The design and evaluation of a task management centred email tool.</title>
<date>2003</date>
<booktitle>In Computer Human Interaction Conference, CHI,</booktitle>
<pages>345--352</pages>
<location>Ft Lauderdale, Florida, USA,</location>
<contexts>
<context position="1917" citStr="Bellotti et al., 2003" startWordPosition="305" endWordPosition="308">cessing tools stand to benefit from better knowledge of this message structure, facilitating focus on relevant content in specific parts of a message. In particular, access to zone information would allow email classification, summarisation and analysis tools to separate or filter out ‘noise’ and focus on the content in specific zones of a message that are relevant to the application at hand. Email contact mining tools such as that developed by Culotta et al. (2004), for example, might access the email signature zone, while tools that attempt to identify tasks or action items in email (e.g., (Bellotti et al., 2003; Corston-Oliver et al., 2004; Bennett and Carbonell, 2007; Lampert et al., 2007)) might restrict themselves to the sender-authored and forwarded content. Despite previous work on this problem, there are no available tools that can reliably extract or identify the different functional zones of an email message. While there is no agreed standard set of email zones, there are clearly different functional parts within the body text of email messages. For example, the content of an email disclaimer is functionally different from the sender-authored content and from the quoted reply content automat</context>
</contexts>
<marker>Bellotti, Ducheneaut, Howard, Smith, 2003</marker>
<rawString>Victoria Bellotti, Nicolas Ducheneaut, Mark Howard, and Ian Smith. 2003. Taking email to task: The design and evaluation of a task management centred email tool. In Computer Human Interaction Conference, CHI, pages 345–352, Ft Lauderdale, Florida, USA, April 5-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul N Bennett</author>
<author>Jaime G Carbonell</author>
</authors>
<title>Combining probability-based rankers for action-item detection.</title>
<date>2007</date>
<booktitle>In Proceedings ofNAACL HLT 2007,</booktitle>
<pages>324--331</pages>
<location>Rochester, NY,</location>
<contexts>
<context position="1975" citStr="Bennett and Carbonell, 2007" startWordPosition="313" endWordPosition="316"> of this message structure, facilitating focus on relevant content in specific parts of a message. In particular, access to zone information would allow email classification, summarisation and analysis tools to separate or filter out ‘noise’ and focus on the content in specific zones of a message that are relevant to the application at hand. Email contact mining tools such as that developed by Culotta et al. (2004), for example, might access the email signature zone, while tools that attempt to identify tasks or action items in email (e.g., (Bellotti et al., 2003; Corston-Oliver et al., 2004; Bennett and Carbonell, 2007; Lampert et al., 2007)) might restrict themselves to the sender-authored and forwarded content. Despite previous work on this problem, there are no available tools that can reliably extract or identify the different functional zones of an email message. While there is no agreed standard set of email zones, there are clearly different functional parts within the body text of email messages. For example, the content of an email disclaimer is functionally different from the sender-authored content and from the quoted reply content automatically included from previous messages in the thread of co</context>
</contexts>
<marker>Bennett, Carbonell, 2007</marker>
<rawString>Paul N Bennett and Jaime G Carbonell. 2007. Combining probability-based rankers for action-item detection. In Proceedings ofNAACL HLT 2007, pages 324–331, Rochester, NY, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vitor R Carvalho</author>
<author>William W Cohen</author>
</authors>
<title>Learning to extract signature reply lines from email.</title>
<date>2004</date>
<booktitle>In Proceedings ofFirst Conference on Email and AntiSpam (CEAS),</booktitle>
<location>Mountain View, CA,</location>
<contexts>
<context position="21604" citStr="Carvalho and Cohen (2004)" startWordPosition="3451" endWordPosition="3454">the most frequent zone annotation) to be the gold-standard zone type for the fragment. This, of course, may mean that we somewhat unfairly penalise the accuracy of our automated classification when Zebra detects a zone that is indeed present in the fragment, but is not the most frequent zone. 4.2 Line Classification Our line-based classification approach simply extracts all non-blank lines from an email message and classifies lines one-by-one, using the same features as for fragment-based classification. This approach is the same as the signature and reply line classification approach used by Carvalho and Cohen (2004). 4.3 Classification Features We use a variety of graphic, orthographic and lexical features for classification in Zebra. The same features are applied in both the line-based and the fragment-based zone classification (to either individual lines or zone fragments). In the description of our features, we refer to both single lines and zone fragments (collections of contiguous lines) as text fragments. 4.3.1 Graphic Features Our graphic features capture information about the presentation and layout of text in an email message, independent of the actual words used. This information is a crucial s</context>
</contexts>
<marker>Carvalho, Cohen, 2004</marker>
<rawString>Vitor R Carvalho and William W Cohen. 2004. Learning to extract signature reply lines from email. In Proceedings ofFirst Conference on Email and AntiSpam (CEAS), Mountain View, CA, July 30-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Chen</author>
<author>Jianying Hu</author>
<author>Richard W Sproat</author>
</authors>
<title>Integrating geometrical and linguistic analysis for email signature block parsing.</title>
<date>1999</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>17</volume>
<issue>4</issue>
<pages>1046--8188</pages>
<contexts>
<context position="5263" citStr="Chen et al. (1999)" startWordPosition="857" endWordPosition="860">and experiments. In Section 4 we describe two approaches to zone classification, one that is linebased and one that is fragment-based. The performance of Zebra across two, three and nine email zone classification tasks is presented and analysed in Section 5. 2 Related Work Segmenting email messages into zones requires both text segmentation and text classification. The main focus of most work on text segmentation is topic-based segmentation of news text (e.g., (Hearst, 1997; Beeferman et al., 1997)), but there have been some previous attempts at identifying functional zones in email messages. Chen et al. (1999) looked at both linguistic and two-dimensional layout cues for extracting structured content from email signature zones in email messages. The focus of their work was on extracting information from already identified signature blocks using a combination of two-dimensional structural analysis and one-dimensional grammatical constraints; the intended application domain was as a component in a system for email textto-speech rendering. The authors claim that their system can be modified to also identify signature blocks within email messages, but their system performs this task with a recall of on</context>
</contexts>
<marker>Chen, Hu, Sproat, 1999</marker>
<rawString>Hao Chen, Jianying Hu, and Richard W Sproat. 1999. Integrating geometrical and linguistic analysis for email signature block parsing. ACM Transactions on Information Systems, 17(4):343–366, October. ISSN: 1046-8188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon H Corston-Oliver</author>
<author>Eric Ringger</author>
<author>Michael Gamon</author>
<author>Richard Campbell</author>
</authors>
<title>Task-focused summarization of email.</title>
<date>2004</date>
<booktitle>In ACL-04 Workshop: Text Summarization Branches Out,</booktitle>
<pages>43--50</pages>
<contexts>
<context position="1946" citStr="Corston-Oliver et al., 2004" startWordPosition="309" endWordPosition="312">benefit from better knowledge of this message structure, facilitating focus on relevant content in specific parts of a message. In particular, access to zone information would allow email classification, summarisation and analysis tools to separate or filter out ‘noise’ and focus on the content in specific zones of a message that are relevant to the application at hand. Email contact mining tools such as that developed by Culotta et al. (2004), for example, might access the email signature zone, while tools that attempt to identify tasks or action items in email (e.g., (Bellotti et al., 2003; Corston-Oliver et al., 2004; Bennett and Carbonell, 2007; Lampert et al., 2007)) might restrict themselves to the sender-authored and forwarded content. Despite previous work on this problem, there are no available tools that can reliably extract or identify the different functional zones of an email message. While there is no agreed standard set of email zones, there are clearly different functional parts within the body text of email messages. For example, the content of an email disclaimer is functionally different from the sender-authored content and from the quoted reply content automatically included from previous</context>
</contexts>
<marker>Corston-Oliver, Ringger, Gamon, Campbell, 2004</marker>
<rawString>Simon H. Corston-Oliver, Eric Ringger, Michael Gamon, and Richard Campbell. 2004. Task-focused summarization of email. In ACL-04 Workshop: Text Summarization Branches Out, pages 43–50, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Ron Bekkerman</author>
<author>Andrew McCallum</author>
</authors>
<title>Extracting social networks and contact information from email and the web.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Email and Anti-Spam (CEAS).</booktitle>
<contexts>
<context position="1766" citStr="Culotta et al. (2004)" startWordPosition="279" endWordPosition="282"> different functional parts such as email signatures, quoted reply content and advertising content. We refer to these as email zones. Many language processing tools stand to benefit from better knowledge of this message structure, facilitating focus on relevant content in specific parts of a message. In particular, access to zone information would allow email classification, summarisation and analysis tools to separate or filter out ‘noise’ and focus on the content in specific zones of a message that are relevant to the application at hand. Email contact mining tools such as that developed by Culotta et al. (2004), for example, might access the email signature zone, while tools that attempt to identify tasks or action items in email (e.g., (Bellotti et al., 2003; Corston-Oliver et al., 2004; Bennett and Carbonell, 2007; Lampert et al., 2007)) might restrict themselves to the sender-authored and forwarded content. Despite previous work on this problem, there are no available tools that can reliably extract or identify the different functional zones of an email message. While there is no agreed standard set of email zones, there are clearly different functional parts within the body text of email message</context>
</contexts>
<marker>Culotta, Bekkerman, McCallum, 2004</marker>
<rawString>Aron Culotta, Ron Bekkerman, and Andrew McCallum. 2004. Extracting social networks and contact information from email and the web. In Proceedings of the Conference on Email and Anti-Spam (CEAS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominique Estival</author>
<author>Tanja Gaustad</author>
<author>Son Bao Pham</author>
<author>Will Radford</author>
<author>Ben Hutchinson</author>
</authors>
<title>Author profiling for English emails.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics,</booktitle>
<pages>263--272</pages>
<location>Melbourne, Australia,</location>
<contexts>
<context position="7581" citStr="Estival et al. (2007)" startWordPosition="1223" endWordPosition="1226">s the separator line between the body and the signature of a message.” Unfortunately, this convention has long since ceased to be observed in email messages. Carvalho and Cohen’s email signature detection approach also benefits greatly from a simplifying assumption that signatures are found in the last 10 lines of an email message. While this holds true for their Usenet message data, it is no longer the case for contemporary email. In attempting to use Carvalho and Cohen’s system to identify signature blocks and reply lines in our own work, we identified similar shortcomings to those noted by Estival et al. (2007). In particular, Jangada did not accurately identify forwarded or reply content in email data from the Enron email corpus. We believe that the use of older Usenet-style messages to train Jangada is a significant factor in the systematic errors the system makes in failing to identify quoted reply, forwarded and signature content in messages formatted in the range of message formats and styles popularised by Microsoft Outlook. These errors are a fundamental problem with Jangada, especially since Outlook is the most common client used to compose messages in our annotated email collection drawn fr</context>
<context position="9507" citStr="Estival et al. (2007)" startWordPosition="1517" endWordPosition="1520">sification, but they report accuracy of 88.16% using a CRF classifier to distinguish three zones: reply, author and signature. We use their classification scheme as the starting point for our own set of email zones. 3 Email Zones As noted earlier, we refer to the different functional components of email messages as email zones. The zones we propose refine and extend the five categories — Author Text, Signature, Advertisement (automatically appended advertising), Quoted Text (extended quotations such as song lyrics or poems), and Reply Lines (including forwarded and reply text) — identified by Estival et al. (2007). We consider that each line of text in the body of an email message belongs to one of nine more fine-grained email zones. We intend our nine email zones to be abstracted and adapted to suit different tasks. To illustrate, we present the zones below abstracted into three classes: senderauthored content, boilerplate content, and content quoted from other conversations. This is the zone partition we use to generate the three-zone results reported in Section 5. This categorisation is useful for problems such as finding action items in email messages: such detection tools would look in text from t</context>
<context position="12743" citStr="Estival et al. (2007)" startWordPosition="2016" endWordPosition="2019">ts, advertising, disclaimers, author content and reply content. 3.3 Boilerplate Zones Boilerplate zones contain content that is reused without modification across multiple email messages. Our proposed boilerplate zones are: 6. Signature: Content containing contact or other information that is automatically inserted in a message. In contrast to disclaimer or advertising content, signature content is usually templated content written once by the email author, and automatically or semiautomatically included in email messages. A 4Although we recognise the need for the Quoted Text zone proposed by Estival et al. (2007), no such data occurs in our collection of annotated email messages. We therefore omit this zone from our current set. 921 user may also use a Signature in place of a Signoff; in such cases, we still mark the text as a Signature. 7. Advertising: Advertising material in an email message. Such material often appears at the end of a message (e.g., Do you Yahoo!?), but may also appear prefixed or inline with the content of the message, (e.g., in sponsored mailing lists). 8. Disclaimer: Legal disclaimers and privacy statements, often automatically appended. 9. Attachment: Automated text indicating </context>
<context position="14086" citStr="Estival et al. (2007)" startWordPosition="2245" endWordPosition="2248">lly authored reference to attachments, nor to the actual content of attachments (which we do not classify). 3.4 Email Data and Annotation The training data for our zone classifier consists of 11881 annotated lines from almost 400 email messages drawn at random from the Enron email corpus (Klimt and Yang, 2004).5 We use the database dump of the corpus released by Andrew Fiore and Jeff Heer.6 This version of the corpus has been processed to remove duplicate messages and to normalise sender and recipient names, resulting in just over 250,000 email messages. No attachments are included. Following Estival et al. (2007), we used only a single annotator since the task revealed itself to be relatively uncontroversial. Each line in the body text of selected messages was marked by the annotator (one of the authors) as belonging to one of the nine zones. After removing blank lines, which we do not attempt to classify, we are left with 7922 annotated lines as training data for Zebra. The frequency of each zone within this annotated dataset is shown in Table 3. Figure 1 shows an example of an email message with each line annotated with the appropriate email zone. Two zone annotations are shown for each line (in sep</context>
<context position="27269" citStr="Estival et al., 2007" startWordPosition="4364" endWordPosition="4368">.36% Table 1: Classification accuracy compared against a majority baseline 2 Zones 3 Zones 9 Zones Zebra Baseline Zebra Baseline Zebra Baseline Lines 90.62% 61.14% 86.56% 58.55% 81.05% 30.94% Fragments 91.14% 62.18% 89.44% 59.44% 82.55% 30.36% Table 2: Classification accuracy, without word n-gram features, compared against a majority baseline directly compare, since not all systems are freely available and they are not trained or tested over the same data, our three-zone classification (identifying sender, boilerplate and quoted reply content) is very similar to the three-zone task for which (Estival et al., 2007) report 88.16% accuracy for their system and 64.22% accuracy using Carvalho and Cohen’s Jangada system. Zebra outperforms both, achieving 91.53% accuracy using a line-based approach. In the two-zone task, where we attempt to identify sender-authored lines, Zebra achieves 93.60% accuracy and an F-measure of 0.918, exceeding the 0.907 F-measure reported for Estival et al.’s system tuned for exactly this task. Interestingly, the line-based approach provides slightly better performance than the fragmentbased approach for each of the two-zone, threezone and nine-zone classification tasks. As noted </context>
</contexts>
<marker>Estival, Gaustad, Pham, Radford, Hutchinson, 2007</marker>
<rawString>Dominique Estival, Tanja Gaustad, Son Bao Pham, Will Radford, and Ben Hutchinson. 2007. Author profiling for English emails. In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics, pages 263–272, Melbourne, Australia, Sept 19-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gellens</author>
</authors>
<title>RFC3676: The text/plain format and delsp parameters,</title>
<date>2004</date>
<contexts>
<context position="6859" citStr="Gellens, 2004" startWordPosition="1100" endWordPosition="1101">eir syntax than contemporary 1http://people.csail.mit.edu/jrennie/20Newsgroups/ email, particularly in terms of how quoted text from previous messages is indicated. As a result, using a very simple metric (a line-initial ‘&gt;’ character) to identify reply lines achieves more than 95% accuracy. In contrast, this same simple metric applied to the Enron email data we annotated detects less than 10% of actual reply or forward lines. Usenet messages are also markedly different from contemporary email when it comes to email signatures. Most Usenet clients produced messages which conformed to RFC3676 (Gellens, 2004), a standard that formalised a “long-standing convention in Usenet news ... of using two hyphens -- as the separator line between the body and the signature of a message.” Unfortunately, this convention has long since ceased to be observed in email messages. Carvalho and Cohen’s email signature detection approach also benefits greatly from a simplifying assumption that signatures are found in the last 10 lines of an email message. While this holds true for their Usenet message data, it is no longer the case for contemporary email. In attempting to use Carvalho and Cohen’s system to identify si</context>
</contexts>
<marker>Gellens, 2004</marker>
<rawString>R. Gellens. 2004. RFC3676: The text/plain format and delsp parameters, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Texttiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="5123" citStr="Hearst, 1997" startWordPosition="837" endWordPosition="838">us work in Section 2. Section 3 then presents our set of email zones, along with details of the email data we use for system training and experiments. In Section 4 we describe two approaches to zone classification, one that is linebased and one that is fragment-based. The performance of Zebra across two, three and nine email zone classification tasks is presented and analysed in Section 5. 2 Related Work Segmenting email messages into zones requires both text segmentation and text classification. The main focus of most work on text segmentation is topic-based segmentation of news text (e.g., (Hearst, 1997; Beeferman et al., 1997)), but there have been some previous attempts at identifying functional zones in email messages. Chen et al. (1999) looked at both linguistic and two-dimensional layout cues for extracting structured content from email signature zones in email messages. The focus of their work was on extracting information from already identified signature blocks using a combination of two-dimensional structural analysis and one-dimensional grammatical constraints; the intended application domain was as a component in a system for email textto-speech rendering. The authors claim that t</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti A. Hearst. 1997. Texttiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Klimt</author>
<author>Yiming Yang</author>
</authors>
<title>Introducing the Enron corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Email and Anti-Spam (CEAS).</booktitle>
<contexts>
<context position="13776" citStr="Klimt and Yang, 2004" startWordPosition="2195" endWordPosition="2198">content of the message, (e.g., in sponsored mailing lists). 8. Disclaimer: Legal disclaimers and privacy statements, often automatically appended. 9. Attachment: Automated text indicating or referring to attached documents, such as that shown in line 16 of Figure 1. Note that this zone does not apply to manually authored reference to attachments, nor to the actual content of attachments (which we do not classify). 3.4 Email Data and Annotation The training data for our zone classifier consists of 11881 annotated lines from almost 400 email messages drawn at random from the Enron email corpus (Klimt and Yang, 2004).5 We use the database dump of the corpus released by Andrew Fiore and Jeff Heer.6 This version of the corpus has been processed to remove duplicate messages and to normalise sender and recipient names, resulting in just over 250,000 email messages. No attachments are included. Following Estival et al. (2007), we used only a single annotator since the task revealed itself to be relatively uncontroversial. Each line in the body text of selected messages was marked by the annotator (one of the authors) as belonging to one of the nine zones. After removing blank lines, which we do not attempt to </context>
</contexts>
<marker>Klimt, Yang, 2004</marker>
<rawString>Bryan Klimt and Yiming Yang. 2004. Introducing the Enron corpus. In Proceedings of the Conference on Email and Anti-Spam (CEAS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Lampert</author>
<author>C´ecile Paris</author>
<author>Robert Dale</author>
</authors>
<title>Can requests-for-action and commitments-to-act be reliably identified in email messages?</title>
<date>2007</date>
<booktitle>In Proceedings of the 12th Australasian Document Computing Symposium,</booktitle>
<pages>48--55</pages>
<location>Melbourne, Australia,</location>
<contexts>
<context position="1998" citStr="Lampert et al., 2007" startWordPosition="317" endWordPosition="320">acilitating focus on relevant content in specific parts of a message. In particular, access to zone information would allow email classification, summarisation and analysis tools to separate or filter out ‘noise’ and focus on the content in specific zones of a message that are relevant to the application at hand. Email contact mining tools such as that developed by Culotta et al. (2004), for example, might access the email signature zone, while tools that attempt to identify tasks or action items in email (e.g., (Bellotti et al., 2003; Corston-Oliver et al., 2004; Bennett and Carbonell, 2007; Lampert et al., 2007)) might restrict themselves to the sender-authored and forwarded content. Despite previous work on this problem, there are no available tools that can reliably extract or identify the different functional zones of an email message. While there is no agreed standard set of email zones, there are clearly different functional parts within the body text of email messages. For example, the content of an email disclaimer is functionally different from the sender-authored content and from the quoted reply content automatically included from previous messages in the thread of conversation. Of course, </context>
</contexts>
<marker>Lampert, Paris, Dale, 2007</marker>
<rawString>Andrew Lampert, C´ecile Paris, and Robert Dale. 2007. Can requests-for-action and commitments-to-act be reliably identified in email messages? In Proceedings of the 12th Australasian Document Computing Symposium, pages 48–55, Melbourne, Australia, December 10.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>