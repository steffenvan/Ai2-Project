<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000415">
<title confidence="0.9990775">
Conundrums in Noun Phrase Coreference Resolution:
Making Sense of the State-of-the-Art
</title>
<author confidence="0.931005">
Veselin Stoyanov Nathan Gilbert Claire Cardie Ellen Riloff
</author>
<affiliation confidence="0.7314645">
Cornell University University of Utah Cornell University University of Utah
Ithaca, NY Salt Lake City, UT Ithaca, NY Salt Lake City, UT
</affiliation>
<email confidence="0.978499">
ves@cs.cornell.edu ngilbert@cs.utah.edu cardie@cs.cornell.edu riloff@cs.utah.edu
</email>
<sectionHeader confidence="0.993255" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999289647058823">
We aim to shed light on the state-of-the-art in NP
coreference resolution by teasing apart the differ-
ences in the MUC and ACE task definitions, the as-
sumptions made in evaluation methodologies, and
inherent differences in text corpora. First, we exam-
ine three subproblems that play a role in coreference
resolution: named entity recognition, anaphoric-
ity determination, and coreference element detec-
tion. We measure the impact of each subproblem on
coreference resolution and confirm that certain as-
sumptions regarding these subproblems in the eval-
uation methodology can dramatically simplify the
overall task. Second, we measure the performance
of a state-of-the-art coreference resolver on several
classes of anaphora and use these results to develop
a quantitative measure for estimating coreference
resolution performance on new data sets.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99165603030303">
As is common for many natural language process-
ing problems, the state-of-the-art in noun phrase
(NP) coreference resolution is typically quantified
based on system performance on manually anno-
tated text corpora. In spite of the availability of
several benchmark data sets (e.g. MUC-6 (1995),
ACE NIST (2004)) and their use in many formal
evaluations, as a field we can make surprisingly
few conclusive statements about the state-of-the-
art in NP coreference resolution.
In particular, it remains difficult to assess the ef-
fectiveness of different coreference resolution ap-
proaches, even in relative terms. For example, the
91.5 F-measure reported by McCallum and Well-
ner (2004) was produced by a system using perfect
information for several linguistic subproblems. In
contrast, the 71.3 F-measure reported by Yang et
al. (2003) represents a fully automatic end-to-end
resolver. It is impossible to assess which approach
truly performs best because of the dramatically
different assumptions of each evaluation.
Results vary widely across data sets. Corefer-
ence resolution scores range from 85-90% on the
ACE 2004 and 2005 data sets to a much lower 60-
70% on the MUC 6 and 7 data sets (e.g. Soon et al.
(2001) and Yang et al. (2003)). What accounts for
these differences? Are they due to properties of
the documents or domains? Or do differences in
the coreference task definitions account for the dif-
ferences in performance? Given a new text collec-
tion and domain, what level of performance should
we expect?
We have little understanding of which aspects
of the coreference resolution problem are handled
well or poorly by state-of-the-art systems. Ex-
cept for some fairly general statements, for exam-
ple that proper names are easier to resolve than
pronouns, which are easier than common nouns,
there has been little analysis of which aspects of
the problem have achieved success and which re-
main elusive.
The goal of this paper is to take initial steps to-
ward making sense of the disparate performance
results reported for NP coreference resolution. For
our investigations, we employ a state-of-the-art
classification-based NP coreference resolver and
focus on the widely used MUC and ACE corefer-
ence resolution data sets.
We hypothesize that performance variation
within and across coreference resolvers is, at least
in part, a function of (1) the (sometimes unstated)
assumptions in evaluation methodologies, and (2)
the relative difficulty of the benchmark text cor-
pora. With these in mind, Section 3 first examines
three subproblems that play an important role in
coreference resolution: named entity recognition,
anaphoricity determination, and coreference ele-
ment detection. We quantitatively measure the im-
pact of each of these subproblems on coreference
resolution performance as a whole. Our results
suggest that the availability of accurate detectors
for anaphoricity or coreference elements could
substantially improve the performance of state-of-
the-art resolvers, while improvements to named
entity recognition likely offer little gains. Our re-
sults also confirm that the assumptions adopted in
</bodyText>
<page confidence="0.983479">
656
</page>
<note confidence="0.994767666666667">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 656–664,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
MUC ACE
Relative Pronouns no yes
Gerunds no yes
Nested non-NP nouns yes no
Nested NEs no GPE &amp; LOC premod
Semantic Types all 7 classes only
Singletons no yes
</note>
<tableCaption confidence="0.974647">
Table 1: Coreference Definition Differences for MUC and
ACE. (GPE refers to geo-political entities.)
</tableCaption>
<bodyText confidence="0.9998525">
some evaluations dramatically simplify the resolu-
tion task, rendering it an unrealistic surrogate for
the original problem.
In Section 4, we quantify the difficulty of a
text corpus with respect to coreference resolution
by analyzing performance on different resolution
classes. Our goals are twofold: to measure the
level of performance of state-of-the-art corefer-
ence resolvers on different types of anaphora, and
to develop a quantitative measure for estimating
coreference resolution performance on new data
sets. We introduce a coreference performance pre-
diction (CPP) measure and show that it accurately
predicts the performance of our coreference re-
solver. As a side effect of our research, we pro-
vide a new set of much-needed benchmark results
for coreference resolution under common sets of
fully-specified evaluation assumptions.
</bodyText>
<sectionHeader confidence="0.965036" genericHeader="method">
2 Coreference Task Definitions
</sectionHeader>
<bodyText confidence="0.999854191176471">
This paper studies the six most commonly used
coreference resolution data sets. Two of those are
from the MUC conferences (MUC-6, 1995; MUC-
7, 1997) and four are from the Automatic Con-
tent Evaluation (ACE) Program (NIST, 2004). In
this section, we outline the differences between the
MUC and ACE coreference resolution tasks, and
define terminology for the rest of the paper.
Noun phrase coreference resolution is the pro-
cess of determining whether two noun phrases
(NPs) refer to the same real-world entity or con-
cept. It is related to anaphora resolution: a NP is
said to be anaphoric if it depends on another NP
for interpretation. Consider the following:
John Hall is the new CEO. He starts on Monday.
Here, he is anaphoric because it depends on its an-
tecedent, John Hall, for interpretation. The two
NPs also corefer because each refers to the same
person, JOHN HALL.
As discussed in depth elsewhere (e.g. van
Deemter and Kibble (2000)), the notions of coref-
erence and anaphora are difficult to define pre-
cisely and to operationalize consistently. Further-
more, the connections between them are extremely
complex and go beyond the scope of this paper.
Given these complexities, it is not surprising that
the annotation instructions for the MUC and ACE
data sets reflect different interpretations and sim-
plifications of the general coreference relation. We
outline some of these differences below.
Syntactic Types. To avoid ambiguity, we will
use the term coreference element (CE) to refer
to the set of linguistic expressions that participate
in the coreference relation, as defined for each of
the MUC and ACE tasks.1 At times, it will be im-
portant to distinguish between the CEs that are in-
cluded in the gold standard — the annotated CEs
— from those that are generated by the corefer-
ence resolution system — the extracted CEs.
At a high level, both the MUC and ACE eval-
uations define CEs as nouns, pronouns, and noun
phrases. However, the MUC definition excludes
(1) “nested” named entities (NEs) (e.g. “Amer-
ica” in “Bank of America”), (2) relative pronouns,
and (3) gerunds, but allows (4) nested nouns (e.g.
“union” in “union members”). The ACE defini-
tion, on the other hand, includes relative pronouns
and gerunds, excludes all nested nouns that are not
themselves NPs, and allows premodifier NE men-
tions of geo-political entities and locations, such
as “Russian” in “Russian politicians”.
Semantic Types. ACE restricts CEs to entities
that belong to one of seven semantic classes: per-
son, organization, geo-political entity, location, fa-
cility, vehicle, and weapon. MUC has no semantic
restrictions.
Singletons. The MUC data sets include annota-
tions only for CEs that are coreferent with at least
one other CE. ACE, on the other hand, permits
“singleton” CEs, which are not coreferent with
any other CE in the document.
These substantial differences in the task defini-
tions (summarized in Table 1) make it extremely
difficult to compare performance across the MUC
and ACE data sets. In the next section, we take a
closer look at the coreference resolution task, ana-
lyzing the impact of various subtasks irrespective
of the data set differences.
</bodyText>
<footnote confidence="0.82397725">
1We define the term CE to be roughly equivalent to (a)
the notion of markable in the MUC coreference resolution
definition and (b) the structures that can be mentions in the
descriptions of ACE.
</footnote>
<page confidence="0.998055">
657
</page>
<sectionHeader confidence="0.992751" genericHeader="method">
3 Coreference Subtask Analysis
</sectionHeader>
<bodyText confidence="0.998871636363636">
Coreference resolution is a complex task that
requires solving numerous non-trivial subtasks
such as syntactic analysis, semantic class tagging,
pleonastic pronoun identification and antecedent
identification to name a few. This section exam-
ines the role of three such subtasks — named en-
tity recognition, anaphoricity determination, and
coreference element detection — in the perfor-
mance of an end-to-end coreference resolution
system. First, however, we describe the corefer-
ence resolver that we use for our study.
</bodyText>
<subsectionHeader confidence="0.9690215">
3.1 The RECONCILEACL09 Coreference
Resolver
</subsectionHeader>
<bodyText confidence="0.999513558823529">
We use the RECONCILE coreference resolution
platform (Stoyanov et al., 2009) to configure a
coreference resolver that performs comparably to
state-of-the-art systems (when evaluated on the
MUC and ACE data sets under comparable as-
sumptions). This system is a classification-based
coreference resolver, modeled after the systems of
Ng and Cardie (2002b) and Bengtson and Roth
(2008). First it classifies pairs of CEs as coreferent
or not coreferent, pairing each identified CE with
all preceding CEs. The CEs are then clustered
into coreference chains2 based on the pairwise de-
cisions. RECONCILE has a pipeline architecture
with four main steps: preprocessing, feature ex-
traction, classification, and clustering. We will
refer to the specific configuration of RECONCILE
used for this paper as RECONCILEACL09.
Preprocessing. The RECONCILEACL09 prepro-
cessor applies a series of language analysis tools
(mostly publicly available software packages) to
the source texts. The OpenNLP toolkit (Baldridge,
J., 2005) performs tokenization, sentence splitting,
and part-of-speech tagging. The Berkeley parser
(Petrov and Klein, 2007) generates phrase struc-
ture parse trees, and the de Marneffe et al. (2006)
system produces dependency relations. We em-
ploy the Stanford CRF-based Named Entity Rec-
ognizer (Finkel et al., 2004) for named entity
tagging. With these preprocessing components,
RECONCILEACL09 uses heuristics to correctly ex-
tract approximately 90% of the annotated CEs for
the MUC and ACE data sets.
Feature Set. To achieve roughly state-of-the-
art performance, RECONCILEACL09 employs a
</bodyText>
<footnote confidence="0.826876">
2A coreference chain refers to the set of CEs that refer to
a particular entity.
</footnote>
<table confidence="0.999107428571428">
dataset docs CEs chains CEs/ch tr/tst split
MUC6 60 4232 960 4.4 30/30 (st)
MUC7 50 4297 1081 3.9 30/20 (st)
ACE-2 159 2630 1148 2.3 130/29 (st)
ACE03 105 3106 1340 2.3 74/31
ACE04 128 3037 1332 2.3 90/38
ACE05 81 1991 775 2.6 57/24
</table>
<tableCaption confidence="0.98118225">
Table 2: Dataset characteristics including the number of
documents, annotated CEs, coreference chains, annotated
CEs per chain (average), and number of documents in the
train/test split. We use st to indicate a standard train/test split.
</tableCaption>
<bodyText confidence="0.970924925925926">
fairly comprehensive set of 61 features introduced
in previous coreference resolution systems (see
Bengtson and Roth (2008)). We briefly summarize
the features here and refer the reader to Stoyanov
et al. (2009) for more details.
Lexical (9): String-based comparisons of the two
CEs, such as exact string matching and head noun
matching.
Proximity (5): Sentence and paragraph-based
measures of the distance between two CEs.
Grammatical (28): A wide variety of syntactic
properties of the CEs, either individually or as a
pair. These features are based on part-of-speech
tags, parse trees, or dependency relations. For ex-
ample: one feature indicates whether both CEs are
syntactic subjects; another indicates whether the
CEs are in an appositive construction.
Semantic (19): Capture semantic information
about one or both NPs such as tests for gender and
animacy, semantic compatibility based on Word-
Net, and semantic comparisons of NE types.
Classification and Clustering. We configure
RECONCILEACL09 to use the Averaged Percep-
tron learning algorithm (Freund and Schapire,
1999) and to employ single-link clustering (i.e.
transitive closure) to generate the final partition-
ing.3
</bodyText>
<subsectionHeader confidence="0.999636">
3.2 Baseline System Results
</subsectionHeader>
<bodyText confidence="0.99978825">
Our experiments rely on the MUC and ACE cor-
pora. For ACE, we use only the newswire portion
because it is closest in composition to the MUC
corpora. Statistics for each of the data sets are
shown in Table 2. When available, we use the
standard test/train split. Otherwise, we randomly
split the data into a training and test set following
a 70/30 ratio.
</bodyText>
<footnote confidence="0.967604666666667">
3In trial runs, we investigated alternative classification
and clustering models (e.g. C4.5 decision trees and SVMs;
best-first clustering). The results were comparable.
</footnote>
<page confidence="0.995323">
658
</page>
<bodyText confidence="0.9913751">
Scoring Algorithms. We evaluate using two
common scoring algorithms4 — MUC and B3.
The MUC scoring algorithm (Vilain et al., 1995)
computes the F1 score (harmonic mean) of preci-
sion and recall based on the identifcation of unique
coreference links. We use the official MUC scorer
implementation for the two MUC corpora and an
equivalent implementation for ACE.
The B3 algorithm (Bagga and Baldwin, 1998)
computes a precision and recall score for each CE:
</bodyText>
<equation confidence="0.8351335">
precision(ce) = |R, ∩ K,|/|R,|
recall(ce) = |R, ∩ K,|/|K�e|,
</equation>
<bodyText confidence="0.95952356">
where RCe is the coreference chain to which ce is
assigned in the response (i.e. the system-generated
output) and KCe is the coreference chain that con-
tains ce in the key (i.e. the gold standard). Pre-
cision and recall for a set of documents are com-
puted as the mean over all CEs in the documents
and the F1 score of precision and recall is reported.
B3 Complications. Unlike the MUC score,
which counts links between CEs, B3 presumes
that the gold standard and the system response are
clusterings over the same set of CEs. This, of
course, is not the case when the system automat-
ically identifies the CEs, so the scoring algorithm
requires a mapping between extracted and anno-
tated CEs. We will use the term twin(ce) to refer
to the unique annotated/extracted CE to which the
extracted/annotated CE is matched. We say that
a CE is twinless (has no twin) if no corresponding
CE is identified. A twinless extracted CE signals
that the resolver extracted a spurious CE, while an
annotated CE is twinless when the resolver fails to
extract it.
Unfortunately, it is unclear how the B3 score
should be computed for twinless CEs. Bengtson
and Roth (2008) simply discard twinless CEs, but
this solution is likely too lenient — it doles no pun-
ishment for mistakes on twinless annotated or ex-
tracted CEs and it would be tricked, for example,
by a system that extracts only the CEs about which
it is most confident.
We propose two different ways to deal with
twinless CEs for B3. One option, B3all, retains
all twinless extracted CEs. It computes the preci-
4We also experimented with the CEAF score (Luo, 2005),
but excluded it due to difficulties dealing with the extracted,
rather than annotated, CEs. CEAF assigns a zero score to
each twinless extracted CE and weights all coreference chains
equally, irrespective of their size. As a result, runs with ex-
tracted CEs exhibit very low CEAF precision, leading to un-
reliable scores.
sion as above when ce has a twin, and computes
the precision as 1/|RCe |if ce is twinless. (Simi-
larly, recall(ce) = 1/|KC  |if ce is twinless.)
The second option, B30, discards twinless
extracted CEs, but penalizes recall by setting
recall(ce) = 0 for all twinless annotated CEs.
Thus, B30 presumes that all twinless extracted
CEs are spurious.
Results. Table 3, box 1 shows the performance
of RECONCILEACL09 using a default (0.5) coref-
erence classifier threshold. The MUC score is
highest for the MUC6 data set, while the four ACE
data sets show much higher B3 scores as com-
pared to the two MUC data sets. The latter occurs
because the ACE data sets include singletons.
The classification threshold, however, can be
gainfully employed to control the trade-off be-
tween precision and recall. This has not tradi-
tionally been done in learning-based coreference
resolution research — possibly because there is
not much training data available to sacrifice as a
validation set. Nonetheless, we hypothesized that
estimating a threshold from just the training data
might be effective. Our results (BASELINE box
in Table 3) indicate that this indeed works well.5
With the exception of MUC6, results on all data
sets and for all scoring algorithms improve; more-
over, the scores approach those for runs using an
optimal threshold (box 3) for the experiment as de-
termined by using the test set. In all remaining ex-
periments, we learn the threshold from the training
set as in the BASELINE system.
Below, we resume our investigation of the role
of three coreference resolution subtasks and mea-
sure the impact of each on overall performance.
</bodyText>
<subsectionHeader confidence="0.996502">
3.3 Named Entities
</subsectionHeader>
<bodyText confidence="0.998476416666667">
Previous work has shown that resolving corefer-
ence between proper names is relatively easy (e.g.
Kameyama (1997)) because string matching func-
tions specialized to the type of proper name (e.g.
person vs. location) are quite accurate. Thus, we
would expect a coreference resolution system to
depend critically on its Named Entity (NE) extrac-
tor. On the other hand, state-of-the-art NE taggers
are already quite good, so improving this compo-
nent may not provide much additional gain.
To study the influence of NE recognition,
we replace the system-generated NEs of
</bodyText>
<footnote confidence="0.988364">
5All experiments sample uniformly from 1000 threshold
values.
</footnote>
<page confidence="0.984854">
659
</page>
<table confidence="0.999973210526316">
ReconcileACL09 MUC6 MUC7 ACE-2 ACE03 ACE04 ACE05
1. DEFAULT THRESHOLD (0.5) MUC 70.40 58.20 65.76 66.73 56.75 64.30
B3all 69.91 62.88 77.25 77.56 73.03 72.82
B30 68.55 62.80 76.59 77.27 72.99 72.43
2. BASELINE MUC 68.50 62.80 65.99 67.87 62.03 67.41
= THRESHOLD ESTIMATION B3all 70.88 65.86 78.29 79.39 76.50 73.71
B30 68.43 64.57 76.63 77.88 75.41 72.47
3. OPTIMAL THRESHOLD MUC 71.20 62.90 66.83 68.35 62.11 67.41
B3all 72.31 66.52 78.50 79.41 76.53 74.25
B30 69.49 64.64 76.83 78.27 75.51 72.94
4. BASELINE with MUC 69.90 - 66.37 70.35 62.88 67.72
perfect NEs B3all 72.31 - 78.06 80.22 77.01 73.92
B30 67.91 - 76.55 78.35 75.22 72.90
5. BASELINE with MUC 85.80* 81.10* 76.39 79.68 76.18 79.42
perfect CEs B3all 76.14 75.88 78.65 80.58 77.79 76.49
B30 76.14 75.88 78.65 80.58 77.79 76.49
6. BASELINE with MUC 82.20* 71.90* 86.63 85.58 83.33 82.84
anaphoric CEs B3all 72.52 69.26 80.29 79.71 76.05 74.33
B30 72.52 69.26 80.29 79.71 76.05 74.33
</table>
<tableCaption confidence="0.993927">
Table 3: Impact of Three Subtasks on Coreference Resolution Performance. A score marked with a * indicates that a 0.5
threshold was used because threshold selection from the training data resulted in an extreme version of the system, i.e. one that
places all CEs into a single coreference chain.
</tableCaption>
<bodyText confidence="0.964009105263158">
RECONCILEACL09 with gold-standard NEs
and retrain the coreference classifier. Results
for each of the data sets are shown in box 4 of
Table 3. (No gold standard NEs are available for
MUC7.) Comparison to the BASELINE system
(box 2) shows that using gold standard NEs
leads to improvements on all data sets with the
exception of ACE2 and ACE05, on which perfor-
mance is virtually unchanged. The improvements
tend to be small, however, between 0.5 to 3
performance points. We attribute this to two
factors. First, as noted above, although far from
perfect, NE taggers generally perform reasonably
well. Second, only 20 to 25% of the coreference
element resolutions required for these data sets
involve a proper name (see Section 4).
Conclusion #1: Improving the performance of NE tag-
gers is not likely to have a large impact on the performance
of state-of-the-art coreference resolution systems.
</bodyText>
<subsectionHeader confidence="0.980469">
3.4 Coreference Element Detection
</subsectionHeader>
<bodyText confidence="0.990995333333334">
We expect CE detection to be an important sub-
problem for an end-to-end coreference system.
Results for a system that assumes perfect CEs
are shown in box 5 of Table 3. For these runs,
RECONCILEACL09 uses only the annotated CEs
for both training and testing. Using perfect CEs
solves a large part of the coreference resolution
task: the annotated CEs divulge anaphoricity in-
formation, perfect NP boundaries, and perfect in-
formation regarding the coreference relation de-
fined for the data set.
We see that focusing attention on all and only
the annotated CEs leads to (often substantial) im-
provements in performance on all metrics over
all data sets, especially when measured using the
MUC score.
Conclusion #2: Improving the ability of coreference re-
solvers to identify coreference elements would likely improve
the state-of-the-art immensely — by 10-20 points in MUC F1
score and from 2-12 F1 points for B3.
This finding explains previously published re-
sults that exhibit striking variability when run with
annotated CEs vs. system-extracted CEs. On the
MUC6 data set, for example, the best published
MUC score using extracted CEs is approximately
71 (Yang et al., 2003), while multiple systems
have produced MUC scores of approximately 85
when using annotated CEs (e.g. Luo et al. (2004),
McCallum and Wellner (2004)).
We argue that providing a resolver with the an-
notated CEs is a rather unrealistic evaluation: de-
termining whether an NP is part of an annotated
coreference chain is precisely the job of a corefer-
ence resolver!
Conclusion #3: Assuming the availability of CEs unre-
alistically simplifies the coreference resolution task.
</bodyText>
<subsectionHeader confidence="0.953634">
3.5 Anaphoricity Determination
</subsectionHeader>
<bodyText confidence="0.9744245">
Finally, several coreference systems have suc-
cessfully incorporated anaphoricity determination
</bodyText>
<page confidence="0.992966">
660
</page>
<bodyText confidence="0.990854842105263">
modules (e.g. Ng and Cardie (2002a) and Bean
and Riloff (2004)). The goal of the module is to
determine whether or not an NP is anaphoric. For
example, pleonastic pronouns (e.g. it is raining)
are special cases that do not require coreference
resolution.
Unfortunately, neither the MUC nor the ACE
data sets include anaphoricity information for all
NPs. Rather, they encode anaphoricity informa-
tion implicitly for annotated CEs: a CE is consid-
ered anaphoric if is not a singleton.6
To study the utility of anaphoricity informa-
tion, we train and test only on the “anaphoric” ex-
tracted CEs, i.e. the extracted CEs that have an
annotated twin that is not a singleton. Note that
for the MUC datasets all extracted CEs that have
twins are considered anaphoric.
Results for this experiment (box 6 in Table 3)
are similar to the previous experiment using per-
fect CEs: we observe big improvements across the
board. This should not be surprising since the ex-
perimental setting is quite close to that for perfect
CEs: this experiment also presumes knowledge
of when a CE is part of an annotated coreference
chain. Nevertheless, we see that anaphoricity info-
mation is important. First, good anaphoricity iden-
tification should reduce the set of extracted CEs
making it closer to the set of annotated CEs. Sec-
ond, further improvements in MUC score for the
ACE data sets over the runs using perfect CEs (box
5) reveal that accurately determining anaphoric-
ity can lead to substantial improvements in MUC
score. ACE data includes annotations for single-
ton CEs, so knowling whether an annotated CE is
anaphoric divulges additional information.
Conclusion #4: An accurate anaphoricity determina-
tion component can lead to substantial improvement in coref-
erence resolution performance.
</bodyText>
<sectionHeader confidence="0.991883" genericHeader="method">
4 Resolution Complexity
</sectionHeader>
<bodyText confidence="0.999692103448276">
Different types of anaphora that have to be han-
dled by coreference resolution systems exhibit dif-
ferent properties. In linguistic theory, binding
mechanisms vary for different kinds of syntactic
constituents and structures. And in practice, em-
pirical results have confirmed intuitions that differ-
ent types of anaphora benefit from different clas-
sifier features and exhibit varying degrees of diffi-
culty (Kameyama, 1997). However, performance
6Also, the first element of a coreference chain is usually
non-anaphoric, but we do not consider that issue here.
evaluations rarely include analysis of where state-
of-the-art coreference resolvers perform best and
worst, aside from general conclusions.
In this section, we analyze the behavior of
our coreference resolver on different types of
anaphoric expressions with two goals in mind.
First, we want to deduce the strengths and weak-
nesses of state-of-the-art systems to help direct
future research. Second, we aim to understand
why current coreference resolvers behave so in-
consistently across data sets. Our hypothesis is
that the distribution of different types of anaphoric
expressions in a corpus is a major factor for coref-
erence resolution performance. Our experiments
confirm this hypothesis and we use our empirical
results to create a coreference performance predic-
tion (CPP) measure that successfully estimates the
expected level of performance on novel data sets.
</bodyText>
<subsectionHeader confidence="0.981376">
4.1 Resolution Classes
</subsectionHeader>
<bodyText confidence="0.999948833333333">
We study the resolution complexity of a text cor-
pus by defining resolution classes. Resolution
classes partition the set of anaphoric CEs accord-
ing to properties of the anaphor and (in some
cases) the antecedent. Previous work has stud-
ied performance differences between pronominal
anaphora, proper names, and common nouns, but
we aim to dig deeper into subclasses of each of
these groups. In particular, we distinguish be-
tween proper and common nouns that can be re-
solved via string matching, versus those that have
no antecedent with a matching string. Intuitively,
we expect that it is easier to resolve the cases
that involve string matching. Similarly, we par-
tition pronominal anaphora into several subcate-
gories that we expect may behave differently. We
define the following nine resolution classes:
Proper Names: Three resolution classes cover
CEs that are named entities (e.g. the PER-
SON, LOCATION, ORGANIZATION and DATE
classes for MUC and ACE) and have a prior ref-
erent7 in the text. These three classes are distin-
guished by the type of antecedent that can be re-
solved against the proper name.
</bodyText>
<listItem confidence="0.998747142857143">
(1) PN-e: a proper name is assigned to this exact string match
class if there is at least one preceding CE in its gold standard
coreference chain that exactly matches it.
(2) PN-p: a proper name is assigned to this partial string
match class if there is at least one preceding CE in its gold
standard chain that has some content words in common.
(3) PN-n: a proper name is assigned to this no string match
</listItem>
<footnote confidence="0.994184">
7We make a rough, but rarely inaccurate, assumption that
there are no cataphoric expressions in the data.
</footnote>
<page confidence="0.987812">
661
</page>
<table confidence="0.998915727272727">
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05 Avg
# % scr # % scr # % scr # % scr # % scr # % scr % scr
PN-e 273 17 .87 249 19 .79 346 24 .94 435 25 .93 267 16 .88 373 31 .92 22 .89
PN-p 157 10 .68 79 6 .59 116 8 .86 178 10 .87 194 11 .71 125 10 .71 9 .74
PN-n 18 1 .18 18 1 .28 85 6 .19 79 4 .15 66 4 .21 89 7 .27 4 .21
CN-e 292 18 .82 276 21 .65 84 6 .40 186 11 .68 165 10 .68 134 11 .79 13 .67
CN-p 229 14 .53 239 18 .49 147 10 .26 168 10 .24 147 9 .40 147 12 .43 12 .39
CN-n 194 12 .27 148 11 .15 152 10 .50 148 8 .90 266 16 .32 121 10 .20 11 .18
1+2Pr 48 3 .70 65 5 .66 122 8 .73 76 4 .73 158 9 .77 51 4 .61 6 .70
G3Pr 160 10 .73 50 4 .79 181 12 .83 237 13 .82 246 14 .84 69 60 .81 10 .80
U3Pr 175 11 .49 142 11 .49 163 11 .45 122 7 .48 153 9 .49 91 7 .49 9 .48
</table>
<tableCaption confidence="0.846007666666667">
Table 4: Frequencies and scores for each resolution class.
class if no preceding CE in its gold standard chain has any
content words in common with it.
</tableCaption>
<bodyText confidence="0.428660333333333">
Common NPs: Three analogous string match
classes cover CEs that have a common noun as a
head: (4) CN-e (5) CN-p (6) CN-n.
</bodyText>
<listItem confidence="0.944066833333333">
Pronouns: Three classes cover pronouns:
(7) 1+2Pr: The anaphor is a 1st or 2nd person pronoun.
(8) G3Pr: The anaphor is a gendered 3rd person pronoun
(e.g. “she”, “him”).
(9) U3Pr: The anaphor is an ungendered 3rd person pro-
noun.
</listItem>
<bodyText confidence="0.9999165">
As noted above, resolution classes are defined for
annotated CEs. We use the twin relationship to
match extracted CEs to annotated CEs and to eval-
uate performance on each resolution class.
</bodyText>
<subsectionHeader confidence="0.999627">
4.2 Scoring Resolution Classes
</subsectionHeader>
<bodyText confidence="0.999730388888889">
To score each resolution class separately, we de-
fine a new variant of the MUC scorer. We compute
a MUC-RC score (for MUC Resolution Class) for
class C as follows: we assume that all CEs that do
not belong to class C are resolved correctly by tak-
ing the correct clustering for them from the gold
standard. Starting with this correct partial cluster-
ing, we run our classifier on all ordered pairs of
CEs for which the second CE is of class C, es-
sentially asking our coreference resolver to deter-
mine whether each member of class C is corefer-
ent with each of its preceding CEs. We then count
the number of unique correct/incorrect links that
the system introduced on top of the correct par-
tial clustering and compute precision, recall, and
F1 score. This scoring function directly measures
the impact of each resolution class on the overall
MUC score.
</bodyText>
<subsectionHeader confidence="0.921164">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.9998056">
Table 4 shows the results of our resolution class
analysis on the test portions of the six data sets.
The # columns show the frequency counts for each
resolution class, and the % columns show the dis-
tributions of the classes in each corpus (i.e. 17%
</bodyText>
<table confidence="0.995601">
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05
0.92 0.95 0.91 0.98 0.97 0.96
</table>
<tableCaption confidence="0.981138">
Table 5: Correlations of resolution class scores with respect
to the average.
</tableCaption>
<bodyText confidence="0.9969018">
of all resolutions in the MUC6 corpus were in the
PN-e class). The scr columns show the MUC-
RC score for each resolution class. The right-hand
side of Table 4 shows the average distribution and
scores across all data sets.
These scores confirm our expectations about the
relative difficulty of different types of resolutions.
For example, it appears that proper names are eas-
ier to resolve than common nouns; gendered pro-
nouns are easier than 1st and 2nd person pronouns,
which, in turn, are easier than ungendered 3rd per-
son pronouns. Similarly, our intuition is confirmed
that many CEs can be accurately resolved based on
exact string matching, whereas resolving against
antecedents that do not have overlapping strings is
much more difficult. The average scores in Table 4
show that performance varies dramatically across
the resolution classes, but, on the surface, appears
to be relatively consistent across data sets.
None of the data sets performs exactly the same,
of course, so we statistically analyze whether the
behavior of each resolution class is similar across
the data sets. For each data set, we compute the
correlation between the vector of MUC-RC scores
over the resolution classes and the average vec-
tor of MUC-RC scores for the remaining five data
sets. Table 5 contains the results, which show high
correlations (over .90) for all six data sets. These
results indicate that the relative performance of the
resolution classes is consistent across corpora.
</bodyText>
<subsectionHeader confidence="0.998021">
4.4 Coreference Performance Prediction
</subsectionHeader>
<bodyText confidence="0.999442666666667">
Next, we hypothesize that the distribution of res-
olution classes in a corpus explains (at least par-
tially) why performance varies so much from cor-
</bodyText>
<page confidence="0.99167">
662
</page>
<table confidence="0.998775">
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05
P 0.59 0.59 0.62 0.65 0.59 0.62
O 0.67 0.61 0.66 0.68 0.62 0.67
</table>
<tableCaption confidence="0.998556">
Table 6: Predicted (P) vs Observed (O) scores.
</tableCaption>
<bodyText confidence="0.431055">
pus to corpus. To explore this issue, we create a
</bodyText>
<subsubsectionHeader confidence="0.563252">
Coreference Performance Prediction (CPP) mea-
</subsubsectionHeader>
<bodyText confidence="0.977780976744186">
sure to predict the performance on new data sets.
The CPP measure uses the empirical performance
of each resolution class observed on previous data
sets and forms a predicton based on the make-up
of resolution classes in a new corpus. The distribu-
tion of resolution classes for a new corpus can be
easily determined because the classes can be rec-
ognized superficially by looking only at the strings
that represent each NP.
We compute the CPP score for each of our six
data sets based on the average resolution class per-
formance measured on the other five data sets.
The predicted score for each class is computed as
a weighted sum of the observed scores for each
resolution class (i.e. the mean for the class mea-
sured on the other five data sets) weighted by the
proportion of CEs that belong to the class. The
predicted scores are shown in Table 6 and com-
pared with the MUC scores that are produced by
RECONCILEACL09.8
Our results show that the CPP measure is a
good predictor of coreference resolution perfor-
mance on unseen data sets, with the exception
of one outlier – the MUC6 data set. In fact,
the correlation between predicted and observed
scores is 0.731 for all data sets and 0.913 exclud-
ing MUC6. RECONCILEACL09’s performance on
MUC6 is better than predicted due to the higher
than average scores for the common noun classes.
We attribute this to the fact that MUC6 includes
annotations for nested nouns, which almost al-
ways fall in the CN-e and CN-p classes. In ad-
dition, many of the features were first created for
the MUC6 data set, so the feature extractors are
likely more accurate than for other data sets.
Overall, results indicate that coreference perfor-
mance is substantially influenced by the mix of
resolution classes found in the data set. Our CPP
measure can be used to produce a good estimate
of the level of performance on a new corpus.
8Observed scores for MUC6 and 7 differ slightly from Ta-
ble 3 because this part of the work did not use the OPTIONAL
field of the key, employed by the official MUC scorer.
</bodyText>
<sectionHeader confidence="0.999728" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999980741935484">
The bulk of the relevant related work is described
in earlier sections, as appropriate. This paper stud-
ies complexity issues for NP coreference resolu-
tion using a “good”, i.e. near state-of-the-art, sys-
tem. For state-of-the-art performance on the MUC
data sets see, e.g. Yang et al. (2003); for state-of-
the-art performance on the ACE data sets see, e.g.
Bengtson and Roth (2008) and Luo (2007). While
other researchers have evaluated NP coreference
resolvers with respect to pronouns vs. proper
nouns vs. common nouns (Ng and Cardie, 2002b),
our analysis focuses on measuring the complexity
of data sets, predicting the performance of coref-
erence systems on new data sets, and quantify-
ing the effect of coreference system subcompo-
nents on overall performance. In the related area
of anaphora resolution, researchers have studied
the influence of subsystems on the overall per-
formance (Mitkov, 2002) as well as defined and
evaluated performance on different classes of pro-
nouns (e.g. Mitkov (2002) and Byron (2001)).
However, due to the significant differences in task
definition, available datasets, and evaluation met-
rics, their conclusions are not directly applicable
to the full coreference task.
Previous work has developed methods to predict
system performance on NLP tasks given data set
characteristics, e.g. Birch et al. (2008) does this for
machine translation. Our work looks for the first
time at predicting the performance of NP corefer-
ence resolvers.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.991215411764706">
We examine the state-of-the-art in NP coreference
resolution. We show the relative impact of perfect
NE recognition, perfect anaphoricity information
for coreference elements, and knowledge of all
and only the annotated CEs. We also measure the
performance of state-of-the-art resolvers on sev-
eral classes of anaphora and use these results to
develop a measure that can accurately estimate a
resolver’s performance on new data sets.
Acknowledgments. We gratefully acknowledge
technical contributions from David Buttler and
David Hysom in creating the Reconcile corefer-
ence resolution platform. This research was sup-
ported in part by the Department of Homeland
Security under ONR Grant N0014-07-1-0152 and
Lawrence Livermore National Laboratory subcon-
tract B573245.
</bodyText>
<page confidence="0.996624">
663
</page>
<bodyText confidence="0.9982342">
Xiaoqiang Luo. 2007. Coreference or Not: A Twin
Model for Coreference Resolution. In Proceedings
of the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL 2007).
</bodyText>
<sectionHeader confidence="0.906497" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995868495049504">
A. Bagga and B. Baldwin. 1998. Algorithms for Scor-
ing Coreference Chains. In In Linguistic Corefer-
ence Workshop at LREC 1998.
Baldridge, J. 2005. The OpenNLP project.
http://opennlp.sourceforge.net/.
D. Bean and E. Riloff. 2004. Unsupervised Learn-
ing of Contextual Role Knowledge for Coreference
Resolution. In Proceedings of the Annual Meeting
of the North American Chapter of the Association
for Computational Linguistics (HLT/NAACL 2004).
Eric Bengtson and Dan Roth. 2008. Understanding
the Value of Features for Coreference Resolution.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
294–303. Association for Computational Linguis-
tics.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting Success in Machine Translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
745–754. Association for Computational Linguis-
tics.
Donna Byron. 2001. The Uncommon Denomina-
tor: A Proposal for Consistent Reporting of Pro-
noun Resolution Results. Computational Linguis-
tics, 27(4):569–578.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
LREC.
J. Finkel, S. Dingare, H. Nguyen, M. Nissim, and
C. Manning. 2004. Exploiting Context for Biomed-
ical Entity Recognition: From Syntax to the Web. In
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications at COLING 2004.
Yoav Freund and Robert E. Schapire. 1999. Large
Margin Classification Using the Perceptron Algo-
rithm. In Machine Learning, pages 277–296.
Megumi Kameyama. 1997. Recognizing Referential
Links: An Information Extraction Perspective. In
Workshop On Operational Factors In Practical Ro-
bust Anaphora Resolution For Unrestricted Texts.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Al-
gorithm Based on the Bell Tree. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics.
X. Luo. 2005. On Coreference Resolution Perfor-
mance Metrics. In Proceedings of the 2005 Human
Language Technology Conference / Conference on
Empirical Methods in Natural Language Process-
ing.
A. McCallum and B. Wellner. 2004. Conditional Mod-
els of Identity Uncertainty with Application to Noun
Coreference. In 18th Annual Conference on Neural
Information Processing Systems.
Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man, London.
MUC-6. 1995. Coreference Task Definition. In Pro-
ceedings of the Sixth Message Understanding Con-
ference (MUC-6), pages 335–344.
MUC-7. 1997. Coreference Task Definition. In
Proceedings of the Seventh Message Understanding
Conference (MUC-7).
V. Ng and C. Cardie. 2002a. Identifying Anaphoric
and Non-Anaphoric Noun Phrases to Improve
Coreference Resolution. In Proceedings of the 19th
International Conference on Computational Lin-
guistics (COLING 2002).
V. Ng and C. Cardie. 2002b. Improving Machine
Learning Approaches to Coreference Resolution. In
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics.
NIST. 2004. The ACE Evaluation Plan.
S. Petrov and D. Klein. 2007. Improved Inference for
Unlexicalized Parsing. In Proceedings of the Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT/NAACL
2007).
W. Soon, H. Ng, and D. Lim. 2001. A Machine
Learning Approach to Coreference of Noun Phrases.
Computational Linguistics, 27(4):521–541.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, Ellen
Riloff, David Buttler, and David Hysom. 2009.
Reconcile: A Coreference Resolution Research Plat-
form. Computer Science Technical Report, Cornell
University, Ithaca, NY.
Kees van Deemter and Rodger Kibble. 2000. On
Coreferring: Coreference in MUC and Related
Annotation Schemes. Computational Linguistics,
26(4):629–637.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A Model-Theoretic Corefer-
ence Scoring Theme. In Proceedings of the Sixth
Message Understanding Conference (MUC-6).
Xiaofeng Yang, Guodong Zhou, Jian Su, and
Chew Lim Tan. 2003. Coreference Resolution Us-
ing Competition Learning Approach. In ACL ’03:
Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics, pages 176–183.
</reference>
<page confidence="0.998133">
664
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.877435">
<title confidence="0.9636105">Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art</title>
<author confidence="0.973801">Veselin Stoyanov Nathan Gilbert Claire Cardie Ellen Riloff</author>
<affiliation confidence="0.999931">Cornell University University of Utah Cornell University University of Utah</affiliation>
<address confidence="0.978135">Ithaca, NY Salt Lake City, UT Ithaca, NY Salt Lake City, UT</address>
<email confidence="0.998097">ves@cs.cornell.edungilbert@cs.utah.educardie@cs.cornell.eduriloff@cs.utah.edu</email>
<abstract confidence="0.999705611111111">We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora. First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection. We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task. Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Algorithms for Scoring Coreference Chains. In</title>
<date>1998</date>
<booktitle>In Linguistic Coreference Workshop at LREC</booktitle>
<contexts>
<context position="13848" citStr="Bagga and Baldwin, 1998" startWordPosition="2149" endWordPosition="2152">training and test set following a 70/30 ratio. 3In trial runs, we investigated alternative classification and clustering models (e.g. C4.5 decision trees and SVMs; best-first clustering). The results were comparable. 658 Scoring Algorithms. We evaluate using two common scoring algorithms4 — MUC and B3. The MUC scoring algorithm (Vilain et al., 1995) computes the F1 score (harmonic mean) of precision and recall based on the identifcation of unique coreference links. We use the official MUC scorer implementation for the two MUC corpora and an equivalent implementation for ACE. The B3 algorithm (Bagga and Baldwin, 1998) computes a precision and recall score for each CE: precision(ce) = |R, ∩ K,|/|R,| recall(ce) = |R, ∩ K,|/|K�e|, where RCe is the coreference chain to which ce is assigned in the response (i.e. the system-generated output) and KCe is the coreference chain that contains ce in the key (i.e. the gold standard). Precision and recall for a set of documents are computed as the mean over all CEs in the documents and the F1 score of precision and recall is reported. B3 Complications. Unlike the MUC score, which counts links between CEs, B3 presumes that the gold standard and the system response are cl</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Algorithms for Scoring Coreference Chains. In In Linguistic Coreference Workshop at LREC 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Baldridge</author>
</authors>
<date>2005</date>
<journal>The OpenNLP</journal>
<note>project. http://opennlp.sourceforge.net/.</note>
<marker>Baldridge, 2005</marker>
<rawString>Baldridge, J. 2005. The OpenNLP project. http://opennlp.sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bean</author>
<author>E Riloff</author>
</authors>
<title>Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution.</title>
<date>2004</date>
<booktitle>In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL</booktitle>
<contexts>
<context position="22155" citStr="Bean and Riloff (2004)" startWordPosition="3522" endWordPosition="3525"> MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al. (2004), McCallum and Wellner (2004)). We argue that providing a resolver with the annotated CEs is a rather unrealistic evaluation: determining whether an NP is part of an annotated coreference chain is precisely the job of a coreference resolver! Conclusion #3: Assuming the availability of CEs unrealistically simplifies the coreference resolution task. 3.5 Anaphoricity Determination Finally, several coreference systems have successfully incorporated anaphoricity determination 660 modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)). The goal of the module is to determine whether or not an NP is anaphoric. For example, pleonastic pronouns (e.g. it is raining) are special cases that do not require coreference resolution. Unfortunately, neither the MUC nor the ACE data sets include anaphoricity information for all NPs. Rather, they encode anaphoricity information implicitly for annotated CEs: a CE is considered anaphoric if is not a singleton.6 To study the utility of anaphoricity information, we train and test only on the “anaphoric” extracted CEs, i.e. the extracted CEs that have an annotated twin that is not a singleto</context>
</contexts>
<marker>Bean, Riloff, 2004</marker>
<rawString>D. Bean and E. Riloff. 2004. Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution. In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengtson</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the Value of Features for Coreference Resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>294--303</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9954" citStr="Bengtson and Roth (2008)" startWordPosition="1546" endWordPosition="1549"> determination, and coreference element detection — in the performance of an end-to-end coreference resolution system. First, however, we describe the coreference resolver that we use for our study. 3.1 The RECONCILEACL09 Coreference Resolver We use the RECONCILE coreference resolution platform (Stoyanov et al., 2009) to configure a coreference resolver that performs comparably to state-of-the-art systems (when evaluated on the MUC and ACE data sets under comparable assumptions). This system is a classification-based coreference resolver, modeled after the systems of Ng and Cardie (2002b) and Bengtson and Roth (2008). First it classifies pairs of CEs as coreferent or not coreferent, pairing each identified CE with all preceding CEs. The CEs are then clustered into coreference chains2 based on the pairwise decisions. RECONCILE has a pipeline architecture with four main steps: preprocessing, feature extraction, classification, and clustering. We will refer to the specific configuration of RECONCILE used for this paper as RECONCILEACL09. Preprocessing. The RECONCILEACL09 preprocessor applies a series of language analysis tools (mostly publicly available software packages) to the source texts. The OpenNLP too</context>
<context position="11834" citStr="Bengtson and Roth (2008)" startWordPosition="1834" endWordPosition="1837">refer to a particular entity. dataset docs CEs chains CEs/ch tr/tst split MUC6 60 4232 960 4.4 30/30 (st) MUC7 50 4297 1081 3.9 30/20 (st) ACE-2 159 2630 1148 2.3 130/29 (st) ACE03 105 3106 1340 2.3 74/31 ACE04 128 3037 1332 2.3 90/38 ACE05 81 1991 775 2.6 57/24 Table 2: Dataset characteristics including the number of documents, annotated CEs, coreference chains, annotated CEs per chain (average), and number of documents in the train/test split. We use st to indicate a standard train/test split. fairly comprehensive set of 61 features introduced in previous coreference resolution systems (see Bengtson and Roth (2008)). We briefly summarize the features here and refer the reader to Stoyanov et al. (2009) for more details. Lexical (9): String-based comparisons of the two CEs, such as exact string matching and head noun matching. Proximity (5): Sentence and paragraph-based measures of the distance between two CEs. Grammatical (28): A wide variety of syntactic properties of the CEs, either individually or as a pair. These features are based on part-of-speech tags, parse trees, or dependency relations. For example: one feature indicates whether both CEs are syntactic subjects; another indicates whether the CEs</context>
<context position="15108" citStr="Bengtson and Roth (2008)" startWordPosition="2370" endWordPosition="2373">This, of course, is not the case when the system automatically identifies the CEs, so the scoring algorithm requires a mapping between extracted and annotated CEs. We will use the term twin(ce) to refer to the unique annotated/extracted CE to which the extracted/annotated CE is matched. We say that a CE is twinless (has no twin) if no corresponding CE is identified. A twinless extracted CE signals that the resolver extracted a spurious CE, while an annotated CE is twinless when the resolver fails to extract it. Unfortunately, it is unclear how the B3 score should be computed for twinless CEs. Bengtson and Roth (2008) simply discard twinless CEs, but this solution is likely too lenient — it doles no punishment for mistakes on twinless annotated or extracted CEs and it would be tricked, for example, by a system that extracts only the CEs about which it is most confident. We propose two different ways to deal with twinless CEs for B3. One option, B3all, retains all twinless extracted CEs. It computes the preci4We also experimented with the CEAF score (Luo, 2005), but excluded it due to difficulties dealing with the extracted, rather than annotated, CEs. CEAF assigns a zero score to each twinless extracted CE</context>
<context position="34008" citStr="Bengtson and Roth (2008)" startWordPosition="5597" endWordPosition="5600"> good estimate of the level of performance on a new corpus. 8Observed scores for MUC6 and 7 differ slightly from Table 3 because this part of the work did not use the OPTIONAL field of the key, employed by the official MUC scorer. 5 Related Work The bulk of the relevant related work is described in earlier sections, as appropriate. This paper studies complexity issues for NP coreference resolution using a “good”, i.e. near state-of-the-art, system. For state-of-the-art performance on the MUC data sets see, e.g. Yang et al. (2003); for state-ofthe-art performance on the ACE data sets see, e.g. Bengtson and Roth (2008) and Luo (2007). While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>Eric Bengtson and Dan Roth. 2008. Understanding the Value of Features for Coreference Resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 294–303. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Predicting Success in Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>745--754</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="34965" citStr="Birch et al. (2008)" startWordPosition="5743" endWordPosition="5746">ystem subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (2002) and Byron (2001)). However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusions are not directly applicable to the full coreference task. Previous work has developed methods to predict system performance on NLP tasks given data set characteristics, e.g. Birch et al. (2008) does this for machine translation. Our work looks for the first time at predicting the performance of NP coreference resolvers. 6 Conclusions We examine the state-of-the-art in NP coreference resolution. We show the relative impact of perfect NE recognition, perfect anaphoricity information for coreference elements, and knowledge of all and only the annotated CEs. We also measure the performance of state-of-the-art resolvers on several classes of anaphora and use these results to develop a measure that can accurately estimate a resolver’s performance on new data sets. Acknowledgments. We grat</context>
</contexts>
<marker>Birch, Osborne, Koehn, 2008</marker>
<rawString>Alexandra Birch, Miles Osborne, and Philipp Koehn. 2008. Predicting Success in Machine Translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 745–754. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Byron</author>
</authors>
<title>The Uncommon Denominator: A Proposal for Consistent Reporting of Pronoun Resolution Results.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="34643" citStr="Byron (2001)" startWordPosition="5698" endWordPosition="5699">other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (2002) and Byron (2001)). However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusions are not directly applicable to the full coreference task. Previous work has developed methods to predict system performance on NLP tasks given data set characteristics, e.g. Birch et al. (2008) does this for machine translation. Our work looks for the first time at predicting the performance of NP coreference resolvers. 6 Conclusions We examine the state-of-the-art in NP coreference resolution. We show the relative impact of perfect NE recognition, perfect anaphorici</context>
</contexts>
<marker>Byron, 2001</marker>
<rawString>Donna Byron. 2001. The Uncommon Denominator: A Proposal for Consistent Reporting of Pronoun Resolution Results. Computational Linguistics, 27(4):569–578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>S Dingare</author>
<author>H Nguyen</author>
<author>M Nissim</author>
<author>C Manning</author>
</authors>
<title>Exploiting Context for Biomedical Entity Recognition: From Syntax to the Web. In</title>
<date>2004</date>
<booktitle>Joint Workshop on Natural Language Processing in Biomedicine and its Applications at COLING</booktitle>
<contexts>
<context position="10887" citStr="Finkel et al., 2004" startWordPosition="1682" endWordPosition="1685">, and clustering. We will refer to the specific configuration of RECONCILE used for this paper as RECONCILEACL09. Preprocessing. The RECONCILEACL09 preprocessor applies a series of language analysis tools (mostly publicly available software packages) to the source texts. The OpenNLP toolkit (Baldridge, J., 2005) performs tokenization, sentence splitting, and part-of-speech tagging. The Berkeley parser (Petrov and Klein, 2007) generates phrase structure parse trees, and the de Marneffe et al. (2006) system produces dependency relations. We employ the Stanford CRF-based Named Entity Recognizer (Finkel et al., 2004) for named entity tagging. With these preprocessing components, RECONCILEACL09 uses heuristics to correctly extract approximately 90% of the annotated CEs for the MUC and ACE data sets. Feature Set. To achieve roughly state-of-theart performance, RECONCILEACL09 employs a 2A coreference chain refers to the set of CEs that refer to a particular entity. dataset docs CEs chains CEs/ch tr/tst split MUC6 60 4232 960 4.4 30/30 (st) MUC7 50 4297 1081 3.9 30/20 (st) ACE-2 159 2630 1148 2.3 130/29 (st) ACE03 105 3106 1340 2.3 74/31 ACE04 128 3037 1332 2.3 90/38 ACE05 81 1991 775 2.6 57/24 Table 2: Datas</context>
</contexts>
<marker>Finkel, Dingare, Nguyen, Nissim, Manning, 2004</marker>
<rawString>J. Finkel, S. Dingare, H. Nguyen, M. Nissim, and C. Manning. 2004. Exploiting Context for Biomedical Entity Recognition: From Syntax to the Web. In Joint Workshop on Natural Language Processing in Biomedicine and its Applications at COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large Margin Classification Using the Perceptron Algorithm.</title>
<date>1999</date>
<booktitle>In Machine Learning,</booktitle>
<pages>277--296</pages>
<contexts>
<context position="12789" citStr="Freund and Schapire, 1999" startWordPosition="1979" endWordPosition="1982">iety of syntactic properties of the CEs, either individually or as a pair. These features are based on part-of-speech tags, parse trees, or dependency relations. For example: one feature indicates whether both CEs are syntactic subjects; another indicates whether the CEs are in an appositive construction. Semantic (19): Capture semantic information about one or both NPs such as tests for gender and animacy, semantic compatibility based on WordNet, and semantic comparisons of NE types. Classification and Clustering. We configure RECONCILEACL09 to use the Averaged Perceptron learning algorithm (Freund and Schapire, 1999) and to employ single-link clustering (i.e. transitive closure) to generate the final partitioning.3 3.2 Baseline System Results Our experiments rely on the MUC and ACE corpora. For ACE, we use only the newswire portion because it is closest in composition to the MUC corpora. Statistics for each of the data sets are shown in Table 2. When available, we use the standard test/train split. Otherwise, we randomly split the data into a training and test set following a 70/30 ratio. 3In trial runs, we investigated alternative classification and clustering models (e.g. C4.5 decision trees and SVMs; b</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large Margin Classification Using the Perceptron Algorithm. In Machine Learning, pages 277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Megumi Kameyama</author>
</authors>
<title>Recognizing Referential Links: An Information Extraction Perspective. In Workshop On Operational Factors In Practical Robust Anaphora Resolution For Unrestricted Texts.</title>
<date>1997</date>
<contexts>
<context position="17640" citStr="Kameyama (1997)" startWordPosition="2795" endWordPosition="2796">ks well.5 With the exception of MUC6, results on all data sets and for all scoring algorithms improve; moreover, the scores approach those for runs using an optimal threshold (box 3) for the experiment as determined by using the test set. In all remaining experiments, we learn the threshold from the training set as in the BASELINE system. Below, we resume our investigation of the role of three coreference resolution subtasks and measure the impact of each on overall performance. 3.3 Named Entities Previous work has shown that resolving coreference between proper names is relatively easy (e.g. Kameyama (1997)) because string matching functions specialized to the type of proper name (e.g. person vs. location) are quite accurate. Thus, we would expect a coreference resolution system to depend critically on its Named Entity (NE) extractor. On the other hand, state-of-the-art NE taggers are already quite good, so improving this component may not provide much additional gain. To study the influence of NE recognition, we replace the system-generated NEs of 5All experiments sample uniformly from 1000 threshold values. 659 ReconcileACL09 MUC6 MUC7 ACE-2 ACE03 ACE04 ACE05 1. DEFAULT THRESHOLD (0.5) MUC 70.</context>
<context position="24302" citStr="Kameyama, 1997" startWordPosition="3866" endWordPosition="3867">divulges additional information. Conclusion #4: An accurate anaphoricity determination component can lead to substantial improvement in coreference resolution performance. 4 Resolution Complexity Different types of anaphora that have to be handled by coreference resolution systems exhibit different properties. In linguistic theory, binding mechanisms vary for different kinds of syntactic constituents and structures. And in practice, empirical results have confirmed intuitions that different types of anaphora benefit from different classifier features and exhibit varying degrees of difficulty (Kameyama, 1997). However, performance 6Also, the first element of a coreference chain is usually non-anaphoric, but we do not consider that issue here. evaluations rarely include analysis of where stateof-the-art coreference resolvers perform best and worst, aside from general conclusions. In this section, we analyze the behavior of our coreference resolver on different types of anaphoric expressions with two goals in mind. First, we want to deduce the strengths and weaknesses of state-of-the-art systems to help direct future research. Second, we aim to understand why current coreference resolvers behave so </context>
</contexts>
<marker>Kameyama, 1997</marker>
<rawString>Megumi Kameyama. 1997. Recognizing Referential Links: An Information Extraction Perspective. In Workshop On Operational Factors In Practical Robust Anaphora Resolution For Unrestricted Texts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>A Mention-Synchronous Coreference Resolution Algorithm Based on the Bell Tree.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="21613" citStr="Luo et al. (2004)" startWordPosition="3442" endWordPosition="3445">ed using the MUC score. Conclusion #2: Improving the ability of coreference resolvers to identify coreference elements would likely improve the state-of-the-art immensely — by 10-20 points in MUC F1 score and from 2-12 F1 points for B3. This finding explains previously published results that exhibit striking variability when run with annotated CEs vs. system-extracted CEs. On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al., 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al. (2004), McCallum and Wellner (2004)). We argue that providing a resolver with the annotated CEs is a rather unrealistic evaluation: determining whether an NP is part of an annotated coreference chain is precisely the job of a coreference resolver! Conclusion #3: Assuming the availability of CEs unrealistically simplifies the coreference resolution task. 3.5 Anaphoricity Determination Finally, several coreference systems have successfully incorporated anaphoricity determination 660 modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)). The goal of the module is to determine whether or not a</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A Mention-Synchronous Coreference Resolution Algorithm Based on the Bell Tree. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>On Coreference Resolution Performance Metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 Human Language Technology Conference / Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="15559" citStr="Luo, 2005" startWordPosition="2452" endWordPosition="2453"> is twinless when the resolver fails to extract it. Unfortunately, it is unclear how the B3 score should be computed for twinless CEs. Bengtson and Roth (2008) simply discard twinless CEs, but this solution is likely too lenient — it doles no punishment for mistakes on twinless annotated or extracted CEs and it would be tricked, for example, by a system that extracts only the CEs about which it is most confident. We propose two different ways to deal with twinless CEs for B3. One option, B3all, retains all twinless extracted CEs. It computes the preci4We also experimented with the CEAF score (Luo, 2005), but excluded it due to difficulties dealing with the extracted, rather than annotated, CEs. CEAF assigns a zero score to each twinless extracted CE and weights all coreference chains equally, irrespective of their size. As a result, runs with extracted CEs exhibit very low CEAF precision, leading to unreliable scores. sion as above when ce has a twin, and computes the precision as 1/|RCe |if ce is twinless. (Similarly, recall(ce) = 1/|KC |if ce is twinless.) The second option, B30, discards twinless extracted CEs, but penalizes recall by setting recall(ce) = 0 for all twinless annotated CEs.</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>X. Luo. 2005. On Coreference Resolution Performance Metrics. In Proceedings of the 2005 Human Language Technology Conference / Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Conditional Models of Identity Uncertainty with Application to Noun Coreference.</title>
<date>2004</date>
<booktitle>In 18th Annual Conference on Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1907" citStr="McCallum and Wellner (2004)" startWordPosition="272" endWordPosition="276"> problems, the state-of-the-art in noun phrase (NP) coreference resolution is typically quantified based on system performance on manually annotated text corpora. In spite of the availability of several benchmark data sets (e.g. MUC-6 (1995), ACE NIST (2004)) and their use in many formal evaluations, as a field we can make surprisingly few conclusive statements about the state-of-theart in NP coreference resolution. In particular, it remains difficult to assess the effectiveness of different coreference resolution approaches, even in relative terms. For example, the 91.5 F-measure reported by McCallum and Wellner (2004) was produced by a system using perfect information for several linguistic subproblems. In contrast, the 71.3 F-measure reported by Yang et al. (2003) represents a fully automatic end-to-end resolver. It is impossible to assess which approach truly performs best because of the dramatically different assumptions of each evaluation. Results vary widely across data sets. Coreference resolution scores range from 85-90% on the ACE 2004 and 2005 data sets to a much lower 60- 70% on the MUC 6 and 7 data sets (e.g. Soon et al. (2001) and Yang et al. (2003)). What accounts for these differences? Are th</context>
<context position="21642" citStr="McCallum and Wellner (2004)" startWordPosition="3446" endWordPosition="3449">ore. Conclusion #2: Improving the ability of coreference resolvers to identify coreference elements would likely improve the state-of-the-art immensely — by 10-20 points in MUC F1 score and from 2-12 F1 points for B3. This finding explains previously published results that exhibit striking variability when run with annotated CEs vs. system-extracted CEs. On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al., 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al. (2004), McCallum and Wellner (2004)). We argue that providing a resolver with the annotated CEs is a rather unrealistic evaluation: determining whether an NP is part of an annotated coreference chain is precisely the job of a coreference resolver! Conclusion #3: Assuming the availability of CEs unrealistically simplifies the coreference resolution task. 3.5 Anaphoricity Determination Finally, several coreference systems have successfully incorporated anaphoricity determination 660 modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)). The goal of the module is to determine whether or not an NP is anaphoric. For exampl</context>
</contexts>
<marker>McCallum, Wellner, 2004</marker>
<rawString>A. McCallum and B. Wellner. 2004. Conditional Models of Identity Uncertainty with Application to Noun Coreference. In 18th Annual Conference on Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Anaphora Resolution.</title>
<date>2002</date>
<publisher>Longman,</publisher>
<location>London.</location>
<contexts>
<context position="34528" citStr="Mitkov, 2002" startWordPosition="5679" endWordPosition="5680">003); for state-ofthe-art performance on the ACE data sets see, e.g. Bengtson and Roth (2008) and Luo (2007). While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (2002) and Byron (2001)). However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusions are not directly applicable to the full coreference task. Previous work has developed methods to predict system performance on NLP tasks given data set characteristics, e.g. Birch et al. (2008) does this for machine translation. Our work looks for the first time at predicting the performance of NP coreference resolvers. 6 Conclusions We examine the state</context>
</contexts>
<marker>Mitkov, 2002</marker>
<rawString>Ruslan Mitkov. 2002. Anaphora Resolution. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-6</author>
</authors>
<title>Coreference Task Definition.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<pages>335--344</pages>
<marker>MUC-6, 1995</marker>
<rawString>MUC-6. 1995. Coreference Task Definition. In Proceedings of the Sixth Message Understanding Conference (MUC-6), pages 335–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-7</author>
</authors>
<title>Coreference Task Definition.</title>
<date>1997</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7).</booktitle>
<marker>MUC-7, 1997</marker>
<rawString>MUC-7. 1997. Coreference Task Definition. In Proceedings of the Seventh Message Understanding Conference (MUC-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Identifying Anaphoric and Non-Anaphoric Noun Phrases to Improve Coreference Resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="9923" citStr="Ng and Cardie (2002" startWordPosition="1541" endWordPosition="1544"> recognition, anaphoricity determination, and coreference element detection — in the performance of an end-to-end coreference resolution system. First, however, we describe the coreference resolver that we use for our study. 3.1 The RECONCILEACL09 Coreference Resolver We use the RECONCILE coreference resolution platform (Stoyanov et al., 2009) to configure a coreference resolver that performs comparably to state-of-the-art systems (when evaluated on the MUC and ACE data sets under comparable assumptions). This system is a classification-based coreference resolver, modeled after the systems of Ng and Cardie (2002b) and Bengtson and Roth (2008). First it classifies pairs of CEs as coreferent or not coreferent, pairing each identified CE with all preceding CEs. The CEs are then clustered into coreference chains2 based on the pairwise decisions. RECONCILE has a pipeline architecture with four main steps: preprocessing, feature extraction, classification, and clustering. We will refer to the specific configuration of RECONCILE used for this paper as RECONCILEACL09. Preprocessing. The RECONCILEACL09 preprocessor applies a series of language analysis tools (mostly publicly available software packages) to th</context>
<context position="22126" citStr="Ng and Cardie (2002" startWordPosition="3517" endWordPosition="3520">iple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al. (2004), McCallum and Wellner (2004)). We argue that providing a resolver with the annotated CEs is a rather unrealistic evaluation: determining whether an NP is part of an annotated coreference chain is precisely the job of a coreference resolver! Conclusion #3: Assuming the availability of CEs unrealistically simplifies the coreference resolution task. 3.5 Anaphoricity Determination Finally, several coreference systems have successfully incorporated anaphoricity determination 660 modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)). The goal of the module is to determine whether or not an NP is anaphoric. For example, pleonastic pronouns (e.g. it is raining) are special cases that do not require coreference resolution. Unfortunately, neither the MUC nor the ACE data sets include anaphoricity information for all NPs. Rather, they encode anaphoricity information implicitly for annotated CEs: a CE is considered anaphoric if is not a singleton.6 To study the utility of anaphoricity information, we train and test only on the “anaphoric” extracted CEs, i.e. the extracted CEs that have an annotate</context>
<context position="34168" citStr="Ng and Cardie, 2002" startWordPosition="5622" endWordPosition="5625">e OPTIONAL field of the key, employed by the official MUC scorer. 5 Related Work The bulk of the relevant related work is described in earlier sections, as appropriate. This paper studies complexity issues for NP coreference resolution using a “good”, i.e. near state-of-the-art, system. For state-of-the-art performance on the MUC data sets see, e.g. Yang et al. (2003); for state-ofthe-art performance on the ACE data sets see, e.g. Bengtson and Roth (2008) and Luo (2007). While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (2002) and Byron (2001)). However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusi</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002a. Identifying Anaphoric and Non-Anaphoric Noun Phrases to Improve Coreference Resolution. In Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving Machine Learning Approaches to Coreference Resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. NIST. 2004. The ACE Evaluation Plan.</booktitle>
<contexts>
<context position="9923" citStr="Ng and Cardie (2002" startWordPosition="1541" endWordPosition="1544"> recognition, anaphoricity determination, and coreference element detection — in the performance of an end-to-end coreference resolution system. First, however, we describe the coreference resolver that we use for our study. 3.1 The RECONCILEACL09 Coreference Resolver We use the RECONCILE coreference resolution platform (Stoyanov et al., 2009) to configure a coreference resolver that performs comparably to state-of-the-art systems (when evaluated on the MUC and ACE data sets under comparable assumptions). This system is a classification-based coreference resolver, modeled after the systems of Ng and Cardie (2002b) and Bengtson and Roth (2008). First it classifies pairs of CEs as coreferent or not coreferent, pairing each identified CE with all preceding CEs. The CEs are then clustered into coreference chains2 based on the pairwise decisions. RECONCILE has a pipeline architecture with four main steps: preprocessing, feature extraction, classification, and clustering. We will refer to the specific configuration of RECONCILE used for this paper as RECONCILEACL09. Preprocessing. The RECONCILEACL09 preprocessor applies a series of language analysis tools (mostly publicly available software packages) to th</context>
<context position="22126" citStr="Ng and Cardie (2002" startWordPosition="3517" endWordPosition="3520">iple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al. (2004), McCallum and Wellner (2004)). We argue that providing a resolver with the annotated CEs is a rather unrealistic evaluation: determining whether an NP is part of an annotated coreference chain is precisely the job of a coreference resolver! Conclusion #3: Assuming the availability of CEs unrealistically simplifies the coreference resolution task. 3.5 Anaphoricity Determination Finally, several coreference systems have successfully incorporated anaphoricity determination 660 modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)). The goal of the module is to determine whether or not an NP is anaphoric. For example, pleonastic pronouns (e.g. it is raining) are special cases that do not require coreference resolution. Unfortunately, neither the MUC nor the ACE data sets include anaphoricity information for all NPs. Rather, they encode anaphoricity information implicitly for annotated CEs: a CE is considered anaphoric if is not a singleton.6 To study the utility of anaphoricity information, we train and test only on the “anaphoric” extracted CEs, i.e. the extracted CEs that have an annotate</context>
<context position="34168" citStr="Ng and Cardie, 2002" startWordPosition="5622" endWordPosition="5625">e OPTIONAL field of the key, employed by the official MUC scorer. 5 Related Work The bulk of the relevant related work is described in earlier sections, as appropriate. This paper studies complexity issues for NP coreference resolution using a “good”, i.e. near state-of-the-art, system. For state-of-the-art performance on the MUC data sets see, e.g. Yang et al. (2003); for state-ofthe-art performance on the ACE data sets see, e.g. Bengtson and Roth (2008) and Luo (2007). While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (2002) and Byron (2001)). However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusi</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002b. Improving Machine Learning Approaches to Coreference Resolution. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. NIST. 2004. The ACE Evaluation Plan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL</booktitle>
<contexts>
<context position="10696" citStr="Petrov and Klein, 2007" startWordPosition="1651" endWordPosition="1654">The CEs are then clustered into coreference chains2 based on the pairwise decisions. RECONCILE has a pipeline architecture with four main steps: preprocessing, feature extraction, classification, and clustering. We will refer to the specific configuration of RECONCILE used for this paper as RECONCILEACL09. Preprocessing. The RECONCILEACL09 preprocessor applies a series of language analysis tools (mostly publicly available software packages) to the source texts. The OpenNLP toolkit (Baldridge, J., 2005) performs tokenization, sentence splitting, and part-of-speech tagging. The Berkeley parser (Petrov and Klein, 2007) generates phrase structure parse trees, and the de Marneffe et al. (2006) system produces dependency relations. We employ the Stanford CRF-based Named Entity Recognizer (Finkel et al., 2004) for named entity tagging. With these preprocessing components, RECONCILEACL09 uses heuristics to correctly extract approximately 90% of the annotated CEs for the MUC and ACE data sets. Feature Set. To achieve roughly state-of-theart performance, RECONCILEACL09 employs a 2A coreference chain refers to the set of CEs that refer to a particular entity. dataset docs CEs chains CEs/ch tr/tst split MUC6 60 4232</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved Inference for Unlexicalized Parsing. In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Soon</author>
<author>H Ng</author>
<author>D Lim</author>
</authors>
<title>A Machine Learning Approach to Coreference of Noun Phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="2438" citStr="Soon et al. (2001)" startWordPosition="361" endWordPosition="364">relative terms. For example, the 91.5 F-measure reported by McCallum and Wellner (2004) was produced by a system using perfect information for several linguistic subproblems. In contrast, the 71.3 F-measure reported by Yang et al. (2003) represents a fully automatic end-to-end resolver. It is impossible to assess which approach truly performs best because of the dramatically different assumptions of each evaluation. Results vary widely across data sets. Coreference resolution scores range from 85-90% on the ACE 2004 and 2005 data sets to a much lower 60- 70% on the MUC 6 and 7 data sets (e.g. Soon et al. (2001) and Yang et al. (2003)). What accounts for these differences? Are they due to properties of the documents or domains? Or do differences in the coreference task definitions account for the differences in performance? Given a new text collection and domain, what level of performance should we expect? We have little understanding of which aspects of the coreference resolution problem are handled well or poorly by state-of-the-art systems. Except for some fairly general statements, for example that proper names are easier to resolve than pronouns, which are easier than common nouns, there has bee</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. Soon, H. Ng, and D. Lim. 2001. A Machine Learning Approach to Coreference of Noun Phrases. Computational Linguistics, 27(4):521–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Nathan Gilbert</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>David Buttler</author>
<author>David Hysom</author>
</authors>
<date>2009</date>
<journal>Reconcile: A Coreference Resolution Research Platform. Computer Science</journal>
<tech>Technical Report,</tech>
<institution>Cornell University,</institution>
<location>Ithaca, NY.</location>
<contexts>
<context position="9649" citStr="Stoyanov et al., 2009" startWordPosition="1501" endWordPosition="1504">olution is a complex task that requires solving numerous non-trivial subtasks such as syntactic analysis, semantic class tagging, pleonastic pronoun identification and antecedent identification to name a few. This section examines the role of three such subtasks — named entity recognition, anaphoricity determination, and coreference element detection — in the performance of an end-to-end coreference resolution system. First, however, we describe the coreference resolver that we use for our study. 3.1 The RECONCILEACL09 Coreference Resolver We use the RECONCILE coreference resolution platform (Stoyanov et al., 2009) to configure a coreference resolver that performs comparably to state-of-the-art systems (when evaluated on the MUC and ACE data sets under comparable assumptions). This system is a classification-based coreference resolver, modeled after the systems of Ng and Cardie (2002b) and Bengtson and Roth (2008). First it classifies pairs of CEs as coreferent or not coreferent, pairing each identified CE with all preceding CEs. The CEs are then clustered into coreference chains2 based on the pairwise decisions. RECONCILE has a pipeline architecture with four main steps: preprocessing, feature extracti</context>
<context position="11922" citStr="Stoyanov et al. (2009)" startWordPosition="1849" endWordPosition="1852"> 4.4 30/30 (st) MUC7 50 4297 1081 3.9 30/20 (st) ACE-2 159 2630 1148 2.3 130/29 (st) ACE03 105 3106 1340 2.3 74/31 ACE04 128 3037 1332 2.3 90/38 ACE05 81 1991 775 2.6 57/24 Table 2: Dataset characteristics including the number of documents, annotated CEs, coreference chains, annotated CEs per chain (average), and number of documents in the train/test split. We use st to indicate a standard train/test split. fairly comprehensive set of 61 features introduced in previous coreference resolution systems (see Bengtson and Roth (2008)). We briefly summarize the features here and refer the reader to Stoyanov et al. (2009) for more details. Lexical (9): String-based comparisons of the two CEs, such as exact string matching and head noun matching. Proximity (5): Sentence and paragraph-based measures of the distance between two CEs. Grammatical (28): A wide variety of syntactic properties of the CEs, either individually or as a pair. These features are based on part-of-speech tags, parse trees, or dependency relations. For example: one feature indicates whether both CEs are syntactic subjects; another indicates whether the CEs are in an appositive construction. Semantic (19): Capture semantic information about on</context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, Buttler, Hysom, 2009</marker>
<rawString>Veselin Stoyanov, Nathan Gilbert, Claire Cardie, Ellen Riloff, David Buttler, and David Hysom. 2009. Reconcile: A Coreference Resolution Research Platform. Computer Science Technical Report, Cornell University, Ithaca, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
<author>Rodger Kibble</author>
</authors>
<date>2000</date>
<booktitle>On Coreferring: Coreference in MUC and Related Annotation Schemes. Computational Linguistics,</booktitle>
<pages>26--4</pages>
<marker>van Deemter, Kibble, 2000</marker>
<rawString>Kees van Deemter and Rodger Kibble. 2000. On Coreferring: Coreference in MUC and Related Annotation Schemes. Computational Linguistics, 26(4):629–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A Model-Theoretic Coreference Scoring Theme.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6).</booktitle>
<contexts>
<context position="13575" citStr="Vilain et al., 1995" startWordPosition="2105" endWordPosition="2108">CE corpora. For ACE, we use only the newswire portion because it is closest in composition to the MUC corpora. Statistics for each of the data sets are shown in Table 2. When available, we use the standard test/train split. Otherwise, we randomly split the data into a training and test set following a 70/30 ratio. 3In trial runs, we investigated alternative classification and clustering models (e.g. C4.5 decision trees and SVMs; best-first clustering). The results were comparable. 658 Scoring Algorithms. We evaluate using two common scoring algorithms4 — MUC and B3. The MUC scoring algorithm (Vilain et al., 1995) computes the F1 score (harmonic mean) of precision and recall based on the identifcation of unique coreference links. We use the official MUC scorer implementation for the two MUC corpora and an equivalent implementation for ACE. The B3 algorithm (Bagga and Baldwin, 1998) computes a precision and recall score for each CE: precision(ce) = |R, ∩ K,|/|R,| recall(ce) = |R, ∩ K,|/|K�e|, where RCe is the coreference chain to which ce is assigned in the response (i.e. the system-generated output) and KCe is the coreference chain that contains ce in the key (i.e. the gold standard). Precision and rec</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A Model-Theoretic Coreference Scoring Theme. In Proceedings of the Sixth Message Understanding Conference (MUC-6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Coreference Resolution Using Competition Learning Approach.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>176--183</pages>
<contexts>
<context position="2057" citStr="Yang et al. (2003)" startWordPosition="296" endWordPosition="299">ra. In spite of the availability of several benchmark data sets (e.g. MUC-6 (1995), ACE NIST (2004)) and their use in many formal evaluations, as a field we can make surprisingly few conclusive statements about the state-of-theart in NP coreference resolution. In particular, it remains difficult to assess the effectiveness of different coreference resolution approaches, even in relative terms. For example, the 91.5 F-measure reported by McCallum and Wellner (2004) was produced by a system using perfect information for several linguistic subproblems. In contrast, the 71.3 F-measure reported by Yang et al. (2003) represents a fully automatic end-to-end resolver. It is impossible to assess which approach truly performs best because of the dramatically different assumptions of each evaluation. Results vary widely across data sets. Coreference resolution scores range from 85-90% on the ACE 2004 and 2005 data sets to a much lower 60- 70% on the MUC 6 and 7 data sets (e.g. Soon et al. (2001) and Yang et al. (2003)). What accounts for these differences? Are they due to properties of the documents or domains? Or do differences in the coreference task definitions account for the differences in performance? Gi</context>
<context position="21495" citStr="Yang et al., 2003" startWordPosition="3423" endWordPosition="3426"> CEs leads to (often substantial) improvements in performance on all metrics over all data sets, especially when measured using the MUC score. Conclusion #2: Improving the ability of coreference resolvers to identify coreference elements would likely improve the state-of-the-art immensely — by 10-20 points in MUC F1 score and from 2-12 F1 points for B3. This finding explains previously published results that exhibit striking variability when run with annotated CEs vs. system-extracted CEs. On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al., 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al. (2004), McCallum and Wellner (2004)). We argue that providing a resolver with the annotated CEs is a rather unrealistic evaluation: determining whether an NP is part of an annotated coreference chain is precisely the job of a coreference resolver! Conclusion #3: Assuming the availability of CEs unrealistically simplifies the coreference resolution task. 3.5 Anaphoricity Determination Finally, several coreference systems have successfully incorporated anaphoricity determination 660 mo</context>
<context position="33919" citStr="Yang et al. (2003)" startWordPosition="5582" endWordPosition="5585"> resolution classes found in the data set. Our CPP measure can be used to produce a good estimate of the level of performance on a new corpus. 8Observed scores for MUC6 and 7 differ slightly from Table 3 because this part of the work did not use the OPTIONAL field of the key, employed by the official MUC scorer. 5 Related Work The bulk of the relevant related work is described in earlier sections, as appropriate. This paper studies complexity issues for NP coreference resolution using a “good”, i.e. near state-of-the-art, system. For state-of-the-art performance on the MUC data sets see, e.g. Yang et al. (2003); for state-ofthe-art performance on the ACE data sets see, e.g. Bengtson and Roth (2008) and Luo (2007). While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitk</context>
</contexts>
<marker>Yang, Zhou, Su, Tan, 2003</marker>
<rawString>Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim Tan. 2003. Coreference Resolution Using Competition Learning Approach. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 176–183.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>