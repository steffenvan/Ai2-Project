<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000116">
<title confidence="0.9939055">
Learning to Rank Definitions to Generate Quizzes for Interactive
Information Presentation
</title>
<author confidence="0.745842">
Ryuichiro Higashinaka and Kohji Dohsaka and Hideki Isozaki
</author>
<affiliation confidence="0.659517">
NTT Communication Science Laboratories, NTT Corporation
</affiliation>
<address confidence="0.828574">
2-4, Hikaridai, Seika-cho, Kyoto 619-0237, Japan
</address>
<email confidence="0.995261">
{rh,dohsaka,isozaki}@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.993818" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99989975">
This paper proposes the idea of ranking def-
initions of a person (a set of biographi-
cal facts) to automatically generate “Who
is this?” quizzes. The definitions are or-
dered according to how difficult they make
it to name the person. Such ranking would
enable users to interactively learn about a
person through dialogue with a system with
improved understanding and lasting motiva-
tion, which is useful for educational sys-
tems. In our approach, we train a ranker
that learns from data the appropriate ranking
of definitions based on features that encode
the importance of keywords in a definition
as well as its content. Experimental results
show that our approach is significantly better
in ranking definitions than baselines that use
conventional information retrieval measures
such as tf*idf and pointwise mutual informa-
tion (PMI).
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999917442307693">
Appropriate ranking of sentences is important, as
noted in sentence ordering tasks (Lapata, 2003), in
effectively delivering content. Whether the task is
to convey news texts or definitions, the objective is
to make it easier for users to understand the content.
However, just conveying it in an encyclopedia-like
or temporal order may not be the best solution, con-
sidering that interaction between a system and a user
improves understanding (Sugiyama et al., 1999) and
that the cognitive load in receiving information is be-
lieved to correlate with memory fixation (Craik and
Lockhart, 1972).
In this paper, we discuss the idea of ranking defi-
nitions as a way to present people’s biographical in-
formation to users, and propose ranking definitions
to automatically generate a “Who is this?” quiz.
Here, we use the term ‘definitions of a person’ to
mean a short series of biographical facts (See Fig. 1).
The definitions are ordered according to how diffi-
cult they make it to name the person. The ranking
also enables users to easily come up with answer
candidates. The definitions are presented to users
one by one as hints until users give the correct name
(See Fig. 2). Although the interaction would take
time, we could expect improved understanding of
people’s biographical information by users through
their deliberation and the long lasting motivation af-
forded by the entertaining nature of quizzes, which
is important in tutorial tasks (Baylor and Ryu, 2003).
Previous work on definition ranking has used
measures such as tf*idf (Xu et al., 2004) or ranking
models trained to encode the likelihood of a defini-
tion being good (Xu et al., 2005). However, such
measures/models may not be suitable for quiz-style
ranking. For example, a definition having a strong
co-occurrence with a person may not be an easy hint
when it is about a very minor detail. Certain de-
scriptions, such as a person’s birthplace, would have
to come early so that users can easily start guessing
who the person is. In our approach, we train a ranker
that learns from data the appropriate ranking of def-
initions. Note that we only focus on the ranking of
definitions and not on the interaction with users in
this paper. We also assume that the definitions to be
ranked are given.
Section 2 describes the task of ranking definitions,
and Section 3 describes our approach. Section 4 de-
scribes our collection of ranking data and the rank-
ing model training using the ranking support vector
machine (SVM), and Section 5 presents the evalu-
ation results. Section 6 summarizes and mentions
future work.
</bodyText>
<sectionHeader confidence="0.962316" genericHeader="method">
2 Ranking Definitions for Quizzes
</sectionHeader>
<bodyText confidence="0.999958111111111">
Figure 1 shows a list of definitions of Natsume
Soseki, a famous Japanese novelist, in their original
ranking at the encyclopedic website goo (http://dic-
tionary.goo.ne.jp/) and in the quiz-style ranking we
aim to achieve. Such a ranking would realize a dia-
logue like that in Fig. 2. At the end of the dialogue,
the user would be able to associate the person and
the definitions better, and it is expected that some
new facts could be learned about that person.
</bodyText>
<page confidence="0.906195">
117
</page>
<bodyText confidence="0.7021255">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 117–120,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<listItem confidence="0.97364375">
Original Ranking: � �
1. Novelist and scholar of British literature. S1 Who is this? First hint: Graduated from the
2. Real name: Kinnosuke. University of Tokyo.
3. Born in Ushigome, Edo. U1 Yoshida Shigeru?
4. Graduated from the University of Tokyo. S2 No, not even close! Second hint: Born in
5. Master of early-modern literature along with Mori Ogai. Ushigome, Edo.
6. After the success of “I Am a Cat”, quit all teaching jobs and joined U2 I don’t know.
Asahi Shimbun. S3 OK. Third hint: Novelist and scholar of
7. Published masterpieces in Asahi Shimbun. British literature.
8. Familiar with Haiku, Chinese poetry, and calligraphy. U3 Murakami Haruki?
9. Works include “Botchan”, “Sanshiro”, etc. S4 Close! Fourth hint: Familiar with Haiku,
⇓ Chinese poetry, and calligraphy.
</listItem>
<table confidence="0.801246">
U4 Mori Ogai?
S5 Very close! Fifth hint: Published master-
pieces in Asahi Shimbun.
U5 Natsume Soseki?
S6 That’s right!
� �
Figure 2: Example dialogue based on the quiz-style
ranking of definitions. S stands for a system utter-
ance and U for a user utterance.
3 Approach
Since it is difficult to know in advance what char-
acteristics are important for quiz-style ranking, we
</table>
<bodyText confidence="0.996293">
learn the appropriate ranking of definitions from
data. The approach is the same as that of (Xu et al.,
2005) in that we adopt a machine learning approach
for definition ranking, but is different in that what is
learned is a quiz-style ranking of sentences that are
already known to be good definitions.
First, we collect ranking data. For this purpose,
we turn to existing encyclopedias for concise biogra-
phies. Then, we annotate the ranking. Secondly, we
devise a set of features for a definition. Since the
existence of keywords that have high scores in IR-
related measures may suggest easy hints, we incor-
porate the scores of IR-related measures as features
(IR-related features).
Certain words tend to appear before or after oth-
ers in a biographical document to convey particular
information about people (e.g., words describing oc-
cupations at the beginning; those describing works
at the end, etc.) Therefore, we use word positions
within the biography of the person in question as
features (positional features). Biographies can be
found in online resources, such as biography.com
(http://www.biography.com/) and Wikipedia. In ad-
dition, to focus on the particular content of the def-
inition, we use bag-of-words (BOW) features, to-
gether with semantic features (e.g., semantic cate-
gories in Nihongo Goi-Taikei (Ikehara et al., 1997)
or word senses in WordNet) to complement the
sparseness of BOW features. We describe the fea-
tures we created in Section 4.2. Finally, we create
a ranking model using a preference learning algo-
</bodyText>
<listItem confidence="0.997137888888889">
Quiz-style Ranking:
1. Graduated from the University of Tokyo.
2. Born in Ushigome, Edo.
3. Novelist and scholar of British literature.
4. Familiar with Haiku, Chinese poetry, and calligraphy.
5. Published masterpieces in Asahi Shimbun.
6. Real name: Kinnosuke.
7. Master of early-modern literature along with Mori Ogai.
8. After the success of “I Am a Cat”, quit all teaching jobs and joined
</listItem>
<figureCaption confidence="0.6964964">
Asahi Shimbun.
9. Works include “Botchan”, “Sanshiro”, etc.
Figure 1: List of definitions of Natsume Soseki, a
famous Japanese novelist, in their original ranking in
the encyclopedia and in the quiz-style ranking. The
</figureCaption>
<bodyText confidence="0.835907707317073">
definitions were translated by the authors.
Ranking definitions is closely related to defini-
tional question answering and sentence ordering
in multi-document summarization. In definitional
question answering, measures related to information
retrieval (IR), such as tf*idf or pointwise mutual in-
formation (PMI), have been used to rank sentences
or information nuggets (Xu et al., 2004; Sun et al.,
2005). Such measures are used under the assump-
tion that outstanding/co-occurring keywords about a
definiendum characterize that definiendum. How-
ever, this assumption may not be appropriate in quiz-
style ranking; most content words in the definitions
are already important in the IR sense, and strong co-
occurrence may not guarantee high ranks for hints
to be presented later because the hint can be too spe-
cific. An approach to creating a ranking model of
definitions in a supervised manner using machine
learning techniques has been reported (Xu et al.,
2005). However, the model is only used to distin-
guish definitions from non-definitions on the basis
of features related mainly to linguistic styles.
In multi-document summarization, the focus has
been mainly on creating cohesive texts. (Lapata,
2003) uses the probability of words in adjacent sen-
tences as constraints to maximize the coherence of
all sentence-pairs in texts. Although we acknowl-
edge that having cohesive definitions is important,
since we are not creating a single text and the dia-
logue that we aim to achieve would involve frequent
user/system interaction (Fig. 2), we do not deal with
the coherence of definitions in this paper.
118
rithm, such as the ranking SVM (Joachims, 2002), in question, we calculated (1)–(4) within the entry,
which learns ranking by reducing the pairwise rank- and calculated tf*idf scores of words in the defini-
ing error. tion using the term frequency in the entry. Again, by
4 Experiment taking the minimum, maximum, and mean values of
4.1 Data Collection (1)–(4) and tf*idf, we yielded 15 (5 x 3) features,
We collected biographies (in Japanese) from the goo for a total of 90 (75 + 15) IR-related features.
encyclopedia. We first mined Wikipedia to calcu- Positional features were derived also using the
late the PageRankTMof people using the hyper-link Wikipedia entry. For each word in the definition, we
structure. After sorting them in descending order by calculated (a) the number of times the word appears
the PageRank score, we extracted the top-150 peo- in the entry, (b) the minimum position of the word in
ple for whom we could find an entry in the goo en- the entry, (c) its maximum position, (d) its mean po-
cyclopedia. Then, 11 annotators annotated rankings sition, and (e) the standard deviation of the positions.
for each of the 150 people individually. The annota- Note that positions are either ordinal or relative; i.e.,
tors were instructed to rank the definitions assuming the relative position is calculated by dividing the or-
that they were creating a “who is this?” quiz; i.e., dinal position by the total number of words in the
to place the definition that is the most characteris- entry. Then, we took the minimum, maximum, and
tic of the person in question at the end. The mean mean values of (a)–(e) for all content words in the
of the Kendall’s coefficients of concordance for the definition as features, deriving 30 (5 x 2 (ordinal or
150 people was sufficiently high at 0.76 with a stan- relative positions) x 3) features.
dard deviation of 0.13. Finally, taking the means of For the BOW features, we first parsed all our
ranks given to each definition, we merged the indi- definitions with CaboCha (a Japanese morphologi-
vidual rankings to create the reference rankings. An cal/dependency parser, http://chasen.org/˜taku/soft-
example of a reference ranking is the bottom one in ware/cabocha/) and extracted all content words to
Fig. 1. There are 958 definition sentences in all, with make binary features representing the existence of
each person having approximately 6–7 definitions. each content word. There are 2,156 BOW features
4.2 Deriving Features in our data.
We derived our IR-related features based on As for the semantic features, we used the seman-
Mainichi newspaper articles (1991–2004) and tic categories in Nihongo Goi-Taikei. Since there are
Wikipedia articles. We used these two different 2,715 semantic categories, we created 2,715 features
sources to take into account the difference in the representing the existence of each semantic category
importance of terms depending on the text. We in the definition. Semantic categories were assigned
also used sentences, sections (for Wikipedia arti- to words in the definition by a morphological ana-
cles only) and documents as units to calculate doc- lyzer that comes with ALT/J-E, a Japanese-English
ument frequency, which resulted in the creation of machine translation system (Ikehara et al., 1991).
five frequency tables: (i) Mainichi-Document, (ii) In total, we have 4,991 features to represent each
Mainichi-Sentence, (iii) Wikipedia-Document, (iv) definition. We calculated all feature values for all
Wikipedia-Section, and (v) Wikipedia-Sentence. definitions in our data to be used for the learning.
Using the five frequency tables, we calculated, for 4.3 Training Ranking Models
each content word (nouns, verbs, adjectives, and un- Using the reference ranking data, we trained a rank-
known words) in the definition, (1) frequency (the ing model using the ranking SVM (Joachims, 2002)
number of documents where the word is found), (2) (with a linear kernel) that minimizes the pairwise
relative frequency (frequency divided by the maxi- ranking error among the definitions of each person.
mum number of documents), (3) co-occurrence fre- 5 Evaluation
quency (the number of documents where both the To evaluate the performance of the ranking model,
word and the person’s name are found), (4) rela- following (Xu et al., 2004; Sun et al., 2005), we
tive co-occurrence frequency, and (5) PMI. Then, we compared it with baselines that use only the scores
took the minimum, maximum, and mean values of of IR-related and positional features for ranking, i.e.,
(1)–(5) for all content words in the definition as fea- sorting. Table 1 shows the performance of the rank-
tures, deriving 75 (5 x 5 x 3) features. Then, using ing model (by the leave-one-out method, predicting
</bodyText>
<table confidence="0.990644857142857">
the Wikipedia article (called an entry) for the person the ranking of definitions of a person by other peo-
119
Rank Description Ranking Error Rank Feature Name Weight
1 Proposed ranking model 0.185 1 Wikipedia-Sentence-PMI-max 0.723
2 Wikipedia-Sentence-PMI-max 0.299 2 SemCat:33 (others/someone) -0.559
3 Wikipedia-Section-PMI-max 0.309 3 SemCat:186 (creator) 0.485
4 Wikipedia-Document-PMI-max 0.312 4 BOW:bakufu (feudal government) 0.451
5 Mainichi-Sentence-PMI-max 0.318 5 SemCat:163 (sovereign/ruler/monarch) 0.422
6 Mainichi-Document-PMI-max 0.325 6 Wikipedia-Document-PMI-max 0.409
7 Mainichi-Sentence-relative-co-occurrence-max 0.338 7 SemCat:2391 (birth) -0.404
8 Wikipedia-Entry-ordinal-Min-max 0.338 8 Wikipedia-Section-PMI-max 0.402
9 Wikipedia-Sentence-relative-co-occurrence-max 0.339 9 SemCat:2595 (unit; e.g., numeral classifier) 0.374
10 Wikipedia-Entry-relative-Min-max 0.340 10 SemCat:2606 (plural; e.g., plural form) -0.368
11 Wikipedia-Entry-ordinal-Mean-mean 0.342
</table>
<tableCaption confidence="0.7650855">
Table 1: Performance of the proposed ranking model
and that of 10 best-performing baselines.
</tableCaption>
<bodyText confidence="0.99985465625">
ple’s rankings) and that of the 10 best-performing
baselines. The ranking error is pairwise ranking er-
ror; i.e., the rate of misordered pairs. A descrip-
tive name is given for each baseline. For example,
Wikipedia-Sentence-PMI-max means that we used
the maximum PMI values of content words in the
definition calculated from Wikipedia, with sentence
as the unit for obtaining frequencies.
Our ranking model outperforms all of the base-
lines. McNemar’s test showed that the difference be-
tween the proposed model and the best-performing
baseline is significant (p&lt;0.00001). The results also
show that PMI is more effective in quiz-style rank-
ing than any other measure. The fact that max is im-
portant probably means that the mere existence of a
word that has a high PMI score is enough to raise the
ranking of a hint. It is also interesting that Wikipedia
gives better ranking, which is probably because peo-
ple’s names and related keywords are close to each
other in such descriptive texts.
Analyzing the ranking model trained by the rank-
ing SVM allows us to calculate the weights given to
the features (Hirao et al., 2002). Table 2 shows the
top-10 features in weights in absolute figures when
all samples were used for training. It can be seen
that high PMI values and words/semantic categories
related to government or creation lead to easy hints,
whereas semantic categories, such as birth and oth-
ers (corresponding to the person in ‘a person from
Tokyo’), lead to early hints. This supports our in-
tuitive notion that birthplaces should be presented
early for users to start thinking about a person.
</bodyText>
<sectionHeader confidence="0.996722" genericHeader="conclusions">
6 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.985952421052632">
This paper proposed ranking definitions of a person
to automatically generate a “Who is this?” quiz.
Using reference ranking data that we created man-
ually, we trained a ranking model using a ranking
SVM based on features that encode the importance
of keywords in a definition as well as its content.
Table 2: Weights of features learned for ranking def-
initions by the ranking SVM. SemCat denotes it is
a semantic-category feature with its semantic cate-
gory ID followed by the description of the category
in parentheses. BOW denotes a BOW feature.
Experimental results show that our ranking model
significantly outperforms baselines that use single
IR-related and positional measures for ranking. We
are currently in the process of building a dialogue
system that uses the quiz-style ranking for definition
presentation. We are planning to examine how the
different rankings affect the understanding and mo-
tivation of users.
</bodyText>
<sectionHeader confidence="0.999053" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999661142857143">
Amy Baylor and Jeeheon Ryu. 2003. Does the presence of
image and animation enhance pedagogical agent persona?
Journal of Educational Computing Research, 28(4):373–
395.
Fergus I. M. Craik and Robert S. Lockhart. 1972. Levels of
processing: A framework for memory research. Journal of
Verbal Learning and Verbal Behavior, 11:671–684.
Tsutomu Hirao, Hideki Isozaki, Eisaku Maeda, and Yuji Mat-
sumoto. 2002. Extracting important sentences with support
vector machines. In Proc. 19th COLING, pages 342–348.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-editing
–Effects of new methods in ALT-J/E–. In Proc. Third Ma-
chine Translation Summit: MT Summit III, pages 101–106.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei – A
Japanese Lexicon. Iwanami Shoten.
Thorsten Joachims. 2002. Optimizing search engines using
clickthrough data. In Proc. KDD, pages 133–142.
Mirella Lapata. 2003. Probabilistic text structuring: Exper-
iments with sentence ordering. In Proc. 41st ACL, pages
545–552.
Akira Sugiyama, Kohji Dohsaka, and Takeshi Kawabata. 1999.
A method for conveying the contents of written texts by spo-
ken dialogue. In Proc. PACLING, pages 54–66.
Renxu Sun, Jing Jiang, Yee Fan Tan, Hang Cui, Tat-Seng Chua,
and Min-Yen Kan. 2005. Using syntactic and semantic rela-
tion analysis in question answering. In Proc. TREC.
Jinxi Xu, Ralph Weischedel, and Ana Licuanan. 2004. Eval-
uation of an extraction-based approach to answering defini-
tional questions. In Proc. SIGIR, pages 418–424.
Jun Xu, Yunbo Cao, Hang Li, and Min Zhao. 2005. Rank-
ing definitions with supervised learning methods. In Proc.
WWW, pages 811–819.
</reference>
<page confidence="0.996283">
120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.631508">
<title confidence="0.999683">Learning to Rank Definitions to Generate Quizzes for Interactive Information Presentation</title>
<author confidence="0.954953">Higashinaka Dohsaka Isozaki</author>
<affiliation confidence="0.99866">NTT Communication Science Laboratories, NTT Corporation</affiliation>
<address confidence="0.952371">2-4, Hikaridai, Seika-cho, Kyoto 619-0237, Japan</address>
<abstract confidence="0.985378571428571">This paper proposes the idea of ranking definitions of a person (a set of biographical facts) to automatically generate “Who is this?” quizzes. The definitions are ordered according to how difficult they make it to name the person. Such ranking would enable users to interactively learn about a person through dialogue with a system with improved understanding and lasting motivation, which is useful for educational systems. In our approach, we train a ranker that learns from data the appropriate ranking of definitions based on features that encode the importance of keywords in a definition as well as its content. Experimental results show that our approach is significantly better in ranking definitions than baselines that use conventional information retrieval measures such as tf*idf and pointwise mutual information (PMI).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amy Baylor</author>
<author>Jeeheon Ryu</author>
</authors>
<title>Does the presence of image and animation enhance pedagogical agent persona?</title>
<date>2003</date>
<journal>Journal of Educational Computing Research,</journal>
<volume>28</volume>
<issue>4</issue>
<pages>395</pages>
<contexts>
<context position="2617" citStr="Baylor and Ryu, 2003" startWordPosition="402" endWordPosition="405"> to mean a short series of biographical facts (See Fig. 1). The definitions are ordered according to how difficult they make it to name the person. The ranking also enables users to easily come up with answer candidates. The definitions are presented to users one by one as hints until users give the correct name (See Fig. 2). Although the interaction would take time, we could expect improved understanding of people’s biographical information by users through their deliberation and the long lasting motivation afforded by the entertaining nature of quizzes, which is important in tutorial tasks (Baylor and Ryu, 2003). Previous work on definition ranking has used measures such as tf*idf (Xu et al., 2004) or ranking models trained to encode the likelihood of a definition being good (Xu et al., 2005). However, such measures/models may not be suitable for quiz-style ranking. For example, a definition having a strong co-occurrence with a person may not be an easy hint when it is about a very minor detail. Certain descriptions, such as a person’s birthplace, would have to come early so that users can easily start guessing who the person is. In our approach, we train a ranker that learns from data the appropriat</context>
</contexts>
<marker>Baylor, Ryu, 2003</marker>
<rawString>Amy Baylor and Jeeheon Ryu. 2003. Does the presence of image and animation enhance pedagogical agent persona? Journal of Educational Computing Research, 28(4):373– 395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fergus I M Craik</author>
<author>Robert S Lockhart</author>
</authors>
<title>Levels of processing: A framework for memory research.</title>
<date>1972</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<pages>11--671</pages>
<contexts>
<context position="1744" citStr="Craik and Lockhart, 1972" startWordPosition="257" endWordPosition="260">information (PMI). 1 Introduction Appropriate ranking of sentences is important, as noted in sentence ordering tasks (Lapata, 2003), in effectively delivering content. Whether the task is to convey news texts or definitions, the objective is to make it easier for users to understand the content. However, just conveying it in an encyclopedia-like or temporal order may not be the best solution, considering that interaction between a system and a user improves understanding (Sugiyama et al., 1999) and that the cognitive load in receiving information is believed to correlate with memory fixation (Craik and Lockhart, 1972). In this paper, we discuss the idea of ranking definitions as a way to present people’s biographical information to users, and propose ranking definitions to automatically generate a “Who is this?” quiz. Here, we use the term ‘definitions of a person’ to mean a short series of biographical facts (See Fig. 1). The definitions are ordered according to how difficult they make it to name the person. The ranking also enables users to easily come up with answer candidates. The definitions are presented to users one by one as hints until users give the correct name (See Fig. 2). Although the interac</context>
</contexts>
<marker>Craik, Lockhart, 1972</marker>
<rawString>Fergus I. M. Craik and Robert S. Lockhart. 1972. Levels of processing: A framework for memory research. Journal of Verbal Learning and Verbal Behavior, 11:671–684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsutomu Hirao</author>
<author>Hideki Isozaki</author>
<author>Eisaku Maeda</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extracting important sentences with support vector machines.</title>
<date>2002</date>
<booktitle>In Proc. 19th COLING,</booktitle>
<pages>342--348</pages>
<contexts>
<context position="16208" citStr="Hirao et al., 2002" startWordPosition="2557" endWordPosition="2560">sed model and the best-performing baseline is significant (p&lt;0.00001). The results also show that PMI is more effective in quiz-style ranking than any other measure. The fact that max is important probably means that the mere existence of a word that has a high PMI score is enough to raise the ranking of a hint. It is also interesting that Wikipedia gives better ranking, which is probably because people’s names and related keywords are close to each other in such descriptive texts. Analyzing the ranking model trained by the ranking SVM allows us to calculate the weights given to the features (Hirao et al., 2002). Table 2 shows the top-10 features in weights in absolute figures when all samples were used for training. It can be seen that high PMI values and words/semantic categories related to government or creation lead to easy hints, whereas semantic categories, such as birth and others (corresponding to the person in ‘a person from Tokyo’), lead to early hints. This supports our intuitive notion that birthplaces should be presented early for users to start thinking about a person. 6 Summary and Future Work This paper proposed ranking definitions of a person to automatically generate a “Who is this?</context>
</contexts>
<marker>Hirao, Isozaki, Maeda, Matsumoto, 2002</marker>
<rawString>Tsutomu Hirao, Hideki Isozaki, Eisaku Maeda, and Yuji Matsumoto. 2002. Extracting important sentences with support vector machines. In Proc. 19th COLING, pages 342–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Satoshi Shirai</author>
<author>Akio Yokoo</author>
<author>Hiromi Nakaiwa</author>
</authors>
<title>Toward an MT system without pre-editing –Effects of new methods in ALT-J/E–.</title>
<date>1991</date>
<booktitle>In Proc. Third Machine Translation Summit: MT Summit III,</booktitle>
<pages>101--106</pages>
<contexts>
<context position="12542" citStr="Ikehara et al., 1991" startWordPosition="2017" endWordPosition="2020">ince there are Wikipedia articles. We used these two different 2,715 semantic categories, we created 2,715 features sources to take into account the difference in the representing the existence of each semantic category importance of terms depending on the text. We in the definition. Semantic categories were assigned also used sentences, sections (for Wikipedia arti- to words in the definition by a morphological anacles only) and documents as units to calculate doc- lyzer that comes with ALT/J-E, a Japanese-English ument frequency, which resulted in the creation of machine translation system (Ikehara et al., 1991). five frequency tables: (i) Mainichi-Document, (ii) In total, we have 4,991 features to represent each Mainichi-Sentence, (iii) Wikipedia-Document, (iv) definition. We calculated all feature values for all Wikipedia-Section, and (v) Wikipedia-Sentence. definitions in our data to be used for the learning. Using the five frequency tables, we calculated, for 4.3 Training Ranking Models each content word (nouns, verbs, adjectives, and un- Using the reference ranking data, we trained a rankknown words) in the definition, (1) frequency (the ing model using the ranking SVM (Joachims, 2002) number of</context>
</contexts>
<marker>Ikehara, Shirai, Yokoo, Nakaiwa, 1991</marker>
<rawString>Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi Nakaiwa. 1991. Toward an MT system without pre-editing –Effects of new methods in ALT-J/E–. In Proc. Third Machine Translation Summit: MT Summit III, pages 101–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Masahiro Miyazaki</author>
<author>Satoshi Shirai</author>
</authors>
<title>Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi.</title>
<date>1997</date>
<booktitle>Goi-Taikei – A Japanese Lexicon. Iwanami Shoten.</booktitle>
<contexts>
<context position="6849" citStr="Ikehara et al., 1997" startWordPosition="1104" endWordPosition="1107">fore or after others in a biographical document to convey particular information about people (e.g., words describing occupations at the beginning; those describing works at the end, etc.) Therefore, we use word positions within the biography of the person in question as features (positional features). Biographies can be found in online resources, such as biography.com (http://www.biography.com/) and Wikipedia. In addition, to focus on the particular content of the definition, we use bag-of-words (BOW) features, together with semantic features (e.g., semantic categories in Nihongo Goi-Taikei (Ikehara et al., 1997) or word senses in WordNet) to complement the sparseness of BOW features. We describe the features we created in Section 4.2. Finally, we create a ranking model using a preference learning algoQuiz-style Ranking: 1. Graduated from the University of Tokyo. 2. Born in Ushigome, Edo. 3. Novelist and scholar of British literature. 4. Familiar with Haiku, Chinese poetry, and calligraphy. 5. Published masterpieces in Asahi Shimbun. 6. Real name: Kinnosuke. 7. Master of early-modern literature along with Mori Ogai. 8. After the success of “I Am a Cat”, quit all teaching jobs and joined Asahi Shimbun.</context>
</contexts>
<marker>Ikehara, Miyazaki, Shirai, 1997</marker>
<rawString>Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei – A Japanese Lexicon. Iwanami Shoten.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data. In</title>
<date>2002</date>
<booktitle>Proc. KDD,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="9302" citStr="Joachims, 2002" startWordPosition="1493" endWordPosition="1494">ns on the basis of features related mainly to linguistic styles. In multi-document summarization, the focus has been mainly on creating cohesive texts. (Lapata, 2003) uses the probability of words in adjacent sentences as constraints to maximize the coherence of all sentence-pairs in texts. Although we acknowledge that having cohesive definitions is important, since we are not creating a single text and the dialogue that we aim to achieve would involve frequent user/system interaction (Fig. 2), we do not deal with the coherence of definitions in this paper. 118 rithm, such as the ranking SVM (Joachims, 2002), in question, we calculated (1)–(4) within the entry, which learns ranking by reducing the pairwise rank- and calculated tf*idf scores of words in the definiing error. tion using the term frequency in the entry. Again, by 4 Experiment taking the minimum, maximum, and mean values of 4.1 Data Collection (1)–(4) and tf*idf, we yielded 15 (5 x 3) features, We collected biographies (in Japanese) from the goo for a total of 90 (75 + 15) IR-related features. encyclopedia. We first mined Wikipedia to calcu- Positional features were derived also using the late the PageRankTMof people using the hyper-l</context>
<context position="13132" citStr="Joachims, 2002" startWordPosition="2105" endWordPosition="2106">em (Ikehara et al., 1991). five frequency tables: (i) Mainichi-Document, (ii) In total, we have 4,991 features to represent each Mainichi-Sentence, (iii) Wikipedia-Document, (iv) definition. We calculated all feature values for all Wikipedia-Section, and (v) Wikipedia-Sentence. definitions in our data to be used for the learning. Using the five frequency tables, we calculated, for 4.3 Training Ranking Models each content word (nouns, verbs, adjectives, and un- Using the reference ranking data, we trained a rankknown words) in the definition, (1) frequency (the ing model using the ranking SVM (Joachims, 2002) number of documents where the word is found), (2) (with a linear kernel) that minimizes the pairwise relative frequency (frequency divided by the maxi- ranking error among the definitions of each person. mum number of documents), (3) co-occurrence fre- 5 Evaluation quency (the number of documents where both the To evaluate the performance of the ranking model, word and the person’s name are found), (4) rela- following (Xu et al., 2004; Sun et al., 2005), we tive co-occurrence frequency, and (5) PMI. Then, we compared it with baselines that use only the scores took the minimum, maximum, and me</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proc. KDD, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In Proc. 41st ACL,</booktitle>
<pages>545--552</pages>
<contexts>
<context position="1250" citStr="Lapata, 2003" startWordPosition="180" endWordPosition="181"> with improved understanding and lasting motivation, which is useful for educational systems. In our approach, we train a ranker that learns from data the appropriate ranking of definitions based on features that encode the importance of keywords in a definition as well as its content. Experimental results show that our approach is significantly better in ranking definitions than baselines that use conventional information retrieval measures such as tf*idf and pointwise mutual information (PMI). 1 Introduction Appropriate ranking of sentences is important, as noted in sentence ordering tasks (Lapata, 2003), in effectively delivering content. Whether the task is to convey news texts or definitions, the objective is to make it easier for users to understand the content. However, just conveying it in an encyclopedia-like or temporal order may not be the best solution, considering that interaction between a system and a user improves understanding (Sugiyama et al., 1999) and that the cognitive load in receiving information is believed to correlate with memory fixation (Craik and Lockhart, 1972). In this paper, we discuss the idea of ranking definitions as a way to present people’s biographical info</context>
<context position="8853" citStr="Lapata, 2003" startWordPosition="1418" endWordPosition="1419">n quizstyle ranking; most content words in the definitions are already important in the IR sense, and strong cooccurrence may not guarantee high ranks for hints to be presented later because the hint can be too specific. An approach to creating a ranking model of definitions in a supervised manner using machine learning techniques has been reported (Xu et al., 2005). However, the model is only used to distinguish definitions from non-definitions on the basis of features related mainly to linguistic styles. In multi-document summarization, the focus has been mainly on creating cohesive texts. (Lapata, 2003) uses the probability of words in adjacent sentences as constraints to maximize the coherence of all sentence-pairs in texts. Although we acknowledge that having cohesive definitions is important, since we are not creating a single text and the dialogue that we aim to achieve would involve frequent user/system interaction (Fig. 2), we do not deal with the coherence of definitions in this paper. 118 rithm, such as the ranking SVM (Joachims, 2002), in question, we calculated (1)–(4) within the entry, which learns ranking by reducing the pairwise rank- and calculated tf*idf scores of words in the</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proc. 41st ACL, pages 545–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akira Sugiyama</author>
<author>Kohji Dohsaka</author>
<author>Takeshi Kawabata</author>
</authors>
<title>A method for conveying the contents of written texts by spoken dialogue.</title>
<date>1999</date>
<booktitle>In Proc. PACLING,</booktitle>
<pages>54--66</pages>
<contexts>
<context position="1618" citStr="Sugiyama et al., 1999" startWordPosition="237" endWordPosition="240">anking definitions than baselines that use conventional information retrieval measures such as tf*idf and pointwise mutual information (PMI). 1 Introduction Appropriate ranking of sentences is important, as noted in sentence ordering tasks (Lapata, 2003), in effectively delivering content. Whether the task is to convey news texts or definitions, the objective is to make it easier for users to understand the content. However, just conveying it in an encyclopedia-like or temporal order may not be the best solution, considering that interaction between a system and a user improves understanding (Sugiyama et al., 1999) and that the cognitive load in receiving information is believed to correlate with memory fixation (Craik and Lockhart, 1972). In this paper, we discuss the idea of ranking definitions as a way to present people’s biographical information to users, and propose ranking definitions to automatically generate a “Who is this?” quiz. Here, we use the term ‘definitions of a person’ to mean a short series of biographical facts (See Fig. 1). The definitions are ordered according to how difficult they make it to name the person. The ranking also enables users to easily come up with answer candidates. T</context>
</contexts>
<marker>Sugiyama, Dohsaka, Kawabata, 1999</marker>
<rawString>Akira Sugiyama, Kohji Dohsaka, and Takeshi Kawabata. 1999. A method for conveying the contents of written texts by spoken dialogue. In Proc. PACLING, pages 54–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renxu Sun</author>
<author>Jing Jiang</author>
<author>Yee Fan Tan</author>
<author>Hang Cui</author>
<author>Tat-Seng Chua</author>
<author>Min-Yen Kan</author>
</authors>
<title>Using syntactic and semantic relation analysis in question answering.</title>
<date>2005</date>
<booktitle>In Proc. TREC.</booktitle>
<contexts>
<context position="8055" citStr="Sun et al., 2005" startWordPosition="1291" endWordPosition="1294">hi Shimbun. 9. Works include “Botchan”, “Sanshiro”, etc. Figure 1: List of definitions of Natsume Soseki, a famous Japanese novelist, in their original ranking in the encyclopedia and in the quiz-style ranking. The definitions were translated by the authors. Ranking definitions is closely related to definitional question answering and sentence ordering in multi-document summarization. In definitional question answering, measures related to information retrieval (IR), such as tf*idf or pointwise mutual information (PMI), have been used to rank sentences or information nuggets (Xu et al., 2004; Sun et al., 2005). Such measures are used under the assumption that outstanding/co-occurring keywords about a definiendum characterize that definiendum. However, this assumption may not be appropriate in quizstyle ranking; most content words in the definitions are already important in the IR sense, and strong cooccurrence may not guarantee high ranks for hints to be presented later because the hint can be too specific. An approach to creating a ranking model of definitions in a supervised manner using machine learning techniques has been reported (Xu et al., 2005). However, the model is only used to distinguis</context>
<context position="13590" citStr="Sun et al., 2005" startWordPosition="2178" endWordPosition="2181"> and un- Using the reference ranking data, we trained a rankknown words) in the definition, (1) frequency (the ing model using the ranking SVM (Joachims, 2002) number of documents where the word is found), (2) (with a linear kernel) that minimizes the pairwise relative frequency (frequency divided by the maxi- ranking error among the definitions of each person. mum number of documents), (3) co-occurrence fre- 5 Evaluation quency (the number of documents where both the To evaluate the performance of the ranking model, word and the person’s name are found), (4) rela- following (Xu et al., 2004; Sun et al., 2005), we tive co-occurrence frequency, and (5) PMI. Then, we compared it with baselines that use only the scores took the minimum, maximum, and mean values of of IR-related and positional features for ranking, i.e., (1)–(5) for all content words in the definition as fea- sorting. Table 1 shows the performance of the ranktures, deriving 75 (5 x 5 x 3) features. Then, using ing model (by the leave-one-out method, predicting the Wikipedia article (called an entry) for the person the ranking of definitions of a person by other peo119 Rank Description Ranking Error Rank Feature Name Weight 1 Proposed r</context>
</contexts>
<marker>Sun, Jiang, Tan, Cui, Chua, Kan, 2005</marker>
<rawString>Renxu Sun, Jing Jiang, Yee Fan Tan, Hang Cui, Tat-Seng Chua, and Min-Yen Kan. 2005. Using syntactic and semantic relation analysis in question answering. In Proc. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
<author>Ana Licuanan</author>
</authors>
<title>Evaluation of an extraction-based approach to answering definitional questions.</title>
<date>2004</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>418--424</pages>
<contexts>
<context position="2705" citStr="Xu et al., 2004" startWordPosition="417" endWordPosition="420">ding to how difficult they make it to name the person. The ranking also enables users to easily come up with answer candidates. The definitions are presented to users one by one as hints until users give the correct name (See Fig. 2). Although the interaction would take time, we could expect improved understanding of people’s biographical information by users through their deliberation and the long lasting motivation afforded by the entertaining nature of quizzes, which is important in tutorial tasks (Baylor and Ryu, 2003). Previous work on definition ranking has used measures such as tf*idf (Xu et al., 2004) or ranking models trained to encode the likelihood of a definition being good (Xu et al., 2005). However, such measures/models may not be suitable for quiz-style ranking. For example, a definition having a strong co-occurrence with a person may not be an easy hint when it is about a very minor detail. Certain descriptions, such as a person’s birthplace, would have to come early so that users can easily start guessing who the person is. In our approach, we train a ranker that learns from data the appropriate ranking of definitions. Note that we only focus on the ranking of definitions and not </context>
<context position="8036" citStr="Xu et al., 2004" startWordPosition="1287" endWordPosition="1290">bs and joined Asahi Shimbun. 9. Works include “Botchan”, “Sanshiro”, etc. Figure 1: List of definitions of Natsume Soseki, a famous Japanese novelist, in their original ranking in the encyclopedia and in the quiz-style ranking. The definitions were translated by the authors. Ranking definitions is closely related to definitional question answering and sentence ordering in multi-document summarization. In definitional question answering, measures related to information retrieval (IR), such as tf*idf or pointwise mutual information (PMI), have been used to rank sentences or information nuggets (Xu et al., 2004; Sun et al., 2005). Such measures are used under the assumption that outstanding/co-occurring keywords about a definiendum characterize that definiendum. However, this assumption may not be appropriate in quizstyle ranking; most content words in the definitions are already important in the IR sense, and strong cooccurrence may not guarantee high ranks for hints to be presented later because the hint can be too specific. An approach to creating a ranking model of definitions in a supervised manner using machine learning techniques has been reported (Xu et al., 2005). However, the model is only</context>
<context position="13571" citStr="Xu et al., 2004" startWordPosition="2174" endWordPosition="2177">erbs, adjectives, and un- Using the reference ranking data, we trained a rankknown words) in the definition, (1) frequency (the ing model using the ranking SVM (Joachims, 2002) number of documents where the word is found), (2) (with a linear kernel) that minimizes the pairwise relative frequency (frequency divided by the maxi- ranking error among the definitions of each person. mum number of documents), (3) co-occurrence fre- 5 Evaluation quency (the number of documents where both the To evaluate the performance of the ranking model, word and the person’s name are found), (4) rela- following (Xu et al., 2004; Sun et al., 2005), we tive co-occurrence frequency, and (5) PMI. Then, we compared it with baselines that use only the scores took the minimum, maximum, and mean values of of IR-related and positional features for ranking, i.e., (1)–(5) for all content words in the definition as fea- sorting. Table 1 shows the performance of the ranktures, deriving 75 (5 x 5 x 3) features. Then, using ing model (by the leave-one-out method, predicting the Wikipedia article (called an entry) for the person the ranking of definitions of a person by other peo119 Rank Description Ranking Error Rank Feature Name </context>
</contexts>
<marker>Xu, Weischedel, Licuanan, 2004</marker>
<rawString>Jinxi Xu, Ralph Weischedel, and Ana Licuanan. 2004. Evaluation of an extraction-based approach to answering definitional questions. In Proc. SIGIR, pages 418–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Xu</author>
<author>Yunbo Cao</author>
<author>Hang Li</author>
<author>Min Zhao</author>
</authors>
<title>Ranking definitions with supervised learning methods.</title>
<date>2005</date>
<booktitle>In Proc. WWW,</booktitle>
<pages>811--819</pages>
<contexts>
<context position="2801" citStr="Xu et al., 2005" startWordPosition="435" endWordPosition="438">come up with answer candidates. The definitions are presented to users one by one as hints until users give the correct name (See Fig. 2). Although the interaction would take time, we could expect improved understanding of people’s biographical information by users through their deliberation and the long lasting motivation afforded by the entertaining nature of quizzes, which is important in tutorial tasks (Baylor and Ryu, 2003). Previous work on definition ranking has used measures such as tf*idf (Xu et al., 2004) or ranking models trained to encode the likelihood of a definition being good (Xu et al., 2005). However, such measures/models may not be suitable for quiz-style ranking. For example, a definition having a strong co-occurrence with a person may not be an easy hint when it is about a very minor detail. Certain descriptions, such as a person’s birthplace, would have to come early so that users can easily start guessing who the person is. In our approach, we train a ranker that learns from data the appropriate ranking of definitions. Note that we only focus on the ranking of definitions and not on the interaction with users in this paper. We also assume that the definitions to be ranked ar</context>
<context position="5622" citStr="Xu et al., 2005" startWordPosition="911" endWordPosition="914">ami Haruki? 9. Works include “Botchan”, “Sanshiro”, etc. S4 Close! Fourth hint: Familiar with Haiku, ⇓ Chinese poetry, and calligraphy. U4 Mori Ogai? S5 Very close! Fifth hint: Published masterpieces in Asahi Shimbun. U5 Natsume Soseki? S6 That’s right! � � Figure 2: Example dialogue based on the quiz-style ranking of definitions. S stands for a system utterance and U for a user utterance. 3 Approach Since it is difficult to know in advance what characteristics are important for quiz-style ranking, we learn the appropriate ranking of definitions from data. The approach is the same as that of (Xu et al., 2005) in that we adopt a machine learning approach for definition ranking, but is different in that what is learned is a quiz-style ranking of sentences that are already known to be good definitions. First, we collect ranking data. For this purpose, we turn to existing encyclopedias for concise biographies. Then, we annotate the ranking. Secondly, we devise a set of features for a definition. Since the existence of keywords that have high scores in IRrelated measures may suggest easy hints, we incorporate the scores of IR-related measures as features (IR-related features). Certain words tend to app</context>
<context position="8608" citStr="Xu et al., 2005" startWordPosition="1380" endWordPosition="1383">nces or information nuggets (Xu et al., 2004; Sun et al., 2005). Such measures are used under the assumption that outstanding/co-occurring keywords about a definiendum characterize that definiendum. However, this assumption may not be appropriate in quizstyle ranking; most content words in the definitions are already important in the IR sense, and strong cooccurrence may not guarantee high ranks for hints to be presented later because the hint can be too specific. An approach to creating a ranking model of definitions in a supervised manner using machine learning techniques has been reported (Xu et al., 2005). However, the model is only used to distinguish definitions from non-definitions on the basis of features related mainly to linguistic styles. In multi-document summarization, the focus has been mainly on creating cohesive texts. (Lapata, 2003) uses the probability of words in adjacent sentences as constraints to maximize the coherence of all sentence-pairs in texts. Although we acknowledge that having cohesive definitions is important, since we are not creating a single text and the dialogue that we aim to achieve would involve frequent user/system interaction (Fig. 2), we do not deal with t</context>
</contexts>
<marker>Xu, Cao, Li, Zhao, 2005</marker>
<rawString>Jun Xu, Yunbo Cao, Hang Li, and Min Zhao. 2005. Ranking definitions with supervised learning methods. In Proc. WWW, pages 811–819.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>