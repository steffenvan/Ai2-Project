<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004636">
<title confidence="0.9899935">
Improving pairwise coreference models through
feature space hierarchy learning
</title>
<author confidence="0.875833">
Emmanuel Lassalle Pascal Denis
</author>
<affiliation confidence="0.392916">
Alpage Project-team Magnet Project
</affiliation>
<address confidence="0.5151135">
INRIA &amp; Univ. Paris Diderot INRIA Lille - Nord Europe
Sorbonne Paris Cit´e, F-75205 Paris Avenue Heloise, 59650 Villeneuve d’Ascq
</address>
<email confidence="0.99359">
emmanuel.lassalle@ens-lyon.org pascal.denis@inria.fr
</email>
<sectionHeader confidence="0.994621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970666666667">
This paper proposes a new method for
significantly improving the performance
of pairwise coreference models. Given a
set of indicators, our method learns how
to best separate types of mention pairs
into equivalence classes for which we con-
struct distinct classification models. In ef-
fect, our approach finds an optimal fea-
ture space (derived from a base feature set
and indicator set) for discriminating coref-
erential mention pairs. Although our ap-
proach explores a very large space of pos-
sible feature spaces, it remains tractable
by exploiting the structure of the hierar-
chies built from the indicators. Our exper-
iments on the CoNLL-2012 Shared Task
English datasets (gold mentions) indicate
that our method is robust relative to dif-
ferent clustering strategies and evaluation
metrics, showing large and consistent im-
provements over a single pairwise model
using the same base features. Our best
system obtains a competitive 67.2 of aver-
age F1 over MUC, B3, and CEAF which,
despite its simplicity, places it above the
mean score of other systems on these
datasets.
</bodyText>
<sectionHeader confidence="0.999154" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998091642857143">
Coreference resolution is the problem of partition-
ing a sequence of noun phrases (or mentions), as
they occur in a natural language text, into a set of
referential entities. A common approach to this
problem is to separate it into two modules: on
the one hand, one defines a model for evaluating
coreference links, in general a discriminative clas-
sifier that detects coreferential mention pairs. On
the other hand, one designs a method for group-
ing the detected links into a coherent global out-
put (i.e. a partition over the set of entity men-
tions). This second step is typically achieved
using greedy heuristics (McCarthy and Lehnert,
1995; Soon et al., 2001; Ng and Cardie, 2002;
Bengston and Roth, 2008), although more so-
phisticated clustering approaches have been used,
too, such as cutting graph methods (Nicolae and
Nicolae, 2006; Cai and Strube, 2010) and Integer
Linear Programming (ILP) formulations (Klenner,
2007; Denis and Baldridge, 2009). Despite its
simplicity, this two-step strategy remains competi-
tive even when compared to more complex models
utilizing a global loss (Bengston and Roth, 2008).
In this kind of architecture, the performance of
the entire coreference system strongly depends on
the quality of the local pairwise classifier.1 Con-
sequently, a lot of research effort on coreference
resolution has focused on trying to boost the per-
formance of the pairwise classifier. Numerous
studies are concerned with feature extraction, typ-
ically trying to enrich the classifier with more
linguistic knowledge and/or more world knowl-
edge (Ng and Cardie, 2002; Kehler et al., 2004;
Ponzetto and Strube, 2006; Bengston and Roth,
2008; Versley et al., 2008; Uryupina et al., 2011).
A second line of work explores the use of dis-
tinct local models for different types of mentions,
specifically for different types of anaphoric men-
tions based on their grammatical categories (such
as pronouns, proper names, definite descriptions)
(Morton, 2000; Ng, 2005; Denis and Baldridge,
2008).2 An important justification for such spe-
</bodyText>
<footnote confidence="0.9927">
1There are however no theoretical guarantees that improv-
ing pair classification will always result in overall improve-
ments if the two modules are optimized independently.
2Sometimes, distinct sample selections are also adopted
</footnote>
<page confidence="0.922503">
497
</page>
<note confidence="0.9159045">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 497–506,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999771833333334">
cialized models is (psycho-)linguistic and comes
from theoretical findings based on salience or ac-
cessibility (Ariel, 1988). It is worth noting that,
from a machine learning point of view, this is re-
lated to feature extraction in that both approaches
in effect recast the pairwise classification problem
in higher dimensional feature spaces.
In this paper, we claim that mention pairs
should not be processed by a single classifier, and
instead should be handled through specific mod-
els. But we are furthermore interested in learning
how to construct and select such differential mod-
els. Our argument is therefore based on statisti-
cal considerations, rather than on purely linguis-
tic ones3. The main question we raise is, given
a set of indicators (such as grammatical types,
distance between two mentions, or named entity
types), how to best partition the pool of mention
pair examples in order to best discriminate coref-
erential pairs from non coreferential ones. In ef-
fect, we want to learn the “best” subspaces for our
different models: that is, subspaces that are neither
too coarse (i.e., unlikely to separate the data well)
nor too specific (i.e., prone to data sparseness and
noise). We will see that this is also equivalent to
selecting a single large adequate feature space by
using the data.
Our approach generalizes earlier approaches in
important ways. For one thing, the definition
of the different models is no longer restricted to
grammatical typing (our model allows for various
other types of indicators) or to the sole typing of
the anaphoric mention (our models can also be
specific to a particular type antecedent or to the
two types of the mention pair). More importantly,
we propose an original method for learning the
best set of models that can be built from a given
set of indicators and a training set. These models
are organized in a hierarchy, wherein each leaf cor-
responds to a mutually disjoint subset of mention
pair examples and the classifier that can be trained
from it. Our models are trained using the Online
Passive-Aggressive algorithm or PA (Crammer et
al., 2006), a large margin version of the percep-
tron. Our method is exact in that it explores the full
space of hierarchies (of size at least 22&amp;quot;) definable
on an indicator sequence, while remaining scal-
able by exploiting the particular structure of these
</bodyText>
<footnote confidence="0.99664225">
during the training of the distinct local models (Ng and
Cardie, 2002; Uryupina, 2004).
3However it should be underlined that the statistical view-
point is complementary to the linguistic work.
</footnote>
<bodyText confidence="0.999444157894737">
hierarchies with dynamic programming. This ap-
proach also performs well, and it largely outper-
forms the single model. As will be shown based
on a variety of experiments on the CoNLL-2012
Shared Task English datasets, these improvements
are consistent across different evaluation metrics
and for the most part independent of the clustering
decoder that was used.
The rest of this paper is organized as follows.
Section 2 discusses the underlying statistical hy-
potheses of the standard pairwise model and de-
fines a simple alternative framework that uses a
simple separation of mention pairs based on gram-
matical types. Next, in section 3, we generalize the
method by introducing indicator hierarchies and
explain how to learn the best models associated
with them. Section 4 provides a brief system de-
scription and Section 5 evaluates the various mod-
els on CoNLL-2012 English datasets.
</bodyText>
<sectionHeader confidence="0.782918" genericHeader="method">
2 Modeling pairs
</sectionHeader>
<bodyText confidence="0.9997054">
Pairwise models basically employ one local clas-
sifier to decide whether two mentions are corefer-
ential or not. When using machine learning tech-
niques, this involves certain assumptions about the
statistical behavior of mention pairs.
</bodyText>
<subsectionHeader confidence="0.998838">
2.1 Statistical assumptions
</subsectionHeader>
<bodyText confidence="0.999973173913043">
Let us adopt a probabilistic point of view to de-
scribe the prototype of pairwise models. Given
a document, the number of mentions is fixed and
each pair of mentions follows a certain distribution
(that we partly observe in a feature space). The ba-
sic idea of pairwise models is to consider mention
pairs independently from each other (that is why a
decoder is necessary to enforce transitivity).
If we use a single classifier to process all
pairs, then they are supposed to be identically dis-
tributed. We claim that pairs should not be pro-
cessed by a single classifier because they are not
identically distributed (or a least the distribution is
too complex for the classifier); rather, we should
separate different “types” on pairs and create a
specific model for each of them.
Separating different kinds of pairs and handling
them with different specific models can lead to
more accurate global models. For instance, some
coreference resolution systems process different
kinds of anaphors separately, which suggests for
example that pairs containing an anaphoric pro-
noun behave differently from pairs with non-
</bodyText>
<page confidence="0.998097">
498
</page>
<bodyText confidence="0.999960111111111">
pronominal anaphors. One could rely on a rich set
of features to capture complex distributions, but
here we actually have a rather limited set of ele-
mentary features (see section 4) and, for instance,
using products of features must be done carefully
to avoid introducing noise in the model. Instead
of imposing heuristic product of features, we will
show that a clever separation of instances leads to
significant improvements of the pairwise model.
</bodyText>
<subsectionHeader confidence="0.9989225">
2.2 Feature spaces
2.2.1 Definitions
</subsectionHeader>
<bodyText confidence="0.981947">
We first introduce the problem more formally. Ev-
ery pair of mentions mi and mj is modeled by a
random variable:
</bodyText>
<equation confidence="0.997394">
Pij : Ω → X × Y
ω 7→ (xij(ω),yij(ω))
</equation>
<bodyText confidence="0.987959428571428">
where Ω classically represents randomness, X is
the space of objects (“mention pairs”) that is not
directly observable and yij(ω) ∈ Y = {+1, −1}
are the labels indicating whether mi and mj are
coreferential or not. To lighten the notations, we
will not always write the index ij. Now we define
a mapping:
</bodyText>
<equation confidence="0.99672">
φF : X → F
x 7→ x
</equation>
<bodyText confidence="0.9992438">
that casts pairs into a feature space F through
which we observe them. For us, F is simply a
vector space over R (in our case many features are
Boolean; they are cast into R as 0 and 1).
For technical coherence, we assume that
φF1(x(ω)) and φF2(x(ω)) have the same values
when projected on the feature space F1 ∩ F2:
it means that common features from two feature
spaces have the same values.
From this formal point of view, the task of
coreference resolution consists in fixing φF, ob-
serving labeled samples {(φF(x),y)t}t∈TrainSet
and, given partially observed new variables
{(φF(x))t}t∈TestSet, recovering the correspond-
ing values of y.
</bodyText>
<subsectionHeader confidence="0.872489">
2.2.2 Formalizing the statistical assumptions
</subsectionHeader>
<bodyText confidence="0.999969348837209">
We claimed before that all mention pairs seemed
not to be identically distributed since, for exam-
ple, pronouns do not behave like nominals. We
can formulate this more rigorously: since the ob-
ject space X is not directly observable, we do not
know its complexity. In particular, when using a
mapping to a too small feature space, the classifier
cannot capture the distribution very well: the data
is too noisy.
Now if we say that pronominal anaphora do not
behave like other anaphora, we distinguish two
kinds of pair i.e. we state that the distribution of
pairs in X is a mixture of two distributions, and
we deterministically separate pairs to their specific
distribution part. In this way, we may separate
positive and negative pairs more easily if we cast
each kind of pair into a specific feature space. Let
us call these feature spaces F1 and F2. We can ei-
ther create two independent classifiers on F1 and
F2 to process each kind of pair or define a single
model on a larger feature space F = F1 ⊕ F2. If
the model is linear (which is our case), these ap-
proaches happen to be equivalent.
So we can actually assume that the random vari-
ables Pij are identically distributed, but drawn
from a complex mixture. A new issue arises: we
need to find a mapping φF that renders the best
view on the distribution of the data.
From a theoretical viewpoint, the higher the di-
mension of the feature space (imagine taking the
direct sum of all feature spaces), the more we get
details on the distribution of mention pairs and the
more we can expect to separate positives and neg-
atives accurately. In practice, we have to cope
with data sparsity: there will not be enough data
to properly train a linear model on such a space.
Finally, we seek a feature space situated between
the two extremes of a space that is too big (sparse-
ness) or too small (noisy data). The core of this
work is to define a general method for choosing
the most adequate space F among a huge num-
ber of possibilities when we do not know a priori
which is the best.
</bodyText>
<subsectionHeader confidence="0.748033">
2.2.3 Linear models
</subsectionHeader>
<bodyText confidence="0.999672285714286">
In this work, we try to linearly separate pos-
itive and negative instances in the large space
F with the Online Passive-Aggressive (PA) algo-
rithm (Crammer et al., 2006): the model learns a
parameter vector w that defines a hyperplane that
cuts the space into two parts. The predicted class
of a pair x with feature vector φF(x) is given by:
</bodyText>
<equation confidence="0.997977">
CF(x) := sign(wT · φF(x))
</equation>
<bodyText confidence="0.9930025">
Linearity implies an equivalence between: (i)
separating instances of two types, t1 and t2, in two
</bodyText>
<page confidence="0.996001">
499
</page>
<bodyText confidence="0.9982535">
independent models with respective feature spaces
F1 and F2 and parameters w1 and w2, and (ii) a
single model on F1⊕F2. To see why, let us define
the map:
</bodyText>
<equation confidence="0.993539">
O.T1®.T2(x) := { ~ ~T
O.T1(x)T 0 if x typed t1
T
~ 0 O.T2 (x)T ~ if x typed t2
� w1 �
</equation>
<bodyText confidence="0.91875">
and the parameter vector w = ∈ F1 ⊕
</bodyText>
<equation confidence="0.868008">
w2
F2. Then we have:
(
C.T1(x) if x typed t1
C.T2(x) if x typed t2
</equation>
<bodyText confidence="0.999131">
Now we check that the same property applies
when the PA fits its parameter w. For each new
instance of the training set, the weight is updated
according to the following rule4:
</bodyText>
<equation confidence="0.9755015">
wt+1 = arg min
WE.T
</equation>
<bodyText confidence="0.9543435">
where l(w; (xt, yt)) = min(0,1−yt(w·O.T(xt))),
so that when F = F1 ⊕ F2, the minimum if x is
</bodyText>
<equation confidence="0.99908925">
� w1 �
t+1
typed t1 is wt+1 = and if x is typed
w2
t
� W11
t2 is wt+1 =Wt where the wit+1 corre-
wt+1
</equation>
<bodyText confidence="0.9997594">
spond to the updates in space Fi independently
from the rest. This result can be extended easily
to the case of n feature spaces. Thus, with a deter-
ministic separation of the data, a large model can
be learned using smaller independent models.
</bodyText>
<subsectionHeader confidence="0.9996">
2.3 An example: separation by gramtype
</subsectionHeader>
<bodyText confidence="0.999983153846154">
To motivate our approach, we first introduce a
simple separation of mention pairs which cre-
ates 9 models obtained by considering all possi-
ble pairs of grammatical types {nominal, name,
pronoun} for both mentions in the pair (a simi-
lar fine-grained separation can be found in (Chen
et al., 2011)). This is equivalent to using 9 differ-
ent feature spaces F1,...,F9 to capture the global
distribution of pairs. With the PA, this is also a sin-
gle model with feature space F = F1 ⊕ · · · ⊕ F9.
We will call it the GRAMTYPE model.
As we will see in Section 5, these separated
models significantly outperform a single model
</bodyText>
<footnote confidence="0.970703666666667">
4The parameter is updated to obtain a margin of a least 1.
It does not change if the instance is already correctly classi-
fied with such margin.
</footnote>
<bodyText confidence="0.99928025">
that uses the same base feature set. But we would
like to define a method that adapts a feature space
to the data by choosing the most adequate separa-
tion of pairs.
</bodyText>
<sectionHeader confidence="0.919888" genericHeader="method">
3 Hierarchizing feature spaces
</sectionHeader>
<bodyText confidence="0.9998918">
In this section, we have to keep in mind that sep-
arating the pairs in different models is the same
as building a large feature space in which the pa-
rameter w can be learned by parts in independent
subspaces.
</bodyText>
<subsectionHeader confidence="0.998209">
3.1 Indicators on pairs
</subsectionHeader>
<bodyText confidence="0.999975772727273">
For establishing a structure on feature spaces, we
use indicators which are deterministic functions
on mention pairs with a small number of outputs.
Indicators classify pairs in predefined categories in
one-to-one correspondence with independent fea-
ture spaces. We can reuse some features of the sys-
tem as indicators, e.g. the grammatical or named
entity types. We can also employ functions that
are not used as features, e.g. the approximate po-
sition of one of the mentions in the text.
The small number of outputs of an indica-
tor is required for practical reasons: if a cate-
gory of pairs is too refined, the associated fea-
ture space will suffer from data sparsity. Accord-
ingly, distance-based indicators must be approxi-
mated by coarse histograms. In our experiments
the outputs never exceeded a dozen values. One
way to reduce the output span of an indicator is
to binarize it like binarizing a tree (many possible
binarizations). This operation produces a hierar-
chy of indicators which is exactly the structure we
exploit in what follows.
</bodyText>
<subsectionHeader confidence="0.967358">
3.2 Hierarchies for separating pairs
</subsectionHeader>
<bodyText confidence="0.999543428571428">
We define hierarchies as combinations of indi-
cators creating finer categories of mention pairs:
given a finite sequence of indicators, a mention
pair is classified by applying the indicators suc-
cessively, each time refining a category into sub-
categories, just like in a decision tree (each node
having the same number of children as the number
of outputs of its indicator). We allow the classifi-
cation to stop before applying the last indicator,
but the behavior must be the same for all the in-
stances. So a hierarchy is basically a sub-tree of
the complete decision tree that contains copies of
the same indicator at each level.
If all the leaves of the decision tree have the
</bodyText>
<equation confidence="0.9868">
C.T1®.T2(x) =
2 kw − wtk2 s.t. l(w; (xt,yt)) = 0
1
</equation>
<page confidence="0.939689">
500
</page>
<bodyText confidence="0.9999032">
same depth, this corresponds to taking the Carte-
sian product of outputs of all indicators for in-
dexing the categories. In that case, we refer to
product-hierarchies. The GRAMTYPE model can
be seen as a two level product-hierarchy (figure 1).
</bodyText>
<figureCaption confidence="0.998802">
Figure 1: GRAMTYPE seen as a product-hierarchy
</figureCaption>
<bodyText confidence="0.99883975">
Product-hierarchies will be the starting point of
our method to find a feature space that fits the data.
Now choosing a relevant sequence of indicators
should be achieved through linguistic intuitions
and theoretical work (gramtype separation is one
of them). The system will find by itself the best
usage of the indicators when optimizing the hier-
archy. The sequence is a parameter of the model.
</bodyText>
<subsectionHeader confidence="0.999026">
3.3 Relation with feature spaces
</subsectionHeader>
<bodyText confidence="0.999985869565217">
Like we did for the GRAMTYPE model, we asso-
ciate a feature space Ti to each leaf of a hierarchy.
Likewise, the sum T = ®i Ti defines a large fea-
ture space. The corresponding parameter w of the
model can be obtained by learning the wi in Ti.
Given a sequence of indicators, the number of
different hierarchies we can define is equal to the
number of sub-trees of the complete decision tree
(each non-leaf node having all its children). The
minimal case is when all indicators are Boolean.
The number of full binary trees of height at most
n can be computed by the following recursion:
T(1) = 1 and T(n + 1) = 1 + T(n)2. So
T(n) &gt; 22&amp;quot;: even with small values of n, the
number of different hierarchies (or large feature
spaces) definable with a sequence of indicators is
gigantic (e.g. T(10) Pz 3.8.1090).
Among all the possibilities for a large feature
space, many are irrelevant because for them the
data is too sparse or too noisy in some subspaces.
We need a general method for finding an ade-
quate space without enumerating and testing each
of them.
</bodyText>
<subsectionHeader confidence="0.992263">
3.4 Optimizing hierarchies
</subsectionHeader>
<bodyText confidence="0.999970930232558">
Let us assume now that the sequence of indicators
is fixed, and let n be its length. To find the best
feature space among a very high number of pos-
sibilities, we need a criterion we can apply with-
out too much additional computation. For that we
only evaluate the feature space locally on pairs,
i.e. without applying a decoder on the output. We
employ 3 measures on pairwise classification re-
sults: precision, recall and F1-score. Now select-
ing the best space for one of these measures can
be achieved by using dynamic programming tech-
niques. In the rest of the paper, we will optimize
the F1-score.
Training the hierarchy Starting from the
product-hierarchy, we associate a classifier and its
proper feature space to each node of the tree5. The
classifiers are then trained as follows: for each in-
stance there is a unique path from the root to a leaf
of the complete tree. Each classifier situated on
the path is updated with this instance. The number
of iterations of the Passive-Aggressive is fixed.
Computing scores After training, we test all the
classifiers on another set of pairs6. Again, a classi-
fier is tested on an instance only if it is situated on
the path from the root to the leaf associated with
the instance. We obtain TP/FP/FN numbers7 on
pair classifications that are sufficient to compute
the F1-score. As for training, the data on which a
classifier at a given node is evaluated is the same
as the union of all data used to evaluate the clas-
sifiers corresponding to the children of this node.
Thus we are able to compare the scores obtained
at a node to the “union of the scores” obtained at
its children.
Cutting down the hierarchy For the moment
we have a complete tree with a classifier at each
node. We use a dynamic programming technique
to compute the best hierarchy by cutting this tree
and only keeping classifiers situated at the leaf.
The algorithm assembles the best local models (or
feature spaces) together to create larger models. It
goes from the leaves to the root and cuts the sub-
tree starting at a node whenever it does not pro-
</bodyText>
<footnote confidence="0.989033285714286">
5In the experiments, the classifiers use a copy of a same
feature space, but not the same data, which corresponds to
crossing the features with the categories of the decision tree.
6The training set is cut into two parts, for training and
testing the hierarchy. We used 10-fold cross-validation in our
experiments.
7True positives, false positives and false negatives.
</footnote>
<page confidence="0.997427">
501
</page>
<bodyText confidence="0.97920825">
vide a better score than the node itself, or on the
contrary propagates the score of the sub-tree when
there is an improvement. The details are given in
algorithm 1.
</bodyText>
<figure confidence="0.903086166666667">
1 list list of nodes given by a breadth-first
search for node in reversed list do
2 if node.children =� 0 then
3 if sum-score(node.children) &gt;
node.score then
4 node.TP/FP/FN
sum-num(node.children)
5 else
6 node.children 0
7 end
8 end
9 end
</figure>
<figureCaption confidence="0.817807">
Algorithm 1: Cutting down a hierarchy
</figureCaption>
<bodyText confidence="0.998228571428571">
Let us briefly discuss the correctness and com-
plexity of the algorithm. Each node is seen two
times so the time complexity is linear in the num-
ber of nodes which is at least 0(2n). However,
only nodes that have encountered at least one
training instance are useful and there are 0(n x
k) such nodes (where k the size of the training
set). So we can optimize the algorithm to run
in time 0(n x k)8. If we scan the list obtained
by breadth-first search backwards, we are ensured
that every node will be processed after its chil-
dren. (node.children) is the set of children of
node, and (node.score) its score. sum-num pro-
vides TP/FP/FN by simply adding those of the
children and sum-score computes the score based
on these new TP/FP/FN numbers. (line 6) cuts the
children of a node when they are not used in the
best score. The algorithm thus propagates the best
scores from the leaves to the root which finally
gives a single score corresponding to the best hi-
erarchy. Only the leaves used to compute the best
score are kept, and they define the best hierarchy.
Relation between cutting and the global feature
space We can see the operation of cutting as re-
placing a group of subspaces by a single subspace
in the sum (see figure 2). So cutting down the
product-hierarchy amounts to reducing the global
initial feature space in an optimal way.
</bodyText>
<footnote confidence="0.983125333333333">
8In our experiments, cutting down the hierarchy was
achieved very quickly, and the total training time was about
five times longer than with a single model.
</footnote>
<figureCaption confidence="0.9342695">
Figure 2: Cutting down the hierarchy reduces the
feature space
</figureCaption>
<bodyText confidence="0.966503666666667">
To sum up, the whole procedure is equivalent to
training more than 0(2n) perceptrons simultane-
ously and selecting the best performing.
</bodyText>
<sectionHeader confidence="0.985923" genericHeader="method">
4 System description
</sectionHeader>
<bodyText confidence="0.999933">
Our system consists in the pairwise model ob-
tained by cutting a hierarchy (the PA with selected
feature space) and using a greedy decoder to cre-
ate clusters from the output. It is parametrized by
the choice of the initial sequence of indicators.
</bodyText>
<subsectionHeader confidence="0.987473">
4.1 The base features
</subsectionHeader>
<bodyText confidence="0.9999831">
We used classical features that can be found in
details in (Bengston and Roth, 2008) and (Rah-
man and Ng, 2011): grammatical type and sub-
type of mentions, string match and substring, ap-
position and copula, distance (number of sepa-
rating mentions/sentences/words), gender/number
match, synonymy/hypernym and animacy (using
WordNet), family name (based on lists), named
entity types, syntactic features (gold parse) and
anaphoricity detection.
</bodyText>
<subsectionHeader confidence="0.91302">
4.2 Indicators
</subsectionHeader>
<bodyText confidence="0.9999953">
As indicators we used: left and right grammati-
cal types and subtypes, entity types, a boolean in-
dicating if the mentions are in the same sentence,
and a very coarse histogram of distance in terms of
sentences. We systematically included right gram-
type and left gramtype in the sequences and added
other indicators, producing sequences of different
lengths. The parameter was optimized by docu-
ment categories using a development set after de-
coding the output of the pairwise model.
</bodyText>
<subsectionHeader confidence="0.997661">
4.3 Decoders
</subsectionHeader>
<bodyText confidence="0.9998965">
We tested 3 classical greedy link selection strate-
gies that form clusters from the classifier decision:
Closest-First (merge mentions with their closest
coreferent mention on the left) (Soon et al., 2001),
</bodyText>
<page confidence="0.993363">
502
</page>
<bodyText confidence="0.99890775">
Best-first (merge mentions with the mention on
the left having the highest positive score) (Ng
and Cardie, 2002; Bengston and Roth, 2008), and
Aggressive-Merge (transitive closure on positive
pairs) (McCarthy and Lehnert, 1995). Each of
these decoders is typically (although not always)
used in tandem with a specific sampling selec-
tion at training. Thus, Closest-First for instance is
used in combination with a sample selection that
generates training instances only for the mentions
that occur between the closest antecedent and the
anaphor (Soon et al., 2001).
</bodyText>
<table confidence="0.9988602">
P R F1
SINGLE MODEL 22.28 63.50 32.99
RIGHT-TYPE 29.31 45.23 35.58
GRAMTYPE 39.12 45.83 42.21
BEST HIERARCHY 45.27 51.98 48.40
</table>
<tableCaption confidence="0.999808">
Table 1: Pairwise scores on CoNLL-2012 test.
</tableCaption>
<sectionHeader confidence="0.994113" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.987745">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.9999775">
We evaluated the system on the English part of the
corpus provided in the CoNLL-2012 Shared Task
(Pradhan et al., 2012), referred to as CoNLL-2012
here. The corpus contains 7 categories of doc-
uments (over 2K documents, 1.3M words). We
used the official train/dev/test data sets. We evalu-
ated our system in the closed mode which requires
that only provided data is used.
</bodyText>
<subsectionHeader confidence="0.999189">
5.2 Settings
</subsectionHeader>
<bodyText confidence="0.999988826086956">
Our baselines are a SINGLE MODEL, the GRAM-
TYPE model (section 2) and a RIGHT-TYPE
model, defined as the first level of the gramtype
product hierarchy (i.e. grammatical type of the
anaphora (Morton, 2000)), with each greedy de-
coder and also the original sampling with a single
model associated with those decoders.
The hierarchies were trained with 10-fold cross-
validation on the training set (the hierarchies are
cut after cumulating the scores obtained by cross-
validation) and their parameters are optimized by
document category on the development set: the
sequence of indicators obtaining the best average
score after decoding was selected as parameter for
the category. The obtained hierarchy is referred to
as the BEST HIERARCHY in the results. We fixed
the number of iterations for the PA for all models.
In our experiments, we consider only the gold
mentions. This is a rather idealized setting but our
focus is on comparing various pairwise local mod-
els rather than on building a full coreference reso-
lution system. Also, we wanted to avoid having to
consider too many parameters in our experiments.
</bodyText>
<subsectionHeader confidence="0.99799">
5.3 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.9999285">
We use the three metrics that are most commonly
used9, namely:
MUC (Vilain et al., 1995) computes for each
true entity cluster the number of system clusters
that are needed to cover it. Precision is this quan-
tity divided by the true cluster size minus one. Re-
call is obtained by reversing true and predicated
clusters. F1 is the harmonic mean.
B3 (Bagga and Baldwin, 1998) computes recall
and precision scores for each mention, based on
the intersection between the system/true clusters
for that mention. Precision is the ratio of the in-
tersection and the true cluster sizes, while recall is
the ratio of the intersection to the system cluster
sizes. Global recall, precision, and F1 scores are
obtained by averaging over the mention scores.
CEAF (Luo, 2005) scores are obtained by com-
puting the best one-to-one mapping between the
system/true partitions, which is equivalent to find-
ing the best optimal alignment in the bipartite
graph formed out of these partitions. We use the
φ4 similarity function from (Luo, 2005).
These metrics were recently used in the CoNLL-
2011 and -2012 Shared Tasks. In addition, these
campaigns use an unweighted average over the F1
scores given by the three metrics. Following com-
mon practice, we use micro-averaging when re-
porting our scores for entire datasets.
</bodyText>
<subsectionHeader confidence="0.812082">
5.4 Results
</subsectionHeader>
<bodyText confidence="0.999065555555555">
The results obtained by the system are reported in
table 2. The original sampling for the single model
associated to Closest-First and Best-First decoder
are referred to as SOON and NGCARDIE.
The P/R/F1 pairwise scores before decoding are
given in table 1. BEST HIERARCHY obtains a
strong improvement in F1 (+15), abetter precision
and a less significant diminution of recall com-
pared to GRAMTYPE and RIGHT-TYPE.
</bodyText>
<footnote confidence="0.9766584">
9BLANC metric (Recasens and Hovy, 2011) results are
not reported since they are not used to compute the CoNLL-
2012 global score. However we can mention that in our ex-
periments, using hierarchies had a positive effect similar to
what was observed on B3 and CEAF.
</footnote>
<page confidence="0.992191">
503
</page>
<table confidence="0.99850595">
MUC B3 CEAF
Closest-First P R F1 P R F1 P R F1 Mean
SOON 79.49 93.72 86.02 26.23 89.43 40.56 49.74 19.92 28.44 51.67
SINGLE MODEL 78.95 75.15 77.0 51.88 68.42 59.01 37.79 43.89 40.61 58.87
RIGHT-TYPE 79.36 67.57 72.99 69.43 56.78 62.47 41.17 61.66 49.37 61.61
GRAMTYPE 80.5 71.12 75.52 66.39 61.04 63.6 43.11 59.93 50.15 63.09
BEST HIERARCHY 83.23 73.72 78.19 73.5 67.09 70.15 47.3 60.89 53.24 67.19
MUC B3 CEAF
Best-First P R F1 P R F1 P R F1 Mean
NGCARDIE 81.02 93.82 86.95 23.33 93.92 37.37 40.31 18.97 25.8 50.04
SINGLE MODEL 79.22 73.75 76.39 40.93 75.48 53.08 30.52 37.59 33.69 54.39
RIGHT-TYPE 77.13 65.09 70.60 48.11 66.21 55.73 31.07 47.30 37.50 54.61
GRAMTYPE 77.21 65.89 71.1 49.77 67.19 57.18 32.08 47.83 38.41 55.56
BEST HIERARCHY 78.11 69.82 73.73 53.62 70.86 61.05 35.04 46.67 40.03 58.27
MUC B3 CEAF
Aggressive-Merge P R F1 P R F1 P R F1 Mean
SINGLE MODEL 83.15 88.65 85.81 35.67 88.18 50.79 36.3 28.27 31.78 56.13
RIGHT-TYPE 83.48 89.79 86.52 36.82 88.08 51.93 45.30 33.84 38.74 59.07
GRAMTYPE 83.12 84.27 83.69 44.73 81.58 57.78 45.02 42.94 43.95 61.81
BEST HIERARCHY 83.26 85.2 84.22 45.65 82.48 58.77 46.28 43.13 44.65 62.55
</table>
<tableCaption confidence="0.999914">
Table 2: CoNLL-2012 test (gold mentions): Closest-First, Best-First and Aggressive-Merge decoders.
</tableCaption>
<bodyText confidence="0.999952162790698">
Despite the use of greedy decoders, we observe
a large positive effect of pair separation in the
pairwise models on the outputs. On the mean
score, the use of distinct models versus a sin-
gle model yields F1 increases from 6.4 up to 8.3
depending on the decoder. Irrespective of the
decoder being used, GRAMTYPE always outper-
forms RIGHT-TYPE and single model and is al-
ways outperformed by BEST HIERARCHY model.
Interestingly, we see that the increment in pair-
wise and global score are not proportional: for
instance, the strong improvement of F1 between
RIGHT-TYPE and GRAMTYPE results in a small
amelioration of the global score.
Depending on the document category, we found
some variations as to which hierarchy was learned
in each setting, but we noticed that parameters
starting with right and left gramtypes often pro-
duced quite good hierarchies: for instance right
gramtype → left gramtype → same sentence →
right named entity type.
We observed that product-hierarchies did not
performed well without cutting (especially when
using longer sequences of indicators, because of
data sparsity) and could obtain scores lower than
the single model. Hopefully, after cutting them the
results always became better as the resulting hier-
archy was more balanced.
Looking at the different metrics, we notice that
overall, pair separation improves B3 and CEAF
(but not always MUC) after decoding the output:
GRAMTYPE provides a better mean score than the
single model, and BEST HIERARCHY gives the
highest B3, CEAF and mean score.
The best classifier-decoder combination reaches
a score of 67.19, which would place it above the
mean score (66.41) of the systems that took part
in the CoNLL-2012 Shared Task (gold mentions
track). Except for the first at 77.22, the best
performing systems have a score around 68-69.
Considering the simple decoding strategy we em-
ployed, our current system sets up a strong base-
line.
</bodyText>
<sectionHeader confidence="0.989889" genericHeader="conclusions">
6 Conclusion and perspectives
</sectionHeader>
<bodyText confidence="0.999731857142857">
In this paper, we described a method for select-
ing a feature space among a very large number of
choices by using linearity and by combining indi-
cators to separate the instances. We employed dy-
namic programming on hierarchies of indicators
to compute the feature space providing the best
pairwise classifications efficiently. We applied this
</bodyText>
<page confidence="0.993031">
504
</page>
<bodyText confidence="0.999982243243243">
method to optimize the pairwise model of a coref-
erence resolution system. Using different kinds
of greedy decoders, we showed a significant im-
provement of the system.
Our approach is flexible in that we can use a va-
riety of indicators. In the future we will apply the
hierarchies on finer feature spaces to make more
accurate optimizations. Observing that the gen-
eral method of cutting down hierarchies is not re-
stricted to modeling mention pairs, but can be ap-
plied to problems having Boolean aspects, we aim
at employing hierarchies to address other tasks in
computational linguistics (e.g. anaphoricity detec-
tion or discourse and temporal relation classifica-
tion wherein position information may help sepa-
rating the data).
In this work, we have only considered standard,
heuristic linking strategies like Closest-First. So,
a natural extension of this work is to combine our
method for learning pairwise models with more
sophisticated decoding strategies (like Bestcut or
using ILP). Then we can test the impact of hierar-
chies with more realistic settings.
Finally, the method for cutting hierarchies
should be compared to more general but similar
methods, for instance polynomial kernels for SVM
and tree-based methods (Hastie et al., 2001). We
also plan to extend our method by breaking the
symmetry of our hierarchies. Instead of cutting
product-hierarchies, we will employ usual tech-
niques to build decision trees10 and apply our cut-
ting method on their structure. The objective is
twofold: first, we will get rid of the sequence of
indicators as parameter. Second, we will avoid
fragmentation or overfitting (which can arise with
classification trees) by deriving an optimal large
margin linear model from the tree structure.
</bodyText>
<sectionHeader confidence="0.99824" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996976">
We thank the ACL 2013 anonymous reviewers for
their valuable comments.
</bodyText>
<sectionHeader confidence="0.999193" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999893966666667">
M. Ariel. 1988. Referring and accessibility. Journal
of Linguistics, pages 65–87.
A. Bagga and B. Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of
LREC 1998, pages 563–566.
10(Bansal and Klein, 2012) show good performances of de-
cision trees on coreference resolution.
Mohit Bansal and Dan Klein. 2012. Coreference se-
mantics from web features. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
389–398. Association for Computational Linguis-
tics.
Eric Bengston and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of EMNLP 2008, pages 294–303, Hon-
olulu, Hawaii.
Jie Cai and Michael Strube. 2010. End-to-end coref-
erence resolution via hypergraph partitioning. In
COLING, pages 143–151.
Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim
Tan. 2011. A unified event coreference resolu-
tion by integrating multiple resolvers. In Proceed-
ings of 5th International Joint Conference on Nat-
ural Language Processing, pages 102–110, Chiang
Mai, Thailand, November. Asian Federation of Nat-
ural Language Processing.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551–585.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In
Proceedings of EMNLP 2008, pages 660–669, Hon-
olulu, Hawaii.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
43.
Trevor Hastie, Robert Tibshirani, and J. H. Friedman.
2001. The elements of statistical learning: data
mining, inference, and prediction: with 200 full-
color illustrations. New York: Springer-Verlag.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004.
The (non)utility of predicate-argument frequencies
for pronoun interpretation. In Proceedings of HLT-
NAACL 2004.
M. Klenner. 2007. Enforcing coherence on corefer-
ence sets. In Proceedings of RANLP 2007.
X. Luo. 2005. On coreference resolution performance
metrics. In Proceedings of HLT-NAACL 2005, pages
25–32.
J. F. McCarthy and W. G. Lehnert. 1995. Using de-
cision trees for coreference resolution. In IJCAI,
pages 1050–1055.
T. Morton. 2000. Coreference for NLP applications.
In Proceedings ofACL 2000, Hong Kong.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Pro-
ceedings of ACL 2002, pages 104–111.
</reference>
<page confidence="0.980031">
505
</page>
<reference confidence="0.999947977272727">
V. Ng. 2005. Supervised ranking for pronoun resolu-
tion: Some recent improvements. In Proceedings of
AAAI 2005.
Cristina Nicolae and Gabriel Nicolae. 2006. Best-
cut: A graph algorithm for coreference resolution.
In EMNLP, pages 275–283.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceed-
ings of the HLT 2006, pages 192–199, New York
City, N.Y.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in ontonotes. In Joint Confer-
ence on EMNLP and CoNLL - Shared Task, pages
1–40, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Altaf Rahman and Vincent Ng. 2011. Narrowing the
modeling gap: a cluster-ranking approach to coref-
erence resolution. J. Artif. Int. Res., 40(1):469–521.
Recasens and Hovy. 2011. Blanc: Implementing the
rand index for coreference evaluation. Natural Lan-
guage Engineering, 17:485–510, 9.
W. M. Soon, H. T. Ng, and D. Lim. 2001. A
machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics,
27(4):521–544.
Olga Uryupina, Massimo Poesio, Claudio Giuliano,
and Kateryna Tymoshenko. 2011. Disambiguation
and filtering methods in using web knowledge for
coreference resolution. In FLAIRS Conference.
O. Uryupina. 2004. Linguistically motivated sample
selection for coreference resolution. In Proceedings
of DAARC 2004, Furnas.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference systems
based on kernels methods. In COLING, pages 961–
968.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proceedings fo the 6th Message
Understanding Conference (MUC-6), pages 45–52,
San Mateo, CA. Morgan Kaufmann.
</reference>
<page confidence="0.998395">
506
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.608524">
<title confidence="0.987784">Improving pairwise coreference models feature space hierarchy learning</title>
<author confidence="0.987072">Emmanuel Lassalle Pascal Denis</author>
<affiliation confidence="0.88252">Alpage Project-team Magnet Project INRIA &amp; Univ. Paris Diderot INRIA Lille - Nord</affiliation>
<address confidence="0.950497">Sorbonne Paris Cit´e, F-75205 Paris Avenue Heloise, 59650 Villeneuve d’Ascq</address>
<email confidence="0.849816">emmanuel.lassalle@ens-lyon.orgpascal.denis@inria.fr</email>
<abstract confidence="0.998289392857143">This paper proposes a new method for significantly improving the performance of pairwise coreference models. Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models. In effect, our approach finds an optimal feature space (derived from a base feature set and indicator set) for discriminating coreferential mention pairs. Although our approach explores a very large space of possible feature spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators. Our experon the Shared Task English datasets (gold mentions) indicate that our method is robust relative to different clustering strategies and evaluation metrics, showing large and consistent improvements over a single pairwise model using the same base features. Our best obtains a competitive aver- F1 over MUC, and CEAF which, despite its simplicity, places it above the mean score of other systems on these datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Ariel</author>
</authors>
<title>Referring and accessibility.</title>
<date>1988</date>
<journal>Journal of Linguistics,</journal>
<pages>65--87</pages>
<contexts>
<context position="3996" citStr="Ariel, 1988" startWordPosition="609" endWordPosition="610">5; Denis and Baldridge, 2008).2 An important justification for such spe1There are however no theoretical guarantees that improving pair classification will always result in overall improvements if the two modules are optimized independently. 2Sometimes, distinct sample selections are also adopted 497 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 497–506, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cialized models is (psycho-)linguistic and comes from theoretical findings based on salience or accessibility (Ariel, 1988). It is worth noting that, from a machine learning point of view, this is related to feature extraction in that both approaches in effect recast the pairwise classification problem in higher dimensional feature spaces. In this paper, we claim that mention pairs should not be processed by a single classifier, and instead should be handled through specific models. But we are furthermore interested in learning how to construct and select such differential models. Our argument is therefore based on statistical considerations, rather than on purely linguistic ones3. The main question we raise is, g</context>
</contexts>
<marker>Ariel, 1988</marker>
<rawString>M. Ariel. 1988. Referring and accessibility. Journal of Linguistics, pages 65–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>563--566</pages>
<contexts>
<context position="27341" citStr="Bagga and Baldwin, 1998" startWordPosition="4632" endWordPosition="4635">ther idealized setting but our focus is on comparing various pairwise local models rather than on building a full coreference resolution system. Also, we wanted to avoid having to consider too many parameters in our experiments. 5.3 Evaluation metrics We use the three metrics that are most commonly used9, namely: MUC (Vilain et al., 1995) computes for each true entity cluster the number of system clusters that are needed to cover it. Precision is this quantity divided by the true cluster size minus one. Recall is obtained by reversing true and predicated clusters. F1 is the harmonic mean. B3 (Bagga and Baldwin, 1998) computes recall and precision scores for each mention, based on the intersection between the system/true clusters for that mention. Precision is the ratio of the intersection and the true cluster sizes, while recall is the ratio of the intersection to the system cluster sizes. Global recall, precision, and F1 scores are obtained by averaging over the mention scores. CEAF (Luo, 2005) scores are obtained by computing the best one-to-one mapping between the system/true partitions, which is equivalent to finding the best optimal alignment in the bipartite graph formed out of these partitions. We </context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of LREC 1998, pages 563–566.</rawString>
</citation>
<citation valid="true">
<title>show good performances of decision trees on coreference resolution.</title>
<date>2012</date>
<marker>2012</marker>
<rawString>10(Bansal and Klein, 2012) show good performances of decision trees on coreference resolution.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Coreference semantics from web features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>389--398</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Bansal, Klein, 2012</marker>
<rawString>Mohit Bansal and Dan Klein. 2012. Coreference semantics from web features. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 389–398. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengston</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the value of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>294--303</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="2125" citStr="Bengston and Roth, 2008" startWordPosition="330" endWordPosition="333">or mentions), as they occur in a natural language text, into a set of referential entities. A common approach to this problem is to separate it into two modules: on the one hand, one defines a model for evaluating coreference links, in general a discriminative classifier that detects coreferential mention pairs. On the other hand, one designs a method for grouping the detected links into a coherent global output (i.e. a partition over the set of entity mentions). This second step is typically achieved using greedy heuristics (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Bengston and Roth, 2008), although more sophisticated clustering approaches have been used, too, such as cutting graph methods (Nicolae and Nicolae, 2006; Cai and Strube, 2010) and Integer Linear Programming (ILP) formulations (Klenner, 2007; Denis and Baldridge, 2009). Despite its simplicity, this two-step strategy remains competitive even when compared to more complex models utilizing a global loss (Bengston and Roth, 2008). In this kind of architecture, the performance of the entire coreference system strongly depends on the quality of the local pairwise classifier.1 Consequently, a lot of research effort on coref</context>
<context position="23620" citStr="Bengston and Roth, 2008" startWordPosition="4035" endWordPosition="4038">as about five times longer than with a single model. Figure 2: Cutting down the hierarchy reduces the feature space To sum up, the whole procedure is equivalent to training more than 0(2n) perceptrons simultaneously and selecting the best performing. 4 System description Our system consists in the pairwise model obtained by cutting a hierarchy (the PA with selected feature space) and using a greedy decoder to create clusters from the output. It is parametrized by the choice of the initial sequence of indicators. 4.1 The base features We used classical features that can be found in details in (Bengston and Roth, 2008) and (Rahman and Ng, 2011): grammatical type and subtype of mentions, string match and substring, apposition and copula, distance (number of separating mentions/sentences/words), gender/number match, synonymy/hypernym and animacy (using WordNet), family name (based on lists), named entity types, syntactic features (gold parse) and anaphoricity detection. 4.2 Indicators As indicators we used: left and right grammatical types and subtypes, entity types, a boolean indicating if the mentions are in the same sentence, and a very coarse histogram of distance in terms of sentences. We systematically </context>
</contexts>
<marker>Bengston, Roth, 2008</marker>
<rawString>Eric Bengston and Dan Roth. 2008. Understanding the value of features for coreference resolution. In Proceedings of EMNLP 2008, pages 294–303, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Cai</author>
<author>Michael Strube</author>
</authors>
<title>End-to-end coreference resolution via hypergraph partitioning.</title>
<date>2010</date>
<booktitle>In COLING,</booktitle>
<pages>143--151</pages>
<contexts>
<context position="2277" citStr="Cai and Strube, 2010" startWordPosition="353" endWordPosition="356">dules: on the one hand, one defines a model for evaluating coreference links, in general a discriminative classifier that detects coreferential mention pairs. On the other hand, one designs a method for grouping the detected links into a coherent global output (i.e. a partition over the set of entity mentions). This second step is typically achieved using greedy heuristics (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Bengston and Roth, 2008), although more sophisticated clustering approaches have been used, too, such as cutting graph methods (Nicolae and Nicolae, 2006; Cai and Strube, 2010) and Integer Linear Programming (ILP) formulations (Klenner, 2007; Denis and Baldridge, 2009). Despite its simplicity, this two-step strategy remains competitive even when compared to more complex models utilizing a global loss (Bengston and Roth, 2008). In this kind of architecture, the performance of the entire coreference system strongly depends on the quality of the local pairwise classifier.1 Consequently, a lot of research effort on coreference resolution has focused on trying to boost the performance of the pairwise classifier. Numerous studies are concerned with feature extraction, typ</context>
</contexts>
<marker>Cai, Strube, 2010</marker>
<rawString>Jie Cai and Michael Strube. 2010. End-to-end coreference resolution via hypergraph partitioning. In COLING, pages 143–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Chen</author>
<author>Jian Su</author>
<author>Sinno Jialin Pan</author>
<author>Chew Lim Tan</author>
</authors>
<title>A unified event coreference resolution by integrating multiple resolvers.</title>
<date>2011</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>102--110</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="14163" citStr="Chen et al., 2011" startWordPosition="2376" endWordPosition="2379">t2 is wt+1 =Wt where the wit+1 correwt+1 spond to the updates in space Fi independently from the rest. This result can be extended easily to the case of n feature spaces. Thus, with a deterministic separation of the data, a large model can be learned using smaller independent models. 2.3 An example: separation by gramtype To motivate our approach, we first introduce a simple separation of mention pairs which creates 9 models obtained by considering all possible pairs of grammatical types {nominal, name, pronoun} for both mentions in the pair (a similar fine-grained separation can be found in (Chen et al., 2011)). This is equivalent to using 9 different feature spaces F1,...,F9 to capture the global distribution of pairs. With the PA, this is also a single model with feature space F = F1 ⊕ · · · ⊕ F9. We will call it the GRAMTYPE model. As we will see in Section 5, these separated models significantly outperform a single model 4The parameter is updated to obtain a margin of a least 1. It does not change if the instance is already correctly classified with such margin. that uses the same base feature set. But we would like to define a method that adapts a feature space to the data by choosing the most</context>
</contexts>
<marker>Chen, Su, Pan, Tan, 2011</marker>
<rawString>Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim Tan. 2011. A unified event coreference resolution by integrating multiple resolvers. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 102–110, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="5976" citStr="Crammer et al., 2006" startWordPosition="938" endWordPosition="941">s for various other types of indicators) or to the sole typing of the anaphoric mention (our models can also be specific to a particular type antecedent or to the two types of the mention pair). More importantly, we propose an original method for learning the best set of models that can be built from a given set of indicators and a training set. These models are organized in a hierarchy, wherein each leaf corresponds to a mutually disjoint subset of mention pair examples and the classifier that can be trained from it. Our models are trained using the Online Passive-Aggressive algorithm or PA (Crammer et al., 2006), a large margin version of the perceptron. Our method is exact in that it explores the full space of hierarchies (of size at least 22&amp;quot;) definable on an indicator sequence, while remaining scalable by exploiting the particular structure of these during the training of the distinct local models (Ng and Cardie, 2002; Uryupina, 2004). 3However it should be underlined that the statistical viewpoint is complementary to the linguistic work. hierarchies with dynamic programming. This approach also performs well, and it largely outperforms the single model. As will be shown based on a variety of exper</context>
<context position="12550" citStr="Crammer et al., 2006" startWordPosition="2062" endWordPosition="2065">actice, we have to cope with data sparsity: there will not be enough data to properly train a linear model on such a space. Finally, we seek a feature space situated between the two extremes of a space that is too big (sparseness) or too small (noisy data). The core of this work is to define a general method for choosing the most adequate space F among a huge number of possibilities when we do not know a priori which is the best. 2.2.3 Linear models In this work, we try to linearly separate positive and negative instances in the large space F with the Online Passive-Aggressive (PA) algorithm (Crammer et al., 2006): the model learns a parameter vector w that defines a hyperplane that cuts the space into two parts. The predicted class of a pair x with feature vector φF(x) is given by: CF(x) := sign(wT · φF(x)) Linearity implies an equivalence between: (i) separating instances of two types, t1 and t2, in two 499 independent models with respective feature spaces F1 and F2 and parameters w1 and w2, and (ii) a single model on F1⊕F2. To see why, let us define the map: O.T1®.T2(x) := { ~ ~T O.T1(x)T 0 if x typed t1 T ~ 0 O.T2 (x)T ~ if x typed t2 � w1 � and the parameter vector w = ∈ F1 ⊕ w2 F2. Then we have: </context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Specialized models and ranking for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP 2008,</booktitle>
<pages>660--669</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="3413" citStr="Denis and Baldridge, 2008" startWordPosition="527" endWordPosition="530">ce of the pairwise classifier. Numerous studies are concerned with feature extraction, typically trying to enrich the classifier with more linguistic knowledge and/or more world knowledge (Ng and Cardie, 2002; Kehler et al., 2004; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Versley et al., 2008; Uryupina et al., 2011). A second line of work explores the use of distinct local models for different types of mentions, specifically for different types of anaphoric mentions based on their grammatical categories (such as pronouns, proper names, definite descriptions) (Morton, 2000; Ng, 2005; Denis and Baldridge, 2008).2 An important justification for such spe1There are however no theoretical guarantees that improving pair classification will always result in overall improvements if the two modules are optimized independently. 2Sometimes, distinct sample selections are also adopted 497 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 497–506, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cialized models is (psycho-)linguistic and comes from theoretical findings based on salience or accessibility (Ariel, 1988). It is worth not</context>
</contexts>
<marker>Denis, Baldridge, 2008</marker>
<rawString>Pascal Denis and Jason Baldridge. 2008. Specialized models and ranking for coreference resolution. In Proceedings of EMNLP 2008, pages 660–669, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Global joint models for coreference resolution and named entity classification.</title>
<date>2009</date>
<booktitle>Procesamiento del Lenguaje Natural,</booktitle>
<pages>43</pages>
<contexts>
<context position="2370" citStr="Denis and Baldridge, 2009" startWordPosition="365" endWordPosition="368"> a discriminative classifier that detects coreferential mention pairs. On the other hand, one designs a method for grouping the detected links into a coherent global output (i.e. a partition over the set of entity mentions). This second step is typically achieved using greedy heuristics (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Bengston and Roth, 2008), although more sophisticated clustering approaches have been used, too, such as cutting graph methods (Nicolae and Nicolae, 2006; Cai and Strube, 2010) and Integer Linear Programming (ILP) formulations (Klenner, 2007; Denis and Baldridge, 2009). Despite its simplicity, this two-step strategy remains competitive even when compared to more complex models utilizing a global loss (Bengston and Roth, 2008). In this kind of architecture, the performance of the entire coreference system strongly depends on the quality of the local pairwise classifier.1 Consequently, a lot of research effort on coreference resolution has focused on trying to boost the performance of the pairwise classifier. Numerous studies are concerned with feature extraction, typically trying to enrich the classifier with more linguistic knowledge and/or more world knowl</context>
</contexts>
<marker>Denis, Baldridge, 2009</marker>
<rawString>Pascal Denis and Jason Baldridge. 2009. Global joint models for coreference resolution and named entity classification. Procesamiento del Lenguaje Natural, 43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
<author>J H Friedman</author>
</authors>
<title>The elements of statistical learning: data mining, inference, and prediction: with 200 fullcolor illustrations.</title>
<date>2001</date>
<publisher>Springer-Verlag.</publisher>
<location>New York:</location>
<marker>Hastie, Tibshirani, Friedman, 2001</marker>
<rawString>Trevor Hastie, Robert Tibshirani, and J. H. Friedman. 2001. The elements of statistical learning: data mining, inference, and prediction: with 200 fullcolor illustrations. New York: Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
<author>D Appelt</author>
<author>L Taylor</author>
<author>A Simma</author>
</authors>
<title>The (non)utility of predicate-argument frequencies for pronoun interpretation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLTNAACL</booktitle>
<contexts>
<context position="3016" citStr="Kehler et al., 2004" startWordPosition="465" endWordPosition="468">his two-step strategy remains competitive even when compared to more complex models utilizing a global loss (Bengston and Roth, 2008). In this kind of architecture, the performance of the entire coreference system strongly depends on the quality of the local pairwise classifier.1 Consequently, a lot of research effort on coreference resolution has focused on trying to boost the performance of the pairwise classifier. Numerous studies are concerned with feature extraction, typically trying to enrich the classifier with more linguistic knowledge and/or more world knowledge (Ng and Cardie, 2002; Kehler et al., 2004; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Versley et al., 2008; Uryupina et al., 2011). A second line of work explores the use of distinct local models for different types of mentions, specifically for different types of anaphoric mentions based on their grammatical categories (such as pronouns, proper names, definite descriptions) (Morton, 2000; Ng, 2005; Denis and Baldridge, 2008).2 An important justification for such spe1There are however no theoretical guarantees that improving pair classification will always result in overall improvements if the two modules are optimized indep</context>
</contexts>
<marker>Kehler, Appelt, Taylor, Simma, 2004</marker>
<rawString>A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The (non)utility of predicate-argument frequencies for pronoun interpretation. In Proceedings of HLTNAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Klenner</author>
</authors>
<title>Enforcing coherence on coreference sets.</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP</booktitle>
<contexts>
<context position="2342" citStr="Klenner, 2007" startWordPosition="363" endWordPosition="364">nks, in general a discriminative classifier that detects coreferential mention pairs. On the other hand, one designs a method for grouping the detected links into a coherent global output (i.e. a partition over the set of entity mentions). This second step is typically achieved using greedy heuristics (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Bengston and Roth, 2008), although more sophisticated clustering approaches have been used, too, such as cutting graph methods (Nicolae and Nicolae, 2006; Cai and Strube, 2010) and Integer Linear Programming (ILP) formulations (Klenner, 2007; Denis and Baldridge, 2009). Despite its simplicity, this two-step strategy remains competitive even when compared to more complex models utilizing a global loss (Bengston and Roth, 2008). In this kind of architecture, the performance of the entire coreference system strongly depends on the quality of the local pairwise classifier.1 Consequently, a lot of research effort on coreference resolution has focused on trying to boost the performance of the pairwise classifier. Numerous studies are concerned with feature extraction, typically trying to enrich the classifier with more linguistic knowl</context>
</contexts>
<marker>Klenner, 2007</marker>
<rawString>M. Klenner. 2007. Enforcing coherence on coreference sets. In Proceedings of RANLP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>25--32</pages>
<contexts>
<context position="27727" citStr="Luo, 2005" startWordPosition="4696" endWordPosition="4697">clusters that are needed to cover it. Precision is this quantity divided by the true cluster size minus one. Recall is obtained by reversing true and predicated clusters. F1 is the harmonic mean. B3 (Bagga and Baldwin, 1998) computes recall and precision scores for each mention, based on the intersection between the system/true clusters for that mention. Precision is the ratio of the intersection and the true cluster sizes, while recall is the ratio of the intersection to the system cluster sizes. Global recall, precision, and F1 scores are obtained by averaging over the mention scores. CEAF (Luo, 2005) scores are obtained by computing the best one-to-one mapping between the system/true partitions, which is equivalent to finding the best optimal alignment in the bipartite graph formed out of these partitions. We use the φ4 similarity function from (Luo, 2005). These metrics were recently used in the CoNLL2011 and -2012 Shared Tasks. In addition, these campaigns use an unweighted average over the F1 scores given by the three metrics. Following common practice, we use micro-averaging when reporting our scores for entire datasets. 5.4 Results The results obtained by the system are reported in t</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>X. Luo. 2005. On coreference resolution performance metrics. In Proceedings of HLT-NAACL 2005, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F McCarthy</author>
<author>W G Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution. In</title>
<date>1995</date>
<booktitle>IJCAI,</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="2059" citStr="McCarthy and Lehnert, 1995" startWordPosition="318" endWordPosition="321">solution is the problem of partitioning a sequence of noun phrases (or mentions), as they occur in a natural language text, into a set of referential entities. A common approach to this problem is to separate it into two modules: on the one hand, one defines a model for evaluating coreference links, in general a discriminative classifier that detects coreferential mention pairs. On the other hand, one designs a method for grouping the detected links into a coherent global output (i.e. a partition over the set of entity mentions). This second step is typically achieved using greedy heuristics (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Bengston and Roth, 2008), although more sophisticated clustering approaches have been used, too, such as cutting graph methods (Nicolae and Nicolae, 2006; Cai and Strube, 2010) and Integer Linear Programming (ILP) formulations (Klenner, 2007; Denis and Baldridge, 2009). Despite its simplicity, this two-step strategy remains competitive even when compared to more complex models utilizing a global loss (Bengston and Roth, 2008). In this kind of architecture, the performance of the entire coreference system strongly depends on the quality of the local pai</context>
<context position="24923" citStr="McCarthy and Lehnert, 1995" startWordPosition="4232" endWordPosition="4235">ndicators, producing sequences of different lengths. The parameter was optimized by document categories using a development set after decoding the output of the pairwise model. 4.3 Decoders We tested 3 classical greedy link selection strategies that form clusters from the classifier decision: Closest-First (merge mentions with their closest coreferent mention on the left) (Soon et al., 2001), 502 Best-first (merge mentions with the mention on the left having the highest positive score) (Ng and Cardie, 2002; Bengston and Roth, 2008), and Aggressive-Merge (transitive closure on positive pairs) (McCarthy and Lehnert, 1995). Each of these decoders is typically (although not always) used in tandem with a specific sampling selection at training. Thus, Closest-First for instance is used in combination with a sample selection that generates training instances only for the mentions that occur between the closest antecedent and the anaphor (Soon et al., 2001). P R F1 SINGLE MODEL 22.28 63.50 32.99 RIGHT-TYPE 29.31 45.23 35.58 GRAMTYPE 39.12 45.83 42.21 BEST HIERARCHY 45.27 51.98 48.40 Table 1: Pairwise scores on CoNLL-2012 test. 5 Experiments 5.1 Data We evaluated the system on the English part of the corpus provided </context>
</contexts>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>J. F. McCarthy and W. G. Lehnert. 1995. Using decision trees for coreference resolution. In IJCAI, pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Morton</author>
</authors>
<title>Coreference for NLP applications.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="3375" citStr="Morton, 2000" startWordPosition="523" endWordPosition="524">g to boost the performance of the pairwise classifier. Numerous studies are concerned with feature extraction, typically trying to enrich the classifier with more linguistic knowledge and/or more world knowledge (Ng and Cardie, 2002; Kehler et al., 2004; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Versley et al., 2008; Uryupina et al., 2011). A second line of work explores the use of distinct local models for different types of mentions, specifically for different types of anaphoric mentions based on their grammatical categories (such as pronouns, proper names, definite descriptions) (Morton, 2000; Ng, 2005; Denis and Baldridge, 2008).2 An important justification for such spe1There are however no theoretical guarantees that improving pair classification will always result in overall improvements if the two modules are optimized independently. 2Sometimes, distinct sample selections are also adopted 497 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 497–506, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cialized models is (psycho-)linguistic and comes from theoretical findings based on salience or access</context>
<context position="26042" citStr="Morton, 2000" startWordPosition="4418" endWordPosition="4419">st. 5 Experiments 5.1 Data We evaluated the system on the English part of the corpus provided in the CoNLL-2012 Shared Task (Pradhan et al., 2012), referred to as CoNLL-2012 here. The corpus contains 7 categories of documents (over 2K documents, 1.3M words). We used the official train/dev/test data sets. We evaluated our system in the closed mode which requires that only provided data is used. 5.2 Settings Our baselines are a SINGLE MODEL, the GRAMTYPE model (section 2) and a RIGHT-TYPE model, defined as the first level of the gramtype product hierarchy (i.e. grammatical type of the anaphora (Morton, 2000)), with each greedy decoder and also the original sampling with a single model associated with those decoders. The hierarchies were trained with 10-fold crossvalidation on the training set (the hierarchies are cut after cumulating the scores obtained by crossvalidation) and their parameters are optimized by document category on the development set: the sequence of indicators obtaining the best average score after decoding was selected as parameter for the category. The obtained hierarchy is referred to as the BEST HIERARCHY in the results. We fixed the number of iterations for the PA for all m</context>
</contexts>
<marker>Morton, 2000</marker>
<rawString>T. Morton. 2000. Coreference for NLP applications. In Proceedings ofACL 2000, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>104--111</pages>
<contexts>
<context position="2099" citStr="Ng and Cardie, 2002" startWordPosition="326" endWordPosition="329">nce of noun phrases (or mentions), as they occur in a natural language text, into a set of referential entities. A common approach to this problem is to separate it into two modules: on the one hand, one defines a model for evaluating coreference links, in general a discriminative classifier that detects coreferential mention pairs. On the other hand, one designs a method for grouping the detected links into a coherent global output (i.e. a partition over the set of entity mentions). This second step is typically achieved using greedy heuristics (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Bengston and Roth, 2008), although more sophisticated clustering approaches have been used, too, such as cutting graph methods (Nicolae and Nicolae, 2006; Cai and Strube, 2010) and Integer Linear Programming (ILP) formulations (Klenner, 2007; Denis and Baldridge, 2009). Despite its simplicity, this two-step strategy remains competitive even when compared to more complex models utilizing a global loss (Bengston and Roth, 2008). In this kind of architecture, the performance of the entire coreference system strongly depends on the quality of the local pairwise classifier.1 Consequently, a lot o</context>
<context position="6291" citStr="Ng and Cardie, 2002" startWordPosition="992" endWordPosition="995">indicators and a training set. These models are organized in a hierarchy, wherein each leaf corresponds to a mutually disjoint subset of mention pair examples and the classifier that can be trained from it. Our models are trained using the Online Passive-Aggressive algorithm or PA (Crammer et al., 2006), a large margin version of the perceptron. Our method is exact in that it explores the full space of hierarchies (of size at least 22&amp;quot;) definable on an indicator sequence, while remaining scalable by exploiting the particular structure of these during the training of the distinct local models (Ng and Cardie, 2002; Uryupina, 2004). 3However it should be underlined that the statistical viewpoint is complementary to the linguistic work. hierarchies with dynamic programming. This approach also performs well, and it largely outperforms the single model. As will be shown based on a variety of experiments on the CoNLL-2012 Shared Task English datasets, these improvements are consistent across different evaluation metrics and for the most part independent of the clustering decoder that was used. The rest of this paper is organized as follows. Section 2 discusses the underlying statistical hypotheses of the st</context>
<context position="24807" citStr="Ng and Cardie, 2002" startWordPosition="4217" endWordPosition="4220">of sentences. We systematically included right gramtype and left gramtype in the sequences and added other indicators, producing sequences of different lengths. The parameter was optimized by document categories using a development set after decoding the output of the pairwise model. 4.3 Decoders We tested 3 classical greedy link selection strategies that form clusters from the classifier decision: Closest-First (merge mentions with their closest coreferent mention on the left) (Soon et al., 2001), 502 Best-first (merge mentions with the mention on the left having the highest positive score) (Ng and Cardie, 2002; Bengston and Roth, 2008), and Aggressive-Merge (transitive closure on positive pairs) (McCarthy and Lehnert, 1995). Each of these decoders is typically (although not always) used in tandem with a specific sampling selection at training. Thus, Closest-First for instance is used in combination with a sample selection that generates training instances only for the mentions that occur between the closest antecedent and the anaphor (Soon et al., 2001). P R F1 SINGLE MODEL 22.28 63.50 32.99 RIGHT-TYPE 29.31 45.23 35.58 GRAMTYPE 39.12 45.83 42.21 BEST HIERARCHY 45.27 51.98 48.40 Table 1: Pairwise s</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of ACL 2002, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Supervised ranking for pronoun resolution: Some recent improvements.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI</booktitle>
<contexts>
<context position="3385" citStr="Ng, 2005" startWordPosition="525" endWordPosition="526"> performance of the pairwise classifier. Numerous studies are concerned with feature extraction, typically trying to enrich the classifier with more linguistic knowledge and/or more world knowledge (Ng and Cardie, 2002; Kehler et al., 2004; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Versley et al., 2008; Uryupina et al., 2011). A second line of work explores the use of distinct local models for different types of mentions, specifically for different types of anaphoric mentions based on their grammatical categories (such as pronouns, proper names, definite descriptions) (Morton, 2000; Ng, 2005; Denis and Baldridge, 2008).2 An important justification for such spe1There are however no theoretical guarantees that improving pair classification will always result in overall improvements if the two modules are optimized independently. 2Sometimes, distinct sample selections are also adopted 497 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 497–506, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cialized models is (psycho-)linguistic and comes from theoretical findings based on salience or accessibility (A</context>
</contexts>
<marker>Ng, 2005</marker>
<rawString>V. Ng. 2005. Supervised ranking for pronoun resolution: Some recent improvements. In Proceedings of AAAI 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Nicolae</author>
<author>Gabriel Nicolae</author>
</authors>
<title>Bestcut: A graph algorithm for coreference resolution. In</title>
<date>2006</date>
<booktitle>EMNLP,</booktitle>
<pages>275--283</pages>
<contexts>
<context position="2254" citStr="Nicolae and Nicolae, 2006" startWordPosition="349" endWordPosition="352"> to separate it into two modules: on the one hand, one defines a model for evaluating coreference links, in general a discriminative classifier that detects coreferential mention pairs. On the other hand, one designs a method for grouping the detected links into a coherent global output (i.e. a partition over the set of entity mentions). This second step is typically achieved using greedy heuristics (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Bengston and Roth, 2008), although more sophisticated clustering approaches have been used, too, such as cutting graph methods (Nicolae and Nicolae, 2006; Cai and Strube, 2010) and Integer Linear Programming (ILP) formulations (Klenner, 2007; Denis and Baldridge, 2009). Despite its simplicity, this two-step strategy remains competitive even when compared to more complex models utilizing a global loss (Bengston and Roth, 2008). In this kind of architecture, the performance of the entire coreference system strongly depends on the quality of the local pairwise classifier.1 Consequently, a lot of research effort on coreference resolution has focused on trying to boost the performance of the pairwise classifier. Numerous studies are concerned with </context>
</contexts>
<marker>Nicolae, Nicolae, 2006</marker>
<rawString>Cristina Nicolae and Gabriel Nicolae. 2006. Bestcut: A graph algorithm for coreference resolution. In EMNLP, pages 275–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT</booktitle>
<pages>192--199</pages>
<location>New York City, N.Y.</location>
<contexts>
<context position="3043" citStr="Ponzetto and Strube, 2006" startWordPosition="469" endWordPosition="472"> remains competitive even when compared to more complex models utilizing a global loss (Bengston and Roth, 2008). In this kind of architecture, the performance of the entire coreference system strongly depends on the quality of the local pairwise classifier.1 Consequently, a lot of research effort on coreference resolution has focused on trying to boost the performance of the pairwise classifier. Numerous studies are concerned with feature extraction, typically trying to enrich the classifier with more linguistic knowledge and/or more world knowledge (Ng and Cardie, 2002; Kehler et al., 2004; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Versley et al., 2008; Uryupina et al., 2011). A second line of work explores the use of distinct local models for different types of mentions, specifically for different types of anaphoric mentions based on their grammatical categories (such as pronouns, proper names, definite descriptions) (Morton, 2000; Ng, 2005; Denis and Baldridge, 2008).2 An important justification for such spe1There are however no theoretical guarantees that improving pair classification will always result in overall improvements if the two modules are optimized independently. 2Sometimes, disti</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proceedings of the HLT 2006, pages 192–199, New York City, N.Y.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes.</title>
<date>2012</date>
<booktitle>In Joint Conference on EMNLP and CoNLL - Shared Task,</booktitle>
<pages>1--40</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="25575" citStr="Pradhan et al., 2012" startWordPosition="4338" endWordPosition="4341">ically (although not always) used in tandem with a specific sampling selection at training. Thus, Closest-First for instance is used in combination with a sample selection that generates training instances only for the mentions that occur between the closest antecedent and the anaphor (Soon et al., 2001). P R F1 SINGLE MODEL 22.28 63.50 32.99 RIGHT-TYPE 29.31 45.23 35.58 GRAMTYPE 39.12 45.83 42.21 BEST HIERARCHY 45.27 51.98 48.40 Table 1: Pairwise scores on CoNLL-2012 test. 5 Experiments 5.1 Data We evaluated the system on the English part of the corpus provided in the CoNLL-2012 Shared Task (Pradhan et al., 2012), referred to as CoNLL-2012 here. The corpus contains 7 categories of documents (over 2K documents, 1.3M words). We used the official train/dev/test data sets. We evaluated our system in the closed mode which requires that only provided data is used. 5.2 Settings Our baselines are a SINGLE MODEL, the GRAMTYPE model (section 2) and a RIGHT-TYPE model, defined as the first level of the gramtype product hierarchy (i.e. grammatical type of the anaphora (Morton, 2000)), with each greedy decoder and also the original sampling with a single model associated with those decoders. The hierarchies were t</context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes. In Joint Conference on EMNLP and CoNLL - Shared Task, pages 1–40, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Narrowing the modeling gap: a cluster-ranking approach to coreference resolution.</title>
<date>2011</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="23646" citStr="Rahman and Ng, 2011" startWordPosition="4040" endWordPosition="4044">n with a single model. Figure 2: Cutting down the hierarchy reduces the feature space To sum up, the whole procedure is equivalent to training more than 0(2n) perceptrons simultaneously and selecting the best performing. 4 System description Our system consists in the pairwise model obtained by cutting a hierarchy (the PA with selected feature space) and using a greedy decoder to create clusters from the output. It is parametrized by the choice of the initial sequence of indicators. 4.1 The base features We used classical features that can be found in details in (Bengston and Roth, 2008) and (Rahman and Ng, 2011): grammatical type and subtype of mentions, string match and substring, apposition and copula, distance (number of separating mentions/sentences/words), gender/number match, synonymy/hypernym and animacy (using WordNet), family name (based on lists), named entity types, syntactic features (gold parse) and anaphoricity detection. 4.2 Indicators As indicators we used: left and right grammatical types and subtypes, entity types, a boolean indicating if the mentions are in the same sentence, and a very coarse histogram of distance in terms of sentences. We systematically included right gramtype an</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011. Narrowing the modeling gap: a cluster-ranking approach to coreference resolution. J. Artif. Int. Res., 40(1):469–521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Recasens</author>
<author>Hovy</author>
</authors>
<title>Blanc: Implementing the rand index for coreference evaluation.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<pages>9</pages>
<contexts>
<context position="28727" citStr="Recasens and Hovy, 2011" startWordPosition="4855" endWordPosition="4858">eighted average over the F1 scores given by the three metrics. Following common practice, we use micro-averaging when reporting our scores for entire datasets. 5.4 Results The results obtained by the system are reported in table 2. The original sampling for the single model associated to Closest-First and Best-First decoder are referred to as SOON and NGCARDIE. The P/R/F1 pairwise scores before decoding are given in table 1. BEST HIERARCHY obtains a strong improvement in F1 (+15), abetter precision and a less significant diminution of recall compared to GRAMTYPE and RIGHT-TYPE. 9BLANC metric (Recasens and Hovy, 2011) results are not reported since they are not used to compute the CoNLL2012 global score. However we can mention that in our experiments, using hierarchies had a positive effect similar to what was observed on B3 and CEAF. 503 MUC B3 CEAF Closest-First P R F1 P R F1 P R F1 Mean SOON 79.49 93.72 86.02 26.23 89.43 40.56 49.74 19.92 28.44 51.67 SINGLE MODEL 78.95 75.15 77.0 51.88 68.42 59.01 37.79 43.89 40.61 58.87 RIGHT-TYPE 79.36 67.57 72.99 69.43 56.78 62.47 41.17 61.66 49.37 61.61 GRAMTYPE 80.5 71.12 75.52 66.39 61.04 63.6 43.11 59.93 50.15 63.09 BEST HIERARCHY 83.23 73.72 78.19 73.5 67.09 70.</context>
</contexts>
<marker>Recasens, Hovy, 2011</marker>
<rawString>Recasens and Hovy. 2011. Blanc: Implementing the rand index for coreference evaluation. Natural Language Engineering, 17:485–510, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="2078" citStr="Soon et al., 2001" startWordPosition="322" endWordPosition="325">artitioning a sequence of noun phrases (or mentions), as they occur in a natural language text, into a set of referential entities. A common approach to this problem is to separate it into two modules: on the one hand, one defines a model for evaluating coreference links, in general a discriminative classifier that detects coreferential mention pairs. On the other hand, one designs a method for grouping the detected links into a coherent global output (i.e. a partition over the set of entity mentions). This second step is typically achieved using greedy heuristics (McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Bengston and Roth, 2008), although more sophisticated clustering approaches have been used, too, such as cutting graph methods (Nicolae and Nicolae, 2006; Cai and Strube, 2010) and Integer Linear Programming (ILP) formulations (Klenner, 2007; Denis and Baldridge, 2009). Despite its simplicity, this two-step strategy remains competitive even when compared to more complex models utilizing a global loss (Bengston and Roth, 2008). In this kind of architecture, the performance of the entire coreference system strongly depends on the quality of the local pairwise classifier.1 </context>
<context position="24690" citStr="Soon et al., 2001" startWordPosition="4198" endWordPosition="4201">es, a boolean indicating if the mentions are in the same sentence, and a very coarse histogram of distance in terms of sentences. We systematically included right gramtype and left gramtype in the sequences and added other indicators, producing sequences of different lengths. The parameter was optimized by document categories using a development set after decoding the output of the pairwise model. 4.3 Decoders We tested 3 classical greedy link selection strategies that form clusters from the classifier decision: Closest-First (merge mentions with their closest coreferent mention on the left) (Soon et al., 2001), 502 Best-first (merge mentions with the mention on the left having the highest positive score) (Ng and Cardie, 2002; Bengston and Roth, 2008), and Aggressive-Merge (transitive closure on positive pairs) (McCarthy and Lehnert, 1995). Each of these decoders is typically (although not always) used in tandem with a specific sampling selection at training. Thus, Closest-First for instance is used in combination with a sample selection that generates training instances only for the mentions that occur between the closest antecedent and the anaphor (Soon et al., 2001). P R F1 SINGLE MODEL 22.28 63.</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. M. Soon, H. T. Ng, and D. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
<author>Massimo Poesio</author>
<author>Claudio Giuliano</author>
<author>Kateryna Tymoshenko</author>
</authors>
<title>Disambiguation and filtering methods in using web knowledge for coreference resolution.</title>
<date>2011</date>
<booktitle>In FLAIRS Conference.</booktitle>
<contexts>
<context position="3114" citStr="Uryupina et al., 2011" startWordPosition="481" endWordPosition="484"> global loss (Bengston and Roth, 2008). In this kind of architecture, the performance of the entire coreference system strongly depends on the quality of the local pairwise classifier.1 Consequently, a lot of research effort on coreference resolution has focused on trying to boost the performance of the pairwise classifier. Numerous studies are concerned with feature extraction, typically trying to enrich the classifier with more linguistic knowledge and/or more world knowledge (Ng and Cardie, 2002; Kehler et al., 2004; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Versley et al., 2008; Uryupina et al., 2011). A second line of work explores the use of distinct local models for different types of mentions, specifically for different types of anaphoric mentions based on their grammatical categories (such as pronouns, proper names, definite descriptions) (Morton, 2000; Ng, 2005; Denis and Baldridge, 2008).2 An important justification for such spe1There are however no theoretical guarantees that improving pair classification will always result in overall improvements if the two modules are optimized independently. 2Sometimes, distinct sample selections are also adopted 497 Proceedings of the 51st Annu</context>
</contexts>
<marker>Uryupina, Poesio, Giuliano, Tymoshenko, 2011</marker>
<rawString>Olga Uryupina, Massimo Poesio, Claudio Giuliano, and Kateryna Tymoshenko. 2011. Disambiguation and filtering methods in using web knowledge for coreference resolution. In FLAIRS Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Uryupina</author>
</authors>
<title>Linguistically motivated sample selection for coreference resolution.</title>
<date>2004</date>
<booktitle>In Proceedings of DAARC 2004,</booktitle>
<location>Furnas.</location>
<contexts>
<context position="6308" citStr="Uryupina, 2004" startWordPosition="996" endWordPosition="997">ning set. These models are organized in a hierarchy, wherein each leaf corresponds to a mutually disjoint subset of mention pair examples and the classifier that can be trained from it. Our models are trained using the Online Passive-Aggressive algorithm or PA (Crammer et al., 2006), a large margin version of the perceptron. Our method is exact in that it explores the full space of hierarchies (of size at least 22&amp;quot;) definable on an indicator sequence, while remaining scalable by exploiting the particular structure of these during the training of the distinct local models (Ng and Cardie, 2002; Uryupina, 2004). 3However it should be underlined that the statistical viewpoint is complementary to the linguistic work. hierarchies with dynamic programming. This approach also performs well, and it largely outperforms the single model. As will be shown based on a variety of experiments on the CoNLL-2012 Shared Task English datasets, these improvements are consistent across different evaluation metrics and for the most part independent of the clustering decoder that was used. The rest of this paper is organized as follows. Section 2 discusses the underlying statistical hypotheses of the standard pairwise m</context>
</contexts>
<marker>Uryupina, 2004</marker>
<rawString>O. Uryupina. 2004. Linguistically motivated sample selection for coreference resolution. In Proceedings of DAARC 2004, Furnas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
<author>Alessandro Moschitti</author>
<author>Massimo Poesio</author>
<author>Xiaofeng Yang</author>
</authors>
<title>Coreference systems based on kernels methods.</title>
<date>2008</date>
<booktitle>In COLING,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="3090" citStr="Versley et al., 2008" startWordPosition="477" endWordPosition="480">lex models utilizing a global loss (Bengston and Roth, 2008). In this kind of architecture, the performance of the entire coreference system strongly depends on the quality of the local pairwise classifier.1 Consequently, a lot of research effort on coreference resolution has focused on trying to boost the performance of the pairwise classifier. Numerous studies are concerned with feature extraction, typically trying to enrich the classifier with more linguistic knowledge and/or more world knowledge (Ng and Cardie, 2002; Kehler et al., 2004; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Versley et al., 2008; Uryupina et al., 2011). A second line of work explores the use of distinct local models for different types of mentions, specifically for different types of anaphoric mentions based on their grammatical categories (such as pronouns, proper names, definite descriptions) (Morton, 2000; Ng, 2005; Denis and Baldridge, 2008).2 An important justification for such spe1There are however no theoretical guarantees that improving pair classification will always result in overall improvements if the two modules are optimized independently. 2Sometimes, distinct sample selections are also adopted 497 Proc</context>
</contexts>
<marker>Versley, Moschitti, Poesio, Yang, 2008</marker>
<rawString>Yannick Versley, Alessandro Moschitti, Massimo Poesio, and Xiaofeng Yang. 2008. Coreference systems based on kernels methods. In COLING, pages 961– 968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings fo the 6th Message Understanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="27057" citStr="Vilain et al., 1995" startWordPosition="4582" endWordPosition="4585">t average score after decoding was selected as parameter for the category. The obtained hierarchy is referred to as the BEST HIERARCHY in the results. We fixed the number of iterations for the PA for all models. In our experiments, we consider only the gold mentions. This is a rather idealized setting but our focus is on comparing various pairwise local models rather than on building a full coreference resolution system. Also, we wanted to avoid having to consider too many parameters in our experiments. 5.3 Evaluation metrics We use the three metrics that are most commonly used9, namely: MUC (Vilain et al., 1995) computes for each true entity cluster the number of system clusters that are needed to cover it. Precision is this quantity divided by the true cluster size minus one. Recall is obtained by reversing true and predicated clusters. F1 is the harmonic mean. B3 (Bagga and Baldwin, 1998) computes recall and precision scores for each mention, based on the intersection between the system/true clusters for that mention. Precision is the ratio of the intersection and the true cluster sizes, while recall is the ratio of the intersection to the system cluster sizes. Global recall, precision, and F1 scor</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proceedings fo the 6th Message Understanding Conference (MUC-6), pages 45–52, San Mateo, CA. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>