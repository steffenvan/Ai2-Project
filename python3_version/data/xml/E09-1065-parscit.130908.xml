<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<title confidence="0.993855">
Text-to-text Semantic Similarity for Automatic Short Answer Grading
</title>
<author confidence="0.998189">
Michael Mohler and Rada Mihalcea
</author>
<affiliation confidence="0.999672">
Department of Computer Science
University of North Texas
</affiliation>
<email confidence="0.998689">
mgm0038@unt.edu, rada@cs.unt.edu
</email>
<sectionHeader confidence="0.993888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966428571429">
In this paper, we explore unsupervised
techniques for the task of automatic short
answer grading. We compare a number of
knowledge-based and corpus-based mea-
sures of text similarity, evaluate the effect
of domain and size on the corpus-based
measures, and also introduce a novel tech-
nique to improve the performance of the
system by integrating automatic feedback
from the student answers. Overall, our
system significantly and consistently out-
performs other unsupervised methods for
short answer grading that have been pro-
posed in the past.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969677419355">
One of the most important aspects of the learn-
ing process is the assessment of the knowledge
acquired by the learner. In a typical examination
setting (e.g., an exam, assignment or quiz), this
assessment implies an instructor or a grader who
provides students with feedback on their answers
to questions that are related to the subject mat-
ter. There are, however, certain scenarios, such
as the large number of worldwide sites with lim-
ited teacher availability, or the individual or group
study sessions done outside of class, in which an
instructor is not available and yet students need an
assessment of their knowledge of the subject. In
these instances, we often have to turn to computer-
assisted assessment.
While some forms of computer-assisted assess-
ment do not require sophisticated text understand-
ing (e.g., multiple choice or true/false questions
can be easily graded by a system if the correct so-
lution is available), there are also student answers
that consist of free text which require an analy-
sis of the text in the answer. Research to date has
concentrated on two main subtasks of computer-
assisted assessment: the grading of essays, which
is done mainly by checking the style, grammati-
cality, and coherence of the essay (cf. (Higgins
et al., 2004)), and the assessment of short student
answers (e.g., (Leacock and Chodorow, 2003; Pul-
man and Sukkarieh, 2005)), which is the focus of
this paper.
An automatic short answer grading system is
one which automatically assigns a grade to an an-
swer provided by a student through a comparison
with one or more correct answers. It is important
to note that this is different from the related task of
paraphrase detection, since a requirement in stu-
dent answer grading is to provide a grade on a cer-
tain scale rather than a binary yes/no decision.
In this paper, we explore and evaluate a set of
unsupervised techniques for automatic short an-
swer grading. Unlike previous work, which has
either required the availability of manually crafted
patterns (Sukkarieh et al., 2004; Mitchell et al.,
2002), or large training data sets to bootstrap such
patterns (Pulman and Sukkarieh, 2005), we at-
tempt to devise an unsupervised method that re-
quires no human intervention. We address the
grading problem from a text similarity perspec-
tive and examine the usefulness of various text-
to-text semantic similarity measures for automati-
cally grading short student answers.
Specifically, in this paper we seek answers to
the following questions. First, given a number
of corpus-based and knowledge-based methods as
previously proposed in the past for word and text
semantic similarity, what are the measures that
work best for the task of short answer grading?
Second, given a corpus-based measure of similar-
ity, what is the impact of the domain and the size
of the corpus on the accuracy of the measure? Fi-
nally, can we use the student answers themselves
to improve the quality of the grading system?
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999544857142857">
There are a number of approaches that have been
proposed in the past for automatic short answer
grading. Several state-of-the-art short answer
graders (Sukkarieh et al., 2004; Mitchell et al.,
2002) require manually crafted patterns which, if
matched, indicate that a question has been an-
swered correctly. If an annotated corpus is avail-
</bodyText>
<note confidence="0.922977">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 567–575,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.995695">
567
</page>
<bodyText confidence="0.99992331">
able, these patterns can be supplemented by learn-
ing additional patterns semi-automatically. The
Oxford-UCLES system (Sukkarieh et al., 2004)
bootstraps patterns by starting with a set of key-
words and synonyms and searching through win-
dows of a text for new patterns. A later implemen-
tation of the Oxford-UCLES system (Pulman and
Sukkarieh, 2005) compares several machine learn-
ing techniques, including inductive logic program-
ming, decision tree learning, and Bayesian learn-
ing, to the earlier pattern matching approach with
encouraging results.
C-Rater (Leacock and Chodorow, 2003)
matches the syntactical features of a student
response (subject, object, and verb) to that of a
set of correct responses. The method specifically
disregards the bag-of-words approach to take
into account the difference between ”dog bites
man” and ”man bites dog” while trying to detect
changes in voice (”the man was bitten by a dog”).
Another short answer grading system, AutoTu-
tor (Wiemer-Hastings et al., 1999), has been de-
signed as an immersive tutoring environment with
a graphical ”talking head” and speech recogni-
tion to improve the overall experience for students.
AutoTutor eschews the pattern-based approach en-
tirely in favor of a bag-of-words LSA approach
(Landauer and Dumais, 1997). Later work on Au-
toTutor (Wiemer-Hastings et al., 2005; Malatesta
et al., 2002) seeks to expand upon the original bag-
of-words approach which becomes less useful as
causality and word order become more important.
These methods are often supplemented with
some light preprocessing, e.g., spelling correc-
tion, punctuation correction, pronoun resolution,
lemmatization and tagging. Likewise, in order to
facilitate their goals of providing feedback to the
student more robust than a simple ”correct” or ”in-
correct,” several systems break the gold-standard
answers into constituent concepts that must indi-
vidually be matched for the answer to be consid-
ered fully correct (Callear et al., 2001). In this way
the system can determine which parts of an answer
a student understands and which parts he or she is
struggling with.
Automatic short answer grading is closely re-
lated to the task of text similarity. While more
general than short answer grading, text similarity
is essentially the problem of detecting and com-
paring the features of two texts. One of the earli-
est approaches to text similarity is the vector-space
model (Salton et al., 1997) with a term frequency
/ inverse document frequency (tfidj) weighting.
This model, along with the more sophisticated
LSA semantic alternative (Landauer and Dumais,
1997), has been found to work well for tasks such
as information retrieval and text classification.
Another approach (Hatzivassiloglou et al.,
1999) has been to use a machine learning algo-
rithm in which features are based on combina-
tions of simple features (e.g., a pair of nouns ap-
pear within 5 words from one another in both
texts). This method also attempts to account for
synonymy, word ordering, text length, and word
classes.
Another line of work attempts to extrapolate
text similarity from the arguably simpler prob-
lem of word similarity. (Mihalcea et al., 2006)
explores the efficacy of applying WordNet-based
word-to-word similarity measures (Pedersen et al.,
2004) to the comparison of texts and found them
generally comparable to corpus-based measures
such as LSA.
An interesting study has been performed at the
University of Adelaide (Lee et al., 2005), compar-
ing simpler word and n-gram feature vectors to
LSA and exploring the types of vector similarity
metrics (e.g., binary vs. count vectors, Jaccard
vs. cosine vs. overlap distance measure, etc.).
In this case, LSA was shown to perform better
than the word and n-gram vectors and performed
best at around 100 dimensions with binary vectors
weighted according to an entropy measure, though
the difference in measures was often subtle.
SELSA (Kanejiya et al., 2003) is a system that
attempts to add context to LSA by supplementing
the feature vectors with some simple syntactical
features, namely the part-of-speech of the previous
word. Their results indicate that SELSA does not
perform as well as LSA in the best case, but it has
a wider threshold window than LSA in which the
system can be used advantageously.
Finally, explicit semantic analysis (ESA)
(Gabrilovich and Markovitch, 2007) uses
Wikipedia as a source of knowledge for text
similarity. It creates for each text a feature vector
where each feature maps to a Wikipedia article.
Their preliminary experiments indicated that ESA
was able to significantly outperform LSA on some
text similarity tasks.
</bodyText>
<sectionHeader confidence="0.98325" genericHeader="method">
3 Data Set
</sectionHeader>
<bodyText confidence="0.999928888888889">
In order to evaluate the methods for short answer
grading, we have created a data set of questions
from introductory computer science assignments
with answers provided by a class of undergradu-
ate students. The assignments were administered
as part of a Data Structures course at the Univer-
sity of North Texas. For each assignment, the stu-
dent answers were collected via the WebCT online
learning environment.
</bodyText>
<page confidence="0.996032">
568
</page>
<bodyText confidence="0.999947170212766">
The evaluations reported in this paper are car-
ried out on the answers submitted for three of the
assignments in this class. Each assignment con-
sisted of seven short-answer questions.1 Thirty
students were enrolled in the class and submitted
answers to these assignments. Thus, the data set
we work with consists of a total of 630 student an-
swers (3 assignments x 7 questions/assignment x
30 student answers/question).
The answers were independently graded by two
human judges, using an integer scale from 0 (com-
pletely incorrect) to 5 (perfect answer). Both hu-
man judges were graduate computer science stu-
dents; one was the teaching assistant in the Data
Structures class, while the other is one of the au-
thors of this paper. Table 1 shows two question-
answer pairs with three sample student answers
each. The grades assigned by the two human
judges are also included.
The evaluations are run using Pearson’s corre-
lation coefficient measured against the average of
the human-assigned grades on a per-question and
a per-assignment basis. In the per-question set-
ting, every question and the corresponding student
answer is considered as an independent data point
in the correlation, and thus the emphasis is placed
on the correctness of the grade assigned to each
answer. In the per-assignment setting, each data
point is an assignment-student pair created by to-
taling the scores given to the student for each ques-
tion in the assignment. In this setting, the em-
phasis is placed on the overall grade a student re-
ceives for the assignment rather than on the grade
received for each independent question.
The correlation between the two human judges
is measured using both settings. In the per-
question setting, the two annotators correlated at
(r=0.6443). For the per-assignment setting, the
correlation was (r=0.7228).
A deeper look into the scores given by the
two annotators indicates the underlying subjectiv-
ity in grading short answer assignments. Of the
630 grades given, only 358 (56.8%) were exactly
agreed upon by the annotators. Even more strik-
ing, a full 107 grades (17.0%) differed by more
than one point on the five point scale, and 19
grades (3.0%) differed by 4 points or more. 2
</bodyText>
<footnote confidence="0.9556285">
1In addition, the assignments had several programming
exercises which have not been considered in any of our ex-
periments.
2An example should suffice to explain this discrepancy in
</footnote>
<construct confidence="0.464142">
annotator scoring: Question: What does a function signature
include? Answer: The name of the function and the types of
the parameters. Student: input parameters and return type.
</construct>
<bodyText confidence="0.954634909090909">
Scores: 1, 5. This example suggests that the graders were
not always consistent in comparing student answers to the in-
structor answer. Additionally, the instructor answer may be
insufficient to account for correct student answers, as ”return
Furthermore, on the occasions when the annota-
tors disagreed, the same annotator gave the higher
grade 79.8% of the time.
Over the course of this work, much attention
was given to our choice of correlation metric.
Previous work in text similarity and short-answer
grading seems split on the use of Pearson’s and
Spearman’s metric. It was not initially clear
that the underlying assumptions necessary for the
proper use of Pearson’s metric (e.g. normal dis-
tribution, interval measurement level, linear cor-
relation model) would be met in our experimental
setup. We considered both Spearman’s and sev-
eral less often used metrics (e.g. Kendall’s tau,
Goodman-Kruskal’s gamma), but in the end, we
have decided to follow previous work using Pear-
son’s so that our scores can be more easily com-
pared.3
</bodyText>
<sectionHeader confidence="0.994258" genericHeader="method">
4 Automatic Short Answer Grading
</sectionHeader>
<bodyText confidence="0.983648027777778">
Our experiments are centered around the use of
measures of similarity for automatic short answer
grading. In particular, we carry out three sets
of experiments, seeking answers to the following
three research questions.
First, what are the measures of semantic sim-
ilarity that work best for the task of short an-
swer grading? To answer this question, we run
several comparative evaluations covering a num-
ber of knowledge-based and corpus-based mea-
sures of semantic similarity. While previous work
has considered such comparisons for the related
task of paraphrase identification (Mihalcea et al.,
2006), to our knowledge no comprehensive eval-
uation has been carried out for the task of short
answer grading which includes all the similarity
measures proposed to date.
Second, to what extent do the domain and the
size of the data used to train the corpus-based
measures of similarity influence the accuracy of
the measures? To address this question, we run
a set of experiments which vary the size and do-
main of the corpus used to train the LSA and the
ESA metrics, and we measure their effect on the
accuracy of short answer grading.
Finally, given a measure of similarity, can we
integrate the answers with the highest scores and
improve the accuracy of the measure? We use
a technique similar to the pseudo-relevance feed-
back method used in information retrieval (Roc-
chio, 1971) and augment the correct answer with
type” does seem to be a valid component of a ”function sig-
nature” according to some literature on the web.
3Consider this an open call for discussion in the NLP
community regarding the proper usage of correlation metrics
with the ultimate goal of consistency within the community.
</bodyText>
<page confidence="0.992274">
569
</page>
<bodyText confidence="0.388417">
Sample questions, correct answers, and student answers Grade
</bodyText>
<construct confidence="0.614320833333333">
Question: What is the role of a prototype program in problem solving?
Correct answer: To simulate the behavior ofportions of the desired software product.
Student answer 1: A prototype program is used in problem solving to collect data for the problem. 1, 2
Student answer 2: It simulates the behavior ofportions of the desired software product. 5, 5
Student answer 3: To find problem and errors in a program before it is finalized. 2, 2
Question: What are the main advantages associated with object-oriented programming?
Correct answer: Abstraction and reusability.
Student answer 1: They make it easier to reuse and adapt previously written code and they separate complex
programs into smaller, easier to understand classes. 5, 4
Student answer 2: Object oriented programming allows programmers to use an object with classes that can be
changed and manipulated while not affecting the entire object at once. 1, 1
Student answer 3: Reusable components, Extensibility, Maintainability, it reduces large problems into smaller
</construct>
<tableCaption confidence="0.723684666666667">
more manageable problems. 4, 4
Table 1: Two sample questions with short answers provided by students and the grades assigned by the
two human judges
</tableCaption>
<bodyText confidence="0.999910846153846">
the student answers receiving the best score ac-
cording to a similarity measure.
In all the experiments, the evaluations are run
on the data set described in the previous section.
The results are compared against a simple baseline
that assigns a grade based on a measurement of
the cosine similarity between the weighted vector-
space representations of the correct answer and the
candidate student answer. The Pearson correla-
tion for this model, using an inverse document fre-
quency derived from the British National Corpus
(BNC), is r=0.3647 for the per-question evaluation
and r=0.4897 for the per-assignment evaluation.
</bodyText>
<sectionHeader confidence="0.977397" genericHeader="method">
5 Text-to-text Semantic Similarity
</sectionHeader>
<bodyText confidence="0.982835454545455">
We run our comparative evaluations using eight
knowledge-based measures of semantic similarity
(shortest path, Leacock &amp; Chodorow, Lesk, Wu
&amp; Palmer, Resnik, Lin, Jiang &amp; Conrath, Hirst &amp;
St. Onge), and two corpus-based measures (LSA
and ESA). For the knowledge-based measures, we
derive a text-to-text similarity metric by using the
methodology proposed in (Mihalcea et al., 2006):
for each open-class word in one of the input texts,
we use the maximum semantic similarity that can
be obtained by pairing it up with individual open-
class words in the second input text. More for-
mally, for each word W of part-of-speech class C
in the instructor answer, we find maxsim(W, C):
maxsim(W, C) = max SIMx(W, wi)
where wi is a word in the student answer of class
C and the SIMx function is one of the functions
described below. All the word-to-word similarity
scores obtained in this way are summed up and
normalized with the length of the two input texts.
We provide below a short description for each of
these similarity metrics.
</bodyText>
<subsectionHeader confidence="0.658981">
5.1 Knowledge-Based Measures
</subsectionHeader>
<bodyText confidence="0.987775">
The shortest path similarity is determined as:
</bodyText>
<equation confidence="0.990568666666667">
1
Simpath = (1)
length
</equation>
<bodyText confidence="0.997081">
where length is the length of the shortest path be-
tween two concepts using node-counting (includ-
ing the end nodes).
The Leacock &amp; Chodorow (Leacock and
Chodorow, 1998) similarity is determined as:
</bodyText>
<equation confidence="0.953161">
Simlch = − log length *D* D (2)
</equation>
<bodyText confidence="0.999970076923077">
where length is the length of the shortest path be-
tween two concepts using node-counting, and D
is the maximum depth of the taxonomy.
The Lesk similarity of two concepts is defined as
a function of the overlap between the correspond-
ing definitions, as provided by a dictionary. It is
based on an algorithm proposed by Lesk (1986) as
a solution for word sense disambiguation.
The Wu &amp; Palmer (Wu and Palmer, 1994) simi-
larity metric measures the depth of two given con-
cepts in the WordNet taxonomy, and the depth of
the least common subsumer (LCS), and combines
these figures into a similarity score:
</bodyText>
<equation confidence="0.998869">
2 ∗ depth(LCS)
Simwup = depth(concept1) + depth(concept2) (3)
</equation>
<bodyText confidence="0.998948666666667">
The measure introduced by Resnik (Resnik, 1995)
returns the information content (IC) of the LCS of
two concepts:
</bodyText>
<equation confidence="0.892859">
Simres = IC(LCS) (4)
</equation>
<bodyText confidence="0.786533">
where IC is defined as:
</bodyText>
<equation confidence="0.995633">
IC(c) = − log P(c) (5)
</equation>
<bodyText confidence="0.9052335">
and P(c) is the probability of encountering an in-
stance of concept c in a large corpus.
</bodyText>
<page confidence="0.976367">
570
</page>
<bodyText confidence="0.99993925">
The measure introduced by Lin (Lin, 1998) builds
on Resnik’s measure of similarity, and adds a
normalization factor consisting of the information
content of the two input concepts:
</bodyText>
<equation confidence="0.9999365">
Simiz = 2 * IC(LCS) (6)
� IC(concept1) + IC(concept2)
</equation>
<bodyText confidence="0.7922715">
We also consider the Jiang &amp; Conrath (Jiang and
Conrath, 1997) measure of similarity:
</bodyText>
<equation confidence="0.995984">
1
Simin, = IC(concept1) + IC(concept2) − 2 * IC(LCS)
(7)
</equation>
<bodyText confidence="0.9997522">
Finally, we consider the Hirst &amp; St. Onge (Hirst
and St-Onge, 1998) measure of similarity, which
determines the similarity strength of a pair of
synsets by detecting lexical chains between the
pair in a text using the WordNet hierarchy.
</bodyText>
<subsectionHeader confidence="0.943881">
5.2 Corpus-Based Measures
</subsectionHeader>
<bodyText confidence="0.99978296875">
Corpus-based measures differ from knowledge-
based methods in that they do not require any en-
coded understanding of either the vocabulary or
the grammar of a text’s language. In many of
the scenarios where CAA would be advantageous,
robust language-specific resources (e.g. Word-
Net) may not be available. Thus, state-of-the-art
corpus-based measures may be the only available
approach to CAA in languages with scarce re-
sources.
One corpus-based measure of semantic similar-
ity is latent semantic analysis (LSA) proposed by
Landauer (Landauer and Dumais, 1997). In LSA,
term co-occurrences in a corpus are captured by
means of a dimensionality reduction operated by a
singular value decomposition (SVD) on the term-
by-document matrix T representing the corpus.
For the experiments reported in this section, we
run the SVD operation on several corpora includ-
ing the BNC (LSA BNC) and the entire English
Wikipedia (LSA Wikipedia).4
Explicit semantic analysis (ESA) (Gabrilovich
and Markovitch, 2007) is a variation on the stan-
dard vectorial model in which the dimensions of
the vector are directly equivalent to abstract con-
cepts. Each article in Wikipedia represents a con-
cept in the ESA vector. The relatedness of a term
to a concept is defined as the tf*idf score for the
term within the Wikipedia article, and the related-
ness between two words is the cosine of the two
concept vectors in a high-dimensional space. We
refer to this method as ESA Wikipedia.
</bodyText>
<footnote confidence="0.9333115">
4Throughout this paper, the references to the Wikipedia
corpus refer to a version downloaded in September 2007.
</footnote>
<subsectionHeader confidence="0.970601">
5.3 Implementation
</subsectionHeader>
<bodyText confidence="0.998393066666666">
For the knowledge-based measures, we use the
WordNet-based implementation of the word-to-
word similarity metrics, as available in the Word-
Net::Similarity package (Patwardhan et al., 2003).
For latent semantic analysis, we use the InfoMap
package.5 For ESA, we use our own imple-
mentation of the ESA algorithm as described in
(Gabrilovich and Markovitch, 2006). Note that
all the word similarity measures are normalized so
that they fall within a 0–1 range. The normaliza-
tion is done by dividing the similarity score pro-
vided by a given measure with the maximum pos-
sible score for that measure.
Table 2 shows the results obtained with each of
these measures on our evaluation data set.
</bodyText>
<figure confidence="0.3374015">
Measure Correlation
Knowledge-based measures
</figure>
<table confidence="0.920680857142857">
Shortest path 0.4413
Leacock &amp; Chodorow 0.2231
Lesk 0.3630
Wu &amp; Palmer 0.3366
Resnik 0.2520
Lin 0.3916
Jiang &amp; Conrath 0.4499
Hirst &amp; St-Onge 0.1961
Corpus-based measures
LSA BNC 0.4071
LSA Wikipedia 0.4286
ESA Wikipedia 0.4681
Baseline
tf*idf 0.3647
</table>
<tableCaption confidence="0.973261">
Table 2: Comparison of knowledge-based and
corpus-based measures of similarity for short an-
swer grading
</tableCaption>
<sectionHeader confidence="0.955847" genericHeader="method">
6 The Role of Domain and Size
</sectionHeader>
<bodyText confidence="0.999932230769231">
One of the key considerations when applying
corpus-based techniques is the extent to which size
and subject matter affect the overall performance
of the system. In particular, based on the underly-
ing processes involved, the LSA and ESA corpus-
based methods are expected to be especially sen-
sitive to changes in domain and size. Building the
language models depends on the relatedness of the
words in the training data which suggests that, for
instance, in a computer science domain the terms
”object” and ”oriented” will be more closely re-
lated than in a more general text. Similarly, a large
amount of training data will lead to less sparse
</bodyText>
<footnote confidence="0.979105">
5http://infomap-nlp.sourceforge.net/
</footnote>
<page confidence="0.997455">
571
</page>
<bodyText confidence="0.999725115384615">
vector spaces, which in turn is expected to affect
the performance of the corpus-based methods.
With this in mind, we developed two training
corpora for use with the corpus-based measures
that covered the computer science domain. The
first corpus (LSA slides) consists of several online
lecture notes associated with the class textbook,
specifically covering topics that are used as ques-
tions in our sample. The second domain-specific
corpus is a subset of Wikipedia (LSA Wikipedia
CS) consisting of articles that contain any of the
following words: computer, computing, computa-
tion, algorithm, recursive, or recursion.
The performance on the domain-specific cor-
pora is compared with the one observed on the
open-domain corpora mentioned in the previ-
ous section, namely LSA Wikipedia and ESA
Wikipedia. In addition, for the purpose of running
a comparison with the LSA slides corpus, we also
created a random subset of the LSA Wikipedia
corpus approximately matching the size of the
LSA slides corpus. We refer to this corpus as LSA
Wikipedia (small).
Table 3 shows an overview of the various cor-
pora used in the experiments, along with the Pear-
son correlation observed on our data set.
</bodyText>
<table confidence="0.9878346">
Measure - Corpus Size Correlation
Training on generic corpora
LSA BNC 566.7MB 0.4071
LSA Wikipedia 1.8GB 0.4286
LSA Wikipedia (small) 0.3MB 0.3518
ESA Wikipedia 1.8GB 0.4681
Training on domain-specific corpora
LSA Wikipedia CS 77.1MB 0.4628
LSA slides 0.3MB 0.4146
ESA Wikipedia CS 77.1MB 0.4385
</table>
<tableCaption confidence="0.8778535">
Table 3: Corpus-based measures trained on cor-
pora from different domains and of different sizes
</tableCaption>
<bodyText confidence="0.999644571428571">
Assuming a corpus of comparable size, we ex-
pect a measure trained on a domain-specific cor-
pus to outperform one that relies on a generic one.
Indeed, by comparing the results obtained with
LSA slides to those obtained with LSA Wikipedia
(small), we see that by using the in-domain com-
puter science slides we obtain a correlation of
r=0.4146, which is higher than the correlation
of r=0.3518 obtained with a corpus of the same
size but open-domain. The effect of the domain
is even more pronounced when we compare the
performance obtained with LSA Wikipedia CS
(r=0.4628) with the one obtained with the full LSA
Wikipedia (r=0.4286).6 The smaller, domain-
</bodyText>
<footnote confidence="0.876337">
6The difference was found significant using a paired t-test
</footnote>
<bodyText confidence="0.999959793103448">
specific corpus performs better, despite the fact
that the generic corpus is 23 times larger and is a
superset of the smaller corpus. This suggests that
for LSA the quality of the texts is vastly more im-
portant than their quantity.
When using the domain-specific subset of
Wikipedia, we observe decreased performance
with ESA compared to the full Wikipedia space.
We suggest that for ESA the high-dimensionality
of the concept space7 is paramount, since many re-
lations between generic words may be lost to ESA
that can be detected latently using LSA.
In tandem with our exploration of the effects
of domain-specific data, we also look at the effect
of size on the overall performance. The main in-
tuitive trends are there, i.e., the performance ob-
tained with the large LSA-Wikipedia is better than
the one that can be obtained with LSA Wikipedia
(small). Similarly, in the domain-specific space,
the LSA Wikipedia CS corpus leads to better per-
formance than the smaller LSA slides data set.
However, an analysis carried out at a finer grained
scale, in which we calculate the performance ob-
tained with LSA when trained on 5%, 10%, ...,
100% fractions of the full LSA Wikipedia corpus,
does not reveal a close correlation between size
and performance, which suggests that further anal-
ysis is needed to determine the exact effect of cor-
pus size on performance.
</bodyText>
<sectionHeader confidence="0.970771" genericHeader="method">
7 Relevance Feedback based on Student
Answers
</sectionHeader>
<bodyText confidence="0.999834315789474">
The automatic grading of student answers im-
plies a measure of similarity between the answers
provided by the students and the correct answer
provided by the instructor. Since we only have
one correct answer, some student answers may be
wrongly graded because of little or no similarity
with the correct answer that we have.
To address this problem, we introduce a novel
technique that feeds back from the student an-
swers themselves in a way similar to the pseudo-
relevance feedback used in information retrieval
(Rocchio, 1971). In this way, the paraphrasing that
is usually observed across student answers will en-
hance the vocabulary of the correct answer, while
at the same time maintaining the correctness of the
gold-standard answer.
Briefly, given a metric that provides similarity
scores between the student answers and the cor-
rect answer, scores are ranked from most similar
</bodyText>
<footnote confidence="0.9185232">
(p&lt;0.001).
7In ESA, all the articles in Wikipedia are used as dimen-
sions, which leads to about 1.75 million dimensions in the
ESA Wikipedia corpus, compared to only 55,000 dimensions
in the ESA Wikipedia CS corpus.
</footnote>
<page confidence="0.994421">
572
</page>
<bodyText confidence="0.999843264705882">
to least. The words of the top N ranked answers
are then added to the gold standard answer. The
remaining answers are then rescored according the
the new gold standard vector. In practice, we hold
the scores from the first run (i.e., with no feed-
back) constant for the top N highest-scoring an-
swers, and the second-run scores for the remaining
answers are multiplied by the first-run score of the
Nth highest-scoring answer. In this way, we keep
the original scores for the top N highest-scoring
answers (and thus prevent them from becoming ar-
tificially high), and at the same time, we guarantee
that none of the lower-scored answers will get a
new score higher than the best answers.
The effects of relevance feedback are shown in
Figure 9, which plots the Pearson correlation be-
tween automatic and human grading (Y axis) ver-
sus the number of student answers that are used
for relevance feedback (X axis).
Overall, an improvement of up to 0.047 on
the 0-1 Pearson scale can be obtained by using
this technique, with a maximum improvement ob-
served after about 4-6 iterations on average. Af-
ter an initial number of high-scored answers, it is
likely that the correctness of the answers degrades,
and thus the decrease in performance observed af-
ter an initial number of iterations. Our results in-
dicate that the LSA and WordNet similarity met-
rics respond more favorably to feedback than the
ESA metric. It is possible that supplementing the
bag-of-words in ESA (with e.g. synonyms and
phrasal differences) does not drastically alter the
resultant concept vector, and thus the overall ef-
fect is smaller.
</bodyText>
<sectionHeader confidence="0.996108" genericHeader="method">
8 Discussion
</sectionHeader>
<bodyText confidence="0.99975845">
Our experiments show that several knowledge-
based and corpus-based measures of similarity
perform comparably when used for the task of
short answer grading. However, since the corpus-
based measures can be improved by account-
ing for domain and corpus size, the highest per-
formance can be obtained with a corpus-based
measure (LSA) trained on a domain-specific cor-
pus. Further improvements were also obtained
by integrating the highest-scored student answers
through a relevance feedback technique.
Table 4 summarizes the results of our experi-
ments. In addition to the per-question evaluations
that were reported throughout the paper, we also
report the per-assignment evaluation, which re-
flects a cumulative score for a student on a single
assignment, as described in Section 3.
Overall, in both the per-question and per-
assignment evaluations, we obtained the best per-
formance by using an LSA measure trained on
</bodyText>
<table confidence="0.977041">
Correlation
Measure per-quest. per-assign.
Baselines
tf*idf 0.3647 0.4897
LSA BNC 0.4071 0.6465
Relevance Feedback based on Student Answers
WordNet shortest path 0.4887 0.6344
LSA Wikipedia CS 0.5099 0.6735
ESA Wikipedia full 0.4893 0.6498
Annotator agreement 0.6443 0.7228
</table>
<tableCaption confidence="0.996294">
Table 4: Summary of results obtained with vari-
</tableCaption>
<bodyText confidence="0.974981424242424">
ous similarity measures, with relevance feedback
based on six student answers. We also list the
tf*idf and the LSA trained on BNC baselines (no
feedback), as well as the annotator agreement up-
per bound.
a medium size domain-specific corpus obtained
from Wikipedia, with relevance feedback from
the four highest-scoring student answers. This
method improves significantly over the tf*idf
baseline and also over the LSA trained on BNC
model, which has been used extensively in previ-
ous work. The differences were found to be sig-
nificant using a paired t-test (p&lt;0.001).
To gain further insights, we made an additional
analysis where we determined the ability of our
system to make a binary accept/reject decision. In
this evaluation, we map the 0-5 human grading of
the data set to an accept/reject annotation by us-
ing a threshold of 2.5. Every answer with a grade
higher than 2.5 is labeled as “accept,” while ev-
ery answer below 2.5 is labeled as “reject.” Next,
we use our best system (LSA trained on domain-
specific data with relevance feedback), and run a
ten-fold cross-validation on the data set. Specif-
ically, for each fold, the system uses the remain-
ing nine folds to automatically identify a thresh-
old to maximize the matching with the gold stan-
dard. The threshold identified in this way is used
to automatically annotate the test fold with “ac-
cept”/”reject” labels. The ten-fold cross validation
resulted in an accuracy of 92%, indicating the abil-
ity of the system to automatically make a binary
accept/reject decision.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999787285714286">
In this paper, we explored unsupervised tech-
niques for automatic short answer grading.
We believe the paper made three important con-
tributions. First, while there are a number of word
and text similarity measures that have been pro-
posed in the past, to our knowledge no previ-
ous work has considered a comprehensive evalu-
</bodyText>
<page confidence="0.993159">
573
</page>
<figure confidence="0.999644235294118">
Correlation
0.55
0.45
0.35
0.5
0.4
LSA-Wiki-full
LSA-Wiki-CS
LSA-slides-CS
ESA-Wiki-full
ESA-Wiki-CS
WN-JCN
WN-PATH
TF*IDF
LSA-BNC
0 5 10 15 20
Number of student answers used for feedback
</figure>
<figureCaption confidence="0.999996">
Figure 1: Effect of relevance feedback on performance
</figureCaption>
<bodyText confidence="0.999820365384615">
ation of all the measures for the task of short an-
swer grading. We filled this gap by running com-
parative evaluations of several knowledge-based
and corpus-based measures on a data set of short
student answers. Our results indicate that when
used in their original form, the results obtained
with the best knowledge-based (WordNet short-
est path and Jiang &amp; Conrath) and corpus-based
measures (LSA and ESA) have comparable per-
formance. The benefit of the corpus-based ap-
proaches over knowledge-based approaches lies in
their language independence and the relative ease
in creating a large domain-sensitive corpus versus
a language knowledge base (e.g., WordNet).
Second, we analysed the effect of domain and
corpus size on the effectiveness of the corpus-
based measures. We found that significant im-
provements can be obtained for the LSA measure
when using a medium size domain-specific corpus
built from Wikipedia. In fact, when using LSA,
our results indicate that the corpus domain may be
significantly more important than corpus size once
a certain threshold size has been reached.
Finally, we introduced a novel technique for in-
tegrating feedback from the student answers them-
selves into the grading system. Using a method
similar to the pseudo-relevance feedback tech-
nique used in information retrieval, we were able
to improve the quality of our system by a few per-
centage points.
Overall, our best system consists of an LSA
measure trained on a domain-specific corpus built
on Wikipedia with feedback from student answers,
which was found to bring a significant absolute
improvement on the 0-1 Pearson scale of 0.14 over
the tf*idf baseline and 0.10 over the LSA BNC
model that has been used in the past.
In future work, we intend to expand our analy-
sis of both the gold-standard answer and the stu-
dent answers beyond the bag-of-words paradigm
by considering basic logical features in the text
(i.e., AND, OR, NOT) as well as the existence
of shallow grammatical features such as predicate-
argument structure(Moschitti et al., 2007) as well
as semantic classes for words. Furthermore, it may
be advantageous to expand upon the existing mea-
sures by applying machine learning techniques to
create a hybrid decision system that would exploit
the advantages of each measure.
The data set introduced in this paper, along with
the human-assigned grades, can be downloaded
from http://lit.csci.unt.edu/index.php/Downloads.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998252">
This work was partially supported by a National
Science Foundation CAREER award #0747340.
The authors are grateful to Samer Hassan for mak-
ing available his implementation of the ESA algo-
rithm.
</bodyText>
<sectionHeader confidence="0.996375" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.95439">
D. Callear, J. Jerrams-Smith, and V. Soh. 2001.
CAA of Short Non-MCQ Answers. Proceedings of
</reference>
<page confidence="0.993803">
574
</page>
<reference confidence="0.993960713114754">
the 5th International Computer Assisted Assessment
conference.
E. Gabrilovich and S. Markovitch. 2006. Overcoming
the brittleness bottleneck using Wikipedia: Enhanc-
ing text categorization with encyclopedic knowl-
edge. In Proceedings of the National Conference on
Artificial Intelligence (AAAI), Boston.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Ex-
plicit Semantic Analysis. Proceedings of the 20th
International Joint Conference on Artificial Intelli-
gence, pages 6–12.
V. Hatzivassiloglou, J. Klavans, and E. Eskin. 1999.
Detecting text similarity over short passages: Ex-
ploring linguistic feature combinations via machine
learning. Proceedings of the Joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
D. Higgins, J. Burstein, D. Marcu, and C. Gentile.
2004. Evaluating multiple aspects of coherence in
student essays. In Proceedings of the annual meet-
ing of the North American Chapter of the Associa-
tion for Computational Linguistics, Boston, MA.
G. Hirst and D. St-Onge, 1998. Lexical chains as rep-
resentations of contexts for the detection and correc-
tion of malaproprisms. The MIT Press.
J. Jiang and D. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, Taiwan.
D. Kanejiya, A. Kumar, and S. Prasad. 2003. Au-
tomatic evaluation of students’ answers using syn-
tactically enhanced LSA. Proceedings of the HLT-
NAACL 03 workshop on Building educational appli-
cations using natural language processing-Volume
2, pages 53–60.
T.K. Landauer and S.T. Dumais. 1997. A solution to
plato’s problem: The latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological Review, 104.
C. Leacock and M. Chodorow. 1998. Combining lo-
cal context and WordNet sense similarity for word
sense identification. In WordNet, An Electronic Lex-
ical Database. The MIT Press.
C. Leacock and M. Chodorow. 2003. C-rater: Au-
tomated Scoring of Short-Answer Questions. Com-
puters and the Humanities, 37(4):389–405.
M.D. Lee, B. Pincombe, and M. Welsh. 2005. An em-
pirical evaluation of models of text document simi-
larity. Proceedings of the 27th Annual Conference
of the Cognitive Science Society, pages 1254–1259.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference 1986, Toronto, June.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning, Madison, WI.
K.I. Malatesta, P. Wiemer-Hastings, and J. Robertson.
2002. Beyond the Short Answer Question with Re-
search Methods Tutor. In Proceedings of the Intelli-
gent Tutoring Systems Conference.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based approaches to
text semantic similarity. In Proceedings of the
American Association for Artificial Intelligence
(AAAI 2006), Boston.
T. Mitchell, T. Russell, P. Broomhead, and N. Aldridge.
2002. Towards robust computerised marking of
free-text responses. Proceedings of the 6th Interna-
tional Computer Assisted Assessment (CAA) Confer-
ence.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion/answer classification. In Proceedings of the
45th Conference of the Association for Computa-
tional Linguistics.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003.
Using measures of semantic relatedness for word
sense disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, Mexico
City, February.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference
on Artificial Intelligence, pages 1024–1025.
S.G. Pulman and J.Z. Sukkarieh. 2005. Automatic
Short Answer Marking. ACL WS Bldg Ed Apps us-
ing NLP.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th
International Joint Conference on Artificial Intelli-
gence, Montreal, Canada.
J. Rocchio, 1971. Relevance feedback in information
retrieval. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Wong, and C.S. Yang. 1997. A vec-
tor space model for automatic indexing. In Read-
ings in Information Retrieval, pages 273–280. Mor-
gan Kaufmann Publishers, San Francisco, CA.
J.Z. Sukkarieh, S.G. Pulman, and N. Raikes. 2004.
Auto-Marking 2: An Update on the UCLES-Oxford
University research into using Computational Lin-
guistics to Score Short, Free Text Responses. In-
ternational Association of Educational Assessment,
Philadephia.
P. Wiemer-Hastings, K. Wiemer-Hastings, and
A. Graesser. 1999. Improving an intelligent tutor’s
comprehension of students with Latent Semantic
Analysis. Artificial Intelligence in Education, pages
535–542.
P. Wiemer-Hastings, E. Arnott, and D. Allbritton.
2005. Initial results and mixed directions for re-
search methods tutor. In AIED2005 - Supplementary
Proceedings of the 12th International Conference on
Artificial Intelligence in Education, Amsterdam.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Lin-
guistics, Las Cruces, New Mexico.
</reference>
<page confidence="0.998475">
575
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.728739">
<title confidence="0.998727">Text-to-text Semantic Similarity for Automatic Short Answer Grading</title>
<author confidence="0.997224">Mohler Mihalcea</author>
<affiliation confidence="0.9996315">Department of Computer Science University of North Texas</affiliation>
<email confidence="0.99947">mgm0038@unt.edu,rada@cs.unt.edu</email>
<abstract confidence="0.9812432">In this paper, we explore unsupervised techniques for the task of automatic short answer grading. We compare a number of knowledge-based and corpus-based measures of text similarity, evaluate the effect of domain and size on the corpus-based measures, and also introduce a novel technique to improve the performance of the system by integrating automatic feedback from the student answers. Overall, our system significantly and consistently outperforms other unsupervised methods for short answer grading that have been proposed in the past.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Callear</author>
<author>J Jerrams-Smith</author>
<author>V Soh</author>
</authors>
<title>CAA of Short Non-MCQ Answers.</title>
<date>2001</date>
<booktitle>Proceedings of the 5th International Computer Assisted Assessment conference.</booktitle>
<contexts>
<context position="6196" citStr="Callear et al., 2001" startWordPosition="978" endWordPosition="981">t al., 2002) seeks to expand upon the original bagof-words approach which becomes less useful as causality and word order become more important. These methods are often supplemented with some light preprocessing, e.g., spelling correction, punctuation correction, pronoun resolution, lemmatization and tagging. Likewise, in order to facilitate their goals of providing feedback to the student more robust than a simple ”correct” or ”incorrect,” several systems break the gold-standard answers into constituent concepts that must individually be matched for the answer to be considered fully correct (Callear et al., 2001). In this way the system can determine which parts of an answer a student understands and which parts he or she is struggling with. Automatic short answer grading is closely related to the task of text similarity. While more general than short answer grading, text similarity is essentially the problem of detecting and comparing the features of two texts. One of the earliest approaches to text similarity is the vector-space model (Salton et al., 1997) with a term frequency / inverse document frequency (tfidj) weighting. This model, along with the more sophisticated LSA semantic alternative (Lan</context>
</contexts>
<marker>Callear, Jerrams-Smith, Soh, 2001</marker>
<rawString>D. Callear, J. Jerrams-Smith, and V. Soh. 2001. CAA of Short Non-MCQ Answers. Proceedings of the 5th International Computer Assisted Assessment conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Overcoming the brittleness bottleneck using Wikipedia: Enhancing text categorization with encyclopedic knowledge.</title>
<date>2006</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI),</booktitle>
<location>Boston.</location>
<contexts>
<context position="21345" citStr="Gabrilovich and Markovitch, 2006" startWordPosition="3446" endWordPosition="3449">he relatedness between two words is the cosine of the two concept vectors in a high-dimensional space. We refer to this method as ESA Wikipedia. 4Throughout this paper, the references to the Wikipedia corpus refer to a version downloaded in September 2007. 5.3 Implementation For the knowledge-based measures, we use the WordNet-based implementation of the word-toword similarity metrics, as available in the WordNet::Similarity package (Patwardhan et al., 2003). For latent semantic analysis, we use the InfoMap package.5 For ESA, we use our own implementation of the ESA algorithm as described in (Gabrilovich and Markovitch, 2006). Note that all the word similarity measures are normalized so that they fall within a 0–1 range. The normalization is done by dividing the similarity score provided by a given measure with the maximum possible score for that measure. Table 2 shows the results obtained with each of these measures on our evaluation data set. Measure Correlation Knowledge-based measures Shortest path 0.4413 Leacock &amp; Chodorow 0.2231 Lesk 0.3630 Wu &amp; Palmer 0.3366 Resnik 0.2520 Lin 0.3916 Jiang &amp; Conrath 0.4499 Hirst &amp; St-Onge 0.1961 Corpus-based measures LSA BNC 0.4071 LSA Wikipedia 0.4286 ESA Wikipedia 0.4681 B</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2006</marker>
<rawString>E. Gabrilovich and S. Markovitch. 2006. Overcoming the brittleness bottleneck using Wikipedia: Enhancing text categorization with encyclopedic knowledge. In Proceedings of the National Conference on Artificial Intelligence (AAAI), Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>6--12</pages>
<contexts>
<context position="8572" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1362" endWordPosition="1365"> and n-gram vectors and performed best at around 100 dimensions with binary vectors weighted according to an entropy measure, though the difference in measures was often subtle. SELSA (Kanejiya et al., 2003) is a system that attempts to add context to LSA by supplementing the feature vectors with some simple syntactical features, namely the part-of-speech of the previous word. Their results indicate that SELSA does not perform as well as LSA in the best case, but it has a wider threshold window than LSA in which the system can be used advantageously. Finally, explicit semantic analysis (ESA) (Gabrilovich and Markovitch, 2007) uses Wikipedia as a source of knowledge for text similarity. It creates for each text a feature vector where each feature maps to a Wikipedia article. Their preliminary experiments indicated that ESA was able to significantly outperform LSA on some text similarity tasks. 3 Data Set In order to evaluate the methods for short answer grading, we have created a data set of questions from introductory computer science assignments with answers provided by a class of undergraduate students. The assignments were administered as part of a Data Structures course at the University of North Texas. For ea</context>
<context position="20396" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="3290" endWordPosition="3293">only available approach to CAA in languages with scarce resources. One corpus-based measure of semantic similarity is latent semantic analysis (LSA) proposed by Landauer (Landauer and Dumais, 1997). In LSA, term co-occurrences in a corpus are captured by means of a dimensionality reduction operated by a singular value decomposition (SVD) on the termby-document matrix T representing the corpus. For the experiments reported in this section, we run the SVD operation on several corpora including the BNC (LSA BNC) and the entire English Wikipedia (LSA Wikipedia).4 Explicit semantic analysis (ESA) (Gabrilovich and Markovitch, 2007) is a variation on the standard vectorial model in which the dimensions of the vector are directly equivalent to abstract concepts. Each article in Wikipedia represents a concept in the ESA vector. The relatedness of a term to a concept is defined as the tf*idf score for the term within the Wikipedia article, and the relatedness between two words is the cosine of the two concept vectors in a high-dimensional space. We refer to this method as ESA Wikipedia. 4Throughout this paper, the references to the Wikipedia corpus refer to a version downloaded in September 2007. 5.3 Implementation For the </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>E. Gabrilovich and S. Markovitch. 2007. Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis. Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 6–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>J Klavans</author>
<author>E Eskin</author>
</authors>
<title>Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning.</title>
<date>1999</date>
<booktitle>Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="6962" citStr="Hatzivassiloglou et al., 1999" startWordPosition="1101" endWordPosition="1104">matic short answer grading is closely related to the task of text similarity. While more general than short answer grading, text similarity is essentially the problem of detecting and comparing the features of two texts. One of the earliest approaches to text similarity is the vector-space model (Salton et al., 1997) with a term frequency / inverse document frequency (tfidj) weighting. This model, along with the more sophisticated LSA semantic alternative (Landauer and Dumais, 1997), has been found to work well for tasks such as information retrieval and text classification. Another approach (Hatzivassiloglou et al., 1999) has been to use a machine learning algorithm in which features are based on combinations of simple features (e.g., a pair of nouns appear within 5 words from one another in both texts). This method also attempts to account for synonymy, word ordering, text length, and word classes. Another line of work attempts to extrapolate text similarity from the arguably simpler problem of word similarity. (Mihalcea et al., 2006) explores the efficacy of applying WordNet-based word-to-word similarity measures (Pedersen et al., 2004) to the comparison of texts and found them generally comparable to corpus</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Eskin, 1999</marker>
<rawString>V. Hatzivassiloglou, J. Klavans, and E. Eskin. 1999. Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning. Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Higgins</author>
<author>J Burstein</author>
<author>D Marcu</author>
<author>C Gentile</author>
</authors>
<title>Evaluating multiple aspects of coherence in student essays.</title>
<date>2004</date>
<booktitle>In Proceedings of the annual meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="2019" citStr="Higgins et al., 2004" startWordPosition="316" endWordPosition="319">these instances, we often have to turn to computerassisted assessment. While some forms of computer-assisted assessment do not require sophisticated text understanding (e.g., multiple choice or true/false questions can be easily graded by a system if the correct solution is available), there are also student answers that consist of free text which require an analysis of the text in the answer. Research to date has concentrated on two main subtasks of computerassisted assessment: the grading of essays, which is done mainly by checking the style, grammaticality, and coherence of the essay (cf. (Higgins et al., 2004)), and the assessment of short student answers (e.g., (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005)), which is the focus of this paper. An automatic short answer grading system is one which automatically assigns a grade to an answer provided by a student through a comparison with one or more correct answers. It is important to note that this is different from the related task of paraphrase detection, since a requirement in student answer grading is to provide a grade on a certain scale rather than a binary yes/no decision. In this paper, we explore and evaluate a set of unsupervised</context>
</contexts>
<marker>Higgins, Burstein, Marcu, Gentile, 2004</marker>
<rawString>D. Higgins, J. Burstein, D. Marcu, and C. Gentile. 2004. Evaluating multiple aspects of coherence in student essays. In Proceedings of the annual meeting of the North American Chapter of the Association for Computational Linguistics, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical chains as representations of contexts for the detection and correction of malaproprisms.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="19208" citStr="Hirst and St-Onge, 1998" startWordPosition="3108" endWordPosition="3111">S of two concepts: Simres = IC(LCS) (4) where IC is defined as: IC(c) = − log P(c) (5) and P(c) is the probability of encountering an instance of concept c in a large corpus. 570 The measure introduced by Lin (Lin, 1998) builds on Resnik’s measure of similarity, and adds a normalization factor consisting of the information content of the two input concepts: Simiz = 2 * IC(LCS) (6) � IC(concept1) + IC(concept2) We also consider the Jiang &amp; Conrath (Jiang and Conrath, 1997) measure of similarity: 1 Simin, = IC(concept1) + IC(concept2) − 2 * IC(LCS) (7) Finally, we consider the Hirst &amp; St. Onge (Hirst and St-Onge, 1998) measure of similarity, which determines the similarity strength of a pair of synsets by detecting lexical chains between the pair in a text using the WordNet hierarchy. 5.2 Corpus-Based Measures Corpus-based measures differ from knowledgebased methods in that they do not require any encoded understanding of either the vocabulary or the grammar of a text’s language. In many of the scenarios where CAA would be advantageous, robust language-specific resources (e.g. WordNet) may not be available. Thus, state-of-the-art corpus-based measures may be the only available approach to CAA in languages w</context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>G. Hirst and D. St-Onge, 1998. Lexical chains as representations of contexts for the detection and correction of malaproprisms. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>D Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Research in Computational Linguistics,</booktitle>
<contexts>
<context position="19060" citStr="Jiang and Conrath, 1997" startWordPosition="3082" endWordPosition="3085">h(LCS) Simwup = depth(concept1) + depth(concept2) (3) The measure introduced by Resnik (Resnik, 1995) returns the information content (IC) of the LCS of two concepts: Simres = IC(LCS) (4) where IC is defined as: IC(c) = − log P(c) (5) and P(c) is the probability of encountering an instance of concept c in a large corpus. 570 The measure introduced by Lin (Lin, 1998) builds on Resnik’s measure of similarity, and adds a normalization factor consisting of the information content of the two input concepts: Simiz = 2 * IC(LCS) (6) � IC(concept1) + IC(concept2) We also consider the Jiang &amp; Conrath (Jiang and Conrath, 1997) measure of similarity: 1 Simin, = IC(concept1) + IC(concept2) − 2 * IC(LCS) (7) Finally, we consider the Hirst &amp; St. Onge (Hirst and St-Onge, 1998) measure of similarity, which determines the similarity strength of a pair of synsets by detecting lexical chains between the pair in a text using the WordNet hierarchy. 5.2 Corpus-Based Measures Corpus-based measures differ from knowledgebased methods in that they do not require any encoded understanding of either the vocabulary or the grammar of a text’s language. In many of the scenarios where CAA would be advantageous, robust language-specific </context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J. Jiang and D. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kanejiya</author>
<author>A Kumar</author>
<author>S Prasad</author>
</authors>
<title>Automatic evaluation of students’ answers using syntactically enhanced LSA.</title>
<date>2003</date>
<booktitle>Proceedings of the HLTNAACL 03 workshop on Building educational applications</booktitle>
<pages>53--60</pages>
<contexts>
<context position="8146" citStr="Kanejiya et al., 2003" startWordPosition="1293" endWordPosition="1296">nd them generally comparable to corpus-based measures such as LSA. An interesting study has been performed at the University of Adelaide (Lee et al., 2005), comparing simpler word and n-gram feature vectors to LSA and exploring the types of vector similarity metrics (e.g., binary vs. count vectors, Jaccard vs. cosine vs. overlap distance measure, etc.). In this case, LSA was shown to perform better than the word and n-gram vectors and performed best at around 100 dimensions with binary vectors weighted according to an entropy measure, though the difference in measures was often subtle. SELSA (Kanejiya et al., 2003) is a system that attempts to add context to LSA by supplementing the feature vectors with some simple syntactical features, namely the part-of-speech of the previous word. Their results indicate that SELSA does not perform as well as LSA in the best case, but it has a wider threshold window than LSA in which the system can be used advantageously. Finally, explicit semantic analysis (ESA) (Gabrilovich and Markovitch, 2007) uses Wikipedia as a source of knowledge for text similarity. It creates for each text a feature vector where each feature maps to a Wikipedia article. Their preliminary expe</context>
</contexts>
<marker>Kanejiya, Kumar, Prasad, 2003</marker>
<rawString>D. Kanejiya, A. Kumar, and S. Prasad. 2003. Automatic evaluation of students’ answers using syntactically enhanced LSA. Proceedings of the HLTNAACL 03 workshop on Building educational applications using natural language processing-Volume 2, pages 53–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<contexts>
<context position="5507" citStr="Landauer and Dumais, 1997" startWordPosition="874" endWordPosition="877">ect, and verb) to that of a set of correct responses. The method specifically disregards the bag-of-words approach to take into account the difference between ”dog bites man” and ”man bites dog” while trying to detect changes in voice (”the man was bitten by a dog”). Another short answer grading system, AutoTutor (Wiemer-Hastings et al., 1999), has been designed as an immersive tutoring environment with a graphical ”talking head” and speech recognition to improve the overall experience for students. AutoTutor eschews the pattern-based approach entirely in favor of a bag-of-words LSA approach (Landauer and Dumais, 1997). Later work on AutoTutor (Wiemer-Hastings et al., 2005; Malatesta et al., 2002) seeks to expand upon the original bagof-words approach which becomes less useful as causality and word order become more important. These methods are often supplemented with some light preprocessing, e.g., spelling correction, punctuation correction, pronoun resolution, lemmatization and tagging. Likewise, in order to facilitate their goals of providing feedback to the student more robust than a simple ”correct” or ”incorrect,” several systems break the gold-standard answers into constituent concepts that must ind</context>
<context position="6819" citStr="Landauer and Dumais, 1997" startWordPosition="1080" endWordPosition="1083">01). In this way the system can determine which parts of an answer a student understands and which parts he or she is struggling with. Automatic short answer grading is closely related to the task of text similarity. While more general than short answer grading, text similarity is essentially the problem of detecting and comparing the features of two texts. One of the earliest approaches to text similarity is the vector-space model (Salton et al., 1997) with a term frequency / inverse document frequency (tfidj) weighting. This model, along with the more sophisticated LSA semantic alternative (Landauer and Dumais, 1997), has been found to work well for tasks such as information retrieval and text classification. Another approach (Hatzivassiloglou et al., 1999) has been to use a machine learning algorithm in which features are based on combinations of simple features (e.g., a pair of nouns appear within 5 words from one another in both texts). This method also attempts to account for synonymy, word ordering, text length, and word classes. Another line of work attempts to extrapolate text similarity from the arguably simpler problem of word similarity. (Mihalcea et al., 2006) explores the efficacy of applying </context>
<context position="19960" citStr="Landauer and Dumais, 1997" startWordPosition="3223" endWordPosition="3226">pair in a text using the WordNet hierarchy. 5.2 Corpus-Based Measures Corpus-based measures differ from knowledgebased methods in that they do not require any encoded understanding of either the vocabulary or the grammar of a text’s language. In many of the scenarios where CAA would be advantageous, robust language-specific resources (e.g. WordNet) may not be available. Thus, state-of-the-art corpus-based measures may be the only available approach to CAA in languages with scarce resources. One corpus-based measure of semantic similarity is latent semantic analysis (LSA) proposed by Landauer (Landauer and Dumais, 1997). In LSA, term co-occurrences in a corpus are captured by means of a dimensionality reduction operated by a singular value decomposition (SVD) on the termby-document matrix T representing the corpus. For the experiments reported in this section, we run the SVD operation on several corpora including the BNC (LSA BNC) and the entire English Wikipedia (LSA Wikipedia).4 Explicit semantic analysis (ESA) (Gabrilovich and Markovitch, 2007) is a variation on the standard vectorial model in which the dimensions of the vector are directly equivalent to abstract concepts. Each article in Wikipedia repres</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T.K. Landauer and S.T. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and WordNet sense similarity for word sense identification. In WordNet, An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="17767" citStr="Leacock and Chodorow, 1998" startWordPosition="2856" endWordPosition="2859">sim(W, C): maxsim(W, C) = max SIMx(W, wi) where wi is a word in the student answer of class C and the SIMx function is one of the functions described below. All the word-to-word similarity scores obtained in this way are summed up and normalized with the length of the two input texts. We provide below a short description for each of these similarity metrics. 5.1 Knowledge-Based Measures The shortest path similarity is determined as: 1 Simpath = (1) length where length is the length of the shortest path between two concepts using node-counting (including the end nodes). The Leacock &amp; Chodorow (Leacock and Chodorow, 1998) similarity is determined as: Simlch = − log length *D* D (2) where length is the length of the shortest path between two concepts using node-counting, and D is the maximum depth of the taxonomy. The Lesk similarity of two concepts is defined as a function of the overlap between the corresponding definitions, as provided by a dictionary. It is based on an algorithm proposed by Lesk (1986) as a solution for word sense disambiguation. The Wu &amp; Palmer (Wu and Palmer, 1994) similarity metric measures the depth of two given concepts in the WordNet taxonomy, and the depth of the least common subsume</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>C. Leacock and M. Chodorow. 1998. Combining local context and WordNet sense similarity for word sense identification. In WordNet, An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<date>2003</date>
<booktitle>C-rater: Automated Scoring of Short-Answer Questions. Computers and the Humanities,</booktitle>
<pages>37--4</pages>
<contexts>
<context position="2100" citStr="Leacock and Chodorow, 2003" startWordPosition="328" endWordPosition="331"> some forms of computer-assisted assessment do not require sophisticated text understanding (e.g., multiple choice or true/false questions can be easily graded by a system if the correct solution is available), there are also student answers that consist of free text which require an analysis of the text in the answer. Research to date has concentrated on two main subtasks of computerassisted assessment: the grading of essays, which is done mainly by checking the style, grammaticality, and coherence of the essay (cf. (Higgins et al., 2004)), and the assessment of short student answers (e.g., (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005)), which is the focus of this paper. An automatic short answer grading system is one which automatically assigns a grade to an answer provided by a student through a comparison with one or more correct answers. It is important to note that this is different from the related task of paraphrase detection, since a requirement in student answer grading is to provide a grade on a certain scale rather than a binary yes/no decision. In this paper, we explore and evaluate a set of unsupervised techniques for automatic short answer grading. Unlike previous work, which has e</context>
<context position="4812" citStr="Leacock and Chodorow, 2003" startWordPosition="765" endWordPosition="768">ssociation for Computational Linguistics 567 able, these patterns can be supplemented by learning additional patterns semi-automatically. The Oxford-UCLES system (Sukkarieh et al., 2004) bootstraps patterns by starting with a set of keywords and synonyms and searching through windows of a text for new patterns. A later implementation of the Oxford-UCLES system (Pulman and Sukkarieh, 2005) compares several machine learning techniques, including inductive logic programming, decision tree learning, and Bayesian learning, to the earlier pattern matching approach with encouraging results. C-Rater (Leacock and Chodorow, 2003) matches the syntactical features of a student response (subject, object, and verb) to that of a set of correct responses. The method specifically disregards the bag-of-words approach to take into account the difference between ”dog bites man” and ”man bites dog” while trying to detect changes in voice (”the man was bitten by a dog”). Another short answer grading system, AutoTutor (Wiemer-Hastings et al., 1999), has been designed as an immersive tutoring environment with a graphical ”talking head” and speech recognition to improve the overall experience for students. AutoTutor eschews the patt</context>
</contexts>
<marker>Leacock, Chodorow, 2003</marker>
<rawString>C. Leacock and M. Chodorow. 2003. C-rater: Automated Scoring of Short-Answer Questions. Computers and the Humanities, 37(4):389–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Lee</author>
<author>B Pincombe</author>
<author>M Welsh</author>
</authors>
<title>An empirical evaluation of models of text document similarity.</title>
<date>2005</date>
<booktitle>Proceedings of the 27th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1254--1259</pages>
<contexts>
<context position="7679" citStr="Lee et al., 2005" startWordPosition="1218" endWordPosition="1221">features (e.g., a pair of nouns appear within 5 words from one another in both texts). This method also attempts to account for synonymy, word ordering, text length, and word classes. Another line of work attempts to extrapolate text similarity from the arguably simpler problem of word similarity. (Mihalcea et al., 2006) explores the efficacy of applying WordNet-based word-to-word similarity measures (Pedersen et al., 2004) to the comparison of texts and found them generally comparable to corpus-based measures such as LSA. An interesting study has been performed at the University of Adelaide (Lee et al., 2005), comparing simpler word and n-gram feature vectors to LSA and exploring the types of vector similarity metrics (e.g., binary vs. count vectors, Jaccard vs. cosine vs. overlap distance measure, etc.). In this case, LSA was shown to perform better than the word and n-gram vectors and performed best at around 100 dimensions with binary vectors weighted according to an entropy measure, though the difference in measures was often subtle. SELSA (Kanejiya et al., 2003) is a system that attempts to add context to LSA by supplementing the feature vectors with some simple syntactical features, namely t</context>
</contexts>
<marker>Lee, Pincombe, Welsh, 2005</marker>
<rawString>M.D. Lee, B. Pincombe, and M. Welsh. 2005. An empirical evaluation of models of text document similarity. Proceedings of the 27th Annual Conference of the Cognitive Science Society, pages 1254–1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the SIGDOC Conference</booktitle>
<location>Toronto,</location>
<contexts>
<context position="18158" citStr="Lesk (1986)" startWordPosition="2928" endWordPosition="2929"> similarity is determined as: 1 Simpath = (1) length where length is the length of the shortest path between two concepts using node-counting (including the end nodes). The Leacock &amp; Chodorow (Leacock and Chodorow, 1998) similarity is determined as: Simlch = − log length *D* D (2) where length is the length of the shortest path between two concepts using node-counting, and D is the maximum depth of the taxonomy. The Lesk similarity of two concepts is defined as a function of the overlap between the corresponding definitions, as provided by a dictionary. It is based on an algorithm proposed by Lesk (1986) as a solution for word sense disambiguation. The Wu &amp; Palmer (Wu and Palmer, 1994) similarity metric measures the depth of two given concepts in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combines these figures into a similarity score: 2 ∗ depth(LCS) Simwup = depth(concept1) + depth(concept2) (3) The measure introduced by Resnik (Resnik, 1995) returns the information content (IC) of the LCS of two concepts: Simres = IC(LCS) (4) where IC is defined as: IC(c) = − log P(c) (5) and P(c) is the probability of encountering an instance of concept c in a large corpus.</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M.E. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the SIGDOC Conference 1986, Toronto, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<location>Madison, WI.</location>
<contexts>
<context position="18804" citStr="Lin, 1998" startWordPosition="3042" endWordPosition="3043">iguation. The Wu &amp; Palmer (Wu and Palmer, 1994) similarity metric measures the depth of two given concepts in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combines these figures into a similarity score: 2 ∗ depth(LCS) Simwup = depth(concept1) + depth(concept2) (3) The measure introduced by Resnik (Resnik, 1995) returns the information content (IC) of the LCS of two concepts: Simres = IC(LCS) (4) where IC is defined as: IC(c) = − log P(c) (5) and P(c) is the probability of encountering an instance of concept c in a large corpus. 570 The measure introduced by Lin (Lin, 1998) builds on Resnik’s measure of similarity, and adds a normalization factor consisting of the information content of the two input concepts: Simiz = 2 * IC(LCS) (6) � IC(concept1) + IC(concept2) We also consider the Jiang &amp; Conrath (Jiang and Conrath, 1997) measure of similarity: 1 Simin, = IC(concept1) + IC(concept2) − 2 * IC(LCS) (7) Finally, we consider the Hirst &amp; St. Onge (Hirst and St-Onge, 1998) measure of similarity, which determines the similarity strength of a pair of synsets by detecting lexical chains between the pair in a text using the WordNet hierarchy. 5.2 Corpus-Based Measures </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, Madison, WI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K I Malatesta</author>
<author>P Wiemer-Hastings</author>
<author>J Robertson</author>
</authors>
<title>Beyond the Short Answer Question with Research Methods Tutor.</title>
<date>2002</date>
<booktitle>In Proceedings of the Intelligent Tutoring Systems Conference.</booktitle>
<contexts>
<context position="5587" citStr="Malatesta et al., 2002" startWordPosition="887" endWordPosition="890">gards the bag-of-words approach to take into account the difference between ”dog bites man” and ”man bites dog” while trying to detect changes in voice (”the man was bitten by a dog”). Another short answer grading system, AutoTutor (Wiemer-Hastings et al., 1999), has been designed as an immersive tutoring environment with a graphical ”talking head” and speech recognition to improve the overall experience for students. AutoTutor eschews the pattern-based approach entirely in favor of a bag-of-words LSA approach (Landauer and Dumais, 1997). Later work on AutoTutor (Wiemer-Hastings et al., 2005; Malatesta et al., 2002) seeks to expand upon the original bagof-words approach which becomes less useful as causality and word order become more important. These methods are often supplemented with some light preprocessing, e.g., spelling correction, punctuation correction, pronoun resolution, lemmatization and tagging. Likewise, in order to facilitate their goals of providing feedback to the student more robust than a simple ”correct” or ”incorrect,” several systems break the gold-standard answers into constituent concepts that must individually be matched for the answer to be considered fully correct (Callear et a</context>
</contexts>
<marker>Malatesta, Wiemer-Hastings, Robertson, 2002</marker>
<rawString>K.I. Malatesta, P. Wiemer-Hastings, and J. Robertson. 2002. Beyond the Short Answer Question with Research Methods Tutor. In Proceedings of the Intelligent Tutoring Systems Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Corley</author>
<author>C Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based approaches to text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the American Association for Artificial Intelligence (AAAI 2006),</booktitle>
<location>Boston.</location>
<contexts>
<context position="7384" citStr="Mihalcea et al., 2006" startWordPosition="1174" endWordPosition="1177">ated LSA semantic alternative (Landauer and Dumais, 1997), has been found to work well for tasks such as information retrieval and text classification. Another approach (Hatzivassiloglou et al., 1999) has been to use a machine learning algorithm in which features are based on combinations of simple features (e.g., a pair of nouns appear within 5 words from one another in both texts). This method also attempts to account for synonymy, word ordering, text length, and word classes. Another line of work attempts to extrapolate text similarity from the arguably simpler problem of word similarity. (Mihalcea et al., 2006) explores the efficacy of applying WordNet-based word-to-word similarity measures (Pedersen et al., 2004) to the comparison of texts and found them generally comparable to corpus-based measures such as LSA. An interesting study has been performed at the University of Adelaide (Lee et al., 2005), comparing simpler word and n-gram feature vectors to LSA and exploring the types of vector similarity metrics (e.g., binary vs. count vectors, Jaccard vs. cosine vs. overlap distance measure, etc.). In this case, LSA was shown to perform better than the word and n-gram vectors and performed best at aro</context>
<context position="13485" citStr="Mihalcea et al., 2006" startWordPosition="2157" endWordPosition="2160">Short Answer Grading Our experiments are centered around the use of measures of similarity for automatic short answer grading. In particular, we carry out three sets of experiments, seeking answers to the following three research questions. First, what are the measures of semantic similarity that work best for the task of short answer grading? To answer this question, we run several comparative evaluations covering a number of knowledge-based and corpus-based measures of semantic similarity. While previous work has considered such comparisons for the related task of paraphrase identification (Mihalcea et al., 2006), to our knowledge no comprehensive evaluation has been carried out for the task of short answer grading which includes all the similarity measures proposed to date. Second, to what extent do the domain and the size of the data used to train the corpus-based measures of similarity influence the accuracy of the measures? To address this question, we run a set of experiments which vary the size and domain of the corpus used to train the LSA and the ESA metrics, and we measure their effect on the accuracy of short answer grading. Finally, given a measure of similarity, can we integrate the answer</context>
<context position="16857" citStr="Mihalcea et al., 2006" startWordPosition="2697" endWordPosition="2700">n correlation for this model, using an inverse document frequency derived from the British National Corpus (BNC), is r=0.3647 for the per-question evaluation and r=0.4897 for the per-assignment evaluation. 5 Text-to-text Semantic Similarity We run our comparative evaluations using eight knowledge-based measures of semantic similarity (shortest path, Leacock &amp; Chodorow, Lesk, Wu &amp; Palmer, Resnik, Lin, Jiang &amp; Conrath, Hirst &amp; St. Onge), and two corpus-based measures (LSA and ESA). For the knowledge-based measures, we derive a text-to-text similarity metric by using the methodology proposed in (Mihalcea et al., 2006): for each open-class word in one of the input texts, we use the maximum semantic similarity that can be obtained by pairing it up with individual openclass words in the second input text. More formally, for each word W of part-of-speech class C in the instructor answer, we find maxsim(W, C): maxsim(W, C) = max SIMx(W, wi) where wi is a word in the student answer of class C and the SIMx function is one of the functions described below. All the word-to-word similarity scores obtained in this way are summed up and normalized with the length of the two input texts. We provide below a short descri</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>R. Mihalcea, C. Corley, and C. Strapparava. 2006. Corpus-based and knowledge-based approaches to text semantic similarity. In Proceedings of the American Association for Artificial Intelligence (AAAI 2006), Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
<author>T Russell</author>
<author>P Broomhead</author>
<author>N Aldridge</author>
</authors>
<title>Towards robust computerised marking of free-text responses.</title>
<date>2002</date>
<booktitle>Proceedings of the 6th International Computer Assisted Assessment (CAA) Conference.</booktitle>
<contexts>
<context position="2808" citStr="Mitchell et al., 2002" startWordPosition="449" endWordPosition="452">nswer grading system is one which automatically assigns a grade to an answer provided by a student through a comparison with one or more correct answers. It is important to note that this is different from the related task of paraphrase detection, since a requirement in student answer grading is to provide a grade on a certain scale rather than a binary yes/no decision. In this paper, we explore and evaluate a set of unsupervised techniques for automatic short answer grading. Unlike previous work, which has either required the availability of manually crafted patterns (Sukkarieh et al., 2004; Mitchell et al., 2002), or large training data sets to bootstrap such patterns (Pulman and Sukkarieh, 2005), we attempt to devise an unsupervised method that requires no human intervention. We address the grading problem from a text similarity perspective and examine the usefulness of various textto-text semantic similarity measures for automatically grading short student answers. Specifically, in this paper we seek answers to the following questions. First, given a number of corpus-based and knowledge-based methods as previously proposed in the past for word and text semantic similarity, what are the measures that</context>
</contexts>
<marker>Mitchell, Russell, Broomhead, Aldridge, 2002</marker>
<rawString>T. Mitchell, T. Russell, P. Broomhead, and N. Aldridge. 2002. Towards robust computerised marking of free-text responses. Proceedings of the 6th International Computer Assisted Assessment (CAA) Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question/answer classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="34564" citStr="Moschitti et al., 2007" startWordPosition="5600" endWordPosition="5603">sists of an LSA measure trained on a domain-specific corpus built on Wikipedia with feedback from student answers, which was found to bring a significant absolute improvement on the 0-1 Pearson scale of 0.14 over the tf*idf baseline and 0.10 over the LSA BNC model that has been used in the past. In future work, we intend to expand our analysis of both the gold-standard answer and the student answers beyond the bag-of-words paradigm by considering basic logical features in the text (i.e., AND, OR, NOT) as well as the existence of shallow grammatical features such as predicateargument structure(Moschitti et al., 2007) as well as semantic classes for words. Furthermore, it may be advantageous to expand upon the existing measures by applying machine learning techniques to create a hybrid decision system that would exploit the advantages of each measure. The data set introduced in this paper, along with the human-assigned grades, can be downloaded from http://lit.csci.unt.edu/index.php/Downloads. Acknowledgments This work was partially supported by a National Science Foundation CAREER award #0747340. The authors are grateful to Samer Hassan for making available his implementation of the ESA algorithm. Referen</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question/answer classification. In Proceedings of the 45th Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>Using measures of semantic relatedness for word sense disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<location>Mexico City,</location>
<contexts>
<context position="21174" citStr="Patwardhan et al., 2003" startWordPosition="3418" endWordPosition="3421">dia represents a concept in the ESA vector. The relatedness of a term to a concept is defined as the tf*idf score for the term within the Wikipedia article, and the relatedness between two words is the cosine of the two concept vectors in a high-dimensional space. We refer to this method as ESA Wikipedia. 4Throughout this paper, the references to the Wikipedia corpus refer to a version downloaded in September 2007. 5.3 Implementation For the knowledge-based measures, we use the WordNet-based implementation of the word-toword similarity metrics, as available in the WordNet::Similarity package (Patwardhan et al., 2003). For latent semantic analysis, we use the InfoMap package.5 For ESA, we use our own implementation of the ESA algorithm as described in (Gabrilovich and Markovitch, 2006). Note that all the word similarity measures are normalized so that they fall within a 0–1 range. The normalization is done by dividing the similarity score provided by a given measure with the maximum possible score for that measure. Table 2 shows the results obtained with each of these measures on our evaluation data set. Measure Correlation Knowledge-based measures Shortest path 0.4413 Leacock &amp; Chodorow 0.2231 Lesk 0.3630</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2003</marker>
<rawString>S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Using measures of semantic relatedness for word sense disambiguation. In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, Mexico City, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>WordNet:: Similarity-Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>1024--1025</pages>
<contexts>
<context position="7489" citStr="Pedersen et al., 2004" startWordPosition="1187" endWordPosition="1190">information retrieval and text classification. Another approach (Hatzivassiloglou et al., 1999) has been to use a machine learning algorithm in which features are based on combinations of simple features (e.g., a pair of nouns appear within 5 words from one another in both texts). This method also attempts to account for synonymy, word ordering, text length, and word classes. Another line of work attempts to extrapolate text similarity from the arguably simpler problem of word similarity. (Mihalcea et al., 2006) explores the efficacy of applying WordNet-based word-to-word similarity measures (Pedersen et al., 2004) to the comparison of texts and found them generally comparable to corpus-based measures such as LSA. An interesting study has been performed at the University of Adelaide (Lee et al., 2005), comparing simpler word and n-gram feature vectors to LSA and exploring the types of vector similarity metrics (e.g., binary vs. count vectors, Jaccard vs. cosine vs. overlap distance measure, etc.). In this case, LSA was shown to perform better than the word and n-gram vectors and performed best at around 100 dimensions with binary vectors weighted according to an entropy measure, though the difference in</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. WordNet:: Similarity-Measuring the Relatedness of Concepts. Proceedings of the National Conference on Artificial Intelligence, pages 1024–1025.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S G Pulman</author>
<author>J Z Sukkarieh</author>
</authors>
<title>Automatic Short Answer Marking. ACL WS Bldg Ed Apps using NLP.</title>
<date>2005</date>
<contexts>
<context position="2129" citStr="Pulman and Sukkarieh, 2005" startWordPosition="332" endWordPosition="336">sted assessment do not require sophisticated text understanding (e.g., multiple choice or true/false questions can be easily graded by a system if the correct solution is available), there are also student answers that consist of free text which require an analysis of the text in the answer. Research to date has concentrated on two main subtasks of computerassisted assessment: the grading of essays, which is done mainly by checking the style, grammaticality, and coherence of the essay (cf. (Higgins et al., 2004)), and the assessment of short student answers (e.g., (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005)), which is the focus of this paper. An automatic short answer grading system is one which automatically assigns a grade to an answer provided by a student through a comparison with one or more correct answers. It is important to note that this is different from the related task of paraphrase detection, since a requirement in student answer grading is to provide a grade on a certain scale rather than a binary yes/no decision. In this paper, we explore and evaluate a set of unsupervised techniques for automatic short answer grading. Unlike previous work, which has either required the availabili</context>
<context position="4576" citStr="Pulman and Sukkarieh, 2005" startWordPosition="733" endWordPosition="736">ch, if matched, indicate that a question has been answered correctly. If an annotated corpus is availProceedings of the 12th Conference of the European Chapter of the ACL, pages 567–575, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 567 able, these patterns can be supplemented by learning additional patterns semi-automatically. The Oxford-UCLES system (Sukkarieh et al., 2004) bootstraps patterns by starting with a set of keywords and synonyms and searching through windows of a text for new patterns. A later implementation of the Oxford-UCLES system (Pulman and Sukkarieh, 2005) compares several machine learning techniques, including inductive logic programming, decision tree learning, and Bayesian learning, to the earlier pattern matching approach with encouraging results. C-Rater (Leacock and Chodorow, 2003) matches the syntactical features of a student response (subject, object, and verb) to that of a set of correct responses. The method specifically disregards the bag-of-words approach to take into account the difference between ”dog bites man” and ”man bites dog” while trying to detect changes in voice (”the man was bitten by a dog”). Another short answer gradin</context>
</contexts>
<marker>Pulman, Sukkarieh, 2005</marker>
<rawString>S.G. Pulman and J.Z. Sukkarieh. 2005. Automatic Short Answer Marking. ACL WS Bldg Ed Apps using NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="18537" citStr="Resnik, 1995" startWordPosition="2991" endWordPosition="2992"> is the maximum depth of the taxonomy. The Lesk similarity of two concepts is defined as a function of the overlap between the corresponding definitions, as provided by a dictionary. It is based on an algorithm proposed by Lesk (1986) as a solution for word sense disambiguation. The Wu &amp; Palmer (Wu and Palmer, 1994) similarity metric measures the depth of two given concepts in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combines these figures into a similarity score: 2 ∗ depth(LCS) Simwup = depth(concept1) + depth(concept2) (3) The measure introduced by Resnik (Resnik, 1995) returns the information content (IC) of the LCS of two concepts: Simres = IC(LCS) (4) where IC is defined as: IC(c) = − log P(c) (5) and P(c) is the probability of encountering an instance of concept c in a large corpus. 570 The measure introduced by Lin (Lin, 1998) builds on Resnik’s measure of similarity, and adds a normalization factor consisting of the information content of the two input concepts: Simiz = 2 * IC(LCS) (6) � IC(concept1) + IC(concept2) We also consider the Jiang &amp; Conrath (Jiang and Conrath, 1997) measure of similarity: 1 Simin, = IC(concept1) + IC(concept2) − 2 * IC(LCS) </context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rocchio</author>
</authors>
<title>Relevance feedback in information retrieval.</title>
<date>1971</date>
<publisher>Prentice Hall,</publisher>
<location>Ing. Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="14264" citStr="Rocchio, 1971" startWordPosition="2293" endWordPosition="2295"> Second, to what extent do the domain and the size of the data used to train the corpus-based measures of similarity influence the accuracy of the measures? To address this question, we run a set of experiments which vary the size and domain of the corpus used to train the LSA and the ESA metrics, and we measure their effect on the accuracy of short answer grading. Finally, given a measure of similarity, can we integrate the answers with the highest scores and improve the accuracy of the measure? We use a technique similar to the pseudo-relevance feedback method used in information retrieval (Rocchio, 1971) and augment the correct answer with type” does seem to be a valid component of a ”function signature” according to some literature on the web. 3Consider this an open call for discussion in the NLP community regarding the proper usage of correlation metrics with the ultimate goal of consistency within the community. 569 Sample questions, correct answers, and student answers Grade Question: What is the role of a prototype program in problem solving? Correct answer: To simulate the behavior ofportions of the desired software product. Student answer 1: A prototype program is used in problem solvi</context>
<context position="27004" citStr="Rocchio, 1971" startWordPosition="4373" endWordPosition="4374">act effect of corpus size on performance. 7 Relevance Feedback based on Student Answers The automatic grading of student answers implies a measure of similarity between the answers provided by the students and the correct answer provided by the instructor. Since we only have one correct answer, some student answers may be wrongly graded because of little or no similarity with the correct answer that we have. To address this problem, we introduce a novel technique that feeds back from the student answers themselves in a way similar to the pseudorelevance feedback used in information retrieval (Rocchio, 1971). In this way, the paraphrasing that is usually observed across student answers will enhance the vocabulary of the correct answer, while at the same time maintaining the correctness of the gold-standard answer. Briefly, given a metric that provides similarity scores between the student answers and the correct answer, scores are ranked from most similar (p&lt;0.001). 7In ESA, all the articles in Wikipedia are used as dimensions, which leads to about 1.75 million dimensions in the ESA Wikipedia corpus, compared to only 55,000 dimensions in the ESA Wikipedia CS corpus. 572 to least. The words of the</context>
</contexts>
<marker>Rocchio, 1971</marker>
<rawString>J. Rocchio, 1971. Relevance feedback in information retrieval. Prentice Hall, Ing. Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1997</date>
<booktitle>In Readings in Information Retrieval,</booktitle>
<pages>273--280</pages>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="6650" citStr="Salton et al., 1997" startWordPosition="1056" endWordPosition="1059">stems break the gold-standard answers into constituent concepts that must individually be matched for the answer to be considered fully correct (Callear et al., 2001). In this way the system can determine which parts of an answer a student understands and which parts he or she is struggling with. Automatic short answer grading is closely related to the task of text similarity. While more general than short answer grading, text similarity is essentially the problem of detecting and comparing the features of two texts. One of the earliest approaches to text similarity is the vector-space model (Salton et al., 1997) with a term frequency / inverse document frequency (tfidj) weighting. This model, along with the more sophisticated LSA semantic alternative (Landauer and Dumais, 1997), has been found to work well for tasks such as information retrieval and text classification. Another approach (Hatzivassiloglou et al., 1999) has been to use a machine learning algorithm in which features are based on combinations of simple features (e.g., a pair of nouns appear within 5 words from one another in both texts). This method also attempts to account for synonymy, word ordering, text length, and word classes. Anot</context>
</contexts>
<marker>Salton, Wong, Yang, 1997</marker>
<rawString>G. Salton, A. Wong, and C.S. Yang. 1997. A vector space model for automatic indexing. In Readings in Information Retrieval, pages 273–280. Morgan Kaufmann Publishers, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Z Sukkarieh</author>
<author>S G Pulman</author>
<author>N Raikes</author>
</authors>
<title>Auto-Marking 2: An Update on the UCLES-Oxford University research into using Computational Linguistics to Score Short, Free Text Responses.</title>
<date>2004</date>
<journal>International Association of Educational Assessment, Philadephia.</journal>
<contexts>
<context position="2784" citStr="Sukkarieh et al., 2004" startWordPosition="445" endWordPosition="448">er. An automatic short answer grading system is one which automatically assigns a grade to an answer provided by a student through a comparison with one or more correct answers. It is important to note that this is different from the related task of paraphrase detection, since a requirement in student answer grading is to provide a grade on a certain scale rather than a binary yes/no decision. In this paper, we explore and evaluate a set of unsupervised techniques for automatic short answer grading. Unlike previous work, which has either required the availability of manually crafted patterns (Sukkarieh et al., 2004; Mitchell et al., 2002), or large training data sets to bootstrap such patterns (Pulman and Sukkarieh, 2005), we attempt to devise an unsupervised method that requires no human intervention. We address the grading problem from a text similarity perspective and examine the usefulness of various textto-text semantic similarity measures for automatically grading short student answers. Specifically, in this paper we seek answers to the following questions. First, given a number of corpus-based and knowledge-based methods as previously proposed in the past for word and text semantic similarity, wh</context>
<context position="4371" citStr="Sukkarieh et al., 2004" startWordPosition="698" endWordPosition="701">at have been proposed in the past for automatic short answer grading. Several state-of-the-art short answer graders (Sukkarieh et al., 2004; Mitchell et al., 2002) require manually crafted patterns which, if matched, indicate that a question has been answered correctly. If an annotated corpus is availProceedings of the 12th Conference of the European Chapter of the ACL, pages 567–575, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 567 able, these patterns can be supplemented by learning additional patterns semi-automatically. The Oxford-UCLES system (Sukkarieh et al., 2004) bootstraps patterns by starting with a set of keywords and synonyms and searching through windows of a text for new patterns. A later implementation of the Oxford-UCLES system (Pulman and Sukkarieh, 2005) compares several machine learning techniques, including inductive logic programming, decision tree learning, and Bayesian learning, to the earlier pattern matching approach with encouraging results. C-Rater (Leacock and Chodorow, 2003) matches the syntactical features of a student response (subject, object, and verb) to that of a set of correct responses. The method specifically disregards t</context>
</contexts>
<marker>Sukkarieh, Pulman, Raikes, 2004</marker>
<rawString>J.Z. Sukkarieh, S.G. Pulman, and N. Raikes. 2004. Auto-Marking 2: An Update on the UCLES-Oxford University research into using Computational Linguistics to Score Short, Free Text Responses. International Association of Educational Assessment, Philadephia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wiemer-Hastings</author>
<author>K Wiemer-Hastings</author>
<author>A Graesser</author>
</authors>
<title>Improving an intelligent tutor’s comprehension of students with Latent Semantic Analysis. Artificial Intelligence in Education,</title>
<date>1999</date>
<pages>535--542</pages>
<contexts>
<context position="5226" citStr="Wiemer-Hastings et al., 1999" startWordPosition="831" endWordPosition="834">hine learning techniques, including inductive logic programming, decision tree learning, and Bayesian learning, to the earlier pattern matching approach with encouraging results. C-Rater (Leacock and Chodorow, 2003) matches the syntactical features of a student response (subject, object, and verb) to that of a set of correct responses. The method specifically disregards the bag-of-words approach to take into account the difference between ”dog bites man” and ”man bites dog” while trying to detect changes in voice (”the man was bitten by a dog”). Another short answer grading system, AutoTutor (Wiemer-Hastings et al., 1999), has been designed as an immersive tutoring environment with a graphical ”talking head” and speech recognition to improve the overall experience for students. AutoTutor eschews the pattern-based approach entirely in favor of a bag-of-words LSA approach (Landauer and Dumais, 1997). Later work on AutoTutor (Wiemer-Hastings et al., 2005; Malatesta et al., 2002) seeks to expand upon the original bagof-words approach which becomes less useful as causality and word order become more important. These methods are often supplemented with some light preprocessing, e.g., spelling correction, punctuation</context>
</contexts>
<marker>Wiemer-Hastings, Wiemer-Hastings, Graesser, 1999</marker>
<rawString>P. Wiemer-Hastings, K. Wiemer-Hastings, and A. Graesser. 1999. Improving an intelligent tutor’s comprehension of students with Latent Semantic Analysis. Artificial Intelligence in Education, pages 535–542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wiemer-Hastings</author>
<author>E Arnott</author>
<author>D Allbritton</author>
</authors>
<title>Initial results and mixed directions for research methods tutor.</title>
<date>2005</date>
<booktitle>In AIED2005 - Supplementary Proceedings of the 12th International Conference on Artificial Intelligence in Education,</booktitle>
<location>Amsterdam.</location>
<contexts>
<context position="5562" citStr="Wiemer-Hastings et al., 2005" startWordPosition="883" endWordPosition="886"> The method specifically disregards the bag-of-words approach to take into account the difference between ”dog bites man” and ”man bites dog” while trying to detect changes in voice (”the man was bitten by a dog”). Another short answer grading system, AutoTutor (Wiemer-Hastings et al., 1999), has been designed as an immersive tutoring environment with a graphical ”talking head” and speech recognition to improve the overall experience for students. AutoTutor eschews the pattern-based approach entirely in favor of a bag-of-words LSA approach (Landauer and Dumais, 1997). Later work on AutoTutor (Wiemer-Hastings et al., 2005; Malatesta et al., 2002) seeks to expand upon the original bagof-words approach which becomes less useful as causality and word order become more important. These methods are often supplemented with some light preprocessing, e.g., spelling correction, punctuation correction, pronoun resolution, lemmatization and tagging. Likewise, in order to facilitate their goals of providing feedback to the student more robust than a simple ”correct” or ”incorrect,” several systems break the gold-standard answers into constituent concepts that must individually be matched for the answer to be considered fu</context>
</contexts>
<marker>Wiemer-Hastings, Arnott, Allbritton, 2005</marker>
<rawString>P. Wiemer-Hastings, E. Arnott, and D. Allbritton. 2005. Initial results and mixed directions for research methods tutor. In AIED2005 - Supplementary Proceedings of the 12th International Conference on Artificial Intelligence in Education, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Wu</author>
<author>M Palmer</author>
</authors>
<title>Verb semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="18241" citStr="Wu and Palmer, 1994" startWordPosition="2941" endWordPosition="2944">ength of the shortest path between two concepts using node-counting (including the end nodes). The Leacock &amp; Chodorow (Leacock and Chodorow, 1998) similarity is determined as: Simlch = − log length *D* D (2) where length is the length of the shortest path between two concepts using node-counting, and D is the maximum depth of the taxonomy. The Lesk similarity of two concepts is defined as a function of the overlap between the corresponding definitions, as provided by a dictionary. It is based on an algorithm proposed by Lesk (1986) as a solution for word sense disambiguation. The Wu &amp; Palmer (Wu and Palmer, 1994) similarity metric measures the depth of two given concepts in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combines these figures into a similarity score: 2 ∗ depth(LCS) Simwup = depth(concept1) + depth(concept2) (3) The measure introduced by Resnik (Resnik, 1995) returns the information content (IC) of the LCS of two concepts: Simres = IC(LCS) (4) where IC is defined as: IC(c) = − log P(c) (5) and P(c) is the probability of encountering an instance of concept c in a large corpus. 570 The measure introduced by Lin (Lin, 1998) builds on Resnik’s measure of simila</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Z. Wu and M. Palmer. 1994. Verb semantics and lexical selection. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, Las Cruces, New Mexico.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>