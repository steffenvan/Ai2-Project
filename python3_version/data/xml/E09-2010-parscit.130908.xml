<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022983">
<title confidence="0.97876525">
Adaptive Natural Language Interaction
Stasinos Konstantopoulos Ion Androutsopoulos
Athanasios Tegos Gerasimos Lampouras
Dimitris Bilidas Prodromos Malakasiotis
</title>
<note confidence="0.5325275">
NCSR ‘Demokritos’, Athens, Greece Athens Univ. of Economics and Business
Greece
</note>
<author confidence="0.994031">
Colin Matheson
</author>
<affiliation confidence="0.9876775">
Human Communication Research Centre Olivier Deroo
Edinburgh University, U.K. Acapela Group, Belgium
</affiliation>
<sectionHeader confidence="0.978258" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999791">
The subject of this demonstration is natu-
ral language interaction, focusing on adap-
tivity and profiling of the dialogue man-
agement and the generated output (text
and speech). These are demonstrated in
a museum guide use-case, operating in a
simulated environment. The main techni-
cal innovations presented are the profiling
model, the dialogue and action manage-
ment system, and the text generation and
speech synthesis systems.
</bodyText>
<sectionHeader confidence="0.998802" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998451">
In this demonstration we present a number of
state-of-the art language technology tools, imple-
menting and integrating the latest discourse and
knowledge representation theories into a complete
application suite, including:
</bodyText>
<listItem confidence="0.9783586">
• dialogue management, natural language gen-
eration, and speech synthesis, all modulated
by a flexible and highly adaptable profiling
mechanism;
• robust speech recognition and language inter-
pretation; and,
• an authoring environment for developing the
representation of the domain of discourse as
well as the associated linguistic and adaptiv-
ity resources.
</listItem>
<bodyText confidence="0.999824333333333">
The system demonstration is based on a use
case of a virtual-tour guide in a museum domain.
Demonstration visitors interact with the guide us-
ing headsets and are able to experiment with load-
ing different interaction profiles and observing the
differences in the guide’s behaviour. The demon-
stration also includes the screening of videos from
an embodied instantiation of the system as a robot
guiding visitors in a museum.
</bodyText>
<sectionHeader confidence="0.997058" genericHeader="method">
2 Technical Content
</sectionHeader>
<bodyText confidence="0.999580413793103">
The demonstration integrates a number of state-of-
the-art language components into a highly adap-
tive natural language interaction system. Adap-
tivity here refers to using interaction profiles that
modulate dialogue management as well as text
generation and speech synthesis. Interaction pro-
files are semantic models that extend the objective
ontological model of the domain of discourse with
subjective information, such as how ‘interesting’
or ‘important’ an entity or statement of the objec-
tive domain model is.
Advanced multimodal dialogue management
capabilities involving and combining input and
output from various interaction modalities and
technologies, such as speech recognition and syn-
thesis, natural language interpretation and gener-
ation, and recognition of/response to user actions,
gestures, and facial expressions.
State-of-the art natural language generation
technology, capable of producing multi-sentence,
coherent natural language descriptions of objects
based on their abstract semantic representation.
The resulting descriptions vary dynamically in
terms of content as well as surface language ex-
pressions used to realize each description, depend-
ing on the interaction history (e.g., comparing
to previously given information) and the adaptiv-
ity parameters (exhibiting system personality and
adapting to user background and interests).
</bodyText>
<sectionHeader confidence="0.993254" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.99996975">
The system is capable of interacting in a vari-
ety of modalities, including non-verbal ones such
as gesture and face-expression recognition, but in
this demonstration we focus on the system’s lan-
guage interaction components. In this modality,
abstract, language-independent system actions are
first planned by the dialogue and action manager
(DAM), then realized into language-specific text
</bodyText>
<note confidence="0.3378435">
Proceedings of the EACL 2009 Demonstrations Session, pages 37–40,
Athens, Greece, 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.999474">
37
</page>
<bodyText confidence="0.999809333333333">
by the natural language generation engine, and fi-
nally synthesized into speech. All three layers are
parametrized by a profiling and adaptivity module.
</bodyText>
<subsectionHeader confidence="0.999808">
3.1 Profiling and Adaptation
</subsectionHeader>
<bodyText confidence="0.999816027777778">
Profiling and adaptation modulates the output of
dialogue management, generation, and speech
synthesis so that the system exhibits a synthetic
personality, while at the same time adapting to
user background and interests.
User stereotypes (e.g., ‘expert’ or ‘child’) pro-
vide generation parameters (such as maximum de-
scription length) and also initialize the dynamic
user model with interest rates for all the ontologi-
cal entities (individuals and properties) of the do-
main of discourse. This same information is also
provided in system profiles reflecting the system’s
(as opposed to the users’) preferences; one can,
for example, define a profile that favours using
the architectural attributes to describe a building
where another profile would choose to concentrate
on historical facts regarding the same building.
Stereotypes and profiles are combined into a
single set of parameters by means of personal-
ity models. Personality models are many-valued
Description Logic definitions of the overall pref-
erence, grounded in stereotype and profile data.
These definitions model recognizable personality
traits so that, for example, an open personality will
attend more to the user’s requests than its own
interests in deriving overall preference (Konstan-
topoulos et al., 2008).
Furthermore, the system dynamically adapts
overall preference according to both interaction
history and the current dialogue state. So, for one,
the initial (static model) interest factor of an ontol-
ogy entity is reduced each time this entity is used
in a description in order to avoid repetitions. On
the other hand, preference will increase if, for ex-
ample, in the current state the user has explicitly
asked about an entity.
</bodyText>
<subsectionHeader confidence="0.999233">
3.2 Dialogue and Action Management
</subsectionHeader>
<bodyText confidence="0.999964173913043">
The DAM is built around the information-state
update dialogue paradigm of the TRINDIKIT
dialogue-engine toolkit (Cooper and Larsson,
1998) and takes into account the combined user-
robot interest factor when determining informa-
tion state updates.
The DAM combines various interaction modal-
ities and technologies in both interpretation/fusion
and generation/fission. In interpreting user ac-
tions the system recognizes spoken utterances,
simple gestures, and touch-screen input, all of
which may be combined into a representation of
a multi-modal user action. Similarly, when plan-
ning robotic actions the DAM coordinates a num-
ber of available output modalities, including spo-
ken language, text (on the touchscreen), the move-
ment and configuration of the robotic platform, fa-
cial expressions, and simple head gestures.1
To handle multimodal input, the DAM uses a fu-
sion module which combines messages from the
language interpretation, gesture, and touchscreen
modules into a single XML structure. Schemati-
cally, this can be represented as:
</bodyText>
<figure confidence="0.59628875">
&lt;userAction&gt;
&lt;userUtterance&gt;hello&lt;/userUtterance&gt;
&lt;userButton content=&amp;quot;13&amp;quot;/&gt;
&lt;/userAction&gt;
</figure>
<bodyText confidence="0.980105133333333">
This structure represents a user pressing some-
thing on the touchscreen and saying hello at the
same time.2
The representation is passed essentially un-
changed to the DAM, to be processed by its up-
date rules, where the ID of button press is inter-
preted in context and matched with the speech.
In most circumstances, the natural language pro-
cessing component (see 3.3) produces a seman-
tic representation of the input which appears in
the userUtterance element; the use of ‘hello’
above is for illustration. An example update rule
which will fire in the context of a greeting from
the user is (in schematic form):
if
in(/latest_utterance/moves, hello)
then
output(start)
Update rules contain a list of conditions and a
list of effects. Here there is one condition (that the
latest moves from the user includes ‘hello’), and
one effect (the ‘start’ procedure). The latter initi-
ates the dialogue by, among other things, having
the system utter a standardised greeting.
As noted above, the DAM is also multimodal
on the output side. An XML representation is
created which can contain robot utterances and
robot movements (both head movements and mo-
bile platform moves). Information can also be pre-
sented on the touchscreen.
</bodyText>
<footnote confidence="0.9908995">
1Expressions and gestures will not be demonstrated, as
they can not be materialized in the simulated robot.
2The precise meaning of ‘at the same time’ is determined
by the fusion module.
</footnote>
<page confidence="0.996429">
38
</page>
<subsectionHeader confidence="0.992082">
3.3 Natural Language Processing
</subsectionHeader>
<bodyText confidence="0.99998125">
The NATURALOWL natural language generation
(NLG) engine (Galanis et al, 2009) produces
multi-sentence, coherent natural language descrip-
tions of objects in multiple languages from a sin-
gle semantic representation; the resulting descrip-
tions are annotated with prosodic markup for driv-
ing the speech synthesisers.
The generated descriptions vary dynamically, in
both content and language expressions, depending
on the interaction profile as well as the dynamic
interaction history. The dynamic preference factor
of the item itself is used to decide the level of de-
tail of the description being generated. The prefer-
ence factors of the properties are used to order the
contents of the descriptions to ensure that, in cases
where not all possible facts are to be presented in
a single turn, the most relevant ones are chosen.
The interaction history is used to check previously
given information to avoid repeating the same in-
formation in different contexts and to create com-
parisons with earlier objects.
NaturalOWL demonstrates the benefits of
adopting NLG on the Semantic Web. Organiza-
tions that need to publish information about ob-
jects, such as exhibits or products, can publish
OWL ontologies instead of texts. NLG engines,
embedded in browsers or Web servers, can then
render the ontologies in natural language, whereas
computer programs may access the ontologies, in
effect logical statements, directly. The descrip-
tions can be very simple and brief, relying on
question answering to provide more information
if such is requested. This way, machine-readable
information can be more naturally inspected and
consulted by users.
In order to generate a list of possible follow
up questions that the system can handle, we ini-
tially construct a list of the particular individuals
or classes that are mentioned in the generated de-
scription; the follow up questions will most likely
refer to them. Only individuals and classes for
which there is further information in the ontology
are extracted.
After identifying the referred individuals and
classes, we proceed to predict definition (e.g.,
‘Who was Ares?’) and property questions (e.g.,
‘Where is Mount Penteli?’) about them that
could be answered by the information in the on-
tology. We avoid generating questions that cannot
be answered. The expected definition questions
are constructed by inserting the names of the re-
ferred individuals and classes into templates such
as ‘who is/was person X?’ or ‘what do you know
about class or entity Y?’.
In the case of referred individuals, we also gen-
erate expected property questions using the pat-
terns NaturalOWL generates the descriptions with.
These patterns, called microplans, show how to
express the properties of the ontology as sentences
of the target languages. For example, if the indi-
vidual templeOfAres has the property excavate-
dIn, and that property has a microplan of the form
‘resource was excavated in period’, we anticipate
questions such as ‘when was the Temple of Ares
excavated?’ and ‘which period was the Temple of
Ares excavated in?’.
Whenever a description (e.g., of a monument)
is generated, the expected follow up questions for
that description (e.g., about the monument’s ar-
chitect) are dynamically included in the rules of
the speech recognizer’s grammar, to increase word
recognition accuracy. The rules include compo-
nents that extract entities, classes, and properties
from the recognized questions, thus allowing the
dialogue and action manager to figure out what the
user wishes to know.
</bodyText>
<subsectionHeader confidence="0.999815">
3.4 Speech Synthesis and Recognition
</subsectionHeader>
<bodyText confidence="0.999983565217391">
The natural language interface demonstrates ro-
bust speech recognition technology, capable of
recognizing spoken phrases in noisy environ-
ments, and advanced speech synthesis, capable of
producing spoken output of very high quality. The
main challenge that the automatic speech recogni-
tion (ASR) module needs to address is background
noise, especially in the robot-embodied use case.
A common technique used in order to handle this
is training acoustic models with the anticipated
background noise, but that is not always possi-
ble. The demonstrated ASR module can be trained
on noise-contaminated data where available, but
also incorporates multi-band acoustic modelling
(Dupont, 2003) for robust recognition under noisy
conditions. Speech recognition rates are also sub-
stantially improved by using the predictions made
by NATURALOWL and the DAM to dynamically
restrict the lexical and phrasal expectations at each
dialogue turn.
The speech synthesis module of the demon-
strated system is based on unit selection technol-
ogy, generally recognized as producing more nat-
</bodyText>
<page confidence="0.997896">
39
</page>
<bodyText confidence="0.9999622">
ural output that previous technologies such as di-
phone concatenation or formant synthesis. The
main innovation that is demonstrated is support for
emotion, a key aspect of increasing the naturalness
of synthetic speech. This is achieved by combin-
ing emotional unit recordings with run-time trans-
formations. With respect to the former, a complete
‘voice’ now comprises three sub-voices (neutral,
happy, and sad), based on recordings of the same
speaker. The recording time needed is substan-
tially decreased by prior linguistic analysis that se-
lects appropriate text covering all phonetic units
needed by the unit selection system. In addition to
the statically defined sub-voices, the speech syn-
thesis module implements dynamic transforma-
tions (e.g., emphasis), pauses, and variable speech
speed. The system combines all these capabilities
in order to dynamically modulate the synthesised
speech to convey the impression of emotionally
modulated speech.
</bodyText>
<subsectionHeader confidence="0.92984">
3.5 Authoring
</subsectionHeader>
<bodyText confidence="0.999953148148148">
The interaction system is complemented by
ELEON (Bilidas et al., 2007), an authoring tool for
annotating domain ontologies with the generation
and adaptivity resources described above. The do-
main ontology can be authored in ELEON, but any
existing OWL ontology can also annotated.
More specifically, ELEON supports author-
ing linguistic resources, including a domain-
dependent lexicon, which associates classes and
individuals of the ontology with nouns and proper
names of the target natural languages; microplans,
which provide the NLG with patterns for realizing
property instances as sentences; and a partial or-
dering of properties, which allows the system to
order the resulting sentences as a coherent text.
The adaptivity and profiling resources include
interest rates, indicating how interesting the enti-
ties of the ontology are in any given profile; and
stereotype parameters that control generation as-
pects such as the number of facts to include in a
description or the maximum sentence length.
Furthermore, ELEON supports the author with
immediate previews, so that the effect of any
change in either the ontology or the associated re-
sources can be directly reviewed. The actual gen-
eration of the preview is relegated to external gen-
eration engines.
</bodyText>
<sectionHeader confidence="0.999654" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.99999175">
The demonstrated system combines semantic rep-
resentation and reasoning technologies with lan-
guage technology into a human-computer interac-
tion system that exhibits a large degree of adapt-
ability to audiences and circumstances and is able
to take advantage of existing domain model cre-
ated independently of the need to build a natural
language interface. Furthermore by clearly sepa-
rating the abstract, semantic layer from that of the
linguistic realization, it allows the re-use of lin-
guistic resources across domains and the domain
model and adaptivity resources across languages.
</bodyText>
<sectionHeader confidence="0.9976" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999995">
The demonstrated system is being developed by
the European (FP6-IST) project INDIGO.3 IN-
DIGO develops and advances human-robot inter-
action technology, enabling robots to perceive nat-
ural human behaviour, as well as making them
act in ways that are more familiar to humans. To
achieve its goals, INDIGO advances various tech-
nologies, which it integrates in a robotic platform.
</bodyText>
<sectionHeader confidence="0.999105" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997954181818182">
Dimitris Bilidas, Maria Theologou, and Vangelis
Karkaletsis. 2007. Enriching OWL ontologies
with linguistic and user-related annotations: the
ELEON system. In Proc. 19th Intl. Conf. on
Tools with Artificial Intelligence (ICTAI-2007).
Robin Cooper and Staffan Larsson. 1998. Dia-
logue Moves and Information States. In: Pro-
ceedings of the 3rd Intl. Workshop on Computa-
tional Semantics (IWCS-3).
St´ephane Dupont. 2003. Robust parameters
for noisy speech recognition. U.S. Patent
2003182114.
Dimitrios Galanis, George Karakatsiotis, Gerasi-
mos Lampouras and Ion Androutsopoulos.
2009. An open-source natural language gener-
ator for OWL ontologies and its use in Prot´eg´e
and Second Life. In this volume.
Stasinos Konstantopoulos, Vangelis Karkaletsis,
and Colin Matheson. 2008. Robot personality:
Representation and externalization. In Proc.
Computational Aspects of Affective and Emo-
tional Interaction (CAFFEi 08), Patras, Greece.
</reference>
<footnote confidence="0.792994">
3http://www.ics.forth.gr/indigo/
</footnote>
<page confidence="0.997265">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.918928">
<title confidence="0.9886905">Adaptive Natural Language Interaction Stasinos Konstantopoulos Ion Androutsopoulos Athanasios Tegos Gerasimos Lampouras Dimitris Bilidas Prodromos Malakasiotis NCSR ‘Demokritos’, Athens, Greece Athens Univ. of Economics and Business Greece</title>
<author confidence="0.999225">Colin Matheson</author>
<affiliation confidence="0.981067">Communication Research Centre Deroo Edinburgh University, U.K. Acapela Group, Belgium</affiliation>
<abstract confidence="0.9997575">The subject of this demonstration is natural language interaction, focusing on adaptivity and profiling of the dialogue management and the generated output (text and speech). These are demonstrated in a museum guide use-case, operating in a simulated environment. The main technical innovations presented are the profiling model, the dialogue and action management system, and the text generation and speech synthesis systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dimitris Bilidas</author>
<author>Maria Theologou</author>
<author>Vangelis Karkaletsis</author>
</authors>
<title>Enriching OWL ontologies with linguistic and user-related annotations: the ELEON system.</title>
<date>2007</date>
<booktitle>In Proc. 19th Intl. Conf. on Tools with Artificial Intelligence (ICTAI-2007).</booktitle>
<contexts>
<context position="13884" citStr="Bilidas et al., 2007" startWordPosition="2081" endWordPosition="2084">d on recordings of the same speaker. The recording time needed is substantially decreased by prior linguistic analysis that selects appropriate text covering all phonetic units needed by the unit selection system. In addition to the statically defined sub-voices, the speech synthesis module implements dynamic transformations (e.g., emphasis), pauses, and variable speech speed. The system combines all these capabilities in order to dynamically modulate the synthesised speech to convey the impression of emotionally modulated speech. 3.5 Authoring The interaction system is complemented by ELEON (Bilidas et al., 2007), an authoring tool for annotating domain ontologies with the generation and adaptivity resources described above. The domain ontology can be authored in ELEON, but any existing OWL ontology can also annotated. More specifically, ELEON supports authoring linguistic resources, including a domaindependent lexicon, which associates classes and individuals of the ontology with nouns and proper names of the target natural languages; microplans, which provide the NLG with patterns for realizing property instances as sentences; and a partial ordering of properties, which allows the system to order th</context>
</contexts>
<marker>Bilidas, Theologou, Karkaletsis, 2007</marker>
<rawString>Dimitris Bilidas, Maria Theologou, and Vangelis Karkaletsis. 2007. Enriching OWL ontologies with linguistic and user-related annotations: the ELEON system. In Proc. 19th Intl. Conf. on Tools with Artificial Intelligence (ICTAI-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Cooper</author>
<author>Staffan Larsson</author>
</authors>
<title>Dialogue Moves and Information States. In:</title>
<date>1998</date>
<booktitle>Proceedings of the 3rd Intl. Workshop on Computational Semantics (IWCS-3).</booktitle>
<contexts>
<context position="5803" citStr="Cooper and Larsson, 1998" startWordPosition="836" endWordPosition="839">topoulos et al., 2008). Furthermore, the system dynamically adapts overall preference according to both interaction history and the current dialogue state. So, for one, the initial (static model) interest factor of an ontology entity is reduced each time this entity is used in a description in order to avoid repetitions. On the other hand, preference will increase if, for example, in the current state the user has explicitly asked about an entity. 3.2 Dialogue and Action Management The DAM is built around the information-state update dialogue paradigm of the TRINDIKIT dialogue-engine toolkit (Cooper and Larsson, 1998) and takes into account the combined userrobot interest factor when determining information state updates. The DAM combines various interaction modalities and technologies in both interpretation/fusion and generation/fission. In interpreting user actions the system recognizes spoken utterances, simple gestures, and touch-screen input, all of which may be combined into a representation of a multi-modal user action. Similarly, when planning robotic actions the DAM coordinates a number of available output modalities, including spoken language, text (on the touchscreen), the movement and configura</context>
</contexts>
<marker>Cooper, Larsson, 1998</marker>
<rawString>Robin Cooper and Staffan Larsson. 1998. Dialogue Moves and Information States. In: Proceedings of the 3rd Intl. Workshop on Computational Semantics (IWCS-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane Dupont</author>
</authors>
<title>Robust parameters for noisy speech recognition.</title>
<date>2003</date>
<tech>U.S. Patent 2003182114.</tech>
<contexts>
<context position="12463" citStr="Dupont, 2003" startWordPosition="1870" endWordPosition="1871">technology, capable of recognizing spoken phrases in noisy environments, and advanced speech synthesis, capable of producing spoken output of very high quality. The main challenge that the automatic speech recognition (ASR) module needs to address is background noise, especially in the robot-embodied use case. A common technique used in order to handle this is training acoustic models with the anticipated background noise, but that is not always possible. The demonstrated ASR module can be trained on noise-contaminated data where available, but also incorporates multi-band acoustic modelling (Dupont, 2003) for robust recognition under noisy conditions. Speech recognition rates are also substantially improved by using the predictions made by NATURALOWL and the DAM to dynamically restrict the lexical and phrasal expectations at each dialogue turn. The speech synthesis module of the demonstrated system is based on unit selection technology, generally recognized as producing more nat39 ural output that previous technologies such as diphone concatenation or formant synthesis. The main innovation that is demonstrated is support for emotion, a key aspect of increasing the naturalness of synthetic spee</context>
</contexts>
<marker>Dupont, 2003</marker>
<rawString>St´ephane Dupont. 2003. Robust parameters for noisy speech recognition. U.S. Patent 2003182114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitrios Galanis</author>
<author>George Karakatsiotis</author>
</authors>
<title>Gerasimos Lampouras and Ion Androutsopoulos.</title>
<date>2009</date>
<marker>Galanis, Karakatsiotis, 2009</marker>
<rawString>Dimitrios Galanis, George Karakatsiotis, Gerasimos Lampouras and Ion Androutsopoulos. 2009. An open-source natural language generator for OWL ontologies and its use in Prot´eg´e and Second Life. In this volume.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stasinos Konstantopoulos</author>
<author>Vangelis Karkaletsis</author>
<author>Colin Matheson</author>
</authors>
<title>Robot personality: Representation and externalization.</title>
<date>2008</date>
<booktitle>In Proc. Computational Aspects of Affective and Emotional Interaction (CAFFEi 08),</booktitle>
<location>Patras, Greece.</location>
<contexts>
<context position="5200" citStr="Konstantopoulos et al., 2008" startWordPosition="741" endWordPosition="745">e that favours using the architectural attributes to describe a building where another profile would choose to concentrate on historical facts regarding the same building. Stereotypes and profiles are combined into a single set of parameters by means of personality models. Personality models are many-valued Description Logic definitions of the overall preference, grounded in stereotype and profile data. These definitions model recognizable personality traits so that, for example, an open personality will attend more to the user’s requests than its own interests in deriving overall preference (Konstantopoulos et al., 2008). Furthermore, the system dynamically adapts overall preference according to both interaction history and the current dialogue state. So, for one, the initial (static model) interest factor of an ontology entity is reduced each time this entity is used in a description in order to avoid repetitions. On the other hand, preference will increase if, for example, in the current state the user has explicitly asked about an entity. 3.2 Dialogue and Action Management The DAM is built around the information-state update dialogue paradigm of the TRINDIKIT dialogue-engine toolkit (Cooper and Larsson, 19</context>
</contexts>
<marker>Konstantopoulos, Karkaletsis, Matheson, 2008</marker>
<rawString>Stasinos Konstantopoulos, Vangelis Karkaletsis, and Colin Matheson. 2008. Robot personality: Representation and externalization. In Proc. Computational Aspects of Affective and Emotional Interaction (CAFFEi 08), Patras, Greece.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>