<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006495">
<title confidence="0.635833">
Language Use: What can it Tell us?
</title>
<figure confidence="0.2459926">
[name] [name] [name]
[address1] [address1] [address1]
[address2] [address2] [address2]
[address3] [address3] [address3]
[email] [email] [email]
</figure>
<sectionHeader confidence="0.986923" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999364363636364">
For 20 years, information extraction has fo-
cused on facts expressed in text. In contrast,
this paper is a snapshot of research in progress
on inferring properties and relationships
among participants in dialogs, even though
these properties/relationships need not be ex-
pressed as facts. For instance, can a machine
detect that someone is attempting to persuade
another to action or to change beliefs or is as-
serting their credibility? We report results on
both English and Arabic discussion forums.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955684210526">
Extracting explicitly stated information has been
tested in MUC1 and ACE2 evaluations. For exam-
ple, for the text Mushaima&apos;a, head of the opposi-
tion Haq movement, an ACE system extracts the
relation LeaderOf(Mushaima&apos;a, HaqMovement). In
TREC QA3 systems answered questions, e.g.
‘When was Mozart born?’, for which the answer is
contained in one or a few extracted text phrases.
Sentiment analysis uses implicit meaning of
text, but has focused primarily on text known to be
rich in opinions (product reviews, editorials) and
delves into only one aspect of implicit meaning.
Our long-term goal is to predict social roles in
informal group discussion from language uses
(LU), even if those roles are not explicitly stated;
for example, using the communication during a
meeting, identify the leader of a group. This paper
provides a snapshot of preliminary, ongoing re-
search in predicting two classes of language use:
</bodyText>
<footnote confidence="0.997170333333333">
1 http://www-nlpir.nist.gov/related_projects/muc/
2 http://www.nist.gov/speech/tests/ace/
3 http://trec.nist.gov/data/qa.html
</footnote>
<bodyText confidence="0.991642352941176">
Establish-Credibility and Attempt-To-Persuade.
Technical challenges include dealing with the facts
that those LUs are rare and subjective and that hu-
man judgments have low agreement.
Our hybrid statistical &amp; rule-based approach
detects those two LUs in English and Arabic. Our
results are that (1) annotation at the message (turn)
level provides training data useful for predicting
rare phenomena at the discussion level while re-
ducing the requirement for turn-level predictions to
be accurate; (2)weighing subjective judgments
overcomes the need for high annotator consistency.
Because the phenomena are rare, always predicting
the absence of a LU is a very high baseline. For
English, the system beats those baselines. For Ara-
bic, more work is required, since only 10-20% of
the amount of training data exists so far.
</bodyText>
<sectionHeader confidence="0.943873" genericHeader="method">
2 Language Uses (LUs)
</sectionHeader>
<bodyText confidence="0.999981235294118">
A language use refers to an aspect of the social
intention of how a communicator uses language.
The information that supports a decision about an
implicit social action or role is likely to be distrib-
uted over more than one turn in a dialog; therefore,
a language use is defined, annotated, and predicted
across a thread in the dialog. Because our current
work uses discussion forums, threads provide a
natural, explicit unit of analysis. Our current work
studies two language uses.
An Attempt-to-Persuade occurs when a poster
tries to convince other participants to change their
beliefs or actions over the course of a thread. Typi-
cally, there is at least some resistance on the part of
the posters being persuaded. To distinguish be-
tween actual persuasion and discussions that in-
volve differing opinions, a poster needs to engage
</bodyText>
<page confidence="0.983705">
341
</page>
<note confidence="0.583281">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 341–345,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998338">
in multiple persuasion posts (turns) to be consid-
ered exhibiting the LU.
Establish-Credibility occurs when a poster at-
tempts to increase their standing within the group.
This can be evidenced with any of several moves,
e.g., explicit statements of authority, demonstration
expertise through knowledge, providing verifiable
information (e.g., from a trusted source or citing
confirmable facts), or providing a justified opinion
(e.g., a logical argument or personal experience).
</bodyText>
<sectionHeader confidence="0.997211" genericHeader="method">
3 Challenges
</sectionHeader>
<bodyText confidence="0.9999114">
There were two significant challenges: (a) sparsity
of the LUs, and (b) inter-annotator agreement. To
address the sparsity of data, we tried to automati-
cally select data that was likely to contain content
of interest. Data selection focused on the number
of messages and posters in a thread, as well as the
frequency of known indicators like quotations.
(withheld). Despite these efforts, the LUs of inter-
est were rare, especially in Arabic.
Annotation was developed using cycles of
guideline development, annotation, evaluation of
agreement, and revision of guidelines. Elsewhere,
similar, iterative annotation processes have yielded
significant improvements in agreement for word
sense and coreference (Hovy et al., 2006). While
LUs were annotated for a poster over the full
thread, annotators also marked specific messages
in the thread for presence of evidence of the lan-
guage use. Table 1 includes annotator consistency
at both the evidence (message) and LU level.
</bodyText>
<table confidence="0.9948044">
English Arabic
Msg LU Msg LU
Agr # Agr # Agr # Agr #
Per. 0.68 4722 0.75 2151 0.57 652 0.49 360
Cred. 0.66 3594 0.68 1609 0.35 652 0.45 360
</table>
<tableCaption confidence="0.9869565">
Table 1: Number of Annotated Data Units and Annota-
tor Agreement (measured as F)
</tableCaption>
<bodyText confidence="0.999984111111111">
The consistency numbers for this task were sig-
nificantly lower than we have seen in other lan-
guage processing tasks. Discussions suggested that
disagreement did not come from a misunderstand-
ing of the task but was the result of differing intui-
tions about difficult-to-define labels. In the
following two sections, we describe how the eval-
uation framework and system development pro-
ceeded despite low levels of consistency.
</bodyText>
<sectionHeader confidence="0.997021" genericHeader="method">
4 Evaluation Framework
</sectionHeader>
<bodyText confidence="0.999966810810811">
Task. The task is to predict for every participant in
a given thread, whether the participant exhibits
Attempt-to-Persuade and/or Establish-Credibility.
If there is insufficient evidence of an LU for a par-
ticipant, then the LU value for that poster is nega-
tive. The external evaluation measured LU
predictions. Internally we measured predictions of
message-level evidence as well.
Corpora. For English, 139 threads from
Google Groups and LiveJournal have been anno-
tated for Attempt-to-Persuade, and 103 threads for
Attempt-to-Establish-Credibility. For Arabic,
threads were collected from al-handasa.net.4 31
threads were annotated for both tasks. Counts of
annotated messages appear in Table 1.
Measures. Due to low annotator agreement, at-
tempting to resolve annotation disagreement by the
standard adjudication process was too time-
consuming. Instead, the evaluation scheme, similar
to the pyramid scheme used for summarization
evaluation, assigns scores to each example based
on its level of agreement among the annotators.
Specifically, each example is assigned positive and
negative scores, p = n+/N and n = n-/N, where n+ is
the number of annotators that annotate the example
as positive, and n- for the negative. N is the total
number of annotators. A system that outputs posi-
tive on the example results in p correct and n incor-
rect. The system gets p incorrect and n correct for
predicting negative. Partial accuracy and F-
measure can then be computed.
Formally, let X = {xi} be a set of examples.
Each example xi is associated with positive and
negative scores, pi and ni. Let ri = 1 if the system
outputs positive for example xi and 0 for negative.
The partial accuracy, recall, precision, and F-
measure can be computed by:
</bodyText>
<equation confidence="0.99989525">
pA = 100x∑i(ripi+(1-ri)ni) / ∑i(pi+ni)
pR = 100x∑iripi / ∑ipi
pP = 100x ∑iripi / ∑iri
pF = 2 pR pP/(pR+pP)
</equation>
<bodyText confidence="0.9997754">
The maximum pA and pF may be less than 100
when there is disagreement between annotators. To
achieve accuracy and F scores on a scale of 100,
pA and pF are normalized using the maximum
achievable scores with respect to the data.
</bodyText>
<equation confidence="0.9982165">
npA = 100xpA/max(pA)
npF = 100xpF/max(pF)
</equation>
<footnote confidence="0.613623">
4 URLs and judgments are available by email.
</footnote>
<page confidence="0.993274">
342
</page>
<sectionHeader confidence="0.986402" genericHeader="method">
5 System and Empirical Results
</sectionHeader>
<bodyText confidence="0.999158666666667">
Our architecture is shown in Figure 1. We process
a thread in three stages: (1) linguistic analysis of
each message (post) to yield features, (2) Predic-
tion of message-level properties using an SVM on
the extracted features, and (3) Simple rules that
predict language uses over the thread.
</bodyText>
<table confidence="0.999718833333333">
Persuade Establish Credibility
npA npF npA npF
En Ar En Ar En Ar En Ar
A 72.5 83.2 0.0 0.0 77.6 95.0 0.0 0.0
P 40.4 29.7 61.1 50.7 33.9 14.4 54.5 30.9
S 86.5 81.3 79.2 61.9 86.7 95.5 73.9 54.0
Persuade Establish Credibility
npA npF npA npF
En Ar En Ar En Ar En Ar
A 90.9 86.7 0.0 0.0 87.7 90.2 0.0 0.0
P 12.1 27.0 23.8 48.2 18.0 21.5 33.7 41.1
S 94.6 88.3 76.8 38.8 95.1 92.4 80.0 36.0
</table>
<tableCaption confidence="0.999033">
Table 54: Cross Validation Performance on Poster LUs
</tableCaption>
<bodyText confidence="0.9552206">
Table 6Table 5 shows LU prediction results
from an external evaluation on held out data. Un-
like our dataset, each example in the external eval-
uation dataset was annotated by 3 annotators. The
results are similar to our internal experiment.
</bodyText>
<table confidence="0.998854166666667">
Persuade Establish Credibility
npA npF npA npF
En Ar En Ar En Ar En Ar
A 96.2 98.4 0.0 0.0 93.6 94.0 93.6 0.0
P 13.1 4.2 27.6 11.7 11.1 10.1 11.1 22.2
S 96.5 94.6 75.1 59.1 97.7 92.5 97.7 24.7
</table>
<bodyText confidence="0.998966615384615">
Weibe, 2009). The MPQA corpus (Weibe, 2005)
annotates polarity for sentences in newswire, but
the focus of this corpus is at the sentence level.
Both the MPQA corpus and the various corpora of
editorials and reviews have tended towards more
formal, edited, non-conversational text. Our work
in contrast, specifically targets interactive discus-
sions in an informal setting. Work outside of com-
putational linguistics that has looked at persuasion
has tended to examine language in a persuasive
context (e.g. sales, advertising, or negotiations).
Like the current work, Strzalkowski, et al.
(2010) investigates language uses over informal
dialogue. Their work focuses on chat transcripts in
an experimental setting designed to be rich in the
phenomena of interest. Like our work, their predic-
tions operate over the conversation, and not a sin-
gle utterance. The specific language uses in their
work (topic/task control, involvement, and disa-
greement) are different than those discussed here.
Our work also differs in the data type of interest.
We work with threaded online discussions in
which the phenomena in question are rare. Our
annotators and system must distinguish between
the language use and text that is opinionated with-
out an intention to persuade or establish credibility.
</bodyText>
<tableCaption confidence="0.9996775">
Table 43: Performance on Message Level Evidence
Table 65: External, Held-Out Results on Poster LUs
</tableCaption>
<page confidence="0.524995">
7 Conclusions and Future Work
</page>
<sectionHeader confidence="0.998377" genericHeader="method">
6 Related Research
</sectionHeader>
<bodyText confidence="0.999973571428572">
Research in authorship profiling (Chung &amp; Penne-
baker, 2007; Argamon et al, in press; and Abbasi
and Chen, 2005) has identified traits, such as sta-
tus, sex, age, gender, and native language. Models
and predictions in this field have primarily used
simple word-based features, e.g. occurrence and
frequency of function words.
Social science researchers have studied how so-
cial roles develop in online communities (Fisher, et
al., 2006), and have attempted to categorize these
roles in multiple ways (Golder and Donath 2004;
Turner et al., 2005). Welser et al. (2007) have in-
vestigated the feasibility of detecting such roles
automatically using posting frequency (but not the
content of the messages).
Sentiment analysis requires understanding the
implicit nature of the text. Work on perspective
and sentiment analysis frequently uses a corpus
known to be rich in sentiment such as reviews or
editorials (e.g. (Hardisty, 2010), (Somasundaran&amp;
In this work in progress, we presented a hybrid
statistical &amp; rule-based approach to detecting prop-
erties not explicitly stated, but evident from lan-
guage use. Annotation at the message (turn) level
provided training data useful for predicting rare
phenomena at the discussion level while reducing
the need for turn-level predictions to be accurate.
Weighing subjective judgments overcame the need
for high annotator consistency. For English, the
system beats both baselines with respect to accura-
cy and F, despite the fact that because the phenom-
ena are rare, always predicting the absence of a
language use is a high baseline. For Arabic, more
work is required, particularly since only 10-20% of
the amount of training data exists so far.
This work has explored LUs, the implicit, social
purpose behind the words of a message. Future
work will explore incorporating LU predictions to
predict the social roles played by the participants in
a thread, for example using persuasion and credi-
bility to establish which participants in a discus-
sion are serving as informal leaders.
</bodyText>
<page confidence="0.99799">
344
</page>
<sectionHeader confidence="0.981923" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999365857142857">
This research was funded by the Office of the Director
of National Intelligence (ODNI), Intelligence Advanced
Research Projects Activity (IARPA), through the _____.
All statements of fact, opinion or conclusions contained
herein are those of the authors and should not be con-
strued as representing the official views or policies of
IARPA, the ODNI or the U.S. Government.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999608044776119">
Argamon, S., Koppel, M., Pennebaker, J.W., and Schler,
J. (2009). “Automatically profiling the author of
an anonymous text”. Communications of the Asso-
ciation for Computing Machinery (CACM). Vol-
ume 52 Issue 2.
Abbasi A., and Chen H. (2005). “Applying authorship
analysis to extremist-group web forum messages”.
In IEEE Intelligent Systems, 20(5), pp. 67–75.
Boyd, D, Golder, S, and Lotan, G. (2010). “Tweet,
Tweet, Retweet: Conversational Aspects of Re-
tweeting on Twitter.” HICSS-43. IEEE: Kauai, HI.
Chung, C.K., and Pennebaker, J.W. (2007). “The psy-
chological functions of function words”. In K.
Fiedler (Ed.), Social communication, pp. 343-359.
New York: Psychology Press.
Golder S., and Donath J. (2004) &amp;quot;Social Roles in Elec-
tronic Communities,&amp;quot; presented at the Association
of Internet Researchers (AoIR). Brighton, England
Hovy E., Marcus M., Palmer M., Ramshaw L., and
Weischedel R. (2006). “Ontonotes: The 90% solu-
tion”. In Proceedings of the Human Language
Technology Conference of the NAACL, Compan-
ion Volume: Short Papers, pp. 57–60. Association
for Computational Linguistics, New York City,
USA.
Joachims, T. (2005), “A Support Vector Method for
Multivariate Performance Measures”, Proceedings
of the International Conference on Machine
Learning (ICML).
Kelly, J., Fisher, D., Smith, D., (2006) “Friends, foes,
and fringe: norms and structure in political discus-
sion networks”, Proceedings of the 2006 interna-
tional conference on Digital government research.
NIST Speech Group. (2008). “The ACE 2008 evalua-
tion plan: Assessment of Detection and Recogni-
tion of Entities and Relations Within and Across
Documents”.
http://www.nist.gov/speech/tests/ace/2008/doc/ace
08 -evalplan.v1.2d.pdf
Ranganath, R., Jurafsky, D., and McFarland, D. (2009)
“It’s Not You, it’s Me: Detecting Flirting and its
Misperception in Speed-Dates” Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 334–342.
Somasundaran, S &amp; Wiebe, J (2009). Recognizing
Stances in Online Debates. ACL-IJCNLP 2009.
Strzalkowski, T, Broadwell, G, Stromer-Galley, J,
Shaikh, S, Taylor, S and Webb, N. (2010) “Model-
ing Socio-Cultural Phenomena in Discourse”.
Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pag-
es 1038–1046, Beijing, August 2010
Turner T. C., Smith M. A., Fisher D., and Welser H. T.
(2005) “Picturing Usenet: Mapping computer-
mediated collective action”. In Journal of Com-
puter-Mediated Communication, 10(4).
Voorhees, E. &amp; Tice, D. (2000).&amp;quot;Building a Question
Answering Test Collection&amp;quot;, Proceedings of
SIGIR, pp. 200-207.
Welser H. T., Gleave E., Fisher D., and Smith M.,
(2007). &amp;quot;Visualizing the signatures of social roles in
online discussion groups,&amp;quot; In The Journal of Social
Structure, vol. 8, no. 2.
Wiebe, J, Wilson, T and Cardie, C (2005). Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, volume 39, is-
sue 2-3, pp. 165-210.
</reference>
<page confidence="0.999136">
345
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.610223">
<title confidence="0.69558">Language Use: What can it Tell us?</title>
<abstract confidence="0.981956352941176">[name] [name] [name] [address1] [address1] [address1] [address2] [address2] [address2] [address3] [address3] [address3] [email] [email] [email] Abstract For 20 years, information extraction has focused on facts expressed in text. In contrast, this paper is a snapshot of research in progress on inferring properties and relationships among participants in dialogs, even though these properties/relationships need not be expressed as facts. For instance, can a machine detect that someone is attempting to persuade another to action or to change beliefs or is asserting their credibility? We report results on both English and Arabic discussion forums.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>M Koppel</author>
<author>J W Pennebaker</author>
<author>J Schler</author>
</authors>
<title>Automatically profiling the author of an anonymous text”.</title>
<date>2009</date>
<journal>Communications of the Association for Computing Machinery (CACM). Volume</journal>
<volume>52</volume>
<marker>Argamon, Koppel, Pennebaker, Schler, 2009</marker>
<rawString>Argamon, S., Koppel, M., Pennebaker, J.W., and Schler, J. (2009). “Automatically profiling the author of an anonymous text”. Communications of the Association for Computing Machinery (CACM). Volume 52 Issue 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Abbasi</author>
<author>H Chen</author>
</authors>
<title>Applying authorship analysis to extremist-group web forum messages”.</title>
<date>2005</date>
<journal>In IEEE Intelligent Systems,</journal>
<volume>20</volume>
<issue>5</issue>
<pages>67--75</pages>
<contexts>
<context position="10640" citStr="Abbasi and Chen, 2005" startWordPosition="1701" endWordPosition="1704">olvement, and disagreement) are different than those discussed here. Our work also differs in the data type of interest. We work with threaded online discussions in which the phenomena in question are rare. Our annotators and system must distinguish between the language use and text that is opinionated without an intention to persuade or establish credibility. Table 43: Performance on Message Level Evidence Table 65: External, Held-Out Results on Poster LUs 7 Conclusions and Future Work 6 Related Research Research in authorship profiling (Chung &amp; Pennebaker, 2007; Argamon et al, in press; and Abbasi and Chen, 2005) has identified traits, such as status, sex, age, gender, and native language. Models and predictions in this field have primarily used simple word-based features, e.g. occurrence and frequency of function words. Social science researchers have studied how social roles develop in online communities (Fisher, et al., 2006), and have attempted to categorize these roles in multiple ways (Golder and Donath 2004; Turner et al., 2005). Welser et al. (2007) have investigated the feasibility of detecting such roles automatically using posting frequency (but not the content of the messages). Sentiment a</context>
</contexts>
<marker>Abbasi, Chen, 2005</marker>
<rawString>Abbasi A., and Chen H. (2005). “Applying authorship analysis to extremist-group web forum messages”. In IEEE Intelligent Systems, 20(5), pp. 67–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Boyd</author>
<author>S Golder</author>
<author>G Lotan</author>
</authors>
<date>2010</date>
<booktitle>Conversational Aspects of Retweeting on Twitter.” HICSS-43.</booktitle>
<publisher>IEEE: Kauai, HI.</publisher>
<location>Tweet, Tweet, Retweet:</location>
<marker>Boyd, Golder, Lotan, 2010</marker>
<rawString>Boyd, D, Golder, S, and Lotan, G. (2010). “Tweet, Tweet, Retweet: Conversational Aspects of Retweeting on Twitter.” HICSS-43. IEEE: Kauai, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Chung</author>
<author>J W Pennebaker</author>
</authors>
<title>The psychological functions of function words”. In</title>
<date>2007</date>
<booktitle>Social communication,</booktitle>
<pages>343--359</pages>
<editor>K. Fiedler (Ed.),</editor>
<publisher>Psychology Press.</publisher>
<location>New York:</location>
<contexts>
<context position="10587" citStr="Chung &amp; Pennebaker, 2007" startWordPosition="1690" endWordPosition="1694">ic language uses in their work (topic/task control, involvement, and disagreement) are different than those discussed here. Our work also differs in the data type of interest. We work with threaded online discussions in which the phenomena in question are rare. Our annotators and system must distinguish between the language use and text that is opinionated without an intention to persuade or establish credibility. Table 43: Performance on Message Level Evidence Table 65: External, Held-Out Results on Poster LUs 7 Conclusions and Future Work 6 Related Research Research in authorship profiling (Chung &amp; Pennebaker, 2007; Argamon et al, in press; and Abbasi and Chen, 2005) has identified traits, such as status, sex, age, gender, and native language. Models and predictions in this field have primarily used simple word-based features, e.g. occurrence and frequency of function words. Social science researchers have studied how social roles develop in online communities (Fisher, et al., 2006), and have attempted to categorize these roles in multiple ways (Golder and Donath 2004; Turner et al., 2005). Welser et al. (2007) have investigated the feasibility of detecting such roles automatically using posting frequen</context>
</contexts>
<marker>Chung, Pennebaker, 2007</marker>
<rawString>Chung, C.K., and Pennebaker, J.W. (2007). “The psychological functions of function words”. In K. Fiedler (Ed.), Social communication, pp. 343-359. New York: Psychology Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Golder</author>
<author>J Donath</author>
</authors>
<title>Social Roles in Electronic Communities,&amp;quot; presented at the Association of Internet Researchers (AoIR).</title>
<date>2004</date>
<location>Brighton, England</location>
<contexts>
<context position="11049" citStr="Golder and Donath 2004" startWordPosition="1764" endWordPosition="1767">e Table 65: External, Held-Out Results on Poster LUs 7 Conclusions and Future Work 6 Related Research Research in authorship profiling (Chung &amp; Pennebaker, 2007; Argamon et al, in press; and Abbasi and Chen, 2005) has identified traits, such as status, sex, age, gender, and native language. Models and predictions in this field have primarily used simple word-based features, e.g. occurrence and frequency of function words. Social science researchers have studied how social roles develop in online communities (Fisher, et al., 2006), and have attempted to categorize these roles in multiple ways (Golder and Donath 2004; Turner et al., 2005). Welser et al. (2007) have investigated the feasibility of detecting such roles automatically using posting frequency (but not the content of the messages). Sentiment analysis requires understanding the implicit nature of the text. Work on perspective and sentiment analysis frequently uses a corpus known to be rich in sentiment such as reviews or editorials (e.g. (Hardisty, 2010), (Somasundaran&amp; In this work in progress, we presented a hybrid statistical &amp; rule-based approach to detecting properties not explicitly stated, but evident from language use. Annotation at the </context>
</contexts>
<marker>Golder, Donath, 2004</marker>
<rawString>Golder S., and Donath J. (2004) &amp;quot;Social Roles in Electronic Communities,&amp;quot; presented at the Association of Internet Researchers (AoIR). Brighton, England</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>Ontonotes: The 90% solution”.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>57--60</pages>
<location>New York City, USA.</location>
<contexts>
<context position="4835" citStr="Hovy et al., 2006" startWordPosition="722" endWordPosition="725">ress the sparsity of data, we tried to automatically select data that was likely to contain content of interest. Data selection focused on the number of messages and posters in a thread, as well as the frequency of known indicators like quotations. (withheld). Despite these efforts, the LUs of interest were rare, especially in Arabic. Annotation was developed using cycles of guideline development, annotation, evaluation of agreement, and revision of guidelines. Elsewhere, similar, iterative annotation processes have yielded significant improvements in agreement for word sense and coreference (Hovy et al., 2006). While LUs were annotated for a poster over the full thread, annotators also marked specific messages in the thread for presence of evidence of the language use. Table 1 includes annotator consistency at both the evidence (message) and LU level. English Arabic Msg LU Msg LU Agr # Agr # Agr # Agr # Per. 0.68 4722 0.75 2151 0.57 652 0.49 360 Cred. 0.66 3594 0.68 1609 0.35 652 0.45 360 Table 1: Number of Annotated Data Units and Annotator Agreement (measured as F) The consistency numbers for this task were significantly lower than we have seen in other language processing tasks. Discussions sugg</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Hovy E., Marcus M., Palmer M., Ramshaw L., and Weischedel R. (2006). “Ontonotes: The 90% solution”. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pp. 57–60. Association for Computational Linguistics, New York City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>A Support Vector Method for Multivariate Performance Measures”,</title>
<date>2005</date>
<booktitle>Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<marker>Joachims, 2005</marker>
<rawString>Joachims, T. (2005), “A Support Vector Method for Multivariate Performance Measures”, Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kelly</author>
<author>D Fisher</author>
<author>D Smith</author>
</authors>
<title>Friends, foes, and fringe: norms and structure in political discussion networks”,</title>
<date>2006</date>
<booktitle>Proceedings of the 2006 international conference on Digital government research.</booktitle>
<marker>Kelly, Fisher, Smith, 2006</marker>
<rawString>Kelly, J., Fisher, D., Smith, D., (2006) “Friends, foes, and fringe: norms and structure in political discussion networks”, Proceedings of the 2006 international conference on Digital government research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST Speech Group</author>
</authors>
<title>evaluation plan: Assessment of Detection and Recognition of Entities and Relations Within and Across Documents”.</title>
<date>2008</date>
<booktitle>The ACE</booktitle>
<marker>Group, 2008</marker>
<rawString>NIST Speech Group. (2008). “The ACE 2008 evaluation plan: Assessment of Detection and Recognition of Entities and Relations Within and Across Documents”.</rawString>
</citation>
<citation valid="false">
<note>http://www.nist.gov/speech/tests/ace/2008/doc/ace 08 -evalplan.v1.2d.pdf</note>
<marker></marker>
<rawString>http://www.nist.gov/speech/tests/ace/2008/doc/ace 08 -evalplan.v1.2d.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ranganath</author>
<author>D Jurafsky</author>
<author>D McFarland</author>
</authors>
<title>It’s Not You, it’s Me: Detecting Flirting and its Misperception</title>
<date>2009</date>
<booktitle>in Speed-Dates” Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>334--342</pages>
<marker>Ranganath, Jurafsky, McFarland, 2009</marker>
<rawString>Ranganath, R., Jurafsky, D., and McFarland, D. (2009) “It’s Not You, it’s Me: Detecting Flirting and its Misperception in Speed-Dates” Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 334–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Wiebe</author>
</authors>
<title>Recognizing Stances in Online Debates.</title>
<date>2009</date>
<publisher>ACL-IJCNLP</publisher>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Somasundaran, S &amp; Wiebe, J (2009). Recognizing Stances in Online Debates. ACL-IJCNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strzalkowski</author>
<author>G Broadwell</author>
<author>J Stromer-Galley</author>
<author>S Shaikh</author>
<author>S Taylor</author>
<author>N Webb</author>
</authors>
<title>Modeling Socio-Cultural Phenomena in Discourse”.</title>
<date>2010</date>
<booktitle>Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1038--1046</pages>
<location>Beijing,</location>
<contexts>
<context position="9693" citStr="Strzalkowski, et al. (2010)" startWordPosition="1551" endWordPosition="1554">.6 75.1 59.1 97.7 92.5 97.7 24.7 Weibe, 2009). The MPQA corpus (Weibe, 2005) annotates polarity for sentences in newswire, but the focus of this corpus is at the sentence level. Both the MPQA corpus and the various corpora of editorials and reviews have tended towards more formal, edited, non-conversational text. Our work in contrast, specifically targets interactive discussions in an informal setting. Work outside of computational linguistics that has looked at persuasion has tended to examine language in a persuasive context (e.g. sales, advertising, or negotiations). Like the current work, Strzalkowski, et al. (2010) investigates language uses over informal dialogue. Their work focuses on chat transcripts in an experimental setting designed to be rich in the phenomena of interest. Like our work, their predictions operate over the conversation, and not a single utterance. The specific language uses in their work (topic/task control, involvement, and disagreement) are different than those discussed here. Our work also differs in the data type of interest. We work with threaded online discussions in which the phenomena in question are rare. Our annotators and system must distinguish between the language use </context>
</contexts>
<marker>Strzalkowski, Broadwell, Stromer-Galley, Shaikh, Taylor, Webb, 2010</marker>
<rawString>Strzalkowski, T, Broadwell, G, Stromer-Galley, J, Shaikh, S, Taylor, S and Webb, N. (2010) “Modeling Socio-Cultural Phenomena in Discourse”. Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1038–1046, Beijing, August 2010</rawString>
</citation>
<citation valid="true">
<authors>
<author>T C Turner</author>
<author>M A Smith</author>
<author>D Fisher</author>
<author>H T Welser</author>
</authors>
<title>Picturing Usenet: Mapping computermediated collective action”.</title>
<date>2005</date>
<journal>In Journal of Computer-Mediated Communication,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="11071" citStr="Turner et al., 2005" startWordPosition="1768" endWordPosition="1771">ld-Out Results on Poster LUs 7 Conclusions and Future Work 6 Related Research Research in authorship profiling (Chung &amp; Pennebaker, 2007; Argamon et al, in press; and Abbasi and Chen, 2005) has identified traits, such as status, sex, age, gender, and native language. Models and predictions in this field have primarily used simple word-based features, e.g. occurrence and frequency of function words. Social science researchers have studied how social roles develop in online communities (Fisher, et al., 2006), and have attempted to categorize these roles in multiple ways (Golder and Donath 2004; Turner et al., 2005). Welser et al. (2007) have investigated the feasibility of detecting such roles automatically using posting frequency (but not the content of the messages). Sentiment analysis requires understanding the implicit nature of the text. Work on perspective and sentiment analysis frequently uses a corpus known to be rich in sentiment such as reviews or editorials (e.g. (Hardisty, 2010), (Somasundaran&amp; In this work in progress, we presented a hybrid statistical &amp; rule-based approach to detecting properties not explicitly stated, but evident from language use. Annotation at the message (turn) level p</context>
</contexts>
<marker>Turner, Smith, Fisher, Welser, 2005</marker>
<rawString>Turner T. C., Smith M. A., Fisher D., and Welser H. T. (2005) “Picturing Usenet: Mapping computermediated collective action”. In Journal of Computer-Mediated Communication, 10(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
<author>D Tice</author>
</authors>
<title>a Question Answering Test Collection&amp;quot;,</title>
<date>2000</date>
<booktitle>Proceedings of SIGIR,</booktitle>
<pages>200--207</pages>
<marker>Voorhees, Tice, 2000</marker>
<rawString>Voorhees, E. &amp; Tice, D. (2000).&amp;quot;Building a Question Answering Test Collection&amp;quot;, Proceedings of SIGIR, pp. 200-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Welser</author>
<author>E Gleave</author>
<author>D Fisher</author>
<author>M Smith</author>
</authors>
<title>Visualizing the signatures of social roles in online discussion groups,&amp;quot;</title>
<date>2007</date>
<journal>In The Journal of Social Structure,</journal>
<volume>8</volume>
<contexts>
<context position="11093" citStr="Welser et al. (2007)" startWordPosition="1772" endWordPosition="1775">er LUs 7 Conclusions and Future Work 6 Related Research Research in authorship profiling (Chung &amp; Pennebaker, 2007; Argamon et al, in press; and Abbasi and Chen, 2005) has identified traits, such as status, sex, age, gender, and native language. Models and predictions in this field have primarily used simple word-based features, e.g. occurrence and frequency of function words. Social science researchers have studied how social roles develop in online communities (Fisher, et al., 2006), and have attempted to categorize these roles in multiple ways (Golder and Donath 2004; Turner et al., 2005). Welser et al. (2007) have investigated the feasibility of detecting such roles automatically using posting frequency (but not the content of the messages). Sentiment analysis requires understanding the implicit nature of the text. Work on perspective and sentiment analysis frequently uses a corpus known to be rich in sentiment such as reviews or editorials (e.g. (Hardisty, 2010), (Somasundaran&amp; In this work in progress, we presented a hybrid statistical &amp; rule-based approach to detecting properties not explicitly stated, but evident from language use. Annotation at the message (turn) level provided training data </context>
</contexts>
<marker>Welser, Gleave, Fisher, Smith, 2007</marker>
<rawString>Welser H. T., Gleave E., Fisher D., and Smith M., (2007). &amp;quot;Visualizing the signatures of social roles in online discussion groups,&amp;quot; In The Journal of Social Structure, vol. 8, no. 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<pages>2--3</pages>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Wiebe, J, Wilson, T and Cardie, C (2005). Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, volume 39, issue 2-3, pp. 165-210.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>