<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000185">
<title confidence="0.997216">
A Bootstrapping Approach to Named Entity Classification Using
Successive Learners
</title>
<author confidence="0.772119">
Cheng Niu, Wei Li, Jihong Ding, Rohini K. Srihari
</author>
<affiliation confidence="0.480145">
Cymfony Inc.
</affiliation>
<address confidence="0.74241">
600 Essjay Road, Williamsville, NY 14221. USA.
</address>
<email confidence="0.97121">
{cniu, wei, jding, rohini}@cymfony.com
</email>
<sectionHeader confidence="0.996603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999813727272727">
This paper presents a new bootstrapping
approach to named entity (NE)
classification. This approach only requires
a few common noun/pronoun seeds that
correspond to the concept for the target
NE type, e.g. he/she/man/woman for
PERSON NE. The entire bootstrapping
procedure is implemented as training two
successive learners: (i) a decision list is
used to learn the parsing-based high
precision NE rules; (ii) a Hidden Markov
Model is then trained to learn string
sequence-based NE patterns. The second
learner uses the training corpus
automatically tagged by the first learner.
The resulting NE system approaches
supervised NE performance for some NE
types. The system also demonstrates
intuitive support for tagging user-defined
NE types. The differences of this
approach from the co-training-based NE
bootstrapping are also discussed.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999853">
Named Entity (NE) tagging is a fundamental task
for natural language processing and information
extraction. An NE tagger recognizes and classifies
text chunks that represent various proper names,
time, or numerical expressions. Seven types of
named entities are defined in the Message
Understanding Conference (MUC) standards,
namely, PERSON (PER), ORGANIZATION
(ORG), LOCATION (LOC), TIME, DATE,
MONEY, and PERCENT1 (MUC-7 1998).
</bodyText>
<footnote confidence="0.5461885">
1 This paper only focuses on classifying proper names. Time and
numerical NEs are not yet explored using this method.
</footnote>
<bodyText confidence="0.999957011627907">
There is considerable research on NE tagging
using different techniques. These include systems
based on handcrafted rules (Krupka 1998), as well
as systems using supervised machine learning,
such as the Hidden Markov Model (HMM) (Bikel
1997) and the Maximum Entropy Model
(Borthwick 1998).
The state-of-the-art rule-based systems and
supervised learning systems can reach near-human
performance for NE tagging in a targeted domain.
However, both approaches face a serious
knowledge bottleneck, making rapid domain
porting difficult. Such systems cannot effectively
support user-defined named entities. That is the
motivation for using unsupervised or weakly-
supervised machine learning that only requires a
raw corpus from a given domain for this NE
research.
(Cucchiarelli &amp; Velardi 2001) discussed
boosting the performance of an existing NE tagger
by unsupervised learning based on parsing
structures. (Cucerzan &amp; Yarowsky 1999), (Collins
&amp; Singer 1999) and (Kim 2002) presented various
techniques using co-training schemes for NE
extraction seeded by a small list of proper names
or handcrafted NE rules. NE tagging has two tasks:
(i) NE chunking; (ii) NE classification. Parsing-
supported NE bootstrapping systems including
ours only focus on NE classification, assuming NE
chunks have been constructed by the parser.
The key idea of co-training is the separation of
features into several orthogonal views. In case of
NE classification, usually one view uses the
context evidence and the other relies on the lexicon
evidence. Learners corresponding to different
views learn from each other iteratively.
One issue of co-training is the error propagation
problem in the process of the iterative learning.
The rule precision drops iteration-by-iteration. In
the early stages, only few instances are available
for learning. This makes some powerful statistical
models such as HMM difficult to use due to the
extremely sparse data.
This paper presents a new bootstrapping
approach using successive learning and concept-
based seeds. The successive learning is as follows.
First, some parsing-based NE rules are learned
with high precision but limited recall. Then, these
rules are applied to a large raw corpus to
automatically generate a tagged corpus. Finally, an
HMM-based NE tagger is trained using this
corpus. There is no iterative learning between the
two learners, hence the process is free of the error
propagation problem. The resulting NE system
approaches supervised NE performance for some
NE types.
To derive the parsing-based learner, instead of
seeding the bootstrapping process with NE
instances from a proper name list or handcrafted
NE rules as (Cucerzan &amp; Yarowsky 1999),
(Collins &amp; Singer 1999) and (Kim 2002), the
system only requires a few common noun or
pronoun seeds that correspond to the concept for
the targeted NE, e.g. he/she/man/woman for
PERSON NE. Such concept-based seeds share
grammatical structures with the corresponding
NEs, hence a parser is utilized to support
bootstrapping. Since pronouns and common nouns
occur more often than NE instances, richer
contextual evidence is available for effective
learning. Using concept-based seeds, the parsing-
based NE rules can be learned in one iteration so
that the error propagation problem in the iterative
learning can be avoided.
This method is also shown to be effective for
supporting NE domain porting and is intuitive for
configuring an NE system to tag user-defined NE
types.
The remaining part of the paper is organized as
follows. The overall system design is presented in
Section 2. Section 3 describes the parsing-based
NE learning. Section 4 presents the automatic
construction of annotated NE corpus by parsing-
based NE classification. Section 5 presents the
string level HMM NE learning. Benchmarks are
shown in Section 6. Section 7 is the Conclusion.
</bodyText>
<sectionHeader confidence="0.97902" genericHeader="introduction">
2 System Design
</sectionHeader>
<bodyText confidence="0.99927735">
Figure 1 shows the overall system architecture.
Before the bootstrapping is started, a large raw
training corpus is parsed by the English parser
from our InfoXtract system (Srihari et al. 2003).
The bootstrapping experiment reported in this
paper is based on a corpus containing ~100,000
news articles and a total of ~88,000,000 words.
The parsed corpus is saved into a repository, which
supports fast retrieval by a keyword-based
indexing scheme.
Although the parsing-based NE learner is found
to suffer from the recall problem, we can apply the
learned rules to a huge parsed corpus. In other
words, the availability of an almost unlimited raw
corpus compensates for the modest recall. As a
result, large quantities of NE instances are
automatically acquired. An automatically
annotated NE corpus can then be constructed by
extracting the tagged instances plus their
neighboring words from the repository.
</bodyText>
<figureCaption confidence="0.996565">
Figure 1. Bootstrapping System Architecture
</figureCaption>
<bodyText confidence="0.978242">
The bootstrapping is performed as follows:
</bodyText>
<listItem confidence="0.9982525">
1. Concept-based seeds are provided by the
user.
2. Parsing structures involving concept-based
seeds are retrieved from the repository to
train a decision list for NE classification.
3. The learned rules are applied to the NE
candidates stored in the repository.
4. The proper names tagged in Step 3 and
their neighboring words are put together as
an NE annotated corpus.
5. An HMM is trained based on the annotated
corpus.
</listItem>
<figure confidence="0.9971615">
Concept-based Seeds
NE
Tagger
HMM
NE Learning
Repository
(parsed corpus)
NE tagging using parsing-based rules
parsing-based NE rules
training corpus
based on tagged NEs
Decision List
NE Learning
Bootstrapping Procedure
</figure>
<sectionHeader confidence="0.960955" genericHeader="method">
3 Parsing-based NE Rule Learning
</sectionHeader>
<bodyText confidence="0.999059357142857">
The training of the first NE learner has three major
properties: (i) the use of concept-based seeds, (ii)
support from the parser, and (iii) representation as
a decision list.
This new bootstrapping approach is based on
the observation that there is an underlying concept
for any proper name type and this concept can be
easily expressed by a set of common nouns or
pronouns, similar to how concepts are defined by
synsets in WordNet (Beckwith 1991).
Concept-based seeds are conceptually
equivalent to the proper name types that they
represent. These seeds can be provided by a user
intuitively. For example, a user can use pill, drug,
medicine, etc. as concept-based seeds to guide the
system in learning rules to tag MEDICINE names.
This process is fairly intuitive, creating a favorable
environment for configuring the NE system to the
types of names sought by the user.
An important characteristic of concept-based
seeds is that they occur much more often than
proper name seeds, hence they are effective in
guiding the non-iterative NE bootstrapping.
A parser is necessary for concept-based NE
bootstrapping. This is due to the fact that concept-
based seeds only share pattern similarity with the
corresponding NEs at structural level, not at string
sequence level. For example, at string sequence
level, PERSON names are often preceded by a set
of prefixing title words Mr./Mrs./Miss/Dr. etc., but
the corresponding common noun seeds
man/woman etc. cannot appear in such patterns.
However, at structural level, the concept-based
seeds share the same or similar linguistic patterns
(e.g. Subject-Verb-Object patterns) with the
corresponding types of proper names.
The rationale behind using concept-based seeds
in NE bootstrapping is similar to that for parsing-
based word clustering (Lin 1998): conceptually
similar words occur in structurally similar context.
In fact, the anaphoric function of pronouns and
common nouns to represent antecedent NEs
indicates the substitutability of proper names by
the corresponding common nouns or pronouns. For
example, this man can be substituted for the proper
name John Smith in almost all structural patterns.
Following the same rationale, a bootstrapping
approach is applied to the semantic lexicon
acquisition task [Thelen &amp; Riloff. 2002].
The InfoXtract parser supports dependency
parsing based on the linguistic units constructed by
our shallow parser (Srihari et al. 2003). Five types
of the decoded dependency relationships are used
for parsing-based NE rule learning. These are all
directional, binary dependency links between
linguistic units:
</bodyText>
<listItem confidence="0.935909423076923">
(1) Has_Predicate: from logical subject to verb
e.g. He said she would want him to join. 4
he: Has_Predicate(say)
she: Has_Predicate(want)
him: Has_Predicate(join)
(2) Object_Of : from logical object to verb
e.g. This company was founded to provide
new telecommunication services. 4
company: Object_Of(found)
service: Object_Of(provide)
(3) Has_Amod: from noun to its adjective modifier
e.g. He is a smart, handsome young man. 4
man: Has_AMod(smart)
man: Has_AMod(handsome)
man: Has_AMod(young)
(4) Possess: from the possessive noun-modifier to
head noun
e.g. His son was elected as mayor of the city. 4
his: Possess(son)
city: Possess(mayor)
(5) IsA: equivalence relation from one NP to
another NP
e.g. Microsoft spokesman John Smith is a
popular man. 4
spokesman: IsA(John Smith)
John Smith: IsA(man)
</listItem>
<bodyText confidence="0.983856">
The concept-based seeds used in the
experiments are:
</bodyText>
<listItem confidence="0.992644125">
1. PER: he, she, his, her, him, man, woman
2. LOC: city, province, town, village
3. ORG: company, firm, organization, bank,
airline, army, committee, government,
school, university
4. PRO: car, truck, vehicle, product, plane,
aircraft, computer, software, operating
system, data-base, book, platform, network
</listItem>
<bodyText confidence="0.999926692307692">
Note that the last target tag PRO (PRODUCT)
is beyond the MUC NE standards: we added this
NE type for the purpose of testing the system’s
capability in supporting user-defined NE types.
From the parsed corpus in the repository, all
instances of the concept-based seeds associated
with one or more of the five dependency relations
are retrieved: 821,267 instances in total in our
experiment. Each seed instance was assigned a
concept tag corresponding to NE. For example,
each instance of he is marked as PER. The marked
instances plus their associated parsing relationships
form an annotated NE corpus, as shown below:
</bodyText>
<equation confidence="0.821942833333333">
he/PER: Has_Predicate(say)
she/PER: Has_Predicate(get)
company/ORG: Object_Of(compel)
city/LOC: Possess(mayor)
car/PRO: Object_Of(manufacture)
HasAmod(high-quality)
</equation>
<bodyText confidence="0.99854125">
This training corpus supports the Decision List
Learning which learns homogeneous rules (Segal
&amp; Etzioni 1994). The accuracy of each rule was
evaluated using Laplace smoothing:
</bodyText>
<equation confidence="0.90680925">
positive 1
+
positive negative NE category No.
+ +
</equation>
<bodyText confidence="0.978518">
It is noteworthy that the PER tag dominates the
corpus due to the fact that the pronouns he and she
occur much more often than the seeded common
nouns. So the proportion of NE types in the
instances of concept-based seeds is not the same as
the proportion of NE types in the proper name
instances. For example, considering a running text
containing one instance of John Smith and one
instance of a city name Rochester, it is more likely
that John Smith will be referred to by he/him than
Rochester by (the) city. Learning based on such a
corpus is biased towards PER as the answer.
To correct this bias, we employ the following
modification scheme for instance count. Suppose
there are a total of NPER PER instances, NLOC
LOC instances, NORG ORG instances, NPRO PRO
instances, then in the process of rule accuracy
evaluation, the involved instance count for any NE
type will be adjusted by the coefficient
</bodyText>
<equation confidence="0.515367333333333">
min (N , N , N
PER LOC ORG
NNE
</equation>
<bodyText confidence="0.996545222222222">
the number of the training instances of PER is ten
times that of PRO, then when evaluating a rule
accuracy, any positive/negative count associated
with PER will be discounted by 0.1 to correct the
bias.
A total of 1,290 parsing-based NE rules are
learned, with accuracy higher than 0.9. The
following are sample rules of the learned decision
list:
</bodyText>
<equation confidence="0.999704863636364">
Possess(wife)4 PER
Possess(husband) 4 PER
Possess(daughter) 4 PER
Possess(bravery) 4 PER
Possess(father) 4 PER
Has_Predicate(divorce) 4 PER
Has_Predicate(remarry) 4 PER
Possess(brother) 4 PER
Possess(son) 4 PER
Possess(mother) 4 PER
Object_Of(deport) 4 PER
Possess(sister) 4 PER
Possess(colleague) 4 PER
Possess(career) 4 PER
Possess(forehead) 4 PER
Has_Predicate(smile) 4 PER
Possess(respiratory system) 4 PER
{Has_Predicate(threaten),
Has_Predicate(kill)} 4PER
Possess(concert hall) 4 LOC
Has_AMod(coastal) 4 LOC
Has_AMod(northern) 4 LOC
Has_AMod(eastern) 4 LOC
Has_AMod(northeastern) 4 LOC
Possess(undersecretary) 4 LOC
Possess(mayor) 4 LOC
Has_AMod(southern) 4 LOC
Has_AMod(northwestern) 4 LOC
Has_AMod(populous) 4 LOC
Has_AMod(rogue) 4 LOC
Has_AMod(southwestern) 4 LOC
Possess(medical examiner) 4 LOC
Has_AMod(edgy) 4 LOC
Has_AMod(broad-base) 4 ORG
Has_AMod(advisory) 4 ORG
Has_AMod(non-profit) 4 ORG
Possess(ceo) 4 ORG
Possess(operate loss) 4 ORG
Has_AMod(multinational) 4 ORG
Has_AMod(non-governmental) 4 ORG
Possess(filings) 4 ORG
accuracy ❑
)
,NPRO
. For example, if
Has_AMod(interim) 4 ORG
Has_AMod(for-profit) 4 ORG
Has_AMod(not-for-profit) 4 ORG
Has_AMod(nongovernmental) 4 ORG
Object_Of(undervalue) 4 ORG
Has_AMod(handheld) 4 PRO
Has_AMod(unman) 4 PRO
Has_AMod(well-sell) 4 PRO
Has_AMod(value-add) 4 PRO
Object_Of(refuel) 4 PRO
Has_AMod(fuel-efficient) 4 PRO
Object_Of(vend) 4 PRO
Has_Predicate(accelerate) 4 PRO
Has_Predicate(collide) 4 PRO
Object_Of(crash) 4 PRO
Has_AMod(scalable) 4 PRO
Possess(patch) 4 PRO
Object_Of(commercialize)4PRO
Has_AMod(custom-design) 4 PRO
Possess(rollout) 4 PRO
Object_Of(redesign) 4 PRO
</equation>
<bodyText confidence="0.9998822">
Due to the unique equivalence nature of the IsA
relation, the above bootstrapping procedure can
hardly learn IsA-based rules. Therefore, we add the
following IsA-based rules to the top of the decision
list: IsA(seed)4 tag of the seed, for example:
</bodyText>
<equation confidence="0.997466">
IsA(man) 4 PER
IsA(city) 4 LOC
IsA(company) 4 ORG
IsA(software) 4 PRO
</equation>
<sectionHeader confidence="0.986963" genericHeader="method">
4 Automatic Construction of Annotated
NE Corpus
</sectionHeader>
<bodyText confidence="0.999902148148148">
In this step, we use the parsing-based first learner
to tag a raw corpus in order to train the second NE
learner.
One issue with the parsing-based NE rules is
modest recall. For incoming documents,
approximately 35%-40% of the proper names are
associated with at least one of the five parsing
relations. Among these proper names associated
with parsing relations, only ~5% are recognized by
the parsing-based NE rules.
So we adopted the strategy of applying the
parsing-based rules to a large corpus (88 million
words), and let the quantity compensate for the
sparseness of tagged instances. A repository level
consolidation scheme is also used to improve the
recall.
The NE classification procedure is as follows.
From the repository, all the named entity
candidates associated with at least one of the five
parsing relationships are retrieved. An NE
candidate is defined as any chunk in the parsed
corpus that is marked with a proper name Part-Of-
Speech (POS) tag (i.e. NNP or NNPS). A total of
1,607,709 NE candidates were retrieved in our
experiment. A small sample of the retrieved NE
candidates with the associated parsing
relationships are shown below:
</bodyText>
<equation confidence="0.976305">
Deep South : Possess(project)
Ramada : Possess(president)
Argentina : Possess(first lady)
</equation>
<bodyText confidence="0.999935396226415">
After applying the decision list to the above the
NE candidates, 33,104 PER names, 16,426 LOC
names, 11,908 ORG names and 6,280 PRO names
were extracted.
It is a common practice in the bootstrapping
research to make use of heuristics that suggest
conditions under which instances should share the
same answer. For example, the one sense per
discourse principle is often used for word sense
disambiguation (Gale et al. 1992). In this research,
we used the heuristic one tag per domain for multi-
word NE in addition to the one sense per discourse
principle. These heuristics were found to be very
helpful in improving the performance of the
bootstrapping algorithm for the purpose of both
increasing positive instances (i.e. tag propagation)
and decreasing the spurious instances (i.e. tag
elimination). The following are two examples to
show how the tag propagation and elimination
scheme works.
Tyco Toys occurs 67 times in the corpus, and 11
instances are recognized as ORG, only one
instance is recognized as PER. Based on the
heuristic one tag per domain for multi-word NE,
the minority tag of PER is removed, and all the 67
instances of Tyco Toys are tagged as ORG.
Three instances of Postal Service are
recognized as ORG, and two instances are
recognized as PER. These tags are regarded as
noise, hence are removed by the tag elimination
scheme.
The tag propagation/elimination scheme is
adopted from (Yarowsky 1995). After this step, a
total of 386,614 proper names were recognized,
including 134,722 PER names, 186,488 LOC
names, 46,231 ORG names and 19,173 PRO
names. The overall precision was ~90%. The
benchmark details will be shown in Section 6.
The extracted proper name instances then led to
the construction of a fairly large training corpus
sufficient for training the second NE learner.
Unlike manually annotated running text corpus,
this corpus consists of only sample string
sequences containing the automatically tagged NE
instances and their left and right neighboring
words within the same sentence. The two
neighboring words are always regarded as common
words while constructing the corpus. This is based
on the observation that the proper names usually
do not occur continuously without any punctuation
in between.
A small sample of the automatically
constructed corpus is shown below:
</bodyText>
<listItem confidence="0.929753166666667">
in &lt;LOC&gt; Argentina &lt;/LOC&gt; .
&lt;LOC&gt; Argentina &lt;/LOC&gt; &apos;s
and &lt;PER&gt; Troy Glaus &lt;/PER&gt; walk
call &lt;ORG&gt; Prudential Associates &lt;/ORG&gt; .
, &lt;PRO&gt; Photoshop &lt;/PRO&gt; has
not &lt;PER&gt; David Bonderman &lt;/PER&gt; ,
</listItem>
<bodyText confidence="0.998899666666667">
This corpus is used for training the second NE
learner based on evidence from string sequences,
to be described in Section 5 below.
</bodyText>
<sectionHeader confidence="0.927003" genericHeader="method">
5 String Sequence-based NE Learning
</sectionHeader>
<bodyText confidence="0.996646380952381">
String sequence-based HMM learning is set as our
final goal for NE bootstrapping because of the
demonstrated high performance of this type of NE
taggers.
In this research, a bi-gram HMM is trained
based on the sample strings in the annotated corpus
constructed in section 4. During the training, each
sample string sequence is regarded as an
independent sentence. The training process is
similar to (Bikel 1997).
The HMM is defined as follows: Given a word
sequence W sequence = w 0 f0 � w n fn (where
fj denotes a single token feature which will be
defined below), the goal for the NE tagging task is
to find the optimal NE tag sequence
T sequence = t0t1t2 � tn , which maximizes the
conditional probability Pr(T sequence  |W sequence)
(Bikel 1997). By Bayesian equality, this is
equivalent to maximizing the joint probability
Pr(W sequence,T sequence) . This joint probability
can be computed by bi-gram HMM as follows:
</bodyText>
<equation confidence="0.99578875">
Pr(W sequence, T sequence)
= ∏ Pr( w , f , t  |w , f , t
i i i i-1 i-1 i − 1
i
</equation>
<bodyText confidence="0.93507125">
The back-off model is as follows,
where V denotes the size of the vocabulary, the
back-off coefficients λ’s are determined using the
Witten-Bell smoothing algorithm. The quantities
</bodyText>
<equation confidence="0.91947825">
P0( wi, fi , ti  |wim,Jim ,tin),P0( wi,fi  |ti, ti−1), P0(ti  |wi-1, ti−1),
P0( wi,fi  |ti), P0(fi  |ti), P0(ti  |wi-1), P0(ti), and
P0 (wi  |ti) are computed by the maximum
likelihood estimation.
</equation>
<bodyText confidence="0.999979666666667">
We use the following single token feature set
for HMM training. The definitions of these
features are the same as in (Bikel 1997).
</bodyText>
<equation confidence="0.988859169230769">
)
-1 i 1
,t −
Pr(t  |w
i i
) Pr(t
λ 3
-
i i - 1
 |w )
= λ3 0 i
P (t  |w ,t )( 1
− +
i-1 i 1
,
Pr(
, t )
i− 1
-1 i-1
, f
fi
wi
)
− −
1 i 1
, t
 |w
i i
P ( w
0 i
, t  |w
i i
,
fi
,
t i
 |w i
-1 i-1
, f
=λ1
- )Pr( w , f  |t , t )Pr(t
λ 1 i i i i− 1
,
)
−1
t i
+
( 1
Pr(
,
w i
fi
 |t i
Pr(t  |w ) P (t  |w ) (1 - ) P ( t
= λ + λ
i i-1 5 0 i i- 1 5 0 i
Pr(w  |t ) P (w |t )(1 -
i i = λ 6 0 i i + λ 6
)
,
)
−1
t i
= λ P (w , f |t , t )
− +
</equation>
<bodyText confidence="0.353249">
2 0 i i i i 1
</bodyText>
<figure confidence="0.914440066666667">
-
,
)
) Pr(
(1
fi
w i
 |t i
λ 2
(w  |t )P (f
i i 0 i
Pr(
,
w i
fi
)
 |t i
= λ P (w , f |t )( 1
+
4 0 i i i
)
-
λ 4
Pr
)
 |t i
1
)
V
)
</figure>
<construct confidence="0.995130714285714">
twoDigitNum, fourDigitNum,
containsDigitAndAlpha,
containsDigitAndDash,
containsDigitAndSlash,
containsDigitAndComma,
containsDigitAndPeriod, otherNum, allCaps,
capPeriod, initCap, lowerCase, other.
</construct>
<sectionHeader confidence="0.969346" genericHeader="method">
6 Benchmarking and Discussion
</sectionHeader>
<bodyText confidence="0.999508578947368">
Two types of benchmarks were measured: (i) the
quality of the automatically constructed NE
corpus, and (ii) the performance of the HMM NE
tagger. The HMM NE tagger is considered to be
the resulting system for application. The
benchmarking shows that this system approaches
the performance of supervised NE tagger for two
of the three proper name NE types in MUC,
namely, PER NE and LOC NE.
We used the same blind testing corpus of
300,000 words containing 20,000 PER, LOC and
ORG instances that were truthed in-house
originally for benchmarking the existing
supervised NE tagger (Srihari, Niu &amp; Li 2000).
This has the benefit of precisely measuring
performance degradation from the supervised
learning to unsupervised learning. The
performance of our supervised NE tagger using the
MUC scorer is shown in Table 1.
</bodyText>
<tableCaption confidence="0.997019">
Table 1. Performance of Supervised NE Tagger
</tableCaption>
<table confidence="0.9993445">
Type Precision Recall F-Measure
PERSON 92.3% 93.1% 92.7%
LOCATION 89.0% 87.7% 88.3%
ORGANIZATION 85.7% 87.8% 86.7%
</table>
<bodyText confidence="0.99985975">
To benchmark the quality of the automatically
constructed corpus (Table 2), the testing corpus is
first processed by our parser and then saved into
the repository. The repository level NE
classification scheme, as discussed in section 4, is
applied. From the recognized NE instances, the
instances occurring in the testing corpus are
compared with the answer key.
</bodyText>
<tableCaption confidence="0.988226">
Table 2. Quality of the Constructed Corpus
</tableCaption>
<sectionHeader confidence="0.77911575" genericHeader="method">
Type Precision
PERSON 94.3%
LOCATION 91.7%
ORGANIZATION 88.5%
</sectionHeader>
<bodyText confidence="0.9570145">
To benchmark the performance of the HMM
tagger, the testing corpus is parsed. The noun
chunks with proper name POS tags (NNP and
NNPS) are extracted as NE candidates. The
preceding word and the succeeding word of the NE
candidates are also extracted. Then we apply the
HMM to the NE candidates with their neighboring
context. The NE classification results are shown in
</bodyText>
<tableCaption confidence="0.993913">
Table 3.
Table 3. Performance of the second HMM NE
</tableCaption>
<table confidence="0.99931225">
Type Precision Recall F-Measure
PERSON 86.6% 88.9% 87.7%
LOCATION 82.9% 81.7% 82.3%
ORGANIZATION 57.1% 48.9% 52.7%
</table>
<bodyText confidence="0.999316606060606">
Compared with our existing supervised NE
tagger, the degradation using the presented
bootstrapping method for PER NE, LOC NE, and
ORG NE are 5%, 6%, and 34% respectively.
The performance for PER and LOC are above
80%, approaching the performance of supervised
learning. The reason for the low recall of ORG
(~50%) is not difficult to understand. For PERSON
and LOCATION, a few concept-based seeds seem
to be sufficient in covering their sub-types (e.g. the
sub-types COUNTRY, CITY, etc for
LOCATION). But there are hundreds of sub-types
of ORG that cannot be covered by less than a
dozen concept-based seeds, which we used. As a
result, the recall of ORG is significantly affected.
Due to the same fact that ORG contains many
more sub-types, the results are also noisier, leading
to lower precision than that of the other two NE
types. Some threshold can be introduced, e.g.
perplexity per word, to remove spurious ORG tags
in improving the precision. As for the recall issue,
fortunately, in a real-life application, the
organization type that a user is interested in usually
is in a fairly narrow spectrum. We believe that the
performance will be better if only company names
or military organization names are targeted.
In addition to the key NE types in MUC, our
system is able to recognize another NE type,
namely, PRODUCT (PRO) NE. We instructed our
truthing team to add this NE type into the testing
corpus which contains ~2,000 PRO instances.
Table 4 shows the performance of the HMM on the
PRO tag.
</bodyText>
<tableCaption confidence="0.986247">
Table 4. Performance of PRODUCT NE
</tableCaption>
<table confidence="0.9397555">
TYPE PRECISION RECALL F-MEASURE
PRODUCT 67.3% 72.5% 69.8%
</table>
<bodyText confidence="0.999938941176471">
Similar to the case of ORG NEs, the number of
concept-based seeds is found to be insufficient to
cover the variations of PRO subtypes. So the
performance is not as good as PER and LOC NEs.
Nevertheless, the benchmark shows the system
works fairly effectively in extracting the user-
specified NEs. It is noteworthy that domain
knowledge such as knowing the major sub-types of
the user-specified NE type is valuable in assisting
the selection of appropriate concept-based seeds
for performance enhancement.
The performance of our HMM tagger is
comparable with the reported performance in
(Collins &amp; Singer 1999). But our benchmarking is
more extensive as we used a much larger data set
(20,000 NE instances in the testing corpus) than
theirs (1,000 NE instances).
</bodyText>
<sectionHeader confidence="0.999412" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999489">
A novel bootstrapping approach to NE
classification is presented. This approach does not
require iterative learning which may suffer from
error propagation. With minimal human
supervision in providing a handful of concept-
based seeds, the resulting NE tagger approaches
supervised NE performance in NE types for
PERSON and LOCATION. The system also
demonstrates effective support for user-defined NE
classification.
</bodyText>
<sectionHeader confidence="0.976674" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999817333333333">
This work was partly supported by a grant from the
Air Force Research Laboratory’s Information
Directorate (AFRL/IF), Rome, NY, under contract
F30602-01-C-0035. The authors wish to thank
Carrie Pine and Sharon Walter of AFRL for
supporting and reviewing this work.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999850245283019">
Bikel, D. M. 1997. Nymble: a high-performance
learning name-finder. Proceedings of ANLP 1997,
194-201, Morgan Kaufmann Publishers.
Beckwith, R. et al. 1991. WordNet: A Lexical Database
Organized on Psycholinguistic Principles. Lexicons:
Using On-line Resources to build a Lexicon, Uri
Zernik, editor, Lawrence Erlbaum, Hillsdale, NJ.
Borthwick, A. et al. 1998. Description of the MENE
named Entity System. Proceedings of MUC-7.
Collins, M. and Y. Singer. 1999. Unsupervised Models
for Named Entity Classification. Proceedings of the
1999 Joint SIGDAT Conference on EMNLP and VLC.
Cucchiarelli, A. and P. Velardi. 2001. Unsupervised
Named Entity Recognition Using Syntactic and Se-
mantic Contextual Evidence. Computational
Linguistics, Volume 27, Number 1, 123-131.
Cucerzan, S. and D. Yarowsky. 1999. Language
Independent Named Entity Recognition Combining
Morphological and Contextual Evidence.
Proceedings of the 1999 Joint SIGDAT Conference on
EMNLP and VLC, 90-99.
Gale, W., K. Church, and D. Yarowsky. 1992. One
Sense Per Discourse. Proceedings of the 4th DARPA
Speech and Natural Language Workshop. 233-237.
Kim, J., I. Kang, and K. Choi. 2002. Unsupervised
Named Entity Classification Models and their
Ensembles. COLING 2002.
Krupka, G. R. and K. Hausman. 1998. IsoQuest Inc:
Description of the NetOwl Text Extraction System as
used for MUC-7. Proceedings of MUC-7.
Lin, D.K. 1998. Automatic Retrieval and Clustering of
Similar Words. COLING-ACL 1998.
MUC-7, 1998. Proceedings of the Seventh Message
Understanding Conference (MUC-7).
Thelen, M. and E. Riloff. 2002. A Bootstrapping
Method for Learning Semantic Lexicons using
Extraction Pattern Contexts. Proceedings of EMNLP
2002.
Segal, R. and O. Etzioni. 1994. Learning decision lists
using homogeneous rules. Proceedings of the 12th
National Conference on Artificial Intelligence.
Srihari, R., W. Li, C. Niu and T. Cornell. 2003.
InfoXtract: An Information Discovery Engine
Supported by New Levels of Information Extraction.
Proceeding of HLT-NAACL 2003 Workshop on
Software Engineering and Architecture of Language
Technology Systems, Edmonton, Canada.
Srihari, R., C. Niu, &amp; W. Li. 2000. A Hybrid Approach
for Named Entity and Sub-Type Tagging.
Proceedings of ANLP 2000, Seattle.
Yarowsky, David. 1995. Unsupervised Word Sense
Disambiguation Rivaling Supervised Method. ACL
1995.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987012">
<title confidence="0.998539">A Bootstrapping Approach to Named Entity Classification Using Successive Learners</title>
<author confidence="0.999796">Cheng Niu</author>
<author confidence="0.999796">Wei Li</author>
<author confidence="0.999796">Jihong Ding</author>
<author confidence="0.999796">Rohini K Srihari</author>
<affiliation confidence="0.999882">Cymfony Inc.</affiliation>
<address confidence="0.999935">600 Essjay Road, Williamsville, NY 14221. USA.</address>
<email confidence="0.999721">wei,jding,rohini}@cymfony.com</email>
<abstract confidence="0.999573956521739">This paper presents a new bootstrapping approach to named entity (NE) classification. This approach only requires a few common noun/pronoun seeds that correspond to the concept for the target type, e.g. PERSON NE. The entire bootstrapping procedure is implemented as training two successive learners: (i) a decision list is used to learn the parsing-based high precision NE rules; (ii) a Hidden Markov Model is then trained to learn string sequence-based NE patterns. The second learner uses the training corpus automatically tagged by the first learner. The resulting NE system approaches supervised NE performance for some NE types. The system also demonstrates intuitive support for tagging user-defined NE types. The differences of this approach from the co-training-based NE bootstrapping are also discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D M Bikel</author>
</authors>
<title>Nymble: a high-performance learning name-finder.</title>
<date>1997</date>
<booktitle>Proceedings of ANLP</booktitle>
<pages>194--201</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="1883" citStr="Bikel 1997" startWordPosition="273" endWordPosition="274">us proper names, time, or numerical expressions. Seven types of named entities are defined in the Message Understanding Conference (MUC) standards, namely, PERSON (PER), ORGANIZATION (ORG), LOCATION (LOC), TIME, DATE, MONEY, and PERCENT1 (MUC-7 1998). 1 This paper only focuses on classifying proper names. Time and numerical NEs are not yet explored using this method. There is considerable research on NE tagging using different techniques. These include systems based on handcrafted rules (Krupka 1998), as well as systems using supervised machine learning, such as the Hidden Markov Model (HMM) (Bikel 1997) and the Maximum Entropy Model (Borthwick 1998). The state-of-the-art rule-based systems and supervised learning systems can reach near-human performance for NE tagging in a targeted domain. However, both approaches face a serious knowledge bottleneck, making rapid domain porting difficult. Such systems cannot effectively support user-defined named entities. That is the motivation for using unsupervised or weaklysupervised machine learning that only requires a raw corpus from a given domain for this NE research. (Cucchiarelli &amp; Velardi 2001) discussed boosting the performance of an existing NE</context>
<context position="19558" citStr="Bikel 1997" startWordPosition="2985" endWordPosition="2986"> &lt;PER&gt; David Bonderman &lt;/PER&gt; , This corpus is used for training the second NE learner based on evidence from string sequences, to be described in Section 5 below. 5 String Sequence-based NE Learning String sequence-based HMM learning is set as our final goal for NE bootstrapping because of the demonstrated high performance of this type of NE taggers. In this research, a bi-gram HMM is trained based on the sample strings in the annotated corpus constructed in section 4. During the training, each sample string sequence is regarded as an independent sentence. The training process is similar to (Bikel 1997). The HMM is defined as follows: Given a word sequence W sequence = w 0 f0 � w n fn (where fj denotes a single token feature which will be defined below), the goal for the NE tagging task is to find the optimal NE tag sequence T sequence = t0t1t2 � tn , which maximizes the conditional probability Pr(T sequence |W sequence) (Bikel 1997). By Bayesian equality, this is equivalent to maximizing the joint probability Pr(W sequence,T sequence) . This joint probability can be computed by bi-gram HMM as follows: Pr(W sequence, T sequence) = ∏ Pr( w , f , t |w , f , t i i i i-1 i-1 i − 1 i The back-off</context>
</contexts>
<marker>Bikel, 1997</marker>
<rawString>Bikel, D. M. 1997. Nymble: a high-performance learning name-finder. Proceedings of ANLP 1997, 194-201, Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Beckwith</author>
</authors>
<title>WordNet: A Lexical Database Organized on Psycholinguistic Principles. Lexicons: Using On-line Resources to build a Lexicon,</title>
<date>1991</date>
<editor>Uri Zernik, editor, Lawrence Erlbaum,</editor>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="7619" citStr="Beckwith 1991" startWordPosition="1163" endWordPosition="1164">ng parsing-based rules parsing-based NE rules training corpus based on tagged NEs Decision List NE Learning Bootstrapping Procedure 3 Parsing-based NE Rule Learning The training of the first NE learner has three major properties: (i) the use of concept-based seeds, (ii) support from the parser, and (iii) representation as a decision list. This new bootstrapping approach is based on the observation that there is an underlying concept for any proper name type and this concept can be easily expressed by a set of common nouns or pronouns, similar to how concepts are defined by synsets in WordNet (Beckwith 1991). Concept-based seeds are conceptually equivalent to the proper name types that they represent. These seeds can be provided by a user intuitively. For example, a user can use pill, drug, medicine, etc. as concept-based seeds to guide the system in learning rules to tag MEDICINE names. This process is fairly intuitive, creating a favorable environment for configuring the NE system to the types of names sought by the user. An important characteristic of concept-based seeds is that they occur much more often than proper name seeds, hence they are effective in guiding the non-iterative NE bootstra</context>
</contexts>
<marker>Beckwith, 1991</marker>
<rawString>Beckwith, R. et al. 1991. WordNet: A Lexical Database Organized on Psycholinguistic Principles. Lexicons: Using On-line Resources to build a Lexicon, Uri Zernik, editor, Lawrence Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
</authors>
<title>Description of the MENE named Entity System.</title>
<date>1998</date>
<booktitle>Proceedings of MUC-7.</booktitle>
<contexts>
<context position="1930" citStr="Borthwick 1998" startWordPosition="280" endWordPosition="281">ions. Seven types of named entities are defined in the Message Understanding Conference (MUC) standards, namely, PERSON (PER), ORGANIZATION (ORG), LOCATION (LOC), TIME, DATE, MONEY, and PERCENT1 (MUC-7 1998). 1 This paper only focuses on classifying proper names. Time and numerical NEs are not yet explored using this method. There is considerable research on NE tagging using different techniques. These include systems based on handcrafted rules (Krupka 1998), as well as systems using supervised machine learning, such as the Hidden Markov Model (HMM) (Bikel 1997) and the Maximum Entropy Model (Borthwick 1998). The state-of-the-art rule-based systems and supervised learning systems can reach near-human performance for NE tagging in a targeted domain. However, both approaches face a serious knowledge bottleneck, making rapid domain porting difficult. Such systems cannot effectively support user-defined named entities. That is the motivation for using unsupervised or weaklysupervised machine learning that only requires a raw corpus from a given domain for this NE research. (Cucchiarelli &amp; Velardi 2001) discussed boosting the performance of an existing NE tagger by unsupervised learning based on parsi</context>
</contexts>
<marker>Borthwick, 1998</marker>
<rawString>Borthwick, A. et al. 1998. Description of the MENE named Entity System. Proceedings of MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification.</title>
<date>1999</date>
<booktitle>Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC.</booktitle>
<contexts>
<context position="2596" citStr="Collins &amp; Singer 1999" startWordPosition="372" endWordPosition="375">d supervised learning systems can reach near-human performance for NE tagging in a targeted domain. However, both approaches face a serious knowledge bottleneck, making rapid domain porting difficult. Such systems cannot effectively support user-defined named entities. That is the motivation for using unsupervised or weaklysupervised machine learning that only requires a raw corpus from a given domain for this NE research. (Cucchiarelli &amp; Velardi 2001) discussed boosting the performance of an existing NE tagger by unsupervised learning based on parsing structures. (Cucerzan &amp; Yarowsky 1999), (Collins &amp; Singer 1999) and (Kim 2002) presented various techniques using co-training schemes for NE extraction seeded by a small list of proper names or handcrafted NE rules. NE tagging has two tasks: (i) NE chunking; (ii) NE classification. Parsingsupported NE bootstrapping systems including ours only focus on NE classification, assuming NE chunks have been constructed by the parser. The key idea of co-training is the separation of features into several orthogonal views. In case of NE classification, usually one view uses the context evidence and the other relies on the lexicon evidence. Learners corresponding to </context>
<context position="4352" citStr="Collins &amp; Singer 1999" startWordPosition="644" endWordPosition="647">ased NE rules are learned with high precision but limited recall. Then, these rules are applied to a large raw corpus to automatically generate a tagged corpus. Finally, an HMM-based NE tagger is trained using this corpus. There is no iterative learning between the two learners, hence the process is free of the error propagation problem. The resulting NE system approaches supervised NE performance for some NE types. To derive the parsing-based learner, instead of seeding the bootstrapping process with NE instances from a proper name list or handcrafted NE rules as (Cucerzan &amp; Yarowsky 1999), (Collins &amp; Singer 1999) and (Kim 2002), the system only requires a few common noun or pronoun seeds that correspond to the concept for the targeted NE, e.g. he/she/man/woman for PERSON NE. Such concept-based seeds share grammatical structures with the corresponding NEs, hence a parser is utilized to support bootstrapping. Since pronouns and common nouns occur more often than NE instances, richer contextual evidence is available for effective learning. Using concept-based seeds, the parsingbased NE rules can be learned in one iteration so that the error propagation problem in the iterative learning can be avoided. Th</context>
<context position="25631" citStr="Collins &amp; Singer 1999" startWordPosition="4124" endWordPosition="4127">% 72.5% 69.8% Similar to the case of ORG NEs, the number of concept-based seeds is found to be insufficient to cover the variations of PRO subtypes. So the performance is not as good as PER and LOC NEs. Nevertheless, the benchmark shows the system works fairly effectively in extracting the userspecified NEs. It is noteworthy that domain knowledge such as knowing the major sub-types of the user-specified NE type is valuable in assisting the selection of appropriate concept-based seeds for performance enhancement. The performance of our HMM tagger is comparable with the reported performance in (Collins &amp; Singer 1999). But our benchmarking is more extensive as we used a much larger data set (20,000 NE instances in the testing corpus) than theirs (1,000 NE instances). 7 Conclusion A novel bootstrapping approach to NE classification is presented. This approach does not require iterative learning which may suffer from error propagation. With minimal human supervision in providing a handful of conceptbased seeds, the resulting NE tagger approaches supervised NE performance in NE types for PERSON and LOCATION. The system also demonstrates effective support for user-defined NE classification. Acknowledgement Thi</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Collins, M. and Y. Singer. 1999. Unsupervised Models for Named Entity Classification. Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cucchiarelli</author>
<author>P Velardi</author>
</authors>
<title>Unsupervised Named Entity Recognition Using Syntactic and Semantic Contextual Evidence.</title>
<date>2001</date>
<journal>Computational Linguistics, Volume</journal>
<volume>27</volume>
<pages>123--131</pages>
<contexts>
<context position="2430" citStr="Cucchiarelli &amp; Velardi 2001" startWordPosition="348" endWordPosition="351">g supervised machine learning, such as the Hidden Markov Model (HMM) (Bikel 1997) and the Maximum Entropy Model (Borthwick 1998). The state-of-the-art rule-based systems and supervised learning systems can reach near-human performance for NE tagging in a targeted domain. However, both approaches face a serious knowledge bottleneck, making rapid domain porting difficult. Such systems cannot effectively support user-defined named entities. That is the motivation for using unsupervised or weaklysupervised machine learning that only requires a raw corpus from a given domain for this NE research. (Cucchiarelli &amp; Velardi 2001) discussed boosting the performance of an existing NE tagger by unsupervised learning based on parsing structures. (Cucerzan &amp; Yarowsky 1999), (Collins &amp; Singer 1999) and (Kim 2002) presented various techniques using co-training schemes for NE extraction seeded by a small list of proper names or handcrafted NE rules. NE tagging has two tasks: (i) NE chunking; (ii) NE classification. Parsingsupported NE bootstrapping systems including ours only focus on NE classification, assuming NE chunks have been constructed by the parser. The key idea of co-training is the separation of features into sever</context>
</contexts>
<marker>Cucchiarelli, Velardi, 2001</marker>
<rawString>Cucchiarelli, A. and P. Velardi. 2001. Unsupervised Named Entity Recognition Using Syntactic and Semantic Contextual Evidence. Computational Linguistics, Volume 27, Number 1, 123-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Language Independent Named Entity Recognition Combining Morphological and Contextual Evidence.</title>
<date>1999</date>
<booktitle>Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC,</booktitle>
<pages>90--99</pages>
<contexts>
<context position="2571" citStr="Cucerzan &amp; Yarowsky 1999" startWordPosition="368" endWordPosition="371">he-art rule-based systems and supervised learning systems can reach near-human performance for NE tagging in a targeted domain. However, both approaches face a serious knowledge bottleneck, making rapid domain porting difficult. Such systems cannot effectively support user-defined named entities. That is the motivation for using unsupervised or weaklysupervised machine learning that only requires a raw corpus from a given domain for this NE research. (Cucchiarelli &amp; Velardi 2001) discussed boosting the performance of an existing NE tagger by unsupervised learning based on parsing structures. (Cucerzan &amp; Yarowsky 1999), (Collins &amp; Singer 1999) and (Kim 2002) presented various techniques using co-training schemes for NE extraction seeded by a small list of proper names or handcrafted NE rules. NE tagging has two tasks: (i) NE chunking; (ii) NE classification. Parsingsupported NE bootstrapping systems including ours only focus on NE classification, assuming NE chunks have been constructed by the parser. The key idea of co-training is the separation of features into several orthogonal views. In case of NE classification, usually one view uses the context evidence and the other relies on the lexicon evidence. L</context>
<context position="4327" citStr="Cucerzan &amp; Yarowsky 1999" startWordPosition="640" endWordPosition="643">llows. First, some parsing-based NE rules are learned with high precision but limited recall. Then, these rules are applied to a large raw corpus to automatically generate a tagged corpus. Finally, an HMM-based NE tagger is trained using this corpus. There is no iterative learning between the two learners, hence the process is free of the error propagation problem. The resulting NE system approaches supervised NE performance for some NE types. To derive the parsing-based learner, instead of seeding the bootstrapping process with NE instances from a proper name list or handcrafted NE rules as (Cucerzan &amp; Yarowsky 1999), (Collins &amp; Singer 1999) and (Kim 2002), the system only requires a few common noun or pronoun seeds that correspond to the concept for the targeted NE, e.g. he/she/man/woman for PERSON NE. Such concept-based seeds share grammatical structures with the corresponding NEs, hence a parser is utilized to support bootstrapping. Since pronouns and common nouns occur more often than NE instances, richer contextual evidence is available for effective learning. Using concept-based seeds, the parsingbased NE rules can be learned in one iteration so that the error propagation problem in the iterative le</context>
</contexts>
<marker>Cucerzan, Yarowsky, 1999</marker>
<rawString>Cucerzan, S. and D. Yarowsky. 1999. Language Independent Named Entity Recognition Combining Morphological and Contextual Evidence. Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC, 90-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>One Sense Per Discourse.</title>
<date>1992</date>
<booktitle>Proceedings of the 4th DARPA Speech and Natural Language Workshop.</booktitle>
<pages>233--237</pages>
<contexts>
<context position="16899" citStr="Gale et al. 1992" startWordPosition="2553" endWordPosition="2556"> small sample of the retrieved NE candidates with the associated parsing relationships are shown below: Deep South : Possess(project) Ramada : Possess(president) Argentina : Possess(first lady) After applying the decision list to the above the NE candidates, 33,104 PER names, 16,426 LOC names, 11,908 ORG names and 6,280 PRO names were extracted. It is a common practice in the bootstrapping research to make use of heuristics that suggest conditions under which instances should share the same answer. For example, the one sense per discourse principle is often used for word sense disambiguation (Gale et al. 1992). In this research, we used the heuristic one tag per domain for multiword NE in addition to the one sense per discourse principle. These heuristics were found to be very helpful in improving the performance of the bootstrapping algorithm for the purpose of both increasing positive instances (i.e. tag propagation) and decreasing the spurious instances (i.e. tag elimination). The following are two examples to show how the tag propagation and elimination scheme works. Tyco Toys occurs 67 times in the corpus, and 11 instances are recognized as ORG, only one instance is recognized as PER. Based on</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, W., K. Church, and D. Yarowsky. 1992. One Sense Per Discourse. Proceedings of the 4th DARPA Speech and Natural Language Workshop. 233-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>I Kang</author>
<author>K Choi</author>
</authors>
<title>Unsupervised Named Entity Classification Models and their Ensembles. COLING</title>
<date>2002</date>
<marker>Kim, Kang, Choi, 2002</marker>
<rawString>Kim, J., I. Kang, and K. Choi. 2002. Unsupervised Named Entity Classification Models and their Ensembles. COLING 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G R Krupka</author>
<author>K Hausman</author>
</authors>
<title>IsoQuest Inc: Description of the NetOwl Text Extraction System as used for MUC-7.</title>
<date>1998</date>
<booktitle>Proceedings of MUC-7.</booktitle>
<marker>Krupka, Hausman, 1998</marker>
<rawString>Krupka, G. R. and K. Hausman. 1998. IsoQuest Inc: Description of the NetOwl Text Extraction System as used for MUC-7. Proceedings of MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<publisher>COLING-ACL</publisher>
<contexts>
<context position="8967" citStr="Lin 1998" startWordPosition="1370" endWordPosition="1371">ity with the corresponding NEs at structural level, not at string sequence level. For example, at string sequence level, PERSON names are often preceded by a set of prefixing title words Mr./Mrs./Miss/Dr. etc., but the corresponding common noun seeds man/woman etc. cannot appear in such patterns. However, at structural level, the concept-based seeds share the same or similar linguistic patterns (e.g. Subject-Verb-Object patterns) with the corresponding types of proper names. The rationale behind using concept-based seeds in NE bootstrapping is similar to that for parsingbased word clustering (Lin 1998): conceptually similar words occur in structurally similar context. In fact, the anaphoric function of pronouns and common nouns to represent antecedent NEs indicates the substitutability of proper names by the corresponding common nouns or pronouns. For example, this man can be substituted for the proper name John Smith in almost all structural patterns. Following the same rationale, a bootstrapping approach is applied to the semantic lexicon acquisition task [Thelen &amp; Riloff. 2002]. The InfoXtract parser supports dependency parsing based on the linguistic units constructed by our shallow par</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D.K. 1998. Automatic Retrieval and Clustering of Similar Words. COLING-ACL 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-7</author>
</authors>
<date>1998</date>
<booktitle>Proceedings of the Seventh Message Understanding Conference (MUC-7).</booktitle>
<marker>MUC-7, 1998</marker>
<rawString>MUC-7, 1998. Proceedings of the Seventh Message Understanding Conference (MUC-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thelen</author>
<author>E Riloff</author>
</authors>
<title>A Bootstrapping Method for Learning Semantic Lexicons using Extraction Pattern Contexts.</title>
<date>2002</date>
<booktitle>Proceedings of EMNLP</booktitle>
<marker>Thelen, Riloff, 2002</marker>
<rawString>Thelen, M. and E. Riloff. 2002. A Bootstrapping Method for Learning Semantic Lexicons using Extraction Pattern Contexts. Proceedings of EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Segal</author>
<author>O Etzioni</author>
</authors>
<title>Learning decision lists using homogeneous rules.</title>
<date>1994</date>
<booktitle>Proceedings of the 12th National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="11826" citStr="Segal &amp; Etzioni 1994" startWordPosition="1782" endWordPosition="1785">ciated with one or more of the five dependency relations are retrieved: 821,267 instances in total in our experiment. Each seed instance was assigned a concept tag corresponding to NE. For example, each instance of he is marked as PER. The marked instances plus their associated parsing relationships form an annotated NE corpus, as shown below: he/PER: Has_Predicate(say) she/PER: Has_Predicate(get) company/ORG: Object_Of(compel) city/LOC: Possess(mayor) car/PRO: Object_Of(manufacture) HasAmod(high-quality) This training corpus supports the Decision List Learning which learns homogeneous rules (Segal &amp; Etzioni 1994). The accuracy of each rule was evaluated using Laplace smoothing: positive 1 + positive negative NE category No. + + It is noteworthy that the PER tag dominates the corpus due to the fact that the pronouns he and she occur much more often than the seeded common nouns. So the proportion of NE types in the instances of concept-based seeds is not the same as the proportion of NE types in the proper name instances. For example, considering a running text containing one instance of John Smith and one instance of a city name Rochester, it is more likely that John Smith will be referred to by he/him</context>
</contexts>
<marker>Segal, Etzioni, 1994</marker>
<rawString>Segal, R. and O. Etzioni. 1994. Learning decision lists using homogeneous rules. Proceedings of the 12th National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Srihari</author>
<author>W Li</author>
<author>C Niu</author>
<author>T Cornell</author>
</authors>
<title>InfoXtract: An Information Discovery Engine Supported by New Levels of Information Extraction.</title>
<date>2003</date>
<booktitle>Proceeding of HLT-NAACL 2003 Workshop on Software Engineering and Architecture of Language Technology Systems,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="5692" citStr="Srihari et al. 2003" startWordPosition="857" endWordPosition="860">o tag user-defined NE types. The remaining part of the paper is organized as follows. The overall system design is presented in Section 2. Section 3 describes the parsing-based NE learning. Section 4 presents the automatic construction of annotated NE corpus by parsingbased NE classification. Section 5 presents the string level HMM NE learning. Benchmarks are shown in Section 6. Section 7 is the Conclusion. 2 System Design Figure 1 shows the overall system architecture. Before the bootstrapping is started, a large raw training corpus is parsed by the English parser from our InfoXtract system (Srihari et al. 2003). The bootstrapping experiment reported in this paper is based on a corpus containing ~100,000 news articles and a total of ~88,000,000 words. The parsed corpus is saved into a repository, which supports fast retrieval by a keyword-based indexing scheme. Although the parsing-based NE learner is found to suffer from the recall problem, we can apply the learned rules to a huge parsed corpus. In other words, the availability of an almost unlimited raw corpus compensates for the modest recall. As a result, large quantities of NE instances are automatically acquired. An automatically annotated NE c</context>
<context position="9592" citStr="Srihari et al. 2003" startWordPosition="1460" endWordPosition="1463">ceptually similar words occur in structurally similar context. In fact, the anaphoric function of pronouns and common nouns to represent antecedent NEs indicates the substitutability of proper names by the corresponding common nouns or pronouns. For example, this man can be substituted for the proper name John Smith in almost all structural patterns. Following the same rationale, a bootstrapping approach is applied to the semantic lexicon acquisition task [Thelen &amp; Riloff. 2002]. The InfoXtract parser supports dependency parsing based on the linguistic units constructed by our shallow parser (Srihari et al. 2003). Five types of the decoded dependency relationships are used for parsing-based NE rule learning. These are all directional, binary dependency links between linguistic units: (1) Has_Predicate: from logical subject to verb e.g. He said she would want him to join. 4 he: Has_Predicate(say) she: Has_Predicate(want) him: Has_Predicate(join) (2) Object_Of : from logical object to verb e.g. This company was founded to provide new telecommunication services. 4 company: Object_Of(found) service: Object_Of(provide) (3) Has_Amod: from noun to its adjective modifier e.g. He is a smart, handsome young man</context>
</contexts>
<marker>Srihari, Li, Niu, Cornell, 2003</marker>
<rawString>Srihari, R., W. Li, C. Niu and T. Cornell. 2003. InfoXtract: An Information Discovery Engine Supported by New Levels of Information Extraction. Proceeding of HLT-NAACL 2003 Workshop on Software Engineering and Architecture of Language Technology Systems, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Srihari</author>
<author>C Niu</author>
<author>W Li</author>
</authors>
<title>A Hybrid Approach for Named Entity and Sub-Type Tagging.</title>
<date>2000</date>
<booktitle>Proceedings of ANLP 2000,</booktitle>
<location>Seattle.</location>
<marker>Srihari, Niu, Li, 2000</marker>
<rawString>Srihari, R., C. Niu, &amp; W. Li. 2000. A Hybrid Approach for Named Entity and Sub-Type Tagging. Proceedings of ANLP 2000, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Method.</title>
<date>1995</date>
<publisher>ACL</publisher>
<contexts>
<context position="17896" citStr="Yarowsky 1995" startWordPosition="2720" endWordPosition="2721">ollowing are two examples to show how the tag propagation and elimination scheme works. Tyco Toys occurs 67 times in the corpus, and 11 instances are recognized as ORG, only one instance is recognized as PER. Based on the heuristic one tag per domain for multi-word NE, the minority tag of PER is removed, and all the 67 instances of Tyco Toys are tagged as ORG. Three instances of Postal Service are recognized as ORG, and two instances are recognized as PER. These tags are regarded as noise, hence are removed by the tag elimination scheme. The tag propagation/elimination scheme is adopted from (Yarowsky 1995). After this step, a total of 386,614 proper names were recognized, including 134,722 PER names, 186,488 LOC names, 46,231 ORG names and 19,173 PRO names. The overall precision was ~90%. The benchmark details will be shown in Section 6. The extracted proper name instances then led to the construction of a fairly large training corpus sufficient for training the second NE learner. Unlike manually annotated running text corpus, this corpus consists of only sample string sequences containing the automatically tagged NE instances and their left and right neighboring words within the same sentence.</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, David. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Method. ACL 1995.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>