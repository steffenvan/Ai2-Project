<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.053360">
<title confidence="0.968799">
Linguistically debatable or just plain wrong?
</title>
<author confidence="0.998109">
Barbara Plank, Dirk Hovy and Anders Søgaard
</author>
<affiliation confidence="0.9982865">
Center for Language Technology
University of Copenhagen, Denmark
</affiliation>
<address confidence="0.826572">
Njalsgade 140, DK-2300 Copenhagen S
</address>
<email confidence="0.9979">
bplank@cst.dk,dirk@cst.dk,soegaard@hum.ku.dk
</email>
<sectionHeader confidence="0.993861" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998108">
In linguistic annotation projects, we typ-
ically develop annotation guidelines to
minimize disagreement. However, in this
position paper we question whether we
should actually limit the disagreements
between annotators, rather than embracing
them. We present an empirical analysis
of part-of-speech annotated data sets that
suggests that disagreements are systematic
across domains and to a certain extend also
across languages. This points to an un-
derlying ambiguity rather than random er-
rors. Moreover, a quantitative analysis of
tag confusions reveals that the majority of
disagreements are due to linguistically de-
batable cases rather than annotation errors.
Specifically, we show that even in the ab-
sence of annotation guidelines only 2% of
annotator choices are linguistically unmo-
tivated.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999992363636363">
In NLP, we often model annotation as if it re-
flected a single ground truth that was guided by
an underlying linguistic theory. If this was true,
the specific theory should be learnable from the
annotated data. However, it is well known that
there are linguistically hard cases (Zeman, 2010),
where no theory provides a clear answer, so an-
notation schemes commit to more or less arbi-
trary decisions. For example, in parsing auxil-
iary verbs may head main verbs, or vice versa,
and in part-of-speech (POS) tagging, possessive
pronouns may belong to the category of deter-
miners or the category of pronouns. This posi-
tion paper argues that annotation projects should
embrace these hard cases rather than pretend they
can be unambiguously resolved. Instead of using
overly specific annotation guidelines, designed to
minimize inter-annotator disagreement (Duffield
et al., 2007), and adjudicating between annotators
of different opinions, we should embrace system-
atic inter-annotator disagreements. To motivate
this, we present an empirical analysis showing
</bodyText>
<listItem confidence="0.913045">
1. that certain inter-annotator disagreements are
systematic, and
2. that actual errors are in fact so infrequent as
to be negligible, even when linguists annotate
without guidelines.
</listItem>
<bodyText confidence="0.999773666666667">
The empirical analysis presented below relies
on text corpora annotated with syntactic cate-
gories or parts-of-speech (POS). POS is part of
most linguistic theories, but nevertheless, there
are still many linguistic constructions – even very
frequent ones – whose POS analysis is widely
debated. The following sentences exemplify some
of these hard cases that annotators frequently
disagree on. Note that we do not claim that both
analyses in each of these cases (1–3) are equally
good, but that there is some linguistic motivation
for either analysis in each case.
</bodyText>
<listItem confidence="0.9818705">
(1) Noam goes out tonight
NOUN VERB ADP/PRT ADV/NOUN
(2) Noam likes social media
NOUN VERB ADJ/NOUN NOUN
(3) Noam likes his car
NOUN VERB DET/PRON NOUN
</listItem>
<bodyText confidence="0.999988545454546">
To substantiate our claims, we first compare
the distribution of inter-annotator disagreements
across domains and languages, showing that most
disagreements are systematic (Section 2). This
suggests that most annotation differences derive
from hard cases, rather than random errors.
We then collect a corpus of such disagreements
and have experts mark which ones are due to ac-
tual annotation errors, and which ones reflect lin-
guistically hard cases (Section 3). The results
show that the majority of disagreements are due
</bodyText>
<page confidence="0.950427">
507
</page>
<bodyText confidence="0.967377083333333">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 507–511,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
to hard cases, and only about 20% of conflict-
ing annotations are actual errors. This suggests
that inter-annotator agreement scores often hide
the fact that the vast majority of annotations are
actually linguistically motivated. In our case, less
than 2% of the overall annotations are linguisti-
cally unmotivated.
Finally, in Section 4, we present an experiment
trying to learn a model to distinguish between hard
cases and annotation errors.
</bodyText>
<sectionHeader confidence="0.976919" genericHeader="method">
2 Annotator disagreements across
domains and languages
</sectionHeader>
<bodyText confidence="0.99996775862069">
In this study, we had between 2-10 individual an-
notators with degrees in linguistics annotate dif-
ferent kinds of English text with POS tags, e.g.,
newswire text (PTB WSJ Section 00), transcripts
of spoken language (from a database containing
transcripts of conversations, Talkbank1), as well
as Twitter posts. Annotators were specifically not
presented with guidelines that would help them re-
solve hard cases. Moreover, we compare profes-
sional annotation to that of lay people. We in-
structed annotators to use the 12 universal POS
tags of Petrov et al. (2012). We did so in or-
der to make comparison between existing data
sets possible. Moreover, this allows us to fo-
cus on really hard cases, as any debatable case in
the coarse-grained tag set is necessarily also part
of the finer-grained tag set.2 For each domain,
we collected exactly 500 doubly-annotated sen-
tences/tweets. Besides these English data sets, we
also obtained doubly-annotated POS data from the
French Social Media Bank project (Seddah et al.,
2012).3 All data sets, except the French one, are
publicly available at http://lowlands.ku.
dk/.
We present disagreements as Hinton diagrams
in Figure 1a–c. Note that the spoken language data
does not include punctuation. The correlations
between the disagreements are highly significant,
with Spearman coefficients ranging from 0.644
</bodyText>
<footnote confidence="0.944552181818182">
1http://talkbank.org/
2Experiments with variation n-grams on WSJ (Dickinson
and Meurers, 2003) and the French data lead us to estimate
that the fine-to-coarse mapping of POS tags disregards about
20% of observed tag-pair confusion types, most of which re-
late to fine-grained verb and noun distinctions, e.g. past par-
ticiple versus past in “[..] criminal lawyers speculated/VBD
vs. VBN that [..]”.
3We mapped POS tags into the universal POS tags using
the mappings available here: https://code.google.
com/p/universal-pos-tags/
</footnote>
<bodyText confidence="0.99987037254902">
(spoken and WSJ) to 0.869 (spoken and Twit-
ter). Kendall’s τ ranges from 0.498 (Twitter and
WSJ) to 0.659 (spoken and Twitter). All diagrams
have a vaguely “dagger”-like shape, with the blade
going down the diagonal from top left to bot-
tom right, and a slightly curved “hilt” across the
counter-diagonal, ending in the more pronounced
ADP/PRT confusion cells.
Disagreements are very similar across all three
domains. In particular, adpositions (ADP) are con-
fused with particles (PRT) (as in the case of “get
out”); adjectives (ADJ) are confused with nouns
(as in “stone lion”); pronouns (PRON) are con-
fused with determiners (DET) (“my house”); nu-
merals are confused with adjectives, determiners,
and nouns (“2nd time”); and adjectives are con-
fused with adverbs (ADV) (“see you later”). In
Twitter, the X category is often confused with
punctuations, e.g., when annotating punctuation
acting as discourse continuation marker.
Our analyses show that a) experts disagree on
the known hard cases when freely annotating text,
and b) that these disagreements are the same
across text types. More surprisingly, though, we
also find that, as discussed next, c) roughly the
same disagreements are also observed when com-
paring the linguistic intuitions of lay people.
More specifically, we had lay annotators on the
crowdsourcing platform Crowdflower re-annotate
the training section of Gimpel et al. (2011). They
collected five annotations per word. Only annota-
tors that had answered correctly on 4 gold items
(randomly chosen from a set of 20 gold items
provided by the authors) were allowed to submit
annotations. In total, 177 individual annotators
supplied answers. We paid annotators a reward
of $0.05 for 10 items. The full data set con-
tains 14,619 items and is described in further de-
tail in Hovy et al. (2014). Annotators were satis-
fied with the task (4.5 on a scale from 1 to 5) and
felt that instructions were clear (4.4/5), and the pay
reasonable (4.1/5). The crowdsourced annotations
aggregated using majority voting agree with the
expert annotations in 79.54% of the cases. If we
pre-filter the data via Wiktionary and use an item-
response model (Hovy et al., 2013) rather than ma-
jority voting, the agreement rises to 80.58%.
Figure 2 presents the Hinton diagram of the dis-
agreements of lay people. Disagreements are very
similar to the disagreements between expert an-
notators, especially on Twitter data (Figure 1b).
</bodyText>
<page confidence="0.959664">
508
</page>
<figure confidence="0.961262">
a) b) c)
</figure>
<figureCaption confidence="0.990421">
Figure 1: Hinton diagrams of inter-annotator disagreement on (a) excerpt from WSJ (Marcus et al.,
1993), (b) random Twitter sample, and (c) pre-transcribed spoken language excerpts from talkbank.org
</figureCaption>
<bodyText confidence="0.98760325">
One difference is that lay people do not confuse
numerals very often, probably because they rely
more on orthographic cues than on distributional
evidence. The disagreements are still strongly cor-
related with the ones observed with expert anno-
tators, but at a slightly lower coefficient (with a
Spearman’s ρ of 0.493 and Kendall’s T of 0.366
for WSJ).
</bodyText>
<figureCaption confidence="0.990574">
Figure 2: Disagreement between lay annotators
</figureCaption>
<bodyText confidence="0.999989125">
Lastly, we compare the disagreements of anno-
tators on a French social media data set (Seddah et
al., 2012), which we mapped to the universal POS
tag set. Again, we see the familiar dagger shape.
The Spearman coefficient with English Twitter is
0.288; Kendall’s T is 0.204. While the correlation
is weaker across languages than across domains, it
remains statistically significant (p &lt; 0.001).
</bodyText>
<sectionHeader confidence="0.857596" genericHeader="method">
3 Hard cases and annotation errors
</sectionHeader>
<bodyText confidence="0.9985782">
In the previous section, we demonstrated that
some disagreements are consistent across domains
and languages. We noted earlier, though, that dis-
agreements can arise both from hard cases and
from annotation errors. This can explain some
</bodyText>
<figureCaption confidence="0.997228">
Figure 3: Disagreement on French social media
</figureCaption>
<bodyText confidence="0.999695304347826">
of the variation. In this section, we investigate
what happens if we weed out obvious errors by
detecting annotation inconsistencies across a cor-
pus. The disagreements that remain are the truly
hard cases.
We use a modified version of the a priori algo-
rithm introduced in Dickinson and Meurers (2003)
to identify annotation inconsistencies. It works
by collecting “variation n-grams”, i.e. the longest
sequence of words (n-gram) in a corpus that has
been observed with a token being tagged differ-
ently in another occurence of the same n-gram in
the same corpus. The algorithm starts off by look-
ing for unigrams and expands them until no longer
n-grams are found.
For each variation n-gram that we found in
WSJ-00, i.e, a word in various contexts and the
possible tags associated with it, we present anno-
tators with the cross product of contexts and tags.
Essentially, we ask for a binary decision: Is the tag
plausible for the given context?
We used 3 annotators with PhD degrees in lin-
guistics. In total, our data set contains 880 items,
</bodyText>
<page confidence="0.99621">
509
</page>
<bodyText confidence="0.999317583333333">
i.e. 440 annotated confusion tag pairs. The raw
agreement was 86%. Figure 4 shows how truly
hard cases are distributed over tag pairs (dark gray
bars), as well as the proportion of confusions with
respect to a given tag pair that are truly hard cases
(light gray bars). The figure shows, for instance,
that the variation n-gram regarding ADP-ADV is
the second most frequent one (dark gray), and
approximately 70% of ADP-ADV disagreements
are linguistically hard cases (light gray). NOUN-
PRON disagreements are always linguistically de-
batable cases, while they are less frequent.
</bodyText>
<figureCaption confidence="0.994218">
Figure 4: Relative frequency of hard cases
</figureCaption>
<bodyText confidence="0.999981428571428">
A survey of hard cases. To further test the idea
of there being truly hard cases that probably can-
not be resolved by linguistic theory, we presented
nine linguistics faculty members with 10 of the
above examples and asked them to pick their fa-
vorite analyses. In 8/10 cases, the faculty mem-
bers disagreed on the right analysis.
</bodyText>
<sectionHeader confidence="0.774636" genericHeader="method">
4 Learning to detect annotation errors
</sectionHeader>
<bodyText confidence="0.99991275">
In this section, we examine whether we can learn
a classifier to distinguish between hard cases and
annotation errors. In order to do so, we train a clas-
sifier on the annotated data set containing 440 tag-
confusion pairs by relying only on surface form
features. If we balance the data set and perform 3-
fold cross-validation, a L2-regularized logistic re-
gression (L2-LR) model achieves an fi-score for
detecting errors at 70% (cf. Table 1), which is
above average, but not very impressive.
The two classes are apparently not easily sepa-
rable using surface form features, as illustrated in
</bodyText>
<equation confidence="0.854443">
fi HARD CASES ERRORS
73%(71-77) 70%(65-75)
76%(76-77) 71%(68-72)
</equation>
<tableCaption confidence="0.946462">
Table 1: Classification results
</tableCaption>
<figureCaption confidence="0.993806">
Figure 5: Hard cases and errors in 2d-PCA
</figureCaption>
<bodyText confidence="0.996738">
the two-dimensional plot in Figure 5, obtained us-
ing PCA. The logistic regression decision bound-
ary is plotted as a solid, black line. This is prob-
ably also why the nearest neighbor (NN) classi-
fier does slightly better, but again, performance is
rather low. While other features may reveal that
the problem is in fact learnable, our initial experi-
ments lead us to conclude that, given the low ratio
of errors over truly hard cases, learning to detect
errors is often not worthwhile.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999865647058824">
Juergens (2014) presents work on detecting lin-
guistically hard cases in the context of word
sense annotations, e.g., cases where expert an-
notators will disagree, as well as differentiat-
ing between underspecified, overspecified and
metaphoric cases. This work is similar to ours in
spirit, but considers a very different task. While
we also quantify the proportion of hard cases and
present an analysis of these cases, we also show
that disagreements are systematic.
Our work also relates to work on automatically
correcting expert annotations for inconsistencies
(Dickinson and Meurers, 2003). This work is
very different in spirit from our work, but shares
an interest in reconsidering expert annotations,
and we made use of their mining algorithm here.
There has also been recent work on adjudicat-
</bodyText>
<note confidence="0.4514415">
L2-LR
NN
</note>
<page confidence="0.978901">
510
</page>
<bodyText confidence="0.999550411764706">
ing noisy crowdsourced annotations (Dawid and
Skene, 1979; Smyth et al., 1995; Carpenter, 2008;
Whitehill et al., 2009; Welinder et al., 2010; Yan
et al., 2010; Raykar and Yu, 2012; Hovy et al.,
2013). Again, their objective is orthogonal to
ours, namely to collapse multiple annotations into
a gold standard rather than embracing disagree-
ments.
Finally, Plank et al. (2014) use small samples of
doubly-annotated POS data to estimate annotator
reliability and show how those metrics can be im-
plemented in the loss function when inducing POS
taggers to reflect confidence we can put in annota-
tions. They show that not biasing the theory to-
wards a single annotator but using a cost-sensitive
learning scheme makes POS taggers more robust
and more applicable for downstream tasks.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999914416666667">
In this paper, we show that disagreements between
professional or lay annotators are systematic and
consistent across domains and some of them are
systematic also across languages. In addition, we
present an empirical analysis of POS annotations
showing that the vast majority of inter-annotator
disagreements are competing, but valid, linguis-
tic interpretations. We propose to embrace such
disagreements rather than using annotation guide-
lines to optimize inter-annotator agreement, which
would bias our models in favor of some linguistic
theory.
</bodyText>
<sectionHeader confidence="0.994947" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99855925">
We would like to thank the anonymous reviewers
for their feedback, as well as Djam´e Seddah for the
French data. This research is funded by the ERC
Starting Grant LOWLANDS No. 313695.
</bodyText>
<sectionHeader confidence="0.998569" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999613380952381">
Bob Carpenter. 2008. Multilevel Bayesian models of
categorical data annotation. Technical report, Ling-
Pipe.
A. Philip Dawid and Allan M. Skene. 1979. Max-
imum likelihood estimation of observer error-rates
using the EM algorithm. Applied Statistics, pages
20–28.
Markus Dickinson and Detmar Meurers. 2003. Detect-
ing errors in part-of-speech annotation. In EACL.
Cecily Duffield, Jena Hwang, Susan Brown, Dmitriy
Dligach, Sarah Vieweg, Jenny Davis, and Martha
Palmer. 2007. Criteria for the manual grouping of
verb senses. In LAW.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-Speech Tagging
for Twitter: Annotation, Features, and Experiments.
In ACL.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with MACE. In NAACL.
Dirk Hovy, Barbara Plank, and Anders Søgaard. 2014.
Experiments with crowdsourced re-annotation of a
POS tagging data set. In ACL.
David Juergens. 2014. An analysis of ambiguity in
word sense annotations. In LREC.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313–330.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Barbara Plank, Dirk Hovy, and Anders Søgaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In EACL.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminat-
ing Spammers and Ranking Annotators for Crowd-
sourced Labeling Tasks. Journal of Machine Learn-
ing Research, 13:491–518.
Djam´e Seddah, Benoit Sagot, Marie Candito, Virginie
Mouilleron, and Vanessa Combet. 2012. The
French Social Media Bank: a treebank of noisy user
generated content. In COLING.
Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Per-
ona, and Pierre Baldi. 1995. Inferring ground truth
from subjective labelling of Venus images. In NIPS.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wisdom
of crowds. In NIPS.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. In NIPS.
Yan Yan, R´omer Rosales, Glenn Fung, Mark Schmidt,
Gerardo Hermosillo, Luca Bogoni, Linda Moy, and
Jennifer Dy. 2010. Modeling annotator expertise:
Learning when everybody knows a bit of something.
In AIStats.
Daniel Zeman. 2010. Hard problems of tagset con-
version. In Proceedings of the Second International
Conference on Global Interoperability for Language
Resources.
</reference>
<page confidence="0.997671">
511
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.738593">
<title confidence="0.9565">Linguistically debatable or just plain wrong?</title>
<author confidence="0.910026">Barbara Plank</author>
<author confidence="0.910026">Dirk Hovy</author>
<author confidence="0.910026">Anders</author>
<affiliation confidence="0.998795">Center for Language University of Copenhagen,</affiliation>
<address confidence="0.994659">Njalsgade 140, DK-2300 Copenhagen</address>
<email confidence="0.994154">bplank@cst.dk,dirk@cst.dk,soegaard@hum.ku.dk</email>
<abstract confidence="0.991825714285714">In linguistic annotation projects, we typically develop annotation guidelines to minimize disagreement. However, in this position paper we question whether we should actually limit the disagreements between annotators, rather than embracing them. We present an empirical analysis of part-of-speech annotated data sets that suggests that disagreements are systematic across domains and to a certain extend also across languages. This points to an underlying ambiguity rather than random errors. Moreover, a quantitative analysis of tag confusions reveals that the majority of disagreements are due to linguistically debatable cases rather than annotation errors. Specifically, we show that even in the absence of annotation guidelines only 2% of annotator choices are linguistically unmotivated.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>Multilevel Bayesian models of categorical data annotation.</title>
<date>2008</date>
<tech>Technical report, LingPipe.</tech>
<contexts>
<context position="14020" citStr="Carpenter, 2008" startWordPosition="2210" endWordPosition="2211">t considers a very different task. While we also quantify the proportion of hard cases and present an analysis of these cases, we also show that disagreements are systematic. Our work also relates to work on automatically correcting expert annotations for inconsistencies (Dickinson and Meurers, 2003). This work is very different in spirit from our work, but shares an interest in reconsidering expert annotations, and we made use of their mining algorithm here. There has also been recent work on adjudicatL2-LR NN 510 ing noisy crowdsourced annotations (Dawid and Skene, 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012; Hovy et al., 2013). Again, their objective is orthogonal to ours, namely to collapse multiple annotations into a gold standard rather than embracing disagreements. Finally, Plank et al. (2014) use small samples of doubly-annotated POS data to estimate annotator reliability and show how those metrics can be implemented in the loss function when inducing POS taggers to reflect confidence we can put in annotations. They show that not biasing the theory towards a single annotator but using a cost-sensitive lear</context>
</contexts>
<marker>Carpenter, 2008</marker>
<rawString>Bob Carpenter. 2008. Multilevel Bayesian models of categorical data annotation. Technical report, LingPipe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Philip Dawid</author>
<author>Allan M Skene</author>
</authors>
<title>Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied Statistics,</title>
<date>1979</date>
<pages>20--28</pages>
<contexts>
<context position="13983" citStr="Dawid and Skene, 1979" startWordPosition="2202" endWordPosition="2205"> This work is similar to ours in spirit, but considers a very different task. While we also quantify the proportion of hard cases and present an analysis of these cases, we also show that disagreements are systematic. Our work also relates to work on automatically correcting expert annotations for inconsistencies (Dickinson and Meurers, 2003). This work is very different in spirit from our work, but shares an interest in reconsidering expert annotations, and we made use of their mining algorithm here. There has also been recent work on adjudicatL2-LR NN 510 ing noisy crowdsourced annotations (Dawid and Skene, 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012; Hovy et al., 2013). Again, their objective is orthogonal to ours, namely to collapse multiple annotations into a gold standard rather than embracing disagreements. Finally, Plank et al. (2014) use small samples of doubly-annotated POS data to estimate annotator reliability and show how those metrics can be implemented in the loss function when inducing POS taggers to reflect confidence we can put in annotations. They show that not biasing the theory towards a single anno</context>
</contexts>
<marker>Dawid, Skene, 1979</marker>
<rawString>A. Philip Dawid and Allan M. Skene. 1979. Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied Statistics, pages 20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
<author>Detmar Meurers</author>
</authors>
<title>Detecting errors in part-of-speech annotation.</title>
<date>2003</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="5683" citStr="Dickinson and Meurers, 2003" startWordPosition="853" endWordPosition="856">ain, we collected exactly 500 doubly-annotated sentences/tweets. Besides these English data sets, we also obtained doubly-annotated POS data from the French Social Media Bank project (Seddah et al., 2012).3 All data sets, except the French one, are publicly available at http://lowlands.ku. dk/. We present disagreements as Hinton diagrams in Figure 1a–c. Note that the spoken language data does not include punctuation. The correlations between the disagreements are highly significant, with Spearman coefficients ranging from 0.644 1http://talkbank.org/ 2Experiments with variation n-grams on WSJ (Dickinson and Meurers, 2003) and the French data lead us to estimate that the fine-to-coarse mapping of POS tags disregards about 20% of observed tag-pair confusion types, most of which relate to fine-grained verb and noun distinctions, e.g. past participle versus past in “[..] criminal lawyers speculated/VBD vs. VBN that [..]”. 3We mapped POS tags into the universal POS tags using the mappings available here: https://code.google. com/p/universal-pos-tags/ (spoken and WSJ) to 0.869 (spoken and Twitter). Kendall’s τ ranges from 0.498 (Twitter and WSJ) to 0.659 (spoken and Twitter). All diagrams have a vaguely “dagger”-lik</context>
<context position="10156" citStr="Dickinson and Meurers (2003)" startWordPosition="1569" endWordPosition="1572"> &lt; 0.001). 3 Hard cases and annotation errors In the previous section, we demonstrated that some disagreements are consistent across domains and languages. We noted earlier, though, that disagreements can arise both from hard cases and from annotation errors. This can explain some Figure 3: Disagreement on French social media of the variation. In this section, we investigate what happens if we weed out obvious errors by detecting annotation inconsistencies across a corpus. The disagreements that remain are the truly hard cases. We use a modified version of the a priori algorithm introduced in Dickinson and Meurers (2003) to identify annotation inconsistencies. It works by collecting “variation n-grams”, i.e. the longest sequence of words (n-gram) in a corpus that has been observed with a token being tagged differently in another occurence of the same n-gram in the same corpus. The algorithm starts off by looking for unigrams and expands them until no longer n-grams are found. For each variation n-gram that we found in WSJ-00, i.e, a word in various contexts and the possible tags associated with it, we present annotators with the cross product of contexts and tags. Essentially, we ask for a binary decision: Is</context>
<context position="13706" citStr="Dickinson and Meurers, 2003" startWordPosition="2156" endWordPosition="2159">ften not worthwhile. 5 Related work Juergens (2014) presents work on detecting linguistically hard cases in the context of word sense annotations, e.g., cases where expert annotators will disagree, as well as differentiating between underspecified, overspecified and metaphoric cases. This work is similar to ours in spirit, but considers a very different task. While we also quantify the proportion of hard cases and present an analysis of these cases, we also show that disagreements are systematic. Our work also relates to work on automatically correcting expert annotations for inconsistencies (Dickinson and Meurers, 2003). This work is very different in spirit from our work, but shares an interest in reconsidering expert annotations, and we made use of their mining algorithm here. There has also been recent work on adjudicatL2-LR NN 510 ing noisy crowdsourced annotations (Dawid and Skene, 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012; Hovy et al., 2013). Again, their objective is orthogonal to ours, namely to collapse multiple annotations into a gold standard rather than embracing disagreements. Finally, Plank et al. (2014) use s</context>
</contexts>
<marker>Dickinson, Meurers, 2003</marker>
<rawString>Markus Dickinson and Detmar Meurers. 2003. Detecting errors in part-of-speech annotation. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecily Duffield</author>
<author>Jena Hwang</author>
<author>Susan Brown</author>
<author>Dmitriy Dligach</author>
<author>Sarah Vieweg</author>
<author>Jenny Davis</author>
<author>Martha Palmer</author>
</authors>
<title>Criteria for the manual grouping of verb senses.</title>
<date>2007</date>
<publisher>In LAW.</publisher>
<contexts>
<context position="1927" citStr="Duffield et al., 2007" startWordPosition="282" endWordPosition="285"> linguistically hard cases (Zeman, 2010), where no theory provides a clear answer, so annotation schemes commit to more or less arbitrary decisions. For example, in parsing auxiliary verbs may head main verbs, or vice versa, and in part-of-speech (POS) tagging, possessive pronouns may belong to the category of determiners or the category of pronouns. This position paper argues that annotation projects should embrace these hard cases rather than pretend they can be unambiguously resolved. Instead of using overly specific annotation guidelines, designed to minimize inter-annotator disagreement (Duffield et al., 2007), and adjudicating between annotators of different opinions, we should embrace systematic inter-annotator disagreements. To motivate this, we present an empirical analysis showing 1. that certain inter-annotator disagreements are systematic, and 2. that actual errors are in fact so infrequent as to be negligible, even when linguists annotate without guidelines. The empirical analysis presented below relies on text corpora annotated with syntactic categories or parts-of-speech (POS). POS is part of most linguistic theories, but nevertheless, there are still many linguistic constructions – even </context>
</contexts>
<marker>Duffield, Hwang, Brown, Dligach, Vieweg, Davis, Palmer, 2007</marker>
<rawString>Cecily Duffield, Jena Hwang, Susan Brown, Dmitriy Dligach, Sarah Vieweg, Jenny Davis, and Martha Palmer. 2007. Criteria for the manual grouping of verb senses. In LAW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Ashish Vaswani</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning whom to trust with MACE.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="8283" citStr="Hovy et al., 2013" startWordPosition="1270" endWordPosition="1273">ided by the authors) were allowed to submit annotations. In total, 177 individual annotators supplied answers. We paid annotators a reward of $0.05 for 10 items. The full data set contains 14,619 items and is described in further detail in Hovy et al. (2014). Annotators were satisfied with the task (4.5 on a scale from 1 to 5) and felt that instructions were clear (4.4/5), and the pay reasonable (4.1/5). The crowdsourced annotations aggregated using majority voting agree with the expert annotations in 79.54% of the cases. If we pre-filter the data via Wiktionary and use an itemresponse model (Hovy et al., 2013) rather than majority voting, the agreement rises to 80.58%. Figure 2 presents the Hinton diagram of the disagreements of lay people. Disagreements are very similar to the disagreements between expert annotators, especially on Twitter data (Figure 1b). 508 a) b) c) Figure 1: Hinton diagrams of inter-annotator disagreement on (a) excerpt from WSJ (Marcus et al., 1993), (b) random Twitter sample, and (c) pre-transcribed spoken language excerpts from talkbank.org One difference is that lay people do not confuse numerals very often, probably because they rely more on orthographic cues than on dist</context>
<context position="14126" citStr="Hovy et al., 2013" startWordPosition="2228" endWordPosition="2231">nalysis of these cases, we also show that disagreements are systematic. Our work also relates to work on automatically correcting expert annotations for inconsistencies (Dickinson and Meurers, 2003). This work is very different in spirit from our work, but shares an interest in reconsidering expert annotations, and we made use of their mining algorithm here. There has also been recent work on adjudicatL2-LR NN 510 ing noisy crowdsourced annotations (Dawid and Skene, 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012; Hovy et al., 2013). Again, their objective is orthogonal to ours, namely to collapse multiple annotations into a gold standard rather than embracing disagreements. Finally, Plank et al. (2014) use small samples of doubly-annotated POS data to estimate annotator reliability and show how those metrics can be implemented in the loss function when inducing POS taggers to reflect confidence we can put in annotations. They show that not biasing the theory towards a single annotator but using a cost-sensitive learning scheme makes POS taggers more robust and more applicable for downstream tasks. 6 Conclusion In this p</context>
</contexts>
<marker>Hovy, Berg-Kirkpatrick, Vaswani, Hovy, 2013</marker>
<rawString>Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with MACE. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Barbara Plank</author>
<author>Anders Søgaard</author>
</authors>
<title>Experiments with crowdsourced re-annotation of a POS tagging data set.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7923" citStr="Hovy et al. (2014)" startWordPosition="1209" endWordPosition="1212">so observed when comparing the linguistic intuitions of lay people. More specifically, we had lay annotators on the crowdsourcing platform Crowdflower re-annotate the training section of Gimpel et al. (2011). They collected five annotations per word. Only annotators that had answered correctly on 4 gold items (randomly chosen from a set of 20 gold items provided by the authors) were allowed to submit annotations. In total, 177 individual annotators supplied answers. We paid annotators a reward of $0.05 for 10 items. The full data set contains 14,619 items and is described in further detail in Hovy et al. (2014). Annotators were satisfied with the task (4.5 on a scale from 1 to 5) and felt that instructions were clear (4.4/5), and the pay reasonable (4.1/5). The crowdsourced annotations aggregated using majority voting agree with the expert annotations in 79.54% of the cases. If we pre-filter the data via Wiktionary and use an itemresponse model (Hovy et al., 2013) rather than majority voting, the agreement rises to 80.58%. Figure 2 presents the Hinton diagram of the disagreements of lay people. Disagreements are very similar to the disagreements between expert annotators, especially on Twitter data </context>
</contexts>
<marker>Hovy, Plank, Søgaard, 2014</marker>
<rawString>Dirk Hovy, Barbara Plank, and Anders Søgaard. 2014. Experiments with crowdsourced re-annotation of a POS tagging data set. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Juergens</author>
</authors>
<title>An analysis of ambiguity in word sense annotations.</title>
<date>2014</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="13129" citStr="Juergens (2014)" startWordPosition="2070" endWordPosition="2071">5-75) 76%(76-77) 71%(68-72) Table 1: Classification results Figure 5: Hard cases and errors in 2d-PCA the two-dimensional plot in Figure 5, obtained using PCA. The logistic regression decision boundary is plotted as a solid, black line. This is probably also why the nearest neighbor (NN) classifier does slightly better, but again, performance is rather low. While other features may reveal that the problem is in fact learnable, our initial experiments lead us to conclude that, given the low ratio of errors over truly hard cases, learning to detect errors is often not worthwhile. 5 Related work Juergens (2014) presents work on detecting linguistically hard cases in the context of word sense annotations, e.g., cases where expert annotators will disagree, as well as differentiating between underspecified, overspecified and metaphoric cases. This work is similar to ours in spirit, but considers a very different task. While we also quantify the proportion of hard cases and present an analysis of these cases, we also show that disagreements are systematic. Our work also relates to work on automatically correcting expert annotations for inconsistencies (Dickinson and Meurers, 2003). This work is very dif</context>
</contexts>
<marker>Juergens, 2014</marker>
<rawString>David Juergens. 2014. An analysis of ambiguity in word sense annotations. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Mary Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="8652" citStr="Marcus et al., 1993" startWordPosition="1330" endWordPosition="1333">(4.4/5), and the pay reasonable (4.1/5). The crowdsourced annotations aggregated using majority voting agree with the expert annotations in 79.54% of the cases. If we pre-filter the data via Wiktionary and use an itemresponse model (Hovy et al., 2013) rather than majority voting, the agreement rises to 80.58%. Figure 2 presents the Hinton diagram of the disagreements of lay people. Disagreements are very similar to the disagreements between expert annotators, especially on Twitter data (Figure 1b). 508 a) b) c) Figure 1: Hinton diagrams of inter-annotator disagreement on (a) excerpt from WSJ (Marcus et al., 1993), (b) random Twitter sample, and (c) pre-transcribed spoken language excerpts from talkbank.org One difference is that lay people do not confuse numerals very often, probably because they rely more on orthographic cues than on distributional evidence. The disagreements are still strongly correlated with the ones observed with expert annotators, but at a slightly lower coefficient (with a Spearman’s ρ of 0.493 and Kendall’s T of 0.366 for WSJ). Figure 2: Disagreement between lay annotators Lastly, we compare the disagreements of annotators on a French social media data set (Seddah et al., 2012)</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell Marcus, Mary Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="4802" citStr="Petrov et al. (2012)" startWordPosition="721" endWordPosition="724">rors. 2 Annotator disagreements across domains and languages In this study, we had between 2-10 individual annotators with degrees in linguistics annotate different kinds of English text with POS tags, e.g., newswire text (PTB WSJ Section 00), transcripts of spoken language (from a database containing transcripts of conversations, Talkbank1), as well as Twitter posts. Annotators were specifically not presented with guidelines that would help them resolve hard cases. Moreover, we compare professional annotation to that of lay people. We instructed annotators to use the 12 universal POS tags of Petrov et al. (2012). We did so in order to make comparison between existing data sets possible. Moreover, this allows us to focus on really hard cases, as any debatable case in the coarse-grained tag set is necessarily also part of the finer-grained tag set.2 For each domain, we collected exactly 500 doubly-annotated sentences/tweets. Besides these English data sets, we also obtained doubly-annotated POS data from the French Social Media Bank project (Seddah et al., 2012).3 All data sets, except the French one, are publicly available at http://lowlands.ku. dk/. We present disagreements as Hinton diagrams in Figu</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Dirk Hovy</author>
<author>Anders Søgaard</author>
</authors>
<title>Learning part-of-speech taggers with inter-annotator agreement loss.</title>
<date>2014</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="14300" citStr="Plank et al. (2014)" startWordPosition="2254" endWordPosition="2257">kinson and Meurers, 2003). This work is very different in spirit from our work, but shares an interest in reconsidering expert annotations, and we made use of their mining algorithm here. There has also been recent work on adjudicatL2-LR NN 510 ing noisy crowdsourced annotations (Dawid and Skene, 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012; Hovy et al., 2013). Again, their objective is orthogonal to ours, namely to collapse multiple annotations into a gold standard rather than embracing disagreements. Finally, Plank et al. (2014) use small samples of doubly-annotated POS data to estimate annotator reliability and show how those metrics can be implemented in the loss function when inducing POS taggers to reflect confidence we can put in annotations. They show that not biasing the theory towards a single annotator but using a cost-sensitive learning scheme makes POS taggers more robust and more applicable for downstream tasks. 6 Conclusion In this paper, we show that disagreements between professional or lay annotators are systematic and consistent across domains and some of them are systematic also across languages. In</context>
</contexts>
<marker>Plank, Hovy, Søgaard, 2014</marker>
<rawString>Barbara Plank, Dirk Hovy, and Anders Søgaard. 2014. Learning part-of-speech taggers with inter-annotator agreement loss. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas C Raykar</author>
<author>Shipeng Yu</author>
</authors>
<title>Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>13--491</pages>
<contexts>
<context position="14106" citStr="Raykar and Yu, 2012" startWordPosition="2224" endWordPosition="2227">ases and present an analysis of these cases, we also show that disagreements are systematic. Our work also relates to work on automatically correcting expert annotations for inconsistencies (Dickinson and Meurers, 2003). This work is very different in spirit from our work, but shares an interest in reconsidering expert annotations, and we made use of their mining algorithm here. There has also been recent work on adjudicatL2-LR NN 510 ing noisy crowdsourced annotations (Dawid and Skene, 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012; Hovy et al., 2013). Again, their objective is orthogonal to ours, namely to collapse multiple annotations into a gold standard rather than embracing disagreements. Finally, Plank et al. (2014) use small samples of doubly-annotated POS data to estimate annotator reliability and show how those metrics can be implemented in the loss function when inducing POS taggers to reflect confidence we can put in annotations. They show that not biasing the theory towards a single annotator but using a cost-sensitive learning scheme makes POS taggers more robust and more applicable for downstream tasks. 6 </context>
</contexts>
<marker>Raykar, Yu, 2012</marker>
<rawString>Vikas C. Raykar and Shipeng Yu. 2012. Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks. Journal of Machine Learning Research, 13:491–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djam´e Seddah</author>
<author>Benoit Sagot</author>
<author>Marie Candito</author>
<author>Virginie Mouilleron</author>
<author>Vanessa Combet</author>
</authors>
<title>The French Social Media Bank: a treebank of noisy user generated content.</title>
<date>2012</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="5259" citStr="Seddah et al., 2012" startWordPosition="796" endWordPosition="799">e hard cases. Moreover, we compare professional annotation to that of lay people. We instructed annotators to use the 12 universal POS tags of Petrov et al. (2012). We did so in order to make comparison between existing data sets possible. Moreover, this allows us to focus on really hard cases, as any debatable case in the coarse-grained tag set is necessarily also part of the finer-grained tag set.2 For each domain, we collected exactly 500 doubly-annotated sentences/tweets. Besides these English data sets, we also obtained doubly-annotated POS data from the French Social Media Bank project (Seddah et al., 2012).3 All data sets, except the French one, are publicly available at http://lowlands.ku. dk/. We present disagreements as Hinton diagrams in Figure 1a–c. Note that the spoken language data does not include punctuation. The correlations between the disagreements are highly significant, with Spearman coefficients ranging from 0.644 1http://talkbank.org/ 2Experiments with variation n-grams on WSJ (Dickinson and Meurers, 2003) and the French data lead us to estimate that the fine-to-coarse mapping of POS tags disregards about 20% of observed tag-pair confusion types, most of which relate to fine-gra</context>
<context position="9252" citStr="Seddah et al., 2012" startWordPosition="1425" endWordPosition="1428">Marcus et al., 1993), (b) random Twitter sample, and (c) pre-transcribed spoken language excerpts from talkbank.org One difference is that lay people do not confuse numerals very often, probably because they rely more on orthographic cues than on distributional evidence. The disagreements are still strongly correlated with the ones observed with expert annotators, but at a slightly lower coefficient (with a Spearman’s ρ of 0.493 and Kendall’s T of 0.366 for WSJ). Figure 2: Disagreement between lay annotators Lastly, we compare the disagreements of annotators on a French social media data set (Seddah et al., 2012), which we mapped to the universal POS tag set. Again, we see the familiar dagger shape. The Spearman coefficient with English Twitter is 0.288; Kendall’s T is 0.204. While the correlation is weaker across languages than across domains, it remains statistically significant (p &lt; 0.001). 3 Hard cases and annotation errors In the previous section, we demonstrated that some disagreements are consistent across domains and languages. We noted earlier, though, that disagreements can arise both from hard cases and from annotation errors. This can explain some Figure 3: Disagreement on French social me</context>
</contexts>
<marker>Seddah, Sagot, Candito, Mouilleron, Combet, 2012</marker>
<rawString>Djam´e Seddah, Benoit Sagot, Marie Candito, Virginie Mouilleron, and Vanessa Combet. 2012. The French Social Media Bank: a treebank of noisy user generated content. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Padhraic Smyth</author>
<author>Usama Fayyad</author>
<author>Mike Burl</author>
<author>Pietro Perona</author>
<author>Pierre Baldi</author>
</authors>
<title>Inferring ground truth from subjective labelling of Venus images.</title>
<date>1995</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="14003" citStr="Smyth et al., 1995" startWordPosition="2206" endWordPosition="2209">o ours in spirit, but considers a very different task. While we also quantify the proportion of hard cases and present an analysis of these cases, we also show that disagreements are systematic. Our work also relates to work on automatically correcting expert annotations for inconsistencies (Dickinson and Meurers, 2003). This work is very different in spirit from our work, but shares an interest in reconsidering expert annotations, and we made use of their mining algorithm here. There has also been recent work on adjudicatL2-LR NN 510 ing noisy crowdsourced annotations (Dawid and Skene, 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012; Hovy et al., 2013). Again, their objective is orthogonal to ours, namely to collapse multiple annotations into a gold standard rather than embracing disagreements. Finally, Plank et al. (2014) use small samples of doubly-annotated POS data to estimate annotator reliability and show how those metrics can be implemented in the loss function when inducing POS taggers to reflect confidence we can put in annotations. They show that not biasing the theory towards a single annotator but using a co</context>
</contexts>
<marker>Smyth, Fayyad, Burl, Perona, Baldi, 1995</marker>
<rawString>Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Perona, and Pierre Baldi. 1995. Inferring ground truth from subjective labelling of Venus images. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Welinder</author>
<author>Steve Branson</author>
<author>Serge Belongie</author>
<author>Pietro Perona</author>
</authors>
<title>The multidimensional wisdom of crowds.</title>
<date>2010</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="14067" citStr="Welinder et al., 2010" startWordPosition="2216" endWordPosition="2219">we also quantify the proportion of hard cases and present an analysis of these cases, we also show that disagreements are systematic. Our work also relates to work on automatically correcting expert annotations for inconsistencies (Dickinson and Meurers, 2003). This work is very different in spirit from our work, but shares an interest in reconsidering expert annotations, and we made use of their mining algorithm here. There has also been recent work on adjudicatL2-LR NN 510 ing noisy crowdsourced annotations (Dawid and Skene, 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012; Hovy et al., 2013). Again, their objective is orthogonal to ours, namely to collapse multiple annotations into a gold standard rather than embracing disagreements. Finally, Plank et al. (2014) use small samples of doubly-annotated POS data to estimate annotator reliability and show how those metrics can be implemented in the loss function when inducing POS taggers to reflect confidence we can put in annotations. They show that not biasing the theory towards a single annotator but using a cost-sensitive learning scheme makes POS taggers more robust and m</context>
</contexts>
<marker>Welinder, Branson, Belongie, Perona, 2010</marker>
<rawString>Peter Welinder, Steve Branson, Serge Belongie, and Pietro Perona. 2010. The multidimensional wisdom of crowds. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Whitehill</author>
<author>Paul Ruvolo</author>
<author>Tingfan Wu</author>
<author>Jacob Bergsma</author>
<author>Javier Movellan</author>
</authors>
<title>Whose vote should count more: Optimal integration of labels from labelers of unknown expertise.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="14044" citStr="Whitehill et al., 2009" startWordPosition="2212" endWordPosition="2215">y different task. While we also quantify the proportion of hard cases and present an analysis of these cases, we also show that disagreements are systematic. Our work also relates to work on automatically correcting expert annotations for inconsistencies (Dickinson and Meurers, 2003). This work is very different in spirit from our work, but shares an interest in reconsidering expert annotations, and we made use of their mining algorithm here. There has also been recent work on adjudicatL2-LR NN 510 ing noisy crowdsourced annotations (Dawid and Skene, 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012; Hovy et al., 2013). Again, their objective is orthogonal to ours, namely to collapse multiple annotations into a gold standard rather than embracing disagreements. Finally, Plank et al. (2014) use small samples of doubly-annotated POS data to estimate annotator reliability and show how those metrics can be implemented in the loss function when inducing POS taggers to reflect confidence we can put in annotations. They show that not biasing the theory towards a single annotator but using a cost-sensitive learning scheme makes POS ta</context>
</contexts>
<marker>Whitehill, Ruvolo, Wu, Bergsma, Movellan, 2009</marker>
<rawString>Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob Bergsma, and Javier Movellan. 2009. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Yan</author>
<author>R´omer Rosales</author>
<author>Glenn Fung</author>
<author>Mark Schmidt</author>
<author>Gerardo Hermosillo</author>
<author>Luca Bogoni</author>
<author>Linda Moy</author>
<author>Jennifer Dy</author>
</authors>
<title>Modeling annotator expertise: Learning when everybody knows a bit of something. In AIStats.</title>
<date>2010</date>
<contexts>
<context position="14085" citStr="Yan et al., 2010" startWordPosition="2220" endWordPosition="2223">oportion of hard cases and present an analysis of these cases, we also show that disagreements are systematic. Our work also relates to work on automatically correcting expert annotations for inconsistencies (Dickinson and Meurers, 2003). This work is very different in spirit from our work, but shares an interest in reconsidering expert annotations, and we made use of their mining algorithm here. There has also been recent work on adjudicatL2-LR NN 510 ing noisy crowdsourced annotations (Dawid and Skene, 1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012; Hovy et al., 2013). Again, their objective is orthogonal to ours, namely to collapse multiple annotations into a gold standard rather than embracing disagreements. Finally, Plank et al. (2014) use small samples of doubly-annotated POS data to estimate annotator reliability and show how those metrics can be implemented in the loss function when inducing POS taggers to reflect confidence we can put in annotations. They show that not biasing the theory towards a single annotator but using a cost-sensitive learning scheme makes POS taggers more robust and more applicable for</context>
</contexts>
<marker>Yan, Rosales, Fung, Schmidt, Hermosillo, Bogoni, Moy, Dy, 2010</marker>
<rawString>Yan Yan, R´omer Rosales, Glenn Fung, Mark Schmidt, Gerardo Hermosillo, Luca Bogoni, Linda Moy, and Jennifer Dy. 2010. Modeling annotator expertise: Learning when everybody knows a bit of something. In AIStats.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
</authors>
<title>Hard problems of tagset conversion.</title>
<date>2010</date>
<booktitle>In Proceedings of the Second International Conference on Global Interoperability for Language Resources.</booktitle>
<contexts>
<context position="1345" citStr="Zeman, 2010" startWordPosition="194" endWordPosition="195">rors. Moreover, a quantitative analysis of tag confusions reveals that the majority of disagreements are due to linguistically debatable cases rather than annotation errors. Specifically, we show that even in the absence of annotation guidelines only 2% of annotator choices are linguistically unmotivated. 1 Introduction In NLP, we often model annotation as if it reflected a single ground truth that was guided by an underlying linguistic theory. If this was true, the specific theory should be learnable from the annotated data. However, it is well known that there are linguistically hard cases (Zeman, 2010), where no theory provides a clear answer, so annotation schemes commit to more or less arbitrary decisions. For example, in parsing auxiliary verbs may head main verbs, or vice versa, and in part-of-speech (POS) tagging, possessive pronouns may belong to the category of determiners or the category of pronouns. This position paper argues that annotation projects should embrace these hard cases rather than pretend they can be unambiguously resolved. Instead of using overly specific annotation guidelines, designed to minimize inter-annotator disagreement (Duffield et al., 2007), and adjudicating</context>
</contexts>
<marker>Zeman, 2010</marker>
<rawString>Daniel Zeman. 2010. Hard problems of tagset conversion. In Proceedings of the Second International Conference on Global Interoperability for Language Resources.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>