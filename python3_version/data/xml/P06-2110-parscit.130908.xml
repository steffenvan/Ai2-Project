<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000159">
<title confidence="0.942331">
Word Vectors and Two Kinds of Similarity
</title>
<author confidence="0.979109">
Akira Utsumi and Daisuke Suzuki
</author>
<affiliation confidence="0.995905">
Department of Systems Engineering
The University of Electro-Communications
</affiliation>
<address confidence="0.865511">
1-5-1 Chofugaoka, Chofushi, Tokyo 182-8585, Japan
</address>
<email confidence="0.991397">
utsumi@se.uec.ac.jp, dajie@utm.se.uec.ac.jp
</email>
<sectionHeader confidence="0.982772" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999931933333334">
This paper examines what kind of similar-
ity between words can be represented by
what kind of word vectors in the vector
space model. Through two experiments,
three methods for constructing word vec-
tors, i.e., LSA-based, cooccurrence-based
and dictionary-based methods, were com-
pared in terms of the ability to represent
two kinds of similarity, i.e., taxonomic
similarity and associative similarity. The
result of the comparison was that the
dictionary-based word vectors better re-
flect taxonomic similarity, while the LSA-
based and the cooccurrence-based word
vectors better reflect associative similarity.
</bodyText>
<sectionHeader confidence="0.995127" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895222222222">
Recently, geometric models have been used to rep-
resent words and their meanings, and proven to
be highly useful both for many NLP applications
associated with semantic processing (Widdows,
2004) and for human modeling in cognitive sci-
ence (G¨ardenfors, 2000; Landauer and Dumais,
1997). There are also good reasons for studying
geometric models in the field of computational lin-
guistics. First, geometric models are cost-effective
in that it takes much less time and less effort to
construct large-scale geometric representation of
word meanings than it would take to construct dic-
tionaries or thesauri. Second, they can represent
the implicit knowledge of word meanings that dic-
tionaries and thesauri cannot do. Finally, geomet-
ric representation is easy to revise and extend.
A vector space model is the most commonly
used geometric model for the meanings of words.
The basic idea of a vector space model is that
words are represented by high-dimensional vec-
tors, i.e., word vectors, and the degree of seman-
tic similarity between any two words can be easily
computed as a cosine of the angle formed by their
vectors.
A number of methods have been proposed for
constructing word vectors. Latent semantic anal-
ysis (LSA) is the most well-known method that
uses the frequency of words in a fraction of doc-
uments to assess the coordinates of word vectors
and singular value decomposition (SVD) to reduce
the dimension. LSA was originally put forward as
a document indexing technique for automatic in-
formation retrieval (Deerwester et al., 1990), but
several studies (Landauer and Dumais, 1997) have
shown that LSA successfully mimics many hu-
man behaviors associated with semantic process-
ing. Other methods use a variety of other informa-
tion: cooccurrence of two words (Burgess, 1998;
Sch¨utze, 1998), occurrence of a word in the sense
definitions of a dictionary (Kasahara et al., 1997;
Niwa and Nitta, 1994) or word association norms
(Steyvers et al., 2004).
However, despite the fact that there are differ-
ent kinds of similarity between words, or differ-
ent relations underlying word similarity such as a
synonymous relation and an associative relation,
no studies have ever examined the relationship be-
tween methods for constructing word vectors and
the type of similarity involved in word vectors in
a systematic way. Some studies on word vec-
tors have compared the performance among dif-
ferent methods on some specific tasks such as se-
mantic disambiguation (Niwa and Nitta, 1994) and
cued/free recall (Steyvers et al., 2004), but it is not
at all clear whether there are essential differences
in the quality of similarity among word vectors
constructed by different methods, and if so, what
kind of similarity is involved in what kind of word
vectors. Even in the field of cognitive psychol-
ogy, although geometric models of similarity such
as multidimensional scaling have long been stud-
ied and debated (Nosofsky, 1992), the possibility
that different methods for word vectors may cap-
</bodyText>
<page confidence="0.443907">
858
</page>
<note confidence="0.911779">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 858–865,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999976285714286">
ture different kinds of word similarity has never
been addressed.
This study, therefore, aims to examine the re-
lationship between the methods for constructing
word vectors and the type of similarity in a sys-
tematic way. Especially this study addresses three
methods, LSA-based, cooccurrence-based, and
dictionary-based methods, and two kinds of sim-
ilarity, taxonomic similarity and associative sim-
ilarity. Word vectors constructed by these meth-
ods are compared in the performance of two tasks,
i.e., multiple-choice synonym test and word asso-
ciation, which measure the degree to which they
reflect these two kinds of similarity.
</bodyText>
<sectionHeader confidence="0.917508" genericHeader="method">
2 Two Kinds of Similarity
</sectionHeader>
<bodyText confidence="0.999985057142857">
In this study, we divide word similarity into two
categories: taxonomic similarity and associative
similarity. Taxonomic similarity, or categorical
similarity, is a kind of semantic similarity between
words in the same level of categories or clusters of
the thesaurus, in particular synonyms, antonyms,
and other coordinates. Associative similarity, on
the other hand, is a similarity between words that
are associated with each other by virtue of seman-
tic relations other than taxonomic one such as a
collocational relation and a proximity relation. For
example, the word writer and the word author are
taxonomically similar because they are synonyms,
while the word writer and the word book are as-
sociatively similar because they are associated by
virtue of an agent-subject relation.
This dichotomy of similarity is practically im-
portant. Some tasks such as automatic thesaurus
updating and paraphrasing need assessing taxo-
nomic similarity, while some other tasks such as
affective Web search and semantic disambiguation
require assessing associative similarity rather than
taxonomic similarity. This dichotomy is also psy-
chologically motivated. Many empirical studies
on word searches and speech disorders have re-
vealed that words in the mind (i.e., mental lex-
icon) are organized by these two kinds of simi-
larity (Aitchison, 2003). This dichotomy is also
essential to some cognitive processes. For ex-
ample, metaphors are perceived as being more
apt when their constituent words are associatively
more similar but categorically dissimilar (Utsumi
et al., 1998). These psychological findings suggest
that people distinguish between these two kinds of
similarity in certain cognitive processes.
</bodyText>
<sectionHeader confidence="0.912174" genericHeader="method">
3 Constructing Word Vectors
</sectionHeader>
<subsectionHeader confidence="0.989999">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.99995975">
In this study, word vectors (or word spaces) are
constructed in the following way. First, all con-
tent words ti in a corpus are represented as m-
dimensional feature vectors wi.
</bodyText>
<equation confidence="0.988447">
wi = (wi1i wi2i ··· i wim) (1)
</equation>
<bodyText confidence="0.99827825">
Each element wij is determined by statistical anal-
ysis of the corpus, whose methods will be de-
scribed in Section 3.3. A matrix M is then con-
structed using n feature vectors as rows.
</bodyText>
<equation confidence="0.777941">
M= �
w1
... (2)
wn
</equation>
<bodyText confidence="0.99838325">
Finally, the dimension of row vectors wi is re-
duced from m to k by means of a SVD tech-
nique. As a result, any words are represented as
k-dimensional vectors.
</bodyText>
<subsectionHeader confidence="0.999104">
3.2 Corpus
</subsectionHeader>
<bodyText confidence="0.999972875">
In this study, we employ three kinds of Japanese
corpora: newspaper articles, novels and a dictio-
nary. As a newspaper corpus, we use 4 months’
worth of Mainichi newspaper articles published
in 1999. They consist of 500,182 sentences in
251,287 paragraphs, and words vectors are con-
structed for 53,512 words that occur three times
or more in these articles. Concerning a corpus of
novels, we use a collection of 100 Japanese nov-
els “Shincho Bunko No 100 Satsu” consisting of
475,782 sentences and 230,392 paragraphs. Word
vectors are constructed for 46,666 words that oc-
cur at least three times. As a Japanese dictionary,
we use “Super Nihongo Daijiten” published by
Gakken, from which 89,007 words are extracted
for word vectors.
</bodyText>
<subsectionHeader confidence="0.9686575">
3.3 Methods for Computing Vector Elements
LSA-based method (LSA)
</subsectionHeader>
<bodyText confidence="0.999931666666667">
In the LSA-based method, a vector element wij
is assessed as a tf-idf score of a word ti in a piece
sj of document.
</bodyText>
<equation confidence="0.851276">
� wij = tfij x log mdf + 1 (3)
i
</equation>
<bodyText confidence="0.9998005">
In this formula, tfij denotes the number of times
the word ti occurs in a piece of text sj, and dfi
denotes the number of pieces in which the word
ti occurs. As a unit of text piece sj, we consider
</bodyText>
<page confidence="0.890166">
859
</page>
<bodyText confidence="0.999949">
a sentence and a paragraph. Hence, for example,
when a sentence is used as a unit, the dimension of
feature vectors wi is equal to the number of sen-
tences in a corpus. We also use two corpora, i.e.,
newspapers and novels, and thus we obtain four
different word spaces by the LSA-based method.
</bodyText>
<subsectionHeader confidence="0.80318">
Cooccurrence-based method (COO)
</subsectionHeader>
<bodyText confidence="0.976520761904762">
In the cooccurrence-based method, a vector ele-
ment wij is assessed as the number of times words
ti and tj occur in the same piece of text, and thus
M is an n x n symmetric matrix. As in the case
of the LSA-based method, we use two units of text
piece (i.e., a sentence or a paragraph) and two cor-
pora (i.e., newspapers or novels), thus resulting in
four different word spaces.
Note that this method is similar to Sch¨utze’s
(1998) method for constructing a semantic space
in that both are based on the word cooccurrence,
not on the word frequency. However they are dif-
ferent in that Sch¨utze’s method uses the cooccur-
rence with frequent content words chosen as in-
dicators of primitive meanings. Burgess’s (1998)
“Hyperspace Analogue to Language (HAL)” is
also based on the word cooccurrence but does not
use any technique of dimensionality reduction.
Dictionary-based method (DIC)
In the dictionary-based method, a vector ele-
ment wij is assessed by the following formula:
</bodyText>
<equation confidence="0.99231475">
� �
��
wij = �fij + α fikfkj + Qfji x log dfj
k
</equation>
<bodyText confidence="0.980135272727273">
(4)
where fij denotes the number of times the word
tj occurs in the sense definitions of the word ti,
and dfj denotes the number of words whose sense
definitions contain the word tj. The second term
in parentheses in Equation (4) means the square
root of the number of times the word tj occurs in
a collection of sense definitions for any words that
are included in the sense definitions of the word ti,
while the third term means the number of times ti
occurs in the sense definitions of tj. The param-
eters α and Q are positive real constants express-
ing the weights for these information. (Following
Kasahara et al. (1997), these parameters are set to
0.2 in this paper.)
Equation (4) was originally put forward by
Kasahara et al. (1997), but our dictionary-based
method differs from their method in terms of how
the dimensions are reduced. Their method groups
together the dimensions for words in the same cat-
egory of a thesaurus, but our method uses SVD as
we will described next.
</bodyText>
<subsectionHeader confidence="0.994359">
3.4 Reducing Dimensions
</subsectionHeader>
<bodyText confidence="0.999974111111111">
Using a SVD technique, a matrix M is factorized
as the product of three matrices UEVT, where
the diagonal matrix E consists of r singular val-
ues that are arranged in nonincreasing order such
that r is the rank of M. When we use a k x k ma-
trix Ek consisting of the largest k singular values,
the matrix M is approximated by UkEkVkT , where
the i-th row of Uk corresponds to a k-dimensional
“reduced word vector” for the word ti.
</bodyText>
<sectionHeader confidence="0.958865" genericHeader="method">
4 Experiment 1: Synonym Judgment
</sectionHeader>
<subsectionHeader confidence="0.996439">
4.1 Method
</subsectionHeader>
<bodyText confidence="0.9999962">
In order to compare different word vectors in
terms of the ability to judge taxonomic similar-
ity between words, we conducted a synonym judg-
ment experiment using a standard multiple-choice
synonym test. Each item of a synonym test con-
sisted of a stem word and five alternative words
from which the test-taker was asked to choose one
with the most similar meaning to the stem word.
In the experiment, we used 32 items from the
synonym portions of Synthetic Personality In-
ventory (SPI) test, which has been widely used
for employment selection in Japanese companies.
These items were selected so that all the vector
spaces could contain the stem word and at least
four of the five alternative words. For comparison
purpose, we also used 38 antonym test items cho-
sen from the same SPI test. Furthermore, in order
to obtain a more reliable, unbiased result, we auto-
matically constructed 200 test items in such a way
that we chose the stem word randomly, one correct
alternative word randomly from words in the same
deepest category of a Japanese thesaurus as the
stem word, and other four alternatives from words
in other categories. As a Japanese thesaurus, we
used “Goi-Taikei” (Ikehara et al., 1999).
In the computer simulation, the computer’s
choices were determined by computing cosine
similarity between the stem word and each of the
five alternative words using the vector spaces and
choosing the word with the highest similarity.
</bodyText>
<subsectionHeader confidence="0.856752">
4.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.998357">
For each of the nine vector spaces, the synonym
judgment simulation described above was con-
</bodyText>
<figure confidence="0.990244732142857">
n
860
Correct Rate
0.9
0.8
Correct Rate
0.7
0.6
0.5
0.4
0.3
0.2
0 100 200 300 400 500 600 700 800 900 1000
Number of Dimensions
(a) SPI test items
0 100 200 300 400 500 600 700 800 900 1000
Number of Dimensions
0.2
0.8
0.7
0.6
0.5
0.4
0.3
+
LSA
+ + COO
DIC
+
+
++++
�
+ + + �
�
+
LSA (synonym)
LSA (antonym)
DIC (synonym)
DIC (an
tonym)
✠
++++++++++++++++++++
✠ ✠ ✠ ✠
✠ ✠ ✠ ✠ ✠
Figure 2: Synonym versus an
tonym judgment
0.7
Correct Rate
0.6
0.5
0.4
0.3
0.2
0 100 200 300 400 500 600 700 800 900 1000
Number of Dimensions
(b) Computer-generated test items
</figure>
<figureCaption confidence="0.997446">
Figure 1: Correct rates of synonym tests
</figureCaption>
<bodyText confidence="0.987783383561644">
dictionary
-based method (all ps &lt; .0001), while
correct rates for computer-generated test items
ducted and the percentage of correct choices was
calculated. This process was repeated using 20
numbers of dimensions, i.e., every 50 dimensions
between 50 and 1000.
Figure 1 shows the percentage of correct
choices for the three methods of matrix construc-
tion. Concerning the LSA-based method (denoted
by LSA) and the cooccurrence-based method (de-
noted by COO), Figure 1 plots the correct rates for
the word vectors derived from the paragraphs of
the newspaper corpus. (Such combination of cor-
pus and text unit was optimal among all combi-
nations, which will be justified later in this sec-
tion.) The most important result shown in Figure 1
is that, regardless of the number of dimensions, the
dictionary-based word vectors outperformed the
other kinds of vectors on both SPI and computer-
generated test items. This result thus suggests
that the dictionary-based vector space reflects tax-
onomic similarity between words better than the
LSA-based and the correlation-based spaces.
Another interesting finding is that there was no
clear peak in the graphs of Figure 1. For SPI test
items, correct rates of the three methods increased
linearly as the number of dimensions increased,
r = .86 for the LSA-based method, r = .72 for
the correlation-based method and r = .93 for the
were steady. Our finding of the absence of any
obvious optimal dimensions is in a sharp contrast
to Landauer and
(1997) finding that the
LSA word vectors with 300 dimensions achieved
the maximum performance of 53% correct rate
in a similar multiple-choice synonym test. Note
that their maximum performance was a little bet-
ter than that of our LSA vectors, but still worse
than that of our dictionary-based vectors.
Figure 2 shows the performance of the LSA-
based and the dictionary-based methods in anto-
nym judgment, together with the result of syn-
onym judgment. (Since the performance of the
cooccurrence-based method did not differ from
that of the LSA-based method, the correct rates
of the cooccurrence-based method are not plotted
in this figure.) The dictionary-based method also
outperformed the LSA-based method in antonym
judgment but their difference was much smaller
than that of synonym judgment; at 200 or lower
dimensions LSA-based method was better than
the dictionary-based method. Interestingly, the
dictionary-based word vectors yielded better per-
formance in synonym judgment than in antonym
judgment, while the LSA-based vectors showed
better performance in antonym judgment. These
contrasting results may be attributed to the differ-
ence of corpus characteristics.
defi-
nitions for antonymous words are likely to involve
different words so that the differences between
their meanings can be made clear. On the other
hand, in newspaper articles (or literary texts), con-
text words with which antonymous words occur
are likely to overlap because their meanings are
about the same domain.
Finally, we show the results of comparison
among four combinations of corpora and text units
for the LSA-based an
Dumais’s
Dictionary’s
d the cooccurrence-based
</bodyText>
<page confidence="0.829187">
861
</page>
<tableCaption confidence="0.991312">
Table 1: Comparison of mean correct rate among
the combinations of two corpora and two text units
</tableCaption>
<table confidence="0.958112444444444">
Newspaper Novel
Method Para Sent Para Sent
SPI test 0.383 0.366 0.238 0.369
LSA
COO 0.413 0.369 0.255 0.280
Computer-generated test
LSA 0.410 0.377 0.346 0.379
COO 0.375 0.363 0.311 0.310
Note. Para = Paragraph; Sent= Sentence.
</table>
<bodyText confidence="0.9996443">
methods. Table 1 lists mean correct rates of SPI
test and computer-generated test averaged over all
the numbers of dimensions. Regardless of con-
struction methods and test items, the word vectors
constructed using newspaper paragraphs achieved
the best performance, which are denoted by bold-
faces. Concerning an effect of corpus difference,
the newspaper corpus was superior to the literary
corpus. The difference of text units did not have a
clear influence on the performance of word spaces.
</bodyText>
<sectionHeader confidence="0.976434" genericHeader="method">
5 Experiment 2: Word Association
</sectionHeader>
<subsectionHeader confidence="0.988325">
5.1 Method
</subsectionHeader>
<bodyText confidence="0.999873956521739">
In order to compare the ability of the word
spaces to judge associative similarity, we con-
ducted a word association experiment using a
Japanese word association norm “Renso Kijun-
hyo” (Umemoto, 1969). This free-association
database was developed based on the responses of
1,000 students to 210 stimulus words. For exam-
ple, when given the word writer as a stimulus, stu-
dents listed the words shown in Table 2. (Table 2
also shows the original words in Japanese.)
For the simulation experiment, we selected 176
stimulus words that all the three corpora con-
tained. These stimuli had 27 associate words on
average. We then removed any associate words
that were synonymous with the stimulus word
(e.g., author in Table 2), since the purpose of this
experiment was to examine the ability to assess
associative similarity between words. Whether or
not each associate is synonymous with the stimu-
lus was determined according to whether they be-
long to the same deepest category of a Japanese
thesaurus “Goi-Taikei” (Ikehara et al., 1999).
In the computer simulation, cosine similarity
</bodyText>
<tableCaption confidence="0.848169">
Table 2: Associates for the stimulus word writer
</tableCaption>
<bodyText confidence="0.990878634146341">
Stimulus: writer
Associates:
novel pen literary work painter
book author best-seller money
write literature play art work
popular human book paper pencil
lucrative writing mystery music
between the stimulus word and each of all the
other words included in the vector space was com-
puted, and all the words were sorted in descending
order of similarity. The top i words were then cho-
sen as associates.
The ability of word spaces to mimic human
word association was evaluated on mean preci-
sion. Precision is the ratio of the number of
human-produced associates chosen by computer
to the number i of computer-chosen associates. A
precision score was calculated every time a new
human-produced associate was found in the top i
words when i was incremented by 1, and after that
mean precision was calculated as the average of all
these precision scores. It must be noted here that,
in order to eliminate the bias caused by the dif-
ference in the number of contained words among
word spaces, we conducted the simulation using
46,000 words that we randomly chose for each
corpus so that they could include all the human-
produced associates.
Although this computational method of produc-
ing associates is sufficient for the present purpose,
it may be inadequate to model the psychological
process of free association. Some empirical stud-
ies of word association (Nelson et al., 1998) re-
vealed that frequent or familiar words were highly
likely to be produced as associates, but our meth-
ods for constructing word vectors may not directly
address such frequency effect on word association.
Hence, we conducted an additional experiment in
which only familiar words were used for comput-
ing similarity to a given stimulus word, i.e., less
familiar words were not used as candidates of as-
</bodyText>
<figure confidence="0.8031995">
862
Number of Dimensions
</figure>
<figureCaption confidence="0.789005">
Figure 3: Mean precision of word association
judgment
</figureCaption>
<figure confidence="0.9961065">
0 100 200 300 400 500 600 700 800 9001000
Number of Dimensions
</figure>
<figureCaption confidence="0.869826">
Figure 4: Average precision of word association
judgment for familiar words
</figureCaption>
<equation confidence="0.9769825">
× × ×
× × ×
× × ×
×
×
+
× × × × ×× ××
++++++++++++++++++
✂ ✂ ✂ ✂ ✂ ✂
� � � � �
+
0 100 200 300 400 500 600 700 800 9001000
LSA (Ave) LSA (Max)
+ + COO (Ave) × × COO (Max)
DIC
× × × ×× × × × × × × ×
✎✏
× × × × × ×
✌ ✌ ✌ ✌
☛☞ ☛☞ ☛☞ ☛☞
</equation>
<table confidence="0.90274925">
+++++++++++++++++
☛☞
+
×
×
+
+
LSA (Ave) LSA (Max)
+ + COO (Ave) × × COO (Max)
DIC
Mean Precision 0.030
0.025
0.020
0.015
0.010
0.005
0
Mean Precision 0.040
0.035
0.030
0.025
0.020
0.015
0.010
</table>
<bodyText confidence="0.999127833333333">
sociates. For a measure of word familiarity, we
used word familiarity scores (ranging from 1 to 7)
provided by “Nihongo no Goitaikei” (Amano and
Kondo, 2003). Using this data, we selected the
words whose familiarity score is 5 or higher as fa-
miliar ones.
</bodyText>
<subsectionHeader confidence="0.980607">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999796984375">
For each of the nine vector spaces, the association
judgment simulation was conducted and the mean
precision was calculated. As in the synonym judg-
ment experiment, this process was repeated by ev-
ery 50 dimensions between 50 and 1000.
Figure 3 shows the result of word association
experiment. For the LSA-based and the cooccur-
rence-based methods, two kinds of mean precision
were plotted: the average of mean precision scores
for the four word vectors and the maximum score
among them. (As we will show in Table 3, the
LSA-based method achieved the maximum preci-
sion when sentences of the newspaper corpus were
used, while the performance of the cooccurrence-
based method was maximal when paragraphs of
the newspaper corpus were used.) The overall
result was that the dictionary-based word vectors
yielded the worst performance, as opposed to the
result of synonym judgment. There was no big
difference in performance between the LSA-based
method and the cooccurrence-based method, but
the maximal cooccurrence-based vectors (con-
structed from newspaper paragraphs) considerably
outperformed the other kinds of word vectors. 1
These results clearly show that the LSA-based and
&apos;These results were replicated even when all the human-
produced associates including synonymous ones were used
for assessing the precision scores.
the cooccurrence-based vector spaces reflect as-
sociative similarity between words more than the
dictionary-based space.
The relation between the number of dimen-
sions and the performance in association judgment
was quite different from the relation observed in
the synonym judgment experiment. Although the
score of the dictionary-based vectors increased as
the dimension of the vectors increased as in the
case of synonym judgment, the scores of both
LSA-based and cooccurrence-based vectors had
a peak around 200 dimensions, as Landauer and
Dumais (1997) demonstrated. This finding seems
to suggest that some hundred dimensions may be
enough to represent the knowledge of associative
similarity.
Figure 4 shows the result of the additional ex-
periment in which familiarity effects were taken
into account. As compared to the result without
familiarity filtering, there was a remarkable im-
provement of the performance of the dictionary-
based method; the dictionary-based method out-
performed the LSA-based method at 350 or higher
dimensions and the cooccurrence-based method at
800 or higher dimensions. This may be because
word occurrence in the sense definitions of a dic-
tionary does not reflect the actual frequency or fa-
miliarity of words, and thus the dictionary-based
method may possibly overestimate the similarity
of infrequent or unfamiliar words. On the other
hand, since the corpus of newspaper articles or
novels is likely to reflect actual word frequency,
the vector spaces derived from these corpora rep-
resent the similarity of infrequent words as appro-
priately as that of familiar words. 2
The result that the cooccurrence-based word
</bodyText>
<page confidence="0.937688">
863
</page>
<tableCaption confidence="0.993453">
Table 3: Comparison of mean precision among the
combinations of two corpora and two text units
</tableCaption>
<table confidence="0.988188">
Newspaper Novel
Method Para Sent Para Sent
All associates
LSA 0.016 0.017 0.015 0.015
COO 0.023 0.018 0.012 0.008
Familiar associates
LSA 0.0261 0.0258 0.024 0.023
COO 0.033 0.027 0.018 0.014
Note. Para=Paragraph; Sent= Sentence.
</table>
<bodyText confidence="0.999379571428571">
vectors constructed from newspaper paragraphs
achieved the best performance was again obtained
in the additional experiment. This consistent result
indicates that the cooccurrence-based method is
particularly useful for representing the knowledge
of associative similarity between words. The rela-
tion between the number of dimensions and mean
precision was unchanged even if a familiarity ef-
fect was considered.
Finally, Table 3 shows the comparison result
among four kinds of word vectors constructed
from different corpora and text units in the exper-
iment with and without familiarity filtering. The
listed values are mean precisions averaged over all
the 20 numbers of dimensions. As in the case of
synonym judgment experiment, word vectors con-
structed from newspaper paragraphs achieved the
best performance, although only the LSA-based
vectors had the highest precision when they were
derived from sentences of newspaper articles. As
in the case of synonym judgment, the newspa-
per corpus showed better performance than the
novel corpus, and especially the cooccurrence-
based method showed a fairly large difference
in performance between two corpora. This find-
ing seems to suggest that word cooccurrence in a
newspaper corpus is more likely to reflect associa-
tive similarity.
</bodyText>
<sectionHeader confidence="0.980816" genericHeader="method">
6 Semantic Network and Similarity
</sectionHeader>
<bodyText confidence="0.999973512195122">
As related work, Steyvers and Tenenbaum (2005)
examined the properties of semantic network, an-
2Indeed, the dictionary-based vector spaces contained a
larger number of unfamiliar words than the other spaces; 63%
of words in the dictionary were judged as unfamiliar, while
only 42% and 50% of words in the newspapers and the novels
were judged as unfamiliar.
other important geometric model for word mean-
ings. They found that three kinds of semantic net-
works — WordNet, Roget’s thesaurus, and word
associations — had a small-world structure and
a scale-free pattern of connectivity, but semantic
networks constructed from the LSA-based vector
spaces did not have these properties. They inter-
preted this finding as indicating a limitation of the
vector space model such as LSA to model human
knowledge of word meanings.
However, we can interpret their finding in a dif-
ferent way by considering a possibility that dif-
ferent semantic networks may capture different
kinds of word similarity. Scale-free networks have
a common characteristic that a small number of
nodes are connected to a very large number of
other nodes (Barab´asi and Albert, 1999). In the se-
mantic networks, such “hub” nodes correspond to
basic and highly polysemous words such as make
and money, and these words are likely to be tax-
onomically similar to many other words. Hence
if semantic networks reflect in large part taxo-
nomic similarity between words, they are likely
to have a scale-free structure. On the other hand,
since it is less likely to assume that only a few
words are associatively similar to a large number
of other words, semantic networks reflecting asso-
ciative similarity may not have a scale-free struc-
ture. Taken together, Steyvers and Tenenbaum’s
(2005) finding can be reinterpreted as suggesting
that WordNet and Roget’s thesaurus better reflect
taxonomic similarity, while the LSA-based word
vectors better reflect associative similarity, which
is consistent with our finding.
</bodyText>
<sectionHeader confidence="0.996741" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.996837666666667">
Through two simulation experiments, we obtained
the following findings:
• The dictionary-based word vectors better re-
flect the knowledge of taxonomic similarity,
while the LSA-based and the cooccurrence-
based word vectors better reflect the knowl-
edge of associative similarity. In particular,
the cooccurrence-based vectors are useful for
representing associative similarity.
</bodyText>
<listItem confidence="0.885213833333333">
• The dictionary-based vectors yielded bet-
ter performance in synonym judgment, but
the LSA-based vectors showed better perfor-
mance in antonym judgment.
• These kinds of word vectors showed the dis-
tinctive patterns of the relationship between
</listItem>
<page confidence="0.831416">
864
</page>
<bodyText confidence="0.999356684210526">
the number of dimensions of word vectors
and their performance.
We are now extending this work to examine in
more detail the relationship between various kinds
of word vectors and the quality of word similarity
involved in these vectors. It would be interesting
for further work to develop a method for extract-
ing the knowledge of a specific similarity from
the word space, e.g., extracting the knowledge
of taxonomic similarity from the dictionary-based
word space. Vector negation (Widdows, 2003)
may be a useful technique for this purpose. At the
same time we are also interested in a method for
combining different word spaces into one space,
e.g., combining the dictionary-based and the LSA-
based spaces into one coherent word space. Addi-
tionally we are trying to simulate cognitive pro-
cesses such as metaphor comprehension (Utsumi,
2006).
</bodyText>
<sectionHeader confidence="0.954223" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.994157333333333">
This research was supported in part by Grant-
in-Aid for Scientific Research(C) (No.17500171)
from Japan Society for the Promotion of Science.
</bodyText>
<sectionHeader confidence="0.995626" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999922316455696">
Jean Aitchison. 2003. Words in the Mind: An Intro-
duction to the Mental Lexicon, 3rd Edition. Oxford,
Basil Blackwell.
Shigeaki Amano and Kimihisa Kondo, editors. 2003.
Nihongo-no Goitokusei CD-ROM (Lexical proper-
ties ofJapanese). Sanseido, Tokyo.
Albert-L´aszl´o Barab´asi and R´eka Albert. 1999. Emer-
gence of scaling in random networks. Science,
286:509–512.
Curt Burgess. 1998. From simple associations to the
building blocks of language: Modeling meaning in
memory with the HAL model. Behavior Research
Methods, Instruments, &amp; Computers, 30(2):188–
198.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas L. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Journal
of the American Society For Information Science,
41(6):391–407.
Peter G¨ardenfors. 2000. Conceptual Spaces: The Ge-
ometry of Thought. MIT Press.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,
Yoshifumi Ooyama, and Yoshihiko Hayashi. 1999.
Goi-Taikei: A Japanese Lexicon CDROM. Iwanami
Shoten, Tokyo.
Kaname Kasahara, Kazumitsu Matsuzawa, and Tsu-
tomu Ishikawa. 1997. A method for judgment of se-
mantic similarity between daily-used words by using
machine readable dictionaries. Transactions of In-
formation Processing Society ofJapan, 38(7):1272–
1283. in Japanese.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to Plato’s problem: The latent seman-
tic analysis theory of the acquisition, induction, and
representation of knowledge. Psychological Review,
104:211–240.
Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.
Schreiber. 1998. The university of south florida
word association, rhyme, and word fragment norms.
http://www.usf.edu/FreeAssociation/.
Yoshiki Niwa and Yoshihiko Nitta. 1994. Co-
occurrence vectors from corpora vs. distance vectors
from dictionaries. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics
(COLING94), pages 304–309.
Robert M. Nosofsky. 1992. Similarity scaling and cog-
nitive process models. Annual Review of Psychol-
ogy, 43:25–53.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
123.
Mark Steyvers and Joshua B. Tenenbaum. 2005. The
large-scale structure of semantic network: Statistical
analyses and a model of semantic growth. Cognitive
Science, 29(1):41–78.
Mark Steyvers, Richard M. Shiffrin, and Douglas L.
Nelson. 2004. Word association spaces for predict-
ing semantic similarity effects in episodic memory.
In Alice F. Healy, editor, Experimental Cognitive
Psychology and Its Applications. American Psycho-
logical Association, 2004.
Takao Umemoto. 1969. Renso Kijunhyo (Free Associ-
ation Norm). Tokyo Daigaku Shuppankai, Tokyo.
Akira Utsumi, Koichi Hori, and Setsuo Ohsuga. 1998.
An affective-similarity-based method for compre-
hending attributional metaphors. Journal ofNatural
Language Processing, 5(3):3–32.
Akira Utsumi. 2006. Computational exploration of
metaphor comprehension processes. In Proceedings
of the 28th Annual Meeting of the Cognitive Science
Society (CogSci 2006).
Dominic Widdows. 2003. Orthogonal negation in vec-
tor spaces for modelling word-meanings and docu-
ment retrieval. In Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 136–143.
Dominic Widdows. 2004. Geometry and Meaning.
CSLI Publications.
</reference>
<page confidence="0.948994">
865
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953901">
<title confidence="0.998028">Word Vectors and Two Kinds of Similarity</title>
<author confidence="0.999375">Akira Utsumi</author>
<author confidence="0.999375">Daisuke Suzuki</author>
<affiliation confidence="0.9989965">Department of Systems Engineering The University of Electro-Communications</affiliation>
<address confidence="0.975794">1-5-1 Chofugaoka, Chofushi, Tokyo 182-8585, Japan</address>
<email confidence="0.987079">utsumi@se.uec.ac.jp,dajie@utm.se.uec.ac.jp</email>
<abstract confidence="0.9994561875">This paper examines what kind of similarity between words can be represented by what kind of word vectors in the vector space model. Through two experiments, three methods for constructing word vectors, i.e., LSA-based, cooccurrence-based and dictionary-based methods, were compared in terms of the ability to represent two kinds of similarity, i.e., taxonomic similarity and associative similarity. The result of the comparison was that the dictionary-based word vectors better reflect taxonomic similarity, while the LSAbased and the cooccurrence-based word vectors better reflect associative similarity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jean Aitchison</author>
</authors>
<title>Words in the Mind: An Introduction to the Mental Lexicon, 3rd Edition.</title>
<date>2003</date>
<location>Oxford, Basil Blackwell.</location>
<contexts>
<context position="5999" citStr="Aitchison, 2003" startWordPosition="920" endWordPosition="921"> because they are associated by virtue of an agent-subject relation. This dichotomy of similarity is practically important. Some tasks such as automatic thesaurus updating and paraphrasing need assessing taxonomic similarity, while some other tasks such as affective Web search and semantic disambiguation require assessing associative similarity rather than taxonomic similarity. This dichotomy is also psychologically motivated. Many empirical studies on word searches and speech disorders have revealed that words in the mind (i.e., mental lexicon) are organized by these two kinds of similarity (Aitchison, 2003). This dichotomy is also essential to some cognitive processes. For example, metaphors are perceived as being more apt when their constituent words are associatively more similar but categorically dissimilar (Utsumi et al., 1998). These psychological findings suggest that people distinguish between these two kinds of similarity in certain cognitive processes. 3 Constructing Word Vectors 3.1 Overview In this study, word vectors (or word spaces) are constructed in the following way. First, all content words ti in a corpus are represented as mdimensional feature vectors wi. wi = (wi1i wi2i ··· i </context>
</contexts>
<marker>Aitchison, 2003</marker>
<rawString>Jean Aitchison. 2003. Words in the Mind: An Introduction to the Mental Lexicon, 3rd Edition. Oxford, Basil Blackwell.</rawString>
</citation>
<citation valid="true">
<date>2003</date>
<booktitle>Nihongo-no Goitokusei CD-ROM (Lexical properties ofJapanese). Sanseido,</booktitle>
<editor>Shigeaki Amano and Kimihisa Kondo, editors.</editor>
<location>Tokyo.</location>
<marker>2003</marker>
<rawString>Shigeaki Amano and Kimihisa Kondo, editors. 2003. Nihongo-no Goitokusei CD-ROM (Lexical properties ofJapanese). Sanseido, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert-L´aszl´o Barab´asi</author>
<author>R´eka Albert</author>
</authors>
<title>Emergence of scaling in random networks.</title>
<date>1999</date>
<journal>Science,</journal>
<pages>286--509</pages>
<marker>Barab´asi, Albert, 1999</marker>
<rawString>Albert-L´aszl´o Barab´asi and R´eka Albert. 1999. Emergence of scaling in random networks. Science, 286:509–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Curt Burgess</author>
</authors>
<title>From simple associations to the building blocks of language: Modeling meaning in memory with the HAL model.</title>
<date>1998</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<volume>30</volume>
<issue>2</issue>
<pages>198</pages>
<contexts>
<context position="2651" citStr="Burgess, 1998" startWordPosition="404" endWordPosition="405">tructing word vectors. Latent semantic analysis (LSA) is the most well-known method that uses the frequency of words in a fraction of documents to assess the coordinates of word vectors and singular value decomposition (SVD) to reduce the dimension. LSA was originally put forward as a document indexing technique for automatic information retrieval (Deerwester et al., 1990), but several studies (Landauer and Dumais, 1997) have shown that LSA successfully mimics many human behaviors associated with semantic processing. Other methods use a variety of other information: cooccurrence of two words (Burgess, 1998; Sch¨utze, 1998), occurrence of a word in the sense definitions of a dictionary (Kasahara et al., 1997; Niwa and Nitta, 1994) or word association norms (Steyvers et al., 2004). However, despite the fact that there are different kinds of similarity between words, or different relations underlying word similarity such as a synonymous relation and an associative relation, no studies have ever examined the relationship between methods for constructing word vectors and the type of similarity involved in word vectors in a systematic way. Some studies on word vectors have compared the performance am</context>
</contexts>
<marker>Burgess, 1998</marker>
<rawString>Curt Burgess. 1998. From simple associations to the building blocks of language: Modeling meaning in memory with the HAL model. Behavior Research Methods, Instruments, &amp; Computers, 30(2):188– 198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas L Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society For Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="2413" citStr="Deerwester et al., 1990" startWordPosition="365" endWordPosition="368">rds are represented by high-dimensional vectors, i.e., word vectors, and the degree of semantic similarity between any two words can be easily computed as a cosine of the angle formed by their vectors. A number of methods have been proposed for constructing word vectors. Latent semantic analysis (LSA) is the most well-known method that uses the frequency of words in a fraction of documents to assess the coordinates of word vectors and singular value decomposition (SVD) to reduce the dimension. LSA was originally put forward as a document indexing technique for automatic information retrieval (Deerwester et al., 1990), but several studies (Landauer and Dumais, 1997) have shown that LSA successfully mimics many human behaviors associated with semantic processing. Other methods use a variety of other information: cooccurrence of two words (Burgess, 1998; Sch¨utze, 1998), occurrence of a word in the sense definitions of a dictionary (Kasahara et al., 1997; Niwa and Nitta, 1994) or word association norms (Steyvers et al., 2004). However, despite the fact that there are different kinds of similarity between words, or different relations underlying word similarity such as a synonymous relation and an associative</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas L. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society For Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter G¨ardenfors</author>
</authors>
<title>Conceptual Spaces: The Geometry of Thought.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<marker>G¨ardenfors, 2000</marker>
<rawString>Peter G¨ardenfors. 2000. Conceptual Spaces: The Geometry of Thought. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
</authors>
<title>Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi.</title>
<date>1999</date>
<marker>Ikehara, 1999</marker>
<rawString>Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi. 1999.</rawString>
</citation>
<citation valid="false">
<title>Goi-Taikei: A Japanese Lexicon CDROM. Iwanami Shoten,</title>
<location>Tokyo.</location>
<marker></marker>
<rawString>Goi-Taikei: A Japanese Lexicon CDROM. Iwanami Shoten, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaname Kasahara</author>
<author>Kazumitsu Matsuzawa</author>
<author>Tsutomu Ishikawa</author>
</authors>
<title>A method for judgment of semantic similarity between daily-used words by using machine readable dictionaries.</title>
<date>1997</date>
<journal>Transactions of Information Processing Society ofJapan,</journal>
<volume>38</volume>
<issue>7</issue>
<pages>1283</pages>
<note>in Japanese.</note>
<contexts>
<context position="2754" citStr="Kasahara et al., 1997" startWordPosition="419" endWordPosition="422">he frequency of words in a fraction of documents to assess the coordinates of word vectors and singular value decomposition (SVD) to reduce the dimension. LSA was originally put forward as a document indexing technique for automatic information retrieval (Deerwester et al., 1990), but several studies (Landauer and Dumais, 1997) have shown that LSA successfully mimics many human behaviors associated with semantic processing. Other methods use a variety of other information: cooccurrence of two words (Burgess, 1998; Sch¨utze, 1998), occurrence of a word in the sense definitions of a dictionary (Kasahara et al., 1997; Niwa and Nitta, 1994) or word association norms (Steyvers et al., 2004). However, despite the fact that there are different kinds of similarity between words, or different relations underlying word similarity such as a synonymous relation and an associative relation, no studies have ever examined the relationship between methods for constructing word vectors and the type of similarity involved in word vectors in a systematic way. Some studies on word vectors have compared the performance among different methods on some specific tasks such as semantic disambiguation (Niwa and Nitta, 1994) and</context>
<context position="10092" citStr="Kasahara et al. (1997)" startWordPosition="1646" endWordPosition="1649">4) where fij denotes the number of times the word tj occurs in the sense definitions of the word ti, and dfj denotes the number of words whose sense definitions contain the word tj. The second term in parentheses in Equation (4) means the square root of the number of times the word tj occurs in a collection of sense definitions for any words that are included in the sense definitions of the word ti, while the third term means the number of times ti occurs in the sense definitions of tj. The parameters α and Q are positive real constants expressing the weights for these information. (Following Kasahara et al. (1997), these parameters are set to 0.2 in this paper.) Equation (4) was originally put forward by Kasahara et al. (1997), but our dictionary-based method differs from their method in terms of how the dimensions are reduced. Their method groups together the dimensions for words in the same category of a thesaurus, but our method uses SVD as we will described next. 3.4 Reducing Dimensions Using a SVD technique, a matrix M is factorized as the product of three matrices UEVT, where the diagonal matrix E consists of r singular values that are arranged in nonincreasing order such that r is the rank of M.</context>
</contexts>
<marker>Kasahara, Matsuzawa, Ishikawa, 1997</marker>
<rawString>Kaname Kasahara, Kazumitsu Matsuzawa, and Tsutomu Ishikawa. 1997. A method for judgment of semantic similarity between daily-used words by using machine readable dictionaries. Transactions of Information Processing Society ofJapan, 38(7):1272– 1283. in Japanese.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<pages>104--211</pages>
<contexts>
<context position="1158" citStr="Landauer and Dumais, 1997" startWordPosition="160" endWordPosition="163">d in terms of the ability to represent two kinds of similarity, i.e., taxonomic similarity and associative similarity. The result of the comparison was that the dictionary-based word vectors better reflect taxonomic similarity, while the LSAbased and the cooccurrence-based word vectors better reflect associative similarity. 1 Introduction Recently, geometric models have been used to represent words and their meanings, and proven to be highly useful both for many NLP applications associated with semantic processing (Widdows, 2004) and for human modeling in cognitive science (G¨ardenfors, 2000; Landauer and Dumais, 1997). There are also good reasons for studying geometric models in the field of computational linguistics. First, geometric models are cost-effective in that it takes much less time and less effort to construct large-scale geometric representation of word meanings than it would take to construct dictionaries or thesauri. Second, they can represent the implicit knowledge of word meanings that dictionaries and thesauri cannot do. Finally, geometric representation is easy to revise and extend. A vector space model is the most commonly used geometric model for the meanings of words. The basic idea of </context>
<context position="2462" citStr="Landauer and Dumais, 1997" startWordPosition="372" endWordPosition="375">, i.e., word vectors, and the degree of semantic similarity between any two words can be easily computed as a cosine of the angle formed by their vectors. A number of methods have been proposed for constructing word vectors. Latent semantic analysis (LSA) is the most well-known method that uses the frequency of words in a fraction of documents to assess the coordinates of word vectors and singular value decomposition (SVD) to reduce the dimension. LSA was originally put forward as a document indexing technique for automatic information retrieval (Deerwester et al., 1990), but several studies (Landauer and Dumais, 1997) have shown that LSA successfully mimics many human behaviors associated with semantic processing. Other methods use a variety of other information: cooccurrence of two words (Burgess, 1998; Sch¨utze, 1998), occurrence of a word in the sense definitions of a dictionary (Kasahara et al., 1997; Niwa and Nitta, 1994) or word association norms (Steyvers et al., 2004). However, despite the fact that there are different kinds of similarity between words, or different relations underlying word similarity such as a synonymous relation and an associative relation, no studies have ever examined the rela</context>
<context position="22790" citStr="Landauer and Dumais (1997)" startWordPosition="3790" endWordPosition="3793">ncluding synonymous ones were used for assessing the precision scores. the cooccurrence-based vector spaces reflect associative similarity between words more than the dictionary-based space. The relation between the number of dimensions and the performance in association judgment was quite different from the relation observed in the synonym judgment experiment. Although the score of the dictionary-based vectors increased as the dimension of the vectors increased as in the case of synonym judgment, the scores of both LSA-based and cooccurrence-based vectors had a peak around 200 dimensions, as Landauer and Dumais (1997) demonstrated. This finding seems to suggest that some hundred dimensions may be enough to represent the knowledge of associative similarity. Figure 4 shows the result of the additional experiment in which familiarity effects were taken into account. As compared to the result without familiarity filtering, there was a remarkable improvement of the performance of the dictionarybased method; the dictionary-based method outperformed the LSA-based method at 350 or higher dimensions and the cooccurrence-based method at 800 or higher dimensions. This may be because word occurrence in the sense defin</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104:211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L Nelson</author>
<author>Cathy L McEvoy</author>
<author>Thomas A Schreiber</author>
</authors>
<title>The university of south florida word association, rhyme, and word fragment norms.</title>
<date>1998</date>
<note>http://www.usf.edu/FreeAssociation/.</note>
<contexts>
<context position="19590" citStr="Nelson et al., 1998" startWordPosition="3235" endWordPosition="3238"> by 1, and after that mean precision was calculated as the average of all these precision scores. It must be noted here that, in order to eliminate the bias caused by the difference in the number of contained words among word spaces, we conducted the simulation using 46,000 words that we randomly chose for each corpus so that they could include all the humanproduced associates. Although this computational method of producing associates is sufficient for the present purpose, it may be inadequate to model the psychological process of free association. Some empirical studies of word association (Nelson et al., 1998) revealed that frequent or familiar words were highly likely to be produced as associates, but our methods for constructing word vectors may not directly address such frequency effect on word association. Hence, we conducted an additional experiment in which only familiar words were used for computing similarity to a given stimulus word, i.e., less familiar words were not used as candidates of as862 Number of Dimensions Figure 3: Mean precision of word association judgment 0 100 200 300 400 500 600 700 800 9001000 Number of Dimensions Figure 4: Average precision of word association judgment fo</context>
</contexts>
<marker>Nelson, McEvoy, Schreiber, 1998</marker>
<rawString>Douglas L. Nelson, Cathy L. McEvoy, and Thomas A. Schreiber. 1998. The university of south florida word association, rhyme, and word fragment norms. http://www.usf.edu/FreeAssociation/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshiki Niwa</author>
<author>Yoshihiko Nitta</author>
</authors>
<title>Cooccurrence vectors from corpora vs. distance vectors from dictionaries.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING94),</booktitle>
<pages>304--309</pages>
<contexts>
<context position="2777" citStr="Niwa and Nitta, 1994" startWordPosition="423" endWordPosition="426">n a fraction of documents to assess the coordinates of word vectors and singular value decomposition (SVD) to reduce the dimension. LSA was originally put forward as a document indexing technique for automatic information retrieval (Deerwester et al., 1990), but several studies (Landauer and Dumais, 1997) have shown that LSA successfully mimics many human behaviors associated with semantic processing. Other methods use a variety of other information: cooccurrence of two words (Burgess, 1998; Sch¨utze, 1998), occurrence of a word in the sense definitions of a dictionary (Kasahara et al., 1997; Niwa and Nitta, 1994) or word association norms (Steyvers et al., 2004). However, despite the fact that there are different kinds of similarity between words, or different relations underlying word similarity such as a synonymous relation and an associative relation, no studies have ever examined the relationship between methods for constructing word vectors and the type of similarity involved in word vectors in a systematic way. Some studies on word vectors have compared the performance among different methods on some specific tasks such as semantic disambiguation (Niwa and Nitta, 1994) and cued/free recall (Stey</context>
</contexts>
<marker>Niwa, Nitta, 1994</marker>
<rawString>Yoshiki Niwa and Yoshihiko Nitta. 1994. Cooccurrence vectors from corpora vs. distance vectors from dictionaries. In Proceedings of the 15th International Conference on Computational Linguistics (COLING94), pages 304–309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert M Nosofsky</author>
</authors>
<title>Similarity scaling and cognitive process models. Annual Review of Psychology,</title>
<date>1992</date>
<pages>43--25</pages>
<contexts>
<context position="3790" citStr="Nosofsky, 1992" startWordPosition="589" endWordPosition="590">ystematic way. Some studies on word vectors have compared the performance among different methods on some specific tasks such as semantic disambiguation (Niwa and Nitta, 1994) and cued/free recall (Steyvers et al., 2004), but it is not at all clear whether there are essential differences in the quality of similarity among word vectors constructed by different methods, and if so, what kind of similarity is involved in what kind of word vectors. Even in the field of cognitive psychology, although geometric models of similarity such as multidimensional scaling have long been studied and debated (Nosofsky, 1992), the possibility that different methods for word vectors may cap858 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 858–865, Sydney, July 2006. c�2006 Association for Computational Linguistics ture different kinds of word similarity has never been addressed. This study, therefore, aims to examine the relationship between the methods for constructing word vectors and the type of similarity in a systematic way. Especially this study addresses three methods, LSA-based, cooccurrence-based, and dictionary-based methods, and two kinds of similarity, taxonomic similarity an</context>
</contexts>
<marker>Nosofsky, 1992</marker>
<rawString>Robert M. Nosofsky. 1992. Similarity scaling and cognitive process models. Annual Review of Psychology, 43:25–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>The large-scale structure of semantic network: Statistical analyses and a model of semantic growth.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="25570" citStr="Steyvers and Tenenbaum (2005)" startWordPosition="4216" endWordPosition="4219">word vectors constructed from newspaper paragraphs achieved the best performance, although only the LSA-based vectors had the highest precision when they were derived from sentences of newspaper articles. As in the case of synonym judgment, the newspaper corpus showed better performance than the novel corpus, and especially the cooccurrencebased method showed a fairly large difference in performance between two corpora. This finding seems to suggest that word cooccurrence in a newspaper corpus is more likely to reflect associative similarity. 6 Semantic Network and Similarity As related work, Steyvers and Tenenbaum (2005) examined the properties of semantic network, an2Indeed, the dictionary-based vector spaces contained a larger number of unfamiliar words than the other spaces; 63% of words in the dictionary were judged as unfamiliar, while only 42% and 50% of words in the newspapers and the novels were judged as unfamiliar. other important geometric model for word meanings. They found that three kinds of semantic networks — WordNet, Roget’s thesaurus, and word associations — had a small-world structure and a scale-free pattern of connectivity, but semantic networks constructed from the LSA-based vector space</context>
</contexts>
<marker>Steyvers, Tenenbaum, 2005</marker>
<rawString>Mark Steyvers and Joshua B. Tenenbaum. 2005. The large-scale structure of semantic network: Statistical analyses and a model of semantic growth. Cognitive Science, 29(1):41–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Richard M Shiffrin</author>
<author>Douglas L Nelson</author>
</authors>
<title>Word association spaces for predicting semantic similarity effects in episodic memory.</title>
<date>2004</date>
<booktitle>Experimental Cognitive Psychology and Its Applications.</booktitle>
<editor>In Alice F. Healy, editor,</editor>
<publisher>American Psychological Association,</publisher>
<contexts>
<context position="2827" citStr="Steyvers et al., 2004" startWordPosition="431" endWordPosition="434">es of word vectors and singular value decomposition (SVD) to reduce the dimension. LSA was originally put forward as a document indexing technique for automatic information retrieval (Deerwester et al., 1990), but several studies (Landauer and Dumais, 1997) have shown that LSA successfully mimics many human behaviors associated with semantic processing. Other methods use a variety of other information: cooccurrence of two words (Burgess, 1998; Sch¨utze, 1998), occurrence of a word in the sense definitions of a dictionary (Kasahara et al., 1997; Niwa and Nitta, 1994) or word association norms (Steyvers et al., 2004). However, despite the fact that there are different kinds of similarity between words, or different relations underlying word similarity such as a synonymous relation and an associative relation, no studies have ever examined the relationship between methods for constructing word vectors and the type of similarity involved in word vectors in a systematic way. Some studies on word vectors have compared the performance among different methods on some specific tasks such as semantic disambiguation (Niwa and Nitta, 1994) and cued/free recall (Steyvers et al., 2004), but it is not at all clear whe</context>
</contexts>
<marker>Steyvers, Shiffrin, Nelson, 2004</marker>
<rawString>Mark Steyvers, Richard M. Shiffrin, and Douglas L. Nelson. 2004. Word association spaces for predicting semantic similarity effects in episodic memory. In Alice F. Healy, editor, Experimental Cognitive Psychology and Its Applications. American Psychological Association, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takao Umemoto</author>
</authors>
<title>Renso Kijunhyo (Free Association Norm). Tokyo Daigaku Shuppankai,</title>
<date>1969</date>
<location>Tokyo.</location>
<contexts>
<context position="17294" citStr="Umemoto, 1969" startWordPosition="2858" endWordPosition="2859">ions. Regardless of construction methods and test items, the word vectors constructed using newspaper paragraphs achieved the best performance, which are denoted by boldfaces. Concerning an effect of corpus difference, the newspaper corpus was superior to the literary corpus. The difference of text units did not have a clear influence on the performance of word spaces. 5 Experiment 2: Word Association 5.1 Method In order to compare the ability of the word spaces to judge associative similarity, we conducted a word association experiment using a Japanese word association norm “Renso Kijunhyo” (Umemoto, 1969). This free-association database was developed based on the responses of 1,000 students to 210 stimulus words. For example, when given the word writer as a stimulus, students listed the words shown in Table 2. (Table 2 also shows the original words in Japanese.) For the simulation experiment, we selected 176 stimulus words that all the three corpora contained. These stimuli had 27 associate words on average. We then removed any associate words that were synonymous with the stimulus word (e.g., author in Table 2), since the purpose of this experiment was to examine the ability to assess associa</context>
</contexts>
<marker>Umemoto, 1969</marker>
<rawString>Takao Umemoto. 1969. Renso Kijunhyo (Free Association Norm). Tokyo Daigaku Shuppankai, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akira Utsumi</author>
<author>Koichi Hori</author>
<author>Setsuo Ohsuga</author>
</authors>
<title>An affective-similarity-based method for comprehending attributional metaphors.</title>
<date>1998</date>
<journal>Journal ofNatural Language Processing,</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="6228" citStr="Utsumi et al., 1998" startWordPosition="952" endWordPosition="955"> while some other tasks such as affective Web search and semantic disambiguation require assessing associative similarity rather than taxonomic similarity. This dichotomy is also psychologically motivated. Many empirical studies on word searches and speech disorders have revealed that words in the mind (i.e., mental lexicon) are organized by these two kinds of similarity (Aitchison, 2003). This dichotomy is also essential to some cognitive processes. For example, metaphors are perceived as being more apt when their constituent words are associatively more similar but categorically dissimilar (Utsumi et al., 1998). These psychological findings suggest that people distinguish between these two kinds of similarity in certain cognitive processes. 3 Constructing Word Vectors 3.1 Overview In this study, word vectors (or word spaces) are constructed in the following way. First, all content words ti in a corpus are represented as mdimensional feature vectors wi. wi = (wi1i wi2i ··· i wim) (1) Each element wij is determined by statistical analysis of the corpus, whose methods will be described in Section 3.3. A matrix M is then constructed using n feature vectors as rows. M= � w1 ... (2) wn Finally, the dimens</context>
</contexts>
<marker>Utsumi, Hori, Ohsuga, 1998</marker>
<rawString>Akira Utsumi, Koichi Hori, and Setsuo Ohsuga. 1998. An affective-similarity-based method for comprehending attributional metaphors. Journal ofNatural Language Processing, 5(3):3–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akira Utsumi</author>
</authors>
<title>Computational exploration of metaphor comprehension processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Cognitive Science Society (CogSci</booktitle>
<marker>Utsumi, 2006</marker>
<rawString>Akira Utsumi. 2006. Computational exploration of metaphor comprehension processes. In Proceedings of the 28th Annual Meeting of the Cognitive Science Society (CogSci 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Orthogonal negation in vector spaces for modelling word-meanings and document retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>136--143</pages>
<marker>Widdows, 2003</marker>
<rawString>Dominic Widdows. 2003. Orthogonal negation in vector spaces for modelling word-meanings and document retrieval. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 136–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Geometry and Meaning.</title>
<date>2004</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="1067" citStr="Widdows, 2004" startWordPosition="148" endWordPosition="149"> i.e., LSA-based, cooccurrence-based and dictionary-based methods, were compared in terms of the ability to represent two kinds of similarity, i.e., taxonomic similarity and associative similarity. The result of the comparison was that the dictionary-based word vectors better reflect taxonomic similarity, while the LSAbased and the cooccurrence-based word vectors better reflect associative similarity. 1 Introduction Recently, geometric models have been used to represent words and their meanings, and proven to be highly useful both for many NLP applications associated with semantic processing (Widdows, 2004) and for human modeling in cognitive science (G¨ardenfors, 2000; Landauer and Dumais, 1997). There are also good reasons for studying geometric models in the field of computational linguistics. First, geometric models are cost-effective in that it takes much less time and less effort to construct large-scale geometric representation of word meanings than it would take to construct dictionaries or thesauri. Second, they can represent the implicit knowledge of word meanings that dictionaries and thesauri cannot do. Finally, geometric representation is easy to revise and extend. A vector space mo</context>
</contexts>
<marker>Widdows, 2004</marker>
<rawString>Dominic Widdows. 2004. Geometry and Meaning. CSLI Publications.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>