<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000543">
<title confidence="0.95107">
Domain Adaptation meets Active Learning
</title>
<author confidence="0.996091">
Piyush Rai, Avishek Saha, Hal Daum´e III, and Suresh Venkatasubramanian
</author>
<affiliation confidence="0.799627">
School of Computing, University of Utah
Salt Lake City, UT 84112
</affiliation>
<email confidence="0.998157">
fpiyush,avishek,hal,sureshl@cs.utah.edu
</email>
<sectionHeader confidence="0.998594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999304">
In this work, we show how active learning
in some (target) domain can leverage infor-
mation from a different but related (source)
domain. We present an algorithm that har-
nesses the source domain data to learn the best
possible initializer hypothesis for doing active
learning in the target domain, resulting in im-
proved label complexity. We also present a
variant of this algorithm which additionally
uses the domain divergence information to se-
lectively query the most informative points in
the target domain, leading to further reduc-
tions in label complexity. Experimental re-
sults on a variety of datasets establish the effi-
cacy of the proposed methods.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953133333333">
Acquiring labeled data to train supervised learning
models can be difficult or expensive in many prob-
lem domains. Active Learning tries to circumvent
this difficultly by only querying the labels of the
most informative examples and, in several cases, has
been shown to achieve exponentially lower label-
complexity (number of queried labels) than super-
vised learning (Cohn et al., 1994). Domain Adap-
tation (Daum´e &amp; Marcu, 2006), although motivated
somewhat differently, attempts to address a seem-
ingly similar problem: lack of labeled data in some
target domain. Domain Adaptation deals with this
problem using labeled data from a different (but re-
lated) source domain.
In this paper, we consider the supervised domain
adaptation setting (Finkel &amp; Manning, 2009; Daum´e
III, 2007) having a large amount of labeled data from
a source domain, a large amount of unlabeled data
from a target domain, and additionally a small bud-
get for acquiring labels in the target domain. We
show how, apart from leveraging information in the
usual domain adaptation sense, the information from
the source domain can be leveraged to intelligently
query labels in the target domain.We achieve this
by first training the best possible classifier without
using target domain labeled data 1 and then using
the learned classifier to leverage the inter-domain in-
formation when we are additionally provided some
fixed budget for acquiring extra labeled target data
(i.e., the active learning setting (Settles, 2009)).
There are several ways in which our “best clas-
sifier” can be utilized. Our first approach uses this
classifier as the initializer while doing (online) ac-
tive learning in the target domain (Section 3). Then
we present a variant augmenting the first approach
using a domain-separator hypothesis which leads to
additionally ruling out querying the labels of those
target examples that appear “similar” to the source
domain (Section 4).
Figure 1 shows our basic setup which uses a
source (or unsupervised domain-adapted source)
classifier vo as an initializer for doing active learn-
ing in the target domain having some small, fixed
budget for querying labels. Our framework consists
of 2 phases: 1) Learning the best possible classi-
</bodyText>
<footnote confidence="0.9833024">
1For instance, either by simply training a supervised classi-
fier on the labeled source data, or by using unsupervised domain
adaptation techniques (Blitzer et al., 2006; Sugiyama et al.,
2007) that use labeled data from the source domain, and ad-
ditionally unlabeled data from the source and target domains.
</footnote>
<page confidence="0.980549">
27
</page>
<note confidence="0.931592">
Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 27–32,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.942589666666667">
Figure 1: Block diagram of our basic approach. Stage-1 can
use any black-box unsupervised domain adaptation approach
(e.g., (Blitzer et al., 2006; Sugiyama et al., 2007))
</figureCaption>
<bodyText confidence="0.99978">
fier v0 using source labeled (LS) and unlabeled data
(US), and target unlabeled (UT) data, and 2) Query-
ing labels for target domain examples by leveraging
information from the classifier learned in phase-1.
</bodyText>
<sectionHeader confidence="0.995319" genericHeader="method">
2 Online Active Learning
</sectionHeader>
<bodyText confidence="0.98125715">
The active learning phase of our algorithm is based
on (Cesa-Bianchi et al., 2006), henceforth referred
to as CBGZ. In this section, we briefly describe this
approach for the sake of completeness.
Their algorithm (Algorithm 1) starts with a zero
initialized weight vector w0T and proceeds in rounds
by querying the label of an example xi with proba-
bility b
b+|ri|, where |ri |is the confidence (in terms of
margin) of the current weight vector on xi. b is a pa-
rameter specifying how aggressively the labels are
queried. A large value of b implies that a large num-
ber of labels will be queried (conservative sampling)
whereas a small value would lead to a small number
of examples being queried. For each label queried,
the algorithm updates the current weight vector if the
label was predicted incorrectly. It is easy to see that
the total number of labels queried by this algorithm
is ETi= 1 E b
[ b+|ri|].
</bodyText>
<sectionHeader confidence="0.995975" genericHeader="method">
3 Active Online Domain Adaptation
</sectionHeader>
<bodyText confidence="0.999775">
In our supervised domain adaptation setting, we are
given a small budget for acquiring labels in a tar-
get domain, which makes it imperative to use active
learning in the target domain. However, our goal
is to additionally also leverage inter-domain related-
ness by exploiting whatever information we might
already have from the source domain. To accom-
plish this, we take the online active learning ap-
</bodyText>
<equation confidence="0.931714941176471">
Algorithm 1 CBGZ
Input: b &gt; 0; T: number of rounds
Initialization: w0T = 0; k = 1;
for i = 1 to T do
ˆxi = xi/||xi||, set ri = wi−1
T ˆxi;
predict ˆyi = SIGN(ri);
sample Zi ∼ Bernoulli(b
b+|ri|);
if Zi = 1 then
query label yi ∈ {+1, −1}
if ˆyi =6 yi then
update: wk T = wk−1
T + yiˆxi; k ← k + 1;
end if
end if
end for
</equation>
<bodyText confidence="0.988340333333333">
proach of (Cesa-Bianchi et al., 2006) described in
Section 2 and adapt it such that the algorithm uses
the best possible classifier learned (without target la-
beled data; see Figure 1) as the initializer hypothesis
in the target domain, and thereafter updates this hy-
pothesis in an online fashion using actively acquired
labels as is done in (Cesa-Bianchi et al., 2006). This
amounts to using w0T = v0 in Algorithm 1. We refer
to this algorithm as Active Online Domain Adapta-
tion (AODA). It can be shown that the modified al-
gorithm (AODA) yields smaller mistake bound and
smaller label complexity than the CBGZ algorithm.
We skip the proofs here and reserve the presentation
for a longer version. It is however possible to pro-
vide an intuitive argument for the smaller label com-
plexity: Since AODA is initialized with a non-zero
(but not randomly chosen) hypothesis v0 learned us-
ing data from a related source domain, the sequence
of hypotheses AODA produces are expected to have
higher confidences margins |r′i |as compared that of
CBGZ which is based on a zero initialized hypothe-
sis. Therefore, at each round, the sampling proba-
bility of AODA given by b
b+|r′ i |will also be smaller,
leading to a smaller number of queried labels since
it is nothing but ET i=1 E[ b
b+|r′ i|].
</bodyText>
<sectionHeader confidence="0.96037" genericHeader="method">
4 Using Domain Separator Hypothesis
</sectionHeader>
<bodyText confidence="0.99982075">
The relatedness of source and target domains can be
additionally leveraged to further improve the algo-
rithm described in Section 3. Since the source and
target domains are assumed to be related, one can
</bodyText>
<figure confidence="0.998778733333333">
LT
UT
LS
DOMAIN
ADAPTATION
v0
ACTIVE
LEARNING
wT
US
ORACLE
��������
��������
��������
��������
</figure>
<page confidence="0.99624">
28
</page>
<bodyText confidence="0.999766714285714">
use this fact to upfront rule out acquiring the labels
of some target domain examples that “appear” to be
similar to the source domain examples. As an il-
lustration, Fig. 2 shows a typical distribution sepa-
rator hypothesis (Blitzer et al., 2007a) which sepa-
rates the source and target examples. If the source
and target domains are reasonably different, then the
separator hypothesis can perfectly distinguish be-
tween the examples drawn from these two domains.
On the other hand, if the domains are similar, one
would expected that there will be some overlap and
therefore some of the target domain examples will
lie on the source side (cf., Fig. 2). Acquiring la-
bels for such examples is not really needed since
the initializing hypothesis v0 (cf., Fig 1) of AODA
would already have taken into account such exam-
ples. Therefore, such target examples can be out-
rightly ignored from being queried for labels. Our
second algorithm (Algorithm 2) is similar to Algo-
rithm 1, but also makes use of the distribution sepa-
rator hypothesis (which can be learned using source
and target unlabeled examples) as a preprocessing
step before doing active learning on each incom-
ing target example. We denote this algorithm by
DS-AODA (for Domain-Separator based AODA).
Since some of the target examples are upfront ruled
out from being queried, this approach resulted even
smaller number of queried labels (Section 5.4).
</bodyText>
<figureCaption confidence="0.915726">
Figure 2: An illustrative diagram showing distribution
separator hypothesis wds separating source data from tar-
get data. w is the actual target hypothesis
</figureCaption>
<sectionHeader confidence="0.999562" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999001">
In this section, we demonstrate the empirical perfor-
mance of our algorithms and compare them with a
</bodyText>
<equation confidence="0.931975769230769">
Algorithm 2 DS-AODA
Input: b &gt; 0; wds: distribution separator hypoth-
esis; v0 : initializing hypothesis ; T: number of
rounds
Initialization: w0T = v0; k = 1;
for i = 1 to T do
ˆxi = xi/||xi||,
if ˆxi does not lie on the source side of w′ds then
set ri = wi−1
T ˆxi;
predict ˆyi = SIGN(ri);
sample Zi — Bernoulli(b
b+|ri|);
</equation>
<bodyText confidence="0.730619333333333">
if Zi = 1 then
query label yi E 1+1, —11
if ˆyi =y� yi then
</bodyText>
<equation confidence="0.796991833333333">
update: wkT = wk−1
T + yiˆxi; k k + 1;
end if
end if
end if
end for
</equation>
<bodyText confidence="0.999512037037037">
number of baselines. Table 1 summarizes the meth-
ods used with a brief description of each. Among the
first three (ID, SDA, FEDA), FEDA (Daum´e III,
2007) is a state-of-the-art supervised domain adap-
tation method but assumes passively acquired la-
bels. The last four, RIAL, ZIAL, SIAL and AODA
methods in Table 1 acquire labels in an active fash-
ion. As the description denotes, RIAL and ZIAL
start active learning in target with a randomly ini-
tialized and zero initialized base hypothesis, respec-
tively. It is also important to distinguish between
SIAL and AODA here: SIAL uses an unmodi-
fied classifier learned only from source labeled data
as the initializer, whereas AODA uses an unsuper-
vised domain-adaptation technique (i.e., without us-
ing labeled target data) to learn the initializer. In our
experiments, we use the instance reweighting ap-
proach (Sugiyama et al., 2007) to perform the unsu-
pervised domain adaptation step. However, we note
that this step can also be performed using any other
unsupervised domain adaptation technique such as
Structural Correspondence Learning (SCL) (Blitzer
et al., 2006). We compare all the approaches based
on classification accuracies achieved for a given
budget of labeled target examples (Section-5.2), and
number of labels requested for a fixed pool of unla-
beled target examples and corresponding accuracies
</bodyText>
<figure confidence="0.7054035">
wds
DS
w
DT
</figure>
<page confidence="0.97519">
29
</page>
<table confidence="0.999028125">
Method Summary Active ?
ID In-domain (DT ) data No
SDA UDA followed by passively chosen labeled target data No
FEDA Frustratingly Easy Domain Adaptation Daum´e III (2007) No
ZIAL Zero initialized active learning Cesa-Bianchi et al. (2006) Yes
RIAL Randomly initialized active learning with fixed label budget Yes
SIAL Source hypothesis initialized active learning Yes
AODA UDA based source hypothesis initialized active learning Yes
</table>
<tableCaption confidence="0.999884">
Table 1: Description of the methods compared
</tableCaption>
<bodyText confidence="0.999899">
(Section-5.3). We use the vanilla Perceptron as the
base classifier of each of the algorithms and each
experiment has been averaged over 20 runs corre-
sponding to random data order permutations.
</bodyText>
<subsectionHeader confidence="0.825472">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999723586206897">
We report our empirical results for the task of senti-
ment classification using data provided by (Blitzer
et al., 2007b) which consists of user reviews of
eight product types (apparel, books, DVD, electron-
ics, kitchen, music, video, and other) from Ama-
zon.com. We also apply PCA to reduce the data-
dimensionality to 50. The sentiment classification
task for this dataset is binary classification which
corresponds to classifying a review as positive or
negative. The sentiment dataset consists of several
domain pairs with varying A-distance (which mea-
sures the domain separation), akin to the sense de-
scribed in (Ben-David et al., 2006). Table 2 presents
the domain pairs used in our experiments and their
corresponding domain divergences in terms of the
A-distance (Ben-David et al., 2006).
To compute the A-distance from finite samples of
source and target domain, we use a surrogate to the
true A-distance (the proxy A-distance) in a manner
similar to (Ben-David et al., 2006): First, we train a
linear classifier to separate the source domain from
the target domain using only unlabeled examples
from both. The average per-instance hinge-loss of
this classifier subtracted from 1 serves as our esti-
mate of the proxy A-distance. A score of 1 means
perfectly separable distributions whereas a score of
0 means that the two distributions are essentially the
same. As a general rule, a high score means that the
two domains are reasonably far apart.
</bodyText>
<table confidence="0.999242142857143">
Source Target A-distance
Dvd (D) Book (B) 0.7616
Dvd (D) Music (M) 0.7314
Books (B) Apparel (A) 0.5970
Dvd (D) Apparel (A) 0.5778
Electronics (E) Apparel (A) 0.1717
Kitchen (K) Apparel (A) 0.0459
</table>
<tableCaption confidence="0.997866">
Table 2: Proxy A-distances between some domain pairs
</tableCaption>
<subsectionHeader confidence="0.996057">
5.2 Classification Accuracies
</subsectionHeader>
<bodyText confidence="0.999997588235294">
In our first experiment, we compare our first ap-
proach of Section 3 (AODA, and also SIAL
which naively uses the unadapted source hypoth-
esis) against other baselines on two domain pairs
from the sentiments dataset: DVD—*BOOKS (large
A distance) and KITCHEN—*APPAREL (small A
distance) with varying target budget (1000 to 5000).
The results are shown in Table 3 and Table 4. As
the results indicate, on both datasets, our approaches
(SIAL, AODA) perform consistently better than the
baseline approaches (Table 1) which also include
one of the state-of-the-art supervised domain adap-
tation algorithms (Daum´e III, 2007). On the other
hand, we observe that the zero-initialized and ran-
domly initialized approaches do not perform as well.
In particular, the latter case suggests that it’s impor-
tant to have a sensible initialization.
</bodyText>
<subsectionHeader confidence="0.999202">
5.3 Label Complexity Results
</subsectionHeader>
<bodyText confidence="0.999987">
Next, we compare the various algorithms on the
basis of the number of labels acquired (and corre-
sponding accuracies) when given the complete pool
of unlabeled examples from the target domain. Ta-
ble 5 shows that our approaches result in much
smaller label complexities as compared to other ac-
</bodyText>
<page confidence="0.992348">
30
</page>
<table confidence="0.998391272727273">
Met- 1000 2000 Target Budget 4000 5000
hod 3000
Acc (Std) Acc (Std) Acc (Std) Acc (Std) Acc (Std)
ID 65.94 (13.40) 66.66 (13.01) 67.00 (12.40) 65.72 (13.98) 66.25 (13.18)
SDA 66.17 (12.57) 66.45 (12.88) 65.31 (13.13) 66.33 (13.51) 66.22 (13.05)
RIAL 51.79 (14.36) 53.12 (14.65) 55.01 (14.20) 57.56 (14.18) 58.57 (12.44)
ZIAL 66.24 (13.16) 66.72 (13.30) 63.97 (14.82) 66.28 (13.61) 66.36 (12.82)
SIAL 68.22 (12.17) 69.65 (11.20) 69.95 (11.55) 70.54 (11.42) 70.97 (10.97)
AODA 67.64 (12.35) 68.89 (11.37) 69.49 (11.63) 70.55 (1.15) 70.65 (10.94)
FEDA 67.31 (13.36) 68.47 (13.15) 68.37 (12.72) 66.95 (3.11) 67.13 (13.16)
Acc: Accuracy Std: Standard Deviation
</table>
<tableCaption confidence="0.993021">
Table 3: Classification accuracies for DVD--+BOOKS, for fixed target budget.
</tableCaption>
<table confidence="0.999816090909091">
Met- 1000 2000 Target Budget 4000 5000
hod 3000
Acc (Std) Acc (Std) Acc (Std) Acc (Std) Acc (Std)
ID 69.64 (13.14) 69.61 (13.17) 69.36 (13.14) 69.77 (13.58) 70.77 (13.05)
SDA 69.70 (12.57) 70.48 (13.42) 70.29 (12.56) 70.86 (13.16) 70.71 (13.65)
RIAL 52.13 (15.44) 56.83 (15.36) 58.09 (14.09) 59.82 (14.16) 62.03 (12.52)
ZIAL 70.09 (13.74) 69.96 (13.27) 68.6 (13.94) 70.06 (12.84) 69.75 (13.26)
SIAL 73.82 (11.47) 74.45 (11.27) 75.11 (10.98) 75.35 (11.30) 75.58 (10.85)
AODA 73.93 (11.84) 74.18 (11.85) 75.13 (11.18) 75.88 (11.32) 76.02 (10.97)
FEDA 70.05 (12.47) 69.34 (13.50) 71.22 (13.00) 71.67 (12.59) 70.80 (13.89)
Acc: Accuracy Std: Standard Deviation
</table>
<tableCaption confidence="0.999802">
Table 4: Classification accuracies for KITCHEN→APPAREL, for fixed target budget.
</tableCaption>
<bodyText confidence="0.999910428571429">
tive learning based baselines and still gives better
classification accuracies. We also note that although
RIAL initializes with a non-zero hypothesis and
queries almost similar number of labels as our algo-
rithms, it actually performs worse than even ZIAL
in terms of classification accuracies, which implies
the significant of a sensible initializing hypothesis.
</bodyText>
<subsectionHeader confidence="0.946288">
5.4 DS-AODA Results
</subsectionHeader>
<bodyText confidence="0.998568916666667">
Finally, we evaluate our distribution separator hy-
pothesis based approach (DS-AODA) discussed in
Section 4. As our experimental results (on four do-
main pairs, Fig. 3) indicate, this approach leads to
considerably smaller number of labels acquired than
our first approach AODA which does not use the
information about domain separation, without any
perceptible loss in classification accuracies. Simi-
lar improvements in label complexity (although not
reported here) were observed when we grafted the
distribution separator hypothesis around SIAL (the
unaltered source initialized hypothesis).
</bodyText>
<table confidence="0.981831">
Met- DVD-BOOK KITCHEN-APPAREL
hod
Acc (Std) Labels Acc (Std) Labels
RIAL 62.74 (±3.00) 7618 62.15 (±4.51) 4871
ZIAL 65.65 (±2.82) 10459 70.19 (±2.64) 6968
SIAL 72.11 (±1.20) 7517 75.62 (±1.14) 4709
AODA 72.00 (±1.31) 7452 75.62 (±0.82) 4752
Acc: Accuracy  |Std: Standard Deviation
</table>
<tableCaption confidence="0.995206">
Table 5: Accuracy and label complexity of DVD→BOOKS
</tableCaption>
<bodyText confidence="0.327104">
and KITCHEN→APPAREL with full target training data
treated as the unlabeled pool.
</bodyText>
<figureCaption confidence="0.96323">
Figure 3: Test accuracy and label complexity of D→B, D→M,
E→A and K→A.
</figureCaption>
<figure confidence="0.998827777777778">
AODA
DS-AODA
D-&gt;B D-&gt;M E-&gt;A K-&gt;A
domains
D-&gt;B D-&gt;M E-&gt;A K-&gt;A
domains
accuracy 80
70
60
50
#labels queried 12K
10K
8K
6K
4K
2K
AODA
DS-AODA
</figure>
<page confidence="0.999535">
31
</page>
<subsectionHeader confidence="0.731419">
5.5 SIAL vs AODA
</subsectionHeader>
<bodyText confidence="0.999988066666667">
Some of the results might indicate from naively ini-
tializing using even the unadapted source trained
classifier (SIAL) tends to be as good as initializing
with a classifer trained using unsupervised domain
adaptation (AODA). However, it is mainly due to
the particular unsupervised domain adaptation tech-
nique (naive instance weighting) we have used here
for the first stage. In some cases, the weights es-
timated using instance weighting may not be accu-
rate and the bias in importance weight estimation is
potentially the reason behind AODA not doing bet-
ter than SIAL in such cases. As mentioned earlier,
however, any other unsupervised domain adaptation
technique can be used here and, in general, AODA
is expected to perform better than SIAL.
</bodyText>
<sectionHeader confidence="0.999989" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999906">
Active learning in a domain adaptation setting has
received little attention so far. One interesting set-
ting was proposed in (Chan &amp; Ng, 2007) where they
apply active learning for word sense disambiguation
in a domain adaptation setting. Their active learn-
ing setting is pool-based whereas ours is a stream-
ing (online) setting. Furthermore, our second algo-
rithm also uses the domain separator hypothesis to
rule out querying the labels of target examples simi-
lar to the source. A combination of transfer learning
with active learning has been presented in (Shi et al.,
2008). One drawback of their approach is the re-
quirement of an initial pool of labeled target domain
data used to train an in-domain classifier. Without
this in-domain classifier, no transfer learning is pos-
sible in their setting.
</bodyText>
<sectionHeader confidence="0.999812" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999995625">
There are several interesting variants of our ap-
proach that can worth investigating. For instance,
one can use a hybrid oracle setting where the source
classifier vo could be used as an oracle that provides
labels for free, whenever it is reasonably highly con-
fident about its prediction (maybe in terms of its rel-
ative confidence as compared to the actual classifier
being learned; it would also be interesting to set,
and possibly adapt, this confidence measure as the
active learning progresses). Besides, in the distri-
bution separator hypothesis based approach of Sec-
tion 4, we empirically observed significant reduc-
tions in label-complexity, and it is supported by in-
tuitive arguments. However, it would be interesting
to be able to precisely quantify the amount by which
the label-complexity is expected to reduce.
</bodyText>
<sectionHeader confidence="0.99848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999937421052631">
Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F.
Analysis of Representations for Domain Adaptation.
In NIPS, 2006.
Blitzer, J., Mcdonald, R., and Pereira, F. Domain Adap-
tation with Structural Correspondence Learning. In
EMNLP, 2006.
Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and
Wortman, J. Learning Bounds for Domain Adaptation.
In NIPS, 2007a.
Blitzer, J., Dredze, M., and Pereira, F. Biographies, Bol-
lywood, Boom-boxes and Blenders: Domain Adapta-
tion for Sentiment Classification. In ACL, 2007b.
Cesa-Bianchi, Nicol`o, Gentile, Claudio, and Zaniboni,
Luca. Worst-Case Analysis of Selective Sampling for
Linear Classification. JMLR, 7, 2006.
Chan, Y. S. and Ng, H. T. Domain adaptation with active
learning for word sense disambiguation. In ACL, 2007.
Cohn, David, Atlas, Les, and Ladner, Richard. Improving
Generalization with Active Learning. Machine Learn-
ing, 15(2), 1994.
Daum´e, III, Hal and Marcu, Daniel. Domain adaptation
for statistical classifiers. Journal of Artificial Intelli-
gence Research, 26(1), 2006.
Daum´e III, H. Frustratingly Easy Domain Adaptation. In
ACL, 2007.
Finkel, Jenny Rose and Manning, Christopher D. Hier-
archical Bayesian domain adaptation. In NAACL, pp.
602–610, Morristown, NJ, USA, 2009.
Settles, B. Active Learning Literature Survey. In Com-
puter Sciences Technical Report 1648, University of
Wisconsin-Madison, 2009.
Shi, Xiaoxiao, Fan, Wei, and Ren, Jiangtao. Actively
Transfer Domain Knowledge. In ECML/PKDD (2),
2008.
Sugiyama, M., Nakajima, S., Kashima, H., von B¨unau,
P., and Kawanabe, M. Direct Importance Estimation
with Model Selection and Its Application to Covariate
Shift Adaptation. In NIPS, 2007.
</reference>
<page confidence="0.999299">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.697330">
<title confidence="0.999634">Domain Adaptation meets Active Learning</title>
<author confidence="0.881651">Piyush Rai</author>
<author confidence="0.881651">Avishek Saha</author>
<author confidence="0.881651">Hal Daum´e</author>
<author confidence="0.881651">Suresh</author>
<affiliation confidence="0.971941">School of Computing, University of</affiliation>
<address confidence="0.787746">Salt Lake City, UT</address>
<abstract confidence="0.998938125">In this work, we show how active learning in some (target) domain can leverage information from a different but related (source) domain. We present an algorithm that harnesses the source domain data to learn the best possible initializer hypothesis for doing active learning in the target domain, resulting in improved label complexity. We also present a variant of this algorithm which additionally uses the domain divergence information to selectively query the most informative points in the target domain, leading to further reductions in label complexity. Experimental results on a variety of datasets establish the efficacy of the proposed methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Ben-David</author>
<author>J Blitzer</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Analysis of Representations for Domain Adaptation. In</title>
<date>2006</date>
<booktitle>NIPS,</booktitle>
<contexts>
<context position="12108" citStr="Ben-David et al., 2006" startWordPosition="2007" endWordPosition="2010">r empirical results for the task of sentiment classification using data provided by (Blitzer et al., 2007b) which consists of user reviews of eight product types (apparel, books, DVD, electronics, kitchen, music, video, and other) from Amazon.com. We also apply PCA to reduce the datadimensionality to 50. The sentiment classification task for this dataset is binary classification which corresponds to classifying a review as positive or negative. The sentiment dataset consists of several domain pairs with varying A-distance (which measures the domain separation), akin to the sense described in (Ben-David et al., 2006). Table 2 presents the domain pairs used in our experiments and their corresponding domain divergences in terms of the A-distance (Ben-David et al., 2006). To compute the A-distance from finite samples of source and target domain, we use a surrogate to the true A-distance (the proxy A-distance) in a manner similar to (Ben-David et al., 2006): First, we train a linear classifier to separate the source domain from the target domain using only unlabeled examples from both. The average per-instance hinge-loss of this classifier subtracted from 1 serves as our estimate of the proxy A-distance. A sc</context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Pereira, 2006</marker>
<rawString>Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F. Analysis of Representations for Domain Adaptation. In NIPS, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>R Mcdonald</author>
<author>F Pereira</author>
</authors>
<title>Domain Adaptation with Structural Correspondence Learning. In</title>
<date>2006</date>
<booktitle>EMNLP,</booktitle>
<contexts>
<context position="3284" citStr="Blitzer et al., 2006" startWordPosition="509" endWordPosition="512">rator hypothesis which leads to additionally ruling out querying the labels of those target examples that appear “similar” to the source domain (Section 4). Figure 1 shows our basic setup which uses a source (or unsupervised domain-adapted source) classifier vo as an initializer for doing active learning in the target domain having some small, fixed budget for querying labels. Our framework consists of 2 phases: 1) Learning the best possible classi1For instance, either by simply training a supervised classifier on the labeled source data, or by using unsupervised domain adaptation techniques (Blitzer et al., 2006; Sugiyama et al., 2007) that use labeled data from the source domain, and additionally unlabeled data from the source and target domains. 27 Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 27–32, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Figure 1: Block diagram of our basic approach. Stage-1 can use any black-box unsupervised domain adaptation approach (e.g., (Blitzer et al., 2006; Sugiyama et al., 2007)) fier v0 using source labeled (LS) and unlabeled data (US), and target unlabeled (UT) data, and</context>
<context position="10530" citStr="Blitzer et al., 2006" startWordPosition="1760" endWordPosition="1763"> respectively. It is also important to distinguish between SIAL and AODA here: SIAL uses an unmodified classifier learned only from source labeled data as the initializer, whereas AODA uses an unsupervised domain-adaptation technique (i.e., without using labeled target data) to learn the initializer. In our experiments, we use the instance reweighting approach (Sugiyama et al., 2007) to perform the unsupervised domain adaptation step. However, we note that this step can also be performed using any other unsupervised domain adaptation technique such as Structural Correspondence Learning (SCL) (Blitzer et al., 2006). We compare all the approaches based on classification accuracies achieved for a given budget of labeled target examples (Section-5.2), and number of labels requested for a fixed pool of unlabeled target examples and corresponding accuracies wds DS w DT 29 Method Summary Active ? ID In-domain (DT ) data No SDA UDA followed by passively chosen labeled target data No FEDA Frustratingly Easy Domain Adaptation Daum´e III (2007) No ZIAL Zero initialized active learning Cesa-Bianchi et al. (2006) Yes RIAL Randomly initialized active learning with fixed label budget Yes SIAL Source hypothesis initia</context>
</contexts>
<marker>Blitzer, Mcdonald, Pereira, 2006</marker>
<rawString>Blitzer, J., Mcdonald, R., and Pereira, F. Domain Adaptation with Structural Correspondence Learning. In EMNLP, 2006.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Blitzer</author>
<author>K Crammer</author>
<author>A Kulesza</author>
<author>F Pereira</author>
<author>J Wortman</author>
</authors>
<title>Learning Bounds for Domain Adaptation. In</title>
<booktitle>NIPS, 2007a.</booktitle>
<marker>Blitzer, Crammer, Kulesza, Pereira, Wortman, </marker>
<rawString>Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Wortman, J. Learning Bounds for Domain Adaptation. In NIPS, 2007a.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Blitzer</author>
<author>M Dredze</author>
<author>F Biographies Pereira</author>
<author>Bollywood</author>
</authors>
<title>Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In</title>
<booktitle>ACL, 2007b.</booktitle>
<marker>Blitzer, Dredze, Pereira, Bollywood, </marker>
<rawString>Blitzer, J., Dredze, M., and Pereira, F. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In ACL, 2007b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicol`o Cesa-Bianchi</author>
<author>Claudio Gentile</author>
<author>Luca Zaniboni</author>
</authors>
<title>Worst-Case Analysis of Selective Sampling for Linear Classification.</title>
<date>2006</date>
<journal>JMLR,</journal>
<volume>7</volume>
<contexts>
<context position="4104" citStr="Cesa-Bianchi et al., 2006" startWordPosition="636" endWordPosition="639"> Learning for Natural Language Processing, pages 27–32, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Figure 1: Block diagram of our basic approach. Stage-1 can use any black-box unsupervised domain adaptation approach (e.g., (Blitzer et al., 2006; Sugiyama et al., 2007)) fier v0 using source labeled (LS) and unlabeled data (US), and target unlabeled (UT) data, and 2) Querying labels for target domain examples by leveraging information from the classifier learned in phase-1. 2 Online Active Learning The active learning phase of our algorithm is based on (Cesa-Bianchi et al., 2006), henceforth referred to as CBGZ. In this section, we briefly describe this approach for the sake of completeness. Their algorithm (Algorithm 1) starts with a zero initialized weight vector w0T and proceeds in rounds by querying the label of an example xi with probability b b+|ri|, where |ri |is the confidence (in terms of margin) of the current weight vector on xi. b is a parameter specifying how aggressively the labels are queried. A large value of b implies that a large number of labels will be queried (conservative sampling) whereas a small value would lead to a small number of examples be</context>
<context position="5719" citStr="Cesa-Bianchi et al., 2006" startWordPosition="937" endWordPosition="940">imperative to use active learning in the target domain. However, our goal is to additionally also leverage inter-domain relatedness by exploiting whatever information we might already have from the source domain. To accomplish this, we take the online active learning apAlgorithm 1 CBGZ Input: b &gt; 0; T: number of rounds Initialization: w0T = 0; k = 1; for i = 1 to T do ˆxi = xi/||xi||, set ri = wi−1 T ˆxi; predict ˆyi = SIGN(ri); sample Zi ∼ Bernoulli(b b+|ri|); if Zi = 1 then query label yi ∈ {+1, −1} if ˆyi =6 yi then update: wk T = wk−1 T + yiˆxi; k ← k + 1; end if end if end for proach of (Cesa-Bianchi et al., 2006) described in Section 2 and adapt it such that the algorithm uses the best possible classifier learned (without target labeled data; see Figure 1) as the initializer hypothesis in the target domain, and thereafter updates this hypothesis in an online fashion using actively acquired labels as is done in (Cesa-Bianchi et al., 2006). This amounts to using w0T = v0 in Algorithm 1. We refer to this algorithm as Active Online Domain Adaptation (AODA). It can be shown that the modified algorithm (AODA) yields smaller mistake bound and smaller label complexity than the CBGZ algorithm. We skip the proo</context>
<context position="11026" citStr="Cesa-Bianchi et al. (2006)" startWordPosition="1839" endWordPosition="1842">d using any other unsupervised domain adaptation technique such as Structural Correspondence Learning (SCL) (Blitzer et al., 2006). We compare all the approaches based on classification accuracies achieved for a given budget of labeled target examples (Section-5.2), and number of labels requested for a fixed pool of unlabeled target examples and corresponding accuracies wds DS w DT 29 Method Summary Active ? ID In-domain (DT ) data No SDA UDA followed by passively chosen labeled target data No FEDA Frustratingly Easy Domain Adaptation Daum´e III (2007) No ZIAL Zero initialized active learning Cesa-Bianchi et al. (2006) Yes RIAL Randomly initialized active learning with fixed label budget Yes SIAL Source hypothesis initialized active learning Yes AODA UDA based source hypothesis initialized active learning Yes Table 1: Description of the methods compared (Section-5.3). We use the vanilla Perceptron as the base classifier of each of the algorithms and each experiment has been averaged over 20 runs corresponding to random data order permutations. 5.1 Datasets We report our empirical results for the task of sentiment classification using data provided by (Blitzer et al., 2007b) which consists of user reviews of</context>
</contexts>
<marker>Cesa-Bianchi, Gentile, Zaniboni, 2006</marker>
<rawString>Cesa-Bianchi, Nicol`o, Gentile, Claudio, and Zaniboni, Luca. Worst-Case Analysis of Selective Sampling for Linear Classification. JMLR, 7, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
</authors>
<title>Domain adaptation with active learning for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<contexts>
<context position="18346" citStr="Chan &amp; Ng, 2007" startWordPosition="2987" endWordPosition="2990">in adaptation technique (naive instance weighting) we have used here for the first stage. In some cases, the weights estimated using instance weighting may not be accurate and the bias in importance weight estimation is potentially the reason behind AODA not doing better than SIAL in such cases. As mentioned earlier, however, any other unsupervised domain adaptation technique can be used here and, in general, AODA is expected to perform better than SIAL. 6 Related Work Active learning in a domain adaptation setting has received little attention so far. One interesting setting was proposed in (Chan &amp; Ng, 2007) where they apply active learning for word sense disambiguation in a domain adaptation setting. Their active learning setting is pool-based whereas ours is a streaming (online) setting. Furthermore, our second algorithm also uses the domain separator hypothesis to rule out querying the labels of target examples similar to the source. A combination of transfer learning with active learning has been presented in (Shi et al., 2008). One drawback of their approach is the requirement of an initial pool of labeled target domain data used to train an in-domain classifier. Without this in-domain class</context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Chan, Y. S. and Ng, H. T. Domain adaptation with active learning for word sense disambiguation. In ACL, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Cohn</author>
<author>Les Atlas</author>
<author>Richard Ladner</author>
</authors>
<title>Improving Generalization with Active Learning.</title>
<date>1994</date>
<booktitle>Machine Learning,</booktitle>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="1280" citStr="Cohn et al., 1994" startWordPosition="192" endWordPosition="195">on to selectively query the most informative points in the target domain, leading to further reductions in label complexity. Experimental results on a variety of datasets establish the efficacy of the proposed methods. 1 Introduction Acquiring labeled data to train supervised learning models can be difficult or expensive in many problem domains. Active Learning tries to circumvent this difficultly by only querying the labels of the most informative examples and, in several cases, has been shown to achieve exponentially lower labelcomplexity (number of queried labels) than supervised learning (Cohn et al., 1994). Domain Adaptation (Daum´e &amp; Marcu, 2006), although motivated somewhat differently, attempts to address a seemingly similar problem: lack of labeled data in some target domain. Domain Adaptation deals with this problem using labeled data from a different (but related) source domain. In this paper, we consider the supervised domain adaptation setting (Finkel &amp; Manning, 2009; Daum´e III, 2007) having a large amount of labeled data from a source domain, a large amount of unlabeled data from a target domain, and additionally a small budget for acquiring labels in the target domain. We show how, a</context>
</contexts>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>Cohn, David, Atlas, Les, and Ladner, Richard. Improving Generalization with Active Learning. Machine Learning, 15(2), 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>26</volume>
<issue>1</issue>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Daum´e, III, Hal and Marcu, Daniel. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research, 26(1), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
</authors>
<title>Frustratingly Easy Domain Adaptation. In</title>
<date>2007</date>
<booktitle>ACL,</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Daum´e III, H. Frustratingly Easy Domain Adaptation. In ACL, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Hierarchical Bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In NAACL,</booktitle>
<pages>602--610</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="1656" citStr="Finkel &amp; Manning, 2009" startWordPosition="250" endWordPosition="253"> circumvent this difficultly by only querying the labels of the most informative examples and, in several cases, has been shown to achieve exponentially lower labelcomplexity (number of queried labels) than supervised learning (Cohn et al., 1994). Domain Adaptation (Daum´e &amp; Marcu, 2006), although motivated somewhat differently, attempts to address a seemingly similar problem: lack of labeled data in some target domain. Domain Adaptation deals with this problem using labeled data from a different (but related) source domain. In this paper, we consider the supervised domain adaptation setting (Finkel &amp; Manning, 2009; Daum´e III, 2007) having a large amount of labeled data from a source domain, a large amount of unlabeled data from a target domain, and additionally a small budget for acquiring labels in the target domain. We show how, apart from leveraging information in the usual domain adaptation sense, the information from the source domain can be leveraged to intelligently query labels in the target domain.We achieve this by first training the best possible classifier without using target domain labeled data 1 and then using the learned classifier to leverage the inter-domain information when we are a</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Finkel, Jenny Rose and Manning, Christopher D. Hierarchical Bayesian domain adaptation. In NAACL, pp. 602–610, Morristown, NJ, USA, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Settles</author>
</authors>
<title>Active Learning Literature Survey. In Computer Sciences</title>
<date>2009</date>
<tech>Technical Report 1648,</tech>
<institution>University of Wisconsin-Madison,</institution>
<contexts>
<context position="2385" citStr="Settles, 2009" startWordPosition="368" endWordPosition="369">om a target domain, and additionally a small budget for acquiring labels in the target domain. We show how, apart from leveraging information in the usual domain adaptation sense, the information from the source domain can be leveraged to intelligently query labels in the target domain.We achieve this by first training the best possible classifier without using target domain labeled data 1 and then using the learned classifier to leverage the inter-domain information when we are additionally provided some fixed budget for acquiring extra labeled target data (i.e., the active learning setting (Settles, 2009)). There are several ways in which our “best classifier” can be utilized. Our first approach uses this classifier as the initializer while doing (online) active learning in the target domain (Section 3). Then we present a variant augmenting the first approach using a domain-separator hypothesis which leads to additionally ruling out querying the labels of those target examples that appear “similar” to the source domain (Section 4). Figure 1 shows our basic setup which uses a source (or unsupervised domain-adapted source) classifier vo as an initializer for doing active learning in the target d</context>
</contexts>
<marker>Settles, 2009</marker>
<rawString>Settles, B. Active Learning Literature Survey. In Computer Sciences Technical Report 1648, University of Wisconsin-Madison, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoxiao Shi</author>
<author>Wei Fan</author>
<author>Jiangtao Ren</author>
</authors>
<title>Actively Transfer Domain Knowledge.</title>
<date>2008</date>
<booktitle>In ECML/PKDD (2),</booktitle>
<contexts>
<context position="18778" citStr="Shi et al., 2008" startWordPosition="3057" endWordPosition="3060">o perform better than SIAL. 6 Related Work Active learning in a domain adaptation setting has received little attention so far. One interesting setting was proposed in (Chan &amp; Ng, 2007) where they apply active learning for word sense disambiguation in a domain adaptation setting. Their active learning setting is pool-based whereas ours is a streaming (online) setting. Furthermore, our second algorithm also uses the domain separator hypothesis to rule out querying the labels of target examples similar to the source. A combination of transfer learning with active learning has been presented in (Shi et al., 2008). One drawback of their approach is the requirement of an initial pool of labeled target domain data used to train an in-domain classifier. Without this in-domain classifier, no transfer learning is possible in their setting. 7 Discussion There are several interesting variants of our approach that can worth investigating. For instance, one can use a hybrid oracle setting where the source classifier vo could be used as an oracle that provides labels for free, whenever it is reasonably highly confident about its prediction (maybe in terms of its relative confidence as compared to the actual clas</context>
</contexts>
<marker>Shi, Fan, Ren, 2008</marker>
<rawString>Shi, Xiaoxiao, Fan, Wei, and Ren, Jiangtao. Actively Transfer Domain Knowledge. In ECML/PKDD (2), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sugiyama</author>
<author>S Nakajima</author>
<author>H Kashima</author>
<author>P von B¨unau</author>
<author>M Kawanabe</author>
</authors>
<title>Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation. In</title>
<date>2007</date>
<booktitle>NIPS,</booktitle>
<marker>Sugiyama, Nakajima, Kashima, von B¨unau, Kawanabe, 2007</marker>
<rawString>Sugiyama, M., Nakajima, S., Kashima, H., von B¨unau, P., and Kawanabe, M. Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation. In NIPS, 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>