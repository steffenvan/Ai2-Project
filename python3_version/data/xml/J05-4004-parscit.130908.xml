<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99584">
Induction of Word and Phrase Alignments
for Automatic Document Summarization
</title>
<author confidence="0.999486">
Hal Daum´e III* Daniel Marcu*
</author>
<affiliation confidence="0.997152">
Information Sciences Institute Information Sciences Institute
University of Southern California University of Southern California
</affiliation>
<bodyText confidence="0.99603395">
Current research in automatic single-document summarization is dominated by two effective,
yet naive approaches: summarization by sentence extraction and headline generation via bag-
of-words models. While successful in some tasks, neither of these models is able to adequately
capture the large set of linguistic devices utilized by humans when they produce summaries.
One possible explanation for the widespread use of these models is that good techniques have
been developed to extract appropriate training data for them from existing document/abstract
and document/headline corpora. We believe that future progress in automatic summarization
will be driven both by the development of more sophisticated, linguistically informed models,
as well as a more effective leveraging of document/abstract corpora. In order to open the doors
to simultaneously achieving both of these goals, we have developed techniques for automatically
producing word-to-word and phrase-to-phrase alignments between documents and their human-
written abstracts. These alignments make explicit the correspondences that exist in such docu-
ment/abstract pairs and create a potentially rich data source from which complex summarization
algorithms may learn. This paper describes experiments we have carried out to analyze the ability
of humans to perform such alignments, and based on these analyses, we describe experiments for
creating them automatically. Our model for the alignment task is based on an extension of the
standard hidden Markov model and learns to create alignments in a completely unsupervised
fashion. We describe our model in detail and present experimental results that show that our
model is able to learn to reliably identify word- and phrase-level alignments in a corpus of
(document, abstract) pairs.
</bodyText>
<sectionHeader confidence="0.961899" genericHeader="abstract">
1. Introduction and Motivation
</sectionHeader>
<subsectionHeader confidence="0.710283">
1.1 Motivation
</subsectionHeader>
<bodyText confidence="0.9998362">
We believe that future success in automatic document summarization will be made
possible by the combination of complex, linguistically motivated models and effective
leveraging of data. Current research in summarization makes a choice between these
two: one either develops sophisticated, domain-specific models that are subsequently
hand-tuned without the aid of data, or one develops naive general models that can
</bodyText>
<note confidence="0.8258918">
* 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292. Email: {hdaume,marcu}@isi.edu.
Submission received: 12 January 2005; revised submission received: 3 May 2005; accepted for
publication: 27 May 2005.
© 2006 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 4
</note>
<figureCaption confidence="0.969652">
Figure 1
</figureCaption>
<subsectionHeader confidence="0.51407">
Example alignment of a single abstract sentence with two document sentences.
</subsectionHeader>
<bodyText confidence="0.999939578947369">
be trained on large amounts of data (in the form of corpora of document/extract or
document/headline pairs). One reason for this is that currently available technologies
are only able to extract very coarse and superficial information that is inadequate
for training complex models. In this article, we propose a method to overcome this
problem: automatically generating word-to-word and phrase-to-phrase alignments
between documents and their human-written abstracts.1
To facilitate discussion and to motivate the problem, we show in Figure 1 a rela-
tively simple alignment between a document fragment and its corresponding abstract
fragment from our corpus.2 In this example, a single abstract sentence (shown along
the top of the figure) corresponds to exactly two document sentences (shown along the
bottom of the figure). If we are able to automatically generate such alignments, one can
envision the development of models of summarization that take into account effects
of word choice, phrasal and sentence reordering, and content selection. Such models
could be simultaneously linguistically motivated and data-driven. Furthermore, such
alignments are potentially useful for current-day summarization techniques, including
sentence extraction, headline generation, and document compression.
A close examination of the alignment shown in Figure 1 leads us to three obser-
vations about the nature of the relationship between a document and its abstract, and
hence about the alignment itself:
</bodyText>
<listItem confidence="0.9846034">
• Alignments can occur at the granularity of words and of phrases.
• The ordering of phrases in an abstract can be different from the ordering of
phrases in the document.
• Some abstract words do not have direct correspondents in the document,
and many document words are never used in an abstract.
</listItem>
<bodyText confidence="0.999927">
In order to develop an alignment model that could recreate such an alignment, we
need our model to be able to operate both at the word level and at the phrase level, we
need it to be able to allow arbitrary reorderings, and we need it to be able to account
for words on both the document and abstract side that have no direct correspondence. In
this paper, we develop an alignment model that is capable of learning all these aspects
of the alignment problem in a completely unsupervised fashion.
</bodyText>
<footnote confidence="0.9991145">
1 We will use the words abstract and summary interchangeably. When we wish to emphasize that a
particular summary is extractive, we will refer to it as an extract.
2 As part of the tokenization step, any possessive form is split off from its noun, and represented as POSS.
In most cases, this involves separating an “’s” and replacing it with POSS, as in “John’s” → “John POSS.”
For consistency, we have treated “it’s”/“its” as a special case: “its” (the possessive) is converted to
“it POSS.”
</footnote>
<page confidence="0.987215">
506
</page>
<note confidence="0.947095">
Daum´e and Marcu Alignments for Automatic Document Summarization
</note>
<subsectionHeader confidence="0.808221">
1.2 Shortcomings of Current Summarization Models
</subsectionHeader>
<bodyText confidence="0.991500864864865">
Current state-of-the-art automatic single-document summarization systems employ
one of three techniques: sentence extraction, bag-of-words headline generation, or
document compression. Sentence extraction systems take full sentences from a doc-
ument and concatenate them to form a summary. Research in sentence extraction
can be traced back to work in the mid 1950s and late 1960s by Luhn (1956) and
Edmundson (1969). Recent techniques are startlingly not terribly divergent from these
original methods; see Mani and Maybury (1999); Marcu (2000); Mani (2001) for a com-
prehensive overview. Headline generation systems, on the other hand, typically extract
individual words from a document to produce a very short headline-style summary;
see Banko, Mittal, and Witbrock (2000); Berger and Mittal (2000); Schwartz, Zajic, and
Dorr (2002) for representative examples. Between these two extremes, there has been
a relatively modest amount of work in sentence simplification (Chandrasekar, Doran,
and Bangalore 1996; Mahesh 1997; Carroll et al. 1998; Grefenstette 1998; Jing 2000;
Knight and Marcu 2002) and document compression (Daum´e III and Marcu 2002;
Daum´e III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases,
and sentences are selected in an extraction process.
While such approaches have enjoyed some success, they all suffer from modeling
shortcomings. Sentence extraction systems and document compression models make
unrealistic assumptions about the summarization task (namely, that extraction is suffi-
cient and that sentences are the appropriate level of granularity). Headline generation
systems employ very weak models that make an incorrect bag-of-words assumption.
This assumption allows such systems to learn limited transformations to produce
headlines from arbitrary documents, but such transformations are not nearly complex
enough to adequately model anything beyond indicative summaries at a length of
around 10 words. Bag-of-words models can learn what the most important words to
keep in a headline are, but say nothing about how to structure them in a well-formed,
grammatical headline.
In our own work on document compression models (Daum´e III and Marcu 2002;
Daum´e III and Marcu 2004), both of which extend the sentence compression model of
Knight and Marcu (2002), we assume that sentences and documents can be summa-
rized exclusively through deletion of contiguous text segments. In Knight and Marcu’s
data, we found that from a corpus of 39,060 abstract sentences, only 1,067 were created
from corresponding document sentences via deletion of contiguous segments. In other
words, only 2.7% of the sentences in real (document, abstract) pairs can be explained
by the model proposed by Knight and Marcu (2002). Such document compression
models do not explain the rich set of linguistic devices employed, for example, in
Figure 1.
</bodyText>
<subsectionHeader confidence="0.717211">
1.3 Prior Work on Alignments
</subsectionHeader>
<bodyText confidence="0.999953875">
In the sentence extraction community, there exists a wide variety of techniques for
(essentially) creating alignments between document sentences and abstract sentences
(Kupiec, Pedersen, and Chen 1995; Teufel and Moens 1997; Marcu 1999); see also
Barzilay and Elhadad (2003); Quirk, Brockett, and Dolan (2004) for work describing
alignments for the monolingual paraphrasing task. These techniques typically take
into account information such as lexical overlap, synonymy, ordering, length, dis-
course structure, and so forth. The sentence alignment problem is a comparatively
simple problem to solve, and current approaches work quite well. Unfortunately, these
</bodyText>
<page confidence="0.988252">
507
</page>
<note confidence="0.802375">
Computational Linguistics Volume 31, Number 4
</note>
<bodyText confidence="0.999813925925926">
alignments are the least useful, because they can only be used to train sentence ex-
traction systems.
In the context of headline generation, simple statistical models are used for aligning
documents and headlines (Banko, Mittal, and Witbrock 2000; Berger and Mittal 2000;
Schwartz, Zajic, and Dorr 2002), based on IBM Model 1 (Brown et al. 1993). These
models treat documents and headlines as simple bags of words and learn probabilistic
word-based mappings between the words in the documents and the words in the
headlines. Such mappings can be considered word-to-word alignments, but as our
results show (see Section 5), these models are too weak for capturing the sophisticated
operations that are employed by humans in summarizing texts.
To date, there has been very little work on the word alignment task in the context
of summarization. The most relevant work is that of Jing (2002), in which a hidden
Markov alignment model is applied to the task of identifying word and phrase-level
correspondences between documents and abstracts. Unfortunately, this model is only
able to align words that are identical up to their stems, and thus suffers from a problem
of recall. This also makes it ill-suited to the task of learning how to perform abstraction,
in which one would desire to know how words get changed. For example, Jing’s model
cannot identify any of the following alignments from Figure 1: (Connecting Point &lt;-4
Connecting Point Systems), (Mac &lt;-4 Macintosh), (retailer &lt;-4 seller), (Macintosh &lt;-4 Apple
Macintosh systems) and (January 1989 &lt;-4 last January).
Word alignment (and, to a lesser degree, phrase alignment) has been an active
topic of research in the machine translation community. Based on these efforts, one
might be initially tempted to use readily available alignment models developed in the
context of machine translation, such as GIZA++ (Och and Ney 2003), to obtain word-
level alignments in (document, abstract) corpora. However, as we will show (Section 5),
the alignments produced by such a system are inadequate for the (document, abstract)
alignment task.
</bodyText>
<subsectionHeader confidence="0.997747">
1.4 Article Structure
</subsectionHeader>
<bodyText confidence="0.99994205">
In this article, we describe a novel, general model for automatically inducing word-
and phrase-level alignments between documents and their human-written abstracts.
Beginning in Section 2, we will describe the results of human annotation of such align-
ments. Based on this annotation, we will investigate the empirical linguistic properties
of such alignments, including lexical transformations and movement. In Section 3, we
will introduce the statistical model we use for deriving such alignments automatically.
The inference techniques are based on those of semi-Markov models, extensions of
hidden Markov models that allow for multiple simultaneous observations.
After our discussion of the model structure and algorithms, we discuss the various
parameterizations we employ in Section 4. In particular, we discuss three distinct mod-
els of movement, two of which are well-known in the machine translation alignment
literature, and a third one that exploits syntax in a novel, “light” manner. We also discuss
several models of lexical rewriting, based on identities, stems, WordNet synonymy,
and automatically induced lexical replacements. In Section 5, we present experimental
results that confirm that our model is able to learn the hidden structure in our corpus
of (document, abstract) pairs. We compare our model against well-known alignment
models designed for machine translation as well as a state-of-the-art alignment model
specifically designed for summarization (Jing 2002). Additionally, we discuss errors that
the model currently makes, supported by some relevant examples and statistics. We
conclude with some directions for future research (Section 6).
</bodyText>
<page confidence="0.997275">
508
</page>
<note confidence="0.796601">
Daum´e and Marcu Alignments for Automatic Document Summarization
</note>
<tableCaption confidence="0.999007">
Table 1
</tableCaption>
<table confidence="0.989035636363636">
Ziff-Davis corpus statistics.
Sub-corpus Annotated
Abstracts Documents Abstracts Documents
Documents 2033 45
Sentences 13k 82k 244 2k
Words 261k 2.5M 6.4k 49k
Unique words 14k 42k 1.9k 5.9k
45k 6k
Sentences/Doc 6.28 40.83 5.42 45.3
Words/Doc 128.52 1229.71 142.33 1986.16
Words/Sent 20.47 28.36 26.25 24.20
</table>
<sectionHeader confidence="0.992539" genericHeader="method">
2. Human-produced Alignments
</sectionHeader>
<bodyText confidence="0.99997225">
In order to decide how to design an alignment model and to judge the quality of the
alignments produced by a system, we first need to create a set of “gold standard”
alignments. To this end, we asked two human annotators to manually construct
such alignments between documents and their abstracts. These (document, abstract)
pairs were drawn from the Ziff-Davis collection (Marcu 1999). Of the roughly 7,000
documents in that corpus, we randomly selected 45 pairs for annotation. We added to
this set of 45 pairs the 2,000 shorter documents from this collection, and all the work
described in the remainder of this paper focuses on this subset of 2,033 (document,
abstract) pairs.3 Statistics for this sub-corpus and for the pairs selected for annotation
are shown in Table 1. As can be simply computed from this table, the compression rate
in this corpus is about 12%. The first five human-produced alignments were completed
separately and then discussed; the last 40 were done independently.
</bodyText>
<subsectionHeader confidence="0.997634">
2.1 Annotation Guidelines
</subsectionHeader>
<bodyText confidence="0.999930571428571">
Annotators were asked to perform word-to-word and phrase-to-phrase alignments
between abstracts and documents, and to classify each alignment as either possible (P)
or sure (S), where S C_ P, following the methodology used in the machine translation
community (Och and Ney 2003). The direction of containment (S C_ P) is because being
a sure alignment is a stronger requirement than being a possible alignment. A full descrip-
tion of the annotation guidelines is available in a document available with the alignment
software on the first author’s web site (http://www.isi.edu/—hdaume/HandAlign).
Here, we summarize the main points.
The most important instruction that annotators were given was to align everything
in the summary to something. This was not always possible, as we will discuss shortly,
but by and large it was an appropriate heuristic. The second major instruction was to
choose alignments with maximal consecutive length: If there are two possible alignments
for a phrase, the annotators were instructed to choose the one that will result in the
longest consecutive alignment. For example, in Figure 1, this rule governs the choice of
</bodyText>
<footnote confidence="0.978264666666667">
3 The reason there are 2,033 pairs, not 2,045, is that 12 of the original 45 pairs were among the 2,000
shortest, so the 2,033 pairs are obtained by taking the 2,000 shortest and adding to them the 33 pairs that
were annotated and not already among the 2,000 shortest.
</footnote>
<page confidence="0.98416">
509
</page>
<note confidence="0.300111">
Computational Linguistics Volume 31, Number 4
</note>
<bodyText confidence="0.999845625">
the alignment of the word Macintosh on the summary side: lexically, it could be aligned
to the final occurrence of the word Macintosh on the document side, but by aligning it to
Apple Macintosh systems, we are able to achieve a longer consecutive sequence of aligned
words.
The remainder of the instructions have to do primarily with clarifying particular
linguistic phenomena including punctuation, anaphora (for entities, annotators are told
to feel free to align names to pronouns, for instance) and metonymy, null elements,
genitives, appositives, and ellipsis.
</bodyText>
<subsectionHeader confidence="0.99894">
2.2 Annotator Agreement
</subsectionHeader>
<bodyText confidence="0.999922473684211">
To compute annotator agreement, we employed the kappa statistic. To do so, we treat
the problem as a sequence of binary decisions: given a single summary word and
document word, should the two be aligned? To account for phrase-to-phrase align-
ments, we first converted these into word-to-word alignments using the “all pairs”
heuristic. By looking at all such pairs, we wound up with 7.2 million items over which
to compute the kappa statistic (with two annotators and two categories). Annotator
agreement was strong for sure alignments and fairly weak for possible alignments.
When considering only sure alignments, the kappa statistic for agreement was 0.63
(though it dropped drastically to 0.42 on possible alignments).
In performing the annotation, we found that punctuation and non-content words
are often very difficult to align (despite the discussion of these issues in the alignment
guidelines). The primary difficulty with function words is that when the summarizers
have chosen to reorder words to use slightly different syntactic structures, there are
lexical changes that are hard to predict.4 Fortunately, for many summarization tasks,
it is much more important to get content words right, rather than function words.
When words on a stop list of 58 function words and punctuation were ignored, the
kappa value rose to 0.68. Carletta (1995) has suggested that kappa values over 0.80
reflect very strong agreement and that kappa values between 0.60 and 0.80 reflect good
agreement.5
</bodyText>
<subsectionHeader confidence="0.99714">
2.3 Results of Annotation
</subsectionHeader>
<bodyText confidence="0.9997497">
After the completion of these alignments, we can investigate some of their properties.
Such an investigation is interesting both from the perspective of designing a model and
from a linguistic perspective.
In the alignments, we found that roughly 16% of the abstract words are left un-
aligned. This figure includes both standard lexical words and punctuation. Of this
16%, 4% are punctuation marks (though not all punctuation is unaligned) and 7%
are function words. The remaining 5% are words that would typically be considered
content words. This rather surprising result tells us that any model we build needs
to be able to account for a reasonable portion of the abstract to not have a direct
correspondence to any portion of the document.
</bodyText>
<footnote confidence="0.992957">
4 For example, the change from I gave a gift to the boy. to The boy received a gift from me. is relatively
straightforward; however, it is a matter of opinion whether to and from should be aligned – they serve the
same role, but certainly do not mean the same thing.
5 All annotator agreement figures are calculated only on the last 40 (document, abstract) pairs, which were
annotated independently.
</footnote>
<page confidence="0.989772">
510
</page>
<note confidence="0.602333">
Daum´e and Marcu Alignments for Automatic Document Summarization
</note>
<bodyText confidence="0.999918482758621">
To get a sense of the importance of producing alignments at the phrase level, we
computed that roughly 75% of the alignments produced by humans involve only one
word on both sides. In 80% of the alignments, the summary side is a single word (thus
in 5% of the cases, a single summary word is aligned to more than one document
word). In 6.1% of the alignments, the summary side involved a phrase of length two,
and in 2.2% of the cases it involved a phrase of length three. In all these numbers, care
should be taken to note that the humans were instructed to produce phrase alignments
only when word alignments were impossible. Thus, it is entirely likely that summary
word i is aligned to document word j and summary word i + 1 is aligned to document
word j + 1, in which case we count this as two singleton alignments, rather than an
alignment of length two. These numbers suggest that looking at phrases in addition to
words is empirically important.
Lexical choice is another important aspect of the alignment process. Of all the
aligned summary words and phrases, the corresponding document word or phrase was
exactly the same as that on the summary side in 51% of the cases. When this constraint
was weakened to looking only at stems (for multi-word phrases, a match meant that
each corresponding word matched up to stem), this number rose to 67%. When broken
down into cases of singletons and non-singletons, we saw that 86% of singletons are
identical up to stem, and 48% of phrases are identical up to stem. This suggests that
looking at stems, rather than lexical items, is useful.
Finally, we investigated the issue of adjacency in the alignments. Specifically, we
consider the following question: Given that a summary phrase ending at position i
is aligned to a document phrase ending at position j, what is a likely position in the
document for the summary phrase beginning at position i + 1? It turns out that this
is overwhelmingly j + 1. In Figure 2, we have plotted the frequencies of such relative
jumps over the human-aligned data. This graph suggests that a model biased toward
stepping forward monotonically in the document is likely to be appropriate. However,
it should also be noted that backward jumps are also quite common, suggesting that a
monotonic alignment model is inappropriate for this task.
</bodyText>
<sectionHeader confidence="0.989347" genericHeader="method">
3. Statistical Alignment Model
</sectionHeader>
<bodyText confidence="0.999865777777778">
Based on linguistic observations from the previous section, we reach several conclu-
sions regarding the development of a statistical model to produce such alignments.
First, the model should be able to produce alignments between phrases of arbitrary
length (but perhaps with a bias toward single words). Second, it should not be con-
strained by any assumptions of monotonicity or word (or stem) identity, but it might
be able to realize that monotonicity and word and stem identity are good indicators of
alignment. Third, our model must be able to account for words on the abstract side
that have no correspondence on the document side (following the terminology from the
machine translation community, we will refer to such words as null generated).
</bodyText>
<subsectionHeader confidence="0.99513">
3.1 Generative Story
</subsectionHeader>
<bodyText confidence="0.9994285">
Based on these observations, and with an eye toward computational tractability, we
posit the following generative story for how a summary is produced, given a document:
</bodyText>
<listItem confidence="0.9860215">
1. Repeat until the whole summary is generated:
(a) Choose a document position j and jump there.
</listItem>
<page confidence="0.986258">
511
</page>
<figure confidence="0.868926">
Computational Linguistics Volume 31, Number 4
</figure>
<figureCaption confidence="0.995141">
Figure 2
</figureCaption>
<bodyText confidence="0.9710185">
Analysis of the motion observed in documents when considering a movement of +1 on the
summary side.
</bodyText>
<listItem confidence="0.99923325">
(b) Choose a document phrase length l.
(c) Generate a summary phrase based on the document phrase
spanning positions j to j + l.
2. Jump to the end of the document.
</listItem>
<bodyText confidence="0.999847428571428">
In order to account for null generated summary words, we augment the above gen-
erative story with the option to jump to a specifically designated null state from which a
summary phrase may be generated without any correspondence in the document. From
inspection of the human-aligned data, most such null generated words are function
words or punctuation; however, in some cases, there are pieces of information in the
summary that truly did not exist in the original document. The null generated words
can account for these as well (additionally, the null generated words allow the model
to “give up” when it cannot do anything better). We require that summary phrases
produced from the null state have length 1, so that in order to generate multiple null
generated words, they must be generated independently.
In Figure 3, we have shown a portion of the generative process that would give rise
to the alignment in Figure 1.
This generative story implicitly induces an alignment between the document and
the summary: the summary phrase is considered to be aligned to the document phrase
</bodyText>
<page confidence="0.977051">
512
</page>
<figure confidence="0.501797">
Daum´e and Marcu Alignments for Automatic Document Summarization
</figure>
<figureCaption confidence="0.914841">
Figure 3
</figureCaption>
<bodyText confidence="0.966546">
Beginning and end of the generative process that gave rise to the alignment in Figure 1, which is
reproduced here for convenience.
</bodyText>
<listItem confidence="0.918599333333333">
that “generated” it. In order to make this computationally tractable, we must introduce
some conditional independence assumptions. Specifically, we assume the following:
1. Decision (a) in our generative story depends only on the position of the
end of the current document phrase (i.e., j + l).
2. Decision (b) is conditionally independent of every other decision.
3. Decision (c) depends only on the phrase at the current document position.
</listItem>
<subsectionHeader confidence="0.995587">
3.2 Statistical Model
</subsectionHeader>
<bodyText confidence="0.9986265">
Based on the generative story and independence assumptions described above, we can
model the entire summary generation process according to two distributions:
</bodyText>
<listItem confidence="0.9344134">
• jump(j&apos; I j + l), the probability of jumping to position j&apos; in the document
when the previous phrase ended at position j + l.
• rewrite(s I dj:j+l), the rewrite probability of generating summary phrase s
given that we are considering the sub-phrase of d beginning at position j
and ending at position j + l.
</listItem>
<bodyText confidence="0.999362266666667">
Specific parameterizations of the distributions jump and rewrite will be discussed in
Section 4 to enable the focus here to be on the more general problems of inference and
decoding in such a model. The model described by these independence assumptions
very much resembles that of a hidden Markov model (HMM), where states in the state
space are document ranges and emissions are summary words. The difference is that
instead of generating a single word in each transition between states, an entire phrase
is generated. This difference is captured by the semi-Markov model or segmental HMM
framework, described in great detail by Ostendorf, Digalakis, and Kimball (1996); see
also Ferguson (1980); Gales and Young (1993); Mitchell, Jamieson, and Harper (1995);
Smyth, Heckerman, and Jordan (1997); Ge and Smyth (2000); Aydin, Altunbasak, and
Borodovsky (2004) for more detailed descriptions of these models as well as other ap-
plications in speech processing and computational biology. In the following subsections,
we will briefly discuss the aspects of inference that are relevant to our problem, but
the interested reader is directed to Ostendorf, Digalakis, and Kimball (1996) for more
details.
</bodyText>
<subsectionHeader confidence="0.999361">
3.3 Creating the State Space
</subsectionHeader>
<bodyText confidence="0.999406">
Given our generative story, we can construct a semi-HMM to calculate precisely the
alignment probabilities specified by our model in an efficient manner. A semi-HMM is
</bodyText>
<page confidence="0.977338">
513
</page>
<note confidence="0.286426">
Computational Linguistics Volume 31, Number 4
</note>
<bodyText confidence="0.999892894736842">
fully defined by a state space (with designated start and end states), an output alpha-
bet, transition probabilities, and observation probabilities. The semi-HMM functions
like an HMM: Beginning at the start state, stochastic transitions are made through the
state space according to the transition probabilities. At each step, one or more observa-
tions are generated. The machine stops when it reaches the end state.
In our case, the state set is large, but well structured. There is a unique initial state
(start), a unique final state (end), and a state for each possible document phrase. That
is, for a document of length n, for all 1 &lt; i &lt; i&apos; &lt; n, there is a state that corresponds
to the document phrase beginning at position i and ending at position i&apos;, which we
will refer to as ri,i,. There is also a null state for each document position ro,i. Thus, S =
{(start), (end)} U {ri,i, : 1 &lt; i &lt; i&apos; &lt; n} U {ro,i : 1 &lt; i &lt; n}. The output alphabet consists
of each word found in S, plus the end-of-sentence word ω. We only allow the word ω
to be emitted on a transition to the end state. The transition probabilities are managed
by the jump model, and the emission probabilities are managed by the rewrite model.
Consider the document a b (the semi-HMM for which is shown in Figure 4) in
the case when the corresponding summary is c d. Suppose the correct alignment is
that c d is aligned to a and b is left unaligned. Then, the path taken through the
semi-HMM is (start) --� a --� (end). During the transition (start) --� a, c d is emitted.
During the transition a --� (end), ω is emitted.
</bodyText>
<subsectionHeader confidence="0.976826">
3.4 Expectation Maximization
</subsectionHeader>
<bodyText confidence="0.9994124">
The alignment task, as described above, is a chicken-and-egg problem: if we knew
the model components (namely, the rewrite and jump tables), we would be able to
efficiently find the best alignment. Similarly, if we knew the correct alignments, we
would be able to estimate the model components. Unfortunately, we have neither.
Expectation maximization is a general technique for learning in such chicken-and-egg
situations (Dempster, Laird, and Rubin 1977; Boyles 1983; Wu 1983). The basic idea is to
make a guess at the alignments, and then use this guess to estimate the parameters for
the relevant distributions. We can use these re-estimated distributions to make a better
guess at the alignments, and then use these (ideally better) alignments to re-estimate
the parameters.
</bodyText>
<figureCaption confidence="0.637545">
Figure 4
</figureCaption>
<bodyText confidence="0.737827">
Schematic drawing of the semi-HMM (with some transition probabilities) for the document a b.
</bodyText>
<page confidence="0.947204">
514
</page>
<note confidence="0.540513">
Daum´e and Marcu Alignments for Automatic Document Summarization
</note>
<bodyText confidence="0.9998385">
Formally, the EM family of algorithms tightly bound the log of an expectation of
a function by the expectation of the log of that function, through the use of Jensen’s
inequality (Jensen 1906). The tightness of the bound means that when we attempt to
estimate the model parameters, we may do so over expected alignments, rather than the
true (but unknown) alignments. EM gives formal guarantees of convergence, but is only
guaranteed to find local maxima.
</bodyText>
<subsectionHeader confidence="0.988623">
3.5 Model Inference
</subsectionHeader>
<bodyText confidence="0.974228">
All the inference techniques utilized in this paper are standard applications of semi-
Markov model techniques. The relevant equations are summarized in Figure 5 and
described here. In all these equations, the variables t and t&apos; range over phrases in the
summary (specifically, the phrase st:t,), and the variables i and j range over phrases in the
document. The interested reader is directed to Ostendorf, Digalakis, and Kimball (1996)
for more details on the generic form of these models and their inference techniques.
Unfortunately, the number of possible alignments for a given (document,
summary) pair is exponential in the length of the summary. This would make a naive
implementation of the computation of p(s  |d) intractable without a more clever solu-
tion. Instead, we are able to employ a variant of the forward algorithm to compute these
probabilities recursively. The basic idea is to compute the probability of generating a
prefix of the summary and ending up at a particular position in the document (this is
known as the forward probability). Since our independence assumptions tell us that
it does not matter how we got to this position, we can use this forward probability
to compute the probability of taking one more step in the summary. At the end, the
desired probability p(s  |d) is simply the forward probability of reaching the end of
the summary and document simultaneously. The forward probabilities are calculated
in the α table in Figure 5. This equation essentially says that the probability of emitting
the first t − 1 words of the summary and ending at position j in the document can
be computed by summing over our previous position (t&apos;) and previous state (i) and
multiplying the probability of getting there (αi(t&apos; + 1)) with the probability of moving
from there to the current position.
Figure 5
Summary of inference equations for a semi-Markov model.
</bodyText>
<page confidence="0.971738">
515
</page>
<note confidence="0.284893">
Computational Linguistics Volume 31, Number 4
</note>
<bodyText confidence="0.999403806451613">
The second standard inference problem is the calculation of the best alignment: the
Viterbi alignment. This alignment can be computed in exactly the same fashion as the
forward algorithm, with two small changes. First, the forward probabilities implicitly
include a sum over all previous states, whereas the Viterbi probabilities replace this
with a max operator. Second, in order to recover the actual Viterbi alignment, we keep
track of which previous state this max operator chose. This is computed by filling out
the Z table from Figure 5. This is almost identical to the computation of the forward
probabilities, except that instead of summing over all possible t&apos; and i, we take the
maximum over those variables.
The final inference problem is parameter re-estimation. In the case of standard
HMMs, this is known as the Baum-Welch, Baum-Eagon or Forward-Backward algo-
rithm (Baum and Petrie 1966; Baum and Eagon 1967). By introducing backward proba-
bilities analogous to the forward probabilities, we can compute alignment probabilities
of suffixes of the summary. The backward table is the β table in Figure 5, which is
analogous to the α table, except that the computation proceeds from the end to the start.
By combining the forward and backward probabilities, we can compute the ex-
pected number of times a particular alignment was made (the E-step in the EM frame-
work). Based on these expectations, we can simply sum and normalize to get new
parameters (the M-step). The expected transitions are computed according to the -r table,
which makes use of the forward and backward probabilities. Finally, the re-estimated
jump probabilities are given by aˆ and the re-estimated rewrite probabilities are given by
ˆb, which are essentially relative frequencies of the fractional counts given by the -rs.
The computational complexity for the Viterbi algorithm and for the parameter re-
estimation is O (N2T2), where N is the length of the summary and T is the number of
states (in our case, T is roughly the length of the document times the maximum phrase
length allowed). However, we will typically bound the maximum length of a phrase;
we are unlikely to otherwise encounter enough training data to get reasonable estimates
of emission probabilities. If we enforce a maximum observation sequence length of l,
then this drops to O (N2Tl). Moreover, if the transition network is sparse, as it is in
our case, and the maximum out-degree of any node is b, then the complexity drops to
0 (NTbl).
</bodyText>
<sectionHeader confidence="0.962792" genericHeader="method">
4. Model Parameterization
</sectionHeader>
<bodyText confidence="0.9999125">
Beyond the conditional independence assumptions made by the semi-HMM, there are
nearly no additional constraints that are imposed on the parameterization (in terms
of the jump and rewrite distributions) of the model. There is one additional technical
requirement involving parameter re-estimation, which essentially says that the expec-
tations calculated during the forward-backward algorithm must be sufficient statistics
for the parameters of the jump and rewrite models. This constraint simply requires that
whatever information we need to re-estimate their parameters is available to us from
the forward-backward algorithm.
</bodyText>
<subsectionHeader confidence="0.997768">
4.1 Parameterizing the Jump Model
</subsectionHeader>
<bodyText confidence="0.99912925">
Recall that the responsibility of the jump model is to compute probabilities of the form
jump(j&apos; I j), where j&apos; is a new position and j is an old position. We have explored several
possible parameterizations of the jump table. The first simply computes a table of
likely jump distances (i.e., jump forward 1, jump backward 3, etc.). The second models
</bodyText>
<page confidence="0.993137">
516
</page>
<note confidence="0.824317">
Daum´e and Marcu Alignments for Automatic Document Summarization
</note>
<tableCaption confidence="0.992756">
Table 2
</tableCaption>
<bodyText confidence="0.917521">
Jump probability decomposition; the source state is either the designated start state, the
designated end state, a document phrase position spanning from i to i&apos; (denoted ri,ii) or a null
state corresponding to position i (denoted r∅,i).
source target probability
</bodyText>
<equation confidence="0.995237">
(start) ri,i, jumprel(i)
ri,i, rj,j� jumprel(j − i�)
ri,j, (end) jumprel(m + 1 − i~)
(start) r∅,i jumprel(0)jumprel(i)
r∅,i rj,jl jumprel(j − i)
r∅,i r∅,j jumprel(0)jumprel(j − i)
r∅,i (end) jumprel(m + 1 − i)
ri,i, r∅,j jumprel(0)jumprel(j − i~)
</equation>
<bodyText confidence="0.993108454545454">
this distribution as a Gaussian (though, based on Figure 2 this is perhaps not the best
model). Both of these models have been explored in the machine translation commu-
nity. Our third parameterization employs a novel syntax-aware jump model that at-
tempts to take advantage of local syntactic information in computing jumps.
4.1.1 The Relative Jump Model. In the relative jump model, we keep a table of counts
for each possible jump distance, and compute jump(j&apos;  |j) = jumprel(j&apos; − j). Each possi-
ble jump type and its associated probability is shown in Table 2. By these calculations,
regardless of document phrase lengths, transitioning forward between two consecutive
segments will result in jumprel(1). When transitioning from the start state p to state
ri,i�, the value we use is a jump length of i. Thus, if we begin at the first word in the
document, we incur a transition probability of j1. There are no transitions into p. We
additionally remember a specific transition jumprel(0) for the probability of transition-
ing to a null state. It is straightforward to estimate these parameters based on the
estimations from the forward-backward algorithm. In particular, jumprel(i) is simply
the relative frequency of length i jumps, and jumprel(0) is simply the count of jumps that
end in a null state to the total number of jumps. The null state remembers the position
we ended in before we jumped there, and so to jump out of a null state, we make a jump
based on this previous position.6
4.1.2 Gaussian Jump Model. The Gaussian jump model attempts to alleviate the spar-
sity of data problem in the relative jump model by assuming a parametric form to the
jumps. In particular, we assume there is a mean jump length µ and a jump variance σ2,
and then the probability of a jump of length i is given by:
</bodyText>
<equation confidence="0.9821935">
[ 1
i � Nor(µ, σ2) ∝ exp σ2 (i − µ)2] (1)
</equation>
<bodyText confidence="0.980169333333333">
Some care must be taken in employing this model, since the normal distribution
is defined over a continuous space. Thus, when we discretize the calculation, the nor-
malizing constant changes slightly from that of a continuous normal distribution. In
</bodyText>
<footnote confidence="0.9777815">
6 In order for the null state to remember where we were, we actually introduce one null state for each
document position, and require that from a document phrase di:j, we can only jump to null state 0j.
</footnote>
<page confidence="0.955933">
517
</page>
<note confidence="0.486151">
Computational Linguistics Volume 31, Number 4
</note>
<bodyText confidence="0.997700485714286">
practice, we normalize by summing over a sufficiently large range of possible is. The pa-
rameters µ and σ2 are estimated by computing the mean jump length in the expectations
and its empirical variance. We model null states identically to the relative jump model.
4.1.3 Syntax-Aware Jump Model. Both of the previously described jump models are
extremely naive in that they look only at the distance jumped and completely ignore
what is being jumped over. In the syntax-aware jump model, we wish to enable the
model to take advantage of syntactic knowledge in a very weak fashion. This is quite
different from the various approaches to incorporating syntactic knowledge into ma-
chine translation systems, wherein strong assumptions about the possible syntactic
operations are made (Yamada and Knight 2001; Eisner 2003; Gildea 2003).
To motivate this model, consider the first document sentence shown with its syn-
tactic parse tree in Figure 6. Though it is not always the case, forward jumps of distance
more than one are often indicative of skipped words. From the standpoint of the rela-
tive jump models, jumping over the four words tripled it ’s sales and jumping over the
four words of Apple Macintosh systems are exactly the same.7 However, intuitively, we
would be much more willing to jump over the latter than the former. The latter phrase
is a full syntactic constituent, while the first phrase is just a collection of nearby words.
Furthermore, the latter phrase is a prepositional phrase (and prepositional phrases
might be more likely dropped than other phrases), while the former phrase includes
a verb, a pronoun, a possessive marker, and a plain noun.
To formally capture this notion, we parameterize the syntax-aware jump model
according to the types of phrases being jumped over. That is, to jump over tripled it ’s
sales would have probability jumpsyn(VBD PRP POS NNS) while to jump over of Apple
Macintosh systems would have probability jumpsyn(PP). In order to compute the prob-
abilities for jumps over many components, we factorize so that the first probabil-
ity becomes jumpsyn(VBD)jumpsyn(PRP)jumpsyn(POS)jumpsyn(NNS). This factorization
explicitly encodes our preference for jumping over single units rather than several
syntactically unrelated units.
In order to work with this model, we must first parse the document side of the
corpus; we used Charniak’s parser (Charniak 1997). Given the document parse trees,
the re-estimation of the components of this probability distribution is done by sim-
ply counting what sorts of phrases are being jumped over. Again, we keep a single
parameter jumpsyn(∅) for jumping to null states. To handle backward jumps, we simply
consider a duplication of the tag set, where jumpsyn(NP-f) denotes a forward jump over
an NP, and jumpsyn(NP-b) denotes a backward jump over an NP.8
</bodyText>
<subsectionHeader confidence="0.997271">
4.2 Parameterizing the Rewrite Model
</subsectionHeader>
<bodyText confidence="0.999267666666667">
As observed from the human-aligned summaries, a good rewrite model should be able
to account for alignments between identical word and phrases, between words that
are identical up to stem, and between different words. Intuition (as well as further
</bodyText>
<footnote confidence="0.974506666666667">
7 As can be seen from this example, we have preprocessed the data to split off possessive terms, such as the
mapping from its to it ’s.
8 In general, there are many ways to get from one position to another. For instance, to get from systems to
January, we could either jump forward over an RB and a JJ, or we could jump forward over an ADVP and
backward over an NN. In our version, we restrict all jumps to the same direction, and take the shortest
jump sequence, in terms of number of nodes jumped over.
</footnote>
<page confidence="0.9706">
518
</page>
<note confidence="0.632415">
Daum´e and Marcu Alignments for Automatic Document Summarization
</note>
<figureCaption confidence="0.971709">
Figure 6
</figureCaption>
<bodyText confidence="0.9963644">
The syntactic tree for an example document sentence.
investigations of the data) also suggest that synonymy is an important factor to take into
consideration in a successful rewrite model. We account for each of these four factors in
four separate components of the model and then take a linear interpolation of them to
produce the final probability:
</bodyText>
<equation confidence="0.998769">
rewrite(s  |d) = λidrewriteid(s  |d) + λstemrewritestem(s  |d) (2)
+ λwnrewritewn(s  |d) + λrwrewriterw(s  |d) (3)
</equation>
<bodyText confidence="0.999910571428572">
where the λs are constrained to sum to unity. The four rewrite distributions used are: id
is a word identity model, which favors alignment of identical words; stem is a model
designed to capture the notion that matches at the stem level are often sufficient for
alignment (i.e., walk and walked are likely to be aligned); wn is a rewrite model based
on similarity according to WordNet; and wr is the basic rewrite model, similar to a
translation table in machine translation. These four models are described in detail in
this section, followed by a description of how to compute their λs during EM.
</bodyText>
<subsubsectionHeader confidence="0.769264">
4.2.1 Word Identity Rewrite Model. The form of the word identity rewrite model is:
</subsubsectionHeader>
<bodyText confidence="0.987433">
rewriteid(s  |d) = δs=d. That is, the probability is 1 exactly when s and d are identical,
and 0 when they differ. This model has no parameters.
</bodyText>
<subsubsectionHeader confidence="0.8345">
4.2.2 Stem Identity Rewrite Model. The form of the stem identity rewrite model is
</subsubsectionHeader>
<bodyText confidence="0.471639">
very similar to that of the word identity model:
</bodyText>
<equation confidence="0.9666495">
rewritestem(s  |d) = 1δ|s|=|d |� |s |δstem(s;)=stem(d;) (4)
Zd ;=1
</equation>
<bodyText confidence="0.9105855">
That is, the probability of a phrase s given d is uniform over all phrases s&apos; that
match d up to stem (and are of the same length, i.e., |s&apos; |= |d|), and zero otherwise. The
</bodyText>
<page confidence="0.982836">
519
</page>
<note confidence="0.485072">
Computational Linguistics Volume 31, Number 4
</note>
<bodyText confidence="0.9665103">
normalization constant is computed offline based on a pre-computed vocabulary. This
model also has no parameters.
4.2.3 WordNet Rewrite Model. In order to account for synonymy, we allow document
phrases to be rewritten to semantically “related” summary phrases. To compute the
value for rewritewn(s  |d), we first require that both s and d can be found in WordNet. If
either cannot be found, then the probability is zero. If they both can be found, then
the graph distance between their first senses is computed (we traverse the hyper-
nymy tree up until they meet). If the two paths do not meet, then the probability is
again taken to be zero. We place an exponential model on the hypernym tree-based
distance:
</bodyText>
<equation confidence="0.979224">
rewritewn(s  |d) = Zd1 exp [−ηdist(s,d)] (5)
</equation>
<bodyText confidence="0.9937654375">
Here, dist is calculated distance, taken to be +oo whenever either of the failure
conditions is met. The single parameter of this model is η, which is computed according
to the maximum likelihood criterion from the expectations during training. The nor-
malization constant Zd is calculated by summing over the exponential distribution for
all s&apos; that occur on the summary side of our corpus.
4.2.4 Lexical Rewrite Model. The lexical rewrite model is the “catch all” model to
handle the cases not handled by the above models. It is analogous to a translation-table
(t-table) in statistical machine translation (we will continue to use this terminology
for the remainder of the article), and simply computes a matrix of (fractional) counts
corresponding to all possible phrase pairs. Upon normalization, this matrix gives the
rewrite distribution.
4.2.5 Estimation of the Weight Parameters. In order to weight the four models, we
need to estimate values for the A components. This computation can be performed
inside of the EM iterations by considering for each rewritten pair its expectation of
belonging to each of the models. We use these expectations to maximize the likelihood
with respect to the As and then normalize them so they sum to one.
</bodyText>
<subsectionHeader confidence="0.999387">
4.3 Model Priors
</subsectionHeader>
<bodyText confidence="0.999695181818182">
In the standard HMM case, the learning task is simply one of parameter estimation,
wherein the maximum likelihood criterion under which the parameters are typically
trained performs well. However, in our model, we are, in a sense, simultaneously
estimating parameters and selecting a model: The model selection is taking place at the
level of deciding how to segment the observed summary. Unfortunately, in such model
selection problems, likelihood increases monotonically with model complexity. Thus,
EM will find for us the most complex model; in our case, this will correspond to a
model in which the entire summary is produced at once, and no generalization will be
possible.
This suggests that a criterion other than maximum likelihood (ML) is more
appropriate. We advocate the maximum a posteriori (MAP) criterion in this case. While
</bodyText>
<page confidence="0.981973">
520
</page>
<note confidence="0.737481">
Daum´e and Marcu Alignments for Automatic Document Summarization
</note>
<bodyText confidence="0.935857045454545">
ML optimizes the probability of the data given the parameters (the likelihood), MAP
optimizes the product of the probability of the parameters with the likelihood (the
unnormalized posterior). The difficulty in our model that makes ML estimation perform
poorly is centered in the lexical rewrite model. Under ML estimation, we will simply
insert an entry in the t-table for the entire summary for some uncommon or unique
document word and are done. However, a priori we do not believe that such a parameter
is likely. The question then becomes how to express this in a way that inference remains
tractable.
From a statistical point of view, the t-table is nothing but a large multinomial model
(technically, one multinomial for each possible document phrase). Under a multinomial
distribution with parameter θ with J-many components (with all θj positive and sum-
ming to one), the probability of an observation x is given by p (x  |θ) = HJj=1 θxj
j (here, we
consider x to be a vector of length J in which all components are zero except for one,
corresponding to the actual observation).
This distribution belongs to the exponential family and therefore has a natural conju-
gate distribution. Informally, two distributions are conjugate if you can multiply them
together and get the original distribution back. In the case of the multinomial, the conju-
gate distribution is the Dirichlet distribution. A Dirichlet distribution is parameterized
by a vector α of length J with αj &gt; 0, but not necessarily summing to one. The Dirichlet
distribution can be used as a prior distribution over multinomial parameters and has
density:
</bodyText>
<equation confidence="0.9968055">
p (θ  |α) = ~~J �
r j=1 αj �J7� 7 e
l .
lj=1 r(Ch) i=1
</equation>
<bodyText confidence="0.99328325">
The fraction before the product is simply a normalization term that ensures that the
integral over all possible θ integrates to one.
The Dirichlet is conjugate to the multinomial because when we compute the
posterior of θ given α and x, we arrive back at a Dirichlet distribution: p (θ  |x, α) ∝
</bodyText>
<equation confidence="0.939237">
p (x  |θ) p (θ  |α) ∝ jIJj=1 θxj+αj−1 . This distribution has the same density as the original
j
</equation>
<bodyText confidence="0.999929157894737">
model, but a “fake count” of αj − 1 has been added to component j. This means that
if we are able to express our prior beliefs about the multinomial parameters found
in the t-table in the form of a Dirichlet distribution, the computation of the MAP
solution can be performed exactly as described before, but with the appropriate fake
counts added to the observed variables (in our case, the observed variables are the
alignments between a document phrase and a summary phrase). The application of
Dirichlet priors to standard HMMs has previously been considered in signal process-
ing (Gauvain and Lee 1994). These fake counts act as a smoothing parameter, sim-
ilar to Laplace smoothing (Laplace smoothing is the special case where αj = 2 for
all j).
In our case, we believe that singleton rewrites are worth 2 fake counts, that lexical
identity rewrites are worth 4 fake counts and that stem identity rewrites are worth
3 fake counts. Indeed, since a singleton alignment between identical words satisfies
all of these criteria, it will receive a fake count of 9. The selection of these counts is
intuitive, but clearly arbitrary. However, this selection was not “tuned” to the data to
get better performance. As we will discuss later, inference in this model over the sizes
of documents and summaries we consider is quite computationally expensive. As is
appropriate, we specified this prior according to our prior beliefs, and left the rest to the
inference mechanism.
</bodyText>
<page confidence="0.990684">
521
</page>
<note confidence="0.456836">
Computational Linguistics Volume 31, Number 4
</note>
<subsectionHeader confidence="0.928392">
4.4 Parameter Initialization
</subsectionHeader>
<bodyText confidence="0.999778666666667">
We initialize all the parameters uniformly, but in the case of the rewrite parameters,
since there is a prior on them, they are effectively initialized to the maximum likelihood
solution under their prior.
</bodyText>
<sectionHeader confidence="0.994761" genericHeader="method">
5. Experimental Results
</sectionHeader>
<bodyText confidence="0.999986619047619">
The experiments we perform are on the same Ziff-Davis corpus described in the intro-
duction. In order to judge the quality of the alignments produced, we compare them
against the gold-standard references annotated by the humans. The standard precision
and recall metrics used in information retrieval are modified slightly to deal with the
sure and possible alignments created during the annotation process. Given the set S
of sure alignments, the set S C_ P of possible alignments, and a set A of hypothesized
alignments, we compute the precision as |A n P|/|A |and the recall as |A n S|/|S|.
One problem with these definitions is that phrase-based models are fond of making
phrases. That is, when given an abstract containing the man and a document also
containing the man, a human will align the to the and man to man. However, a phrase-
based model will almost always prefer to align the entire phrase the man to the man. This
is because it results in fewer probabilities being multiplied together.
To compensate for this, we define soft precision (SoftP in the tables) by counting
alignments where a b is aligned to a b the same as ones in which a is aligned to a and b is
aligned to b. Note, however, that this is not the same as a aligned to a b and b aligned to
b. This latter alignment will, of course, incur a precision error. The soft precision metric
induces a new, soft F-Score, labeled SoftF.
Often, even humans find it difficult to align function words and punctuation. A list
of 58 function words and punctuation marks that appeared in the corpus (henceforth
called the ignore-list) was assembled. We computed precision and recall scores both on
all words and on all words that do not appear in the ignore-list.
</bodyText>
<subsectionHeader confidence="0.993961">
5.1 Systems Compared
</subsectionHeader>
<bodyText confidence="0.999988823529412">
Overall, we compare various parameter settings of our model against three other sys-
tems. First, we compare against two alignment models developed in the context of
machine translation. Second, we compare against the Cut and Paste model developed in
the context of “summary decomposition” by Jing (2002). Each of these systems will be
discussed in more detail shortly. However, the machine translation alignment models
assume sentence pairs as input. Moreover, even though the semi-Markov model is based
on efficient dynamic programming techniques, it is still too inefficient to run on very
long (document, abstract) pairs.
To alleviate both of these problems, we preprocess our (document, abstract) corpus
down to an (extract, abstract) corpus, and then subsequently apply our models to this
smaller corpus (see Figure 7). In our data, doing so does not introduce significant
noise. To generate the extracts, we paired each abstract sentence with three sentences
from the corresponding document, selected using the techniques described by Marcu
(1999). In an informal evaluation, 20 such pairs were randomly extracted and evaluated
by a human. Each pair was ranked as 0 (document sentences contain little to none
of the information in the abstract sentence), 1 (document sentences contain some of
the information in the abstract sentence) or 2 (document sentences contain all of the
</bodyText>
<page confidence="0.990665">
522
</page>
<note confidence="0.608004">
Daum´e and Marcu Alignments for Automatic Document Summarization
</note>
<figureCaption confidence="0.759340666666667">
Figure 7
Pictorial representation of the conversion of the (document, abstract) corpus to an (extract,
abstract) corpus.
</figureCaption>
<bodyText confidence="0.979296464285714">
information). Of the 20 random examples, none were labeled as 0; 5 were labeled as 1;
and 15 were labeled as 2, giving a mean rating of 1.75. We refer to the resulting corpus
as the (extract, abstract) corpus, statistics for which are shown in Table 3. Finally, for fair
comparison, we also run the Cut and Paste model only on the extracts.9
5.1.1 Machine Translation Models. We compare against several competing systems,
the first of which is based on the original IBM Model 4 for machine translation
(Brown et al. 1993) and the HMM machine translation alignment model (Vogel, Ney,
and Tillmann 1996) as implemented in the GIZA++ package (Och and Ney 2003).
We modified the code slightly to allow for longer inputs and higher fertilities, but
otherwise made no changes. In all of these setups, 5 iterations of Model 1 were run,
followed by five iterations of the HMM model. For Model 4, 5 iterations of Model 4
were subsequently run.
In our model, the distinction between the summary and the document is clear, but
when using a model from machine translation, it is unclear which of the summary
and the document should be considered the source language and which should be
considered the target language. By making the summary the source language, we are
effectively requiring that the fertility of each summary word be very high, or that many
words are null generated (since we must generate all of the document). By making
the document the source language, we are forcing the model to make most document
words have zero fertility. We have performed experiments in both directions, but the
latter (document as source) performs better in general.
In order to seed the machine translation model so that it knows that word identity
is a good solution, we appended our corpus with sentence pairs consisting of one source
word and one target word, which were identical. This is common practice in the ma-
chine translation community when one wishes to cheaply encode knowledge from a
dictionary into the alignment model.
5.1.2 Cut and Paste Model. We also tested alignments using the Cut and Paste sum-
mary decomposition method (Jing 2002), based on a non-trainable HMM. Briefly,
</bodyText>
<footnote confidence="0.93584">
9 Interestingly, the Cut and Paste method actually achieves higher performance scores when run on only
the extracts rather than the full documents.
</footnote>
<page confidence="0.995553">
523
</page>
<table confidence="0.416971">
Computational Linguistics Volume 31, Number 4
</table>
<tableCaption confidence="0.994086">
Table 3
</tableCaption>
<table confidence="0.9948658">
Ziff-Davis extract corpus statistics.
Abstracts Extracts Documents
Documents 2033 2033
Sentences 13k 41k 82k
Words 261k 1M 2.5M
Unique words 14k 26k 42k
29k
Sentences/Doc 6.28 21.51 40.83
Words/Doc 128.52 510.99 1229.71
Words/Sent 20.47 23.77 28.36
</table>
<bodyText confidence="0.988877235294118">
the Cut and Paste HMM searches for long contiguous blocks of words in the
document and abstract that are identical (up to stem). The longest such sequences
are aligned. By fixing a length cutoff of n and ignoring sequences of length less
than n, one can arbitrarily increase the precision of this method. We found that
n = 2 yields the best balance between precision and recall (and the highest F-measure).
On this task, this model drastically outperforms the machine translation models.
5.1.3 The Semi-Markov Model. While the semi-HMM is based on a dynamic pro-
gramming algorithm, the effective search space in this model is enormous, even for
moderately sized (document, abstract) pairs. The semi-HMM system was then trained
on this (extract, abstract) corpus. We also restrict the state-space with a beam, sized at
50% of the unrestricted state-space. With this configuration, we run ten iterations of the
forward-backward algorithm. The entire computation time takes approximately 8 days
on a 128-node cluster computer.
We compare three settings of the semi-HMM. The first, semi-HMM-relative, uses
the relative movement jump table; the second, semi-HMM-Gaussian, uses the Gaussian
parameterized jump table; the third, semi-HMM-syntax, uses the syntax-based jump
model.
</bodyText>
<subsectionHeader confidence="0.999614">
5.2 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999977076923077">
The results, in terms of precision, recall, and F-score, are shown in Table 4. The first three
columns are when these three statistics are computed over all words. The next three
columns are when these statistics are only computed over words that do not appear in
our ignore list of 58 stop words. Under the methodology for combining the two human
annotations by taking the union, either of the human scores would achieve a precision
and recall of 1.0. To give a sense of how well humans actually perform on this task, we
compare each human against the other.
As we can see from Table 4, none of the machine translation models is well suited
to this task, achieving, at best, an F-score of 0.298. The flipped models, in which the
document sentences are the source language and the abstract sentences are the target
language perform significantly better (comparatively). Since the MT models are not
symmetric, going the bad way requires that many document words have zero fertility,
which is difficult for these models to cope with.
</bodyText>
<page confidence="0.996832">
524
</page>
<note confidence="0.905486">
Daum´e and Marcu Alignments for Automatic Document Summarization
</note>
<tableCaption confidence="0.99848">
Table 4
</tableCaption>
<table confidence="0.972218923076923">
Results on the Ziff-Davis corpus.
System All Words Non-Stop Words
SoftP Recall SoftF SoftP Recall SoftF
Human1 0.727 0.746 0.736 0.751 0.801 0.775
Human2 0.680 0.695 0.687 0.730 0.722 0.726
HMM (Sum=Src) 0.120 0.260 0.164 0.139 0.282 0.186
Model 4 (Sum=Src) 0.117 0.260 0.161 0.135 0.283 0.183
HMM (Doc=Src) 0.295 0.250 0.271 0.336 0.267 0.298
Model 4 (Doc=Src) 0.280 0.247 0.262 0.327 0.268 0.295
Cut and Paste 0.349 0.379 0.363 0.431 0.385 0.407
semi-HMM-relative 0.456 0.686 0.548 0.512 0.706 0.593
semi-HMM-Gaussian 0.328 0.573 0.417 0.401 0.588 0.477
semi-HMM-syntax 0.504 0.701 0.586 0.522 0.712 0.606
</table>
<bodyText confidence="0.999629944444445">
The Cut and Paste method performs significantly better, which is to be expected,
since it is designed specifically for summarization. As one would expect, this method
achieves higher precision than recall, though not by very much. The fact that the Cut
and Paste model performs so well, compared to the MT models, which are able to learn
non-identity correspondences, suggests that any successful model should be able to
take advantage of both, as ours does.
Our methods significantly outperform both the IBM models and the Cut and Paste
method, achieving a precision of 0.522 and a recall of 0.712, yielding an overall F-score of
0.606 when stop words are not considered. This is still below the human-against-human
F-score of 0.775 (especially considering that the true human-against-human scores are
1.0), but significantly better than any of the other models.
Among the three settings of our jump table, the syntax-based model performs
best, followed by the relative jump model, with the Gaussian model coming in worst
(though still better than any other approach). Inspecting Figure 2, the fact that the
Gaussian model does not perform well is not surprising; the data shown there is very
non-Gaussian. A double-exponential model might be a better fit, but it is unlikely that
such a model will outperform the syntax based model, so we did not perform this
experiment.
</bodyText>
<subsectionHeader confidence="0.999016">
5.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.99999025">
The first mistake frequently made by our model is to not align summary words to null.
In effect, this means that our model of null-generated summary words is lacking. An
example of this error is shown in Example 1 in Figure 8. In this example, the model
has erroneously aligned from DOS in the abstract to from DOS in the document (the
error is shown in bold). This alignment is wrong because the context of from DOS in the
document is completely different from the context it appears in the summary. However,
the identity rewrite model has overwhelmed the locality model and forced this incor-
rect alignment. To measure the frequency of such errors, we have post-processed our
system’s alignments so that whenever a human alignment contains a null-generated
summary word, our model also predicts that this word is null-generated. Doing so will
not change our system’s recall, but it can improve the precision. Indeed, in the case
of the relative jump model, the precision jumps from 0.456 to 0.523 (F-score increases
</bodyText>
<page confidence="0.993292">
525
</page>
<figure confidence="0.712537">
Computational Linguistics Volume 31, Number 4
</figure>
<figureCaption confidence="0.844576">
Figure 8
Erroneous alignments are in bold. (Top) Example of an error made by our model (from file
ZF207-585-936). From DOS should be null generated, but the model has erroneously aligned
it to an identical phrase that appeared 11 sentences earlier in the document. (Bottom) Error
(from ZF207-772-628); The DMP 300 should be aligned to the printer but is instead aligned to
a far-away occurrence of The DMP 300.
</figureCaption>
<bodyText confidence="0.999899677419355">
from 0.548 to 0.594) in the case of all words and from 0.512 to 0.559 (F-score increases
from 0.593 to 0.624). This corresponds to a relative improvement of roughly 8% F-score.
Increases in score for the syntax-based model are roughly the same.
The second mistake our model frequently makes is to trust the identity rewrite
model too strongly. This problem has to do either with synonyms that do not appear
frequently enough for the system to learn reliable rewrite probabilities, or with corefer-
ence issues, in which the system chooses to align, for instance, Microsoft to Microsoft,
rather than Microsoft to the company, as might be correct in context. As suggested
by this example, this problem is typically manifested in the context of coreferential
noun phrases. It is difficult to perform a similar analysis of this problem as for the
aforementioned problem (to achieve an upper bound on performance), but we can
provide some evidence. As mentioned before, in the human alignments, roughly 51% of
all aligned phrases are lexically identical. In the alignments produced by our model (on
the same documents), this number is 69%. In the case of stem identity, the hand-aligned
data suggests that stem identity should hold in 67% of the cases; in our alignments,
this number was 81%. An example of this sort of error is shown in Example 2 in
Figure 8. Here, the model has aligned The DMP 300 in the abstract to The DMP 300 in
the document, while it should have been aligned to the printer due to locality constraints
(note that the model also misses the (produces ↔ producing) alignment, likely as a side-
effect of it making the error depicted in bold).
In Table 5, we have shown examples of common errors made by our system (these
were randomly selected from a much longer list of errors). These examples are shown
out of their contexts, but in most cases, the error is clear even so. In the first column, we
show the summary phrase in question. In the second column, we show the document
phrase to which it should be aligned, and in the third column, we show the document
phrase that our model aligned it to (or null). In the right column, we classify the model’s
alignment as incorrect or partially correct.
The errors shown in Table 5 show several weaknesses of the model. For instance,
in the first example, it aligns to port with to port, which seems correct without con-
text, but the chosen occurrence of to port in the document is in the discussion of
a completely different porting process than that referred to in the summary (and is
</bodyText>
<page confidence="0.99686">
526
</page>
<note confidence="0.905265">
Daum´e and Marcu Alignments for Automatic Document Summarization
</note>
<tableCaption confidence="0.991892">
Table 5
</tableCaption>
<bodyText confidence="0.8457504">
Ten example phrase alignments from the hand-annotated corpus; the last column indicates
whether the semi-HMM correctly aligned this phrase.
Summary Phrase True Phrase Aligned Phrase Class
to port can port to port incorrect
OS - 2 the OS / 2 OS / 2 partial
will use will be using will using partial
word processing programs word processors word processing incorrect
consists of also includes null of partial
will test will also have to test will test partial
the potential buyer many users the buyers incorrect
The new software Crosstalk for Windows new software incorrect
are generally powered by run on null incorrect
Oracle Corp. the software publisher Oracle Corp. incorrect
several sentences away). The seventh and tenth examples (The new software and Oracle
Corp., respectively) show instances of the coreference error that occurs commonly.
</bodyText>
<sectionHeader confidence="0.919995" genericHeader="conclusions">
6. Conclusion and Discussion
</sectionHeader>
<bodyText confidence="0.999973535714286">
Currently, summarization systems are limited to either using hand-annotated data or
using weak alignment models at the granularity of sentences, which serve as suitable
training data only for sentence extraction systems. To train more advanced extraction
systems, such as those used in document compression models or in next-generation
abstraction models, we need to better understand the lexical correspondences between
documents and their human written abstracts. Our work is motivated by the desire to
leverage the vast number of (document, abstract) pairs that are freely available on the
Internet and in other collections and to create word- and phrase-aligned (document,
abstract) corpora automatically.
This article presents a statistical model for learning such alignments in a com-
pletely unsupervised manner. The model is based on an extension of a hidden Markov
model, in which multiple emissions are made in the course of one transition. We have
described efficient algorithms in this framework, all based on dynamic programming.
Using this framework, we have experimented with complex models of movement and
lexical correspondences. Unlike the approaches used in machine translation, where only
very simple models are used, we have shown how to efficiently and effectively leverage
such disparate knowledge sources and WordNet, syntax trees, and identity models.
We have empirically demonstrated that our model is able to learn the complex struc-
ture of (document, abstract) pairs. Our system outperforms competing approaches, in-
cluding the standard machine translation alignment models (Brown et al. 1993; Vogel,
Ney, and Tillmann 1996) and the state-of-the-art Cut and Paste summary alignment
technique (Jing 2002).
We have analyzed two sources of error in our model, including issues of null-
generated summary words and lexical identity. Within the model itself, we have already
suggested two major sources of error in our alignment procedure. Clearly more work
needs to be done to fix these problems. One approach that we believe will be particu-
larly fruitful would be to add a fifth model to the linearly interpolated rewrite model
based on lists of synonyms automatically extracted from large corpora. Additionally,
</bodyText>
<page confidence="0.978133">
527
</page>
<note confidence="0.563922">
Computational Linguistics Volume 31, Number 4
</note>
<bodyText confidence="0.999950555555556">
investigating the possibility of including some sort of weak coreference knowledge into
the model might serve to help with the second class of errors made by the model.
One obvious aspect of our method that may reduce its general usefulness is the
computation time. In fact, we found that despite the efficient dynamic programming
algorithms available for this model, the state space and output alphabet are simply
so large and complex that we were forced to first map documents down to extracts
before we could process them (and even so, computation took roughly 1,000 processor
hours). Though we have not pursued it in this work, we do believe that there is
room for improvement computationally, as well. One obvious first approach would
be to run a simpler model for the first iteration (for example, Model 1 from machine
translation (Brown et al. 1993), which tends to be very recall oriented) and use this to see
subsequent iterations of the more complex model. By doing so, one could recreate the
extracts at each iteration using the previous iteration’s parameters to make better and
shorter extracts. Similarly, one might only allow summary words to align to words found
in their corresponding extract sentences, which would serve to significantly speed up
training and, combined with the parameterized extracts, might not hurt performance.
A final option, but one that we do not advocate, would be to give up on phrases and
train the model in a word-to-word fashion. This could be coupled with heuristic phrasal
creation as is done in machine translation (Och and Ney 2000), but by doing this, one
completely loses the probabilistic interpretation that makes this model so pleasing.
Aside from computational considerations, the most obvious future effort along the
lines of this model is to incorporate it into a full document summarization system. Since
this can be done in many ways, including training extraction systems, compression
systems, headline generation systems, and even extraction systems, we left this to
future work so that we could focus specifically on the alignment task in this article.
Nevertheless, the true usefulness of this model will be borne out by its application to
true summarization tasks.
</bodyText>
<sectionHeader confidence="0.997297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992469666666667">
We wish to thank David Blei for helpful
theoretical discussions related to this project
and Franz Josef Och for sharing his technical
expertise on issues that made the
computations discussed in this paper
possible. We sincerely thank the anonymous
reviewers of an original conference version
of this article as well reviewers of this longer
version, all of whom gave very useful
suggestions. Some of the computations
described in this work were made possible
by the High Performance Computing Center
at the University of Southern California.
This work was partially supported by
DARPA-ITO grant N66001-00-1-9814, NSF
grant IIS-0097846, NSF grant IIS-0326276,
and a USC Dean Fellowship to Hal
Daum´e III.
</bodyText>
<sectionHeader confidence="0.997971" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992937172413793">
Aydin, Zafer, Yucel Altunbasak, and Mark
Borodovsky. 2004. Protein secondary
structure prediction with semi-Markov
HMMs. In Proceedings of the IEEE
International Conference on Acoustics, Speech
and Signal Processing (ICASSP), May 17–21.
Banko, Michele, Vibhu Mittal, and Michael
Witbrock. 2000. Headline generation based
on statistical translation. In Proceedings
of the Conference of the Association for
Computational Linguistics (ACL),
pages 318–325, Hong Kong, October 1–8.
Barzilay, Regina and Noemie Elhadad. 2003.
Sentence alignment for monolingual
comparable corpora. In Proceedings of
the Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 25–32.
Baum, Leonard E. and J. E. Eagon.1967. An
inequality with applications to statistical
estimation for probabilistic functions of
Markov processes and to a model of
ecology. Bulletins of the American
Mathematical Society, 73:360–363.
Baum, Leonard E. and Ted Petrie. 1966.
Statistical inference for probabilistic
functions of finite state Markov chains.
Annals of Mathematical Statistics,
37:1554–1563.
</reference>
<page confidence="0.996144">
528
</page>
<note confidence="0.893577">
Daum´e and Marcu Alignments for Automatic Document Summarization
</note>
<reference confidence="0.999339440677966">
Berger, Adam and Vibhu Mittal. 2000.
Query-relevant summarization using
FAQs. In Proceedings of the Conference of the
Association for Computational Linguistics
(ACL), pages 294–301, Hong Kong,
October 1–8.
Boyles, Russell A. 1983. On the convergence
of the EM algorithm. Journal of the Royal
Statistical Society, B(44):47–50.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263–311.
Carletta, Jean. 1995. Assessing agreement
on classification tasks: The kappa
statistic. Computational Linguistics, 22(2):
249–254.
Carroll, John, Guido Minnen, Yvonne
Canning, Siobhan Devlin, and John Tait.
1998. Practical simplification of English
newspaper text to assist aphasic readers.
In Proceedings of the AAAI-98 Workshop on
Integrating Artificial Intelligence and
Assistive Technology.
Chandrasekar, Raman, Christy Doran, and
Srinivas Bangalore. 1996. Motivations and
methods for text simplification. In
Proceedings of the International Conference on
Computational Linguistics (COLING),
pages 1041–1044, Copenhagen,
Denmark.
Charniak, Eugene. 1997. Statistical parsing
with a context-free grammar and word
statistics. In Proceedings of the Fourteenth
National Conference on Artificial Intelligence
(AAAI’97), pages 598–603, Providence, RI,
July 27–31.
Daum´e III, Hal and Daniel Marcu. 2002. A
noisy-channel model for document
compression. In Proceedings of the
Conference of the Association for
Computational Linguistics (ACL),
pages 449–456.
Daum´e III, Hal and Daniel Marcu. 2004. A
tree-position kernel for document
compression. In Proceedings of the Fourth
Document Understanding Conference (DUC
2004), Boston, MA, May 6–7.
Dempster, Arthur P., Nan M. Laird, and
Donald B. Rubin. 1977. Maximum
likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical
Society, B39(1):1–37.
Edmundson, H. P. 1969. New methods in
automatic abstracting. Journal of the
Association for Computing Machinery,
16(2):264–285. Reprinted in Advances in
Automatic Text Summarization, I. Mani and
M. T. Maybury, (eds.).
Eisner, Jason. 2003. Learning non-isomorphic
tree mappings for machine translation. In
Proceedings of the Conference of the
Association for Computational Linguistics
(ACL), Sapporo, Japan.
Ferguson, Jack D. 1980. Variable duration
models for speech. In Proceedings
of the Symposium on the Application of
Hidden Markov Models to Text
and Speech, pages 143–179, October,
Princeton, NJ.
Gales, Mark J. F. and Steve J. Young.1993.
The theory of segmental hidden Markov
models. Technical report, Cambridge
University Engineering Department.
Gauvain, Jean-Luc and Chin-Hui Lee. 1994.
Maximum a-posteriori estimation for
multivariate Gaussian mixture
observations of Markov chains. IEEE
Transactions Speech and Audio Processing,
2:291–298.
Ge, Xianping and Padhraic Smyth. 2000.
Segmental semi-Markov models for
change-point detection with applications
to semiconductor manufacturing.
Technical report, University of California
at Irvine, March.
Gildea, Daniel. 2003. Loosely tree-based
alignment for machine translation. In
Proceedings of the Conference of the
Association for Computational Linguistics
(ACL), pages 80–87, Sapporo, Japan.
Grefenstette, Gregory. 1998. Producing
intelligent telegraphic text reduction to
provide an audio scanning service for
the blind. In Working Notes of the AAAI
Spring Symposium on Intelligent Text
Summarization, pages 111–118, Stanford
University, Stanford, CA.
Jensen, J. L. W. V. 1906. Sur les fonctions
convexes et les in´egalit´es entre les valeurs
moyennes. Acta Mathematica, 30:175–193.
Jing, Hongyan. 2000. Sentence reduction for
automatic text summarization. In
Proceedings of the Conference of the North
American Chapter of the Association for
Computational Linguistics (NAACL),
pages 310–315, Seattle, WA.
Jing, Hongyan. 2002. Using hidden Markov
modeling to decompose human-written
summaries. Computational Linguistics,
28(4):527–544.
Knight, Kevin and Daniel Marcu. 2002.
Summarization beyond sentence
extraction: A probabilistic approach to
sentence compression. Artificial Intelligence,
139(1):91–107.
</reference>
<page confidence="0.961416">
529
</page>
<reference confidence="0.993681525773196">
Computational Linguistics Volume 31, Number 4
Kupiec, Julian, Jan O. Pedersen, and Francine
Chen. 1995. A trainable document
summarizer. In Proceedings of the Annual
ACM Conference on Research and
Development in Information Retrieval,
pages 68–73.
Luhn, H. P. 1956. The automatic creation of
literature abstracts. In I. Mani and M.
Maybury, editors, Advances in Automatic
Text Summarization. MIT Press, Cambridge,
MA, pages 58–63.
Mahesh, Kavi. 1997. Hypertext summary
extraction for fast document browsing. In
Proceedings of the AAAI Spring Symposium
on Natural Language Processing for the World
Wide Web, pages 95–103, Stanford, CA.
Mani, Inderjeet. 2001. Automatic
Summarization, volume 3 of Natural
Language Processing. John Benjamins,
Amsterdam/Philadelphia.
Mani, Inderjeet and Mark Maybury, editors.
1999. Advances in Automatic Text
Summarization. MIT Press, Cambridge,
MA.
Marcu, Daniel. 1999. The automatic
construction of large-scale corpora for
summarization research. In Proceedings of
the 22nd Conference on Research and
Development in Information Retrieval
(SIGIR–99), pages 137–144, Berkeley, CA,
August 15–19.
Marcu, Daniel. 2000. The Theory and Practice
of Discourse Parsing and Summarization.
MIT Press, Cambridge, MA.
Mitchell, Carl D., Leah H. Jamieson, and
Mary P. Harper. 1995. On the complexity of
explicit duration HMMs. IEEE Transactions
on Speech and Audio Processing, 3(3).
Och, Franz Josef and Hermann Ney. 2000.
Improved statistical alignment models.
In Proceedings of the Conference of the
Association for Computational Linguistics
(ACL), pages 440–447, October,
Hong Kong, China.
Och, Franz Josef and Hermann Ney. 2003. A
systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19–51.
Ostendorf, Mari, Vassilis Digalakis, and
Owen Kimball. 1996. From HMMs to
segment models: A unified view of
stochastic modeling for speech
recognition. IEEE Transactions on Speech
and Audio Processing, 4(5):360–378,
September.
Quirk, Chris, Chris Brockett, and William
Dolan. 2004. Monolingual machine
translation for paraphrase generation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 142–149, Barcelona, Spain.
Schwartz, Richard, David Zajic, and Bonnie
Dorr. 2002. Automatic headline generation
for newspaper stories. In Proceedings of the
Document Understanding Conference (DUC),
pages 78–85, Philadelphia.
Smyth, Padhraic, David Heckerman, and
Michael I. Jordan. 1997. Probabilistic
independence networks for hidden
Markov probability models. Neural
Computation, 9(2):227–269.
Teufel, Simone and Mark Moens. 1997.
Sentence extraction as a classification task.
In In ACL/EACL-97 Workshop on Intelligent
and Scalable Text Summarization,
pages 58–65.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann.1996. HMM-based
word alignment in statistical translation.
In Proceedings of the International Conference
on Computational Linguistics (COLING),
pages 836–841.
Wu, Jeff C. F. 1983. On the convergence
properties of the EM algorithm. The Annals
of Statistics, 11:95–103.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model.
In Proceedings of the Conference of the
Association for Computational Linguistics
(ACL), pages 523–530.
Zajic, David, Bonnie Dorr, and Richard
Schwartz. 2004. BBN/UMD at DUC-2004:
Topiary. In Proceedings of the Fourth
Document Understanding Conference (DUC
2004), pages 112–119, Boston, MA,
May 6–7.
</reference>
<page confidence="0.99711">
530
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.450931">
<title confidence="0.99877">Induction of Word and Phrase Alignments for Automatic Document Summarization</title>
<author confidence="0.597546">Daum´e</author>
<affiliation confidence="0.99754">Information Sciences Institute Information Sciences Institute University of Southern California University of Southern California</affiliation>
<abstract confidence="0.986318842105263">Current research in automatic single-document summarization is dominated by two effective, yet naive approaches: summarization by sentence extraction and headline generation via bagof-words models. While successful in some tasks, neither of these models is able to adequately capture the large set of linguistic devices utilized by humans when they produce summaries. One possible explanation for the widespread use of these models is that good techniques have been developed to extract appropriate training data for them from existing document/abstract and document/headline corpora. We believe that future progress in automatic summarization will be driven both by the development of more sophisticated, linguistically informed models, as well as a more effective leveraging of document/abstract corpora. In order to open the doors to simultaneously achieving both of these goals, we have developed techniques for automatically producing word-to-word and phrase-to-phrase alignments between documents and their humanwritten abstracts. These alignments make explicit the correspondences that exist in such document/abstract pairs and create a potentially rich data source from which complex summarization algorithms may learn. This paper describes experiments we have carried out to analyze the ability of humans to perform such alignments, and based on these analyses, we describe experiments for creating them automatically. Our model for the alignment task is based on an extension of the standard hidden Markov model and learns to create alignments in a completely unsupervised fashion. We describe our model in detail and present experimental results that show that our model is able to learn to reliably identify wordand phrase-level alignments in a corpus of</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Zafer Aydin</author>
<author>Yucel Altunbasak</author>
<author>Mark Borodovsky</author>
</authors>
<title>Protein secondary structure prediction with semi-Markov HMMs.</title>
<date>2004</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<marker>Aydin, Altunbasak, Borodovsky, 2004</marker>
<rawString>Aydin, Zafer, Yucel Altunbasak, and Mark Borodovsky. 2004. Protein secondary structure prediction with semi-Markov HMMs. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 17–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Vibhu Mittal</author>
<author>Michael Witbrock</author>
</authors>
<title>Headline generation based on statistical translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL),</booktitle>
<pages>318--325</pages>
<location>Hong Kong,</location>
<marker>Banko, Mittal, Witbrock, 2000</marker>
<rawString>Banko, Michele, Vibhu Mittal, and Michael Witbrock. 2000. Headline generation based on statistical translation. In Proceedings of the Conference of the Association for Computational Linguistics (ACL), pages 318–325, Hong Kong, October 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>25--32</pages>
<contexts>
<context position="8968" citStr="Barzilay and Elhadad (2003)" startWordPosition="1339" endWordPosition="1342">ing document sentences via deletion of contiguous segments. In other words, only 2.7% of the sentences in real (document, abstract) pairs can be explained by the model proposed by Knight and Marcu (2002). Such document compression models do not explain the rich set of linguistic devices employed, for example, in Figure 1. 1.3 Prior Work on Alignments In the sentence extraction community, there exists a wide variety of techniques for (essentially) creating alignments between document sentences and abstract sentences (Kupiec, Pedersen, and Chen 1995; Teufel and Moens 1997; Marcu 1999); see also Barzilay and Elhadad (2003); Quirk, Brockett, and Dolan (2004) for work describing alignments for the monolingual paraphrasing task. These techniques typically take into account information such as lexical overlap, synonymy, ordering, length, discourse structure, and so forth. The sentence alignment problem is a comparatively simple problem to solve, and current approaches work quite well. Unfortunately, these 507 Computational Linguistics Volume 31, Number 4 alignments are the least useful, because they can only be used to train sentence extraction systems. In the context of headline generation, simple statistical mode</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>Barzilay, Regina and Noemie Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 25–32.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Leonard E Baum</author>
<author>J E Eagon 1967</author>
</authors>
<title>An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model of ecology.</title>
<journal>Bulletins of the American Mathematical Society,</journal>
<pages>73--360</pages>
<marker>Baum, 1967, </marker>
<rawString>Baum, Leonard E. and J. E. Eagon.1967. An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model of ecology. Bulletins of the American Mathematical Society, 73:360–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
<author>Ted Petrie</author>
</authors>
<title>Statistical inference for probabilistic functions of finite state Markov chains.</title>
<date>1966</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>37--1554</pages>
<contexts>
<context position="32684" citStr="Baum and Petrie 1966" startWordPosition="5178" endWordPosition="5181">revious states, whereas the Viterbi probabilities replace this with a max operator. Second, in order to recover the actual Viterbi alignment, we keep track of which previous state this max operator chose. This is computed by filling out the Z table from Figure 5. This is almost identical to the computation of the forward probabilities, except that instead of summing over all possible t&apos; and i, we take the maximum over those variables. The final inference problem is parameter re-estimation. In the case of standard HMMs, this is known as the Baum-Welch, Baum-Eagon or Forward-Backward algorithm (Baum and Petrie 1966; Baum and Eagon 1967). By introducing backward probabilities analogous to the forward probabilities, we can compute alignment probabilities of suffixes of the summary. The backward table is the β table in Figure 5, which is analogous to the α table, except that the computation proceeds from the end to the start. By combining the forward and backward probabilities, we can compute the expected number of times a particular alignment was made (the E-step in the EM framework). Based on these expectations, we can simply sum and normalize to get new parameters (the M-step). The expected transitions </context>
</contexts>
<marker>Baum, Petrie, 1966</marker>
<rawString>Baum, Leonard E. and Ted Petrie. 1966. Statistical inference for probabilistic functions of finite state Markov chains. Annals of Mathematical Statistics, 37:1554–1563.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Vibhu Mittal</author>
</authors>
<title>Query-relevant summarization using FAQs.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL),</booktitle>
<pages>294--301</pages>
<location>Hong Kong,</location>
<contexts>
<context position="6587" citStr="Berger and Mittal (2000)" startWordPosition="982" endWordPosition="985">pression. Sentence extraction systems take full sentences from a document and concatenate them to form a summary. Research in sentence extraction can be traced back to work in the mid 1950s and late 1960s by Luhn (1956) and Edmundson (1969). Recent techniques are startlingly not terribly divergent from these original methods; see Mani and Maybury (1999); Marcu (2000); Mani (2001) for a comprehensive overview. Headline generation systems, on the other hand, typically extract individual words from a document to produce a very short headline-style summary; see Banko, Mittal, and Witbrock (2000); Berger and Mittal (2000); Schwartz, Zajic, and Dorr (2002) for representative examples. Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and Bangalore 1996; Mahesh 1997; Carroll et al. 1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression (Daum´e III and Marcu 2002; Daum´e III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases, and sentences are selected in an extraction process. While such approaches have enjoyed some success, they all suffer from modeling shortcomings. Sentence extraction</context>
<context position="9674" citStr="Berger and Mittal 2000" startWordPosition="1441" endWordPosition="1444">al paraphrasing task. These techniques typically take into account information such as lexical overlap, synonymy, ordering, length, discourse structure, and so forth. The sentence alignment problem is a comparatively simple problem to solve, and current approaches work quite well. Unfortunately, these 507 Computational Linguistics Volume 31, Number 4 alignments are the least useful, because they can only be used to train sentence extraction systems. In the context of headline generation, simple statistical models are used for aligning documents and headlines (Banko, Mittal, and Witbrock 2000; Berger and Mittal 2000; Schwartz, Zajic, and Dorr 2002), based on IBM Model 1 (Brown et al. 1993). These models treat documents and headlines as simple bags of words and learn probabilistic word-based mappings between the words in the documents and the words in the headlines. Such mappings can be considered word-to-word alignments, but as our results show (see Section 5), these models are too weak for capturing the sophisticated operations that are employed by humans in summarizing texts. To date, there has been very little work on the word alignment task in the context of summarization. The most relevant work is t</context>
</contexts>
<marker>Berger, Mittal, 2000</marker>
<rawString>Berger, Adam and Vibhu Mittal. 2000. Query-relevant summarization using FAQs. In Proceedings of the Conference of the Association for Computational Linguistics (ACL), pages 294–301, Hong Kong, October 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Russell A Boyles</author>
</authors>
<title>On the convergence of the EM algorithm.</title>
<date>1983</date>
<journal>Journal of the Royal Statistical Society, B(44):47–50.</journal>
<contexts>
<context position="28891" citStr="Boyles 1983" startWordPosition="4562" endWordPosition="4563"> --� (end). During the transition (start) --� a, c d is emitted. During the transition a --� (end), ω is emitted. 3.4 Expectation Maximization The alignment task, as described above, is a chicken-and-egg problem: if we knew the model components (namely, the rewrite and jump tables), we would be able to efficiently find the best alignment. Similarly, if we knew the correct alignments, we would be able to estimate the model components. Unfortunately, we have neither. Expectation maximization is a general technique for learning in such chicken-and-egg situations (Dempster, Laird, and Rubin 1977; Boyles 1983; Wu 1983). The basic idea is to make a guess at the alignments, and then use this guess to estimate the parameters for the relevant distributions. We can use these re-estimated distributions to make a better guess at the alignments, and then use these (ideally better) alignments to re-estimate the parameters. Figure 4 Schematic drawing of the semi-HMM (with some transition probabilities) for the document a b. 514 Daum´e and Marcu Alignments for Automatic Document Summarization Formally, the EM family of algorithms tightly bound the log of an expectation of a function by the expectation of the</context>
</contexts>
<marker>Boyles, 1983</marker>
<rawString>Boyles, Russell A. 1983. On the convergence of the EM algorithm. Journal of the Royal Statistical Society, B(44):47–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="9749" citStr="Brown et al. 1993" startWordPosition="1455" endWordPosition="1458">such as lexical overlap, synonymy, ordering, length, discourse structure, and so forth. The sentence alignment problem is a comparatively simple problem to solve, and current approaches work quite well. Unfortunately, these 507 Computational Linguistics Volume 31, Number 4 alignments are the least useful, because they can only be used to train sentence extraction systems. In the context of headline generation, simple statistical models are used for aligning documents and headlines (Banko, Mittal, and Witbrock 2000; Berger and Mittal 2000; Schwartz, Zajic, and Dorr 2002), based on IBM Model 1 (Brown et al. 1993). These models treat documents and headlines as simple bags of words and learn probabilistic word-based mappings between the words in the documents and the words in the headlines. Such mappings can be considered word-to-word alignments, but as our results show (see Section 5), these models are too weak for capturing the sophisticated operations that are employed by humans in summarizing texts. To date, there has been very little work on the word alignment task in the context of summarization. The most relevant work is that of Jing (2002), in which a hidden Markov alignment model is applied to </context>
<context position="54265" citStr="Brown et al. 1993" startWordPosition="8761" endWordPosition="8764">sentation of the conversion of the (document, abstract) corpus to an (extract, abstract) corpus. information). Of the 20 random examples, none were labeled as 0; 5 were labeled as 1; and 15 were labeled as 2, giving a mean rating of 1.75. We refer to the resulting corpus as the (extract, abstract) corpus, statistics for which are shown in Table 3. Finally, for fair comparison, we also run the Cut and Paste model only on the extracts.9 5.1.1 Machine Translation Models. We compare against several competing systems, the first of which is based on the original IBM Model 4 for machine translation (Brown et al. 1993) and the HMM machine translation alignment model (Vogel, Ney, and Tillmann 1996) as implemented in the GIZA++ package (Och and Ney 2003). We modified the code slightly to allow for longer inputs and higher fertilities, but otherwise made no changes. In all of these setups, 5 iterations of Model 1 were run, followed by five iterations of the HMM model. For Model 4, 5 iterations of Model 4 were subsequently run. In our model, the distinction between the summary and the document is clear, but when using a model from machine translation, it is unclear which of the summary and the document should b</context>
<context position="67368" citStr="Brown et al. 1993" startWordPosition="10893" endWordPosition="10896">mework, all based on dynamic programming. Using this framework, we have experimented with complex models of movement and lexical correspondences. Unlike the approaches used in machine translation, where only very simple models are used, we have shown how to efficiently and effectively leverage such disparate knowledge sources and WordNet, syntax trees, and identity models. We have empirically demonstrated that our model is able to learn the complex structure of (document, abstract) pairs. Our system outperforms competing approaches, including the standard machine translation alignment models (Brown et al. 1993; Vogel, Ney, and Tillmann 1996) and the state-of-the-art Cut and Paste summary alignment technique (Jing 2002). We have analyzed two sources of error in our model, including issues of nullgenerated summary words and lexical identity. Within the model itself, we have already suggested two major sources of error in our alignment procedure. Clearly more work needs to be done to fix these problems. One approach that we believe will be particularly fruitful would be to add a fifth model to the linearly interpolated rewrite model based on lists of synonyms automatically extracted from large corpora</context>
<context position="68890" citStr="Brown et al. 1993" startWordPosition="11141" endWordPosition="11144">ss is the computation time. In fact, we found that despite the efficient dynamic programming algorithms available for this model, the state space and output alphabet are simply so large and complex that we were forced to first map documents down to extracts before we could process them (and even so, computation took roughly 1,000 processor hours). Though we have not pursued it in this work, we do believe that there is room for improvement computationally, as well. One obvious first approach would be to run a simpler model for the first iteration (for example, Model 1 from machine translation (Brown et al. 1993), which tends to be very recall oriented) and use this to see subsequent iterations of the more complex model. By doing so, one could recreate the extracts at each iteration using the previous iteration’s parameters to make better and shorter extracts. Similarly, one might only allow summary words to align to words found in their corresponding extract sentences, which would serve to significantly speed up training and, combined with the parameterized extracts, might not hurt performance. A final option, but one that we do not advocate, would be to give up on phrases and train the model in a wo</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<pages>249--254</pages>
<contexts>
<context position="18031" citStr="Carletta (1995)" startWordPosition="2742" endWordPosition="2743">e annotation, we found that punctuation and non-content words are often very difficult to align (despite the discussion of these issues in the alignment guidelines). The primary difficulty with function words is that when the summarizers have chosen to reorder words to use slightly different syntactic structures, there are lexical changes that are hard to predict.4 Fortunately, for many summarization tasks, it is much more important to get content words right, rather than function words. When words on a stop list of 58 function words and punctuation were ignored, the kappa value rose to 0.68. Carletta (1995) has suggested that kappa values over 0.80 reflect very strong agreement and that kappa values between 0.60 and 0.80 reflect good agreement.5 2.3 Results of Annotation After the completion of these alignments, we can investigate some of their properties. Such an investigation is interesting both from the perspective of designing a model and from a linguistic perspective. In the alignments, we found that roughly 16% of the abstract words are left unaligned. This figure includes both standard lexical words and punctuation. Of this 16%, 4% are punctuation marks (though not all punctuation is unal</context>
</contexts>
<marker>Carletta, 1995</marker>
<rawString>Carletta, Jean. 1995. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2): 249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Yvonne Canning</author>
<author>Siobhan Devlin</author>
<author>John Tait</author>
</authors>
<title>Practical simplification of English newspaper text to assist aphasic readers.</title>
<date>1998</date>
<booktitle>In Proceedings of the AAAI-98 Workshop on Integrating Artificial Intelligence and Assistive Technology.</booktitle>
<contexts>
<context position="6830" citStr="Carroll et al. 1998" startWordPosition="1017" endWordPosition="1020"> techniques are startlingly not terribly divergent from these original methods; see Mani and Maybury (1999); Marcu (2000); Mani (2001) for a comprehensive overview. Headline generation systems, on the other hand, typically extract individual words from a document to produce a very short headline-style summary; see Banko, Mittal, and Witbrock (2000); Berger and Mittal (2000); Schwartz, Zajic, and Dorr (2002) for representative examples. Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and Bangalore 1996; Mahesh 1997; Carroll et al. 1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression (Daum´e III and Marcu 2002; Daum´e III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases, and sentences are selected in an extraction process. While such approaches have enjoyed some success, they all suffer from modeling shortcomings. Sentence extraction systems and document compression models make unrealistic assumptions about the summarization task (namely, that extraction is sufficient and that sentences are the appropriate level of granularity). Headline generation systems employ very wea</context>
</contexts>
<marker>Carroll, Minnen, Canning, Devlin, Tait, 1998</marker>
<rawString>Carroll, John, Guido Minnen, Yvonne Canning, Siobhan Devlin, and John Tait. 1998. Practical simplification of English newspaper text to assist aphasic readers. In Proceedings of the AAAI-98 Workshop on Integrating Artificial Intelligence and Assistive Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raman Chandrasekar</author>
<author>Christy Doran</author>
<author>Srinivas Bangalore</author>
</authors>
<title>Motivations and methods for text simplification.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1041--1044</pages>
<location>Copenhagen, Denmark.</location>
<marker>Chandrasekar, Doran, Bangalore, 1996</marker>
<rawString>Chandrasekar, Raman, Christy Doran, and Srinivas Bangalore. 1996. Motivations and methods for text simplification. In Proceedings of the International Conference on Computational Linguistics (COLING), pages 1041–1044, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI’97),</booktitle>
<pages>598--603</pages>
<location>Providence, RI,</location>
<contexts>
<context position="40646" citStr="Charniak 1997" startWordPosition="6484" endWordPosition="6485">ed over. That is, to jump over tripled it ’s sales would have probability jumpsyn(VBD PRP POS NNS) while to jump over of Apple Macintosh systems would have probability jumpsyn(PP). In order to compute the probabilities for jumps over many components, we factorize so that the first probability becomes jumpsyn(VBD)jumpsyn(PRP)jumpsyn(POS)jumpsyn(NNS). This factorization explicitly encodes our preference for jumping over single units rather than several syntactically unrelated units. In order to work with this model, we must first parse the document side of the corpus; we used Charniak’s parser (Charniak 1997). Given the document parse trees, the re-estimation of the components of this probability distribution is done by simply counting what sorts of phrases are being jumped over. Again, we keep a single parameter jumpsyn(∅) for jumping to null states. To handle backward jumps, we simply consider a duplication of the tag set, where jumpsyn(NP-f) denotes a forward jump over an NP, and jumpsyn(NP-b) denotes a backward jump over an NP.8 4.2 Parameterizing the Rewrite Model As observed from the human-aligned summaries, a good rewrite model should be able to account for alignments between identical word</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Charniak, Eugene. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI’97), pages 598–603, Providence, RI, July 27–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daum´e Hal</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel model for document compression.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL),</booktitle>
<pages>449--456</pages>
<marker>Hal, Marcu, 2002</marker>
<rawString>Daum´e III, Hal and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proceedings of the Conference of the Association for Computational Linguistics (ACL), pages 449–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daum´e Hal</author>
<author>Daniel Marcu</author>
</authors>
<title>A tree-position kernel for document compression.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth Document Understanding Conference (DUC 2004),</booktitle>
<location>Boston, MA,</location>
<marker>Hal, Marcu, 2004</marker>
<rawString>Daum´e III, Hal and Daniel Marcu. 2004. A tree-position kernel for document compression. In Proceedings of the Fourth Document Understanding Conference (DUC 2004), Boston, MA, May 6–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<issue>1</issue>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, Arthur P., Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B39(1):1–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic abstracting.</title>
<date>1969</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<booktitle>Reprinted in Advances in Automatic Text Summarization,</booktitle>
<volume>16</volume>
<issue>2</issue>
<editor>I. Mani and M. T. Maybury, (eds.).</editor>
<contexts>
<context position="6203" citStr="Edmundson (1969)" startWordPosition="928" endWordPosition="929"> “it’s”/“its” as a special case: “its” (the possessive) is converted to “it POSS.” 506 Daum´e and Marcu Alignments for Automatic Document Summarization 1.2 Shortcomings of Current Summarization Models Current state-of-the-art automatic single-document summarization systems employ one of three techniques: sentence extraction, bag-of-words headline generation, or document compression. Sentence extraction systems take full sentences from a document and concatenate them to form a summary. Research in sentence extraction can be traced back to work in the mid 1950s and late 1960s by Luhn (1956) and Edmundson (1969). Recent techniques are startlingly not terribly divergent from these original methods; see Mani and Maybury (1999); Marcu (2000); Mani (2001) for a comprehensive overview. Headline generation systems, on the other hand, typically extract individual words from a document to produce a very short headline-style summary; see Banko, Mittal, and Witbrock (2000); Berger and Mittal (2000); Schwartz, Zajic, and Dorr (2002) for representative examples. Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and Bangalore 1996; Mahes</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>Edmundson, H. P. 1969. New methods in automatic abstracting. Journal of the Association for Computing Machinery, 16(2):264–285. Reprinted in Advances in Automatic Text Summarization, I. Mani and M. T. Maybury, (eds.).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL),</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="39063" citStr="Eisner 2003" startWordPosition="6231" endWordPosition="6232">e model null states identically to the relative jump model. 4.1.3 Syntax-Aware Jump Model. Both of the previously described jump models are extremely naive in that they look only at the distance jumped and completely ignore what is being jumped over. In the syntax-aware jump model, we wish to enable the model to take advantage of syntactic knowledge in a very weak fashion. This is quite different from the various approaches to incorporating syntactic knowledge into machine translation systems, wherein strong assumptions about the possible syntactic operations are made (Yamada and Knight 2001; Eisner 2003; Gildea 2003). To motivate this model, consider the first document sentence shown with its syntactic parse tree in Figure 6. Though it is not always the case, forward jumps of distance more than one are often indicative of skipped words. From the standpoint of the relative jump models, jumping over the four words tripled it ’s sales and jumping over the four words of Apple Macintosh systems are exactly the same.7 However, intuitively, we would be much more willing to jump over the latter than the former. The latter phrase is a full syntactic constituent, while the first phrase is just a colle</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Eisner, Jason. 2003. Learning non-isomorphic tree mappings for machine translation. In Proceedings of the Conference of the Association for Computational Linguistics (ACL), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack D Ferguson</author>
</authors>
<title>Variable duration models for speech.</title>
<date>1980</date>
<booktitle>In Proceedings of the Symposium on the Application of Hidden Markov Models to Text and Speech,</booktitle>
<pages>143--179</pages>
<location>Princeton, NJ.</location>
<contexts>
<context position="26054" citStr="Ferguson (1980)" startWordPosition="4080" endWordPosition="4081">d in Section 4 to enable the focus here to be on the more general problems of inference and decoding in such a model. The model described by these independence assumptions very much resembles that of a hidden Markov model (HMM), where states in the state space are document ranges and emissions are summary words. The difference is that instead of generating a single word in each transition between states, an entire phrase is generated. This difference is captured by the semi-Markov model or segmental HMM framework, described in great detail by Ostendorf, Digalakis, and Kimball (1996); see also Ferguson (1980); Gales and Young (1993); Mitchell, Jamieson, and Harper (1995); Smyth, Heckerman, and Jordan (1997); Ge and Smyth (2000); Aydin, Altunbasak, and Borodovsky (2004) for more detailed descriptions of these models as well as other applications in speech processing and computational biology. In the following subsections, we will briefly discuss the aspects of inference that are relevant to our problem, but the interested reader is directed to Ostendorf, Digalakis, and Kimball (1996) for more details. 3.3 Creating the State Space Given our generative story, we can construct a semi-HMM to calculate </context>
</contexts>
<marker>Ferguson, 1980</marker>
<rawString>Ferguson, Jack D. 1980. Variable duration models for speech. In Proceedings of the Symposium on the Application of Hidden Markov Models to Text and Speech, pages 143–179, October, Princeton, NJ.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mark J F Gales</author>
<author>Steve J Young 1993</author>
</authors>
<title>The theory of segmental hidden Markov models.</title>
<tech>Technical report,</tech>
<institution>Cambridge University Engineering Department.</institution>
<marker>Gales, 1993, </marker>
<rawString>Gales, Mark J. F. and Steve J. Young.1993. The theory of segmental hidden Markov models. Technical report, Cambridge University Engineering Department.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Luc Gauvain</author>
<author>Chin-Hui Lee</author>
</authors>
<title>Maximum a-posteriori estimation for multivariate Gaussian mixture observations of Markov chains.</title>
<date>1994</date>
<booktitle>IEEE Transactions Speech and Audio Processing,</booktitle>
<pages>2--291</pages>
<contexts>
<context position="49252" citStr="Gauvain and Lee 1994" startWordPosition="7933" endWordPosition="7936"> the original j model, but a “fake count” of αj − 1 has been added to component j. This means that if we are able to express our prior beliefs about the multinomial parameters found in the t-table in the form of a Dirichlet distribution, the computation of the MAP solution can be performed exactly as described before, but with the appropriate fake counts added to the observed variables (in our case, the observed variables are the alignments between a document phrase and a summary phrase). The application of Dirichlet priors to standard HMMs has previously been considered in signal processing (Gauvain and Lee 1994). These fake counts act as a smoothing parameter, similar to Laplace smoothing (Laplace smoothing is the special case where αj = 2 for all j). In our case, we believe that singleton rewrites are worth 2 fake counts, that lexical identity rewrites are worth 4 fake counts and that stem identity rewrites are worth 3 fake counts. Indeed, since a singleton alignment between identical words satisfies all of these criteria, it will receive a fake count of 9. The selection of these counts is intuitive, but clearly arbitrary. However, this selection was not “tuned” to the data to get better performance</context>
</contexts>
<marker>Gauvain, Lee, 1994</marker>
<rawString>Gauvain, Jean-Luc and Chin-Hui Lee. 1994. Maximum a-posteriori estimation for multivariate Gaussian mixture observations of Markov chains. IEEE Transactions Speech and Audio Processing, 2:291–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianping Ge</author>
<author>Padhraic Smyth</author>
</authors>
<title>Segmental semi-Markov models for change-point detection with applications to semiconductor manufacturing.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>University of California at Irvine,</institution>
<contexts>
<context position="26175" citStr="Ge and Smyth (2000)" startWordPosition="4096" endWordPosition="4099"> The model described by these independence assumptions very much resembles that of a hidden Markov model (HMM), where states in the state space are document ranges and emissions are summary words. The difference is that instead of generating a single word in each transition between states, an entire phrase is generated. This difference is captured by the semi-Markov model or segmental HMM framework, described in great detail by Ostendorf, Digalakis, and Kimball (1996); see also Ferguson (1980); Gales and Young (1993); Mitchell, Jamieson, and Harper (1995); Smyth, Heckerman, and Jordan (1997); Ge and Smyth (2000); Aydin, Altunbasak, and Borodovsky (2004) for more detailed descriptions of these models as well as other applications in speech processing and computational biology. In the following subsections, we will briefly discuss the aspects of inference that are relevant to our problem, but the interested reader is directed to Ostendorf, Digalakis, and Kimball (1996) for more details. 3.3 Creating the State Space Given our generative story, we can construct a semi-HMM to calculate precisely the alignment probabilities specified by our model in an efficient manner. A semi-HMM is 513 Computational Ling</context>
</contexts>
<marker>Ge, Smyth, 2000</marker>
<rawString>Ge, Xianping and Padhraic Smyth. 2000. Segmental semi-Markov models for change-point detection with applications to semiconductor manufacturing. Technical report, University of California at Irvine, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Loosely tree-based alignment for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL),</booktitle>
<pages>80--87</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="39077" citStr="Gildea 2003" startWordPosition="6233" endWordPosition="6234">states identically to the relative jump model. 4.1.3 Syntax-Aware Jump Model. Both of the previously described jump models are extremely naive in that they look only at the distance jumped and completely ignore what is being jumped over. In the syntax-aware jump model, we wish to enable the model to take advantage of syntactic knowledge in a very weak fashion. This is quite different from the various approaches to incorporating syntactic knowledge into machine translation systems, wherein strong assumptions about the possible syntactic operations are made (Yamada and Knight 2001; Eisner 2003; Gildea 2003). To motivate this model, consider the first document sentence shown with its syntactic parse tree in Figure 6. Though it is not always the case, forward jumps of distance more than one are often indicative of skipped words. From the standpoint of the relative jump models, jumping over the four words tripled it ’s sales and jumping over the four words of Apple Macintosh systems are exactly the same.7 However, intuitively, we would be much more willing to jump over the latter than the former. The latter phrase is a full syntactic constituent, while the first phrase is just a collection of nearb</context>
</contexts>
<marker>Gildea, 2003</marker>
<rawString>Gildea, Daniel. 2003. Loosely tree-based alignment for machine translation. In Proceedings of the Conference of the Association for Computational Linguistics (ACL), pages 80–87, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Producing intelligent telegraphic text reduction to provide an audio scanning service for the blind.</title>
<date>1998</date>
<booktitle>In Working Notes of the AAAI Spring Symposium on Intelligent Text Summarization,</booktitle>
<pages>111--118</pages>
<institution>Stanford University,</institution>
<location>Stanford, CA.</location>
<contexts>
<context position="6849" citStr="Grefenstette 1998" startWordPosition="1021" endWordPosition="1022">lingly not terribly divergent from these original methods; see Mani and Maybury (1999); Marcu (2000); Mani (2001) for a comprehensive overview. Headline generation systems, on the other hand, typically extract individual words from a document to produce a very short headline-style summary; see Banko, Mittal, and Witbrock (2000); Berger and Mittal (2000); Schwartz, Zajic, and Dorr (2002) for representative examples. Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and Bangalore 1996; Mahesh 1997; Carroll et al. 1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression (Daum´e III and Marcu 2002; Daum´e III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases, and sentences are selected in an extraction process. While such approaches have enjoyed some success, they all suffer from modeling shortcomings. Sentence extraction systems and document compression models make unrealistic assumptions about the summarization task (namely, that extraction is sufficient and that sentences are the appropriate level of granularity). Headline generation systems employ very weak models that make </context>
</contexts>
<marker>Grefenstette, 1998</marker>
<rawString>Grefenstette, Gregory. 1998. Producing intelligent telegraphic text reduction to provide an audio scanning service for the blind. In Working Notes of the AAAI Spring Symposium on Intelligent Text Summarization, pages 111–118, Stanford University, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L W V Jensen</author>
</authors>
<title>Sur les fonctions convexes et les in´egalit´es entre les valeurs moyennes.</title>
<date>1906</date>
<journal>Acta Mathematica,</journal>
<pages>30--175</pages>
<contexts>
<context position="29566" citStr="Jensen 1906" startWordPosition="4671" endWordPosition="4672">, and then use this guess to estimate the parameters for the relevant distributions. We can use these re-estimated distributions to make a better guess at the alignments, and then use these (ideally better) alignments to re-estimate the parameters. Figure 4 Schematic drawing of the semi-HMM (with some transition probabilities) for the document a b. 514 Daum´e and Marcu Alignments for Automatic Document Summarization Formally, the EM family of algorithms tightly bound the log of an expectation of a function by the expectation of the log of that function, through the use of Jensen’s inequality (Jensen 1906). The tightness of the bound means that when we attempt to estimate the model parameters, we may do so over expected alignments, rather than the true (but unknown) alignments. EM gives formal guarantees of convergence, but is only guaranteed to find local maxima. 3.5 Model Inference All the inference techniques utilized in this paper are standard applications of semiMarkov model techniques. The relevant equations are summarized in Figure 5 and described here. In all these equations, the variables t and t&apos; range over phrases in the summary (specifically, the phrase st:t,), and the variables i a</context>
</contexts>
<marker>Jensen, 1906</marker>
<rawString>Jensen, J. L. W. V. 1906. Sur les fonctions convexes et les in´egalit´es entre les valeurs moyennes. Acta Mathematica, 30:175–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>310--315</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="6860" citStr="Jing 2000" startWordPosition="1023" endWordPosition="1024"> divergent from these original methods; see Mani and Maybury (1999); Marcu (2000); Mani (2001) for a comprehensive overview. Headline generation systems, on the other hand, typically extract individual words from a document to produce a very short headline-style summary; see Banko, Mittal, and Witbrock (2000); Berger and Mittal (2000); Schwartz, Zajic, and Dorr (2002) for representative examples. Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and Bangalore 1996; Mahesh 1997; Carroll et al. 1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression (Daum´e III and Marcu 2002; Daum´e III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases, and sentences are selected in an extraction process. While such approaches have enjoyed some success, they all suffer from modeling shortcomings. Sentence extraction systems and document compression models make unrealistic assumptions about the summarization task (namely, that extraction is sufficient and that sentences are the appropriate level of granularity). Headline generation systems employ very weak models that make an incorrec</context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>Jing, Hongyan. 2000. Sentence reduction for automatic text summarization. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 310–315, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Using hidden Markov modeling to decompose human-written summaries.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="10292" citStr="Jing (2002)" startWordPosition="1545" endWordPosition="1546">rtz, Zajic, and Dorr 2002), based on IBM Model 1 (Brown et al. 1993). These models treat documents and headlines as simple bags of words and learn probabilistic word-based mappings between the words in the documents and the words in the headlines. Such mappings can be considered word-to-word alignments, but as our results show (see Section 5), these models are too weak for capturing the sophisticated operations that are employed by humans in summarizing texts. To date, there has been very little work on the word alignment task in the context of summarization. The most relevant work is that of Jing (2002), in which a hidden Markov alignment model is applied to the task of identifying word and phrase-level correspondences between documents and abstracts. Unfortunately, this model is only able to align words that are identical up to their stems, and thus suffers from a problem of recall. This also makes it ill-suited to the task of learning how to perform abstraction, in which one would desire to know how words get changed. For example, Jing’s model cannot identify any of the following alignments from Figure 1: (Connecting Point &lt;-4 Connecting Point Systems), (Mac &lt;-4 Macintosh), (retailer &lt;-4 s</context>
<context position="13001" citStr="Jing 2002" startWordPosition="1953" endWordPosition="1954">nown in the machine translation alignment literature, and a third one that exploits syntax in a novel, “light” manner. We also discuss several models of lexical rewriting, based on identities, stems, WordNet synonymy, and automatically induced lexical replacements. In Section 5, we present experimental results that confirm that our model is able to learn the hidden structure in our corpus of (document, abstract) pairs. We compare our model against well-known alignment models designed for machine translation as well as a state-of-the-art alignment model specifically designed for summarization (Jing 2002). Additionally, we discuss errors that the model currently makes, supported by some relevant examples and statistics. We conclude with some directions for future research (Section 6). 508 Daum´e and Marcu Alignments for Automatic Document Summarization Table 1 Ziff-Davis corpus statistics. Sub-corpus Annotated Abstracts Documents Abstracts Documents Documents 2033 45 Sentences 13k 82k 244 2k Words 261k 2.5M 6.4k 49k Unique words 14k 42k 1.9k 5.9k 45k 6k Sentences/Doc 6.28 40.83 5.42 45.3 Words/Doc 128.52 1229.71 142.33 1986.16 Words/Sent 20.47 28.36 26.25 24.20 2. Human-produced Alignments In </context>
<context position="52472" citStr="Jing (2002)" startWordPosition="8478" endWordPosition="8479">ult to align function words and punctuation. A list of 58 function words and punctuation marks that appeared in the corpus (henceforth called the ignore-list) was assembled. We computed precision and recall scores both on all words and on all words that do not appear in the ignore-list. 5.1 Systems Compared Overall, we compare various parameter settings of our model against three other systems. First, we compare against two alignment models developed in the context of machine translation. Second, we compare against the Cut and Paste model developed in the context of “summary decomposition” by Jing (2002). Each of these systems will be discussed in more detail shortly. However, the machine translation alignment models assume sentence pairs as input. Moreover, even though the semi-Markov model is based on efficient dynamic programming techniques, it is still too inefficient to run on very long (document, abstract) pairs. To alleviate both of these problems, we preprocess our (document, abstract) corpus down to an (extract, abstract) corpus, and then subsequently apply our models to this smaller corpus (see Figure 7). In our data, doing so does not introduce significant noise. To generate the ex</context>
<context position="55877" citStr="Jing 2002" startWordPosition="9034" endWordPosition="9035">fertility. We have performed experiments in both directions, but the latter (document as source) performs better in general. In order to seed the machine translation model so that it knows that word identity is a good solution, we appended our corpus with sentence pairs consisting of one source word and one target word, which were identical. This is common practice in the machine translation community when one wishes to cheaply encode knowledge from a dictionary into the alignment model. 5.1.2 Cut and Paste Model. We also tested alignments using the Cut and Paste summary decomposition method (Jing 2002), based on a non-trainable HMM. Briefly, 9 Interestingly, the Cut and Paste method actually achieves higher performance scores when run on only the extracts rather than the full documents. 523 Computational Linguistics Volume 31, Number 4 Table 3 Ziff-Davis extract corpus statistics. Abstracts Extracts Documents Documents 2033 2033 Sentences 13k 41k 82k Words 261k 1M 2.5M Unique words 14k 26k 42k 29k Sentences/Doc 6.28 21.51 40.83 Words/Doc 128.52 510.99 1229.71 Words/Sent 20.47 23.77 28.36 the Cut and Paste HMM searches for long contiguous blocks of words in the document and abstract that are</context>
<context position="67479" citStr="Jing 2002" startWordPosition="10911" endWordPosition="10912">nd lexical correspondences. Unlike the approaches used in machine translation, where only very simple models are used, we have shown how to efficiently and effectively leverage such disparate knowledge sources and WordNet, syntax trees, and identity models. We have empirically demonstrated that our model is able to learn the complex structure of (document, abstract) pairs. Our system outperforms competing approaches, including the standard machine translation alignment models (Brown et al. 1993; Vogel, Ney, and Tillmann 1996) and the state-of-the-art Cut and Paste summary alignment technique (Jing 2002). We have analyzed two sources of error in our model, including issues of nullgenerated summary words and lexical identity. Within the model itself, we have already suggested two major sources of error in our alignment procedure. Clearly more work needs to be done to fix these problems. One approach that we believe will be particularly fruitful would be to add a fifth model to the linearly interpolated rewrite model based on lists of synonyms automatically extracted from large corpora. Additionally, 527 Computational Linguistics Volume 31, Number 4 investigating the possibility of including so</context>
</contexts>
<marker>Jing, 2002</marker>
<rawString>Jing, Hongyan. 2002. Using hidden Markov modeling to decompose human-written summaries. Computational Linguistics, 28(4):527–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: A probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="6884" citStr="Knight and Marcu 2002" startWordPosition="1025" endWordPosition="1028">from these original methods; see Mani and Maybury (1999); Marcu (2000); Mani (2001) for a comprehensive overview. Headline generation systems, on the other hand, typically extract individual words from a document to produce a very short headline-style summary; see Banko, Mittal, and Witbrock (2000); Berger and Mittal (2000); Schwartz, Zajic, and Dorr (2002) for representative examples. Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and Bangalore 1996; Mahesh 1997; Carroll et al. 1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression (Daum´e III and Marcu 2002; Daum´e III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases, and sentences are selected in an extraction process. While such approaches have enjoyed some success, they all suffer from modeling shortcomings. Sentence extraction systems and document compression models make unrealistic assumptions about the summarization task (namely, that extraction is sufficient and that sentences are the appropriate level of granularity). Headline generation systems employ very weak models that make an incorrect bag-of-words assumptio</context>
<context position="8544" citStr="Knight and Marcu (2002)" startWordPosition="1276" endWordPosition="1279">our own work on document compression models (Daum´e III and Marcu 2002; Daum´e III and Marcu 2004), both of which extend the sentence compression model of Knight and Marcu (2002), we assume that sentences and documents can be summarized exclusively through deletion of contiguous text segments. In Knight and Marcu’s data, we found that from a corpus of 39,060 abstract sentences, only 1,067 were created from corresponding document sentences via deletion of contiguous segments. In other words, only 2.7% of the sentences in real (document, abstract) pairs can be explained by the model proposed by Knight and Marcu (2002). Such document compression models do not explain the rich set of linguistic devices employed, for example, in Figure 1. 1.3 Prior Work on Alignments In the sentence extraction community, there exists a wide variety of techniques for (essentially) creating alignments between document sentences and abstract sentences (Kupiec, Pedersen, and Chen 1995; Teufel and Moens 1997; Marcu 1999); see also Barzilay and Elhadad (2003); Quirk, Brockett, and Dolan (2004) for work describing alignments for the monolingual paraphrasing task. These techniques typically take into account information such as lexic</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Knight, Kevin and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to sentence compression. Artificial Intelligence, 139(1):91–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan O Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of the Annual ACM Conference on Research and Development in Information Retrieval,</booktitle>
<pages>68--73</pages>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Kupiec, Julian, Jan O. Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of the Annual ACM Conference on Research and Development in Information Retrieval, pages 68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1956</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<pages>58--63</pages>
<editor>In I. Mani and M. Maybury, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="6182" citStr="Luhn (1956)" startWordPosition="925" endWordPosition="926"> we have treated “it’s”/“its” as a special case: “its” (the possessive) is converted to “it POSS.” 506 Daum´e and Marcu Alignments for Automatic Document Summarization 1.2 Shortcomings of Current Summarization Models Current state-of-the-art automatic single-document summarization systems employ one of three techniques: sentence extraction, bag-of-words headline generation, or document compression. Sentence extraction systems take full sentences from a document and concatenate them to form a summary. Research in sentence extraction can be traced back to work in the mid 1950s and late 1960s by Luhn (1956) and Edmundson (1969). Recent techniques are startlingly not terribly divergent from these original methods; see Mani and Maybury (1999); Marcu (2000); Mani (2001) for a comprehensive overview. Headline generation systems, on the other hand, typically extract individual words from a document to produce a very short headline-style summary; see Banko, Mittal, and Witbrock (2000); Berger and Mittal (2000); Schwartz, Zajic, and Dorr (2002) for representative examples. Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and </context>
</contexts>
<marker>Luhn, 1956</marker>
<rawString>Luhn, H. P. 1956. The automatic creation of literature abstracts. In I. Mani and M. Maybury, editors, Advances in Automatic Text Summarization. MIT Press, Cambridge, MA, pages 58–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kavi Mahesh</author>
</authors>
<title>Hypertext summary extraction for fast document browsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Natural Language Processing for the World Wide Web,</booktitle>
<pages>95--103</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="6809" citStr="Mahesh 1997" startWordPosition="1015" endWordPosition="1016">1969). Recent techniques are startlingly not terribly divergent from these original methods; see Mani and Maybury (1999); Marcu (2000); Mani (2001) for a comprehensive overview. Headline generation systems, on the other hand, typically extract individual words from a document to produce a very short headline-style summary; see Banko, Mittal, and Witbrock (2000); Berger and Mittal (2000); Schwartz, Zajic, and Dorr (2002) for representative examples. Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and Bangalore 1996; Mahesh 1997; Carroll et al. 1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression (Daum´e III and Marcu 2002; Daum´e III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases, and sentences are selected in an extraction process. While such approaches have enjoyed some success, they all suffer from modeling shortcomings. Sentence extraction systems and document compression models make unrealistic assumptions about the summarization task (namely, that extraction is sufficient and that sentences are the appropriate level of granularity). Headline generation sy</context>
</contexts>
<marker>Mahesh, 1997</marker>
<rawString>Mahesh, Kavi. 1997. Hypertext summary extraction for fast document browsing. In Proceedings of the AAAI Spring Symposium on Natural Language Processing for the World Wide Web, pages 95–103, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Automatic Summarization,</title>
<date>2001</date>
<journal>of Natural Language Processing. John Benjamins, Amsterdam/Philadelphia.</journal>
<volume>3</volume>
<contexts>
<context position="6345" citStr="Mani (2001)" startWordPosition="948" endWordPosition="949">tion 1.2 Shortcomings of Current Summarization Models Current state-of-the-art automatic single-document summarization systems employ one of three techniques: sentence extraction, bag-of-words headline generation, or document compression. Sentence extraction systems take full sentences from a document and concatenate them to form a summary. Research in sentence extraction can be traced back to work in the mid 1950s and late 1960s by Luhn (1956) and Edmundson (1969). Recent techniques are startlingly not terribly divergent from these original methods; see Mani and Maybury (1999); Marcu (2000); Mani (2001) for a comprehensive overview. Headline generation systems, on the other hand, typically extract individual words from a document to produce a very short headline-style summary; see Banko, Mittal, and Witbrock (2000); Berger and Mittal (2000); Schwartz, Zajic, and Dorr (2002) for representative examples. Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and Bangalore 1996; Mahesh 1997; Carroll et al. 1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression (Daum´e III and Marcu 2002; Daum´e </context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Mani, Inderjeet. 2001. Automatic Summarization, volume 3 of Natural Language Processing. John Benjamins, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Mark Maybury</author>
<author>editors</author>
</authors>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Mani, Maybury, editors, 1999</marker>
<rawString>Mani, Inderjeet and Mark Maybury, editors. 1999. Advances in Automatic Text Summarization. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The automatic construction of large-scale corpora for summarization research.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Conference on Research and Development in Information Retrieval (SIGIR–99),</booktitle>
<pages>137--144</pages>
<location>Berkeley, CA,</location>
<contexts>
<context position="8930" citStr="Marcu 1999" startWordPosition="1335" endWordPosition="1336">reated from corresponding document sentences via deletion of contiguous segments. In other words, only 2.7% of the sentences in real (document, abstract) pairs can be explained by the model proposed by Knight and Marcu (2002). Such document compression models do not explain the rich set of linguistic devices employed, for example, in Figure 1. 1.3 Prior Work on Alignments In the sentence extraction community, there exists a wide variety of techniques for (essentially) creating alignments between document sentences and abstract sentences (Kupiec, Pedersen, and Chen 1995; Teufel and Moens 1997; Marcu 1999); see also Barzilay and Elhadad (2003); Quirk, Brockett, and Dolan (2004) for work describing alignments for the monolingual paraphrasing task. These techniques typically take into account information such as lexical overlap, synonymy, ordering, length, discourse structure, and so forth. The sentence alignment problem is a comparatively simple problem to solve, and current approaches work quite well. Unfortunately, these 507 Computational Linguistics Volume 31, Number 4 alignments are the least useful, because they can only be used to train sentence extraction systems. In the context of headli</context>
<context position="13983" citStr="Marcu 1999" startWordPosition="2101" endWordPosition="2102">entences 13k 82k 244 2k Words 261k 2.5M 6.4k 49k Unique words 14k 42k 1.9k 5.9k 45k 6k Sentences/Doc 6.28 40.83 5.42 45.3 Words/Doc 128.52 1229.71 142.33 1986.16 Words/Sent 20.47 28.36 26.25 24.20 2. Human-produced Alignments In order to decide how to design an alignment model and to judge the quality of the alignments produced by a system, we first need to create a set of “gold standard” alignments. To this end, we asked two human annotators to manually construct such alignments between documents and their abstracts. These (document, abstract) pairs were drawn from the Ziff-Davis collection (Marcu 1999). Of the roughly 7,000 documents in that corpus, we randomly selected 45 pairs for annotation. We added to this set of 45 pairs the 2,000 shorter documents from this collection, and all the work described in the remainder of this paper focuses on this subset of 2,033 (document, abstract) pairs.3 Statistics for this sub-corpus and for the pairs selected for annotation are shown in Table 1. As can be simply computed from this table, the compression rate in this corpus is about 12%. The first five human-produced alignments were completed separately and then discussed; the last 40 were done indepe</context>
<context position="53222" citStr="Marcu (1999)" startWordPosition="8591" endWordPosition="8592">as input. Moreover, even though the semi-Markov model is based on efficient dynamic programming techniques, it is still too inefficient to run on very long (document, abstract) pairs. To alleviate both of these problems, we preprocess our (document, abstract) corpus down to an (extract, abstract) corpus, and then subsequently apply our models to this smaller corpus (see Figure 7). In our data, doing so does not introduce significant noise. To generate the extracts, we paired each abstract sentence with three sentences from the corresponding document, selected using the techniques described by Marcu (1999). In an informal evaluation, 20 such pairs were randomly extracted and evaluated by a human. Each pair was ranked as 0 (document sentences contain little to none of the information in the abstract sentence), 1 (document sentences contain some of the information in the abstract sentence) or 2 (document sentences contain all of the 522 Daum´e and Marcu Alignments for Automatic Document Summarization Figure 7 Pictorial representation of the conversion of the (document, abstract) corpus to an (extract, abstract) corpus. information). Of the 20 random examples, none were labeled as 0; 5 were labele</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Marcu, Daniel. 1999. The automatic construction of large-scale corpora for summarization research. In Proceedings of the 22nd Conference on Research and Development in Information Retrieval (SIGIR–99), pages 137–144, Berkeley, CA, August 15–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6332" citStr="Marcu (2000)" startWordPosition="946" endWordPosition="947">ment Summarization 1.2 Shortcomings of Current Summarization Models Current state-of-the-art automatic single-document summarization systems employ one of three techniques: sentence extraction, bag-of-words headline generation, or document compression. Sentence extraction systems take full sentences from a document and concatenate them to form a summary. Research in sentence extraction can be traced back to work in the mid 1950s and late 1960s by Luhn (1956) and Edmundson (1969). Recent techniques are startlingly not terribly divergent from these original methods; see Mani and Maybury (1999); Marcu (2000); Mani (2001) for a comprehensive overview. Headline generation systems, on the other hand, typically extract individual words from a document to produce a very short headline-style summary; see Banko, Mittal, and Witbrock (2000); Berger and Mittal (2000); Schwartz, Zajic, and Dorr (2002) for representative examples. Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and Bangalore 1996; Mahesh 1997; Carroll et al. 1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression (Daum´e III and Marcu </context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Marcu, Daniel. 2000. The Theory and Practice of Discourse Parsing and Summarization. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl D Mitchell</author>
<author>Leah H Jamieson</author>
<author>Mary P Harper</author>
</authors>
<title>On the complexity of explicit duration HMMs.</title>
<date>1995</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>3</volume>
<issue>3</issue>
<marker>Mitchell, Jamieson, Harper, 1995</marker>
<rawString>Mitchell, Carl D., Leah H. Jamieson, and Mary P. Harper. 1995. On the complexity of explicit duration HMMs. IEEE Transactions on Speech and Audio Processing, 3(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL),</booktitle>
<pages>440--447</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="69616" citStr="Och and Ney 2000" startWordPosition="11260" endWordPosition="11263">. By doing so, one could recreate the extracts at each iteration using the previous iteration’s parameters to make better and shorter extracts. Similarly, one might only allow summary words to align to words found in their corresponding extract sentences, which would serve to significantly speed up training and, combined with the parameterized extracts, might not hurt performance. A final option, but one that we do not advocate, would be to give up on phrases and train the model in a word-to-word fashion. This could be coupled with heuristic phrasal creation as is done in machine translation (Och and Ney 2000), but by doing this, one completely loses the probabilistic interpretation that makes this model so pleasing. Aside from computational considerations, the most obvious future effort along the lines of this model is to incorporate it into a full document summarization system. Since this can be done in many ways, including training extraction systems, compression systems, headline generation systems, and even extraction systems, we left this to future work so that we could focus specifically on the alignment task in this article. Nevertheless, the true usefulness of this model will be borne out </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the Conference of the Association for Computational Linguistics (ACL), pages 440–447, October, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="11290" citStr="Och and Ney 2003" startWordPosition="1701" endWordPosition="1704"> would desire to know how words get changed. For example, Jing’s model cannot identify any of the following alignments from Figure 1: (Connecting Point &lt;-4 Connecting Point Systems), (Mac &lt;-4 Macintosh), (retailer &lt;-4 seller), (Macintosh &lt;-4 Apple Macintosh systems) and (January 1989 &lt;-4 last January). Word alignment (and, to a lesser degree, phrase alignment) has been an active topic of research in the machine translation community. Based on these efforts, one might be initially tempted to use readily available alignment models developed in the context of machine translation, such as GIZA++ (Och and Ney 2003), to obtain wordlevel alignments in (document, abstract) corpora. However, as we will show (Section 5), the alignments produced by such a system are inadequate for the (document, abstract) alignment task. 1.4 Article Structure In this article, we describe a novel, general model for automatically inducing wordand phrase-level alignments between documents and their human-written abstracts. Beginning in Section 2, we will describe the results of human annotation of such alignments. Based on this annotation, we will investigate the empirical linguistic properties of such alignments, including lexi</context>
<context position="14896" citStr="Och and Ney 2003" startWordPosition="2244" endWordPosition="2247"> Statistics for this sub-corpus and for the pairs selected for annotation are shown in Table 1. As can be simply computed from this table, the compression rate in this corpus is about 12%. The first five human-produced alignments were completed separately and then discussed; the last 40 were done independently. 2.1 Annotation Guidelines Annotators were asked to perform word-to-word and phrase-to-phrase alignments between abstracts and documents, and to classify each alignment as either possible (P) or sure (S), where S C_ P, following the methodology used in the machine translation community (Och and Ney 2003). The direction of containment (S C_ P) is because being a sure alignment is a stronger requirement than being a possible alignment. A full description of the annotation guidelines is available in a document available with the alignment software on the first author’s web site (http://www.isi.edu/—hdaume/HandAlign). Here, we summarize the main points. The most important instruction that annotators were given was to align everything in the summary to something. This was not always possible, as we will discuss shortly, but by and large it was an appropriate heuristic. The second major instruction</context>
<context position="54401" citStr="Och and Ney 2003" startWordPosition="8783" endWordPosition="8786"> none were labeled as 0; 5 were labeled as 1; and 15 were labeled as 2, giving a mean rating of 1.75. We refer to the resulting corpus as the (extract, abstract) corpus, statistics for which are shown in Table 3. Finally, for fair comparison, we also run the Cut and Paste model only on the extracts.9 5.1.1 Machine Translation Models. We compare against several competing systems, the first of which is based on the original IBM Model 4 for machine translation (Brown et al. 1993) and the HMM machine translation alignment model (Vogel, Ney, and Tillmann 1996) as implemented in the GIZA++ package (Och and Ney 2003). We modified the code slightly to allow for longer inputs and higher fertilities, but otherwise made no changes. In all of these setups, 5 iterations of Model 1 were run, followed by five iterations of the HMM model. For Model 4, 5 iterations of Model 4 were subsequently run. In our model, the distinction between the summary and the document is clear, but when using a model from machine translation, it is unclear which of the summary and the document should be considered the source language and which should be considered the target language. By making the summary the source language, we are e</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari Ostendorf</author>
<author>Vassilis Digalakis</author>
<author>Owen Kimball</author>
</authors>
<title>From HMMs to segment models: A unified view of stochastic modeling for speech recognition.</title>
<date>1996</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>4</volume>
<issue>5</issue>
<marker>Ostendorf, Digalakis, Kimball, 1996</marker>
<rawString>Ostendorf, Mari, Vassilis Digalakis, and Owen Kimball. 1996. From HMMs to segment models: A unified view of stochastic modeling for speech recognition. IEEE Transactions on Speech and Audio Processing, 4(5):360–378, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>142--149</pages>
<location>Barcelona,</location>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Quirk, Chris, Chris Brockett, and William Dolan. 2004. Monolingual machine translation for paraphrase generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–149, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Schwartz</author>
<author>David Zajic</author>
<author>Bonnie Dorr</author>
</authors>
<title>Automatic headline generation for newspaper stories.</title>
<date>2002</date>
<booktitle>In Proceedings of the Document Understanding Conference (DUC),</booktitle>
<pages>78--85</pages>
<location>Philadelphia.</location>
<marker>Schwartz, Zajic, Dorr, 2002</marker>
<rawString>Schwartz, Richard, David Zajic, and Bonnie Dorr. 2002. Automatic headline generation for newspaper stories. In Proceedings of the Document Understanding Conference (DUC), pages 78–85, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Padhraic Smyth</author>
<author>David Heckerman</author>
<author>Michael I Jordan</author>
</authors>
<title>Probabilistic independence networks for hidden Markov probability models.</title>
<date>1997</date>
<journal>Neural Computation,</journal>
<volume>9</volume>
<issue>2</issue>
<marker>Smyth, Heckerman, Jordan, 1997</marker>
<rawString>Smyth, Padhraic, David Heckerman, and Michael I. Jordan. 1997. Probabilistic independence networks for hidden Markov probability models. Neural Computation, 9(2):227–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Mark Moens</author>
</authors>
<title>Sentence extraction as a classification task. In</title>
<date>1997</date>
<booktitle>In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization,</booktitle>
<pages>58--65</pages>
<contexts>
<context position="8917" citStr="Teufel and Moens 1997" startWordPosition="1331" endWordPosition="1334">nces, only 1,067 were created from corresponding document sentences via deletion of contiguous segments. In other words, only 2.7% of the sentences in real (document, abstract) pairs can be explained by the model proposed by Knight and Marcu (2002). Such document compression models do not explain the rich set of linguistic devices employed, for example, in Figure 1. 1.3 Prior Work on Alignments In the sentence extraction community, there exists a wide variety of techniques for (essentially) creating alignments between document sentences and abstract sentences (Kupiec, Pedersen, and Chen 1995; Teufel and Moens 1997; Marcu 1999); see also Barzilay and Elhadad (2003); Quirk, Brockett, and Dolan (2004) for work describing alignments for the monolingual paraphrasing task. These techniques typically take into account information such as lexical overlap, synonymy, ordering, length, discourse structure, and so forth. The sentence alignment problem is a comparatively simple problem to solve, and current approaches work quite well. Unfortunately, these 507 Computational Linguistics Volume 31, Number 4 alignments are the least useful, because they can only be used to train sentence extraction systems. In the cont</context>
</contexts>
<marker>Teufel, Moens, 1997</marker>
<rawString>Teufel, Simone and Mark Moens. 1997. Sentence extraction as a classification task. In In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization, pages 58–65.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph</author>
</authors>
<title>Tillmann.1996. HMM-based word alignment in statistical translation.</title>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>836--841</pages>
<marker>Vogel, Ney, Christoph, </marker>
<rawString>Vogel, Stephan, Hermann Ney, and Christoph Tillmann.1996. HMM-based word alignment in statistical translation. In Proceedings of the International Conference on Computational Linguistics (COLING), pages 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff C F Wu</author>
</authors>
<title>On the convergence properties of the EM algorithm. The Annals of Statistics,</title>
<date>1983</date>
<pages>11--95</pages>
<contexts>
<context position="28901" citStr="Wu 1983" startWordPosition="4564" endWordPosition="4565">uring the transition (start) --� a, c d is emitted. During the transition a --� (end), ω is emitted. 3.4 Expectation Maximization The alignment task, as described above, is a chicken-and-egg problem: if we knew the model components (namely, the rewrite and jump tables), we would be able to efficiently find the best alignment. Similarly, if we knew the correct alignments, we would be able to estimate the model components. Unfortunately, we have neither. Expectation maximization is a general technique for learning in such chicken-and-egg situations (Dempster, Laird, and Rubin 1977; Boyles 1983; Wu 1983). The basic idea is to make a guess at the alignments, and then use this guess to estimate the parameters for the relevant distributions. We can use these re-estimated distributions to make a better guess at the alignments, and then use these (ideally better) alignments to re-estimate the parameters. Figure 4 Schematic drawing of the semi-HMM (with some transition probabilities) for the document a b. 514 Daum´e and Marcu Alignments for Automatic Document Summarization Formally, the EM family of algorithms tightly bound the log of an expectation of a function by the expectation of the log of th</context>
</contexts>
<marker>Wu, 1983</marker>
<rawString>Wu, Jeff C. F. 1983. On the convergence properties of the EM algorithm. The Annals of Statistics, 11:95–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL),</booktitle>
<pages>523--530</pages>
<contexts>
<context position="39050" citStr="Yamada and Knight 2001" startWordPosition="6227" endWordPosition="6230">ts empirical variance. We model null states identically to the relative jump model. 4.1.3 Syntax-Aware Jump Model. Both of the previously described jump models are extremely naive in that they look only at the distance jumped and completely ignore what is being jumped over. In the syntax-aware jump model, we wish to enable the model to take advantage of syntactic knowledge in a very weak fashion. This is quite different from the various approaches to incorporating syntactic knowledge into machine translation systems, wherein strong assumptions about the possible syntactic operations are made (Yamada and Knight 2001; Eisner 2003; Gildea 2003). To motivate this model, consider the first document sentence shown with its syntactic parse tree in Figure 6. Though it is not always the case, forward jumps of distance more than one are often indicative of skipped words. From the standpoint of the relative jump models, jumping over the four words tripled it ’s sales and jumping over the four words of Apple Macintosh systems are exactly the same.7 However, intuitively, we would be much more willing to jump over the latter than the former. The latter phrase is a full syntactic constituent, while the first phrase is</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Yamada, Kenji and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the Conference of the Association for Computational Linguistics (ACL), pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Zajic</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>BBN/UMD at DUC-2004: Topiary.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth Document Understanding Conference (DUC 2004),</booktitle>
<pages>112--119</pages>
<location>Boston, MA,</location>
<marker>Zajic, Dorr, Schwartz, 2004</marker>
<rawString>Zajic, David, Bonnie Dorr, and Richard Schwartz. 2004. BBN/UMD at DUC-2004: Topiary. In Proceedings of the Fourth Document Understanding Conference (DUC 2004), pages 112–119, Boston, MA, May 6–7.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>