<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000319">
<title confidence="0.99498">
Reducing Grounded Learning Tasks to Grammatical Inference
</title>
<author confidence="0.998004">
Benjamin Börschinger
</author>
<affiliation confidence="0.996476">
Department of Computing
Macquarie University
</affiliation>
<address confidence="0.566085">
Sydney, Australia
</address>
<email confidence="0.996425">
benjamin.borschinger@mq.edu.au
</email>
<author confidence="0.986537">
Bevan K. Jones
</author>
<affiliation confidence="0.848118666666667">
School of Informatics
University of Edinburgh
Edinburgh, UK
</affiliation>
<email confidence="0.995674">
b.k.jones@sms.ed.ac.uk
</email>
<author confidence="0.996623">
Mark Johnson
</author>
<affiliation confidence="0.99645">
Department of Computing
Macquarie University
</affiliation>
<address confidence="0.566641">
Sydney, Australia
</address>
<email confidence="0.997718">
mark.johnson@mq.edu.au
</email>
<sectionHeader confidence="0.995625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998947785714286">
It is often assumed that ‘grounded’ learning
tasks are beyond the scope of grammatical in-
ference techniques. In this paper, we show
that the grounded task of learning a seman-
tic parser from ambiguous training data as dis-
cussed in Kim and Mooney (2010) can be re-
duced to a Probabilistic Context-Free Gram-
mar learning task in a way that gives state
of the art results. We further show that ad-
ditionally letting our model learn the lan-
guage’s canonical word order improves its
performance and leads to the highest seman-
tic parsing f-scores previously reported in the
literature.1
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950642857143">
One of the most fundamental ideas about language
is that we use it to express our thoughts. Learning a
natural language, then, amounts to (at least) learning
a mapping between the things we utter and the things
we think, and can therefore be seen as the task of
learning a semantic parser, i.e. something that maps
natural language expressions such as sentences into
meaning representations such as logical forms. Ob-
viously, this learning can neither take place in a fully
supervised nor in a fully unsupervised fashion: the
learner does not ‘hear’ the meanings of the sentences
she observes, but she is also not treating them as
merely meaningless strings. Rather, it seems plau-
sible to assume that she uses extra-linguistic context
</bodyText>
<footnote confidence="0.9864825">
1The source code used for our experiments and the evalua-
tion is available as supplementary material for this article.
</footnote>
<bodyText confidence="0.999020029411764">
to assign certain meanings to the linguistic input she
is confronted with.
In this sense, learning a semantic parser seems
to go beyond the well-studied task of unsupervised
grammar induction. It involves not only learning
a grammar for the form-side of language, i.e. lan-
guage expressions such as sentences, but also the
‘grounding’ of this structure in meaning represen-
tations. It requires going beyond the mere linguistic
input to incorporate, for example, perceptual infor-
mation that provides a clue to the meaning of the ob-
served forms. Essentially, it seems as if ‘grounded’
learning tasks like this require dealing with two
different kinds of information, the purely formal
(phonemic) and meaningful (semantic) aspects of
language. Grammatical inference seems to be lim-
ited to dealing with one level of formal information
(Chang and Maia, 2001). For this reason, probably,
approaches to the task of learning a semantic parser
employ a variety of sophisticated and task-specific
techniques that go beyond (but often elaborate on)
the techniques used for grammatical inference (Lu
et al., 2008; Chen and Mooney, 2008; Liang et al.,
2009; Kim and Mooney, 2010; Chen et al., 2010).
In this paper, we show that one can reduce the
task of learning a semantic parser to a Probabilistic
Context Free Grammar (PCFG) learning task, and
more generally, that grounded learning tasks are not
in principle beyond the scope of grammatical infer-
ence techniques. In particular, we show how to for-
mulate the task of learning a semantic parser as dis-
cussed by Chen, Kim and Mooney (2008, 2010) as
the task of learning a PCFG from strings. Our model
does not only constitute a proof of concept that this
</bodyText>
<page confidence="0.978876">
1416
</page>
<note confidence="0.9578785">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1416–1425,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999870346153846">
reduction is possible for certain cases, it also yields
highly competitive results.2
By reducing the problem to the well understood
PCFG formalism, it also becomes easy to consider
extensions, leading to our second contribution. We
demonstrate that a slight modification to our model
so that it also learns the language’s canonical word
order improves its performance even beyond the best
results previously reported in the literature. This
language-independent and linguistically well moti-
vated elaboration allows the model to learn a global
fact about the language’s syntax, its canonical word
order.
Our contribution is two-fold. We provide an illus-
tration of how to reduce grounded learning tasks to
grammatical inference. Secondly, we show that ex-
tending the model so that it can learn linguistically
well motivated generalizations such as the canonical
word order can lead to better results.
The structure of the paper is as follows. First we
give a short overview of the previous work by Chen,
Kim and Mooney and describe their dataset. Then,
we show how to reduce the parsing task addressed
by them to a PCFG-learning task. Finally, we ex-
plain how to let our model additionally learn the lan-
guage’s canonical word order.
</bodyText>
<sectionHeader confidence="0.8753455" genericHeader="introduction">
2 Previous Work by Chen, Kim and
Mooney
</sectionHeader>
<bodyText confidence="0.945900333333333">
In a series of recent papers, Chen, Kim and Mooney
approach the task of learning a semantic parser from
ambiguous training data (Chen and Mooney, 2008;
Kim and Mooney, 2010; Chen et al., 2010). This
goes beyond previous work on semantic parsing
such as Lu et al. (2008) or Zettlemoyer and Collins
(2005) which rely on unambiguous training data
where every sentence is paired only with its mean-
ing. In contrast, Chen, Kim and Mooney allow
their training examples to exhibit the kind of uncer-
tainty about sentence meanings human learners are
likely to have to deal with by allowing for sentences
to be associated with a set of candidate-meanings,
2It has been pointed out to us by one reviewer that the task
we address falls short of what is often called ‘grounded learn-
ing’. We acknowledge that semantic parsing constitutes a very
limited kind of grounded learning but want to point out that the
task has been introduced as an instance of grounded learning in
the previous literature such as Chen and Mooney (2008).
and the correct meaning might not even be in this
set. They create the training data by first collect-
ing humanly generated written language comments
on four different RoboCup games. The comments
are recorded with a time-stamp and then associated
with all game events automatically extracted from
the games which occured up to five seconds before
the comment was made. This leads to an ambigu-
ous pairing of comments with candidate meanings
that can be considered similar to the &amp;quot;linguistic in-
put in the context of a rich, relevant, perceptual en-
vironment&amp;quot; to which real language learners prob-
ably have access (Chen and Mooney, 2008). For
evaluation purposes, they manually create a gold-
standard which contains unambiguous natural lan-
guage comment / event pairs. Due to the fact
that some comments refer to events not detected
by their extraction-algorithm, not every natural lan-
guage sentence has a gold matching meaning repre-
sentation. In addition to the inherent ambiguity of
the training examples, the learner therefore has to
somehow deal with those examples which only have
‘wrong’ meanings associated with them.
Datasets exist for both Korean and English, each
comprising training and gold data for four games.3
Some details about this data are given in Table 1,
such as the number of examples, their average am-
biguity and the number of misleading examples.
For the following short discussion of previous ap-
proaches, we mainly focus on Kim and Mooney
(2010). This is the most recent publication and re-
ports the highest scores.
</bodyText>
<subsectionHeader confidence="0.996083">
2.1 The parsing task
</subsectionHeader>
<bodyText confidence="0.999689727272727">
Learning a semantic parser from the ambiguous data
is, in fact, just one of three tasks discussed by Kim
and Mooney (2010), henceforth KM. In addition to
parsing, they discuss matching and natural language
generation. We are ignoring the generation task as
we are currently only interested in the parsing prob-
lem, and we treat the matching task, picking the cor-
rect meaning from the set of candidates, merely as
a byproduct of parsing, rather than as a completely
separate task: parsing implicitly requires the model
to disambiguate the data it is learning from.
</bodyText>
<footnote confidence="0.999615">
3The datasets are freely available at http://www.cs.
utexas.edu/~ml/clamp/sportscasting/. We re-
trieved the data used here on March 29th, 2011.
</footnote>
<page confidence="0.982243">
1417
</page>
<table confidence="0.999359857142857">
Number of comments Ambiguity
# Training # Training with # Training with # Gold Noise Avg. # of MRs
Gold Match correct MR
English dataset
total 1872 1492 1360 1539 0.2735 2.20
Korean dataset
total 1914 1763 1733 1763 0.0946 2.39
</table>
<tableCaption confidence="0.692562125">
Table 1: Statistics for the Korean and the English datasets. The numbers are basically identical to those reported in
Chen et al. (2010) except for minimal differences in the number of training examples (we give one more for every
English training set, and one more for the 2004 Korean training set). In addition, our calculation of the average
sentential ambiguity (Avg. # of MRs) differs because we assume that mutiple occurences of the same event in a
context do not add to the overall ambiguity, and our calculation of the noise (fraction of training examples without
the correct meaning in their context) takes into account that there are training examples which do not have their gold
meaning associated with them in the training data and is therefore slightly higher than the one reported in Chen et al.
(2010).
</tableCaption>
<bodyText confidence="0.999932">
KM’s model builds on previous work by Lu et al.
(2008) and is a generative model which defines a
joint probability distribution over natural language
sentences (NLs), meaning representations (MRs)
and hybrid trees. The NLs are the natural language
comments to the games, the MRs are simple log-
ical formulae describing game events and playing
the role of sentence meanings, and a hybrid tree is
a tree structure that represents the correspondence
between a sentence and its meaning. More specif-
ically, if some NL W has as its meaning an MR
m, and m has been generated by a meaning gram-
mar (MG) G, the hybrid tree corresponding to the
pair (W,m) has as its internal nodes those rules of
G used in the derivation of m, and as its leaves the
words making up W.4 An example hybrid tree for
the pair (THE PINK GOALIE PASSES THE BALL TO
PINK11,pass(pink1,pink11)) is given in Figure 1.
Their model is trained by a variant of the Inside-
Outside algorithm which deals with the hybrid tree
structure and takes into account the ambiguity of the
training examples.
In addition to learning directly from the ambigu-
ous training data, they also train a semantic parser
in a supervised fashion on data that has been pre-
viously disambiguated by their matching model.
This slightly improves their system’s performance.
Consequently, there are two scores for each of the
</bodyText>
<equation confidence="0.973941">
-+ pink11
1 1
H E PI
</equation>
<bodyText confidence="0.321044333333333">
4We use SMALL CAPS for words, sans serif for MRs and
MR constituents (concepts), and italics for non-terminals and
Grammars.
</bodyText>
<figure confidence="0.8492972">
S
S -+ pass PLAYER PLAYER
PLAYER PASSES THE BALL TO PLAYER
PLAYER -+ pink1 PLAYER
T PINK
</figure>
<page confidence="0.956678">
1418
</page>
<bodyText confidence="0.978372921568628">
niques from strings which incorporate the contextual
information. In this section, we show how to reduce
hybrid trees to such ‘standard’ trees. In effect, we
show via construction that ‘grounded’ learning tasks
such as learning a semantic parser from semantically
enriched and ambiguous data can be reduced to ‘un-
grounded’ tasks such as grammatical inference.
Instead of taking the internal nodes of the trees
generated by our model as corresponding to MG
production rules, we take them to correspond to MR
constituents. The MR pass(pink1,pink11), for exam-
ple, has 4 constituents: the whole MR, the predicate
pass, and the two arguments pink1 and pink11. Fig-
ure 2 gives the tree we assume instead of Figure 1
for the sentence-meaning pair (THE PINK GOALIE
PASSES THE BALL TO PINK11,pass(pink1,pink11)).
Its root is assumed to correspond to the whole
MR and is labeled Spass(pink1,pink11). The remain-
ing three MR constituents correspond to the root’s
daughters which we label Phrasepink1, Phrasepass
and Phrasepink11. Generally speaking, we assume a
special non-terminal Sm for every MR m generated
by the MG, and a special non-terminal Phrasecon for
each of the terminals of the MG (which loosely cor-
respond to concepts). This is only possible for MGs
which create a finite set of MRs, but the MG used by
Kim and Mooney (2010) obeys this restriction.5
The tree’s terminals are the words that make up
the sentence, and we assume them to be dominated
by concept-specific pre-terminals Wordcon which
correspond to concept-specific probability distribu-
tions over the language’s vocabulary. Since each
Phrasecon may span multiple words, we give trees
rooted in Phrasecon a left-recursive structure that
corresponds to a unigram Markov-process. This
process generates an arbitrary sequence of words
semantically related to con, dominated by the cor-
responding pre-terminal Wordcon in our model, and
words not directly semantically related to con, dom-
inated by a special word pre-terminal Word∅. The
sole further restriction is that every Phrasecon must
contain at least one Wordcon.
Trees like the one in Figure 2 can be generated by
a Context-Free Grammar (CFG) which, in turn, can
be trained on strings to yield a PCFG which embod-
5This grammar is given in the Appendix to Chen et al.
(2010) and generates a total of 2048 MRs.
ies a semantic parser as will be discussed in Section
3.3. We now describe how to set up such a CFG in a
systematic way and how to train it on the data used
by KM.
</bodyText>
<subsectionHeader confidence="0.997938">
3.1 Setting up the PCFG
</subsectionHeader>
<bodyText confidence="0.999868051282051">
The training data expresses information of two dif-
ferent kinds – form and meaning. Every training ex-
ample consists of a natural language string (the for-
mal information) and a set of candidate meanings
for the string (the semantic information, its context),
allowing for the possibility that none of the mean-
ings in the context is the correct one. In order to
learn from data like this within a grammatical in-
ference framework, we have to encode the semantic
information as part of the string. Assigning a spe-
cific MR m to a string corresponds, in our frame-
work, to analyzing it as a tree with 5,,,, as its root.
A sentence’s context constrains which of the many
possible meanings might be expressed by the string.
Thus the role played by the context is adequately
modelled if we ensure that if a string W is associated
with a context {m1,...,mn}, the model only considers
the possibilities that this string might be analyzed as
Sm1,...,Smn.
There are 959 different contexts, i.e. 959 dif-
ferent sets of MRs, in the English data set (984
for the Korean data), and we therefore introduce
959 new terminal symbols which play the role of
context-identifiers, for example C1 to C959.6 For-
mally speaking, a context-identifier is a terminal
like any other word of the language and we can
therefore prefix every comment in the training data
with the context-identifier standing for the set of
MRs associated with this comment, an idea taken
from previous work such as Johnson et al. (2010).
Thus having incorporated the contextual informa-
tion into the string, we go on to show how our model
makes use of this information, considering the MR
pass(pink1,pink11) as an example. A formal de-
scription of the model is given Figure 3.
Assume that pass(pink1,pink11) is associated
with only one training example and therefore occurs
only in one specific context. If the context-identifier
introduced for this context is C1, we require the
</bodyText>
<footnote confidence="0.855593666666667">
6If we were to consider every possible context, we would
have to consider 22048 contexts because the MG generates 2048
MRs.
</footnote>
<page confidence="0.989363">
1419
</page>
<figure confidence="0.99905625">
Root
Spass(pink1,pink11)
Phr
a sep
</figure>
<page confidence="0.943502">
1420
</page>
<bodyText confidence="0.9996491875">
schemata used to generate the CFG is given in Fig-
ure 3.7 Instantiating all those schemata leads to a
grammar with 33,101 rules for the English data and
30,731 rules for the Korean data. The difference in
size is due to differences in the size of the vocabu-
lary and the different number of contexts in the data
sets.
These CFGs can now be trained on the training
data using the Inside-Outside algorithm (Lari and
Young, 1990). After training, the resulting PCFG
embodies a semantic parser in the sense that, with
a slight modification we describe in section 3.3, it
can be used to parse a string into its meaning rep-
resentation by determining the most likely syntactic
analysis and reading off the meaning assigned by our
model at the Sm-node.
</bodyText>
<subsectionHeader confidence="0.997401">
3.2 Possible objections to our reduction
</subsectionHeader>
<bodyText confidence="0.999315551724138">
Before we go on to discuss the details of training
and evaluation of our model, we want to address an
objection that might seem tempting. Isn’t our reduc-
tion impractical and unrealistic as even a highly ab-
stract model of language learning – after all, setting
up the huge CFG requires knowledge about the vo-
cabulary, the MG and all the complicated rules dis-
cussed which, presumably, is more knowledge than
we want to provide a language learner with, lest we
trivialize the task. To this we reply firstly, that it is
true that our reduction only works for offline or batch
grounded learning tasks where all the data is avail-
able to the model before the actual learning begins
so that it ‘knows’ the words, the meanings and the
contexts present in the data. This offline constraint
is, however, true of all models which are trained by
iterating multiple times over training data such as
KM’s model. Secondly, the intimidating CFG can in
principle be reduced to a hand-full of intuitive prin-
ciples and is easy to generate automatically.
First of all, the many specific Sm-rewrite rules re-
duce to the heuristic that every semantic constituent
should correspond to a syntactic constituent, and the
fact that natural language expressions are linearly or-
dered. Note that our model does not contain knowl-
edge about the specific word order of the language.
7In our description, we use context-identifiers such as C1
with a systematic ambiguity, letting them stand for the terminal
symbol representing a context and, in contexts such as M∈C1,
for the represented context itself.
It simply allows for the constituents of an MR to oc-
cur in every possible order which is a very unbiased
and empiricist assumption. Of course, this leads to
some limited kind of ‘implicit learning’ of word or-
der in the sense that for every meaning and for every
context, our model might (and in most cases will) as-
sign different probabilities to the different rules for
every word order; so it can learn that certain specific
MRs such as pass(pink1,pink11) are more often lin-
earized in one way than in any other. It cannot, how-
ever, generalize this to other (or even unseen) MRs,
i.e. it does not learn a global fact about the language.
In a way, it lacks the knowledge that there is such a
thing as word order, a point which we will elaborate
on in Section 4.
The many re-write rules for the pre-terminal
Word,,s are nothing but an explicit version of the
assumption that every word the model encounters
might, in principle, be semantically related to every
concept it knows. Again, this seems to us to be a
reasonable assumption.
Finally, the complicated looking set of rules for
the internal structure of Phrase,,s corresponds to
a simple unigram Markov-process for generating
strings. All in all, we do not see that we make any
more assumptions than other approaches; our for-
mulation may make explicit how rich those assump-
tions are but we have not qualitatively changed them.
</bodyText>
<subsectionHeader confidence="0.997723">
3.3 Training and Evaluation
</subsectionHeader>
<bodyText confidence="0.9999964">
The CFG described in the previous section is trained
on the same training data used by KM, except that
we reduce it to strings (without changing the infor-
mation present in the original data) by prefixing ev-
ery sentence with a context-identifier. For training
we run the Inside-Outside algorithm8 with uniform
initialization weights until convergence. For En-
glish, this results in an average number of 76 itera-
tions for each fold, for Korean the average number of
iterations is 50. To deal with the fact that the model
might not observe certain meanings during training,
we apply a simple smoothing technique by using a
Dirichlet prior of α=0.1 on the rule probabilities. In
effect, this provides our system with a small number
of pseudo-observations for each rule which prevents
</bodyText>
<footnote confidence="0.635147333333333">
8We use Mark Johnson’s freely available implementa-
tion, available at http://web.science.mq.edu.au/
~mjohnson/Software.htm.
</footnote>
<page confidence="0.846315">
1421
</page>
<equation confidence="0.949261875">
Root → Sm m ∈ M ∪ {∅}
Sm → c Phrasep(m) c ∈ C, m ∈ c, m ∈ Pred0(M)
Sm → c {Phrasep(m), Phrasea1(m)} c ∈ C, m ∈ c, m ∈ Pred1(M)
Sm → c {Phrasep(m), Phrasea1(m), Phrasea2(m)} c ∈ C, m ∈ c, m ∈ Pred2(M)
S∅ → c Phrase∅ c ∈ C
Phrase∅ → Word∅
Phrase∅ → Phrase∅ Word∅
Phrasex → Wordx x ∈ T
Phrasex → PhXx Wordx x ∈ T
Phrasex → Phx Word∅ x ∈ T
PhXx → Wordr x ∈ T, r ∈ {x, ∅}
PhXx → PhXx Wordr x ∈ T, r ∈ {x, ∅}
Phx → PhXx Wordx x ∈ T
Phx → Phx Word∅ x ∈ T
Phx → Wordx x ∈ T
Wordx → v x ∈ T ∪ {∅},v ∈ V
</equation>
<figureCaption confidence="0.996763142857143">
Figure 3: The rule-schemata used to generate the NoWo-PCFG. Root is the unique start-symbol, M is the set of all
MRs present in the corpus, C is set the of all context-identifiers present in the corpus, T is the set of terminals of the
MG, V is the vocabulary of the corpus. Pred0(M) is the subset of all MRs in M of the form predicate, Pred1(M)
is the subset of all MRs in M of the form predicate(arg1) and Pred2(M) is the subset of all MRs in M of the form
predicate(arg1,arg2). p(m) is the predicate of the MR m, a1(m) is the first argument of the MR m, a2(m) is the
second argument of the MR m. The rules expanding Phrasex ensure that it contains at least one Wordx. A set on the
right-hand side of a rule is shorthand for all possible orderings of the elements of the set.
</figureCaption>
<bodyText confidence="0.970838666666667">
the automatic assignment of zero probability to rules
not used during training.9
For parsing, the resulting PCFG is slightly mod-
ified by removing the context-identifiers. This is
done because the task of a semantic parser is to es-
tablish a mapping between NLs and MRs, irrespec-
tive of contexts which were only used for learning
the parser and should not play a role in its final per-
formance. To do this, we add up the probability of
all rules which differ only in the context-identifier
which can be thought of as marginalizing out the dif-
ferent contexts, giving our first model which we call
NoWo-PCFG.10
Note that the context-deletion (and the simple
smoothing) enables NoWo-PCFG to parse sentences
into meanings not present in the data it was trained
on which, in fact, happens. For example, there are
81 meanings in the training data for the first English
9We experimented with α=0.1, α=0.5 and α=1.0 and found
that overall, 0.1 yields the best results. We also tried jittering
the initial rule weights during training and found that our re-
sults are very robust and seem to be independent of a specific
initialization.
10NoWo because this model, unlike the one described in Sec-
tion 4, does not make explicit use of word order generalisa-
tions.
match that are not present in any of the other games’
training data. The PCFG trained on games 2, 3 and 4
is still able to correctly assign 12 of those 81 mean-
ings which it has not seen during the training phase
which shows the effectiveness of the bottom-up con-
straint.
For evaluation, we employ 4-fold cross validation
as described in detail in Chen and Mooney (2008)
and used by KM: the model is trained on all possible
combinations of 3 of the 4 games and is then used
to produce an MR for all sentences of the held-out
game for which there is a matching gold-standard
meaning. For an NL W, our model produces an MR
m by finding the most probable parse of W with the
CKY algorithm and reading off m at the Sm-node.11
An MR is considered correct if and only if it matches
the gold-standard MR exactly; the final evaluation
result is averaged over all 4 folds. Our evaluation
results for NoWo-PCFG are given in Table 2. All
scores are reported in F-measure which is the har-
monic mean of Precision and Recall. In this specific
case, precision is the fraction of correct parses out
</bodyText>
<footnote confidence="0.870171">
11For parsing, we use Mark Johnson’s freely available CKY
implementation which can be downloaded at http://web.
science.mq.edu.au/~mjohnson/Software.htm.
</footnote>
<page confidence="0.944806">
1422
</page>
<table confidence="0.9993715">
English Korean
KM 0.742 0.764
KM ‘supervised’ 0.810 0.808
Chen et al. (2010) 0.801 0.812
NoWo-PCFG 0.742 0.718
WO-PCFG 0.860 0.829
</table>
<tableCaption confidence="0.981034714285714">
Table 2: A summary of results for the parsing task, in F-
measure. We also show the results of Chen et al. (2010),
as given in Kim and Mooney (2010), which to our knowl-
edge are the highest previously reported scores for Ko-
rean. WO-PCFG, described in Section 4 performs better
than all previously reported models, but only slightly so
for Korean.
</tableCaption>
<bodyText confidence="0.999959045454546">
of the total number of parses the model returns. Re-
call is the fraction of correct parses out of the total
number of test sentences.12
NoWo-PCFG performs a little worse than KM’s
model. Its scores are virtually identical for English
(0.742) and worse for Korean (0.718 vs 0.764). We
are not sure as to why our model performs worse on
the Korean data, but it might have to do with the fact
that the Korean average ambiguity is higher than for
the English data.
This shows that it is not only possible to re-
duce the task of learning a semantic parser to stan-
dard grammatical inference, but that this way of ap-
proaching the problem yields comparable results.
The remainder of the paper focuses on our second
main point: that letting the model learn additional
kinds of information, such as the language’s canoni-
cal word order, can further improve its performance.
In order to do this we propose a model that learns
the word order as well as the mapping from NLs
to MRs, and compare its performance to that of the
other models.
</bodyText>
<sectionHeader confidence="0.960025" genericHeader="method">
4 Extending NoWo-PCFG to WO-PCFG
</sectionHeader>
<bodyText confidence="0.994961263157895">
We already pointed out that our model considers ev-
ery possible linear order of syntactic constituents.
Our NoWo-PCFG model considers each of the pos-
sible word orders for every meaning and context in
isolation: it is unable to infer from the fact that most
meanings it has observed are most likely to be ex-
pressed with a certain word order that new meanings
12Because our model parses every sentence, for it Recall and
Precision are identical and F-measure is identical to Accuracy.
it will encounter are also more likely to be expressed
with this word order. It seems, however, to be at
least a soft fact about languages that they do have
a canonical word order that is more likely to be re-
alized in its sentences than any other possible word
order. In order to test whether trying to learn this
order helps our model, we modify the CFG used for
NoWo-PCFG so it can learn word order generaliza-
tions, and train it in the same way to yield another
semantic parser, WO-PCFG.
</bodyText>
<subsectionHeader confidence="0.999902">
4.1 Setting up WO-PCFG
</subsectionHeader>
<bodyText confidence="0.976710333333333">
For every possible ordering of the constituents cor-
responding to an MR, our grammar contains a rule.
In NoWo-PCFG, these different rules all share the
same parent which prevents the model from learn-
ing the probability of the different word orders cor-
responding to the many rules. A straight-forward
way to overcome this is to annotate every Sm node
with the word order of its daughter. We split every
Sm non-terminal in multiple Swo_m non-terminals,
where wo ∈ {v,sv,vs,svo,sov,osv,ovs,vso,vos} indi-
cates the linear order of the constituents the non-
terminal rewrites as.13
This in itself does not yet allow our model to use
word order as a means of generalization. To model
that whenever it encounters a specific example that
is indicative of a certain word order, this word or-
der becomes slightly more probable for every other
example as well, we have to make a further slight
change to the CFG which we now describe. A for-
mally explicit description of the necessary changes
which we go on to describe is given in Figure 4.
We introduce six new non-terminals, correspond-
ing to the six possible word orders SVO, SOV, VSO,
VOS, OSV and OVS and require every Swo_m non-
terminal to be dominated by the non-terminal com-
patible with its daughters linear order. As an exam-
ple, consider the two syntactic non-terminals cor-
responding to the MR kick(pink1), Svs_kick(pink11)
and Ssv_kick(pink11). Whenever an example is suc-
cessfully analyzed as Svs_kick(pink11), this should
strengthen our model’s expectation of encountering
13We assume, somewhat simplifying, that an MR’s predicate
corresponds to a V(erb), its first argument corresponds to the
S(ubject) and its second argument corresponds to the O(bject).
These are purely formal categories that are not constrained to
correspond to specific linguistic categories.
</bodyText>
<page confidence="0.936897">
1423
</page>
<figure confidence="0.990640833333333">
Root → wo wo ∈ WO
wo → Sx_m wo ∈ WO, x ∈ WOS, x ⊂ wo, m ∈ M
Sv_m → c Phrasep(m) c ∈ C, m ∈ c, m ∈ Pred0(M)
Sx_m → c {Phrasep(m), Phrasea1(m)} c ∈ C, m ∈ c, m ∈ Pred1(M), x ∈ {sv, vs}
Sx_m → c {Phrasep(m), Phrasea1(m), Phrasea2(m)} c ∈ C, m ∈ c, m ∈ Pred2(M), x ∈ WOS
Sv_∅ → c Phrase∅ c ∈ C
</figure>
<figureCaption confidence="0.999609">
Figure 4: In order to turn NoWo-PCFG described in Figure 3 into the WO-PCFG described in the
</figureCaption>
<bodyText confidence="0.99042975">
text, substitute the first five rule-schemata with the schemata given here. WO is the set of word
order non-terminals {SV O, SOV, OSV, OVS, VSO, VOS}, WOS is the set of word order annotations
{v, sv, vs, svo, svo, ovs, osv, vso, vos}. We take x ⊂ wo to mean that x is compatible with wo, where v is com-
patible with all word orders, sv is compatible with SVO,SOV and OSV, and so on. For rule-schemata 4 and 5, the
choice of x determines the order of the elements of the set on the right-hand side. All other symbols have the same
meaning as explained in Figure 3.
more examples where the verb precedes the sub-
ject, i.e. of the language being pre-dominantly VSO,
VOS or OVS. Therefore, we allow VSO, VOS and
OVS to be rewritten as Svs_kick(pink11). More gener-
ally, every word order non-terminal can rewrite as
any of the Swo_m non-terminals that are compatible
with it. Adding this additional layer of word order
abstraction leads to a grammar with 36,019 rules for
English and a grammar with 33,715 rules for Ko-
rean.
</bodyText>
<subsectionHeader confidence="0.997964">
4.2 Evaluation of WO-PCFG
</subsectionHeader>
<bodyText confidence="0.999954857142858">
Training and evaluating WO-PCFG in exactly the
same way as the previous grammar gives an F-
measure of 0.860 for English and an F-measure of
0.829 for Korean. Those scores are, to our knowl-
edge, the highest scores previously reported for this
parsing task and establish our second main point:
letting the model learn the language’s word order in
addition to learning the mapping from sentences to
MR increases semantic parsing accuracy.14
An intuitive explanation for the increase in perfor-
mance is that by allowing the model to learn word
order, we are providing it with a new dimension
along which it can generalize.
In this sense, we can look at our refinement as
providing the model with abstract linguistic knowl-
edge, namely that languages tend to have a canon-
14Liang et al. (2009)’s model can be seen as capturing some-
thing similar to our word order generalization with the help of
a Field Choice Model which primarily captures discourse co-
herence and salience properties. It differs, however, in that it
can only learn one generalization for each predicate type and
no language wide generalization.
ical word order. The usefulness of this kind of in-
formation is impressive – for English, it improves
the accuracy of semantic parsing by almost 12% in
F-measure and for Korean by 11.1%. In addition,
our model correctly learns that English’s predomi-
nant word order is SVO and that Korean is predomi-
nantly SOV, assigning by far the highest probability
to the corresponding Root rewrite rule (0.91 for En-
glish and 0.98 for Korean). This kind of information
is useful in its own right and could, for example, be
exploited by coupling word order with other linguis-
tic properties, perhaps following Greenberg (1966)’s
implicational universals.
In this sense, the reduction of grounded learning
problems to grammatical inference does not only
make possible the application of a wide variety of
tools and insights developed over years of research,
it might also make it easier to bring abstract (and not
so abstract) linguistic knowledge to bear on those
tasks.
The overall slightly worse performance of our
system on Korean data might stem from the fact that
Korean, unlike English, has a rich morphology, and
that our model does not learn anything about mor-
phology at all. We plan on further investigating ef-
fects like this in the future, as well as applying more
advanced grammatical inference algorithms.
</bodyText>
<sectionHeader confidence="0.995377" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999915">
We have shown that certain grounded learning tasks
such as learning a semantic parser from semantically
enriched training data can be reduced to a gram-
matical inference problem over strings. This allows
</bodyText>
<page confidence="0.979894">
1424
</page>
<bodyText confidence="0.999982631578947">
for the application of techniques and insights devel-
oped for grammatical inference to grounded learn-
ing tasks. In addition, we have shown that letting
the model learn the language’s canonical word or-
der improves parsing performance, beyond the top
scores previously reported, thus illustrating the use-
fullnes of linguistic knowledge for tasks like this.
In future research, we plan to address the limi-
tation of our model to a finite set of meaning rep-
resentations, in particular through the use of non-
parametric Bayesian models such as the Infinite
PCFG model of Liang et al. (2007) and the Infi-
nite Tree model of Finkel et al. (2007); both allow
for a potentially infinite set of non-terminals, hence
directly addressing this problem. In addition, we
are thinking about using an extension of the PCFG
formalism that allows for some kind of ‘feature-
passing’ which could lead to much smaller and more
general grammars.
</bodyText>
<sectionHeader confidence="0.9991" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999778773584906">
N. C. Chang and T. V. Maia. 2001. Grounded learning
of grammatical constructions. In 2001 AAAI Spring
Symposium on Learning Grounded Representations.
David L. Chen and Raymond J. Mooney. 2008. Learning
to sportscast: A test of grounded language acquisition.
In Proceedings of the 25th International Conference
on Machine Learning (ICML).
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a multilingual sportscaster: Using per-
ceptual context to learn language. Journal ofArtificial
Intelligence Research, 37:397–435.
Jenny R. Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 272–279.
Joseph H. Greenberg. 1966. Some universals of gram-
mar with particular reference to the order of meaning-
ful elements. In Joseph H. Greenberg, editor, Univer-
sals of Language, chapter 5, pages 73–113. The MIT
Press, Cambridge, Massachusetts.
Mark Johnson, Katherine Demuth, Michael Frank, and
Bevan Jones. 2010. Synergies in learning words
and their referents. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors,
Advances in Neural Information Processing Systems
23, pages 1018–1026.
Joohyun Kim and Raymond J. Mooney. 2010. Genera-
tive alignment and semantic parsing for learning from
ambiguous supervision. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING 2010).
K. Lari and S.J. Young. 1990. The estimation of Stochas-
tic Context-Free Grammars using the Inside-Outside
algorithm. Computer Speech and Language, 4(35-56).
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 688–697.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proceedings of the 47th Annual Meeting of the
ACL and the 4th IJCNLP of the AFNLP.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A generative model for parsing natu-
ral language to meaning representations. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 783–792.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of UAI 2005.
</reference>
<page confidence="0.992499">
1425
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.314706">
<title confidence="0.999292">Reducing Grounded Learning Tasks to Grammatical Inference</title>
<author confidence="0.996626">Benjamin</author>
<affiliation confidence="0.888121">Department of Macquarie</affiliation>
<address confidence="0.93577">Sydney, Australia</address>
<email confidence="0.995982">benjamin.borschinger@mq.edu.au</email>
<author confidence="0.999582">K Bevan</author>
<affiliation confidence="0.999691">School of University of</affiliation>
<address confidence="0.757181">Edinburgh, UK</address>
<email confidence="0.992929">b.k.jones@sms.ed.ac.uk</email>
<author confidence="0.993057">Mark</author>
<affiliation confidence="0.8742515">Department of Macquarie</affiliation>
<address confidence="0.839431">Sydney, Australia</address>
<email confidence="0.996399">mark.johnson@mq.edu.au</email>
<abstract confidence="0.986412857142857">It is often assumed that ‘grounded’ learning tasks are beyond the scope of grammatical inference techniques. In this paper, we show that the grounded task of learning a semantic parser from ambiguous training data as discussed in Kim and Mooney (2010) can be reduced to a Probabilistic Context-Free Grammar learning task in a way that gives state of the art results. We further show that additionally letting our model learn the language’s canonical word order improves its performance and leads to the highest semantic parsing f-scores previously reported in the</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N C Chang</author>
<author>T V Maia</author>
</authors>
<title>Grounded learning of grammatical constructions.</title>
<date>2001</date>
<booktitle>In 2001 AAAI Spring Symposium on Learning Grounded Representations.</booktitle>
<contexts>
<context position="2674" citStr="Chang and Maia, 2001" startWordPosition="412" endWordPosition="415">ar for the form-side of language, i.e. language expressions such as sentences, but also the ‘grounding’ of this structure in meaning representations. It requires going beyond the mere linguistic input to incorporate, for example, perceptual information that provides a clue to the meaning of the observed forms. Essentially, it seems as if ‘grounded’ learning tasks like this require dealing with two different kinds of information, the purely formal (phonemic) and meaningful (semantic) aspects of language. Grammatical inference seems to be limited to dealing with one level of formal information (Chang and Maia, 2001). For this reason, probably, approaches to the task of learning a semantic parser employ a variety of sophisticated and task-specific techniques that go beyond (but often elaborate on) the techniques used for grammatical inference (Lu et al., 2008; Chen and Mooney, 2008; Liang et al., 2009; Kim and Mooney, 2010; Chen et al., 2010). In this paper, we show that one can reduce the task of learning a semantic parser to a Probabilistic Context Free Grammar (PCFG) learning task, and more generally, that grounded learning tasks are not in principle beyond the scope of grammatical inference techniques</context>
</contexts>
<marker>Chang, Maia, 2001</marker>
<rawString>N. C. Chang and T. V. Maia. 2001. Grounded learning of grammatical constructions. In 2001 AAAI Spring Symposium on Learning Grounded Representations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to sportscast: A test of grounded language acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="2944" citStr="Chen and Mooney, 2008" startWordPosition="454" endWordPosition="457">lue to the meaning of the observed forms. Essentially, it seems as if ‘grounded’ learning tasks like this require dealing with two different kinds of information, the purely formal (phonemic) and meaningful (semantic) aspects of language. Grammatical inference seems to be limited to dealing with one level of formal information (Chang and Maia, 2001). For this reason, probably, approaches to the task of learning a semantic parser employ a variety of sophisticated and task-specific techniques that go beyond (but often elaborate on) the techniques used for grammatical inference (Lu et al., 2008; Chen and Mooney, 2008; Liang et al., 2009; Kim and Mooney, 2010; Chen et al., 2010). In this paper, we show that one can reduce the task of learning a semantic parser to a Probabilistic Context Free Grammar (PCFG) learning task, and more generally, that grounded learning tasks are not in principle beyond the scope of grammatical inference techniques. In particular, we show how to formulate the task of learning a semantic parser as discussed by Chen, Kim and Mooney (2008, 2010) as the task of learning a PCFG from strings. Our model does not only constitute a proof of concept that this 1416 Proceedings of the 2011 C</context>
<context position="5135" citStr="Chen and Mooney, 2008" startWordPosition="813" endWordPosition="816">tically well motivated generalizations such as the canonical word order can lead to better results. The structure of the paper is as follows. First we give a short overview of the previous work by Chen, Kim and Mooney and describe their dataset. Then, we show how to reduce the parsing task addressed by them to a PCFG-learning task. Finally, we explain how to let our model additionally learn the language’s canonical word order. 2 Previous Work by Chen, Kim and Mooney In a series of recent papers, Chen, Kim and Mooney approach the task of learning a semantic parser from ambiguous training data (Chen and Mooney, 2008; Kim and Mooney, 2010; Chen et al., 2010). This goes beyond previous work on semantic parsing such as Lu et al. (2008) or Zettlemoyer and Collins (2005) which rely on unambiguous training data where every sentence is paired only with its meaning. In contrast, Chen, Kim and Mooney allow their training examples to exhibit the kind of uncertainty about sentence meanings human learners are likely to have to deal with by allowing for sentences to be associated with a set of candidate-meanings, 2It has been pointed out to us by one reviewer that the task we address falls short of what is often call</context>
<context position="6631" citStr="Chen and Mooney, 2008" startWordPosition="1067" endWordPosition="1070">orrect meaning might not even be in this set. They create the training data by first collecting humanly generated written language comments on four different RoboCup games. The comments are recorded with a time-stamp and then associated with all game events automatically extracted from the games which occured up to five seconds before the comment was made. This leads to an ambiguous pairing of comments with candidate meanings that can be considered similar to the &amp;quot;linguistic input in the context of a rich, relevant, perceptual environment&amp;quot; to which real language learners probably have access (Chen and Mooney, 2008). For evaluation purposes, they manually create a goldstandard which contains unambiguous natural language comment / event pairs. Due to the fact that some comments refer to events not detected by their extraction-algorithm, not every natural language sentence has a gold matching meaning representation. In addition to the inherent ambiguity of the training examples, the learner therefore has to somehow deal with those examples which only have ‘wrong’ meanings associated with them. Datasets exist for both Korean and English, each comprising training and gold data for four games.3 Some details a</context>
<context position="23059" citStr="Chen and Mooney (2008)" startWordPosition="3918" endWordPosition="3921">rule weights during training and found that our results are very robust and seem to be independent of a specific initialization. 10NoWo because this model, unlike the one described in Section 4, does not make explicit use of word order generalisations. match that are not present in any of the other games’ training data. The PCFG trained on games 2, 3 and 4 is still able to correctly assign 12 of those 81 meanings which it has not seen during the training phase which shows the effectiveness of the bottom-up constraint. For evaluation, we employ 4-fold cross validation as described in detail in Chen and Mooney (2008) and used by KM: the model is trained on all possible combinations of 3 of the 4 games and is then used to produce an MR for all sentences of the held-out game for which there is a matching gold-standard meaning. For an NL W, our model produces an MR m by finding the most probable parse of W with the CKY algorithm and reading off m at the Sm-node.11 An MR is considered correct if and only if it matches the gold-standard MR exactly; the final evaluation result is averaged over all 4 folds. Our evaluation results for NoWo-PCFG are given in Table 2. All scores are reported in F-measure which is t</context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>David L. Chen and Raymond J. Mooney. 2008. Learning to sportscast: A test of grounded language acquisition. In Proceedings of the 25th International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Joohyun Kim</author>
<author>Raymond J Mooney</author>
</authors>
<title>Training a multilingual sportscaster: Using perceptual context to learn language.</title>
<date>2010</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>37--397</pages>
<contexts>
<context position="3006" citStr="Chen et al., 2010" startWordPosition="466" endWordPosition="469">s if ‘grounded’ learning tasks like this require dealing with two different kinds of information, the purely formal (phonemic) and meaningful (semantic) aspects of language. Grammatical inference seems to be limited to dealing with one level of formal information (Chang and Maia, 2001). For this reason, probably, approaches to the task of learning a semantic parser employ a variety of sophisticated and task-specific techniques that go beyond (but often elaborate on) the techniques used for grammatical inference (Lu et al., 2008; Chen and Mooney, 2008; Liang et al., 2009; Kim and Mooney, 2010; Chen et al., 2010). In this paper, we show that one can reduce the task of learning a semantic parser to a Probabilistic Context Free Grammar (PCFG) learning task, and more generally, that grounded learning tasks are not in principle beyond the scope of grammatical inference techniques. In particular, we show how to formulate the task of learning a semantic parser as discussed by Chen, Kim and Mooney (2008, 2010) as the task of learning a PCFG from strings. Our model does not only constitute a proof of concept that this 1416 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</context>
<context position="5177" citStr="Chen et al., 2010" startWordPosition="821" endWordPosition="824">s the canonical word order can lead to better results. The structure of the paper is as follows. First we give a short overview of the previous work by Chen, Kim and Mooney and describe their dataset. Then, we show how to reduce the parsing task addressed by them to a PCFG-learning task. Finally, we explain how to let our model additionally learn the language’s canonical word order. 2 Previous Work by Chen, Kim and Mooney In a series of recent papers, Chen, Kim and Mooney approach the task of learning a semantic parser from ambiguous training data (Chen and Mooney, 2008; Kim and Mooney, 2010; Chen et al., 2010). This goes beyond previous work on semantic parsing such as Lu et al. (2008) or Zettlemoyer and Collins (2005) which rely on unambiguous training data where every sentence is paired only with its meaning. In contrast, Chen, Kim and Mooney allow their training examples to exhibit the kind of uncertainty about sentence meanings human learners are likely to have to deal with by allowing for sentences to be associated with a set of candidate-meanings, 2It has been pointed out to us by one reviewer that the task we address falls short of what is often called ‘grounded learning’. We acknowledge tha</context>
<context position="8628" citStr="Chen et al. (2010)" startWordPosition="1395" endWordPosition="1398">letely separate task: parsing implicitly requires the model to disambiguate the data it is learning from. 3The datasets are freely available at http://www.cs. utexas.edu/~ml/clamp/sportscasting/. We retrieved the data used here on March 29th, 2011. 1417 Number of comments Ambiguity # Training # Training with # Training with # Gold Noise Avg. # of MRs Gold Match correct MR English dataset total 1872 1492 1360 1539 0.2735 2.20 Korean dataset total 1914 1763 1733 1763 0.0946 2.39 Table 1: Statistics for the Korean and the English datasets. The numbers are basically identical to those reported in Chen et al. (2010) except for minimal differences in the number of training examples (we give one more for every English training set, and one more for the 2004 Korean training set). In addition, our calculation of the average sentential ambiguity (Avg. # of MRs) differs because we assume that mutiple occurences of the same event in a context do not add to the overall ambiguity, and our calculation of the noise (fraction of training examples without the correct meaning in their context) takes into account that there are training examples which do not have their gold meaning associated with them in the training </context>
<context position="13187" citStr="Chen et al. (2010)" startWordPosition="2157" endWordPosition="2160"> left-recursive structure that corresponds to a unigram Markov-process. This process generates an arbitrary sequence of words semantically related to con, dominated by the corresponding pre-terminal Wordcon in our model, and words not directly semantically related to con, dominated by a special word pre-terminal Word∅. The sole further restriction is that every Phrasecon must contain at least one Wordcon. Trees like the one in Figure 2 can be generated by a Context-Free Grammar (CFG) which, in turn, can be trained on strings to yield a PCFG which embod5This grammar is given in the Appendix to Chen et al. (2010) and generates a total of 2048 MRs. ies a semantic parser as will be discussed in Section 3.3. We now describe how to set up such a CFG in a systematic way and how to train it on the data used by KM. 3.1 Setting up the PCFG The training data expresses information of two different kinds – form and meaning. Every training example consists of a natural language string (the formal information) and a set of candidate meanings for the string (the semantic information, its context), allowing for the possibility that none of the meanings in the context is the correct one. In order to learn from data l</context>
<context position="24007" citStr="Chen et al. (2010)" startWordPosition="4082" endWordPosition="4085">f m at the Sm-node.11 An MR is considered correct if and only if it matches the gold-standard MR exactly; the final evaluation result is averaged over all 4 folds. Our evaluation results for NoWo-PCFG are given in Table 2. All scores are reported in F-measure which is the harmonic mean of Precision and Recall. In this specific case, precision is the fraction of correct parses out 11For parsing, we use Mark Johnson’s freely available CKY implementation which can be downloaded at http://web. science.mq.edu.au/~mjohnson/Software.htm. 1422 English Korean KM 0.742 0.764 KM ‘supervised’ 0.810 0.808 Chen et al. (2010) 0.801 0.812 NoWo-PCFG 0.742 0.718 WO-PCFG 0.860 0.829 Table 2: A summary of results for the parsing task, in Fmeasure. We also show the results of Chen et al. (2010), as given in Kim and Mooney (2010), which to our knowledge are the highest previously reported scores for Korean. WO-PCFG, described in Section 4 performs better than all previously reported models, but only slightly so for Korean. of the total number of parses the model returns. Recall is the fraction of correct parses out of the total number of test sentences.12 NoWo-PCFG performs a little worse than KM’s model. Its scores are </context>
</contexts>
<marker>Chen, Kim, Mooney, 2010</marker>
<rawString>David L. Chen, Joohyun Kim, and Raymond J. Mooney. 2010. Training a multilingual sportscaster: Using perceptual context to learn language. Journal ofArtificial Intelligence Research, 37:397–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Finkel</author>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>The infinite tree.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>272--279</pages>
<marker>Finkel, Grenager, Manning, 2007</marker>
<rawString>Jenny R. Finkel, Trond Grenager, and Christopher D. Manning. 2007. The infinite tree. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 272–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph H Greenberg</author>
</authors>
<title>Some universals of grammar with particular reference to the order of meaningful elements.</title>
<date>1966</date>
<booktitle>Universals of Language, chapter 5,</booktitle>
<pages>73--113</pages>
<editor>In Joseph H. Greenberg, editor,</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="31410" citStr="Greenberg (1966)" startWordPosition="5387" endWordPosition="5388">ation. ical word order. The usefulness of this kind of information is impressive – for English, it improves the accuracy of semantic parsing by almost 12% in F-measure and for Korean by 11.1%. In addition, our model correctly learns that English’s predominant word order is SVO and that Korean is predominantly SOV, assigning by far the highest probability to the corresponding Root rewrite rule (0.91 for English and 0.98 for Korean). This kind of information is useful in its own right and could, for example, be exploited by coupling word order with other linguistic properties, perhaps following Greenberg (1966)’s implicational universals. In this sense, the reduction of grounded learning problems to grammatical inference does not only make possible the application of a wide variety of tools and insights developed over years of research, it might also make it easier to bring abstract (and not so abstract) linguistic knowledge to bear on those tasks. The overall slightly worse performance of our system on Korean data might stem from the fact that Korean, unlike English, has a rich morphology, and that our model does not learn anything about morphology at all. We plan on further investigating effects l</context>
</contexts>
<marker>Greenberg, 1966</marker>
<rawString>Joseph H. Greenberg. 1966. Some universals of grammar with particular reference to the order of meaningful elements. In Joseph H. Greenberg, editor, Universals of Language, chapter 5, pages 73–113. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Katherine Demuth</author>
<author>Michael Frank</author>
<author>Bevan Jones</author>
</authors>
<title>Synergies in learning words and their referents.</title>
<date>2010</date>
<booktitle>Advances in Neural Information Processing Systems 23,</booktitle>
<pages>1018--1026</pages>
<editor>In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors,</editor>
<contexts>
<context position="14891" citStr="Johnson et al. (2010)" startWordPosition="2461" endWordPosition="2464">ly considers the possibilities that this string might be analyzed as Sm1,...,Smn. There are 959 different contexts, i.e. 959 different sets of MRs, in the English data set (984 for the Korean data), and we therefore introduce 959 new terminal symbols which play the role of context-identifiers, for example C1 to C959.6 Formally speaking, a context-identifier is a terminal like any other word of the language and we can therefore prefix every comment in the training data with the context-identifier standing for the set of MRs associated with this comment, an idea taken from previous work such as Johnson et al. (2010). Thus having incorporated the contextual information into the string, we go on to show how our model makes use of this information, considering the MR pass(pink1,pink11) as an example. A formal description of the model is given Figure 3. Assume that pass(pink1,pink11) is associated with only one training example and therefore occurs only in one specific context. If the context-identifier introduced for this context is C1, we require the 6If we were to consider every possible context, we would have to consider 22048 contexts because the MG generates 2048 MRs. 1419 Root Spass(pink1,pink11) Phr </context>
</contexts>
<marker>Johnson, Demuth, Frank, Jones, 2010</marker>
<rawString>Mark Johnson, Katherine Demuth, Michael Frank, and Bevan Jones. 2010. Synergies in learning words and their referents. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1018–1026.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joohyun Kim</author>
<author>Raymond J Mooney</author>
</authors>
<title>Generative alignment and semantic parsing for learning from ambiguous supervision.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="630" citStr="Kim and Mooney (2010)" startWordPosition="80" endWordPosition="83">Grounded Learning Tasks to Grammatical Inference Benjamin Börschinger Department of Computing Macquarie University Sydney, Australia benjamin.borschinger@mq.edu.au Bevan K. Jones School of Informatics University of Edinburgh Edinburgh, UK b.k.jones@sms.ed.ac.uk Mark Johnson Department of Computing Macquarie University Sydney, Australia mark.johnson@mq.edu.au Abstract It is often assumed that ‘grounded’ learning tasks are beyond the scope of grammatical inference techniques. In this paper, we show that the grounded task of learning a semantic parser from ambiguous training data as discussed in Kim and Mooney (2010) can be reduced to a Probabilistic Context-Free Grammar learning task in a way that gives state of the art results. We further show that additionally letting our model learn the language’s canonical word order improves its performance and leads to the highest semantic parsing f-scores previously reported in the literature.1 1 Introduction One of the most fundamental ideas about language is that we use it to express our thoughts. Learning a natural language, then, amounts to (at least) learning a mapping between the things we utter and the things we think, and can therefore be seen as the task </context>
<context position="2986" citStr="Kim and Mooney, 2010" startWordPosition="462" endWordPosition="465">ssentially, it seems as if ‘grounded’ learning tasks like this require dealing with two different kinds of information, the purely formal (phonemic) and meaningful (semantic) aspects of language. Grammatical inference seems to be limited to dealing with one level of formal information (Chang and Maia, 2001). For this reason, probably, approaches to the task of learning a semantic parser employ a variety of sophisticated and task-specific techniques that go beyond (but often elaborate on) the techniques used for grammatical inference (Lu et al., 2008; Chen and Mooney, 2008; Liang et al., 2009; Kim and Mooney, 2010; Chen et al., 2010). In this paper, we show that one can reduce the task of learning a semantic parser to a Probabilistic Context Free Grammar (PCFG) learning task, and more generally, that grounded learning tasks are not in principle beyond the scope of grammatical inference techniques. In particular, we show how to formulate the task of learning a semantic parser as discussed by Chen, Kim and Mooney (2008, 2010) as the task of learning a PCFG from strings. Our model does not only constitute a proof of concept that this 1416 Proceedings of the 2011 Conference on Empirical Methods in Natural </context>
<context position="5157" citStr="Kim and Mooney, 2010" startWordPosition="817" endWordPosition="820">generalizations such as the canonical word order can lead to better results. The structure of the paper is as follows. First we give a short overview of the previous work by Chen, Kim and Mooney and describe their dataset. Then, we show how to reduce the parsing task addressed by them to a PCFG-learning task. Finally, we explain how to let our model additionally learn the language’s canonical word order. 2 Previous Work by Chen, Kim and Mooney In a series of recent papers, Chen, Kim and Mooney approach the task of learning a semantic parser from ambiguous training data (Chen and Mooney, 2008; Kim and Mooney, 2010; Chen et al., 2010). This goes beyond previous work on semantic parsing such as Lu et al. (2008) or Zettlemoyer and Collins (2005) which rely on unambiguous training data where every sentence is paired only with its meaning. In contrast, Chen, Kim and Mooney allow their training examples to exhibit the kind of uncertainty about sentence meanings human learners are likely to have to deal with by allowing for sentences to be associated with a set of candidate-meanings, 2It has been pointed out to us by one reviewer that the task we address falls short of what is often called ‘grounded learning’</context>
<context position="7462" citStr="Kim and Mooney (2010)" startWordPosition="1201" endWordPosition="1204">ion-algorithm, not every natural language sentence has a gold matching meaning representation. In addition to the inherent ambiguity of the training examples, the learner therefore has to somehow deal with those examples which only have ‘wrong’ meanings associated with them. Datasets exist for both Korean and English, each comprising training and gold data for four games.3 Some details about this data are given in Table 1, such as the number of examples, their average ambiguity and the number of misleading examples. For the following short discussion of previous approaches, we mainly focus on Kim and Mooney (2010). This is the most recent publication and reports the highest scores. 2.1 The parsing task Learning a semantic parser from the ambiguous data is, in fact, just one of three tasks discussed by Kim and Mooney (2010), henceforth KM. In addition to parsing, they discuss matching and natural language generation. We are ignoring the generation task as we are currently only interested in the parsing problem, and we treat the matching task, picking the correct meaning from the set of candidates, merely as a byproduct of parsing, rather than as a completely separate task: parsing implicitly requires th</context>
<context position="12228" citStr="Kim and Mooney (2010)" startWordPosition="2006" endWordPosition="2009">e 1 for the sentence-meaning pair (THE PINK GOALIE PASSES THE BALL TO PINK11,pass(pink1,pink11)). Its root is assumed to correspond to the whole MR and is labeled Spass(pink1,pink11). The remaining three MR constituents correspond to the root’s daughters which we label Phrasepink1, Phrasepass and Phrasepink11. Generally speaking, we assume a special non-terminal Sm for every MR m generated by the MG, and a special non-terminal Phrasecon for each of the terminals of the MG (which loosely correspond to concepts). This is only possible for MGs which create a finite set of MRs, but the MG used by Kim and Mooney (2010) obeys this restriction.5 The tree’s terminals are the words that make up the sentence, and we assume them to be dominated by concept-specific pre-terminals Wordcon which correspond to concept-specific probability distributions over the language’s vocabulary. Since each Phrasecon may span multiple words, we give trees rooted in Phrasecon a left-recursive structure that corresponds to a unigram Markov-process. This process generates an arbitrary sequence of words semantically related to con, dominated by the corresponding pre-terminal Wordcon in our model, and words not directly semantically re</context>
<context position="24208" citStr="Kim and Mooney (2010)" startWordPosition="4120" endWordPosition="4123">FG are given in Table 2. All scores are reported in F-measure which is the harmonic mean of Precision and Recall. In this specific case, precision is the fraction of correct parses out 11For parsing, we use Mark Johnson’s freely available CKY implementation which can be downloaded at http://web. science.mq.edu.au/~mjohnson/Software.htm. 1422 English Korean KM 0.742 0.764 KM ‘supervised’ 0.810 0.808 Chen et al. (2010) 0.801 0.812 NoWo-PCFG 0.742 0.718 WO-PCFG 0.860 0.829 Table 2: A summary of results for the parsing task, in Fmeasure. We also show the results of Chen et al. (2010), as given in Kim and Mooney (2010), which to our knowledge are the highest previously reported scores for Korean. WO-PCFG, described in Section 4 performs better than all previously reported models, but only slightly so for Korean. of the total number of parses the model returns. Recall is the fraction of correct parses out of the total number of test sentences.12 NoWo-PCFG performs a little worse than KM’s model. Its scores are virtually identical for English (0.742) and worse for Korean (0.718 vs 0.764). We are not sure as to why our model performs worse on the Korean data, but it might have to do with the fact that the Kore</context>
</contexts>
<marker>Kim, Mooney, 2010</marker>
<rawString>Joohyun Kim and Raymond J. Mooney. 2010. Generative alignment and semantic parsing for learning from ambiguous supervision. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The estimation of Stochastic Context-Free Grammars using the Inside-Outside algorithm.</title>
<date>1990</date>
<journal>Computer Speech and Language,</journal>
<pages>4--35</pages>
<contexts>
<context position="15926" citStr="Lari and Young, 1990" startWordPosition="2637" endWordPosition="2640">ntext is C1, we require the 6If we were to consider every possible context, we would have to consider 22048 contexts because the MG generates 2048 MRs. 1419 Root Spass(pink1,pink11) Phr a sep 1420 schemata used to generate the CFG is given in Figure 3.7 Instantiating all those schemata leads to a grammar with 33,101 rules for the English data and 30,731 rules for the Korean data. The difference in size is due to differences in the size of the vocabulary and the different number of contexts in the data sets. These CFGs can now be trained on the training data using the Inside-Outside algorithm (Lari and Young, 1990). After training, the resulting PCFG embodies a semantic parser in the sense that, with a slight modification we describe in section 3.3, it can be used to parse a string into its meaning representation by determining the most likely syntactic analysis and reading off the meaning assigned by our model at the Sm-node. 3.2 Possible objections to our reduction Before we go on to discuss the details of training and evaluation of our model, we want to address an objection that might seem tempting. Isn’t our reduction impractical and unrealistic as even a highly abstract model of language learning –</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S.J. Young. 1990. The estimation of Stochastic Context-Free Grammars using the Inside-Outside algorithm. Computer Speech and Language, 4(35-56).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite PCFG using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>688--697</pages>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 688–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP.</booktitle>
<contexts>
<context position="2964" citStr="Liang et al., 2009" startWordPosition="458" endWordPosition="461">he observed forms. Essentially, it seems as if ‘grounded’ learning tasks like this require dealing with two different kinds of information, the purely formal (phonemic) and meaningful (semantic) aspects of language. Grammatical inference seems to be limited to dealing with one level of formal information (Chang and Maia, 2001). For this reason, probably, approaches to the task of learning a semantic parser employ a variety of sophisticated and task-specific techniques that go beyond (but often elaborate on) the techniques used for grammatical inference (Lu et al., 2008; Chen and Mooney, 2008; Liang et al., 2009; Kim and Mooney, 2010; Chen et al., 2010). In this paper, we show that one can reduce the task of learning a semantic parser to a Probabilistic Context Free Grammar (PCFG) learning task, and more generally, that grounded learning tasks are not in principle beyond the scope of grammatical inference techniques. In particular, we show how to formulate the task of learning a semantic parser as discussed by Chen, Kim and Mooney (2008, 2010) as the task of learning a PCFG from strings. Our model does not only constitute a proof of concept that this 1416 Proceedings of the 2011 Conference on Empiric</context>
<context position="30481" citStr="Liang et al. (2009)" startWordPosition="5230" endWordPosition="5234">ores are, to our knowledge, the highest scores previously reported for this parsing task and establish our second main point: letting the model learn the language’s word order in addition to learning the mapping from sentences to MR increases semantic parsing accuracy.14 An intuitive explanation for the increase in performance is that by allowing the model to learn word order, we are providing it with a new dimension along which it can generalize. In this sense, we can look at our refinement as providing the model with abstract linguistic knowledge, namely that languages tend to have a canon14Liang et al. (2009)’s model can be seen as capturing something similar to our word order generalization with the help of a Field Choice Model which primarily captures discourse coherence and salience properties. It differs, however, in that it can only learn one generalization for each predicate type and no language wide generalization. ical word order. The usefulness of this kind of information is impressive – for English, it improves the accuracy of semantic parsing by almost 12% in F-measure and for Korean by 11.1%. In addition, our model correctly learns that English’s predominant word order is SVO and that </context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervision. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>A generative model for parsing natural language to meaning representations.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>783--792</pages>
<contexts>
<context position="2921" citStr="Lu et al., 2008" startWordPosition="450" endWordPosition="453">that provides a clue to the meaning of the observed forms. Essentially, it seems as if ‘grounded’ learning tasks like this require dealing with two different kinds of information, the purely formal (phonemic) and meaningful (semantic) aspects of language. Grammatical inference seems to be limited to dealing with one level of formal information (Chang and Maia, 2001). For this reason, probably, approaches to the task of learning a semantic parser employ a variety of sophisticated and task-specific techniques that go beyond (but often elaborate on) the techniques used for grammatical inference (Lu et al., 2008; Chen and Mooney, 2008; Liang et al., 2009; Kim and Mooney, 2010; Chen et al., 2010). In this paper, we show that one can reduce the task of learning a semantic parser to a Probabilistic Context Free Grammar (PCFG) learning task, and more generally, that grounded learning tasks are not in principle beyond the scope of grammatical inference techniques. In particular, we show how to formulate the task of learning a semantic parser as discussed by Chen, Kim and Mooney (2008, 2010) as the task of learning a PCFG from strings. Our model does not only constitute a proof of concept that this 1416 Pr</context>
<context position="5254" citStr="Lu et al. (2008)" startWordPosition="835" endWordPosition="838">er is as follows. First we give a short overview of the previous work by Chen, Kim and Mooney and describe their dataset. Then, we show how to reduce the parsing task addressed by them to a PCFG-learning task. Finally, we explain how to let our model additionally learn the language’s canonical word order. 2 Previous Work by Chen, Kim and Mooney In a series of recent papers, Chen, Kim and Mooney approach the task of learning a semantic parser from ambiguous training data (Chen and Mooney, 2008; Kim and Mooney, 2010; Chen et al., 2010). This goes beyond previous work on semantic parsing such as Lu et al. (2008) or Zettlemoyer and Collins (2005) which rely on unambiguous training data where every sentence is paired only with its meaning. In contrast, Chen, Kim and Mooney allow their training examples to exhibit the kind of uncertainty about sentence meanings human learners are likely to have to deal with by allowing for sentences to be associated with a set of candidate-meanings, 2It has been pointed out to us by one reviewer that the task we address falls short of what is often called ‘grounded learning’. We acknowledge that semantic parsing constitutes a very limited kind of grounded learning but w</context>
<context position="9365" citStr="Lu et al. (2008)" startWordPosition="1521" endWordPosition="1524">one more for the 2004 Korean training set). In addition, our calculation of the average sentential ambiguity (Avg. # of MRs) differs because we assume that mutiple occurences of the same event in a context do not add to the overall ambiguity, and our calculation of the noise (fraction of training examples without the correct meaning in their context) takes into account that there are training examples which do not have their gold meaning associated with them in the training data and is therefore slightly higher than the one reported in Chen et al. (2010). KM’s model builds on previous work by Lu et al. (2008) and is a generative model which defines a joint probability distribution over natural language sentences (NLs), meaning representations (MRs) and hybrid trees. The NLs are the natural language comments to the games, the MRs are simple logical formulae describing game events and playing the role of sentence meanings, and a hybrid tree is a tree structure that represents the correspondence between a sentence and its meaning. More specifically, if some NL W has as its meaning an MR m, and m has been generated by a meaning grammar (MG) G, the hybrid tree corresponding to the pair (W,m) has as its</context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer. 2008. A generative model for parsing natural language to meaning representations. In Empirical Methods in Natural Language Processing (EMNLP), pages 783–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of UAI</booktitle>
<contexts>
<context position="5288" citStr="Zettlemoyer and Collins (2005)" startWordPosition="840" endWordPosition="843">rst we give a short overview of the previous work by Chen, Kim and Mooney and describe their dataset. Then, we show how to reduce the parsing task addressed by them to a PCFG-learning task. Finally, we explain how to let our model additionally learn the language’s canonical word order. 2 Previous Work by Chen, Kim and Mooney In a series of recent papers, Chen, Kim and Mooney approach the task of learning a semantic parser from ambiguous training data (Chen and Mooney, 2008; Kim and Mooney, 2010; Chen et al., 2010). This goes beyond previous work on semantic parsing such as Lu et al. (2008) or Zettlemoyer and Collins (2005) which rely on unambiguous training data where every sentence is paired only with its meaning. In contrast, Chen, Kim and Mooney allow their training examples to exhibit the kind of uncertainty about sentence meanings human learners are likely to have to deal with by allowing for sentences to be associated with a set of candidate-meanings, 2It has been pointed out to us by one reviewer that the task we address falls short of what is often called ‘grounded learning’. We acknowledge that semantic parsing constitutes a very limited kind of grounded learning but want to point out that the task has</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of UAI 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>