<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000537">
<title confidence="0.9890905">
Self-Training for Enhancement and Domain Adaptation of
Statistical Parsers Trained on Small Datasets
</title>
<author confidence="0.955738">
Roi Reichart
</author>
<affiliation confidence="0.912306">
ICNC
Hebrew University of Jerusalem
</affiliation>
<email confidence="0.993643">
roiri@cs.huji.ac.il
</email>
<author confidence="0.992886">
Ari Rappoport
</author>
<affiliation confidence="0.9995935">
Institute of Computer Science
Hebrew University of Jerusalem
</affiliation>
<email confidence="0.992367">
arir@cs.huji.ac.il
</email>
<sectionHeader confidence="0.995545" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999925666666667">
Creating large amounts of annotated data to
train statistical PCFG parsers is expensive,
and the performance of such parsers declines
when training and test data are taken from
different domains. In this paper we use self-
training in order to improve the quality of
a parser and to adapt it to a different do-
main, using only small amounts of manually
annotated seed data. We report significant
improvement both when the seed and test
data are in the same domain and in the out-
of-domain adaptation scenario. In particu-
lar, we achieve 50% reduction in annotation
cost for the in-domain case, yielding an im-
provement of 66% over previous work, and a
20-33% reduction for the domain adaptation
case. This is the first time that self-training
with small labeled datasets is applied suc-
cessfully to these tasks. We were also able
to formulate a characterization of when self-
training is valuable.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987776487179487">
State of the art statistical parsers (Collins, 1999;
Charniak, 2000; Koo and Collins, 2005; Charniak
and Johnson, 2005) are trained on manually anno-
tated treebanks that are highly expensive to create.
Furthermore, the performance of these parsers de-
creases as the distance between the genres of their
training and test data increases. Therefore, enhanc-
ing the performance of parsers when trained on
small manually annotated datasets is of great impor-
tance, both when the seed and test data are taken
616
from the same domain (the in-domain scenario) and
when they are taken from different domains (the out-
of-domain or parser adaptation scenario). Since the
problem is the expense in manual annotation, we de-
fine ‘small’ to be 100-2,000 sentences, which are the
sizes of sentence sets that can be manually annotated
by constituent structure in a few hours1.
Self-training is a method for using unannotated
data when training supervised models. The model is
first trained using manually annotated (‘seed’) data,
then the model is used to automatically annotate a
pool of unannotated (‘self-training’) data, and then
the manually and automatically annotated datasets
are combined to create the training data for the fi-
nal model. Self-training of parsers trained on small
datasets is of enormous potential practical impor-
tance, due to the huge amounts of unannotated data
that are becoming available today and to the high
cost of manual annotation.
In this paper we use self-training to enhance the
performance of a generative statistical PCFG parser
(Collins, 1999) for both the in-domain and the parser
adaptation scenarios, using only small amounts of
manually annotated data. We perform four experi-
ments, examining all combinations of in-domain and
out-of-domain seed and self-training data.
Our results show that self-training is of substantial
benefit for the problem. In particular, we present:
</bodyText>
<listItem confidence="0.73862775">
• 50% reduction in annotation cost when the seed
and test data are taken from the same domain,
which is 66% higher than any previous result
with small manually annotated datasets.
</listItem>
<footnote confidence="0.988261">
1We note in passing that quantitative research on the cost of
annotation using various annotation schemes is clearly lacking.
</footnote>
<note confidence="0.8559525">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 616–623,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<listItem confidence="0.992609">
• The first time that self-training improves a gen-
erative parser when the seed and test data are
from the same domain.
• 20-33% reduction in annotation cost when the
seed and test data are from different domains.
• The first time that self-training succeeds in
adapting a generative parser between domains
using a small manually annotated dataset.
• The first formulation (related to the number of
unknown words in a sentence) of when self-
training is valuable.
</listItem>
<bodyText confidence="0.999740833333333">
Section 2 discusses previous work, and Section 3
compares in-depth our protocol to a previous one.
Sections 4 and 5 present the experimental setup and
our results, and Section 6 analyzes the results in an
attempt to shed light on the phenomenon of self-
training.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998591945205479">
Self-training might seem a strange idea: why should
a parser trained on its own output learn anything
new? Indeed, (Clark et al., 2003) applied self-
training to POS-tagging with poor results, and
(Charniak, 1997) applied it to a generative statisti-
cal PCFG parser trained on a large seed set (40K
sentences), without any gain in performance.
Recently, (McClosky et al., 2006a; McClosky et
al., 2006b) have successfully applied self-training to
various parser adaptation scenarios using the rerank-
ing parser of (Charniak and Johnson, 2005). A
reranking parser (see also (Koo and Collins, 2005))
is a layered model: the base layer is a generative sta-
tistical PCFG parser that creates a ranked list of k
parses (say, 50), and the second layer is a reranker
that reorders these parses using more detailed fea-
tures. McClosky et al (2006a) use sections 2-21 of
the WSJ PennTreebank as seed data and between
50K to 2,500K unlabeled NANC corpus sentences
as self-training data. They train the PCFG parser and
the reranker with the manually annotated WSJ data,
and parse the NANC data with the 50-best PCFG
parser. Then they proceed in two directions. In
the first, they reorder the 50-best parse list with the
reranker to create a new 1-best list. In the second,
they leave the 1-best list produced by the genera-
tive PCFG parser untouched. Then they combine the
1-best list (each direction has its own list) with the
WSJ training set, to retrain the PCFG parser. The
final PCFG model and the reranker (trained only on
annotated WSJ material) are then used to parse the
test section (23) of WSJ.
There are two major differences between these pa-
pers and the current one, stemming from their usage
of a reranker and of large seed data. First, when
their 1-best list of the base PCFG parser was used
as self training data for the PCFG parser (the sec-
ond direction), the performance of the base parser
did not improve. It had improved only when the 1-
best list of the reranker was used. In this paper we
show how the 1-best list of a base (generative) PCFG
parser can be used as a self-training material for the
base parser itself and enhance its performance, with-
out using any reranker. This reveals a noteworthy
characteristic of generative PCFG models and offers
a potential direction for parser improvement, since
the quality of a parser-reranker combination criti-
cally depends on that of the base parser.
Second, these papers did not explore self-training
when the seed is small, a scenario whose importance
has been discussed above. In general, PCFG mod-
els trained on small datasets are less likely to parse
the self-training data correctly. For example, the f-
score of WSJ data parsed by the base PCFG parser
of (Charniak and Johnson, 2005) when trained on
the training sections of WSJ is between 89% to
90%, while the f-score of WSJ data parsed with the
Collins’ model that we use, and a small seed, is be-
tween 40% and 80%. As a result, the good results of
(McClosky et al, 2006a; 2006b) with large seed sets
do not immediately imply success with small seed
sets. Demonstration of such success is a contribu-
tion of the present paper.
Bacchiani et al (2006) explored the scenario of
out-of-domain seed data (the Brown training set
containing about 20K sentences) and in-domain
self-training data (between 4K to 200K sentences
from the WSJ) and showed an improvement over
the baseline of training the parser with the seed data
only. However, they did not explore the case of small
seed datasets (the effort in manually annotating 20K
is substantial) and their work addresses only one of
our scenarios (OI, see below).
</bodyText>
<page confidence="0.973975">
617
</page>
<bodyText confidence="0.982732286821706">
A work closely related to ours is (Steedman et domain adaptation of statistical parsers trained on
al., 2003a), which applied co-training (Blum and small datasets. No previous work has succeeded in
Mitchell, 1998) and self-training to Collins’ pars- identifying such a protocol for this task. In this sec-
ing model using a small seed dataset (500 sentences tion we try to understand why.
for both methods and 1,000 sentences for co-training In the protocol we apply, the self-training set con-
only). The seed, self-training and test datasets they tains several thousand sentences A parser trained
used are similar to those we use in our II experi- with a small seed set parses the self-training set, and
ment (see below), but the self-training protocols are then the whole automatically annotated self-training
different. They first train the parser with the seed set is combined with the manually annotated seed
sentences sampled from WSJ sections 2-21. Then, set to retrain the parser. This protocol and that of
iteratively, 30 sentences are sampled from these sec- Steedman et al (2003a) were applied to the problem,
tions, parsed by the parser, and the 20 best sentences with the same seed, self-training and test sets. As
(in terms of parser confidence defined as probability we show below (see Section 4 and Section 5), while
of top parse) are selected and combined with the pre- Steedman’s protocol does not improve over the base-
viously annotated data to retrain the parser. The co- line of using only the seed data, our protocol does.
training protocol is similar except that each parser There are four differences between the protocols.
is trained with the 20 best sentences of the other First, Steedman et al’s seed set consists of consecu-
parser. Self-training did not improve parser perfor- tive WSJ sentences, while we select them randomly.
mance on the WSJ test section (23). Steedman et In the next section we show that this difference is
al (2003b) followed a similar co-training protocol immaterial. Second, Steedman et al’s protocol looks
except that the selection function (three functions for sentences of high quality parse, while our pro-
were explored) considered the differences between tocol prefers to use many sentences without check-
the confidence scores of the two parsers. In this pa- ing their parse quality. Third, their protocol is itera-
per we show a self-training protocol that achieves tive while ours uses a single step. Fourth, our self-
better results than all of these methods (Table 2). training set is orders of magnitude larger than theirs.
The next section discusses possible explanations for To examine the parse quality issue, we performed
the difference in results. Steedman et al (2003b) and their experiment using their setting but selecting the
Hwa et al, (2003) also used several versions of cor- high quality parse sentences using their f-score rel-
rected co-training which are not comparable to ours ative to the gold standard annotation from secs 2-
and other suggested methods because their evalua- 21 rather than a quality estimate. No improvement
tion requires different measures (e.g. reviewed and over the baseline was achieved even with this or-
corrected constituents are separately counted). acle. Thus the problem with their protocol does
As far as we know, (Becker and Osborne, 2005) not lie with the parse quality assessment function;
is the only additional work that tries to improve a no other function would produce results better than
generative PCFG parsers using small seed data. The the oracle. To examine the iteration issue, we per-
techniques used are based on active learning (Cohn formed their experiment in a single step, selecting at
et al., 1994). The authors test two novel methods, once the oracle-best 2,000 among 3,000 sentences2,
along with the tree entropy (TE) method of (Hwa, which produced only a mediocre improvement. We
2004). The seed, the unannotated and the test sets, thus conclude that the size of the self-training set is a
as well as the parser used in that work, are similar major factor responsible for the difference between
to those we use in our II experiment. Our results are the protocols.
superior, as shown in Table 3. 4 Experimental Setup
3 Self-Training Protocols We used a reimplementation of Collins’ parsing
There are many possible ways to do self-training. model 2 (Bikel, 2004). We performed four experi-
A main goal of this paper is to identify a self- ments, II, IO, OI, and OO, two with in-domain seed
training protocol most suitable for enhancement and
618
2Corresponding to a 100 iterations of 30 sentences each.
(II, IO) and two with out-of-domain seed (OI, OO), The results were very similar and will not be detailed
examining in-domain self-training (II, OI) and out- here due to space constraints.
of-domain self-training (IO, OO). Note that being
‘in’ or ‘out’ of domain is determined by the test data.
Each experiment contained 19 runs. In each run a
different seed size was used, from 100 sentences on-
wards, in steps of 100. For statistical significance,
we repeated each experiment five times, in each rep-
etition randomly sampling different manually anno-
tated sentences to form the seed dataset3.
The seed data were taken from WSJ sections 2-
21. For II and IO, the test data is WSJ section 23
(2416 sentences) and the self-training data are either
WSJ sections 2-21 (in II, excluding the seed sen-
tences) or the Brown training section (in IO). For
OI and OO, the test data is the Brown test section
(2424 sentences), and the self-training data is either
the Brown training section (in OI) or WSJ sections
2-21 (in OO). We removed the manual annotations
from the self-training sections before using them.
For the Brown corpus, we based our division
on (Bacchiani et al., 2006; McClosky et al., 2006b).
The test and training sections consist of sentences
from all of the genres that form the corpus. The
training division consists of 90% (9 of each 10 con-
secutive sentences) of the data, and the test section
are the remaining 10% (We did not use any held out
data). Parsing performance is measured by f-score,
f = 2�� ��
� �� , where P, R are labeled precision and
recall.
To further demonstrate our results for parser adap-
tation, we also performed the OI experiment where
seed data is taken from WSJ sections 2-21 and both
self-training and test data are taken from the Switch-
board corpus. The distance between the domains of
these corpora is much greater than the distance be-
tween the domains of WSJ and Brown. The Brown
and Switchboard corpora were divided to sections in
the same way.
We have also performed all four experiments with
the seed data taken from the Brown training section.
5 Results
5.1 In-domain seed data
In these two experiments we show that when the
seed and test data are taken from the same domain, a
very significant enhancement of parser performance
can be achieved, whether the self-training material
is in-domain (II) or out-of-domain (IO). Figure 1
shows the improvement in parser f-score when self-
training data is used, compared to when it is not
used. Table 1 shows the reduction in manually an-
notated seed data needed to achieve certain f-score
levels. The enhancement in performance is very im-
pressive in the in-domain self-training data scenario
– a reduction of 50% in the number of manually an-
notated sentences needed for achieving 75 and 80 f-
score values. A significant improvement is achieved
in the out-of-domain self-training scenario as well.
Table 2 compares our results with self-training
and co-training results reported by (Steedman et al,
20003a; 2003b). As stated earlier, the experimental
setup of these works is similar to ours, but the self-
training protocols are different. For self-training,
our II improves an absolute 3.74% over their 74.3%
result, which constitutes a 14.5% reduction in error
(from 25.7%).
The table shows that for both seed sizes our
self training protocol outperforms both the self-
training and co-training protocols of (Steedman et
al, 20003a; 2003b). Results are not included in the
table only if they are not reported in the relevant pa-
per. The self-training protocol of (Steedman et al.,
2003a) does not actually improve over the baseline
of using only the seed data. Section 3 discussed a
possible explanation to the difference in results.
In Table 3 we compare our results to the results of
the methods tested in (Becker and Osborne, 2005)
(including TE)4. To do that, we compare the reduc-
tion in manually annotated data needed to achieve
an f-score value of 80 on WSJ section 23 achieved
</bodyText>
<table confidence="0.853675684210526">
by each method. We chose this measure since it is
3 (Steedman et al., 2003a) used the first 500 sentences of
WSJ training section as seed data. For direct comparison, we
performed our protocol in the II scenario using the first 500 or
1000 sentences of WSJ training section as seed data and got
similar results to those reported below for our protocol with ran-
dom selection. We also applied the protocol of Steedman et al
to scenario II with 500 randomly selected sentences, getting no
improvement over the random baseline.
4The measure is constituents and not sentences because this
is how results are reported in (Becker and Osborne, 2005).
However, the same reduction is obtained when sentences are
counted, because the number of constituents is averaged when
taking many sentences.
619
f-score 75 80
Seed data only 600(0%) 1400(0%)
II 300(50%) 700(50%)
IO 500(17%) 1200(14.5%)
</table>
<tableCaption confidence="0.97583375">
Table 1: Number of in-domain seed sentences
needed for achieving certain f-scores. Reductions
compared to no self-training (line 1) are given in
parentheses.
</tableCaption>
<table confidence="0.998966428571428">
Seed our our Steedman Steedman Steedman
size II IO ST CT CT
2003a 2003b
500 78.04 75.81 74.3 76.9 —-
sent.
1,000 81.43 79.49 —- 79 81.2
sent.
</table>
<tableCaption confidence="0.888105">
Table 2: F-scores of our in-domain-seed self-
training vs. self-training (ST) and co-training (CT)
of (Steedman et al, 20003a; 2003b).
</tableCaption>
<bodyText confidence="0.999776821428571">
the only explicitly reported number in that work. As
the table shows, our method is superior: our reduc-
tion of 50% constitutes an improvement of 66% over
their best reduction of 30.6%.
When applying self-training to a parser trained
with a small dataset we expect the coverage of the
parser to increase, since the combined training set
should contain items that the seed dataset does not.
On the other hand, since the accuracy of annota-
tion of such a parser is poor (see the no self-training
curve in Figure 1) the combined training set surely
includes inaccurate labels that might harm parser
performance. Figure 2 (left) shows the increase in
coverage achieved for in-domain and out-of-domain
self-training data. The improvements induced by
both methods are similar. This is quite surpris-
ing given that the Brown sections we used as self-
training data contain science, fiction, humor, ro-
mance, mystery and adventure texts while the test
section in these experiments, WSJ section 23, con-
tains only news articles.
Figure 2 also compares recall (middle) and preci-
sion (right) for the different methods. For II there
is a significant improvement in both precision and
recall even though many more sentences are parsed.
For IO, there is a large gain in recall and a much
smaller loss in precision, yielding a substantial im-
provement in f-score (Figure 1).
</bodyText>
<table confidence="0.947899333333333">
F - This Becker Becker en- Hwa
score work - II unparsed tropy/unparsed TE
80 50% 29.4% 30.6% -5.7%
</table>
<tableCaption confidence="0.99006">
Table 3: Reduction of the number of manually anno-
</tableCaption>
<bodyText confidence="0.932381">
tated constituents needed for achieving f score value
of 80 on section 23 of the WSJ. In all cases the seed
and additional sentences selected to train the parser
are taken from sections 02-21 of WSJ.
</bodyText>
<subsectionHeader confidence="0.999435">
5.2 Out-of-domain seed data
</subsectionHeader>
<bodyText confidence="0.99986872972973">
In these two experiments we show that self-training
is valuable for adapting parsers from one domain to
another. Figure 3 compares out-of-domain seed data
used with in-domain (OI) or out-of-domain (OO)
self-training data against the baseline of training
only with the out-of-domain seed data.
The left graph shows a significant improvement
in f-score. In the middle and right graphs we exam-
ine the quality of the parses produced by the model
by plotting recall and precision vs. seed size. Re-
garding precision, the difference between the three
conditions is small relative to the f-score difference
shown in the left graph. The improvement in the
recall measure is much greater than the precision
differences, and this is reflected in the f-score re-
sult. The gain in coverage achieved by both meth-
ods, which is not shown in the figure, is similar to
that reported for the in-domain seed experiments.
The left graph along with the increase in coverage
show the power of self-training in parser adaptation
when small seed datasets are used: not only do OO
and OI parse many more sentences than the baseline,
but their f-score values are consistently better.
To see how much manually annotated data can
be saved by using out-of-domain seed, we train the
parsing model with manually annotated data from
the Brown training section, as described in Sec-
tion 4. We assume that given a fixed number of
training sentences the best performance of the parser
without self-training will occur when these sen-
tences are selected from the domain of the test sec-
tion, the Brown corpus. We compare the amounts of
manually annotated data needed to achieve certain f-
score levels in this condition with the corresponding
amounts of data needed by OI and OO. The results
are summarized in Table 4. We compare to two base-
lines using in- and out-of-domain seed data without
</bodyText>
<page confidence="0.974557">
620
</page>
<figure confidence="0.999449454545454">
f score
f score
90
80
70
60
50
400 200 400 600 800 1000
number of manually annotated sentences
84
83
82
81
80
no self−training
79 wsj self−training
8 brown self−training
7000 1200 1400 1600 1800 2000
number of manually annotated sentences
no self training
wsj self−training
brown self−training
</figure>
<figureCaption confidence="0.996065">
Figure 1: Number of seed sentences vs. f-score, for the two in-domain seed experiments: II (triangles) and
IO (squares), and for the no self-training baseline. Self-training provides a substantial improvement.
</figureCaption>
<bodyText confidence="0.661316666666667">
number of manually annotated sentences
number of manually annotated sentences
number of manually annotated sentences
</bodyText>
<figure confidence="0.998069518518518">
2500
2000
1500
10000 500 1000 1500 2000
100
80
60
40
20
0 500 1000 1500 2000
number of covered sentences
recall
no self−training
wsj self−training
brown self−training
no self−training
wsj self−training
brown self−training
precision
85
80
75
70
650 500 1000 1500 2000
no self−training
wsj self−training
brown self−training
</figure>
<figureCaption confidence="0.823859666666667">
Figure 2: Number of seed sentences vs. coverage (left), recall (middle) and precision (right) for the two
in-domain seed experiments: II (triangles) and IO (squares), and for the no self-training baseline.
any self-training. The second line (ID) serves as a
</figureCaption>
<bodyText confidence="0.990937583333333">
reference to compute how much manual annotation
of the test domain was saved, and the first line (OD)
serves as a reference to show by how much self-
training improves the out-of-domain baseline. The
table stops at an f-score of 74 because that is the
best that the baselines can do.
A significant reduction in annotation cost over the
ID baseline is achieved where the seed size is be-
tween 100 and 1200. Improvement over the OD
baseline is for the whole range of seed sizes. Both
OO and OI achieve 20-33% reduction in manual an-
notation compared to the ID baseline and enhance
the performance of the parser by as much as 42.9%.
The only previous work that adapts a parser
trained on a small dataset between domains is that
of (Steedman et al., 2003a), which used co-training
(no self-training results were reported there or else-
where). In order to compare with that work, we per-
formed OI with seed taken from the Brown corpus
and self-training and test taken from WSJ, which
is the setup they use, obtaining a similar improve-
ment to that reported there. However, co-training is
a more complex method that requires an additional
parser (LTAG in their case).
To further substantiate our results for the parser
adaptation scenario, we used an additional corpus,
Switchboard. Figure 4 shows the results of an OI
experiment with WSJ seed and Switchboard self-
training and test data. Although the domains of these
two corpora are very different (more so than WSJ
and Brown), self-training provides a substantial im-
provement.
We have also performed all four experiments with
Brown and WSJ trading places. The results obtained
were very similar to those reported here, and will not
be detailed due to lack of space.
</bodyText>
<sectionHeader confidence="0.982266" genericHeader="method">
6 Analysis
</sectionHeader>
<bodyText confidence="0.9950164">
In this section we try to better understand the ben-
efit in using self-training with small seed datasets.
We formulate the following criterion: the number of
words in a test sentence that do not appear in the
seed data (‘unknown words’) is a strong indicator
</bodyText>
<page confidence="0.983992">
621
</page>
<figure confidence="0.957589">
number of manually annotated sentences
number of manually annotated sentences
number of manually annotated sentences
80
70
60
50
no self−training
wsj self−training
brown self−training
300 500 1000 1500 2000
80
70
60
50
40
30
200 500 1000 1500 2000
82
80
precision
78
76
74
no self−training
wsj self−training
brown self−training
72 0500 1000 1500 2000
f score
recall
40
no self−training
wsj self−training
brown self−training
</figure>
<figureCaption confidence="0.9968325">
Figure 3: Number of seed sentences vs. f-score (left), recall (middle) and precision (right), for the two
out-of-domain seed data experiments: OO (triangles) and OI (squares), and for the no self-training baseline.
</figureCaption>
<table confidence="0.999372">
f-sc. 66 68 70 72 74
OD 600 800 1, 000 1, 400 –
ID 600 700 800 1, 000 1, 200
OO 400 500 600 800 1100
33, 33 28.6, 37.5 33, 40 20, 42.9 8, –
OI 400 500 600 800 1, 300
33, 33 28.6, 37.5 33, 40 20, 42.9 −8, –
</table>
<tableCaption confidence="0.970956">
Table 4: Number of manually annotated seed sen-
</tableCaption>
<bodyText confidence="0.9462965">
tences needed for achieving certain f-score values.
The first two lines show the out-of-domain and in-
domain seed baselines. The reductions compared to
the baselines is given as ID, OD.
Figure 4: Number of seed sentences vs. f-score,
for the OI experiment using WSJ seed data and
SwitchBoard self-training and test data. In spite of
the strong dissimilarity between the domains, self-
training provides a substantial improvement.
to whether it is worthwhile to use small seed self-
training. Figure 5 shows the number of unknown
words in a sentence vs. the probability that the self-
training model will parse a sentence no worse (up-
per curve) or better (lower curve) than the baseline
model.
The upper curve shows that regardless of the
</bodyText>
<figure confidence="0.99289545">
50
40
30
20
100 500 1000 1500 2000
number of manually annotated sentences
f score
switchboard self−training
no self−training
probability
0.8
0.6
0.4
0.2
0
0 10 20 30 40 50
1
ST &gt; baseline
ST &gt;= baseline
number of unknown words
</figure>
<figureCaption confidence="0.991401">
Figure 5: For sentences having the same number of
</figureCaption>
<bodyText confidence="0.938805619047619">
unknown words, we show the probability that the
self-training model parses a sentence from the set
no worse (upper curve) or better (lower curve) than
the baseline model.
number of unknown words in the sentence, there is
more than 50% chance that the self-training model
will not harm the result. This probability decreases
from almost 1 for a very small number of unknown
words to about 0.55 for 50 unknown words. The
lower curve shows that when the number of un-
known words increases, the probability that the
self-training model will do better than the baseline
model increases from almost 0 (for a very small
number of unknown words) to about 0.55. Hence,
the number of unknown words is an indication for
the potential benefit (value on the lower curve)
and risk (1 minus the value on the upper curve) in
using the self-training model compared to using the
baseline model. Unknown words were not identified
in (McClosky et al., 2006a) as a useful predictor for
the benefit of self-training.
</bodyText>
<page confidence="0.994953">
622
</page>
<bodyText confidence="0.999973">
We also identified a length effect similar to that
studied by (McClosky et al., 2006a) for self-training
(using a reranker and large seed, as detailed in Sec-
tion 2). Due to space limitations we do not discuss
it here.
</bodyText>
<sectionHeader confidence="0.999487" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.99984772972973">
Self-training is usually not considered to be a valu-
able technique in improving the performance of gen-
erative statistical parsers, especially when the man-
ually annotated seed sentence dataset is small. In-
deed, in the II scenario, (Steedman et al., 2003a;
McClosky et al., 2006a; Charniak, 1997) reported
no improvement of the base parser for small (500
sentences, in the first paper) and large (40K sen-
tences, in the last two papers) seed datasets respec-
tively. In the II, OO, and OI scenarios, (McClosky et
al, 2006a; 2006b) succeeded in improving the parser
performance only when a reranker was used to re-
order the 50-best list of the generative parser, with a
seed size of 40K sentences. Bacchiani et al (2006)
improved the parser performance in the OI scenario
but their seed size was large (about 20K sentences).
In this paper we have shown that self-training
can enhance the performance of generative parsers,
without a reranker, in four in- and out-of-domain
scenarios using a small seed dataset. For the II, IO
and OO scenarios, we are the first to show improve-
ment by self-training for generative parsers. We
achieved a 50% (20-33%) reduction in annotation
cost for the in-domain (out-of-domain) seed data
scenarios. Previous work with small seed datasets
considered only the II and OI scenarios. Our results
for the former are better than any previous method,
and our results for the latter (which are the first
reported self-training results) are similar to previ-
ous results for co-training, a more complex method.
We demonstrated our results using three corpora of
varying degrees of domain difference.
A direction for future research is combining
self-training data from various domains to enhance
parser adaptation.
Acknowledgement. We would like to thank Dan
Roth for his constructive comments on this paper.
</bodyText>
<sectionHeader confidence="0.996264" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998591862745098">
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat, 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41–68.
Markus Becker and Miles Osborne, 2005. A two-stage
method for active learning of statistical grammars. IJ-
CAI ’05.
Daniel Bikel, 2004. Code developed at University of
Pennsylvania. http://www.cis.upenn.edu.bikel.
Avrim Blum and Tom M. Mitchell, 1998. Combining la-
beled and unlabeled data with co-training. COLT ’98.
Eugene Charniak, 1997. Statistical parsing with a
context-free grammar and word statistics. AAAI ’97.
Eugene Charniak, 2000. A maximum-entropy-inspired
parser. ANLP ’00.
Eugene Charniak and Mark Johnson, 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. ACL ’05.
Stephen Clark, James Curran, and Miles Osborne,
2003. Bootstrapping pos taggers using unlabelled
data. CoNLL ’03.
David A. Cohn, Les Atlas, and Richard E. Ladner, 1994.
Improving generalization with active learning. Ma-
chine Learning, 15(2):201–221.
Michael Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Rebecca Hwa, Miles Osborne, Anoop Sarkar and Mark
Steedman, 2003. Corrected co-training for statistical
parsers. In ICML ’03, Workshop on the Continuum
from Labeled to Unlabeled Data in Machine Learning
and Data Mining.
Rebecca Hwa, 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253–276.
Terry Koo and Michael Collins, 2005. Hidden-variable
models for discriminative reranking. EMNLP ’05.
David McClosky, Eugene Charniak, and Mark John-
son, 2006a. Effective self-training for parsing. HLT-
NAACL ’06.
David McClosky, Eugene Charniak, and Mark Johnson,
2006b. Reranking and self-training for parser adapta-
tion. ACL-COLING ’06.
Mark Steedman, Anoop Sarkar, Miles Osborne, Rebecca
Hwa, Stephen Clark, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim, 2003a. Bootstrap-
ping statistical parsers from small datasets. EACL ’03.
Mark Steedman, Rebecca Hwa, Stephen Clark, Miles
Osborne, Anoop Sarkar, Julia Hockenmaier, Paul
Ruhlen,Steven Baker, and Jeremiah Crim, 2003b. Ex-
ample selection for bootstrapping statistical parsers.
NAACL ’03.
</reference>
<page confidence="0.999027">
623
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.954530">
<title confidence="0.998773">Self-Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on Small Datasets</title>
<author confidence="0.999916">Roi Reichart</author>
<affiliation confidence="0.9938975">ICNC Hebrew University of Jerusalem</affiliation>
<email confidence="0.994988">roiri@cs.huji.ac.il</email>
<author confidence="0.996568">Ari Rappoport</author>
<affiliation confidence="0.999829">Institute of Computer Science Hebrew University of Jerusalem</affiliation>
<email confidence="0.995011">arir@cs.huji.ac.il</email>
<abstract confidence="0.999184181818182">Creating large amounts of annotated data to train statistical PCFG parsers is expensive, and the performance of such parsers declines when training and test data are taken from different domains. In this paper we use selftraining in order to improve the quality of a parser and to adapt it to a different domain, using only small amounts of manually annotated seed data. We report significant improvement both when the seed and test data are in the same domain and in the outof-domain adaptation scenario. In particular, we achieve 50% reduction in annotation cost for the in-domain case, yielding an improvement of 66% over previous work, and a 20-33% reduction for the domain adaptation case. This is the first time that self-training with small labeled datasets is applied successfully to these tasks. We were also able to formulate a characterization of when selftraining is valuable.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Michael Riley</author>
<author>Brian Roark</author>
<author>Richard Sproat</author>
</authors>
<title>MAP adaptation of stochastic grammars.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="7456" citStr="Bacchiani et al (2006)" startWordPosition="1224" endWordPosition="1227"> general, PCFG models trained on small datasets are less likely to parse the self-training data correctly. For example, the fscore of WSJ data parsed by the base PCFG parser of (Charniak and Johnson, 2005) when trained on the training sections of WSJ is between 89% to 90%, while the f-score of WSJ data parsed with the Collins’ model that we use, and a small seed, is between 40% and 80%. As a result, the good results of (McClosky et al, 2006a; 2006b) with large seed sets do not immediately imply success with small seed sets. Demonstration of such success is a contribution of the present paper. Bacchiani et al (2006) explored the scenario of out-of-domain seed data (the Brown training set containing about 20K sentences) and in-domain self-training data (between 4K to 200K sentences from the WSJ) and showed an improvement over the baseline of training the parser with the seed data only. However, they did not explore the case of small seed datasets (the effort in manually annotating 20K is substantial) and their work addresses only one of our scenarios (OI, see below). 617 A work closely related to ours is (Steedman et domain adaptation of statistical parsers trained on al., 2003a), which applied co-trainin</context>
<context position="13705" citStr="Bacchiani et al., 2006" startWordPosition="2257" endWordPosition="2260">notated sentences to form the seed dataset3. The seed data were taken from WSJ sections 2- 21. For II and IO, the test data is WSJ section 23 (2416 sentences) and the self-training data are either WSJ sections 2-21 (in II, excluding the seed sentences) or the Brown training section (in IO). For OI and OO, the test data is the Brown test section (2424 sentences), and the self-training data is either the Brown training section (in OI) or WSJ sections 2-21 (in OO). We removed the manual annotations from the self-training sections before using them. For the Brown corpus, we based our division on (Bacchiani et al., 2006; McClosky et al., 2006b). The test and training sections consist of sentences from all of the genres that form the corpus. The training division consists of 90% (9 of each 10 consecutive sentences) of the data, and the test section are the remaining 10% (We did not use any held out data). Parsing performance is measured by f-score, f = 2�� �� � �� , where P, R are labeled precision and recall. To further demonstrate our results for parser adaptation, we also performed the OI experiment where seed data is taken from WSJ sections 2-21 and both self-training and test data are taken from the Swit</context>
<context position="28413" citStr="Bacchiani et al (2006)" startWordPosition="4752" endWordPosition="4755">ance of generative statistical parsers, especially when the manually annotated seed sentence dataset is small. Indeed, in the II scenario, (Steedman et al., 2003a; McClosky et al., 2006a; Charniak, 1997) reported no improvement of the base parser for small (500 sentences, in the first paper) and large (40K sentences, in the last two papers) seed datasets respectively. In the II, OO, and OI scenarios, (McClosky et al, 2006a; 2006b) succeeded in improving the parser performance only when a reranker was used to reorder the 50-best list of the generative parser, with a seed size of 40K sentences. Bacchiani et al (2006) improved the parser performance in the OI scenario but their seed size was large (about 20K sentences). In this paper we have shown that self-training can enhance the performance of generative parsers, without a reranker, in four in- and out-of-domain scenarios using a small seed dataset. For the II, IO and OO scenarios, we are the first to show improvement by self-training for generative parsers. We achieved a 50% (20-33%) reduction in annotation cost for the in-domain (out-of-domain) seed data scenarios. Previous work with small seed datasets considered only the II and OI scenarios. Our res</context>
</contexts>
<marker>Bacchiani, Riley, Roark, Sproat, 2006</marker>
<rawString>Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat, 2006. MAP adaptation of stochastic grammars. Computer Speech and Language, 20(1):41–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Becker</author>
<author>Miles Osborne</author>
</authors>
<title>A two-stage method for active learning of statistical grammars.</title>
<date>2005</date>
<journal>IJCAI</journal>
<volume>05</volume>
<contexts>
<context position="11255" citStr="Becker and Osborne, 2005" startWordPosition="1841" endWordPosition="1844">an et al (2003b) and their experiment using their setting but selecting the Hwa et al, (2003) also used several versions of cor- high quality parse sentences using their f-score relrected co-training which are not comparable to ours ative to the gold standard annotation from secs 2- and other suggested methods because their evalua- 21 rather than a quality estimate. No improvement tion requires different measures (e.g. reviewed and over the baseline was achieved even with this orcorrected constituents are separately counted). acle. Thus the problem with their protocol does As far as we know, (Becker and Osborne, 2005) not lie with the parse quality assessment function; is the only additional work that tries to improve a no other function would produce results better than generative PCFG parsers using small seed data. The the oracle. To examine the iteration issue, we pertechniques used are based on active learning (Cohn formed their experiment in a single step, selecting at et al., 1994). The authors test two novel methods, once the oracle-best 2,000 among 3,000 sentences2, along with the tree entropy (TE) method of (Hwa, which produced only a mediocre improvement. We 2004). The seed, the unannotated and t</context>
<context position="16347" citStr="Becker and Osborne, 2005" startWordPosition="2704" endWordPosition="2707">% result, which constitutes a 14.5% reduction in error (from 25.7%). The table shows that for both seed sizes our self training protocol outperforms both the selftraining and co-training protocols of (Steedman et al, 20003a; 2003b). Results are not included in the table only if they are not reported in the relevant paper. The self-training protocol of (Steedman et al., 2003a) does not actually improve over the baseline of using only the seed data. Section 3 discussed a possible explanation to the difference in results. In Table 3 we compare our results to the results of the methods tested in (Becker and Osborne, 2005) (including TE)4. To do that, we compare the reduction in manually annotated data needed to achieve an f-score value of 80 on WSJ section 23 achieved by each method. We chose this measure since it is 3 (Steedman et al., 2003a) used the first 500 sentences of WSJ training section as seed data. For direct comparison, we performed our protocol in the II scenario using the first 500 or 1000 sentences of WSJ training section as seed data and got similar results to those reported below for our protocol with random selection. We also applied the protocol of Steedman et al to scenario II with 500 rand</context>
</contexts>
<marker>Becker, Osborne, 2005</marker>
<rawString>Markus Becker and Miles Osborne, 2005. A two-stage method for active learning of statistical grammars. IJCAI ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Bikel</author>
</authors>
<title>Code developed at</title>
<date>2004</date>
<institution>University of Pennsylvania.</institution>
<note>http://www.cis.upenn.edu.bikel.</note>
<contexts>
<context position="12297" citStr="Bikel, 2004" startWordPosition="2017" endWordPosition="2018">-best 2,000 among 3,000 sentences2, along with the tree entropy (TE) method of (Hwa, which produced only a mediocre improvement. We 2004). The seed, the unannotated and the test sets, thus conclude that the size of the self-training set is a as well as the parser used in that work, are similar major factor responsible for the difference between to those we use in our II experiment. Our results are the protocols. superior, as shown in Table 3. 4 Experimental Setup 3 Self-Training Protocols We used a reimplementation of Collins’ parsing There are many possible ways to do self-training. model 2 (Bikel, 2004). We performed four experiA main goal of this paper is to identify a self- ments, II, IO, OI, and OO, two with in-domain seed training protocol most suitable for enhancement and 618 2Corresponding to a 100 iterations of 30 sentences each. (II, IO) and two with out-of-domain seed (OI, OO), The results were very similar and will not be detailed examining in-domain self-training (II, OI) and out- here due to space constraints. of-domain self-training (IO, OO). Note that being ‘in’ or ‘out’ of domain is determined by the test data. Each experiment contained 19 runs. In each run a different seed si</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel Bikel, 2004. Code developed at University of Pennsylvania. http://www.cis.upenn.edu.bikel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom M Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<journal>COLT</journal>
<volume>98</volume>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom M. Mitchell, 1998. Combining labeled and unlabeled data with co-training. COLT ’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<journal>AAAI</journal>
<volume>97</volume>
<contexts>
<context position="4517" citStr="Charniak, 1997" startWordPosition="715" endWordPosition="716">ated dataset. • The first formulation (related to the number of unknown words in a sentence) of when selftraining is valuable. Section 2 discusses previous work, and Section 3 compares in-depth our protocol to a previous one. Sections 4 and 5 present the experimental setup and our results, and Section 6 analyzes the results in an attempt to shed light on the phenomenon of selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features. M</context>
<context position="27994" citStr="Charniak, 1997" startWordPosition="4680" endWordPosition="4681">d in (McClosky et al., 2006a) as a useful predictor for the benefit of self-training. 622 We also identified a length effect similar to that studied by (McClosky et al., 2006a) for self-training (using a reranker and large seed, as detailed in Section 2). Due to space limitations we do not discuss it here. 7 Discussion Self-training is usually not considered to be a valuable technique in improving the performance of generative statistical parsers, especially when the manually annotated seed sentence dataset is small. Indeed, in the II scenario, (Steedman et al., 2003a; McClosky et al., 2006a; Charniak, 1997) reported no improvement of the base parser for small (500 sentences, in the first paper) and large (40K sentences, in the last two papers) seed datasets respectively. In the II, OO, and OI scenarios, (McClosky et al, 2006a; 2006b) succeeded in improving the parser performance only when a reranker was used to reorder the 50-best list of the generative parser, with a seed size of 40K sentences. Bacchiani et al (2006) improved the parser performance in the OI scenario but their seed size was large (about 20K sentences). In this paper we have shown that self-training can enhance the performance o</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak, 1997. Statistical parsing with a context-free grammar and word statistics. AAAI ’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<journal>ANLP</journal>
<volume>00</volume>
<contexts>
<context position="1244" citStr="Charniak, 2000" startWordPosition="194" endWordPosition="195">ed seed data. We report significant improvement both when the seed and test data are in the same domain and in the outof-domain adaptation scenario. In particular, we achieve 50% reduction in annotation cost for the in-domain case, yielding an improvement of 66% over previous work, and a 20-33% reduction for the domain adaptation case. This is the first time that self-training with small labeled datasets is applied successfully to these tasks. We were also able to formulate a characterization of when selftraining is valuable. 1 Introduction State of the art statistical parsers (Collins, 1999; Charniak, 2000; Koo and Collins, 2005; Charniak and Johnson, 2005) are trained on manually annotated treebanks that are highly expensive to create. Furthermore, the performance of these parsers decreases as the distance between the genres of their training and test data increases. Therefore, enhancing the performance of parsers when trained on small manually annotated datasets is of great importance, both when the seed and test data are taken 616 from the same domain (the in-domain scenario) and when they are taken from different domains (the outof-domain or parser adaptation scenario). Since the problem is</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak, 2000. A maximum-entropy-inspired parser. ANLP ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<journal>ACL</journal>
<volume>05</volume>
<contexts>
<context position="1296" citStr="Charniak and Johnson, 2005" startWordPosition="200" endWordPosition="203">rovement both when the seed and test data are in the same domain and in the outof-domain adaptation scenario. In particular, we achieve 50% reduction in annotation cost for the in-domain case, yielding an improvement of 66% over previous work, and a 20-33% reduction for the domain adaptation case. This is the first time that self-training with small labeled datasets is applied successfully to these tasks. We were also able to formulate a characterization of when selftraining is valuable. 1 Introduction State of the art statistical parsers (Collins, 1999; Charniak, 2000; Koo and Collins, 2005; Charniak and Johnson, 2005) are trained on manually annotated treebanks that are highly expensive to create. Furthermore, the performance of these parsers decreases as the distance between the genres of their training and test data increases. Therefore, enhancing the performance of parsers when trained on small manually annotated datasets is of great importance, both when the seed and test data are taken 616 from the same domain (the in-domain scenario) and when they are taken from different domains (the outof-domain or parser adaptation scenario). Since the problem is the expense in manual annotation, we define ‘small’</context>
<context position="4843" citStr="Charniak and Johnson, 2005" startWordPosition="763" endWordPosition="766">the results in an attempt to shed light on the phenomenon of selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features. McClosky et al (2006a) use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data. They train the PCFG parser and the reranker with the manually annotated WSJ data, and parse the NANC data with the 50-best PCFG parser. Then they proceed in two directi</context>
<context position="7039" citStr="Charniak and Johnson, 2005" startWordPosition="1146" endWordPosition="1149">rser itself and enhance its performance, without using any reranker. This reveals a noteworthy characteristic of generative PCFG models and offers a potential direction for parser improvement, since the quality of a parser-reranker combination critically depends on that of the base parser. Second, these papers did not explore self-training when the seed is small, a scenario whose importance has been discussed above. In general, PCFG models trained on small datasets are less likely to parse the self-training data correctly. For example, the fscore of WSJ data parsed by the base PCFG parser of (Charniak and Johnson, 2005) when trained on the training sections of WSJ is between 89% to 90%, while the f-score of WSJ data parsed with the Collins’ model that we use, and a small seed, is between 40% and 80%. As a result, the good results of (McClosky et al, 2006a; 2006b) with large seed sets do not immediately imply success with small seed sets. Demonstration of such success is a contribution of the present paper. Bacchiani et al (2006) explored the scenario of out-of-domain seed data (the Brown training set containing about 20K sentences) and in-domain self-training data (between 4K to 200K sentences from the WSJ) </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson, 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James Curran</author>
<author>Miles Osborne</author>
</authors>
<title>Bootstrapping pos taggers using unlabelled data.</title>
<date>2003</date>
<journal>CoNLL</journal>
<volume>03</volume>
<contexts>
<context position="4441" citStr="Clark et al., 2003" startWordPosition="702" endWordPosition="705">eds in adapting a generative parser between domains using a small manually annotated dataset. • The first formulation (related to the number of unknown words in a sentence) of when selftraining is valuable. Section 2 discusses previous work, and Section 3 compares in-depth our protocol to a previous one. Sections 4 and 5 present the experimental setup and our results, and Section 6 analyzes the results in an attempt to shed light on the phenomenon of selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second la</context>
</contexts>
<marker>Clark, Curran, Osborne, 2003</marker>
<rawString>Stephen Clark, James Curran, and Miles Osborne, 2003. Bootstrapping pos taggers using unlabelled data. CoNLL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Cohn</author>
<author>Les Atlas</author>
<author>Richard E Ladner</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>1994</date>
<booktitle>Machine Learning,</booktitle>
<volume>15</volume>
<issue>2</issue>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>David A. Cohn, Les Atlas, and Richard E. Ladner, 1994. Improving generalization with active learning. Machine Learning, 15(2):201–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1228" citStr="Collins, 1999" startWordPosition="192" endWordPosition="193">anually annotated seed data. We report significant improvement both when the seed and test data are in the same domain and in the outof-domain adaptation scenario. In particular, we achieve 50% reduction in annotation cost for the in-domain case, yielding an improvement of 66% over previous work, and a 20-33% reduction for the domain adaptation case. This is the first time that self-training with small labeled datasets is applied successfully to these tasks. We were also able to formulate a characterization of when selftraining is valuable. 1 Introduction State of the art statistical parsers (Collins, 1999; Charniak, 2000; Koo and Collins, 2005; Charniak and Johnson, 2005) are trained on manually annotated treebanks that are highly expensive to create. Furthermore, the performance of these parsers decreases as the distance between the genres of their training and test data increases. Therefore, enhancing the performance of parsers when trained on small manually annotated datasets is of great importance, both when the seed and test data are taken 616 from the same domain (the in-domain scenario) and when they are taken from different domains (the outof-domain or parser adaptation scenario). Sinc</context>
<context position="2739" citStr="Collins, 1999" startWordPosition="432" endWordPosition="433">odel is first trained using manually annotated (‘seed’) data, then the model is used to automatically annotate a pool of unannotated (‘self-training’) data, and then the manually and automatically annotated datasets are combined to create the training data for the final model. Self-training of parsers trained on small datasets is of enormous potential practical importance, due to the huge amounts of unannotated data that are becoming available today and to the high cost of manual annotation. In this paper we use self-training to enhance the performance of a generative statistical PCFG parser (Collins, 1999) for both the in-domain and the parser adaptation scenarios, using only small amounts of manually annotated data. We perform four experiments, examining all combinations of in-domain and out-of-domain seed and self-training data. Our results show that self-training is of substantial benefit for the problem. In particular, we present: • 50% reduction in annotation cost when the seed and test data are taken from the same domain, which is 66% higher than any previous result with small manually annotated datasets. 1We note in passing that quantitative research on the cost of annotation using vario</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins, 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Miles Osborne</author>
<author>Anoop Sarkar</author>
<author>Mark Steedman</author>
</authors>
<title>Corrected co-training for statistical parsers.</title>
<date>2003</date>
<booktitle>In ICML ’03, Workshop on the Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining.</booktitle>
<contexts>
<context position="10723" citStr="Hwa et al, (2003)" startWordPosition="1757" endWordPosition="1760">differences between tocol prefers to use many sentences without checkthe confidence scores of the two parsers. In this pa- ing their parse quality. Third, their protocol is iteraper we show a self-training protocol that achieves tive while ours uses a single step. Fourth, our selfbetter results than all of these methods (Table 2). training set is orders of magnitude larger than theirs. The next section discusses possible explanations for To examine the parse quality issue, we performed the difference in results. Steedman et al (2003b) and their experiment using their setting but selecting the Hwa et al, (2003) also used several versions of cor- high quality parse sentences using their f-score relrected co-training which are not comparable to ours ative to the gold standard annotation from secs 2- and other suggested methods because their evalua- 21 rather than a quality estimate. No improvement tion requires different measures (e.g. reviewed and over the baseline was achieved even with this orcorrected constituents are separately counted). acle. Thus the problem with their protocol does As far as we know, (Becker and Osborne, 2005) not lie with the parse quality assessment function; is the only add</context>
</contexts>
<marker>Hwa, Osborne, Sarkar, Steedman, 2003</marker>
<rawString>Rebecca Hwa, Miles Osborne, Anoop Sarkar and Mark Steedman, 2003. Corrected co-training for statistical parsers. In ICML ’03, Workshop on the Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample selection for statistical parsing.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<marker>Hwa, 2004</marker>
<rawString>Rebecca Hwa, 2004. Sample selection for statistical parsing. Computational Linguistics, 30(3):253–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Hidden-variable models for discriminative reranking.</title>
<date>2005</date>
<journal>EMNLP</journal>
<volume>05</volume>
<contexts>
<context position="1267" citStr="Koo and Collins, 2005" startWordPosition="196" endWordPosition="199"> report significant improvement both when the seed and test data are in the same domain and in the outof-domain adaptation scenario. In particular, we achieve 50% reduction in annotation cost for the in-domain case, yielding an improvement of 66% over previous work, and a 20-33% reduction for the domain adaptation case. This is the first time that self-training with small labeled datasets is applied successfully to these tasks. We were also able to formulate a characterization of when selftraining is valuable. 1 Introduction State of the art statistical parsers (Collins, 1999; Charniak, 2000; Koo and Collins, 2005; Charniak and Johnson, 2005) are trained on manually annotated treebanks that are highly expensive to create. Furthermore, the performance of these parsers decreases as the distance between the genres of their training and test data increases. Therefore, enhancing the performance of parsers when trained on small manually annotated datasets is of great importance, both when the seed and test data are taken 616 from the same domain (the in-domain scenario) and when they are taken from different domains (the outof-domain or parser adaptation scenario). Since the problem is the expense in manual </context>
<context position="4897" citStr="Koo and Collins, 2005" startWordPosition="772" endWordPosition="775">f selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features. McClosky et al (2006a) use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data. They train the PCFG parser and the reranker with the manually annotated WSJ data, and parse the NANC data with the 50-best PCFG parser. Then they proceed in two directions. In the first, they reorder the 50-best parse list</context>
</contexts>
<marker>Koo, Collins, 2005</marker>
<rawString>Terry Koo and Michael Collins, 2005. Hidden-variable models for discriminative reranking. EMNLP ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<journal>HLTNAACL</journal>
<volume>06</volume>
<contexts>
<context position="4679" citStr="McClosky et al., 2006" startWordPosition="740" endWordPosition="743">work, and Section 3 compares in-depth our protocol to a previous one. Sections 4 and 5 present the experimental setup and our results, and Section 6 analyzes the results in an attempt to shed light on the phenomenon of selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features. McClosky et al (2006a) use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data. They</context>
<context position="7278" citStr="McClosky et al, 2006" startWordPosition="1194" endWordPosition="1197">itically depends on that of the base parser. Second, these papers did not explore self-training when the seed is small, a scenario whose importance has been discussed above. In general, PCFG models trained on small datasets are less likely to parse the self-training data correctly. For example, the fscore of WSJ data parsed by the base PCFG parser of (Charniak and Johnson, 2005) when trained on the training sections of WSJ is between 89% to 90%, while the f-score of WSJ data parsed with the Collins’ model that we use, and a small seed, is between 40% and 80%. As a result, the good results of (McClosky et al, 2006a; 2006b) with large seed sets do not immediately imply success with small seed sets. Demonstration of such success is a contribution of the present paper. Bacchiani et al (2006) explored the scenario of out-of-domain seed data (the Brown training set containing about 20K sentences) and in-domain self-training data (between 4K to 200K sentences from the WSJ) and showed an improvement over the baseline of training the parser with the seed data only. However, they did not explore the case of small seed datasets (the effort in manually annotating 20K is substantial) and their work addresses only </context>
<context position="13728" citStr="McClosky et al., 2006" startWordPosition="2261" endWordPosition="2264">m the seed dataset3. The seed data were taken from WSJ sections 2- 21. For II and IO, the test data is WSJ section 23 (2416 sentences) and the self-training data are either WSJ sections 2-21 (in II, excluding the seed sentences) or the Brown training section (in IO). For OI and OO, the test data is the Brown test section (2424 sentences), and the self-training data is either the Brown training section (in OI) or WSJ sections 2-21 (in OO). We removed the manual annotations from the self-training sections before using them. For the Brown corpus, we based our division on (Bacchiani et al., 2006; McClosky et al., 2006b). The test and training sections consist of sentences from all of the genres that form the corpus. The training division consists of 90% (9 of each 10 consecutive sentences) of the data, and the test section are the remaining 10% (We did not use any held out data). Parsing performance is measured by f-score, f = 2�� �� � �� , where P, R are labeled precision and recall. To further demonstrate our results for parser adaptation, we also performed the OI experiment where seed data is taken from WSJ sections 2-21 and both self-training and test data are taken from the Switchboard corpus. The dis</context>
<context position="27406" citStr="McClosky et al., 2006" startWordPosition="4580" endWordPosition="4583">decreases from almost 1 for a very small number of unknown words to about 0.55 for 50 unknown words. The lower curve shows that when the number of unknown words increases, the probability that the self-training model will do better than the baseline model increases from almost 0 (for a very small number of unknown words) to about 0.55. Hence, the number of unknown words is an indication for the potential benefit (value on the lower curve) and risk (1 minus the value on the upper curve) in using the self-training model compared to using the baseline model. Unknown words were not identified in (McClosky et al., 2006a) as a useful predictor for the benefit of self-training. 622 We also identified a length effect similar to that studied by (McClosky et al., 2006a) for self-training (using a reranker and large seed, as detailed in Section 2). Due to space limitations we do not discuss it here. 7 Discussion Self-training is usually not considered to be a valuable technique in improving the performance of generative statistical parsers, especially when the manually annotated seed sentence dataset is small. Indeed, in the II scenario, (Steedman et al., 2003a; McClosky et al., 2006a; Charniak, 1997) reported no</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson, 2006a. Effective self-training for parsing. HLTNAACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<journal>ACL-COLING</journal>
<volume>06</volume>
<contexts>
<context position="4679" citStr="McClosky et al., 2006" startWordPosition="740" endWordPosition="743">work, and Section 3 compares in-depth our protocol to a previous one. Sections 4 and 5 present the experimental setup and our results, and Section 6 analyzes the results in an attempt to shed light on the phenomenon of selftraining. 2 Related Work Self-training might seem a strange idea: why should a parser trained on its own output learn anything new? Indeed, (Clark et al., 2003) applied selftraining to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Recently, (McClosky et al., 2006a; McClosky et al., 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson, 2005). A reranking parser (see also (Koo and Collins, 2005)) is a layered model: the base layer is a generative statistical PCFG parser that creates a ranked list of k parses (say, 50), and the second layer is a reranker that reorders these parses using more detailed features. McClosky et al (2006a) use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data. They</context>
<context position="7278" citStr="McClosky et al, 2006" startWordPosition="1194" endWordPosition="1197">itically depends on that of the base parser. Second, these papers did not explore self-training when the seed is small, a scenario whose importance has been discussed above. In general, PCFG models trained on small datasets are less likely to parse the self-training data correctly. For example, the fscore of WSJ data parsed by the base PCFG parser of (Charniak and Johnson, 2005) when trained on the training sections of WSJ is between 89% to 90%, while the f-score of WSJ data parsed with the Collins’ model that we use, and a small seed, is between 40% and 80%. As a result, the good results of (McClosky et al, 2006a; 2006b) with large seed sets do not immediately imply success with small seed sets. Demonstration of such success is a contribution of the present paper. Bacchiani et al (2006) explored the scenario of out-of-domain seed data (the Brown training set containing about 20K sentences) and in-domain self-training data (between 4K to 200K sentences from the WSJ) and showed an improvement over the baseline of training the parser with the seed data only. However, they did not explore the case of small seed datasets (the effort in manually annotating 20K is substantial) and their work addresses only </context>
<context position="13728" citStr="McClosky et al., 2006" startWordPosition="2261" endWordPosition="2264">m the seed dataset3. The seed data were taken from WSJ sections 2- 21. For II and IO, the test data is WSJ section 23 (2416 sentences) and the self-training data are either WSJ sections 2-21 (in II, excluding the seed sentences) or the Brown training section (in IO). For OI and OO, the test data is the Brown test section (2424 sentences), and the self-training data is either the Brown training section (in OI) or WSJ sections 2-21 (in OO). We removed the manual annotations from the self-training sections before using them. For the Brown corpus, we based our division on (Bacchiani et al., 2006; McClosky et al., 2006b). The test and training sections consist of sentences from all of the genres that form the corpus. The training division consists of 90% (9 of each 10 consecutive sentences) of the data, and the test section are the remaining 10% (We did not use any held out data). Parsing performance is measured by f-score, f = 2�� �� � �� , where P, R are labeled precision and recall. To further demonstrate our results for parser adaptation, we also performed the OI experiment where seed data is taken from WSJ sections 2-21 and both self-training and test data are taken from the Switchboard corpus. The dis</context>
<context position="27406" citStr="McClosky et al., 2006" startWordPosition="4580" endWordPosition="4583">decreases from almost 1 for a very small number of unknown words to about 0.55 for 50 unknown words. The lower curve shows that when the number of unknown words increases, the probability that the self-training model will do better than the baseline model increases from almost 0 (for a very small number of unknown words) to about 0.55. Hence, the number of unknown words is an indication for the potential benefit (value on the lower curve) and risk (1 minus the value on the upper curve) in using the self-training model compared to using the baseline model. Unknown words were not identified in (McClosky et al., 2006a) as a useful predictor for the benefit of self-training. 622 We also identified a length effect similar to that studied by (McClosky et al., 2006a) for self-training (using a reranker and large seed, as detailed in Section 2). Due to space limitations we do not discuss it here. 7 Discussion Self-training is usually not considered to be a valuable technique in improving the performance of generative statistical parsers, especially when the manually annotated seed sentence dataset is small. Indeed, in the II scenario, (Steedman et al., 2003a; McClosky et al., 2006a; Charniak, 1997) reported no</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson, 2006b. Reranking and self-training for parser adaptation. ACL-COLING ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Anoop Sarkar</author>
<author>Miles Osborne</author>
<author>Rebecca Hwa</author>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
<author>Paul Ruhlen</author>
<author>Steven Baker</author>
<author>Jeremiah Crim</author>
</authors>
<title>Bootstrapping statistical parsers from small datasets.</title>
<date>2003</date>
<journal>EACL</journal>
<volume>03</volume>
<contexts>
<context position="9006" citStr="Steedman et al (2003" startWordPosition="1475" endWordPosition="1478">the self-training set cononly). The seed, self-training and test datasets they tains several thousand sentences A parser trained used are similar to those we use in our II experi- with a small seed set parses the self-training set, and ment (see below), but the self-training protocols are then the whole automatically annotated self-training different. They first train the parser with the seed set is combined with the manually annotated seed sentences sampled from WSJ sections 2-21. Then, set to retrain the parser. This protocol and that of iteratively, 30 sentences are sampled from these sec- Steedman et al (2003a) were applied to the problem, tions, parsed by the parser, and the 20 best sentences with the same seed, self-training and test sets. As (in terms of parser confidence defined as probability we show below (see Section 4 and Section 5), while of top parse) are selected and combined with the pre- Steedman’s protocol does not improve over the baseviously annotated data to retrain the parser. The co- line of using only the seed data, our protocol does. training protocol is similar except that each parser There are four differences between the protocols. is trained with the 20 best sentences of t</context>
<context position="10644" citStr="Steedman et al (2003" startWordPosition="1744" endWordPosition="1747">s for sentences of high quality parse, while our prowere explored) considered the differences between tocol prefers to use many sentences without checkthe confidence scores of the two parsers. In this pa- ing their parse quality. Third, their protocol is iteraper we show a self-training protocol that achieves tive while ours uses a single step. Fourth, our selfbetter results than all of these methods (Table 2). training set is orders of magnitude larger than theirs. The next section discusses possible explanations for To examine the parse quality issue, we performed the difference in results. Steedman et al (2003b) and their experiment using their setting but selecting the Hwa et al, (2003) also used several versions of cor- high quality parse sentences using their f-score relrected co-training which are not comparable to ours ative to the gold standard annotation from secs 2- and other suggested methods because their evalua- 21 rather than a quality estimate. No improvement tion requires different measures (e.g. reviewed and over the baseline was achieved even with this orcorrected constituents are separately counted). acle. Thus the problem with their protocol does As far as we know, (Becker and Osb</context>
<context position="16098" citStr="Steedman et al., 2003" startWordPosition="2661" endWordPosition="2664"> results reported by (Steedman et al, 20003a; 2003b). As stated earlier, the experimental setup of these works is similar to ours, but the selftraining protocols are different. For self-training, our II improves an absolute 3.74% over their 74.3% result, which constitutes a 14.5% reduction in error (from 25.7%). The table shows that for both seed sizes our self training protocol outperforms both the selftraining and co-training protocols of (Steedman et al, 20003a; 2003b). Results are not included in the table only if they are not reported in the relevant paper. The self-training protocol of (Steedman et al., 2003a) does not actually improve over the baseline of using only the seed data. Section 3 discussed a possible explanation to the difference in results. In Table 3 we compare our results to the results of the methods tested in (Becker and Osborne, 2005) (including TE)4. To do that, we compare the reduction in manually annotated data needed to achieve an f-score value of 80 on WSJ section 23 achieved by each method. We chose this measure since it is 3 (Steedman et al., 2003a) used the first 500 sentences of WSJ training section as seed data. For direct comparison, we performed our protocol in the I</context>
<context position="23340" citStr="Steedman et al., 2003" startWordPosition="3876" endWordPosition="3879">o show by how much selftraining improves the out-of-domain baseline. The table stops at an f-score of 74 because that is the best that the baselines can do. A significant reduction in annotation cost over the ID baseline is achieved where the seed size is between 100 and 1200. Improvement over the OD baseline is for the whole range of seed sizes. Both OO and OI achieve 20-33% reduction in manual annotation compared to the ID baseline and enhance the performance of the parser by as much as 42.9%. The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al., 2003a), which used co-training (no self-training results were reported there or elsewhere). In order to compare with that work, we performed OI with seed taken from the Brown corpus and self-training and test taken from WSJ, which is the setup they use, obtaining a similar improvement to that reported there. However, co-training is a more complex method that requires an additional parser (LTAG in their case). To further substantiate our results for the parser adaptation scenario, we used an additional corpus, Switchboard. Figure 4 shows the results of an OI experiment with WSJ seed and Switchboard</context>
<context position="27952" citStr="Steedman et al., 2003" startWordPosition="4672" endWordPosition="4675">baseline model. Unknown words were not identified in (McClosky et al., 2006a) as a useful predictor for the benefit of self-training. 622 We also identified a length effect similar to that studied by (McClosky et al., 2006a) for self-training (using a reranker and large seed, as detailed in Section 2). Due to space limitations we do not discuss it here. 7 Discussion Self-training is usually not considered to be a valuable technique in improving the performance of generative statistical parsers, especially when the manually annotated seed sentence dataset is small. Indeed, in the II scenario, (Steedman et al., 2003a; McClosky et al., 2006a; Charniak, 1997) reported no improvement of the base parser for small (500 sentences, in the first paper) and large (40K sentences, in the last two papers) seed datasets respectively. In the II, OO, and OI scenarios, (McClosky et al, 2006a; 2006b) succeeded in improving the parser performance only when a reranker was used to reorder the 50-best list of the generative parser, with a seed size of 40K sentences. Bacchiani et al (2006) improved the parser performance in the OI scenario but their seed size was large (about 20K sentences). In this paper we have shown that s</context>
</contexts>
<marker>Steedman, Sarkar, Osborne, Hwa, Clark, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>Mark Steedman, Anoop Sarkar, Miles Osborne, Rebecca Hwa, Stephen Clark, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim, 2003a. Bootstrapping statistical parsers from small datasets. EACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Rebecca Hwa</author>
<author>Stephen Clark</author>
<author>Miles Osborne</author>
<author>Anoop Sarkar</author>
<author>Julia Hockenmaier</author>
<author>Paul Ruhlen</author>
<author>Steven Baker</author>
<author>Jeremiah Crim</author>
</authors>
<title>Example selection for bootstrapping statistical parsers.</title>
<date>2003</date>
<journal>NAACL</journal>
<volume>03</volume>
<contexts>
<context position="9006" citStr="Steedman et al (2003" startWordPosition="1475" endWordPosition="1478">the self-training set cononly). The seed, self-training and test datasets they tains several thousand sentences A parser trained used are similar to those we use in our II experi- with a small seed set parses the self-training set, and ment (see below), but the self-training protocols are then the whole automatically annotated self-training different. They first train the parser with the seed set is combined with the manually annotated seed sentences sampled from WSJ sections 2-21. Then, set to retrain the parser. This protocol and that of iteratively, 30 sentences are sampled from these sec- Steedman et al (2003a) were applied to the problem, tions, parsed by the parser, and the 20 best sentences with the same seed, self-training and test sets. As (in terms of parser confidence defined as probability we show below (see Section 4 and Section 5), while of top parse) are selected and combined with the pre- Steedman’s protocol does not improve over the baseviously annotated data to retrain the parser. The co- line of using only the seed data, our protocol does. training protocol is similar except that each parser There are four differences between the protocols. is trained with the 20 best sentences of t</context>
<context position="10644" citStr="Steedman et al (2003" startWordPosition="1744" endWordPosition="1747">s for sentences of high quality parse, while our prowere explored) considered the differences between tocol prefers to use many sentences without checkthe confidence scores of the two parsers. In this pa- ing their parse quality. Third, their protocol is iteraper we show a self-training protocol that achieves tive while ours uses a single step. Fourth, our selfbetter results than all of these methods (Table 2). training set is orders of magnitude larger than theirs. The next section discusses possible explanations for To examine the parse quality issue, we performed the difference in results. Steedman et al (2003b) and their experiment using their setting but selecting the Hwa et al, (2003) also used several versions of cor- high quality parse sentences using their f-score relrected co-training which are not comparable to ours ative to the gold standard annotation from secs 2- and other suggested methods because their evalua- 21 rather than a quality estimate. No improvement tion requires different measures (e.g. reviewed and over the baseline was achieved even with this orcorrected constituents are separately counted). acle. Thus the problem with their protocol does As far as we know, (Becker and Osb</context>
<context position="16098" citStr="Steedman et al., 2003" startWordPosition="2661" endWordPosition="2664"> results reported by (Steedman et al, 20003a; 2003b). As stated earlier, the experimental setup of these works is similar to ours, but the selftraining protocols are different. For self-training, our II improves an absolute 3.74% over their 74.3% result, which constitutes a 14.5% reduction in error (from 25.7%). The table shows that for both seed sizes our self training protocol outperforms both the selftraining and co-training protocols of (Steedman et al, 20003a; 2003b). Results are not included in the table only if they are not reported in the relevant paper. The self-training protocol of (Steedman et al., 2003a) does not actually improve over the baseline of using only the seed data. Section 3 discussed a possible explanation to the difference in results. In Table 3 we compare our results to the results of the methods tested in (Becker and Osborne, 2005) (including TE)4. To do that, we compare the reduction in manually annotated data needed to achieve an f-score value of 80 on WSJ section 23 achieved by each method. We chose this measure since it is 3 (Steedman et al., 2003a) used the first 500 sentences of WSJ training section as seed data. For direct comparison, we performed our protocol in the I</context>
<context position="23340" citStr="Steedman et al., 2003" startWordPosition="3876" endWordPosition="3879">o show by how much selftraining improves the out-of-domain baseline. The table stops at an f-score of 74 because that is the best that the baselines can do. A significant reduction in annotation cost over the ID baseline is achieved where the seed size is between 100 and 1200. Improvement over the OD baseline is for the whole range of seed sizes. Both OO and OI achieve 20-33% reduction in manual annotation compared to the ID baseline and enhance the performance of the parser by as much as 42.9%. The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al., 2003a), which used co-training (no self-training results were reported there or elsewhere). In order to compare with that work, we performed OI with seed taken from the Brown corpus and self-training and test taken from WSJ, which is the setup they use, obtaining a similar improvement to that reported there. However, co-training is a more complex method that requires an additional parser (LTAG in their case). To further substantiate our results for the parser adaptation scenario, we used an additional corpus, Switchboard. Figure 4 shows the results of an OI experiment with WSJ seed and Switchboard</context>
<context position="27952" citStr="Steedman et al., 2003" startWordPosition="4672" endWordPosition="4675">baseline model. Unknown words were not identified in (McClosky et al., 2006a) as a useful predictor for the benefit of self-training. 622 We also identified a length effect similar to that studied by (McClosky et al., 2006a) for self-training (using a reranker and large seed, as detailed in Section 2). Due to space limitations we do not discuss it here. 7 Discussion Self-training is usually not considered to be a valuable technique in improving the performance of generative statistical parsers, especially when the manually annotated seed sentence dataset is small. Indeed, in the II scenario, (Steedman et al., 2003a; McClosky et al., 2006a; Charniak, 1997) reported no improvement of the base parser for small (500 sentences, in the first paper) and large (40K sentences, in the last two papers) seed datasets respectively. In the II, OO, and OI scenarios, (McClosky et al, 2006a; 2006b) succeeded in improving the parser performance only when a reranker was used to reorder the 50-best list of the generative parser, with a seed size of 40K sentences. Bacchiani et al (2006) improved the parser performance in the OI scenario but their seed size was large (about 20K sentences). In this paper we have shown that s</context>
</contexts>
<marker>Steedman, Hwa, Clark, Osborne, Sarkar, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>Mark Steedman, Rebecca Hwa, Stephen Clark, Miles Osborne, Anoop Sarkar, Julia Hockenmaier, Paul Ruhlen,Steven Baker, and Jeremiah Crim, 2003b. Example selection for bootstrapping statistical parsers. NAACL ’03.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>